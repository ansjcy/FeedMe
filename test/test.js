"use strict";
(self.webpackChunk_N_E = self.webpackChunk_N_E || []).push([[16], {
    8016: e => {
        e.exports = JSON.parse('{"sourceUrl":"https://slack.engineering/feed/","title":"Engineering at Slack","description":"Hear directly from Slack\'s engineers about what we build, why and how we build it, and how you can use it.","link":"https://slack.engineering","items":[{"title":"优化我们的端到端流水线 (原标题: Optimizing Our E2E Pipeline)","link":"https://slack.engineering/speedup-e2e-testing/","pubDate":"Mon, 14 Apr 2025 09:00:30 +0000","isoDate":"2025-04-14T09:00:30.000Z","content":"<p>In the world of DevOps and Developer Experience (DevXP), speed and efficiency can make a big difference on an engineer’s day-to-day tasks. Today, we’ll dive into how Slack’s DevXP team took some existing tools and used them to optimize an end-to-end (E2E) testing pipeline. This lowered build times and reduced redundant processes, saving both time&#8230;</p>\\n<p>The post <a rel=\\"nofollow\\" href=\\"https://slack.engineering/speedup-e2e-testing/\\">Optimizing Our E2E Pipeline</a> appeared first on <a rel=\\"nofollow\\" href=\\"https://slack.engineering\\">Engineering at Slack</a>.</p>\\n","contentSnippet":"In the world of DevOps and Developer Experience (DevXP), speed and efficiency can make a big difference on an engineer’s day-to-day tasks. Today, we’ll dive into how Slack’s DevXP team took some existing tools and used them to optimize an end-to-end (E2E) testing pipeline. This lowered build times and reduced redundant processes, saving both time…\\nThe post Optimizing Our E2E Pipeline appeared first on Engineering at Slack.","creator":"Dan Carton","encodedSnippet":"<p>In the world of DevOps and Developer Experience (DevXP), speed and efficiency can make a big difference on an engineer’s day-to-day tasks. Today, we’ll dive into how Slack’s DevXP team took some existing tools and used them to optimize an end-to-end (E2E) testing pipeline. This lowered build times and reduced redundant processes, saving both time&#8230;</p>\\n<p>The post <a rel=\\"nofollow\\" href=\\"https://slack.engineering/speedup-e2e-testing/\\">Optimizing Our E2E Pipeline</a> appeared first on <a rel=\\"nofollow\\" href=\\"https://slack.engineering\\">Engineering at Slack</a>.</p>\\n\\nIn the world of DevOps and Developer Experience (DevXP), speed and efficiency can make a big difference on an engineer’s day-to-day tasks. Today, we’ll dive into how Slack’s DevXP team took some existing tools and used them to optimize an end-to-end (E2E) testing pipeline. This lowered build times and reduced redundant processes, saving both time and resources for engineers at Slack.\\nThe Problem: Unnecessary Frontend Builds\\nFor one of our largest code repositories (a monolithic repository, or monorepo), Slack has a CI/CD pipeline that runs E2E tests before merging code into the <span style=\\"font-weight: 400\\">main</span> branch. This is critical for ensuring that changes are validated across the entire stack for the Slack application: frontend, backend, database, and the handful of services in between. However, we noticed a bottleneck: building the frontend code was taking longer than expected and occurred too frequently, even when there were no frontend-related changes. Here’s the breakdown:\\nDeveloper Workflow: A developer makes changes and pushes to a branch.\\nBuild Process: The frontend code is built (~5 minutes).\\nDeployment: The build is deployed to a QA environment.\\nTesting: We run over 200 E2E tests, taking another 5 minutes.\\nThis entire process took about 10 minutes per run. Half of that time, around 5 minutes, was consumed by frontend builds, even when no frontend changes were involved.\\n\\nGiven that hundreds of pull requests (PRs) are merged daily, these redundant builds were not only time-consuming, but costly:\\nThousands of frontend builds per week, storing nearly a gigabyte of data per build in AWS S3.\\n\\nHalf of these builds do not contain frontend changes compared to the last merge into <span style=\\"font-weight: 400\\">main</span>, causing terabytes of duplicate data.\\n5 minutes per build, adding unnecessary delays to pipelines (thousands of hours a week).\\nThe Solution: Smarter Build Strategy with Cached Frontend Assets\\nTo tackle this, we leveraged existing tools to rethink our build strategy.\\nStep 1: Conditional Frontend Builds\\nOur first step was determining whether a fresh frontend build was necessary. We detected changes by utilizing <span style=\\"font-weight: 400\\">git diff</span> and its 3-dot notation to identify the difference between the latest common commit of the current checked-out branch and <span style=\\"font-weight: 400\\">main</span>. If changes were detected, we invoke a frontend build job. If no changes were detected, we skipped the build entirely and reused a prebuilt version.\xa0\\nStep 2: Prebuilt Assets and Internal CDN\\nWhen a frontend build is not needed, we locate an existing build from AWS S3. To be efficient, we use a recent frontend build that is still in Production. We delegate the task of serving the prebuilt frontend assets for our E2E tests to an internal CDN. This reduced the need for creating a new build on each PR, while still ensuring we test on current assets.\\nThe Challenges: Efficiency at Scale\\nWhile the approach seemed straightforward, scaling this solution to our monorepo presented a few challenges:\\nIdentifying Frontend Changes: Our repository contains over 100,000 tracked files. Determining whether frontend changes were present required efficient file tracking, which git handled in just a couple of seconds.\xa0\\nFinding Prebuilt Assets: With hundreds of PRs merged into this repository daily, identifying a prebuilt version that was fresh enough required robust asset management. By using straightforward S3 storage concepts, we were able to balance recency, coherent file naming, and performance to manage our assets.\\nBeing Fast: We were able to distinguish if a frontend build was unnecessary and find a recent build artifact in just under 3 seconds on average.\\n\\n\\n\\nThe Results: A 60% Drop in Build Frequency and 50% Drop in Build Time\\nOur efforts paid off, delivering remarkable improvements:\\n60% Reduction in Build Frequency: By intelligently reusing prebuilt frontend assets, we reduced the number of unnecessary frontend builds by over half.\\nHundreds of Hours Saved Monthly: Cloud compute time and developer wait times are reduced.\\nSeveral Terabytes of Storage Savings: We reduced our AWS S3 storage by several terabytes each month. These duplicate assets would have otherwise been stored for one year.\\n50% Build Time Improvement: This was the second major project by the Frontend DevXP Team and its partnering teams. The first project, which upgraded our Webpack setup, reduced the average build from ~10 minutes to ~5 minutes. This project took the average build from ~5 minutes down to just ~2 minutes. With both projects being successful, we reduced our average build time for E2E pipelines from ~10 minutes to ~2 minutes: a huge improvement for the year!\\n\\nTwo unexpected outcomes:\\nMore Reliable and Trustworthy E2E Results: Our test flakiness, which refers to tests failing intermittently or inconsistently despite no code changes, was significantly reduced. This improvement resulted from the optimized pipeline, decreased likelihood of needing complex frontend builds, and consistent asset delivery. We observed our lowest percentage of test flakiness as a result, based on monthly measurements.\\nRediscovering Legacy Code: Implementing this optimization required a deep dive into legacy code of multiple systems that hadn’t been significantly modified in a long time. This exploration yielded valuable insights, prompted new questions about the codebase’s behavior, and generated a backlog of tasks for future enhancements.\\nConclusion: Rethinking Frontend Build Efficiency\\nBy strategically utilizing existing tools like <span style=\\"font-weight: 400\\">git diff</span> and internal CDNs, we managed to save valuable developer time, reduce cloud costs, and improve overall build efficiency.\\nFor teams in other companies facing similar bottlenecks in DevOps and DevXP, the lesson is to question what’s truly necessary in your pipeline and optimize accordingly. The improvement from this project seems obvious in hind-sight, but it’s common to overlook inefficiencies in systems that haven’t outright failed. In our case, rethinking how we handled frontend assets turned into a massive win for the organization.\\nAcknowledgments\\nThere are a lot of moving parts in a project like this: complex pipelines for building and testing, cloud infrastructure, an internal CDN, intricate build systems for frontend code, and existing custom setups throughout our entire system. It includes code written in Python, JavaScript, Bash, PHP/Hack, Rust, YAML, and Ruby. We achieved this without any downtime! Okay, almost. There was ten minutes of internal downtime for our deployment pipeline, but it was fixed pretty quickly.\\nThis work was not possible without contributions from:\\nAnirudh Janga, Josh Cartmell, Armin\xe9 Iradian, Anupama Jasthi, Matt Jennings, Zack Weeden, John Long, Issac Gerges, Andrew MacDonald, Vani Anantha and Dave Harrington\\nInterested in taking on interesting projects, making people’s work lives easier, or just building some pretty cool forms? We’re hiring!\xa0\\n Explore Opportunities\\n\\t\\t\\n\xa0\\n\xa0\\nThe post Optimizing Our E2E Pipeline appeared first on Engineering at Slack.","summary":"Slack 的 DevXP 团队成功优化了其端到端（E2E）测试流水线，显著提升了效率并降低了成本。\\\\n\\\\n**主要问题：**\\\\n*   针对大型单体仓库，E2E 测试前的前端构建耗时（约5分钟）且过于频繁，即使没有前端代码变更也进行构建。\\\\n*   这导致每次 E2E 运行总耗时约10分钟，并产生大量冗余数据（每月数 TB）和数千小时的额外等待时间。\\\\n\\\\n**解决方案：**\\\\n*   **条件式前端构建：** 利用 `git diff` 精确检测前端代码变更。若无变更，则跳过构建。\\\\n*   **预构建资产复用：** 若跳过构建，则从 AWS S3 获取并复用最新的生产环境预构建前端资产，通过内部 CDN 提供给 E2E 测试。\\\\n\\\\n**成果：**\\\\n*   前端构建频率降低60%。\\\\n*   每月节省数百小时的计算时间和开发者等待时间。\\\\n*   每月节省数 TB 的 AWS S3 存储空间。\\\\n*   E2E 流水线总构建时间从约10分钟大幅缩短至约2分钟。\\\\n*   意外收获：测试稳定性显著提升，并对遗留代码库进行了深入了解。\\\\n\\\\n**结论：**\\\\n文章强调，通过审视并优化流水线中不必要的环节，即使是看似微小的低效也能为组织带来巨大的时间与成本节约。","translated_title":"优化我们的端到端流水线"},{"title":"我们如何构建安全私密的企业搜索 (原标题: How we built enterprise search to be secure and private)","link":"https://slack.engineering/how-we-built-enterprise-search-to-be-secure-and-private/","pubDate":"Fri, 07 Mar 2025 01:14:37 +0000","isoDate":"2025-03-07T01:14:37.000Z","content":"<p>Many don’t know that “Slack” is in fact a backronym—it stands for “Searchable Log of all Communication and Knowledge”. And these days, it’s not just a searchable log: with Slack AI, Slack is now an intelligent log, leveraging the latest in generative AI to securely surface powerful, time-saving insights. We built Slack AI from the&#8230;</p>\\n<p>The post <a rel=\\"nofollow\\" href=\\"https://slack.engineering/how-we-built-enterprise-search-to-be-secure-and-private/\\">How we built enterprise search to be secure and private</a> appeared first on <a rel=\\"nofollow\\" href=\\"https://slack.engineering\\">Engineering at Slack</a>.</p>\\n","contentSnippet":"Many don’t know that “Slack” is in fact a backronym—it stands for “Searchable Log of all Communication and Knowledge”. And these days, it’s not just a searchable log: with Slack AI, Slack is now an intelligent log, leveraging the latest in generative AI to securely surface powerful, time-saving insights. We built Slack AI from the…\\nThe post How we built enterprise search to be secure and private appeared first on Engineering at Slack.","creator":"Ian Hoffman","encodedSnippet":"<p>Many don’t know that “Slack” is in fact a backronym—it stands for “Searchable Log of all Communication and Knowledge”. And these days, it’s not just a searchable log: with Slack AI, Slack is now an intelligent log, leveraging the latest in generative AI to securely surface powerful, time-saving insights. We built Slack AI from the&#8230;</p>\\n<p>The post <a rel=\\"nofollow\\" href=\\"https://slack.engineering/how-we-built-enterprise-search-to-be-secure-and-private/\\">How we built enterprise search to be secure and private</a> appeared first on <a rel=\\"nofollow\\" href=\\"https://slack.engineering\\">Engineering at Slack</a>.</p>\\n\\nMany don’t know that “Slack” is in fact a backronym—it stands for “Searchable Log of all Communication and Knowledge”. And these days, it’s not just a searchable log: with Slack AI, Slack is now an intelligent log, leveraging the latest in generative AI to securely surface powerful, time-saving insights. We built Slack AI from the ground up to be secure and private following principles that mirror our existing enterprise grade compliance standards:\\n\\n\\nCustomer data never leaves Slack.\\nWe do not train large language models (LLMs) on customer data.\\nSlack AI only operates on the data that the user can already see.\\nSlack AI integrates seamlessly with our existing enterprise grade compliance and security offerings.\\nNow, with enterprise search, Slack is more than a log of all content and knowledge inside Slack—it also includes knowledge from your key applications. Users can now surface up-to-date, relevant content that is permissioned to them directly in Slack’s search. We’re starting with Google Drive and GitHub, and you’ll see many more of your connected apps as the year goes on. With these new apps, Slack search and AI Answers are all the more powerful, pulling in context from across key tools to satisfy your queries.\\nWe built enterprise search to uphold the same Enterprise-grade security and privacy standards as Slack AI:\\nWe never store data from external sources in our databases.\\nExternal data and permissions are up to date with the external system.\\nUsers and admins must explicitly grant Slack access to external sources and may revoke that access at any time.\\nWe uphold the principle of least privilege, only requesting the authorizations we need to satisfy search queries.\\nThis blog post will explain how these principles guided the architecture of enterprise search.\\nHow enterprise search upholds the Slack AI principles\\nFirst, a refresher: how does Slack AI uphold our security principles?\\n\\n\\nSlack uses AWS to host closed-source large language models (LLMs) in an escrow VPC. This structure ensures that the model provider never has access to Slack customer data and customer data never leaves Slack’s trust boundary—whether it’s Slack messages, enterprise search results, or anything in between.\\nWe use Retrieval Augmented Generation (RAG) instead of training LLMs. Using RAG, we supply an LLM with only the content needed to complete the task. This content is permissioned to the user and only available to the LLM at runtime, meaning the LLM doesn’t retain any of your data, ever.\\nTo provide a private, permissions-aware AI product, Slack uses the requesting user’s Access Control List (ACL) to ensure that the LLM only receives data the user can already access in Slack.\\nFinally, we re-use all our existing compliance infrastructure (such as Encryption Key Management and International Data Residency) when storing and processing LLM-generated content. And we don’t even store Search Answer summaries—we just show them to the requesting user and immediately discard them.\\nEnterprise search is built atop Slack AI and benefits from many of the innovations we developed for Slack AI. We use the same LLMs in the same escrow VPC; we use RAG to avoid training LLMs on user data; and we don’t store Search Answers in the database (whether or not they contain external content). However, enterprise search adds a new twist. We can now provide permissioned content from external sources to the LLM and in your search results.\xa0\\nHow enterprise search upholds our security principles\\nWe never store data from external sources\\nWhen developing enterprise search, we decided not to store external source data in our database. Instead, we opted for a federated, real-time approach. Building atop Slack’s app platform, we use public search APIs from our partners to return the most up-to-date, permissioned results for a given user. Note that the Slack client may cache data between reloads to performantly serve product features like filtering and previews.\\n\\nExternal data and permissions are up to date\xa0\\nWhen searching for external data, it’s essential that we only fetch data which the user can access in the external system (this mirrors our Slack AI principle #3, “Slack AI only operates on the data that the user can already see”) and that this data is up to date.\\nUsing a real-time instead of an index-based approach helps us uphold this principle. Because we’re always fetching data from external sources in response to a user query, we never risk that data getting stale. There’s nothing stored on our side, so staleness simply isn’t possible.\xa0\\nBut how do we scope down queries to just data that the querying user can access in the external system? The Slack platform already provides powerful primitives for connecting external systems to Slack, chief among them being OAuth. The OAuth protocol allows a user to securely authorize Slack to take agreed-upon actions on their behalf, like reading files the user can access in the external system. By leveraging OAuth, we ensure that enterprise search can never perform an action the user did not authorize the system to perform in the external system, and that the actions we perform are a subset of those the user could themselves perform.\\nUsers must explicitly grant access to external sources\\nWe believe that your external data should be yours to control. As such, Slack admins must opt in each external source for use in their organization’s search results and Search Answers. They can also revoke this access (for both search results and Search Answers) at any time.\\nNext, Slack users also explicitly grant access before we integrate any external sources in their search. Users may also revoke access to any source at any time. This level of control is possible due to the OAuth-based approach mentioned above.\\nPrinciple of Least Privilege\\nAn important security principle is that a system should never request more privileges than it requires. For enterprise search, this means that when we connect to an external system, we only request the OAuth scopes which are necessary to satisfy search queries—specifically read scopes.\\nNot only do we adhere to the principle of least privilege, we show admins and end users the scopes we plan to request when they enable an external source for use in enterprise search. This means that admins and end users always know which authorizations Slack requires to integrate with an external source.\\nConclusion\\nAt Salesforce, trust is our #1 value. We’re proud to have built an enterprise search experience that puts security and privacy front and center, building atop the robust security principles already instilled by Slack AI. We’re excited to see how our customers use this powerful new functionality, secure in the knowledge that their external data is always in good hands.\\nThe post How we built enterprise search to be secure and private appeared first on Engineering at Slack.","summary":"# Slack企业搜索的安全与隐私构建\\\\n\\\\nSlack已从“所有通信和知识的可搜索日志”（Searchable Log of all Communication and Knowledge，即Slack的词源）发展为借助生成式AI提供智能洞察的“智能日志”，这得益于Slack AI的推出。在此基础上，Slack进一步推出了企业搜索功能，旨在将搜索范围扩展到外部应用，并严格遵循与Slack AI相同的企业级安全和隐私标准。\\\\n\\\\n## Slack AI的核心安全与隐私原则\\\\n\\\\nSlack AI的设计理念是确保用户数据的安全和隐私，主要体现在以下几点：\\\\n\\\\n*   **数据不出Slack边界：** 客户数据始终保留在Slack的信任边界内，通过在AWS托管的托管VPC（escrow VPC）中运行闭源大型语言模型（LLM），确保模型提供商无法访问客户数据。\\\\n*   **不使用客户数据训练LLM：** 采用检索增强生成（RAG）技术，仅在运行时向LLM提供用户有权访问的相关内容，LLM不会保留任何用户数据。\\\\n*   **权限感知：** Slack AI仅处理用户已有权限查看的数据，通过利用请求用户的访问控制列表（ACL）来确保数据隐私。\\\\n*   **复用现有合规基础设施：** 沿用现有的加密密钥管理和国际数据驻留等合规基础设施，且搜索答案摘要不会被存储，使用后即刻丢弃。\\\\n\\\\n## 企业搜索的安全与隐私实践\\\\n\\\\n企业搜索在继承Slack AI原则的基础上，针对外部数据源引入了额外的安全保障措施：\\\\n\\\\n*   **不存储外部数据：** 采用联邦式、实时的方法，不将外部数据存储在Slack的数据库中，而是通过公共搜索API实时从外部源获取最新、有权限的结果。\\\\n*   **外部数据与权限实时同步：** 实时获取数据确保了数据的时效性，避免了数据过时。通过OAuth协议，确保搜索结果仅包含用户在外部系统中拥有访问权限的数据。\\\\n*   **明确的用户授权：** 管理员需明确选择启用外部数据源，用户也需通过OAuth明确授权Slack访问其外部数据，并可随时撤销授权。\\\\n*   **最小权限原则：** 连接外部系统时，仅请求满足搜索查询所需的最小OAuth权限范围（即只读权限），并向管理员和用户透明地展示这些权限。\\\\n\\\\n## 总结\\\\n\\\\nSlack的企业搜索功能在设计和实现上将安全与隐私置于核心地位，通过继承和扩展Slack AI的强大安全原则，确保了用户在跨应用搜索时的数据安全和控制权。","translated_title":"我们如何构建安全私密的企业搜索"},{"title":"Slack 的自动化无障碍测试 (原标题: Automated Accessibility Testing at Slack)","link":"https://slack.engineering/automated-accessibility-testing-at-slack/","pubDate":"Tue, 07 Jan 2025 20:46:20 +0000","isoDate":"2025-01-07T20:46:20.000Z","content":"<p>At Slack, customer love is our first priority and accessibility is a core tenet of customer trust. We have our own Slack Accessibility Standards that product teams follow to guarantee their features are compliant with Web Content Accessibility Guidelines (WCAG). Our dedicated accessibility team supports developers in following these guidelines throughout the development process. We&#8230;</p>\\n<p>The post <a rel=\\"nofollow\\" href=\\"https://slack.engineering/automated-accessibility-testing-at-slack/\\">Automated Accessibility Testing at Slack</a> appeared first on <a rel=\\"nofollow\\" href=\\"https://slack.engineering\\">Engineering at Slack</a>.</p>\\n","contentSnippet":"At Slack, customer love is our first priority and accessibility is a core tenet of customer trust. We have our own Slack Accessibility Standards that product teams follow to guarantee their features are compliant with Web Content Accessibility Guidelines (WCAG). Our dedicated accessibility team supports developers in following these guidelines throughout the development process. We…\\nThe post Automated Accessibility Testing at Slack appeared first on Engineering at Slack.","creator":"nstormann","encodedSnippet":"<p>At Slack, customer love is our first priority and accessibility is a core tenet of customer trust. We have our own Slack Accessibility Standards that product teams follow to guarantee their features are compliant with Web Content Accessibility Guidelines (WCAG). Our dedicated accessibility team supports developers in following these guidelines throughout the development process. We&#8230;</p>\\n<p>The post <a rel=\\"nofollow\\" href=\\"https://slack.engineering/automated-accessibility-testing-at-slack/\\">Automated Accessibility Testing at Slack</a> appeared first on <a rel=\\"nofollow\\" href=\\"https://slack.engineering\\">Engineering at Slack</a>.</p>\\n\\nAt Slack, customer love is our first priority and accessibility is a core tenet of customer trust. We have our own Slack Accessibility Standards that product teams follow to guarantee their features are compliant with Web Content Accessibility Guidelines (WCAG). Our dedicated accessibility team supports developers in following these guidelines throughout the development process. We also frequently collaborate with external manual testers that specialize in accessibility.\xa0\\nIn 2022, we started to supplement Slack’s accessibility strategy by setting up automated accessibility tests for desktop to catch a subset of accessibility violations throughout the development process. At Slack, we see automated accessibility testing as a valuable addition to our broader testing strategy. This broader strategy also includes involving people with disabilities early in the design process, conducting design and prototype review with these users, and performing manual testing across all of the assistive technologies we support. Automated tools can overlook nuanced accessibility issues that require human judgment, such as screen reader usability. Additionally, these tools can also flag issues that don’t align with the product’s specific design considerations.\\nDespite that, we still felt there would be value in integrating an accessibility testing tool into our test frameworks as part of the overall, comprehensive testing strategy. Ideally, we were hoping to add another layer of support by integrating the accessibility validation directly into our existing frameworks so test owners could easily add checks, or better yet, not have to think about adding checks at all.\\nExploration and Limitations\xa0\\nUnexpected Complexities: Axe, Jest, and React Testing Library (RTL)\xa0\\nWe chose to work with Axe, a popular and easily configurable accessibility testing tool, for its extensive capabilities and compatibility with our current end-to-end (E2E) test frameworks. Axe checks against a wide variety of accessibility guidelines, most of which correspond to specific success criteria from WCAG, and it does so in a way that minimizes false positives.\\nInitially we explored the possibility of embedding Axe accessibility checks directly into our React Testing Library (RTL) framework. By wrapping RTL’s render method with a custom render function that included the Axe check, we could remove a lot of friction from the developer workflow. However, we immediately encountered an issue related to the way we’ve customized our Jest set up at Slack. Running accessibility checks through a separate Jest configuration worked, but would require developers to write tests specifically for accessibility, which we wanted to avoid. Reworking our custom Jest setup was deemed too tricky and not worth the time and resource investment, so we pivoted to focus on our Playwright framework.\\nThe Best Solution for Axe Checks: Playwright\\nWith Jest ruled out as a candidate for Axe, we turned to Playwright, the E2E test framework utilized at Slack. Playwright supports accessibility testing with Axe through the @axe-core/playwright package. Axe Core provides most of what you’ll need to filter and customize accessibility checks. It provides an exclusion method right out of the box, to prevent certain rules and selectors from being analyzed. It also comes with a set of accessibility tags to further specify the type of analysis to conduct (‘wcag2a‘, ‘wcag2aa‘, etc.).\xa0\\nOur initial goal was to “bake” accessibility checks directly into Playwright’s interaction methods, such as clicks and navigation, to automatically run Axe without requiring test authors to explicitly call it.\xa0\\nIn working towards that goal, we found that the main challenge with this approach stems from Playwright’s Locator object. The Locator object is designed to simplify interaction with page elements by managing auto-waiting, loading, and ensuring the element is fully interactable before any action is performed. This automatic behavior is integral to Playwright’s ability to maintain stable tests, but it complicated our attempts to embed Axe into the framework.\xa0\\nAccessibility checks should run when the entire page or key components are fully rendered, but Playwright’s Locator only ensures the readiness of individual elements, not the overall page. Modifying the Locator could lead to unreliable audits because accessibility issues might go undetected if checks were run at the wrong time.\\nAnother option, using deprecated methods like <span style=\\"font-weight: 400\\">waitForElement </span>to control when accessibility checks are triggered, was also problematic. These older methods are less optimized, causing performance degradation, potential duplication of errors, and conflicts with the abstraction model that Playwright follows.\xa0\\nSo while embedding Axe checks into Playwright’s core interaction methods seemed ideal, the complexity of Playwright’s internal mechanisms required us to explore some further solutions.\\n\\nCustomizations and Workarounds\xa0\\nTo circumvent the roadblocks we encountered with embedding accessibility checks into the frameworks, we decided to make some concessions while still prioritizing a simplified developer workflow. We continued to focus on Playwright because it offered more flexibility in how we could selectively hide or apply accessibility checks, allowing us to more easily manage when and where these checks were run. Additionally, Axe Core came with some great customization features, such as filtering rules and using specific accessibility tags.\xa0\\nUsing the @axe-core/playwright package, we can describe the flow of our accessibility check:\\nPlaywright test lands on a page/view\\nAxe analyzes the page\\nPre-defined exclusions are filtered out\\nViolations and artifacts are saved to a file\xa0\\nFirst, we set up our main function, <span style=\\"font-weight: 400\\">runAxeAndSaveViolations</span>, and customized the scope using what the AxeBuilder class provides.\xa0\\nWe wanted to check for compliance with WCAG 2.1, Levels A and AA.\xa0\\nconstructor(page: Page) { \\n     this.page = page; \\n     this.defaultTags = [\'wcag2a\', \'wcag2aa\', \'wcag21a\', \'wcag21aa\']; \\n     this.bodyText = \'\'; \\n     this.baseFileName = `${test.info().title}-violations`.replace(/\\\\//g, \'-\'); \\n     A11y.filenameCounter = 0; \\n}\\nWe created a list of selectors to exclude from our violations report. These fell into two main categories:\\n\\n\\n\\nKnown accessibility issues – issues that we are aware of and have already been ticketed\xa0\\nRules that don’t apply – Axe rules outside of the scope of how Slack is designed for accessibility\\n// Exclude selectors for known bugs and elements that we do not consider accessibility issues\\nconstants.ACCESSIBILITY.AXE_EXCLUDED_SELECTORS.forEach((excludedSelector) => {\\n     axe.exclude(excludedSelector);\\n});\\nWe also wanted to filter for duplication and severity level. We created methods to check for the uniqueness of each violation and filter out duplication. We chose to report only the violations deemed Critical according to the WCAG. Serious, Moderate, and Mild are other possible severity levels that we may add in the future.\\n/**\\n* Filter violations based on criticality, then ensure we\\n* are removing any duplicate violations within a single test file\\n* Please note: this only removes duplicates on a single test, not the entire run\\n*/\\nprivate filterAndRemoveDuplicateViolations(violations: Violation[]) {\\n   return violations\\n\\t.filter((violation) => [\'critical\'].includes(violation.impact))\\n\\t.map(this.mapViolation)\\n\\t.filter(this.isUniqueViolation.bind(this));\\n}\\nWe took advantage of the Playwright fixture model. Fixtures are Playwright’s way to build up and teardown state outside of the test itself. Within our framework we’ve created a custom fixture called <span style=\\"font-weight: 400\\">slack</span> which provides access to all of our API calls, UI views, workflows and utilities related to Slack. Using this fixture, we can access all of these resources directly in our tests without having to go through the setup process every time.\\nWe moved our accessibility helper to be part of the pre-existing <span style=\\"font-weight: 400\\">slack</span> fixture. This allowed us to call it directly in the test spec, minimizing some of the overhead for our test authors.\\n// Run accessibility checks and get the violations\\nawait slack.utils.a11y.runAxeAndSaveViolations();\\nWe also took advantage of the ability to customize Playwright’s <span style=\\"font-weight: 400\\">test.step</span>. We added the custom label “Running accessibility checks in runAxeAndSaveViolations” to make it easier to detect where an accessibility violation has occurred:\\nTest Steps\\n- Before Hooks\\n- apiResponse.json— ../support/api/api.ts:137\\n- browserContext.waitForEvent— ../support/workflows/login.workflow.ts:273\\n- Running accessibility checks in runAxeAndSaveViolations— ../support/utils/accessibility.ts:54\\n\xa0\\nPlacement of Accessibility Checks in End to End Tests\\nTo kick the project off, we set up a test suite that mirrored our suite for testing critical functionality at Slack. We renamed the suite to make it clear it was for accessibility tests, and we set it to run as non-blocking. This meant developers would see the test results, but a failure or violation would not prevent them from merging their code to production. This initial suite encompassed 91 tests in total.\\nStrategically, we considered the placement of accessibility checks within these critical flow tests. In general, we aimed to add an accessibility check for each new view, page, or flow covered in the test. In most cases, this meant placing a check directly after a button click, for example, or a link that leads to navigation. In other scenarios, our accessibility check needed to be placed after signing in as a second user or after a redirect.\xa0\\nIt was important to make sure the same view wasn’t being analyzed twice in one test, or potentially twice across multiple tests with the same UI flow. Duplication like this would result in unnecessary error messages and saved artifacts, and slow down our tests. We were also careful to place our Axe calls only after the page or view had fully loaded and all content had rendered.\\nWith this approach, we needed to be deeply familiar with the application and the context of each test case.\xa0\\n\\nViolations Reporting\\nWe spent some time iterating on our accessibility violations report. Initially, we created a simple text file to save the results of a local run, storing it in an artifacts folder. A few developers gave us early feedback and requested screenshots of the pages where accessibility violations occurred. To achieve this, we integrated Playwright’s screenshot functionality and began saving these screenshots alongside our text report in the same artifact folder.\\nTo make our reports more coherent and readable, we leveraged the Playwright HTML Reporter. This tool not only aggregates test results but also allows us to attach artifacts such as screenshots and violation reports to the HTML output. By configuring the HTML reporter, we were able to display all of our accessibility artifacts, including screenshots and detailed violation reports, in a single test report.\xa0\\nLastly, we wanted our violation error message to be helpful and easy to understand, so we wrote some code to pull out key pieces of information from the violation. We also customized how the violations were displayed in the reports and on the console, by parsing and condensing the error message.\xa0\xa0\xa0\\nError - [A11Y]: CRITICAL \\nDescription: Ensures an element\'s role supports its ARIA attributes \\nHelp: Elements must only use supported ARIA attributes Target selector: #add-channel-tab \\nFix all of the following:\\nARIA attribute is not allowed: aria-selected=\\"false\\" \\nHTML: <button class=\\"c-button-unstyled addTab__brBMy c-tabs__tab js-tab\\" data-qa=\\"unstyled-button\\"\\n\xa0\\nEnvironment Setup and Running Tests\xa0\\nOnce we had integrated our Axe checks and set up our test suite, we needed to determine how and when developers should run them. To streamline the process for developers, we introduced an environment flag, <span style=\\"font-weight: 400\\">A11Y_ENABLE</span>, to control the activation of accessibility checks within our framework. By default, we set the flag to false, preventing unnecessary runs.\xa0\\nThis setup allowed us to offer developers the following options:\\nOn-Demand Testing: Developers can manually enable the flag when they need to run accessibility checks locally on their branch.\\nScheduled Runs: Developers can configure periodic runs during off-peak hours. We have a daily regression run configured in Buildkite to pipe accessibility test run results into a Slack alert channel on a daily cadence.\\nCI Integration: Optionally, the flag can be enabled in continuous integration pipelines for thorough testing before merging significant changes.\xa0\\nTriage and Ownership\\nOwnership of individual tests and test suites is often a hot topic when it comes to maintaining tests. Once we had added Axe calls to the critical flows in our Playwright E2E tests, we needed to decide who would be responsible for triaging accessibility issues discovered via our automation and who would own the test maintenance for existing tests.\xa0\\nAt Slack, we enable developers to own test creation and maintenance for their tests. To support developers to better understand the framework changes and new accessibility automation, we created documentation and partnered with the internal Slack accessibility team to come up with a comprehensive triage process that would fit into their existing workflow for triaging accessibility issues.\xa0\\nThe internal accessibility team at Slack had already established a process for triaging and labeling incoming accessibility issues, using the internal Slack Accessibility Standards as a guideline. To enhance the process, we created a new label for “automated accessibility” so we could track the issues discovered via our automation.\\nTo make cataloging these issues easier, we set up a Jira workflow in our alerts channel that would spin up a Jira ticket with a pre-populated template. The ticket is created via the workflow and automatically labeled with automated accessibility and placed in a Jira Epic for triaging.\\nA11Y Automation Bug Ticket Creator - \\nAutomatically create JIRA bug tickets for A11Y automation violations\\n\\nHi there, Would you like to create a new JIRA defect? \\nButton clicked. \\nA new JIRA bug ticket, A11YAUTO-37, was created. \\n\\nWhat to do next: \\n1. Please fill out all of the necessary information listed here: \\nhttps://jira.tinyspeck.com/browse/A11YAUTO-37. \\n\\n2. Please add this locator to the list of known issues \\nand include the new JIRA bug ticket in the comment.\\nConducting Audits\xa0\\nWe perform regular audits of our accessibility Playwright calls to check for duplication of Axe calls, and ensure proper coverage of accessibility checks across tests and test suites.\\nWe developed a script and an environment flag specifically to facilitate the auditing process. Audits can be performed either through sandbox test runs (ideal for suite-wide audits) or locally (for specific tests or subsets). When performing an audit, running the script allows us to take a screenshot of every page that performs an Axe call. The screenshots are then saved to a folder and can be easily compared to spot duplicates.\xa0\\nThis process is more manual than we like, and we are looking into ways to eliminate this step, potentially leaning on AI assistance to perform the audit for us – or have AI add our accessibility calls to each new page/view, thereby eliminating the need to perform any kind of audit at all.\\nWhat’s Next\xa0\\nWe plan to continue partnering with the internal accessibility team at Slack to design a small blocking test suite. These tests will be dedicated to the flows of core features within Slack, with a focus on keyboard navigation.\xa0\\nWe’d also like to explore AI-driven approaches to the post-processing of accessibility test results and look into the option of having AI assistants audit our suites to determine the placement of our accessibility checks, further reducing the manual effort for developers.\xa0\\nClosing Thoughts\xa0\\nWe had to make some unexpected trade-offs in this project, balancing the practical limitations of automated testing tools with the goal of reducing the burden on developers. While we couldn’t integrate accessibility checks completely into our frontend frameworks, we made significant strides towards that goal. We simplified the process for developers to add accessibility checks, ensured test results were easy to interpret, provided clear documentation, and streamlined triage through Slack workflows. In the end, we were able to add test coverage for accessibility in the Slack product, ensuring that our customers that require accessibility features have a consistent experience.\\nOur automated Axe checks have reduced our reliance on manual testing and now complement other essential forms of testing—like manual testing and usability studies. At the moment, developers need to manually add checks, but we’ve laid the groundwork to make the process as straightforward as possible with the possibility for AI-driven creation of accessibility tests.\\nRoadblocks like framework complexity or setup difficulties shouldn’t discourage you from pursuing automation as part of a broader accessibility strategy. Even when it’s not feasible to hide the automated checks completely behind the scenes of the framework, there are ways to make the work impactful by focusing on the developer experience. This project has not only strengthened our overall accessibility testing approach, it’s also reinforced the culture of accessibility that has always been central to Slack. We hope it inspires others to look more closely at how automated accessibility might fit into your testing strategy, even if it requires navigating a few technical hurdles along the way.\\n\xa0\\nThank you to everyone who spent significant time on the editing and revision of this blog post – Courtney Anderson-Clark, Lucy Cheng, Miriam Holland and Sergii Gorbachov.\\nAnd a massive thank you to the Accessibility Team at Slack, Chanan Walia, Yura Zenevich, Chris Xu and Hye Jung Choi, for your help with everything related to this project, including editing this blog post!\\nThe post Automated Accessibility Testing at Slack appeared first on Engineering at Slack.","summary":"### Slack 的自动化无障碍测试策略\\\\n\\\\nSlack 将客户信任和无障碍性视为核心，通过内部标准和专业团队确保产品符合 Web 内容无障碍指南（WCAG）。为补充现有策略，Slack 于 2022 年引入桌面自动化无障碍测试，将其作为全面测试策略的重要组成部分，该策略还包括早期用户参与和手动测试。\\\\n\\\\n**自动化测试的价值与挑战：**\\\\n*   自动化测试是其全面测试策略的重要补充，但自动化工具存在局限性，可能忽略细微问题或误报。\\\\n\\\\n**工具选择与实施：**\\\\n*   选择了 **Axe** 作为无障碍测试工具，因其功能强大且与现有端到端（E2E）测试框架兼容。\\\\n*   最初尝试与 Jest/React Testing Library 集成受阻于自定义 Jest 配置。\\\\n*   转而使用 **Playwright** 框架，通过 `@axe-core/playwright` 包进行集成。\\\\n*   在 Playwright 中直接嵌入 Axe 检查遇到挑战，如 Locator 对象的行为和性能问题。\\\\n\\\\n**定制化与解决方案：**\\\\n*   通过 Playwright 的灵活性和 Axe Core 的定制功能（如过滤规则、特定标签）来管理检查。\\\\n*   设置 `runAxeAndSaveViolations` 函数，检查 WCAG 2.1 A/AA 级别。\\\\n*   排除已知问题和不适用的规则，并过滤并去除重复的“关键”级别违规。\\\\n*   将无障碍辅助功能集成到 Playwright 的 `slack` fixture 中，简化开发者调用。\\\\n*   自定义 `test.step` 标签，提高违规检测的可读性。\\\\n\\\\n**测试放置与报告：**\\\\n*   在关键功能测试套件中添加无障碍检查，通常在页面/视图加载后，并避免重复分析同一视图。\\\\n*   违规报告迭代：从简单的文本文件到集成 Playwright HTML Reporter，包含截图和详细、简洁的错误信息。\\\\n\\\\n**激活、管理与审计：**\\\\n*   通过 `A11Y_ENABLE` 环境变量控制测试运行，支持按需、定期和 CI 集成。\\\\n*   与内部无障碍团队合作，建立无障碍问题分类和处理流程，包括新的“自动化无障碍”标签和 Jira 自动化创建工单。\\\\n*   定期审计 Axe 调用，检查重复和覆盖率，未来计划引入 AI 辅助审计。\\\\n\\\\n**未来展望：**\\\\n*   计划开发针对核心功能（如键盘导航）的小型阻塞性测试套件。\\\\n*   探索 AI 驱动的测试结果后处理和无障碍检查放置审计。\\\\n\\\\n**总结：**\\\\n尽管面临技术挑战，Slack 成功地简化了自动化无障碍测试流程，减少了对人工测试的依赖，并为 AI 驱动的测试创建奠定了基础，强化了其无障碍文化。该项目强调了即使在存在技术障碍的情况下，通过关注开发者体验，自动化无障碍测试也能发挥重要作用。","translated_title":"Slack 的自动化无障碍测试"},{"title":"迁移自动化：借助AI简化Jenkins到GHA的迁移 (原标题: Migration Automation: Easing the Jenkins → GHA shift with help from AI)","link":"https://slack.engineering/migration-automation-easing-the-jenkins-%e2%86%92-gha-shift-with-help-from-ai/","pubDate":"Mon, 16 Dec 2024 09:00:03 +0000","isoDate":"2024-12-16T09:00:03.000Z","content":"<p>Overview The past few months have been exciting times for Slack’s CI infrastructure. After years of developer frustration with Jenkins (everything from security issues to downtime to generally poor UX) internal pressure led us to move a majority of Slack’s CI jobs from Jenkins to GitHub Actions.\xa0 My intern project at Slack this summer involved&#8230;</p>\\n<p>The post <a rel=\\"nofollow\\" href=\\"https://slack.engineering/migration-automation-easing-the-jenkins-%e2%86%92-gha-shift-with-help-from-ai/\\">Migration Automation: Easing the Jenkins → GHA shift with help from AI</a> appeared first on <a rel=\\"nofollow\\" href=\\"https://slack.engineering\\">Engineering at Slack</a>.</p>\\n","contentSnippet":"Overview The past few months have been exciting times for Slack’s CI infrastructure. After years of developer frustration with Jenkins (everything from security issues to downtime to generally poor UX) internal pressure led us to move a majority of Slack’s CI jobs from Jenkins to GitHub Actions.\xa0 My intern project at Slack this summer involved…\\nThe post Migration Automation: Easing the Jenkins → GHA shift with help from AI appeared first on Engineering at Slack.","creator":"Zhengyu Shen","encodedSnippet":"<p>Overview The past few months have been exciting times for Slack’s CI infrastructure. After years of developer frustration with Jenkins (everything from security issues to downtime to generally poor UX) internal pressure led us to move a majority of Slack’s CI jobs from Jenkins to GitHub Actions.\xa0 My intern project at Slack this summer involved&#8230;</p>\\n<p>The post <a rel=\\"nofollow\\" href=\\"https://slack.engineering/migration-automation-easing-the-jenkins-%e2%86%92-gha-shift-with-help-from-ai/\\">Migration Automation: Easing the Jenkins → GHA shift with help from AI</a> appeared first on <a rel=\\"nofollow\\" href=\\"https://slack.engineering\\">Engineering at Slack</a>.</p>\\n\\nOverview\\nThe past few months have been exciting times for Slack’s CI infrastructure. After years of developer frustration with Jenkins (everything from security issues to downtime to generally poor UX) internal pressure led us to move a majority of Slack’s CI jobs from Jenkins to GitHub Actions.\xa0\\nMy intern project at Slack this summer involved creating a conversion tool that could automatically migrate Jenkins pipelines to GitHub Actions, saving developers time and accelerating the migration. The project was successful, and is expected to cut the migration time by half and save over 1,300 hours. This blog is focused on that conversion tool, and the 7 week journey to design, implement, and improve it.\xa0\\nBefore we start though, you should know some terminology:\\nPipeline: A set of Jenkins plugins used for organizing Jenkins jobs. “Jenkins pipeline” refers to a set of one or more ordered Jenkins jobs. At the beginning of this project we had242 Jenkins pipelines to migrate.\\nInstance: A Jenkins instance is where Jenkins pipelines are hosted. We were migrating off of 3 Jenkins instances.\\nWorkflow: a GitHub Actions CI job.\\nGHA: GitHub Actions.\\nPlanning for the Project\\nPondering the Problem\\nThe success criteria for the project are as follows:\\nCreate tooling that automates the difficulties of manually migrating a Jenkins pipeline to GHA\\nRun this tooling on a large subset of existing Jenkins jobs\\nCreate documentation around the conversion process\xa0\\nTools of the Trade\\nAfter doing some research, we identified a few useful tools to use to convert from Jenkins to GHA:\\nThe GitHub Actions Importer. GitHub publishes a tool that can audit a Jenkins instance and either fully convert, partially convert, or fail to convert all pipelines on it to GitHub Actions. Although generally most pipelines fully convert, not all do. So a few more tools will be needed after using the Importer.\\nPython scripting: It seemed likely that some amount of the Importer’s errors would be easy to fix with just regex scripts. We expected to use Large Language Models (LLMs) for any errors that were too intricate to fix with regular expressions.\\nThe Solution\\nPart 1 – The Importer\\nOver the span of a week, we ran the Importer on several of our Jenkins instances. The tool was simple. You install it from the GitHub CLI, configure it, and run a command like this:\\ngh actions-importer audit jenkins --output-dir <output-dir>\\nThe Importer would then output GitHub Actions workflow files for each Jenkins pipeline on the Jenkins instance being audited. Upon running the Importer, we were curious as to how well it performed. Fortunately, the Importer creates an audit summary upon completing its audit of a Jenkins instance. The audit file contains information about how many Jenkins jobs were fully converted, partially converted, and failed to convert from Jenkins to GHA.\\nAbout 50% of the time the Importer moved the pipeline from Jenkins to GHA without error. 5% of the time the importer failed completely, and the rest of the jobs were partially-imported — it created a workflow YAML file, but wasn’t able to populate it fully.\\nIn addition to the success and failure rates, we were curious as to exactly what Jenkins build steps and environments were unsupported by the Importer. Fortunately the audit summaries contain information about this as well. Glancing at the audit summary, it was initially concerning to see that there were hundreds of Jenkins pipelines that weren’t able to fully convert to GitHub Actions. A deeper inspection revealed a pleasant surprise – only eight unsupported Jenkins build steps and environments were responsible for over 90% of those failures.\\nThis was good news. Instead of needing to spread ourselves thin looking at dozens of unsupported Jenkins steps, we instead only had to look at eight. For those key few unsupported items, we’d need to look at the following:\\nIs this actually unsupported by GHA, or do equivalents exist?\\nIf equivalents do exist, is it easy to write scripts to edit the Importer’s outputs accordingly?\\nPart 2 – Research\\nAt this point in the project, it was known that the Importer’s workflows had a lot of room for improvement. Around half of the pipelines didn’t fully convert, so a big objective was to look into the unsupported Jenkins steps that caused them to not fully convert. Another task though was to investigate the pipelines that fully converted. Can we really trust the Importer when it claims that it could fully convert something? So really every pipeline that got (or didn’t get) converted to a workflow had to be looked at.\xa0\\nAfter looking through several dozen of the YAML files the Importer made, it was clear that there were 4 main types of corrections we’d need to make to the workflow YAML files:\\nReplacing rate-limited actions with internal mirrors\xa0\\nReplacing actions with other actions\\nAdding useful comments for end users\xa0\\nRemoving unnecessary comments\\nOver the course of 2 weeks, we studied each of these:\\n1 – Replacing rate limited actions with internal mirrors\\nRoI: Very High\\nOverview\\nBy default, if you use an arbitrary action from the GitHub Actions marketplace, like say actions/github-script, any requests made by the action will hit the github.com API. There are rate limits for this. Actions that hit our self-hosted GitHub Enterprise instance have far higher rate limits however. So a task we had to do was replace rate-limited actions with internal mirrors of those actions that we have on our GitHub Enterprise instance.\\nEnd-User Value\\nCorrecting rate-limited actions is simple to do on our end, and prevents end users from dealing with the headache that is having jobs fail from rate limits. So high RoI here.\\n2 – Replacing actions with other actions\\nRoI: High\\nOverview\\nThere were a few instances where, to perform a certain task, the Importer would use one action whereas we’d prefer if it had used another. For instance, for sending Slack messages the Importer often elected to use the rtCamp/action-slack-notify action. Naturally, Slack has internal actions for sending Slack messages, and we’d rather use internal libraries for this purpose. So something to look into was replacing rtCamp/action-slack-notify with Slack’s internal messaging action.\\nThis type of task seemed suitable for an LLM to do, since two actions often have disparate syntax which is hard to convert between by just using Python’s string methods. Preliminary testing also showed perfect LLM performance for this task which was reassuring.\\nEnd-User Value\\nReplacing one action with a different action would require end users to know both actions well. So automating this saves quite a bit of time and effort on their end, and it’s not that hard to write LLM prompts for these tasks. The RoI is thus high.\\n3 – Adding Useful Comments for End Users\\nRoI: Medium\\nOverview\\nSome action items in the workflow files could only be done manually. For example, the syntax used in the workflow files to access secrets assumed that the secrets would be stored on GitHub. However, we don’t store our secrets on GitHub, and it would be time prohibitive and against our security policies to move them to GitHub. It would instead be better for end users to edit the workflow files such that secrets get read from where we store them instead of from GitHub. This couldn’t really be automated on the conversion tool side, since only the end users know where their secrets are and how to access them. All the conversion tool could do in such situations was leave a comment like: “Change these lines so that secrets are being read from where we store them instead of from GitHub. Go to a certain slack channel to find relevant docs about this.”\\nEnd-User Value\\nIt’s definitely useful for end users to know where they should look to fix problems, but they still have to do the actual work of fixing the problem. So overall, medium RoI.\xa0\\n4 – Removing Unnecessary Comments\\nRoI: Low-medium\\nOverview\\nRecall that around 45% of pipelines only partially converted. What this means is that the Importer couldn’t find a GHA equivalent for a Jenkins environment or build step. When this happened, the Importer left a comment in the workflow that looked something like “X Jenkins item was unsupported.”\xa0\\nAs it turns out, the reason why a lot of Jenkins pipelines were unsupported is because that piece of functionality in Jenkins was either already included or not necessary in GHA. So most of those “X was unsupported” comments weren’t too important and would probably be better off removed.\\nEnd-User Value\\nThe benefits here to end users was small, as all it really does is declutter the workflows. So overall this had a low return. However, a script to delete strings is also very easy to write. So overall low-medium RoI.\\nPart 3 – Corrections Tool\\nOverview\\nThe last part of the implementation is the corrections tool, which would correct the Importer’s workflows. We knew from the start that the tool would have a balance of using Python’s string methods and using LLMs. From the last section, note that out of the 4 categories of action items for the correction tool, the only one that needed AI was “replacing actions with other actions”. So the corrections tool was largely non-AI. It took around 3 weeks to make the corrections tool.\\nImplementation Architecture\\nHere’s the bird’s-eye view. The input to the corrections tool was the path to a directory containing workflow yaml files. The following chain of events then happened:\\nThe path to each yaml file in that directory was written to an array. Then, for each element in that array:\\n\\nThe contents of the yaml file at that path were read. Then:\\n\\nEvery relevant non-AI correction was made to those file contents, and written back to the original file\\nEvery relevant AI correction was made to those file contents, and written back to the original file\\nThe end result is that every yaml file in the directory inputted to the corrections tool was edited in place.\\nImplementing the Non-AI Corrections\\nAs mentioned previously, a large part of what needed to happen to the Importer’s yaml files boiled down to “add a comment after a line containing a certain string” or “replace a string with another string”. Python’s String.replace() method was used extensively here.\xa0\\nImplementing the AI Corrections\\nPrompt engineering is both science and art. It’s science, because there’s a formula that’s useful for doing prompt engineering:\\nUse the following structure:\\n\\nContext Setting (let the LLM know what the task is and why it’s doing it)\\nSpecific Steps (Several instructions, each about very specific individual tasks the LLM must do. Too long of a prompt is much less dangerous than too vague of a prompt)\\nOutput Instructions (Instructions about how the LLM should present its output)\\nIn the context setting part of the prompt, include any relevant syntax and documentation\\nPrompt engineering is also art, because experimenting with the prompt and seeing how you feel about the AI’s output is a key part too. There were a few rounds of this process:\\nStart with a very basic prompt\\nSee the LLM’s response to it\\nAdd detail and context to the prompt to prevent any errors the LLM made\\nAn example of a prompt we’d end up at:\\nI’m going to provide you with a GitHub Actions workflow yaml file. In the file an action called rtCamp/action-slack-notify@v2.2.1 is being used. Your job is to replace every rtCamp/action-slack-notify@v2.2.1 action with the slack/message-action@v1.0 action, and output the resulting workflow yaml file. For reference, I will provide you with the syntax for the slack/message-action@v1.0 action. I’ll put it between <syntax></syntax> XML tags:\\n<syntax>\\n\\n- uses: slack/message-action@v1.0\\n\\n\xa0 with:\\n\\n\xa0\xa0\xa0channel: channel-name\\n\\n\xa0\xa0\xa0text: text-message\\n\\n</syntax>\\nNow, perform the following steps:\\n Find every instance of the rtCamp/action-slack-notify@v2.2.1 action being used.\\n Replace each instance with the slack/message-action@v1.0 action with the aforementioned syntax.\\n Replace channel-name with the value of the SLACK_CHANNEL field in the rtCamp/action-slack-notify@v2.2.1 action.\\n Replace text-message with the value of the SLACK_MESSAGE field in the rtCamp/action-slack-notify@v2.2.1 action.\\nAlso, there are a few things you should absolutely never do:\\n Do not remove any comments from the file\\n Do not alter any other action in the file besides rtCamp/action-slack-notify@v2.2.1\\nI will now provide you with the workflow file you will be making edits to between <workflow></workflow> XML tags. When providing your output, only provide the contents of a YAML file. Do not describe what you did. Do not provide any justifications for your decisions. Only provide the corrected GitHub Actions yaml file, with nothing else before or after the yaml file in your output. Now, please perform the instructions I gave you:\\nWe examined about 2 dozen of the LLM’s outputs and noticed something stunning — 100% accuracy in performing the requested tasks. A big concern with asking LLMs to do these corrections were hallucinations and other unexpected side effects, so seeing the LLM’s performance was reassuring. Overall, the LLM ended up being really useful for the migration. Even though prompt engineering isn’t trivial, it doesn’t take that long either, and after doing it the LLM could perform as well as any human for these “replace A with B” tasks. LLMs filled a crucial gap for fixing those errors that were too hard to fix with Python’s string methods.\\nImpact and End User Feedback\\nImpact\\nFirst, consider the process of converting a pipeline from Jenkins to GHA manually:\\nLooking at your Jenkins pipeline as a reference, write a GHA workflow yaml file.\\nTest that workflow yaml and do debugging as needed.\\nWrite the documentation for the workflow.\\nDelete the pipeline from its Jenkins instance.\\nNote how the steps are in descending order of difficulty and time cost.\xa0\\nThe conversion tool lets end users skip step 1 entirely and have a much easier time with step 2. End users are given workflow files that have few, if any, flaws. They can skip straight to debugging, which will be accelerated since the workflows they get are highly accurate.\\nThere are about 242 pipelines that this tool attempted to convert from Jenkins to GitHub Actions. The amount of time it would take a developer to manually convert one Jenkins pipeline to GHA varies. Here are some estimates for averages based on the type of developer:\\nHighly experienced with Jenkins and GHA: 2 hours\\n\\nSo basically, a good chunk of a day\\nHighly experienced with Jenkins but not GHA: 5 hours\\n\\nThese folks would need to learn GHA syntax. So most of a day\\nNot experienced with either: 10 hours\\n\\nThis could take over a day for someone who’s brand new to Jenkins and GHA. This number includes the time it may take others to help them.\\nAssuming 10% of end users are type 1, 40% are type 2 and 50% are type 3, it gives a total amount of time of about 1700 hours to convert all pipelines to GHA\\nSuppose that with this tool, half of pipelines run with no edits necessary (this is realistic, since half of pipelines are fully converted), and that the other half may take an average of 3 hours to debug. It would then take about 360 hours to convert all pipelines to GHA.\\nThis tool is thus projected to save over 1,300 hours, 80% of the time it would take to move every Jenkins pipeline to GHA.\xa0\\nFinal Comments\\nAt time of writing, two weeks have passed since the outputs from the conversion tool have been published, and end-user feedback is now starting to arrive. It’s clear that developers at Slack have found the tool useful, but I’ve also heard that there are a few errors in the generated workflows. Even from the very start of my internship, I expected this to happen, but was worried that it might be hard to triage those incidents.\\nAs it turns out, there wasn’t a problem. I made a Canvas doc for this conversion, and whenever end-users of the tool encountered an issue I updated the canvas with details about the problem and its solution. As more users used the tool, I got more feedback, and the canvas became more comprehensive.\\nIf I were to pick the best thing about Slack, both the app and the company, it would be the ease of communication and collaboration. It’s easy to let people know things, to find out who to go to for help, to meet people, to take notes and keep track of info. All these things have made my internship experience more fun, pleasant, and educational than it would’ve been otherwise. And, if you join us, it’d make your time here great too .\\nAcknowledgments\\nThank you everyone who contributed your knowledge and helped this project get to where it is:\\nJerry Shen\\nBuddhadev Veeramallu\\nEllen Wong\\nShane Gearon\\nSergii Gorbachov\\nCatherine Li\\nNick Matute\\n\\n\\n\\t\\t\\t\\nNow more than ever, Slack needs GHA specialists. Does that sound like you? If so, we’re hiring !\\nApply now\\n\\t\\t\\n\xa0\\nThe post Migration Automation: Easing the Jenkins → GHA shift with help from AI appeared first on Engineering at Slack.","summary":"# Slack CI/CD 迁移自动化项目\\\\n\\\\n## 概述\\\\n\\\\n*   **背景：** Slack的CI/CD基础设施因Jenkins的安全、停机和用户体验问题，决定将大部分CI作业从Jenkins迁移到GitHub Actions (GHA)。\\\\n*   **项目目标：** 实习生项目旨在开发一个自动化工具，将Jenkins管道自动迁移到GHA，以节省开发时间并加速迁移。\\\\n*   **预期成果：** 该工具预计将迁移时间缩短一半，节省超过1300小时。\\\\n\\\\n## 项目规划与工具\\\\n\\\\n*   **成功标准：** 创建自动化工具、应用于现有Jenkins作业、生成转换过程文档。\\\\n*   **核心工具：**\\\\n    *   **GitHub Actions Importer：** 用于初步审计和转换Jenkins实例上的管道。\\\\n    *   **Python脚本：** 处理Importer无法完全转换的简单错误（如正则表达式）。\\\\n    *   **大型语言模型 (LLMs)：** 解决复杂且难以用正则表达式修复的错误。\\\\n\\\\n## 解决方案实施\\\\n\\\\n### 1. GitHub Actions Importer 的应用\\\\n\\\\n*   **转换表现：** 约50%的Jenkins管道被完全转换，5%完全失败，其余为部分转换。\\\\n*   **关键发现：** 超过90%的转换失败是由仅8个不受支持的Jenkins构建步骤和环境引起的，这极大地简化了后续的修正工作。\\\\n\\\\n### 2. 修正工具 (Corrections Tool) 的开发\\\\n\\\\n*   **目的：** 改进 Importer 的输出，并处理未完全转换的管道。\\\\n*   **主要修正类型：**\\\\n    *   **替换限速操作为内部镜像 (高回报)：** 将默认的GitHub Actions市场操作替换为Slack内部GitHub Enterprise实例上的镜像，以避免API限速。\\\\n    *   **替换操作为其他操作 (高回报)：** 例如，将 `rtCamp/action-slack-notify` 替换为Slack内部的消息发送操作。此任务因涉及不同语法，被确定为LLM的最佳应用场景，并实现了100%的测试准确率。\\\\n    *   **为终端用户添加有用注释 (中回报)：** 对于无法自动化的手动操作（如秘密访问），工具会添加注释指导用户进行修改。\\\\n    *   **删除不必要注释 (低中回报)：** 清理 Importer 留下的“X 不受支持”等冗余注释，以简化工作流文件。\\\\n\\\\n### 3. AI (LLM) 在修正中的应用\\\\n\\\\n*   **应用场景：** 主要用于“替换操作为其他操作”这类复杂任务。\\\\n*   **提示工程：** 采用结构化方法（上下文设置、具体步骤、输出指令），并迭代优化提示，以确保LLM的准确性和可靠性。\\\\n*   **效果：** 经过测试，LLM在执行指定任务时达到了100%的准确率，有效填补了Python字符串方法难以处理的复杂转换空白。\\\\n\\\\n## 项目影响与用户反馈\\\\n\\\\n*   **项目影响：**\\\\n    *   **效率提升：** 自动化工具使开发者能够跳过手动编写GHA工作流的步骤，并大大简化调试过程。\\\\n    *   **时间节省：** 预计将242个Jenkins管道迁移到GHA的总时间从约1700小时减少到约360小时，节省了超过1300小时（总时间的80%）。\\\\n*   **用户反馈：**\\\\n    *   开发者普遍认为该工具非常有用，尽管生成的工作流中仍存在少量错误。\\\\n    *   通过创建Canvas文档来收集和解决用户反馈，该文档随着用户使用而变得日益完善。\\\\n*   **总结：** 该项目不仅显著提高了迁移效率，也体现了Slack在沟通和协作方面的优势。\\\\n\\\\n## 招聘信息\\\\n\\\\n*   Slack正在招聘GHA专家。","translated_title":"迁移自动化：借助AI简化Jenkins到GHA的迁移"},{"title":"有目的地破坏 (原标题: Break Stuff on Purpose)","link":"https://slack.engineering/break-stuff-on-purpose/","pubDate":"Tue, 10 Dec 2024 09:00:41 +0000","isoDate":"2024-12-10T09:00:41.000Z","content":"<p>\xa0“A complex system can fail in an infinite number of ways.” -“Systemantics” by John Gall &#160; Incidents are stressful but inevitable. Even services designed for availability will eventually encounter a failure. Engineers naturally find it daunting to defend their systems against the “infinite number of ways” things can go wrong.\xa0 Our team found ourselves in&#8230;</p>\\n<p>The post <a rel=\\"nofollow\\" href=\\"https://slack.engineering/break-stuff-on-purpose/\\">Break Stuff on Purpose</a> appeared first on <a rel=\\"nofollow\\" href=\\"https://slack.engineering\\">Engineering at Slack</a>.</p>\\n","contentSnippet":"“A complex system can fail in an infinite number of ways.” -“Systemantics” by John Gall \xa0 Incidents are stressful but inevitable. Even services designed for availability will eventually encounter a failure. Engineers naturally find it daunting to defend their systems against the “infinite number of ways” things can go wrong.\xa0 Our team found ourselves in…\\nThe post Break Stuff on Purpose appeared first on Engineering at Slack.","creator":"Sean Madden","encodedSnippet":"<p>\xa0“A complex system can fail in an infinite number of ways.” -“Systemantics” by John Gall &#160; Incidents are stressful but inevitable. Even services designed for availability will eventually encounter a failure. Engineers naturally find it daunting to defend their systems against the “infinite number of ways” things can go wrong.\xa0 Our team found ourselves in&#8230;</p>\\n<p>The post <a rel=\\"nofollow\\" href=\\"https://slack.engineering/break-stuff-on-purpose/\\">Break Stuff on Purpose</a> appeared first on <a rel=\\"nofollow\\" href=\\"https://slack.engineering\\">Engineering at Slack</a>.</p>\\n\\n“A complex system can fail in an infinite number of ways.”\\n-“Systemantics” by John Gall\\n\xa0\\nIncidents are stressful but inevitable. Even services designed for availability will eventually encounter a failure. Engineers naturally find it daunting to defend their systems against the “infinite number of ways” things can go wrong.\xa0\\nOur team found ourselves in this position when a service we use internally for dashboards went down, recovery failed, and we lost our teammates configurations. However, with creativity and a dash of mischievousness, we developed an exercise that addressed the cause of the problem, energized our teammates, and brought excitement and fun to the dry job of system maintenance. Come along as we share our journey from incident panic to peace of mind.\xa0\\nThe incident\\nSlack engineers use Kibana with Elasticsearch to save custom dashboards and visualizations of important application performance data. On January 29th, 2024, our Kibana cluster—and subsequently, the dashboards—started to fail due to a lack of disk space. We began investigating and realized this was the unfortunate downstream effect of an earlier architectural decision. You can configure Elasticsearch as a stand-alone cluster for Kibana to use, which decouples the object storage from the Kibana application itself. However, our Kibana cluster was configured to use an Elasticsearch instance on the same hosts as the Kibana application. This tied the storage and the application together on the same nodes, and those nodes were now failing. Slack engineers couldn’t load the data they needed to ensure their applications were healthy.\\nEventually, the cluster got into such a bad state that it couldn’t be saved, and we had to rebuild it from a clean slate. We thought we could stand up a new cluster by cycling in new hosts and restoring the Kibana objects from a backup. However, we were shocked and disappointed to discover our most recent backup was almost two years old. The backup and restore method hadn’t gotten a lot of love after its first configuration, and it didn’t have alerts to tell us if it wasn’t running correctly. On top of that, our runbook was out of date, and the old backup failed when we tried to restore from it. We lost our internal employees’ links and visualizations, we were forced to recreate indexes and index patterns by hand.\\nExplaining to our teammates that our recovery procedure had failed and their data was lost was not fun. We didn’t notice our backups were failing until it was too late.\xa0\\nNo one is immune to situations like these. Unless you actively exercise your processes, procedures, and runbooks, they will become obsolete and fail when you need them the most. Incident response is about restoring service as quickly as possible, but what you do when the dust settles determines whether they are ultimately a benefit or a liability.\\nBreaking stuff is fun\\nWe were determined to turn this incident into tangible benefits. Our post-incident tasks included making sure that our Elasticsearch clusters in every environment were backed up with a scheduled backup script, fixing our runbooks based on the experience, and checking that the Amazon S3 retention policies were set correctly.\\nWe wanted to test our improvements to make sure they worked. Our team came up with an unconventional but exciting idea: we would break one of our development Kibana clusters and try the new backup and restore process. The development cluster is configured similarly to production clusters, and it would provide a realistic environment for testing. To ensure success, we carefully planned which cluster we would break, how we would break it, and how we would restore service.\\nRunning the exercise\\nWe planned the testing event for a quiet Thursday morning and invited the whole team. Folks showed up energized and delighted at the opportunity to break something at work on purpose. We filled the disk on our Kibana nodes, watched them fail in real time, and successfully triggered our alerts. We worked through the new runbook steps and cycled the entire cluster into a fresh rebuild. Our system recovered successfully from our staged incident.\\nAlthough the recovery was successful, we fell short of our goal of being able to recover in less than one hour. A lot of the commands in the runbook were not well understood and hard to grok during a stressful incident. Even trying to copy and paste from the runbook was a challenge due to formatting issues. Despite these rough edges, the backups ended up restoring the cluster state completely. Additionally, we found some firewall rules that needed to be added to our infrastructure as code. This was a bonus discovery from running the exercise — we did not expect to find firewall issues, but fixing them saved us future headaches.\\nIn a final test of our new recovery process, we migrated the general development Kibana instance and Elasticsearch cluster to run on Kubernetes. This was an excellent opportunity to test our improved backup script on a high-use Kibana cluster. Thanks to our improved understanding of the process, and the updated provisioning scripts, we successfully completed the migration with about 30 minutes of downtime.\\nDuring both exercises, we ran into minor issues with our new runbooks and restoration process. We spent time figuring out where the runbook was lacking and improved it. Inspired by the exercise, we took it upon ourselves to automate the entire process by updating the scheduled backup script tool to be a full-featured CLI backup and restore program. Now we are able to completely restore a Kibana backup from cloud storage with a single command. “Breaking stuff” wasn’t just fun: it was an incredibly valuable investment of our time to save us from future stress.\\nChaos is everywhere—might as well use it\\n“Complex systems usually operate in failure mode.”\\n– John Gall\\n\xa0\\nEvery production system is broken in a way that hasn’t been uncovered yet. Yes, even yours. Take the time and effort to find those issues and plan how to recover from them before it’s critical. Generate a lot of traffic and load test services before customers do. Turn services off to simulate unexpected outages. Upgrade dependencies often. Routine maintenance in software is often neglected because it can be dry and boring, but we pay for it when an incident inevitably hits.\\nWe discovered we can make system testing and maintenance exciting and fresh with strategic chaos: planned opportunities to break things. Not only is it simply exciting to diverge from the usual job of fixing, it puts us in unique and realistic situations we would have never discovered if we had approached maintenance the traditional way.\\nWe encourage you to take the time to break your own systems. Restore them and then do it again. Each iteration will make the process and tooling better for when you inevitably have to use it in a stressful situation.\\nFinally, remember to celebrate World Backup Day every March 31st. I know we will!\\nAcknowledgments\\nKyle Sammons – for pairing with me on the planning and execution of the recovery exercise\\nMark Carey and Renning Bruns – for getting the tooling functioning properly and automating the process\\nEmma Montross, Shelly Wu, and Bryan Burkholder – for incident response and support during the recovery exercise\\nGeorge Luong and Ryan Katkov – for giving us the autonomy to make things better\\n\xa0\\nInterested in taking on interesting projects, making people’s work lives easier, or just building some pretty cool forms? We’re hiring! \xa0 \xa0 Apply Now\\nThe post Break Stuff on Purpose appeared first on Engineering at Slack.","summary":"### 摘要：有目的地破坏以增强系统韧性\\\\n\\\\n*   **背景与问题：** 复杂系统故障不可避免。Slack团队曾因内部Kibana集群故障，导致数据丢失，暴露出备份机制失效、操作手册过时等问题，凸显了未经验证的流程在紧急情况下的脆弱性。\\\\n\\\\n*   **应对策略：** 团队决定将此次事故转化为学习机会，通过“有目的地破坏”开发环境中的Kibana集群来测试和改进系统韧性。\\\\n\\\\n*   **实践过程：**\\\\n    *   团队精心策划，模拟磁盘空间耗尽导致的服务故障，并成功触发警报。\\\\n    *   测试了新的备份和恢复流程，团队成员积极参与，共同执行更新后的操作手册。\\\\n\\\\n*   **主要成果与价值：**\\\\n    *   成功恢复系统，验证了改进后的备份和恢复流程的有效性。\\\\n    *   发现了操作手册中需要改进的细节（如命令理解、格式问题），并进行了优化。\\\\n    *   意外发现并修复了潜在的防火墙配置问题，避免了未来的隐患。\\\\n    *   促使团队将备份和恢复过程自动化，实现了一键式操作，大大提高了效率。\\\\n    *   将枯燥的系统维护工作变得有趣且富有成效，提升了团队士气和参与度。\\\\n\\\\n*   **核心启示：** 文章强调，主动制造故障（即混沌工程）是发现系统脆弱点、验证恢复流程和工具有效性的关键方法。通过定期“破坏”和恢复系统，可以不断优化流程和工具，从而在真正的紧急情况下更有效地应对，将潜在的风险转化为实际的收益。","translated_title":"有目的地破坏"},{"title":"Slack 审计日志与异常事件 (原标题: Slack Audit Logs and Anomalies)","link":"https://slack.engineering/slack-audit-logs-and-anomalies/","pubDate":"Mon, 09 Dec 2024 17:02:13 +0000","isoDate":"2024-12-09T17:02:13.000Z","content":"<p>What are Slack Audit Logs? Like many Software as a Service (SaaS) offerings, Slack provides audit logs to Enterprise Grid customers that record when entities take an action on the platform. For example, when a user logs in, when a user updates their profile, when an app downloads a file, etc. The actual list of&#8230;</p>\\n<p>The post <a rel=\\"nofollow\\" href=\\"https://slack.engineering/slack-audit-logs-and-anomalies/\\">Slack Audit Logs and Anomalies</a> appeared first on <a rel=\\"nofollow\\" href=\\"https://slack.engineering\\">Engineering at Slack</a>.</p>\\n","contentSnippet":"What are Slack Audit Logs? Like many Software as a Service (SaaS) offerings, Slack provides audit logs to Enterprise Grid customers that record when entities take an action on the platform. For example, when a user logs in, when a user updates their profile, when an app downloads a file, etc. The actual list of…\\nThe post Slack Audit Logs and Anomalies appeared first on Engineering at Slack.","creator":"Ryan Katkov","encodedSnippet":"<p>What are Slack Audit Logs? Like many Software as a Service (SaaS) offerings, Slack provides audit logs to Enterprise Grid customers that record when entities take an action on the platform. For example, when a user logs in, when a user updates their profile, when an app downloads a file, etc. The actual list of&#8230;</p>\\n<p>The post <a rel=\\"nofollow\\" href=\\"https://slack.engineering/slack-audit-logs-and-anomalies/\\">Slack Audit Logs and Anomalies</a> appeared first on <a rel=\\"nofollow\\" href=\\"https://slack.engineering\\">Engineering at Slack</a>.</p>\\n\\nWhat are Slack Audit Logs?\\nLike many Software as a Service (SaaS) offerings, Slack provides audit logs to Enterprise Grid customers that record when entities take an action on the platform. For example, when a user logs in, when a user updates their profile, when an app downloads a file, etc. The actual list of actions that are captured in the audit logs is quite extensive and it is worth perusing periodically for any new additions. The documentation also presents an example audit log and discusses the fields in detail.\xa0 We suggest reviewing this documentation before proceeding further, and we’ve included an aesthetically pleasing example audit log that you can use to test your newfound audit log expertise:\\n\\nWhere are Slack Audit Logs Available?\\nSlack audit logs are available to Org Admins, Owners, and those with the Audit Logs Admin role via the Audit Log Dashboard by clicking Tools and Settings → Manage Audit Logs .\xa0 We will go over a more detailed example of using the UI to interact with audit logs in a subsequent section.\xa0 The audit logs are also available via an API, and many vendors have connectors available to ingest the audit logs into their platforms.\xa0 A few examples include:\\nSplunk\\nAWS AppFabric\\nDataDog\\nThe Audit Log API allows for filtering by attributes like when the logs were generated, the action (up to 30 actions may be specified), actor, and entity.\xa0 For example, if an enterprise was only interested in consuming the user login events from the audit logs, they could specify <span style=\\"font-weight: 400\\">user_login</span> for the action parameter when calling the API.\xa0 Please see the API documentation for more detail.\\nAnomaly Events\\nWhat are Anomalies?\\nIncluded in the stream of audit logs are anomaly events (events with an action of anomaly).\xa0 These differ from other audit logs events because, instead of recording an action that an entity performed on the platform, they indicate that Slack’s analysis pipelines have detected an entity performing an anomalous action (or set of actions) or the circumstances under which an entity performed an action are anomalous. Anomalies serve as indicators of unusual or potentially suspicious activity within your Slack workspace. For a comprehensive list of currently deployed anomalies and guidance on how to interpret them, please refer to our documentation. We strongly recommend reviewing this resource to gain a deeper understanding of available anomalies before proceeding further.\\nHow to use Anomalies?\\nAs with any new event source that an organization onboards, Slack anomalies require some investigation, experimentation, and analysis before they can be operationalized by a Detection and Response team.\xa0\\nIn general, anomalies are not something that an organization would want to directly raise as an incident.\xa0 They indicate that something unexpected occurred and further investigation may be warranted.\xa0 The importance of an anomaly varies depending on each organization’s policies and permitted activities. For instance, if a <span style=\\"font-weight: 400\\">user_agent</span> anomaly is detected with a new User Agent like <span style=\\"font-weight: 400\\">Go-http-client/2.0</span>, it might indicate someone is using an automated tool to interact with Slack, rather than accessing it through the Slack app or website.\xa0 For some organizations, using unsanctioned Slack clients may not be allowed by policy, so the security team would want to follow up with this user.\xa0 Other organizations may allow their users more latitude, so this anomaly would be less interesting to them.\xa0\xa0\\nAllowlisting CIDR Ranges and ASNs\\nIf an organization knows that certain IP addresses or network ranges are associated with legitimate activities, Slack provides a way for customers to allowlist these sources. These API endpoints allow one to add trusted CIDR ranges and ASNs, providing flexible options to fine-tune anomaly detection and reduce anomaly volume.\\nCorrelating Anomalies\\nCorrelating multiple anomalies can provide valuable insights into potential security concerns. For instance, consider a scenario where a <span style=\\"font-weight: 400\\">user_agent</span> anomaly occurs near an <span style=\\"font-weight: 400\\">excessive_downloads</span> anomaly. This combination could suggest a scraping tool is being used (note there is also a newly released, high fidelity <span style=\\"font-weight: 400\\">unexpected_scraping</span> anomaly for certain scraping scenarios). The significance of this activity will depend on your organization’s policies. However, the situation becomes more critical if an <span style=\\"font-weight: 400\\">ip_address</span> or <span style=\\"font-weight: 400\\">session_fingerprint</span> anomaly accompanies the previously mentioned anomalies. This combination could indicate that an external party has obtained a user’s cookie and is using it to scrape data. In such cases, most organizations would likely prioritize a thorough investigation.\\nAggregating Anomalies\\nFinally, we can aggregate some anomalies to surface scenarios we are interested in.\xa0 For example, an organization might observe multiple users generating <span style=\\"font-weight: 400\\">excessive_downloads</span> anomalies each day, but the anomalies may not represent malicious activity (benign outliers).\xa0 Examining how many <span style=\\"font-weight: 400\\">excessive_downloads</span> anomalies a user generated over some period of time and comparing the total to historical norms could help uncover situations where a user is performing unwanted activity like scraping large amounts of data.\xa0 For example, if an organization has never seen one of their users generate more than three <span style=\\"font-weight: 400\\">excessive_downloads</span> events in a day, they could look for cases where four or more <span style=\\"font-weight: 400\\">excessive_downloads</span> anomalies occur for a user and investigate those more closely.\xa0\xa0\\nContext from Audit Log Events\\nAnomaly logs have substantially lower volume than audit logs, typically by at least two orders of magnitude, so if an organization is interested in remaining aware of anomaly logs but cannot handle the full volume of their audit logs, they can filter for just anomalies using the method outlined earlier. That said, we highly recommend consuming the entirety of your audit logs whenever possible so you have the most amount of surrounding context possible when investigating anomaly logs. For example, the surrounding <span style=\\"font-weight: 400\\">file_downloaded</span> audit logs events provide additional context when a user triggers an <span style=\\"font-weight: 400\\">excessive_downloads</span> anomaly, allowing you to confirm which files were actually downloaded.\xa0 Furthermore, in an Incident Response scenario, having the audit logs available locally in a easily queryable form can save time and stress.\\nAudit Log UI Example\\nIf you would like to review your Slack audit logs without an external service, Slack provides a UI that can be reached from Tools and Settings → Manage Audit Logs.\xa0 To demonstrate how to use the UI, we can walk through some activity from a salesperson, Matt, who is leaving the fictional company, Acme Corp.\xa0 First we’ll load the main Audit logs tab:\\n\\nThen, we’ll switch to the Security Detections tab to see any anomalies that Matt has generated:\\n\\nWe see that Matt generated several anomalies.\xa0 By clicking on the ••• menu and then View Full Log Details, we can find out more information about an anomaly.\xa0 The first one is a twofer and has the reasons <span style=\\"font-weight: 400\\">unexpected_scraping</span> and user_agent. The former indicates that scraping was detected from Matt, and the latter indicates that Matt’s user agent changed – in this case to Scrapers Inc.\xa0 Scrapers Inc is not a user agent associated with any sanctioned Slack client and therefore we would not expect user activity from it.\xa0 Note: if an <span style=\\"font-weight: 400\\">unexpected_scraping</span> anomaly appeared by itself, the User Agent would still be included, so strictly speaking we did not need the <span style=\\"font-weight: 400\\">user_agent</span> anomaly to determine that Matt is using an unexpected client.\\n\\nThe other anomaly that Matt generated is <span style=\\"font-weight: 400\\">excessive_downloads</span>, and it is further evidence that Matt is scraping data from the Slack workspace.\xa0 Again, we can see that the activity is coming from a client with user agent Scrapers Inc.\\n\\nSince Matt is leaving the company, we would like to know what sort of files he is downloading, so we flip back to the Audit Logs tab and filter for <span style=\\"font-weight: 400\\">file_downloaded</span> events:\\n\\nThen, we examine one of the files by clicking the ••• menu and then View Full Log Details.\xa0 The file is named Glengarry-Leads.xlsx which is probably not something Acme Corp would want a salesperson to be retrieving as they depart.\xa0\\n\\nTerminating a User’s Active Sessions\\nIf you identify suspicious activity associated with a user’s account and would like to take immediate action, it’s possible to terminate a user’s active sessions directly from the\xa0Audit Logs dashboard. To do this, navigate to the specific anomaly log entry belonging to the user in question, click the ••• menu, and select\xa0Sign Out Of Slack. This action immediately invalidates all active sessions for the user, forcing them to re-authenticate before regaining access to their account on any of their devices. (Alternatively, you can selectively terminate the user’s active sessions by device type, such as mobile or desktop only, if you require more granular control over how and where the user is impacted).\\n\\nOther Detection Thoughts\\nAn <span style=\\"font-weight: 400\\">ip_address</span> anomaly was not generated for Matt’s activity in this situation, but, if it were, an analyst would want to determine if the triggering IP is one that they would expect Matt to be accessing Slack from (has he used it historically when accessing company systems, is it a VPN endpoint, is it known to be malicious, etc.).\xa0 If the IP is not one that Matt is expected to use, then this activity could indicate that a malicious actor has obtained access to Matt’s account and is scraping data.\xa0 In general, we recommend checking the IP address included in anomalies under investigation regardless of whether an <span style=\\"font-weight: 400\\">ip_address</span> anomaly was generated for the user.\\nSimilarly, if a <span style=\\"font-weight: 400\\">session_fingerprint</span> anomaly were generated for Matt around the time of these anomalies, it might indicate that Matt’s session cookie was exfiltrated and is being used by a malicious actor to scrape data.\xa0 Again, we recommend investigating the IP address contained in the anomaly, and using other sources of telemetry like logs from agents running on Matt’s endpoint to ensure that he is responsible for the activity.\\nAs we discussed earlier, the user agent associated with anomalies like <span style=\\"font-weight: 400\\">excessive_downloads</span>, <span style=\\"font-weight: 400\\">user_agent</span> and <span style=\\"font-weight: 400\\">ip_address</span> can be a good signal that scraping or other unwanted activity is occurring.\xa0 We can operationalize this insight by surfacing anomalies with an unexpected user agent.\xa0 Rather than attempting to enumerate all possible unexpected user agents, it’s much easier to look for cases where the user agent is not in a set of expected values for an organization (think allowlist vs blocklist).\xa0\\nThe post Slack Audit Logs and Anomalies appeared first on Engineering at Slack.","summary":"# Slack 审计日志与异常事件概述\\\\n\\\\n*   **Slack 审计日志**\\\\n    *   Slack 为企业网格（Enterprise Grid）客户提供审计日志，记录平台上的实体行为，例如用户登录、资料更新、应用下载文件等。\\\\n    *   日志内容全面，建议定期查阅官方文档了解新增项。\\\\n    *   **可用性：** 可通过审计日志控制面板（UI）供组织管理员、所有者和审计日志管理员访问；也可通过 API 获取，并有 Splunk、AWS AppFabric、DataDog 等第三方连接器支持。\\\\n    *   API 支持按时间、操作（最多30项）、执行者和实体进行过滤。\\\\n\\\\n*   **异常事件（Anomalies）**\\\\n    *   **定义：** 审计日志流中包含的特殊事件，表示 Slack 的分析管道检测到实体执行了异常操作或在异常环境下执行了操作，是工作区内异常或潜在可疑活动的指示器。\\\\n    *   **用途：** 异常事件通常不直接作为事件上报，而是作为需要进一步调查的信号。其重要性取决于组织的策略和允许的活动。\\\\n    *   **管理与利用：**\\\\n        *   **IP/ASN 白名单：** 允许客户将已知可信的 IP 地址或网络范围列入白名单，以优化异常检测并减少误报。\\\\n        *   **异常关联：** 关联多个异常事件（如 `user_agent` 和 `excessive_downloads`）可提供更深入的安全洞察，例如指示数据抓取行为；若再结合 `ip_address` 或 `session_fingerprint` 异常，可能表明外部方利用窃取凭证进行数据窃取。\\\\n        *   **异常聚合：** 通过聚合一段时间内的异常事件（如 `excessive_downloads`）并与历史基线对比，可发现用户潜在的非预期行为。\\\\n        *   **上下文信息：** 异常日志量远低于审计日志，但强烈建议尽可能消费完整的审计日志，以便在调查异常时获得最全面的上下文信息。\\\\n\\\\n*   **操作示例与检测思路**\\\\n    *   文章通过一个销售人员离职的 UI 示例，展示了如何利用审计日志和异常事件来调查可疑活动（如 `unexpected_scraping`、`user_agent` 异常和 `excessive_downloads`）。\\\\n    *   **会话终止：** 对于可疑活动，可直接从审计日志控制面板终止用户的活跃会话，强制其重新认证。\\\\n    *   **其他检测考量：** 即使未生成 `ip_address` 或 `session_fingerprint` 异常，也应检查相关 IP 地址和会话指纹，结合用户代理等信息，判断活动是否来自预期来源，以识别潜在的恶意行为者。建议通过白名单方式管理预期用户代理，而非黑名单。","translated_title":"Slack 审计日志与异常事件"},{"title":"Astra 动态数据块：我们如何通过重新设计 Astra 的关键部分来节省成本 (原标题: Astra Dynamic Chunks: How We Saved by Redesigning a Key Part of Astra)","link":"https://slack.engineering/astra-dynamic-chunks-how-we-saved-by-redesigning-a-key-part-of-astra/","pubDate":"Mon, 18 Nov 2024 22:06:23 +0000","isoDate":"2024-11-18T22:06:23.000Z","content":"<p>Introduction Slack handles a lot of log data. In fact, we consume over 6 million log messages per second. That equates to over 10 GB of data per second! And it’s all stored using Astra, our in-house, open-source log search engine. To make this data searchable, Astra groups it by time and splits the data&#8230;</p>\\n<p>The post <a rel=\\"nofollow\\" href=\\"https://slack.engineering/astra-dynamic-chunks-how-we-saved-by-redesigning-a-key-part-of-astra/\\">Astra Dynamic Chunks: How We Saved by Redesigning a Key Part of Astra</a> appeared first on <a rel=\\"nofollow\\" href=\\"https://slack.engineering\\">Engineering at Slack</a>.</p>\\n","contentSnippet":"Introduction Slack handles a lot of log data. In fact, we consume over 6 million log messages per second. That equates to over 10 GB of data per second! And it’s all stored using Astra, our in-house, open-source log search engine. To make this data searchable, Astra groups it by time and splits the data…\\nThe post Astra Dynamic Chunks: How We Saved by Redesigning a Key Part of Astra appeared first on Engineering at Slack.","creator":"George Luong","encodedSnippet":"<p>Introduction Slack handles a lot of log data. In fact, we consume over 6 million log messages per second. That equates to over 10 GB of data per second! And it’s all stored using Astra, our in-house, open-source log search engine. To make this data searchable, Astra groups it by time and splits the data&#8230;</p>\\n<p>The post <a rel=\\"nofollow\\" href=\\"https://slack.engineering/astra-dynamic-chunks-how-we-saved-by-redesigning-a-key-part-of-astra/\\">Astra Dynamic Chunks: How We Saved by Redesigning a Key Part of Astra</a> appeared first on <a rel=\\"nofollow\\" href=\\"https://slack.engineering\\">Engineering at Slack</a>.</p>\\n\\nIntroduction\\nSlack handles a lot of log data. In fact, we consume over 6 million log messages per second. That equates to over 10 GB of data per second! And it’s all stored using Astra, our in-house, open-source log search engine. To make this data searchable, Astra groups it by time and splits the data into blocks that we refer to as “chunks”.\\nInitially, we built Astra with the assumption that all chunks would be the same size. However, that assumption has led to inefficiencies from unused disk space and resulted in extra spend for our infrastructure. \\n\\nWe decided to tackle that problem in a pursuit to decrease the cost to operate Astra.\\nThe Problem with Fixed-Size Chunks\\nThe biggest problem with fixed-sized chunks was the fact that not all of our chunks were fully utilized, leading to differently sized chunks. While assuming fixed-sized chunks simplified the code, it also led to us allocating more space than required on our cache nodes, resulting in unnecessary spend.\xa0\\nPreviously, each cache node was given a fixed number of slots, where each slot would be assigned a chunk. While this simplified the code, it meant that undersized chunks of data would have more space allocated for them than required.\\nFor instance, on a 3TB cache node, we would have 200 slots, where each slot was expected to hold a 15GB chunk. However, if any chunks were undersized (say 10GB instead of 15GB), this would result in extra space (5GB) being allocated but not used. On clusters where we’d have thousands of chunks, this quickly led to a rather large percentage of space being allocated but unused.\\nAn additional problem with fixed-sized chunks was that some chunks were actually bigger than our assumed size. This could potentially happen whenever Astra created a recovery task to catch up on older data. We create recovery tasks based on the number of messages that we’re behind and not the size of data we’re behind. If the average size of each message is bigger than we expect, this can result in an oversized chunk being created, which is even worse than undersized chunks as it means we aren’t allocating enough space.\\nDesigning Dynamic Chunks\\nAstra’s architecture diagram – “ZK” is the Zookeeper store which holds metadata.\\nIn order to build dynamic chunks, we had to modify two parts of Astra: the Cluster Manager and the Cache.\\nRedesigning Cache Nodes\\nWe first looked at how cache nodes are structured: Previously, whenever a cache node came online, it would advertise its number of slots in Zookeeper (our centralized coordination store). Then, the Astra manager would assign each slot a chunk, and the cache node would go and download and serve that chunk.\xa0\\nEach cache node has a lifecycle:\\nCache node comes online, advertises the # of slots it has.\\nManager picks up on the slots, and assigns a chunk to each one.\\nEach cache node downloads the chunks assigned to its slots.\\nThis had the benefit of the slots being ephemeral, meaning whenever a cache node went offline, its slots would disappear from Zookeeper, and the manager would reassign the chunks the slots used to hold.\\nHowever, with dynamic chunks, each cache node could only advertise their capacity, as it would not know ahead of time how many chunks it would be assigned. This meant we unfortunately could no longer rely on slots to provide these benefits to us.\xa0\\nTo fix these two problems, we decided to persist two new types of data in Zookeeper: the cache node assignment and the cache node metadata.\\nHere’s a quick breakdown:\\nCache Node Assignment: a mapping of chunk ID to cache node\\nCache Node Metadata: metadata about each cache node, including capacity, hostname, etc.\\nUtilizing these two new types of data, the new flow looks like this:\\nCache node comes online, advertises its disk space.\\nManager picks up on the disk space each cache node has, and creates assignments for each cache node, utilizing bin packing to minimize the number of cache nodes it uses.\\nCache nodes pick up on the assignments that were created for it, and downloads its chunks.\\nRedesigning the manager\\nThe next change was in the manager, upgrading it to utilize the two new types of data we introduced: the cache node assignments and the cache node metadata.\\nTo utilize the cache node assignments, we decided to implement first-fit bin packing to decide which cache node should be assigned which chunk. We then used the cache node metadata in order to make appropriate decisions regarding whether or not we could fit a certain chunk into a given cache node.\\nPreviously, the logic for assigning slots was:\\nGrab the list of slots\\nGrab the list of chunks to assign\\nZip down both lists, assigning a slot to a chunk\\nNow, the logic looks like this:\\nGrab list of chunks to assign\\nGrab list of cache nodes\\nFor each chunk\\n\\nPerform first-fit bin packing to determine which cache node it should be assigned to\\nPersist the mapping of cache node to chunk\\nBin Packing\\nThe most juicy part of redesigning the manager was implementing the first-fit bin packing. It’s a well-known problem of minimizing the number of bins (cache nodes) used to hold a certain amount of items (chunks). We decided to use first-fit bin packing, favoring it for its speed and ease of implementation.\\nUsing pseudocode, we describe the bin-packing algorithm:\\nfor each chunk\\n  for each cache node\\n    if the current chunk fits into the cache node:\\n      assign the chunk\\n    else:\\n      move on to the next cache node\\n  if there aren’t any cache nodes left and the chunk hasn’t been assigned:\\n    create a new cache node\\nThis helped ensure that we were able to pack the cache nodes as tightly as possible, resulting in a higher utilization of allocated space.\\nRolling it out\\nOverall, this was a significant change to the Astra codebase. It touched many key parts of Astra, essentially rewriting all of the logic that handled the assignment and downloading of chunks. With such a change, we wanted to be careful with the roll out to ensure that nothing would break.\xa0\\nTo ensure nothing would break we did the following:\\nHosted two replicas of the same data\\nPlaced all dynamic chunk code behind a feature flag\\nWe leaned heavily on these two guardrails in order to ensure a safe roll out.\xa0\\nHosting two replicas of the same data allowed us to incrementally deploy to one of the two replicas and monitor its behavior. It also ensured that if our changes ever broke anything, we’d still have a second replica able to serve the data.\\nHaving all the code behind a feature flag allowed us to merge the code into master early on, as it wouldn’t run unless explicitly enabled. It also allowed us to incrementally roll out and test our changes. We started with smaller clusters, before moving on to bigger and bigger clusters after verifying everything worked.\\nResults\\nWhat kind of results did we end up seeing from this? For starters, we were able to reduce the # of cache nodes required by up to 50% for our clusters with many undersized chunks! Overall our cache node costs were reduced by 20%, giving us significant cost savings for operating Astra.\\nAcknowledgments\\nA huge shout out to everyone who has helped along the way to bring dynamic chunks to life:\\nBryan Burkholder\\nGeorge Luong\\nRyan Katkov\\n\xa0\\nInterested in building innovative projects and making developers’ work lives easier? We’re hiring \\nApply now\\n\xa0\\nThe post Astra Dynamic Chunks: How We Saved by Redesigning a Key Part of Astra appeared first on Engineering at Slack.","summary":"Slack 的内部日志搜索引擎 Astra 每秒处理海量日志数据（超过 600 万条消息，10 GB）。Astra 将数据分组为“数据块”（chunks）以供搜索。\\\\n\\\\n**核心问题：**\\\\n*   最初，Astra 假设所有数据块大小固定，但这导致了效率低下和成本增加。\\\\n*   许多数据块未被充分利用（即“欠尺寸”数据块），造成大量磁盘空间浪费。\\\\n*   有时数据块会超出预期大小（即“过尺寸”数据块），导致分配空间不足。\\\\n\\\\n**解决方案：**\\\\n*   重新设计 Astra，引入“动态数据块”机制，以优化存储利用率并降低运营成本。\\\\n\\\\n**实现细节：**\\\\n*   **缓存节点重构：**\\\\n    *   旧方案中，缓存节点广告“槽位”数量。新方案中，缓存节点广告其“容量”。\\\\n    *   在 Zookeeper 中持久化两种新数据：`缓存节点分配`（数据块 ID 到缓存节点的映射）和`缓存节点元数据`（容量、主机名等）。\\\\n    *   新流程：缓存节点上线 -> 广告磁盘空间 -> 管理器利用“装箱算法”创建分配 -> 缓存节点下载其数据块。\\\\n*   **管理器重构：**\\\\n    *   管理器采用“首次适应装箱算法”（first-fit bin packing）来决定将哪些数据块分配给哪个缓存节点。\\\\n    *   该算法旨在最大限度地利用现有缓存节点空间，确保数据块尽可能紧密地打包，提高空间利用率。\\\\n\\\\n**部署策略：**\\\\n*   由于是重大改动，团队采取了谨慎的部署策略：\\\\n    *   托管两份相同数据的副本，允许逐步部署和监控，并作为故障恢复保障。\\\\n    *   将所有动态数据块代码置于功能标志后，实现增量发布和测试，从小型集群逐步推广到大型集群。\\\\n\\\\n**成果：**\\\\n*   对于包含大量欠尺寸数据块的集群，所需缓存节点数量减少了高达 50%。\\\\n*   整体缓存节点成本降低了 20%，显著节省了 Astra 的运营成本。","translated_title":"Astra 动态数据块：我们如何通过重新设计 Astra 的关键部分来节省成本"},{"title":"没有免费的午餐！ (原标题: There’s No Such Thing as a Free Lunch!)","link":"https://slack.engineering/theres-no-such-thing-as-a-free-lunch/","pubDate":"Thu, 14 Nov 2024 09:00:08 +0000","isoDate":"2024-11-14T09:00:08.000Z","content":"<p>Incident Management takes time Incidents need responders that are trained and experienced.\xa0 At Slack, training is a foundation of our incident management program. Self-service training and live courses based mainly on prepared content are one piece of the puzzle, but there can be a missing piece in many organizations. How can staff get practical experience&#8230;</p>\\n<p>The post <a rel=\\"nofollow\\" href=\\"https://slack.engineering/theres-no-such-thing-as-a-free-lunch/\\">There’s No Such Thing as a Free Lunch!</a> appeared first on <a rel=\\"nofollow\\" href=\\"https://slack.engineering\\">Engineering at Slack</a>.</p>\\n","contentSnippet":"Incident Management takes time Incidents need responders that are trained and experienced.\xa0 At Slack, training is a foundation of our incident management program. Self-service training and live courses based mainly on prepared content are one piece of the puzzle, but there can be a missing piece in many organizations. How can staff get practical experience…\\nThe post There’s No Such Thing as a Free Lunch! appeared first on Engineering at Slack.","creator":"Scott Nelson Windels","encodedSnippet":"<p>Incident Management takes time Incidents need responders that are trained and experienced.\xa0 At Slack, training is a foundation of our incident management program. Self-service training and live courses based mainly on prepared content are one piece of the puzzle, but there can be a missing piece in many organizations. How can staff get practical experience&#8230;</p>\\n<p>The post <a rel=\\"nofollow\\" href=\\"https://slack.engineering/theres-no-such-thing-as-a-free-lunch/\\">There’s No Such Thing as a Free Lunch!</a> appeared first on <a rel=\\"nofollow\\" href=\\"https://slack.engineering\\">Engineering at Slack</a>.</p>\\n\\nIncident Management takes time\\nIncidents need responders that are trained and experienced.\xa0 At Slack, training is a foundation of our incident management program.\\nSelf-service training and live courses based mainly on prepared content are one piece of the puzzle, but there can be a missing piece in many organizations. How can staff get practical experience with incident response before joining a real incident?\\nOur first experience with the Incident Lunch\\nOur first experience with what we now call the Incident Lunch exercise was in a training session with the team from Blackrock 3 Partners in March 2018. They ran a two-day training for a team at Slack centered around The Incident Management System (IMS) and how it can be used to build an incident response program. During our sessions, they ran an exercise they call The Lunch Break exercise. They assign some roles to the group and set a limited time box for the group to get lunch to the training room. The focus was on having some constraints in place, teaching through role modeling some of the incident roles, and putting time pressure on the group. It was a lot of fun and we took that exercise and turned it into a regular occurrence in our Slack training options. It also gave the engineer that led the program an opportunity to get Slack to buy them lunch once a week.\\nThose key elements of this exercise to reiterate were:\\nTime pressure,\\nRole playing,\\nConstraints (they called them considerations, but imagine simple rules like no pizza or fast food),\\nAnd it was fun to boot!\\nBringing the Incident Lunch to the rest of Slack\\nThe team who took our initial incident training sessions wanted to bring things back to a wider group of engineers across Slack. The Incident Lunch we started is easily accessible for anyone in the company — there is no setup or expertise required for the participants. It turns out everyone is a subject matter expert at ordering and eating lunch.\\nFolks are invited to a two-hour incident training exercise at lunchtime, and told that lunch will be provided. When they arrive, everybody gets a 15 minute refresher on our incident process, and then we drop the bomb: the lunch order fell through, so their exercise now is to obtain lunch for everybody in the room, subject to a few constraints. They need to do this using our incident response practices; somebody needs to be the incident commander, we need to communicate in an incident Slack channel, we need to post periodic status reports in the channel, etc.\\nThe trainer acts as referee and coach for the exercise. So far, no team has failed to get lunch, though there have been some close calls!\\nFrom the perspective of our Incident Management Program, minimal resources are needed to repeat the exercise on a regular cadence.\xa0 The framework for running the lunch is:\\nAn outline for the setup and kickoff of the in-person exercise\\n\\nWe use a GitHub repository with a markdown file that we set up to run as a GitHub Pages slide deck. The outline has the 15 minute introduction and refresher into what IMS is and a brief overview of how we respond to incidents at Slack.\\nA conference room or two\\nA workflow to announce and invite people to the exercise\\nOne or two staff to facilitate the exercise (did we mention they get free lunch every time they run the exercise‽)\\nA small budget to pay for lunch, probably around $300-500 per session, an order of magnitude less than any third-party firm might charge for an hour of on-site training.\\nA former lead engineer on a team called App Ops, created a key element for our Slack version that sprinkled some extra fun and more time pressure into the exercise. We’ve come to call what they added, the “Chaos Cards”. These added new key elements to the exercise which are variability and unpredictability. The sets of cards have different actions or events that can change the course of the exercise. One card might be something like the Laptop Trouble card where you pick an SME at random that can no longer use their laptop for the rest of the exercise, or it might be a card that says Eerily Quiet (pick a new card in two minutes!). We play these cards throughout the exercise at a timed cadence, usually starting at five minutes, to add an unpredictable element to the exercise. It often makes participants more uncomfortable, which is something that happens in incidents.\xa0\\nWhy the Incident Lunch has been a success at Slack\\nThere are a few key reasons why this particular exercise has been a success. First I’ll share why it works really well for the team organizing and running it.\\n“I’m sure you all have spare capacity sitting around for engineering folks to create games from scratch and play them with other engineers,” said no one in most companies. Even if you were lucky enough to have some engineers create something like this — were they able to keep it up? I’ve heard tales of companies running a Dungeons and Dragons-like incident game. There is a blog post by Paul Kirk, where he has a great discussion about teaching incident response with games centered around using Keep Talking and Nobody Explodes. Games with a detailed story and choosing your own adventure setup can be really engaging for engineers, but they take a huge amount of investment to set up and keep running or up-to-date. A game centered around something like Keep Talking and Nobody Explodes can be limited to a small number of users and takes some amount of setup by each person participating. Scaling it up can be a growing investment of time. When we started running the Incident Lunch exercise regularly we found some great benefits as a small team trying to keep time for real incidents in our schedule.\xa0\\nWe created a Slack channel for coordinating folks who facilitated our lunch exercise and have things set up so that it can be run in remote offices where you have willing facilitators.\\nIt also works really well for your staff. Any staff can attend. We include our Customer Experience teams and we’ve included folks from Customer Success, Sales, and other non-technical teams in the organization. There is nothing they need to do to prepare for the training, though they should probably bring a laptop and they’ll need to block out two hours of their day. We don’t have any required preparation listed and we only tell them that we’re having an “exercise”. We fib a little bit and part of the setup is that we tell them lunch will be provided so they have an incentive to attend.\\nHow does it work in practice?\\nWhen we run the lunch exercise there are a few simple steps for the facilitator.\\n\xa0Exercise set up\\nInvite folks, make a calendar invite\\nReserve a conference room\\nFind a volunteer who can be in the Incident Commander role. This is often someone who has taken the exercise before and wants to level-up their skills while keeping the exercise surprise\\nCheck that our slides are up to date with any incident process updates\\n\\nIf you are in a new location you may add a map slide with the lunch exclusion zone\\nFacilitating the exercise\\nRun through the training introduction\\n\\nGive some background on IMS and/or Incident Response — the basics of why are we here\\nWalk through some of the common roles in your Incident Response process\\nTalk through how an incident starts at your company\\nWhat is the main goal of your incident response process?\\n\\nAt most places it is to restore service as quickly as possible — say this part out loud\\nGive some tips and tricks commonly used in incident response\\n\\nBe clear and concise\\nDevelop multiple plans\\nUse time boxing to keep things moving\\nFocus on roles not individuals\\nOnce you’ve introduced those basics it’s time to spring the surprise on them.\\n\\nLunch has fallen through. The front desk notified us that the bike messenger delivering lunch ended up on the other side of town.\\nNow they must come together as an incident response team and get lunch delivered as quickly as possible — hopefully before the two hours is up\\nSet the ground rules / constraints\\n\\nOrder must be made outside the lunch exclusion zone, to avoid making things too easy — no running to the Subway across the street\\nThey can pick up or order in (delivery timing is often unpredictable)\\nSet a per-person budget limit (approx $25 USD/person is a good spot)\\nLunch is expensed, so keep receipts (someone has to submit an expense report)\\nThey can use whatever resources at hand — laptops, phones, Slack, Zoom, etc.\\nAnyone with real dietary restrictions must be accommodated (and the chaos cards simulate some dietary restrictions as well)\\nHave the group pick an Incident Commander (we recommend planting this person if you can); for groups entirely made of people new to the process, often the facilitator acts as The Incident Commander.\\nHand it over and start your timer for Chaos Cards\\n\\nHave someone from the group (or the facilitator) pick a chaos card every five minutes: if they’re doing a great job you can speed them up, and if the group is struggling you can slow them down a little bit (remember this should still be fun!)\\nHopefully, once lunch is delivered, we run a quick retrospective while we eat. The facilitator should share some things they noticed but lean into the group to see what insights they had during the exercise.\\nClean up the conference room and you’re all done!\\nWhat have we learned as we run these exercises?\\nAdding the Chaos Cards into the exercise really helped compound the time pressure with a level of unpredictability — that feels more realistic as that is how incidents often unfold in complex systems. The Chaos Cards give you some levers to make things easier or harder as you can slow or speed up the picking of them (or skip them if it would really ruin the day of everyone playing). If you have return participants you can make sure some of the harder chaos cards show up at the top of the stack — playing a card like Network Outage early in the exercise and making everyone figure out how to tether via their cell phones can get pretty spicy.\\nAfter a few runs, we discovered that it can be good to find someone who is willing to be the IC up front. That isn’t always possible, but the worst experiences we had were when no one really was ready to role play the IC and it turned into a struggle. We often didn’t let the person who agreed to start as the IC know too much about the exercise so it was often still a surprise for them. That leads into another great facet of this exercise — you’ll discover folks who have a predisposed skill set for facilitating an incident that you couldn’t uncover in a normal training session. This became an internal recruiting tool for us. It also can be a tool to build up ICs who want more experience and are willing to play along. If you end up in the situation where someone in the IC role is struggling, you can coach them, ask questions to prompt them, and slow down the cadence of drawing additional chaos cards.\\nPeople will quickly forget or become sidetracked in pursuit of their primary goal. Mitigating the issue and restoring service as quickly as possible is the goal in a real incident response. If you run these you’ll find that teams that finish quickly can get their order in within 15-20 minutes of starting the exercise. When a team decides they want to take a poll across the assembled responders about where to eat (not too different from asking everyone in an incident to weigh in on the best solution), you’d better hope you’ll get lunch within the two hour window. Incident Response isn’t an exercise in democracy — it’s about making decisions quickly and efficiently and often making trade offs that you wouldn’t make without time pressure — like choosing your favorite food of the day instead of thinking about what will be the fastest food option. If they do take a poll, that’s a great opportunity to talk about gaining fast consensus in an incident context by using the, “Are there any strong objections?” tactic.\\nWe also see that choosing delivery instead of picking up in person is more likely to slow things down. You’re adding more complexity into your response by adding new dependencies. We had one lunch where the order was never delivered to the restaurant from the online ordering system; we ended up with a pretty late lunch that day.\\nWhat improvements are still on the table?\\nA big caveat for this exercise is that it works best in person. We never figured out a virtual substitute during the pandemic that seemed to satisfy how easy this is to run in real life. Finding alternate versions of the exercise that were as simple but work in a hybrid or remote work environment would be a great upgrade. As we experimented with running these we would often include one or two remote employees but didn’t have a requirement to deliver lunch to them, we’d prepare them beforehand if they needed their own lunch. This did give an option that meant they had to include someone remote though and added a nice realism to how our work environment is day-to-day.\\nWe don’t use our daily incident tooling during The Incident Lunch. Setting up a large group of users in our staging environment would add too much overhead. Having an ability to use our internal tooling in dry-run or demo mode would add a nice touch to the exercise by giving participants hands-on experience with the tooling they’ll use in a real incident.\\nKeep a log of your retrospective insights and notes about what happened in each lunch. They can be helpful as you look back to help you evolve your program.\\nWe hope that you’ll share with us if you implement The Incident Lunch exercise for your teams and let us know what you learn and how the exercise evolves in your environment. Thanks for reading!\\nReferences\\nSev0 Conference 2024: There is no such thing as a free lunch. How Slack runs their incident lunch exercise talk by Scott Nelson Windels\\nPagerDuty Summit 2021: It all Starts with a Page – How CE and Engineering respond to Incidents together at Slack talk by Niamh Tighe and Scott Nelson Windels\\nSRECon 2021: Evolution of Incident Management at Slack talk by Brent Chapman\\nSlack Engineering Blog All Hands on Deck by Ryan Katkov\\nMedium blog post Teach Incident Response with Games by Paul Kirk\\nIncident Lunch example public repository\\nAcknowledgments\\nThis work could not have happened without the original training and ideas from the Blackrock 3 Partners. The dedication of staff at Slack who made sure this got off the ground, Tricia Bogen, Joe Smith, and Brent Chapman, was key to making this a success.\\nThe post There’s No Such Thing as a Free Lunch! appeared first on Engineering at Slack.","summary":"### Slack的“事件午餐”：实用的事件管理培训\\\\n\\\\n文章详细介绍了Slack如何通过其独特的“事件午餐”练习，为员工提供实用的事件响应经验，以弥补传统培训中实践环节的缺失。该练习旨在模拟真实事件的压力和复杂性，同时保持趣味性。\\\\n\\\\n**核心理念与起源：**\\\\n*   **问题：** 员工如何在实际事件发生前获得事件响应的实践经验？\\\\n*   **灵感：** 2018年，Slack团队在Blackrock 3 Partners的培训中接触到“午餐休息练习”，该练习通过角色扮演、时间压力和限制条件来模拟事件响应。\\\\n\\\\n**“事件午餐”练习详解：**\\\\n*   **目标：** 让员工在模拟环境中运用事件管理流程（如指定事件指挥官、使用Slack频道沟通、发布状态报告）来解决问题。\\\\n*   **设置：** 参与者被邀请参加两小时的午餐培训，并被告知会提供午餐。到达后，他们被告知午餐订单出了问题，其任务是使用事件响应实践为所有人获取午餐。\\\\n*   **关键要素：**\\\\n    *   **时间压力：** 必须在有限时间内获取午餐。\\\\n    *   **角色扮演：** 参与者扮演事件指挥官等角色。\\\\n    *   **约束条件：** 例如，有预算限制、不能在特定区域点餐、需满足饮食限制等。\\\\n    *   **趣味性：** 整个过程被设计得充满乐趣。\\\\n    *   **“混沌卡片”（Chaos Cards）：** 引入随机性和不可预测性，模拟真实事件中的突发情况（如笔记本故障、网络中断），增加挑战和真实感。\\\\n\\\\n**成功的原因：**\\\\n*   **组织者角度：** 相比复杂的模拟游戏，该练习设置和维护成本低，易于定期开展。\\\\n*   **参与者角度：**\\\\n    *   **高度可及性：** 对公司内所有员工（包括非技术团队）开放，无需专业知识或前期准备。\\\\n    *   **发现人才：** 有助于发现和培养具有事件协调和指挥潜力的员工。\\\\n    *   **实践决策：** 强调在时间压力下快速高效地做出决策，而非民主投票，这与真实事件响应的核心目标一致。\\\\n\\\\n**实践中的经验与教训：**\\\\n*   “混沌卡片”有效增加了真实感和挑战性。\\\\n*   预先指定事件指挥官有助于练习顺利进行。\\\\n*   团队在时间压力下容易偏离主要目标（尽快恢复服务），强调快速决策的重要性。\\\\n*   选择外卖而非自取可能增加复杂性。\\\\n\\\\n**未来改进方向：**\\\\n*   寻找适用于混合或远程工作环境的虚拟替代方案。\\\\n*   将日常事件工具集成到练习中，提供更真实的工具操作经验。\\\\n*   记录每次练习的复盘洞察和笔记，以持续改进项目。\\\\n\\\\n该练习以其简单、有效和趣味性，成为Slack事件管理培训项目的重要组成部分，帮助员工在低风险环境中磨练事件响应技能。","translated_title":"没有免费的午餐！"}],"lastUpdated":"2025-05-28T18:16:17.328Z"}')
    }
}]);
