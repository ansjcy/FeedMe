{
  "sourceUrl": "https://rsshub.rssforever.com/google/research",
  "title": "Google Research Blog",
  "description": "Google Research Blog - Powered by RSSHub",
  "link": "https://research.google/blog",
  "items": [
    {
      "title": "MLE-STAR：一种最先进的机器学习工程代理 (原标题: MLE-STAR: A state-of-the-art machine learning engineering agent)",
      "link": "https://research.google/blog/mle-star-a-state-of-the-art-machine-learning-engineering-agents/",
      "pubDate": "Thu, 31 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-31T16:00:00.000Z",
      "creator": "Google",
      "summary": "# MLE-STAR：一种最先进的机器学习工程代理\n\n## 引言：机器学习工程代理的兴起与局限\n机器学习（ML）的快速发展推动了高性能应用在各种现实场景中的广泛部署。然而，构建这些模型对机器学习工程师来说仍然是一项艰巨的任务，需要大量的迭代实验和复杂的数据工程。为了简化这些高要求的工作流程，近期研究集中于利用大型语言模型（LLMs）作为机器学习工程（MLE）代理。这些代理利用其固有的编码和推理能力，将ML任务概念化为代码优化挑战，并探索潜在的代码解决方案，最终根据提供的任务描述和数据集生成可执行代码（如Python脚本）。\n\n![MLE-STAR-1-MLEAgents](https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-1-MLEAgents.width-1250.png)\n*ML工程代理旨在通过分析任务描述和跨多种模态的数据集来解决各种机器学习挑战。*\n\n尽管现有MLE代理取得了初步进展，但它们面临一些限制，这些限制削弱了其有效性：\n*   **知识偏见：** 过度依赖LLM的现有知识，导致偏向熟悉和常用方法（例如，表格数据使用scikit-learn库），可能忽略更优的任务特定方法。\n*   **探索策略不足：** 通常采用同时修改整个代码结构的探索策略，导致代理过早地将重点转移到其他阶段（例如模型选择或超参数调整），因为它们缺乏在特定管道组件（如特征工程）内进行深入迭代探索的能力。\n\n## 介绍MLE-STAR\n我们提出了一种新颖的ML工程代理——MLE-STAR，它集成了网络搜索和有针对性的代码块细化。与现有替代方案不同，MLE-STAR通过以下方式解决ML挑战：\n1.  **网络搜索：** 首先通过网络搜索寻找合适的模型，以奠定坚实的基础。\n2.  **代码细化：** 通过测试代码中最重要的部分来仔细改进这个基础。\n3.  **模型融合：** 利用一种新方法将多个模型融合在一起，以获得更好的结果。\n\nMLE-STAR表现出色，在MLE-Bench-Lite的Kaggle竞赛中赢得了63%的奖牌，显著优于其他替代方案。\n\n## MLE-STAR的工作原理\n### 1. 初始解决方案生成\nMLE-STAR利用网络搜索检索相关且可能是最先进的方法，以构建初始解决方案代码。\n\n### 2. 解决方案增强：有针对性的代码块细化\nMLE-STAR通过以下步骤增强解决方案：\n*   **提取代码块：** 提取代表不同ML管道组件（如特征工程或集成构建）的特定代码块。\n*   **集中探索：** 专注于探索针对该组件的策略，并根据之前的尝试进行反馈。\n*   **消融研究：** 进行消融研究以确定对性能影响最大的代码块。\n*   **迭代细化：** 重复此细化过程，修改各种代码块。\n\n![MLE-STAR-2-Overview](https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-2-Overview.width-1250.png)\n*概述。(a) MLE-STAR首先使用网络搜索查找并将任务特定模型整合到初始解决方案中。(b) 对于每个细化步骤，它进行消融研究以找出对性能影响最大的代码块。(c) 识别出的代码块随后根据LLM建议的计划进行迭代细化，这些计划利用先前实验的反馈探索各种策略。选择和细化目标代码块的过程重复进行，其中(c)中改进的解决方案成为(b)中下一个细化步骤的起点。*\n\n### 3. 新颖的集成方法\nMLE-STAR首先提出多个候选解决方案。然后，它不依赖于基于验证分数的简单投票机制，而是使用代理自身提出的集成策略将这些候选方案合并为一个改进的解决方案。这种集成策略会根据先前策略的性能进行迭代细化。\n\n![MLE-STAR-3-Ensembling](https://storage.com/gweb-research2023-media/images/MLE-STAR-3-Ensembling.width-1250.png)\n*解决方案集成：MLE-STAR在连续尝试中细化其集成策略，有效地将多个并行生成的解决方案组合成一个改进的解决方案。*\n\n### 4. 增强鲁棒性的附加模块\nMLE-STAR还包含三个附加模块以增强其鲁棒性：\n*   **(i) 调试代理：** 如果Python脚本执行触发错误，MLE-STAR会使用调试模块尝试纠正。\n*   **(ii) 数据泄露检查器：** 分析解决方案脚本，防止数据泄露（例如，在训练数据准备期间不当访问测试数据集信息）。\n*   **(iii) 数据使用检查器：** 确保利用所有提供的相关数据源，因为LLM生成的脚本有时会忽略非CSV等简单格式的数据。\n\n## 评估与结果\n我们使用MLE-Bench-Lite中的Kaggle竞赛对MLE-STAR进行了全面评估。结果表明，MLE-STAR仅需最少的人工干预（例如，定义可泛化到任何任务的初始提示），就显著优于之前的替代方案，包括那些需要人工收集Kaggle策略的方案。\n\n![MLE-STAR-4-Results](https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-4-Results.width-1250.png)\n*MLE-Bench-Lite的主要结果。分数表示在MLE-Bench-Lite的Kaggle竞赛中获得奖牌的平均百分比。*\n\n具体而言，与表现最佳的基线相比，MLE-STAR在获得任何奖牌方面的提升显著，从25.8%提高到63.6%。\n\n## 对MLE-STAR性能提升的深入分析\n为了理解MLE-STAR性能提升的来源，我们从多个角度进行了分析：\n*   **模型使用：** MLE-STAR主要使用更新、更具竞争力的模型（如EfficientNet或ViT），而其他代理可能仍使用过时模型（如ResNet），这是性能提升的原因之一。\n*   **人工干预：** MLE-STAR能够以最少的人工干预轻松采用更新的模型。例如，通过手动添加RealMLP的模型描述，MLE-STAR成功将其训练代码集成到框架中。\n\n![MLE-STAR-5-ClassificationIntervention](https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-5-ClassificationIntervention.width-1250.png)\n*左图：图像分类竞赛中的模型使用率（%）。右图：展示人工干预：MLE-STAR根据手动模型描述集成模型的训练代码。*\n\n*   **LLM不当行为及纠正：**\n    *   **幻觉/不切实际的代码：** LLM生成的代码有时内容不切实际（例如，使用测试数据统计信息进行预处理）。MLE-STAR的数据泄露检查器能识别并纠正此类问题。\n    *   **忽略数据源：** LLM生成的脚本有时会忽略部分提供的数据源。MLE-STAR的数据使用检查器会重新检查任务描述，确保所有给定数据都被利用。\n\n![MLE-STAR-6-LeakageUsage](https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-6-LeakageUsage.width-1250.png)\n*左图：MLE-STAR的数据泄露检查器确保适当的预处理。右图：MLE-STAR的数据使用检查器识别并整合了以前未使用的信息。*\n\n## 结论\n我们提出了MLE-STAR，一种专为多样化ML任务设计的新型机器学习工程代理。其核心思想是利用网络搜索检索有效的模型，然后探索针对特定ML管道组件的各种策略以改进解决方案。MLE-STAR的有效性通过在MLE-Bench-Lite Kaggle竞赛中赢得63%的奖牌（其中36%为金牌）得到验证。\n\n通过自动化复杂的ML任务，MLE-STAR可以降低个人和组织利用ML的门槛，从而促进各行各业的创新。此外，由于最先进的模型不断更新和改进，MLE-STAR生成的解决方案性能预计将自动提升，因为它利用搜索引擎从网络检索有效模型。这种固有的适应性确保了随着ML领域的发展，MLE-STAR能够持续提供越来越好的解决方案。最后，开发者和研究人员现在可以通过我们新发布的基于Agent Development Kit (ADK) 构建的MLE-STAR开源代码库来加速他们的机器学习项目。",
      "shortSummary": "MLE-STAR是一种新型机器学习工程代理，通过结合网络搜索和有针对性的代码块细化来解决ML任务。它能自动寻找最新模型，并迭代优化特定代码组件。MLE-STAR还包含创新的模型集成方法及调试、数据泄露和数据使用检查器，增强了鲁棒性。在Kaggle竞赛中，MLE-STAR以最少的人工干预赢得了63%的奖牌，显著优于现有方案。它降低了ML应用门槛，并能随ML领域发展自动适应最新模型。其代码库已开源。",
      "translated_title": "MLE-STAR：一种最先进的机器学习工程代理",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-1-MLEAgents.width-1250.png",
          "alt": "MLE-STAR-1-MLEAgents",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-2-Overview.width-1250.png",
          "alt": "MLE-STAR-2-Overview",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-3-Ensembling.width-1250.png",
          "alt": "MLE-STAR-3-Ensembling",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-4-Results.width-1250.png",
          "alt": "MLE-STAR-4-Results",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-5-ClassificationIntervention.width-1250.png",
          "alt": "MLE-STAR-5-ClassificationIntervention",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"gew25\">The rise of machine learning (ML) has fueled the development of high-performance applications across a wide array of real-world scenarios, from <a href=\"https://arxiv.org/pdf/1603.02754\" target=\"_blank\" rel=\"noopener noreferrer\">tabular classification</a> to <a href=\"https://vciba.springeropen.com/articles/10.1186/s42492-019-0016-7\" target=\"_blank\" rel=\"noopener noreferrer\">image denoising</a>. However, crafting these models remains an arduous endeavor for machine learning engineers, demanding extensive iterative experimentation and data engineering. To streamline these demanding workflows, <a href=\"https://arxiv.org/pdf/2502.13138\" target=\"_blank\" rel=\"noopener noreferrer\">recent investigations</a> have concentrated on leveraging large language models (LLMs) as machine learning engineering (MLE) agents. By capitalizing on their inherent coding and reasoning skills, these agents conceptualize ML tasks as code optimization challenges. They then explore potential code solutions, ultimately generating executable code (such as a Python script) based on a provided task description and datasets.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-1-MLEAgents.width-1250.png\" alt=\"MLE-STAR-1-MLEAgents\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-1-MLEAgents.width-1250.png\" alt=\"MLE-STAR-1-MLEAgents\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8ay0k\"><i>ML engineering agents are built to tackle diverse machine learning challenges by analyzing a task description and datasets that can span various modalities. Their ultimate goal is to pinpoint the best solution for the given problem.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"gew25\">Despite their promising initial strides, current MLE agents face several limitations that curtail their efficacy. First, their heavy reliance on pre-existing LLM knowledge often leads to a bias towards familiar and frequently used methods (e.g., the <a href=\"https://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">scikit-learn</a> library for tabular data), overlooking potentially superior task-specific approaches. Furthermore, <a href=\"https://arxiv.org/pdf/2402.17453\" target=\"_blank\" rel=\"noopener noreferrer\">these agents</a> typically employ an exploration strategy that modifies the entire code structure simultaneously in each iteration. This frequently causes agents to prematurely shift focus to other stages (e.g., model selection or hyperparameter tuning) because they lack the capacity for deep, iterative exploration within specific pipeline components, such as exhaustively experimenting with different feature engineering options.</p><p data-block-key=\"cgeb0\">In our recent <a href=\"https://arxiv.org/abs/2506.15692\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>, we introduce MLE-STAR, a novel ML engineering agent that integrates <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-google-search\" target=\"_blank\" rel=\"noopener noreferrer\">web search</a> and targeted code block refinement. Unlike alternatives, MLE-STAR tackles ML challenges by first searching the web for proper models to get a solid foundation. It then carefully improves this foundation by testing which parts of the code are most important. MLE-STAR also utilizes a new method to blend several models together for even better results. This approach is very successful — it won medals in 63% of the Kaggle competitions in MLE-Bench-Lite, significantly outperforming the alternatives.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Introducing MLE-STAR</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"gew25\">To generate initial solution code, MLE-STAR uses web search to retrieve relevant and potentially state-of-the-art approaches that could be effective for building a model.<footnote id=\"da8046d1-1b0f-4379-b092-7434a3d4b87d\">[da8046]</footnote> To enhance the solution, MLE-STAR extracts a specific code block representing a distinct ML pipeline component, like feature engineering or ensemble building. It then concentrates on exploring strategies tailored to that component, reflecting on previous attempts as feedback. To identify the code block with the most significant impact on performance, MLE-STAR conducts an ablation study that evaluates the contribution of each ML component. This refinement process is repeated, modifying various code blocks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-2-Overview.width-1250.png\" alt=\"MLE-STAR-2-Overview\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-2-Overview.width-1250.png\" alt=\"MLE-STAR-2-Overview\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8ay0k\"><i>Overview. (</i><b><i>a</i></b><i>) MLE-STAR begins by using web search to find and incorporate task-specific models into an initial solution. (</i><b><i>b</i></b><i>) For each refinement step, it conducts an ablation study to pinpoint the code block with the most significant impact on performance. (</i><b><i>c</i></b><i>) The identified code block then undergoes iterative refinement based on LLM-suggested plans, which explore various strategies using feedback from prior experiments. This process of selecting and refining target code blocks repeats, where the improved solution from (</i><b><i>c</i></b><i>) becomes the starting point for the next refinement step in (</i><b><i>b</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"gew25\">Additionally, we present a novel method for generating ensembles. MLE-STAR first proposes multiple candidate solutions. Then, instead of relying on a simple voting mechanism based on validation scores, MLE-STAR merges these candidates into a single, improved solution using an ensemble strategy proposed by the agent itself. This ensemble strategy is iteratively refined based on the performance of the preceding strategies.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-3-Ensembling.width-1250.png\" alt=\"MLE-STAR-3-Ensembling\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-3-Ensembling.width-1250.png\" alt=\"MLE-STAR-3-Ensembling\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8ay0k\"><i>Ensembling Solutions: MLE-STAR refines its ensemble strategies over successive attempts, efficiently combining multiple parallel-generated solutions into a single, improved solution.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"gew25\">Last but not least, MLE-STAR incorporates three additional modules to enhance its robustness: (i) a debugging agent, (ii) a data leakage checker, and (iii) a data usage checker. For the debugging agent, if the execution of a Python script triggers an error, leading to a record (such as a traceback), MLE-STAR employs a debugging module to attempt correction. Regarding the data leakage checker, we've observed that LLM-generated Python scripts carry the risk of introducing data leakage, for instance, by improperly accessing information from a test dataset during training data preparation. To address this, we've introduced a checker agent that analyzes the solution script prior to its execution. As for the data usage checker, we've noticed that LLM-generated scripts sometimes neglect to use all provided data sources, focusing solely on simple formats like CSVs. To ensure the utilization of all relevant provided data, MLE-STAR includes a data usage checker agent.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Evaluations and results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"gew25\">To validate its effectiveness, we conducted comprehensive evaluations of MLE-STAR using the <a href=\"https://www.kaggle.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Kaggle</a> competitions within <a href=\"https://arxiv.org/pdf/2410.07095\" target=\"_blank\" rel=\"noopener noreferrer\">MLE-Bench-Lite</a>. Here, we utilized an additional agent that takes the task description and the final solution as input, and outputs the code that incorporates loading the test sample and creating a submission file.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-4-Results.width-1250.png\" alt=\"MLE-STAR-4-Results\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-4-Results.width-1250.png\" alt=\"MLE-STAR-4-Results\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8ay0k\"><i>Main results from MLE-Bench-Lite. Scores represent the average % of achievements in Kaggle competitions in MLE-Bench-Lite.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"gew25\">The experimental results presented in the figure above demonstrate that MLE-STAR, requiring only minimal human effort (e.g., defining initial prompts that are generalizable to any tasks), significantly outperforms previous alternatives, including <a href=\"https://arxiv.org/pdf/2402.17453\" target=\"_blank\" rel=\"noopener noreferrer\">those</a> necessitating manual labor to collect strategies from Kaggle. Specifically, MLE-STAR achieves a substantial gain in any medal achievement, improving it from 25.8% to 63.6% when compared to the top-performing <a href=\"https://arxiv.org/pdf/2502.13138\" target=\"_blank\" rel=\"noopener noreferrer\">baseline</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">In-depth analysis of MLE-STAR's gains</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"gew25\">To understand the sources of MLE-STAR's performance gains, we conducted several analyses from various perspectives. Here, we examined (i) the types of ML models that MLE-STAR utilizes, (ii) how MLE-STAR can be extended with human intervention, and (iii) how the additional data leakage and usage checkers further improve MLE-STAR's performance.</p><ul><li data-block-key=\"4ijkt\"><i>Model usage</i>: Consider model usage by two MLE-agents. <a href=\"https://arxiv.org/pdf/2502.13138\" target=\"_blank\" rel=\"noopener noreferrer\">AIDE</a> primarily employs <a href=\"https://arxiv.org/pdf/1512.03385\" target=\"_blank\" rel=\"noopener noreferrer\">ResNet</a> for image classification. However, <a href=\"https://arxiv.org/pdf/1512.03385\" target=\"_blank\" rel=\"noopener noreferrer\">ResNet</a>, released in 2015, is now considered outdated and can result in suboptimal performance. In contrast, MLE-STAR primarily utilizes more recent and competitive models like <a href=\"https://arxiv.org/pdf/1905.11946\" target=\"_blank\" rel=\"noopener noreferrer\">EfficientNet</a> or <a href=\"https://arxiv.org/pdf/2010.11929\" target=\"_blank\" rel=\"noopener noreferrer\">ViT</a>, leading to the observed performance gain.<br></li><li data-block-key=\"9jkbs\"><i>Human intervention</i>: MLE-STAR readily adopts even more recent models with minimal human intervention. While MLE-STAR automatically constructs a model description using web search, a natural extension involves leveraging human expertise for this construction. By manually adding a model description for <a href=\"https://arxiv.org/pdf/2407.04491\" target=\"_blank\" rel=\"noopener noreferrer\">RealMLP</a>, MLE-STAR successfully integrates its training code into the framework, a model not previously retrieved.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-5-ClassificationIntervention.width-1250.png\" alt=\"MLE-STAR-5-ClassificationIntervention\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-5-ClassificationIntervention.width-1250.png\" alt=\"MLE-STAR-5-ClassificationIntervention\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8ay0k\"><b><i>Left:</i></b> <i>Model usage (%) in image classification competitions.</i> <b><i>Right:</i></b><i> Demonstrating human intervention: MLE-STAR integrates a model's training code based on a manual model description.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <ul><li data-block-key=\"gew25\"><i>LLM misbehavior and corrections</i>: We observed that while the code generated by the LLM executed correctly, its content was sometimes unrealistic, exhibiting hallucination. For example, the figure below (<b>left</b>) illustrates an impractical approach where test data is pre-processed using its own statistics. Since test data must remain unseen, correction in the code is necessary, for which MLE-STAR employs a data leakage checker to identify such issues and refine the generated script if a problem is detected.</li><li data-block-key=\"5bfa0\">We also observed that LLMs often generate scripts that overlook some of the provided data sources. To address this, MLE-STAR employs a data usage checker, which re-examines the task description to ensure that all given data is utilized. As shown in (<b>right</b>), this design enables MLE-STAR to incorporate previously neglected data.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-6-LeakageUsage.width-1250.png\" alt=\"MLE-STAR-6-LeakageUsage\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-6-LeakageUsage.width-1250.png\" alt=\"MLE-STAR-6-LeakageUsage\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8ay0k\"><b><i>Left:</i></b><i> MLE-STAR's data leakage checker ensures appropriate preprocessing.</i> <b><i>Right:</i></b><i> MLE-STAR's data usage checker identifies and incorporates previously unused information.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"gew25\">We proposed MLE-STAR, a novel machine learning engineering agent designed for diverse ML tasks. Our core idea is to utilize <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-google-search\" target=\"_blank\" rel=\"noopener noreferrer\">web search</a> to retrieve effective models and then explore various strategies targeting specific ML pipeline components to improve the solution. The effectiveness of MLE-STAR is validated by winning medals in 63% (36% of which are gold medals) of the <a href=\"https://arxiv.org/pdf/2410.07095\" target=\"_blank\" rel=\"noopener noreferrer\">MLE-Bench-Lite</a> Kaggle competitions.</p><p data-block-key=\"bmbh8\">By automating complex ML tasks, MLE-STAR could lower the barrier to entry for individuals and organizations seeking to leverage ML, potentially fostering innovation across various sectors. Furthermore, as state-of-the-art models are continually updated and improved, the performance of solutions generated by MLE-STAR is expected to automatically boost. This is because our framework leverages a search engine to retrieve effective models from the web to form its solutions. This inherent adaptability ensures that MLE-STAR continues to provide increasingly better solutions as the field of ML advances. Last but not least, developers and researchers can now accelerate their machine learning projects by using our newly released <a href=\"https://github.com/google/adk-samples/tree/main/python/agents/machine-learning-engineering\" target=\"_blank\" rel=\"noopener noreferrer\">open-source codebase</a> of MLE-STAR, built with the <a href=\"https://github.com/google/adk-samples/tree/main/python/agents\" target=\"_blank\" rel=\"noopener noreferrer\">Agent Development Kit</a> (ADK).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"gew25\"><i>We gratefully acknowledge the contributions of Jiefeng Chen, Jinwoo Shin, Sercan O Arik, Raj Sinha, and Tomas Pfister.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "使用回归语言模型模拟大型系统 (原标题: Simulating large systems with Regression Language Models)",
      "link": "https://research.google/blog/simulating-large-systems-with-regression-language-models/",
      "pubDate": "Mon, 28 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-28T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 使用回归语言模型模拟大型系统\n\n本文介绍了一种名为“回归语言模型”（Regression Language Model, RLM）的新方法，旨在扩展大型语言模型（LLM）的能力，使其不仅能处理主观的人类反馈，还能从原始、多样化的操作数据中预测数值结果。这使得LLM能够执行回归任务，即根据输入 `x` 预测指标 `y`。\n\n## 传统回归方法的局限性\n\n*   **依赖表格输入：** 传统的回归方法通常要求输入是表格形式，即固定长度的数值向量。\n*   **数据转换困难：** 将复杂的非结构化数据（如配置、系统日志、硬件模式）转换为表格格式非常耗时且具有挑战性。\n*   **适应性差：** 当出现新的数据类型时，整个过程往往需要从头开始。\n\n## 回归语言模型（RLM）方法\n\n基于“通过文本到文本回归进行大型系统性能预测”的研究，RLM提供了一种简单、通用且可扩展的方法：\n\n*   **输入输出：** RLM能够读取输入 `x` 的字符串表示，并以结构化文本字符串的形式输出数值 `y`。例如，工业系统的所有配置和参数可以表示为文本字符串 `x`，RLM则输出性能指标 `y`。\n*   **训练机制：** RLM可以预训练或随机初始化。对于新的回归任务，它可以通过“下一个词元预测”（next token prediction）和交叉熵损失进行训练，其中 `x` 作为提示，`y` 作为目标。\n*   **核心优势：**\n    *   避免了特征工程和数据归一化。\n    *   支持对新任务进行少样本适应。\n    *   能够普遍近似输出概率分布。\n\n## 应用案例：预测 Google Borg 的资源效率\n\n该方法被应用于预测 Google 大规模计算基础设施 Borg 上的资源效率，具体是预测“每 Google 计算单元百万指令数”（MIPS per GCU），这是 Borg 系统的一个关键效率指标。\n\n*   **任务目标：** 准确预测配置的 MIPS per GCU 对于优化数千台机器的资源分配和调度至关重要。\n*   **实施细节：**\n    *   RLM 采用一个仅包含 6000 万参数的两层编码器-解码器架构。\n    *   **数据收集：** 从多个回归任务中收集大量的 `(x,y)` 对，其中系统状态 `x` 使用 YAML 或 JSON 表示，包含活跃作业列表、执行轨迹和文本元数据。\n    *   **处理长输入：** 尽管单个数据点 `x` 可能包含多达 100 万个词元，但 RLM 的词元限制为 8k。通过预处理，将最重要的特征重新排序到文本字符串的开头，确保在截断时只丢失次要特征。\n    *   **预训练与适应：** RLM 在预处理数据上进行预训练，以便通过少样本梯度更新更容易适应新类型的输入数据。\n    *   **数值表示：** 数值直接以文本形式表示，无需归一化。多次采样解码输出可以有效捕获 `y` 值的密度，适用于建模随机或噪声情况。\n\n## RLM 的关键能力\n\nRLM 展示了作为通用回归重要组成部分的三项能力：\n\n1.  **密度捕获：**\n    *   通过多次采样 RLM 的输出，模型能够很好地捕获 `y` 值的概率分布（即密度），甚至跨越不同的时间段。\n    *   这超越了简单的点预测，提供了对 MIPS per GCU 值固有变异性和潜在范围的洞察。\n    *   能够捕获偶然不确定性（系统固有的随机性）并识别认知指标（由于有限观察或特征引起的不确定性），从而更全面地理解系统行为。\n\n2.  **不确定性量化：**\n    *   RLM 的预测不确定性与残差平方误差相关，这使得模型能够量化其预测的置信度。\n    *   当不确定时，预测分布会更宽，表明预测应更谨慎对待。\n    *   这有助于决定何时更多地依赖回归器，以及何时可能回退到更慢但更准确的 bin-packing 模拟来管理计算集群。\n    *   ![预测不确定性与回归器误差相关](https://storage.googleapis.com/gweb-research2023-media/images/RLM-2.width-1250.png)\n        *   左图：预测不确定性与回归器误差相关。\n        *   右图：RLM 预测的 KDE 图有效捕获了目标点。\n\n3.  **近乎完美、低成本的回归：**\n    *   RLM 是一种低资源、高效的模型，在各种任务上实现了非常精确的点回归。\n    *   散点图显示出近乎完美的 Spearman 秩相关性，表明预测的 MIPS per GCU 排名与实际排名高度一致。\n    *   该模型能够少样本适应不同服务器上的多样化预测任务，成为 Borg 的一个适应性强、通用性高的预测器。\n    *   ![RLM 预测与真实目标值散点图](https://storage.googleapis.com/gweb-research2023-media/images/RLM-3.width-1250.jpg)\n        *   散点图显示了 RLM 预测（x 轴）与真实目标 y 值（y 轴）在多个回归任务上的关系。图例显示了 Spearman 秩相关系数（⍴）。\n\n## 资源与未来方向\n\n*   研究表明，相对简单的编码器-解码器 RLM 能够有效训练富文本、非表格输入，提供高精度预测并高效适应新任务。\n*   这种稳健且可扩展的方法直接从原始文本预测指标结果，显著减少了对手动特征工程的依赖。\n*   为通用系统模拟器和复杂的奖励机制铺平了道路。\n*   通过建模多样化的数值反馈，RLM 以一种将促成语言模型强化学习未来突破的方式，将“经验”操作化。\n*   论文和开源代码已发布。",
      "shortSummary": "回归语言模型（RLM）扩展了大型语言模型的能力，使其能从非结构化文本数据中预测数值结果，克服了传统表格回归的局限。该方法避免了特征工程，能快速适应新任务，并量化预测不确定性。RLM已成功应用于预测Google Borg计算集群的资源效率（MIPS per GCU），展现出高精度和效率。这为通用系统模拟和先进的强化学习奖励机制奠定了基础。",
      "translated_title": "使用回归语言模型模拟大型系统",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RLM-2.width-1250.png",
          "alt": "RLM-2",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RLM-3.width-1250.jpg",
          "alt": "RLM-3",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"4w5ob\">Large language models (LLMs) often improve by <a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback\" target=\"_blank\" rel=\"noopener noreferrer\">learning from human preferences and ratings</a>, a process where a <i>reward model</i> is trained to take prompts and responses as input in order to guide further model training. This focus on subjective human feedback has dramatically improved their ability to generate helpful, harmless, and coherent text and has been transformative for conversational assistants (e.g., <a href=\"https://blog.google/technology/ai/google-gemini-ai/#sundar-note\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a>).</p><p data-block-key=\"fvuh0\">Another pathway to extend the reward model beyond human subjectivity is to process raw, diverse operational data and treat the observed numerical outcome as a reward signal. This ability can open doors for <a href=\"https://en.wikipedia.org/wiki/Performance_prediction\" target=\"_blank\" rel=\"noopener noreferrer\">predicting the performance</a> of vast software infrastructures, the efficiency of industrial processes, or the results of scientific experiments. Fundamentally, we want LLMs to perform regression (i.e., predict a metric <i>y</i>, given input <i>x</i>). Previously, traditional regression methods have relied on inputs being <i>tabular</i>, i.e., fixed length numeric vectors that can be aggregated as a single table. However, converting complex, unstructured data into a tabular format can be very laborious, and the sheer diversity and dynamic nature of real-world data (e.g., intricate configuration files, system logs, and ever-evolving hardware or workload patterns) make this task even more challenging. When new data types emerge, the process often has to be restarted from scratch.</p><p data-block-key=\"db7oa\">In “<a href=\"https://arxiv.org/abs/2506.21718\" target=\"_blank\" rel=\"noopener noreferrer\">Performance Prediction for Large Systems via Text-to-Text Regression</a>”, we describe a simple, general and scalable approach, based on our earlier work on universal regression, <a href=\"https://arxiv.org/abs/2402.14547\" target=\"_blank\" rel=\"noopener noreferrer\">OmniPred</a>. This approach enables a Regression Language Model (RLM) to read a string representation of the input, and output the number as a structured text string. For example, we can represent the state (<i>x</i>) of an industrial system — including all its configurations, parameters, and contextual information — as a structured text string, and the RLM then writes the performance metric (<i>y</i>) as a string. The RLM can be pre-trained or even randomly initialized, and when handling a new regression task, it can be trained using next token prediction via <a href=\"https://en.wikipedia.org/wiki/Cross-entropy\" target=\"_blank\" rel=\"noopener noreferrer\">cross-entropy</a> loss, with (<i>x</i>) as the prompt and (<i>y</i>) as the target. We describe how this new paradigm has several advantages, such as avoiding feature engineering or normalizations, few-shot adaptation to new tasks, and universal approximation of output probability distributions. We apply the RLM in the context of predicting resource efficiency on <a href=\"https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/\">Borg</a>, Google’s large-scale compute infrastructure for cluster management. We also released an <a href=\"https://github.com/google-deepmind/regress-lm\" target=\"_blank\" rel=\"noopener noreferrer\">open-source library</a> for the research community to leverage for any use-case.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Predicting efficiency in Google's compute clusters</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"4j7mw\">The critical task of predicting the <a href=\"https://arxiv.org/abs/2406.09645\" target=\"_blank\" rel=\"noopener noreferrer\">Millions of Instructions Per Second per Google Compute Unit</a> (MIPS per GCU) is a key efficiency metric for Google's Borg system. Accurately forecasting the MIPS per GCU for configurations is crucial for optimizing resource allocation and scheduling across thousands of machines. We applied the text-to-text regression method to predict the MIPS per GCU of Google’s <a href=\"https://en.wikipedia.org/wiki/Digital_twin\" target=\"_blank\" rel=\"noopener noreferrer\"><i>digital twin</i></a> of Borg, a sophisticated backtesting framework to replicate the state of real-world clusters. The end metric is to predict the numeric result of a specialized <a href=\"https://en.wikipedia.org/wiki/Bin_packing_problem\" target=\"_blank\" rel=\"noopener noreferrer\">bin-packing algorithm</a> used to efficiently allocate tasks to resources.</p><p data-block-key=\"amou3\">Our approach uses an RLM that only requires a two-layer encoder-decoder of 60 million parameters. For training, we collect large amounts of data from multiple regression tasks with pairs (<i>x</i>,<i>y</i>) that include the system state (<i>x</i>) represented using <a href=\"https://en.wikipedia.org/wiki/YAML\" target=\"_blank\" rel=\"noopener noreferrer\">YAML</a> or <a href=\"https://en.wikipedia.org/wiki/JSON\" target=\"_blank\" rel=\"noopener noreferrer\">JSON</a>, containing the lists of active jobs, execution traces, and textual metadata. Each data point (<i>x</i>) can take up to 1M tokens if we include every feature (i.e., detailed information) about that data point. Since the RLM has a token limit of 8k, we pre-process the data by reordering the most important features at the beginning of the text string. When the string is truncated to fit the token limit, only the less important features are lost.</p><p data-block-key=\"hnfm\">We pre-train the RLM on the pre-processed data to enable the model to more easily adapt to new types of input data from new tasks, using few-shot gradient updates. Since numbers are represented as text, they can be represented as-is without normalization. If we sample decoded outputs multiple times, this effectively also captures the density of the <i>y</i>-values, important for modeling stochastic or noisy situations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/RLM-teaser-finalmp4.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"adh81\"><i>Our method uses RLMs to directly regress numerical performance metrics (y) from complex, textually represented system states (x), such as those from Google's compute clusters across diverse workloads (GMail, YouTube, Maps, etc.) and hardware (CPUs and TPUs).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"4j7mw\">Below, we demonstrate three resulting capabilities of RLMs that serve as important components for universal regression.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Density capture</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"4j7mw\">By sampling the RLM’s output multiple times, we can capture <a href=\"https://en.wikipedia.org/wiki/Stochastic_process\" target=\"_blank\" rel=\"noopener noreferrer\">probability distributions</a> (i.e., densities) of y-values remarkably well even across different time durations. This density estimation is useful because it goes beyond simple point predictions. By modeling the full distribution of possible outcomes, we gain insight into the inherent variability and potential range of MIPS per GCU values. This capability allows us to capture both <a href=\"https://en.wikipedia.org/wiki/Uncertainty_quantification\" target=\"_blank\" rel=\"noopener noreferrer\">aleatoric uncertainty</a> (inherent randomness in the system, like stochastic load demand) and potentially identify epistemic indicators (uncertainty due to limited observation or features), giving us a more complete understanding of system behavior.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/RLM-density-final.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"adh81\"><i>The RLM provides density estimates that align remarkably well with the target instructions per second distribution across time durations, as shown by the regressor density curves (3D) and the target</i> <a href=\"https://en.wikipedia.org/wiki/Kernel_density_estimation\" target=\"_blank\" rel=\"noopener noreferrer\"><i>kernel density estimate (KDE) plot</i></a><i> (XY plane).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Uncertainty quantification</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"4j7mw\">The RLM’s prediction uncertainty is correlated with residual squared error, allowing us to quantify the model’s confidence in its predictions. When uncertain, the predicted distribution is broader, signalling that the predictions should be treated with more caution. This enables us to know when to rely more heavily on the regressor, and when to potentially fall-back to slower but more accurate bin-packing simulations in managing the compute clusters.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RLM-2.width-1250.png\" alt=\"RLM-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RLM-2.width-1250.png\" alt=\"RLM-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"adh81\"><b><i>Left:</i></b><i> Prediction uncertainty is correlated with regressor error.</i> <b><i>Right:</i></b><i> KDE plot of RLM predictions effectively capture the target points.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Near-perfect, low-cost regression</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"4j7mw\">Beyond density and uncertainty quantification, our RLM is a low-resource, efficient model, achieving very precise pointwise regression on a diverse set of tasks. We present scatter plots with near-perfect <a href=\"https://en.wikipedia.org/wiki/Spearman_rank_correlation\" target=\"_blank\" rel=\"noopener noreferrer\">Spearman rank-correlation</a>, demonstrating a strong alignment between predicted and actual MIPS per GCU rankings. The model can few-shot adapt to diverse prediction tasks on distinct servers, serving as an adaptable, universal predictor for Borg.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RLM-3.width-1250.jpg\" alt=\"RLM-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RLM-3.width-1250.jpg\" alt=\"RLM-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"adh81\"><i>Scatterplot between RLM prediction (</i><b><i>x-axis</i></b><i>) and true target y-value (</i><b><i>y-axis</i></b><i>) over multiple regression tasks. Legend displays Spearman rank (⍴).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Resources and future directions</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"4j7mw\">We demonstrate that our relatively simple encoder-decoder RLM effectively trains on rich, non-tabular inputs to deliver highly accurate predictions and adapt to new tasks efficiently. This robust and scalable approach predicts metric outcomes directly from raw text, significantly reducing reliance on manual feature engineering and paving the way for both universal system simulators and sophisticated reward mechanisms. By modeling diverse numerical feedback, RLMs <a href=\"https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">operationalize ”experience”</a> in a manner that will enable future breakthroughs in reinforcement learning for language models. See the <a href=\"https://arxiv.org/abs/2506.21718\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a> and <a href=\"https://github.com/google-deepmind/regress-lm\" target=\"_blank\" rel=\"noopener noreferrer\">open-source code</a> for more information.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"4j7mw\"><i>This research was conducted by core members Yash Akhauri (Cornell University and Google Research), Bryan Lewandowski (Google Platforms), and Xingyou Song (Google DeepMind), with contributions from Cheng-Hsi Lin, Adrian Reyes, Grant C. Forbes, Arissa Wongpanich, Bangding Yang, Mohamed S. Abdelfattah, and Sagi Perel.</i></p><p data-block-key=\"b484a\"><i>We would like to thank previous collaborators throughout this broad research arc: Oscar Li, Chansoo Lee, Daiyi Peng, Yutian Chen, Tung Nguyen, Qiuyi Zhang, Jorg Bornschein, Yingjie Miao, Eric Tang, Dara Bahri, and Mangpo Phothilimthana. We further thank Michal Lukasik, Uri Alon, Amir Yazdanbakhsh, Shao-Hua Sun, Kuang-Huei Lee, Zi Wang, Xinyun Chen, Jiyoun Ha, Aviral Kumar, Jonathan Lai, Ke Xue, Rong-Xi Tan, and David Smalling for useful discussions. We also thank Olena Bogdanov for designing the animation for this post. Lastly, we thank Yili Zheng, Safeen Huda, Asaf Aharoni, Srinadh Bhojanapalli, David Lo, Martin Dixon, Daniel Golovin, Denny Zhou, Claire Cui, Ed Chi, and Benoit Schillings for continued support.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "SensorLM：学习可穿戴传感器的语言 (原标题: SensorLM: Learning the language of wearable sensors)",
      "link": "https://research.google/blog/sensorlm-learning-the-language-of-wearable-sensors/",
      "pubDate": "Sun, 27 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-27T16:00:00.000Z",
      "creator": "Google",
      "summary": "## SensorLM：弥合可穿戴数据与人类语言之间的鸿沟\n\n可穿戴设备（如智能手表和健身追踪器）已普及，持续捕获大量关于我们生活的数据。然而，这些原始传感器数据往往缺乏“为什么”的背景信息（例如，心率150次/分钟是由于“剧烈上坡跑”还是“压力大的公开演讲”），这限制了其在个性化健康和福祉方面的潜力。主要挑战在于缺乏将传感器记录与丰富描述性文本配对的大规模数据集，因为手动标注成本高昂且耗时。\n\n### 引入 SensorLM\n\n为了解决这一问题，研究人员推出了 **SensorLM**，一个传感器-语言基础模型家族。SensorLM 旨在弥合传感器数据与人类语言之间的鸿沟，通过学习直接从数据中解释和生成细致入微、人类可读的描述，从而在传感器数据理解方面树立了新的技术标准。\n\n### 训练 SensorLM 模型\n\n为了创建 SensorLM 所需的传感器数据集，研究团队从127个国家的103,643人那里采样了近250万个人日的去识别化数据。这些数据收集于2024年3月1日至5月1日期间，来源于 Fitbit 或 Pixel Watch 设备，参与者同意将其去识别化数据用于研究。\n\n为了克服标注瓶颈，研究人员开发了一种新颖的分层管道，通过计算统计数据、识别趋势和描述传感器数据本身的事件，自动生成描述性文本标题。这一过程使得他们能够整理出迄今为止已知最大的传感器-语言数据集，其规模比以往研究中使用的数据集大几个数量级。\n\n![SensorLM 预训练](https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-2-Pre-Training.width-1250.png)\n\nSensorLM 的预训练支持个性化洞察的新功能，例如零样本传感器理解、传感器-文本对齐和检索、少样本学习以及传感器标题生成。SensorLM 架构建立并统一了流行的多模态预训练策略，如对比学习和生成式预训练：\n\n*   **对比学习**：模型学习将传感器数据片段与其对应的文本描述从一组选项中进行匹配。这教会它区分不同的活动和状态（例如，区分“轻松游泳”和“力量训练”）。\n*   **生成式预训练**：模型学习直接从传感器数据生成文本标题。这赋予了它从理解高维传感器信号中生成丰富、上下文感知描述的能力。\n\n通过将这些方法整合到一个统一的框架中，SensorLM 形成了对传感器信号和语言之间关系的深刻多模态理解。\n\n### 关键能力与扩展行为\n\n研究团队在人类活动识别和医疗保健领域的各种真实任务中评估了 SensorLM，结果显示其比现有最先进的模型有了显著进步。\n\n*   **活动识别与检索**：\n    SensorLM 在标记数据有限的任务中表现出色。它在活动方面实现了卓越的零样本分类，无需任何微调即可准确分类20种活动，并在少样本学习中表现出色，仅通过少量示例即可快速学习。这使得模型能够以最少的数据高度适应新任务和用户。此外，SensorLM 实现了强大的跨模态检索，允许传感器数据和语言描述之间的跨模态理解。这使得研究人员能够使用传感器输入查询描述，或使用自然语言查找特定的传感器模式。\n\n    ![SensorLM 零样本活动识别](https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-3-ActivityRecognition.width-1250.jpg)\n    在零样本人类活动识别中，SensorLM 在各项任务中（以 AUROC 衡量）展示了强大的零样本能力，而基线大型语言模型表现接近随机。\n\n*   **生成能力**：\n    除了其分类能力，SensorLM 还展示了令人印象深刻的标题生成能力。仅给定可穿戴设备的高维传感器信号，SensorLM 就能生成分层且上下文相关的标题。实验结果表明，这些生成的标题比非专业大型语言模型生成的标题更连贯、更符合事实。\n\n    ![SensorLM 文本生成性能](https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-4-Captioning.width-1250.jpg)\n    SensorLM 和基线模型的标题生成性能，以 BERTScore（精确率、召回率、F1）衡量。\n\n    ![SensorLM 生成的描述示例](https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-5-CaptionExamples.width-1250.png)\n    左：输入可穿戴传感器数据。右：真实描述和不同模型生成的描述。SensorLM 直接从传感器数据生成连贯准确的标题，提供比通用语言模型更详细和准确的信息。\n\n*   **扩展行为**：\n    实验还表明，SensorLM 的性能随着数据量、模型大小和计算量的增加而持续提高，这与既定的扩展定律相符。这种持续增长表明，大规模传感器-语言预训练的潜力才刚刚被触及，进一步研究这一范式具有高度价值。\n\n    ![SensorLM 性能随规模扩展](https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-6a-Scaling.width-1250.png)\n    通过对 SensorLM 模型进行系统性扩展实验，我们发现增加计算量（左）、数据量（中）和模型大小（右）能持续提高零样本活动识别的性能。\n\n### 结论与展望\n\n这项研究通过新颖的分层标题生成管道和迄今为止最大的传感器-语言数据集，为通过自然语言理解可穿戴传感器数据奠定了基础。SensorLM 模型家族在使个人健康数据可理解和可操作方面取得了重大进展。通过教会人工智能理解我们身体的语言，我们可以超越简单的指标，走向真正个性化的洞察。\n\n展望未来，研究团队计划将预训练数据扩展到新领域，包括代谢健康和详细的睡眠分析，以应对消费健康设备复杂的现实。他们设想 SensorLM 将引领下一代数字健康教练、临床监测工具和个人健康应用程序，这些应用能够通过自然语言查询、交互和生成提供建议。",
      "shortSummary": "SensorLM是一种新型传感器-语言基础模型，旨在弥合可穿戴设备原始数据与人类语言描述之间的鸿沟。它通过对来自10万多人的近6000万小时多模态传感器数据进行预训练，学习自动生成上下文丰富的自然语言描述。SensorLM在零样本活动识别和文本生成方面表现出色，性能优于现有模型。该研究为通过自然语言理解可穿戴数据奠定了基础，有望推动个性化健康和数字健康应用的发展。",
      "translated_title": "SensorLM：学习可穿戴传感器的语言",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-2-Pre-Training.width-1250.png",
          "alt": "SensorLM-2-Pre-Training",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-3-ActivityRecognition.width-1250.jpg",
          "alt": "SensorLM-3-ActivityRecognition",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-4-Captioning.width-1250.jpg",
          "alt": "SensorLM-4-Captioning",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-5-CaptionExamples.width-1250.png",
          "alt": "SensorLM-5-CaptionExamples",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-6a-Scaling.width-1250.png",
          "alt": "SensorLM-6a-Scaling",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"0iruv\">Wearable devices, from smartwatches to fitness trackers, have become ubiquitous, continuously capturing a rich stream of data about our lives. They record our heart rate, count our steps, track our <a href=\"https://store.google.com/intl/en/ideas/articles/get-to-know-your-pixel-watch/\" target=\"_blank\" rel=\"noopener noreferrer\">fitness and sleep</a>, and much more. This deluge of information holds immense potential for personalized health and wellness. However, while we can easily see <i>what</i> our body is doing (e.g., a heart rate of 150 bpm), the crucial context of <i>why</i> (say, \"a brisk uphill run\" vs. \"a stressful public speaking event\") is often missing. This gap between raw sensor data and its real-world meaning has been a major barrier to unlocking the full potential of these devices.</p><p data-block-key=\"ert0m\">The primary challenge lies in the scarcity of large-scale datasets that pair sensor recordings with rich, descriptive text. Manually annotating millions of hours of data is prohibitively expensive and time-consuming. To solve this, and to truly let wearable data \"speak for itself\", we need models that can learn the intricate connections between <a href=\"https://research.google/blog/scaling-wearable-foundation-models/\">sensor signals</a> and human language directly from the data.</p><p data-block-key=\"ahaq4\">In “<a href=\"https://arxiv.org/abs/2506.09108\" target=\"_blank\" rel=\"noopener noreferrer\">SensorLM: Learning the Language of Wearable Sensors</a>”, we introduce SensorLM, a family of sensor–language <a href=\"https://arxiv.org/abs/2108.07258\" target=\"_blank\" rel=\"noopener noreferrer\">foundation models</a> that bridges this gap. Pre-trained on an unprecedented 59.7 million hours of <a href=\"https://cloud.google.com/use-cases/multimodal-ai\" target=\"_blank\" rel=\"noopener noreferrer\">multimodal</a> sensor data from over 103,000 individuals, SensorLM learns to interpret and generate nuanced, human-readable descriptions from high-dimensional wearable data, setting a new state of the art in sensor data understanding.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/SensorLM-1-Overview.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"qk0jn\"><i>SensorLM translates complex, multimodal wearable sensor data into meaningful, natural language descriptions across statistical, structural, and semantic dimensions.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Training the SensorLM models</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"0iruv\">To create the sensor dataset needed for SensorLM, we sampled nearly 2.5M person-days of <a href=\"https://en.wikipedia.org/wiki/De-identification\" target=\"_blank\" rel=\"noopener noreferrer\">de-identified</a> data from 103,643 people across 127 countries. This data was collected between March 1st and May 1st, 2024, from Fitbit or Pixel Watch devices, with participants consenting to the use of their de-identified data for research to contribute to general knowledge about health and science.</p><p data-block-key=\"a0f10\">To overcome the annotation bottleneck, we developed a novel hierarchical pipeline that automatically generates descriptive text captions by calculating statistics, identifying trends, and describing events from the sensor data itself. This process allowed us to curate the largest-known sensor-language dataset to date, orders of magnitude larger than those used in previous studies.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-2-Pre-Training.width-1250.png\" alt=\"SensorLM-2-Pre-Training\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-2-Pre-Training.width-1250.png\" alt=\"SensorLM-2-Pre-Training\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"qk0jn\"><i>SensorLM pre-training enables new capabilities for personalized insights, such as zero-shot sensor understanding, sensor-text alignment and retrieval, few-shot learning, and sensor caption generation.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"0iruv\">The SensorLM architecture builds on and unifies prominent multimodal pre-training strategies, such as <a href=\"https://en.wikipedia.org/wiki/Self-supervised_learning#Contrastive_self-supervised_learning\" target=\"_blank\" rel=\"noopener noreferrer\">contrastive learning</a> and <a href=\"https://research.google/blog/simvlm-simple-visual-language-model-pre-training-with-weak-supervision/\">generative pre-training</a>.</p><ul><li data-block-key=\"9n60q\"><i>Contrastive Learning</i>: The model learns to match a segment of sensor data with its corresponding text description from a set of options. This teaches it to discriminate between different activities and states (e.g., distinguishing a \"light swim\" from a \"strength workout\").<br></li><li data-block-key=\"cee3m\"><i>Generative Pre-training</i>: The model learns to generate text captions directly from the sensor data. This equips it with the ability to produce rich, context-aware descriptions from understanding the high-dimensional sensor signals.</li></ul><p data-block-key=\"a559p\">By integrating these approaches into a single, cohesive framework, SensorLM develops a deep, multimodal understanding of the relationship between sensor signals and language.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Key capabilities and scaling behaviors</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o6s2n\">We evaluated SensorLM on a wide range of real-world tasks in <a href=\"https://en.wikipedia.org/wiki/Activity_recognition\" target=\"_blank\" rel=\"noopener noreferrer\">human activity recognition</a> and healthcare. The results demonstrate significant advances over previous state-of-the-art models.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Activity recognition and retrieval</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o6s2n\">SensorLM shines in tasks with limited labeled data. It achieves remarkable <a href=\"https://research.google/blog/image-text-pre-training-with-contrastive-captioners/\">zero-shot classification</a> for activity, accurately classifying from 20 activities without any fine-tuning, and excels in <a href=\"https://research.google/blog/image-text-pre-training-with-contrastive-captioners/\">few-shot learning</a>, quickly learning from just a handful of examples. This makes the model highly adaptable to new tasks and users with minimal data. Furthermore, SensorLM enables powerful <a href=\"https://research.google/blog/image-text-pre-training-with-contrastive-captioners/\">cross-modal retrieval</a>, allowing cross-modal understanding between sensor data and language descriptions. This allows us to query descriptions using sensor input, or find specific sensor patterns using natural language, facilitating expert-driven analysis (see further results can be found in the <a href=\"https://arxiv.org/pdf/2506.09108\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-3-ActivityRecognition.width-1250.jpg\" alt=\"SensorLM-3-ActivityRecognition\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-3-ActivityRecognition.width-1250.jpg\" alt=\"SensorLM-3-ActivityRecognition\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"1wd3b\"><i>In zero-shot human activity recognition, SensorLM demonstrates strong zero-shot capabilities across tasks (measured by</i> <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve\" target=\"_blank\" rel=\"noopener noreferrer\"><i>AUROC</i></a><i>), while baseline LLMs perform near random.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Generative capabilities</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o6s2n\">Beyond its classification power, SensorLM demonstrates impressive caption generation capabilities. Given only the high-dimensional sensor signals from a wearable device, SensorLM can produce hierarchical and contextually relevant captions. Experimental results indicate that these generated captions were more coherent and factually correct than those produced by powerful non-specialist LLMs.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-4-Captioning.width-1250.jpg\" alt=\"SensorLM-4-Captioning\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-4-Captioning.width-1250.jpg\" alt=\"SensorLM-4-Captioning\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"1wd3b\"><i>Captioning performance of SensorLM and baselines, measured by</i> <a href=\"https://arxiv.org/abs/1904.09675\" target=\"_blank\" rel=\"noopener noreferrer\"><i>BERTScore</i></a><i> (Precision, Recall, F1).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-5-CaptionExamples.width-1250.png\" alt=\"SensorLM-5-CaptionExamples\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-5-CaptionExamples.width-1250.png\" alt=\"SensorLM-5-CaptionExamples\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"1wd3b\"><b><i>Left:</i></b><i> Input wearable sensor data.</i> <b><i>Right:</i></b><i> The ground truth description and descriptions generated by different models. SensorLM generates coherent and accurate captions directly from sensor data, providing more detail and accuracy than generic language models.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Scaling behavior</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o6s2n\">Our experiments also revealed that SensorLM's performance consistently improves with more data, larger model sizes, and increased computation, aligning with established <a href=\"https://arxiv.org/abs/2001.08361\" target=\"_blank\" rel=\"noopener noreferrer\">scaling laws</a>. This sustained growth suggests we have only scratched the surface of what is possible with large-scale sensor-language pre-training, indicating that further investigation into this paradigm is highly valuable.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-6a-Scaling.width-1250.png\" alt=\"SensorLM-6a-Scaling\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-6a-Scaling.width-1250.png\" alt=\"SensorLM-6a-Scaling\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"1wd3b\"><i>Through systematic scaling experiments of SensorLM models, we show that increased compute (</i><b><i>left</i></b><i>), data (</i><b><i>middle</i></b><i>), and model size (</i><b><i>right</i></b><i>) consistently improves performance on zero-shot activity recognition.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o6s2n\">Our research establishes a foundation for unlocking the understanding of wearable sensor data through natural language, enabled by a novel hierarchical captioning pipeline and the largest sensor-language dataset to date. The SensorLM family of models represents a major advance in making personal health data understandable and actionable. By teaching AI to comprehend the language of our bodies, we can move beyond simple metrics and toward truly personalized insights.</p><p data-block-key=\"4h22s\">Looking forward, we plan to scale pre-training data into new domains, including metabolic health and detailed sleep analysis, to address the messy reality of consumer health devices. We envision SensorLM leading to a future generation of digital health coaches, clinical monitoring tools, and personal wellness applications that can offer advice through natural language query, interaction, and generation. Any future products or applications inspired by this foundational research may require further assessment of any clinical and regulatory considerations that may be applicable.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o6s2n\"><i>The research described here is joint work across Google Research, Google Health, Google DeepMind, and partnering teams. The following researchers contributed to this work: Yuwei Zhang, Kumar Ayush, Siyuan Qiao, A. Ali Heydari, Girish Narayanswamy, Maxwell A. Xu, Ahmed Metwally, Shawn Xu, Jake Garrison, Xuhai Xu, Tim Althoff, Yun Liu, Pushmeet Kohli, Jiening Zhan, Mark Malhotra, Shwetak Patel, Cecilia Mascolo, Xin Liu, Daniel McDuff, and Yuzhe Yang. We would also like to thank participants who contributed their data for this study.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "合成与联邦：利用大型语言模型为移动应用实现隐私保护的领域适应 (原标题: Synthetic and federated: Privacy-preserving domain adaptation with LLMs for mobile applications)",
      "link": "https://research.google/blog/synthetic-and-federated-privacy-preserving-domain-adaptation-with-llms-for-mobile-applications/",
      "pubDate": "Wed, 23 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-23T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 合成与联邦：利用大型语言模型为移动应用实现隐私保护的领域适应\n\n本文探讨了Gboard如何利用合成数据和联邦学习（Federated Learning, FL）结合差分隐私（Differential Privacy, DP）来改进移动打字应用中的语言模型（LMs）和大型语言模型（LLMs），同时严格保护用户隐私。\n\n### 挑战与解决方案\n\n*   **数据需求与隐私风险**：机器学习模型需要大量高质量数据，但直接使用用户数据存在隐私风险，例如敏感信息的记忆。\n*   **合成数据**：\n    *   大型语言模型（LLMs）能够生成模仿用户数据的高质量合成数据，从而避免了隐私泄露的风险。\n    *   这些合成数据可以像公共数据一样用于模型训练，简化了隐私保护模型训练的流程。\n    *   Gboard将此方法应用于其小型LMs和LLMs，遵循数据最小化和数据匿名化的隐私原则。\n*   **联邦学习与差分隐私（DP-FL）**：\n    *   对于小型LMs，Gboard采用FL与DP相结合的方式，确保用户设备上的数据在训练过程中仅有最小的暴露，并防止模型记忆敏感信息。\n    *   所有Gboard生产环境中训练于用户数据的LMs现已全部采用DP-FL，取代了旧的仅FL模型。\n    *   研究进展包括采用新的DP算法BLT-DP-FTRL（提供更好的隐私-效用权衡和易用性）以及SI-CIFG模型架构（实现高效的设备端训练和DP兼容性）。\n\n### 通过公共LLMs生成合成数据以促进隐私训练\n\n*   Gboard利用强大的公共LLMs（在公开可访问数据上训练）来合成高质量、领域特定的数据，这些数据类似于用户打字数据，但无需访问任何私有用户数据。\n*   **合成数据生成方法**：\n    *   通过精心设计的提示词，LLMs被指示：\n        1.  从大型公共数据集中筛选出具有移动用户交互特征的文本（例如：“这个话题是否可能通过手机进行讨论？”）。\n        2.  将选定的文本转换为对话格式（例如：“将这篇文章转换为您可能通过手机发送的消息对话。”）。\n        3.  直接根据特定人工场景生成类似对话的文本（例如：“想象您是一名通过手机给家人发消息的用户。生成聊天内容。”）。\n*   **效果**：合成数据结合了LLMs从网络数据中学到的公共知识与开发者关于移动应用的领域特定知识。在Gboard的评估中，使用合成数据进行预训练，相比基线网络爬取数据，下一词预测（NWP）准确率相对提高了22.8%，并实现了更快的收敛和略高的后训练NWP准确率。然而，使用DP-FL在用户数据上进行隐私保护的后训练对于提升用户打字体验仍然至关重要。\n\n![SynthFed1-Final](https://storage.googleapis.com/gweb-research2023-media/images/SynthFed1-Final.width-1250.png)\n\n*   **图示说明**：小型模型在用户数据上的隐私训练系统，目前支持跨设备联邦学习和带有可信服务器的中央差分隐私。训练好的模型部署在移动设备上。使用合成数据进行公共预训练可改善隐私-效用权衡和计算效率。\n\n### 领域自适应合成数据：为LLMs生成并用于LLMs\n\n*   除了私有训练小型LMs，领域自适应合成数据也对改进和部署该领域的LLMs非常有用。\n*   在“为移动大型语言模型应用合成和适应错误纠正数据”的研究中，Gboard扩展了合成打字数据，以改进LLMs的错误纠正功能。\n*   **方法**：\n    *   在不访问用户数据的情况下，利用Gemini模型，通过提示词（例如：“这里有一些常见的语法错误：……现在将这些语法错误应用于原始句子，并生成不符合语法的句子。”）来融入关于常见语法和打字错误的领域知识。\n    *   Gemini接收模仿移动设备用户打字的合成数据，引入错误并创建“损坏-干净”的句子对数据集。\n    *   Gemini还会进一步验证干净文本的正确性（例如：“最后，纠正生成的不符合语法句子的语法错误。除了纠正语法错误外，不要修改句子。”），这在将方法推广到英语以外的语言时尤其有效。\n*   **应用**：这些合成数据集用于将LLM的错误纠正知识迁移到更小、更高效、更专注于移动应用的特定模型中，从而为新的ML Kit GenAI API中的校对功能做出贡献。\n\n### 隐私保护的支撑模块（Buttress Module）以改进合成数据\n\n*   **挑战**：仅凭公共LLMs和开发者知识难以完全捕捉移动用户数据的独特特征。直接在移动设备上训练LLMs进行DP训练对当前的联邦学习系统资源要求很高。\n*   **解决方案**：通过DP-FL从用户数据中收集隐私保护信号，以指导合成数据的生成。\n*   **支撑模块**：Gboard利用其小型LMs（通过DP-FL微调）作为“支撑模块”，支持LLMs生成更具代表性的合成数据。\n*   **工作原理**：开发一个基于小型LM初始分数的重加权模型，该模型学习预测LLMs在移动应用中的生产性能。然后，通过该重加权模型调整合成数据：被模型识别为更具私有领域代表性的样本将获得更高的权重。这种重加权后的合成数据集有助于更好地微调Gboard的校对LLM。\n\n![SynthFed3-FedButtressHEROFinal](https://storage.googleapis.com/gweb-research2023-media/images/SynthFed3-FedButtressHEROFinal.width-1250.png)\n\n*   **图示说明**：隐私训练的进步被用于训练小型支撑模块，而不是直接改进特定任务（小型）模型。支撑模块与公共LLMs结合，生成隐私保护的合成数据。合成数据是一种可组合的资产，连接了公共知识和私有信息，可用于标准训练流程以改进小型和大型模型。\n\n### 讨论与未来展望\n\n*   将合成数据应用于小型和大型模型已被证明对移动应用有效。\n*   合成数据不仅能高效利用LLM生成器中的公共知识，还能简化整个技术栈，因为生成的合成数据可用于训练各种模型，包括LLMs。\n*   合成数据可用于调试应用开发和隐私保护审计。Gboard在所有生产使用中都采用了纵深防御策略，并在多个阶段应用了行业级的PII检测和模型级安全措施。\n*   通过轻量级支撑模块利用隐私保护的用户信号，对于进一步提升LLMs在特定任务领域的能力前景广阔。\n*   未来研究将继续推进隐私保护合成数据的发展，包括开发带有可信执行环境的新型FL系统以及结合轻量级DP模块与DP微调生成器的新算法，以显著提高去中心化数据中隐私保护合成数据的质量，服务于移动设备上的各种应用。",
      "shortSummary": "Gboard通过结合大型语言模型（LLMs）生成的合成数据和联邦学习（FL）与差分隐私（DP）技术，显著提升了移动打字应用的性能，同时严格保护用户隐私。合成数据在不访问私有信息的情况下，提高了模型预训练的准确性和效率。DP-FL确保用户数据在设备上的训练过程中最小化暴露。此外，Gboard还利用“支撑模块”从用户数据中提取隐私保护信号，指导LLMs生成更具代表性的合成数据，进一步优化了模型，尤其是在错误纠正等高级功能上，实现了性能与隐私的平衡。",
      "translated_title": "合成与联邦：利用大型语言模型为移动应用实现隐私保护的领域适应",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SynthFed1-Final.width-1250.png",
          "alt": "SynthFed1-Final",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SynthFed3-FedButtressHEROFinal.width-1250.png",
          "alt": "SynthFed3-FedButtressHEROFinal",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"p460v\">The recent success of machine learning models relies on not only large-scale, but also high-quality data. The paradigm of pre-training on massive data collected on the web and post-training on smaller high-quality data is used to train both <a href=\"https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/\" target=\"_blank\" rel=\"noopener noreferrer\">large</a> and <a href=\"https://research.google/blog/advances-in-private-training-for-production-on-device-language-models/\">small</a> language models (LMs). For large models, post-training has proven vital for <a href=\"https://openai.com/index/instruction-following/\" target=\"_blank\" rel=\"noopener noreferrer\">aligning models to user intent</a>, and post-training of small models to adapt to the user domain has yielded significant results, for example, achieving <a href=\"https://arxiv.org/abs/2305.18465\" target=\"_blank\" rel=\"noopener noreferrer\">3%–13% improvements in key production metrics</a> for mobile typing applications.</p><p data-block-key=\"8e0nj\">However, in complex LM training systems, there are potential privacy risks, such as the <a href=\"https://arxiv.org/abs/2402.13659\" target=\"_blank\" rel=\"noopener noreferrer\">memorization</a> of sensitive <a href=\"https://arxiv.org/abs/2402.13659\" target=\"_blank\" rel=\"noopener noreferrer\">user instruction</a> data. Privacy-preserving <i>synthetic</i> data provides one path to access user interaction data to improve models while systematically minimizing privacy risks. With the generation capabilities of large LMs (LLMs), synthetic data can be created to mimic user data without risk of memorization. This synthetic data can then be used in model training just as public data is used, simplifying privacy-preserving model training.</p><p data-block-key=\"54rqd\"><a href=\"https://play.google.com/store/apps/details?id=com.google.android.inputmethod.latin&amp;hl=en_US&amp;gl=US\" target=\"_blank\" rel=\"noopener noreferrer\">Gboard</a> uses both small LMs and LLMs to improve billions of users’ typing experience. Small LMs support core features like <a href=\"https://support.google.com/gboard/answer/2811346\" target=\"_blank\" rel=\"noopener noreferrer\">slide to type</a>, <a href=\"https://arxiv.org/abs/1811.03604\" target=\"_blank\" rel=\"noopener noreferrer\">next word prediction</a> (NWP), <a href=\"https://support.google.com/gboard/answer/7068415\" target=\"_blank\" rel=\"noopener noreferrer\">smart compose</a>, <a href=\"https://support.google.com/gboard/answer/7068415\" target=\"_blank\" rel=\"noopener noreferrer\">smart completion</a> and <a href=\"https://support.google.com/gboard/answer/7068415\" target=\"_blank\" rel=\"noopener noreferrer\">suggestion</a>; LLMs support advanced features like <a href=\"https://support.google.com/gboard/answer/7068415\" target=\"_blank\" rel=\"noopener noreferrer\">proofread</a>. In this blog post, we share our exploration over the past few years on generating and using synthetic data to improve LMs for mobile typing applications. We focus on approaches adhering to the <a href=\"https://queue.acm.org/detail.cfm?id=3501293\" target=\"_blank\" rel=\"noopener noreferrer\">privacy principles</a> of both data minimization and data anonymization, and show how they are making a real-world impact in small and large models in Gboard. Particularly, our recent paper, “<a href=\"https://arxiv.org/abs/2505.18488\" target=\"_blank\" rel=\"noopener noreferrer\">Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications</a>”, discusses the advances in privacy-preserving synthetic data for LLMs in production, building upon our continuous research efforts discussed below [<a href=\"https://arxiv.org/abs/2408.08868\" target=\"_blank\" rel=\"noopener noreferrer\">1</a>, <a href=\"https://arxiv.org/abs/2404.04360\" target=\"_blank\" rel=\"noopener noreferrer\">2</a>, <a href=\"https://arxiv.org/abs/2410.08892\" target=\"_blank\" rel=\"noopener noreferrer\">3</a>, <a href=\"https://arxiv.org/abs/2403.08100\" target=\"_blank\" rel=\"noopener noreferrer\">4</a>, <a href=\"https://arxiv.org/abs/2305.12132\" target=\"_blank\" rel=\"noopener noreferrer\">5</a>].</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/SynthFed1-GIFfinal.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ezgae\"><i>Gboard completion, correction, and prediction features powered by small decoder LMs, and the advanced proofreading feature for error correction powered by LLMs.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"uurkd\">Learning from public and private data in practice</h2><p data-block-key=\"185rs\">Our <a href=\"https://research.google/blog/advances-in-private-training-for-production-on-device-language-models/\">2024 blog</a> discussed the best practices of privacy-preserving training on user data to adapt small LMs to the domain of mobile typing text. <a href=\"https://research.google/blog/federated-learning-collaborative-machine-learning-without-centralized-training-data/\">Federated learning</a> (FL) with <a href=\"https://research.google/blog/federated-learning-with-formal-differential-privacy-guarantees/\">differential privacy</a> (DP) is applied so that user data stored on one’s own device has only <a href=\"https://queue.acm.org/detail.cfm?id=3501293\" target=\"_blank\" rel=\"noopener noreferrer\">minimum exposure</a> (i.e., restrictive access control) during training, and is not memorized by the trained models. Pre-training on web data improves the performance of private post-training, empowering the deployment of user-level DP in production. For the purposes of today’s blog, we consider user data generated in applications as private data, and accessible web data and models trained on them as public information (where we apply a privacy defense-in-depth strategy to mitigate concerns on <a href=\"https://arxiv.org/abs/2212.06470\" target=\"_blank\" rel=\"noopener noreferrer\">potential information leakage in public data</a>).</p><p data-block-key=\"fc24\">Today, all Gboard production LMs trained on user data use FL with DP guarantees, including <a href=\"https://arxiv.org/abs/2410.15575\" target=\"_blank\" rel=\"noopener noreferrer\">key decoder models</a> and the <a href=\"https://research.google/blog/advances-in-private-training-for-production-on-device-language-models/\">2024 NWP models</a>. This milestone is achieved by launching dozens of new LMs trained with federated learning and differential privacy (DP-FL LMs), and replacing <a href=\"https://arxiv.org/abs/2306.14793\" target=\"_blank\" rel=\"noopener noreferrer\">all older FL-only models</a>. Research advances have continued rapidly since 2024: we employ a new DP algorithm, <a href=\"https://arxiv.org/abs/2408.08868\" target=\"_blank\" rel=\"noopener noreferrer\">BLT-DP-FTRL</a>, which offers strong privacy-utility trade-offs and ease-of-use in deployment; we adopt the <a href=\"https://arxiv.org/abs/2403.08100\" target=\"_blank\" rel=\"noopener noreferrer\">SI-CIFG model architecture</a> for efficient on-device training and compatibility with DP; and we use <a href=\"https://arxiv.org/abs/2404.04360\" target=\"_blank\" rel=\"noopener noreferrer\">synthetic data</a> from LLMs to improve pre-training. The dedication to privacy-preserving learning to improve small LMs has not only delivered substantial user benefits, but has also helped improve LLMs in mobile typing applications, bridged by synthetic data.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"uurkd\">Synthetic data via public LLMs to boost private training</h2><p data-block-key=\"cdd6i\">We describe our use of synthetic data for pre-training small LMs that are later post-trained with DP and FL in \"<a href=\"https://arxiv.org/abs/2404.04360\" target=\"_blank\" rel=\"noopener noreferrer\">Prompt Public Large Language Models to Synthesize Data for Private On-device Applications</a>.\" We use powerful LLMs trained on publicly accessible data to synthesize high-quality and domain-specific data that resembles user typing data without accessing any private user data. This approach involves carefully designed prompts to instruct LLMs (1) to filter large public datasets to select text that is characteristic of mobile user interactions (sample prompt: “Is this topic likely discussed by people over their mobile phones?”); and (2) to transform selected text into a conversational format (prompt: “Convert this article to a conversation that you may message over your mobile phone.”); or (3) to directly generate conversation-like text based on specific and artificial scenarios (prompt: “Imagine you are a user messaging family on a mobile phone. Generate the chat.”)</p><p data-block-key=\"c2d87\">The resulting synthetic data combines the public knowledge LLMs have learned from web data with developers’ domain-specific knowledge about mobile applications. Synthetic data do not expose user data that was not accessed during creation, and they can be inspected before being used in training. As evaluated in Gboard, pre-training on this synthetic data achieves a 22.8% relative improvement in NWP accuracy compared to pre-training on the baseline web-crawled data, and consistently achieves faster convergence and slightly higher NWP accuracy in post-training. Not too surprisingly, privacy-preserving post-training with DP-FL on user data significantly improves the model, and is still critical when launching these small LMs to improve the user typing experience.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SynthFed1-Final.width-1250.png\" alt=\"SynthFed1-Final\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SynthFed1-Final.width-1250.png\" alt=\"SynthFed1-Final\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"50o9g\"><a href=\"https://research.google/blog/advances-in-private-training-for-production-on-device-language-models/\"><i>Private training</i></a><i> of small models on user data in a system currently supporting cross-device federated learning and central differential privacy with a trusted server. Trained models are deployed on mobile devices. Using synthetic data for public pre-training improves both the privacy-utility trade-off and computation efficiency.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"uurkd\">Domain-adaptive synthetic data from LLMs and for LLMs</h2><p data-block-key=\"2c9l2\">Synthetic data adapted to the domain of mobile applications is easy to use in any training pipeline. In addition to privately training small LMs, they are also useful for improving and deploying LLMs in this domain. In “<a href=\"https://arxiv.org/abs/2505.18488\" target=\"_blank\" rel=\"noopener noreferrer\">Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications</a>”, we extend our synthetic typing data to improve LLMs’ error correction for mobile applications. Without accessing user data, we leverage <a href=\"https://blog.google/technology/ai/google-gemini-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a> to incorporate domain knowledge about common grammatical and typing errors via prompts such as “Here are some common grammatical errors: …Now apply these grammatical errors to the original sentences, and generate the ungrammatical sentences.” The prompted <a href=\"https://blog.google/technology/ai/google-gemini-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a> takes our synthetic data, which mimics user typing on mobile devices, to introduce error and create a dataset of corrupted–clean sentence pairs. We also prompt <a href=\"https://blog.google/technology/ai/google-gemini-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a> to further verify the correctness of the clean text (prompt: “Finally, correct the grammatical errors in the generated ungrammatical sentences. Do not modify the sentences except correcting the grammatical errors.”), which is particularly effective when we generalize the approach to languages beyond English. The synthetic datasets are used to transfer the error correction knowledge of the LLM to smaller, efficient, and focused models for mobile applications. These methods contribute to proofreading in the new <a href=\"https://android-developers.googleblog.com/2025/05/on-device-gen-ai-apis-ml-kit-gemini-nano.html\" target=\"_blank\" rel=\"noopener noreferrer\">ML Kit GenAI API</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"uurkd\">Privacy-preserving buttress module to improve synthetic data</h2><p data-block-key=\"53i7\">While prompting public LLMs is powerful, the domain of mobile user data has unique characteristics that are challenging to capture with only public data and developer knowledge, as indicated by studying both <a href=\"https://arxiv.org/abs/2404.04360\" target=\"_blank\" rel=\"noopener noreferrer\">small</a> and <a href=\"https://arxiv.org/abs/2505.18488\" target=\"_blank\" rel=\"noopener noreferrer\">large</a> pre-trained LMs in Gboard production. <a href=\"https://research.google/blog/protecting-users-with-differentially-private-synthetic-training-data/\">DP training</a> on user data is a promising approach to further adapt the generator model to the private domain. However, training LLMs on mobile devices stresses <a href=\"https://arxiv.org/abs/2410.08892\" target=\"_blank\" rel=\"noopener noreferrer\">current federated learning systems</a>, which may have limited computation and communication resources.</p><p data-block-key=\"3i1d3\">To maintain strong data minimization and anonymization, we collect privacy-preserving signals from user data with DP-FL to guide synthetic data generation. <a href=\"https://arxiv.org/abs/2505.18488\" target=\"_blank\" rel=\"noopener noreferrer\">Our early exploration</a> uses the small LMs in Gboard to capture domain information. We call these small LMs fine-tuned with DP-FL <i>buttress modules</i> to reflect their role in supporting LLMs in generating representative synthetic data for various tasks. More specifically, we develop a reweighting model based on initial scores from small LMs, which learns to predict production performance of LLMs in mobile applications from the offline evaluation. Then we adapt the synthetic data by using the reweighting model: samples that the reweighting model identifies as more representative of the private domain receive higher weights. The reweighted synthetic dataset helps to better fine-tune the Gboard <a href=\"https://support.google.com/gboard/answer/7068415\" target=\"_blank\" rel=\"noopener noreferrer\">proofread</a> LLM.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SynthFed3-FedButtressHEROFinal.width-1250.png\" alt=\"SynthFed3-FedButtressHEROFinal\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SynthFed3-FedButtressHEROFinal.width-1250.png\" alt=\"SynthFed3-FedButtressHEROFinal\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"50o9g\"><i>Advances in private training are used to train a small buttress module instead of directly improving task-specific (small) models. The buttress module is combined with public LLMs to generate privacy-preserving synthetic data. The synthetic data is a composable asset that bridges public knowledge and private information, which can be used in a standard training pipeline to improve small and large models.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"uurkd\">Discussion and next steps</h2><p data-block-key=\"9gs07\">Applying synthetic data for both small and large models is proving effective for mobile applications. In addition to efficiently leveraging public knowledge in LLM generators, synthetic data potentially simplify the entire technical stack as the generated synthetic data can be used to train various models, including LLMs. Synthetic data can be inspected for debugging application development and audited for privacy protection. Throughout all our production usage, we also employ a <a href=\"https://en.wikipedia.org/wiki/Defense_in_depth_(computing)\" target=\"_blank\" rel=\"noopener noreferrer\">defense-in-depth</a> strategy and apply industry-level PII detection and model-level safeguards at multiple stages.</p><p data-block-key=\"95tkg\">Moreover, leveraging privacy-preserving user signals via lightweight buttress modules is promising for further improving LLM capabilities in task-specific domains. Research rapidly advances for privacy-preserving synthetic data. With the development of <a href=\"https://arxiv.org/abs/2410.08892\" target=\"_blank\" rel=\"noopener noreferrer\">new FL systems with trusted execution environments</a> to improve capacity and new algorithms combining lightweight DP modules with DP fine-tuned generators [<a href=\"https://arxiv.org/abs/2402.13659\" target=\"_blank\" rel=\"noopener noreferrer\">1</a>, <a href=\"https://arxiv.org/abs/2503.12347\" target=\"_blank\" rel=\"noopener noreferrer\">2</a>], the quality of privacy-preserving synthetic data from decentralized data can be significantly improved to serve various applications on mobile devices.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"uurkd\">Acknowledgments</h2><p data-block-key=\"9mf6i\"><i>The authors specially thank Shanshan Wu for her technical contribution in generating synthetic data for Gboard applications. We thank Brendan McMahan and Daniel Ramage for the feedback on the blog post itself and the leadership support; Yuanbo Zhang and Daniel Ramage for inspiring and collaborating on the research; Zachary Garrett, Haicheng Sun, and Shumin Zhai for additional discussion and support. We thank Shaofeng Li for helping with the animated figures, and the teams at Google that helped with algorithm design, infrastructure implementation, and production maintenance.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "LSM-2：从不完整可穿戴传感器数据中学习 (原标题: LSM-2: Learning from incomplete wearable sensor data)",
      "link": "https://research.google/blog/lsm-2-learning-from-incomplete-wearable-sensor-data/",
      "pubDate": "Mon, 21 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-21T16:00:00.000Z",
      "creator": "Google",
      "summary": "可穿戴设备通过提供连续、多模态的生理和行为数据，彻底改变了健康监测。然而，尽管传感器技术不断进步，数据标注成本高昂，且现实世界中的可穿戴传感器数据流不可避免地存在缺失（“缺失性”），这给传统的自监督学习（SSL）方法带来了挑战，因为它们通常假设数据是完整无缺的。传统的解决方案，如数据插补或激进的数据过滤，都存在引入偏差或丢弃宝贵数据的缺点。\n\n**缺失数据普遍存在**\n\n研究发现，在160万个为期一天的窗口样本中，没有一个样本的缺失率为0%。缺失数据是可穿戴传感器记录中普遍存在的现象，常见模式包括设备移除、充电、间歇性松动、运动伪影、省电模式或环境噪声。\n\n![LSM2-1](https://storage.googleapis.com/gweb-research2023-media/images/LSM2-1.width-1250.jpg)\n\n**LSM-2与自适应继承掩码（AIM）框架**\n\n为了解决这一关键限制，研究人员在“LSM-2：从不完整可穿戴传感器数据中学习”中提出了**自适应继承掩码（Adaptive and Inherited Masking, AIM）**，这是一种新颖的SSL训练框架，可以直接从不完整数据中学习。AIM不将数据缺失视为错误测量，而是将其视为真实世界数据的自然产物。利用AIM，他们开发了**大型传感器模型2 (LSM-2)**，该模型在可穿戴传感器数据基础模型（LSM-1）的基础上进行了改进。LSM-2即使在传感器故障或时间窗口被移除的情况下也能表现出色，其性能下降远小于在插补数据上训练的模型。\n\n**AIM的核心创新**\n\nAIM的核心在于其处理传感器数据中不可避免的缺失的独特方法。它扩展了掩码自编码器（MAE）预训练框架，通过重建掩码输入样本来学习传感器数据的底层结构。与传统MAE依赖固定掩码比例不同，AIM通过将**令牌丢弃（token drop out）**与**注意力掩码（attention masking）**相结合来处理传感器数据中不可预测的碎片化。\n\n*   **预训练阶段**：被掩码的令牌集包括从可穿戴传感器数据中“继承”的（自然缺失的）和为重建训练目标而“固有”的（故意掩码的）。AIM首先对固定数量的掩码令牌应用丢弃，以提高计算效率。然后，它通过编码器Transformer块中的注意力掩码，自适应地处理任何剩余的掩码令牌（无论是自然缺失还是重建任务的一部分）。\n*   **微调和评估阶段**：此时，掩码令牌仅包含自然发生的数据缺失，AIM对所有掩码令牌采用注意力掩码。通过这种双重掩码方法，并将自然发生和人工掩码的令牌视为等效，AIM教会模型处理可穿戴传感器固有的可变碎片化。\n\n![LSM2-3](https://storage.googleapis.com/gweb-research2023-media/images/LSM2-3.width-1250.png)\n\n**训练与评估**\n\n*   **数据集**：使用了从2024年3月至5月期间，从60,000多名参与者那里收集的4000万小时匿名可穿戴数据，这些数据来自Fitbit和Google Pixel智能手表和追踪器。\n*   **预训练**：LSM-2采用AIM的掩码重建训练目标，学习理解和推断数据，包括缺失性。\n*   **下游任务评估**：\n    *   **判别性任务**：二元高血压分类、二元焦虑分类和20类活动识别（如跑步、滑雪、皮划艇、打高尔夫）。\n    *   **生成能力**：随机插补、时间插值、时间外推（预测）和传感器插补。\n    *   **生理学建模**：年龄和BMI回归任务。\n\n**关键结果**\n\n基于AIM的LSM-2模型展现出卓越的多功能性，在三个关键领域超越了其前身LSM-1：\n\n*   **健康状况和活动分类**：在分类高血压、焦虑和20类活动方面表现更优。\n*   **缺失数据重建**：例如，恢复缺失的传感器信号，具有更低的重建误差。\n*   **连续健康指标预测**：如BMI，具有更高的相关性。\n\n![LSM2-4](https://storage.googleapis.com/gweb-research2023-media/images/LSM2-4.width-1250.png)\n\nLSM-2在传感器故障或数据不完整（例如，整个传感器馈送或一天中的部分数据缺失）的现实场景中表现出色，对这些缺失的鲁棒性优于LSM-1。\n\n![LSM2-2](https://storage.googleapis.com/gweb-research2023-media/images/LSM2-2.width-1250.jpg)\n\n此外，LSM-2在用户、数据量、计算和模型规模方面显示出比LSM-1更好的扩展性。LSM-1显示出平台期迹象，而LSM-2随着更多数据持续改进，尚未达到饱和。\n\n![LSM2-5](https://storage.googleapis.com/gweb-research2023-media/images/LSM2-5.width-1250.png)\n\n**结论**\n\nLSM-2基础模型，通过AIM预训练，代表了可穿戴健康技术在实用性和可用性方面的进步。AIM从根本上教会LSM-2理解并利用真实世界传感器数据流中的自然缺失，从而从不完美的数据中得出可靠的见解。这一创新意味着可穿戴AI最终能够拥抱传感器数据混乱的现实，在利用所有可用信息的同时，保持数据完整性。",
      "shortSummary": "可穿戴传感器数据普遍存在缺失，对现有自监督学习方法构成挑战。研究人员提出了自适应继承掩码（AIM）框架，它直接从不完整数据中学习，将缺失视为自然特征。基于AIM，他们开发了LSM-2模型，该模型在健康状况分类、缺失数据重建和健康指标预测方面显著优于前代LSM-1。LSM-2对数据缺失更具鲁棒性，并展现出更好的可扩展性，使可穿戴AI更能适应现实世界的复杂数据。",
      "translated_title": "LSM-2：从不完整可穿戴传感器数据中学习",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/LSM2-1.width-1250.jpg",
          "alt": "LSM2-1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/LSM2-3.width-1250.png",
          "alt": "LSM2-3",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/LSM2-4.width-1250.png",
          "alt": "LSM2-4",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/LSM2-2.width-1250.jpg",
          "alt": "LSM2-2",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/LSM2-5.width-1250.png",
          "alt": "LSM2-5",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"hl77l\">Wearable devices have revolutionized health monitoring by providing continuous, multimodal physiological and behavioral data — from <a href=\"https://blog.google/products/pixel/pixel-watch-3-loss-of-pulse-detection/\" target=\"_blank\" rel=\"noopener noreferrer\">heart signals</a> and <a href=\"https://store.google.com/intl/en/ideas/articles/get-to-know-your-pixel-watch/\" target=\"_blank\" rel=\"noopener noreferrer\">sleep patterns</a> to <a href=\"https://store.google.com/intl/en/ideas/articles/get-to-know-your-pixel-watch/\" target=\"_blank\" rel=\"noopener noreferrer\">activity levels</a> and <a href=\"https://store.google.com/intl/en/ideas/articles/stress-management/\" target=\"_blank\" rel=\"noopener noreferrer\">stress indicators</a>. Due to advances in sensor technology, it is increasingly feasible to capture a large volume of data, but the cost of labeling remains high, requiring real-time user annotations or laborious clinical studies. <a href=\"https://en.wikipedia.org/wiki/Self-supervised_learning\" target=\"_blank\" rel=\"noopener noreferrer\">Self-supervised learning</a> (SSL) addresses this limitation by directly using the unlabeled data to learn underlying structures, such as subtle physiological relationships. When applied at scale, SSL can enable the creation of <a href=\"https://en.wikipedia.org/wiki/Foundation_model\" target=\"_blank\" rel=\"noopener noreferrer\">foundation models</a> that produce rich and generalizable representations useful for a <a href=\"https://developers.google.com/health-ai-developer-foundations\" target=\"_blank\" rel=\"noopener noreferrer\">wide variety of downstream health tasks</a>.</p><p data-block-key=\"8b7qo\">However, when applying SSL to the wearable domain a critical limitation exists: state-of-the-art SSL methods assume complete, uninterrupted data — a rarity in real-world wearable sensor streams where gaps inevitably occur due to device removal, charging, intermittent loosening, motion artifacts, battery-saving modes, or environmental noise, which we quantify as “missingness”. In fact, we find that no samples, amongst our 1.6 million day-long windows, had 0% missingness<b>.</b> Historically, the challenge of fragmented data has forced researchers to rely on either <a href=\"https://en.wikipedia.org/wiki/Imputation_(statistics)\" target=\"_blank\" rel=\"noopener noreferrer\">imputation</a> methods to fill missing segments, or aggressive <a href=\"https://www.tableau.com/learn/articles/what-is-data-cleaning\" target=\"_blank\" rel=\"noopener noreferrer\">filtering</a> to remove instances with incomplete data. Neither pose an optimal solution, as the former may introduce unintended biases, while the latter discards valuable data.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/LSM2-1.width-1250.jpg\" alt=\"LSM2-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/LSM2-1.width-1250.jpg\" alt=\"LSM2-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cghmf\"><i>Missing data is ubiquitous in wearable sensor recordings. Common modes of missingness are highlighted above in a day-long sample of multimodal wearable sensor data. We note that no samples amongst our 1.6 million day-long windows have 0% missingness.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"hl77l\">In “<a href=\"https://arxiv.org/pdf/2506.05321\" target=\"_blank\" rel=\"noopener noreferrer\">LSM-2: Learning from Incomplete Wearable Sensor Data</a>”, we present Adaptive and Inherited Masking (AIM), a novel SSL training framework that learns directly from incomplete data. Rather than treating data-gaps as erroneous measures that must be filled in, AIM learns directly from incomplete recordings by treating missingness as a natural artifact of real-world data. Leveraging AIM, we develop a Large Sensor Model (LSM-2) that improves upon our previous foundation model for wearable sensor data (<a href=\"https://research.google/blog/scaling-wearable-foundation-models/\">LSM-1</a>, presented at <a href=\"https://openreview.net/forum?id=yb4QE6b22f\" target=\"_blank\" rel=\"noopener noreferrer\">ICLR ‘25</a>). We demonstrate that LSM-2 achieves strong performance even when sensors fail or temporal windows are removed, exhibiting significantly less degradation than models trained on imputed data.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Taking AIM with adaptive inherited masking</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"hl77l\">At the heart of AIM's innovation is its unique approach to handling the inevitable gaps in real-world sensor data. Unlike traditional SSL methods that either discard incomplete data or attempt to fill in missing values, AIM embraces these gaps as natural features of wearable data. As an extension of the <a href=\"https://arxiv.org/abs/2111.06377\" target=\"_blank\" rel=\"noopener noreferrer\">masked autoencoder</a> (MAE) pre-training framework, AIM learns the underlying structure of sensor data by reconstructing masked input samples.</p><p data-block-key=\"8ue17\">However, while traditional MAE methods rely on a fixed masking ratio to enable the efficient <i>drop out</i> of masked tokens (i.e., a fixed number of masked tokens are not passed through the encoder thus reducing computational complexity), fragmentation in sensor data is unpredictable, resulting in a variable number of masked tokens. AIM addresses this fundamental challenge of wearable data by pairing token <a href=\"https://arxiv.org/abs/2111.06377\" target=\"_blank\" rel=\"noopener noreferrer\">drop out</a> with <a href=\"https://research.google/pubs/attention-is-all-you-need/\">attention masking</a>. During pre-training the set of tokens to be masked consists of those inherited from and inherent in the wearable sensor data plus those deliberately masked for the reconstruction training objective.</p><p data-block-key=\"8u8bc\">AIM first applies drop out to a fixed number of masked tokens, improving pre-training computational efficiency by reducing the sequence length processed by the encoder. AIM then adaptively handles any remaining masked tokens — whether naturally missing or part of the reconstruction task — via attention masking in the encoder’s transformer block. During discriminative-task fine-tuning and evaluation, where masked tokens solely consist of naturally occurring data gaps, AIM employs attention masking for all masked tokens. Through this dual masking approach, and by treating naturally occurring and artificially masked tokens as equivalent, AIM teaches the model to work with variable fragmentation inherent to wearable sensors.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/LSM2-3.width-1250.png\" alt=\"LSM2-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/LSM2-3.width-1250.png\" alt=\"LSM2-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cghmf\"><i>AIM pre-training (</i><b><i>A</i></b><i>) and evaluation (</i><b><i>B</i></b><i>) for LSM-2. During pre-training, AIM uses the artificial mask to learn reconstruction and the inherited mask to model real-world missingness. Then, during evaluation, we can use the missingness-aware embedding to predict health targets, such as hypertension, directly from the inherently fragmented sensor data.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Training and evaluation</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"hl77l\">We leverage a dataset with 40 million hours of wearable data sampled from over 60,000 participants during the period from March to May 2024. The dataset was thoroughly anonymized or <a href=\"https://en.wikipedia.org/wiki/De-identification\" target=\"_blank\" rel=\"noopener noreferrer\">de-identified</a> to ensure that participant information was removed and privacy was maintained. Subjects wore a variety of Fitbit and Google Pixel smartwatches and trackers and consented for their data to be used for research and development of new health and wellness products and services. The subjects were asked to self-report sex, age, and weight.</p><p data-block-key=\"du81h\">To pre-train LSM-2, we employ the AIM SSL technique introduced in the previous section. AIM implements a masked reconstruction training objective, and learns to understand data that is naturally missing, and impute data that is artificially masked. This unified framework allows LSM-2 to learn the underlying structure (including missingness) inherent in wearable sensor data.</p><p data-block-key=\"cem7e\">We curate a set of downstream tasks to evaluate the pre-trained model, using meta-data that was collected alongside the sensor signals for the purposes of research and development. These include user annotated activities from a set of 20 different categories (such as running, skiing, kayaking and playing golf) and self-reported diagnoses of hypertension and anxiety. These data were split into fine-tuning and evaluation sets where data from each individual was only in either the tuning or the evaluation set and not both. Data from individuals used in the pretraining stage was also not included in the fine-tuning or evaluation stages.</p><p data-block-key=\"7mm1e\">The generative capabilities of LSM-2 are evaluated through the tasks of random imputation, temporal interpolation, temporal extrapolation (forecasting), and sensor imputation, described in our <a href=\"https://research.google/blog/scaling-wearable-foundation-models/\">LSM-1</a> work.</p><p data-block-key=\"2a5gd\">The utility of the LSM-2 embeddings are evaluated via <a href=\"https://openreview.net/pdf?id=HJ4-rAVtl\" target=\"_blank\" rel=\"noopener noreferrer\">linear probe</a> on a number of discriminative tasks. Specifically we gauge the applicability of the LSM-2 embeddings to the tasks of binary hypertension classification, binary anxiety classification, and 20-class activity recognition. We evaluate LSM-2’s ability to model physiology via age and BMI regression tasks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Key results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"hl77l\">The AIM-based LSM-2 model demonstrates remarkable versatility, outperforming its predecessor, <a href=\"https://research.google/blog/scaling-wearable-foundation-models/\">LSM-1</a>, across three key areas: classifying health conditions and activities (like hypertension, anxiety, and 20-class activity recognition), reconstructing missing data (e.g., recovery of missing sensor signals), and predicting continuous health metrics (like BMI with improved correlation). Additional comparisons to supervised and pre-trained baselines may be found in our <a href=\"https://arxiv.org/pdf/2506.05321\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/LSM2-4.width-1250.png\" alt=\"LSM2-4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/LSM2-4.width-1250.png\" alt=\"LSM2-4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cghmf\"><i>LSM-2 models real-world missingness without imputation, allowing it to achieve lower reconstruction error (</i><b><i>left</i></b><i>) and higher classification scores (</i><b><i>right</i></b><i>) as compared to LSM-1</i>.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"hl77l\">LSM-2 excels in realistic scenarios where sensors fail or data is incomplete. The figure below simulates situations where whole sensor feeds or data for entire portions of the day may be missing. This mirrors the reality that different wearables may host different sensor load-outs, or that an individual may only wear their device for portions of the day. Here we find that the AIM-based LSM-2 proves more robust to these ablations as compared to LSM-1.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/LSM2-2.width-1250.jpg\" alt=\"LSM2-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/LSM2-2.width-1250.jpg\" alt=\"LSM2-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cghmf\"><i>LSM-2 is more robust to missing data than LSM-1, degrading less from its original performance (</i><b><i>dotted line</i></b><i>) than its predecessor when whole sensor feeds or periods of the day are ablated.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"hl77l\">Finally, LSM-2 exhibits improved scaling across users, data volume, compute, and model size as compared to LSM-1. While its predecessor shows signs of plateauing, LSM-2 continues to improve with more data and has yet to saturate.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/LSM2-5.width-1250.png\" alt=\"LSM2-5\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/LSM2-5.width-1250.png\" alt=\"LSM2-5\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cghmf\"><i>LSM-2 exhibits improved scaling over LSM-1 across subjects, data, compute, and model size.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"hl77l\">The LSM-2 foundation model, pre-trained with AIM represents progress towards more useful and usable wearable health technology. Fundamentally, AIM teaches LSM-2 to understand and leverage the natural gaps in real-world sensor streams to derive reliable insights from imperfect data. This innovation means wearable AI can finally embrace the messy reality of sensor data, preserving data integrity, while utilizing all available information.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"hl77l\"><i>The research described here is joint work across Google Research, Google Health, Google DeepMind, and partnering teams. The following researchers contributed to this work: Maxwell A. Xu, Girish Narayanswamy, Kumar Ayush, Dimitris Spathis, Shun Liao, Shyam Tailor, Ahmed Metwally, A. Ali Heydari, Yuwei Zhang, Jake Garrison, Samy Abdel-Ghaffar, Xuhai Xu, Ken Gu, Jacob Sunshine, Ming-Zher Poh, Yun Liu, Tim Althoff, Shrikanth Narayanan, Pushmeet Kohli, Mark Malhotra, Shwetak Patel, Yuzhe Yang, James M. Rehg, Xin Liu, and Daniel McDuff. We would also like to thank participants who contributed their data for this study.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "使用消费级超宽带雷达测量心率 (原标题: Measuring heart rate with consumer ultra-wideband radar)",
      "link": "https://research.google/blog/measuring-heart-rate-with-consumer-ultra-wideband-radar/",
      "pubDate": "Wed, 16 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-16T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 使用消费级超宽带雷达测量心率：谷歌的最新研究\n\n### 引言\n\n随着消费设备功能日益强大，内置的各种传感器可用于监测健康和福祉。谷歌此前在Nest Hub中推出的Soli雷达技术已用于分析睡眠模式。最近，谷歌进一步展示了Soli平台所基于的调频连续波（FMCW）雷达技术能够以完全非接触的方式追踪睡眠和冥想期间的生命体征，如心率和呼吸率。\n\n现在，谷歌在题为“UWB雷达心率监测：一种迁移学习方法”的新研究中，展示了已广泛应用于许多手机的超宽带（UWB）技术，也可用于基于雷达的心率测量。尽管UWB在安全车辆解锁和精确物品定位等功能中得到广泛应用，但其在雷达传感方面的潜力尚未被充分挖掘。这项研究展示了如何利用现有硬件进行生命体征监测，例如测量心率（HR）。\n\n### 消费设备上的雷达传感器\n\n在消费设备中，最有希望用于生命体征测量的雷达系统包括毫米波调频连续波（mm-wave FMCW）和脉冲式超宽带（IR-UWB）雷达系统。谷歌之前在Soli雷达平台上进行睡眠、运动和手势感应的进展使用了FMCW技术，这使得谷歌拥有大量针对这些任务（包括FMCW雷达心率监测）训练的现有数据集、研究和机器学习算法。\n\n同时，UWB作为一种多功能技术，其普及度不断提高，并越来越多地出现在许多当前手机型号和其他消费设备中，也提供了雷达功能。然而，UWB的雷达功能迄今为止在很大程度上尚未被开发，当前的UWB应用更多地倾向于非雷达用途，如定位和追踪、车辆解锁功能或数据传输。\n\n### 克服非接触式传感的挑战\n\n使用雷达以非接触方式检测心率极具挑战性，因为心跳引起的微小胸壁运动很容易被呼吸和身体大范围运动所掩盖。这时，雷达信号的独特特性就发挥了作用：\n\n*   **空间分辨率：** 雷达的3D空间分辨率利用距离和方向来聚焦测量，从而在人体躯干周围定义一个精确的“测量区域”。这使得雷达能够隔离来自胸部区域的反射，同时忽略静止的背景物体或该区域之外的运动。\n*   **时间分辨率：** 同时，其高时间分辨率能够足够快地（高达200Hz）采样信号，以捕捉心跳本身细微而快速的运动。\n\n研究人员开发了一种新方法，充分利用雷达信号这些独特的二维时空特性，以实现高精度的心率测量。\n\n### 弥合雷达类型之间的鸿沟：迁移学习\n\n研究人员探讨了是否可以将从FMCW雷达（拥有大量现有数据集和研究的优势）中学习到的特征迁移到UWB雷达。这两种雷达系统基于完全不同的物理原理运行：\n\n*   **FMCW：** 发射连续正弦波，其频率随时间线性增加，周期性地扫描一个频率范围。\n*   **UWB：** 发射持续时间在几百皮秒到几纳秒之间的极短脉冲。\n\n这项研究首次表明，学习到的特征可以在不同雷达类型之间进行迁移，用于生命体征测量。研究人员选择心率作为初始任务，因为它具有高潜在实用性和挑战性。\n\n![模型迁移的高层架构](https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-2-Architecture.width-1250.jpg)\n\n### 开发用于雷达心率测量的新型深度学习模型\n\n为了完成这项任务，研究人员开发了一个新颖的深度学习框架，旨在模拟雷达信号中复杂的时空关系以进行心率估计。该架构首先使用一个2D ResNet处理输入数据，其中一个轴代表时间，另一个轴代表空间测量。这个初始阶段旨在从胸壁运动产生的精细时空模式中提取特征。\n\n在此步骤之后，模型通过平均池化折叠空间维度。然后，将生成的特征集输入到1D ResNet中，该网络旨在专门沿时间维度分析信号。这第二个阶段从第一阶段提取的特征中识别出心跳特有的长程周期性模式。\n\n当使用FMCW数据集进行训练时，该模型在心率测量方面实现了0.85次/分钟（bpm）的平均绝对误差（MAE）。这一发现代表了在该数据集上比现有最先进结果的显著提升，将之前的错误率减半。\n\n![FMCW雷达数据上现有最先进模型与我们模型性能的比较](https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-3-Comparison.width-1250.png)\n\n![测试集上夜间会话性能的代表性示例](https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-4-Example.width-1250.png)\n\n### 将学习到的特征迁移到超宽带雷达\n\n随后，研究人员进行了一项研究，收集了UWB雷达数据，并以心电图（ECG）和光电容积描记图（PPG）数据作为心率的真实值。实验设置将UWB雷达传感器放置在用户通常握持手机的位置，即放在面前的桌子上或大腿上。与980小时的FMCW数据集相比，UWB雷达数据集要小得多，只有37.3小时。由于UWB雷达配置接近手机上可行的配置，带宽更低，其距离分辨率远低于FMCW数据集。\n\n为了确保模型针对UWB数据集的迁移进行优化，研究人员在进行额外的预处理步骤后重新训练了模型，以修改毫米波FMCW雷达数据，使其更接近目标IR-UWB数据，从而有效地降低其距离分辨率。然后，研究人员在IR-UWB数据集上对该模型进行了微调，实现了4.1 bpm的MAE和6.3%的平均绝对百分比误差（MAPE），比基线错误率降低了25%。UWB雷达性能的基线是5.4 bpm MAE和8.4% MAPE，这是通过从头开始在UWB数据集上训练的最佳模型实现的。通过迁移学习，UWB雷达能够满足消费技术协会（CTA）对消费设备心率测量标准的要求：精度达到5 bpm MAE和10% MAPE。\n\n![IR-UWB雷达数据上基线模型与迁移学习模型性能的比较](https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-5-IR-UWB.width-1250.png)\n\n![测试集中三名选定参与者在雷达放置在参与者面前的桌子上（a）和参与者大腿上（b）的会话中模型性能（蓝色）与真实值（橙色）的代表性示例](https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-6-Performance.width-1250.png)\n\n### 确保不同场景下的准确性\n\n为了确保模型的准确性和可靠性，研究人员分析了其在每个数据集中捕获的各种场景和用户条件下的性能。对于两种类型的雷达，研究人员发现心率测量性能在充分代表的情况下保持一致。例如，在收集夜间睡眠会话数据的FMCW雷达上，性能在各种睡眠姿势下甚至在人们在姿势之间移动时也能保持。对于UWB雷达，心率测量对于两种测试的设备相对于用户的位置（放在面前的桌子上或大腿上）都同样准确。有关此子组分析和其他结果的更多详细信息，请参阅完整的研\n\n### 宏观视角：日常健康监测\n\n心率测量对于一系列健康、健身和福祉应用都非常有用，它能提供对个体心血管状况和各种健康状况下生理反应的基本洞察。这项心率测量的演示可能成为使用移动设备测量心脏和大血管更复杂、更细微健康信号的一步。\n\n虽然健身手环和戒指等可穿戴设备已普及了健康和健身的连续监测，但使用消费级雷达传感器以非接触方式测量心率的能力，使得这项技术的益处能够惠及更广泛的智能手机用户。这项研究侧重于睡眠期间的心率（对于FMCW）以及雷达传感器放置在手机通常使用时所处位置的设置（UWB）。随着技术的发展，连续监测可以扩展到各种日常环境，无缝融入用户的日常活动。\n\n### 对未来设备的影响\n\n这项工作使我们更接近于实现使用消费设备进行非接触式心率测量，特别是随着超宽带（UWB）技术在手机中变得越来越普遍。尽管本研究未包含在真实世界环境中直接使用手机进行测试，但这项研究为未来的此类应用奠定了关键基础。\n\n这项工作的核心发现是，证明了在一个雷达类型（FMCW）上训练的模型可以成功地适应另一个雷达类型（UWB）来测量心率。这种迁移学习方法是向前迈出的重要一步。它为未来的研究和开发提供了一条更高效的途径，可以利用现有大型数据集的基础知识来开发新设备。这种方法无需为每种新硬件从头开始进行大量数据收集，从而实现了更精简的流程，加速了此类功能推向消费设备的时间线。",
      "shortSummary": "谷歌最新研究展示了如何利用手机中常见的超宽带（UWB）雷达进行非接触式心率测量。通过开发新的深度学习模型和创新性地将FMCW雷达数据学习到的特征迁移到UWB雷达，研究团队成功克服了非接触式心率测量的挑战，并达到了消费技术协会的精度标准。这项突破性工作为未来在消费级移动设备上集成更广泛的健康监测功能奠定了基础，有望让更多智能手机用户享受到便捷的健康监测服务。",
      "translated_title": "使用消费级超宽带雷达测量心率",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-2-Architecture.width-1250.jpg",
          "alt": "RadarUWB-2-Architecture",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-3-Comparison.width-1250.png",
          "alt": "RadarUWB-3-Comparison",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-4-Example.width-1250.png",
          "alt": "RadarUWB-4-Example",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-5-IR-UWB.width-1250.png",
          "alt": "RadarUWB-5-IR-UWB",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-6-Performance.width-1250.png",
          "alt": "RadarUWB-6-Performance",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ng5yj\">Consumer devices are becoming increasingly capable, featuring various sensors useful for monitoring fitness and wellbeing. A few years ago, we launched <a href=\"https://research.google/blog/contactless-sleep-sensing-in-nest-hub/\">sleep sensing in the Nest Hub</a>, which used radar technology, called Soli, to analyze sleep patterns<footnote id=\"da2d8402-1a4c-4976-8152-684e51081729\">[da2d84]</footnote> while the device is placed near the bedside. More recently, we showed that the frequency modulated continuous wave (FMCW) radar technology underlying the Soli radar platform can <a href=\"https://research.google/pubs/soli-enabled-non-contact-heart-rate-detection-for-sleep-and-meditation-tracking/\">track vital signs like heart rate and breathing rate during sleep and meditation</a> in a fully contactless manner.</p><p data-block-key=\"8k4g8\">Today, in “<a href=\"https://arxiv.org/abs/2507.14195\" target=\"_blank\" rel=\"noopener noreferrer\">UWB Radar-based heart rate monitoring: A transfer learning approach</a>”, we present new research showing that the ultra-wideband (UWB) technology, already common in many mobile phones, can be used for radar-based heart rate measurement. While UWB is widely adopted for features like secure vehicle unlocking and precise item location, its potential for radar sensing has been largely untapped. We demonstrate how this existing hardware can be leveraged for vital sign monitoring, such as measuring heart rate (HR).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/RadarUWB-1b-Overview.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"scntp\"><i>Detecting heart rate in a contactless manner with UWB radar, similar to that in current mobile phones, using a deep learning ML model.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Radar sensors available on consumer devices</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ng5yj\">The radar systems that have shown the most promise for vital sign measurement from consumer devices include <a href=\"https://www.everythingrf.com/community/what-is-a-fmcw-radar\" target=\"_blank\" rel=\"noopener noreferrer\">millimeter wave frequency-modulated continuous wave</a> (mm-wave FMCW) and <a href=\"https://www.everythingrf.com/community/what-is-impulse-radio-ultra-wideband-ir-uwb-radar-technology\" target=\"_blank\" rel=\"noopener noreferrer\">impulse-radio ultra-wideband</a> (IR-UWB) radar systems. Google’s previous advances on <a href=\"https://blog.google/products/google-nest/new-nest-hub-soli/\" target=\"_blank\" rel=\"noopener noreferrer\">sensing sleep</a>, <a href=\"https://research.google/blog/soli-radar-based-perception-and-interaction-in-pixel-4/\">motion and gestures</a> on the Soli radar platform used FMCW technology. This meant that we already had extensive datasets, studies, and machine learning algorithms trained for these tasks, <a href=\"https://research.google/pubs/soli-enabled-non-contact-heart-rate-detection-for-sleep-and-meditation-tracking/\">including for heart rate</a> monitoring using FMCW radar.</p><p data-block-key=\"78eoa\">Meanwhile UWB — a multipurpose technology that has grown in popularity and is <a href=\"https://www.firaconsortium.org/resource-hub/blog/the-present-and-future-of-uwb\" target=\"_blank\" rel=\"noopener noreferrer\">increasingly available on many current mobile phone models</a> and other consumer devices, also offers radar capabilities. The radar capabilities of UWB have been thus far largely untapped, with current UWB applications leaning more on non-radar uses like localization and tracking, vehicle unlock features, or data transfer.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Overcoming the challenge of contactless sensing</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ng5yj\">Detecting HR in a contactless manner with radar is challenging because the tiny movements of the chest wall caused by the heartbeat are easily obscured by the far larger movements from breathing and general body motion. This is where the distinct nature of the radar signal comes into play. Its spatial resolution works in three dimensions, using both distance and direction to focus its measurement. This allows the radar to define a precise “measurement zone” around a person's torso. As a result, it can isolate reflections coming from the chest area while ignoring stationary background objects or movements occurring outside this zone. Simultaneously, its high temporal resolution samples the signal fast enough (up to 200Hz) to capture the subtle, rapid motion of the heartbeat itself. We developed a new method that makes optimal use of these unique 2-dimensional spatio-temporal properties of the radar signal to achieve highly accurate heart rate measurement.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Bridging the gap between radar types</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ng5yj\">We investigated if we could transfer the features learned from FMCW radar — where we had the benefit of large existing datasets and studies — to the UWB radar. The two radar systems operate using completely different physical principles. Mm-wave FMCW transmits a continuous sinusoidal wave whose frequency increases linearly with time, periodically sweeping a frequency range, while UWB transmits very short pulses with duration on the order of a few hundred picoseconds to a few nanoseconds. Our study is the first to show that learned features can be transferred between radar types for vital sign measurement. We chose heart rate as an initial task, for both its high potential utility and level of challenge.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-2-Architecture.width-1250.jpg\" alt=\"RadarUWB-2-Architecture\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-2-Architecture.width-1250.jpg\" alt=\"RadarUWB-2-Architecture\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"scntp\"><i>High level architecture of model transfer.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Developing a new deep learning model for heart rate from radar</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ng5yj\">To accomplish this task, we developed a novel deep learning framework designed to model the complex spatial-temporal relationships in radar signals for HR estimation. The architecture first uses a 2D <a href=\"https://arxiv.org/abs/1512.03385\" target=\"_blank\" rel=\"noopener noreferrer\">ResNet</a> to process the input data, in which one axis represents time and the other represents the spatial measurements. This initial stage is designed to extract features from the fine-grained spatio-temporal patterns created by chest wall movements.</p><p data-block-key=\"5lvb7\">Following this step, the model collapses the spatial dimension via average pooling. The resulting feature set is then fed into a 1D ResNet, which is designed to analyze the signal exclusively along the temporal dimension. This second stage identifies the longer-range, periodic patterns characteristic of a heartbeat from the features extracted in the first stage.</p><p data-block-key=\"fs2i2\">When trained with our FMCW dataset, the model achieves a mean absolute error (MAE) of 0.85 beats per minute (bpm) for heart rate measurement. This finding represents a substantial gain over prior state-of-the-art results on this dataset, halving the previous error rate.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-3-Comparison.width-1250.png\" alt=\"RadarUWB-3-Comparison\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-3-Comparison.width-1250.png\" alt=\"RadarUWB-3-Comparison\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"scntp\"><i>Comparison of the previous state of the art and our model's performance on FMCW radar data, including MAE with 95% confidence interval and mean absolute precision error (MAPE).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-4-Example.width-1250.png\" alt=\"RadarUWB-4-Example\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-4-Example.width-1250.png\" alt=\"RadarUWB-4-Example\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"scntp\"><i>Representative example of overnight session performance on the test set. The top plot shows the model performance (blue) compared to the ground truth (orange). The middle plot shows the body position, and the bottom plot shows the estimated user distance from the radar.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Transferring learned features to ultra-wideband radar</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ng5yj\">We then ran a study that collected UWB radar data, along with electrocardiogram (ECG) and photoplethysmogram (PPG) data as our ground truth for heart rate, using a setup that placed the UWB radar sensor in positions where users typically hold their phone, i.e., on a table in front of them or on their lap. Compared to the FMCW dataset, which was 980 hours of data, the UWB radar dataset was much smaller, with 37.3 hours. As the UWB radar configuration was close to what is feasible on a mobile phone, with a much lower bandwidth, its range resolution was far lower than the FMCW dataset.</p><p data-block-key=\"9o9i4\">To ensure that our model was optimized to transfer to the UWB dataset, we retrained it after performing additional pre-processing steps to modify the mm-wave FMCW radar data to better resemble the target IR-UWB data, effectively lowering its range resolution. We then fine-tuned this model on the IR-UWB dataset, achieving an MAE of 4.1 bpm and mean absolute percentage error (MAPE) of 6.3%, a 25% reduction over the baseline error rate. Our baseline for performance on UWB radar was 5.4 bpm MAE and 8.4% MAPE, achieved by selecting the best model trained from scratch on our UWB dataset. With transfer learning, we enabled the UWB radar to meet the <a href=\"https://shop.cta.tech/collections/standards/products/cta-2065\" target=\"_blank\" rel=\"noopener noreferrer\">Consumer Technology Association standards</a> for heart rate measurement for consumer devices: an accuracy of up to 5 bpm MAE and 10% MAPE.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-5-IR-UWB.width-1250.png\" alt=\"RadarUWB-5-IR-UWB\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-5-IR-UWB.width-1250.png\" alt=\"RadarUWB-5-IR-UWB\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"scntp\"><i>Comparison of the baseline and transfer learning models performance on IR-UWB radar data, including MAE with 95% confidence interval and MAPE.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-6-Performance.width-1250.png\" alt=\"RadarUWB-6-Performance\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-6-Performance.width-1250.png\" alt=\"RadarUWB-6-Performance\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"scntp\"><i>Representative examples of the model performance (blue) compared to the ground truth (orange) for three selected participants from the test set for the session where the radar was located on the table in front of the participant (</i><b><i>a</i></b><i>) and at the participant's lap (</i><b><i>b</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Ensuring accuracy in different scenarios</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ng5yj\">To make sure our model is both accurate and reliable, we analyzed its performance across the various scenarios and user conditions captured in each dataset. For both types of radar, we found that performance on heart rate measurement is consistent in situations that were adequately represented. For example, on the FMCW radar, which collected data during overnight sleep sessions, the performance is maintained across various sleep positions and even when a person is moving between positions. For UWB radar, heart rate measurement is equally accurate for both tested device positions relative to the user — on a table in front of them or in their lap. For more details on this subgroup analysis and other results, see the <a href=\"https://arxiv.org/abs/2507.14195\" target=\"_blank\" rel=\"noopener noreferrer\">full research paper</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The big picture: Everyday health monitoring</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ng5yj\">Heart rate measurement is useful for a range of health, fitness, and wellness applications, offering fundamental insight into an individual's cardiovascular status and physiological responses across various health conditions. This demonstration of heart rate measurement could be a step towards using mobile devices to measure even more complex and subtle health signals from the heart and large blood vessels.</p><p data-block-key=\"367u5\">While wearable devices like fitness bands and rings have popularized continuous monitoring of health and fitness, the ability to measure heart rate in a contactless manner with consumer-device–grade radar sensors allows the benefits of this technology to reach a much wider audience of smartphone users. For this study, we focused on heart rate while sleeping (for FMCW) and on a setup where the radar sensor was in positions where the phone is usually held during use (UWB). As technology evolves, continuous monitoring could extend to various daily settings, integrating seamlessly into a user's routine activities.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">What this means for future devices</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ng5yj\">This work moves us closer to enabling contactless heart rate measurement using consumer devices, especially as ultra-wideband (UWB) technology becomes more prevalent in mobile phones. Although our study did not include direct testing using mobile phones in a real-world setting, this research establishes the crucial groundwork for such future applications.</p><p data-block-key=\"6s3q0\">A core finding of this work is the demonstration that a model trained on one type of radar (FMCW) can be successfully adapted for another (UWB) to measure heart rate. This transfer learning approach is a significant step forward. It suggests a more efficient path for future research and development, where the foundational knowledge from existing, large datasets can be leveraged for new devices. Instead of starting from scratch with extensive data collection for each new piece of hardware, this method allows for a more streamlined process, accelerating the timeline for bringing such features to consumer devices.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ng5yj\"><i>This research is a result of a collaborative effort between Google Research and Google Platforms &amp; Devices teams. We would like to thank our co-authors Sebastien Baur, Matthew Baugh, Mathias Bellaiche, Sharanya Srinivas, Octavio Ponce, Matthew Thompson, Pramod Rudrapatna, Michael Sanchez, Lawrence Cai, Tim Chico, Robert Storey, Emily Maz, Umesh Telang, Shravya Shetty, and Mayank Daswani for their significant contributions. We are also grateful to Abhijit Guha Roy, Michał Matuszak, Florence Thng, Yun Liu, and Shwetak Patel for their expert review. We also want to thank Tiya Tiyasirichokchai for designing the graphic for this post.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Android地震预警：一个全球性的早期预警系统 (原标题: Android Earthquake Alerts: A global system for early warning)",
      "link": "https://research.google/blog/android-earthquake-alerts-a-global-system-for-early-warning/",
      "pubDate": "Wed, 16 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-16T16:00:00.000Z",
      "creator": "Google",
      "summary": "### Android地震预警系统：一个全球性的早期预警方案\n\n地震对全球社区构成持续威胁。尽管我们对地震可能发生的地点有了更好的了解，但地震发生时仍会带来毁灭性后果。如果能在震动开始前提供几秒钟的宝贵预警，这些时间足以让人从梯子上下来、远离危险物品并寻找掩护。多年来，这一直是地震早期预警（EEW）系统的目标。然而，许多地震多发地区缺乏昂贵的地震网络。\n\n#### Android手机如何实现地震检测和预警\n\n在《科学》杂志发表的“使用Android手机进行全球地震检测和预警”一文中，我们展示了如何将全球Android智能手机网络转变为一个强大的、口袋大小的地震检测系统，以补充官方预警系统。\n\n*   **工作原理：**\n    *   Android手机中的加速度计（与横向翻转屏幕相同的传感器）可以检测地震引起的地面震动。\n    *   当一部静止的手机检测到地震的初始、传播速度更快的P波时，它会向地震检测服务器发送信号，并附带震动发生的大致位置。\n    *   系统随后迅速分析来自多部手机的数据，以确认地震正在发生，并估算其位置和震级。\n    *   目标是在地震传播速度较慢、破坏性更大的S波到达之前，尽可能多地向人们发出警告。\n*   **预警类型：**\n    *   **BeAware 警报：** 针对预计的轻微震动。\n    *   **TakeAction 警报：** 针对预计的更强震动，会占据手机屏幕并播放响亮的声音。\n*   **接收条件：** 用户必须开启Wi-Fi和/或蜂窝数据连接，并启用Android地震预警和位置设置。警报基于保护隐私的设备粗略位置发送。用户可以在设备设置中关闭地震警报。\n\n![EQdetection1_Map](https://storage.googleapis.com/gweb-research2023-media/images/EQdetection1_Map.width-1250.png)\n*图示：浅绿色区域显示了Android地震预警系统当前正在检测并发送警报的国家。红色区域表示发生强烈震动（MMI 5+）时发出警报的区域，黄色区域表示轻微震动（MMI 3-4）时发出警报的区域。灰色圆圈表示未发出警报的区域中的其他Android检测。Android还在加利福尼亚州、俄勒冈州和华盛顿州（深绿色）提供由ShakeAlert生成的警报。*\n\n#### 全球覆盖与显著影响\n\n*   **推广历程：** 2021年4月，系统开始在新西兰和希腊推出由Android检测生成的警报。到2023年底，该系统已在98个国家/地区启用。\n*   **检测成果：** 系统已检测到超过18,000次地震，包括M1.9的小型震颤到M7.8的重大地震。对于足以向人们发出警告的事件，系统已对2000多次地震发出了警报，总计向全球手机发送了7.9亿次警报。\n*   **EEW覆盖范围的巨大提升：** 2019年，全球只有约2.5亿人能够使用EEW系统。如今，主要得益于Android系统，这一数字已增至25亿，实现了约10倍的增长。\n\n![EQdetection2_Reach](https://storage.googleapis.com/gweb-research2023-media/images/EQdetection2_Reach.width-1250.png)\n*图示：随着Android系统的引入，EEW的全球覆盖范围显著扩大。*\n\n#### 震级估算的挑战与改进\n\n*   **挑战：** 实时估算地震震级是EEW系统中最棘手的部分之一。震级决定了震动传播的距离以及需要警报的人群。准确估算至关重要——低估可能无法警告处于危险中的人，高估则可能发出虚假警报，损害公众信任。挑战在于速度和准确性之间的权衡。地震开始的最初几秒提供的数据有限，但每延迟一秒发出警报，受影响区域的人们获得的预警时间就减少一秒。\n*   **持续改进：** 过去三年，系统持续改进了震级估算。首次震级估算的中位数绝对误差已从0.50降至0.25。与传统的地震网络相比，Android系统的准确性相似，在某些情况下甚至更好。\n\n![EQdetection3_Error](https://storage.googleapis.com/gweb-research2023-media/images/EQdetection3_Error.width-1250.png)\n*图示：过去三年Android地震预警系统地震震级误差的演变。对于一次地震，系统会随着新数据的收集进行多次检测。首次估算很重要，因为它提供了最长的预警时间，而最大震级估算则为最大区域生成警报。*\n\n#### 具体案例分析\n\n*   **2023年11月菲律宾6.7级地震：** 系统在地震开始后18.3秒发出了首次警报。震中附近受影响最严重的人们获得了长达15秒的预警，而较远但仍感受到中度震动的人们获得了长达一分钟的预警。总计近250万人收到警报。\n*   **2023年11月尼泊尔5.7级地震：** 首次警报在地震开始后15.6秒发出。经历中度至强烈震动的人们获得了10到60秒的预警时间。此次事件共发送了超过1000万次警报。\n\n![EQdetection4_Warning](https://storage.googleapis.com/gweb-research2023-media/images/EQdetection4_Warning.width-1250.png)\n*图示：这些图表显示了人们根据与震中的距离和所经历的震动强度获得的预警时间。*\n\n*   **2025年4月土耳其6.2级地震：** 首次警报在地震开始后8.0秒发出。经历中度至强烈震动的人们获得了几秒到20秒的预警时间。此次事件共发送了超过1600万次警报。\n\n#### 用户反馈：系统有效性的真实检验\n\n系统在警报中加入了简单的调查，反馈结果非常积极。在超过150万名受访者中，85%的人认为警报“非常有帮助”。\n\n*   **关键发现：**\n    *   即使没有感受到震动，人们也感谢预警。令人惊讶的是，79%收到警报但未感受到地震的人仍然认为警报非常有帮助。这表明人们重视了解其区域内的潜在危险。\n    *   正确的警报很重要。收到“TakeAction”警报的人中，报告感受到强烈震动的比例远高于收到“BeAware”警报的人。这表明系统在区分轻微震动和潜在破坏性震动方面做得很好。\n    *   人们正在采取行动。对于收到“TakeAction”警报的人来说，最常见的反应是“趴下、掩护、抓牢”。这是一个非常好的结果，表明这些警报正在促使人们采取正确的、挽救生命的行动。\n\n![EQdetection6_UserFeedback](https://storage.googleapis.com/gweb-research2023-media/images/EQdetection6_UserFeedback.width-1250.png)\n*图示：超过150万用户对地震警报体验的反馈快照。*\n\n#### 地震早期预警的未来\n\n最令人兴奋的是，该系统正在不断学习和改进。收集到的数据有助于更好地理解地震并建立更准确的预测模型。未来，该系统不仅可以提供预警，还可以向应急响应人员提供快速的震后信息，帮助他们迅速评估最需要帮助的区域。",
      "shortSummary": "Android地震预警系统利用全球Android手机的加速度计，构建了一个大规模的地震检测网络。该系统能检测地震P波，快速分析并估算震级和位置，在破坏性S波到达前向用户发送警报。自2021年推出以来，系统已覆盖近百个国家，检测了数万次地震，发送了数亿次警报，将全球地震预警覆盖人数从2.5亿提升至25亿。系统持续优化震级估算准确性，并通过用户反馈证实了其有效性，促使人们采取正确的避险行动。未来，它有望提供更准确的预警和震后信息。",
      "translated_title": "Android地震预警：一个全球性的早期预警系统",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EQdetection1_Map.width-1250.png",
          "alt": "EQdetection1_Map",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EQdetection2_Reach.width-1250.png",
          "alt": "EQdetection2_Reach",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EQdetection3_Error.width-1250.png",
          "alt": "EQdetection3_Error",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EQdetection4_Warning.width-1250.png",
          "alt": "EQdetection4_Warning",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EQdetection6_UserFeedback.width-1250.png",
          "alt": "EQdetection6_UserFeedback",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"rgp22\">Earthquakes are a constant threat to communities around the globe. While we’ve gotten good at knowing where they’re likely to strike, we still face devastating consequences when they do. What if we could give people a few precious seconds of warning before the shaking starts? Those seconds can be enough time to get off a ladder, move away from dangerous objects and take cover. For years, that’s been the goal of <a href=\"https://en.wikipedia.org/wiki/Earthquake_early_warning_system\" target=\"_blank\" rel=\"noopener noreferrer\">earthquake early warning</a> (EEW) systems. But the expensive seismic networks on which they rely just don’t exist in many of the world’s most earthquake-prone regions.</p><p data-block-key=\"6l06b\">In “<a href=\"https://www.science.org/doi/10.1126/science.ads4779\" target=\"_blank\" rel=\"noopener noreferrer\">Global earthquake detection and warning using Android phones</a>”, published in <a href=\"https://www.science.org/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Science</i></a>, we show how we've turned the global network of Android smartphones into a powerful, pocket-sized earthquake detection system to help supplement official warnings systems. Over the last four years, the <a href=\"https://crisisresponse.google/android-alerts/\" target=\"_blank\" rel=\"noopener noreferrer\">Android Earthquake Alerts system</a> has detected thousands of earthquakes and sent alerts to millions of people in nearly 100 countries, often giving them the crucial moments they need to take cover before the shaking arrives. Evaluation of thousands of earthquakes, analysis of specific earthquake examples and direct user feedback allows the system to continuously improve its performance in key areas, like magnitude estimation, making the alerts more effective over time.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"rgp22\">How it works: A global network of tiny seismometers</h2><p data-block-key=\"fempe\">The accelerometer in an Android phone, the same sensor that flips the screen when it’s turned sideways, can also detect the ground shaking from an earthquake. If a stationary phone detects the initial, faster-moving <a href=\"https://en.wikipedia.org/wiki/P_wave\" target=\"_blank\" rel=\"noopener noreferrer\">P-wave</a> of an earthquake, it sends a signal to our earthquake detection server, along with a coarse location of where the shaking occurred.</p><p data-block-key=\"17h5m\">The system then quickly analyzes data from many phones to confirm that an earthquake is happening and estimate its location and magnitude. The goal is to warn as many people as possible before the slower, more damaging <a href=\"https://en.wikipedia.org/wiki/S_wave\" target=\"_blank\" rel=\"noopener noreferrer\">S-wave</a> of an earthquake reaches them. The system sends out two types of alerts:</p><ul><li data-block-key=\"2fhu9\"><i>BeAware alerts</i> — for estimated light shaking.</li><li data-block-key=\"3nd9u\"><i>TakeAction alerts</i> — for estimated stronger shaking, which take over the phone's screen and play a loud sound.</li></ul><p data-block-key=\"dil0d\">To receive alerts, users must have Wi-Fi and/or cellular data connectivity, and both Android Earthquake Alerts and location settings enabled. Alerts are sent based on a privacy-preserving, coarse location of the device. Users who do not wish to receive these alerts can <a href=\"https://support.google.com/android/answer/9319337?hl=en#zippy=%2Cget-alerts-for-nearby-earthquakes\" target=\"_blank\" rel=\"noopener noreferrer\">turn off Earthquake Alerts in device settings</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EQdetection1_Map.width-1250.png\" alt=\"EQdetection1_Map\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EQdetection1_Map.width-1250.png\" alt=\"EQdetection1_Map\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"i9nm7\"><i>Light green areas show the countries where the Android Earthquake Alerts System is currently detecting and delivering alerts. The areas alerted in individual earthquakes are shown in red where there was strong shaking (</i><a href=\"https://en.wikipedia.org/wiki/Modified_Mercalli_intensity_scale\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Modified Mercalli Intensity</i></a><i> (MMI) 5+) and yellow for lighter shaking (MMI 3-4). The gray circles indicate other Android detections in regions where alerts were not issued. Android also delivers alerts generated by</i> <a href=\"https://www.shakealert.org/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>ShakeAlert</i></a><i> in California, Oregon and Washington (dark green).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"rgp22\">A global safety net</h2><p data-block-key=\"9f2ts\">In April 2021, we began rolling out alerts generated by Android detections, starting in New Zealand and Greece. By the end of 2023, the system was active in 98 countries.</p><p data-block-key=\"91eke\">The system has now detected over 18,000 earthquakes, from small tremors of <a href=\"https://en.wikipedia.org/wiki/Moment_magnitude_scale\" target=\"_blank\" rel=\"noopener noreferrer\">M1.9</a> to major quakes reaching M7.8. For the events significant enough to warn people, alerts were issued for over 2000 earthquakes, culminating in 790 million alerts being sent to phones worldwide.</p><p data-block-key=\"3jk2o\">The impact has been a ~10x change in the number of people with access to EEW systems. In 2019, only about 250 million people had access. Today, thanks in large part to the Android system, that number has increased to 2.5 billion.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EQdetection2_Reach.width-1250.png\" alt=\"EQdetection2_Reach\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EQdetection2_Reach.width-1250.png\" alt=\"EQdetection2_Reach\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"i9nm7\"><i>The global reach of EEW has expanded dramatically with the introduction of the Android system.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"rgp22\">The challenge of estimating an earthquake's power</h2><p data-block-key=\"bkme8\">One of the trickiest parts of an EEW system is estimating the <a href=\"https://en.wikipedia.org/wiki/Moment_magnitude_scale\" target=\"_blank\" rel=\"noopener noreferrer\">magnitude</a> of an earthquake in real-time. The magnitude tells us how big the earthquake is, which in turn determines how far the shaking will travel and who needs to be alerted. Getting this right is crucial — underestimate, and you might not warn people in danger; overestimate, and you risk sending out false alarms that erode public trust.</p><p data-block-key=\"86rp3\">The challenge lies in the trade-off between speed and accuracy. The first few seconds of an earthquake provide limited data, but every second you wait to issue an alert is a second less of warning for those in the path of the shaking.</p><p data-block-key=\"d0k8r\">Over the last three years, we've continuously improved our magnitude estimation. The median absolute error of our first magnitude estimate has dropped from 0.50 to just 0.25. When we compare our system to established, traditional seismic networks, our accuracy is similar, and in some cases, even better.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EQdetection3_Error.width-1250.png\" alt=\"EQdetection3_Error\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EQdetection3_Error.width-1250.png\" alt=\"EQdetection3_Error\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"i9nm7\"><i>Evolution of the Android Earthquake Alerts System’s earthquake magnitude error over the last three years. For one earthquake, the system does several detections as new data is gathered. The first estimate is important as it provides the maximum warning time, whereas the maximum magnitude estimate generates an alert for the largest area.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"rgp22\">Specific examples</h2><p data-block-key=\"2bpsu\">So, how well does it work in a real earthquake? Let's look at three examples.</p><p data-block-key=\"rp37\">During a magnitude 6.7 earthquake in the Philippines in November 2023, our system sent out the first alert just 18.3 seconds after the quake started. People closest to the epicenter, who experienced the most intense shaking, received up to 15 seconds of warning. Those farther away, who still felt moderate shaking, got up to a minute of warning. In total, nearly 2.5 million people were alerted.</p><p data-block-key=\"8fgml\">In a magnitude 5.7 earthquake in Nepal in November 2023, the first alert was issued 15.6 seconds after the earthquake began. People who experienced moderate to strong shaking had a warning time of 10 to 60 seconds. In this event, over 10 million alerts were delivered.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EQdetection4_Warning.width-1250.png\" alt=\"EQdetection4_Warning\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EQdetection4_Warning.width-1250.png\" alt=\"EQdetection4_Warning\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"i9nm7\"><i>These charts show the warning time people received based on how far they were from the epicenter and the intensity of the shaking they experienced.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"rgp22\">Moreover, in a magnitude 6.2 earthquake in Turkey in April 2025, the first alert was issued 8.0 seconds after the earthquake began. People who experienced moderate to strong shaking had a warning time of a few to 20 seconds. In this event, over 11 million alerts were delivered.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/EQdetection5_ExampleFinal.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ofl7l\"><i>Animation showing phones detecting shaking as the 6.2 earthquake in Turkey progressed. Yellow dots are phones that detect shaking. The yellow circle is the P-wave’s estimated location and the red circle is for the S-wave. Note that phones can detect shaking for reasons other than an earthquake which is a source of noise the system needs to handle. UTC time during the event is displayed in the upper left.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"bx736\">Learning from user feedback</h2><p data-block-key=\"jou3\">The true test of any alert system is if people find it helpful. We included a simple survey in our alerts, and the feedback has been overwhelmingly positive. Of the more than 1.5 million people who responded, 85% found the alerts to be \"very helpful.\"</p><p data-block-key=\"79tab\">Here are some of the key takeaways:</p><ul><li data-block-key=\"3sv1e\"><i>People appreciate the warning, even if they don't feel shaking.</i> A surprising 79% of people who received an alert but didn't feel the earthquake still found the alert to be very helpful. This tells us that people value being informed about potential hazards in their area.</li><li data-block-key=\"c4u75\"><i>The right alert matters.</i> A much higher percentage of people who received a TakeAction alert reported feeling strong shaking compared to those who got a BeAware alert. This shows that our system is doing a good job of distinguishing between light and potentially damaging shaking.</li><li data-block-key=\"9oe5f\"><i>People are taking action.</i> For those who received a TakeAction alert, the most common response was to \"Drop, Cover, and Hold On.\" This is a fantastic result and shows that these alerts are prompting people to take the correct, life-saving actions.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EQdetection6_UserFeedback.width-1250.png\" alt=\"EQdetection6_UserFeedback\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EQdetection6_UserFeedback.width-1250.png\" alt=\"EQdetection6_UserFeedback\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"xsckk\"><i>A snapshot of what over 1.5 million users told us about their experience with the earthquake alerts.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"bx736\">The future of earthquake early warning</h2><p data-block-key=\"6sjpo\">What's most exciting is that our system is constantly learning and improving. The data we collect is helping us to better understand earthquakes and build more accurate prediction models. In the future, this system could not only provide warnings but also deliver rapid post-earthquake information to emergency responders, helping them to quickly assess the areas most in need.</p><p data-block-key=\"f10ud\">We’re excited to continue to show how the devices in so many of our pockets can be used to create a more informed and safer world.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "用于关系数据的图基础模型 (原标题: Graph foundation models for relational data)",
      "link": "https://research.google/blog/graph-foundation-models-for-relational-data/",
      "pubDate": "Wed, 09 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-09T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 用于关系数据的图基础模型\n\n### 引言\n\n关系数据库是企业数据的主要形式，为谷歌及日常服务（如内容推荐、交通预测）提供支持。然而，大多数复杂应用涉及多张表（谷歌有些应用甚至需要维护数百张表），从这些表网络中提取有价值的信息并非易事。传统的表格机器学习（ML）方法（如决策树）难以充分利用这些关系模式的连接结构。尽管图神经网络（GNN）在图结构数据方面取得了进展，但它们通常固定于特定训练图，无法泛化到具有新节点、边类型、特征和标签的新图。目前，尚未出现一个能够学习关系数据中有意义表示并处理所有节点、链接和图级别预测任务的通用模型。\n\n### 目标\n\n本文探讨了设计一个单一模型的可能性，该模型不仅能在互联的关系表中表现出色，还能在无需额外训练的情况下，泛化到任意的表、特征和任务集。谷歌分享了在开发此类图基础模型（GFM）方面的最新进展，这些模型将图学习和表格机器学习的边界推向了标准基线之外。\n\n### 将关系表转换为图\n\n*   **核心理念**：利用表之间的连接结构是实现有效机器学习算法和更好下游性能的关键，即使表格特征数据（如价格、大小、类别）稀疏或嘈杂。\n*   **数据准备**：唯一的准备步骤是将表集合转换为一个单一的异构图。\n    *   **过程**：每个表成为一个独特的节点类型，表中的每一行成为一个节点。对于表中的每一行，其外键关系成为指向其他表中相应节点的带类型边，而其余列则被视为节点特征（通常为数值或分类值）。此外，还可以选择将时间信息保留为节点或边特征。\n    *   **挑战**：将关系表转换为图后，每个目标域都会产生具有不同节点类型、边类型、节点特征和节点标签的独立图。下一个挑战是创建一个单一的、可泛化的ML模型，该模型可以在一个图（一组表）上训练，并在结构和模式存在差异的任何未见图上执行推理。\n\n### 图基础模型 (GFM)\n\n*   **构建方法**：构建基础模型的典型方法是使用高容量神经网络（如Transformer）在大量多样化数据上进行训练。\n*   **GFM的独特挑战**：图缺乏通用的“标记化”机制。与语言和视觉模型不同，图无法通过预定义的词汇表或图像块进行统一编码。\n*   **解决方案**：这需要可迁移的方法来编码任意数据库模式（无论节点类别和边类型的数量如何）并处理节点特征。这包括为具有不同数量和类型的特征（例如，三个连续浮点特征或三十个分类特征）的节点推导出固定大小的表示。为了使模型能够泛化到任意表和节点类型（例如，在引文图上训练并在产品图上进行推理），不能依赖硬编码的节点类型嵌入表。同样，对于节点特征，模型需要能够从训练时的“长度”和“季节”等特征泛化到“价格”和“大小”等任意浮点和分类特征。\n*   **关键发现**：模型如果依赖“绝对”数据集特征（即硬编码的嵌入表或特定于给定特征分布的投影），则无法泛化。相反，捕捉特征在多样化任务中如何相互作用，才能带来更好的泛化能力。\n\n### 成果\n\n*   **谷歌规模应用**：在谷歌规模下，处理数十亿节点和边的图，JAX环境和可扩展的TPU基础设施发挥了关键作用。如此大的数据量有利于训练通用模型。\n*   **内部任务表现**：GFM在多个内部分类任务中进行了测试，例如广告中的垃圾邮件检测，这些任务涉及数十个大型且相互连接的关系表。\n*   **性能提升**：与最佳调优的单表基线相比，GFM带来了显著的性能提升。根据下游任务的不同，GFM在平均精度方面带来了3到40倍的增益，这表明关系表中的图结构为ML模型提供了至关重要的信号。\n\n**图片：GFM在关系数据上的表现提升**\n\n![GFM4RelationalData-4](https://storage.googleapis.com/gweb-research2023-media/images/GFM4RelationalData-4.width-1250.png)\n\n### 结论\n\n利用数据结构改进ML模型是一个日益重要的领域，在人工智能中具有广泛应用。将基础模型方法应用于图学习，开辟了模型重用的新途径，并显著改善了零样本和少样本泛化能力。通过进一步的扩展、多样化的训练数据收集以及对泛化更深入的理论理解，这些结果有望得到进一步提升。",
      "shortSummary": "关系数据库广泛应用，但传统机器学习难以利用其复杂连接。现有图神经网络（GNN）缺乏泛化能力。谷歌提出“图基础模型”（GFM），通过将关系表转换为异构图，并学习可泛化的图表示。GFM克服了图数据“标记化”挑战，通过捕捉特征交互实现泛化。在谷歌内部任务中，GFM比传统方法性能提升3-40倍，证明图结构为机器学习模型提供了关键信号，显著改善了零样本和少样本泛化能力。",
      "translated_title": "用于关系数据的图基础模型",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GFM4RelationalData-4.width-1250.png",
          "alt": "GFM4RelationalData-4",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"m73wf\">Relational databases constitute the main bulk of enterprise data formats and power many prediction services across Google as well as other services people use every day, like content recommendation or traffic prediction. Most non-trivial applications employ multiple tables — in fact, some elaborate applications at Google might require maintaining hundreds of tables — and extracting an actionable value from such networks of tables is rather non-trivial. Traditional tabular machine learning (ML) methods (like <a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning\" target=\"_blank\" rel=\"noopener noreferrer\">decision trees</a>) often struggle to fully leverage the connectivity structure of these relational schemas.</p><p data-block-key=\"lh3b\">On the other hand, recent advances in ML offer a suite of tools to build <a href=\"https://research.google/blog/graph-neural-networks-in-tensorflow/\">graph neural networks</a> (GNN) tailored for <a href=\"https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)\" target=\"_blank\" rel=\"noopener noreferrer\">graph</a>-structured data, where industry-relevant tasks can be framed as <a href=\"https://research.google/blog/graph-neural-networks-in-tensorflow/\">node classification</a> (or regression) or <a href=\"https://research.google/blog/insights-into-population-dynamics-a-foundation-model-for-geospatial-inference/\">graph-level predictions</a>. However, most GNNs are fixed to a particular graph on which the model has been trained and cannot generalize to novel graphs with new nodes, edge types, features, and node labels. For example, a model trained on a large 100M-node citation graph benchmark can’t be re-used for your own graph (e.g., transactions between users and products) since the feature and label spaces are vastly different, so you’ll have to re-train the same model from scratch on your own data. While some initial attempts have demonstrated the viability of the concept in specific <a href=\"https://arxiv.org/abs/2310.04562\" target=\"_blank\" rel=\"noopener noreferrer\">link prediction</a> and <a href=\"https://arxiv.org/abs/2405.20445\" target=\"_blank\" rel=\"noopener noreferrer\">node classification</a> tasks, there has yet to be a generalist model that can learn meaningful representations across relational data and tackle all node-, link-, and graph-level prediction tasks.</p><p data-block-key=\"b1clf\">Today, we explore the possibility of designing a single model that can excel on interconnected relational tables and at the same time generalize to any arbitrary set of tables, features, and tasks without additional training. We are excited to share our recent progress on developing such graph foundation models (GFM) that push the frontiers of graph learning and tabular ML well beyond standard baselines.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Relational tables as graphs</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"m73wf\">We argue that leveraging the connectivity structure between tables is key for effective ML algorithms and better downstream performance, even when tabular feature data (e.g., price, size, category) is sparse or noisy. To this end, the only data preparation step consists of transforming a collection of tables into a single heterogeneous graph.</p><p data-block-key=\"202mb\">The process is rather straightforward and can be executed at scale: each table becomes a unique node type and each row in a table becomes a node. For each row in a table, its <a href=\"https://en.wikipedia.org/wiki/Foreign_key\" target=\"_blank\" rel=\"noopener noreferrer\">foreign key</a> relations become typed edges to respective nodes from other tables while the rest of the columns are treated as node features (typically, with numerical or categorical values). Optionally, we can also keep temporal information as node or edge features.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/GFM4RelationalData-2.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8l8yd\"><i>Data preparation consists of transforming tables into a single graph, where each row of a table becomes a node of the respective node type, and foreign key columns become edges between the nodes. Connections between five tables shown become edges in the resulting graph.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"9c5o9\">Transforming relational tables into graphs for each target domain results in separate graphs with a different number of node types, edge types, node features, and node labels. The next challenge is to create a single generalizable ML model, which can be trained on one graph (a set of tables) and perform inference on any unseen graph despite the differences in the structure and schema.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Graph foundation models</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9c5o9\">A typical recipe for building foundation models is to use a high-capacity neural network (like a <a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" rel=\"noopener noreferrer\">Transformer</a>) trained on large amounts of diverse data. A unique challenge of GFMs is the lack of a common tokenization mechanism for graphs. In contrast, when applying a Transformer to language and vision models, every possible string can be represented via <a href=\"https://research.google/blog/a-fast-wordpiece-tokenization-system/\">tokens from a prepared vocabulary</a> or images and videos can be <a href=\"https://research.google/blog/improving-vision-transformer-efficiency-and-accuracy-by-learning-to-tokenize/\">encoded via image patches</a>, respectively.</p><p data-block-key=\"eke0d\">When applied to heterogeneous graphs made of relational data, this requires transferable methods for encoding arbitrary database schemas — regardless of the number of node (class) and edge types between them — and handling node features. This includes deriving a fixed-size representation for nodes with, for example, three continuous float features or thirty categorical features. Since we want a single model that can generalize to arbitrary tables and node types — for example, training on citation graphs and running inference on product graphs — we cannot rely on hard-coded embedding tables of node types. Similarly for node features, we want a model to generalize from training on features like “length” and “season” to arbitrary floats and categorical features like “price” and “size”.</p><p data-block-key=\"bmg6d\">Our key finding is that models trained on “absolute” dataset features, i.e., hard-coded embedding tables or projections specific to a given feature distribution, do not generalize, whereas capturing how features interact with each other in diverse tasks does lead to better generalization.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/GFM4RelationalData-1.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"w8ha8\">Similar to frontier language and vision models like <a href=\"https://deepmind.google/models/gemini/pro/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a>, a GFM is <i>a single model that learns transferable graph representations that can generalize to any new, previously unseen graph, including its schema, structure, and features.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9c5o9\">Operating at Google scale means processing graphs of billions of nodes and edges where our <a href=\"https://github.com/jax-ml/jax\" target=\"_blank\" rel=\"noopener noreferrer\">JAX</a> environment and scalable <a href=\"https://en.wikipedia.org/wiki/Tensor_Processing_Unit\" target=\"_blank\" rel=\"noopener noreferrer\">TPU</a> infrastructure particularly shines. Such data volumes are amenable for training generalist models, so we probed our GFM on several internal classification tasks like spam detection in ads, which involves dozens of large and connected relational tables. Typical tabular baselines, albeit scalable, do not consider connections between rows of different tables, and therefore miss context that might be useful for accurate predictions. Our experiments vividly demonstrate that gap.</p><p data-block-key=\"ebjse\">We observe a significant performance boost compared to the best tuned single-table baselines. Depending on the downstream task, GFM brings 3x – 40x gains in average <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\" target=\"_blank\" rel=\"noopener noreferrer\">precision</a>, which indicates that the graph structure in relational tables provides a crucial signal to be leveraged by ML models.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GFM4RelationalData-4.width-1250.png\" alt=\"GFM4RelationalData-4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GFM4RelationalData-4.width-1250.png\" alt=\"GFM4RelationalData-4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusions</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9c5o9\">Leveraging the structure of data to improve ML models is an area of growing importance, <a href=\"https://research.google/blog/talk-like-a-graph-encoding-graphs-for-large-language-models/\">with broad applications in artificial intelligence</a>. We’ve observed that adapting a foundation model approach to graph learning enables new avenues for model reuse, and substantially improved zero-shot and few-shot generalization. These results can be further improved by additional scaling and diverse training data collection together with a deeper theoretical understanding of generalization.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9c5o9\"><i>The following researchers contributed to this work: Michael Galkin, Brandon Mayer, Hamed Sadeghi, Mathieu Gillaume-Bert, Arjun Gopalan, Saurabh Nagrecha, Pramod Doguparty, Bryan Perozzi, Jonathan Halcrow, Silvio Lattanzi, Vahab Mirrokni, and the Google Research</i> <a href=\"https://research.google/teams/graph-mining/\"><i>Graph Mining team</i></a><i>. We would also like to thank Kimberly Schwede for creating the illustrations in this post.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "MedGemma：我们最强大的健康AI开发开源模型 (原标题: MedGemma: Our most capable open models for health AI development)",
      "link": "https://research.google/blog/medgemma-our-most-capable-open-models-for-health-ai-development/",
      "pubDate": "Tue, 08 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-08T16:00:00.000Z",
      "creator": "Google",
      "summary": "## MedGemma：我们最强大的健康AI开发开源模型\n\n### 引言：医疗AI的需求与HAI-DEF\n\n医疗保健领域正日益广泛地采用人工智能（AI）来改进工作流管理、患者沟通以及诊断和治疗支持。构建高性能、高效且保护隐私的AI系统至关重要。基于这些考量，我们构建并发布了**健康AI开发者基础（HAI-DEF）**。HAI-DEF是一系列轻量级开源模型，旨在为开发者提供强大的起点，以进行健康研究和应用开发。由于HAI-DEF模型是开源的，开发者可以完全控制隐私、基础设施和模型的修改。\n\n### MedGemma系列扩展与新模型发布\n\n2023年5月，我们通过**MedGemma**扩展了HAI-DEF系列。MedGemma是一系列基于Gemma 3的生成模型，旨在加速医疗保健和生命科学AI开发。今天，我们自豪地宣布该系列中的两款新模型：\n\n*   **MedGemma 27B 多模态：** 补充了此前发布的4B多模态和27B纯文本模型，新增了对复杂多模态和纵向电子健康记录解读的支持。\n*   **MedSigLIP：** 一款轻量级图像和文本编码器，用于分类、搜索及相关任务。MedSigLIP基于与4B和27B MedGemma模型相同的图像编码器。\n\nMedGemma和MedSigLIP是医疗研究和产品开发的强大起点。MedGemma适用于需要生成自由文本的医疗文本或图像任务，例如报告生成或视觉问答。MedSigLIP推荐用于涉及结构化输出（如分类或检索）的图像任务。所有上述模型都可以在单个GPU上运行，而MedGemma 4B和MedSigLIP甚至可以适应在移动硬件上运行。MedGemma和MedSigLIP开发的完整细节和评估可在MedGemma技术报告中找到。\n\n![MedGemma-1a-Overview](https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-1a-Overview.width-1250.png)\n\n### MedGemma：医疗领域的多模态生成模型\n\nMedGemma系列包括4B和27B两种尺寸的变体，两者现在都接受图像和文本输入并生成文本输出。\n\n*   **MedGemma 4B 多模态：** 在MedQA上得分64.4%，使其位列最佳超小型（<8B）开源模型之列。在一项非盲法研究中，81%的MedGemma 4B生成的胸部X光报告被美国认证放射科医生判断为准确性足以导致与原始放射科医生报告相似的患者管理。它还在医学图像分类任务上取得了与特定任务的SOTA模型相当的性能。\n\n*   **MedGemma 27B 文本和MedGemma 27B 多模态：** 根据内部和已发布的评估，MedGemma 27B模型在MedQA医学知识和推理基准测试中是表现最佳的小型开源模型（<50B）之一；其文本变体得分87.7%，与领先的开源模型DeepSeek R1仅差3分，但推理成本约为其十分之一。MedGemma 27B模型在包括电子健康记录数据检索和解读在内的各种基准测试中，与大型模型具有竞争力。\n\n![MedGemma-2a-MedQA](https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-2a-MedQA.width-1250.png)\n\n*在MedQA上，MedGemma 4B和27B是同等规模模型中表现最佳的。*\n\n![MedGemma-3b-CXR](https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-3b-CXR.width-1250.jpg)\n\n*根据美国心脏胸腔放射科医生的审查，我们发现81%的MedGemma胸部X光报告将导致与原始放射科医生报告相似的患者管理。*\n\n这些模型是通过训练医学优化的图像编码器（独立发布为MedSigLIP，下文描述），然后将Gemma 3模型的相应4B和27B版本在医疗数据上进行训练而开发的。在此过程中，我们注意保留了Gemma的通用（非医疗）能力。这使得MedGemma在混合医疗和非医疗信息的任务中表现良好，并保留了指令遵循和非英语语言能力。\n\n![MedGemma-4a-MedSigLIP](https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-4a-MedSigLIP.width-1250.png)\n\n这些模型的一个关键方面是其适应性。例如，在微调后，MedGemma 4B能够在胸部X光报告生成方面达到最先进的性能，RadGraph F1得分为30.3。开发者能够直接提高其目标应用程序性能的能力，突显了MedGemma作为构建医疗AI的开发者起点的价值。\n\n### MedSigLIP：医疗领域的专用图像编码器\n\nMedSigLIP是一款轻量级图像编码器，仅有4亿参数，采用Sigmoid损失语言图像预训练（SigLIP）架构。MedSigLIP通过使用多样化的医学影像数据（包括胸部X光、组织病理学切片、皮肤病学图像和眼底图像）进行微调，使其能够学习特定于这些模态的细微特征。重要的是，我们也确保MedSigLIP在原始SigLIP模型训练的自然图像上保持强大的性能，从而保持其多功能性。\n\nMedSigLIP旨在通过将医学图像和医学文本编码到共同的嵌入空间中，弥合两者之间的差距。与特定任务的视觉嵌入模型相比，MedSigLIP实现了相似或改进的分类性能，同时在医学影像领域更具通用性。\n\nMedSigLIP非常适合：\n\n*   **传统图像分类：** 构建高性能模型以分类医学图像。\n*   **零样本图像分类：** 无需特定训练示例即可通过比较图像嵌入与文本类别标签的嵌入来分类图像。\n*   **语义图像检索：** 从大型医学图像数据库中查找视觉或语义相似的图像。\n\n### 开源模型的优势\n\n由于MedGemma系列是开源的，模型可以被下载、在此基础上构建并进行微调，以支持开发者的特定需求。特别是在医疗领域，这种开放方法比基于API的模型具有几个明显的优势：\n\n*   **灵活性和隐私：** 模型可以在开发者偏好的环境中（包括Google Cloud Platform或本地）运行，解决隐私问题或机构政策。\n*   **定制化以实现高性能：** 模型可以进行微调和修改，以在目标任务和数据集上实现最佳性能。\n*   **可复现性和稳定性：** 由于模型以快照形式分发，其参数是固定的，不像API那样随时间意外变化。这种稳定性对于一致性和可复现性至关重要的医疗应用尤为关键。\n\n为了确保广泛的可访问性和易用性，我们的Hugging Face集合以流行的Hugging Face safetensors格式提供MedSigLIP和MedGemma。\n\n### 开发者应用案例\n\n研究人员和开发者一直在探索MedGemma模型，并发现这些模型能够很好地解决一些关键问题：\n\n*   美国马萨诸塞州DeepHealth的开发者正在探索MedSigLIP，以改进其胸部X光分诊和结节检测。\n*   台湾长庚纪念医院的研究人员指出，MedGemma与繁体中文医学文献配合良好，并能很好地回答医务人员的问题。\n*   印度古尔冈Tap Health的开发者评论了MedGemma卓越的医学基础，指出其在需要对临床背景敏感的任务（如总结病程记录或建议符合指南的提示）上的可靠性。\n\n我们很高兴能继续从开发者那里了解这些以及其他用例，因为他们正在使用MedGemma和MedSigLIP创建下一代健康AI工具。\n\n### 开始使用\n\n为了帮助开发者入门，我们在GitHub上提供了MedGemma和MedSigLIP的详细Jupyter Notebook，演示了如何在Hugging Face上创建MedSigLIP和MedGemma实例以进行推理和微调。当开发者准备好进行扩展时，MedGemma和MedSigLIP可以无缝部署到Vertex AI作为专用端点，我们还在GitHub中提供了如何在这些端点上运行推理的示例。我们还在HAI-DEF Hugging Face演示集合中添加了一个新演示，展示了如何将MedGemma构建到应用程序中，以简化患者就诊前的预访信息收集。\n\n![MedGemma-6-Summary](https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-6-Summary.width-1250.png)\n\n### 训练数据集说明与免责声明\n\n模型使用公共和私人去识别化数据集混合训练。Google及其合作伙伴利用经过严格匿名化或去识别化的数据集，以确保保护个人研究参与者和患者隐私。\n\n**免责声明：** MedGemma和MedSigLIP旨在作为高效开发下游医疗应用的起点，涉及医疗文本和图像。MedGemma和MedSigLIP不应在未经开发者适当验证、适应和/或进行有意义修改的情况下直接用于其特定用例。这些模型生成的输出不旨在直接用于临床诊断、患者管理决策、治疗建议或任何其他直接临床实践应用。性能基准突出显示了相关基准上的基线能力，但即使对于构成大量训练数据的图像和文本领域，也可能出现不准确的模型输出。所有模型输出都应被视为初步结果，需要通过既定的研究和开发方法进行独立验证、临床关联和进一步调查。",
      "shortSummary": "Google发布了MedGemma和MedSigLIP，进一步扩展其健康AI开发者基础（HAI-DEF）开源模型系列。MedGemma是基于Gemma 3的多模态生成模型，包含4B和27B版本，擅长医疗文本和图像生成，如报告生成。MedSigLIP是轻量级图像和文本编码器，适用于医疗图像分类和检索。这些模型旨在加速医疗AI开发，提供灵活性、隐私保护和定制化能力，并在MedQA等基准测试中展现出卓越性能，为开发者构建下一代健康AI工具提供强大起点。",
      "translated_title": "MedGemma：我们最强大的健康AI开发开源模型",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-1a-Overview.width-1250.png",
          "alt": "MedGemma-1a-Overview",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-2a-MedQA.width-1250.png",
          "alt": "MedGemma-2a-MedQA",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-3b-CXR.width-1250.jpg",
          "alt": "MedGemma-3b-CXR",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-4a-MedSigLIP.width-1250.png",
          "alt": "MedGemma-4a-MedSigLIP",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-6-Summary.width-1250.png",
          "alt": "MedGemma-6-Summary",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"vfkaa\">Healthcare is increasingly embracing AI to improve workflow management, patient communication, and diagnostic and treatment support. It’s critical that these AI-based systems are not only high-performing, but also efficient and privacy-preserving. It’s with these considerations in mind that we built and recently released <a href=\"http://goo.gle/hai-def\" target=\"_blank\" rel=\"noopener noreferrer\">Health AI Developer Foundations</a> (HAI-DEF). HAI-DEF is a collection of lightweight open models designed to offer developers robust starting points for their own health research and application development. Because HAI-DEF models are open, developers retain full control over privacy, infrastructure and modifications to the models. In <a href=\"https://research.google/blog/google-research-at-google-io-2025/\">May</a> of this year, we expanded the HAI-DEF collection with <a href=\"https://deepmind.google/models/gemma/medgemma/\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma</a>, a collection of generative models based on <a href=\"https://deepmind.google/models/gemma/gemma-3/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma 3</a> that are designed to accelerate healthcare and lifesciences AI development.</p><p data-block-key=\"57k5v\">Today, we’re proud to announce two new models in this collection. The first is MedGemma 27B Multimodal, which complements the previously-released 4B Multimodal and 27B text-only models by adding support for complex multimodal and longitudinal electronic health record interpretation. The second new model is MedSigLIP, a lightweight image and text encoder for classification, search, and related tasks. MedSigLIP is based on the same image encoder that powers the 4B and 27B MedGemma models.</p><p data-block-key=\"748kc\">MedGemma and MedSigLIP are strong starting points for medical research and product development. MedGemma is useful for medical text or imaging tasks that require generating free text, like report generation or visual question answering. MedSigLIP is recommended for imaging tasks that involve structured outputs like classification or retrieval. All of the above models can be run on a single GPU, and MedGemma 4B and MedSigLIP can even be adapted to run on mobile hardware.</p><p data-block-key=\"3rv2m\">Full details of MedGemma and MedSigLIP development and evaluation can be found in the <a href=\"https://arxiv.org/abs/2507.05201\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma technical report</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-1a-Overview.width-1250.png\" alt=\"MedGemma-1a-Overview\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-1a-Overview.width-1250.png\" alt=\"MedGemma-1a-Overview\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">MedGemma: A multimodal generative model for health</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ndfm1\">The MedGemma collection includes variants in 4B and 27B sizes, both of which now accept image and text inputs and produce text outputs.</p><ul><li data-block-key=\"fbsaf\"><b>MedGemma 4B Multimodal</b>: MedGemma 4B scores 64.4% on <a href=\"https://arxiv.org/abs/2009.13081\" target=\"_blank\" rel=\"noopener noreferrer\">MedQA</a>, which ranks it among the best very small (&lt;8B) open models. In an unblinded study, 81% of MedGemma 4B–generated chest X-ray reports were judged by a US board certified radiologist to be of sufficient accuracy to result in similar patient management compared to the original radiologist reports. It additionally achieves performance on medical image classification tasks that is competitive with task-specific state-of-the-art models.<br><br></li><li data-block-key=\"2qp0\"><b>MedGemma 27B Text</b> and<b> MedGemma 27B Multimodal</b>: Based on internal and published evaluations, the MedGemma 27B models are among the best performing small open models (&lt;50B) on the MedQA medical knowledge and reasoning benchmark; the text variant scores 87.7%, which is within 3 points of <a href=\"https://github.com/deepseek-ai/DeepSeek-R1\" target=\"_blank\" rel=\"noopener noreferrer\">DeepSeek R1</a>, a leading open model, but at approximately one tenth the inference cost. The MedGemma 27B models are competitive with larger models across a variety of benchmarks, including retrieval and interpretation of electronic health record data.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-2a-MedQA.width-1250.png\" alt=\"MedGemma-2a-MedQA\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-2a-MedQA.width-1250.png\" alt=\"MedGemma-2a-MedQA\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ou0n6\">On MedQA, MedGemma 4B and 27B are among the best performing models of their size. Note that in this plot, cost estimates are made based on <a href=\"http://legacy.lmarena.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">legacy.lmarena.ai</a> price analysis and <a href=\"http://together.ai/pricing\" target=\"_blank\" rel=\"noopener noreferrer\">together.ai/pricing</a>. For models not present on the leaderboard, we used price data from the models from which they were derived.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-3b-CXR.width-1250.jpg\" alt=\"MedGemma-3b-CXR\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-3b-CXR.width-1250.jpg\" alt=\"MedGemma-3b-CXR\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"h7d8g\">Based on review by a US board-certified cardiothoracic radiologist, we found that 81% of MedGemma chest X-ray reports would lead to similar patient management compared to the original radiologist reports.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ndfm1\">We developed these models by training a medically optimized image encoder (independently released as MedSigLIP, described below), followed by training the corresponding 4B and 27B versions of the <a href=\"https://huggingface.co/docs/transformers/main/en/model_doc/gemma3\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma 3 model</a> on medical data. We took care to retain the general (non-medical) capabilities of Gemma throughout this process. This allows MedGemma to perform well on tasks that mix medical and non-medical information and preserve instruction-following and capabilities in non-English languages.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-4a-MedSigLIP.width-1250.png\" alt=\"MedGemma-4a-MedSigLIP\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-4a-MedSigLIP.width-1250.png\" alt=\"MedGemma-4a-MedSigLIP\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ndfm1\">A key aspect of these models is their adaptability. For instance, after fine-tuning, MedGemma 4B is able to achieve state-of-the-art performance on chest X-ray report generation, with a <a href=\"https://arxiv.org/abs/2106.14463\" target=\"_blank\" rel=\"noopener noreferrer\">RadGraph F1</a> score of 30.3. The straightforward ability for developers to improve performance on their target applications highlights the value of MedGemma as a starting point for developers looking to build AI for healthcare.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">MedSigLIP: A specialized image encoder for healthcare</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ndfm1\">MedSigLIP is a lightweight image encoder of only 400M parameters that uses the <a href=\"https://arxiv.org/abs/2303.15343\" target=\"_blank\" rel=\"noopener noreferrer\">Sigmoid loss for Language Image Pre-training</a> (SigLIP) architecture. MedSigLIP was adapted from SigLIP via tuning with diverse medical imaging data, including chest X-rays, <a href=\"https://en.wikipedia.org/wiki/Histopathology\" target=\"_blank\" rel=\"noopener noreferrer\">histopathology</a> patches, dermatology images, and <a href=\"https://en.wikipedia.org/wiki/Fundus_photography\" target=\"_blank\" rel=\"noopener noreferrer\">fundus images</a>, allowing the model to learn nuanced features specific to these modalities. Importantly, we also took care to ensure that MedSigLIP retains strong performance on the natural images on which the original SigLIP model was trained, maintaining its versatility.</p><p data-block-key=\"8g76m\">MedSigLIP is designed to bridge the gap between medical images and medical text by encoding them into a common embedding space. MedSigLIP achieves similar or improved classification performance compared to task-specific vision embedding models while being far more versatile across medical imaging domains.</p><p data-block-key=\"673ca\">MedSigLIP is ideal for:</p><ul><li data-block-key=\"3ecjd\"><i>Traditional image classification:</i> Build performant models to classify medical images.<br><br></li><li data-block-key=\"8sio3\"><i>Zero-shot image classification:</i> Classify images without specific training examples by comparing image embeddings to the embeddings of textual class labels.<br><br></li><li data-block-key=\"9m877\"><i>Semantic image retrieval:</i> Find visually or semantically similar images from large medical image databases.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The power of open models</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ndfm1\">Because the MedGemma collection is open, the models can be downloaded, built upon, and fine-tuned to support developers’ specific needs. Particularly in the medical space, this open approach offers several distinct advantages over API-based models:</p><ul><li data-block-key=\"fqifp\"><i>Flexibility and privacy:</i> Models can be run on proprietary hardware in the developer’s preferred environment, including on Google Cloud Platform or locally, which can address privacy concerns or institutional policies.<br><br></li><li data-block-key=\"1j8r8\"><i>Customization for high performance:</i> Models can be fine-tuned and modified to achieve optimal performance on target tasks and datasets.<br><br></li><li data-block-key=\"b0rvo\"><i>Reproducibility and stability:</i> Because the models are distributed as snapshots, their parameters are frozen and unlike an API, will not change unexpectedly over time. This stability is particularly crucial for medical applications where consistency and reproducibility are paramount.</li></ul><p data-block-key=\"bbula\">To ensure broad accessibility and ease of use, our <a href=\"https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4\" target=\"_blank\" rel=\"noopener noreferrer\">Hugging Face collection</a> offers MedSigLIP and MedGemma in the popular <a href=\"https://huggingface.co/docs/safetensors/en/index\" target=\"_blank\" rel=\"noopener noreferrer\">Hugging Face safetensors</a> format.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">What developers are building with MedGemma &amp; MedSigLIP</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ndfm1\">Researchers and developers have been exploring the MedGemma models for their use cases and have found the models adept at solving some crucial problems. Developers at <a href=\"https://deephealth.com/\" target=\"_blank\" rel=\"noopener noreferrer\">DeepHealth</a> in Massachusetts, USA have been exploring MedSigLIP to improve their chest X-ray triaging and nodule detection. Researchers at <a href=\"https://www.cgmh.org.tw/eng\" target=\"_blank\" rel=\"noopener noreferrer\">Chang Gung Memorial Hospital</a> in Taiwan noted that MedGemma works well with traditional Chinese-language medical literature and can respond well to medical staff questions. Developers at <a href=\"https://tap.health/\" target=\"_blank\" rel=\"noopener noreferrer\">Tap Health</a> in Gurgaon, India, remarked on MedGemma’s superior medical grounding, noting its reliability on tasks that require sensitivity to clinical context, such as summarizing progress notes or suggesting guideline-aligned nudges.</p><p data-block-key=\"fisuk\">We’re excited to continue to learn about these and other use cases from developers as they create the next generation of Health AI tools with MedGemma and MedSigLIP.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/MedGemma-0a-HeroVid.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Get started and explore</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ndfm1\">To help developers get started, we’ve provided detailed notebooks on GitHub for <a href=\"https://github.com/google-health/medgemma\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma</a> and <a href=\"https://github.com/google-health/medsiglip\" target=\"_blank\" rel=\"noopener noreferrer\">MedSigLIP</a> that demonstrate how to create instances of MedSigLIP and MedGemma for both inference and fine-tuning on Hugging Face. When developers are ready to scale, MedGemma and MedSigLIP can be seamlessly deployed in <a href=\"https://cloud.google.com/vertex-ai\" target=\"_blank\" rel=\"noopener noreferrer\">Vertex AI</a> as dedicated endpoints, and we provide examples in GitHub of how to run inference on these endpoints. We’ve also added a <a href=\"https://huggingface.co/spaces/google/appoint-ready\" target=\"_blank\" rel=\"noopener noreferrer\">new demo</a> to our HAI-DEF Hugging Face <a href=\"https://huggingface.co/collections/google/hai-def-concept-apps-6837acfccce400abe6ec26c1\" target=\"_blank\" rel=\"noopener noreferrer\">demo collection</a> that shows how MedGemma can be built into an application to streamline pre-visit information gathering ahead of a patient appointment.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/MedGemma-5a-Demos.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"7t2ez\">This demo illustrates how MedGemma can be built into an application to streamline pre-visit information gathering ahead of a patient appointment. Code for the demo is available on <a href=\"https://huggingface.co/spaces/google/appoint-ready\" target=\"_blank\" rel=\"noopener noreferrer\">its Hugging Face site</a>.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xdhby\">Refer to the following table to understand which model from the MedGemma family is ideal for your use case.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-6-Summary.width-1250.png\" alt=\"MedGemma-6-Summary\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-6-Summary.width-1250.png\" alt=\"MedGemma-6-Summary\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --left\">\n        <p data-block-key=\"zr19a\">* For pathology-specific applications that do not require language alignment, <a href=\"https://developers.google.com/health-ai-developer-foundations/path-foundation\" target=\"_blank\" rel=\"noopener noreferrer\">Path Foundation</a> provides high performance for data-efficient classification and lower compute requirements.</p><p data-block-key=\"45ch6\">** <a href=\"https://en.wikipedia.org/wiki/Fast_Healthcare_Interoperability_Resources\" target=\"_blank\" rel=\"noopener noreferrer\">Fast Healthcare Interoperability Resources</a> (FHIR) records are text-based, but have a unique structure. Electronic health record data was included in the training of the MedGemma 27B multimodal model only.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ndfm1\">Please visit <a href=\"https://goo.gle/hai-def\" target=\"_blank\" rel=\"noopener noreferrer\">the HAI-DEF site</a> for these resources and to learn more about the MedGemma collection and other Health AI Developer Foundations models. The <a href=\"https://discuss.ai.google.dev/c/hai-def/62\" target=\"_blank\" rel=\"noopener noreferrer\">HAI-DEF forum</a> is available for questions or feedback.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Note on training datasets</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ndfm1\">Models were trained on a mix of public and private de-identified datasets. Google and its partners utilize datasets that have been rigorously anonymized or de-identified to ensure the protection of individual research participants and patient privacy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Disclaimer</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ndfm1\">MedGemma and MedSigLIP are intended to be used as a starting point that enables efficient development of downstream healthcare applications involving medical text and images. MedGemma and MedSigLIP are not intended to be used without appropriate validation, adaptation and/or making meaningful modification by developers for their specific use case. The outputs generated by these models are not intended to directly inform clinical diagnosis, patient management decisions, treatment recommendations, or any other direct clinical practice applications. Performance benchmarks highlight baseline capabilities on relevant benchmarks, but even for image and text domains that constitute a substantial portion of training data, inaccurate model output is possible. All model outputs should be considered preliminary and require independent verification, clinical correlation, and further investigation through established research and development methodologies.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"yb79i\"><i>MedGemma is the product of a collaboration between Google Research and Google DeepMind. We thank the many people who contributed to this work, including the engineering and cross-functional members of the Google Health AI and Gemma teams, as well as our sponsors in Google Research and Google Deepmind.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "通过声音定位提升群组对话的可访问性 (原标题: Making group conversations more accessible with sound localization)",
      "link": "https://research.google/blog/making-group-conversations-more-accessible-with-sound-localization/",
      "pubDate": "Tue, 01 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-01T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 通过声音定位提升群组对话的可访问性：SpeechCompass\n\n### 引言：现有问题与挑战\n\n当前移动设备上的语音转文本（ASR）功能（如Live Transcribe）在听力与言语辅助、语言翻译、笔记记录和会议记录方面发挥着重要作用。然而，在多人对话中，现有的移动ASR应用通常会将所有转录的语音内容简单地拼接在一起，导致用户难以区分是谁在说话。这种限制增加了用户的认知负担，他们需要同时处理文本、识别说话者并参与对话。此外，依赖机器学习（ML）的现有解决方案在移动场景中设置复杂，例如，音视频语音分离需要说话者在摄像头可见范围内，而说话者嵌入方法则需要模型确定每个说话者独特的声纹。\n\n### SpeechCompass 方案概述\n\n在荣获CHI 2025最佳论文奖的“SpeechCompass: 通过多麦克风定位增强移动字幕的说话人分离和方向引导”研究中，我们探索了一种通过说话人分离（在ASR文本中区分说话者）和实时传入声音定位来增强移动字幕的方法。SpeechCompass通过为每个说话者提供颜色编码的视觉分离和方向指示器（箭头），帮助用户确定声音的来源方向，从而为群组对话创建用户友好的文本记录。\n\n![SpeechCompass-2](https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-2.width-1250.jpg)\n左图：现有移动转录应用将转录文本拼接在一起。右图：SpeechCompass指示传入语音的方向，在用户界面中通过颜色和方向指示器（如箭头）实现视觉分离的文本。\n\n### SpeechCompass 的优势\n\n与基于机器学习的单源说话人分离方法不同，SpeechCompass的多麦克风方法具有以下优势：\n\n*   **更低的计算和内存成本**：由于没有模型或权重，算法可以在内存和计算能力有限的小型微控制器上运行。\n*   **更低的延迟**：提取说话人嵌入需要足够的信息来聚类说话者，这可能导致延迟。\n*   **更好的隐私保护**：SpeechCompass假设不同的说话者物理上位于不同的位置，不需要视频或任何独特的个人身份信息（如说话者嵌入）。\n*   **语言无关性**：SpeechCompass关注音频波形之间的差异，不预设内容，适用于语音以外的声音。\n*   **即时重新配置**：通过移动手机，SpeechCompass可以即时重新配置。\n\n### 实现方式\n\n我们以两种形式实现了SpeechCompass：\n\n1.  **手机壳原型**：一个带有四个麦克风并连接到低功耗微控制器的手机壳，提供最佳麦克风放置以实现360度声音定位。\n2.  **现有手机软件**：针对带有两个或更多麦克风的现有手机（如Pixel手机）的软件实现，提供180度定位。\n\n在这两种实现中，手机用于语音识别，并通过移动应用程序可视化文本记录。\n\n![SpeechCompass-5](https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-5.width-1250.png)\n原型手机壳及其内部电子元件的实现。A) 带有安装原型壳的移动应用程序界面。B) 带有柔性PCB麦克风支架和主PCB的原型壳。C) 主PCB (STM32) 的顶视图和底视图。\n\n### 声音定位算法\n\n由于声音频率较低，在室内环境中容易反弹产生混响，使得音频（尤其是语音）难以精确本地化。为解决此问题，我们应用了一种基于到达时间差（TDOA）的定位算法。音频信号以略微不同的时间到达每个麦克风，因此算法通过互相关估计麦克风对之间的TDOA，以预测声音的到达角度。具体来说，我们使用广义互相关相位变换（GCC-PHAT）来提高噪声鲁棒性并加快计算速度。然后，我们应用统计估计（如核密度估计）来提高定位精度。使用两个全向麦克风总是存在“前后”混淆（即，阵列前方或后方的信号对麦克风阵列来说是相同的），因此只能实现180度定位。通过使用三个或更多麦克风可以解决此问题，从而实现360度定位。\n\n![SpeechCompass-1](https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-1.width-1250.png)\nSpeechCompass系统图，包括手机壳硬件和手机应用程序。\n\n### 用户界面可视化\n\n我们利用Android的语音转文本功能开发了一个移动应用程序，通过USB接收来自原型手机壳麦克风的定位数据，并增强语音文本记录。Android应用程序提供多种语音文本可视化样式来指示说话者方向：\n\n*   **彩色文本**：使用不同颜色的文本区分说话者。\n*   **方向符号**：文本框周围的箭头、圆形刻度盘和颜色高亮指示每个说话者的位置。\n*   **小地图**：一个类似雷达的小型显示屏显示当前说话者的位置。\n*   **边缘指示器**：屏幕边缘的视觉提示突出显示说话者方向。\n*   **不必要语音抑制**：用户可以点击屏幕两侧来抑制来自这些方向的语音。这可用于移除自己的语音，或从文本记录中移除不相关的附近对话，从而增强附近说话者的隐私。\n\n![SpeechCompass-3](https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-3.width-1250.png)\n各种可视化样式增强了语音文本记录。\n\n### 技术评估\n\n为评估SpeechCompass软件，我们将手机壳放置在旋转平台上，并用一个固定扬声器播放语音或噪音。平台以10度增量旋转，并测量每个角度的到达角。评估结果显示，SpeechCompass能够准确地定位声音方向，对于正常对话响度（60-65 dB），平均误差为11°-22°。其准确性大致与人类定位能力相当（例如，如果一个人被问及声音从他们身后何处传来，他们的答案通常会有高达20度的误差）。SpeechCompass系统在不同材料和不同环境噪音条件下均表现良好。\n\n![SpeechCompass-6](https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-6.width-1250.png)\n不同音频级别和声源角度下的定位误差。\n\n在说话人分离方面，我们使用说话人分离错误率（DER），这是衡量界面中颜色编码说话人分离正确性的标准指标。我们的测试表明，四麦克风配置始终优于三麦克风设置，在不同信噪比（SNR）条件下，相对DER改善了23%–35%。\n\n![SpeechCompass-7](https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-7.width-1250.jpg)\n在不同信噪比下，3麦克风和4麦克风配置的说话人分离错误率（DER）。\n\n### 用户评估与反馈\n\n为理解当前移动字幕技术的局限性，我们对263名字幕技术常用用户进行了在线调查。结果显示，现有解决方案存在一个显著的局限性——无法区分说话者，这使得它们在群组对话中难以使用。\n\n![SpeechCompass-8](https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-8.width-1250.jpg)\n移动字幕常用用户调查结果。\n\n其次，我们向八位移动语音转文本的常用用户展示了原型并收集了反馈。原型用于分离和可视化研究人员之间的对话。我们发现彩色文本和方向箭头是最受欢迎的可视化方法。所有参与者都认同方向引导对于群组对话的价值。\n\n![SpeechCompass-4](https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-4.width-1250.png)\n工作原型用户研究结果。A) 对不同可视化技术的偏好。B) 方向反馈对用户的价值。\n\n### 未来展望\n\n多麦克风定位技术在移动转录方面具有广泛的实际应用前景，例如在课堂环境中帮助学生更轻松地跟踪师生讨论，或在商务会议、采访和社交聚会中帮助用户跟踪多人对话中的说话者变化。SpeechCompass在群组对话的移动字幕方面取得了显著改进，未来有许多可能的开发方向：\n\n*   与智能眼镜、智能手表等更多可穿戴设备集成。\n*   通过机器学习方法增强噪声鲁棒性。\n*   进一步定制可视化偏好。\n*   进行长期研究以了解日常场景中的采用和行为。\n\n我们希望这项研究能激发持续创新，使沟通对每个人都更易于访问和包容。",
      "shortSummary": "SpeechCompass通过多麦克风声音定位技术，显著提升了群组对话的移动语音转文本体验。它解决了现有应用无法区分说话者的问题，通过颜色编码和方向指示器清晰地分离说话者，从而降低用户认知负担。该方案相比机器学习方法，具有更低的成本、延迟和更高的隐私性。技术评估和用户反馈均证实了其在声音定位和说话人分离方面的有效性，为群组沟通提供了更便捷、更具包容性的解决方案。",
      "translated_title": "通过声音定位提升群组对话的可访问性",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-2.width-1250.jpg",
          "alt": "SpeechCompass-2",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-5.width-1250.png",
          "alt": "SpeechCompass-5",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-1.width-1250.png",
          "alt": "SpeechCompass-1",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-3.width-1250.png",
          "alt": "SpeechCompass-3",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-6.width-1250.png",
          "alt": "SpeechCompass-6",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"k94rq\"><a href=\"https://research.google/blog/an-all-neural-on-device-speech-recognizer/\">Speech-to-text</a> capabilities on mobile devices, such as <a href=\"https://research.google/blog/real-time-continuous-transcription-with-live-transcribe/\">Live Transcribe</a>, have become invaluable for hearing and speech accessibility, language translation, note-taking, and meeting transcripts. However, when multiple people participate in a conversation, existing mobile automatic speech recognition (ASR) apps typically concatenate all transcribed speech together, making it difficult to follow who is saying what. This limitation creates cognitive overload for users who need to simultaneously process the transcript, identify speakers, and participate in the conversation. Solutions have been deployed, but are currently impractical to set up in mobile scenarios. For example, <a href=\"https://research.google/blog/looking-to-listen-audio-visual-speech-separation/\">audio-visual speech separation</a> requires speakers to be visible to a camera and <a href=\"https://research.google/blog/who-said-what-recorders-on-device-solution-for-labeling-speakers/\">speaker embedding</a> approaches require a model to determine and register the unique voiceprint of each speaker.</p><p data-block-key=\"4qb0t\">In “<a href=\"https://dl.acm.org/doi/pdf/10.1145/3706598.3713631\" target=\"_blank\" rel=\"noopener noreferrer\">SpeechCompass: Enhancing Mobile Captioning with Diarization and Directional Guidance via Multi-Microphone Localization</a>”, recipient of a <a href=\"https://programs.sigchi.org/chi/2025/program/content/189502\" target=\"_blank\" rel=\"noopener noreferrer\">Best Paper Award</a> at <a href=\"https://programs.sigchi.org/chi/2025\" target=\"_blank\" rel=\"noopener noreferrer\">CHI 2025</a>, we explore an approach that enhances mobile captioning with <a href=\"https://en.wikipedia.org/wiki/Speaker_diarisation\" target=\"_blank\" rel=\"noopener noreferrer\">speaker diarization</a> (separating speakers in an ASR transcript) and real-time localization of incoming sound. SpeechCompass creates user-friendly transcripts for group conversations by providing color-coded visual separation for each speaker and directional indicators (arrows) to help users determine the direction from which speech is coming. This multi-microphone approach lowers computational costs, reduces latency, and enhances privacy preservation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-2.width-1250.jpg\" alt=\"SpeechCompass-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-2.width-1250.jpg\" alt=\"SpeechCompass-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"zcbwu\"><b><i>Left</i></b><i>: Existing mobile transcription apps concatenate transcribed text together.</i> <b><i>Right</i></b><i>: SpeechCompass indicates the direction of incoming speech, allowing visually separated transcripts with colors and directional indicators (such as arrows) in the user interface.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Efficient real-time audio localization</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rtnsv\">We implement SpeechCompass in two different forms: as a phone case prototype with four microphones connected to a low-power microcontroller and as software for existing phones with two microphones. The phone case design provides optimal microphone placement to enable 360-degree sound localization. The software implementation offers only 180-degree localization on devices with two or more microphones, such as the Pixel phone. In both implementations, the phone is used for speech recognition and transcripts are visualized using a mobile application.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-5.width-1250.png\" alt=\"SpeechCompass-5\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-5.width-1250.png\" alt=\"SpeechCompass-5\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vzr66\"><i>Implementation of the prototype phone-case and its internal electronics.</i> <b><i>A</i></b><i>) Mobile application interface with a mounted prototype case.</i> <b><i>B</i></b><i>) Prototype case with a flexible</i> <a href=\"https://en.wikipedia.org/wiki/Printed_circuit_board\" target=\"_blank\" rel=\"noopener noreferrer\"><i>PCB</i></a><i> microphone mount and a main PCB.</i> <b><i>C</i></b><i>) Top and bottom view of the main PCB (</i><a href=\"https://en.wikipedia.org/wiki/STM32\" target=\"_blank\" rel=\"noopener noreferrer\"><i>STM32</i></a><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"rtnsv\">Because sound has low frequency, it bounces around indoor environments causing reverberations and making audio, especially speech, difficult to localize precisely. To address this challenge, we apply a localization algorithm based on <a href=\"https://en.wikipedia.org/wiki/Time_of_arrival\" target=\"_blank\" rel=\"noopener noreferrer\">time-difference of arrival</a> (TDOA). Audio signals arrive at each microphone at slightly different times, so the algorithm estimates the TDOA between microphone pairs with <a href=\"https://en.wikipedia.org/wiki/Cross-correlation\" target=\"_blank\" rel=\"noopener noreferrer\">cross-correlation</a> to predict the <a href=\"https://en.wikipedia.org/wiki/Angle_of_arrival\" target=\"_blank\" rel=\"noopener noreferrer\">angle of arrival</a> for the sound. Specifically, we use <a href=\"https://xavieranguera.com/phdthesis/node92.html\" target=\"_blank\" rel=\"noopener noreferrer\">Generalized Cross Correlation with Phase Transform</a> (GCC-PHAT) to improve noise robustness and increase compute speed. We then apply statistical estimations, such as <a href=\"https://en.wikipedia.org/wiki/Kernel_density_estimation\" target=\"_blank\" rel=\"noopener noreferrer\">kernel density estimation</a>, to improve localizer precision. The use of two omnidirectional microphones will always have “front–back” confusion (i.e., when the signals in front or in the back of the array appear identical to the <a href=\"https://en.wikipedia.org/wiki/Microphone_array\" target=\"_blank\" rel=\"noopener noreferrer\">microphone array</a>), thus allowing only 180 degree localization. This issue is solved by using three or more microphones, making 360 degree localization possible.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-1.width-1250.png\" alt=\"SpeechCompass-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-1.width-1250.png\" alt=\"SpeechCompass-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vzr66\"><i>SpeechCompass system diagram, including the phone case hardware and the phone application.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"rtnsv\">Unlike ML approaches to single-source speaker diarization, the SpeechCompass multi-microphone approach offers several advantages:</p><ul><li data-block-key=\"ag2i2\"><i>Lower computational and memory costs:</i> Since there is no model nor weights, the algorithm can run on small microcontrollers with limited memory and compute.</li><li data-block-key=\"17dpt\"><i>Reduced latency:</i> SpeechCompass does not rely on capturing distinguishing voice characteristics. Instead, it extracts directional information from basic sound properties, allowing it to operate in real-time with minimal lag.</li><li data-block-key=\"7p4fj\"><i>Greater privacy preservation:</i> SpeechCompass assumes that different speakers are physically in separate places and does not require video or any unique personally identifying information, like speaker embeddings (unique identity of individual’s voice).</li><li data-block-key=\"adsip\"><i>Language-agnostic operation:</i> SpeechCompass looks at differences between the audio waveforms, without prior assumptions about the content and works for sounds beyond speech.</li><li data-block-key=\"b3kk5\"><i>Instant reconfiguration</i>: SpeechCompass can be reconfigured instantly by moving the phone.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">User interface for visualizing speaker direction</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rtnsv\">We used Android’s speech-to-text <a href=\"https://developer.android.com/reference/android/speech/SpeechRecognizer\" target=\"_blank\" rel=\"noopener noreferrer\">capabilities</a> to develop a mobile application that augments speech transcripts with localization data sent from the prototype phone case’s microphones via <a href=\"https://en.wikipedia.org/wiki/USB#:~:text=Universal%20Serial%20Bus%20(USB)%20is,between%20many%20types%20of%20electronics.\" target=\"_blank\" rel=\"noopener noreferrer\">USB</a>. The Android application provides multiple visualization styles to indicate speaker direction:</p><ol><li data-block-key=\"711vq\"><i>Colored text</i>: Speakers are separated using different colored text.</li><li data-block-key=\"au4pl\"><i>Directional glyphs</i>: Arrows, dials in a circle and color highlights on the boxes around the text point to the location of each speaker.</li><li data-block-key=\"1fuub\"><i>Minimap</i>: A small radar-like display shows the current speaker's position.</li><li data-block-key=\"40qtr\"><i>Edge indicators</i>: Visual cues around the screen edges highlight speaker direction.</li><li data-block-key=\"1ji9e\"><i>Unwanted speech suppression</i>: The user can click on the sides of the screen to suppress speech coming from those directions. This can be used to remove their own speech. Irrelevant nearby conversation can be removed from the transcript, which enhances the privacy of nearby speakers.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-3.width-1250.png\" alt=\"SpeechCompass-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-3.width-1250.png\" alt=\"SpeechCompass-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vzr66\"><i>Various visualization styles augment speech transcripts.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Technical evaluation</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rtnsv\">To evaluate the SpeechCompass software, we placed a phone case on a rotating platform with a stationary speaker playing speech or noise. The platform was rotated at 10 degree increments and the angle of arrival was measured for each angle. Our evaluation shows that SpeechCompass can accurately localize sound direction with an average error of 11°–22° for normal conversational loudness (60–65 <a href=\"https://en.wikipedia.org/wiki/Sound_pressure#Sound_pressure_level\" target=\"_blank\" rel=\"noopener noreferrer\">dB</a>). The accuracy is roughly comparable to human localization abilities. For example, if a person were asked where sound was heard behind them, their answer would typically have up to 20 degrees error. The SpeechCompass system performs well across different materials and under varying ambient noise conditions, see the <a href=\"https://dl.acm.org/doi/pdf/10.1145/3706598.3713631\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a> for more details.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-6.width-1250.png\" alt=\"SpeechCompass-6\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-6.width-1250.png\" alt=\"SpeechCompass-6\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vzr66\"><i>Error in localization at different audio levels and source angles.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"rtnsv\">For diarization, we used <a href=\"https://pyannote.github.io/pyannote-metrics/reference.html#diarization\" target=\"_blank\" rel=\"noopener noreferrer\">diarization error rate</a> (DER), a standard metric for diarization that corresponds to correctness of color coded speaker diarization in the interface. Our tests showed the four-microphone configuration consistently outperformed the three-microphone setup, with relative DER improvements of 23%–35% across different <a href=\"https://en.wikipedia.org/wiki/Signal-to-noise_ratio\" target=\"_blank\" rel=\"noopener noreferrer\">signal-to-noise</a> (SNR) conditions.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-7.width-1250.jpg\" alt=\"SpeechCompass-7\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-7.width-1250.jpg\" alt=\"SpeechCompass-7\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vzr66\"><i>Diarization error rate (DER) with 3- and 4-microphone configurations at different signal-to-noise ratios.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">User evaluation and feedback</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rtnsv\">To understand the limitations of current mobile captioning technology, we conducted an online survey with 263 frequent users of captioning technology. The results show that current solutions struggle with a significant limitation — the inability to distinguish between speakers makes them challenging to use in group conversations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-8.width-1250.jpg\" alt=\"SpeechCompass-8\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-8.width-1250.jpg\" alt=\"SpeechCompass-8\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vzr66\"><i>The results of the survey with frequent users of mobile captioning.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"rtnsv\">Second, we demonstrated the prototype to eight frequent users of mobile speech-to-text and gathered feedback. The prototype was used to diarize and visualize a conversation between the researchers. We found that colored text and directional arrows were the most preferred visualization methods. All participants agreed on the value of directional guidance for group conversations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-4.width-1250.png\" alt=\"SpeechCompass-4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-4.width-1250.png\" alt=\"SpeechCompass-4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vzr66\"><i>Results of a user study with the working prototype.</i> <b><i>A</i></b><i>) Preferences for the different visualization techniques.</i> <b><i>B</i></b><i>) Value of the directional feedback to the users.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">What's next?</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rtnsv\">We imagine that multi-microphone localization for mobile transcription could have numerous practical applications. One example could be in the classroom setting<b>,</b> where students could more easily follow discussions between instructors and classmates. Similarly in business meetings, interviews or social gatherings, users could track speaker changes in multi-person conversations.</p><p data-block-key=\"65sir\">SpeechCompass demonstrates significant improvements for mobile captioning in group conversations, and there are numerous possible directions for additional development:</p><ul><li data-block-key=\"5m8pb\">Integration with additional wearable form factors like smart glasses and smartwatches</li><li data-block-key=\"25pn2\">Enhanced noise robustness through machine learning approaches</li><li data-block-key=\"e031l\">Further customization of visualization preferences</li><li data-block-key=\"ebktc\">Longitudinal studies to understand adoption and behavior in everyday scenarios</li></ul><p data-block-key=\"31ksu\">We hope that this research inspires continued innovation in making communication more accessible and inclusive for everyone.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rtnsv\"><i>We thank Artem Dementyev, Alex Olwal, Mathieu Parvaix, Chiong Lai and Dimitri Kanevsky for their work on the SpeechCompass publication and research. Dmitrii Votintcev for ideas on prototypes and interaction designs. We are grateful to Pascal Getreuer, Richard Lyon, Alex Huang, Shao-Fu Shih, and Chet Gnegy for their help with algorithms. We also thank Shaun Kane, James Landay, Malcolm Slaney, and Meredith Morris for their feedback on this paper. We appreciate the contributions of Carson Lau for the phone case mechanical design and Ngan Nguyen for electronics assembly. Finally, we thank Mei Lu, Don Barnett, Ryan Geraghty, and Sanjay Batra for UX research and design.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    }
  ],
  "lastUpdated": "2025-08-06T10:35:23.208Z"
}