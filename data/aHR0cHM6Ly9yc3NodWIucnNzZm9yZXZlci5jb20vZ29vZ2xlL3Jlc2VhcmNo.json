{
  "sourceUrl": "https://rsshub.rssforever.com/google/research",
  "title": "Google Research Blog",
  "description": "Google Research Blog - Powered by RSSHub",
  "link": "https://research.google/blog",
  "items": [
    {
      "title": "优化基于LLM的旅行规划 (原标题: Optimizing LLM-based trip planning)",
      "link": "https://research.google/blog/optimizing-llm-based-trip-planning/",
      "pubDate": "Thu, 05 Jun 2025 16:00:00 GMT",
      "isoDate": "2025-06-05T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 优化基于LLM的旅行规划\n\n## 引言\n\n现实世界的规划任务通常涉及“硬性”的量化约束（如预算、日程安排、开放时间）和“软性”的定性目标（如自然语言表达的用户偏好）。大型语言模型（LLM）因其庞大的训练数据集而擅长处理旅行规划中非量化部分，例如判断餐厅是否适合儿童或观赏风景的最佳时间。然而，LLM在处理需要详细和最新现实世界信息（如公交票价、火车时刻表）或复杂交互要求的量化物流约束方面可靠性较低，这可能导致生成不切实际的计划，例如建议参观一个已关闭的博物馆。\n\n## 解决方案：搜索中的AI旅行建议\n\n为了克服LLM在处理量化约束方面的不足，我们最近在“搜索”中推出了“AI旅行建议”功能，旨在提供实用且可行的逐日行程。我们的解决方案采用了一种混合系统：它首先利用LLM建议一个初始计划，然后结合一个优化算法，该算法在确保与LLM计划相似性的同时，也考虑旅行时间、开放时间等现实世界因素。这种方法有效地将LLM处理软性需求的能力与满足硬性物流约束所需的算法精度相结合。\n\n## 工作原理\n\n### 1. LLM初始计划生成\n\n系统首先将用户查询传递给LLM（我们使用的是最新Gemini模型的一个版本）。LLM会根据用户兴趣生成一个初始旅行计划，其中包含活动列表、建议持续时间及重要性等详细信息。尽管此初始计划能很好地满足用户兴趣，但可能存在可行性问题，例如建议访问一个已关闭的场所。\n\n![混合LLM和优化系统示意图](https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs2_Diagram.width-1250.png)\n\n*图：我们的混合LLM和优化系统示意图。LLM建议初始计划，然后我们进行优化，结合现实世界约束，生成最终行程。*\n\n### 2. 可行性增强与优化\n\n为了解决初始计划的可行性问题，我们在LLM之上添加了几个组件：\n\n*   **数据校准：** 我们首先根据最新的开放时间和旅行时间对初始行程进行校准。\n*   **替代活动检索：** 同时，我们还使用搜索后端检索额外的相关活动，作为LLM建议计划可能需要修改时的潜在替代方案。\n*   **优化输入：** 初始计划、替代活动和校准数据随后被输入到优化算法中，以找到一个与初始计划相似且确保可行性的旅行计划。\n\n## 优化算法\n\n该算法主要分为两个阶段：\n\n### 阶段1：单日优化\n\n*   对于旅行中单日内的每个活动子集（最多合理的最大尺寸），算法会确定这些活动在一天内的最佳调度。\n*   然后，根据与原始计划的相似性以及受开放时间和旅行时间约束的调度可行性来分配一个质量得分（完全不可行的调度将获得零分）。\n*   由于一天内的活动数量较少，通过优化的动态规划实现，可以通过穷举搜索计算得分。\n\n### 阶段2：整体行程优化（多日）\n\n*   在第二阶段，算法寻找一个整体行程（即每天的活动集合），在满足两天活动不重叠的约束下，最大化每天的总得分。\n*   这是一个加权集合打包问题，虽然是NP完全问题，但鉴于优化目标旨在保持与初始行程的接近，局部搜索启发式方法被证明是有效的。\n*   从初始行程开始，算法通过在两天之间交换活动来进行局部调整，只要这能增加总得分。此过程重复进行，直到收敛，从而产生最终行程。\n\n![行程优化示意图](https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs3_GIF.width-800.gif)\n\n*图：通过局部调整优化行程的示意图。初始行程包括在科学博物馆关闭后参观。算法将其重新安排到第一天，并移至更早的时段。这使得有时间将第三天的上午徒步旅行重新安排到第二天，第二天有地理位置更近的活动，从而形成最终行程。*\n\n## 案例演示\n\n### 案例1：处理定性偏好（纽约博物馆）\n\n以下查询说明了LLM提供的详细理解如何比传统检索系统更好地满足用户需求：\n\n**查询：** “为我规划一个纽约周末之旅，参观许多鲜为人知的博物馆，并避开大量人群。”\n\n*   **混合系统：** 我们的系统生成了一个符合用户请求的行程，建议了电影与电视博物馆和纽约交通博物馆等鲜为人知的博物馆。\n*   **仅依赖搜索：** 如果移除LLM建议，仅依赖搜索检索到的活动，生成的行程会包含一些鲜为人知的博物馆，但也包括大都会艺术博物馆和古根海姆博物馆，这些著名博物馆直接与用户的请求相矛盾。\n\n![混合系统与仅搜索系统生成的行程对比](https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs4_ItineraryFinal.width-1250.png)\n\n*图：混合系统（左）生成的行程完全包含鲜为人知的博物馆，符合用户请求。仅依赖搜索检索到的活动，一些著名博物馆会进入旅行计划（右）。*\n\n### 案例2：纠正量化约束问题（旧金山旅行）\n\n以下查询演示了混合系统纠正原始LLM建议行程中物流问题的情况：\n\n**查询：** “为我规划一次旧金山之旅。我想参观艺术博物馆，并去一些可以俯瞰城市全景的地方。”\n\n*   **LLM初始计划：** 在测试中，LLM建议了不错的景点（如笛洋美术馆和科伊特塔），但其中一天的活动安排不自然，要求用户跨城市长途旅行。\n*   **优化纠正：** 优化步骤成功纠正了此问题，生成了一个物流上可行的计划，同时保留了原始意图。\n\n![LLM建议与优化后行程的地图对比](https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs5_Maps.width-1250.png)\n\n*图：LLM建议的行程（左）在其中一天包含跨城市的长时间旅行。应用优化算法后，我们获得了更自然的活动分组（右）。*\n\n## 未来展望\n\n除了旅行规划，LLM还可以应用于组织活动或安排差事等其他日常任务。对于这些应用，开发能够让LLM可靠地应对现实世界约束的系统至关重要。我们的旅行规划工作是解决这一挑战的更大持续努力的一部分。\n\n## 致谢\n\n本博客文章基于与Wei Chen、Lucas Guzman、Shirley Loayza Sanchez和Weiye Yao合作完成的工作。我们还要感谢Sreenivas Gollapudi、Kostas Kollias和Gokul Varadhan提供的有益指导。",
      "shortSummary": "文章介绍了优化基于LLM的旅行规划的混合系统。该系统结合了LLM处理用户偏好等定性需求的能力，以及优化算法处理开放时间、旅行时间等量化约束的精度。LLM生成初始计划，随后通过算法进行校准和优化，以确保行程的实用性和可行性。案例表明，该方法能更好地满足用户特定偏好并纠正物流上的不合理安排，为现实世界规划任务提供了更可靠的解决方案。",
      "translated_title": "优化基于LLM的旅行规划",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs2_Diagram.width-1250.png",
          "alt": "AlgoLLMs2_Diagram",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs3_GIF.width-800.gif",
          "alt": "AlgoLLMs3_GIF",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs4_ItineraryFinal.width-1250.png",
          "alt": "AlgoLLMs4_ItineraryFinal",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs5_Maps.width-1250.png",
          "alt": "AlgoLLMs5_Maps",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fhg4u\">Many real-world planning tasks involve both harder “quantitative” constraints (e.g., budgets or scheduling requirements) and softer “qualitative” objectives (e.g., user preferences expressed in natural language). Consider someone planning a week-long vacation. Typically, this planning would be subject to various clearly quantifiable constraints, such as budget, travel logistics, and visiting attractions only when they are open, in addition to a number of constraints based on personal interests and preferences that aren’t easily quantifiable.</p><p data-block-key=\"18c09\">Large language models (LLMs) are trained on massive datasets and have internalized an impressive amount of world knowledge, often including an understanding of typical human preferences. As such, they are generally good at taking into account the not-so-quantifiable parts of trip planning, such as the ideal time to visit a scenic view or whether a restaurant is kid-friendly. However, they are less reliable at handling quantitative logistical constraints, which may require detailed and up-to-date real-world information (e.g., bus fares, train schedules, etc.) or complex interacting requirements (e.g., minimizing travel across multiple days). As a result, LLM-generated plans can at times include impractical elements, such as visiting a museum that would be closed by the time you can travel there.</p><p data-block-key=\"f91cn\">We recently introduced <a href=\"https://blog.google/products/search/summer-travel-tips-ai-overviews-hotel-price-tracking/\" target=\"_blank\" rel=\"noopener noreferrer\">AI trip ideas in Search</a>, a feature that suggests day-by-day itineraries in response to trip-planning queries. In this blog, we describe some of the work that went into overcoming one of the key challenges in launching this feature: ensuring the produced itineraries are practical and feasible. Our solution employs a hybrid system that uses an LLM to suggest an initial plan combined with an algorithm that jointly optimizes for similarity to the LLM plan and real-world factors, such as travel time and opening hours. This approach integrates the LLM’s ability to handle soft requirements with the algorithmic precision needed to meet hard logistical constraints.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"fhg4u\">How it works</h2><p data-block-key=\"1d4pi\">Given an incoming user query, we first pass the query to an LLM, which for our system is a version of our latest <a href=\"https://deepmind.google/models/gemini/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini models</a>. The LLM suggests an initial trip plan consisting of a list of activities along with relevant details, such as the suggested duration and level of importance to the user query. The initial plan is well-tailored to the user’s interests but may have feasibility issues, like suggesting an establishment that has recently closed.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs2_Diagram.width-1250.png\" alt=\"AlgoLLMs2_Diagram\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs2_Diagram.width-1250.png\" alt=\"AlgoLLMs2_Diagram\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"gqk8g\"><i>Diagram of our hybrid LLM and optimization system. The LLM suggests the initial plan, and we then perform optimization incorporating real-world constraints to produce the final itinerary.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fhg4u\">To address the feasibility issues, we add a few components on top of the LLM. We start by grounding the initial itinerary with up-to-date opening hours and travel times. In parallel, we also use search backends to retrieve additional relevant activities that serve as potential substitutes in case the LLM-suggested plan needs to be modified. The initial plan, substitute activities, and grounding data are then fed into an optimization algorithm to find a trip plan similar to the initial plan that also ensures feasibility.</p><p data-block-key=\"3b4fm\">There are two main stages to the algorithm. The first stage operates on the level of a single day within the trip. For each subset of activities (up to a reasonable maximum size), we determine an optimal scheduling of those activities in a day. We then assign it a quality score determined primarily based on similarity to the original plan and feasibility of the scheduling subject to opening hour and travel time constraints (e.g., a completely infeasible schedule would receive a score of zero). Since the number of activities within a day is small, we found that the scores can be computed by exhaustive search with a sufficiently optimized dynamic programming-based implementation.</p><p data-block-key=\"8tafc\">In the second stage, we search for an overall itinerary (i.e., sets of activities for each day) that maximizes the total score of the days, subject to the constraint that no two days' activities can overlap. This is a weighted variant of the <a href=\"https://en.wikipedia.org/wiki/Set_packing\" target=\"_blank\" rel=\"noopener noreferrer\">set packing problem</a>, which is well-known to be <a href=\"https://en.wikipedia.org/wiki/NP-completeness\" target=\"_blank\" rel=\"noopener noreferrer\">NP-complete</a> and thus computationally intractable. However, given that our optimization objective tries to stay close to the initial itinerary by design, we found that local search heuristics were effective. Starting from the initial itinerary, we make local adjustments by exchanging activities between pairs of days so long as this would increase the total score. This procedure is repeated until convergence, resulting in the final itinerary.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs3_GIF.width-800.gif\" alt=\"AlgoLLMs3_GIF\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs3_GIF.width-800.gif\" alt=\"AlgoLLMs3_GIF\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"gqk8g\"><i>Illustration of optimizing an itinerary with local adjustments. The initial itinerary includes a visit to the science museum after it closes. The algorithm reschedules it to day 1, where it gets moved to an earlier slot. This leaves time to reschedule the morning hike from day 3 to day 2, which has geographically closer activities, resulting in the final itinerary.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"fhg4u\">Examples</h2><p data-block-key=\"fnll7\">The following query illustrates how the detailed understanding LLMs provide can better serve the user’s needs than a traditional retrieval system.</p><p data-block-key=\"c83v5\"><i>Query: “Plan me a weekend trip to NYC visiting lots of lesser known museums and avoiding large crowds.”</i></p><p data-block-key=\"9pab9\">Our system produces an itinerary matching the user’s request, suggesting museums such as the Museum of the Moving Image and the New York Transit Museum. If we remove the LLM suggestion and rely only on the search-retrieved activities, the resulting itinerary includes some lesser known museums but also the Metropolitan Museum of Art and the Guggenheim Museum, famous museums directly contradicting the user’s request.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs4_ItineraryFinal.width-1250.png\" alt=\"AlgoLLMs4_ItineraryFinal\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs4_ItineraryFinal.width-1250.png\" alt=\"AlgoLLMs4_ItineraryFinal\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"gqk8g\"><i>The itinerary produced by the hybrid system (</i><b><i>left</i></b><i>) includes exclusively lesser known museums, per the user’s request. Relying solely on Search-retrieved activities, some of the famous museums make it into the trip plan (</i><b><i>right</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fhg4u\">On the other hand, the next query demonstrates a case where the hybrid system corrects an issue with the original LLM-suggested itinerary:</p><p data-block-key=\"50f3l\"><i>Query: “Plan me a trip to San Francisco. I want to visit art museums and go somewhere with panoramic views of the city.”</i></p><p data-block-key=\"7i4ja\">During our testing, the LLM suggested a number of good attractions, such as the de Young museum (which also includes an observation tower) and the iconic Coit Tower. However, the itinerary scheduled the activities for one of the days in an unnatural way, requiring the user to travel across the city. We were able to correct this in the optimization step, resulting in a logistically feasible plan that preserves the original intent.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs5_Maps.width-1250.png\" alt=\"AlgoLLMs5_Maps\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs5_Maps.width-1250.png\" alt=\"AlgoLLMs5_Maps\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"gqk8g\"><i>The LLM-suggested itinerary (</i><b><i>left</i></b><i>) includes a long travel across the city on one of the days. After applying an optimization algorithm, we obtain a more natural grouping of activities (</i><b><i>right</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"fhg4u\">Future work</h2><p data-block-key=\"14u23\">Beyond trip planning, LLMs can be applied to many other everyday tasks, such as organizing an event or scheduling errands. For these types of applications, it is crucial to develop systems that allow LLMs to reliably navigate the constraints of the real world. Our trip planning work is part of a larger ongoing effort to address this challenge — stay tuned for further updates in this space.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"fhg4u\">Acknowledgements</h2><p data-block-key=\"2ug7e\"><i>This blog post is based on work done in collaboration with Wei Chen, Lucas Guzman, Shirley Loayza Sanchez, and Weiye Yao. We also thank Sreenivas Gollapudi, Kostas Kollias, and Gokul Varadhan for helpful guidance.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "放大：利用生成式人工智能进行高效区域环境风险评估 (原标题: Zooming in: Efficient regional environmental risk assessment with generative AI)",
      "link": "https://research.google/blog/zooming-in-efficient-regional-environmental-risk-assessment-with-generative-ai/",
      "pubDate": "Wed, 04 Jun 2025 16:00:00 GMT",
      "isoDate": "2025-06-04T16:00:00.000Z",
      "creator": "Google",
      "summary": "地球系统模型是预测和应对地球未来环境变化的最佳工具，但其高分辨率运行的巨大计算成本限制了其在精细尺度（如城市级别，约10公里）进行区域预测的能力。传统的地球系统模型通常只能达到夏威夷岛屿大小（约100公里）的分辨率。\n\n为了弥补这一分辨率差距，研究人员提出了一种新颖的生成式人工智能方法，称为**动态-生成式降尺度（dynamical-generative downscaling）**，并已发表在《美国国家科学院院刊》上。该方法利用概率扩散模型（一种强大的生成式AI）将全球气候预测转化为局部（约10公里）的当前和未来环境风险评估，且成本远低于现有技术。\n\n### 传统降尺度方法的局限性\n\n*   **动态降尺度：** 通过区域气候模型（RCMs）对全球模型数据进行更精细的模拟。它能提供最物理真实的局部预测，但计算成本极高，不适合处理大量气候预测数据。\n*   **统计降尺度：** 速度更快，但难以准确捕捉复杂的局部天气模式（尤其是极端事件），且难以可靠地推广到未训练的未来条件。\n\n### 动态-生成式降尺度方法\n\n该方法结合了动态降尺度的物理真实性和人工智能的速度与模式识别能力，分两步进行：\n\n1.  **基于物理的第一步：** 区域气候模型将全球地球系统数据降尺度到中间分辨率（例如50公里）。这一步计算成本较低，但关键在于它将不同全球模型的输出转换为统一的网格和物理表示，为AI系统的高效学习奠定基础。\n2.  **AI添加精细细节：** 新开发的生成式AI系统，即**区域残差扩散降尺度模型（R2D2）**，接管任务。R2D2通过学习高分辨率天气数据的示例，学会向中间分辨率输出添加逼真的精细尺度细节（如复杂地形的影响），从而高效地将其提升到目标高分辨率（通常小于10公里）。该模型专注于学习中间分辨率和高分辨率场之间的“残差”差异，这使得学习任务更容易，并提高了对未见环境条件的泛化能力。\n\n![DynGenDown-1-Cartoon](https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-1-Cartoon.width-1250.png)\n\n*降尺度方法提高了地球有限区域气候预测的分辨率。这使我们能够理解地形和区域过程对极端天气的影响。* \n\n![DynGenDown-2-Example](https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-2-Example.width-1250.png)\n\n*动态-生成式降尺度通过两步过程从地球系统模型中提取区域气候信息。区域气候模型（RCM）提供基于物理的第一步，生成式AI添加更精细的细节。*\n\n### 评估与结果\n\n研究团队使用“美国西部动态降尺度数据集（WUS-D3）”对模型进行了训练和评估。结果显示：\n\n*   **更高的准确性：** 相比统计方法，动态-生成式降尺度将温度、降水、相对湿度和风速等各种天气变量的精细尺度误差降低了40%以上。它还能有效纠正粗分辨率气候模拟中的系统偏差。\n*   **逼真的天气模式：** AI生成的结果捕捉了逼真的空间模式、不同天气变量之间的相关性（如风速和风向，或热量和湿度），以及复合极端事件（如同时发生的热浪和干旱）的可能性。\n*   **更好的不确定性估计：** 通过降尺度大量气候预测集合，该框架提供了比统计方法或仅动态降尺度少量地球系统模型更全面的未来环境条件范围图景。\n*   **捕捉区域极端事件：** 该方法在捕捉由区域现象引起的复杂环境风险方面表现出卓越的技能，例如南加州圣安娜风引起的野火风险。\n\n![DynGenDown-3a-Performance](https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-3a-Performance.width-1250.jpg)\n\n*我们的模型（R2D2）与统计降尺度基线（BCSD、STAR-ESDM）在风速（左）、相对湿度（中）和温度（右）集合预测中的精细尺度预测误差（a-c）和能量谱（d-f）比较。新方法显示出更低的误差（CRPS）和更逼真的天气模式，通过R2D2和目标能量谱的对齐来衡量。底部面板比较了夏季炎热干燥极端事件（g-i）和秋季风极端事件（j-l）的协变。*\n\n![DynGenDown-4a-Wildfire](https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-4a-Wildfire.width-1250.jpg)\n\n*在21世纪末气候预测的环境条件下，南加州强圣安娜风事件期间的野火风险模型预测。野火风险通过圣安娜野火威胁指数的天气分量来衡量。动态-生成式方法（a）与更昂贵的动态降尺度方法（b）相比，能很好地捕捉野火风险。统计降尺度方法如STAR-ESDM（c）和BCSD（d）通常难以捕捉此类复合风险的幅度和空间结构。*\n\n### 突破的意义\n\n动态-生成式降尺度是获得低于10公里可操作尺度的全面未来区域气候预测的重大一步。它使得对大型地球系统模型集合进行降尺度在计算上变得可行——研究估计，对于测试的8模型集合，计算成本节省了85%，对于更大的集合，这一数字还会增加。这种快速高效的AI推理步骤类似于谷歌的SEEDS和GenCast天气预报模型的运行方式，能够对区域环境风险进行全面评估。\n\n通过以一小部分计算成本提供更准确、更具概率完整性的区域气候预测，动态-生成式降尺度可以显著改善环境风险评估。这有助于在农业、水资源管理、能源基础设施和自然灾害防备等关键领域做出更明智的适应和弹性政策决策。",
      "shortSummary": "一项新颖的生成式AI方法——“动态-生成式降尺度”——解决了地球系统模型在精细尺度进行区域环境风险预测的计算成本高昂问题。该方法结合了物理模型和AI（R2D2），首先通过区域气候模型进行粗略降尺度，再由AI添加精细细节。与传统方法相比，它能以85%的成本节省实现更高的预测准确性、更逼真的天气模式、更好的不确定性估计，并有效捕捉区域极端事件（如野火风险）。这项突破将显著改善环境风险评估，支持农业、水资源管理和灾害防备等领域的决策。",
      "translated_title": "放大：利用生成式人工智能进行高效区域环境风险评估",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-1-Cartoon.width-1250.png",
          "alt": "DynGenDown-1-Cartoon",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-2-Example.width-1250.png",
          "alt": "DynGenDown-2-Example",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-3a-Performance.width-1250.jpg",
          "alt": "DynGenDown-3a-Performance",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-4a-Wildfire.width-1250.jpg",
          "alt": "DynGenDown-4a-Wildfire",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7espt\"><a href=\"https://www.nobelprize.org/prizes/physics/2021/popular-information/\" target=\"_blank\" rel=\"noopener noreferrer\">Earth system models</a> represent our best tool to predict and prepare for future changes to Earth’s environment. However, the immense computational cost of running these models at high resolution limits their ability to make regional projections at fine scales. Indeed, a typical limiting scale for these models is comparable in size to the island of Hawai’i (~100 km). Obtaining more granular projections, for instance at the city level (~10 km), is critical for planning everything from farming strategies and water management to protecting communities from floods, heatwaves, and wildfires.</p><p data-block-key=\"b8dk0\">To address this need, we are excited to announce a novel generative AI method that bridges the resolution gap between Earth system models and downstream users’ needs. <a href=\"https://doi.org/10.1073/pnas.2420288122\" target=\"_blank\" rel=\"noopener noreferrer\">Published</a> in the <a href=\"https://www.pnas.org/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Proceedings of the National Academy of Sciences</i></a>, we present a dynamical-generative downscaling method that applies probabilistic diffusion models — a powerful class of generative AI capable of learning complex data distributions — to the output of well-established physics-based models to translate global climate projections into local (~10 km) assessments of present and future environmental risk. Dynamical-generative downscaling produces detailed local environmental risk assessments at a small fraction of the cost of existing state-of-the-art techniques, which are too computationally expensive to apply to the wealth of climate projection data that is <a href=\"https://wcrp-cmip.org/\" target=\"_blank\" rel=\"noopener noreferrer\">now available</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">From global projections to regional environmental risk</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7espt\">To capture local changes in environmental conditions at resolutions of 10 km or higher, scientists typically use a technique called \"dynamical downscaling\". This involves taking coarse information from global Earth system models and running much finer-grained simulations with regional climate models (RCMs) over a specific area. Think of it like using a magnifying glass on a global map.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-1-Cartoon.width-1250.png\" alt=\"DynGenDown-1-Cartoon\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-1-Cartoon.width-1250.png\" alt=\"DynGenDown-1-Cartoon\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"n3zhp\"><i>Downscaling methods increase the resolution of climate projections over limited-area regions of the Earth. This enables us to understand the effect of topography and regional processes on weather extremes. Rendering generated by</i> <a href=\"https://deepmind.google/technologies/imagen-3/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Imagen 3</i></a><i>.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7espt\">While dynamical downscaling provides the most physically realistic local projections, it has a major drawback: it is computationally expensive. Running these detailed simulations takes substantial computing power, making it impractical to downscale the many different climate projections needed to fully capture the <a href=\"https://www.ipcc.ch/report/ar6/wg1/chapter/chapter-4/\" target=\"_blank\" rel=\"noopener noreferrer\">range</a> of possible future environmental conditions. Faster statistical downscaling methods exist, but they often struggle to accurately capture complex local weather patterns (especially extreme events) or to generalize reliably to future conditions for which they were not trained.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">A more efficient approach: Physics meets generative AI</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7espt\">Instead, we propose to combine the physical realism of dynamical downscaling with the speed and pattern-recognition power of artificial intelligence. The dynamical-generative downscaling process works in two steps:</p><ol><li data-block-key=\"d3jm0\"><i>Physics-based first pass:</i> First, a regional climate model downscales global Earth system data, but only to an intermediate, and still coarse, resolution (e.g., 50 km). This step is much cheaper computationally than going straight to very high resolution, but crucially, it translates the varied outputs of different global models into a common grid and physical representation of the Earth. This process sets the stage for efficient learning by AI systems.<br><br></li><li data-block-key=\"587l3\"><i>AI adds the fine details:</i> Next, a newly developed generative AI system, the Regional Residual Diffusion-based Downscaling model (“R2D2”), takes over. Trained on examples of high-resolution weather data, R2D2 learns to add realistic, fine-scale details (like the effects of complex terrain) to the intermediate-resolution output, efficiently bringing it up to the target high resolution (typically less than 10 km). The model focuses on learning the difference, or \"residual\", between the intermediate and high-resolution fields, which makes the learning task easier and improves generalization to unseen environmental conditions.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-2-Example.width-1250.png\" alt=\"DynGenDown-2-Example\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-2-Example.width-1250.png\" alt=\"DynGenDown-2-Example\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"n3zhp\"><i>Dynamical-generative downscaling extracts regional climate information from Earth system models in a two-step process. A regional climate model (RCM) provides a physics-based first pass, and generative AI adds the finer details.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7espt\">This hybrid approach leverages the strengths of both methods: the RCM provides a physically grounded base and handles the diversity of global models, while the AI excels at efficiently generating the high-resolution details and capturing the full range of regional environmental conditions. Importantly, the R2D2 model only needs training data from one dynamically downscaled Earth system model to learn how to effectively downscale outputs originating from different Earth system models. This enables our model to amortize the training cost when applied to large ensembles of climate projections.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Efficient and reliable regional climate projections</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7espt\">We trained and evaluated our model using the Western United States Dynamically Downscaled Dataset (<a href=\"https://doi.org/10.5194/gmd-17-2265-2024\" target=\"_blank\" rel=\"noopener noreferrer\">WUS-D3</a>). WUS-D3 contains an ensemble of regional climate projections over the Western United States, downscaled to 9 km resolution using the \"gold standard\", but expensive dynamical downscaling <a href=\"https://www.mmm.ucar.edu/models/wrf\" target=\"_blank\" rel=\"noopener noreferrer\">WRF</a> model. We trained our model on a single WUS-D3 climate projection, and evaluated its skill by downscaling 7 additional climate projections from the WUS-D3 ensemble. We compared the results against the computationally expensive dynamical downscaling, our target, and two popular statistical downscaling methods: <a href=\"https://doi.org/10.1029/2001JD000659\" target=\"_blank\" rel=\"noopener noreferrer\">BCSD</a> and <a href=\"https://doi.org/10.1029/2023EF004107\" target=\"_blank\" rel=\"noopener noreferrer\">STAR-ESDM</a>. The results were compelling:</p><ul><li data-block-key=\"6jkrq\"><i>Higher accuracy:</i> Dynamical-generative downscaling reduces fine-scale errors by over 40% compared to statistical methods across various weather variables like temperature, precipitation, relative humidity, and wind speed, as measured by the <a href=\"https://doi.org/10.1175/1520-0434(2000)015%3C0559:DOTCRP%3E2.0.CO;2\" target=\"_blank\" rel=\"noopener noreferrer\">continuous ranked probability score</a> (CRPS). It also effectively corrects systematic biases in the coarse-resolution climate simulations, critical for accurate environmental risk assessment of coastal and mountainous regions.<br><br></li><li data-block-key=\"2j4p0\"><i>Realistic weather patterns:</i> The AI-generated results capture realistic spatial patterns, correlations between different weather variables (like wind speed and direction, or heat and humidity), and importantly, the likelihood of compound extreme events (like concurrent heat and drought). Identifying spatial patterns and correlations is important for downstream applications in hydrology, energy forecasting, or natural hazard risk assessment.<br><br></li><li data-block-key=\"6f3sm\"><i>Better uncertainty estimates:</i> By downscaling large ensembles of climate projections, our framework provides a more comprehensive picture of the range of potential future environmental conditions than could be achieved by either the statistical methods or by dynamically downscaling only a smaller subset of Earth system models. This greater reliability is crucial for robust risk assessment. For example, dynamical-generative downscaling reduces the error in projections of extreme summer heat and winter precipitation by over 20% and 10% compared to those baselines, respectively, as measured by the mean absolute error in the 99th climatological percentile.<br><br></li><li data-block-key=\"84tof\"><i>Capturing regional extremes:</i> The method shows remarkable skill in capturing complex environmental risks due to regional phenomena, e.g., wildfire risk due to Santa Ana winds in Southern California. Projecting wildfire risk requires the accurate detection of fine-scale correlations between temperature, humidity, and wind extremes. Statistical downscaling methods, such as BCSD and STAR-ESDM, struggle at capturing granular correlations between meteorological fields, underestimating the risk of concurrent hazards.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-3a-Performance.width-1250.jpg\" alt=\"DynGenDown-3a-Performance\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-3a-Performance.width-1250.jpg\" alt=\"DynGenDown-3a-Performance\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"n3zhp\"><i>A comparison of fine-scale prediction errors (</i><b><i>a</i></b><i>–</i><b><i>c</i></b><i>) and</i> <a href=\"https://en.wikipedia.org/wiki/Energy_cascade\" target=\"_blank\" rel=\"noopener noreferrer\"><i>energy spectra</i></a><i> (</i><b><i>d–f</i></b><i>) between our model (R2D2) and the statistical downscaling baselines (BCSD, STAR-ESDM) for ensemble projections of wind speed (</i><b><i>left</i></b><i>), relative humidity (</i><b><i>center</i></b><i>), and temperature (</i><b><i>right</i></b><i>). The new method shows lower error (CRPS) and more realistic weather patterns, as measured by the alignment of the R2D2 and target energy spectra. The bottom panels compare the covariation of hot and dry summer extremes (</i><b><i>g</i></b><i>–</i><b><i>i</i></b><i>) and wind extremes in the fall (</i><b><i>j–l</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-4a-Wildfire.width-1250.jpg\" alt=\"DynGenDown-4a-Wildfire\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-4a-Wildfire.width-1250.jpg\" alt=\"DynGenDown-4a-Wildfire\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"n3zhp\"><i>Model projections of wildfire risk during a strong Santa Ana wind event in Southern California under environmental conditions taken from an end-of-century climate projection. Wildfire risk is measured by the weather component of the</i> <a href=\"https://www.fs.usda.gov/science-technology/fire/forecasting/santa-ana-wildfire-threat-index\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Santa Ana Wildfire Threat Index</i></a><i>. The dynamical-generative approach (</i><b><i>a</i></b><i>) captures wildfire risk well compared to the more expensive dynamical downscaling method (</i><b><i>b</i></b><i>). Statistical downscaling methods such as STAR-ESDM (</i><b><i>c</i></b><i>) and BCSD (</i><b><i>d</i></b><i>) often struggle to capture the magnitude and spatial structure of such compound risks.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Why this breakthrough matters</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7espt\">Dynamical-generative downscaling represents a significant step towards obtaining comprehensive future regional climate projections at actionable scales below 10 km. It makes downscaling large ensembles of Earth system models computationally feasible — our study estimates computational cost savings of 85% for the 8-model ensemble tested, a figure that would increase for larger ensembles. The fast and efficient AI inference step is similar to how Google’s <a href=\"https://research.google/blog/generative-ai-to-quantify-uncertainty-in-weather-forecasting/\">SEEDS</a> and <a href=\"https://deepmind.google/discover/blog/gencast-predicts-weather-and-the-risks-of-extreme-conditions-with-sota-accuracy/\" target=\"_blank\" rel=\"noopener noreferrer\">GenCast</a> weather forecasting models operate, enabling a thorough assessment of regional environmental risk.</p><p data-block-key=\"chlkh\">By providing more accurate and probabilistically complete regional climate projections at a fraction of the computational cost, dynamical-generative downscaling can dramatically improve environmental risk assessments. This enables better-informed decisions for adaptation and resilience policies across vital sectors like agriculture, water resource management, energy infrastructure, and natural hazard preparedness.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7espt\"><i>We would like to thank our co-authors Zhong Yi Wan, Leonardo Zepeda-Núñez, Tapio Schneider, John Anderson, and Fei Sha. We would also like to acknowledge Stephan Hoyer, Lizao Li, Alex Hall, and Stefan Rahimi for insightful comments on our work.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "学习澄清：基于行动的对比自训练多轮对话 (原标题: Learning to clarify: Multi-turn conversations with Action-Based Contrastive Self-Training)",
      "link": "https://research.google/blog/learning-to-clarify-multi-turn-conversations-with-action-based-contrastive-self-training/",
      "pubDate": "Mon, 02 Jun 2025 16:00:00 GMT",
      "isoDate": "2025-06-02T16:00:00.000Z",
      "creator": "Google",
      "summary": "大型语言模型（LLMs）在开发智能对话代理方面取得了显著进展，但它们在多轮对话中仍缺乏关键技能，例如在面对歧义时进行澄清。LLMs往往会过度猜测或隐含地推断用户意图，而非主动提问以寻求澄清。高质量对话样本的稀缺性限制了LLMs学习最佳对话行动的能力。\n\n在ICLR 2025上发表的论文《学习澄清：基于行动的对比自训练多轮对话》中，研究人员提出了**基于行动的对比自训练（ACT）**算法。ACT是一种基于直接偏好优化（DPO）的准在线偏好优化算法，旨在实现数据高效的多轮对话策略学习。该方法在表格问答和机器阅读理解等多个真实世界对话任务中，展示了其在数据高效微调场景下的有效性。此外，论文还引入了**AmbigSQL**，一项用于澄清复杂结构化查询语言（SQL）代码生成的信息查询请求的新任务，以促进数据分析代理的开发。研究还提出通过评估LLMs隐式识别和推理对话中歧义的能力来衡量其作为对话代理的功能。\n\nACT在对话建模方面比监督微调（SFT）和DPO等标准微调方法有显著改进。\n\n**对话代理的澄清能力**\n\n一个能够进行消歧的对话代理，可以在存在歧义时识别并提出澄清问题，从而获得更准确的最终答案。\n\n![对话代理澄清能力概述](https://storage.googleapis.com/gweb-research2023-media/images/L2C1-OverviewHero.width-1250.png)\n\n**对话推理的新视角**\n\n传统的神经对话代理通常包含两个核心组件：对话理解和规划模块（例如，判断是否需要提出澄清问题的二元预测任务），以及执行这些对话行动的生成模块（即形成澄清问题或尝试回答）。然而，在现代交互范式中，LLMs通常被端到端地应用于对话应用程序，而没有中间规划阶段。ACT提出将对话行动规划直接优化为响应生成的一个隐式子任务，称之为**隐式行动规划**。\n\nLLM的下游使用训练通常包括三个阶段：预训练、用于指令遵循的监督微调（SFT），以及与人类偏好对齐的微调。DPO是常用的最终对齐算法，但它通常与多轮对话的本质不符。ACT算法旨在解决这些问题。\n\n**ACT算法阶段**\n\n1.  **阶段1：基于行动的对比数据生成**\n    *   目标：构建一个偏好数据集，包含一对对话响应，其中一个代表“获胜”行动，另一个代表“失败”行动。\n    *   过程：从初始对话数据集开始，对于数据集中的每个回合，使用对话历史和必要的任务特定上下文作为输入提示。将该回合视为“获胜响应”（表达一个行动，例如“澄清”）。然后使用条件生成模型合成一个“被拒绝的响应”（代表相反的行动，例如“回答”）。此阶段的结果是一个配对数据集，其中每个被拒绝的响应都是合成生成的。\n\n    ![ACT数据生成阶段概述](https://storage.googleapis.com/gweb-research2023-media/images/L2C2-DataGeneration.width-1250.png)\n\n2.  **阶段2：对比自训练**\n    *   目标：使用DPO目标微调策略模型，但采用“在线策略学习”（on-policy learning）。\n    *   核心理念：DPO类算法通过优化分配给获胜和失败响应的对数概率来工作。在线策略响应采样产生高概率的token序列。对话改进需要多轮优化，这难以仅用单轮对比配对来表达。\n    *   过程：不直接使用固定对比对进行离线梯度更新，而是执行在线策略采样。首先确定响应是否表达了正确的行动（例如，一个澄清问题），如果是，则模拟轨迹结果并根据原始对话中给出的信息查询意图评估结果。根据结果是否正确，用模拟的多轮轨迹替换阶段1中对比对的获胜或失败响应。策略模型使用DPO目标进行更新。\n\n    ![ACT微调阶段概述](https://storage.googleapis.com/gweb-research2023-media/images/L2C4-Final.width-1250.png)\n\n**推动最先进的多轮建模能力**\n\n研究人员使用开放权重LLMs在多样化的多轮对话数据集上对ACT进行了实验，包括PACIFIC（表格与文本推理）、Abg-CoQA（密集段落推理）和AmbigSQL（文本到SQL生成）。实验结果显示：\n\n*   **PACIFIC数据集表现：** 在PACIFIC的所有三种数据高效设置中，ACT在所有指标上都取得了最强的性能，优于SFT和提示Gemini（后者具有额外的测试时间计算优势）。\n    *   在仅使用50个对话作为微调数据时，ACT在衡量模型隐式识别歧义的能力方面，比SFT实现了高达19.1%的相对改进（Macro F1从69.0提高到82.2）。\n    *   与基于适配器的SFT与Gemini Pro相比，ACT在多轮任务性能方面的数据效率大大提高，轨迹级DROP F1的相对改进高达35.7%（从45.6提高到61.9）。\n    *   在这些有限数据设置下，使用ACT进行微调使模型能够匹配或超越使用上下文学习的前沿LLMs，尽管在推理过程中没有上下文示例。\n    *   **关键发现：** 在线策略学习和多轮轨迹模拟对于改进多轮目标完成至关重要。\n\n    ![ACT在数据高效对话建模中显著优于标准微调方法](https://storage.googleapis.com/gweb-research2023-media/images/L2C4-Results.width-1250.png)\n\n**ACT性能增益归因（消融研究）**\n\n研究人员进行了多项实验以理解ACT各组件的益处：\n\n*   **基于行动的偏好是否必要？** “ACT w/ Random Actions”的表现不如正常ACT，表明行动选择的重要性。\n*   **是否需要在线策略采样？** “ACT w/o on-policy sampling”（在阶段1构建的数据集上评估正常离线策略DPO）显示出一些改进，但远不如完整的ACT。这可能是因为离线策略的负面响应不一定位于策略模型的语言流形中，分布偏移可能难以通过离线学习克服。\n*   **轨迹模拟是否必要？** “ACT w/ sampling w/o simulation”的结果表明，轨迹级模拟对于提高多轮性能至关重要，特别是策略模型推理其自身澄清问题的能力。\n*   **ACT是否模型无关？** 实验表明，ACT可以提高性能，无论基础模型是否预先与人类反馈对齐，尽管对齐可以作为改进的模型初始化。总体而言，ACT在提高基础模型性能方面是模型无关的。\n\n![ACT各组件的消融研究（使用PACIFIC数据集）](https://storage.googleapis.com/gweb-research2023-media/images/L2C5-Table.width-1250.png)\n\n**多轮对话建模的未来**\n\nACT是一种模型无关的准在线对比微调方法，适用于样本高效的对话任务适应，并提供了一套对话代理评估的工作流程。研究结果表明，ACT在有限数据条件下对任务适应非常有效。未来的工作可能考虑将ACT与现有复杂任务（如文本到SQL生成）的复杂微调方法相结合，以及推广到大规模数据和多任务环境。",
      "shortSummary": "大型语言模型（LLMs）在多轮对话中常缺乏澄清能力。为解决此问题，研究提出“基于行动的对比自训练”（ACT）算法。ACT是一种数据高效的准在线偏好优化方法，通过隐式行动规划和多轮轨迹模拟，显著提升了LLMs在歧义识别和澄清方面的表现。实验证明，ACT在数据受限场景下，相比传统微调和DPO等方法，能大幅提高对话模型性能，甚至超越使用上下文学习的前沿LLMs，为构建更智能的对话代理提供了新途径。",
      "translated_title": "学习澄清：基于行动的对比自训练多轮对话",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/L2C1-OverviewHero.width-1250.png",
          "alt": "L2C1-OverviewHero",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/L2C2-DataGeneration.width-1250.png",
          "alt": "L2C2-DataGeneration",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/L2C4-Final.width-1250.png",
          "alt": "L2C4-Final",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/L2C4-Results.width-1250.png",
          "alt": "L2C4-Results",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/L2C5-Table.width-1250.png",
          "alt": "L2C5-Table",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"4n8wb\">Large language models (LLMs) that have been optimized through human feedback have rapidly emerged as a <a href=\"https://arxiv.org/abs/2312.11805\" target=\"_blank\" rel=\"noopener noreferrer\">leading paradigm</a> for developing intelligent conversational agents. However, despite their strong performance across many benchmarks, LLM-based agents can still lack multi-turn conversational skills such as disambiguation — when they are faced with ambiguity, they often overhedge or implicitly guess users' true intents rather than asking clarifying questions. Yet high-quality conversation samples are often limited, constituting a bottleneck for LLMs' ability to learn optimal dialogue actions.</p><p data-block-key=\"3at5l\">In “<a href=\"https://openreview.net/pdf?id=SIE6VFps9x\" target=\"_blank\" rel=\"noopener noreferrer\">Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training</a>” (presented at <a href=\"https://iclr.cc/Conferences/2025\" target=\"_blank\" rel=\"noopener noreferrer\">ICLR 2025</a>), we propose Action-Based Contrastive Self-Training (ACT), a quasi-online preference optimization algorithm based on <a href=\"https://neurips.cc/virtual/2023/oral/73865\" target=\"_blank\" rel=\"noopener noreferrer\">Direct Preference Optimization</a> (DPO), which enables data-efficient dialogue policy learning in multi-turn conversation modeling. We demonstrate ACT's efficacy under data-efficient tuning scenarios using multiple real-world conversational tasks such as <a href=\"https://aclanthology.org/2022.emnlp-main.469/\" target=\"_blank\" rel=\"noopener noreferrer\">tabular-grounded question-answering</a> and <a href=\"https://www.akbc.ws/2021/assets/pdfs/SlDZ1o8FsJU.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">machine reading comprehension</a>. We also introduce AmbigSQL, a novel task for disambiguating information-seeking requests for complex <a href=\"https://cloud.google.com/sql\" target=\"_blank\" rel=\"noopener noreferrer\">Structured Query Language</a> (SQL) code generation to facilitate the development of data analysis agents. Additionally, we propose evaluating the ability of LLMs to function as conversational agents by examining whether they can implicitly recognize and reason about ambiguity in conversation. ACT demonstrates substantial conversation modeling improvements over standard tuning approaches like supervised fine-tuning and DPO.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/L2C1-OverviewHero.width-1250.png\" alt=\"L2C1-OverviewHero\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/L2C1-OverviewHero.width-1250.png\" alt=\"L2C1-OverviewHero\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"v70yh\"><i>A conversational agent capable of disambiguation could recognize when there is ambiguity and ask a clarifying question towards a more accurate final answer.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qbpr9\">A novel perspective on conversational reasoning</h2><p data-block-key=\"7gplf\">Traditional neural approaches to building conversational agents typically consist of two core components: a module for dialogue understanding and planning (e.g., a binary prediction task to determine whether it is appropriate to ask a clarifying question), and a generation module which can execute such conversational actions (i.e., forming a clarifying question or answer attempt). However, in the modern interaction paradigm, LLMs are typically adapted for end-to-end usage in conversational applications without an intermediate planning stage. We propose directly optimizing conversational action planning as an implicit subtask of response generation. We refer to this paradigm as implicit action planning.</p><p data-block-key=\"8s2r5\">Training an LLM for downstream use consists of three phases: pre-training, <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/samples/generativeaionvertexai-tuning-basic\" target=\"_blank\" rel=\"noopener noreferrer\">supervised fine-tuning</a> (SFT) for instruction-following, and tuning for <a href=\"https://cloud.google.com/blog/products/ai-machine-learning/rlhf-on-google-cloud\" target=\"_blank\" rel=\"noopener noreferrer\">alignment with human preferences</a>. A common algorithm used for this final alignment phase is <a href=\"https://openreview.net/pdf?id=HPuSIXJaa9\" target=\"_blank\" rel=\"noopener noreferrer\">DPO, an off-policy contrastive learning algorithm</a> designed to optimize the probabilities of winning and losing sequences such as conversation responses. However, such algorithms are typically still misaligned with the multi-turn nature of conversations. The proposed ACT algorithm seeks to address these issues.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"qbpr9\">Phase 1: Action-based contrastive data generation</h3><p data-block-key=\"3h973\">The first phase of building ACT involves constructing a preference dataset, consisting of pairs of conversational responses where one resembles a winning action and one resembles a losing action.</p><p data-block-key=\"55dje\">We start from an initial conversational dataset. For each turn in the dataset, we use the conversation history as part of the input prompt (“Show me information…” in the figure below) in addition to any necessary task-specific context (such as a SQL database schema) and treat that turn as the winning response (“What specific …”, below). This winning response expresses an action (here, “Clarify”) and thus we synthesize a rejected response representing some converse action (here, “Answer”) using some <a href=\"https://aclanthology.org/2023.acl-short.82/\" target=\"_blank\" rel=\"noopener noreferrer\">conditional generation model</a>. The result of this stage is a pairwise dataset where each of the rejected responses is synthetically generated.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/L2C2-DataGeneration.width-1250.png\" alt=\"L2C2-DataGeneration\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/L2C2-DataGeneration.width-1250.png\" alt=\"L2C2-DataGeneration\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"v70yh\"><i>Overview of the data generation phase of ACT.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"qbpr9\">Phase 2: Contrastive self-training</h3><p data-block-key=\"8pqj9\">The second phase involves tuning the policy model using the DPO objective. We can use the prompts from Phase 1, but rather than directly running DPO using the previously constructed contrastive pairs, we perform on-policy learning according to a few intuitions:</p><ul><li data-block-key=\"6st4r\">DPO-like algorithms work by optimizing the log probabilities assigned to the winning and losing responses.</li><li data-block-key=\"1q7d\">By construction, on-policy response sampling yields high-probability token sequences.</li><li data-block-key=\"9mji7\">Conversational improvements require multi-turn optimization, which are difficult to express using only single-turn contrast pairings.</li></ul><p data-block-key=\"2cl9h\">The figure below demonstrates how ACT works according to these intuitions. Rather than directly running an offline gradient update using the fixed contrastive pairs, we perform on-policy sampling. We first determine whether the response expresses the correct action (e.g., a clarifying question), and if so, then we <i>simulate the result of the trajectory</i> and evaluate the outcome against the information-seeking intent given in the original conversation. Depending on if the outcome is correct, we replace either the winning or losing response from the contrastive pair in Phase 1 with the simulated <i>multi-turn trajectory</i>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/L2C4-Final.width-1250.png\" alt=\"L2C4-Final\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/L2C4-Final.width-1250.png\" alt=\"L2C4-Final\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"v70yh\"><i>Overview of the tuning phase of ACT. For each initial contrastive pairing from the Phase 1 preference dataset, we sample an on-policy response from the model being tuned. We evaluate the trajectory resulting from the sampled response then update the contrastive pairing by either replacing the existing winning or losing response. The policy is updated using the DPO objective.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qbpr9\">Pushing state-of-the-art multi-turn modeling capabilities</h2><p data-block-key=\"70a\">We experimented with ACT using open-weight LLMs on a diverse set of multi-turn conversational datasets: <a href=\"https://aclanthology.org/2022.emnlp-main.469/\" target=\"_blank\" rel=\"noopener noreferrer\">PACIFIC</a> (reasoning over tables mixed with text), <a href=\"https://openreview.net/forum?id=SlDZ1o8FsJU\" target=\"_blank\" rel=\"noopener noreferrer\">Abg-CoQA</a> (reasoning over dense passages), and AmbigSQL (text-to-SQL generation). We compared against various competitive baselines, including:</p><ul><li data-block-key=\"1kgvc\">Supervised fine-tuning with cross-entropy loss (SFT)</li><li data-block-key=\"cd5qp\">Iterative Reasoning Preference Optimization (IRPO)</li><li data-block-key=\"3736h\">Prompting Gemini 1.5 with in-context learning (ICL) examples</li><li data-block-key=\"nit9\">Prompting Claude 3.5 with ICL examples</li></ul><h3 data-block-key=\"2g6u0\">Conversational question answering with tabular grounding on PACIFIC</h3><p data-block-key=\"4d49g\">In the figure below, we see that across all three data-efficient settings considered for PACIFIC, ACT achieves the strongest performance across all metrics compared to both SFT and prompting Gemini, which has the advantage of additional test-time computation. In particular, ACT achieves up to a 19.1% relative improvement over SFT when measuring the tuned model's ability to implicitly recognize ambiguity (from 69.0 to 82.2 <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\" target=\"_blank\" rel=\"noopener noreferrer\">Macro F1</a>) given only 50 conversations as tuning data. We also observe that ACT has greatly improved data efficiency compared to adapter-based SFT with Gemini Pro, with a relative improvement of as high as 35.7% in multi-turn task performance (from 45.6 to 61.9 in terms of trajectory-level <a href=\"https://aclanthology.org/N19-1246/\" target=\"_blank\" rel=\"noopener noreferrer\">DROP F1</a>). Additionally, tuning with ACT in these limited data settings grants the model the ability to match or outperform frontier LLMs used with in-context learning despite having zero in-context examples during inference. Overall, we find that on-policy learning and multi-turn trajectory simulation are crucial for improved multi-turn goal completion.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/L2C4-Results.width-1250.png\" alt=\"L2C4-Results\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/L2C4-Results.width-1250.png\" alt=\"L2C4-Results\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"v70yh\"><i>ACT greatly outperforms standard tuning approaches in data-efficient settings for conversational modeling.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qbpr9\">In our <a href=\"https://openreview.net/pdf?id=SIE6VFps9x\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>, we present extended results on the PACIFIC corpus, where we demonstrate that ACT outperforms IRPO. There, we additionally present our findings on Abg-CoQA and AmbigSQL.</p><p data-block-key=\"q5ps\"></p><h3 data-block-key=\"8au6j\">Attributing the performance gains from ACT</h3><p data-block-key=\"brt9m\">We conducted several experiments to understand the benefits of each component of ACT, the results of which are below:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/L2C5-Table.width-1250.png\" alt=\"L2C5-Table\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/L2C5-Table.width-1250.png\" alt=\"L2C5-Table\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"v70yh\"><i>Ablation study of various components of ACT using PACIFIC.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qbpr9\"><b><i>Are action-based preferences necessary?</i></b> One of the key factors of ACT is that the contrastive pairs highlight differences between conversational actions. In “ACT w/ Random Actions”, we additionally examine the importance of action selection by randomly sampling both the winning and losing action when constructing the preference pair, and observe this underperforms normal ACT.</p><p data-block-key=\"83hrr\"><b><i>Do we need on-policy sampling?</i></b> In “ACT w/o on-policy sampling”, we examine the importance of on-policy sampling by evaluating normal off-policy DPO on the dataset as constructed in Phase 1. While we do observe some improvements over SFT (e.g., from 69.0 to 74.8 Macro F1), the overall improvements are much larger when using on-policy sampling as with full ACT. This may be due to the fact that the off-policy negative responses are not guaranteed to lie in the language manifold of the policy model, and distribution shift may be too difficult to overcome with off-policy learning.</p><p data-block-key=\"dn7kt\"><b><i>Is trajectory simulation necessary?</i></b> ACT is better-aligned with multi-turn conversations due to its trajectory simulation. Without multi-turn simulation, our approach can be viewed similarly to on-policy DPO variants like IRPO, but with a conversation-specific reward signal which accounts for conversation actions and task heuristics. In “ACT w/ sampling w/o simulation”, we find that this trajectory-level simulation is critical to improving multi-turn performance, especially the policy model’s ability to reason about its own clarification questions.</p><p data-block-key=\"f50m1\"><b><i>Is ACT model agnostic?</i></b> The base model in our main experiments, <a href=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\" target=\"_blank\" rel=\"noopener noreferrer\">Zephyr</a>, is obtained by aligning <a href=\"https://mistral.ai/news/announcing-mistral-7b\" target=\"_blank\" rel=\"noopener noreferrer\">Mistral</a>. In “ACT with unaligned foundation models” we observe a performance gap of 6.5 Action F1 and 4.3 Trajectory F1 after ACT tuning for the two models. However, our results demonstrate ACT can improve performance regardless of pre-existing alignment with human feedback, although it can help as an improved model initialization. Overall, we find that improving base model performance with ACT is model agnostic.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qbpr9\">The future of multi-turn conversation modeling</h2><p data-block-key=\"6ihu3\">We propose ACT, a model agnostic quasi-online contrastive tuning approach for sample-efficient conversational task adaptation, along with a workflow for evaluation of conversational agents. We demonstrate encouraging evidence that ACT is highly effective for task adaptation in the limited data regime. Future work may also consider combining ACT with existing sophisticated tuning approaches for complex tasks like text-to-SQL generation, as well as generalization to large-scale data and multi-task environments.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qbpr9\">Acknowledgements</h2><p data-block-key=\"1srhj\"><i>We are deeply grateful for helpful feedback on our manuscript from Hanjun Dai, and advice from Ta-Chung Chi and Kun Qian. We would also like to recognize Chris Baron and Vipin Nair, whose efforts have been crucial to the success of this work. This work was completed in Google Cloud AI Research where Maximillian Chen was a Student Researcher and Ruoxi Sun was a Research Scientist.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "使用用户级差分隐私微调大型语言模型 (原标题: Fine-tuning LLMs with user-level differential privacy)",
      "link": "https://research.google/blog/fine-tuning-llms-with-user-level-differential-privacy/",
      "pubDate": "Thu, 22 May 2025 16:00:00 GMT",
      "isoDate": "2025-05-22T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 使用用户级差分隐私微调大型语言模型\n\n## 引言与背景\n*   **模型微调与隐私挑战**：现代机器学习（ML）模型常需针对特定领域数据进行微调以优化性能，然而这些数据往往涉及隐私敏感信息。差分隐私（DP）通过在训练过程中注入噪声，严格保证训练模型尊重训练数据的隐私。\n*   **个体示例级DP的局限性**：大多数DP研究关注个体示例的隐私（示例级DP）。但如果一个用户贡献了大量示例，攻击者即使无法了解单个示例，也可能推断出该用户的某些信息。\n*   **用户级DP的重要性**：用户级DP是一种更强的隐私形式，它保证攻击者无法通过模型了解用户是否包含在训练数据集中。这更符合当今社会的数据所有权模式，并常用于联邦学习中，因为分布式设备（如手机）通常包含由单个用户拥有的多个示例，需要更严格的隐私保障。\n    *   ![用户级DP概述](https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-1-Overview.width-1250.png)\n    *   用户级DP确保无法判断某个用户的所有数据是否被包含在训练中，而不仅仅是其数据的一部分。\n*   **用户级DP的挑战**：与示例级DP相比，用户级DP的学习难度更高，需要注入更多的噪声，且模型越大，问题越严重。\n\n## 在数据中心扩展用户级DP到大型语言模型（LLMs）\n*   **研究目标**：论文《在固定计算预算下使用用户级差分隐私进行学习》旨在将用户级DP扩展到在数据中心训练的大型语言模型（LLMs）。\n*   **数据中心训练的优势**：与联邦学习相比，数据中心训练更灵活，可以查询单个示例或整个用户，并且可以在每个训练轮次选择要查询的用户。\n*   **研究核心问题**：如何利用数据中心训练的这种灵活性来获得更好的训练结果？\n*   **聚焦LLM微调**：由于DP训练计算成本高昂，不适用于整个LLM的训练，且微调更可能涉及私有领域特定数据，因此研究重点放在LLM的DP微调上。目标是确定最佳算法并利用数据中心灵活性进一步优化它们，因为即使是微小的噪声减少也能带来显著的质量提升。\n\n## 使用用户级隐私训练模型\n*   **DP-SGD原理**：随机梯度下降（SGD）通过将训练数据分成小批次，计算每个示例的“梯度”并应用于模型。DP-SGD通过向梯度添加随机噪声来实现DP，从而使模型获得不完美但保护隐私的信息。\n    *   ![DP-SGD步骤可视化](https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-2a-Visualization.width-800.gif)\n    *   DP-SGD非常适合实现示例级DP，因为它直接限制了每个示例对模型的影响。\n*   **实现用户级DP的两种方法**：\n    1.  **预处理**：首先限制每个用户对训练数据集贡献的示例数量（“贡献上限”）。\n    2.  **两种采样方法**：\n        *   **示例级采样（ELS）**：直接应用DP-SGD，但通过添加更多噪声将示例级DP保证转换为用户级DP。它随机采样示例来形成批次。\n        *   **用户级采样（ULS）**：随机采样用户，然后将所有来自这些采样用户的示例组成批次。ULS在数据中心环境中类似于联邦学习（但在实际联邦学习中没有随机采样）。\n    *   ![用户贡献上限示例](https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-3a-Example.width-1250.png)\n    *   ![ELS与ULS批次形成对比](https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-4a-ELSvULS.width-800.gif)\n    *   **关键参数**：两种算法都有一个关键参数需要优化——预处理中使用的“贡献上限”。\n\n## 针对LLMs的算法优化\n*   **ELS的噪声优化**：发现以往工作添加的噪声比实际需要的多几个数量级。通过新的证明，可以在保持相同隐私保证的前提下显著减少噪声，从而提高模型质量。\n*   **ELS和ULS的贡献上限优化**：\n    *   “默认”选择是使用每个用户都满足的贡献上限（即不进行预处理），但这会导致为保护大量数据贡献者而添加大量噪声。\n    *   设置较小的贡献上限可以减少所需噪声，但代价是丢弃大量数据。\n    *   由于LLM训练成本高昂，无法尝试训练多个模型来选择最佳贡献上限，因此需要一种在训练前有效选择贡献上限的策略。\n    *   **ELS策略**：经过大规模实验，发现将贡献上限设置为每个用户持有的示例数量的中位数是一种有效策略。\n    *   **ULS策略**：提供一个总噪声随贡献上限变化的预测函数，选择使该预测最小化的贡献上限是一种有效策略。\n\n## 实验结果\n*   **ELS噪声减少**：与以往工作相比，新分析证明所需的噪声水平呈指数级降低（隐私保证ε仅呈近似线性衰减，而非指数衰减）。\n    *   ![贡献上限与隐私保证](https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-5-ContributionBound.width-1250.png)\n*   **ELS与ULS对比**：在语言模型微调任务中，使用相同总示例数进行训练，对3.5亿参数的Transformer模型在StackOverflow和CC-News数据集上进行微调。\n    *   ![CCNews和StackOverflow上的ELS、ULS和无微调对比](https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-6a-ELSvULS.width-1250.png)\n    *   ![CCNews和StackOverflow上的ELS、ULS和无微调对比](https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-6b-ELSvULS.width-1250.png)\n    *   **结果**：在大多数情况下，ULS是更好的算法。例外情况（至少对于CC-News）是需要更高隐私或计算量较少的情况。值得注意的是，由于这些优化，尽管有严格的隐私要求，两种方法都比预训练模型表现更好。\n\n## 结论\n*   **主要贡献**：优化了ELS和ULS两种DP-SGD变体在数据中心实现用户级DP的性能。\n*   **具体优化**：为ELS提供了新的隐私保证，并为ELS和ULS开发了无需多次训练即可设置贡献上限的启发式方法。这些优化为重要参数提供了有充分依据的选择。\n*   **可行性与优势**：尽管用户级DP是严格的隐私定义，且使用DP训练大型模型面临挑战，但实验证明，通过这些优化，使用用户级DP微调LLMs是可行且优于仅使用预训练模型的。\n*   **实际意义**：这项工作使模型训练者能够对敏感数据集进行模型微调，同时为用户提供强大的隐私保护。",
      "shortSummary": "该研究专注于使用用户级差分隐私（DP）微调大型语言模型（LLMs），以解决敏感数据隐私问题。文章优化了两种DP-SGD变体：示例级采样（ELS）和用户级采样（ULS），使其适用于数据中心训练。通过为ELS提供新的隐私保证和为两种方法开发贡献上限的启发式选择策略，显著减少了所需噪声并提高了模型质量。实验表明，ULS通常表现更优，且经过优化的DP微调方法在严格隐私要求下，仍能超越预训练模型，实现对敏感数据的有效模型训练。",
      "translated_title": "使用用户级差分隐私微调大型语言模型",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-1-Overview.width-1250.png",
          "alt": "UserLvlDP-1-Overview",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-2a-Visualization.width-800.gif",
          "alt": "UserLvlDP-2a-Visualization",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-3a-Example.width-1250.png",
          "alt": "UserLvlDP-3a-Example",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-4a-ELSvULS.width-800.gif",
          "alt": "UserLvlDP-4a-ELSvULS",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-5-ContributionBound.width-1250.png",
          "alt": "UserLvlDP-5-ContributionBound",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ifmk7\">The machine learning community has consistently found that while modern machine learning (ML) models are powerful, they <a href=\"https://arxiv.org/pdf/2103.08493\" target=\"_blank\" rel=\"noopener noreferrer\">often need to be fine-tuned on domain-specific data</a> to maximize performance. This can be problematic or even impossible, as informative data is often privacy-sensitive. Differential privacy (DP) allows us to train ML models while rigorously guaranteeing that the learned model respects the privacy of its training data, by injecting noise into the training process.</p><p data-block-key=\"eauhh\">Most work on DP focuses on the privacy of individual examples (i.e., <a href=\"https://desfontain.es/blog/differential-privacy-in-more-detail.html\" target=\"_blank\" rel=\"noopener noreferrer\">example-level DP</a>). This has drawbacks. If a particular user has many examples training the model, attackers may be able to learn something about that user even if they can’t learn about their individual examples.</p><p data-block-key=\"8jgo8\"><a href=\"https://arxiv.org/abs/1710.06963\" target=\"_blank\" rel=\"noopener noreferrer\">User-level DP</a> is a stronger form of privacy that guarantees that an attacker who uses a model can’t learn things about the user, like whether or not the user’s data is included in the training dataset. User-level DP better reflects how data is actually owned in today’s society. It’s used frequently in <a href=\"https://research.google/blog/federated-learning-collaborative-machine-learning-without-centralized-training-data/\">federated learning</a> to train models across distributed devices like cell phones. Those devices often have many examples, all owned by a single user, and require more stringent privacy guarantees.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-1-Overview.width-1250.png\" alt=\"UserLvlDP-1-Overview\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-1-Overview.width-1250.png\" alt=\"UserLvlDP-1-Overview\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"37bs3\"><i>User-level DP makes it so you can’t tell if</i> <b><i>all</i></b><i> of someone’s data was included in training or not, rather than just one piece of their data.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ifmk7\">Learning with user-level DP is strictly harder than example-level DP, and requires adding significantly more noise. This is a problem that gets worse as the model gets larger!</p><p data-block-key=\"2tkl7\">In “<a href=\"https://arxiv.org/abs/2407.07737\" target=\"_blank\" rel=\"noopener noreferrer\">Learning with User-Level Differential Privacy Under Fixed Compute Budgets</a>”, we set out to scale user-level DP to large language models (LLMs) trained in the datacenter. Datacenter training is much more flexible than federated learning. In federated learning, one can only perform queries on users and not on individual examples, and it is not possible to choose which users are available to query. In datacenter training, one can query both individual examples and whole users, and it is possible to choose which ones to query every round. Our central question is, how can we use this increased flexibility to achieve better training results?</p><p data-block-key=\"630i4\">Rather than training the full LLM with DP, we focus on LLM DP fine-tuning as DP requires more computation, which might be unaffordable for full LLM training, and fine-tuning is more likely to require private domain-specific data. We determine which algorithms worked best and how to use the flexibility of datacenter training to further optimize them. This optimization is important for LLMs as even small reductions in noise can result in significant quality gains. We also show that even with the added flexibility in the datacenter, our proposed training strategy looks more like an algorithm for federated learning.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Training models with user-level privacy</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ifmk7\"><a href=\"https://en.wikipedia.org/wiki/Stochastic_gradient_descent\" target=\"_blank\" rel=\"noopener noreferrer\">Stochastic gradient descent</a> (SGD) is a common model training algorithm that randomly divides training data into small batches, computes model updates, called “gradients”, for each example in the batch, and applies them to the model. To train with DP, we modify this slightly by adding random noise to the gradients, essentially combining DP with SGD in a process referred to as DP-SGD. The noise makes it so the model gets imperfect information about the examples during training, which is good for privacy! But since we are giving the model imperfect information, its ability to learn from the examples is necessarily weaker than if we gave it perfect information. However, DP is a prerequisite for using private data, and imperfect information about the private data is better than no information at all.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-2a-Visualization.width-800.gif\" alt=\"UserLvlDP-2a-Visualization\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-2a-Visualization.width-800.gif\" alt=\"UserLvlDP-2a-Visualization\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"37bs3\"><i>Visualizing the steps in (DP-)SGD.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ifmk7\">DP-SGD is great for achieving example-level DP because it directly limits how much each example affects the model. It’s been <a href=\"https://research.google/blog/bridging-the-gap-in-differentially-private-model-training/\">extensively</a> <a href=\"https://research.google/blog/differential-privacy-accounting-by-connecting-the-dots/\">studied</a> at Google, including use cases such as <a href=\"https://research.google/blog/private-ads-prediction-with-dp-sgd/\">ads modelling</a> and <a href=\"https://research.google/blog/protecting-users-with-differentially-private-synthetic-training-data/\">synthetic data generation</a>.</p><p data-block-key=\"did4l\">How do we change DP-SGD if we want user-level DP? We need to make sure to limit the effect each user has on the model.</p><p data-block-key=\"7c1mm\">There are two ways to achieve this. For both, we first pre-process the dataset so that each user only contributes a bounded number of examples to the training dataset. Then we either:</p><ol><li data-block-key=\"dp1dq\">Apply DP-SGD as-is. By adding more noise than before, we can turn our example-level DP guarantee into user-level DP (this is the hard part!).</li><li data-block-key=\"d00is\">Instead of sampling random examples in DP-SGD to form batches, sample random users, and then take all examples from the sampled users to form the batch.</li></ol><p data-block-key=\"eskpp\">The big difference between these methods is in the data we sample. The first samples random examples, so we call it “Example-Level Sampling” (ELS). The second samples random users, so we call it “User-Level Sampling” (ULS). ULS looks a lot like federated learning in the datacenter (and is actually used in real federated learning settings, but without random sampling). Note that because of the random sampling step, both algorithms are not feasible in federated learning because devices are not necessarily available for every round of training!</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-3a-Example.width-1250.png\" alt=\"UserLvlDP-3a-Example\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-3a-Example.width-1250.png\" alt=\"UserLvlDP-3a-Example\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"37bs3\"><i>An example dataset before and after we bound the contribution of each user to at most 3.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-4a-ELSvULS.width-800.gif\" alt=\"UserLvlDP-4a-ELSvULS\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-4a-ELSvULS.width-800.gif\" alt=\"UserLvlDP-4a-ELSvULS\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"37bs3\"><i>A visual comparison of how ELS and ULS form batches.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ifmk7\">Both ELS and ULS have a key parameter to optimize: the bound on the number of examples each user can contribute to the dataset, which we call the “contribution bound”, that we use in pre-processing. As we will discuss later, this parameter needs to be carefully chosen to optimize performance.</p><p data-block-key=\"f4umd\">Which of these algorithms works better, especially at scale? It’s not obvious, and it isn’t something that we found an answer to in the literature. That’s what we set out to find.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Making these algorithms work for LLMs</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ifmk7\">If we run these algorithms “out-of-the-box” for LLMs, things go badly. So, we came up with optimizations to the algorithms that fix the key issues with running them “out-of-the-box”.</p><p data-block-key=\"5g4e3\">For ELS, we had to go from example-level DP guarantees to user-level DP guarantees. We found that <a href=\"https://privacytools.seas.harvard.edu/sites/g/files/omnuum6656/files/privacytools/files/manuscript_2016.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">previous work</a> was adding orders of magnitude more noise than was actually necessary. We were able to prove that we can add significantly less noise, making the model much better while retaining the same privacy guarantees.</p><p data-block-key=\"7hcld\">For both ELS and ULS, we had to figure out how to optimize the contribution bound. A “default” choice is to choose a contribution bound that every user already satisfies; that is, we don’t do any pre-processing. However, some users may contribute a large amount of data, and we will need to add large amounts of noise to provide privacy to these users. Setting a smaller contribution bound reduces the amount of noise we need to add, but the cost is having to discard a lot of data. Because LLM training runs are expensive, we can’t afford to try training a bunch of models with different contribution bounds and pick the best one — we need an effective strategy to pick the contribution bound <i>before</i> we start training.</p><p data-block-key=\"a8hkp\">After lengthy experimentation at scale, for ELS we found that setting the contribution bound to be the median number of examples held by each user was an effective strategy. For ULS, we give a prediction for the total noise added as a function of the contribution bound, and found that choosing the contribution bound minimizing this prediction was an effective strategy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ifmk7\">We first compared the amount of noise we proved was necessary for ELS to the noise <a href=\"https://salil.seas.harvard.edu/sites/g/files/omnuum4266/files/salil/files/the_complexity_of_differential_privacy.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">past work</a> suggested was necessary. The noise levels we determined by our analyses reflected an exponential reduction in the noise needed:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-5-ContributionBound.width-1250.png\" alt=\"UserLvlDP-5-ContributionBound\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-5-ContributionBound.width-1250.png\" alt=\"UserLvlDP-5-ContributionBound\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"37bs3\"><i>Black-box calculations suggest that privacy guarantees (ε) decay exponentially in the contribution bound. Our new bound shows the privacy guarantee only decays near-linearly in the contribution bound!</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ifmk7\">We next compared ELS and ULS, both using our optimizations, on language model fine-tuning tasks, where each algorithm saw the same total number of examples per round of training. We fine-tuned a 350 million parameter transformer model on the <a href=\"https://arxiv.org/abs/2307.09619\" target=\"_blank\" rel=\"noopener noreferrer\">StackOverflow and CC-News datasets</a>, two standard research datasets for studying user-level DP.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-6a-ELSvULS.width-1250.png\" alt=\"UserLvlDP-6a-ELSvULS\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-6a-ELSvULS.width-1250.png\" alt=\"UserLvlDP-6a-ELSvULS\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-6b-ELSvULS.width-1250.png\" alt=\"UserLvlDP-6b-ELSvULS\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-6b-ELSvULS.width-1250.png\" alt=\"UserLvlDP-6b-ELSvULS\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"37bs3\"><i>Comparing ELS, ULS, and no fine-tuning on CCNews and StackOverflow.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ifmk7\">We found that in most cases, ULS is the better algorithm. The exception (at least for CC-News) was in cases where we wanted more privacy or don’t use much compute. Notably, in part thanks to our optimizations, both methods performed better than the pre-trained model, despite the strict privacy requirement.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ifmk7\">In this work, we optimized the performance of ELS and ULS, two variants of DP-SGD that achieve user-level DP and are enabled by training in the datacenter. We optimized ELS by giving new privacy guarantees for it and by developing an heuristic for setting the contribution bound without needing to do multiple training runs. We similarly optimized ULS by developing an heuristic for setting the contribution bound, again without needing to do training runs. Our optimizations provide well-justified choices for important parameters that previously were chosen in an <i>ad hoc</i> manner.</p><p data-block-key=\"cgs4i\">Despite user-level DP being a strict privacy definition and the challenges of training large models with DP, our experiments demonstrated that fine-tuning LLMs with user-level DP is both feasible and advantageous over sticking to pre-trained models thanks to our optimizations. Our work thus enables model trainers to fine-tune their models to sensitive datasets while still providing strong protections to their users.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Google 研究在 Google I/O 2025 (原标题: Google Research at Google I/O 2025)",
      "link": "https://research.google/blog/google-research-at-google-io-2025/",
      "pubDate": "Wed, 21 May 2025 16:00:00 GMT",
      "isoDate": "2025-05-21T16:00:00.000Z",
      "creator": "Google",
      "summary": "# Google 研究在 Google I/O 2025\n\n在每年的 Google I/O 大会上，Google 都会分享其最先进的技术，展示它们如何提供帮助、带来新体验，以及开发者和社区如何利用它们进行创新。许多这些新技术源于 Google Research 多年的工作，通常与其他团队合作，基于人工智能及其他计算机科学领域的多次突破。今年的 I/O 强调了将研究变为现实的影响。正如 Sundar Pichai 所说：“所有这些进展意味着我们正处于 AI 平台转变的新阶段。数十年的研究现在正在全球范围内成为人们、企业和社区的现实。”\n\n除了 Google Research 对 Gemini 和 I/O 舞台上重点介绍的生成式 AI 产品的贡献外，以下是今年的一些亮点，它们都利用了 Google Research 多年来的努力，以实现研究的“魔力循环”。\n\n## 医疗健康领域的 AI 进展：MedGemma 和 AMIE\n\n自 2022 年首次推出 Med-PaLM 以来，Google 研究团队持续推进 AI，以使医疗健康更易于获取和有效。\n\n*   **MedGemma**：\n    *   Google 在 I/O 大会上宣布了 MedGemma，这是其在多模态医学文本和图像理解方面最强大的开放模型。\n    *   它基于 Gemma 3，旨在作为开发者构建健康应用（如分析放射影像或总结临床数据）的起点。\n    *   其小尺寸使其能高效地针对特定需求进行微调。\n    *   在 MedQA 基准测试中，其在临床知识和推理任务上的基线性能与大得多的模型相似。\n    *   MedGemma 是开放的，可在开发者首选环境（包括 Google Cloud Platform 或本地）中运行。\n    *   MedGemma 4B 和 27B（仅文本）模型现已作为健康 AI 开发者基础（HAI-DEF）的一部分，在 HuggingFace 和 Vertex Model Garden 上提供。\n    *   ![MedGemma 的基线性能](https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-2-CostPerformance.width-1250.png)\n        *   *MedGemma 在临床知识和推理任务上的基线性能与大得多的模型相似。*\n*   **AMIE**：\n    *   与 Google DeepMind 合作开发的 AMIE 也在 I/O 上被重点提及。\n    *   AMIE 是一种用于医疗诊断对话的研究型 AI 代理。\n    *   新的多模态版本能够智能地解释和推理视觉医学信息，帮助临床医生实现更准确的诊断。\n\n## 学习领域的 AI 进展：LearnLM\n\n近两年来，Google Research 和 Google 内部团队一直与教育专家合作开发 LearnLM，这是一个针对学习进行微调的模型家族。\n\n*   **LearnLM 在 Gemini 2.5 中可用**：\n    *   在 I/O 上宣布，LearnLM 将直接在 Gemini 2.5 中提供，使其成为全球领先的学习模型。\n    *   最新技术报告显示，Gemini 2.5 Pro 在学习科学原理方面优于其他模型，是教育工作者的首选。\n    *   它具有先进的 STEM 推理、多模态理解、测验和评估能力等。\n    *   ![LearnLM 的教学法](https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-3-Pedagogy.width-1250.png)\n*   **新的 Gemini 测验体验**：\n    *   Google Research 团队帮助设计和优化了 Gemini 中的新测验体验。\n    *   学生（18 岁以上）可以要求 Gemini 根据他们的课堂笔记或课程文档创建自定义测验，并提供关于正确和错误答案的反馈和解释。\n*   **LearnLM 提示指南与合作**：\n    *   探索 LearnLM 提示指南，以最大化 Gemini 的教学价值，例如，要求它扮演生物老师或调整文本难度。\n    *   Google 正在与合作伙伴合作，将 LearnLM 模型的强大功能引入教育环境，例如与 Kayma 合作在加纳高中试点自动评估。\n\n## Gemma 的多语言和效率：让模型普惠可用\n\n作为 Google 使世界信息普遍可访问使命的一部分，Google 正在推进多语言研究，以确保 LLM 在不同语言中产生可靠输出。\n\n*   **Gemma3 的多语言能力**：\n    *   两个月前，Google 推出了 Gemma3，其研究帮助 Gemma 扩展到 140 多种语言，使其成为当今最佳的多语言开放模型。\n*   **Gemma3n 的效率**：\n    *   在 I/O 上宣布，这些功能现已通过 Gemmaverse 的最新成员 Gemma3n 提供，该模型只需 2GB 内存即可运行，专为设备上应用而构建。\n    *   效率方面的努力使 Gemma3n 模型能够降低延迟并更节能。\n*   **ECLeKTic 基准**：\n    *   Google Research 最近推出了 ECLeKTic，这是一个用于评估 LLM 中跨语言知识迁移的新型基准。\n\n## 高效且有根据的模型：助力搜索中的 AI 模式\n\n随着 LLM 规模的扩大和需求的增加，提高模型效率同时保持甚至提升其质量，是实现这些高性能模型普惠化的关键。\n\n*   **效率突破**：\n    *   Google Research 在效率方面取得了突破，例如在推测解码和级联方面的工作，这些已成为行业标准。\n*   **事实一致性与准确性**：\n    *   Google 发布了关于事实一致性技术和评估的研究，并通过双重检查和 FACTS Grounding 排行榜（与 Google DeepMind 和 Kaggle 合作发布）设定了事实性和依据性的标准。\n*   **AI 模式**：\n    *   Google 将其研究贡献给了搜索中的 AI 模式，以显著改善用户体验。\n    *   在 I/O 上宣布的 AI 模式是 Google 最强大的 AI 搜索，具有高级推理能力。\n    *   它正在向美国所有用户推出，允许人们通过后续问题和相关网站链接进行更深入的研究。\n    *   效率工作使模型运行更可靠，输出更快；事实性研究改进了 AI 模式的网页搜索方式，确保提供的答案高度准确并基于多个来源。\n\n## 多模态事实性：助力 Imagen4、Gemini 2.5 和 Vids 中的 AI 头像\n\n随着多模态内容的普及，Google 的事实性团队正在推进多模态事实性研究，以确保 Google 产品的高准确性标准。\n\n*   **Imagen4**：改进了 Gemini 应用中 Imagen4 的质量，该模型可提供逼真的视觉效果。\n*   **Vids 中的 AI 头像**：帮助评估模型和图像字幕的质量，使用户能够在几秒钟内创建带有 AI 头像的视频内容。\n*   **Gemini 2.5**：显著增强了 Gemini 2.5 模型的视频理解能力，特别是高运动理解，使其更能评估健康和健身领域的人体运动。\n\n## Sparkify：将任何问题转化为动画视频\n\nGoogle 团队支持推出了新的 Labs 实验 Sparkify。\n\n*   它结合了 Gemini、MusicLM、AudioLM 和 Veo 的强大功能。\n*   允许用户将任何问题或想法转化为选择设计风格的短小引人入胜的动画视频。\n*   该项目建立在底层模型及其事实性之上。\n\n## FireSat：实现更早地检测小型野火\n\n作为 Google 长期以来帮助减少野火破坏性影响的努力的一部分，Google Research 与 Earth Fire Alliance、Moore Foundation 和 Muon Space 合作开发了 FireSat。\n\n*   **FireSat 卫星星座**：\n    *   FireSat 是一个卫星星座，旨在实现更早、更准确的全球野火检测。\n    *   它使用高分辨率多光谱卫星图像和 AI 为急救人员提供近乎实时的洞察，并允许科学家和机器学习专家研究火灾蔓延。\n    *   3 月份，星座中的 50 多颗卫星中的第一颗已发射。\n*   **扩展现有工作**：\n    *   这项工作扩展了 Google 的野火边界跟踪（在搜索和地图中提供关键信息）和合成 Firebench 数据集（在 Google Cloud Platform 上发布以推进该领域的科学研究）。\n\n## 量子 AI：真实世界应用的切实潜力\n\n在 Dialogues 舞台上，WAYE 创始人 Sinead Bovell 和 Google 量子硬件团队高级总监 Julian Kelly 讨论了量子计算的前景以及仍需克服的工程和科学挑战。\n\n*   **量子 AI 进展**：\n    *   Julian 强调了 Google Research 量子 AI 团队的最新进展，包括 Willow 芯片和量子纠错等领域的进展。\n    *   经典计算机无法完成的计算可以在量子芯片上在几分钟内完成，为未来各种真实世界应用铺平道路。\n    *   彻底改变药物发现和能源效率等领域的潜力正变得越来越切实。\n*   **互动体验**：\n    *   Google 还为 I/O 现场的参与者创建了互动量子 AI 游戏体验：量子迷宫跑者。\n\n## AI 联合科学家：加速科学发现\n\nGoogle 的 AI 联合科学家（在 I/O 上提及，与 Google DeepMind 合作开发）是一个基于 Gemini 的多智能体系统。\n\n*   **功能与应用**：\n    *   它能够合成信息并执行复杂的推理任务。\n    *   它被设计为科学家的协作工具，帮助他们创建新颖的假设和研究提案，并加速生物医学发现。\n    *   它在急性髓系白血病的药物再利用和肝纤维化新治疗靶点的假设提出等领域展现了潜力。\n*   **其他科学研究加速努力**：\n    *   **地理空间推理**：旨在推进公共卫生、城市规划、综合业务规划、气候科学等。\n    *   **神经科学**：\n        *   LICONN：首个使用常用光学显微镜全面绘制脑组织中神经元及其连接的方法。\n        *   斑马鱼活动预测基准（ZAPBench）：首次允许研究人员调查整个脊椎动物大脑中结构连接和动态神经活动之间的关系。\n    *   **基因组学**：\n        *   REGLE：一种无监督深度学习模型，帮助研究人员发现与遗传变异的关联。\n        *   开源新的 DeepVariant 模型：作为个性化泛基因组参考合作的一部分，可将分析不同祖先基因组时的错误减少 30%。\n\n## 结论\n\n本文重点介绍的研究代表了 Google Research 团队正在进行的一些工作，这些团队正在推动各个领域的突破并将其变为现实。在这个研究的黄金时代，研究与现实世界应用之间的“魔力循环”越来越快，范围也越来越广，I/O 是展示这如何对人类、企业、科学和社会产生更大影响的绝佳机会。",
      "shortSummary": "Google I/O 2025 展示了 Google Research 的最新 AI 突破，将研究变为现实。亮点包括：医疗健康领域的 MedGemma 和 AMIE；教育领域的 LearnLM，使 Gemini 成为领先学习模型；Gemma 模型的跨语言和高效率；助力搜索中 AI 模式的改进；多模态事实性提升 Imagen4 和 Gemini 2.5；创意工具 Sparkify；用于早期野火检测的 FireSat 卫星；量子 AI 的实际应用潜力；以及通过 AI 联合科学家加速科学发现，涵盖地理空间、神经科学和基因组学等领域。",
      "translated_title": "Google 研究在 Google I/O 2025",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-2-CostPerformance.width-1250.png",
          "alt": "GRatIO25-2-CostPerformance",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-3-Pedagogy.width-1250.png",
          "alt": "GRatIO25-3-Pedagogy",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wrbp3\">Each year at Google I/O, we share some of Google’s most advanced technologies. We show how they can be helpful and provide new experiences, and how developers and other communities can use them to innovate. Many of these new technologies emerged from years of work within Google Research, many in collaboration with other teams, building on multiple successive breakthroughs in AI and other areas of computer science. This year’s I/O highlights the impact of bringing <a href=\"https://blog.google/technology/ai/io-2025-keynote/\" target=\"_blank\" rel=\"noopener noreferrer\">research to reality</a>. As Sundar put it: “What all this progress means is that we’re in a new phase of the AI platform shift. Where decades of research are now becoming reality for people, businesses and communities all over the world.”</p><p data-block-key=\"dbi8g\">In addition to Google Research’s contributions to Gemini and the generative AI products highlighted on the I/O stage, here are some of our favorites this year, tapping into years-long efforts from Google Research to realize the <a href=\"https://blog.google/technology/research/what-is-google-research/\" target=\"_blank\" rel=\"noopener noreferrer\">magic cycle</a> of research.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">MedGemma and AMIE: Advancing healthcare with AI</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">Since we first introduced <a href=\"https://www.nature.com/articles/s41586-023-06291-2\" target=\"_blank\" rel=\"noopener noreferrer\">Med-PaLM</a> in 2022, followed by <a href=\"https://www.nature.com/articles/s41591-024-03423-7\" target=\"_blank\" rel=\"noopener noreferrer\">Med-PaLM2</a> and <a href=\"https://research.google/blog/advancing-medical-ai-with-med-gemini/\">Med-Gemini</a>, our research teams have been continuously advancing AI to make healthcare more accessible and effective. At I/O, we announced <a href=\"https://blog.google/technology/developers/google-ai-developer-updates-io-2025/\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma</a>, Google’s most capable open model for multimodal medical text and image comprehension. It has the potential to speed up the development of new healthcare products.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wrbp3\">MedGemma is based on <a href=\"https://blog.google/technology/developers/gemma-3/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma 3</a> and designed to be a starting point for developers building health applications, such as analyzing radiology images or summarizing clinical data. Its small size makes it efficient for fine-tuning for specific needs, and when evaluated on the <a href=\"https://arxiv.org/pdf/2009.13081\" target=\"_blank\" rel=\"noopener noreferrer\">MedQA benchmark</a>, its baseline performance on clinical knowledge and reasoning tasks is similar to that of much larger models. Since MedGemma is open, it can be run in the developer’s preferred environment, including on the Google Cloud Platform or locally. Both MedGemma 4B and the 27B text-only models are <a href=\"https://developers.google.com/health-ai-developer-foundations/medgemma\" target=\"_blank\" rel=\"noopener noreferrer\">now available</a> on HuggingFace and Vertex Model Garden as part of our <a href=\"http://goo.gle/hai-def\" target=\"_blank\" rel=\"noopener noreferrer\">Health AI Developer Foundations</a> (HAI-DEF).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-2-CostPerformance.width-1250.png\" alt=\"GRatIO25-2-CostPerformance\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-2-CostPerformance.width-1250.png\" alt=\"GRatIO25-2-CostPerformance\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"xpbds\"><i>MedGemma’s baseline performance on clinical knowledge and reasoning tasks is similar to that of much larger models.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wrbp3\">MedGemma follows our recent announcement about AMIE, developed in collaboration with Google DeepMind, that was also highlighted at I/O. <a href=\"https://research.google/blog/amie-a-research-ai-system-for-diagnostic-medical-reasoning-and-conversations/\">AMIE</a> is a research <a href=\"https://www.nature.com/articles/s41586-025-08869-4\" target=\"_blank\" rel=\"noopener noreferrer\">AI agent</a> for medical <a href=\"https://www.nature.com/articles/s41586-025-08866-7\" target=\"_blank\" rel=\"noopener noreferrer\">diagnostic conversations</a>. The new <a href=\"https://research.google/blog/amie-gains-vision-a-research-ai-agent-for-multi-modal-diagnostic-dialogue/\">multimodal version</a> can intelligently interpret and reason about visual medical information, helping clinicians towards more accurate diagnoses.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">LearnLM: Making Gemini the world’s leading model for learning</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">For nearly two years, our teams at Google Research and across Google have been collaborating with educational experts on <a href=\"https://storage.googleapis.com/deepmind-media/LearnLM/LearnLM_paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">LearnLM</a>, a family of fine-tuned models for learning. At I/O we announced that LearnLM will now be <a href=\"https://cloud.google.com/solutions/learnlm?e=48754805&amp;hl=en\" target=\"_blank\" rel=\"noopener noreferrer\">available</a> directly in Gemini 2.5, making it the world’s <a href=\"https://blog.google/outreach-initiatives/education/google-gemini-learnlm-update/\" target=\"_blank\" rel=\"noopener noreferrer\">leading model for learning</a>. Our latest <a href=\"https://storage.googleapis.com/deepmind-media/LearnLM/learnLM_may25.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">tech report</a> demonstrates that Gemini 2.5 Pro outperforms alternative models on learning science principles and is the preferred choice for educators. It has advanced STEM reasoning, multimodal understanding, quizzing and assessment capabilities, and much more.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-3-Pedagogy.width-1250.png\" alt=\"GRatIO25-3-Pedagogy\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-3-Pedagogy.width-1250.png\" alt=\"GRatIO25-3-Pedagogy\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wrbp3\">We also launched a new <a href=\"https://blog.google/products/gemini/gemini-app-updates-io-2025/#quizzes\" target=\"_blank\" rel=\"noopener noreferrer\">quiz experience in Gemini</a>, which our Research team helped to design and optimize for learning. Students (ages 18+) can ask Gemini to create custom quizzes to help them study any topic, based on their class notes or course documents, and it will offer feedback and explanations about right and wrong answers.</p><p data-block-key=\"enldb\">Explore our <a href=\"https://services.google.com/fh/files/misc/learnlm_prompt_guide.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">LearnLM prompt guide</a> to maximize the pedagogical value of Gemini, for example, by asking it to act as a biology teacher or to adjust the difficulty level of text for a particular school grade.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        <div class=\"glue-ambient-video \">\n            <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/GRatIO25-4-Releveling.mp4\" type=\"video/mp4\">\n            </video>\n            <div class=\"glue-ambient-video__button glue-ambient-video__button--paused\" aria-label=\"Video Play/pause\">\n                <div class=\"glue-ambient-video__tooltip\">\n                  <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                  <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                </div>\n                <div class=\"glue-ambient-video__icon\">\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                </div>\n              </div>\n        </div>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wrbp3\">As well as infusing pedagogy into Google products, we’re working with partners to bring the powerful capabilities of our LearnLM models to educational settings. Along with <a href=\"https://kayma.com/kayma-english.html\" target=\"_blank\" rel=\"noopener noreferrer\">Kayma</a>, we piloted the <a href=\"http://milgo.io/en\" target=\"_blank\" rel=\"noopener noreferrer\">automatic assessment</a> of both short and long-form content with thousands of students and educators in high schools in Ghana, and we’re working to scale to more students and countries.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Multilinguality and Efficiency in Gemma: Making our models accessible and useful for everyone</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">As part of Google’s mission to make the world’s information universally accessible, we are advancing research into multilinguality to ensure that LLMs produce reliable outputs in different languages and are truly useful for everyone around the world. Two months ago, Google introduced <a href=\"https://blog.google/technology/developers/gemma-3/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma3</a>, and our research helped Gemma expand to over 140 languages, making it today’s best multilingual open model. At I/O, we announced that these capabilities are now available with the latest addition to the Gemmaverse, Gemma3n, a model that can run on as little as two gigabytes of RAM and is built for on-device applications. Our efforts around efficiency enable the Gemma3n model to reduce latency and be more energy consumption friendly.</p><p data-block-key=\"48b9m\">To help developers build and improve multilingual models, Google Research recently introduced <a href=\"https://research.google/blog/eclektic-a-novel-benchmark-for-evaluating-cross-lingual-knowledge-transfer-in-llms/\">ECLeKTic</a>, a novel benchmark for evaluating cross-lingual knowledge transfer in LLMs.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Efficient and grounded models: Contributing to AI Mode in Search</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">As LLMs grow larger and demand increases, our ability to improve model efficiency while maintaining and even elevating their quality determines our success in democratizing access to these high-performing models. Google Research has made breakthroughs in efficiency that have become industry standards, for example, our work on <a href=\"https://research.google/blog/looking-back-at-speculative-decoding/\">speculative decoding</a> and <a href=\"https://arxiv.org/pdf/2307.02764\" target=\"_blank\" rel=\"noopener noreferrer\">cascades</a>.</p><p data-block-key=\"d9btd\">We have published research on factual consistency <a href=\"https://arxiv.org/abs/2306.00186\" target=\"_blank\" rel=\"noopener noreferrer\">techniques</a> and <a href=\"https://arxiv.org/abs/2204.04991\" target=\"_blank\" rel=\"noopener noreferrer\">evaluations</a>, and set the bar on factuality and grounding with features like <a href=\"https://blog.google/products/gemini/google-gemini-new-features-july-2024/\" target=\"_blank\" rel=\"noopener noreferrer\">double-check</a> and the <a href=\"https://storage.googleapis.com/deepmind-media/FACTS/FACTS_grounding_paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">FACTS Grounding leaderboard</a>, released in collaboration with Google DeepMind and Kaggle. Now, we have contributed our research to <a href=\"https://blog.google/products/search/google-search-ai-mode-update/#ai-mode-search\" target=\"_blank\" rel=\"noopener noreferrer\">AI Mode</a>, to meaningfully improve the experience for users.</p><p data-block-key=\"3kd8n\">Announced at I/O, AI Mode is Google’s most powerful AI search yet with advanced reasoning capabilities. It is rolling out to all users in the U.S., allowing people to conduct deeper research with follow-up questions and links to relevant sites. Our work on efficiency enables the models to run more reliably and serve quicker outputs, and our factuality research has improved the way AI Mode searches the web, helping to ensure that the answers provided are highly accurate and grounded in multiple sources with relevant links.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Multimodal factuality: Contributing to Imagen4, Gemini 2.5, and AI Avatars in Vids</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">As multimodal content becomes ubiquitous, our factuality team is advancing research around multimodal factuality to ensure high accuracy standards across Google products. We improved the quality of <a href=\"https://blog.google/products/gemini/gemini-app-updates-io-2025/#imagen-4-veo-3\" target=\"_blank\" rel=\"noopener noreferrer\">Imagen4</a> on the Gemini app, the latest image model announced at I/O, which can deliver visuals with lifelike detail. For <a href=\"https://www.youtube.com/watch?v=X2G3utAQuAU\" target=\"_blank\" rel=\"noopener noreferrer\">AI avatars</a> in Vids, a new feature that enables users to create video content with their chosen AI avatars in a matter of seconds, we helped to evaluate the quality of the model and image captions. We also delivered significant enhancements to the video understanding capabilities of Gemini 2.5 models, specifically targeting high motion understanding so that Gemini is more capable of assessing human motion across health and fitness domains.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Sparkify: Turning any question into an animated video</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">Our teams helped support the <a href=\"https://blog.google/outreach-initiatives/education/google-gemini-learnlm-update/\" target=\"_blank\" rel=\"noopener noreferrer\">launch</a> of the new Labs experiment, <a href=\"https://sparkify.withgoogle.com/explore\" target=\"_blank\" rel=\"noopener noreferrer\">Sparkify</a>. Bringing together the power of Gemini, <a href=\"https://musiclm.com/\" target=\"_blank\" rel=\"noopener noreferrer\">MusicLM</a>, <a href=\"https://research.google/blog/audiolm-a-language-modeling-approach-to-audio-generation/\">AudioLM</a>, and <a href=\"https://deepmind.google/models/veo/\" target=\"_blank\" rel=\"noopener noreferrer\">Veo</a>, Sparkify allows users to turn any question or idea into a short and engaging animated video in the design style of their choice. The project builds on the underlying models and their factuality. <a href=\"http://sparkify.withgoogle.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Sign up for the waitlist</a> for a chance to try it out.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme- --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n    <div data-gt-id=\"media\" data-gt-component-name=\"\">\n        \n\n        \n\n\n\n\n        \n            \n                \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"lstWBbY-Iqk\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=lstWBbY-Iqk\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n                \n            \n        \n    </div>\n\n\n\n    </div>\n</section>\n\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">FireSat: Enabling the detection of smaller wildfires earlier</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">As part of our long-established <a href=\"https://sites.research.google/gr/wildfires/\">efforts</a> to help reduce the devastating impacts of wildfires, Google Research has partnered with the <a href=\"https://www.earthfirealliance.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Earth Fire Alliance</a>, the <a href=\"https://www.moore.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Moore Foundation</a> and <a href=\"https://www.muonspace.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Muon Space</a> to develop <a href=\"https://sites.research.google/gr/wildfires/firesat/\">FireSat</a>. FireSat is a constellation of satellites built for earlier and more accurate global wildfire detection. It uses high-res multispectral <a href=\"https://blog.google/outreach-initiatives/sustainability/google-ai-wildfire-detection/\" target=\"_blank\" rel=\"noopener noreferrer\">satellite imagery and AI</a> to provide near real-time insights for first responders, and to allow scientists and ML experts to study fire propagation. In March, we launched the first of over 50 satellites in the constellation. This work expands upon our <a href=\"https://blog.google/outreach-initiatives/sustainability/google-wildfire-boundary-maps-europe-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">wildfire boundary tracking</a>, which makes critical information available in Search and Maps, and the synthetic <a href=\"https://arxiv.org/abs/2406.08589\" target=\"_blank\" rel=\"noopener noreferrer\">Firebench dataset</a>, which we released on the <a href=\"https://github.com/google-research/firebench\" target=\"_blank\" rel=\"noopener noreferrer\">Google Cloud Platform</a> to advance scientific research in the field.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        <div class=\"glue-ambient-video \">\n            <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/GRatIO25-6-FireSat.mp4\" type=\"video/mp4\">\n            </video>\n            <div class=\"glue-ambient-video__button glue-ambient-video__button--paused\" aria-label=\"Video Play/pause\">\n                <div class=\"glue-ambient-video__tooltip\">\n                  <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                  <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                </div>\n                <div class=\"glue-ambient-video__icon\">\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                </div>\n              </div>\n        </div>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"xpbds\"><i>FireSat is the first satellite constellation for the early detection of wildfires in high resolution imagery.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Quantum AI: Tangible potential for real-world applications</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">On the Dialogues Stage, Sinead Bovell, founder of WAYE, and Julian Kelly, Senior Director, from our Quantum Hardware team, discussed the promise of quantum computing and the engineering and scientific challenges that still need to be overcome. Julian highlighted the recent advances from Google Research's <a href=\"https://quantumai.google/\" target=\"_blank\" rel=\"noopener noreferrer\">Quantum AI</a> team, including our <a href=\"https://blog.google/technology/research/google-willow-quantum-chip/\" target=\"_blank\" rel=\"noopener noreferrer\">Willow chip</a> and progress in areas like <a href=\"https://research.google/blog/making-quantum-error-correction-work/\">quantum error correction</a>. Computations that are beyond reach for classical computers can be completed on a quantum chip in minutes, paving the way for various <a href=\"https://blog.google/technology/research/google-quantum-computer-real-world-applications/\" target=\"_blank\" rel=\"noopener noreferrer\">real-world applications</a> in the future. The potential to revolutionize fields like drug discovery and energy efficiency is becoming increasingly tangible.</p><p data-block-key=\"b55a5\">We also created an interactive Quantum AI game experience for attendees on the ground at I/O: the Quantum Maze Runner. Players had to race against the clock to complete the maze, and then see how the quantum computer would solve it.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">AI Co-Scientist: Accelerating scientific discovery</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">Our <a href=\"https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/\">AI co-scientist,</a> mentioned at I/O and developed in collaboration with Google DeepMind, is a multi-agent system based on Gemini that can synthesize information and perform complex reasoning tasks. It is designed as a collaborative tool for scientists, to aid them in creating novel hypotheses and research proposals, and to help accelerate biomedical discoveries. It has <a href=\"https://arxiv.org/abs/2502.18864\" target=\"_blank\" rel=\"noopener noreferrer\">demonstrated</a> potential in areas such as drug repurposing for acute myeloid leukemia and proposing hypotheses for novel treatment targets for liver fibrosis.</p><p data-block-key=\"biv0h\">It is one of our many efforts to <a href=\"https://blog.google/technology/research/google-research-scientific-discovery/\" target=\"_blank\" rel=\"noopener noreferrer\">accelerate scientific research</a> across the wider ecosystem. Our new <a href=\"https://research.google/blog/geospatial-reasoning-unlocking-insights-with-generative-ai-and-multiple-foundation-models/\">Geospatial Reasoning</a> initiative aims to advance public health, urban planning, integrated business planning, climate science and more. We’re also advancing neuroscience, with our recent publication on <a href=\"https://www.nature.com/articles/s41586-025-08985-1\" target=\"_blank\" rel=\"noopener noreferrer\">LICONN,</a> the first-ever method for using commonly available light microscopes to comprehensively map neurons and their connections in brain tissue, and with the release of the <a href=\"https://research.google/blog/improving-brain-models-with-zapbench/\">Zebrafish Activity Prediction Benchmark</a> (ZAPBench), which allows researchers to <a href=\"https://arxiv.org/abs/2503.02618\" target=\"_blank\" rel=\"noopener noreferrer\">investigate the relationship</a> between the structural wiring and dynamic neural activity across an entire vertebrate brain for the first time. We’re also advancing research into genomics to help diagnose rare diseases; <a href=\"https://www.nature.com/articles/s41588-024-01831-6\" target=\"_blank\" rel=\"noopener noreferrer\">REGLE</a> is an unsupervised deep learning model that helps researchers discover associations with genetic variants. And we open sourced new <a href=\"https://github.com/google/deepvariant/blob/r1.8/docs/pangenome-aware-wgs-vg-case-study.md\" target=\"_blank\" rel=\"noopener noreferrer\">DeepVariant models</a> as part of a collaboration on <a href=\"https://www.nature.com/articles/s41592-024-02407-2\" target=\"_blank\" rel=\"noopener noreferrer\">Personalized Pangenome References</a>, which can reduce errors by 30% when analyzing genomes of diverse ancestries.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">The research highlighted here represents some of the ongoing work done by the Google Research teams who are driving breakthroughs across a variety of fields and bringing them to reality. In this <a href=\"https://blog.google/technology/research/what-is-google-research/\" target=\"_blank\" rel=\"noopener noreferrer\">golden age of research</a>, the “magic cycle” between research and real-world application is increasingly faster and broader in scope, and I/O was a great opportunity to showcase how this leads to greater impact on people, businesses, science and society.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\"><i>With thanks to the many teams and collaborators who have contributed to this blog and the work represented here.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "深入理解检索增强生成：充足上下文的作用 (原标题: Deeper insights into retrieval augmented generation: The role of sufficient context)",
      "link": "https://research.google/blog/deeper-insights-into-retrieval-augmented-generation-the-role-of-sufficient-context/",
      "pubDate": "Tue, 13 May 2025 16:00:00 GMT",
      "isoDate": "2025-05-13T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 深入理解检索增强生成：充足上下文的作用\n\n### 引言：RAG的挑战与“充足上下文”的提出\n\n检索增强生成（RAG）系统通过向大型语言模型（LLM）提供相关的外部上下文来增强其能力，例如在问答任务中整合来自网页、文档库或知识图谱的信息。理想情况下，LLM应给出正确答案，或在信息不足时回应“我不知道”。然而，RAG系统面临的主要挑战是可能产生幻觉（即不正确的信息），误导用户。以往的研究多关注上下文的“相关性”，但本文认为更关键的是上下文是否提供了“足够的信息”来回答问题。\n\n在ICLR 2025发表的论文《充足上下文：检索增强生成系统的新视角》中，研究团队深入探讨了RAG系统中的“充足上下文”概念。他们证明了可以判断LLM何时拥有足够信息来提供正确答案，并量化了上下文充足性对事实准确性的影响，从而分析RAG系统成功或失败的因素。\n\n此外，这些研究成果已被应用于Vertex AI RAG引擎中的LLM Re-Ranker功能，该功能允许用户根据查询相关性重新排序检索到的片段，从而提升检索指标（如nDCG）和RAG系统准确性。\n\n![SufficientContext2_RAG](https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext2_RAG.width-1250.png)\n*RAG系统中，LLM利用检索到的上下文对输入问题提供响应。* \n\n### 核心概念：充足上下文的定义\n\n研究将上下文定义为“充足”：如果它包含提供查询明确答案所需的所有必要信息。反之，如果上下文缺乏必要信息、不完整、不确定或包含矛盾信息，则定义为“不充足”。\n\n**示例：**\n*   **输入查询：** “页面未找到”的错误代码以404房间命名，该房间曾存储错误消息的中央数据库，位于哪个著名实验室？\n*   **充足上下文：** “页面未找到”错误，通常显示为404代码，以欧洲核子研究组织（CERN）的404房间命名。这是存储包括页面未找到错误在内的中央错误消息数据库的房间。\n*   **不充足上下文：** 404错误或“页面未找到”错误表示Web服务器找不到请求的页面。这可能由于URL中的拼写错误、页面被移动或删除，或网站的临时问题等各种原因。\n\n第二个上下文虽然与用户查询高度相关，但并未回答问题，因此被视为不充足。\n\n### 开发充足上下文自动评估器\n\n为了量化上下文充足性，研究团队首先开发了一个基于LLM的自动评估器（“autorater”），用于评估查询-上下文对。为了验证自动评估器的有效性，他们首先请人类专家分析了115个问答-上下文示例，以确定上下文是否足以回答问题，这成为了“黄金标准”。随后，LLM对相同的问答和上下文进行评估，输出“true”（充足）或“false”（不充足）。\n\n为优化模型的任务解决能力，研究团队通过思维链提示和提供单样本示例等策略改进了提示词。结果显示，使用优化后的提示词，模型能够以极高的准确率（至少93%）对充足上下文进行分类。表现最佳的方法是未经微调的Gemini 1.5 Pro。与依赖真实答案的方法（如TRUE-NLI和“Contains GT”）相比，Gemini表现更优，这可能归因于其卓越的语言理解能力。\n\n![SufficientContext3_AutoRater](https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext3_AutoRater.width-1250.png)\n*我们的充足上下文自动评估方法。我们使用一个经过提示的LLM来评估由输入查询和检索到的上下文组成的示例。模型输出一个二元真/假标签，表示充足与不充足上下文。*\n\n![SufficientContext4_Accuracy](https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext4_Accuracy.width-1250.png)\n*分类充足上下文的准确率，衡量各种自动方法与人工标注标签之间的一致性。*\n\n### RAG系统的关键洞察\n\n利用充足上下文自动评估器，研究团队分析了各种LLM和数据集的性能，得出以下关键发现：\n\n*   **最先进的大型模型：** 如Gemini、GPT和Claude，在提供充足上下文时通常能很好地回答查询，但缺乏识别并在上下文不充足时避免生成错误答案的能力。\n*   **小型开源模型：** 分析揭示，即使上下文足以正确回答问题，开源模型仍存在较高的幻觉和拒绝回答率。\n*   **不充足上下文的价值：** 有时模型在上下文被评为不充足时也能生成正确答案。这表明不充足上下文仍可能有用，例如弥补模型知识空白或澄清查询歧义。\n*   **可操作的建议：** 基于这些发现，研究团队提出了改进RAG系统的建议，包括在生成前添加充足性检查、检索更多上下文或重新排序、以及根据置信度和上下文信号调整拒绝回答阈值。\n\n### 充足上下文背后的研究：数据集分析\n\n研究分析表明，许多标准基准数据集包含大量不充足上下文的实例，例如FreshQA、HotPotQA和MuSiQue。上下文充足实例比例较高的数据集（如FreshQA）往往是那些上下文来源于人工整理的支持文档的数据集。\n\n![SufficientContext5_Sufficiency](https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext5_Sufficiency.width-1250.png)\n*比较三个数据集（x轴）中包含充足上下文的示例百分比（y轴）。*\n\n### 悖论：添加上下文反而增加幻觉\n\n一个令人惊讶的发现是，尽管RAG通常能提高整体性能，但它却矛盾地降低了模型在适当情况下拒绝回答的能力。引入额外上下文似乎增加了模型的信心，导致其更倾向于产生幻觉而非拒绝回答。\n\n为了理解这一现象，研究团队使用Gemini评估每个模型响应，并将其与可能的真实答案集进行比较。他们将每个响应分类为“正确”、“幻觉”（即不正确答案）或“拒绝回答”（例如，说“我不知道”）。通过这种方法，他们发现，例如，Gemma在没有上下文时错误回答问题的比例为10.2%，而在使用不充足上下文时，这一比例上升到66.1%。\n\n![SufficientContext6_Analysis](https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext6_Analysis.width-1250.png)\n*在四种不同RAG设置下对三个LLM的详细分析。*\n\n### 选择性生成以减少幻觉\n\n作为另一项贡献，研究团队开发了一个“选择性生成”框架，该框架利用充足上下文信息来指导拒绝回答。他们考虑了以下指标：选择性准确率（模型在回答的问题中正确答案的比例）和覆盖率（回答问题的比例）。\n\n选择性生成方法结合了充足上下文信号和模型自评置信度分数，以做出何时拒绝回答的明智决策。这比简单地在上下文不充足时拒绝回答更为精细，因为模型有时即使在有限上下文下也能提供正确答案。研究团队利用这些信号训练了一个逻辑回归模型来预测幻觉，然后设置了一个覆盖率-准确率权衡阈值，决定模型何时应拒绝回答。\n\n该方法使用两个主要信号进行拒绝回答：\n\n1.  **自评置信度：** 采用两种策略：P(True)（多次采样答案并提示模型标记每个样本为正确或不正确）和P(Correct)（用于查询成本较高的模型，获取模型响应及其估计的正确概率）。\n2.  **充足上下文信号：** 使用来自自动评估器模型（FLAMe）的二元标签来指示上下文是否充足。关键在于，确定充足上下文标签不需要真实答案，因此可以在回答问题时使用此信号。\n\n结果表明，与仅使用模型置信度相比，这种方法在选择性准确率-覆盖率权衡方面表现更优。通过使用充足上下文标签，模型在回答问题上的准确率有时可提高高达10%。\n\n![SufficientContext8_ResultsFinal](https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext8_ResultsFinal.width-1250.png)\n*选择性准确率（即在回答问题中的准确率）与覆盖率（即回答问题的比例）。*\n\n### 结论\n\n这项工作通过引入和利用“充足上下文”的概念，为分析和增强RAG系统提供了新的见解和有价值的工具。研究表明，RAG系统中的幻觉可能源于上下文不充足，并且选择性生成可以有效缓解这一问题。未来的工作将分析不同检索方法如何影响上下文充足性，并探索如何利用检索质量信号来改进模型后训练。",
      "shortSummary": "该研究深入探讨了检索增强生成（RAG）系统中大型语言模型（LLM）的幻觉问题，指出其根源在于上下文的“充足性”而非仅仅“相关性”。文章定义了“充足上下文”，并开发了一个基于LLM的自动评估器，能以高准确率判断上下文是否充足。研究发现，即使添加上下文，LLM仍可能因上下文不充足而产生更多幻觉。为解决此问题，提出了“选择性生成”框架，结合上下文充足信号和模型置信度，有效减少了幻觉并提高了RAG系统的准确性。",
      "translated_title": "深入理解检索增强生成：充足上下文的作用",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext2_RAG.width-1250.png",
          "alt": "SufficientContext2_RAG",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext3_AutoRater.width-1250.png",
          "alt": "SufficientContext3_AutoRater",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext4_Accuracy.width-1250.png",
          "alt": "SufficientContext4_Accuracy",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext5_Sufficiency.width-1250.png",
          "alt": "SufficientContext5_Sufficiency",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext6_Analysis.width-1250.png",
          "alt": "SufficientContext6_Analysis",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"766t5\"><a href=\"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\" target=\"_blank\" rel=\"noopener noreferrer\">Retrieval augmented generation</a> (RAG) enhances large language models (LLMs) by providing them with relevant external context. For example, when using a RAG system for a question-answer (QA) task, the LLM receives a context that may be a combination of information from multiple sources, such as public webpages, private document corpora, or knowledge graphs. Ideally, the LLM either produces the correct answer or responds with “I don’t know” if certain key information is lacking.</p><p data-block-key=\"brfo6\">A main challenge with RAG systems is that they may mislead the user with <i>hallucinated</i> (and therefore incorrect) information. Another challenge is that most prior work only considers how <i>relevant</i> the context is to the user query. But we believe that the context’s relevance alone is the wrong thing to measure — we really want to know whether it provides enough information for the LLM to answer the question or not.</p><p data-block-key=\"dou1v\">In “<a href=\"https://arxiv.org/abs/2411.06037\" target=\"_blank\" rel=\"noopener noreferrer\">Sufficient Context: A New Lens on Retrieval Augmented Generation Systems</a>”, which appeared at <a href=\"https://iclr.cc/\" target=\"_blank\" rel=\"noopener noreferrer\">ICLR 2025</a>, we study the idea of \"sufficient context” in RAG systems. We show that it’s possible to know when an LLM has enough information to provide a correct answer to a question. We study the role that context (or lack thereof) plays in factual accuracy, and develop a way to quantify context sufficiency for LLMs. Our approach allows us to investigate the factors that influence the performance of RAG systems and to analyze when and why they succeed or fail.</p><p data-block-key=\"9jr23\">Moreover, we have used these ideas to launch the <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/retrieval-and-ranking#llm_reranker\" target=\"_blank\" rel=\"noopener noreferrer\">LLM Re-Ranker</a> in the <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/rag-overview\" target=\"_blank\" rel=\"noopener noreferrer\">Vertex AI RAG Engine</a>. Our feature allows users to re-rank retrieved snippets based on their relevance to the query, leading to better retrieval metrics (e.g., <a href=\"https://en.wikipedia.org/wiki/Discounted_cumulative_gain\" target=\"_blank\" rel=\"noopener noreferrer\">nDCG</a>) and better RAG system accuracy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext2_RAG.width-1250.png\" alt=\"SufficientContext2_RAG\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext2_RAG.width-1250.png\" alt=\"SufficientContext2_RAG\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"bzxw5\"><i>In a RAG system, an LLM uses retrieved context to provide a response to the input question.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"766t5\">Main conceptual contribution: Sufficient context</h2><p data-block-key=\"feoif\">We define context as “sufficient” if it contains all the necessary information to provide a definitive answer to the query and “insufficient” if it lacks the necessary information, is incomplete, inconclusive, or contains contradictory information. For example:</p><p data-block-key=\"6vl6g\"><i>Input query:</i> The error code for “Page Not Found” is named after room 404, which had stored the central database of error messages in what famous laboratory?</p><ul><li data-block-key=\"shs1\"><i>Sufficient context:</i> The “Page Not Found” error, often displayed as a 404 code, is named after Room 404 at CERN, the European Organization for Nuclear Research. This was the room where the central database of error messages was stored, including the one for a page not being found.</li><li data-block-key=\"6i6tu\"><i>Insufficient context:</i> A 404 error, or “Page Not Found” error, indicates that the web server cannot find the requested page. This can happen due to various reasons, including typos in the URL, a page being moved or deleted, or temporary issues with the website.</li></ul><p data-block-key=\"6n82k\">The second context is very relevant to the user’s query, but it does not answer the question, and hence it is insufficient.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"766t5\">Developing a sufficient context autorater</h2><p data-block-key=\"6d4og\">With this definition, we first develop an LLM-based automatic rater (“autorater”) that evaluates query-context pairs. To evaluate the autorater, we first had human experts analyze 115 question-and-context examples to determine if the context was sufficient to answer the question. This became the “gold standard” to which we compared the LLM’s judgements. Then, we had the LLM evaluate the same questions and contexts, where it outputs either “true” for sufficient context or “false” for insufficient context.</p><p data-block-key=\"9qrhu\">To optimize the model’s ability to solve this task, we also improved the prompt with various <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-design-strategies\" target=\"_blank\" rel=\"noopener noreferrer\">prompting strategies</a>, such as chain-of-thought prompting and providing a 1-shot example. We then measured classification performance based on how often the LLM’s true/false labels matched the gold standard labels.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext3_AutoRater.width-1250.png\" alt=\"SufficientContext3_AutoRater\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext3_AutoRater.width-1250.png\" alt=\"SufficientContext3_AutoRater\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"bzxw5\"><i>Our automatic rating method (autorater) for sufficient context. We use a prompted LLM to rate examples consisting of an input query and the retrieved context. The model outputs a binary true/false label that represents sufficient vs. insufficient context.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"766t5\">Using our optimized prompt, we show that we can classify sufficient context with very high accuracy (at least 93% of the time). It also turns out that the best performing method we tried is a prompted Gemini 1.5 Pro, without any fine-tuning. As baselines, we show that <a href=\"https://arxiv.org/abs/2407.10817\" target=\"_blank\" rel=\"noopener noreferrer\">FLAMe</a> (fine-tuned PaLM 24B) is slightly worse than Gemini but could be a more computationally efficient alternative. We also compare our approach to methods that rely on ground truth answers, such as <a href=\"https://github.com/google-research/true\" target=\"_blank\" rel=\"noopener noreferrer\">TRUE-NLI</a> (a fine-tuned entailment model), and a method that checks if the ground truth answer appears in the context (\"Contains GT\"). Gemini outperforms the alternatives, likely because it has better language understanding abilities.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext4_Accuracy.width-1250.png\" alt=\"SufficientContext4_Accuracy\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext4_Accuracy.width-1250.png\" alt=\"SufficientContext4_Accuracy\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"bzxw5\"><i>Accuracy of classifying sufficient context, where we measure the agreement between various automatic methods to the human-annotated labels.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"766t5\">This autorater enables us to scalably label instances and analyze model responses based on sufficient vs. insufficient context.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"766t5\">Key insights into RAG systems</h2><p data-block-key=\"cus6j\">Using our sufficient context autorater, we analyzed the performance of various LLMs and datasets, leading to several key findings:</p><ul><li data-block-key=\"8fv0s\"><i>State-of-the-art large models:</i> Models like <a href=\"https://ai.google.dev/gemini-api/docs/models\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a>, <a href=\"https://platform.openai.com/docs/models\" target=\"_blank\" rel=\"noopener noreferrer\">GPT</a>, and <a href=\"https://claude.ai/login?returnTo=%2F%3F\" target=\"_blank\" rel=\"noopener noreferrer\">Claude</a> generally excel at answering queries when provided with sufficient context but lack the ability to recognize and avoid generating incorrect answers when the provided context is insufficient.</li><li data-block-key=\"2bmoi\"><i>Smaller open-source models:</i> Our analysis uncovers the specific issue that open-source models have a high rate of hallucination and abstention even when the context is sufficient to answer the question correctly.</li><li data-block-key=\"4batf\"><i>Insufficient context</i>: Sometimes models generate correct answers when we rate the context as insufficient. This highlights the fact that insufficient context can still be useful. For example, it can bridge gaps in the model's knowledge or clarify ambiguities in the query.</li></ul><p data-block-key=\"5johu\"><i>Actionable insights:</i> Based on our findings, we now have recommendations for how to improve RAG systems. For example, it could be beneficial to (i) add a sufficiency check before generation, (ii) retrieve more context or re-rank the retrieved contexts, or (iii) tune an abstention threshold with confidence and context signals.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"766t5\">Diving into the research behind sufficient context</h2><p data-block-key=\"d09lm\">Our analysis reveals that several standard benchmark datasets contain many instances with insufficient context. We consider three datasets: <a href=\"https://github.com/freshllms/freshqa\" target=\"_blank\" rel=\"noopener noreferrer\">FreshQA</a>, <a href=\"https://hotpotqa.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">HotPotQA</a>, and <a href=\"https://paperswithcode.com/dataset/musique-ans\" target=\"_blank\" rel=\"noopener noreferrer\">MuSiQue</a>. Datasets with a higher percentage of sufficient context instances, such as FreshQA, tend to be those where the context is derived from human-curated supporting documents.</p><p data-block-key=\"79cha\"><a href=\"https://docs.google.com/spreadsheets/d/1pR1ETI3cX9_295HhfPfRUOHNlEYE5aJhbNFqN7DJ5Xk/edit?gid=334049794#gid=334049794\" target=\"_blank\" rel=\"noopener noreferrer\">Example 182 from FreshQA</a> is the question <i>“How many food allergens with mandatory labeling are there in the United States?”</i> This is a tricky question because the answer is nine, after <a href=\"https://www.fda.gov/food/food-allergies/faster-act-sesame-ninth-major-food-allergen\" target=\"_blank\" rel=\"noopener noreferrer\">sesame was added in 2023</a>. The supporting document in the dataset is the Wikipedia article on <a href=\"https://en.wikipedia.org/wiki/Food_allergy#Regulation_of_labelling\" target=\"_blank\" rel=\"noopener noreferrer\">Food Allergies</a>, where the table under “Regulation of labeling” provides the list of the nine allergens with mandatory labeling.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext5_Sufficiency.width-1250.png\" alt=\"SufficientContext5_Sufficiency\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext5_Sufficiency.width-1250.png\" alt=\"SufficientContext5_Sufficiency\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"bzxw5\"><i>Comparing the percentage of examples with sufficient context (y-axis) across three datasets (x-axis).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"766t5\">Adding context leads to more hallucinations</h2><p data-block-key=\"4h26e\">A surprising observation is that while RAG generally improves overall performance, it paradoxically reduces the model's ability to abstain from answering when appropriate. The introduction of additional context seems to increase the model's confidence, leading to a higher propensity for hallucination rather than abstention.</p><p data-block-key=\"79br5\">To understand this, we use Gemini to rate each model response by comparing it to the set of possible ground truth answers. We classify each response as “Correct”, “Hallucinate” (i.e., incorrect answer), or ”Abstain” (e.g., saying “I don’t know”). With this approach we find, for example, that Gemma goes from providing incorrect answers on 10.2% of questions with no context up to 66.1% when using insufficient context.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext6_Analysis.width-1250.png\" alt=\"SufficientContext6_Analysis\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext6_Analysis.width-1250.png\" alt=\"SufficientContext6_Analysis\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"bzxw5\"><i>Detailed analysis of three LLMs in four different RAG settings.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"766t5\">Selective generation to reduce hallucinations</h2><p data-block-key=\"fm6ak\">As another contribution, we develop a “selective generation” framework that leverages sufficient context information to guide abstention. We consider the following metrics: <i>selective accuracy</i> measures the model’s fraction of correct answers among the ones it answers, and <i>coverage</i> is the fraction of answered questions (non-abstentions).</p><p data-block-key=\"bbb7\">Our selective generation approach combines the sufficient context signal with the model's self-rated confidence scores to make informed decisions about when to abstain. This is more nuanced than simply abstaining whenever the context is insufficient, as models can sometimes provide correct answers even with limited context. We use these signals to train a logistic regression model to predict hallucinations. We then set a coverage-accuracy trade-off threshold, determining when the model should abstain from answering.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        <div class=\"glue-ambient-video \">\n            <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/SufficientContext7_PipelineFinal.mp4\" type=\"video/mp4\">\n            </video>\n            <div class=\"glue-ambient-video__button glue-ambient-video__button--paused\" aria-label=\"Video Play/pause\">\n                <div class=\"glue-ambient-video__tooltip\">\n                  <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                  <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                </div>\n                <div class=\"glue-ambient-video__icon\">\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                </div>\n              </div>\n        </div>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"bzxw5\"><i>The pipeline for our selective generation method to reduce hallucinations.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"766t5\">We use two main signals for abstention:</p><ul><li data-block-key=\"8faa0\"><i>Self-rated confidence:</i> We use two strategies: <i>P</i>(True) and <i>P</i>(Correct). <i>P</i>(True) involves sampling answers multiple times and prompting the model to label each sample as correct or incorrect. <i>P</i>(Correct) is used for models where extensive querying is expensive; it involves obtaining the model's response and its estimated probability of correctness.</li><li data-block-key=\"av834\"><i>Sufficient context signal:</i> We use the binary label from an autorater model (FLAMe) to indicate whether the context is sufficient. Crucially, we do not need the ground truth answer to determine the sufficient context label, and so we can use this signal when answering the question.</li></ul><p data-block-key=\"aaf07\">Our results demonstrate that this approach leads to a better selective accuracy-coverage trade-off compared to using model confidence alone. By using the sufficient context label, we can increase the accuracy on the questions that the model answers, sometimes by up to 10% of the time.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext8_ResultsFinal.width-1250.png\" alt=\"SufficientContext8_ResultsFinal\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext8_ResultsFinal.width-1250.png\" alt=\"SufficientContext8_ResultsFinal\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"bzxw5\"><i>Selective accuracy (i.e., accuracy on the fraction of answered questions) vs. coverage (i.e., fraction of answered questions).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"766t5\">Conclusion</h2><p data-block-key=\"7f3r9\">Our work provides new insights into the behavior of LLMs within RAG systems. By introducing and utilizing the concept of sufficient context, we have developed a valuable tool for analyzing and enhancing these systems. We showed that hallucinations in RAG systems may be due to insufficient context, and we demonstrated that selective generation can mitigate this issue. Future work will analyze how different retrieval methods influence context sufficiency and explore how signals about retrieval quality can be used to improve model post-training.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"766t5\">Acknowledgments</h2><p data-block-key=\"1gba9\"><i>This work is in collaboration with Hailey Joren (lead student author), Jianyi Zhang, Chun-Sung Ferng, and Ankur Taly. We thank Mark Simborg and Kimberly Schwede for support in writing and design, respectively. We also thank Alyshia Olsen for help in designing the graphics.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "信任图上的差分隐私 (原标题: Differential privacy on trust graphs)",
      "link": "https://research.google/blog/differential-privacy-on-trust-graphs/",
      "pubDate": "Mon, 12 May 2025 16:00:00 GMT",
      "isoDate": "2025-05-12T16:00:00.000Z",
      "creator": "Google",
      "summary": "### 信任图上的差分隐私 (Trust Graph Differential Privacy, TGDP)\n\n本文介绍了信任图差分隐私（TGDP），这是一种新的分布式差分隐私（DP）模型，旨在更精细地控制参与者的信任假设，以适应现实世界中用户对数据共享的不同信任级别。\n\n#### 差分隐私（DP）概述\n\n*   **定义**：DP是一种数学上严谨且广泛研究的隐私框架，确保随机算法的输出在单个用户数据发生变化时仍具有统计上的不可区分性。\n*   **主要模型**：\n    *   **中心模型**：一个受信任的策展人访问原始数据并生成差分隐私输出。\n    *   **局部模型**：用户设备发送的所有消息本身就是差分隐私的，无需受信任的策展人。\n*   **局限性**：局部模型虽然信任要求最低，但通常会导致比中心模型显著更高的效用损失。现有DP模型通常采用二元信任假设，无法反映现实中用户基于关系的不同信任程度。\n\n#### 信任图差分隐私（TGDP）\n\n*   **动机**：现实世界中，用户对不同关系的人有不同的信任级别（例如，与家人分享位置数据，但不与陌生人分享）。这种不对称性促使需要超越二元信任假设的隐私框架。\n*   **模型构建**：\n    *   使用**信任图**来建模关系，其中顶点代表用户，连接的顶点表示相互信任。\n    *   **隐私保证**：TGDP确保用户（或其受信任邻居）与任何不受信任的用户之间交换的消息分布在统计上不可区分，即使该用户的输入数据发生变化。\n    *   **示例**：\n        ![DPTrustGraph1_ExampleHero](https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph1_ExampleHero.width-1250.png)\n        *信任图与位置共享示例。Alice与Bob共享位置，Bob与Carol和Dave共享，Carol和Dave与Eve共享。根据TGDP定义，Carol、Dave和Eve即使汇集所有数据，也无法识别Alice的信息。*\n    *   **插值作用**：TGDP自然地介于中心模型和局部模型之间：\n        *   **中心模型**：等同于星形图上的TGDP，中心是受信任的策展人，连接所有其他用户。\n        *   **局部模型**：等同于完全不连通图上的TGDP，其中没有用户信任其他用户。\n        ![DPTrustGraph2_Comparison](https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph2_Comparison.width-1250.png)\n        *左图：对应于中心DP模型的“星形”信任图，所有用户信任一个中心策展人。右图：对应于局部DP模型的完全不连通信任图，没有用户信任其他用户。*\n*   **精度量化**：\n    *   已知中心DP算法比局部DP算法更准确。TGDP允许在更一般的信任关系下量化算法精度。\n    *   通过一个简单的**聚合任务**来量化精度：估计所有用户私有数据值（实数）的总和，目标是最小化误差（均方误差）。\n\n#### TGDP算法与误差界限\n\n本文为满足TGDP的聚合算法提供了误差的上限和下限。\n\n1.  **基于支配集的算法（上限）**：\n    *   **支配集（Dominating Set）**：图顶点的一个子集T，使得T中每个不在T中的顶点都与T中至少一个顶点相邻。\n    *   **示例**：\n        ![DPTrustGraph3_DomSet](https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph3_DomSet.width-1250.png)\n        *信任图中支配集的示意图。Alice和Eve（蓝色高亮）是支配集T的一部分。*\n    *   **算法步骤**：\n        1.  找到信任图的支配集T。\n        2.  每个用户将其数据发送给T中的一个受信任邻居。\n        3.  T中的每个用户广播其接收到的所有数据之和加上适当的拉普拉斯噪声。\n        4.  最终估计是所有带噪声广播的总和。\n    *   **误差**：该算法的误差不大于支配集大小的函数。最小化误差需要找到最小支配集（其大小称为支配数）。\n    *   **改进**：论文中还提出了一种基于线性规划的更好算法和更紧的上限。\n\n2.  **通过填充数（Packing Number）的下限**：\n    *   **填充数**：图中一组顶点（最大尺寸）的集合，它们之间不共享任何邻居。\n    *   **误差下限**：论文证明，任何算法的误差都不可能小于填充数的某个常数倍。\n    *   **上下限之间的差距**：通常，支配数总是大于或等于填充数，但两者之间可能存在差距。\n        ![DPTrustGraph4_Graph](https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph4_Graph.width-1250.png)\n        *支配数为4，填充数为1的图。*\n        这种差距意味着本文所示的误差上限和下限之间存在差距，弥合这一差距是TGDP算法理论中的一个开放问题。\n\n#### 信任图DP在机器学习中的应用\n\n*   上述支配集协议可以很容易地适应聚合向量而非实数。\n*   向量聚合是联邦分析和联邦学习中的关键任务，可用于计算跨设备聚合的梯度。\n*   通过将TGDP应用于向量聚合，可以确保最终的机器学习模型满足TGDP。\n\n#### 结论\n\nTGDP模型为分布式差分隐私提供了一种新范式，允许更细粒度地控制信任假设，并自然地插值于中心DP和局部DP之间。它有助于更普遍地理解信任关系与DP算法精度之间的联系。该定义在平台向多样化信任模型发展（如医疗数据共享、社交平台个人数据共享）的背景下具有实际意义。",
      "shortSummary": "本文提出信任图差分隐私（TGDP），一种新的分布式DP模型，旨在适应现实世界中用户间细粒度的信任关系。TGDP介于传统中心化和局部DP模型之间，通过信任图建模隐私。研究通过聚合任务量化精度，提出了基于支配集的算法和基于填充数的误差下限，并讨论了其在联邦学习中的应用。该模型有助于理解信任与DP算法精度间的关系，并弥补现有模型的不足。",
      "translated_title": "信任图上的差分隐私",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph1_ExampleHero.width-1250.png",
          "alt": "DPTrustGraph1_ExampleHero",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph2_Comparison.width-1250.png",
          "alt": "DPTrustGraph2_Comparison",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph3_DomSet.width-1250.png",
          "alt": "DPTrustGraph3_DomSet",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph4_Graph.width-1250.png",
          "alt": "DPTrustGraph4_Graph",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xalxs\"><a href=\"https://en.wikipedia.org/wiki/Differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">Differential privacy</a> (DP) is a mathematically rigorous and widely studied privacy framework that ensures the output of a randomized algorithm remains statistically indistinguishable even if the data of a single user changes. This framework has been extensively studied in both theory and practice, with many applications in analytics and machine learning (e.g., <a href=\"https://research.google/blog/deep-learning-with-label-differential-privacy/\">1</a>, <a href=\"https://research.google/blog/differential-privacy-accounting-by-connecting-the-dots/\">2</a>, <a href=\"https://research.google/blog/advances-in-private-training-for-production-on-device-language-models/\">3</a>, <a href=\"https://research.google/blog/protecting-users-with-differentially-private-synthetic-training-data/\">4</a>, <a href=\"https://research.google/blog/improving-gboard-language-models-via-private-federated-analytics/\">5</a>, <a href=\"https://research.google/blog/bridging-the-gap-in-differentially-private-model-training/\">6</a>, <a href=\"https://research.google/blog/generating-synthetic-data-with-differentially-private-llm-inference/\">7</a>).</p><p data-block-key=\"616kf\">The two main models of DP are the <i>central model</i> and the <i>local model</i>. In the <a href=\"https://en.wikipedia.org/wiki/Differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">central model</a><i>,</i> a trusted curator has access to raw data and is responsible for producing an output that is differentially private. The <a href=\"https://en.wikipedia.org/wiki/Local_differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">local model</a> requires that all messages sent from a user’s device are themselves differentially private, removing the need for a trusted curator. While the local model is appealing due to its minimal trust requirements, it often comes with significantly higher utility degradation compared to the central model.</p><p data-block-key=\"eme60\">In real-world data-sharing scenarios, users often place varying levels of trust in others, depending on their relationships. For instance, someone might feel comfortable sharing their location data with family or close friends but would hesitate to allow strangers to access the same information. This asymmetry aligns with philosophical views of privacy as control over personal information, where individuals specify with whom they are willing to share their data. Such nuanced privacy preferences highlight the need for frameworks that go beyond the binary trust assumptions of existing differentially private models, accommodating more realistic trust dynamics in privacy-preserving systems.</p><p data-block-key=\"82np2\">In “<a href=\"https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.ITCS.2025.53\" target=\"_blank\" rel=\"noopener noreferrer\">Differential Privacy on Trust Graphs</a>”, published at the <i>Innovations in Theoretical Computer Science Conference (ITCS 2025)</i>, we use a trust graph to model relationships, where the vertices represent users, and connected vertices trust each other (see below). We explore how to apply DP to these trust graphs, ensuring that the privacy guarantee applies to messages shared between a user (or their trusted neighbors) and everyone else they do not trust. In particular, the distribution of messages exchanged by each user <i>u</i> or one of their neighbors with any other user not trusted by <i>u</i> should be statistically indistinguishable if the input held by <i>u</i> changes, which we call trust graph DP (TGDP).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph1_ExampleHero.width-1250.png\" alt=\"DPTrustGraph1_ExampleHero\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph1_ExampleHero.width-1250.png\" alt=\"DPTrustGraph1_ExampleHero\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"fckq9\"><i>Example of a trust graph with location sharing. Alice shares her location with Bob, who shares his location with Carol and Dave. Carol and Dave share their location with Eve. Under the definition of trust graph DP that we introduce, it is required that Carol, Dave, and Eve cannot identify Alice’s information, even if they pooled together all of their data and any data shared with them.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xalxs\">Trust graph DP</h2><p data-block-key=\"bcd7l\">TGDP interpolates between the central and local models in a natural way. The central model is equivalent to TGDP over a <a href=\"https://en.wikipedia.org/wiki/Star_(graph_theory)\" target=\"_blank\" rel=\"noopener noreferrer\">star graph</a>, with the trusted central data curator at the center connected to all other users (the top figure, below). The local model, on the other hand, is equivalent to TGDP over a fully unconnected graph, in which no users trust any other users (the bottom figure, below).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph2_Comparison.width-1250.png\" alt=\"DPTrustGraph2_Comparison\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph2_Comparison.width-1250.png\" alt=\"DPTrustGraph2_Comparison\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"fckq9\"><b><i>Left</i></b><i>: Example of a “star” trust graph that corresponds with the central model of DP, in which all users trust a central curator.</i> <b><i>Right</i></b><i>: Example of a fully unconnected trust graph that corresponds with the local model of DP, in which no users trust any other users.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xalxs\">It is known that algorithms that must satisfy central DP can be more <i>accurate</i> than algorithms that must satisfy local DP, since there is more flexibility to share data under central DP. With TGDP, we can quantify algorithm accuracy under more general trust relationships in between.</p><p data-block-key=\"2ka66\">We quantify accuracy specifically through a simple <i>aggregation</i> task, which is a building block to more complicated machine learning tasks. Suppose each user has a private data value, <b><i>x</i></b><b><i><sub class=\"subscript\">i</sub></i></b>, which is a real number bounded within some range. The goal of the aggregation task is to devise an algorithm that can estimate the sum of all users’ values, <b><i>Σ</i></b><b><i><sub class=\"subscript\">i</sub></i></b><b><i> x</i></b><b><i><sub class=\"subscript\">i</sub></i></b>, with as low error as possible (measured as <a href=\"https://en.wikipedia.org/wiki/Mean_squared_error\" target=\"_blank\" rel=\"noopener noreferrer\">mean-squared error</a>).</p><p data-block-key=\"edtag\">In the next sections, we provide both upper and lower bounds on the error of algorithms for aggregation that satisfy TGDP. These bounds quantify the limits of TGDP algorithms: one can do at least as well as the upper bound, and no TGDP algorithm can do better than the lower bound.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xalxs\">An algorithm based on dominating set</h2><p data-block-key=\"a2ko5\">We provide an algorithm for aggregation that satisfies TGDP. Both the algorithm and the upper bound rely on a <a href=\"https://en.wikipedia.org/wiki/Dominating_set\" target=\"_blank\" rel=\"noopener noreferrer\"><i>dominating set</i></a> of the trust graph.</p><p data-block-key=\"abo2n\">A dominating set <i>T</i> is a subset of graph vertices such that every vertex not in <i>T</i> is adjacent to at least one vertex in <i>T</i>. For example, the figure below shows a dominating set in blue.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph3_DomSet.width-1250.png\" alt=\"DPTrustGraph3_DomSet\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph3_DomSet.width-1250.png\" alt=\"DPTrustGraph3_DomSet\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"fckq9\"><i>Illustration of a dominating set in a trust graph. Alice and Eve, highlighted in blue, are part of the dominating set</i> T<i>.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xalxs\">By definition of a dominating set, every user trusts at least one user in the set. This allows us to build an algorithm by letting each user send their raw data to their trusted neighbor in the set, after which the trusted neighbor can aggregate all data it receives and add an appropriate noise to achieve differential privacy. More specifically, our <i>dominating set algorithm</i> works as follows<b>:</b></p><ol><li data-block-key=\"1h2ka\">Find a dominating set <i>T</i> of the trust graph.</li><li data-block-key=\"63pj4\">Each user sends their data to a neighbor in the dominating set <i>T</i>.</li><li data-block-key=\"8jrrj\">Each user in the dominating set <i>T</i> broadcasts the sum of all numbers it receives plus <a href=\"https://en.wikipedia.org/wiki/Additive_noise_differential_privacy_mechanisms#The_Laplace_Mechanism\" target=\"_blank\" rel=\"noopener noreferrer\">Laplace noise</a>.</li><li data-block-key=\"5e01l\">The estimate is the sum of all noisy broadcasts.</li></ol><p data-block-key=\"5qc97\">The error of this algorithm is no greater than a function of the size of the dominating set <i>T</i>. This error is minimized for the smallest possible dominating set, or the minimum dominating set. The size of the minimum dominating set in a graph <i>G</i> is called its <i>domination number</i>.</p><p data-block-key=\"181lm\">Thus, if we are able to find the minimum dominating set in step 1 above, then the dominating set algorithm can have error at most a constant times the domination number.</p><p data-block-key=\"drkfm\">A natural question is whether the dominating set algorithm is the <i>best</i> algorithm. It turns out that we can actually do better using linear programming, and we give a better algorithm and upper bound in <a href=\"https://arxiv.org/abs/2410.12045\" target=\"_blank\" rel=\"noopener noreferrer\">the paper</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xalxs\">A lower bound via packing number</h2><p data-block-key=\"f6leu\">More generally, we need a lower bound to characterize what the <i>best possible</i> error could be. It turns out that there’s a fairly intuitive lower bound that depends on the <i>packing number</i> of the graph: the maximum size of a set of vertices that share no neighbors.</p><p data-block-key=\"52vjb\">For example, for the graph in the first figure, the packing number is 2, since Alice and Eve do not share any neighbors. For the star graph, the packing number is 1.</p><p data-block-key=\"bmeu0\">While the dominating set algorithm in the previous section had error upper bounded by the domination number, we prove <a href=\"https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.ITCS.2025.53\" target=\"_blank\" rel=\"noopener noreferrer\">in the paper</a> that <i>no</i> algorithm can possibly have error smaller than some constant times the packing number.</p><p data-block-key=\"fb8ks\">In general, the domination number is always greater than or equal to the packing number (see e.g., <a href=\"https://arxiv.org/abs/2503.05562v1\" target=\"_blank\" rel=\"noopener noreferrer\">this paper</a>), but there can be a gap. For example, below is a graph with a domination number of 4 and a packing number of 1.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph4_Graph.width-1250.png\" alt=\"DPTrustGraph4_Graph\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph4_Graph.width-1250.png\" alt=\"DPTrustGraph4_Graph\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"fckq9\"><i>Graph with a gap between the domination number (4) and the packing number (1).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xalxs\">The gap between the domination number and the packing number implies that there is a gap between the upper bound and lower bound on error that we’ve shown here (in the full paper we give an even better upper bound using a linear programming based algorithm, but a gap is still there). Closing this gap between the upper and lower bounds is an open problem in the theory of TGDP algorithms.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xalxs\">ML with trust graph DP</h2><p data-block-key=\"9hudo\">The above dominating set protocol can be easily adapted to aggregate <i>vectors</i> instead of real-numbers. Vector aggregation is a <a href=\"https://research.google/blog/federated-analytics-collaborative-data-science-without-data-collection/\">key task in federated analytics and federated learning</a>; for ML applications, vector aggregation can be used to compute the aggregated gradients across different devices. When performing this step with differential privacy, we also get that the final model satisfies differential privacy. This has been the main paradigm for achieving differential privacy in federated learning. Our approach naturally fits into this framework: by using vector aggregation with TGDP, we immediately get that the final ML model satisfies TGDP.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xalxs\">Conclusion</h2><p data-block-key=\"78vrt\">In this work, we introduce trust graph DP (TGDP), a new model for distributed DP which allows a finer-grained control of the trust assumption of each participant. Our TGDP model interpolates between central DP and local DP, allowing us to reason more generally about how trust relationships relate to algorithm accuracy in DP. We provided an algorithm and a lower bound for the aggregation problem. As mentioned above, an interesting open theoretical question is to close the gap between our upper and lower bounds. Our definition may be of practical interest as platforms move towards varied models of trust, from data sharing initiatives in healthcare, to individual data sharing choices on social platforms.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xalxs\">Acknowledgments</h2><p data-block-key=\"1s979\"><i>This blog post is based on joint work with Badih Ghazi and Ravi Kumar. We also thank them for providing us with helpful feedback during the preparation of this blog post.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "利用生成式AI将3D可购物产品带到线上 (原标题: Bringing 3D shoppable products online with generative AI)",
      "link": "https://research.google/blog/bringing-3d-shoppable-products-online-with-generative-ai/",
      "pubDate": "Sun, 11 May 2025 16:00:00 GMT",
      "isoDate": "2025-05-11T16:00:00.000Z",
      "creator": "Google",
      "summary": "### 引言：在线购物的挑战与生成式AI的解决方案\n\n每天有数十亿人进行在线购物，但屏幕上的体验难以复制实体店中“亲手挑选和检查”产品的直观感受。为商家大规模创建高质量的3D产品可视化工具，通常成本高昂且耗时。为解决这一问题，Google开发了新的生成式AI技术，仅需最少三张产品图片即可生成高质量、可购物的3D产品可视化。这项最新进展基于Google先进的视频生成模型Veo，目前已在Google购物上实现了多种产品类别的交互式3D视图。\n\n### 第一代：神经辐射场（NeRFs）\n\n*   **时间：** 2022年\n*   **目标：** Google研究人员开始利用神经辐射场（NeRF）技术，从五张或更多产品图片中学习产品的3D表示，以渲染新颖视图，例如360°旋转。\n*   **挑战：** 该方法面临输入信号噪声（如不准确的相机姿态）和稀疏输入视图带来的模糊性问题。在重建凉鞋和高跟鞋等具有薄结构和复杂几何形状的产品时，这些挑战尤为突出。\n*   **技术细节：** 该方法结合了多种3D技术，包括NOCS用于XYZ预测、CamP用于相机优化，以及Zip-NeRF用于从稀疏视图集进行最先进的新颖视图合成。\n\n![第一代NeRF方法](https://storage.googleapis.com/gweb-research2023-media/images/GenAI3d2_FirstGen.width-1250.png)\n\n### 第二代：基于视图条件扩散先验的规模化\n\n*   **时间：** 2023年\n*   **目标：** 引入了第二代方法，使用视图条件扩散先验来解决第一代方法的局限性。这意味着模型可以根据一张鞋的顶部图片，预测其正面或任何其他视角的样貌，即使只有有限的视角照片。\n*   **原理：** 实践中，该方法采用DreamFusion中首次提出的分数蒸馏采样（SDS）的变体。通过渲染3D模型并与视图条件扩散模型生成的图像进行比较，计算出一个分数来指导3D模型的优化，从而提高其质量和真实感。\n*   **成果：** 这一第二代方法带来了显著的规模化优势，使得Google能够为Google购物上日常浏览的大量鞋类产品（包括凉鞋、高跟鞋、靴子等）生成3D表示。\n\n![第二代视图条件扩散模型](https://storage.googleapis.com/gweb-research2023-media/images/3dGenAI3_SecondGen.width-1250.png)\n\n### 第三代：利用Veo实现泛化\n\n*   **基础：** 最新的突破建立在Google最先进的视频生成模型Veo之上。Veo的关键优势在于其能够生成捕捉光线、材质、纹理和几何形状之间复杂相互作用的视频。\n*   **训练：** 为了微调Veo以将产品图片转换为一致的360°视频，Google首先整理了数百万高质量的3D合成资产数据集。然后，从各种相机角度和光照条件下渲染这些3D资产，并创建了图像-视频配对数据集，监督Veo根据一张或多张图片生成360°旋转视频。\n*   **成果：** 这种方法有效地泛化到家具、服装、电子产品等多样化产品类别。Veo不仅能够生成符合现有产品图片的新颖视图，还能捕捉复杂的光照和材质交互（如闪亮表面），这对于前两代方法来说是具有挑战性的。此外，第三代方法避免了从稀疏的以物体为中心的图片中估计精确相机姿态的需求，简化了问题并提高了可靠性。\n*   **输入要求：** 尽管Veo在仅有一张图片时也能生成逼真的3D物体表示，但它需要对未见视图的细节进行“幻觉”。实践证明，仅需三张捕捉物体大部分表面的图片，就足以显著提高3D图像的质量并减少幻觉。\n\n### 结论与展望\n\n在过去几年中，3D生成式AI取得了巨大进步，从NeRF到视图条件扩散模型，再到现在的Veo。每项技术都在使在线购物更具触感和互动性方面发挥了关键作用。展望未来，Google将继续推动这一领域的发展，使在线购物对用户而言越来越愉快、信息丰富且引人入胜。",
      "shortSummary": "Google正利用生成式AI革新在线购物体验。通过从第一代NeRFs到第二代视图条件扩散模型，再到最新基于Veo的第三代技术，Google现在仅需最少三张产品图片，即可生成高质量、可购物的3D产品可视化。这项技术已广泛应用于Google购物，覆盖鞋类、家具等多种商品，旨在弥补线上购物缺乏“亲身体验”的不足，让用户能更直观地检查产品，提升购物的沉浸感和互动性。",
      "translated_title": "利用生成式AI将3D可购物产品带到线上",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GenAI3d2_FirstGen.width-1250.png",
          "alt": "GenAI3d2_FirstGen",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/3dGenAI3_SecondGen.width-1250.png",
          "alt": "3dGenAI3_SecondGen",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ssx28\">Every day, billions of people shop online, hoping to replicate the best parts of in-store shopping. Seeing something that catches your eye, picking it up and inspecting it for yourself can be a key part of how we connect with products. But capturing the intuitive, hands-on nature of the store experience is nuanced and can be challenging to replicate on a screen. We know that technology can help bridge the gap, bringing key details to your fingertips with a quick scroll. But these online tools can be costly and time consuming for businesses to create at scale.</p><p data-block-key=\"crqem\">To address this, we developed new generative AI techniques to create high quality and shoppable 3D product visualizations from as few as three product images. Today, we're excited to share the latest advancement, powered by Google’s state-of-the-art video generation model, <a href=\"https://deepmind.google/technologies/veo/veo-2/\" target=\"_blank\" rel=\"noopener noreferrer\">Veo</a>. This technology is already enabling the generation of interactive 3D views for a wide range of product categories on Google Shopping</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme- --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n    <div data-gt-id=\"media\" data-gt-component-name=\"\">\n        \n\n        \n\n\n\n\n        \n            \n            <div>\n                <div class=\"glue-ambient-video \">\n                    <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\" preload=\"auto\">\n                        <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/product_collage_borderless.mp4\" type=\"video/mp4\">\n                    </video>\n                    <div class=\"glue-ambient-video__button glue-ambient-video__button--paused\" aria-label=\"Video Play/pause\">\n                        <div class=\"glue-ambient-video__tooltip\">\n                            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                        </div>\n                        <div class=\"glue-ambient-video__icon\">\n                            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                        </div>\n                    </div>\n                </div>\n                \n                    <div class=\"caption --center\"><p data-block-key=\"hjiit\"><i>Examples of 3D product visualizations generated from photos.</i></p></div>\n                \n            </div>\n            \n        \n    </div>\n\n\n\n    </div>\n</section>\n\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"ssx28\">First generation: Neural Radiance Fields (NeRFs)</h2><p data-block-key=\"b8169\">In 2022, researchers from across Google came together to develop technologies to make product visualization more immersive. The initial efforts focused on using <a href=\"https://arxiv.org/abs/2003.08934\" target=\"_blank\" rel=\"noopener noreferrer\">Neural Radiance Fields (NeRF)</a> to learn a 3D representation of products to render novel views (i.e., novel view synthesis), like 360° spins, from five or more images of the product. This required solving many sub-problems, including selecting the most informative images, removing unwanted backgrounds, predicting 3D priors, estimating camera positions from a sparse set of object-centric images, and optimizing a 3D representation of the product.</p><p data-block-key=\"51aqd\">That same year, we <a href=\"https://blog.google/products/shopping/search-on-2022-shopping/\" target=\"_blank\" rel=\"noopener noreferrer\">announced</a> this breakthrough and launched the first milestone, interactive 360° visualizations of shoes on Google Search. While this technology was promising, it suffered from noisy input signals (e.g., inaccurate camera poses) and ambiguity from sparse input views. This challenge became apparent when attempting to reconstruct sandals and heels, whose thin structures and more complex geometry was tricky to reconstruct from just a handful of images.</p><p data-block-key=\"evm86\">This led us to wonder: could the recent advancements in generative diffusion models help us improve the learned 3D representation?</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GenAI3d2_FirstGen.width-1250.png\" alt=\"GenAI3d2_FirstGen\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GenAI3d2_FirstGen.width-1250.png\" alt=\"GenAI3d2_FirstGen\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ubcz9\"><i>Our first-generation approach used neural radiance fields (</i><a href=\"https://arxiv.org/abs/2003.08934\" target=\"_blank\" rel=\"noopener noreferrer\"><i>NeRF</i></a><i>) to render novel views, combining several 3D techniques like</i> <a href=\"https://geometry.stanford.edu/projects/NOCS_CVPR2019/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>NOCS</i></a><i> for XYZ prediction,</i> <a href=\"https://camp-nerf.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>CamP</i></a> <i>for camera optimization, and</i> <a href=\"https://jonbarron.info/zipnerf/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Zip-NeRF</i></a><i> for state-of-the-art novel view synthesis from a sparse set of views.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"ssx28\">Second generation: Scaling with a view-conditioned diffusion prior</h2><p data-block-key=\"c3e50\">In 2023, we introduced a second-generation approach which used a view-conditioned diffusion prior to address the limitations of the first approach. Being <i>view-conditioned</i> means that you can give it an image of the top of a shoe and ask the model <i>“what does the front of this shoe look like?”</i> In this way, we can use the<i> view-conditioned diffusion model</i> to help predict what the shoe looks like from any viewpoint, even if we only have photos of limited viewpoints.</p><p data-block-key=\"23khb\">In practice, we employ a variant of <i>score distillation sampling</i> (SDS), first proposed in <a href=\"https://dreamfusion3d.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">DreamFusion</a>. During training, we render the 3D model from a random camera view. We then use the view-conditioned diffusion model and the available posed images to generate a target from the same camera view. Finally, we calculate a score by comparing the rendered image and the generated target. This score directly informs the optimization process, refining the 3D model's parameters and enhancing its quality and realism.</p><p data-block-key=\"58lgn\">This second-generation approach led to significant scaling advantages enabling us to generate 3D representations for many of the shoes viewed daily on Google Shopping. Today, you can find interactive 360° visualizations for sandals, heels, boots, and other footwear categories when you shop on Google, the majority of which are created by this technology!</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/3dGenAI3_SecondGen.width-1250.png\" alt=\"3dGenAI3_SecondGen\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/3dGenAI3_SecondGen.width-1250.png\" alt=\"3dGenAI3_SecondGen\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ubcz9\"><i>The second-generation approach used a view-conditioned diffusion model based on the</i> <a href=\"https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_TryOnDiffusion_A_Tale_of_Two_UNets_CVPR_2023_paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\"><i>TryOn</i></a><i> architecture. The diffusion model acts as a learned prior using score distillation sampling proposed in</i> <a href=\"https://dreamfusion3d.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>DreamFusion</i></a><i> to improve the quality and fidelity of novel views.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"ssx28\">Third generation: Generalizing with Veo</h2><p data-block-key=\"6svfv\">Our latest breakthrough builds on <a href=\"https://deepmind.google/technologies/veo/veo-1/\" target=\"_blank\" rel=\"noopener noreferrer\">Veo</a>, Google's state-of-the-art video generation. A key strength of Veo is its ability to generate videos that capture complex interactions between light, material, texture, and geometry. Its powerful diffusion-based architecture and its ability to be finetuned on a variety of multi-modal tasks enable it to excel at novel view synthesis.</p><p data-block-key=\"aftc9\">To finetune Veo to transform product images into a consistent 360° video, we first curated a dataset of millions of high quality, 3D synthetic assets. We then rendered the 3D assets from various camera angles and lighting conditions. Finally, we created a dataset of paired images and videos and supervised Veo to generate 360° spins conditioned on one or more images.</p><p data-block-key=\"2p7s5\">We discovered that this approach generalized effectively across a diverse set of product categories, including furniture, apparel, electronics and more. Veo was not only able to generate novel views that adhered to the available product images, but it was also able to capture complex lighting and material interactions (i.e., shiny surfaces), something which was challenging for the first- and second-generation approaches.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        <div class=\"glue-ambient-video \">\n            <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/3dGenAI4_ThirdGen.mp4\" type=\"video/mp4\">\n            </video>\n            <div class=\"glue-ambient-video__button glue-ambient-video__button--paused\" aria-label=\"Video Play/pause\">\n                <div class=\"glue-ambient-video__tooltip\">\n                  <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                  <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                </div>\n                <div class=\"glue-ambient-video__icon\">\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                </div>\n              </div>\n        </div>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ubcz9\"><i>The third-generation approach builds on Veo to generate 360° spins from one or more product images.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ssx28\">Furthermore, this third-generation approach avoided the need to estimate precise camera poses from a sparse set of object-centric product images, simplifying the problem and increasing reliability. The fine-tuned Veo approach is powerful — with one image, you can generate a realistic, 3D representation of the object. But like any generative 3D technology, Veo will need to hallucinate details from unseen views, for example, the back of the object when only a view of the front is available. As the number of input images increases, so does Veo's ability to generate high fidelity and high quality novel views. In practice, we found that as few as three images capturing most object surfaces are sufficient to improve the quality of the 3D images and reduce hallucinations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"ssx28\">Conclusion and future outlook</h2><p data-block-key=\"8c9rs\">Over the last several years, there has been tremendous progress in 3D generative AI, from NeRF to view-conditioned diffusion models and now Veo. Each technology has played a key part in making online shopping feel more tangible and interactive. Looking ahead, we are excited to continue to push boundaries in this space and to help make shopping online increasingly delightful, informative, and engaging for our users.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"ssx28\">Acknowledgements</h2><p data-block-key=\"dq34d\"><i>This work was made possible by Philipp Henzler, Matthew Burruss, Matthew Levine, Laurie Zhang, Ke Yu, Chung-Yi Weng, Jason Y. Zhang, Changchang Wu, Ira Kemelmacher-Shlizerman, Carlos Hernandez, Keunhong Park, and Ricardo Martin-Brualla. We thank Aleksander Holynski, Ben Poole, Jon Barron, Pratul Srinivasan, Howard Zhou, Federico Tombari, and many more from Google Labs, Google DeepMind, and Google Shopping.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "神经连接研究的新突破 (原标题: A new light on neural connections)",
      "link": "https://research.google/blog/a-new-light-on-neural-connections/",
      "pubDate": "Tue, 06 May 2025 16:00:00 GMT",
      "isoDate": "2025-05-06T16:00:00.000Z",
      "creator": "Google",
      "summary": "# LICONN：神经连接研究的新突破\n\n## 引言：显微镜技术与神经科学的挑战\n自17世纪安东尼·范·列文虎克首次用自制光学显微镜观察到微生物以来，光学显微镜已成为生命科学研究的基石，广泛应用于细胞、器官和组织的识别以及疾病诊断。然而，在神经科学的“连接组学”（connectomics）领域，光学显微镜一直未能发挥作用。连接组学旨在全面绘制大脑区域内的所有神经元及其连接，此前主要依赖于电子显微镜（EM）技术，尽管EM能提供超高分辨率的结构信息，但其设备昂贵、操作复杂，限制了大多数神经科学实验室的普及使用。\n\n## LICONN：基于光学显微镜的连接组学新方法\n为了克服电子显微镜的局限性，Google与奥地利科学技术研究所（ISTA）的合作团队在《自然》杂志上发表了一项突破性研究，报告了首个利用光学显微镜全面绘制小鼠脑组织中所有神经元及其连接的方法——**LICONN**（light microscopy-based connectomics，基于光学显微镜的连接组学）。\n\nLICONN通过整合和定制多项成熟技术，形成了一个独特的实验流程。其核心创新在于ISTA团队开发的一种协议：在保持结构完整性的前提下，物理性地膨胀脑组织，同时对所有蛋白质进行化学标记，以提供追踪神经元和检测突触等细胞结构所需的图像对比度。\n\n## LICONN的优势与能力\n*   **可及性高：** 相较于昂贵且操作复杂的电子显微镜，LICONN基于光学显微镜，成本更低，更易于普及，使得更多实验室能够进行连接组学研究。\n*   **多模态信息获取：** LICONN能够同时测量组织样本中的结构信息和分子信息，这为理解大脑功能提供了前所未有的新机会。光学显微镜能够捕捉不同波长的光，从而可视化各种荧光标记物，揭示蛋白质、神经递质和其他分子的分布，这些分子对于理解神经元和回路的功能至关重要。LICONN以前所未有的精度和便捷性，将这些分子标记与神经通路详细的结构图谱对齐，有助于深入理解大脑如何产生认知、感知和行为。\n\n## LICONN的工作原理：组织膨胀显微技术\n为了解决光学显微镜分辨率有限的问题，ISTA团队将重点放在了样本本身。他们采用了**膨胀显微技术**，该技术利用水凝胶（与尿布吸水材料相同）吸收水分并膨胀的原理。\n\nLICONN的组织膨胀协议是现有膨胀显微技术的重大改进：\n1.  将微小的脑组织块切成50微米厚的切片。\n2.  用三种不同的水凝胶依次处理每个切片。其中两种水凝胶在组织内形成独特交织的聚合物网络，每种使组织膨胀四倍；第三种水凝胶用于稳定这些网络。\n3.  最终，组织在每个方向上膨胀约16倍，相当于将一块方糖膨胀成一盒纸巾的大小。\n\n为了使神经元可见，研究人员用绿色荧光染料孵育脑组织切片，有效“标记”细胞内的所有蛋白质。在某些样本中，他们还使用了其他染料来标记特定的蛋白质、神经递质或其他对确定神经元细胞类型或功能很重要的分子。\n\n## 自动化重建与验证\n在图像分析和机器学习方面，Google团队应用了其为连接组学开发的一系列工具，包括用于区分脑组织图像中不同结构的“洪水填充网络”（flood-filling networks）算法，以及用于对齐和拼接连续电子显微镜图像的SOFIMA算法。\n\n研究团队对LICONN进行了大规模验证：\n*   **自动化重建：** 对近一百万立方微米的小鼠皮层体积进行了自动化重建。\n*   **可追溯性验证：** 全面验证了小鼠海马组织中约0.5米神经突的追踪能力，结果表明LICONN与基于电子显微镜的连接组学表现相当。\n\n### 验证结果展示\n![LICONN-introfinal](https://storage.googleapis.com/gweb-research2023-media/images/LICONN-introfinal.width-1250.png)\n*连接组学利用基于显微镜的成像结合计算处理，重建脑组织中的单个细胞及其复杂的连接。*\n\n研究的主要目标之一是证明LICONN与基于电子显微镜的连接组学一样可靠。\n1.  **手动追踪对比：** LICONN的重建结果与对组织子区域内所有蔓延的树突和细小弯曲的轴突进行手动追踪的结果高度吻合。经过优化，自动化重建算法的性能与电子显微镜数据的重建结果相当。\n    ![LICONN1-RenderingsFinal](https://storage.googleapis.com/gweb-research2023-media/images/LICONN1-RenderingsFinal.width-1250.png)\n    *使用洪水填充网络自动分割重建的LICONN体积渲染图。该图像描绘了原始组织体积中5.8%的轴突（上）和27.3%的树突及少量轴突（下）的重建。*\n2.  **突触连接识别：** LICONN能够可靠地识别突触连接。其多模态能力——在同一样本中同时成像结构（形态）、连接以及特定目标分子——使得研究人员能够直接标记区分组织中突触前和突触后区域的特定蛋白质。这些标记还被用作训练机器学习算法的“真值”，以便仅从结构观察中识别这些突触区域，这与电子显微镜连接组学的方法类似。\n3.  **超越结构信息的分子洞察：** LICONN还利用蛋白质标记揭示了仅凭结构无法可靠获取的分子信息，从而超越了电子显微镜连接组学的能力。例如，标记与两种不同神经递质相关的蛋白质，可以直接区分抑制性突触和兴奋性突触。标记另一种蛋白质则能精确定位电突触，这些突触在神经系统中普遍存在，但由于其微小且微妙的结构外观，通常在基于电子显微镜的连接组学重建中被忽略。在脑组织中明确识别这些分子特征，并将其置于整体脑形态和连接的背景下，将显著扩展连接组图谱的实用性。\n    ![LICONN2-HeroFinal](https://storage.googleapis.com/gweb-research2023-media/images/LICONN2-HeroFinal.width-1250.png)\n    *树突（通过洪水填充网络分割）和通过免疫标记检测到的兴奋性突触连接（白色条）的3D渲染图和放大视图。*\n\n## 前景展望\n目前，研究团队正在努力扩大LICONN的应用规模，以捕获更大组织体积的数据。未来，他们计划与美国国立卫生研究院（NIH）资助的合作项目共同绘制小鼠大脑，从海马体开始。此外，LICONN还将用于研究阿尔茨海默病等疾病背景下大脑结构的变化。\n\nLICONN的成功验证是实现这些目标的重要一步，它有望大幅提高连接组学的可及性，并使科学家能够提取结合神经元、突触和以前无法获得的分子细节的“多模态”连接组。LICONN的实用性已得到证实，自2024年3月预印本发布以来，已有多个实验室成功复制了该技术。这项工作被视为实现神经科学高级目标的关键一步，有望在临床及其他领域解锁重要的应用。",
      "shortSummary": "Google和ISTA团队开发了一种名为LICONN的新型光学显微镜技术，首次实现了基于光学显微镜对哺乳动物脑组织中神经元及其连接的全面绘制。传统连接组学依赖昂贵且复杂的电子显微镜。LICONN通过独特的组织膨胀和化学标记协议，克服了光学显微镜的分辨率限制，并能同时获取结构和分子信息。该技术已验证与电子显微镜效果相当，显著提高了脑连接组学研究的可及性和效率，为理解大脑功能和疾病提供了前所未有的新工具。",
      "translated_title": "神经连接研究的新突破",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/LICONN-introfinal.width-1250.png",
          "alt": "LICONN-introfinal",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/LICONN1-RenderingsFinal.width-1250.png",
          "alt": "LICONN1-RenderingsFinal",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/LICONN2-HeroFinal.width-1250.png",
          "alt": "LICONN2-HeroFinal",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"tdt8j\">In the 1660s, with the help of a simple, homemade light microscope that magnified samples more than 250 times, a Dutch fabric merchant named <a href=\"https://en.wikipedia.org/wiki/Antonie_van_Leeuwenhoek\" target=\"_blank\" rel=\"noopener noreferrer\">Antoine van Leeuwenhoek</a> became the first person to document a close-up view of bacteria, red blood cells, sperm cells, and many other scientific sights. Since then, <a href=\"https://en.wikipedia.org/wiki/Optical_microscope\" target=\"_blank\" rel=\"noopener noreferrer\">light microscopy</a> has solidified its place as a bedrock technique in our quest to understand living organisms. Today, it is nearly ubiquitous in life science laboratories, enabling biologists to identify and characterize cells, organs and tissues and to diagnose many diseases.</p><p data-block-key=\"8mfs1\">One field that light microscopy has not managed to penetrate, however, is connectomics — an area of neuroscience in which Google <a href=\"https://research.google/blog/ten-years-of-neuroscience-at-google-yields-maps-of-human-brain/\">has made fundamental contributions over the past decade</a>. Efforts to comprehensively map all the neurons in a region — including our previous connectomics work — have instead relied on a technique called <a href=\"https://en.wikipedia.org/wiki/Electron_microscope\" target=\"_blank\" rel=\"noopener noreferrer\">electron microscopy</a>, which can capture an extremely close-up view of structural information within a cell. Electron microscopy has a major limitation, however: it requires expensive, highly specialized equipment that is not readily accessible to most neuroscience labs.</p><p data-block-key=\"e7ion\">Today, in collaboration with colleagues at the <a href=\"https://ista.ac.at/en/home/\" target=\"_blank\" rel=\"noopener noreferrer\">Institute of Science and Technology Austria</a> (ISTA), we published in the journal <a href=\"https://www.nature.com/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature</i></a>, “<a href=\"https://www.nature.com/articles/s41586-025-08985-1\" target=\"_blank\" rel=\"noopener noreferrer\">Light-microscopy based connectomic reconstruction of mammalian brain tissue</a>”, in which we report the first-ever method for using light microscopy to comprehensively map all the neurons and their connections in a block of mouse brain tissue. We achieved this by customizing several well-established and validated techniques and combining them into a single workflow that we call LICONN (light microscopy-based connectomics). Our colleagues at ISTA led the project’s key innovation — a protocol that physically expands brain tissue while preserving structural integrity, and at the same time chemically labels all proteins in order to provide the image contrast necessary for tracing neurons and detecting other cellular structures such as synapses.</p><p data-block-key=\"aiv5p\">We iterated with ISTA on the details of the protocol, applying our suite of image analysis and machine learning (ML) tools for connectomics, and ultimately validating LICONN at scale by providing an automated reconstruction of a nearly one-million cubic micron volume of mouse cortex. We then comprehensively verified the traceability of all ~0.5 meters of <a href=\"https://en.wikipedia.org/wiki/Neurite\" target=\"_blank\" rel=\"noopener noreferrer\">neurites</a> packed within a smaller volume of mouse hippocampus tissue, demonstrating that LICONN works comparably well to electron microscope–based connectomics. We also showed that LICONN unlocks the ability to simultaneously measure structural and molecular information in a tissue sample, which will enable fundamental new opportunities to understand the workings of the brain.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/LICONN-introfinal.width-1250.png\" alt=\"LICONN-introfinal\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/LICONN-introfinal.width-1250.png\" alt=\"LICONN-introfinal\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"pquzq\"><i>Connectomics utilizes microscopy-based imaging coupled with computational processing to reconstruct individual cells and their intricate connections within brain tissue.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"tdt8j\">The need for new approaches</h2><p data-block-key=\"51hf8\">Electron microscopes produce images based on how a beam of electrons scatters when it interacts with a sample, enabling it to have a much higher resolution than light microscopy — it can image features on the scale of a nanometer compared to hundreds of nanometers for (visible) light microscopy. But despite light microscopy’s limited resolution, its accessibility is a major advantage. The electron microscopes used for connectomics research can cost millions of dollars, and operating them requires extensive specialized training, making them often available only to neuroscientists at large, well-funded institutions.</p><p data-block-key=\"52i81\">Researchers have used a handful of “super-resolution” light microscopy techniques, which can image beyond light’s diffraction limit to <a href=\"https://www.science.org/doi/full/10.1126/science.aau8302\" target=\"_blank\" rel=\"noopener noreferrer\">achieve nanoscale resolution</a>, to label and trace relatively small subsets of neurons in various ways. But these techniques have not been able to achieve “dense” connectomics — i.e., mapping <i>all</i> densely packed neurons at the scale of the finest fibers.</p><p data-block-key=\"8in9d\">A key benefit of light microscopy is that it can capture light at many different wavelengths, so researchers can use it to visualize a rainbow of fluorescent markers for proteins, neurotransmitters, and other molecules that provide clues about how neurons and circuits function or malfunction. LICONN unlocks the ability to align these molecular markers with detailed structural maps of neuronal pathways with unprecedented precision and ease, which will help yield important insights into how the brain generates cognition, perception and behavior.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"tdt8j\">Expanding the possibilities</h2><p data-block-key=\"5epo6\">To get around the problem of limited resolution with light microscopy, our ISTA colleagues turned their efforts toward the sample itself. They used a technique called expansion microscopy, which uses a substance called a <a href=\"https://www.sciencedirect.com/topics/chemistry/hydrogel#:~:text=Hydrogels%20are%20three%2Ddimensional%20networks,properties%20to%20the%20hydrogel%20structures.\" target=\"_blank\" rel=\"noopener noreferrer\">hydrogel</a> (the same material that makes diapers absorbent). Hydrogels absorb moisture by crosslinking with water molecules, swelling as they do so — a principle that researchers at the Boyden Lab at the Massachusetts Institute of Technology relied on to <a href=\"https://www.science.org/doi/10.1126/science.1260088\" target=\"_blank\" rel=\"noopener noreferrer\">develop a tissue expansion protocol for microscopy</a> in 2015.</p><p data-block-key=\"9tdtg\">Although expansion microscopy is used in many labs today, existing tissue expansion protocols do not preserve the tissue well enough to trace densely labeled neuronal structures, so our ISTA collaborators developed a new expansion protocol for LICONN. It involved first cutting a tiny block of tissue into 50 micrometer sections, then treating each section with a sequence of three different hydrogels. Two of the hydrogels created distinct, interweaving polymer networks within the tissue, with each one expanding the tissue by a factor of four, and the third hydrogel served to stabilize those networks. With this protocol, the tissue expanded by about 16 times in each direction, which is loosely comparable to starting with a sugar cube and ending up with a box of tissues.</p><p data-block-key=\"6j963\">To make the neurons visible, our colleagues incubated the brain tissue sections with a green fluorescent dye, effectively \"labeling\" all proteins inside cells. In some samples, they also used several other dyes that home in on specific proteins, neurotransmitters or other molecules that are important in determining a neuron’s cell type or function.</p><p data-block-key=\"9og7f\">Each tissue section is essentially a serial slice of a thick swathe of neurons enmeshed with other cells such as <a href=\"https://en.wikipedia.org/wiki/Glia\" target=\"_blank\" rel=\"noopener noreferrer\">glia</a>. Tracing this dense mass of brain cell fragments in expanded tissue can be challenging. We accomplished this using algorithms that we have previously developed for automated reconstruction of neurons through multiple tissue slices. These tools have achieved state-of-the-art accuracy for delineating distinct structures within brain tissue images acquired with electron microscopy (<a href=\"https://www.nature.com/articles/s41592-018-0049-4\" target=\"_blank\" rel=\"noopener noreferrer\">flood-filling networks</a>) and for aligning and stitching together serial electron microscopy images (<a href=\"https://github.com/google-research/sofima\" target=\"_blank\" rel=\"noopener noreferrer\">SOFIMA</a>).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"tdt8j\">Validating LICONN</h2><p data-block-key=\"6apns\">A chief aim of our study was to demonstrate that LICONN is as robust as electron microscopy–based connectomics. First, we showed that our results matched closely with manual tracing of all the <a href=\"https://neuroglancer-demo.appspot.com/#!gs://liconn-public/ng_states/expid82_fig2_dends.json\" target=\"_blank\" rel=\"noopener noreferrer\">sprawling dendrites</a> and <a href=\"https://neuroglancer-demo.appspot.com/#!gs://liconn-public/ng_states/expid82_fig2_axons.json\" target=\"_blank\" rel=\"noopener noreferrer\">thin, twisty axons</a> within a subsection of tissue. We then trained our automated reconstruction algorithm on the manually drawn data. After optimizing the algorithm’s accuracy, we showed that it performed comparably to reconstructions from electron microscopy data.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/LICONN1-RenderingsFinal.width-1250.png\" alt=\"LICONN1-RenderingsFinal\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/LICONN1-RenderingsFinal.width-1250.png\" alt=\"LICONN1-RenderingsFinal\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"0fic3\"><i>Renderings of the reconstructed LICONN volume with automated segmentation from flood-filling networks. The image depicts a reconstruction of 5.8% of axons (</i><b><i>top</i></b><i>) and 27.3% of dendrites and a small number of axons (</i><b><i>bottom</i></b><i>) from the original tissue volume.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"28qpp\">Next, we confirmed that synaptic connections are robustly identifiable in LICONN volumes. LICONN’s multimodal capabilities — to simultaneously image structure (morphology), connectivity, and also specific molecules of interest within the same sample — enabled us to directly label specific proteins that distinguish pre-synaptic versus post-synaptic regions in the tissue. We were further able to use these labels as ground truth for training ML algorithms to identify these synaptic areas purely from structural observations, as is done in electron microscopy connectomics.</p><p data-block-key=\"pa4l\">Finally, we also used protein labeling to reveal molecular information in connectomics maps that cannot be reliably gleaned from structure alone, thus going well beyond the capabilities of electron microscopy connectomics. For example, labeling proteins associated with two different neurotransmitters allowed us to directly differentiate between inhibitory and excitatory <a href=\"https://www.ncbi.nlm.nih.gov/books/NBK11001/#:~:text=Such%20communication%20is%20made%20possible,electrical%20synapses%20and%20chemical%20synapses.\" target=\"_blank\" rel=\"noopener noreferrer\">synapses</a>. Labeling yet another protein pinpointed <a href=\"https://www.ncbi.nlm.nih.gov/books/NBK11164/\" target=\"_blank\" rel=\"noopener noreferrer\">electrical synapses</a>, which are ubiquitous in the nervous system, but are typically neglected from electron microscopy–based connectomic reconstructions due to their small and subtle structural appearance. The ability to conclusively identify these molecular features in brain tissue, and place them in the context of overall brain morphology and connectivity, will significantly expand the utility of connectomic maps.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/LICONN2-HeroFinal.width-1250.png\" alt=\"LICONN2-HeroFinal\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/LICONN2-HeroFinal.width-1250.png\" alt=\"LICONN2-HeroFinal\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"0fic3\"><i>3D-rendering and magnified views of a dendrite (segmented with flood filling networks) and excitatory synaptic connections (white bars) detected via</i> <a href=\"https://en.wikipedia.org/wiki/Immunolabeling#:~:text=Immunolabeling%20is%20a%20biochemical%20process,cell%2C%20tissue%2C%20or%20organ.\" target=\"_blank\" rel=\"noopener noreferrer\"><i>immunolabeling</i></a><i>.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"28qpp\">In our study, the block of mouse brain we mapped was just a small percentage of the <a href=\"https://www.science.org/doi/10.1126/science.adk4858\" target=\"_blank\" rel=\"noopener noreferrer\">1 square millimeter block of human brain tissue</a>, about the size of a grain of rice, that we mapped with colleagues at Harvard University last year using electron microscopy–based connectomics. We are currently working on scaling up LICONN to be able to capture data from larger tissue volumes.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"28qpp\">The path ahead</h2><p data-block-key=\"1sb6b\">We recently announced a joint effort <a href=\"https://research.google/blog/google-research-embarks-on-effort-to-map-a-mouse-brain/\">to map a mouse brain, starting with the hippocampus</a> — one of several connectomics collaborations funded by the <a href=\"https://braininitiative.nih.gov/\" target=\"_blank\" rel=\"noopener noreferrer\">National Institutes of Health</a>. We are also working towards understanding <a href=\"https://www.biorxiv.org/content/10.1101/2023.10.24.563674v3.abstract\" target=\"_blank\" rel=\"noopener noreferrer\">how brain structures change in the context of diseases like Alzheimer's</a>. However, fully achieving these goals will require innovations that make brain mapping more efficient by orders of magnitude.</p><p data-block-key=\"7uj0g\">The validation of LICONN takes a clear step toward that goal, holding great promise both to increase the accessibility of connectomics and to enable scientists to extract “multimodal” connectomes that combine information about neurons and synapses with molecular details that have previously been inaccessible. LICONN’s utility to the neuroscience community is evidenced by the fact that already, several labs have successfully replicated the technique <a href=\"https://www.biorxiv.org/content/10.1101/2024.03.01.582884v2\" target=\"_blank\" rel=\"noopener noreferrer\">based on our preprint</a>, first posted in March 2024. We see this work as a step toward achieving high-level goals of neuroscience that could unlock important applications in the clinic and beyond.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"28qpp\">Acknowledgements</h2><p data-block-key=\"6r92r\"><i>We thank our academic collaborators in the Danzl Lab (ISTA), and acknowledge core contributions from the Connectomics Team at Google. We are grateful to Alla Katsnelson and Elise Kleeman for their help. Thanks to Lizzie Dorfman, Michael Brenner, John Platt, and Yossi Matias for their support, coordination and leadership.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "让复杂文本易于理解：基于Gemini的最小信息损失文本简化 (原标题: Making complex text understandable: Minimally-lossy text simplification with Gemini)",
      "link": "https://research.google/blog/making-complex-text-understandable-minimally-lossy-text-simplification-with-gemini/",
      "pubDate": "Mon, 05 May 2025 16:00:00 GMT",
      "isoDate": "2025-05-05T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 让复杂文本易于理解：基于Gemini的最小信息损失文本简化\n\n## 引言与背景\n在数字时代，尽管知识获取途径日益增多，但大量信息仍被复杂语言和专业术语所阻碍。这种复杂性在专家交流中是必要的，但在用户需要理解关键信息（如健康、法律或金融信息）时，却成为障碍。能够将复杂文本简化为易于理解版本的工具，可以赋能用户更好地参与这些文本。\n\n## Google Research 的创新系统\nGoogle Research 在其论文《基于LLM的文本简化及其对用户理解和认知负荷的影响》中，介绍了一个使用Gemini模型专门设计用于**最小信息损失（高保真）文本简化**的系统。\n*   **目标**：在增强清晰度的同时，精确保留原始含义、细节和细微差别。\n*   **区别**：这不同于摘要（允许信息丢失）或解释（通常添加信息）。\n*   **应用**：该系统已作为一项新功能“Simplify”在iOS版Google应用中推出。\n\n实现这一目标要求模型准确地转述复杂思想，而不引入错误或遗漏关键细节。重写后的文本必须帮助读者理解挑战性材料，同时不牺牲原始信息的完整性。\n\n## 主要贡献\n这项工作提供了两项主要贡献：\n1.  **新颖的系统**：引入了一个具有自动化评估和迭代提示词优化循环的系统。这使得Gemini模型能够以手动优化无法达到的规模和速度，通过迭代发现最有效的提示词，实现高保真文本简化。\n2.  **严格的大规模随机研究**：通过一项大规模随机研究，证明文本简化可显著提高用户理解能力并降低认知负荷。\n\n## 基于Gemini的自动化评估与提示词优化系统\n为了实现目标，研究人员开发了一种利用Gemini模型进行简化质量评估和提示词自我优化的自动化方法。\n*   **挑战**：为细致的简化（既要提高可读性又不牺牲含义或细节）制作提示词极具挑战性。自动化系统通过实现发现最有效提示词所需的广泛试错来解决这一挑战。\n\n### 自动化评估\n手动评估对于快速迭代是不切实际的。该系统采用了两个新颖的评估组件：\n*   **可读性评估**：超越了Flesch-Kincaid等简单指标，使用Gemini提示词对文本可读性进行1-10评分。该提示词根据人工判断进行迭代优化，从而实现对理解难度的更细致评估。测试表明，这种基于LLM的可读性评估与人类判断的一致性优于Flesch-Kincaid。\n*   **忠实度评估**：确保意义保留至关重要。使用Gemini 1.5 Pro，研究人员实现了一个将原始文本中的主张映射到简化版本的过程。此方法识别特定错误类型，如信息丢失、增益或扭曲，每种错误根据严重程度加权，从而提供对原始含义忠实度（完整性和蕴含）的细致衡量。\n\n### 迭代提示词优化：LLM优化LLM\n最终简化文本（由Gemini 1.5 Flash生成）的质量在很大程度上取决于初始提示词。研究人员通过一个提示词优化循环实现了提示词优化过程的自动化：\n*   利用可读性和忠实度的自动评估分数，另一个Gemini 1.5 Pro模型分析简化提示词的性能，并为下一次迭代提出改进的提示词。\n*   这创建了一个强大的反馈循环，其中LLM系统根据性能指标迭代改进其自身指令，减少了对手动提示词工程的依赖，并能够发现高效的简化策略。在此项工作中，该循环运行了824次迭代，直到性能趋于稳定。\n\n这种自动化过程，即一个LLM评估另一个LLM的输出，并根据性能指标（可读性和忠实度）和细粒度错误来优化其指令（提示词），代表了一项关键创新。它超越了繁琐的手动提示词工程，使系统能够自主地在数百次迭代中发现细致简化的有效策略。\n\n![简化系统概述](https://storage.googleapis.com/gweb-research2023-media/images/Simplify1_SummaryHero.width-1250.png)\n基于Gemini的最小信息损失文本简化方法概述。\n\n## 衡量影响：大规模随机研究\n为了验证通过此方法简化的文本在实际世界中的有效性，研究人员进行了一项随机对照研究。\n\n### 研究设计\n*   **参与者**：招募了4,563名同意参与者，并对他们的主题专业知识进行了筛选。\n*   **文本**：使用了31篇多样化的真实世界文本摘录，涵盖了以复杂性著称的领域：医学研究、生物学、法律、金融、文学、哲学、航空航天和计算机科学。\n*   **比较**：采用随机完全区组设计，参与者被随机分配阅读原始文本、简化版本或两者。为了评估文本简化对内容短期记忆的进一步影响，用户在两种条件下进行了测试：一种是回答问题时可以参考文本，另一种是不能。\n*   **测量**：通过仔细审查的多项选择题（MCQs）评估理解能力；通过自我报告的信心评估；通过简化的NASA任务负荷指数评估认知负荷。\n\n![研究设计图](https://storage.googleapis.com/gweb-research2023-media/images/Simplify2_DesignFinal.width-1250.png)\n评估简化模型与真实文本的研究设计。\n\n### 结果\n这项研究包含了近50,000个MCQ答案，产生了具有统计学意义的结果，证明了简化的价值。\n*   **定量发现**：\n    *   阅读简化文本的参与者，MCQ整体准确率比阅读原始文本的参与者**绝对提高了4%**。\n    *   对于高度复杂的PubMed文本，影响最为显著，**准确率绝对提高了15%**。\n    *   在金融（6%）、法律（4%）和技术（航空/计算机科学，4%）领域也观察到显著提升。\n    *   即使参与者无法参考文本，这些提升也依然稳健，表明对即时理解和短期记忆均有益。\n    *   除了准确率，参与者报告对答案的信心更高（在-2到2的量表上平均提高0.24），并且在使用简化文本时认为任务更容易（在-2到2的量表上平均提高0.33）。\n*   **定性洞察**：\n    *   审查简化显著提高参与者MCQ准确率的例子（一篇医学研究文本提高了38%）揭示了其价值。简化通过定义专业术语（如“肺气肿”和“纤维化”）、分解密集句子以及澄清复杂关系来增强清晰度。\n    *   简化对于基线理解较低的文本特别有帮助。\n\n![简化前后文本对比示例](https://storage.googleapis.com/gweb-research2023-media/images/Simplify3_Excerpt.width-1250.png)\n*摘自一篇生物医学文章，PMC10177208，知识共享许可（CC BY 4.0）。*\n\n### 研究局限性\n*   通过调查平台招募参与者，可能无法完全反映积极寻求理解复杂信息的用户群体。\n*   尽管系统旨在实现高保真，但LLM错误仍可能发生，需要持续警惕。\n*   MCQ虽然可扩展，但对深度理解的衡量不完整。\n\n## 作为新“Simplify”功能推出\n从今天起，这项功能已在iOS版Google应用中作为新功能“Simplify”提供。用户可以在Google应用中访问的网页上选择任何复杂文本，然后点击出现的“Simplify”图标，即可查看文本的新简化版本，无需停止阅读或离开页面。这使得人们在网上学习新知识时，更容易掌握新或复杂的主题。\n\n## 结论\nGoogle开发并严格验证了一个使用Gemini的自动化系统，该系统通过迭代学习来简化文本，同时保持对原始文本的忠实度。通过显著弥合复杂信息的理解鸿沟，这项功能显著增强了用户在关键领域的理解能力并降低了认知负荷。",
      "shortSummary": "Google Research推出了一项基于Gemini模型的文本简化系统，旨在以最小信息损失（高保真）的方式，让复杂文本更易理解。该系统通过自动化评估和迭代提示词优化实现高效简化。一项大规模研究表明，简化文本能显著提高用户理解力并降低认知负负荷，尤其在医学、法律和金融等复杂领域效果显著。此功能已作为“Simplify”在iOS版Google应用中推出，帮助用户轻松掌握复杂信息。",
      "translated_title": "让复杂文本易于理解：基于Gemini的最小信息损失文本简化",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Simplify1_SummaryHero.width-1250.png",
          "alt": "Simplify1_SummaryHero",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Simplify2_DesignFinal.width-1250.png",
          "alt": "Simplify2_DesignFinal",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Simplify3_Excerpt.width-1250.png",
          "alt": "Simplify3_Excerpt",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qwfyl\">The digital age offers ever growing access to vast amounts of knowledge, yet much remains locked behind complex language and specialist jargon. While complexity is often necessary in expert discourse, it can become a barrier when users need to understand information critical to their lives, such as navigating health information, understanding legal language, or grasping financial details. Tools that let users produce a simplified version of complex text that they encounter online can empower them to engage with those texts when they wouldn't have been able to otherwise.</p><p data-block-key=\"8jn6b\">Today, in “<a href=\"https://arxiv.org/abs/2505.01980\" target=\"_blank\" rel=\"noopener noreferrer\">LLM-based Text Simplification and its Effect on User Comprehension and Cognitive Load</a>”, Google Research introduces a system using <a href=\"https://ai.google.dev/gemini-api/docs/models\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini models</a> specifically designed for <i>minimally-lossy (high fidelity) text simplification</i>. The goal of this system is to enhance clarity while meticulously preserving the original meaning, detail, and nuance. This is&nbsp;a distinct goal from summarization (which is ok with dropping information) or explanation (which often adds information). We also launch this system in a new feature in the Google app for iOS, <a href=\"https://blog.google/products/search/simplify-google-app-ios\" target=\"_blank\" rel=\"noopener noreferrer\">Simplify</a>.</p><p data-block-key=\"3unkf\">Achieving this requires models to paraphrase complex ideas accurately without introducing errors or omitting key details. The re-written text must help the reader understand challenging material without sacrificing the integrity of the original information.</p><p data-block-key=\"3ub23\">This work offers two primary contributions. First, we present a novel system featuring an automated evaluation and iterative prompt refinement loop: this enables Gemini models to discover the most effective prompt for high-fidelity text simplification by iterating at a scale and speed impractical to achieve with manual prompt optimization. Second, through a rigorous, large-scale randomized study, we demonstrate that text simplification measurably improves user comprehension and reduces cognitive load.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qwfyl\">Gemini-powered automatic evaluation and prompt refinement system</h2><p data-block-key=\"8ib90\">In order to achieve our goals, we developed an automated approach leveraging Gemini models for evaluation of simplification quality and self-refinement of prompts. However, crafting prompts for nuanced simplification, where readability must improve without sacrificing meaning or detail, is challenging. An automated system addresses this challenge by enabling the extensive trial-and-error needed to discover the most effective prompt.</p><h3 data-block-key=\"d64vs\">Automated evaluation</h3><p data-block-key=\"4meke\">Manual evaluation is impractical for rapid iteration. Our system employs two novel evaluation components:</p><ol><li data-block-key=\"78n7\"><i>Readability assessment</i>: Moving beyond simplistic metrics like <a href=\"https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests\" target=\"_blank\" rel=\"noopener noreferrer\">Flesch-Kincaid</a>, we used a Gemini prompt to score text readability on a 1-10 scale. This prompt was iteratively refined against human judgment, enabling a more nuanced assessment of comprehension ease. We observed in testing that this LLM-based readability assessment aligns better with human readability assessments than Flesch-Kincaid.</li><li data-block-key=\"ddfcv\"><i>Fidelity assessment</i>: Ensuring meaning preservation is critical. Using Gemini 1.5 Pro, we implemented a process that maps claims from the original text to the simplified version. This method identifies specific error types like information loss, gain, or distortion, each weighted by severity, providing a granular measure of faithfulness to the original meaning (completeness and entailment).</li></ol><h3 data-block-key=\"8j5c6\">Iterative prompt refinement: LLMs optimizing LLMs</h3><p data-block-key=\"3e0ac\">The quality of the final simplification (generated by Gemini 1.5 Flash) heavily depends on the initial prompt. We automated the prompt optimization process itself via a <i>prompt refinement loop:</i> using the autoeval scores for readability and fidelity, another Gemini 1.5 Pro model analyzed the simplification prompt's performance and proposed refined prompts for the next iteration.</p><p data-block-key=\"4hmkp\">This creates a powerful feedback loop where an LLM system iteratively improves its own instructions based on performance metrics, reducing reliance on manual prompt engineering and enabling the discovery of highly effective simplification strategies. For this work, the loop ran for 824 iterations until performance plateaued.</p><p data-block-key=\"f383v\">This automated process, where one LLM evaluates the output of another and refines its instructions (prompts) based on performance metrics (readability and fidelity) and granular errors, represents a key innovation. It moves beyond laborious manual prompt engineering, enabling the system to autonomously discover highly effective strategies for nuanced simplification over hundreds of iterations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Simplify1_SummaryHero.width-1250.png\" alt=\"Simplify1_SummaryHero\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Simplify1_SummaryHero.width-1250.png\" alt=\"Simplify1_SummaryHero\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"dped5\"><i>Summary of Gemini-based approach for minimally-lossy text simplification.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qwfyl\">Measuring impact: A large-scale randomized study</h2><p data-block-key=\"6p6s5\">To validate the real-world effectiveness of text simplified with this approach, we conducted a <a href=\"https://en.wikipedia.org/wiki/Randomized_experiment\" target=\"_blank\" rel=\"noopener noreferrer\">randomized controlled study</a>.</p><h3 data-block-key=\"1lpbp\">Study design</h3><ul><li data-block-key=\"1lsge\"><i>Participants</i>: A large cohort of 4,563 consenting participants was recruited after screening for topic expertise.</li><li data-block-key=\"3utc\"><i>Texts</i>: We used 31 diverse, real-world text excerpts across domains known for complexity: medical research, biology, law, finance, literature, philosophy, aerospace, and computer science.</li><li data-block-key=\"831ev\"><i>Comparison</i>: Using a randomized complete block design (a study design that compares groups while accounting for variations), participants were randomly assigned to read either the original text, the simplified version, or both. To evaluate any further effects of text simplification on short-term retention of the content, users were tested under two conditions: one where they could refer back to the text while answering questions, and one where they could not.</li><li data-block-key=\"c4i02\"><i>Measurements</i>: We assessed comprehension using carefully reviewed multiple-choice questions (MCQs); self-reported confidence; and cognitive load via a simplified <a href=\"https://humansystems.arc.nasa.gov/groups/tlx/downloads/TLXScale.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">NASA Task Load Index</a>.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Simplify2_DesignFinal.width-1250.png\" alt=\"Simplify2_DesignFinal\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Simplify2_DesignFinal.width-1250.png\" alt=\"Simplify2_DesignFinal\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"dped5\"><i>Study design to evaluate the simplification model with real texts.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"0wr7z\">Results</h3><p data-block-key=\"5e9af\">Our study, encompassing nearly 50,000 MCQ answers, yielded statistically significant results demonstrating the value of simplification.</p><h4 data-block-key=\"3cu24\">Quantitative findings</h4><p data-block-key=\"49cvi\">Participants reading the simplified text achieved a 4% absolute increase in MCQ accuracy overall compared to those reading the original. The impact was most pronounced for the highly complex PubMed texts, which saw a 15% absolute accuracy gain. Significant gains were also observed in finance (6%), legal (4%), and technical —&nbsp;aeronautics/computer science (4%) — domains. These gains were robust even when participants could not refer back to the text, suggesting benefits for both immediate comprehension and short-term retention.</p><p data-block-key=\"crkop\">Beyond accuracy, participants reported higher confidence in their answers (average improvement of 0.24 on a -2 to 2 scale) and found the task easier (average improvement of 0.33 on a -2 to 2 scale, simplified from the task load index) when interacting with simplified text.</p><h4 data-block-key=\"fpbju\">Qualitative insights</h4><p data-block-key=\"222ri\">Reviewing examples where simplification significantly boosted participant MCQ accuracy (by 38% for one medical research text) reveals <i>how</i> it adds value. Consider the original passage below compared to its simplified version. The simplification enhances clarity by defining jargon (like 'emphysema' and 'fibrosis'), breaking down a dense sentence, and clarifying complex relationships.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Simplify3_Excerpt.width-1250.png\" alt=\"Simplify3_Excerpt\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Simplify3_Excerpt.width-1250.png\" alt=\"Simplify3_Excerpt\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"p6h08\"><i>*Excerpt from a biomedical article,</i> <a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC10177208/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>PMC10177208</i></a><i>, Creative Commons license (CC BY 4.0).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"oc9db\">Simplification proved particularly helpful for texts where baseline comprehension was low, as further illustrated by additional examples in <a href=\"https://arxiv.org/abs/2505.01980\" target=\"_blank\" rel=\"noopener noreferrer\">our paper</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"oc9db\">Study limitations</h3><p data-block-key=\"cgdkr\">Our study, while large-scale, has limitations. We leveraged a survey platform to recruit study participants, and this may not fully reflect a group of users actively seeking to understand complex information. While our system aims for high fidelity, LLM errors are possible, requiring ongoing vigilance. Lastly, MCQs, while scalable, offer an incomplete measure of deep understanding.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"oc9db\">Available as a new Simplify feature</h2><p data-block-key=\"e3f9d\"><a href=\"https://blog.google/products/search/simplify-google-app-ios\" target=\"_blank\" rel=\"noopener noreferrer\">Starting today</a> this capability is available in a new feature on the Google app for iOS, Simplify. To use it, users can select any complex text on a web page they’re visiting in the Google app, then tap the “Simplify” icon that appears to see a new, simpler version of the text, without having to stop reading or leave the page. This makes it easier for people to grasp new or complex topics they might encounter when trying to learn something new on the web.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"8u3tr\">Conclusion</h2><p data-block-key=\"788gc\">We developed and rigorously validated an automated system using Gemini that iteratively learns to simplify text while maintaining fidelity to the original. By demonstrably bridging the comprehension gap for complex information, this capability significantly enhances understanding and reduces cognitive load for users across critical domains.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"8u3tr\">Acknowledgements</h2><p data-block-key=\"85rcv\"><i>This work involved the efforts of a multidisciplinary team of software engineers, researchers, clinicians and cross functional contributors. Key contributors to this project include: Theo Guidroz, Jimmy Li, Adam Mansour, Paul Jhun, Nina Gonzalez, Xiang Ji, Mike Sanchez, Mathias MJ Bellaiche, Miguel Ángel Garrido, Faruk Ahmed, Divyansh Choudhary, Jay Hartford, Chenwei Xu, Henry Javier Serrano Echeverria, Yifan Wang, Jeff Shaffer, Eric (Yifan) Cao, Yossi Matias, Avinatan Hassidim, Dale R Webster, Yun Liu, Sho Fujiwara, Peggy Bui, Quang Duong.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    }
  ],
  "lastUpdated": "2025-06-15T10:29:51.570Z"
}