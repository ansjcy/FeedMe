{
  "sourceUrl": "https://rsshub.rssforever.com/google/research",
  "title": "Google Research Blog",
  "description": "Google Research Blog - Powered by RSSHub",
  "link": "https://research.google/blog",
  "items": [
    {
      "title": "使用 JAX-Privacy 实现大规模差分隐私机器学习 (原标题: Differentially private machine learning at scale with JAX-Privacy)",
      "link": "https://research.google/blog/differentially-private-machine-learning-at-scale-with-jax-privacy/",
      "pubDate": "Tue, 11 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-11T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 使用 JAX-Privacy 实现大规模差分隐私机器学习\n\n人工智能模型在改善生活和变革行业方面发挥着关键作用，其影响和准确性往往取决于所使用数据的质量。为了开发准确且具有代表性的人工智能模型，高质量的大型数据集至关重要，但同时必须以保护个人隐私的方式使用这些数据。JAX 和 JAX-Privacy 正是为了解决这一挑战而生。\n\n## JAX 简介\n\nJAX 是一个于2020年推出的高性能数值计算库，专为大规模机器学习而设计。其核心特性包括自动微分、即时编译以及跨多个加速器的无缝扩展，使其成为高效构建和训练复杂模型的理想平台。JAX 已成为推动人工智能边界的研究人员和工程师的基石，其生态系统包含Flax（简化神经网络架构实现）和Optax（实现最先进优化器）等领域特定库。\n\n## JAX-Privacy 简介\n\nJAX-Privacy 构建于 JAX 之上，是一个强大的工具包，用于构建和审计差分隐私（DP）模型。它使研究人员和开发人员能够快速高效地实现差分隐私算法，用于在大数据集上训练深度学习模型，并提供将隐私训练集成到现代分布式训练工作流所需的核心工具。JAX-Privacy 的最初版本于2022年推出，旨在帮助外部研究人员复现和验证谷歌在隐私训练方面的一些进展。此后，它已发展成为一个中心，谷歌内部的各个研究团队在此集成他们关于 DP 训练和审计算法的最新研究成果。\n\n### JAX-Privacy 1.0 发布\n\n谷歌自豪地宣布发布 JAX-Privacy 1.0 版本。该新版本集成了最新的研究进展，并重新设计以提高模块化，使研究人员和开发人员能够比以往更轻松地构建结合最先进 DP 算法和 JAX 提供可扩展性的 DP 训练管道。\n\n## JAX-Privacy 的必要性\n\n多年来，差分隐私（DP）一直被认为是量化和限制隐私泄露的黄金标准。DP 保证算法的输出几乎相同，无论数据集中是否包含单个个体（或示例）。\n\n尽管 DP 理论已经成熟，但其在大规模机器学习中的实际实现却充满挑战。最常见的方法——差分隐私随机梯度下降（DP-SGD）——需要定制的批处理程序、逐例梯度裁剪以及添加经过仔细校准的噪声。这个过程计算密集，难以正确高效地实现，尤其是在现代基础模型的规模下。\n\n![JAXPrivacy2_Overview](https://storage.googleapis.com/gweb-research2023-media/images/JAXPrivacy2_Overview.width-1250.png)\n\nJAX-Privacy 通过其用于梯度裁剪和相关噪声生成的原始构建块，实现了在分布式环境中高效工作，从而使研究人员和开发人员能够使用最先进的差分隐私算法，以可扩展和高效的方式在私有数据上训练和微调基础模型。现有框架虽有进步，但在可扩展性或灵活性方面往往不足。谷歌的工作一直致力于推动隐私机器学习的边界，从开创新的 DP 算法到开发复杂的审计技术。因此，需要一个能够跟上研究步伐的工具——一个不仅正确高效，而且从一开始就设计用于处理最先进模型的并行性和复杂性的库。JAX 的函数式范式和强大的转换（如 vmap 和 shard_map）提供了坚实的基础。通过在 JAX 上构建，谷歌创建了一个开箱即用的并行化库，支持跨多个加速器和超级计算机训练大规模模型。JAX-Privacy 是这项努力的结晶，一个经过时间考验的库，已为内部生产集成提供支持，现在正与更广泛的社区共享。\n\n## JAX-Privacy 的核心功能\n\nJAX-Privacy 通过提供一套精心设计的组件，简化了 DP 的复杂性：\n\n*   **核心构建块**：该库提供了基本 DP 原语的正确高效实现，包括逐例梯度裁剪、噪声添加和数据批次构建。这些组件使开发人员能够自信地构建 DP-SGD 和 DP-FTRL 等知名算法。\n*   **最先进的算法**：JAX-Privacy 不仅限于基础功能，还支持高级方法，如依赖于在迭代中注入相关噪声的 DP 矩阵分解，这已被证明可以提高性能。这使得研究人员可以轻松尝试尖端的隐私训练技术。\n*   **可扩展性**：所有组件都设计为与 JAX 的原生并行化功能无缝协作。这意味着您可以训练需要数据和模型并行化的大规模模型，而无需复杂的自定义代码，从而使大型模型的隐私训练成为现实。JAX-Privacy 还提供微批处理和填充等工具，用于无缝处理通常需要获得最佳隐私/效用权衡的巨大、可变大小的批次。\n*   **正确性和审计**：该库建立在谷歌最先进的 DP 审计库之上，确保噪声校准在数学上是正确的，并且尽可能紧密。这些关于隐私损失的正式界限可以通过量化经验隐私损失的指标来补充，从而提供训练管道隐私属性的更完整视图。用户可以轻松测试和开发自己的审计技术，例如谷歌在“差分隐私机器学习的严格审计”方面的获奖工作，该工作通过注入“金丝雀”（已知数据点）并在每个步骤计算审计指标来发挥作用。\n\nJAX-Privacy 实现了多种基础工具，用于裁剪、噪声添加、批次选择、审计和核算，这些工具可以以各种方式组合，以构建端到端的 DP 训练计划。\n\n## 从研究到实践：自信地微调大型语言模型（LLMs）\n\nJAX-Privacy 最令人兴奋的方面之一是其实际应用。该库旨在支持用于预训练和微调 LLM 的现代机器学习框架。一个显著的例子是谷歌最近在训练 VaultGemma（世界上最强大的差分隐私 LLM）时使用了 JAX-Privacy 构建块。\n\n通过此次开源发布，谷歌希望开发人员能够通过流行的 Keras 框架，仅用几行代码即可轻松微调大型模型。特别是，谷歌提供了用于微调 Gemma 系列模型（由 Google DeepMind 基于 Gemini 构建的开源模型集合）的完整功能示例。这些示例展示了如何将 JAX-Privacy 应用于对话摘要和合成数据生成等任务，表明该库即使在处理最先进的模型时也能提供最先进的结果。\n\n通过简化 DP 的集成，JAX-Privacy 赋能开发人员从头开始构建隐私保护应用程序，无论是为医疗保健应用微调聊天机器人，还是为个性化金融建议微调模型。它降低了隐私保护机器学习的入门门槛，使强大、负责任的人工智能更易于访问。\n\n## 展望未来\n\n谷歌很高兴与研究社区分享 JAX-Privacy。此次发布是多年不懈努力的成果，代表着对隐私保护机器学习领域的重大贡献。谷歌希望通过提供这些工具，能够开启新一波的研究和创新，造福所有人。谷歌将继续支持和开发该库，整合新的研究进展并响应社区的需求。鼓励用户查看 GitHub 上的存储库或 PIP 包，立即开始训练隐私保护机器学习模型。\n\n**致谢**：JAX-Privacy 包含来自 Leonard Berrada, Robert Stanforth, Brendan McMahan, Christopher A. Choquette-Choo, Galen Andrew, Mikhail Pravilov, Sahra Ghalebikesabi, Aneesh Pappu, Michael Reneer, Jamie Hayes, Vadym Doroshenko, Keith Rush, Dj Dvijotham, Zachary Charles, Peter Kairouz, Soham De, Samuel L. Smith, Judy Hanwen Shen 的贡献。",
      "shortSummary": "JAX-Privacy 1.0 是一个基于高性能 JAX 库的工具包，旨在实现大规模差分隐私（DP）机器学习。它解决了在大型模型中高效实施 DP 的挑战，提供核心构建块、先进算法、可扩展性和审计功能。JAX-Privacy 简化了 DP 训练管道的构建，支持在保护隐私的同时训练和微调大型语言模型（如 VaultGemma 和 Gemma 系列），从而降低了隐私保护机器学习的门槛，使负责任的 AI 更易于实现。",
      "translated_title": "使用 JAX-Privacy 实现大规模差分隐私机器学习",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/JAXPrivacy2_Overview.width-1250.png",
          "alt": "JAXPrivacy2_Overview",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xtj9j\">From personalized recommendations to scientific advances, AI models are helping to improve lives and transform industries. But the impact and accuracy of these AI models is often determined by the quality of data they use. Large, high-quality datasets are crucial for developing accurate and representative AI models, however, they must be used in ways that preserve individual privacy.</p><p data-block-key=\"b23gn\">That’s where <a href=\"https://docs.jax.dev/en/latest/\" target=\"_blank\" rel=\"noopener noreferrer\">JAX</a> and <a href=\"https://github.com/google-deepmind/jax_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">JAX-Privacy</a> come in. Introduced in 2020, JAX is a high-performance numerical computing library designed for large-scale machine learning (ML). Its core features — including <a href=\"https://en.wikipedia.org/wiki/Automatic_differentiation\" target=\"_blank\" rel=\"noopener noreferrer\">automatic differentiation</a>, <a href=\"https://en.wikipedia.org/wiki/Just-in-time_compilation\" target=\"_blank\" rel=\"noopener noreferrer\">just-in-time compilation</a>, and seamless scaling across multiple accelerators — make it an ideal platform for building and training complex models efficiently. JAX <a href=\"https://github.com/jax-ml/jax/network/dependents\" target=\"_blank\" rel=\"noopener noreferrer\">has become a cornerstone</a> for researchers and engineers pushing the boundaries of AI. Its surrounding ecosystem includes a robust set of domain-specific libraries, including <a href=\"https://flax.readthedocs.io/en/latest/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">Flax</a>, which simplifies the implementation of neural network architectures, and <a href=\"https://github.com/google-deepmind/optax\" target=\"_blank\" rel=\"noopener noreferrer\">Optax</a>, which implements state-of-the-art optimizers.</p><p data-block-key=\"fkr7i\">Built on JAX, JAX-Privacy is a robust toolkit for building and auditing differentially private models. It enables researchers and developers to quickly and efficiently implement <a href=\"https://en.wikipedia.org/wiki/Differential_privacyhttps://en.wikipedia.org/wiki/Differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">differentially private</a> (DP) algorithms for training deep learning models on large datasets, and provides the core tools needed to integrate private training into modern distributed training workflows. The original version of JAX-Privacy was introduced in 2022 to enable external researchers to reproduce and validate some of our <a href=\"https://github.com/google-deepmind/jax_privacy#reproducing-results\" target=\"_blank\" rel=\"noopener noreferrer\">advances on private training</a>. It has since evolved into a hub where research teams across Google integrate their novel research insights into DP training and auditing algorithms.</p><p data-block-key=\"857iv\">Today, we are proud to announce the release of <a href=\"https://github.com/google-deepmind/jax_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">JAX-Privacy 1.0</a>. Integrating our latest research advances and re-designed for modularity, this new version makes it easier than ever for researchers and developers to build DP training pipelines that combine state-of-the-art DP algorithms with the scalability provided by JAX.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xtj9j\">How we got here: The need for JAX-Privacy</h2><p data-block-key=\"d6p4s\">For years, researchers have turned to DP as the gold standard for quantifying and bounding privacy leakage. DP guarantees that the output of an algorithm is nearly the same whether or not a single individual (or example) is included in the dataset.</p><p data-block-key=\"efhk8\">While the theory of DP is well-established, its practical implementation in large-scale ML can be a challenge. The most common approach, <a href=\"https://arxiv.org/abs/1607.00133\" target=\"_blank\" rel=\"noopener noreferrer\">differentially private stochastic gradient descent</a> (DP-SGD), requires customized batching procedures, per-example gradient clipping, and the addition of carefully calibrated noise. This process is computationally intensive and can be difficult to implement correctly and efficiently, especially at the scale of modern foundation models.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/JAXPrivacy2_Overview.width-1250.png\" alt=\"JAXPrivacy2_Overview\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/JAXPrivacy2_Overview.width-1250.png\" alt=\"JAXPrivacy2_Overview\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"q4d8f\"><i>JAX-Privacy enables researchers and developers to train and fine-tune foundation models on private data using state-of-the-art differentially private algorithms in a scalable and efficient way thanks to its primitive building blocks for gradient clipping and correlated noise generation, both of which work effectively in distributed environments.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xtj9j\">Existing frameworks have made strides, but they often fall short in scalability or flexibility. Our work has consistently pushed the boundaries of private ML, from <a href=\"https://arxiv.org/abs/1607.00133\" target=\"_blank\" rel=\"noopener noreferrer\">pioneering new DP algorithms</a> to <a href=\"https://arxiv.org/abs/2302.07956\" target=\"_blank\" rel=\"noopener noreferrer\">developing sophisticated auditing techniques</a>. We needed a tool that could keep pace with our research — a library that was not only correct and efficient but also designed from the ground up to handle the parallelism and complexity of state-of-the-art models.</p><p data-block-key=\"1shto\">JAX's functional paradigm and powerful transformations, like <code>vmap</code> (for automatic vectorization) and <code>shard_map</code> (for single-program multiple-data parallelization), provided a strong foundation. By building on JAX, we could create a library that was parallelism-ready out-of-the-box, supporting the training of large-scale models across multiple accelerators and supercomputers. JAX-Privacy is the culmination of this effort, a time-tested library that has powered internal production integrations and is now being shared with the broader community.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xtj9j\">What JAX-Privacy delivers</h2><p data-block-key=\"ba6f3\">JAX-Privacy simplifies the complexities of DP by providing a suite of carefully engineered components:</p><ul><li data-block-key=\"1qgme\"><i>Core building blocks</i>: The library offers correct and efficient implementations of the fundamental DP primitives, including <a href=\"https://github.com/google-deepmind/jax_privacy/blob/main/jax_privacy/dp_sgd/grad_clipping.py\" target=\"_blank\" rel=\"noopener noreferrer\">per-example gradient clipping</a>, <a href=\"https://github.com/google-deepmind/jax_privacy/tree/main/jax_privacy/noise_addition\" target=\"_blank\" rel=\"noopener noreferrer\">noise addition</a>, and <a href=\"https://github.com/google-deepmind/jax_privacy/blob/main/jax_privacy/experimental/batch_selection.py\" target=\"_blank\" rel=\"noopener noreferrer\">data batch construction</a>. These components enable developers to build well-known algorithms like DP-SGD and <a href=\"https://arxiv.org/abs/2103.00039\" target=\"_blank\" rel=\"noopener noreferrer\">DP-FTRL</a> with confidence.</li><li data-block-key=\"7gq93\"><i>State-of-the-art algorithms</i>: JAX-Privacy goes beyond the basics, supporting advanced methods like <a href=\"https://arxiv.org/abs/2506.08201\" target=\"_blank\" rel=\"noopener noreferrer\">DP matrix factorization</a> that rely on injecting correlated noise across iterations, which have been shown to improve performance. This makes it easy for researchers to experiment with cutting-edge private training techniques.</li><li data-block-key=\"430g0\"><i>Scalability</i>: All components are designed to work seamlessly with JAX's native parallelism features. This means you can train large-scale models that require data and model parallelism without complex, custom code, making private training on large models a reality. JAX-Privacy also provides tools like micro-batching and padding for seamlessly handling massive, variable-sized batches that are typically needed to obtain the best privacy/utility trade-offs.</li><li data-block-key=\"7nhv3\"><i>Correctness and</i> <a href=\"https://github.com/google-deepmind/jax_privacy/tree/main/jax_privacy/auditing\" target=\"_blank\" rel=\"noopener noreferrer\"><i>auditing</i></a>: The library is built on Google's state-of-the-art <a href=\"https://github.com/google/differential-privacy/tree/main/python/dp_accounting\" target=\"_blank\" rel=\"noopener noreferrer\">DP accounting library</a>, ensuring the noise calibration is both mathematically correct and as tight as possible. These formal bounds on the privacy loss can be complemented with metrics that quantify the empirical privacy loss, providing a more complete view of the privacy properties of a training pipeline. Users can easily test and develop their own auditing techniques, like our award-winning work on \"<a href=\"https://www.usenix.org/conference/usenixsecurity23/presentation/nasr\" target=\"_blank\" rel=\"noopener noreferrer\">Tight Auditing of Differentially Private Machine Learning</a>\", which works by injecting \"canaries\" — known data points — and computing auditing metrics at each step.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/JAXPrivacy3_Animation.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"q4d8f\"><i>JAX-Privacy implements a variety of foundational tools for clipping, noise addition, batch selection, accounting, and auditing that can be combined in various ways to construct end-to-end DP training plans.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xtj9j\">From research to practice: Fine-tuning LLMs with confidence</h2><p data-block-key=\"9h6ls\">One of the most exciting aspects of JAX-Privacy is its practical application. The library is designed to support modern ML frameworks used for pre-training and fine-tuning LLMs. A notable example is our recent use of JAX-Privacy building blocks in the training of <a href=\"https://research.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/\">VaultGemma</a>, the world's most capable differentially private LLM.</p><p data-block-key=\"6fss3\">With this open-source release, we want to enable developers to easily fine-tune large models with just a few lines of code via the popular <a href=\"https://keras.io/examples/nlp/\" target=\"_blank\" rel=\"noopener noreferrer\">Keras</a> framework. In particular, we include <a href=\"https://github.com/google-deepmind/jax_privacy/tree/main/examples\" target=\"_blank\" rel=\"noopener noreferrer\">fully-functional examples</a> for fine-tuning models in the <a href=\"https://developers.googleblog.com/en/gemma-explained-overview-gemma-model-family-architectures/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma family</a>, a collection of open models built by Google DeepMind based on Gemini. These examples demonstrate how to apply JAX-Privacy to tasks like dialogue summarization and synthetic data generation, showing that this library can deliver state-of-the-art results even when working with the most advanced models.</p><p data-block-key=\"b2918\">By simplifying the integration of DP, JAX-Privacy empowers developers to build privacy-preserving applications from the ground up, whether they are fine-tuning a chatbot for a healthcare application or a model for personalized financial advice. It lowers the barrier to entry for privacy-preserving ML and makes powerful, responsible AI more accessible.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xtj9j\">Looking ahead</h2><p data-block-key=\"7gt08\">We are excited to share JAX-Privacy with the research community. This release is the result of years of dedicated effort and represents a significant contribution to the field of privacy-preserving ML. We hope that by providing these tools, we can enable a new wave of research and innovation that benefits everyone.</p><p data-block-key=\"7ne51\">We will continue to support and develop the library, incorporating new research advances and responding to the needs of the community. We look forward to seeing what you build using JAX-Privacy. Check out the <a href=\"https://github.com/google-deepmind/jax_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">repository on GitHub</a> or the <a href=\"https://pypi.org/project/jax-privacy/\" target=\"_blank\" rel=\"noopener noreferrer\">PIP package</a> to start training privacy-preserving ML models today.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xtj9j\">Acknowledgements</h2><p data-block-key=\"5vdv8\"><i>JAX-Privacy includes contributions from: Leonard Berrada, Robert Stanforth, Brendan McMahan, Christopher A. Choquette-Choo, Galen Andrew, Mikhail Pravilov, Sahra Ghalebikesabi, Aneesh Pappu, Michael Reneer, Jamie Hayes, Vadym Doroshenko, Keith Rush, Dj Dvijotham, Zachary Charles, Peter Kairouz, Soham De, Samuel L. Smith, Judy Hanwen Shen.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "介绍嵌套学习：一种用于持续学习的新机器学习范式 (原标题: Introducing Nested Learning: A new ML paradigm for continual learning)",
      "link": "https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/",
      "pubDate": "Thu, 06 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-06T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 介绍嵌套学习：一种用于持续学习的新机器学习范式\n\n### 引言与挑战\n\n机器学习（ML）在过去十年取得了显著进步，但**持续学习**——模型在不遗忘旧知识的情况下主动获取新知识和技能——仍然是一个核心挑战。大型语言模型（LLM）尤其面临“**灾难性遗忘**”（CF）问题，即学习新任务会牺牲对旧任务的熟练度。人脑通过**神经可塑性**实现持续学习，而当前LLM的知识受限于即时上下文或预训练信息。传统上，研究人员通过架构调整或优化规则来解决CF，但他们将模型架构和优化算法视为独立实体，阻碍了统一高效学习系统的实现。\n\n### 嵌套学习范式\n\n在NeurIPS 2025发表的论文《嵌套学习：深度学习架构的幻象》中，研究人员引入了**嵌套学习**，旨在弥合这一差距。嵌套学习将单个ML模型视为一个由相互连接、多层次学习问题组成的系统，这些问题同时进行优化。其核心观点是：模型架构和训练规则（优化算法）本质上是相同的概念，它们只是不同“级别”的优化，各自拥有独特的“上下文流”和更新速率。通过识别这种内在结构，嵌套学习为设计更强大的AI提供了新的维度，允许构建具有更深计算深度的学习组件，从而有助于解决灾难性遗忘等问题。\n\n该范式揭示，一个复杂的ML模型实际上是一组相互嵌套或并行运行的连贯、相互连接的优化问题，每个问题都有其独特的上下文流。这种视角暗示，现有深度学习方法通过压缩其内部上下文流来工作。\n\n### 关联记忆的视角\n\n文章通过关联记忆的概念来阐释嵌套学习：\n*   **训练过程**（特别是反向传播）可以建模为关联记忆，模型学习将给定数据点映射到其局部误差值，作为数据点“意外”程度的度量。\n*   **关键架构组件**，如Transformer中的注意力机制，也可以形式化为简单的关联记忆模块，学习序列中token之间的映射。\n\n### 人脑与嵌套学习的类比\n\n![Diagram comparing biological brain waves and neuroplasticity to the uniform structure and multi-frequency updates used in Nested Learning models.](https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-1a-Inspiration.width-1250.png)\n\n人脑中统一可重用的结构以及多时间尺度更新是人类持续学习的关键组成部分。嵌套学习允许大脑每个组件进行多时间尺度更新，并表明Transformer和记忆模块等知名架构实际上是具有不同频率更新的线性层。通过定义更新频率（即每个组件的权重调整频率），可以将这些相互连接的优化问题组织成“级别”，形成嵌套学习范式的核心。\n\n### 嵌套学习的应用\n\n嵌套学习的视角提供了改进现有算法和架构的原则性方法：\n*   **深度优化器**：将优化器视为关联记忆模块，通过将底层目标改为L2回归损失等标准损失指标，推导出新的动量公式，使其对不完善数据更具弹性。\n*   **连续记忆系统（CMS）**：将Transformer的短期记忆（序列模型）和长期记忆（前馈网络）概念扩展。记忆被视为一系列模块，每个模块以不同的特定频率更新，从而为持续学习创建更丰富、更有效的记忆系统。\n\n### Hope：一种自修改架构\n\n作为概念验证，研究人员利用嵌套学习原理设计了**Hope**，它是Titans架构的变体。Hope是一种自修改的循环架构，能够利用无限级别的上下文学习，并通过CMS模块扩展到更大的上下文窗口。它可以通过自指过程优化自身的记忆，创建一个具有无限循环学习级别的架构。\n\n### 实验与结果\n\n实验评估了深度优化器和Hope在语言建模、长上下文推理、持续学习和知识整合任务上的有效性。结果证实了嵌套学习、连续记忆系统设计和自修改Titans的强大能力：\n\n*   在多样化的语言建模和常识推理任务上，Hope架构展现出比现代循环模型和标准Transformer更低的困惑度和更高的准确性。\n\n![Bar chart that shows the Hope model outperforming Titans, Samba, and Transformer on both language modeling and common-sense reasoning performance metrics.](https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-1-Performance.width-1250.png)\n\n*   在长上下文“大海捞针”（NIAH）下游任务中，Hope展现出卓越的记忆管理能力，证明CMS能更高效地处理扩展信息序列。\n\n![Bar chart showing Hope and Titans models consistently outperforming TTT and Mamba2 across long-context tasks of three difficulty levels.](https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-2-LongContext.width-1250.png)\n\n### 结论\n\n嵌套学习范式通过将架构和优化视为一个连贯的嵌套优化问题系统，为深度学习设计开辟了新维度。由此产生的模型，如Hope架构，表明统一这些元素的原则性方法可以带来更具表现力、能力更强、效率更高的学习算法。该范式为弥合当前LLM的局限性与人脑卓越的持续学习能力之间的差距提供了坚实基础。",
      "shortSummary": "嵌套学习是一种新的机器学习范式，它将模型架构和优化算法统一为相互连接的多层次学习问题。该范式通过识别不同优化级别的“上下文流”和更新速率，旨在解决大型语言模型中的“灾难性遗忘”问题，并实现持续学习。概念验证模型“Hope”展示了在语言建模和长上下文记忆管理方面的卓越性能，为构建能够自我改进的AI提供了新途径，弥合了当前LLM与人脑持续学习能力之间的差距。",
      "translated_title": "介绍嵌套学习：一种用于持续学习的新机器学习范式",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-1a-Inspiration.width-1250.png",
          "alt": "Diagram comparing biological brain waves and neuroplasticity to the uniform structure and multi-frequency updates used in Nested Learning models.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-1-Performance.width-1250.png",
          "alt": "Bar chart that shows the Hope model outperforming Titans, Samba, and Transformer on both language modeling and common-sense reasoning performance metrics.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-2-LongContext.width-1250.png",
          "alt": "Bar chart showing Hope and Titans models consistently outperforming TTT and Mamba2 across long-context tasks of three difficulty levels.",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"1guni\">The last decade has seen incredible progress in machine learning (ML), primarily driven by powerful neural network architectures and the algorithms used to train them. However, despite the success of large language models (LLMs), a few fundamental challenges persist, especially around continual learning, the ability for a model to actively acquire new knowledge and skills over time without forgetting old ones.</p><p data-block-key=\"9an83\">When it comes to continual learning and self-improvement, the human brain is the gold standard. It adapts through neuroplasticity — the remarkable capacity to change its structure in response to new experiences, memories, and learning. Without this ability, a person is limited to immediate context (like <a href=\"https://en.wikipedia.org/wiki/Anterograde_amnesia\" target=\"_blank\" rel=\"noopener noreferrer\">anterograde amnesia</a>). We see a similar limitation in current LLMs: their knowledge is confined to either the immediate context of their input window or the static information that they learn during pre-training.</p><p data-block-key=\"fh441\">The simple approach, continually updating a model's parameters with new data, often leads to “<a href=\"https://en.wikipedia.org/wiki/Catastrophic_interference\" target=\"_blank\" rel=\"noopener noreferrer\">catastrophic forgetting</a>” (CF), where learning new tasks sacrifices proficiency on old tasks. Researchers traditionally combat CF through architectural tweaks or better optimization rules. However, for too long, we have treated the model's architecture (the network structure) and the optimization algorithm (the training rule) as two separate things, which prevents us from achieving a truly unified, efficient learning system.</p><p data-block-key=\"9ifb6\">In our paper, “<a href=\"http://abehrouz.github.io/files/NL.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Nested Learning: The Illusion of Deep Learning Architectures</a>”, published at <a href=\"https://neurips.cc/virtual/2025/poster/116123\" target=\"_blank\" rel=\"noopener noreferrer\">NeurIPS 2025</a>, we introduce Nested Learning, which bridges this gap. Nested Learning treats a single ML model not as one continuous process, but as a system of interconnected, multi-level learning problems that are optimized simultaneously. We argue that the model's architecture and the rules used to train it (i.e., the optimization algorithm) are fundamentally the same concepts; they are just different \"levels\" of optimization, each with its own internal flow of information (\"context flow\") and update rate. By recognizing this inherent structure, Nested Learning provides a new, previously invisible dimension for designing more capable AI, allowing us to build learning components with deeper computational depth, which ultimately helps solve issues like catastrophic forgetting.</p><p data-block-key=\"cp75j\">We test and validate Nested Learning through a proof-of-concept, self-modifying architecture that we call “Hope”, which achieves superior performance in language modeling and demonstrates better long-context memory management than existing state-of-the-art models.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The Nested Learning paradigm</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1guni\">Nested Learning reveals that a complex ML model is actually a set of coherent, interconnected optimization problems nested within each other or running in parallel. Each of these internal problems has its own <i>context flow</i> — its own distinct set of information from which it is trying to learn.</p><p data-block-key=\"aog9\">This perspective implies that existing deep learning methods work by essentially <i>compressing</i> their internal context flows. More importantly, Nested Learning reveals a new dimension for designing models, allowing us to build learning components with deeper computational depth.</p><p data-block-key=\"28ofj\">To illustrate this paradigm, we look at the concept of <a href=\"https://en.wikipedia.org/wiki/Associative_memory_(psychology)\" target=\"_blank\" rel=\"noopener noreferrer\">associative memory</a> — the ability to map and recall one thing based on another (like recalling a name when you see a face).</p><ul><li data-block-key=\"565p0\">We show that the training process itself, specifically the <a href=\"https://en.wikipedia.org/wiki/Backpropagation\" target=\"_blank\" rel=\"noopener noreferrer\">backpropagation</a> process, can be modeled as an associative memory. The model learns to map a given data point to the value of its local error, which serves as a measure of how \"surprising\" or unexpected that data point was.</li><li data-block-key=\"d9pfu\">Similarly, following previous studies (e.g., <a href=\"https://arxiv.org/pdf/2504.13173\" target=\"_blank\" rel=\"noopener noreferrer\">Miras</a>), key architectural components, such as the <a href=\"https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\" target=\"_blank\" rel=\"noopener noreferrer\">attention mechanism in transformers</a>, can also be formalized as simple associative memory modules that learn the mapping between tokens in a sequence.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-1a-Inspiration.width-1250.png\" alt=\"Diagram comparing biological brain waves and neuroplasticity to the uniform structure and multi-frequency updates used in Nested Learning models.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-1a-Inspiration.width-1250.png\" alt=\"Diagram comparing biological brain waves and neuroplasticity to the uniform structure and multi-frequency updates used in Nested Learning models.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"48tdd\">The uniform and reusable structure as well as multi-time–scale update in the brain are the key components of continual learning in humans. Nested Learning allows for multi-time–scale updates for each component of the brain, while showing that well-known architectures such as transformers and memory modules are in fact linear layers with different frequency updates.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"1guni\">By defining an update frequency rate, i.e., how often each component's weights are adjusted, we can order these interconnected optimization problems into \"levels.\" This ordered set forms the heart of the Nested Learning paradigm.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Putting Nested Learning to work</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1guni\">The Nested Learning perspective immediately gives us principled ways to improve existing algorithms and architectures:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Deep optimizers</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1guni\">Since Nested Learning views optimizers (e.g., momentum-based optimizers) as associative memory modules, it allows us to apply principles from associative memory perspective to them. We observed that many standard optimizers rely on simple <a href=\"https://math.stackexchange.com/questions/689022/how-does-the-dot-product-determine-similarity\" target=\"_blank\" rel=\"noopener noreferrer\">dot-product similarity</a> (a measure of how alike two vectors are by calculating the sum of the products of their corresponding components) whose update doesn't account for how different data samples relate to each other. By changing the underlying objective of the optimizer to a more standard loss metric, such as <a href=\"https://en.wikipedia.org/wiki/Ridge_regression\" target=\"_blank\" rel=\"noopener noreferrer\">L2 regression loss</a> (a common loss function in regression tasks that quantifies the error by summing the squares of the differences between predicted and true values), we derive new formulations for core concepts like momentum, making them more resilient to imperfect data.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Continuum memory systems</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1guni\">In a standard Transformer, the sequence model acts as a short-term memory, holding the immediate context, while the <a href=\"https://en.wikipedia.org/wiki/Feedforward_neural_network\" target=\"_blank\" rel=\"noopener noreferrer\">feedforward neural networks</a> act as long-term memory, storing pre-training knowledge. The Nested Learning paradigm extends this concept into what we call a “continuum memory system” (CMS), where memory is seen as a spectrum of modules, each updating at a different, specific frequency rate. This creates a much richer and more effective memory system for continual learning.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Hope: A self-modifying architecture with continuum memory</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1guni\">As a proof-of-concept, we used Nested Learning principles to design Hope, a variant of the <a href=\"https://arxiv.org/abs/2501.00663\" target=\"_blank\" rel=\"noopener noreferrer\">Titans</a> architecture. Titans architectures are long-term memory modules that prioritize memories based on how surprising they are. Despite their powerful memory management, they only have two levels of parameters update, resulting in a first-order in-context learning. Hope, however, is a self-modifying recurrent architecture that can take advantage of unbounded levels of in-context learning and also is augmented with CMS blocks to scale to larger context windows. It can essentially optimize its own memory through a <a href=\"https://people.idsia.ch/~juergen/selfref1992.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">self-referential process</a>, creating an architecture with infinite, looped learning levels.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Experiments</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1guni\">We conducted experiments to evaluate the effectiveness of our deep optimizers and the performance of Hope on language modeling, long-context reasoning, continual learning, and knowledge incorporation tasks. The full results are available in our <a href=\"http://abehrouz.github.io/files/NL.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Results</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1guni\">Our experiments confirm the power of Nested Learning, the design of continuum memory systems, and self-modifying Titans.</p><p data-block-key=\"ca57m\">On a diverse set of commonly used and public language modeling and common-sense reasoning tasks, the Hope architecture demonstrates lower perplexity and higher accuracy compared to modern recurrent models and standard transformers.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-1-Performance.width-1250.png\" alt=\"Bar chart that shows the Hope model outperforming Titans, Samba, and Transformer on both language modeling and common-sense reasoning performance metrics.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-1-Performance.width-1250.png\" alt=\"Bar chart that shows the Hope model outperforming Titans, Samba, and Transformer on both language modeling and common-sense reasoning performance metrics.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"48tdd\">Comparison of performance on language modeling (<a href=\"https://en.wikipedia.org/wiki/Perplexity\" target=\"_blank\" rel=\"noopener noreferrer\">perplexity</a>; left) and common-sense reasoning (accuracy; right) tasks between different architectures: Hope, Titans, <a href=\"https://arxiv.org/pdf/2406.07522\" target=\"_blank\" rel=\"noopener noreferrer\">Samba</a> and a baseline Transformer.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"1guni\">Hope showcases superior memory management in long-context Needle-In-Haystack (NIAH) downstream tasks, proving that the CMSs offer a more efficient and effective way to handle extended sequences of information.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-2-LongContext.width-1250.png\" alt=\"Bar chart showing Hope and Titans models consistently outperforming TTT and Mamba2 across long-context tasks of three difficulty levels.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-2-LongContext.width-1250.png\" alt=\"Bar chart showing Hope and Titans models consistently outperforming TTT and Mamba2 across long-context tasks of three difficulty levels.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"48tdd\">Performance comparison on long-context tasks with different levels of difficulty between different architectures: Hope, Titans, <a href=\"https://arxiv.org/pdf/2407.04620\" target=\"_blank\" rel=\"noopener noreferrer\">TTT</a>, and <a href=\"https://arxiv.org/pdf/2405.21060\" target=\"_blank\" rel=\"noopener noreferrer\">Mamba2</a>. NIAH-PK, NIAH-H, and NIAH-W are needle-in-a-haystack tasks with pass-key, number, and word, respectively.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1guni\">The Nested Learning paradigm represents a step forward in our understanding of deep learning. By treating architecture and optimization as a single, coherent system of nested optimization problems, we unlock a new dimension for design, stacking multiple levels. The resulting models, like the Hope architecture, show that a principled approach to unifying these elements can lead to more expressive, capable, and efficient learning algorithms.</p><p data-block-key=\"ejnn7\">We believe the Nested Learning paradigm offers a robust foundation for closing the gap between the limited, forgetting nature of current LLMs and the remarkable continual learning abilities of the human brain. We are excited for the research community to explore this new dimension and help us build the next generation of self-improving AI.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1guni\"><i>This research was conducted by Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, and Vahab Mirrokni. We thank Praneeth Kacham and Corinna Cortes for reviewing the work and their valuable suggestions. We also thank Yuan Deng and Zeman Li. Finally, we thank Mark Simborg and Kimberly Schwede for their help in crafting this blog post.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "DS-STAR：一种最先进的多功能数据科学智能体 (原标题: DS-STAR: A state-of-the-art versatile data science agent)",
      "link": "https://research.google/blog/ds-star-a-state-of-the-art-versatile-data-science-agent/",
      "pubDate": "Wed, 05 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-05T16:00:00.000Z",
      "creator": "Google",
      "summary": "# DS-STAR：一种最先进的多功能数据科学智能体\n\n## 引言：数据科学的挑战与现有智能体的局限性\n\n数据科学致力于将原始数据转化为有意义、可操作的洞察，在解决现实世界挑战中扮演着关键角色。然而，数据科学过程通常复杂，需要计算机科学和统计学等领域的高度专业知识，涉及文档解释、数据处理和统计分析等耗时活动。\n\n为了简化这一复杂流程，近期研究利用现成的大型语言模型（LLMs）创建自主数据科学智能体，旨在将自然语言问题转化为可执行代码。尽管取得了显著进展，但当前的数据科学智能体存在局限性，阻碍了其实际应用：\n*   **高度依赖结构化数据**：它们主要依赖CSV文件等结构化数据，忽略了JSON、非结构化文本和Markdown文件等多样化异构数据格式中包含的宝贵信息。\n*   **难以验证开放式问题**：许多数据科学问题是开放式的，缺乏真实标签，难以验证智能体推理的正确性。\n\n![DS-STAR -1](https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-1.width-1250.png)\n*DS-STAR通过生成操作不同数据格式的代码来回答用户查询。代码执行后，智能体提供最终解决方案，形式可能为训练模型、处理后的数据库、可视化或文本格式答案。*\n\n## DS-STAR：创新与设计\n\n为解决上述问题，研究人员提出了DS-STAR，一种旨在解决数据科学问题的新型智能体。DS-STAR引入了三项关键创新：\n1.  **数据文件分析模块**：自动从包括非结构化数据在内的各种数据格式中提取上下文。\n2.  **验证阶段**：一个基于LLM的判断器在每个步骤评估计划的充分性。\n3.  **顺序规划过程**：根据反馈迭代地完善初始计划。这种迭代式精炼使DS-STAR能够处理从多个数据源中提取可验证洞察的复杂分析。\n\n![DS-STAR -2](https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-2.width-1250.png)\n*DS-STAR通过提取关键信息来创建Python脚本以分析多样化的数据文件。*\n\n### DS-STAR 框架的两大阶段\n\nDS-STAR框架主要在两个阶段运行：\n\n1.  **第一阶段：文件自动分析与摘要生成**\n    *   DS-STAR自动检查目录中的所有文件，并创建其结构和内容的文本摘要。这个摘要成为处理当前任务的重要上下文来源。\n\n2.  **第二阶段：规划、实施与验证的循环**\n    *   **规划器 (Planner) 智能体**：首先创建一个高层计划。\n    *   **编码器 (Coder) 智能体**：将计划转化为代码脚本。\n    *   **验证器 (Verifier) 智能体**：评估代码解决问题的有效性。验证器是一个基于LLM的判断器，用于确定当前计划是否充分。\n    *   **路由器 (Router) 智能体**：如果判断器认为计划不足，DS-STAR会通过修改或添加步骤（由路由器智能体决定）来完善计划，然后重复循环。\n    *   **迭代循环机制**：DS-STAR采用一种模仿专家分析师使用Google Colab等工具顺序构建计划、审查中间结果后再进行下一步的方法。这个迭代循环持续进行，直到计划被认为满意或达到最大轮数（10轮），此时最终代码作为解决方案交付。\n\n![DS-STAR -3](https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-3.width-1250.png)\n*DS-STAR的工作流程是一个迭代循环。它首先执行一个简单的计划，并使用验证器智能体检查其是否充分。如果计划不足，路由器智能体通过添加步骤或纠正任何错误来指导精炼，然后循环重复。这个过程持续进行，直到验证器批准计划或达到最大轮数。*\n\n## 性能评估\n\n为了评估DS-STAR的有效性，研究人员将其性能与现有最先进方法（AutoGen、DA-Agent）进行了比较，使用了DABStep、KramaBench和DA-Code等知名数据科学基准。这些基准评估了涉及多个数据源和格式的复杂任务（如数据整理、机器学习和可视化）的性能。\n\n结果显示，DS-STAR在所有测试场景中都显著优于AutoGen和DA-Agent。与最佳替代方案相比，DS-STAR在DABStep上的准确率从41.0%提高到45.2%，在KramaBench上从39.8%提高到44.7%，在DA-Code上从37.0%提高到38.5%。值得注意的是，DS-STAR还在DABStep基准的公共排行榜上获得了第一名（截至2025年9月18日）。在简单任务（答案在单个文件中）和困难任务（需要多个文件）中，DS-STAR始终超越竞争基线，展示了其处理多个异构数据源的卓越能力。\n\n![DS-STAR -4](https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-4.width-1250.png)\n*此图表显示了DABStep、KramaBench和DA-Code基准中简单（单文件）和困难（多文件）任务的标准化准确率（%）。DS-STAR始终优于竞争基线，在需要处理多个异构数据文件的困难任务中表现出特别强的优势。*\n\n## DS-STAR 的深入分析\n\n研究人员还进行了消融研究，以验证DS-STAR各个组件的有效性，并分析精炼轮次数量的影响。\n\n### 消融研究结果\n\n*   **数据文件分析器 (Data File Analyzer)**：该智能体对高性能至关重要。如果没有它生成的描述（变体1），DS-STAR在DABStep基准中困难任务的准确率急剧下降到26.98%，这强调了丰富数据上下文对于有效规划和实施的重要性。\n*   **路由器 (Router)**：路由器智能体判断是否需要新步骤或修复错误步骤的能力至关重要。当移除它时（变体2），DS-STAR仅按顺序添加新步骤，导致在简单和困难任务上的性能均下降。这表明纠正计划中的错误比继续添加可能存在缺陷的步骤更有效。\n*   **跨LLM的泛化能力**：通过使用GPT-5作为基础模型测试DS-STAR的适应性，在DABStep基准上取得了有希望的结果，表明该框架的泛化能力。有趣的是，使用GPT-5的DS-STAR在简单任务上表现更好，而Gemini-2.5-Pro版本在困难任务上表现更优。\n\n![DS-STAR - table](https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-_table.width-1250.png)\n*DS-STAR在DABStep基准上的消融研究结果，评估了单个智能体有效性和LLM兼容性。*\n\n### 精炼过程分析\n\n分析表明，困难任务自然需要更多的迭代。在DABStep基准上，硬任务平均需要5.6轮才能解决，而简单任务仅需3.0轮。此外，超过一半的简单任务在单轮内完成。\n\n![DS-STAR -5](https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-5.width-1250.png)\n*对DABStep基准精炼轮次的分析显示，困难任务需要更多迭代。硬任务平均需要5.6轮，而简单任务为3.0轮，超过50%的简单任务在第一轮就解决了。*\n\n## 结论\n\nDS-STAR是一种能够自主解决数据科学问题的新型智能体。该框架由两项核心创新定义：对多样化文件格式的自动分析，以及使用新型基于LLM的验证系统的迭代式顺序规划过程。DS-STAR在DABStep、KramaBench和DA-Code基准上建立了新的最先进性能，超越了最佳替代方案。通过自动化复杂的数据科学任务，DS-STAR有望使数据科学对个人和组织更易于访问，从而推动许多不同领域的创新。",
      "shortSummary": "DS-STAR是一种最先进的多功能数据科学智能体，旨在解决数据科学流程的复杂性和现有智能体对结构化数据的依赖。它通过三大创新实现：自动分析多样化数据格式、基于LLM的验证阶段和迭代式顺序规划。DS-STAR在DABStep、KramaBench和DA-Code等基准测试中表现卓越，显著超越现有方法，尤其擅长处理异构数据。通过自动化复杂任务，DS-STAR有望提高数据科学的可及性，推动各领域创新。",
      "translated_title": "DS-STAR：一种最先进的多功能数据科学智能体",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-1.width-1250.png",
          "alt": "DS-STAR -1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-2.width-1250.png",
          "alt": "DS-STAR -2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-3.width-1250.png",
          "alt": "DS-STAR -3",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-4.width-1250.png",
          "alt": "DS-STAR -4",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-_table.width-1250.png",
          "alt": "DS-STAR - table",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"o8cvr\">Data science is a field dedicated to transforming raw data into meaningful, actionable insights, playing an essential role in solving real-world challenges. Businesses often depend on <a href=\"https://link.springer.com/article/10.1007/s42979-021-00765-8\" target=\"_blank\" rel=\"noopener noreferrer\">data-driven insights</a> to make pivotal strategic decisions. However, the data science process is frequently complex, demanding a high level of expertise in fields like computer science and statistics. This workflow consists of many time-intensive activities, from interpreting various documents to performing complex data processing and statistical analysis.</p><p data-block-key=\"397lt\">To streamline this complex workflow, <a href=\"https://arxiv.org/pdf/2402.18679\" target=\"_blank\" rel=\"noopener noreferrer\">recent research</a> has focused on using off-the-shelf large language models (LLMs) to create autonomous data science agents. The goal of these agents is to convert natural language questions into executable code for a desired task. But despite making significant <a href=\"https://arxiv.org/pdf/2505.14738\" target=\"_blank\" rel=\"noopener noreferrer\">progress</a>, current data science agents have several limitations that hinder their practical use. A major issue is their heavy reliance on well-structured data, like CSV files in relational databases. This limited focus ignores the valuable information contained in the diverse and <a href=\"https://huggingface.co/blog/dabstep\" target=\"_blank\" rel=\"noopener noreferrer\">heterogeneous data formats</a>, such as JSON, unstructured text, and markdown files, that are common in real-world applications. Another challenge is that many <a href=\"https://arxiv.org/pdf/2506.06541\" target=\"_blank\" rel=\"noopener noreferrer\">data science problems</a> are open-ended and lack ground-truth labels, making it difficult to verify if an agent's reasoning is correct.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-1.width-1250.png\" alt=\"DS-STAR -1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-1.width-1250.png\" alt=\"DS-STAR -1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ig76q\"><i>Data science agents answer user queries by generating code that operates on diverse data formats. Following the code's execution, the agent provides a final solution, which may take the form of a trained model, a processed database, a visualization, or a text-formatted answer.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"o8cvr\">To that end, we present <a href=\"https://arxiv.org/pdf/2509.21825\" target=\"_blank\" rel=\"noopener noreferrer\">DS-STAR</a>, a new agent designed to solve data science problems. DS-STAR introduces three key innovations: (1) a data file analysis module that automatically extracts context from varied data formats, including unstructured ones; (2) a verification stage where an LLM-based judge assesses the plan’s sufficiency at each step; and (3) a sequential planning process that iteratively refines the initial plan based on feedback. This iterative refinement allows DS-STAR to handle complex analyses that draw verifiable insights from multiple data sources. We demonstrate that DS-STAR achieves state-of-the-art performance on challenging benchmarks like <a href=\"https://arxiv.org/pdf/2506.23719\" target=\"_blank\" rel=\"noopener noreferrer\">DABStep</a>, <a href=\"https://arxiv.org/pdf/2506.06541\" target=\"_blank\" rel=\"noopener noreferrer\">KramaBench</a>, and <a href=\"https://arxiv.org/pdf/2410.07331\" target=\"_blank\" rel=\"noopener noreferrer\">DA-Code</a>. It especially excels with tasks involving diverse, heterogeneous data files.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">DS-STAR</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o8cvr\">The DS-STAR framework operates in two main stages. First, it automatically examines all files in a directory and creates a textual summary of their structure and contents. This summary becomes a vital source of context for tackling the task at hand.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-2.width-1250.png\" alt=\"DS-STAR -2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-2.width-1250.png\" alt=\"DS-STAR -2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ig76q\"><i>DS-STAR creates a Python script to analyze diverse data files by extracting key information.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"o8cvr\">Second, DS-STAR engages in a primary loop of planning, implementing, and verifying. The Planner agent first creates a high-level plan, which the Coder agent then transforms into a code script. Subsequently, the Verifier agent evaluates the code's effectiveness in solving the problem. The Verifier agent is an LLM-based judge prompted to determine if the current plan is adequate. If the judge finds the plan insufficient, DS-STAR refines it by altering or adding steps (determined by the Router agent) and then repeats the cycle. Importantly, DS-STAR uses a method that mimics how an expert analyst uses tools like <a href=\"https://colab.sandbox.google.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Google colab</a> to build a plan sequentially, reviewing intermediate results before proceeding. This iterative cycle continues until a plan is deemed satisfactory or the maximum number of rounds (10) is reached, at which point the final code is delivered as the solution.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-3.width-1250.png\" alt=\"DS-STAR -3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-3.width-1250.png\" alt=\"DS-STAR -3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ig76q\"><i>DS-STAR's workflow is an iterative loop. It starts by executing a simple plan and uses a Verifier agent to check if it's sufficient. If the plan is inadequate, a Router agent guides the refinement by adding a step or correcting any errors before the cycle repeats. The process continues until the Verifier approves the plan or the maximum number of rounds is reached.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Evaluation</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o8cvr\">To evaluate DS-STAR’s effectiveness, we compared its performance to existing state-of-the-art methods (<a href=\"https://arxiv.org/pdf/2308.08155\" target=\"_blank\" rel=\"noopener noreferrer\">AutoGen</a>, <a href=\"https://arxiv.org/pdf/2410.07331\" target=\"_blank\" rel=\"noopener noreferrer\">DA-Agent</a>) using a set of well-regarded data science benchmarks, <a href=\"https://arxiv.org/pdf/2506.23719\" target=\"_blank\" rel=\"noopener noreferrer\">DABStep</a>, <a href=\"https://arxiv.org/pdf/2506.06541\" target=\"_blank\" rel=\"noopener noreferrer\">KramaBench</a>, and <a href=\"https://arxiv.org/pdf/2410.07331\" target=\"_blank\" rel=\"noopener noreferrer\">DA-Code</a>. These benchmarks evaluate performance on complex tasks like data wrangling, machine learning, and visualization that use multiple data sources and formats.</p><p data-block-key=\"7mulp\">The results show that DS-STAR substantially outperforms AutoGen and DA-Agent in all test scenarios. Compared to the best alternative, DS-STAR raised the accuracy from 41.0% to 45.2% on DABStep, 39.8% to 44.7% on KramaBench, and 37.0% to 38.5% on DA-Code. Notably, DS-STAR also secured the top rank on the <a href=\"https://huggingface.co/spaces/adyen/DABstep\" target=\"_blank\" rel=\"noopener noreferrer\">public leaderboard</a> for the DABStep benchmark (as of 9/18/2025). On both easy tasks (where the answer is in a single file) and hard tasks (requiring multiple files), DS-STAR consistently surpasses competing baselines, demonstrating its superior ability to work with multiple, heterogeneous data sources.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-4.width-1250.png\" alt=\"DS-STAR -4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-4.width-1250.png\" alt=\"DS-STAR -4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ig76q\"><i>This chart shows the normalized accuracy (%) on both easy (single-file) and hard (multi-file) tasks from the DABStep, KramaBench, and DA-Code benchmarks. DS-STAR consistently outperforms competing baselines, showing a particularly strong advantage in hard tasks that require processing multiple, heterogeneous data files.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">In-depth analysis of DS-STAR</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o8cvr\">Next, we conducted ablation studies to verify the effectiveness of DS-STAR’s individual components and analyze the impact of the number of refinement rounds, specifically by measuring the iterations required to generate a sufficient plan.</p><p data-block-key=\"c9oqo\"><b>Data File Analyzer</b>: This agent is essential for high performance. Without the descriptions it generates (Variant 1), DS-STAR's accuracy on difficult tasks within the DABStep benchmark sharply dropped to 26.98%, underscoring the importance of rich data context for effective planning and implementation.</p><p data-block-key=\"dtbf7\"><b>Router</b>: The Router agent’s ability to determine if a new step is needed or to fix an incorrect step is vital. When we removed it (Variant 2), DS-STAR only added new steps sequentially, leading to worse performance on both easy and hard tasks. This demonstrated that it is more effective to correct mistakes in a plan than to keep adding potentially flawed steps.</p><p data-block-key=\"e5c11\"><b>Generalizability Across LLMs</b>: We also tested DS-STAR's adaptability by using <a href=\"https://openai.com/index/introducing-gpt-5/\" target=\"_blank\" rel=\"noopener noreferrer\">GPT-5</a> as the base model. This yielded promising results on the DABStep benchmark, indicating the framework's generalizability. Interestingly, DS-STAR with GPT-5 performed better on easy tasks, while the <a href=\"https://deepmind.google/models/gemini/pro/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini-2.5-Pro</a> version performed better on hard tasks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-_table.width-1250.png\" alt=\"DS-STAR - table\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-_table.width-1250.png\" alt=\"DS-STAR - table\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ig76q\"><i>Ablation study results for DS-STAR on the DABStep benchmark, evaluating individual agent effectiveness and LLM compatibility.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"o8cvr\"><b>An analysis of the refinement process</b>: The figure below shows that difficult tasks naturally require more iterations. On the DABStep benchmark, hard tasks needed an average of 5.6 rounds to solve, whereas easy tasks required only 3.0 rounds. Furthermore, over half of the easy tasks were completed in just a single round.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-5.width-1250.png\" alt=\"DS-STAR -5\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DS-STAR_-5.width-1250.png\" alt=\"DS-STAR -5\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ig76q\"><i>An analysis of refinement rounds on the DABStep benchmark shows that difficult tasks require more iterations. Hard tasks average 5.6 rounds versus 3.0 for easy tasks, with over 50% of easy tasks being solved in the first round alone.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o8cvr\">In this work, we introduced DS-STAR, a new agent that can autonomously solve data science problems. The framework is defined by two core innovations: the automatic analysis of diverse file formats and an iterative, sequential planning process that uses a novel LLM-based verification system. DS-STAR establishes a new state-of-the-art on the DABStep, KramaBench, and DA-Code benchmarks, outperforming the best alternative. By automating complex data science tasks, DS-STAR has the potential to make data science more accessible for individuals and organizations, helping to drive innovation across many different fields.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o8cvr\"><i>We would like to thank Jiefeng Chen, Jinwoo Shin, Raj Sinha, Mihir Parmar, George Lee, Vishy Tirumalashetty, Tomas Pfister and Burak Gokturk for their valuable contributions to this work.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "利用AI预测森林的未来：从统计损失到预测风险 (原标题: Forecasting the future of forests with AI: From counting losses to predicting risk)",
      "link": "https://research.google/blog/forecasting-the-future-of-forests-with-ai-from-counting-losses-to-predicting-risk/",
      "pubDate": "Tue, 04 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-04T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 利用AI预测森林的未来：从统计损失到预测风险\n\n森林是地球气候、经济和生命的重要支柱，它们储存碳、调节降雨、缓解洪水并承载着地球上大部分的陆地生物多样性。然而，尽管其重要性，全球森林正以惊人的速度消失。去年，全球每分钟损失相当于18个足球场大小的热带森林，总计670万公顷，创历史新高，是前一年的两倍。栖息地转化已成为陆地生物多样性面临的最大威胁。\n\n## 从回顾性监测到前瞻性预测\n\n多年来，卫星数据一直是衡量森林损失的重要工具。最近，与世界资源研究所合作，我们绘制了2000-2024年间森林损失的潜在驱动因素（如农业、伐木、采矿和火灾）地图，分辨率高达1平方公里。这些洞察力虽然至关重要，但仅限于回顾过去。现在，是时候展望未来了。\n\n我们很高兴发布“ForestCast：利用深度学习大规模预测森林砍伐风险”项目，以及首个专门用于训练深度学习模型预测森林砍伐风险的公开基准数据集。这一转变——从仅仅监测已发生的损失到预测未来面临风险的区域——改变了游戏规则。\n\n## 预测森林砍伐的挑战\n\n森林砍伐本质上是一个由经济、政治和环境因素复杂交织驱动的人类过程。它由商品驱动的扩张（如牛、棕榈油和大豆）、野火、伐木、定居点和基础设施的扩张以及硬矿物和能源的开采所推动。因此，预测未来损失的地点和时间极其困难。\n\n以往最先进的方法通过收集尽可能多的专业地理空间信息来解决这个问题，例如道路地图、经济指标和政策执行数据。这种方法在某些地区和某些时间提供了准确的预测，但它不具备普遍的可扩展性，因为这些输入地图通常不完整、不一致，并且需要为每个区域单独组装。此外，这种方法也不是面向未来的，因为输入地图往往会迅速过时，并且无法保证何时（如果会）刷新。\n\n## 可扩展的纯卫星方法\n\n为了克服这些挑战，我们采用了一种“纯卫星”模型，其中所有输入都源自卫星数据。\n\n### 模型输入与训练\n\n*   **原始卫星输入**：来自Landsat和Sentinel 2卫星的原始数据。\n*   **“变化历史”输入**：一种卫星衍生输入，识别每个已被砍伐的像素并提供砍伐发生的年份。\n*   **训练与评估**：模型使用卫星衍生的森林砍伐标签进行训练和评估。\n\n### 优势与发现\n\n*   **一致性**：纯卫星方法确保了我们可以在地球上任何地方应用完全相同的方法，从而实现不同区域之间的有意义比较。\n*   **面向未来**：卫星数据流将持续多年，我们可以重复该方法提供更新的风险预测，并检查风险随时间的变化。\n*   **准确性与可扩展性**：\n    *   我们开发了一个基于视觉Transformer的定制模型。\n    *   模型接收一整块卫星像素作为输入，这对于捕捉景观的空间背景和近期森林砍伐（通过变化历史捕获）至关重要。\n    *   模型一次性输出一整块像素的预测结果，使其能够扩展到大区域。\n    *   我们的模型能够复制甚至超越基于专业输入（如道路）的方法的准确性，准确预测森林砍伐量在不同图块间的变化，并在图块内部准确预测哪些像素最有可能接下来被砍伐。\n*   **最重要输入**：令人惊讶的是，我们发现最重要的卫星输入是“变化历史”——它最简单，但信息密度极高。仅使用此输入的模型所提供的预测准确性指标与使用完整原始卫星数据的模型几乎没有区别。它包含了近期森林砍伐率在不同图块间的变化、这些变化随时间的趋势，以及图块内移动的森林砍伐前沿信息。\n\n## 促进透明度和可重复性\n\n为了促进透明度和可重复性，我们发布了这项工作中使用的训练和评估数据作为基准。这使得更广泛的机器学习社区能够验证我们的结果，深入理解模型做出某些预测的原因，并最终构建和比较改进的森林砍伐风险模型。\n\n此外，我们的基准和论文为在全球范围内推广这种方法提供了清晰的模板——用于模拟拉丁美洲和非洲的热带森林砍伐，并最终扩展到温带和北方纬度地区，那里的森林损失通常由不同的动态驱动，如牧牛和火灾。\n\n## 结论与影响\n\n土地利用变化，特别是热带森林砍伐和森林转化，约占全球人为温室气体排放的10%，并威胁着地球上绝大多数的陆地生命。森林砍伐风险预测可能是一个至关重要的工具，用于将资源投向能够最大程度地遏制这些排放和保护自然的领域。\n\n这种预测风险的能力使政府、企业和社区能够在仍有时间防止损失时及早采取行动，而不是在损害已经发生后才做出反应。例如：\n\n*   政府机构可以向新兴的森林砍伐前沿地区的社区提供支持和保护激励。\n*   公司可以主动管理其供应链，以减少和消除森林砍伐。\n*   原住民社区可以将稀缺资源用于保护其风险最大的土地。\n\n因此，这样的预测并非对不可避免未来的预言，而是一种旨在改变未来结果的工具。目标是与能够采取行动的人分享这些信息，帮助他们在为时过晚之前将资源引导到最脆弱的地区，并赋予他们权力，确保这些高风险森林得以保留。通过结合开放数据和先进AI，我们正在打造一个强大的新工具来保护自然。",
      "shortSummary": "Google发布了“ForestCast”项目，利用深度学习和纯卫星数据预测全球森林砍伐风险。该项目从传统的森林损失监测转向前瞻性风险预测，克服了以往方法数据不一致、易过时的缺点。通过分析卫星图像和“变化历史”数据，ForestCast模型能准确识别高风险区域，并已发布基准数据集以促进社区协作。此工具旨在帮助政府、企业和社区在森林损失发生前采取主动措施，将资源导向最脆弱地区，从而有效保护全球森林和生物多样性。",
      "translated_title": "利用AI预测森林的未来：从统计损失到预测风险",
      "images": [],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"1ktpu\">Nature underpins our climate, our economies, and our very lives. And within nature, forests stand as <a href=\"https://www.fao.org/family-farming/detail/en/c/1507071/\" target=\"_blank\" rel=\"noopener noreferrer\">one of the most powerful pillars</a> — storing carbon, regulating rainfall, mitigating floods, and harboring the majority of the planet’s terrestrial biodiversity.</p><p data-block-key=\"2al57\">Yet, despite their critical importance, the world continues to lose forests at an alarming rate. Last year alone, we lost the equivalent of 18 soccer fields of tropical forest every minute, totaling 6.7 million hectares — a record high and double the amount <a href=\"https://gfr.wri.org/latest-analysis-deforestation-trends\" target=\"_blank\" rel=\"noopener noreferrer\">lost the year before</a>. Today, habitat conversion is <a href=\"https://www.ufz.de/index.php?en=36336&amp;webc_pm=36/2022#:~:text=The%20conversion%20of%20natural%20forests,alien%20species%20in%20fifth%20place\" target=\"_blank\" rel=\"noopener noreferrer\">the greatest threat</a> to biodiversity on land.</p><p data-block-key=\"f002a\">For years, satellite data has been our essential tool for measuring this loss. More recently, in collaboration with the <a href=\"https://www.wri.org/\" target=\"_blank\" rel=\"noopener noreferrer\">World Resources Institute</a>, we helped map the underlying <a href=\"https://www.wri.org/insights/forest-loss-drivers-data-trends\" target=\"_blank\" rel=\"noopener noreferrer\">drivers</a> of that loss — from agriculture and logging to mining and fire — for the years 2000–2024. These maps, which are at an unprecedented 1km<sup class=\"superscript\">2</sup> resolution, provide a basis for a wide range of forest protection measures. However those insights, critical as they are, only look backward. Now, it's time to look ahead.</p><p data-block-key=\"lbrb\">We're excited to announce the release of “<a href=\"https://eartharxiv.org/repository/view/10674/\" target=\"_blank\" rel=\"noopener noreferrer\">ForestCast: Forecasting Deforestation Risk at Scale with Deep Learning</a>”, along with the first publicly available <a href=\"https://console.cloud.google.com/storage/browser/nature-trace/datasets/tfds/forestcast/southeast_asia/1.0.0\" target=\"_blank\" rel=\"noopener noreferrer\">benchmark dataset</a> dedicated to training deep learning models to predict deforestation risk. This shift from merely monitoring what's already gone to forecasting what's at risk in the future changes the game. Previous approaches to risk have depended on assembling patchily-available input maps, such as roads and population density, which can quickly go out of date. By contrast, we have developed an efficient approach based on pure satellite data that can be applied consistently, in any region, and can be readily updated in the future when more data becomes available. We found that this approach could match or exceed the accuracy of previous approaches. To ensure the community can reproduce and build on our work, we are releasing all of the input, training, and evaluation data as a public benchmark dataset.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"1ktpu\">Why predicting deforestation is so difficult</h2><p data-block-key=\"curlv\">Deforestation is fundamentally a human process driven by a complex web of economic, political, and environmental factors. <a href=\"https://iopscience.iop.org/article/10.1088/1748-9326/add606\" target=\"_blank\" rel=\"noopener noreferrer\">It's fueled by</a> commodity-driven expansion for products like cattle, palm oil, and soy, but also by wildfires, logging, the expansion of settlements and infrastructure, and the extraction of hard minerals and energy. Predicting the location and timing of future loss is therefore incredibly hard.</p><p data-block-key=\"48o6n\">The <a href=\"https://www.wwf.org.ec/?392413/Forest-Foresight-technology-for-early-action-against-deforestation\" target=\"_blank\" rel=\"noopener noreferrer\">current state-of-the-art approach</a> tries to solve this by assembling specialized geospatial information on as many of those factors as possible: maps of roads, economic indicators, policy enforcement data, etc. This approach <a href=\"https://www.wri.org/predicting-future-forest-loss-democratic-republic-congos-carpe-landscapes\" target=\"_blank\" rel=\"noopener noreferrer\">has provided accurate predictions</a> for some regions at some times. However, it is not generally scalable because those input maps are often patchy, inconsistent, and need to be assembled separately for each region. This approach is also not future-proof, because the input maps tend to quickly go out of date, and there is no guarantee when, if ever, they may be refreshed.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"1ktpu\">A scalable satellite approach</h2><p data-block-key=\"28r39\">To overcome these challenges, we adopt a “pure satellite” model, where the only inputs are derived from satellites. We tested raw satellite inputs from the <a href=\"https://landsat.gsfc.nasa.gov/data/\" target=\"_blank\" rel=\"noopener noreferrer\">Landsat</a> and <a href=\"https://sentinels.copernicus.eu/web/sentinel/copernicus/sentinel-2\" target=\"_blank\" rel=\"noopener noreferrer\">Sentinel 2</a> satellites. We also included a satellite-derived input we refer to as “change history”, which identifies each pixel that has already been deforested and provides a year for when that deforestation occurred. We trained and evaluated the model using satellite-derived labels of deforestation.</p><p data-block-key=\"4efe7\">The pure satellite approach provides consistency, in that we can apply the exact same method anywhere on Earth, allowing for meaningful comparisons between different regions. It also makes our model future proof — these satellite data streams will continue for years to come, so we can repeat the method to give updated predictions of risk and examine how risk is changing through time.</p><p data-block-key=\"k776\">To achieve accuracy and scalability, we developed a custom model based on <a href=\"https://en.wikipedia.org/wiki/Vision_transformer\" target=\"_blank\" rel=\"noopener noreferrer\">vision transformers</a>. The model receives a whole tile of satellite pixels as input, which is crucial to capture the spatial context of the landscape and recent deforestation (as captured in the change history). It then outputs a whole tile’s worth of predictions in one pass, which makes the model scalable to large regions.</p><p data-block-key=\"feut9\">We found that our model was able to reproduce, or exceed, the accuracy of methods based on specialized inputs (such as roads), accurately predicting tile-to-tile variation in the amount of deforestation, and, within tiles, accurately predicting which pixels were the most likely to become deforested next.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Forestcast_Timeline.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"fjxxw\"><i>Our deep learning vision model analyzes satellite time series and historical forest loss to forecast deforestation risk.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ix787\">Surprisingly, we found that by far the most important satellite input was the simplest, the change history. So much so that a model receiving only this input could provide predictions with accuracy metrics indistinguishable from models using the full, raw satellite data. In retrospect we can see that the change history is a small, but highly information dense, model input — including information on tile-to-tile variation in recent deforestation rates, and how these are trending through time, and also capturing moving deforestation fronts within tiles.</p><p data-block-key=\"ca5fm\">To promote transparency and repeatability, we are releasing the training and evaluation data used in this work, <a href=\"https://console.cloud.google.com/storage/browser/nature-trace/datasets/tfds/forestcast/southeast_asia/1.0.0\" target=\"_blank\" rel=\"noopener noreferrer\">as a benchmark</a>. This allows the wider machine learning community to verify our results; to potentially extract deeper understanding of why the model makes certain predictions; and ultimately, to build and <a href=\"https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13953\" target=\"_blank\" rel=\"noopener noreferrer\">compare</a> improved deforestation risk models.</p><p data-block-key=\"7dkm7\">Moreover, our benchmark and paper provide a clear template for scaling this approach globally — to model tropical deforestation across Latin America and Africa, and eventually, to temperate and boreal latitudes where forest loss is often driven by different dynamics, such as cattle ranching and fire.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"1ktpu\">Conclusion</h2><p data-block-key=\"ctdg4\">Land-use change, especially tropical deforestation and forest conversion, is responsible for <a href=\"https://www.ipcc.ch/srccl/\" target=\"_blank\" rel=\"noopener noreferrer\">roughly 10% of</a> global anthropogenic greenhouse-gas emissions and threatens the vast majority of <a href=\"https://science.sciencemag.org/content/366/6471/eaax3100\" target=\"_blank\" rel=\"noopener noreferrer\">the planet's terrestrial life</a>. Forecasts of deforestation risk could be a vital tool for targeting resources where they can have the greatest impact in curbing those emissions and protecting nature.</p><p data-block-key=\"dfu81\">This ability to anticipate risk allows governments, companies, and communities to act early, when there’s still time to prevent loss, rather than reacting to damage that’s already done. For example:</p><ul><li data-block-key=\"57eo1\">A government agency can channel support and conservation incentives to communities in emerging deforestation frontiers.</li><li data-block-key=\"9qidc\">A company can proactively manage their supply chains to reduce and eliminate deforestation.</li><li data-block-key=\"8ukps\">An indigenous community can deploy scarce resources towards protecting their land at greatest risk.</li></ul><p data-block-key=\"doa81\">Thus, a forecast like this isn't a prediction of an unavoidable future. Instead, it's a tool designed to change a future outcome. The goal is to share this information with those who can act, helping them channel resources to the most vulnerable areas before it's too late, and empowering them to ensure these high-risk forests remain standing. By combining open data and advanced AI, we're forging a powerful new tool to safeguard nature.</p><p data-block-key=\"9nqlv\">Learn more about our AI and sustainability efforts by checking out <a href=\"https://blog.google/technology/research/new-updates-and-more-access-to-google-earth-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Earth AI</a>, <a href=\"https://cloud.google.com/blog/topics/sustainability/look-back-at-a-year-of-earth-engine-advancements\" target=\"_blank\" rel=\"noopener noreferrer\">Google Earth Engine</a>, and <a href=\"https://deepmind.google/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/\" target=\"_blank\" rel=\"noopener noreferrer\">AlphaEarth Foundations</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"1ktpu\">Acknowledgements</h2><p data-block-key=\"cs4a2\"><i>This research was co-developed by Google Deepmind and Google Research.</i></p><p data-block-key=\"cikd1\"><i>Google Deepmind: Matt Overlan, Arianna Manzini, Drew Purves, Julia Haas, Maxim Neumann, Mélanie Rey.</i></p><p data-block-key=\"5o4tc\"><i>Google Research: Charlotte Stanton, Michelangelo Conserva.</i></p><p data-block-key=\"f6an2\"><i>We’d also like to thank our additional collaborators Kira Prabhu, Youngin Shin and Kuan Lu, as well as Peter Battaglia and Kat Chou for their support.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "探索一种基于太空的可扩展AI基础设施系统设计 (原标题: Exploring a space-based, scalable AI infrastructure system design)",
      "link": "https://research.google/blog/exploring-a-space-based-scalable-ai-infrastructure-system-design/",
      "pubDate": "Mon, 03 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-03T16:00:00.000Z",
      "creator": "Google",
      "summary": "本文介绍了Google的“捕日者计划”（Project Suncatcher），这是一项旨在构建基于太空、可扩展AI基础设施的开创性研究项目。该计划设想利用太阳在太空中的巨大能量，通过紧凑的太阳能卫星星座，搭载Google TPU并利用自由空间光链路进行连接，以解锁AI的全部潜力。\n\n## 核心理念与动机\n\n*   **AI的变革潜力**：人工智能被视为一项基础技术，能够推动科学发现并解决人类面临的重大挑战。\n*   **太空能源优势**：太阳是太阳系中终极的能量来源。在合适的轨道上，太阳能电池板的生产效率可达地球上的8倍，并能几乎持续发电，从而减少对电池的需求。这使得太空成为未来扩展AI计算的最佳场所。\n*   **最小化地球资源影响**：将AI计算转移到太空有助于减少对地球资源的消耗。\n\n## 系统设计与关键挑战\n\n该系统由一个联网卫星星座组成，可能运行在黎明-黄昏太阳同步低地球轨道，以确保几乎持续的日照。为实现这一愿景，需要克服以下几个主要技术挑战：\n\n### 1. 实现数据中心规模的星间链路\n\n*   **需求**：大规模机器学习工作负载需要高带宽、低延迟的连接来分配任务，目标是达到每秒数十太比特的星间链路性能，媲美地面数据中心。\n*   **方法**：通过多通道密集波分复用（DWDM）收发器和空间复用技术实现。\n*   **解决方案**：卫星需以非常紧密的队形（几公里或更短）飞行，以克服信号功率损耗，实现所需的接收功率水平。\n*   **进展**：团队已通过台架演示器验证了该方法，成功实现了单收发器对每向800 Gbps（总计1.6 Tbps）的传输。\n\n### 2. 控制大型、紧密集群的卫星编队\n\n*   **需求**：高带宽星间链路要求卫星以比现有系统更紧凑的编队飞行，卫星间距仅数百米。\n*   **方法**：开发了数值和分析物理模型来分析此类星座的轨道动力学，包括基于Hill-Clohessy-Wiltshire方程的近似模型和基于JAX的可微分模型，以考虑地球引力场的非球形性和可能的空气阻力等扰动。\n*   **结果**：模型表明，在数百米间距下，可能只需要适度的站位保持机动即可在所需的太阳同步轨道上维持稳定的星座。\n\n### 3. TPU的辐射耐受性\n\n*   **测试**：对Google的Trillium (v6e Cloud TPU) 进行了67MeV质子束测试，以评估总电离剂量（TID）和单粒子效应（SEE）的影响。\n*   **结果**：测试结果令人鼓舞。高带宽内存（HBM）子系统是最敏感的组件，但在累积剂量达到2 krad(Si)后才开始出现异常，这几乎是五年任务预期（屏蔽后）剂量750 rad(Si)的三倍。在单个芯片上，高达15 krad(Si)的最大测试剂量未导致硬性故障，表明Trillium TPU对于空间应用具有出乎意料的辐射硬度。\n\n### 4. 经济可行性与发射成本\n\n*   **历史障碍**：高昂的发射成本一直是大型空间系统面临的主要障碍。\n*   **分析**：对历史和预测发射定价数据的分析表明，随着持续的学习曲线，到2030年代中期，价格可能降至每公斤200美元以下。\n*   **影响**：届时，发射和运营一个基于太空的数据中心的成本，在每千瓦/年基础上，可能与同等地面数据中心的能源成本大致相当。\n\n## 未来方向\n\n初步分析表明，基于太空的机器学习计算的核心概念并未受到基本物理原理或不可逾越的经济障碍的阻碍。然而，仍存在重大的工程挑战，例如热管理、高带宽地面通信和在轨系统可靠性。\n\n*   **下一步**：与Planet公司合作，计划在2027年初发射两颗原型卫星，进行学习任务。该实验将测试模型和TPU硬件在太空中的运行情况，并验证光学星间链路在分布式机器学习任务中的应用。\n*   **长期愿景**：千兆瓦规模的星座可能需要更激进的卫星设计，将更适合空间环境的新计算架构与紧密集成太阳能收集、计算和热管理的机械设计相结合。",
      "shortSummary": "Google的“捕日者计划”旨在构建基于太空的可扩展AI基础设施。该计划设想利用太阳能卫星星座，搭载Google TPU并通过光学链路连接，以利用太空丰富的太阳能。项目正解决高带宽星间通信、紧密编队控制和TPU辐射耐受性等关键挑战，并取得了初步进展。随着发射成本预计在2030年代中期下降，该系统有望实现经济可行性。2027年初将发射原型卫星进行测试。",
      "translated_title": "探索一种基于太空的可扩展AI基础设施系统设计",
      "images": [],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"zba46\">Artificial intelligence (AI) is a foundational technology that could reshape our world, driving new scientific discoveries and helping us tackle humanity's greatest challenges. Now, we're asking where we can go to unlock its fullest potential.</p><p data-block-key=\"46d19\">The Sun is the ultimate energy source in our solar system, emitting more power than 100 trillion times humanity’s total electricity production. In the right orbit, a solar panel can be up to 8 times more productive than on earth, and produce power nearly continuously, reducing the need for batteries. In the future, space may be the best place to scale AI compute. Working backwards from there, our new research moonshot, Project Suncatcher, envisions compact constellations of solar-powered satellites, carrying <a href=\"https://cloud.google.com/tpu\" target=\"_blank\" rel=\"noopener noreferrer\">Google TPUs</a> and connected by <a href=\"https://en.wikipedia.org/wiki/Free-space_optical_communication\" target=\"_blank\" rel=\"noopener noreferrer\">free-space optical links</a>. This approach would have tremendous potential for scale, and also minimizes impact on terrestrial resources.</p><p data-block-key=\"fdfdk\">We’re excited about this growing area of exploration, and our early research, shared today in “<a href=\"https://goo.gle/4qGsU8X\" target=\"_blank\" rel=\"noopener noreferrer\">Towards a future space-based, highly scalable AI infrastructure system design,</a>” a preprint paper, which describes our progress toward tackling the foundational challenges of this ambitious endeavor — including high-bandwidth communication between satellites, orbital dynamics, and radiation effects on computing. By focusing on a modular design of smaller, interconnected satellites, we are laying the groundwork for a highly scalable, future space-based AI infrastructure.</p><p data-block-key=\"e98hm\">Project Suncatcher is part of Google’s long tradition of taking on moonshots that tackle tough scientific and engineering problems. Like all moonshots, there will be unknowns, but it’s in this spirit that we embarked on building a large-scale quantum computer <a href=\"https://research.google/blog/launching-the-quantum-artificial-intelligence-lab/\">a decade ago</a> — before it was considered a realistic engineering goal — and envisioned an autonomous vehicle <a href=\"https://waymo.com/blog/2016/12/say-hello-to-waymo-whats-next-for\" target=\"_blank\" rel=\"noopener noreferrer\">over 15 years ago</a>, which eventually became Waymo and now serves millions of passenger trips around the globe.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">System design and key challenges</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"zba46\">The proposed system consists of a constellation of networked satellites, likely operating in a dawn–dusk <a href=\"https://en.wikipedia.org/wiki/Sun-synchronous_orbit\" target=\"_blank\" rel=\"noopener noreferrer\">sun-synchronous low earth orbit</a>, where they would be exposed to near-constant sunlight. This orbital choice maximizes solar energy collection and reduces the need for heavy onboard batteries. For this system to be viable, several technical hurdles must be overcome:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">1. Achieving data center-scale inter-satellite links</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"zba46\">Large-scale ML workloads require distributing tasks across numerous accelerators with high-bandwidth, low-latency connections. Delivering performance comparable to terrestrial data centers requires links between satellites that support tens of terabits per second. Our analysis indicates that this should be possible with multi-channel <a href=\"https://en.wikipedia.org/wiki/Wavelength-division_multiplexing#Dense_WDM\" target=\"_blank\" rel=\"noopener noreferrer\">dense wavelength-division multiplexing</a> (DWDM) transceivers and <a href=\"https://en.wikipedia.org/wiki/Spatial_multiplexing\" target=\"_blank\" rel=\"noopener noreferrer\">spatial multiplexing</a>.</p><p data-block-key=\"4n9f2\">However, achieving this kind of bandwidth requires received power levels thousands of times higher than typical in conventional, long-range deployments. Since received power scales inversely with the square of the distance, we can overcome this challenge by flying the satellites in a very close formation (kilometers or less), thus closing the <a href=\"https://en.wikipedia.org/wiki/Link_budget\" target=\"_blank\" rel=\"noopener noreferrer\">link budget</a> (i.e., the accounting of the end-to-end signal power losses in the communications system). Our team has already begun validating this approach with a bench-scale demonstrator that successfully achieved 800 Gbps each-way transmission (1.6 Tbps total) using a single transceiver pair.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-vertical-padding --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">2. Controlling large, tightly-clustered satellite formations</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"zba46\">High-bandwidth inter-satellite links require our satellites to fly in a much more compact formation than any current system. We developed numerical and analytic physics models to analyze the orbital dynamics of such a constellation. We used an approximation starting from the <a href=\"https://en.wikipedia.org/wiki/Clohessy%E2%80%93Wiltshire_equations\" target=\"_blank\" rel=\"noopener noreferrer\">Hill-Clohessy-Wiltshire equations</a> (which describe the orbital motion of a satellite relative to a circular reference orbit in a <a href=\"https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion\" target=\"_blank\" rel=\"noopener noreferrer\">Keplerian approximation</a>) and a <a href=\"https://en.wikipedia.org/wiki/JAX_(software)\" target=\"_blank\" rel=\"noopener noreferrer\">JAX</a>-based differentiable model for the numerical refinement that accounts for further perturbations.</p><p data-block-key=\"4p566\">At the altitude of our planned constellation, the <a href=\"https://en.wikipedia.org/wiki/Geopotential_spherical_harmonic_model\" target=\"_blank\" rel=\"noopener noreferrer\">non-sphericity</a> of Earth's gravitational field, and potentially atmospheric drag, are the dominant non-Keplerian effects impacting satellite orbital dynamics. In the figure below, we show trajectories (over one full orbit) for an illustrative 81-satellite constellation configuration in the orbital plane, at a mean cluster altitude of 650 km. The cluster radius is R=1 km, with the distance between next-nearest-neighbor satellites oscillating between ~100–200m, under the influence of Earth’s gravity.</p><p data-block-key=\"2q6pu\">The models show that, with satellites positioned just hundreds of meters apart, we will likely only require modest station-keeping maneuvers to maintain stable constellations within our desired sun-synchronous orbit.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Suncatcher-1.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ej5x8\"><i>Evolution of a free-fall (“no thrust”) constellation under Earth’s gravitational attraction, modeled to the level of detail required to obtain sun-synchronous orbits, in a non-rotating coordinate system, relative to a central reference satellite S0. Arrow points towards Earth’s center. Magenta: nearest neighbors of satellite S0. Orange: Example \"peripheral\" satellite S1. Orange dashed: S1’s positions relative to the cluster center (in the non-rotating coordinate frame).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">3. Radiation tolerance of TPUs</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"zba46\">For ML accelerators to be effective in space, they must withstand the environment of low-Earth orbit. We tested <a href=\"https://cloud.google.com/blog/products/compute/introducing-trillium-6th-gen-tpus\" target=\"_blank\" rel=\"noopener noreferrer\">Trillium</a>, Google’s v6e Cloud TPU, in a 67MeV proton beam to test for impact from <a href=\"https://en.wikipedia.org/wiki/Absorbed_dose\" target=\"_blank\" rel=\"noopener noreferrer\">total ionizing dose</a> (TID) and <a href=\"https://en.wikipedia.org/wiki/Single-event_upset\" target=\"_blank\" rel=\"noopener noreferrer\">single event effects</a> (SEEs).</p><p data-block-key=\"7dcbv\">The results were promising. While the <a href=\"https://en.wikipedia.org/wiki/High_Bandwidth_Memory\" target=\"_blank\" rel=\"noopener noreferrer\">High Bandwidth Memory</a> (HBM) subsystems were the most sensitive component, they only began showing irregularities after a cumulative dose of 2 <a href=\"https://en.wikipedia.org/wiki/Gray_(unit)\" target=\"_blank\" rel=\"noopener noreferrer\">krad(Si)</a> — nearly three times the expected (shielded) five year mission dose of 750 rad(Si). No hard failures were attributable to TID up to the maximum tested dose of 15 krad(Si) on a single chip, indicating that Trillium TPUs are surprisingly radiation-hard for space applications.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">4. Economic feasibility and launch costs</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"zba46\">Historically, high launch costs have been a primary barrier to large-scale space-based systems. However, our analysis of historical and projected launch pricing data suggests that with a sustained learning rate, prices may fall to less than $200/kg by the mid-2030s<b>.</b> At that price point, the cost of launching and operating a space-based data center could become roughly comparable to the reported energy costs of an equivalent terrestrial data center on a per-kilowatt/year basis<footnote id=\"6081124a-bd7e-49be-8102-2964efb8c10d\">[608112]</footnote>. See the preprint <a href=\"https://goo.gle/4qGsU8X\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a> for more details.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Future directions</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"zba46\">Our initial analysis shows that the core concepts of space-based ML compute are not precluded by fundamental physics or insurmountable economic barriers. However, significant engineering challenges remain, such as thermal management, high-bandwidth ground communications, and on-orbit system reliability.</p><p data-block-key=\"bd2e9\">To begin addressing these challenges, our next milestone is a learning mission in partnership with <a href=\"https://www.planet.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Planet</a>, slated to launch two prototype satellites by early 2027. This experiment will test how our models and TPU hardware operate in space and validate the use of optical inter-satellite links for distributed ML tasks.</p><p data-block-key=\"gp8d\">Eventually, gigawatt-scale constellations may benefit from a more radical satellite design; this may combine new compute architectures more naturally suited to the space environment with a mechanical design in which solar power collection, compute, and thermal management are tightly integrated. Just as the development of complex system-on-chip technology was motivated by and enabled by modern smartphones, scale and integration will advance what’s possible in space.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"zba46\"><i>“Towards a future space-based, highly scalable AI infrastructure system design” was authored by Blaise Agüera y Arcas, Travis Beals, Maria Biggs, Jessica V. Bloom, Thomas Fischbacher, Konstantin Gromov, Urs Köster, Rishiraj Pravahan and James Manyika.</i></p><p data-block-key=\"20j2s\"><i>We thank Amaan Pirani for critical contributions to cost modeling and overall feasibility analysis, Marcin Kowalczyk for independent numerical validation calculations, Paul Epp and Stephen Palese for input on the ISL concept, Thomas Zurbuchen for his contributions to the systems and architecture concepts, and Kenny Vassigh and Jerry Chiu for technical input on system and thermal design. We also thank Muon Space for general discussions and for technical and economic feasibility analysis of the concept.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "加速研究突破与实际应用的神奇循环 (原标题: Accelerating the magic cycle of research breakthroughs and real-world applications)",
      "link": "https://research.google/blog/accelerating-the-magic-cycle-of-research-breakthroughs-and-real-world-applications/",
      "pubDate": "Thu, 30 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-30T16:00:00.000Z",
      "creator": "Google",
      "summary": "在最近的Research@活动中，Google Research分享了其在地球理解、基因组学和量子计算等领域的最新进展，并强调了研究突破与实际应用之间“神奇循环”的加速。这个循环通过更强大的模型、加速科学发现的智能工具以及开放平台得以推动。\n\n### 最新研究突破\n\n文章重点介绍了三项关键突破：\n\n*   **Google Earth AI：前所未有的地球理解**\n    *   Earth AI是一系列强大的地理空间AI模型和推理系统，旨在解决全球性挑战，提供对地球事件前所未有的理解。\n    *   多年来，Google开发了洪水、野火、气旋、空气质量、花粉、天气预报、农业、人口动态等先进的地理空间AI模型，已帮助全球数百万人。\n    *   已扩展对新的遥感基础模型和全球人口动态基础模型的访问。\n    *   河流洪水预报模型已覆盖150个国家的20亿人口。\n    *   Earth AI整合并合成大量的真实世界图像、人口和环境数据，利用大型语言模型（LLMs）及其推理能力，使非专业用户也能通过自然语言提问并获取洞察。\n    *   该技术将赋能Google Earth的Gemini功能，允许用户在卫星图像中搜索物体，并已向Google Cloud的受信任测试者开放。\n\n*   **DeepSomatic与Cell2Sentence：迈向癌症精准医疗**\n    *   DeepSomatic是Google在《自然生物技术》上发表的最新AI工具，旨在帮助科学界和医疗从业者。\n    *   该工具建立在Google十年基因组学研究的基础上，通过将基因测序数据转化为图像，并使用卷积神经网络区分参考基因组、非癌种系变异和肿瘤中的癌变体细胞变异。\n    *   识别癌症变异有望带来全新的疗法，并帮助临床医生选择治疗方案（如化疗或免疫疗法）。\n    *   Children's Mercy医院正利用DeepSomatic为患者提供个性化治疗方案。\n    *   Google还与Google DeepMind合作发布了用于单细胞分析的270亿参数基础模型C2S-Scale，为癌症细胞行为提供了新的假设。\n\n*   **量子回声：迈向实际应用的一大步**\n    *   Google对量子计算进行了战略性长期投资，其硬件里程碑是预计2024年末推出的Willow芯片。\n    *   量子AI硬件首席科学家Michel Devoret等人在1980年代的研究为今天的超导量子比特奠定了基础，并因此成为2025年诺贝尔物理学奖得主。\n    *   Google在《自然》杂志封面发表了一项新的可验证量子优势成果——“量子回声”算法，在Willow芯片上运行速度比世界上最快的超级计算机上的最佳经典算法快13,000倍。\n    *   该算法提供了一种新方法来解释核磁共振光谱中观察到的真实世界分子中原子间的相互作用，是世界上第一个展示可验证量子优势的算法，预示着量子计算在药物设计和实现聚变能源方面的实际应用潜力。\n    *   预计五年内将看到量子计算的实际应用。\n\n### 从加速科学发现到算法创新\n\n文章还分享了Google在多个领域的工作，推动突破性研究和加速实际解决方案：\n\n*   **健康与科学**：AI联合科学家（多智能体AI系统）加速科学和生物医学发现；Gemini支持的编码智能体帮助科学家编写专家级经验软件；AMIE（对话式医疗AI智能体）在临床推理和沟通方面与初级保健医生相当，正与Beth Israel Deaconess Medical Center合作进行真实患者测试；MedGemma（Google最强大的多模态医疗理解开放模型）已获得超过100万次下载和4万名独立用户。\n*   **事实性与效率**：持续推进LLMs的事实性和基础研究，包括LLMs如何表达不确定性、编码事实知识的能力等；扩展到多模态内容，如时间对齐字幕和对比序列视频扩散方法，以提高图像和视频模型的质量；通过推测解码和推测级联等技术，提高LLMs的效率和能源创新。\n*   **算法创新**：新的广告模型、大规模优化、Google地图路线增强、印度语音搜索改进；隐私研究包括机密联邦分析、差分隐私合成数据和可证明的AI使用隐私洞察；TimesFM每月处理数亿次查询，并引入了上下文微调的新方法；探索通过LearnLM和Learn Your Way改进学习和教育；利用扩散模型实现实时游戏引擎，模拟沉浸式世界环境。\n\n### AI作为人类智慧的放大器\n\n研究的“神奇循环”正迅速加速，由更强大的模型、如AI联合科学家和基于AI的专家级经验软件等智能工具，以及MedGemma、HAI-DEF和DeepSomatic等开放平台推动。AI不再仅仅是一个工具，而是不可或缺的伙伴和合作者，赋能研究人员、工程师、医护人员和教育工作者。人类智慧与AI强大能力的融合将进一步推动创新，加速其在全球范围内的影响，为全人类开创科学发现的新时代。",
      "shortSummary": "Google Research在Research@活动中展示了其在地球AI、癌症精准医疗（DeepSomatic）和量子计算（量子回声）方面的最新突破。这些进展加速了研究突破与实际应用之间的“神奇循环”，通过强大的模型、智能工具和开放平台推动创新。Google Earth AI提供前所未有的地球理解；DeepSomatic助力癌症基因变异识别；量子回声算法展示可验证量子优势，预示五年内实际应用。AI正成为人类智慧的放大器，赋能各领域，加速全球科学发现和创新。",
      "translated_title": "加速研究突破与实际应用的神奇循环",
      "images": [],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"k9jkp\">Last week at our flagship Research@ event in Mountain View, we shared some of Google Research’s latest announcements, from understanding earth to advancements in genomics to advancements in quantum computing. Working collaboratively with colleagues across the company, our teams drive breakthrough research and accelerate real-world solutions for products, businesses, science and society. As research comes to reality, we uncover new research opportunities, driving innovation further and faster. I call this powerful, cyclical relationship between research and real-world impact <a href=\"https://blog.google/technology/research/what-is-google-research/\" target=\"_blank\" rel=\"noopener noreferrer\">the magic cycle of research</a>.</p><p data-block-key=\"dg3fp\">This cycle is accelerating significantly these days, propelled by more powerful models, new agentic tools that help accelerate scientific discovery, and open platforms and tools. We see this momentum <a href=\"https://research.google/blog/google-research-2024-breakthroughs-for-impact-at-every-scale/\">across domains</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"3ZRpjMniPus\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=3ZRpjMniPus\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"jb2tj\">Our latest research breakthroughs</h2><p data-block-key=\"b64n9\">At Research@MTV last week, we <a href=\"https://blog.google/technology/research/google-research-team-tackles-big-challenges-with-science/\" target=\"_blank\" rel=\"noopener noreferrer\">highlighted</a> three of our latest breakthroughs: Google Earth AI, DeepSomatic, and Quantum Echoes.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"jb2tj\">Google Earth AI: Unprecedented planetary understanding</h3><p data-block-key=\"aci11\"><a href=\"https://ai.google/earth-ai\" target=\"_blank\" rel=\"noopener noreferrer\">Earth AI</a> is a powerful collection of geospatial AI models and reasoning designed to address critical global challenges; it gives users an unprecedented level of understanding about what is happening across the planet.</p><p data-block-key=\"bf8e2\">For years we’ve been developing state-of-the-art geo-spatial AI models including <a href=\"https://blog.google/technology/ai/google-ai-global-flood-forecasting/\" target=\"_blank\" rel=\"noopener noreferrer\">floods</a>, <a href=\"https://blog.google/outreach-initiatives/sustainability/google-wildfire-boundary-maps-europe-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">wildfires</a>, <a href=\"https://blog.google/technology/google-deepmind/weather-lab-ai-cyclone-prediction-tracking/\" target=\"_blank\" rel=\"noopener noreferrer\">cyclones</a>, <a href=\"https://blog.google/products/maps/google-maps-apis-environment-sustainability/\" target=\"_blank\" rel=\"noopener noreferrer\">air quality</a>, <a href=\"https://blog.google/products/maps/google-maps-apis-environment-sustainability/\" target=\"_blank\" rel=\"noopener noreferrer\">pollen</a>, weather <a href=\"https://blog.google/intl/en-africa/products/explore-get-answers/nowcasting-on-search-is-bringing-ai-powered-weather-forecasts-to-users-across-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">nowcasting</a> and <a href=\"https://deepmind.google/science/weathernext/\" target=\"_blank\" rel=\"noopener noreferrer\">long range forecasting</a>, <a href=\"https://blog.google/intl/en-in/company-news/new-milestones-in-our-journey-to-build-inclusive-and-helpful-ai-for-india/\" target=\"_blank\" rel=\"noopener noreferrer\">agriculture</a>, <a href=\"https://research.google/blog/insights-into-population-dynamics-a-foundation-model-for-geospatial-inference/\">population dynamics</a>, <a href=\"https://deepmind.google/discover/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/\" target=\"_blank\" rel=\"noopener noreferrer\">AlphaEarth Foundations</a> and <a href=\"https://research.google/blog/introducing-mobility-ai-advancing-urban-transportation/\">mobility</a>. These models, developed by teams across Google, are already helping millions of people worldwide and we keep making progress. We have just expanded access to our new Remote Sensing Foundations and new global Population Dynamics Foundations. And we can now share that our riverine flood models — expanded over the years to <a href=\"https://blog.google/technology/ai/expanding-flood-forecasting-coverage-helping-partners/\" target=\"_blank\" rel=\"noopener noreferrer\">cover 700 million</a> people in 100 countries — now provide forecasts covering over 2B people in 150 countries for significant riverine flood events.</p><p data-block-key=\"5637f\"><a href=\"https://blog.google/technology/research/new-updates-and-more-access-to-google-earth-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Earth AI</a> is a Google-wide program building on our long-standing efforts. Our latest research updates to Earth AI integrate and synthesize these vast amounts of real-world imagery, population and environmental data. Using LLMs and their reasoning capabilities, the Earth AI geospatial reasoning agent can understand nuanced concepts and discover correlations across multiple datasets and models. This agent allows users to ask complex questions and receive answers in plain language, making Earth AI capabilities accessible even to non-experts. Users can quickly generate insights from business logic use cases and supply chain management to crisis resilience and international policy.</p><p data-block-key=\"5gu9e\">In our evaluations, Geospatial Reasoning Agent improved responses over baseline models that did not have access to Earth AI models and tools. We share the results in our <a href=\"https://research.google/blog/google-earth-ai-unlocking-geospatial-insights-with-foundation-models-and-cross-modal-reasoning/\">research blog</a> and our <a href=\"https://arxiv.org/abs/2510.18318\" target=\"_blank\" rel=\"noopener noreferrer\">technical report</a>.</p><p data-block-key=\"93hn3\">Google Earth with <a href=\"https://medium.com/google-earth/a-whole-new-way-to-work-onearth-introducing-google-earths-new-professional-plans-f80c20b944c9\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini capabilities</a> will soon be powered by our Earth AI imagery models, enabling users to search for objects in satellite imagery. Plus, our powerful models are now available to <a href=\"https://forms.gle/VmdqBHrMah6g9z948\" target=\"_blank\" rel=\"noopener noreferrer\">trusted testers</a> on Google Cloud. And we continue to hear from our partners about diverse important use cases, including testimonials from <a href=\"https://www.youtube.com/watch?v=8-macH8ozr4&amp;list=PL95lT3XlM14ROFtYnlBDYZbKipH3JLaC7&amp;index=2\" target=\"_blank\" rel=\"noopener noreferrer\">Give Directly</a>, <a href=\"https://www.youtube.com/watch?v=8-macH8ozr4\" target=\"_blank\" rel=\"noopener noreferrer\">McGill and Partners</a>, <a href=\"https://medium.com/cooper-smith/unlocking-next-generation-health-predictions-with-googles-geospatial-foundational-models-a-0139832cf23f\" target=\"_blank\" rel=\"noopener noreferrer\">Cooper/Smith</a>, <a href=\"https://www.youtube.com/watch?v=ZxmB8Z5i1Ls&amp;list=PL95lT3XlM14ROFtYnlBDYZbKipH3JLaC7&amp;index=3\" target=\"_blank\" rel=\"noopener noreferrer\">WPP</a>, <a href=\"https://www.youtube.com/watch?v=_4GVYutXE_I\" target=\"_blank\" rel=\"noopener noreferrer\">WHO AFRO</a>, <a href=\"https://www.youtube.com/watch?v=FviGaVEByS4&amp;list=PL95lT3XlM14ROFtYnlBDYZbKipH3JLaC7&amp;index=4\" target=\"_blank\" rel=\"noopener noreferrer\">Planet Labs and Airbus</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"n625h\">DeepSomatic &amp; Cell2Sentence: Toward precision medicine to fight cancer</h3><p data-block-key=\"3af5e\"><a href=\"https://research.google/blog/using-ai-to-identify-genetic-variants-in-tumors-with-deepsomatic/\">DeepSomatic</a>, published in <a href=\"https://www.nature.com/articles/s41587-025-02839-x\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature Biotechnology</i></a><i>,</i> is our newest of many AI tools designed to help the scientific community and health practitioners.</p><p data-block-key=\"3lkui\">DeepSomatic builds on <a href=\"https://blog.google/technology/research/ten-years-google-genomics/\" target=\"_blank\" rel=\"noopener noreferrer\">10 years of genomics</a> research at Google. Since 2015, we’ve been building models like <a href=\"https://research.google/pubs/deepconsensus-improves-the-accuracy-of-sequences-with-a-gap-aware-sequence-transformer/\">DeepConsensus</a> and <a href=\"https://research.google/blog/learning-deepvariants-hidden-powers/\">DeepVariant</a> to help us better understand the genome. With these models, we’ve helped map <a href=\"https://www.nature.com/articles/s41587-019-0217-9\" target=\"_blank\" rel=\"noopener noreferrer\">human</a> and <a href=\"https://www.irri.org/news-and-events/news/google-blog-analyzing-3k-rice-genomes-characterized-deepvariant\" target=\"_blank\" rel=\"noopener noreferrer\">non-human</a> genomes and used this information to inform our understanding of disease.</p><p data-block-key=\"c4hi1\">Some cancers have complex genetic signatures that may make them targets for tailored treatments based on their specific mutations. So, we asked ourselves if we could sequence the genomes of these cancerous cells more precisely. The result, DeepSomatic, is our new <a href=\"https://github.com/google/deepsomatic\" target=\"_blank\" rel=\"noopener noreferrer\">open-source</a> AI-powered tool to help scientists and doctors make sense of genetic variants in cancer cells.</p><p data-block-key=\"d4v3a\">The model works by first turning genetic sequencing data into a <a href=\"https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/\" target=\"_blank\" rel=\"noopener noreferrer\">set of images</a> and then using a <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\" target=\"_blank\" rel=\"noopener noreferrer\">convolutional neural network</a> to differentiate between the reference genome, the non-cancer germline variants in that individual, and the cancer-caused somatic variants in the tumor.</p><p data-block-key=\"bc4f7\">Identifying cancer variants could potentially lead to brand-new therapies, and it could help clinicians decide between treatments such as chemotherapy and immunotherapy. Our partners at <a href=\"https://www.childrensmercy.org/childrens-mercy-research-institute/about/\" target=\"_blank\" rel=\"noopener noreferrer\">Children’s Mercy</a> are using it to pinpoint <a href=\"https://www.medrxiv.org/content/10.1101/2024.11.05.24316078v1\" target=\"_blank\" rel=\"noopener noreferrer\">how and why a particular form of cancer</a> affects a patient in order to create personalized cures.</p><p data-block-key=\"2m7e2\">DeepSomatic follows other breakthroughs which share the same goal of using AI to help fight cancer. We also just released a 27 billion parameter foundation model for single-cell analysis, <a href=\"https://www.biorxiv.org/content/10.1101/2025.04.14.648850v3\" target=\"_blank\" rel=\"noopener noreferrer\">C2S-Scale</a>, in collaboration with Google DeepMind. This builds upon <a href=\"https://research.google/blog/teaching-machines-the-language-of-biology-scaling-large-language-models-for-next-generation-single-cell-analysis/\">our work from earlier this year</a>, in collaboration with Yale, and recently generated a <a href=\"https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/\" target=\"_blank\" rel=\"noopener noreferrer\">novel hypothesis</a> about cancer cellular behavior. With more clinical tests, this may reveal a promising new pathway for developing therapies to fight cancer.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"n625h\">Quantum Echoes: A big step toward real-world applications</h3><p data-block-key=\"50jv4\">To accelerate the next exponential wave of scientific discovery, we’re looking to our strategic, long-term investment in quantum computing.</p><p data-block-key=\"7d8h5\">Our foundation rests on decades of research, leading to our hardware milestone on the <a href=\"https://research.google/blog/making-quantum-error-correction-work/\">Willow chip</a> in late 2024. This work is supported by Michel Devoret, our Chief Scientist of Quantum Hardware, who together with with former Quantum AI hardware lead John Martinis, and John Clarke of the University of California, Berkeley, became <a href=\"https://blog.google/inside-google/company-announcements/googler-michel-devoret-awarded-the-nobel-prize-in-physics/\" target=\"_blank\" rel=\"noopener noreferrer\">2025 Physics Nobel Laureates</a> for their research in the 1980s that laid the groundwork for today's superconducting qubits.</p><p data-block-key=\"88g91\">Now we’ve announced a new <a href=\"https://research.google/blog/a-verifiable-quantum-advantage/\">verifiable quantum advantage</a>, published in the cover of <a href=\"https://www.nature.com/articles/s41586-025-09526-6\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature</i></a>. Our “<a href=\"https://blog.google/technology/research/quantum-echoes-willow-verifiable-quantum-advantage/\" target=\"_blank\" rel=\"noopener noreferrer\">Quantum Echoes</a>” algorithm runs on our Willow chip 13,000 times faster than the best classical algorithm on one of the world’s fastest supercomputers. It offers a new way to explain interactions between atoms in a real world molecule observed <a href=\"https://arxiv.org/abs/2510.19550\" target=\"_blank\" rel=\"noopener noreferrer\">using nuclear magnetic resonance spectroscopy</a>. This is the world’s first algorithm to demonstrate verifiable quantum advantage and points towards practical applications of quantum computing that are beyond the capabilities of classical computers.</p><p data-block-key=\"9jol8\">Quantum computing has the potential to meaningfully advance drug design and help make fusion energy a reality. And given our latest breakthrough, we’re optimistic that we’ll start to see real-world applications within five years.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"ltTMP_ACkIA\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=ltTMP_ACkIA\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"bs40u\">Fireside Chat about Quantum AI with James Manyika and Hartmut Neven.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"9mtq0\">From accelerating scientific discovery to algorithmic innovation</h2><p data-block-key=\"dpmhs\">We also shared some of the work across various domains where teams are driving breakthrough research and accelerating real-world solutions. The breadth and depth of the opportunities is ever increasing. Here are a few recent examples.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"9mtq0\">Health &amp; Science</h3><p data-block-key=\"dok25\"><a href=\"https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/\">AI co-scientist</a> is a multi-agent AI system built as a virtual scientific collaborator to help scientists generate novel hypotheses and research proposals, and to accelerate scientific and biomedical discoveries. Our new <a href=\"https://research.google/blog/accelerating-scientific-discovery-with-ai-powered-empirical-software/\">AI-powered empirical software</a> system, a Gemini-backed coding agent, helps scientists write expert-level empirical software. It accelerates the historically slow task of creating custom software to evaluate and iteratively improve scientific hypotheses. This opens the door to a future where scientists can easily, rapidly, and systematically investigate hundreds or thousands of potential solutions to the problems that motivate their research.</p><p data-block-key=\"e77td\"><a href=\"https://research.google/blog/amie-a-research-ai-system-for-diagnostic-medical-reasoning-and-conversations/\">AMIE</a>, a conversational medical AI agent, demonstrates clinical reasoning and communication on par with primary care physicians in both <a href=\"https://research.google/blog/amie-gains-vision-a-research-ai-agent-for-multi-modal-diagnostic-dialogue/\">multimodal</a> and <a href=\"https://research.google/blog/from-diagnosis-to-treatment-advancing-amie-for-longitudinal-disease-management/\">multi-visit settings</a>. As we explore how AMIE may translate to real-world environments, we are testing it under <a href=\"https://research.google/blog/enabling-physician-centered-oversight-for-amie/\">physician oversight</a>, including in a <a href=\"https://research.google/blog/advancing-amie-towards-specialist-care-and-real-world-validation/\">partnership</a> with Beth Israel Deaconess Medical Center to evaluate AMIE with real-world patients.</p><p data-block-key=\"6vr16\"><a href=\"https://research.google/blog/medgemma-our-most-capable-open-models-for-health-ai-development/\">MedGemma</a>, part of our Health AI Developer Foundations (<a href=\"https://developers.google.com/health-ai-developer-foundations\" target=\"_blank\" rel=\"noopener noreferrer\">HAI-DEF</a>) collection, is Google's most capable open model for multimodal medical comprehension. Since launch MedGemma and HAI-DEF have &gt;1M downloads and &gt;40K unique users.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"9mtq0\">Factuality &amp; Efficiency</h3><p data-block-key=\"110h0\">We continue advancing our research on factuality and grounding for LLMs, including studying how LLMs <a href=\"https://arxiv.org/abs/2505.24858\" target=\"_blank\" rel=\"noopener noreferrer\">convey uncertainty</a>, assessing whether LLMs <a href=\"https://arxiv.org/abs/2503.15299\" target=\"_blank\" rel=\"noopener noreferrer\">encode more factual knowledge</a> in their parameters than they express in their outputs, and more. We expand to multimodal content - for example, <a href=\"https://arxiv.org/abs/2405.04682\" target=\"_blank\" rel=\"noopener noreferrer\">Time-Aligned Captions</a> and our <a href=\"https://arxiv.org/abs/2407.11814\" target=\"_blank\" rel=\"noopener noreferrer\">contrastive sequential video diffusion</a> method focus on making scenes in videos visually consistent, helping improve the quality of our image and video models.</p><p data-block-key=\"ac56q\">Improving the efficiency of LLMs remains a high priority goal across the industry. Building on our <a href=\"https://research.google/blog/looking-back-at-speculative-decoding/\">speculative decoding</a> work which enabled substantial efficiency gains without any compromise on quality, we keep seeing many new <a href=\"https://scholar.google.co.il/scholar?oi=bibs&amp;hl=en&amp;cites=13896317670253353040&amp;as_sdt=5\" target=\"_blank\" rel=\"noopener noreferrer\">approaches</a>, such as our recent <a href=\"https://research.google/blog/speculative-cascades-a-hybrid-approach-for-smarter-faster-llm-inference/\">speculative cascades</a>. We keep advancing other techniques for efficiency and for <a href=\"https://arxiv.org/abs/2508.15734\" target=\"_blank\" rel=\"noopener noreferrer\">energy innovation</a> techniques.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"9mtq0\">Algorithmic innovation</h3><p data-block-key=\"hu69\">Algorithmic research contributes to new Ads model connecting advertisers to customers, continued research on our large-scale optimisations, enhancements to Google Maps <a href=\"https://research.google/blog/how-we-created-hov-specific-etas-in-google-maps/\">routing</a> and <a href=\"https://research.google/blog/speech-to-retrieval-s2r-a-new-approach-to-voice-search/\">improved voice search</a> in India. Privacy research includes recent advances such as <a href=\"https://research.google/blog/discovering-new-words-with-confidential-federated-analytics/\">confidential federated analytics</a>, <a href=\"https://research.google/blog/generating-synthetic-data-with-differentially-private-llm-inference/\">differentially private synthetic data</a> and <a href=\"https://research.google/blog/toward-provably-private-insights-into-ai-use/\">provably private insights into AI use</a>. We are making progress on <a href=\"https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/\">TimesFM</a> which has hundreds of millions of queries per month in BigQuery alone, and recently introduced a novel approach using <a href=\"https://research.google/blog/time-series-foundation-models-can-be-few-shot-learners/\">in-context fine-tuning</a>.</p><p data-block-key=\"c0npb\">We keep exploring new ways to improve learning and education, building on our earlier work on <a href=\"https://cloud.google.com/solutions/learnlm?hl=en&amp;e=13802955\" target=\"_blank\" rel=\"noopener noreferrer\">LearnLM</a>, such as <a href=\"https://research.google/blog/learn-your-way-reimagining-textbooks-with-generative-ai/\">Learn Your Way</a> to improve learning efficacy. And we keep exploring AI innovations such as the use of diffusion models for <a href=\"https://arxiv.org/abs/2408.14837\" target=\"_blank\" rel=\"noopener noreferrer\">real-time game engines</a>, which inspire new horizons for simulating immersive world environments.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"rnq9kQCclvo\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=rnq9kQCclvo\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"gxown\"><i>At Research@ Mountain View, Yossi Matias joins</i> <i>Alex Kantrowitz on the Big Technology Podcast to discuss our research efforts in areas like cancer treatment and Quantum.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"9mtq0\">AI as an amplifier of human ingenuity</h2><p data-block-key=\"90fa7\">The magic cycle of research is quickly gaining momentum. This is propelled by more powerful models, by agentic tools like the <a href=\"https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/\">AI co-scientist</a> and AI-based <a href=\"https://research.google/blog/accelerating-scientific-discovery-with-ai-powered-empirical-software/\">expert-level empirical software</a> that help accelerate scientific discovery, and open platforms and tools like <a href=\"https://research.google/blog/medgemma-our-most-capable-open-models-for-health-ai-development/\">MedGemma</a>, <a href=\"https://developers.google.com/health-ai-developer-foundations\" target=\"_blank\" rel=\"noopener noreferrer\">HAI-DEF</a> and <a href=\"https://research.google/blog/using-ai-to-identify-genetic-variants-in-tumors-with-deepsomatic/\">DeepSomatic</a>. Innovation today is happening at unprecedented speed.</p><p data-block-key=\"1mnrg\">The latest advancements point to a world where AI is not just a tool, but an essential partner and collaborator. This partnership is already taking shape in tangible ways, empowering researchers, engineers, healthcare workers, and educators. With humans at the steering wheel, we can leverage AI to bring new ideas to life and take on the challenges that matter most.</p><p data-block-key=\"34jir\">This fusion of human ingenuity with the powerful capabilities of AI will fuel further innovation and accelerate its impact for people at a global scale, defining a new era of scientific discovery for the benefit of everyone, everywhere.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "迈向可验证的AI使用隐私洞察 (原标题: Toward provably private insights into AI use)",
      "link": "https://research.google/blog/toward-provably-private-insights-into-ai-use/",
      "pubDate": "Wed, 29 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-29T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 迈向可验证的AI使用隐私洞察\n\n## 引言：生成式AI与隐私挑战\n生成式AI（GenAI）赋能个性化体验，并生成包括摘要、转录等在内的非结构化数据。开发者需要了解AI的实际使用情况，以通过识别常见应用和故障模式来增强工具。尤其当这些工具应用于设备端数据时，在洞察生成过程中提供日益强大的隐私保障至关重要。\n\n## 可验证的隐私洞察 (PPI) 简介\n本文介绍了“可验证的隐私洞察（Provably Private Insights, PPI）”，这是一个新的目标，旨在生成关于人们如何使用大型语言模型（LLMs）和GenAI工具的动态洞察，同时保证个人数据不可检查且聚合洞察是匿名的。\n\nGoogle宣布推出首个此类PPI系统，该系统结合了LLM、差分隐私（Differential Privacy, DP）和可信执行环境（Trusted Execution Environments, TEEs）来分析非结构化GenAI数据。该系统证明服务器端处理仅限于隐私保护计算，并且可以完全进行外部检查。\n\n## PPI系统的工作原理\n1.  **“数据专家”LLM**：GenAI工具开发者可以使用一个“数据专家”LLM来分析用户交互，回答诸如“正在讨论什么主题？”或“用户是否感到沮丧？”等问题。\n2.  **差分隐私 (DP)**：LLM的答案通过DP进行聚合，从而提供用户群体中GenAI功能使用情况的全面视图，而不会暴露未聚合的原始数据。\n3.  **可信执行环境 (TEE)**： “数据专家”LLM本身驻留在TEE中，确保其安全运行。\n\n### 赋能技术：机密联邦分析 (CFA)\nPPI由机密联邦分析（Confidential Federated Analytics, CFA）技术支持，该技术最初部署在Gboard中。CFA允许开源分析软件在TEE中运行，为服务器端数据处理的机制和隐私属性提供完全透明性。Google的CFA利用机密计算在处理过程中保护未聚合的用户数据，并且只发布具有正式（用户级）DP保证的输出。CFA提供强大的数据隔离和匿名化保证，无论分析师运行何种查询。\n\n### 数据处理流程与隐私保障\n*   用户设备首先决定哪些数据应上传进行分析。设备加密并上传这些数据，同时上传服务器被授权用于解密的处理步骤。\n*   上传的数据由TEE托管的密钥管理服务管理的加密密钥保护，该服务仅向设备批准的处理步骤发布解密密钥。\n*   设备可以验证密钥管理服务是预期的开源代码（包含在公共、防篡改的透明度日志Rekor中），并且代码正在正确配置的、Google无法访问的TEE中运行。\n*   密钥管理服务反过来验证批准的公共处理步骤正在TEE中运行。数据上不能执行其他分析，任何人都无法访问来自单个设备的数据。\n\n![PPI系统概览](https://storage.googleapis.com/gweb-research2023-media/images/ProvablyPrivateInsights_Overview.width-1250.png)\n\n### 洞察生成步骤\n1.  **结构化摘要**：通过一系列明确定义的处理步骤获取私有洞察。首先，非结构化原始数据由LLM分析，旨在提取特定问题的答案，例如输入的类别或主题。\n2.  **差分隐私聚合**：处理过程首先使用开源的Gemma 3模型将转录内容分类到感兴趣的类别中。然后对这些类别进行求和，以计算带有差分隐私噪声的直方图，确保输出直方图不会受到任何一个用户的强烈影响。\n3.  **LLM提示的灵活性**：LLM的提示可以频繁更改，因为DP保证适用于聚合算法，无论LLM提示如何。即使开发者提出的问题旨在识别单个用户，差分隐私统计数据也不会泄露。\n\n## 端到端可验证性\n系统所有与隐私相关的部分（从私有聚合算法到TEE堆栈，以及LLM本身）都是开源且可复现构建的。用于分析数据的工作流签名也是公开的。结合TEE证明运行软件系统状态的能力，数据处理管道的每个部分都可以与已发布的代码进行可验证的关联。这使得外部方能够验证Google的隐私声明，确保数据仅按声称的代码处理。\n\n## PPI在Pixel Recorder应用中的应用\nGoogle Pixel上的Recorder应用提供强大的AI功能，如转录、摘要和说话人识别。开发者面临的关键挑战是理解用户如何与这些功能互动。例如，用户是在创建“备忘录”、“提醒”还是录制“商务会议”？\n\n在Recorder应用中，一部分转录内容（来自在设置中启用“为所有人改进”的用户）使用公共密钥进行加密，该密钥由中央TEE托管的密钥库管理，并通过Google的Project Oak证明堆栈在AMD安全加密虚拟化-安全嵌套分页（SEV-SNP）CPU上运行。密钥库确保上传的数据只能由预先批准的处理步骤解密，这些步骤本身也经过证明在TEE中运行预期的处理步骤。一个在AMD SEV-SNP TEE中运行的Gemma 3 4B模型将转录内容分类为主题，然后通过差分隐私进行聚合。外部方可以验证原始转录内容从未离开TEE的安全环境，并且只有摘要输出类别的私有总和被发布给Google。\n\n![Recorder转录内容主题的差分隐私分布示例](https://storage.googleapis.com/gweb-research2023-media/images/ProvablyPrivateInsights2_Example.width-1250.png)\n*   图片：Gemma分类的Recorder转录内容在各种主题上的差分隐私分布示例。内部矩形大小与相对主题频率成比例。\n\n### 性能评估\nPPI还可以帮助评估设备端GenAI功能的性能，例如Recorder生成的摘要的准确性。CFA可以在结构化摘要组件中运行LLM自动评估器，该评估器也驻留在TEE中，可以评估设备端模型的结果，从而确保更准确和隐私保护的评估。这允许开发者根据真实用户交互微调设备端模型，而不会损害个人隐私。\n\n## 未来展望\n这项工作表明，可验证的隐私洞察是可行的：利用LLM分析真实世界的GenAI工具使用情况，然后聚合为差分隐私统计数据，所有这些都伴随着服务器端处理步骤的完全透明性。洞察生成过程的每一步都旨在提供最先进的数据隔离和匿名化，外部验证者可以检查方法的源代码以及Google运行这些方法的证明。\n\n未来将推出更多应用，包括差分隐私聚类和合成数据生成，所有这些都将具有相同的可验证性和机密性。通过未来支持更高吞吐量加速器（如Google TPU）的机密使用，将实现更丰富的分析，包括详细的转录分析和自动评估。现在，在不将敏感用户数据暴露在机密计算边界之外的情况下，可以生成具有强大用户级DP保证的洞察。",
      "shortSummary": "Google推出“可验证的隐私洞察（PPI）”系统，旨在安全分析生成式AI（GenAI）工具的实际使用情况。该系统结合大型语言模型（LLM）进行数据分析、差分隐私（DP）进行匿名聚合，以及可信执行环境（TEE）保障处理过程的机密性。PPI确保个人数据不可检查，聚合洞察匿名化，且服务器端处理可外部验证。例如，Pixel Recorder应用已采用PPI，利用开源Gemma模型在TEE中对用户转录内容进行分类和隐私聚合，从而在保护用户隐私的前提下，帮助开发者改进AI功能。所有隐私相关部分均开源且可验证。",
      "translated_title": "迈向可验证的AI使用隐私洞察",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ProvablyPrivateInsights_Overview.width-1250.png",
          "alt": "ProvablyPrivateInsights_Overview",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ProvablyPrivateInsights2_Example.width-1250.png",
          "alt": "ProvablyPrivateInsights2_Example",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"343fi\">Generative AI (GenAI) enables personalized experiences and powers the creation of unstructured data, including summaries, transcriptions, and more. Insights into real-world AI use [<a href=\"https://arxiv.org/abs/2412.13678\" target=\"_blank\" rel=\"noopener noreferrer\">1</a>, <a href=\"https://www.nber.org/system/files/working_papers/w34255/w34255.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">2</a>] can help GenAI developers enhance their tools by understanding common applications and identifying failure modes. And especially when those tools are applied to on-device data, our goal is to offer increasingly robust privacy guarantees during the insight generation process. This post introduces provably private insights (PPI), a new north star for generating dynamic insights into how people use LLMs and GenAI tools while guaranteeing that individual data is not inspectable and that aggregate insights are anonymous.</p><p data-block-key=\"3aguf\">Today we announce a first-of-its kind PPI system that leverages the power of large language models (LLMs), <a href=\"https://en.wikipedia.org/wiki/Differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">differential privacy</a> (DP), and <a href=\"https://en.wikipedia.org/wiki/Trusted_execution_environment\" target=\"_blank\" rel=\"noopener noreferrer\">trusted execution environments</a> (TEEs) to analyze unstructured GenAI data. This system proves that server-side processing is limited to privacy-preserving computations and can be fully externally inspected. With our system, GenAI tool developers can analyze interactions using a “data expert” LLM, tasked with answering questions like “what topic is being discussed?” or “is the user frustrated?” The LLM’s answers are aggregated with DP to provide a comprehensive view of GenAI feature usage across the user population without exposing unaggregated data. The “data expert” LLM itself resides within the TEE. PPI is enabled by <a href=\"https://research.google/blog/discovering-new-words-with-confidential-federated-analytics/\">confidential federated analytics</a> (CFA), a technique first deployed in <a href=\"https://play.google.com/store/apps/details?id=com.google.android.inputmethod.latin&amp;hl=en_US\" target=\"_blank\" rel=\"noopener noreferrer\">Gboard</a>, where open source analysis software runs in TEEs, offering complete transparency into the mechanisms and privacy properties of server-side data processing. Our deployment of PPI in the <a href=\"https://play.google.com/store/apps/details?id=com.google.android.apps.recorder&amp;hl=en_US\" target=\"_blank\" rel=\"noopener noreferrer\">Recorder</a> application for Pixel leverages Google’s latest open-source <a href=\"https://blog.google/technology/developers/gemma-3/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma models</a> as the “data expert” to offer insights into Recorder usage.</p><p data-block-key=\"9b1vb\">To encourage the external community to verify our claims, we’ve <a href=\"https://github.com/google-parfait/confidential-federated-compute\" target=\"_blank\" rel=\"noopener noreferrer\">open-sourced LLM-powered privacy preserving insights</a> as part of confidential federated analytics in <a href=\"https://github.com/google-parfait\" target=\"_blank\" rel=\"noopener noreferrer\">Google Parfait</a>, along with the rest of our TEE-hosted confidential federated analytics stack.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"343fi\">How provably private insights are possible</h2><p data-block-key=\"93q2\">Google’s CFA leverages <a href=\"https://en.wikipedia.org/wiki/Confidential_computing\" target=\"_blank\" rel=\"noopener noreferrer\">confidential computing</a> to protect unaggregated user data during processing, and only releases outputs with a formal (user-level) DP guarantee. CFA provides strong data isolation and anonymization guarantees regardless of what query an analyst runs.</p><p data-block-key=\"c0n9v\">In this technique, user devices first decide what data should be uploaded for analysis. Devices encrypt and upload this data, along with the processing steps that the server is authorized to use for decryption. Uploaded data is protected with encryption keys managed by a TEE-hosted key management service, which releases decryption keys only to device-approved processing steps. Devices can verify that the key management service is the expected open source code (included in a public, tamper-resistant transparency log, <a href=\"https://docs.sigstore.dev/logging/overview/\" target=\"_blank\" rel=\"noopener noreferrer\">Rekor</a>), and that the code is running in a properly configured TEE that is inaccessible to Google. The key management service in turn verifies that the approved, public processing steps are running in TEEs. No other analyses can be performed on the data and no human can access data from individual devices.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ProvablyPrivateInsights_Overview.width-1250.png\" alt=\"ProvablyPrivateInsights_Overview\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ProvablyPrivateInsights_Overview.width-1250.png\" alt=\"ProvablyPrivateInsights_Overview\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"343fi\">Private insights are derived by passing the data through a well-defined set of processing steps. First, unstructured raw data is analyzed by an LLM tasked with extracting the answer to a specific question, such as the category or topic of the input (“structured summarization”). Processing begins by using an <a href=\"https://www.kaggle.com/models/google/gemma-3\" target=\"_blank\" rel=\"noopener noreferrer\">open-source Gemma 3 model</a> to classify transcripts into categories of interest. These classes are then summed to compute a histogram of topics with <a href=\"https://en.wikipedia.org/wiki/Differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">differentially private</a> noise guaranteeing that the output histogram cannot be strongly influenced by any one user. The LLM’s prompt can be changed frequently, because the DP guarantee applies to the aggregation algorithm regardless of the LLM prompt. Even if the developer asked a question designed to single out one user, the differential private statistics would not reveal it.</p><p data-block-key=\"fnsv4\">All privacy-relevant parts of our system are open source and <a href=\"https://en.wikipedia.org/wiki/Reproducible_builds\" target=\"_blank\" rel=\"noopener noreferrer\">reproducibly buildable</a> — from the <a href=\"https://github.com/google-parfait/tensorflow-federated/tree/main/tensorflow_federated/cc/core/impl/aggregation/core\" target=\"_blank\" rel=\"noopener noreferrer\">private aggregation algorithm</a> to the <a href=\"https://github.com/google-parfait/confidential-federated-compute\" target=\"_blank\" rel=\"noopener noreferrer\">TEE stack</a> — and the LLM itself is also open source. The <a href=\"https://github.com/google-parfait/confidential-federated-compute/tree/main/reference_values/access_policy\" target=\"_blank\" rel=\"noopener noreferrer\">signatures</a> of the workflows used to analyze the data are also public. When combined with TEEs' ability to attest to the state of the system running the software, every part of the data processing pipeline can be verifiably linked to published code. This provides external parties the ability to verify our privacy claims. This commitment to end-to-end verifiability is how the system makes progress toward being provable — we anchor on this capability, allowing third parties to inspect the open-source code and confirm that it is exactly the code we claim to run, thereby proving to clients that this is the only code their data will be processed with, subject to <a href=\"https://www.arxiv.org/abs/2506.15924\" target=\"_blank\" rel=\"noopener noreferrer\">known weaknesses</a> in current-generation TEEs.</p><p data-block-key=\"7gel5\">In short, provably private insights can be generated by an LLM-powered structured summarization workflow in confidential federated analytics. The combination of structured summarization with differentially private histogram generation enables deeper understanding into how the GenAI tools are used in the real world, all while guaranteeing privacy. Technical details of the system can be found in the <a href=\"https://arxiv.org/abs/2510.21684\" target=\"_blank\" rel=\"noopener noreferrer\">whitepaper</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"343fi\">How provably private insights are used in Recorder</h2><p data-block-key=\"3ssdg\"><a href=\"https://play.google.com/store/apps/details?id=com.google.android.apps.recorder&amp;hl=en_US\" target=\"_blank\" rel=\"noopener noreferrer\">Google’s Recorder app</a> on Pixel offers powerful AI features, such as transcription, summarization, and speaker labeling. A key challenge for the application developers is to understand how users interact with these features. For instance, are users creating \"Notes to self,\" \"Reminders,\" or recording \"Business meetings\"? Traditional count-based analytics are insufficient to analyze such data without the help of structured summarization or another form of classification. In a traditional setting, a system would log these transcripts to a central server for classification, and then run (differentially) private count queries on the results. PPI operates in a similar way but without the risk of data being used for any other purpose.</p><p data-block-key=\"bl5mh\">In the Recorder application, a subset of transcripts (from users who have enabled “Improve for everyone” in settings) are encrypted with a <a href=\"https://github.com/google-parfait/confidential-federated-compute/blob/main/reference_values/access_policy/recorder.txtpb\" target=\"_blank\" rel=\"noopener noreferrer\">public key</a> managed by a central TEE-hosted keystore protected via <a href=\"https://github.com/project-oak/oak\" target=\"_blank\" rel=\"noopener noreferrer\">Google’s Project Oak</a> attestation stack running on <a href=\"https://www.amd.com/en/developer/sev.html\" target=\"_blank\" rel=\"noopener noreferrer\">AMD Secure Encrypted Virtualization-Secure Nested Paging</a> (SEV-SNP) CPUs. The keystore ensures that the uploaded data can be decrypted only by pre-approved processing steps, themselves attested to running the expected processing steps in TEEs. A <a href=\"https://www.kaggle.com/models/google/gemma-3\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma 3 4B model</a> running within the AMD SEV-SNP TEE classifies the transcripts into topics, which are then aggregated with differential privacy. External parties can verify that raw transcripts never leave the secure environment of the TEE, and only private sums of the summarized output categories are released to Google.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-vertical-padding --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ProvablyPrivateInsights2_Example.width-1250.png\" alt=\"ProvablyPrivateInsights2_Example\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ProvablyPrivateInsights2_Example.width-1250.png\" alt=\"ProvablyPrivateInsights2_Example\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"skkud\"><i>An example differentially private distribution of Recorder transcripts across various topics, as categorized by Gemma. Inner rectangle size is proportional to relative topic frequency.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"343fi\">PPI can also help evaluate the performance of on-device GenAI features, such as the accuracy of summaries generated by Recorder. Instead of relying solely on synthetic data, which may not accurately represent real-world use, CFA can run an LLM auto-rater as a part of the structured summarization component. This auto-rater LLM also resides within the TEE and can assess the results of the on-device model, ensuring a more accurate and privacy-preserving evaluation. This allows developers to fine-tune the on-device model based on real user interactions without compromising individual privacy.</p><p data-block-key=\"fkaue\">The configuration we’re running in Recorder is available in <a href=\"https://github.com/google-parfait/confidential-federated-compute/tree/main/reference_values/access_policy\" target=\"_blank\" rel=\"noopener noreferrer\">our GitHub repository</a> which can be connected to the specific code paths and privacy guarantees by following <a href=\"https://github.com/google-parfait/confidential-federated-compute/tree/main/docs\" target=\"_blank\" rel=\"noopener noreferrer\">these instructions</a>. The Recorder configuration guarantees that whatever LLM query is run, it is passed through the auto-tuned DP histogram aggregator with <a href=\"https://dl.acm.org/doi/10.1561/0400000042\" target=\"_blank\" rel=\"noopener noreferrer\">strict privacy guarantees</a> (user-level ε = 1 used in the figure above).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"343fi\">What’s next?</h2><p data-block-key=\"ac91u\">This work demonstrates that provably private insights are possible: real-world GenAI tool use is analyzed with LLMs and then aggregated into differentially private statistics, all with full transparency into the server-side processing steps. Every step of the insight generation process has been designed to offer state-of-the-art data isolation and anonymization, and external verifiers can check the source code of the methods and the proof that we run them.</p><p data-block-key=\"3ms7\">Moreover, we’ve shared LLM-powered structured summarization as a first application. We expect others, including differentially private <a href=\"https://arxiv.org/abs/2506.04681\" target=\"_blank\" rel=\"noopener noreferrer\">clustering</a> and <a href=\"https://arxiv.org/pdf/2510.18232\" target=\"_blank\" rel=\"noopener noreferrer\">synthetic data generation</a> to follow, all with the same level of verifiability and confidentiality. And with future work to enable confidential use of higher-throughput accelerators such as <a href=\"https://cloud.google.com/tpu\" target=\"_blank\" rel=\"noopener noreferrer\">Google TPUs</a>, richer analyses will become possible, including detailed transcript analysis and auto-rating. Insight generation is now possible without exposing sensitive user data outside of the confidential computation boundary, and with strong user-level DP guarantees for generated insights. We are excited that the technology for provably private insights is maturing just as GenAI tools are beginning to apply to on-device and sensitive-data experiences.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"343fi\">Acknowledgements</h2><p data-block-key=\"cjf7c\"><i>We thank the teams at Google that helped with algorithm design, infrastructure implementation, and production maintenance of this system, in particular teams led by Marco Gruteser, Peter Kairouz, and Timon Van Overveldt, with product manager Prem Eruvbetine, including: Albert Cheu, Brett McLarnon, Chunxiang (Jake) Zheng, Edo Roth, Emily Glanz, Grace Ni, James Bell-Clark, Kassem Fawaz, Katharine Daly, Krzysztof Ostrowski, Maya Spivak, Mira Holford, Nova Fallen, Rakshita Tandon, Ren Yi, Stanislav Chiknavaryan, Stefan Dierauf, Steve He, and Zoe Gong. We also thank close partners who supported this system through technologies and the Recorder integration, including: Allen Su, Austin Hsu, Console Chen, Daniel Minare Ho, Dennis Cheng, Jan Wassenberg, Kristi Bradford, Ling Li, Mina Askari, Miranda Huang, Tam Le, Yao-Nan Chen, and Zhimin Yao. This work was supported by Corinna Cortes, Jay Yagnik, Ramanan Rajeswaran, Seang Chau, and Yossi Matias. We additionally thank Peter Kairouz, Marco Gruteser, Mark Simborg, and Kimberly Schwede for feedback and contributions to the writing of this post.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "StreetReaderAI：通过情境感知多模态AI使街景无障碍化 (原标题: StreetReaderAI: Towards making street view accessible via context-aware multimodal AI)",
      "link": "https://research.google/blog/streetreaderai-towards-making-street-view-accessible-via-context-aware-multimodal-ai/",
      "pubDate": "Tue, 28 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-28T16:00:00.000Z",
      "creator": "Google",
      "summary": "# StreetReaderAI：通过情境感知多模态AI使街景无障碍化\n\n## 1. 背景与问题\n*   当前主流地图服务的街景工具虽然革新了虚拟导航和探索方式，但对盲人及低视力用户而言，屏幕阅读器无法解读街景图像，也缺乏替代文本，导致其无法使用。\n*   多模态AI和图像理解技术为解决这一问题提供了机会，有望使拥有超过2200亿张图像、覆盖110多个国家和地区的Google街景对视障社区更具包容性，提供沉浸式视觉体验并开辟新的探索可能性。\n\n## 2. StreetReaderAI 介绍\n*   在UIST’25上发布的论文“StreetReaderAI：通过情境感知多模态AI使街景无障碍化”中，研究人员推出了StreetReaderAI。\n*   这是一个概念验证（proof-of-concept）的无障碍街景原型，利用情境感知、实时AI和无障碍导航控制。\n*   该项目由盲人和视力正常的可访问性研究人员团队迭代设计，借鉴了无障碍第一人称游戏和导航工具（如Shades of Doom、BlindSquare、SoundScape）的经验。\n\n## 3. 核心功能\n*   **实时AI生成描述：** 提供附近道路、交叉口和地点的实时AI生成描述。\n*   **动态对话：** 与多模态AI代理就场景和当地地理进行动态对话。\n*   **无障碍平移和移动：** 通过语音命令或键盘快捷键在全景图像之间进行无障碍平移和移动。\n\n## 4. 技术实现\n*   StreetReaderAI通过将地理信息源和用户当前视野输入到Gemini中，提供情境感知的街景场景描述。\n*   利用Gemini Live实现关于场景和当地地理特征的实时互动对话。\n\n## 5. 导航体验\n*   StreetReaderAI提供沉浸式的第一人称探索体验，类似于以音频为主要界面的视频游戏。\n*   支持键盘和语音无缝交互：\n    *   **视角调整：** 用户可以使用左右箭头键调整视角。AI会提供方向（如“当前朝向：北”或“东北”）的音频反馈，并指示是否可以向前移动以及是否正对着附近的地标或地点。\n    *   **虚拟移动：** 用户可以使用上/下箭头键进行“虚拟步行”。AI会描述移动距离和附近的地理信息。\n    *   **快速移动：** 用户还可以使用“跳跃”或“传送”功能快速移动到新位置。\n\n## 6. AI子系统：虚拟向导\nStreetReaderAI的核心是两个由Gemini支持的AI子系统：AI Describer（AI描述器）和AI Chat（AI聊天）。两者都接收静态提示、可选的用户配置文件以及关于用户当前位置的动态信息（如附近地点、道路信息和当前视野图像）。\n\n### 6.1 AI Describer (AI描述器)\n*   作为情境感知场景描述工具，它结合动态地理信息和当前街景图像分析，生成实时音频描述。\n*   **两种模式：**\n    *   “默认”模式：强调盲人行人的导航和安全。\n    *   “导游”模式：提供额外的旅游信息（如历史和建筑背景）。\n*   利用Gemini预测盲人或低视力旅行者可能感兴趣的、与当前场景和当地地理相关的后续问题。\n*   **AI描述器如何结合多模态数据支持情境感知场景描述的图示：**\n    ![AI Describer Diagram](https://storage.googleapis.com/gweb-research2023-media/images/StreetReaderAI-3.width-1250.png)\n\n### 6.2 AI Chat (AI聊天)\n*   在AI Describer的基础上，允许用户询问关于当前视图、过去视图和附近地理的问题。\n*   使用Google的多模态Live API，支持实时交互、功能调用，并临时保留单个会话中的所有交互记忆。\n*   系统会跟踪并发送每次平移或移动交互以及用户的当前视图和地理上下文。\n*   **强大的“记忆”功能：** 会话上下文窗口最大可达1,048,576个输入令牌（约相当于4000多张输入图像），使其能够记住用户的位置和上下文。例如，用户可以走过一个公交车站，转弯后问“等等，那个公交车站在哪里？”，AI代理能回忆起之前的上下文并回答。\n\n## 7. 用户测试与结果\n*   研究人员与11位盲人屏幕阅读器用户进行了面对面实验室研究，参与者学习并使用StreetReaderAI探索多个地点和评估潜在步行路线。\n*   **用户反馈：**\n    *   总体评价积极，有用性评分中位数为7（1-7级量表，平均6.4，标准差0.9）。\n    *   参与者强调了虚拟导航与AI的结合、AI Chat界面的无缝性以及所提供信息的价值。\n    *   被认为是对现有街景工具无障碍性不足的重大改进。\n    *   互动式AI聊天功能被描述为使关于街道和地点的对话既引人入胜又实用。\n*   **使用数据：**\n    *   参与者访问了350多个全景图，发出了1000多次AI请求。\n    *   AI Chat的使用频率是AI Describer的六倍，表明用户更倾向于个性化、对话式查询。\n*   **改进空间：** 参与者有时难以正确辨别方向、区分AI回答的真实性以及确定AI知识的局限性。\n\n## 8. AI Chat交互问题类型分析\n研究分析了917次AI Chat交互，并标记了23类问题。最常见的四种问题类型包括：\n1.  **空间定位 (27.0%)：** 关注物体的位置和距离，例如“公交车站离我有多远？”和“垃圾桶在长凳的哪一边？”\n2.  **物体存在 (26.5%)：** 查询关键特征（如人行道、障碍物、门）是否存在，例如“这里有斑马线吗？”\n3.  **一般描述 (18.4%)：** 请求当前视图的摘要，例如“我前面有什么？”\n4.  **物体/地点位置 (14.9%)：** 询问事物在哪里，例如“最近的交叉口在哪里？”或“你能帮我找到门吗？”\n\n## 9. StreetReaderAI 准确性\n在参与者向AI Chat提出的816个问题中：\n*   **正确回答：** 703个 (86.3%)\n*   **不正确：** 32个 (3.9%)\n*   **部分正确：** 26个 (3.2%)\n*   **AI拒绝回答：** 54个 (6.6%)\n在32个不正确回答中：\n*   20个 (62.5%) 是假阴性（例如，说自行车架不存在但实际存在）。\n*   12个 (37.5%) 是误识别（例如，将黄色减速带误认为斑马线）或由于AI Chat尚未在街景中看到目标而导致的错误。\n*   研究指出，需要在实验室环境之外探索StreetReaderAI的性能。\n\n## 10. 未来展望\nStreetReaderAI是使街景工具对所有人无障碍化的一个有前景的开端。未来的扩展机会包括：\n*   **迈向地理视觉代理：** 设想更自主的AI Chat代理，可以自行探索（例如，用户询问“这条路上的下一个公交车站在哪里？”，代理自动导航并报告）。\n*   **支持路线规划：** 支持完整的起点到终点路线规划（例如，代理可以“预先步行”路线，生成对盲人友好的摘要，指出潜在障碍）。\n*   **更丰富的音频界面：** 探索更丰富的非语言反馈，包括空间化音频和从图像本身合成的沉浸式3D音频景观。",
      "shortSummary": "StreetReaderAI是一个概念验证原型，旨在通过情境感知多模态AI使街景对盲人及低视力用户无障碍化。它提供实时AI描述、与AI代理的动态对话以及无障碍导航控制。该系统由AI Describer和AI Chat组成，后者具有强大的记忆功能。用户测试显示其有用性高，尤其偏爱AI Chat。尽管存在方向辨别和AI准确性挑战，StreetReaderAI在无障碍导航方面取得了显著进展，并为未来的自主地理视觉代理和路线规划奠定了基础。",
      "translated_title": "StreetReaderAI：通过情境感知多模态AI使街景无障碍化",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/StreetReaderAI-3.width-1250.png",
          "alt": "StreetReaderAI-3",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"mnsbf\">Interactive streetscape tools, available today in every major mapping service, have revolutionized how people virtually navigate and explore the world — from previewing routes and inspecting destinations to remotely visiting world-class tourist locations. But to date, screen readers have not been able to interpret street view imagery, and alt text is unavailable. We now have an opportunity to redefine this immersive streetscape experience to be inclusive for all with multimodal AI and image understanding. This could eventually allow a service like Google Street View, which has over 220 billion images spanning 110+ countries and territories, to be more accessible to people in the blind and low-vision community, offering an immersive visual experience and opening up new possibilities for exploration.</p><p data-block-key=\"a7psv\">In “<a href=\"https://arxiv.org/pdf/2508.08524\" target=\"_blank\" rel=\"noopener noreferrer\">StreetReaderAI: Making Street View Accessible Using Context-Aware Multimodal AI</a>”, presented at <a href=\"https://uist.acm.org/2025/\" target=\"_blank\" rel=\"noopener noreferrer\">UIST’25</a>, we introduce StreetReaderAI, a proof-of-concept accessible street view prototype that uses context-aware, real-time AI and accessible navigation controls. StreetReaderAI was designed iteratively by a team of blind and sighted accessibility researchers, drawing on previous work in accessible first-person gaming and navigation tools, such as <a href=\"https://www.gmagames.com/sod.html\" target=\"_blank\" rel=\"noopener noreferrer\">Shades of Doom</a>, <a href=\"https://www.blindsquare.com/\" target=\"_blank\" rel=\"noopener noreferrer\">BlindSquare</a>, and <a href=\"https://www.microsoft.com/en-us/research/product/soundscape/\" target=\"_blank\" rel=\"noopener noreferrer\">SoundScape</a>. Key capabilities include:</p><ul><li data-block-key=\"9pu1u\">Real-time AI-generated descriptions of nearby roads, intersections, and places.</li><li data-block-key=\"5qf3g\">Dynamic conversation with a multimodal AI agent about scenes and local geography.</li><li data-block-key=\"9oaqi\">Accessible panning and movement between panoramic images using voice commands or keyboard shortcuts.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/StreetReaderAI-AIDescriber.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"xrzw7\"><i>StreetReaderAI provides a context-aware description of the street view scene by inputting geographic information sources and the user’s current field-of-view into Gemini. For the full audio-video experience, including sound, please refer to this</i> <a href=\"https://youtu.be/cm9gd-502TI\" target=\"_blank\" rel=\"noopener noreferrer\"><i>YouTube video</i></a><i>.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/AIChat_BigBen-PrivacyRedacted.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"xrzw7\"><i>StreetReaderAI uses</i> <a href=\"https://ai.google.dev/gemini-api/docs/live\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Gemini Live</i></a><i> to provide a real-time, interactive conversation about the scene and local geographic features. For the full audio-video experience, including sound, please refer to this</i> <a href=\"https://youtu.be/unsNaq-ea3s\" target=\"_blank\" rel=\"noopener noreferrer\"><i>YouTube video</i></a><i>.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Navigating in StreetReaderAI</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">StreetReaderAI offers an immersive, first-person exploration experience, much like a video game where audio is the primary interface.</p><p data-block-key=\"f9d2t\">StreetReaderAI provides seamless navigation through both keyboard and voice interaction. Users can explore their surroundings using the left and right arrow keys to shift their view. As the user pans, StreetReaderAI shares audio feedback, voicing the current heading as a cardinal or intercardinal direction (<i>e.g.,</i> “<i>Now facing: North</i>” or “<i>Northeast</i>”). It also expresses whether the user can move forward and if they are currently facing a nearby landmark or place.</p><p data-block-key=\"9ccll\">To move, the user can take “virtual steps” using the up arrow or move backward with the down arrow. As a user moves through the virtual streetscape, StreetReaderAI describes how far the user traveled and key geographic information, such as nearby places. Users can also use “jump” or “teleport” features to quickly move to new locations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How StreetReaderAI serves as a virtual guide</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">The core of StreetReaderAI is its two underlying AI subsystems backed by Gemini: AI Describer and AI Chat. Both subsystems take in a static prompt and optional user profile as well as dynamic information about the user’s current location, such as nearby places, road information, and the current field-of-view image (i.e., what’s being shown in Street View).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">AI Describer</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">AI Describer functions as a context-aware scene description tool that combines dynamic geographic information about the user’s virtual location along with an analysis of the current Street View image to generate a real-time audio description.</p><p data-block-key=\"6gha8\">It has two modes: a “default<i>”</i> prompt emphasizing navigation and safety for blind pedestrians, and a “tour guide<i>”</i> prompt that provides additional tourism information (e.g., historic and architectural context). We also use Gemini to predict likely follow-up questions specific to the current scene and local geography that may be of interest to blind or low-vision travelers.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/StreetReaderAI-3.width-1250.png\" alt=\"StreetReaderAI-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/StreetReaderAI-3.width-1250.png\" alt=\"StreetReaderAI-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"xrzw7\"><i>A diagram of how AI Describer combines multimodal data to support context-aware scene descriptions.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">AI Chat</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">AI Chat builds on AI Describer but allows users to ask questions about their current view, past views, and nearby geography. The chat agent uses <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/live-api\" target=\"_blank\" rel=\"noopener noreferrer\">Google's Multimodal Live API</a>, which supports real-time interaction, function calling, and temporarily retains memory of all interactions within a single session. We track and send each pan or movement interaction along with the user's current view and geographic context (e.g., nearby places, current heading).</p><p data-block-key=\"6v4fs\">What makes AI Chat so powerful is its ability to hold a temporary “memory” of the user's session — the context window is set to a maximum of 1,048,576 input tokens, which is roughly equivalent to over 4k input images. Because AI Chat receives the user's view and location with every virtual step, it collects information about the user’s location and context. A user can virtually walk past a bus stop, turn a corner, and then ask, “<i>Wait, where was that bus stop?</i>” The agent can recall its previous context, analyze the current geographic input, and answer, “<i>The bus stop is behind you, approximately 12 meters away.</i>”</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Testing StreetReaderAI with blind users</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">To evaluate StreetReaderAI, we conducted an in-person lab study with eleven blind screen reader users. During the sessions, participants learned about StreetReaderAI and used it to explore multiple locations and evaluate potential walking routes to destinations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/StreetReaderAI-BusStopTask.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"jvl9c\"><i>A blind participant using StreetReaderAI to explore potential travel to a bus stop and inquire about bus stop features, such as the existence of benches and a shelter. For the full audio-video experience, including sound, please refer to this</i> <a href=\"https://youtu.be/rt7Dlqv0eoA\" target=\"_blank\" rel=\"noopener noreferrer\"><i>YouTube video</i></a><i>.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"mnsbf\">Overall, participants reacted positively to StreetReaderAI, rating the overall usefulness 6.4 (median=7; SD=0.9) on a <a href=\"https://en.wikipedia.org/wiki/Likert_scale\" target=\"_blank\" rel=\"noopener noreferrer\">Likert scale</a> from 1–7 (where 1 was ‘not at all useful’ and 7 was ‘very useful’), emphasizing the interplay between virtual navigation and AI, the seamlessness of the interactive AI Chat interface, and the value of information provided. Qualitative feedback from participants consistently highlighted StreetReaderAI's significant accessibility advancement for navigation, noting that existing street view tools lack this level of accessibility. The interactive AI chat feature was also described as making conversations about streets and places both engaging and helpful.</p><p data-block-key=\"evnc8\">During the study, participants visited over 350 panoramas and made over 1,000 AI requests. Interestingly, AI Chat was used six times more often than AI Describer, indicating a clear preference for personalized, conversational inquiries. While participants found value in StreetReaderAI and adeptly combined virtual world navigation with AI interactions, there is room for improvement: participants sometimes struggled with properly orienting themselves, distinguishing the veracity of AI responses, and determining the limits of AI knowledge.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/StreetReaderAI-PlaygroundTask.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"jvl9c\"><i>In one study task, participants were given the instruction, “Find out about an unfamiliar playground to plan a trip with your two young nieces.” This video clip illustrates the diversity of questions asked and the responsiveness of StreetReaderAI. For the full audio-video experience, including sound, please refer to this</i> <a href=\"https://youtu.be/Uxj5fSCp1Dg\" target=\"_blank\" rel=\"noopener noreferrer\"><i>YouTube video</i></a><i>.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Results</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">As the first study of an accessible street view system, our research also provides the first-ever analysis of the types of questions blind people ask about streetscape imagery. We analyzed all 917 AI Chat interactions and annotated each with up to three tags drawn from an emergent list of 23 question type categories. The four most common question types included:</p><ul><li data-block-key=\"c3cvo\"><b>Spatial orientation</b>: 27.0% of participants were most interested in the location and distance of objects, e.g., “<i>How far is the bus stop from where I'm standing?</i>” and <i>“Which side are the garbage cans next to the bench?”</i></li><li data-block-key=\"akthh\"><b>Object existence</b>: 26.5% of participants queried for the presence of key features like sidewalks, obstacles, and doors; <i>“Is there a crosswalk here?”</i></li><li data-block-key=\"e9jjp\"><b>General description</b>: 18.4% of participants started AI Chat by requesting a summary of the current view, often asking, “<i>What's in front of me?</i>”</li><li data-block-key=\"84eds\"><b>Object/place location</b>: 14.9% of participants asked <i>where</i> things were, such as, “<i>Where is the nearest intersection?</i>” or “<i>Can you help me find the door?</i>”</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">StreetReaderAI accuracy</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">Because StreetReaderAI relies so significantly on AI, a critical challenge is response accuracy. Of the 816 questions that participants asked AI Chat:</p><ul><li data-block-key=\"4i79q\">703 (86.3%) were correctly answered.</li><li data-block-key=\"e5j91\">32 (3.9%) were incorrect (3.9%).</li><li data-block-key=\"7tdgb\">The remaining were either: partially correct (26; 3.2%) or the AI refused to answer (54; 6.6%).</li></ul><p data-block-key=\"2rj55\">Of the 32 incorrect responses:</p><ul><li data-block-key=\"20m48\">20 (62.5%) were false negatives, <i>e.g.,</i> stating that a bike rack did not exist when it did.</li><li data-block-key=\"98cf5\">12 (37.5%) were misidentifications (<i>e.g.,</i> a yellow speed bump interpreted as a crosswalk) or misc errors due to AI Chat not yet seeing the target in street view.</li></ul><p data-block-key=\"61i5r\">More work is necessary to explore how StreetReaderAI performs in other contexts and beyond lab settings.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">What’s next?</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">StreetReaderAI is a promising first step toward making streetscape tools accessible to all. Our <a href=\"https://research.google/pubs/streetviewai-making-street-view-accessible-using-context-aware-multimodal-ai/\">study</a> highlights <i>what</i> information blind users desire from and ask about streetscape imagery and the potential for multimodal AI to answer their questions.</p><p data-block-key=\"bp879\">There are several other opportunities to expand on this work:</p><ul><li data-block-key=\"mbdd\"><b>Towards Geo-visual Agents:</b> We envision a more autonomous AI Chat agent that can explore on its own. For example, a user could ask, “<i>What’s the next bus stop down this road?</i>” and the agent could automatically navigate the Street View network, find the stop, analyze its features (benches, shelters), and report back.</li><li data-block-key=\"946n0\"><b>Supporting Route Planning:</b> Similarly, StreetReaderAI does not yet support full origin-to-destination routing. Imagine asking, “<i>What’s the walk like from the nearest subway station to the library?</i>” A future AI agent could “pre-walk” the route, analyzing every Street View image to generate a blind-friendly summary, noting potential obstacles, and identifying the exact location of the library’s door.</li><li data-block-key=\"987rq\"><b>Richer Audio Interface:</b> The primary output of StreetReaderAI is speech. We are also exploring richer, non-verbal feedback, including spatialized audio and fully immersive 3D audio soundscapes synthesized from the images themselves.</li></ul><p data-block-key=\"fokff\">Though a “proof-of-concept” research prototype, StreetReaderAI helps demonstrate the potential of making immersive streetscape environments accessible.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\"><i>This research was conducted by Jon E. Froehlich, Alexander J. Fiannaca, Nimer Jaber, Victor Tsaran, Shaun K. Kane, and Philip Nelson. We thank Project Astra and the Google Geo teams for their feedback as well as our participants. Diagram icons are from Noun Project, including: “</i><a href=\"https://thenounproject.com/icon/ai-prompt-7906362/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>prompt icon</i></a><i>” by Firdaus Faiz, “</i><a href=\"https://thenounproject.com/icon/html-7643378/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>command functions</i></a><i>” by Kawalan Icon, “</i><a href=\"https://thenounproject.com/icon/place-7892484/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>dynamic geo-context</i></a><i>” by Didik Darmanto, and “</i><a href=\"https://thenounproject.com/icon/ai-7865331/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>MLLM icon</i></a><i>” by Funtasticon.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "我们如何构建个人健康教练 (原标题: How we are building the personal health coach)",
      "link": "https://research.google/blog/how-we-are-building-the-personal-health-coach/",
      "pubDate": "Sun, 26 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-26T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 我们如何构建个人健康教练\n\n## 解决现有健康旅程的痛点\n\n文章指出，当前的健康和健身旅程存在碎片化、通用性和难以获取的问题。例如，初级保健医生可能会建议专科就诊或减肥以更好地管理糖尿病，但通常不会提供营养师或健身教练的联系方式，这使得用户需要自行连接这些服务。\n\n## 我们的愿景：AI 驱动的个人健康教练\n\n为了解决上述问题，我们的愿景是提供一个主动、个性化和适应性强的 AI 驱动的个人健康教练。该教练将无缝实现以下功能：\n\n*   **主动洞察**：提供关于睡眠、健身和健康的预见性洞察。\n*   **个性化指导**：基于行为科学、既定的健康和福祉原则，以及个人健康指标（如活动和其他生理数据）提供个性化指导。\n*   **个性化辅导**：通过可操作、适应性强的计划，帮助用户设定目标并养成可持续的习惯。\n\n## 推出与可用性\n\n从明天开始，我们将在接下来的一周内，为符合条件的美国 Fitbit Premium Android 用户推出健康教练的可选公开预览版，并很快扩展到 iOS 用户。选择参与的用户将被要求同意提供其 Fitbit 数据访问权限，以接收个性化洞察。\n\n## 技术基础与创新\n\n这项创新得益于 Gemini 模型的进步，以及 Fitbit 应用中全新的 AI 优先个人健康教练体验，以及 Fitbit、Google Research 和 Google DeepMind 在尖端研究方面的持续进展。构建一个健康和福祉产品需要时间和严谨性，我们以深思熟虑和迭代的方式进行，以科学为基础，并结合科学界和用户的反馈。\n\n### 引导 Gemini 进行健康辅导\n\n回答“我锻炼后睡眠会更好吗？”这类看似简单的问题，需要教练具备主动、个性化和适应性，这涉及多项技术创新：\n\n1.  **理解和数值推理生理时间序列数据**：教练需要理解并对睡眠和活动等生理时间序列数据进行数值推理，其能力类似于 PH-LLM 所展示的。对于此类问题，教练会验证近期数据的可用性，选择正确的指标，对比相关日期，根据个人基线和人群统计数据进行情境化分析，整合与教练的过往互动，并最终利用分析结果提供量身定制的答案和洞察。\n2.  **多智能体框架**：我们利用一个多智能体框架来协调专家子智能体，以提供清晰、一致和全面的支持，包括：\n    *   **对话智能体**：负责多轮对话、意图理解、智能体编排、上下文收集和响应生成。\n    *   **数据科学智能体**：迭代使用工具来获取、分析和总结相关数据（例如，睡眠和锻炼数据），并根据需要利用代码生成能力。\n    *   **领域专家智能体**：例如健身专家，分析用户数据以生成个性化健身计划，并根据进展和上下文变化进行调整。\n\n    ![个人健康教练的工作原理示意图](https://storage.googleapis.com/gweb-research2023-media/images/PH-coach-1.width-1250.png)\n    *个人健康教练的工作原理示意图。用户可以进行对话式提问。在后台，对话智能体处理对话、收集上下文并协调其他智能体。其他智能体包括专注于检索和分析相关数据的数据科学智能体，以及理解特定领域（如健身）的领域专家智能体。*\n\n3.  **对基础模型的精心引导**：尽管基础模型能力强大，但在健康和福祉领域，需要仔细引导才能使其发挥作用。我们开发了基于消费者健康和福祉需求的评估方法，以指导系统指令并改进 Gemini 协助用户核心能力。\n\n这些创新共同带来了更个性化和有益的指导。\n\n## 专家验证和用户迭代设计的重要性\n\n我们深知仅有技术卓越是不够的，可靠性和安全性至关重要。\n\n### 科学基础与专家反馈\n\n*   我们将教练建立在科学和成熟的辅导及健身框架之上。\n*   我们采用以人为本的设计，整合了专家和用户反馈，包括：\n    *   召集由顶尖专家组成的消费者健康咨询小组，为教练的开发提供反馈和指导。\n    *   通过专业健身教练的意见扩展运动科学基础，以纳入特定情境的方法。\n    *   开发新颖的方法与专家合作，在个人健康和福祉的细微领域促进共识。\n    *   在大型同意研究中（例如，来自 Fitbit Insights Explorer、睡眠和症状检查器实验室）积极征求了数千名用户的反馈。\n\n### 持续评估与改进\n\n我们使用 SHARP 评估框架（安全性、有用性、准确性、相关性和个性化）持续验证个人健康教练。这种多层次评估涉及超过 100 万次人工标注和超过 10 万小时的由通才和运动、睡眠、家庭医学、心脏病学、内分泌学、运动和行为科学等各个领域的专家进行的人工评估。这一过程通过自动评估器进一步扩展和规模化，以确保健康建议的科学准确性。该框架还整合了教练的实际表现，从而在最关键的领域实现持续改进。\n\n## 帮助我们构建您的教练\n\n加入公开预览版，并在应用内或通过我们的社区论坛分享您的反馈。您将帮助塑造教练，使其能为您做更多事情，并与您共同成长。",
      "shortSummary": "我们正在构建一个由AI驱动的个人健康教练，旨在解决当前健康旅程的碎片化问题。该教练基于Gemini模型和多智能体框架，提供主动、个性化和适应性的睡眠、健身及健康指导，帮助用户设定目标并养成习惯。它通过理解生理数据、专家系统和持续的用户反馈与科学验证（SHARP框架）来确保可靠性和准确性。目前已向符合条件的美国Fitbit Premium Android用户推出公开预览版。",
      "translated_title": "我们如何构建个人健康教练",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PH-coach-1.width-1250.png",
          "alt": "PH-coach-1",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fl9us\">Historically, health and fitness journeys have been fragmented, generic and inaccessible, whether within existing apps or through general health and fitness journeys outside of apps. For instance, a primary care provider might suggest seeing a specialist or losing weight for better diabetes management, but often without providing connections to a nutritionist or fitness coach. This leaves users with the burden of connecting these dots themselves.</p><p data-block-key=\"2u7lq\">Our vision is to address this by offering a proactive, personalized and adaptive AI-powered personal health coach. This coach will seamlessly enable:</p><ul><li data-block-key=\"b8to4\">Proactive insights on sleep, fitness and health</li><li data-block-key=\"9pvk4\">Personalized guidance grounded in behavioral science, established health and wellness principles and individual health metrics, such as activity and other physiological data</li><li data-block-key=\"1756a\">Personalized coaching to set goals and build sustainable habits through actionable, adaptive plans</li></ul><p data-block-key=\"1n2cb\">Starting tomorrow and over the next week, we are rolling out an optional <a href=\"https://blog.google/products/fitbit/personal-health-coach-public-preview/\" target=\"_blank\" rel=\"noopener noreferrer\">public preview of the health coach</a> for eligible US-based <a href=\"https://store.google.com/product/fitbit_premium?hl=en-US&amp;pli=1\" target=\"_blank\" rel=\"noopener noreferrer\">Fitbit Premium</a> Android users and expanding to iOS users soon. Users who opt in to participate will be presented the consent to provide access to their Fitbit data to receive personalized insights.</p><p data-block-key=\"a9d5t\">This innovation is powered by advances in <a href=\"https://ai.google.dev/gemini-api/docs/models\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini models</a> plus our new AI-first personal health coach experience on the Fitbit app, and our continuous progress in cutting-edge research across Fitbit, Google Research and Google DeepMind. Building from the ground up takes time and rigor, a commitment especially important for health and wellness. We are approaching this thoughtfully and iteratively, grounded in science, incorporating feedback from the scientific community and users. We will continue to be open about our work through publications and updates, and here we provide a quick peek into what went into building the personal health coach.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Steering Gemini for health coaching</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fl9us\">“Do I get better sleep after exercising?” sounds like a simple question, but answering it like a proactive, personalized and adaptive coach required several technical innovations.</p><p data-block-key=\"4rl0\">First, we need the coach to understand and do numerical reasoning on physiological time series data such as sleep and activity, using capabilities similar to those showcased by <a href=\"https://research.google/blog/advancing-personal-health-and-wellness-insights-with-ai/\">PH-LLM</a>. For questions like this, the coach verifies recent data availability, chooses the right metrics, contrasts relevant days, contextualizes results against personal baselines and population-level statistics, incorporates prior interactions with the coach, and finally uses the analysis to provide tailored answers and insights.</p><p data-block-key=\"f4inb\">Second, we utilize a <a href=\"https://research.google/blog/the-anatomy-of-a-personal-health-agent/\">multi-agent framework</a> that coordinates expert sub-agents to provide clear, consistent and holistic support, such as (1) a <a href=\"https://arxiv.org/pdf/2503.19328\" target=\"_blank\" rel=\"noopener noreferrer\">conversational agent</a> for multi-turn conversations, intent understanding, agent orchestration, context gathering and response generation; (2) a data science agent that iteratively uses tools to fetch, analyze, and summarize relevant data (e.g., sleep and workout data), <a href=\"https://arxiv.org/pdf/2406.06464\" target=\"_blank\" rel=\"noopener noreferrer\">leveraging code-generation capabilities as needed</a>; and (3) a domain expert, such as a fitness expert that analyzes user data to generate personalized fitness plans and adapt them as progress and context change.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PH-coach-1.width-1250.png\" alt=\"PH-coach-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PH-coach-1.width-1250.png\" alt=\"PH-coach-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"jj9h8\"><i>Schematic of how the personal health coach works. The user can ask questions conversationally. Behind the scenes, a conversation agent handles the conversation, gathers context, and orchestrates other agents. The other agents include a data science agent focusing on retrieving and analyzing relevant data, and a domain expert agent that understands specific fields such as fitness.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fl9us\">Third, while foundational models are incredibly capable, careful steer is required for it to be useful in the health and wellness context. We developed evaluations based on consumer health and wellness needs to inform the system instructions and improve upon Gemini’s core capabilities in assisting our users.</p><p data-block-key=\"einqj\"><a href=\"https://research.google/blog/the-anatomy-of-a-personal-health-agent/\">Working together</a>, these innovations result in more personalized and beneficial guidance.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The critical role of expert validation and iterative design with users</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fl9us\">We know that technical excellence alone isn’t enough, and reliability and safety are paramount.</p><p data-block-key=\"81mub\">First, we grounded the coach in scientific and well established <a href=\"https://services.google.com/fh/files/blogs/evolving_cardio_load_2025.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">coaching and fitness frameworks</a>.</p><p data-block-key=\"7sbql\">We also used human-centered design to integrate expert and user feedback including:</p><ul><li data-block-key=\"a61ei\">Convening a <a href=\"https://store.google.com/intl/en/ideas/articles/google-consumer-health-advisory-panel/\" target=\"_blank\" rel=\"noopener noreferrer\">Consumer Health Advisory Panel</a> of leading experts to provide feedback and guidance for the development of the coach.</li><li data-block-key=\"86hoc\">Extending the sport science foundation with input with professional fitness coaches to incorporate context-specific approaches.</li><li data-block-key=\"3mvn\">Developing <a href=\"https://arxiv.org/pdf/2508.09349\" target=\"_blank\" rel=\"noopener noreferrer\">novel methods</a> to collaborate with experts, fostering consensus in nuanced areas of personal health and wellness.</li><li data-block-key=\"ih7j\">Actively soliciting feedback from thousands of users in large-scale consented research studies (e.g., from Fitbit Insights Explorer, Sleep and Symptom Checker <a href=\"https://support.google.com/fitbit/answer/14566053?hl=en#zippy=%2Cwhat-personalized-sleep-schedule-lab-is\" target=\"_blank\" rel=\"noopener noreferrer\">Labs</a>).</li></ul><p data-block-key=\"cq03p\">Finally, we continuously validate the personal health coach using dimensions of safety, helpfulness, accuracy, relevance, and personalization, collectively known as the <a href=\"https://services.google.com/fh/files/blogs/winslow_2025_sharp_framework.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">SHARP evaluation framework</a>. This multi-level assessment involves over 1 million human annotations and more than 100k hours of human evaluation by generalists and experts across various fields such as sports, sleep, family medicine, cardiology, endocrinology, exercise and behavioral science. This process is further extended and scaled with autoraters to ensure that wellness recommendations are scientifically accurate. The framework also incorporates the coach’s real-world performance, enabling ongoing improvements in the most critical areas.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Help us build your coach</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fl9us\">Join the <a href=\"https://blog.google/products/fitbit/personal-health-coach-public-preview/\" target=\"_blank\" rel=\"noopener noreferrer\">public preview</a> and share your feedback in the app or through our <a href=\"https://community.fitbit.com/t5/Personal-health-coach-public-preview/bd-p/PHAPP\" target=\"_blank\" rel=\"noopener noreferrer\">community forum</a>. You will help shape the coach, so it can do more for and with you.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Google Earth AI：利用基础模型和跨模态推理解锁地理空间洞察 (原标题: Google Earth AI: Unlocking geospatial insights with foundation models and cross-modal reasoning)",
      "link": "https://research.google/blog/google-earth-ai-unlocking-geospatial-insights-with-foundation-models-and-cross-modal-reasoning/",
      "pubDate": "Wed, 22 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-22T16:00:00.000Z",
      "creator": "Google",
      "summary": "Google Earth AI 旨在通过结合强大的基础模型和基于 Gemini 的地理空间推理代理，解决跨领域洞察的复杂挑战，实现行星尺度的复杂推理。该系统将复杂问题分解为多步骤计划，利用基础模型、海量数据存储和地理空间工具执行计划，并整合结果以提供全面的答案。\n\n### 最新创新\n\nGoogle Earth AI 引入了以下新功能：\n\n*   **新的图像和人口基础模型**：提供了详细的技术信息和评估，展示了最先进的性能。\n*   **地理空间推理代理演示**：展示了如何利用这些模型解决复杂的多步骤地理空间查询。\n\n### Earth AI 的构建模块\n\nEarth AI 的核心在于其先进的基础模型：\n\n*   **图像模型（遥感基础模型）**\n    *   **能力**：包括视觉-语言模型、开放词汇目标检测和适应性视觉骨干。\n    *   **自然语言查询**：用户可以使用自然语言提问，例如在风暴后的图像中“查找所有被洪水淹没的道路”，并获得快速准确的答案。\n    *   **性能**：模型在大量高分辨率航拍图像和文本描述语料库上进行训练，在多个公共地球观测基准上取得了最先进的结果。例如，在基于文本的图像搜索任务中平均改进超过16%，零样本新目标检测模型的基线准确率翻倍。\n    *   **图片**：\n        *   Earth AI 流程图，展示了其如何结合最先进的模型与地理空间推理代理来解决关键的全球挑战。\n        ![Flowchart of Earth AI](https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-1a-Overview.width-1250.png)\n        *   模型评估显示，遥感优化型 RS-OWL-ViT-v2 模型（“我们的”）在零样本设置下，相对于 OWL-ViT-v2 开放词汇检测模型，平均精度（AP50）有显著提高，并说明了 FLAME + RS-OWL-ViT-v2 组合方法（“我们的”）在新型类别少样本检测中相对于 SIoU 的优势。\n        ![EarthAI-2-RemoteSensingEvalFinal](https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-2-RemoteSensingEvalFinal.width-1250.png)\n\n*   **人口模型（人口动态基础模型）**\n    *   **目标**：理解人与地点之间复杂的相互作用。\n    *   **创新**：引入了两个关键创新：跨17个国家的全球一致嵌入，以及每月更新的嵌入，以捕捉人类活动不断变化的动态，这对于时间敏感的预测至关重要。\n    *   **效果**：在独立研究中表现出显著效果，例如，牛津大学研究人员发现，将这些嵌入整合到巴西登革热预测模型中，将12个月预测的长期 R²（衡量模型解释实际疾病率的指标）从0.456提高到0.656。\n    *   **图片**：\n        *   人口动态基础模型在17个国家/地区的评估；R² 分数（范围0-1，越高越好）按国家/地区预测人口密度、树木覆盖、夜间灯光和海拔。全球趋势与我们最初在美国展示的强大性能相符。\n        ![Earth AI Population Dynamics plot](https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-3-PopDynEval.width-1250.png)\n\n*   **环境模型**\n    *   **预测能力**：提供中程天气、季风、空气质量和河流洪水的最新预测。\n    *   **扩展**：最近已扩展到为全球提供降水临近预报，并为20亿人提供最严重的河流洪水预报。\n\n### 模型组合的预测能力增强\n\n研究发现，结合不同模型能够产生更强大的预测能力。这种协同方法可以更全面、准确地理解现实世界现象，并显著改善关键应用的预测。例如，通过融合人口动态基础模型中的社会经济特征嵌入和 AlphaEarth 基础模型中的景观特征，FEMA 国家风险指数对20种不同灾害的预测平均 R² 提高了11%，其中龙卷风风险预测提高了25%，河流洪水风险预测提高了17%。\n\n### 通过地理空间推理解决复杂问题\n\n由 Gemini 驱动的地理空间推理代理简化了 Earth AI 洞察的协调。该代理能够将复杂的自然语言查询分解为动态的多步骤计划，并调用“专家”子代理，利用 Earth AI 模型、Data Commons、Earth Engine 和地理空间特定工具来执行每个步骤。这种模块化的代理网络具有可扩展性和可定制性。\n\n**案例演示：识别易受风暴影响的人口**\n\n1.  调用环境模型识别可能受到飓风风力影响的特定地理区域。\n2.  查询 Data Commons 获取人口统计数据，以识别预测登陆区域内人口较多的县。\n3.  从 BigQuery 的公共数据集中检索相关县的官方边界。\n4.  对风区和官方县边界进行空间交叉。\n5.  利用人口动态基础模型和县级统计数据，动态训练模型以识别最脆弱的邮政编码。\n6.  使用遥感基础模型的目标检测功能，识别最脆弱邮政编码区域内的关键基础设施。\n\n**代理评估**\n\n*   **Q&A 基准测试**：地理空间推理代理在 Q&A 基准测试中取得了0.82的总体准确率，显著优于基线 Gemini 2.5 Pro (0.50) 和 Gemini 2.5 Flash (0.39) 代理。\n*   **图片**：\n    *   代理在 Q&A 基准测试中的表现可视化。地理空间推理代理在描述性和检索类别中比基线 Gemini 2.5 Pro 代理高出37%，在更复杂的分析和关系类别中高出124%，总分高出64%。\n    ![Earth AI performance on QA](https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-7-QAPerformance.width-1250.png)\n*   **危机响应案例研究**：论文通过案例研究展示了协调环境、遥感和人口动态等多样化洞察的益处。\n\n### 共同释放地球潜力\n\nEarth AI 代表着行星理解的根本性飞跃。多模态、基于推理的方法，建立在最先进的地理空间 AI 模型基础上，能够解锁单一分析无法实现的洞察。Google 致力于扩大访问权限，以帮助全球社区应对地球最紧迫的挑战。\n\n**实际应用案例**：\n\n*   **Bellwether (Google X)**：利用 Earth AI 预测风暴前的建筑物损坏，帮助保险客户更快支付索赔。\n*   **联合国全球脉动**：使用 Earth AI 图像模型评估自然灾害后的损失，实现快速危机响应。\n*   **GiveDirectly**：利用地理空间推理和洪水预报识别高风险社区并提供现金援助。\n*   **Google.org 资助合作伙伴**：如 Khushi Baby、Cooper/Smith 等，利用人口动态基础模型建模传染病并改善全球公共卫生行动。\n*   **新企业用户**：包括 Public Storage、CARTO 和 Visiona Space Technology。\n\nGoogle 鼓励组织表达对早期访问遥感基础模型（Vertex AI 中的图像模型）、人口动态基础模型和地理空间推理的兴趣。",
      "shortSummary": "Google Earth AI 结合强大的基础模型和基于 Gemini 的地理空间推理代理，旨在实现行星尺度的复杂地理空间洞察。它引入了新的图像和人口基础模型，并展示了其推理代理如何处理多步骤查询。通过整合遥感、人口动态和环境模型，Earth AI 显著提升了预测能力，例如在灾害风险评估和传染病建模方面。该平台已应用于预测风暴损害、评估灾后损失和提供灾害援助等领域，致力于帮助全球应对地球挑战，并邀请开发者和企业表达合作兴趣。",
      "translated_title": "Google Earth AI：利用基础模型和跨模态推理解锁地理空间洞察",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-1a-Overview.width-1250.png",
          "alt": "Flowchart of Earth AI",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-2-RemoteSensingEvalFinal.width-1250.png",
          "alt": "EarthAI-2-RemoteSensingEvalFinal",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-3-PopDynEval.width-1250.png",
          "alt": "Earth AI Population Dynamics plot",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-7-QAPerformance.width-1250.png",
          "alt": "Earth AI performance on QA",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"445ko\">For years, Google has developed AI models that enhance our understanding of the planet. These models help keep Google products fresh, for example, ensuring Maps is accurate by <a href=\"https://blog.google/technology/research/open-buildings-ai-powered-maps-for-a-changing-world/\" target=\"_blank\" rel=\"noopener noreferrer\">analyzing satellite images</a> and giving Search users the most up-to-date alerts about <a href=\"https://blog.google/intl/en-africa/products/explore-get-answers/nowcasting-on-search-is-bringing-ai-powered-weather-forecasts-to-users-across-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">weather</a> and <a href=\"https://blog.google/technology/ai/expanding-flood-forecasting-coverage-helping-partners/\" target=\"_blank\" rel=\"noopener noreferrer\">natural</a> <a href=\"https://blog.google/outreach-initiatives/sustainability/google-wildfire-boundary-maps-europe-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">disasters</a>.</p><p data-block-key=\"f8odl\">As individual models grow more powerful, we’ve learned that many real-world questions require the combination of insights <i>across</i> domains. Answering complex queries like, <i>\"Where is a hurricane likely to make landfall? Which communities are most vulnerable and how should they prepare?\"</i> requires reasoning about imagery, population and the environment.</p><p data-block-key=\"a9dl3\">Earlier this year, we introduced <a href=\"https://blog.google/technology/ai/google-earth-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Earth AI</a> to solve this core challenge. By pairing our family of powerful foundation models with a <a href=\"https://research.google/blog/geospatial-reasoning-unlocking-insights-with-generative-ai-and-multiple-foundation-models/\">geospatial reasoning</a> agent, which uses our latest Gemini models, it’s becoming possible to perform complex, real-world reasoning at planetary scale. The models provide detailed understanding of our planet, grounded in real-world data. The agent, in turn, acts as an intelligent orchestrator. It deconstructs a complex question into a multi-step plan; executes the plan by calling on these foundation models, querying vast datastores, and using geospatial tools; and finally fuses the results at each step into a holistic answer.</p><p data-block-key=\"pma0\">Today, we're <a href=\"http://goo.gle/earthai-techreport\" target=\"_blank\" rel=\"noopener noreferrer\">introducing new Earth AI innovations</a>:</p><ol><li data-block-key=\"5pdsv\">New Imagery and Population foundation models, along with technical details and evaluations showing state-of-the-art performance.</li><li data-block-key=\"22iue\">Demonstrations of our geospatial reasoning agent using these models to solve complex, multi-step geospatial queries.</li></ol><p data-block-key=\"22evo\">To learn more, we invite you to read our full technical paper, \"<a href=\"http://goo.gle/earthai-techreport\" target=\"_blank\" rel=\"noopener noreferrer\">Google Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning</a>\". You can also get involved by <a href=\"https://forms.gle/1DPfcuys2AU63HgZ8\" target=\"_blank\" rel=\"noopener noreferrer\">expressing interest</a> as we expand access to these new capabilities for developers and enterprises.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-1a-Overview.width-1250.png\" alt=\"Flowchart of Earth AI\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-1a-Overview.width-1250.png\" alt=\"Flowchart of Earth AI\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"hz039\">Earth AI unites state-of-the-art models with geospatial reasoning agents to address critical global challenges.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Building blocks of Earth AI: State-of-the-art foundation models</h2>\n            \n        \n        \n    </div>\n\n\n\n    \n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Imagery</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"445ko\">Our new Remote Sensing Foundations models simplify and accelerate satellite imagery analysis using three core capabilities: vision-language models, open-vocabulary object detection, and adaptable vision backbones. Users can ask natural language queries, like <i>\"find all flooded roads\"</i> in an image captured after a storm, and get rapid, accurate answers. Our models are trained on a large corpus of high-resolution overhead imagery, paired with text descriptions. They achieve state-of-the-art results on multiple public Earth observation benchmarks. For instance, we achieve &gt;16% average improvement on text-based image search tasks, while our zero-shot model for novel object detection more than doubles the baseline accuracy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-2-RemoteSensingEvalFinal.width-1250.png\" alt=\"EarthAI-2-RemoteSensingEvalFinal\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-2-RemoteSensingEvalFinal.width-1250.png\" alt=\"EarthAI-2-RemoteSensingEvalFinal\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"1oe99\">Model evaluation shows significant Average Precision (AP50) improvement of our Remote-Sensing optimized RS-OWL-ViT-v2 model (“Ours”) over the OWL-ViT-v2 open vocabulary detection model in a zero-shot setting and illustrates the advantage of the combined FLAME + RS-OWL-ViT-v2 approach (\"Ours\") over SIoU for few-shot detection on novel classes.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Population</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"445ko\">This area of research, which includes <a href=\"https://research.google/blog/introducing-mobility-ai-advancing-urban-transportation/\">Mobility AI</a> and <a href=\"https://research.google/blog/insights-into-population-dynamics-a-foundation-model-for-geospatial-inference/\">Population Dynamics Foundations</a>, aims to understand the complex interplay between people and places. Our latest research in Population Dynamics Foundations introduces two key innovations: globally-consistent embeddings across 17 countries and monthly updated embeddings that capture the changing dynamics of human activity, which are critical for time-sensitive predictions. Population Dynamics Foundations has shown remarkable effectiveness in independent studies; for example, researchers at the <a href=\"https://www.ox.ac.uk/\" target=\"_blank\" rel=\"noopener noreferrer\">University of Oxford</a> found that incorporating these embeddings into a forecasting model for <a href=\"https://en.wikipedia.org/wiki/Dengue_fever\" target=\"_blank\" rel=\"noopener noreferrer\">Dengue fever</a> in Brazil improved long-range <a href=\"https://en.wikipedia.org/wiki/Coefficient_of_determination\" target=\"_blank\" rel=\"noopener noreferrer\">R²</a> (a metric that measures how well a model explains the actual disease rates) from 0.456 to 0.656 for 12-month predictions.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-3-PopDynEval.width-1250.png\" alt=\"Earth AI Population Dynamics plot\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-3-PopDynEval.width-1250.png\" alt=\"Earth AI Population Dynamics plot\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"1oe99\">Evaluation of our Population Dynamics Foundations across 17 countries; R2 score (range is 0–1, higher is better) by country for predicting population density, tree cover, night time lights, and elevation. The global trend matches the strong performance we <a href=\"https://research.google/blog/insights-into-population-dynamics-a-foundation-model-for-geospatial-inference/\">originally demonstrated</a> in the US only.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/EarthAI-4-PopDynEmbeddings.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"1oe99\">Similarity per dimension of the Population Dynamics Foundations embeddings, visualized by US zip code. The patterns across dimensions capture the diverse characteristics of the US population.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Environment</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"445ko\">Our previously-published research demonstrates state-of-the-art forecasts for <a href=\"https://deepmind.google/science/weathernext/\" target=\"_blank\" rel=\"noopener noreferrer\">medium-range weather</a>, <a href=\"https://blog.google/technology/research/indian-farmers-monsoon-prediction/\" target=\"_blank\" rel=\"noopener noreferrer\">monsoon onsets</a>, <a href=\"https://blog.google/products/maps/google-maps-apis-environment-sustainability/\" target=\"_blank\" rel=\"noopener noreferrer\">air quality</a> and <a href=\"https://blog.google/technology/ai/expanding-flood-forecasting-coverage-helping-partners/\" target=\"_blank\" rel=\"noopener noreferrer\">riverine floods</a>. We've recently expanded these Environment models to make <a href=\"https://arxiv.org/abs/2510.13050\" target=\"_blank\" rel=\"noopener noreferrer\">precipitation nowcasts for the entire planet</a>, and we’re now covering 2 billion people with forecasts for the most significant riverine floods.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Increased predictive power by combining models</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"445ko\">While each foundation model provides powerful insights, our findings confirm that combining models yields even more predictive power. This synergistic approach produces a more comprehensive and accurate understanding of real-world phenomena and dramatically improves predictions across critical applications.</p><p data-block-key=\"ca795\">For example, <a href=\"https://www.fema.gov/flood-maps/products-tools/national-risk-index\" target=\"_blank\" rel=\"noopener noreferrer\">FEMA’s National Risk Index</a> shows which communities are most at risk to natural hazards like floods and storms, based on a variety of factors including economic and social vulnerability as well as physical and environmental risk. By fusing embeddings that capture socio-economic features from our Population Dynamics Foundations and landscape features from <a href=\"https://deepmind.google/discover/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/\" target=\"_blank\" rel=\"noopener noreferrer\">AlphaEarth Foundations</a>, we improved prediction of FEMA’s National Risk Index by an average of 11% in <a href=\"https://en.wikipedia.org/wiki/Coefficient_of_determination\" target=\"_blank\" rel=\"noopener noreferrer\">R²</a> across 20 different hazards, versus using either data source alone, with the most significant gains in predicting risk from tornadoes (+25% R²) and riverine flooding (+17% R²).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Complex problem-solving via Geospatial Reasoning</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"445ko\">The example above illustrates that tackling real-world problems requires insights from multiple models with diverse capabilities. Orchestrating these Earth AI insights is simplified by our new Gemini-powered Geospatial Reasoning agent. The agent deconstructs complex, natural language queries and plans a dynamic, multi-step path to an answer. To execute each step, the agent can call on “expert” sub-agents that are equipped with Earth AI models described above, as well as the vast, real-world data found in <a href=\"https://datacommons.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Data Commons</a>, <a href=\"https://earthengine.google.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Earth Engine</a>, and geospatial-specific tools. This modular network of agents allows for extensibility and customization.</p><p data-block-key=\"1vriv\">To see how it works, consider a user who wishes to identify specific populations that are vulnerable to the risk of an oncoming storm. The agent executes a transparent series of reasoning steps:</p><ol><li data-block-key=\"dtr4d\">Invoke the Environment model to identify the specific geographic areas that are forecast to be at risk of hurricane force winds.</li><li data-block-key=\"30lur\">Query <a href=\"https://datacommons.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Data Commons</a> for demographic statistics to identify higher-population counties in the area of predicted landfall.</li><li data-block-key=\"iok3\">Retrieve official boundaries for the counties of interest from <a href=\"https://cloud.google.com/bigquery/public-data\" target=\"_blank\" rel=\"noopener noreferrer\">BigQuery’s public datasets</a>.</li><li data-block-key=\"5o23i\">Perform a spatial intersection between the wind zones and official county boundaries.</li><li data-block-key=\"4vp8\">Identify the most vulnerable postal codes by training a model on the fly using our Population Dynamics Foundations and county level statistics.</li><li data-block-key=\"2te22\">Use Remote Sensing Foundations object detection model to identify critical infrastructure in satellite imagery taken over one of the most vulnerable postal codes.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/EarthAI-5a-GeospatialDemo.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"prcis\">To assist a user in understanding vulnerability to an oncoming storm, our Gemini-powered Geospatial Reasoning agent uses our Environment model to identify the likely path of hurricane force winds, intersects this with country boundaries and population density from Big Query and Data Commons, and reasons across all of this data to pick the most critical locations. It also trains a model on the fly to generate higher resolution vulnerability data using Population Dynamics Foundations. And identifies critical infrastructure in satellite imagery using Remote Sensing Foundations.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"445ko\">To evaluate the agent, we developed two new methods for evaluation: a <i>Q&amp;A benchmark</i> for fact-finding and analysis with verifiable ground truth answers based on publicly available data and <i>Crisis Response</i> case studies for complex, predictive scenarios (e.g., solving the entire challenge above).</p><p data-block-key=\"1ffva\">On the Q&amp;A benchmark, our Geospatial Reasoning Agent achieved an overall accuracy of 0.82, significantly outperforming the baseline Gemini 2.5 Pro (0.50) and Gemini 2.5 Flash (0.39) agents (scores derived from <a href=\"https://en.wikipedia.org/wiki/ROUGE_(metric)\" target=\"_blank\" rel=\"noopener noreferrer\">ROUGE-L</a> F1 and percentage error, higher is better). This highlights the importance of giving agents access to specialized geospatial models and tools for these types of queries.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-7-QAPerformance.width-1250.png\" alt=\"Earth AI performance on QA\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-7-QAPerformance.width-1250.png\" alt=\"Earth AI performance on QA\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"1oe99\">Visualizing the performance of agents on the Q&amp;A benchmark. The Geospatial Reasoning agent outperformed the baseline Gemini 2.5 Pro agent by 37% in the Descriptive and Retrieval category, and 124% in the more complex Analytical and Relational category, for an overall 64% higher score (scores derived from ROUGE-L F1 and percentage error).</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"445ko\">In the more complex <i>Crisis Response</i> scenarios, our <a href=\"http://goo.gle/earthai-techreport\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a> demonstrates the benefit of orchestrating a diverse set of Environment, Remote Sensing and Population Dynamics insights via case studies. Leveraging specialized sub-agents for geospatial and demographic analysis, we’re able to solve real-world analysis tasks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Unlocking our planet's potential, together</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"445ko\">Earth AI represents a fundamental leap in planetary understanding. Our findings show that a multimodal, reasoning-based approach, built upon a foundation of state-of-the-art geospatial AI models, can unlock insights that are intractable with siloed analysis alone.</p><p data-block-key=\"2jlht\">We are just beginning to explore the full potential of Earth AI and are committed to expanding access in order to help the global community address the planet’s most pressing challenges. For example:</p><ul><li data-block-key=\"1a7l8\"><a href=\"https://x.company/projects/bellwether/\" target=\"_blank\" rel=\"noopener noreferrer\">Bellwether</a>, a Google X moonshot, is using our weather forecasts, Population Dynamics Foundations embeddings, satellite image analysis and property databases to predict building damage before a storm strikes, helping their insurance clients pay claims faster so homeowners can start rebuilding sooner — saving them time, money and stress.</li><li data-block-key=\"25e3m\">United Nations Global Pulse uses Earth AI Imagery models to <a href=\"https://www.unglobalpulse.org/ai-from-google-research-and-un-boosts-humanitarian-disaster-response-wider-coverage-faster-damage-assessments/\" target=\"_blank\" rel=\"noopener noreferrer\">assess damage after natural disasters</a>, enabling governments and international organizations to rapidly respond to crises.</li><li data-block-key=\"bvpp3\"><a href=\"https://www.givedirectly.org/\" target=\"_blank\" rel=\"noopener noreferrer\">GiveDirectly</a> is using Geospatial Reasoning with our flood forecasts to identify at-risk communities and <a href=\"https://www.givedirectly.org/flood-forecast-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">send cash aid</a> to help households prepare for and mitigate disaster.</li></ul><p data-block-key=\"36bsf\">In addition to supporting UN Global Pulse, GiveDirectly, and other organizations using Earth AI, <a href=\"http://google.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Google.org</a> is providing funds to partners like <a href=\"https://www.khushibaby.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Khushi Baby</a>, <a href=\"https://coopersmith.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Cooper/Smith</a>, <a href=\"https://www.directrelief.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Direct Relief</a> and <a href=\"http://froncort.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Froncort.ai</a> who are utilizing Population Dynamics Foundations to model infectious diseases and improve public health action globally. New enterprise users of Earth AI include <a href=\"https://www.publicstorage.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Public Storage</a>, <a href=\"https://carto.com/\" target=\"_blank\" rel=\"noopener noreferrer\">CARTO</a> and <a href=\"https://visionaespacial.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Visiona Space Technology</a> (part of <a href=\"https://www.embraer.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Embraer</a>).</p><p data-block-key=\"e60f0\">We want to hear how Earth AI might be helpful to you. We encourage organizations to <a href=\"https://forms.gle/VmdqBHrMah6g9z948\" target=\"_blank\" rel=\"noopener noreferrer\">express interest</a> in getting early access to Remote Sensing Foundations (available as <a href=\"https://console.cloud.google.com/vertex-ai/model-garden/google/earth-ai\" target=\"_blank\" rel=\"noopener noreferrer\">Imagery models in Vertex AI</a>), Population Dynamics Foundations, and Geospatial Reasoning.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    }
  ],
  "lastUpdated": "2025-11-13T10:34:18.412Z"
}