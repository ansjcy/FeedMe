{
  "sourceUrl": "https://rsshub.rssforever.com/google/research",
  "title": "Google Research Blog",
  "description": "Google Research Blog - Powered by RSSHub",
  "link": "https://research.google/blog",
  "items": [
    {
      "title": "解决虚拟机难题：AI 如何优化云计算 (原标题: Solving virtual machine puzzles: How AI is optimizing cloud computing)",
      "link": "https://research.google/blog/solving-virtual-machine-puzzles-how-ai-is-optimizing-cloud-computing/",
      "pubDate": "Thu, 16 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-16T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 解决虚拟机难题：AI 如何优化云计算\n\n云计算数据中心面临着一个持续的挑战：如何高效地分配处理任务（即虚拟机，VM）。这类似于一个俄罗斯方块游戏，但“方块”的生命周期未知且变化迅速。低效的VM分配会导致“资源搁浅”（浪费容量）和“空闲主机”不足（系统更新和大型VM部署所需）。传统的AI方法通过预测VM生命周期来解决这一经典的装箱问题，但单次预测的错误可能导致效率下降。\n\n### LAVA系统：基于学习分布和适应误预测的生命周期感知VM分配\n\n为了克服这些挑战，研究人员引入了LAVA系统，该系统包含三个核心算法：\n\n*   **非侵入式生命周期感知调度（NILAS）**\n*   **生命周期感知VM分配（LAVA）**\n*   **生命周期感知重新调度（LARS）**\n\n该系统的核心是“持续再预测”机制，它不依赖于VM创建时的一次性生命周期预测，而是持续自动更新VM的预期剩余生命周期预测。\n\n### VM的秘密生命：再预测与概率分布\n\n研究发现，VM的生命周期通常是不可预测的，并遵循长尾分布。例如，绝大多数VM（88%）的生命周期不到一小时，但它们仅消耗总资源的极小部分（2%）。相反，少数长生命周期的VM对整体资源效率影响巨大。\n\n![VM数量和计算份额的相对贡献图](https://storage.googleapis.com/gweb-research2023-media/images/LAVA-2-Contributions.width-1250.png)\n\n*VM生命周期分布（左）与资源消耗（右）。最短作业（0-10分钟，深蓝色）占数量的53%，但资源消耗可忽略不计。相比之下，运行时间最长的作业（>30天，橙色）占资源消耗的18%，但数量占比可忽略不计。*\n\n为了应对这种不确定性，LAVA系统设计了一个机器学习模型，该模型预测VM生命周期的概率分布，而非单一平均值。这种方法受生存分析启发，能够捕捉VM行为的内在不确定性。更重要的是，系统利用此分布持续更新预测。例如，一个VM运行了五天后，系统会重新评估其预期剩余生命周期，随着VM运行时间的增加，预测准确性也随之提高。\n\n![VM生命周期分布图](https://storage.googleapis.com/gweb-research2023-media/images/LAVA-1-Lifetimes.width-1250.png)\n\n*VM生命周期分布。VM调度时，预期（平均）生命周期为0.2天。运行1天后，预期剩余生命周期为4天。运行7天后，预期剩余生命周期为10天。*\n\n### 新型调度算法\n\n基于这种更强大的预测模型，研究人员开发了三种新颖的算法来改进VM分配：\n\n1.  **非侵入式生命周期感知调度（NILAS）**：\n    *   将生命周期预测整合到现有评分函数中，通过优先选择所有VM预计在相似时间退出的主机，旨在创建更多空闲机器。\n    *   由于使用再预测，对初始预测准确性不那么敏感，能够纠正错误。\n    *   已部署在Google的大规模集群管理器Borg上，显著改善了VM分配。\n\n2.  **生命周期感知VM分配（LAVA）**：\n    *   将短生命周期的VM放置在带有一个或多个长生命周期VM的主机上，以填补资源空白。\n    *   目标是让短生命周期的VM迅速退出，而不延长主机的整体生命周期。\n    *   通过在VM超出预期截止日期时增加主机的预期生命周期来主动适应误预测。\n    *   模拟结果表明，该策略能最大限度地减少碎片化并确保主机最终被释放。\n\n3.  **生命周期感知重新调度（LARS）**：\n    *   在碎片整理和维护期间，根据预测的剩余生命周期对主机上的VM进行排序，并优先迁移生命周期最长的VM。\n    *   短生命周期的VM在迁移前自然退出。\n    *   模拟结果显示，LARS有望将所需的总迁移次数减少约4.5%。\n\n### 解决大规模部署的挑战\n\n开发强大的模型和算法只是解决方案的一部分。在大规模环境中可靠地运行它们需要重新思考模型部署方法：\n\n*   **避免循环依赖**：传统的将机器学习模型部署在专用推理服务器上的做法会创建循环依赖，因为这些服务器本身运行在集群调度系统上。解决方案是将模型直接编译到Borg调度器二进制文件中，消除了循环依赖，并确保了模型以与调度器其他代码更改相同的严格流程进行测试和部署。这还带来了额外的好处：模型的中位延迟仅为9微秒（µs），比使用独立模型服务器的方法快780倍，这对于频繁的再预测和性能敏感任务至关重要。\n*   **解决预测瓶颈**：对于最大的区域，所需的预测数量可能成为瓶颈。通过引入主机生命周期评分缓存来解决，该缓存仅在VM添加到或从主机移除时，或当主机的预期生命周期到期时才更新预测。这种缓存机制确保了高性能，并允许系统在整个集群中部署。\n\n### 成果\n\nNILAS算法自2024年初以来已在Google的生产数据中心运行，取得了显著成果：\n\n*   **空闲主机增加**：生产试点和全集群推广显示，空闲主机增加了2.3-9.2个百分点（pp）。这直接与效率相关，因为1个百分点的改进通常相当于节省集群容量的1%。\n*   **资源搁浅减少**：在一些试点实验中，NILAS将CPU搁浅减少了约3%，内存搁浅减少了2%。这意味着主机更多的资源可供新VM使用。\n*   **LAVA模拟**：表明它将在NILAS的基础上进一步提高约0.4个百分点。\n*   **LARS模拟**：表明它有可能将维护所需的VM实时迁移次数减少4.5%。\n\n### 结论\n\n这项工作是迈向机器学习系统日益优化数据中心管理未来的基础性一步。所开发的技术，特别是再预测的使用以及模型和系统的协同设计，具有普适性。研究表明，在不牺牲可靠性或延迟的情况下，将先进的机器学习技术集成到系统基础设施堆栈的最底层是可行的，同时还能实现显著的效率提升。",
      "shortSummary": "Google开发了LAVA系统，利用AI优化云计算中的虚拟机（VM）分配。该系统通过“持续再预测”技术，动态更新VM生命周期预测，解决了传统方法中单次预测不准确的问题。LAVA包含NILAS、LAVA和LARS三种算法，旨在提高资源利用率、减少资源搁浅并优化维护。NILAS已在Google生产环境部署，显著增加了空闲主机并减少了资源搁浅。通过将模型直接编译到调度器中，解决了大规模部署的挑战，确保了低延迟和高可靠性。这项工作为ML驱动的数据中心管理奠定了基础。",
      "translated_title": "解决虚拟机难题：AI 如何优化云计算",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/LAVA-2-Contributions.width-1250.png",
          "alt": "Graph of relative contributions by number and compute fraction.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/LAVA-1-Lifetimes.width-1250.png",
          "alt": "Plot showing distribution of VM lifetimes",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"pt8nw\">Imagine a puzzle game similar to Tetris with pieces rapidly falling onto a stack. Some fit perfectly. Others don’t. The goal is to pack the blocks as tightly and efficiently as possible. This game is a loose analogy to the challenge faced by cloud data centers several times every second as they try to allocate processing jobs (called virtual machines or VMs) as efficiently as possible. But in this case, the “pieces” (or VMs) appear and disappear, some with a lifespan of only minutes, and others, days. In spite of the initially unknown VM lifespans, we still want to fill as much of the physical servers as possible with these VMs for the sake of efficiency. If only we knew the approximate lifespan of a job, we could clearly allocate much better.</p><p data-block-key=\"483c4\">At the scale of large data centers, efficient resource use is especially critical for both economic and environmental reasons. Poor VM allocation can lead to \"resource stranding\", where a server's remaining resources are too small or unbalanced to host new VMs, effectively wasting capacity. Poor VM allocation also reduces the number of \"empty hosts\", which are essential for tasks like system updates and provisioning large, resource-intensive VMs.</p><p data-block-key=\"3s50u\">This classic <a href=\"https://en.wikipedia.org/wiki/Bin_packing_problem\" target=\"_blank\" rel=\"noopener noreferrer\">bin packing problem</a> is made more complex by this incomplete information about VM behavior. AI can help with this problem by using learned models to predict VM lifetimes. However, this often relies on a single prediction at the VM's creation. The challenge with this approach is that a single misprediction can tie up an entire host for an extended period, degrading efficiency.</p><p data-block-key=\"8v1nd\">In “<a href=\"https://arxiv.org/abs/2412.09840v1\" target=\"_blank\" rel=\"noopener noreferrer\">LAVA: Lifetime-Aware VM Allocation with Learned Distributions and Adaptation to Mispredictions</a>”, we introduce a trio of algorithms — non-invasive lifetime aware scoring (NILAS), lifetime-aware VM allocation (LAVA), and lifetime-aware rescheduling (LARS) — which are designed to solve the bin packing problem of efficiently fitting VMs onto physical servers. This system uses a process we call “continuous reprediction”, which means it doesn’t rely on the initial, one-time guess of a VM’s lifespan made at its creation. Instead, the model constantly and automatically updates its prediction for a VM's expected remaining lifetime as the VM continues to run.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The secret life of VMs: Repredictions and probability distributions</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">One of the key insights driving this research is the recognition that VM lifetimes are often unpredictable and follow a <a href=\"https://en.wikipedia.org/wiki/Long_tail\" target=\"_blank\" rel=\"noopener noreferrer\">long-tailed distribution</a>. For example, while the vast majority of VMs (88%) live for less than an hour, these short-lived VMs consume only a tiny fraction (2%) of the total resources. This means that the placement of a small number of long-lived VMs has a disproportionately large impact on overall resource efficiency.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/LAVA-2-Contributions.width-1250.png\" alt=\"Graph of relative contributions by number and compute fraction.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/LAVA-2-Contributions.width-1250.png\" alt=\"Graph of relative contributions by number and compute fraction.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"b4xcm\"><i>Distribution of VM lifetimes of scheduled VMs (</i><b><i>left</i></b><i>) vs. their resource consumption (</i><b><i>right</i></b><i>). Interestingly, the shortest jobs (0–10 min, dark blue), which account for 53% by number, take a negligible fraction of resources. In contrast, the longest running jobs (&gt;30 days, orange), which take considerable resources (18%), amount to a negligible fraction by number.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"pt8nw\">Instead of trying to predict a single, average lifetime, which can be misleading for VMs with bi-modal or highly varied lifespans, we designed an ML model that predicts a probability distribution for a VM's lifetime. This approach, inspired by <a href=\"https://en.wikipedia.org/wiki/Survival_analysis\" target=\"_blank\" rel=\"noopener noreferrer\">survival analysis</a>, allows the model to capture the inherent uncertainty of a VM's behavior.</p><p data-block-key=\"91kfi\">More importantly, our system uses this distribution to continuously update its predictions. We ask, “Given a VM has been running for five days, what is its expected remaining lifetime?” As a VM continues to run, the system gains more information, and its lifetime prediction becomes more accurate. Our algorithms are specifically co-designed to leverage these repredictions, actively responding to mispredictions and improving the accuracy over time.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/LAVA-1-Lifetimes.width-1250.png\" alt=\"Plot showing distribution of VM lifetimes\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/LAVA-1-Lifetimes.width-1250.png\" alt=\"Plot showing distribution of VM lifetimes\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"b4xcm\"><i>Lifetime distribution of VM lifetimes. When the VM is scheduled, the expected (average) lifetime is 0.2 days. After it has run for 1 day, the expected remaining lifetime is 4 days. After 7 days, the expected remaining lifetime is 10 days.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">A new class of scheduling algorithms</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">With this new, more robust prediction model, we developed three novel algorithms to improve VM allocation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">1. Non-Invasive Lifetime Aware Scheduling (NILAS)</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">NILAS is a non-invasive algorithm that incorporates lifetime predictions into an existing scoring function. It ranks potential hosts for a new VM by considering the repredicted exit times of all existing VMs on that host. By prioritizing hosts where all VMs are expected to exit at a similar time, NILAS aims to create more empty machines. Our use of repredictions is less sensitive to prediction accuracy and allows NILAS to correct for errors. The NILAS algorithm has been deployed on our <a href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43438.pdf\">large-scale cluster manager</a>, Borg, where it significantly improves VM allocation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">2. Lifetime-Aware VM Allocation (LAVA)</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">LAVA is a more fundamental departure from existing scheduling mechanisms. While NILAS aims to pack VMs with similar lifetimes, LAVA does the opposite: it puts shorter-lived VMs on hosts with one or more long-lived VMs. The goal is to fill in resource gaps with short-lived VMs that are at least an order of magnitude shorter than the host’s anticipated lifespan, so that they exit quickly without extending the host’s overall lifespan. LAVA also actively adapts to mispredictions by increasing a host’s anticipated lifespan if a VM outlives its expected deadline. Simulations show that this strategy minimizes fragmentation and ensures that hosts are eventually freed up.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">3. Lifetime-Aware Rescheduling (LARS)</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">LARS uses our lifetime predictions to minimize VM disruptions during defragmentation and maintenance. When a host needs to be defragmented, LARS sorts the VMs on that host by their predicted remaining lifetime and migrates the longest-lived VMs first. Shorter-lived VMs exit naturally before migration. Simulations with LARS indicate it has the potential to reduce the total number of migrations required by around 4.5%.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Addressing the challenge of deployment at scale</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">Developing powerful models and algorithms is only one part of the solution. Getting them to work reliably at large scale required us to rethink our approach to model deployment.</p><p data-block-key=\"cj0j9\">A common practice is to serve machine learning models on dedicated inference servers. However, this would have created a <a href=\"https://en.wikipedia.org/wiki/Circular_dependency\" target=\"_blank\" rel=\"noopener noreferrer\">circular dependency</a>, as these servers would themselves run on our cluster scheduling system. A failure in the model serving layer could then cause a cascading failure in the scheduler itself, which is unacceptable for a mission-critical system.</p><p data-block-key=\"34gvd\">Our solution was to compile the model directly into the <a href=\"https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/\">Borg scheduler binary</a>. This approach eliminated the circular dependency and ensured that the model was tested and rolled out with the same rigorous process as any other code change to the scheduler. This also yielded an additional benefit: the model's median latency is just 9 microseconds (µs), which is 780 times faster than a comparable approach that uses separate model servers. This low latency is crucial for running repredictions frequently and for using the model in performance-sensitive tasks, like maintenance and defragmentation.</p><p data-block-key=\"dpkp\">We also found that for our largest zones, the number of required predictions could become a bottleneck. We addressed this by introducing a host lifetime score cache, which only updates predictions when a VM is added or removed from a host, or when a host's expected lifetime expires. This caching mechanism ensures high performance and allows us to deploy our system fleet-wide.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">Our NILAS algorithm has been running in Google's production data centers since early 2024. The results are clear and significant.</p><ul><li data-block-key=\"27kt6\"><i>Increased empty hosts:</i> Our production pilots and fleet-wide rollouts have shown an increase in empty hosts by 2.3–9.2 percentage points (pp). This metric directly correlates with efficiency, as a 1 pp improvement is typically equivalent to saving 1% of a cluster's capacity.</li><li data-block-key=\"arb0l\"><i>Reduced resource stranding:</i> In some pilot experiments, NILAS reduced CPU stranding by approximately 3% and memory stranding by 2%. This means more of a host's resources are available to be used by new VMs.</li></ul><p data-block-key=\"7qme9\">Simulations running LAVA suggest it will provide a further ~0.4 pp improvement over NILAS. Similarly, simulations with LARS indicate that it has the potential to reduce the number of VM live migrations needed for maintenance by 4.5%.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">We believe this work is a foundational step towards a future where data center management is increasingly optimized by machine learning systems. The techniques we developed, particularly the use of repredictions and the co-design of models and systems, are generalizable to other tasks. We have demonstrated that it is possible to integrate advanced machine learning techniques into the lowest layers of a system’s infrastructure stack without sacrificing reliability or latency, while still delivering significant efficiency gains.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\"><i>LAVA is a large collaborative project that spanned multiple teams across Google, including Google Cloud, Google DeepMind, Google Research, and SystemsResearch@Google. Key contributors include Jianheng Ling, Pratik Worah, Yawen Wang, Yunchuan Kong, Anshul Kapoor, Chunlei Wang, Clifford Stein, Diwakar Gupta, Jason Behmer, Logan A. Bush, Prakash Ramanan, Rajesh Kumar, Thomas Chestna, Yajing Liu, Ying Liu, Ye Zhao, Kathryn S. McKinley, Meeyoung Park, and Martin Maas.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "使用AI通过DeepSomatic识别肿瘤中的基因变异 (原标题: Using AI to identify genetic variants in tumors with DeepSomatic)",
      "link": "https://research.google/blog/using-ai-to-identify-genetic-variants-in-tumors-with-deepsomatic/",
      "pubDate": "Wed, 15 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-15T16:00:00.000Z",
      "creator": "Google",
      "summary": "DeepSomatic是Google Research与加州大学圣克鲁斯基因组学研究所等合作开发的一款人工智能工具，旨在更准确地识别肿瘤细胞中的基因变异。这项工作已发表在《自然生物技术》杂志上，是Google利用AI方法理解和治疗癌症的广泛努力的一部分，目标是加速癌症研究并推进精准医疗。\n\n### 癌症与基因变异的挑战\n*   **癌症的本质**：癌症是一种基因疾病，细胞分裂的遗传控制出现异常。识别肿瘤细胞中的基因突变是研究癌症和制定治疗方案的关键步骤。\n*   **体细胞变异的复杂性**：与DeepVariant（Google早期用于识别遗传性生殖系变异的工具）不同，DeepSomatic专注于识别癌症中更复杂的体细胞变异。这些变异是在出生后获得的，可能由环境暴露或DNA复制错误引起。\n*   **识别难度**：识别体细胞变异比识别遗传变异更困难，因为肿瘤细胞可能包含不同频率的多种变异，且测序错误率可能高于样本中体细胞变异的实际频率。\n\n### DeepSomatic的工作原理与训练\n*   **核心技术**：DeepSomatic是一个灵活的模型，利用卷积神经网络（CNN）来识别肿瘤变异。它适用于所有主要的测序平台，支持不同类型的样本处理，并能将其学习能力扩展到未包含在训练中的癌症类型。\n*   **数据处理流程**：\n    1.  将基因测序数据（来自肿瘤细胞和非癌细胞）转换为一组图像，这些图像代表测序数据、染色体比对、输出质量及其他变量。\n    2.  DeepSomatic的卷积神经网络处理这些图像。\n    3.  区分参考基因组、个体中的非癌生殖系变异以及肿瘤中由癌症引起的体细胞变异，同时排除测序过程中产生的微小错误。\n    4.  最终输出一份癌症相关变异（突变）列表。\n\n    ![DeepSomatic概览](https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-1-Overview.width-1250.png)\n    *DeepSomatic检测基因组数据中的癌症变异。首先，将肿瘤细胞和非癌细胞的测序数据转换为图像。DeepSomatic通过其卷积神经网络处理这些图像，以区分参考基因组、个体中的非癌生殖系变异以及肿瘤中由癌症引起的体细胞变异，同时排除微小的测序错误。结果是癌症引起的变异或突变列表。*\n\n*   **训练数据（CASTLE）**：\n    *   为训练准确模型，研究团队与UC Santa Cruz和美国国家癌症研究所合作，创建了一个新的高质量训练和评估数据集。\n    *   对来自四个乳腺癌样本和两个肺癌样本（研究细胞系）的肿瘤细胞和伴随的正常细胞进行了测序。\n    *   使用Illumina短读长测序、PacBio长读长测序和Oxford Nanopore Technology长读长测序三种领先平台进行全基因组测序。\n    *   结合所有三个平台的输出，以消除平台特异性错误，创建了一个名为“癌症标准长读长评估数据集”（CASTLE）的单一、准确的参考数据集。\n\n    ![突变率图](https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-2-Variants.width-1250.png)\n    *用于训练DeepSomatic的基准数据集。每个条形图显示了在四个乳腺癌样本和两个肺癌样本中发现的突变数量，颜色代表不同类型的突变。肺癌显示出由环境毒素引起的一种显著突变类型，包括图中绿色的SBS4。但即使是同一种癌症，其突变特征也显示出巨大差异。这些个体差异可以预测其对治疗的反应效果。*\n\n*   **“仅肿瘤”模式**：DeepSomatic还能够在没有非肿瘤序列可用的“仅肿瘤”模式下识别体细胞变异，例如在白血病等血癌中，很难从血液样本中获取纯净的正常细胞。\n\n### DeepSomatic的性能表现\n*   **高准确性**：DeepSomatic模型在所有三个主要测序平台上均优于其他现有方法，以更高的准确性识别出更多的肿瘤变异。\n*   **擅长识别插入和缺失（Indels）**：\n    *   在Illumina测序数据上，DeepSomatic在识别Indels方面的F1-score达到90%，而次优方法为80%。\n    *   在Pacific Biosciences测序数据上，DeepSomatic的F1-score超过80%，而次优方法不到50%。\n\n    ![乳腺癌准确性图](https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-3-BreastCancer.width-1250.png)\n    *DeepSomatic（紫色）在研究中广泛使用的乳腺癌样本上的结果，与其他工具进行比较。有几种软件工具可以识别Illumina数据中的癌症变异，而对于PacBio和Oxford Nanopore Technologies生成的长读长测序数据，只有一种替代方案（粉色）。F1-score衡量发现的变异数量和准确性。DeepSomatic在识别单字母基因代码变异（单核苷酸变异）方面表现略好，并在涉及Indels的变异方面显示出显著改进。*\n\n*   **处理复杂样本的能力**：\n    *   在福尔马林固定石蜡包埋（FFPE）样本（一种常见的组织保存方法，会引入DNA损伤）上，DeepSomatic表现出色。\n    *   在全外显子组测序（WES）数据（一种更经济的方法，仅关注基因组中编码蛋白质的约1%）上，DeepSomatic也优于其他工具。\n    *   这表明DeepSomatic可用于分析质量较低或历史肿瘤样本，并适用于仅进行外显子组测序的临床数据。\n\n    ![FFPE和WES准确性图](https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-4-FFPE-WES.width-1250.png)\n    *DeepSomatic在经过更复杂预处理步骤的样本上具有显著更高的准确性，包括：福尔马林固定石蜡包埋（FFPE），一种用于保存组织样本的方法（左），以及全外显子组测序（WES），一种仅对基因组中编码蛋白质的部分进行测序的方法（右）。中间部分显示了一个经过FFPE保存并使用全外显子组测序的样本。*\n\n### 推广到其他癌症类型\n*   **胶质母细胞瘤**：DeepSomatic成功识别了这种侵袭性脑癌的变异，展示了其学习泛化能力。\n*   **儿童白血病**：与堪萨斯城儿童慈善医院合作，DeepSomatic分析了八个已测序的儿童白血病样本，不仅识别了已知的变异，还发现了10个新变异，证明了其在“仅肿瘤”样本上的有效性。\n\n### 未来展望\n*   研究实验室和临床医生有望开始使用DeepSomatic工具。\n*   检测已知癌症变异有助于选择现有治疗方案（如化疗、免疫疗法）。\n*   识别新癌症变异可能促成全新的疗法。\n*   最终目标是更深入地了解每个肿瘤，发现其驱动因素，并为患者提供最有效的治疗。",
      "shortSummary": "DeepSomatic是Google Research推出的一款AI工具，利用卷积神经网络更准确地识别肿瘤中的体细胞基因变异。它能处理多种测序平台数据，并支持肿瘤-正常样本配对及仅肿瘤样本模式。DeepSomatic在识别插入和缺失（Indels）方面表现优异，并能有效分析FFPE和WES等复杂样本。该工具已开源，旨在加速癌症研究，推动精准医疗，帮助临床医生选择现有疗法并发现潜在新疗法。",
      "translated_title": "使用AI通过DeepSomatic识别肿瘤中的基因变异",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-1-Overview.width-1250.png",
          "alt": "Overview of DeepSomatic",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-2-Variants.width-1250.png",
          "alt": "Plot of mutation rates",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-3-BreastCancer.width-1250.png",
          "alt": "Plot of accuracy on breast cancer",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-4-FFPE-WES.width-1250.png",
          "alt": "Plot of accuracy on FFPE & WES",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ry233\">Cancer is fundamentally a genetic disease in which the genetic controls on cell division go awry. Many types of cancer exist, and each poses unique challenges as it can have distinct genetic underpinnings. A powerful way to study cancer, and a critical step toward creating a treatment plan, is to identify the genetic mutations in tumor cells. Indeed, clinicians will now often sequence the genomes of biopsied tumor cells to inform treatment plans that specifically disrupt how that cancer grows.</p><p data-block-key=\"ftn7v\">With partners at the University of California, Santa Cruz <a href=\"https://genomics.ucsc.edu/\" target=\"_blank\" rel=\"noopener noreferrer\">Genomics Institute</a> and other federal and academic researchers, our new paper, “<a href=\"https://www.nature.com/articles/s41587-025-02839-x\" target=\"_blank\" rel=\"noopener noreferrer\">DeepSomatic: Accurate somatic small variant discovery for multiple sequencing technologies</a>” in <a href=\"https://www.nature.com/nbt/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature Biotechnology</i></a> presents a tool that leverages machine learning to identify genetic variants in tumor cells more accurately than current methods. DeepSomatic is a flexible model that uses convolutional neural networks to identify tumor variants. It works on data from all major sequencing platforms, for different types of sample processing, and can extend its learning to cancer types not included in training.</p><p data-block-key=\"8b6u7\">We have made both <a href=\"https://github.com/google/deepsomatic\" target=\"_blank\" rel=\"noopener noreferrer\">the tool</a> and the <a href=\"https://github.com/CASTLE-Panel/castle\" target=\"_blank\" rel=\"noopener noreferrer\">high-quality training dataset</a> we created openly available to the research community. This work is part of broader Google efforts to develop AI methods to understand cancer and help scientists treat cancer, including analyzing mammogram images for <a href=\"https://health.google/mammography/\" target=\"_blank\" rel=\"noopener noreferrer\">breast cancer screening</a>, CT scans for <a href=\"https://research.google/blog/computer-aided-diagnosis-for-lung-cancer-screening/\">lung cancer screening</a>, as well as a partnership aimed at using AI to <a href=\"https://blog.google/technology/health/google-ai-institute-womens-cancers/\" target=\"_blank\" rel=\"noopener noreferrer\">advance research on gynecological cancers</a>. Our hope is to speed cancer research and further the goal of precision medicine.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Genetic variation acquired after birth</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\">Genome sequencing is used in research and medical clinics to identify genetic variations between an individual and the <a href=\"https://www.genome.gov/genetics-glossary/Human-Genome-Reference-Sequence\" target=\"_blank\" rel=\"noopener noreferrer\">human reference genome</a>. Distinguishing between real variants and simple errors made during the sequencing process is challenging. That’s why almost a decade ago Google Research introduced <a href=\"https://research.google/blog/deepvariant-highly-accurate-genomes-with-deep-neural-networks/\">DeepVariant</a> to identify inherited variants, also called <a href=\"https://www.genomicseducation.hee.nhs.uk/genotes/knowledge-hub/constitutional-germline-vs-somatic-tumour-variants/\" target=\"_blank\" rel=\"noopener noreferrer\">germline variants</a>, that came from parents and are found in all of the body’s cells.</p><p data-block-key=\"c63af\">The genetics of cancer is more complex. Cancer is often driven by variants acquired after birth. Environmental exposure that damages DNA, such as UV light or chemical carcinogens, as well as random errors that occur during DNA replication, can cause cells in the body, known as somatic cells, to acquire new variants. Sometimes, these acquired variants change the normal behavior of cells, and can cause them to replicate when they shouldn’t. This process drives the initial development of cancer, as well as its later progression to more fast-growing and invasive stages.</p><p data-block-key=\"1tabm\">Identifying variants specific to some of a person’s somatic cells is much harder than identifying inherited variants. Tumor cells can contain a diverse set of acquired variants at different frequencies, and the error rate of sequencing can be higher than the rate a somatic variant is present in a sample.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Training DeepSomatic to spot genetic variation in tumor cells</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\">We developed <a href=\"https://github.com/google/deepsomatic\" target=\"_blank\" rel=\"noopener noreferrer\">DeepSomatic</a> to address these challenges and accurately identify somatic variants. In most clinical and research settings, cancer is studied by sequencing the tumor cells acquired through biopsy, as well as normal cells that are unaffected by the tumor growth and contain more typical inherited genetic variations. DeepSomatic is trained to identify variations observed in tumor cells that are not inherited variants. These types of variations can provide critical insights about which variations are driving the tumor growth. DeepSomatic is also able to identify somatic variation in tumor-only mode where a non-tumor sequence is not available, for example in a blood cancer like leukemia where it is hard to get only normal cells from a blood draw. The ability to extend to different types of use-cases that follow common ways clinicians and researchers study cancer makes DeepSomatic applicable to many research and clinical settings.</p><p data-block-key=\"ae6vl\">Like our earlier tool, <a href=\"https://research.google/blog/deepvariant-highly-accurate-genomes-with-deep-neural-networks/\">DeepVariant</a>, the DeepSomatic model works by first turning genetic sequencing data into a <a href=\"https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/\" target=\"_blank\" rel=\"noopener noreferrer\">set of images</a>. The images represent the sequencing data, alignment along the chromosome, the quality of the output, and other variables. DeepSomatic then uses its <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\" target=\"_blank\" rel=\"noopener noreferrer\">convolutional neural network</a> on data from tumor cells and non-cancerous cells to differentiate between the reference genome, the non-cancer germline variants in that individual, and the cancer-caused somatic variants in the tumor, while discarding variations caused by small errors acquired during the sequencing process. The result is a list of cancer-related variants, or mutations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-1-Overview.width-1250.png\" alt=\"Overview of DeepSomatic\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-1-Overview.width-1250.png\" alt=\"Overview of DeepSomatic\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"48u3i\">DeepSomatic detects cancer variants in genomic data. First, sequencing data from the tumor cells and non-cancerous cells are turned into an image. DeepSomatic passes these images through its convolutional neural network to differentiate between the reference genome, the non-cancer germline variants in that individual, and the cancer-caused somatic variants in the tumor, while discarding variations caused by small sequencing errors. The result is a list of cancer-caused variants, or mutations.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ry233\">Training accurate models that can identify genetic variation for different cancer types requires comprehensive, high-quality data and truth sets. For this work we created a new training and evaluation dataset for detecting variants in tumor cells. With our partners at UC Santa Cruz and the <a href=\"https://www.cancer.gov/\" target=\"_blank\" rel=\"noopener noreferrer\">National Cancer Institute</a>, we sequenced tumor cells and accompanying normal cells from four breast cancer samples and two lung cancer samples from research cell lines.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-2-Variants.width-1250.png\" alt=\"Plot of mutation rates\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-2-Variants.width-1250.png\" alt=\"Plot of mutation rates\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"48u3i\">Benchmark dataset used to train DeepSomatic. Each bar shows the number of mutations found in four breast cancer samples and two lung cancer samples, with color representing different types of mutations. Lung cancer displays a notable type of mutation caused by environmental toxins, including <a href=\"https://signal.mutationalsignatures.com/explore/referenceCancerSignature/63/prevalence\" target=\"_blank\" rel=\"noopener noreferrer\">SBS4</a> shown in green. But even the same type of cancer shows big differences in its mutational signature. These individual differences can predict how well it will respond to a treatment.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ry233\">To create an accurate training dataset, we did whole-genome sequencing of these six samples using three leading platforms: <a href=\"https://www.illumina.com/systems/sequencing-platforms.html\" target=\"_blank\" rel=\"noopener noreferrer\">Illumina’s short-read sequencing</a>, <a href=\"https://www.pacb.com/technology/hifi-sequencing/\" target=\"_blank\" rel=\"noopener noreferrer\">PacBio’s long-read sequencing</a>, and <a href=\"https://nanoporetech.com/platform\" target=\"_blank\" rel=\"noopener noreferrer\">Oxford Nanopore Technology’s long-read sequencing</a>. Output from all three platforms was combined to remove platform-specific errors and create a single, accurate reference dataset we call the <a href=\"https://github.com/CASTLE-Panel/castle\" target=\"_blank\" rel=\"noopener noreferrer\">Cancer Standards Long-read Evaluation dataset</a> (CASTLE) for genetic diversity in tumor and normal cells.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Testing DeepSomatic’s ability to spot cancer-related variants</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\">We trained DeepSomatic on three of the breast cancer genomes and the two lung cancer genomes in the CASTLE reference dataset. We then tested DeepSomatic’s performance in several ways, including on the single breast cancer genome that was not included in its training data, and on chromosome 1 from each sample, which we also excluded from the training.</p><p data-block-key=\"1u39v\">Results show that DeepSomatic models developed for each of the three major sequencing platforms performed better than other methods, identifying more tumor variants with higher accuracy. The tools used for comparison on short-read sequencing data were <a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC4242521/\" target=\"_blank\" rel=\"noopener noreferrer\">SomaticSniper</a>, <a href=\"https://gatk.broadinstitute.org/hc/en-us/articles/360037593851-Mutect2\" target=\"_blank\" rel=\"noopener noreferrer\">MuTect2</a> and <a href=\"https://www.nature.com/articles/s41592-018-0051-x\" target=\"_blank\" rel=\"noopener noreferrer\">Strelka2</a> (with SomaticSniper specifically for single nucleotide variants, or SNVs). For long-read sequencing data we compared against <a href=\"https://www.biorxiv.org/content/10.1101/2023.08.17.553778v1\" target=\"_blank\" rel=\"noopener noreferrer\">ClairS</a>, a deep learning model trained on synthetic data.</p><p data-block-key=\"9ghlt\">In our tests DeepSomatic identified 329,011 somatic variants across the six reference cell lines and a seventh preserved sample. DeepSomatic does particularly well at identifying cancer variations that involve insertions and deletions (“Indels”) of genetic code. For these types of variants, DeepSomatic substantially increased the <a href=\"https://en.wikipedia.org/wiki/F-score\" target=\"_blank\" rel=\"noopener noreferrer\">F1-score</a>, a balanced measure of how well the model finds true variants in a sample (recall) while not making false positives (precision). On Illumina sequencing data the next-best method scored 80% at identifying Indels, while DeepSomatic scored 90%. On Pacific Biosciences sequencing data, the next-best method scored less than 50% at identifying Indels, and DeepSomatic scored more than 80%.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-3-BreastCancer.width-1250.png\" alt=\"Plot of accuracy on breast cancer\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-3-BreastCancer.width-1250.png\" alt=\"Plot of accuracy on breast cancer\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"48u3i\">DeepSomatic results (<b>purple</b>) for a breast cancer sample widely used in research, compared to other tools. Several software tools identify cancer variants in Illumina’s data, while only a single alternative (<b>pink</b>) exists for the long-read sequencing data generated by PacBio and Oxford Nanopore Technologies. The F1-score measures how many variants are discovered and with what accuracy. DeepSomatic performs slightly better for single-letter variations in genetic code, known as single nucleotide variations, and shows major improvements for variations that involve Indels.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ry233\">The seventh sample was one of the previously used research cell lines of a breast cancer tumor that was preserved using <a href=\"https://en.wikipedia.org/wiki/Fixation_(histology)\" target=\"_blank\" rel=\"noopener noreferrer\">formalin-fixed-paraffin-embedded</a> (FFPE). This common preservation method introduces additional patterns of DNA damage that can complicate genetic analysis. This sample was also sequenced using whole <a href=\"https://en.wikipedia.org/wiki/Exome_sequencing\" target=\"_blank\" rel=\"noopener noreferrer\">exome sequencing</a> (WES), a more affordable method that focuses only on the roughly 1% of the genome that codes for proteins. When DeepSomatic was trained on these types of sample data and then tested on chromosome 1, which was reserved from training, it again outperformed other tools, suggesting it can be used to identify variants in lower-quality or historic tumor samples, potentially rescuing samples that have been harder to sequence, and working on clinical data where only the exome was sequenced.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-4-FFPE-WES.width-1250.png\" alt=\"Plot of accuracy on FFPE &amp; WES\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-4-FFPE-WES.width-1250.png\" alt=\"Plot of accuracy on FFPE &amp; WES\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"48u3i\">DeepSomatic has notably higher accuracy on samples prepared with more complicated pre-processing steps involving: <a href=\"https://en.wikipedia.org/wiki/Fixation_(histology)\" target=\"_blank\" rel=\"noopener noreferrer\">fixed formalin paraffin embedded</a> (FFPE), a method used to preserve tissue samples (<b>left</b>), and whole <a href=\"https://en.wikipedia.org/wiki/Exome_sequencing\" target=\"_blank\" rel=\"noopener noreferrer\">exome sequencing</a> (WES), a method to sequence only the parts of the genome that code for proteins (<b>right</b>). The middle section shows a sample that was preserved with FFPE and also sequenced using whole exome sequencing.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Applying DeepSomatic to other cancers</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\">To test DeepSomatic’s performance on other types of cancers, we analyzed a single sample of <a href=\"https://www.mayoclinic.org/diseases-conditions/glioblastoma/symptoms-causes/syc-20569077\" target=\"_blank\" rel=\"noopener noreferrer\">glioblastoma</a>, an aggressive form of brain cancer that arises from a small number of variants. DeepSomatic was able to pinpoint those variants, showing that it can generalize its learning to apply it to a different cancer type.</p><p data-block-key=\"2ga4q\">We also worked with partners at <a href=\"https://www.childrensmercy.org/childrens-mercy-research-institute/about/\" target=\"_blank\" rel=\"noopener noreferrer\">Children’s Mercy</a> in Kansas City to analyze eight previously sequenced samples of <a href=\"https://www.childrensmercy.org/departments-and-clinics/division-of-pediatric-hematology-oncology-and-blood-and-marrow-transplantation/cancer-center/leukemia-and-lymphoma-program/understanding-leukemia/\" target=\"_blank\" rel=\"noopener noreferrer\">pediatric leukemia</a>, a cancer of the white blood cells that is the most common childhood cancer. Leukemia exists in the bloodstream, so a “normal” non-cancer blood sample is not possible. Despite that challenge, DeepSomatic identified the previously known variants as well as 10 new ones, showing that it can work with a tumor-only sample.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">What’s next</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\">Our hope is that research labs and clinicians can begin to use this tool. Detecting known cancer variants could help choose between existing treatments, such as chemotherapy, immunotherapy or other methods. Identifying new cancer variants could potentially lead to brand-new therapies. We hope people can take these tools and learn more about each cancer tumor, find what’s driving it, and ultimately deliver the most effective treatments to patients.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\"><i>We thank all research participants whose participation in research programs and donation of cell lines made this work and other biomedical research possible. We thank our collaborators at UC Santa Cruz Genomics Institute, the National Cancer Institute, the Frederick National Laboratory for Cancer Research, Children’s Mercy Hospital, and NYU. We thank Hannah Hickey for writing contributions. We thank Avinatan Hassidim, Katherine Chou, Lizzie Dorfman, and Yossi Matias for research leadership support. We thank Resham Parikh and Isha Mishra for communications support.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Coral NPU：面向边缘AI的全栈平台 (原标题: Coral NPU: A full-stack platform for Edge AI)",
      "link": "https://research.google/blog/coral-npu-a-full-stack-platform-for-edge-ai/",
      "pubDate": "Tue, 14 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-14T16:00:00.000Z",
      "creator": "Google",
      "summary": "# Coral NPU：面向边缘AI的全栈平台\n\n生成式AI极大地改变了我们对技术的期望，但未来的技术飞跃在于将AI智能直接嵌入到我们的个人环境中，使其能在我们穿戴和携带的设备上运行，实现真正的辅助性、私密且全天候的体验。然而，这带来了三大核心挑战：\n\n*   **性能差距**：复杂的机器学习模型需要大量计算，远超边缘设备的功耗、散热和内存限制。\n*   **碎片化成本**：为多样化的专有处理器编译和优化ML模型既困难又昂贵，阻碍了设备间性能的一致性。\n*   **用户信任赤字**：个人AI必须优先考虑个人数据的隐私和安全。\n\n为解决这些问题，我们推出了 **Coral NPU**，这是一个全栈平台，旨在为硬件设计者和ML开发者提供构建下一代私密、高效边缘AI设备所需的工具。Coral NPU与Google Research和Google DeepMind合作设计，是一种AI优先的硬件架构，旨在实现超低功耗、始终在线的边缘AI。它提供统一的开发体验，简化了环境感知等应用的部署，并专为在可穿戴设备上实现全天候AI而设计，同时最大限度地减少电池使用，并可配置以支持更高性能的用例。\n\n## Coral NPU：AI优先的架构\n\n为低功耗边缘设备进行开发的开发者面临一个基本权衡：选择通用CPU还是专用加速器。通用CPU提供灵活性和广泛的软件支持，但缺乏针对ML工作负载的领域特定架构，导致性能低下且功耗效率不高。相反，专用加速器提供高ML效率，但缺乏灵活性，难以编程，不适合通用任务。\n\nCoral NPU架构通过颠覆传统的芯片设计来直接解决这个问题。它优先考虑ML矩阵引擎而非标量计算，从芯片层面优化AI架构，创建了一个专为更高效的设备端推理而构建的平台。\n\n作为一套完整的参考神经网络处理单元（NPU）架构，Coral NPU为下一代节能、ML优化系统级芯片（SoC）提供了构建模块。该架构基于一套符合RISC-V ISA的架构IP块，旨在实现最小功耗，非常适合始终在线的环境感知。基础设计提供512 GOPS（每秒十亿次操作）的性能，同时仅消耗几毫瓦，从而为边缘设备、听戴设备、AR眼镜和智能手表提供强大的设备端AI能力。\n\n![Coral NPU 生态系统](https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-1.width-1250.png)\n*Coral NPU生态系统的统一视图，展示了面向SoC设计者和ML开发者的端到端堆栈。基于RISC-V的开放和可扩展架构使SoC设计者能够灵活修改基础设计，或将其用作预配置的NPU。*\n\nCoral NPU架构包括以下组件：\n\n*   **标量核心**：一个轻量级、C语言可编程的RISC-V前端，管理数据流到后端核心，采用简单的“运行至完成”模型，实现超低功耗和传统CPU功能。\n*   **向量执行单元**：一个强大的单指令多数据（SIMD）协处理器，符合RISC-V向量指令集（RVV）v1.0，能够同时对大型数据集进行操作。\n*   **矩阵执行单元**：一个高效的量化外积乘累加（MAC）引擎，专为加速基本神经网络操作而构建。（注：该矩阵执行单元仍在开发中，将于今年晚些时候在GitHub上发布。）\n\n![Coral NPU 架构转变](https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-2.width-1250.png)\n*可视化从传统设计到Coral NPU的架构转变。*\n\n## 统一的开发者体验\n\nCoral NPU架构是一个简单、C语言可编程的目标，可以与IREE和TFLM等现代编译器无缝集成。这使得它能够轻松支持TensorFlow、JAX和PyTorch等ML框架。\n\nCoral NPU包含一个全面的软件工具链，包括针对TensorFlow的TFLM编译器、通用MLIR编译器、C编译器、自定义内核和模拟器等专业解决方案。这为开发者提供了灵活的路径。例如，来自JAX等框架的模型首先使用StableHLO方言导入到MLIR格式。然后，这个中间文件被输入到IREE编译器，该编译器应用一个硬件特定的插件来识别Coral NPU的架构。之后，编译器执行渐进式降低——这是一个关键的优化步骤，代码通过一系列方言系统地转换，使其更接近机器的本地语言。优化后，工具链生成一个最终的、紧凑的二进制文件，准备在边缘设备上高效执行。这套行业标准的开发工具简化了ML模型的编程，并可以在各种硬件目标上提供一致的体验。\n\n![Coral NPU 编译器工具链](https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-3.width-1250.png)\n*Coral NPU编译器工具链，展示了从ML模型创建到优化、编译再到设备端部署的完整流程。*\n\nCoral NPU的协同设计过程侧重于两个关键领域：首先，该架构高效加速了当今设备端视觉和音频应用中领先的基于编码器的架构。其次，我们正与Gemma团队紧密合作，优化Coral NPU以支持小型Transformer模型，确保加速器架构支持下一代边缘生成式AI。这种双重关注意味着Coral NPU有望成为第一个开放、基于标准、低功耗的NPU，旨在将LLM带到可穿戴设备上。\n\n## 目标应用\n\nCoral NPU旨在实现超低功耗、始终在线的边缘AI应用，特别关注环境感知系统。其主要目标是在可穿戴设备、手机和物联网（IoT）设备上实现全天候AI体验，同时最大限度地减少电池使用。\n\n潜在用例包括：\n\n*   **情境感知**：检测用户活动（例如，步行、跑步）、接近度或环境（例如，室内/室外、移动中），以启用“请勿打扰”模式或其他情境感知功能。\n*   **音频处理**：语音和语音检测、关键词识别、实时翻译、转录以及基于音频的辅助功能。\n*   **图像处理**：人物和物体检测、面部识别、手势识别和低功耗视觉搜索。\n*   **用户交互**：通过手势、音频提示或其他传感器驱动输入实现控制。\n\n## 硬件强制隐私\n\nCoral NPU的一个核心原则是通过硬件强制安全来建立用户信任。我们的架构正在设计中，以支持CHERI等新兴技术，该技术提供细粒度的内存级安全和可扩展的软件隔离。通过这种方法，我们希望能够将敏感的AI模型和个人数据隔离在硬件强制的沙箱中，从而减轻基于内存的攻击。\n\n## 构建生态系统\n\n开放硬件项目的成功依赖于强大的合作伙伴关系。为此，我们正与Synaptics合作，Synaptics是我们在嵌入式计算、无线连接和物联网多模态传感领域的首个战略硅合作伙伴和领导者。Synaptics在其技术日上宣布了其新的Astra™ SL2610系列AI原生物联网处理器。该产品线采用了其Torq™ NPU子系统，这是业界首个Coral NPU架构的生产实现。该NPU的设计支持Transformer模型和动态操作符，使开发者能够为消费和工业物联网构建面向未来的边缘AI系统。\n\n此次合作支持了我们对统一开发者体验的承诺。Synaptics Torq™边缘AI平台基于IREE和MLIR的开源编译器和运行时构建。这次合作是朝着为智能、情境感知设备构建共享、开放标准迈出的重要一步。\n\n## 解决边缘核心危机\n\n通过Coral NPU，我们正在为个人AI的未来构建一个基础层。我们的目标是提供一个通用、开源和安全的平台，供行业在此基础上进行构建，从而培育一个充满活力的生态系统。这使得开发者和硅供应商能够超越当今碎片化的格局，在边缘计算的共享标准上进行协作，从而加速创新。",
      "shortSummary": "Coral NPU是一个面向边缘AI的全栈平台，旨在解决将生成式AI引入个人设备所面临的性能、碎片化和隐私挑战。它是一种AI优先、超低功耗的NPU架构，基于RISC-V，专为可穿戴设备和物联网的始终在线边缘AI设计。Coral NPU提供统一的开发体验，支持主流ML框架和编译器，可实现环境感知、情境识别和实时处理等应用。它通过硬件强制安全保障用户隐私，并与Synaptics合作，已实现首次生产部署，旨在构建一个开放、安全的边缘AI生态系统。",
      "translated_title": "Coral NPU：面向边缘AI的全栈平台",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-1.width-1250.png",
          "alt": "Coral NPU-1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-2.width-1250.png",
          "alt": "Coral NPU-2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-3.width-1250.png",
          "alt": "Coral NPU-3",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"tf5oi\">Generative AI has fundamentally reshaped our expectations of technology. We've seen the power of large-scale cloud-based models to create, reason and assist in incredible ways. However, the next great technological leap isn't just about making cloud models bigger; it's about embedding their intelligence directly into our immediate, personal environment. For AI to be truly assistive — proactively helping us navigate our day, translating conversations in real-time, or understanding our physical context — it must run on the devices we wear and carry. This presents a core challenge: embedding ambient AI onto battery-constrained edge devices, freeing them from the cloud to enable truly private, all-day assistive experiences.</p><p data-block-key=\"1a1a\">To move from the cloud to personal devices, we must solve three critical problems:</p><ul><li data-block-key=\"eu56\"><i>The performance gap:</i> Complex, state-of-the-art machine learning (ML) models demand more compute, far exceeding the limited power, thermal, and memory budgets of an edge device.</li><li data-block-key=\"7buqm\"><i>The fragmentation tax:</i> Compiling and optimizing ML models for a diverse landscape of proprietary processors is difficult and costly, hindering consistent performance across devices.</li><li data-block-key=\"a3ap1\"><i>The user trust deficit:</i> To be truly helpful, personal AI must prioritize the privacy and security of personal data and context.</li></ul><p data-block-key=\"ejp6o\">Today we introduce <a href=\"https://developers.google.com/coral\" target=\"_blank\" rel=\"noopener noreferrer\">Coral NPU</a>, a full-stack platform that builds on our original work from <a href=\"https://gweb-coral-full.uc.r.appspot.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Coral</a> to provide hardware designers and ML developers with the tools needed to build the next generation of private, efficient edge AI devices. Co-designed in partnership with Google Research and Google DeepMind, Coral NPU is an AI-first hardware architecture built to enable the next generation of ultra-low-power, always-on edge AI. It offers a unified developer experience, making it easier to deploy applications like ambient sensing. It's specifically designed to enable all-day AI on wearable devices while minimizing battery usage and being configurable for higher performance use cases. We’ve released our <a href=\"https://developers.google.com/coral\" target=\"_blank\" rel=\"noopener noreferrer\">documentation and tools</a> so that developers and designers can start building today.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Coral NPU: An AI-first architecture</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tf5oi\">Developers building for low-power edge devices face a fundamental trade-off, choosing between general purpose CPUs and specialized accelerators. General-purpose CPUs offer crucial flexibility and broad software support but lack the domain-specific architecture for demanding ML workloads, making them less performant and power-inefficient. Conversely, specialized accelerators provide high ML efficiency but are inflexible, difficult to program, and ill-suited for general tasks.</p><p data-block-key=\"1fta9\">This hardware problem is magnified by a highly fragmented software ecosystem. With starkly different programming models for CPUs and ML blocks, developers are often forced to use proprietary compilers and complex command buffers. This creates a steep learning curve and makes it difficult to combine the unique strengths of different compute units. Consequently, the industry lacks a mature, low-power architecture that can easily and effectively support multiple ML development frameworks.</p><p data-block-key=\"a5efi\">The Coral NPU architecture directly addresses this by reversing traditional chip design. It prioritizes the ML matrix engine over scalar compute, optimizing architecture for AI from silicon up and creating a platform purpose-built for more efficient, on-device inference.</p><p data-block-key=\"43d11\">As a complete, reference <a href=\"https://en.wikipedia.org/wiki/Neural_processing_unit\" target=\"_blank\" rel=\"noopener noreferrer\">neural processing unit</a> (NPU) architecture, Coral NPU provides the building blocks for the next generation of energy-efficient, ML-optimized <a href=\"https://en.wikipedia.org/wiki/System_on_a_chip\" target=\"_blank\" rel=\"noopener noreferrer\">systems on chip</a> (SoCs). The architecture is based on a set of <a href=\"https://riscv.org/specifications/ratified/\" target=\"_blank\" rel=\"noopener noreferrer\">RISC-V ISA</a> compliant architectural IP blocks and is designed for minimal power consumption, making it ideal for always-on ambient sensing. The base design delivers performance in the 512 <a href=\"https://en.wikipedia.org/wiki/Floating_point_operations_per_second\" target=\"_blank\" rel=\"noopener noreferrer\">giga operations per second</a> (GOPS) range while consuming just a few milliwatts, thus enabling powerful on-device AI for edge devices, hearables, AR glasses, and smartwatches.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-1.width-1250.png\" alt=\"Coral NPU-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-1.width-1250.png\" alt=\"Coral NPU-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vajmb\"><i>A unified view of the Coral NPU ecosystem, showcasing end-to-end stack for SoC designers and ML developers.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-vertical-padding --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"tf5oi\">The open and extensible <a href=\"https://developers.google.com/coral/guides/architecture\" target=\"_blank\" rel=\"noopener noreferrer\">architecture</a> based on RISC-V gives SoC designers flexibility to modify the base design, or use it as a pre-configured NPU. The Coral NPU architecture includes the following components:</p><ul><li data-block-key=\"bclpo\"><i>A scalar core:</i> A lightweight, C-programmable RISC-V frontend that manages data flow to the back-end cores, using a simple \"run-to-completion\" model for ultra-low power consumption and traditional CPU functions.</li><li data-block-key=\"1u961\"><i>A vector execution unit:</i> A robust single instruction multiple data (<a href=\"https://www.sciencedirect.com/topics/computer-science/single-instruction-multiple-data\" target=\"_blank\" rel=\"noopener noreferrer\">SIMD</a>) co-processor compliant with the RISC-V Vector instruction set (RVV) v1.0, enabling simultaneous operations on large data sets.</li><li data-block-key=\"2ju85\"><i>A matrix execution unit:</i> A highly efficient quantized outer product <a href=\"https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation\" target=\"_blank\" rel=\"noopener noreferrer\">multiply-accumulate</a> (MAC) engine purpose-built to accelerate fundamental neural network operations. Note that the matrix execution unit is still under development and will be released on <a href=\"https://github.com/google-coral/coralnpu\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a> later this year.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-2.width-1250.png\" alt=\"Coral NPU-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-2.width-1250.png\" alt=\"Coral NPU-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vajmb\"><i>Visualizing the architectural shift from traditional design to the Coral NPU.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Unified developer experience</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tf5oi\">The Coral NPU architecture is a simple, C-programmable target that can seamlessly integrate with modern compilers like <a href=\"https://iree.dev/\" target=\"_blank\" rel=\"noopener noreferrer\">IREE</a> and <a href=\"https://github.com/tensorflow/tflite-micro\" target=\"_blank\" rel=\"noopener noreferrer\">TFLM</a>. This enables easy support for ML frameworks like <a href=\"https://www.tensorflow.org/\" target=\"_blank\" rel=\"noopener noreferrer\">TensorFlow</a>, <a href=\"https://docs.jax.dev/en/latest/\" target=\"_blank\" rel=\"noopener noreferrer\">JAX</a>, and <a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">PyTorch</a>.</p><p data-block-key=\"46n8n\">Coral NPU incorporates a comprehensive software toolchain, including specialized solutions like the TFLM compiler for TensorFlow, alongside a general-purpose <a href=\"https://mlir.llvm.org/\" target=\"_blank\" rel=\"noopener noreferrer\">MLIR</a> compiler, C compiler, custom kernels, and a simulator. This provides developers with flexible pathways. For example, a model from a framework like JAX is first imported into the MLIR format using the <a href=\"https://openxla.org/stablehlo\" target=\"_blank\" rel=\"noopener noreferrer\">StableHLO</a> dialect. This intermediate file is then fed into the IREE compiler, which applies a hardware-specific plug-in to recognize the Coral NPU's architecture. From there, the compiler performs progressive lowering — a critical optimization step where the code is systematically translated through a series of dialects, moving closer to the machine's native language. After optimization, the toolchain generates a final, compact binary file ready for efficient execution on the edge device. This suite of industry-standard developer tools helps simplify the programming of ML models and can allow for a consistent experience across various hardware targets.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-3.width-1250.png\" alt=\"Coral NPU-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-3.width-1250.png\" alt=\"Coral NPU-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vajmb\"><i>The Coral NPU compiler toolchain, illustrating the complete flow from ML model creation through optimization and compilation to on-device deployment.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"tf5oi\">Coral NPU’s co-design process focuses on two key areas. First, the architecture efficiently accelerates the leading encoder-based architectures used in today's on-device vision and audio applications. Second, we are collaborating closely with the <a href=\"https://deepmind.google/models/gemma/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma</a> team to optimize Coral NPU for small transformer models, helping to ensure the accelerator architecture supports the next generation of generative AI at the edge.</p><p data-block-key=\"brs14\">This dual focus means Coral NPU is on track to be the first open, standards-based, low-power NPU designed to bring LLMs to wearables. For developers, this provides a single, validated path to deploy both current and future models with maximum performance at minimal power.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Target applications</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tf5oi\">Coral NPU is designed to enable ultra-low-power, always-on edge AI applications, particularly focused on ambient sensing systems. Its primary goal is to enable all day AI-experiences on wearables, mobile phones and <a href=\"https://en.wikipedia.org/wiki/Internet_of_things\" target=\"_blank\" rel=\"noopener noreferrer\">Internet of Things</a> (IoT) devices minimizing battery usage.</p><p data-block-key=\"54nt5\">Potential use cases include:</p><ul><li data-block-key=\"7himc\"><i>Contextual awareness:</i> Detecting user activity (e.g., walking, running), proximity, or environment (e.g., indoors/outdoors, on-the-go) to enable \"do-not-disturb\" modes or other context-aware features.</li><li data-block-key=\"ff339\"><i>Audio processing:</i> Voice and speech detection, keyword spotting, live translation, transcription, and audio-based accessibility features.</li><li data-block-key=\"ai106\"><i>Image processing:</i> Person and object detection, facial recognition, gesture recognition, and low-power visual search.</li><li data-block-key=\"9m2mn\"><i>User interaction:</i> Enabling control via hand gestures, audio cues, or other sensor-driven inputs.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Hardware-enforced privacy</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tf5oi\">A core principle of Coral NPU is building user trust through hardware-enforced security. Our architecture is being designed to support emerging technologies like <a href=\"https://www.cl.cam.ac.uk/research/security/ctsrd/cheri/\" target=\"_blank\" rel=\"noopener noreferrer\">CHERI</a>, which provides fine-grained memory-level safety and scalable software compartmentalization. With this approach, we hope to enable sensitive AI models and personal data to be isolated in a hardware-enforced sandbox, mitigating memory-based attacks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Building an ecosystem</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tf5oi\">Open hardware projects rely on strong partnerships to succeed. To that end, we’re collaborating with <a href=\"https://www.synaptics.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Synaptics</a>, our first strategic silicon partner and a leader in embedded compute, wireless connectivity, and multimodal sensing for the IoT. Today, at their Tech Day, Synaptics announced their new <a href=\"https://www.synaptics.com/sl2610-press-release\" target=\"_blank\" rel=\"noopener noreferrer\">Astra™ SL2610</a> line of <a href=\"https://www.synaptics.com/sl2610-product-line\" target=\"_blank\" rel=\"noopener noreferrer\">AI-Native IoT Processors</a>. This product line features their <a href=\"https://www.synaptics.com/torq-github\" target=\"_blank\" rel=\"noopener noreferrer\">Torq™ NPU</a> subsystem, the industry’s first production implementation of the Coral NPU architecture. The NPU’s design is transformer-capable and supports dynamic operators, enabling developers to build future-ready Edge AI systems for consumer and industrial IoT.</p><p data-block-key=\"4r0e3\">This partnership supports our commitment to a unified developer experience. The Synaptics Torq™ Edge AI platform is built on an open-source compiler and runtime based on IREE and MLIR. This collaboration is a significant step toward building a shared, open standard for intelligent, context-aware devices.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Solving core crises of the Edge</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tf5oi\">With Coral NPU, we are building a foundational layer for the future of personal AI. Our goal is to foster a vibrant ecosystem by providing a common, open-source, and secure platform for the industry to build upon. This empowers developers and silicon vendors to move beyond today's fragmented landscape and collaborate on a shared standard for edge computing, enabling faster innovation. Learn more about <a href=\"https://developers.google.com/coral\" target=\"_blank\" rel=\"noopener noreferrer\">Coral NPU</a> and start building today.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9lzmb\"><i>We would like to thank the core contributors and leadership team for this work, particularly Billy Rutledge, Ben Laurie, Derek Chow, Michael Hoang, Naveen Dodda, Murali Vijayaraghavan, Gregory Kielian, Matthew Wilson, Bill Luan, Divya Pandya, Preeti Singh, Akib Uddin, Stefan Hall, Alex Van Damme, David Gao, Lun Dong, Julian Mullings-Black, Roman Lewkow, Shaked Flur, Yenkai Wang, Reid Tatge, Tim Harvey, Tor Jeremiassen, Isha Mishra, Kai Yick, Cindy Liu, Bangfei Pan, Ian Field, Srikanth Muroor, Jay Yagnik, Avinatan Hassidim, and Yossi Matias.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "XR Blocks：加速AI + XR创新 (原标题: XR Blocks: Accelerating AI + XR innovation)",
      "link": "https://research.google/blog/xr-blocks-accelerating-ai-xr-innovation/",
      "pubDate": "Wed, 08 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-08T16:00:00.000Z",
      "creator": "Google",
      "summary": "## XR Blocks：加速AI + XR创新\n\n### 引言：弥合AI与XR之间的鸿沟\n\n当前，人工智能（AI）与扩展现实（XR）的结合有望开启沉浸式智能计算的新范式。然而，这两个领域之间存在显著的生态系统差距。AI的研发得益于JAX、PyTorch、TensorFlow等成熟框架以及ImageNet、LMArena等基准测试的加速，而AI驱动的XR交互原型开发仍是一个高摩擦过程，通常需要开发者手动集成感知、渲染和交互等低级系统。\n\n为了弥合这一鸿沟，我们推出了 **XR Blocks**（在ACM UIST 2025上发布），这是一个跨平台框架，旨在加速以人为中心的AI + XR创新。这是我们之前针对非XR用例、通过可视化编程简化机器学习管道原型的Visual Blocks for ML研究的重大进展。\n\n### XR Blocks框架概述\n\nXR Blocks提供了一个模块化架构，包含即插即用组件，用于AI + XR中的核心抽象：用户、世界、界面、AI和代理。其核心使命是加速感知型AI + XR应用的快速原型开发。该工具包基于WebXR、threejs、LiteRT和Gemini等易于访问的技术构建，降低了XR创作者的入门门槛。我们通过一系列开源模板、实时演示和GitHub上的源代码展示了其效用，目标是赋能社区快速将概念转化为交互式原型。更多能力概述可在我们的方向性论文和预告视频中找到。\n\n### 设计原则\n\n我们的架构和API设计选择遵循以下三个原则：\n\n*   **拥抱简洁性和可读性：** 受Python禅意的启发，我们优先考虑清晰、人类可读的抽象。开发者的脚本应像对所需体验的高级描述。简单的任务应易于实现，复杂的逻辑应保持明确和可理解。\n*   **优先考虑创作者体验：** 我们的主要目标是使智能和感知型XR应用的创作尽可能无缝。我们认为创作者应专注于用户体验，而不是传感器融合、AI模型集成或跨平台交互逻辑等低级“管道”工作。\n*   **实用主义而非完整性：** 鉴于AI和XR领域发展迅速，我们遵循实用主义的设计理念。一个试图做到完美、全面复杂的框架在发布时可能已经过时。我们倾向于一个简单、模块化、适应性强的架构，它可以在桌面模拟器和Android XR设备上运行，适用于广泛的应用。\n\n### 框架工作原理\n\nXR Blocks框架从Visual Blocks for ML和InstructPipe中汲取灵感，提供了一个高层级、以人为中心的抽象层，将交互的“意图”（Script，下文详述）与低级实现的“方式”分离。\n\n![XR Blocks框架](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks1_Framework.width-1250.png)\n\nXR Blocks加速了桌面模拟器和Android XR设备上实时AI + XR应用的原型开发。例如：\n*   **(a) XR真实感：** 在模拟中原型化深度感知、基于物理的交互，并将相同的代码部署到真实世界的XR设备。\n*   **(b) XR交互：** 将自定义手势模型无缝集成到桌面模拟器和设备上的XR部署。\n*   **(c) AI + XR集成：** 构建智能、上下文感知的助手，例如提供主动建议和不显眼交互的Sensible Agent原型。\n\n### 抽象层：现实模型 (Reality Model)\n\n我们提出了一个由高级抽象组成的新“现实模型”，以指导XR Blocks框架的实现。与为端到端无监督训练设计的“世界模型”不同，我们的现实模型由可替换的XR交互模块组成。我们设计的核心是 **Script**，它是应用程序的叙事和逻辑中心。Script操作六个核心原语：\n\n*   **用户与物理世界：** 我们的模型以用户为中心，包括手部、凝视和虚拟形象。物理世界允许Script查询感知的现实，例如深度、估计的光照条件和对象。\n*   **虚拟界面与上下文：** 该模型通过虚拟UI元素增强混合现实，从2D面板到完全3D资产。感知管道分析环境、活动和交互历史的上下文。一个示例应用可在Sensible Agent中找到。\n*   **智能与社交实体：** 我们将AI驱动的代理和远程人类伙伴视为模型中的主要实体。这使得在DialogLab中实现混合人机对话中的动态群组对话。\n\n![XR Blocks框架的现实模型概念图](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks2_RealityModel.width-1250.png)\n\n### 实现：核心引擎 (Core Engine)\n\n这个现实模型由XR Blocks的模块化核心引擎实现，该引擎提供高级API，使开发者能够利用以下子系统，而无需掌握其底层实现：\n\n*   **感知与输入管道：** 摄像头、深度和声音模块持续为现实模型提供并更新物理现实的表示。输入模块标准化来自各种设备的用户操作，为XR Blocks提供解释的原始数据。\n*   **AI作为核心工具：** `ai`模块充当中央神经系统，提供简单而强大的函数（`.query`、`.runModel`），使大型模型成为可访问的工具。\n*   **体验与可视化工具包：** 为了实现快速创建，该工具包提供了一个常用功能库。`ux`模块提供可重用的交互行为，如`.selectable`和`.draggable`，而`ui`和`effect`模块处理界面渲染和复杂视觉效果，如遮挡。\n\n![XR Blocks核心引擎的模块化架构](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks3_Architecture.width-1250.png)\n\n通过将抽象的现实模型与具体的核心引擎分离，XR Blocks实现了强大的新创意工作流。目标是让创作者更快地从高层级、以人为中心的想法转向交互式原型。我们设想未来，任何声明性提示，例如“当用户捏住一个物体时，代理应该为它生成一首诗”，都可以直接转化为XR Blocks中的高级指令：\n\n![XR Blocks中的高层级指令](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks4_Instructions.width-1250.png)\n\n因此，创作者的提示不再是伪代码，而是实现逻辑的直接总结。我们设想这个框架能更无缝地将用户意图转化为系统级执行流，组合来自输入、声音、AI、世界、UI和代理模块的功能，以通过用户交互生成新兴的智能行为。\n\n![XR Blocks的交互语法](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks5_Interaction.width-1250.png)\n\n### 应用场景\n\n我们提供了一套交互式应用程序，以展示XR Blocks框架的表达能力和灵活性。这些示例展示了我们的框架如何实现以前过于复杂和昂贵而无法构建的复杂体验的快速原型开发，从而促进创建逼真、交互式和智能的混合现实世界：\n\n![XR Blocks的应用](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks_Applications.width-1250.png)\n\nXR Blocks的应用包括：\n1.  **XR真实感：** 深度感知和基于物理的球池和泼水游戏；几何感知阴影、带遮挡的3D高斯泼溅和光照估计。\n2.  **XR交互：** 由自定义ML模型、动态滑动识别、与物理世界的触摸和抓取赋能的沉浸式表情符号和剪刀石头布游戏。\n3.  **AI + XR：** 与对话式AI、XR对象、XR中的眼镜模拟以及用真实世界摄像头生成诗歌的集成。\n\n当这个现实模型与生成式AI深度集成以创建动态、个性化环境时，框架的真正力量得以实现。我们通过构建诸如“增强对象智能”（XR-Objects）之类的系统来证明这一点，该系统赋予日常物理对象交互式数字功能，例如动态虚拟按钮。XR Blocks也作为Sensible Agent（在ACM UIST 2025上发布）的基础，这是一个用于主动和不显眼AR辅助的系统。我们的架构提供了代理的核心感知和交互逻辑，提供了我们主要目标的一个例子：通过提供强大、高层级的工具，XR Blocks赋能人机交互研究人员绕过低级实现，直接专注于人机协作的认知原理等更高阶的挑战。SDK演示视频展示了XR Blocks与对话式AI的结合、Android XR上的物理碰撞与深度感知，以及在设备上运行LiteRT与自定义手势模型以触发XR动画。\n\n### 结论与未来展望\n\n创建智能XR体验目前过于碎片化，在创作者的愿景和实现之间设置了主要障碍。我们介绍了XR Blocks，一个通过提供高层级抽象层来消除这种复杂性的架构和工具包，它将“意图”（what）与“方式”（how，低级实现）分离，极大地加速了上下文感知应用的快速原型开发。这是迈向未来编程、设计和对话界限消失的奠基性一步，使我们能够像编写故事一样流畅地编写现实。XR Blocks远非完美，这项工作是最初的愿景文档，旨在邀请更多创作者加入我们的旅程，基于我们相信拥有正确工具集，每个人都可以通过AI释放内在创造力。\n\n### 致谢\n\n这项工作是Google多个团队的联合协作成果，由David Li和Ruofei Du（同等主要贡献）、Nels Numan、Xun Qian、Yanhe Chen和Zhongyi Zhou（同等次要贡献，按字母顺序排列）以及Evgenii Alekseev、Geonsun Lee、Alex Cooper、Min Xia、Scott Chung、Jeremy Nelson、Xiuxiu Yuan、Jolica Dias、Tim Bettridge、Benjamin Hersh、Michelle Huynh、Konrad Piascik、Ricardo Cabello和David Kim等研究人员和工程师共同贡献。我们感谢Mahdi Tayarani、Max Dzitsiuk、Patrick Hackett、Seeyam Qiu、Brian Collins、Steve Toh、Eric Gonzalez、Nicolás Peña Moreno、Yi-Fei Li、Ziyi Liu、Jing Jin对我们早期提案和WebXR实验的反馈和讨论。我们感谢Max Spear、Adarsh Kowdle和Guru Somadder的方向性贡献和周到评审。",
      "shortSummary": "XR Blocks是一个跨平台框架，旨在弥合人工智能（AI）和扩展现实（XR）生态系统之间的鸿沟。它通过提供模块化架构和高层级抽象，加速感知型AI + XR应用的快速原型开发。该框架基于WebXR等技术，简化了AI模型集成、传感器融合和跨平台交互等复杂任务，使创作者能够专注于用户体验。XR Blocks通过现实模型和核心引擎，支持从高层级意图到交互式原型的快速转化，并已通过多种应用场景展示其强大功能，旨在赋能社区释放AI创造力。",
      "translated_title": "XR Blocks：加速AI + XR创新",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks1_Framework.width-1250.png",
          "alt": "XRBlocks1_Framework",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks2_RealityModel.width-1250.png",
          "alt": "XRBlocks2_RealityModel",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks3_Architecture.width-1250.png",
          "alt": "XRBlocks3_Architecture",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks4_Instructions.width-1250.png",
          "alt": "XRBlocks4_Instructions",
          "title": "",
          "position": 5
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks5_Interaction.width-1250.png",
          "alt": "XRBlocks5_Interaction",
          "title": "",
          "position": 6
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"x6cvv\">The combination of artificial intelligence (AI) and extended reality (XR) has the potential to unlock a new paradigm of immersive intelligent computing. However, a significant gap exists between the ecosystems of these two fields today. AI research and development is accelerated by mature frameworks like <a href=\"https://jax.readthedocs.io/en/latest/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">JAX</a>, <a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">PyTorch</a>, <a href=\"https://www.tensorflow.org/\" target=\"_blank\" rel=\"noopener noreferrer\">TensorFlow</a>, and benchmarks like <a href=\"https://www.image-net.org/\" target=\"_blank\" rel=\"noopener noreferrer\">ImageNet</a> and <a href=\"https://lmarena.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">LMArena</a>. Meanwhile, prototyping novel AI-driven XR interactions remains a high-friction process, often requiring practitioners to manually integrate disparate, low-level systems for perception, rendering, and interaction.</p><p data-block-key=\"5pqcb\">To bridge this gap, we introduce <a href=\"http://xrblocks.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">XR Blocks</a> (presented at <a href=\"https://uist.acm.org/2025/\" target=\"_blank\" rel=\"noopener noreferrer\">ACM UIST 2025</a>), a cross-platform framework designed to accelerate human-centered AI + XR innovation. This is a significant step from our prior research in <a href=\"https://research.google/blog/visual-blocks-for-ml-accelerating-machine-learning-prototyping-with-interactive-tools/\">Visual Blocks for ML</a>, which targets non-XR use cases and streamlines prototyping machine learning pipelines with visual programming. XR Blocks provides a modular architecture with plug-and-play components for core abstraction in AI + XR: <i>user</i>, <i>world</i>, <i>interface</i>, <i>AI</i>, and <i>agents</i>. Crucially, it is designed with the mission of accelerating rapid prototyping of perceptive AI + XR apps. Built upon accessible technologies (<a href=\"https://immersiveweb.dev/\" target=\"_blank\" rel=\"noopener noreferrer\">WebXR</a>, <a href=\"https://threejs.org/\" target=\"_blank\" rel=\"noopener noreferrer\">threejs</a>, <a href=\"https://ai.google.dev/edge/litert\" target=\"_blank\" rel=\"noopener noreferrer\">LiteRT</a>, <a href=\"https://gemini.google.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a>), our toolkit lowers the barrier to entry for XR creators. We demonstrate its utility through a set of open-source <a href=\"https://xrblocks.github.io/docs/templates/Basic/\" target=\"_blank\" rel=\"noopener noreferrer\">templates</a>, <a href=\"https://xrblocks.github.io/docs/samples/ModelViewer/\" target=\"_blank\" rel=\"noopener noreferrer\">live demos</a>, and <a href=\"https://github.com/google/xrblocks\" target=\"_blank\" rel=\"noopener noreferrer\">source code on GitHub</a>, with the goal of empowering the community to quickly move from concept to interactive prototype. You can find an overview of these capabilities in our <a href=\"https://arxiv.org/abs/2509.25504\" target=\"_blank\" rel=\"noopener noreferrer\">directional paper</a> and <a href=\"https://www.youtube.com/watch?v=75QJHTsAoB8\" target=\"_blank\" rel=\"noopener noreferrer\">teaser video</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"75QJHTsAoB8\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=75QJHTsAoB8\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kk7t9\"><i>Introductory video of XR Blocks.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xfvek\">Design principles</h2><p data-block-key=\"1ctfe\">Our architectural and API design choices are guided by three principles:</p><ul><li data-block-key=\"f851k\"><i>Embrace simplicity and readability:</i> Inspired by <a href=\"https://en.wikipedia.org/wiki/Zen_of_Python\" target=\"_blank\" rel=\"noopener noreferrer\">Python's Zen</a>, we prioritize clean, human-readable abstractions. A developer's script should read like a high-level description of the desired experience. Simple tasks should be simple to implement, and complex logic should remain explicit and understandable.</li><li data-block-key=\"43oej\"><i>Prioritize the creator experience</i>: Our primary goal is to make authoring intelligent and perceptive XR applications as seamless as possible. We believe that creators should focus on the user experience, not on the low-level “plumbing” of sensor fusion, AI model integration, or cross-platform interaction logic.</li><li data-block-key=\"5dabh\"><i>Pragmatism over completeness</i>: We follow a design philosophy of pragmatism, since the fields of AI and XR are evolving quickly. A comprehensive, complex framework that attempts to be perfect will be obsolete upon release. We favor a simple, modular, and adaptable architecture that runs on both desktop and <a href=\"https://www.android.com/xr/\" target=\"_blank\" rel=\"noopener noreferrer\">Android XR</a> devices for a wide range of applications.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xfvek\">XR Blocks framework</h2><p data-block-key=\"6k5u2\">Drawing inspiration from <a href=\"https://visualblocks.withgoogle.com/#/\" target=\"_blank\" rel=\"noopener noreferrer\">Visual Blocks for ML</a> and <a href=\"https://research.google/blog/instructpipe-generating-visual-blocks-pipelines-with-human-instructions-and-llms/\">InstructPipe</a>, we designed the XR Blocks framework to provide a high-level, human-centered abstraction layer that separates the <i>what</i> of an interaction (denoted as <i>Script</i>, described more below) from the <i>how</i> of its low-level implementation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks1_Framework.width-1250.png\" alt=\"XRBlocks1_Framework\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks1_Framework.width-1250.png\" alt=\"XRBlocks1_Framework\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kk7t9\"><i>XR Blocks accelerates the prototyping of real-time AI + XR applications across desktop simulators and</i> <a href=\"http://android.com/xr\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Android XR</i></a><i> devices. Examples: (a) XR Realism: Prototype depth-aware, physics-based interactions in simulation and deploy the same code to real-world XR devices. (b) XR Interactions: Seamlessly integrate custom gesture models to desktop simulator and on-device XR deployment. (c) AI + XR Integration: Build intelligent, context-aware assistants, like the</i> <a href=\"https://research.google/blog/sensible-agent-a-framework-for-unobtrusive-interaction-with-proactive-ar-agents/\"><i>Sensible Agent</i></a><i> prototype that provides proactive suggestions with unobtrusive interactions.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"xfvek\">Abstractions</h3><p data-block-key=\"5j9q1\">We propose a new <i>Reality Model</i> composed of high-level abstractions to guide the implementation of the XR Blocks framework. Unlike the <a href=\"https://arxiv.org/abs/1803.10122\" target=\"_blank\" rel=\"noopener noreferrer\">World Model</a> designed for end-to-end unsupervised training, our <i>Reality Model</i> consists of replaceable modules for XR interaction. At the heart of our design is <i>Script</i>, the narrative and logical center of an application. <i>Script</i> operates on six first-class primitives (described and visualized below):</p><ul><li data-block-key=\"7hci8\"><i>User &amp; the physical world:</i> Our model is centered around the <i>User</i>, consisting of hands, gaze, and avatar. The physical <i>world</i> allows <i>Script</i> to query the perceived reality such as depth (<a href=\"https://xrblocks.github.io/docs/samples/DepthMap\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>), estimated lighting condition (<a href=\"https://xrblocks.github.io/docs/samples/Lighting\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>), and objects (<a href=\"https://xrblocks.github.io/docs/samples/Gemini-XRObject\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>).</li><li data-block-key=\"b8t27\"><i>Virtual interfaces &amp; context:</i> The model augments the blended reality with virtual UI elements, from 2D panels (<a href=\"https://xrblocks.github.io/docs/samples/UI\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>) to fully 3D assets (<a href=\"https://xrblocks.github.io/docs/samples/ModelViewer/\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>). The perception pipeline analyzes the context of environment, activities, and histories of interaction. An example application can be found in <a href=\"https://research.google/blog/sensible-agent-a-framework-for-unobtrusive-interaction-with-proactive-ar-agents/\">Sensible Agent</a> (discussed more below).</li><li data-block-key=\"e7sot\"><i>Intelligent &amp; Social Entities</i>: We treat AI-driven <i>agents</i> and remote human <i>peers</i> as primary entities within the model. This enables dynamic group conversations in hybrid human-AI conversations in <a href=\"https://dl.acm.org/doi/10.1145/3746059.3747696\" target=\"_blank\" rel=\"noopener noreferrer\">DialogLab</a>.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks2_RealityModel.width-1250.png\" alt=\"XRBlocks2_RealityModel\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks2_RealityModel.width-1250.png\" alt=\"XRBlocks2_RealityModel\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kk7t9\"><i>The conceptual Reality Model of the XR Blocks framework. At the center, Script contains the application’s logic and operates on a unified model of first-class primitives including the user, the physical world, AI agents, and the application context.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"xfvek\">Implementation</h3><p data-block-key=\"c09j3\">This Reality Model is realized by XR Blocks’s modular Core engine, which provides high-level APIs that enable developers to harness the following subsystems without needing to master the implementation:</p><ul><li data-block-key=\"e15ud\"><i>Perception &amp; input pipeline:</i> The <code>camera</code>, <code>depth</code>, and <code>sound</code> modules continuously feed and update the Reality Model’s representation of physical reality. The <code>input</code> module normalizes user actions from various devices, providing the raw data for XR Blocks to interpret.</li><li data-block-key=\"2r6v8\"><i>AI as a core utility:</i> The <code>ai</code> module acts as a central nervous system, providing simple yet powerful functions (<code>.query</code>, <code>.runModel</code>) that make large models an accessible utility.</li><li data-block-key=\"fahdq\"><i>Experience &amp; visualization toolkit:</i> To enable rapid creation, the toolkit provides a library of common affordances. The <code>ux</code> module offers reusable interaction behaviors like <code>.selectable</code> and <code>.draggable</code> (<a href=\"https://xrblocks.github.io/docs/samples/ModelViewer/\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>), while the <code>ui</code> and <code>effect</code> modules handle the rendering of interfaces and complex visual effects like occlusion (<a href=\"https://xrblocks.github.io/docs/samples/Occlusion\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>).</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks3_Architecture.width-1250.png\" alt=\"XRBlocks3_Architecture\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks3_Architecture.width-1250.png\" alt=\"XRBlocks3_Architecture\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kk7t9\"><i>The modular architecture of the XR Blocks’s core engine, which consists of essential subsystems to realize the framework’s high-level abstractions, spanning perception (</i><code><i>depth</i></code><i>,</i> <code><i>input</i></code><i>), AI integration (</i><code><i>ai</i></code><i>,</i> <code><i>agent</i></code><i>), and user experience (</i><code><i>ui</i></code><i>,</i> <code><i>ux</i></code><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xfvek\">By separating the abstract Reality Model from the concrete Core engine, XR Blocks enables a powerful new creative workflow. The goal is to allow creators to move from high level, human-centric ideas to interactive prototypes much more quickly. We envision a future where any declarative prompt, <i>“When the user pinches at an object, an agent should generate a poem of it”</i>, could be directly translated to high-level instructions in XR Blocks:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks4_Instructions.width-1250.png\" alt=\"XRBlocks4_Instructions\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks4_Instructions.width-1250.png\" alt=\"XRBlocks4_Instructions\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xfvek\">Hence, the creator’s prompt is no longer pseudocode but a direct summary of the implementation logic. We envision this framework to more seamlessly translate such user intent into a system-level execution flow, composing capabilities from the <code>input</code>, <code>sound</code>, <code>ai</code>, <code>world</code>, <code>ui</code>, and <code>agent</code> modules to generate an emergent, intelligent behavior with user interaction.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks5_Interaction.width-1250.png\" alt=\"XRBlocks5_Interaction\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks5_Interaction.width-1250.png\" alt=\"XRBlocks5_Interaction\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kk7t9\"><i>The Interaction Grammar of XR Blocks, which abstracts user input by distinguishing between two types of interaction. Explicit events are direct, low-level inputs (e.g., a touch or click), while implicit intents are higher-level interpretations (e.g., a gesture or voice command), allowing creators to build interaction against user intent.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"fbybx\">Application scenarios</h2><p data-block-key=\"5nimk\">We provide a suite of interactive applications to demonstrate the expressive power and flexibility of the XR Blocks framework. These examples showcase how our framework enables the rapid prototyping of sophisticated experiences that were previously too complex and costly to build, facilitating the creation of realistic, interactive, and intelligent mixed-reality worlds:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks_Applications.width-1250.png\" alt=\"XRBlocks_Applications\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks_Applications.width-1250.png\" alt=\"XRBlocks_Applications\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8ze35\"><i>Applications of XR Blocks. (1) XR Realism: Depth-aware and physics-based ball pit (</i><a href=\"https://xrblocks.github.io/docs/samples/Ballpit\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>) and splash games (</i><a href=\"https://xrblocks.github.io/docs/samples/Splash\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>); geometry-aware shadows (</i><a href=\"https://xrblocks.github.io/docs/samples/Occlusion\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>), 3D Gaussian splatting with occlusion, and lighting estimation. (2) XR Interaction: Immersive emoji (</i><a href=\"https://xrblocks.github.io/docs/samples/XR-Emoji\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>) and rock paper scissors game (</i><a href=\"https://xrblocks.github.io/docs/samples/RockPaperScissors\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>) empowered by custom ML models, dynamic swipe recognition, touch and grab with the physical world. (3) AI + XR: Integration with conversational AI (</i><a href=\"https://xrblocks.github.io/docs/samples/Gemini-Icebreakers\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>), XR objects (</i><a href=\"https://xrblocks.github.io/docs/samples/Gemini-XRObject\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>), glasses simulation in XR, and poem generation with a real-world camera.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fbybx\">The true power of the framework is realized when this Reality Model is deeply integrated with generative AI to create dynamic, personalized environments. We demonstrate this by building systems like Augmented Object Intelligence (<a href=\"https://research.google/blog/augmented-object-intelligence-with-xr-objects/\">XR-Objects</a>), which imbues everyday physical objects with interactive digital affordances, such as dynamic virtual buttons. XR Blocks also serves as the foundation for <a href=\"https://research.google/blog/sensible-agent-a-framework-for-unobtrusive-interaction-with-proactive-ar-agents/\">Sensible Agent</a> (published on ACM UIST 2025), a system for proactive and unobtrusive AR assistance. Our architecture provides the agent's core perception and interaction logic, providing an example of our primary goal: by providing robust, high-level tools, XR Blocks empowers Human-Computer Interaction researchers to bypass low-level implementation and focus directly on higher-order challenges like the cognitive principles of human-agent collaboration.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/XRBlocks6_Demos.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8ze35\"><i>Demonstrations of XR Blocks SDK. (1) Using XR Blocks with conversational AI to automatically generate and test user prompts. (2) Running physical collision with depth sensing on Android XR. (3) Running LiteRT on a device with a custom gesture model to trigger XR animation.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"fbybx\">Conclusion and future directions</h2><p data-block-key=\"c4i24\">Creating intelligent XR experiences is currently too fragmented, placing a major barrier between a creator's vision and its realization. We presented <a href=\"https://xrblocks.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">XR Blocks</a>, an architecture and toolkit that dissolves this complexity by providing a high-level abstraction layer that separates <i>what</i> (the intent) from the <i>how</i> (the low-level implementation), dramatically accelerating the prototyping of context-aware applications. This is a foundational step toward a future where the boundaries between programming, design, and conversation disappear, enabling us to script realities as fluidly as we script stories. XR Blocks is far from perfect, and this work serves as <a href=\"https://arxiv.org/abs/2509.25504\" target=\"_blank\" rel=\"noopener noreferrer\">an initial visionary document</a> to invite more creators to join our journey, based on our belief that <i>with the right set of tools, everyone can unleash their inner creativity with AI</i>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"fbybx\">Acknowledgements</h2><p data-block-key=\"ccr67\"><i>This work is a joint collaboration across multiple teams at Google. The following researchers and engineers contributed to this work: David Li and Ruofei Du (equal primary contributions), Nels Numan, Xun Qian, Yanhe Chen, and Zhongyi Zhou, (equal secondary contributions, sorted alphabetically), as well as Evgenii Alekseev, Geonsun Lee, Alex Cooper, Min Xia, Scott Chung, Jeremy Nelson, Xiuxiu Yuan, Jolica Dias, Tim Bettridge, Benjamin Hersh, Michelle Huynh, Konrad Piascik, Ricardo Cabello, and David Kim. We would like to thank Mahdi Tayarani, Max Dzitsiuk, Patrick Hackett, Seeyam Qiu, Brian Collins, Steve Toh, Eric Gonzalez, Nicolás Peña Moreno, Yi-Fei Li, Ziyi Liu, Jing Jin for their feedback and discussion on our early-stage proposal and WebXR experiments. We thank Max Spear, Adarsh Kowdle, and Guru Somadder for the directional contribution and thoughtful reviews.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "语音到检索 (S2R)：一种新的语音搜索方法 (原标题: ​​Speech-to-Retrieval (S2R): A new approach to voice search)",
      "link": "https://research.google/blog/speech-to-retrieval-s2r-a-new-approach-to-voice-search/",
      "pubDate": "Mon, 06 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-06T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 语音到检索 (S2R)：一种新的语音搜索方法\n\n## 传统语音搜索的挑战\n\n*   **级联模型 (Cascade Modeling)**：当前的语音搜索系统通常采用级联模型，先通过自动语音识别 (ASR) 将语音输入转换为文本查询，然后基于文本进行搜索。\n*   **ASR 错误的影响**：这种方法的主要挑战在于，ASR 阶段的微小错误可能显著改变查询的含义，导致不相关的搜索结果。\n    *   **示例**：将“The Scream painting”（《呐喊》画作）错误识别为“screen painting”（屏幕画作），将返回关于屏幕绘画技巧的结果，而非蒙克杰作的信息。\n*   **问题根源**：级联模型存在信息丢失和错误传播的问题，ASR 系统一旦早期误解音频，错误就会传递给搜索系统，且搜索系统通常无法纠正。\n\n## 引入语音到检索 (S2R)\n\n*   **核心理念**：S2R 是一种直接从语音查询中解释和检索信息的技术，完全绕过了中间的、可能存在缺陷的文本转录步骤。\n*   **范式转变**：它代表了机器处理人类语音的根本性架构和理念转变。传统系统关注“说了什么词？”，而 S2R 旨在回答更强大的问题：“正在寻求什么信息？”\n*   **目标**：弥补当前语音搜索体验中存在的显著质量差距。\n\n## SVQ 数据集\n\n*   **开放资源**：作为 Massive Sound Embedding Benchmark (MSEB) 的一部分，谷歌开放了 Simple Voice Questions (SVQ) 数据集。\n*   **内容**：该数据集包含以 17 种不同语言和 26 个区域录制的短音频问题集合。\n*   **用途**：用于评估 S2R 的性能潜力。\n\n## S2R 潜力评估实验\n\n*   **实验目的**：模拟理想的 ASR 性能，以揭示当前系统与理论最佳性能之间的差距。\n*   **实验方法**：\n    1.  收集代表性语音搜索查询。\n    2.  由人工标注员手动转录这些查询，创建“完美 ASR”场景（即“groundtruth”）。\n    3.  **建立两个搜索系统进行比较**：\n        *   **级联 ASR (Cascade ASR)**：模拟真实世界设置，语音通过 ASR 转换为文本，然后传递给检索系统。\n        *   **级联真值 (Cascade Groundtruth)**：模拟“完美”级联模型，将无缺陷的真值文本直接发送给相同的检索系统。\n    4.  人类评估员比较两个系统的搜索结果质量。\n*   **评估指标**：\n    *   **ASR 质量**：词错误率 (WER)。\n    *   **搜索性能**：平均倒数排名 (MRR)，用于评估对查询列表响应的正确性概率。\n*   **关键发现**：\n    *   **WER 与 MRR 的复杂关系**：较低的 WER 并不总能可靠地带来较高的 MRR，错误的具体性质（而非仅仅存在）是关键的、依赖于语言的因素。\n    *   **显著的性能差距**：在所有测试语言中，两个系统之间存在显著的 MRR 差异。这表明当前级联设计与完美语音识别理论上可能达到的性能之间存在巨大差距，为 S2R 模型提供了巨大的改进潜力。\n\n![SVQ数据集中语音搜索语言的ASR模型词错误率](https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval3_WER.width-1250.png)\n*SVQ数据集中语音搜索语言的ASR模型词错误率*\n\n![当前真实世界（“级联 ASR”；蓝色）模型与真值（即完美；“级联真值”；绿色）的 MRR](https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval4_MRRCurrent.width-1250.png)\n*当前真实世界（“级联 ASR”；蓝色）模型与真值（即完美；“级联真值”；绿色）的 MRR*\n\n## S2R 的架构：从声音到意义\n\n*   **核心架构**：双编码器 (dual-encoder) 架构。\n*   **组件**：\n    *   **音频编码器**：处理原始音频查询，将其转换为捕获语义含义的丰富向量表示。\n    *   **文档编码器**：学习文档的类似向量表示。\n*   **训练机制**：\n    *   使用大量配对的音频查询和相关文档数据集进行训练。\n    *   系统同时调整两个编码器的参数。\n    *   **训练目标**：确保音频查询的向量在表示空间中与相应文档的向量在几何上接近。\n*   **优势**：该架构使模型能够直接从音频中学习更接近检索所需的核心意图，从而绕过级联设计中脆弱的逐字转录中间步骤。\n\n![音频和文档嵌入之间相似性损失的差异](https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval5_SimilarityLoss.width-1250.png)\n*音频和文档嵌入之间相似性损失的差异*\n\n## S2R 模型的工作原理\n\n1.  用户说出查询。\n2.  音频流式传输到预训练的音频编码器，生成一个查询向量（音频嵌入）。\n3.  该向量用于通过复杂的搜索排名过程，从索引中高效识别一组高度相关的候选结果。\n    *   **示例**：用户语音请求“The Scream painting”被音频编码器转换为音频嵌入。\n    *   该嵌入扫描大量文档索引，浮现出相似度高的初始候选结果（如维基百科页面、蒙克博物馆网站）。\n4.  搜索排名系统结合初始分数和数百个其他信号，深入理解相关性和质量，在极短时间内编排最终排名，确保向用户呈现最有帮助和最值得信赖的信息。\n\n## S2R 评估结果\n\n*   在 SVQ 数据集上对 S2R 系统进行了评估。\n*   **关键结果**：\n    *   S2R 模型显著优于基线级联 ASR 模型。\n    *   其性能接近由级联真值模型设定的上限。\n    *   尽管结果令人鼓舞，但剩余的差距表明仍需进一步研究。\n\n![当前真实世界（“级联 ASR”；蓝色）模型与真值（即完美；“级联真值”；绿色）以及 S2R 模型性能（“S2R”橙色条）的 MRR](https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval7_Results.width-1250.png)\n*当前真实世界（“级联 ASR”；蓝色）模型与真值（即完美；“级联真值”；绿色）以及 S2R 模型性能（“S2R”橙色条）的 MRR*\n\n## 语音搜索的新时代\n\n*   **已投入实际应用**：S2R 驱动的语音搜索已在多种语言中为用户提供服务，实现了超越传统级联系统的显著准确性提升。\n*   **推动领域发展**：谷歌开放了 SVQ 数据集作为 MSEB 的一部分，旨在加速整个领域的发展。\n*   **呼吁合作**：邀请全球研究社区利用这些数据，在公共基准上测试新方法，共同构建下一代真正智能的语音界面。",
      "shortSummary": "传统语音搜索依赖ASR将语音转文本，易因识别错误导致结果不准确。语音到检索（S2R）是一种新方法，它直接从语音查询中理解并检索信息，绕过文本转录。S2R采用双编码器架构，将音频直接映射到语义意图。实验表明，S2R显著优于传统ASR系统，并接近理论最佳性能。谷歌已将S2R投入实际应用，并在多语言环境中提升了语音搜索准确性，同时开放了SVQ数据集以推动该领域发展。",
      "translated_title": "语音到检索 (S2R)：一种新的语音搜索方法",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval3_WER.width-1250.png",
          "alt": "SpeechToRetrieval3_WER",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval4_MRRCurrent.width-1250.png",
          "alt": "SpeechToRetrieval4_MRRCurrent",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval5_SimilarityLoss.width-1250.png",
          "alt": "SpeechToRetrieval5_SimilarityLoss",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval7_Results.width-1250.png",
          "alt": "SpeechToRetrieval7_Results",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"42rfj\">Voice-based web search has been around a long time and continues to be used by many people, with the underlying technology evolving rapidly to allow for expanded use cases. Google’s initial voice search solution used <a href=\"https://en.wikipedia.org/wiki/Speech_recognition\" target=\"_blank\" rel=\"noopener noreferrer\">automatic speech recognition</a> (ASR) to turn the voice input into a text query, and then searched for documents matching that text query. However, a challenge with this <a href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36340.pdf\">cascade modeling approach</a> is that any slight errors in the speech recognition phase can significantly alter the meaning of the query, producing the wrong results.</p><p data-block-key=\"9va6b\">For example, imagine someone does a voice-based web search for the famous painting, “<a href=\"https://en.wikipedia.org/wiki/The_Scream\" target=\"_blank\" rel=\"noopener noreferrer\">The Scream</a>”, by Edvard Munch. The search engine uses the typical approach of cascade modeling, first converting the voice query to text via ASR before passing the text to the search system. Ideally, the ASR transcribes the query perfectly. The search system then receives the correct text — “the Scream painting” — and provides relevant results, like the painting’s history, its meaning, and where it’s displayed. However, what if the ASR system mistakes the “m” of “scream” for an “n”? It misinterprets the query as “screen painting” and returns irrelevant results about screen painting techniques instead of details about Munch's masterpiece.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/SpeechToRetrieval2_Cascade.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"255p4\"><i>ASR accuracy is key for voice search. See what happens when a system correctly transcribes a query versus when it transcribes it incorrectly.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"42rfj\">To prevent such errors in web search systems, what if the system could map directly from speech to the desired retrieval intent, bypassing the textual transcription entirely?</p><p data-block-key=\"8hvip\">Enter Speech-to-Retrieval (S2R). At its core, S2R is a technology that directly interprets and retrieves information from a spoken query without the intermediate, and potentially flawed, step of having to create a perfect text transcript. It represents a fundamental architectural and philosophical shift in how machines process human speech. Where today's common voice search technologies are focused on the question, \"What words were said?\", S2R is designed to answer a more powerful question: \"What information is being sought?\" This post explores the substantial quality gap in current voice search experiences and demonstrates how the S2R model is poised to fill it. In addition, we are open-sourcing the <a href=\"https://huggingface.co/datasets/google/svq#:~:text=Simple%20Voice%20Questions%20(SVQ)%20is,languages%20under%20multiple%20audio%20conditions.\" target=\"_blank\" rel=\"noopener noreferrer\">Simple Voice Questions</a> (SVQ) dataset, a collection of short audio questions recorded in 17 different languages and 26 locales, which we used to evaluate the performance potential of S2R. The SVQ dataset is part of the new <a href=\"https://github.com/google-research/mseb\" target=\"_blank\" rel=\"noopener noreferrer\">Massive Sound Embedding Benchmark</a> benchmark.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"42rfj\">Evaluating the potential of S2R</h2><p data-block-key=\"e2hqh\">When a traditional ASR system converts audio into a single text string, it may lose contextual cues that could help disambiguate the meaning (i.e., information loss). If the system misinterprets the audio early on, that error is passed along to the search engine, which typically lacks the ability to correct it (i.e., error propagation). As a result, the final search result may not reflect the user's intent.</p><p data-block-key=\"fmodj\">To investigate this relationship, we conducted an experiment designed to simulate an ideal ASR performance. We began by collecting a representative set of test queries reflecting typical voice search traffic. Crucially, these queries were then manually transcribed by human annotators, effectively creating a \"perfect ASR\" scenario where the transcription is the absolute truth.</p><p data-block-key=\"esgbn\">We then established two distinct search systems for comparison (see chart below):</p><ul><li data-block-key=\"crdkj\">Cascade ASR represents a typical real-world setup, where speech is converted to text by an automatic speech recognition (ASR) system, and that text is then fed to a retrieval system.</li><li data-block-key=\"103o6\">Cascade groundtruth simulates a \"perfect\" cascade model by sending the flawless ground-truth text directly to the same retrieval system.</li></ul><p data-block-key=\"d2bvm\">The retrieved documents from both systems (cascade ASR and cascade groundtruth) were then presented to human evaluators, or \"raters\", alongside the original true query. The evaluators were tasked with comparing the search results from both systems, providing a subjective assessment of their respective quality.</p><p data-block-key=\"edlf6\">We use <a href=\"https://en.wikipedia.org/wiki/Word_error_rate\" target=\"_blank\" rel=\"noopener noreferrer\">word error rate</a> (WER) to measure the ASR quality and to measure the search performance, we use <a href=\"https://en.wikipedia.org/wiki/Mean_reciprocal_rank\" target=\"_blank\" rel=\"noopener noreferrer\">mean reciprocal rank</a> (MRR) — a statistical metric for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness and calculated as the average of the reciprocals of the rank of the first correct answer across all queries. The difference in MRR and WER between the real-world system and the groundtruth system reveals the potential performance gains <a href=\"https://vocal.media/education/the-rise-of-voice-search-and-its-impact-on-indian-businesses\" target=\"_blank\" rel=\"noopener noreferrer\">across some of the most commonly used voice search languages</a> in the SVQ dataset (shown below).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval3_WER.width-1250.png\" alt=\"SpeechToRetrieval3_WER\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval3_WER.width-1250.png\" alt=\"SpeechToRetrieval3_WER\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"255p4\"><i>The word error rate (WER) of the ASR model across voice search languages in the SVQ dataset.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval4_MRRCurrent.width-1250.png\" alt=\"SpeechToRetrieval4_MRRCurrent\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval4_MRRCurrent.width-1250.png\" alt=\"SpeechToRetrieval4_MRRCurrent\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"255p4\"><i>MRR of current real-world (“Cascade ASR”; blue) models vs ground truth (i.e., perfect; “Cascade Groundtruth”; green).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"42rfj\">The results of this comparison lead to two critical observations. First, and as can be seen by comparing both charts above, we found that a lower WER does not reliably lead to a higher MRR across different languages. The relationship is complex, suggesting that the impact of transcription errors on downstream tasks is not fully captured by the WER metric. The specific nature of an error — not just its existence — appears to be a critical, language-dependent factor. Second, and more importantly, there’s a significant MRR difference between the two systems across all tested languages. This reveals a substantial performance gap between current cascade designs and what is theoretically possible with perfect speech recognition. This gap represents the clear potential for S2R models to fundamentally improve voice search quality.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"h9kvr\">The architecture of S2R: From sound to meaning</h2><p data-block-key=\"aej9r\">At the heart of our S2R model is a dual-encoder architecture. This design features two specialized neural networks that learn from vast amounts of data to understand the relationship between speech and information. An audio encoder processes the raw audio of a query, converting it into a rich vector representation that captures its semantic meaning. In parallel, a document encoder learns a similar vector representation for documents.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval5_SimilarityLoss.width-1250.png\" alt=\"SpeechToRetrieval5_SimilarityLoss\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval5_SimilarityLoss.width-1250.png\" alt=\"SpeechToRetrieval5_SimilarityLoss\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"iex25\"><i>Difference in similarity loss between audio and document embedding.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"h9kvr\">The key to this model is how it is trained. Using a large dataset of paired audio queries and relevant documents, the system learns to adjust the parameters of both encoders simultaneously.</p><p data-block-key=\"3mkue\">The training objective ensures that the vector for an audio query is geometrically close to the vectors of its corresponding documents in the representation space. This architecture allows the model to learn something closer to the essential <i>intent</i> required for retrieval directly from the audio, bypassing the fragile intermediate step of transcribing every word, which is the principal weakness of the cascade design.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"h9kvr\">How the S2R model works</h2><p data-block-key=\"7b96s\">When a user speaks a query, the audio is streamed to the pre-trained audio encoder, which generates a query vector. This vector is then used to efficiently identify a highly relevant set of candidate results from our index through a complex search ranking process.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/SpeechToRetrieval6_Final.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"iex25\"><i>How S2R processes a spoken query.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"h9kvr\">The animation above illustrates how S2R understands and answers a spoken query. It starts with a user's voice request for “The Scream painting”. An audio encoder translates the sound into a rich <a href=\"https://dev.to/josethz00/audio-embeddings-understanding-the-basics-4pc1\" target=\"_blank\" rel=\"noopener noreferrer\">audio embedding</a>, a vector that represents the deep meaning of the query. This embedding is then used to scan a massive index of documents, surfacing initial candidates with high similarity scores, like the Wikipedia page for “The Scream” (0.8) and the Munch Museum website (0.7).</p><p data-block-key=\"3p31t\">But finding relevant documents is just the beginning. The crucial final step is orchestrated by the search ranking system. This powerful intelligence goes far beyond the initial scores, weaving them together with hundreds of other signals to deeply understand relevance and quality. It weighs all this information in a fraction of a second to choreograph the final ranking, ensuring the most helpful and trustworthy information is presented to the user.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"h9kvr\">Evaluating S2R</h2><p data-block-key=\"9ef7v\">We evaluated the S2R system described above on the SVQ dataset:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval7_Results.width-1250.png\" alt=\"SpeechToRetrieval7_Results\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval7_Results.width-1250.png\" alt=\"SpeechToRetrieval7_Results\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"iex25\"><i>MRR of current real-world (“Cascade ASR”; blue) models vs ground truth (i.e., perfect; “Cascade Groundtruth”; green) and the S2R model's performance (\"S2R\" orange bar).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"h9kvr\">The S2R model's performance (orange bar) shows two key results:</p><ul><li data-block-key=\"2hrqi\">It significantly outperforms the baseline cascade ASR model.</li><li data-block-key=\"elct4\">Its performance approaches the upper bound established by the cascade ground truth model.</li></ul><p data-block-key=\"a341o\">While promising, the remaining gap indicates that further research is required.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"h9kvr\">The new era for voice search is now live</h2><p data-block-key=\"bmjeq\">The move to S2R-powered voice search isn’t a theoretical exercise; it’s a live reality. In a close collaboration between Google Research and Search, these advanced models are now serving users in multiple languages, delivering a significant leap in accuracy beyond conventional cascade systems.</p><p data-block-key=\"cr6fk\">To help propel the entire field forward, we are also open-sourcing the <a href=\"https://huggingface.co/datasets/google/svq\" target=\"_blank\" rel=\"noopener noreferrer\">SVQ dataset</a> as part of the <a href=\"https://github.com/google-research/mseb\" target=\"_blank\" rel=\"noopener noreferrer\">Massive Sound Embedding Benchmark</a> (MSEB). We believe shared resources and transparent evaluation accelerates progress. In that spirit, we invite the global research community to use this data, test new approaches on public benchmarks, and join the effort to build the next generation of truly intelligent voice interfaces.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"h9kvr\">Acknowledgements</h2><p data-block-key=\"9o630\"><i>The authors sincerely thank all who contributed to this project, whose critical input made it possible. We are especially grateful to our colleagues Hawi Abraham, Cyril Allauzen, Tom Bagby, Karthik Kumar Bandi, Stefan Buettcher, Dave Dopson, Lucy Hadden, Georg Heigold, Sanjit Jhala, Shankar Kumar, Ji Ma, Eyal Mizrachi, Pandu Nayak, Pew Putthividhya, David Rybach, Jungshik Shin, Venkat Subramanian, Sundeep Tirumalareddy and Trystan Upstill. We also wish to acknowledge those who helped prepare this post: Mark Simborg for his extensive editing, Kimberly Schwede for the wonderful illustrations, and Mickey Wurts for his valuable assistance.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "图像生成中的协作方法 (原标题: A collaborative approach to image generation)",
      "link": "https://research.google/blog/a-collaborative-approach-to-image-generation/",
      "pubDate": "Wed, 01 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-01T16:00:00.000Z",
      "creator": "Google",
      "summary": "在文本到图像（T2I）模型日益强大的今天，用户仍常面临生成结果与心中所想不符的困境，且难以通过反复修改提示词来弥合差距。为了解决这一问题，研究人员提出了 **PASTA（偏好自适应和序列文本到图像代理）**，这是一种强化学习（RL）代理，旨在通过与用户协作，逐步完善T2I生成结果，从而消除用户对试错式提示词优化的依赖。\n\n### PASTA 的核心理念与优势\n\n*   **协作式对话**：将图像生成过程转变为与用户的协作式对话，而非单向的提示词输入。\n*   **消除试错**：用户无需反复修改提示词，PASTA会根据用户反馈逐步调整。\n*   **用户满意度高**：通过人类评估，PASTA生成的图像被用户评价为更令人满意。\n*   **数据驱动**：创建并发布了一个包含7000多次人类交互的序列偏好数据集。\n\n### PASTA 的工作原理\n\n为了训练PASTA有效适应用户的个性化偏好，需要大量多样的交互数据。鉴于获取真实用户数据的挑战（如隐私问题），PASTA采用了两阶段训练策略：\n\n1.  **高质量基础数据集收集**：\n    *   收集了超过7000名评估者与PASTA的序列交互数据。\n    *   这些交互包括由Gemini Flash大型多模态模型生成的提示词扩展，以及由Stable Diffusion XL (SDXL) T2I模型生成的相应图像。\n    *   这些真实的偏好数据被用于训练用户模拟器。\n\n2.  **用户模拟器训练**：\n    *   **用户模型**：包含两个关键组件：\n        *   **效用模型**：预测用户对任何图像集的喜好程度。\n        *   **选择模型**：预测用户在面对多个图像集时会选择哪一个。\n    *   **模型构建**：使用预训练的CLIP编码器，并添加用户特定组件。\n    *   **训练算法**：采用期望最大化算法，同时学习用户偏好细节并发现潜在的“用户类型”（即具有相似品味的集群，如偏爱动物、风景或抽象艺术）。\n    *   **数据生成**：训练后的用户模拟器能够对生成的图像提供反馈并表达偏好，从而生成超过30,000条模拟交互轨迹，为PASTA代理的强化学习训练提供了大规模受控环境。\n\n    ![PASTA-1](https://storage.googleapis.com/gweb-research2023-media/images/PASTA-1.width-1250.png)\n    *图示：用户模拟器从偏好数据中识别出不同的用户类型。每一行展示了新兴用户档案的顶级图像，揭示了对“动物”或“食物”等类别的清晰偏好。*\n\n### PASTA 代理的交互流程\n\n一旦PASTA训练并部署，用户与PASTA的交互过程如下：\n\n1.  **初始提示**：用户输入一个初始提示词。\n2.  **候选生成**：PASTA使用候选生成器（一个大型多模态模型）创建一组多样化的潜在提示词扩展。\n3.  **候选选择**：训练有素的RL代理（候选选择器）选择最佳的四组扩展建议。\n4.  **图像生成与展示**：根据选定的扩展生成相应图像，并展示给用户。\n5.  **用户反馈**：用户选择最符合其愿景的图像，此选择作为反馈指导PASTA的下一组建议。\n6.  **迭代优化**：这种协作式的来回交互使模型能够即时学习用户的偏好，逐步引导创作过程达到用户的理想目标。\n\n### 性能评估与结果\n\n研究人员通过隐式Q学习（IQL）训练了PASTA，并评估了不同训练数据对性能的影响。他们创建了三个版本的代理：仅使用真实志愿者数据训练、仅使用模拟数据训练、以及结合真实和模拟数据集训练。这些代理与一个基线模型（未经过额外训练的基础Gemini Flash和SDXL模型）进行了比较，评估指标包括Pick-a-Pic准确性、Spearman秩相关、选择模型准确性和跨轮次准确性。\n\n**主要发现**：\n\n*   仅使用合成数据训练的PASTA未能超越基线模型。\n*   仅使用真实人类数据训练的PASTA表现出显著改进，但仍未超越基线。\n*   **结合真实和模拟数据训练的PASTA表现最佳**，这证实了用户模拟器成功捕捉了人类交互的关键动态，并提供了强化学习训练所需的规模。\n*   在直接比较中，**85%的评估者更喜欢PASTA生成的最终图像**，尤其是在处理抽象提示词时（如“爱的图像”、“幸福的图像”），PASTA能根据不同用户类型生成截然不同的、高度个性化的结果。\n\n    ![PASTA-3](https://storage.googleapis.com/gweb-research2023-media/images/PASTA-3.width-1250.png)\n    *图示：训练后的用户模型准确性性能（y轴）与考虑的用户类型数量（x轴）的关系。上排显示模型在Pick-a-Pic测试集上的准确性（左）和在HPS测试集上的Spearman秩相关（右）。下排显示模型的选择准确性（左）和跨轮次偏好准确性（右），均在人类评估测试集上进行评估。*\n\n    ![PASTA-4](https://storage.googleapis.com/gweb-research2023-media/images/PASTA-4.width-1250.png)\n    *图示：从相同的起始提示“幸福的图像”开始，PASTA为两种不同的用户类型（用户类型A和用户类型B）生成了截然不同的结果，展示了其适应个体独特创意风格的能力。*\n\n### 未来展望\n\nPASTA的研究表明，生成式AI的未来将更加互动、适应偏好和协作。所开发的方法，特别是强大的用户模拟器，可应用于许多其他生成任务，以创建更符合和适应人类用户的AI。为了促进进一步研究，该团队已开源了其序列评估者数据集和模拟用户数据。",
      "shortSummary": "PASTA（偏好自适应和序列文本到图像代理）是一种强化学习代理，旨在通过协作对话解决文本到图像模型难以捕捉用户细微创意意图的问题。它通过结合真实人类反馈和大规模用户模拟进行训练，学习并适应用户的独特偏好。PASTA能够逐步优化图像生成结果，用户评估显示其生成的图像比基线模型更令人满意。该研究表明，生成式AI的未来将更加互动、适应偏好和协作。",
      "translated_title": "图像生成中的协作方法",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PASTA-1.width-1250.png",
          "alt": "PASTA-1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PASTA-3.width-1250.png",
          "alt": "PASTA-3",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PASTA-4.width-1250.png",
          "alt": "PASTA-4",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"cmj9x\">You have a perfect image in your mind. You enter a prompt, hit generate, and the result is close to what you were thinking, but not quite right. You try refining the prompt, adding more detail, but you can't seem to bridge the gap between your idea and the final image.</p><p data-block-key=\"bkhg8\">This is a common experience. While <a href=\"https://en.wikipedia.org/wiki/Text-to-image_model\" target=\"_blank\" rel=\"noopener noreferrer\">text-to-image</a> (T2I) models are incredibly powerful, they often struggle to capture the nuance and specificity of an individual's unique creative intent given just a single prompt. What if we could turn image generation into a collaborative conversation?</p><p data-block-key=\"34fn3\">In this post, we describe our research “<a href=\"https://arxiv.org/abs/2412.10419\" target=\"_blank\" rel=\"noopener noreferrer\">Preference Adaptive and Sequential Text-to-image Agent</a>” (PASTA), a reinforcement learning (RL) agent that collaborates with users to progressively refine T2I results. This approach eliminates the need for users to rely on trial-and-error prompt refinement to reach a desirable image. Through human evaluations, we created a novel dataset of sequential preferences, which we then used to compare PASTA with a baseline state-of-the-art model. The results demonstrated that PASTA, trained with our mix of real and simulated data, consistently produced images that users rated as more satisfying. We’ve also released our <a href=\"https://www.kaggle.com/datasets/googleai/pasta-data\" target=\"_blank\" rel=\"noopener noreferrer\">foundational dataset</a> with a collection of over 7,000 human rater interactions with PASTA.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How PASTA works</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"cmj9x\">To effectively train an AI agent to adapt to a user's individual preferences, a large, diverse set of interaction data is needed. However, gathering this data from real users is challenging due to several factors, including user privacy. To address this, we trained PASTA using a two-stage strategy that combines real human feedback with large-scale user simulation.</p><p data-block-key=\"cnmkf\">First, we collected a <a href=\"https://www.kaggle.com/datasets/googleai/pasta-data\" target=\"_blank\" rel=\"noopener noreferrer\">high-quality foundational dataset</a> with over 7,000 raters' sequential interactions. These interactions included prompt expansions generated by a <a href=\"https://arxiv.org/pdf/2403.05530\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini Flash</a> large multimodal model and corresponding images generated by a <a href=\"https://arxiv.org/pdf/2307.01952\" target=\"_blank\" rel=\"noopener noreferrer\">Stable Diffusion XL</a> (SDXL) T2I model. This initial seed of authentic preference data was then used to train a user simulator, designed to generate additional data that replicate real human choices and preferences.</p><p data-block-key=\"f45gr\">At the heart of our method is a user model, comprising two key components: 1) a utility model that predicts the degree to which a user will like any set of images, and 2) a choice model that predicts which set of images they will select when presented with several sets. We constructed the user model using pre-trained <a href=\"https://arxiv.org/abs/2103.00020\" target=\"_blank\" rel=\"noopener noreferrer\">CLIP encoders</a> and added user-specific components. We trained the model using an <a href=\"https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm\" target=\"_blank\" rel=\"noopener noreferrer\">expectation-maximization</a> algorithm that allows us to simultaneously learn the specifics of user preferences while also discovering latent “user types,” that is, clusters of users with similar tastes (e.g., tendencies to prefer images with animals, scenic views, or abstract art).</p><p data-block-key=\"bpgbn\">The trained user simulator can provide feedback and express preferences on generated images, and make selections from sets of proposed images. This allows us to generate over 30,000 simulated interaction trajectories.. Our approach does more than just create more data; it gives us a controlled environment in which to explore a vast range of user behaviors so we can train the PASTA agent to effectively collaborate with users.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PASTA-1.width-1250.png\" alt=\"PASTA-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PASTA-1.width-1250.png\" alt=\"PASTA-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"00pr5\"><i>Our user simulator learns to identify distinct user types from preference data. Each row shows the top-rated images for an emergent user profile, revealing clear preferences for categories like \"Animals\" or \"Food.\"</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"cmj9x\">With this robust, data-driven foundation, the PASTA agent is trained to effectively engage with arbitrary users to generate images that match their preferences. The agent itself is a <a href=\"https://arxiv.org/abs/1312.5602\" target=\"_blank\" rel=\"noopener noreferrer\">value-based reinforcement learning</a> model that learns to select the best \"slate\" of prompt expansions (i.e., elaborations of the current prompt used to generate subsequent images) to show the user at each turn. Its goal is to maximize the user's cumulative satisfaction over the entire interaction.</p><p data-block-key=\"da0lr\">Once PASTA is trained and deployed, a user initiates the engagement with an initial prompt. PASTA first uses a candidate generator (a large multimodal model) to create a diverse set of potential prompt expansions. Then, a candidate selector (our trained RL agent) selects the optimal slate of four such expansions, which are used to generate corresponding images to present to the user. The user selects the image that is closest to their vision, which provides feedback that guides PASTA's next set of suggestions. This collaborative back-and-forth allows the model to learn the user's preferences on the fly, steering the creative process toward their ideal goal with each step.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/PASTA-mp4.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"00pr5\"><i>Starting with a simple prompt for \"A white cat\", PASTA engages the user in a visually grounded dialogue. The user's selections (highlighted in blue) help the agent quickly learn their preference for a more fantastical and colorful style.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Putting PASTA to the test</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"cmj9x\">To evaluate our approach, we trained PASTA as a value-based reinforcement learning agent using <a href=\"https://arxiv.org/abs/2110.06169\" target=\"_blank\" rel=\"noopener noreferrer\">implicit Q-learning</a> (IQL). We specifically wanted to see how the use of different training data impacted performance. We created three versions of the agent: 1) trained only on the real volunteer-rater data, 2) trained only on the simulated data, and 3) trained on a combination of real and simulated datasets.</p><p data-block-key=\"p6im\">We then ran a series of human evaluations comparing these agents to a baseline model (i.e., base Gemini Flash and SDXL models with no additional training) across four metrics: accuracy over the <a href=\"https://proceedings.neurips.cc/paper_files/paper/2023/file/73aacd8b3b05b4b503d58310b523553c-Paper-Conference.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Pick-a-Pic</a> dataset, <a href=\"https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient\" target=\"_blank\" rel=\"noopener noreferrer\">Spearman’s rank correlation</a>, choice model accuracy, and cross turn accuracy. Pick-a-Pic accuracy and Spearman's rank correlation assess the model's ability to predict user preferences and rankings on existing, large-scale, single-turn datasets. Choice model accuracy and cross-turn accuracy measure the model's ability to predict which image a user will choose at a given turn and whether the selected images are an improvement over the previous turn, respectively.</p><p data-block-key=\"cp9o4\">The results demonstrated that training PASTA on synthetic data alone didn't beat the baseline and while the agent trained on real human data showed significant improvement, it also didn’t outperform the baseline. However, the agent trained on the combination of both real and simulated data offered the best performance, confirming that our user simulation successfully captures key dynamics of human interaction while providing the scale needed for robust RL training.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PASTA-3.width-1250.png\" alt=\"PASTA-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PASTA-3.width-1250.png\" alt=\"PASTA-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"00pr5\"><i>The graphs above present the accuracy performance of a trained user model (y axis) as a function of the number of user types considered (x axis). The top row displays the model’s accuracy on the Pick-a-Pic test set (</i><b><i>left</i></b><i>) and its Spearman’s rank correlation on the</i> <a href=\"https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Human_Preference_Score_Better_Aligning_Text-to-Image_Models_with_Human_Preference_ICCV_2023_paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\"><i>HPS test</i></a><i> set (</i><b><i>right</i></b><i>). The bottom row shows the model’s choice accuracy (</i><b><i>left</i></b><i>) and cross-turn preference accuracy (</i><b><i>right</i></b><i>), both evaluated on our human-rated test set</i>.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"cmj9x\">When we asked raters to directly compare the final images from our best-performing agent against the baseline, 85% preferred PASTA's generated images. The difference is especially striking with abstract prompts. Starting with a simple idea like \"an image of love\", PASTA adapted to different user types to create a wide variety of results, from tender portraits to abstract, geometric art.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PASTA-4.width-1250.png\" alt=\"PASTA-4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PASTA-4.width-1250.png\" alt=\"PASTA-4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"00pr5\"><i>With the same starting prompt, \"An image of happiness\", PASTA produces dramatically different results for two distinct user types (User Type A and User Type B), showcasing its ability to adapt to an individual's unique creative style. For example, the result for Type A corresponds to a prompt like “Abstract happy faces, Art Deco inspired geometric shapes, muted jewel-toned background.”</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">What's next?</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"cmj9x\">PASTA shows that the future of generative AI can be more interactive, preference adaptive, and collaborative. The methods we developed, particularly the use of robust user simulators, can be applied to many other generative tasks to create AI that better aligns and adapts to human users.</p><p data-block-key=\"1ads2\">To help spur further research, we have <a href=\"https://www.kaggle.com/datasets/googleai/pasta-data\" target=\"_blank\" rel=\"noopener noreferrer\">open-sourced</a> our sequential rater dataset and our simulated user data. We can't wait to see what the community builds with it.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"cmj9x\"><i>The author list is:</i> <i>Ofir Nabati,</i> <i>Guy Tennenholtz,</i> <i>ChihWei Hsu,</i> <i>Moonkyung Ryu,</i> <i>Deepak Ramachandran,</i> <i>Yinlam Chow,</i> <i>Xiang Li, and</i> <i>Craig Boutilier. Special thanks to Mark Simborg for his help crafting this blog post and Kimberly Schwede for creating the figures in this post.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Snapseed 推出交互式设备端分割功能 (原标题: Introducing interactive on-device segmentation in Snapseed)",
      "link": "https://research.google/blog/introducing-interactive-on-device-segmentation-in-snapseed/",
      "pubDate": "Tue, 30 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-30T16:00:00.000Z",
      "creator": "Google",
      "summary": "Snapseed 引入了名为“对象画笔”（Object Brush）的新功能，旨在简化移动设备上的选择性图像调整。这项功能通过强大的设备端人工智能模型“交互式分割器”（Interactive Segmenter）实现，使用户能够通过简单的笔触或点击，快速准确地选择并编辑照片中的特定对象。\n\n### 核心功能与用户体验\n\n*   **直观的对象编辑**：用户只需在想要编辑的对象上绘制一笔，模型便会在20毫秒内立即检测并选择完整的对象，生成精确匹配其边界的掩膜。这使得在不影响图像其他部分的情况下，对前景主体、天空或特定物品进行亮度、颜色等调整变得轻而易举。\n*   **实时反馈**：模型提供实时反馈，用户可以即时添加或删除选区，直到达到理想效果。\n*   **设备端运行**：整个过程完全在设备上运行，由 MediaPipe 和 LiteRT 的 GPU 加速提供支持，确保了快速流畅的体验。\n\n### 交互式分割器模型的技术原理\n\n“交互式分割器”旨在成为一个通用分割模型，不限于特定类别的对象或场景。其开发过程涉及以下关键步骤：\n\n1.  **师生模型训练（Teacher-Student Training）**：\n    *   **“交互式分割器：教师”（Interactive Segmenter: Teacher）**：首先，团队使用一个预训练且高度泛化的模型，通过对350多种不同对象类别的约30,000个高质量像素级图像掩膜进行微调，训练出一个高精度的交互式分割模型。该模型质量高，但由于速度和大小限制，不适合设备端使用。\n    *   **“交互式分割器：边缘”（Interactive Segmenter: Edge）**：为了实现设备端应用，团队开发了一个更小、更专业的模型。该模型通过知识蒸馏（Knowledge Distillation）从“教师”模型中学习，利用包含200多万张图像和数百个不同类别的弱标注数据集进行训练。在蒸馏过程中，“教师”模型会实时生成高质量的真值掩膜，并模拟用户提示（如前景涂鸦、背景涂鸦、点和框选），以训练“边缘”模型。\n\n    ![Table comparing Interactive Segmenter: Edge and Interactive Segmenter: Teacher models using the IOU metric.](https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-3-IOU.width-1250.png)\n    *   **提示生成**：为了模拟用户选择对象，模型在训练时会在真值掩膜内部绘制随机涂鸦作为前景提示，在外部绘制随机涂鸦作为背景提示，并模拟点击和套索选择。\n\n    ![Schematic of teacher–student training for Interactive Segmenter.](https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-4a-Training.width-1250.png)\n\n2.  **高精度与低延迟的平衡**：\n    *   为了在分割质量和实时交互延迟之间取得平衡，模型将图像理解和提示理解解耦为两个独立的子模型。\n    *   **重量级图像编码器**：对每张图像只运行一次，提取丰富的语义特征，并在用户开始使用交互式分割时立即运行，有效隐藏了延迟。\n    *   **轻量级交互式编码器-解码器**：在此预计算的特征上运行，接收用户的触摸提示并在20毫秒预算内生成最终分割掩膜。\n\n    ![Schematic of the Interactive Segmenter neural network architecture.](https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-5-Architecture.width-1250.png)\n    ![Table showing model inference latency when running Interactive Segmenter: Edge on-device.](https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-6-Latency.width-1250.png)\n\n3.  **图像尺寸掩膜上采样**：\n    *   为在高分辨率图像上保持最佳编辑质量，模型预测768x768分辨率的掩膜，并通过高效的 GPU 实现边缘保留联合双边上采样方法，将其上采样至图像原始分辨率（最高4K）。\n    *   为提高延迟，上采样仅在用户完成手势（抬起手指）后应用。\n\n    ![Comparison of original Interactive Segmenter mask and upsampled mask.](https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-7-Upsampling.width-1250.png)\n\n### 结论与展望\n\n“交互式分割器”的推出，使 Snapseed 的图像编辑变得前所未有的简单和强大。它将简单的点击和笔触转化为精确的选择，帮助用户轻松实现编辑想法。这项底层技术不仅已应用于 Chromebook Plus 14 的图库应用中的 AI 图像编辑功能，Google 还计划将其集成到更多图像和创意编辑产品中。",
      "shortSummary": "Snapseed 推出“对象画笔”功能，通过设备端 AI 模型“交互式分割器”实现快速、直观的对象选择和编辑。用户只需简单笔触，模型便能在20毫秒内精确分割图像中的对象。该技术采用师生模型蒸馏训练，平衡了分割质量与实时性，并支持高分辨率掩膜。这项创新使高级照片编辑更易用，并计划集成到更多 Google 图像和创意编辑产品中，包括已应用于 Chromebook Plus 14。",
      "translated_title": "Snapseed 推出交互式设备端分割功能",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-3-IOU.width-1250.png",
          "alt": "Table comparing Interactive Segmenter: Edge and Interactive Segmenter: Teacher models using the IOU metric.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-4a-Training.width-1250.png",
          "alt": "Schematic of teacher–student training for Interactive Segmenter.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-5-Architecture.width-1250.png",
          "alt": "Schematic of the Interactive Segmenter neural network architecture.",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-6-Latency.width-1250.png",
          "alt": "Table showing model inference latency when running Interactive Segmenter: Edge on-device.",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-7-Upsampling.width-1250.png",
          "alt": "Comparison of original Interactive Segmenter mask and upsampled mask.",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wunwg\">The key to elevating a good photo often lies in selective image adjustments: brightening a subject in the foreground, enhancing the sky, or making the color of a jacket pop. Yet, isolating specific elements with existing tools that offer subject, background, sky, or color-based selections has remained a frustrating and complex endeavor. This challenge has been particularly acute on mobile devices, where imprecise touch input and limited processing have made detailed selections and edits very difficult.</p><p data-block-key=\"978qd\">Now, we have made object-based image adjustments quick and easy. The new Object Brush in <a href=\"https://apps.apple.com/us/app/snapseed-photo-editor/id439438619\" target=\"_blank\" rel=\"noopener noreferrer\">Snapseed</a> on iOS, accessible in the \"Adjust\" tool, now lets you edit objects intuitively. It allows you to simply draw a stroke on the object you want to edit and then adjust how you want it to look, separate from the rest of the image. Give it a try as we roll this new capability out in the coming week!</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Snapseed-1-ObjBrush.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Selective editing using Snapseed's Object Brush.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Intuitive editing through interactive on-device segmentation</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">At its core, Object Brush is powered by our Interactive Segmenter, a powerful AI model that runs entirely on device. With a simple gesture — just a tap or tracing a quick line — you can choose an object or person in the frame. The model will then immediately detect and select the complete object or person, in less than 20ms. The model generates a mask for the object, which accurately matches its boundaries, whether it's a person, a pet, or the clouds in the sky. This real-time feedback lets you refine your selection on the fly, easily adding or subtracting areas until it's just right. This entire process is powered by <a href=\"https://ai.google.dev/edge/mediapipe/framework\" target=\"_blank\" rel=\"noopener noreferrer\">MediaPipe</a> and <a href=\"https://ai.google.dev/edge/litert/performance/gpu\" target=\"_blank\" rel=\"noopener noreferrer\">LiteRT’s GPU acceleration</a> for a fast and seamless experience.</p><p data-block-key=\"c4tan\">This powerful fusion of a simple, intuitive user interface with an effective and efficient machine learning model makes advanced photo editing more accessible, enjoyable, and more precise than ever before, all running seamlessly on your own device.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Snapseed-0-Hero.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Use foreground prompts (green) to select parts of an image and background prompts (red) to refine the selection.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Training the Interactive Segmenter model</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">The Interactive Segmenter model is designed to be a universally capable segmentation model, not limited to any specific class of objects or scenes. To avoid having to annotate large amounts of data to cover all areas, we chose to follow the <a href=\"https://arxiv.org/abs/1912.11370\" target=\"_blank\" rel=\"noopener noreferrer\">Big Transfer</a> approach and use a general pre-trained image encoder for pseudo-annotation to complement small amounts of manually annotated images.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Teacher for Interactive Segmenter</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">We started with a pre-trained and highly-generalizable model, fine-tuned for interactive segmentation. We took samples for 350+ different object categories and asked annotators to precisely annotate object masks with pixel-perfect quality. Through this process, we obtained ~30,000 high-quality image masks for these categories. While insufficient for direct training of a small mobile model, large pre-trained models can successfully be fine-tuned on this data to predict high accuracy masks. Using this dataset we trained an interactive segmentation model, which we call “Interactive Segmenter: Teacher”.</p><p data-block-key=\"2dclt\">Interactive Segmenter: Teacher produces high-quality segmentation masks; however, its speed and size hinder its use in on-device scenarios. To overcome this challenge, we developed “Interactive Segmenter: Edge”, a specialized model tailored for on-device use cases by leveraging the knowledge distilled from the original Interactive Segmenter: Teacher model.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Distillation</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">Since the on-device model is significantly smaller, it has limited generalization capabilities, and the 30,000 annotated images we used for fine-tuning aren't sufficient to train a new model. At the same time the small model size implies we won’t see significant gains from pre-training on different domains or tasks.</p><p data-block-key=\"bqvot\">For knowledge transfer from Interactive Segmenter: Teacher to Interactive Segmenter: Edge, we need millions of images and realistic prompts for a diverse range of object categories. So, we leveraged a large, weakly annotated dataset, which contains over 2 million images with masks across hundreds of different categories.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-3-IOU.width-1250.png\" alt=\"Table comparing Interactive Segmenter: Edge and Interactive Segmenter: Teacher models using the IOU metric.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-3-IOU.width-1250.png\" alt=\"Table comparing Interactive Segmenter: Edge and Interactive Segmenter: Teacher models using the IOU metric.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Interactive Segmenter: Edge yields a similar quality as Interactive Segmenter: Teacher for a given, fixed input prompt, as measured by the intersection over union (IOU) metric.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Prompt generation</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">The segmentation masks in the distillation dataset are not pixel-perfect, because they were generated through <a href=\"https://arxiv.org/pdf/1903.10830\" target=\"_blank\" rel=\"noopener noreferrer\">automated or semi-automated procedures</a>, and are not ideal for training high-quality segmenters. Nevertheless, they are suitable for creating realistic prompts for interactive segmentation. In this process, the ground truth mask is produced on-the-fly by Interactive Segmenter: Teacher, which acts as a teacher model in a process known as <a href=\"https://arxiv.org/abs/2106.05237\" target=\"_blank\" rel=\"noopener noreferrer\">knowledge distillation</a>. Importantly, both the teacher as well as the student model use the same prompts during training, ensuring consistency across models.</p><p data-block-key=\"6276u\">We attempt to simulate a user selecting objects in an image. We draw random scribbles within the (eroded) ground truth mask to get foreground prompts (i.e., what the user wants to select, shown in red in the image below) and random scribbles outside the ground truth mask to get background prompts (i.e., what the user explicitly does not want to select, shown in blue). We simulate tapping by drawing random points as well as random scribbles. Furthermore, to support lasso selection we also expose the model during training to box prompts around an object.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-4a-Training.width-1250.png\" alt=\"Schematic of teacher–student training for Interactive Segmenter.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-4a-Training.width-1250.png\" alt=\"Schematic of teacher–student training for Interactive Segmenter.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">By utilizing a teacher model we can train on data with low-quality ground truth annotations, reducing labeling costs without sacrificing model quality.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">High quality vs. low latency</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">A central challenge was reconciling the conflicting demands of segmentation quality versus real-time, interactive latency. To reach the right balance, we decouple image and prompt understanding into distinct sub-models. First, a powerful, heavyweight image encoder is run once per image to extract a rich set of semantic features. This image encoder can be run as soon as the user’s intent to use interactive segmentation becomes apparent, thus effectively hiding the latency from the user. Second, a lightweight interactive encoder-decoder operates on these pre-computed features. This network takes the user's touch prompts and generates the final segmentation mask, executing well under our 20ms budget. This separation into two models allows Interactive Segmenter to harness the image understanding of a large model while delivering the instantaneous responsiveness of a small one.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-5-Architecture.width-1250.png\" alt=\"Schematic of the Interactive Segmenter neural network architecture.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-5-Architecture.width-1250.png\" alt=\"Schematic of the Interactive Segmenter neural network architecture.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Interactive Segmenter neural network architecture.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-6-Latency.width-1250.png\" alt=\"Table showing model inference latency when running Interactive Segmenter: Edge on-device.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-6-Latency.width-1250.png\" alt=\"Table showing model inference latency when running Interactive Segmenter: Edge on-device.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Model inference latency when running Interactive Segmenter: Edge on-device.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wunwg\">The final student models (encoder + super decoder) are quantized to 8 bits and both run on <a href=\"https://ai.google.dev/edge/litert/performance/gpu\" target=\"_blank\" rel=\"noopener noreferrer\">LiteRT's GPU acceleration</a> with decoder inference latencies of 7.4ms on an iPhone 16 Pro, enabling seamless and intuitive image editing.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Image-size mask upsampling</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">To preserve the best image editing quality on high-resolution images, we need high-resolution segmentation masks. To achieve this, we train our segmentation model to predict a mask in 768x768 resolution and further upsample it to image resolution (capped at 4k to have it fit within a single GPU buffer). We use an efficient GPU implementation of the <a href=\"https://dl.acm.org/doi/10.1145/1276377.1276497\" target=\"_blank\" rel=\"noopener noreferrer\">edge-preserving joint-bilateral upsampling method</a>. To improve latency, we only apply upsampling once a user completes a gesture by lifting their finger.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-7-Upsampling.width-1250.png\" alt=\"Comparison of original Interactive Segmenter mask and upsampled mask.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-7-Upsampling.width-1250.png\" alt=\"Comparison of original Interactive Segmenter mask and upsampled mask.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Original Interactive Segmenter mask (<b>left</b>) and upsampled mask (<b>right</b>).</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">With the new Interactive Segmenter in <a href=\"https://apps.apple.com/us/app/snapseed-photo-editor/id439438619\" target=\"_blank\" rel=\"noopener noreferrer\">Snapseed</a> image editing has become easier and more powerful than ever. Simple taps and strokes are translated into accurate selections, allowing users to translate their editing ideas into reality. Download Snapseed for iOS <a href=\"https://apps.apple.com/us/app/snapseed-photo-editor/id439438619\" target=\"_blank\" rel=\"noopener noreferrer\">here</a> and let your photos shine. Object Brush will be rolled out to more tools in Snapseed in the coming months. The underlying model powers a wide range of image editing and manipulation tasks and serves as a foundational technology for intuitive selective editing. It has also been shipped in the new <a href=\"https://blog.google/products/chromebooks/lenovo-chromebook-plus-14/\" target=\"_blank\" rel=\"noopener noreferrer\">Chromebook Plus 14</a> to power AI image editing in the Gallery app. Next, we plan to integrate it across more image and creative editing products at Google.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\"><i>Special thanks to all members who worked on the tech with us: Valentin Bazarevsky, Daniel Fenner, Lutz Justen, Ronald Wotzlaw, Tai-Yu Daniel Pan, Jason Chang, Matthew Harries, Giles Ochs, Jonathan Horsman, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Karthik Raveendran, Matsvei Zhdanovich, Mogan Shieh, Chris Parsons, Jianing Wei, and Matthias Grundmann.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "AI作为研究伙伴：利用AlphaEvolve推进理论计算机科学 (原标题: AI as a research partner: Advancing theoretical computer science with AlphaEvolve)",
      "link": "https://research.google/blog/ai-as-a-research-partner-advancing-theoretical-computer-science-with-alphaevolve/",
      "pubDate": "Mon, 29 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-29T16:00:00.000Z",
      "creator": "Google",
      "summary": "大型语言模型（LLMs）在竞技数学和编程中展现出卓越能力，但在数学发现（如证明新定理或揭示新组合结构）方面相对较少成功，因为数学和理论计算机科学要求绝对的正确性。任何AI驱动的数学发现方法都必须提供可计算验证的正确性证明，或由领域专家进行认证。\n\n## AlphaEvolve：LLM驱动的数学发现工具\n\nGoogle DeepMind的最新研究论文《组合结构强化生成：在复杂性理论中的应用》展示了LLM驱动的编码智能体如何帮助发现新的数学结构，从而推动复杂性理论（理论计算机科学的一个子领域）的理解边界。这项工作利用了AlphaEvolve系统，该系统通过以下方式迭代进化代码：\n\n*   **反馈循环：** AlphaEvolve从代码片段集合开始，评估这些代码片段生成的结构。\n*   **LLM驱动的优化：** 利用LLM将最成功的代码片段演变为更好的解决方案。\n\n这种方法在复杂性理论的两个不同领域取得了新成果：\n\n1.  改进了MAX-4-CUT问题（将节点划分为四个集合的最大割问题）近似不可行性（即我们近似结果能力的极限）的最新界限。\n2.  收紧了认证随机图性质的平均情况硬度界限。\n\n## AI辅助数学研究的模式\n\nAI辅助数学研究主要有两种模式：\n\n*   **LLM直接生成：** 人类调用LLM来总结文献、规划新定理的研究路线，或直接生成部分（或全部）证明。\n*   **AI工具辅助生成：** 人类使用AlphaEvolve等AI工具生成更好的证明元素。本文的工作属于第二类，AlphaEvolve生成的证明元素可以由计算机程序自动验证。\n\n## 提升的力量：从有限构造到普遍陈述\n\n将AI用于理论计算机科学研究的一个根本挑战在于所研究问题的普遍性。AI系统可能找到特定问题实例的解决方案（例如，50个特定城市的旅行推销员最优路线），但计算机科学家通常寻求对所有问题实例和大小都普遍成立的定理（表示为∀n）。\n\n为了使AlphaEvolve能够证明普遍陈述，研究采用了“提升”（lifting）技术。如果将证明视为一个长字符串，可以将证明中的一个片段（对应于某个有限结构）进行演化，以支持一个更强的普遍陈述，同时保持与证明其余部分的接口不变。这种方法的优势在于，要认证整体正确性，只需认证已演化的有限结构的正确性。\n\n![提升：使用AI对有限结构进行变形，同时保持与更广泛证明的接口不变。](https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-1-Lifting.width-1250.png)\n\n在复杂性理论中，研究人员经常使用依赖于特定高度优化有限结构的既定证明框架。如果能找到更好的结构，整个证明框架就会将这种改进“提升”为一个更好的普遍结果。一个关键例子是“小工具归约”（gadget reduction），通过将已知难解的源问题映射到目标问题来证明目标问题的计算难度。小工具是局部转换源问题片段到目标问题片段的方法，它们是有限结构，找到最优小工具通常是一个艰苦的手工过程。通过让AlphaEvolve寻找更好的小工具，研究人员发现了比以往已知更复杂的结构。这些有限的发现，当插入到现有数学框架中时，立即产生了复杂性理论中的新普遍定理。\n\n## 复杂性理论中的新定理\n\n### MAX-4-CUT：新的最先进成果\n\n研究人员将此方法应用于MAX-k-CUT问题。给定一个图，目标是将节点划分为k个不同的集合，使不同集合之间交叉的边数最大化。这是一个经典的难解（NP-hard）问题，因此研究重点是近似算法。关键问题是：近似的极限是什么？\n\n对于MAX-4-CUT（划分为四个集合），之前最佳结果证明其近似解的因子为0.9883是NP-hard。AlphaEvolve被部署来寻找MAX-4-CUT的新小工具归约。系统发现了一个涉及19个变量（节点）的复杂小工具，具有复杂的加权方案。这一发现确立了新的近似不可行性界限为0.987。尽管这一改进看似微小，但在近似硬度这一成熟领域，此类进展通常需要重大的新技术或组合洞察力。\n\n![AlphaEvolve为MAX-4-CUT归约发现的小工具的图示。](https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-2-Gadget.width-1250.png)\n\n### 平均情况硬度和Ramanujan图\n\n研究还探讨了问题在平均情况下的硬度，而非最坏情况。具体来说，研究了稀疏随机图的MAX-2-CUT（以及最大独立集）界限认证的难度。最近的工作将此问题与特定Ramanujan图的存在联系起来——这些确定性图“看起来”像稀疏随机图。他们推测，具有异常大割的Ramanujan图的存在意味着认证随机图的MAX-2-CUT在计算上是困难的。先前的工作使用计算机辅助找到了多达10个节点的此类图。改进他们的结果需要找到更多节点上更极端的Ramanujan图，这极其难以发现和验证。AlphaEvolve成功地在这一广阔的搜索空间中导航，发现了多达163个节点上具有更大割的Ramanujan图。\n\n![AlphaEvolve发现的具有大2-割的4-正则Ramanujan图。](https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-3-4RegGraph.width-1250.png)\n\n这些发现显著改善了平均情况硬度的下限。此外，结合新的算法进展（非AI驱动），研究人员能够将这些计算硬度问题几乎解决，将上限和下限匹配到小数点后第三位。\n\n## 验证正确性的关键作用\n\n这项工作的一个关键区别在于其结果附带了正确性证明。当LLM被直接提示生成数学证明时，它通常会产生一个需要大量人工干预才能验证和完成的证明草图或论证。幻觉或细微错误可能使输出无用。数学的正确性标准是绝对的。相比之下，这里采用的方法是利用AI发现证明中的结构，而不是证明本身。最终定理的有效性依赖于两个组成部分：提升框架的正确性，以及所发现结构的验证。虽然框架是健全的，但验证AlphaEvolve发现的结构在计算上是密集的。值得注意的是，AlphaEvolve通过实施复杂的剪枝（branch-and-bound）策略和系统级优化，将验证过程加速了10,000倍。这一巨大的加速是研究的关键推动因素，使得系统能够探索更大、更复杂的小工具。最重要的是，最终发现的小工具仍然使用原始的暴力算法进行验证，确保了定理的绝对正确性。\n\n## AI辅助理论的未来\n\n尽管这些初步研究结果远非结论性的，但它们表明AI有望成为数学发现中一个有益的合作者。研究人员观察到AlphaEvolve中的模型生成了复杂的数学对象，有时展现出初步的推理能力。然而，随着我们进入一个证明可能越来越多地归因于AI的时代，验证这项关键任务将成为一个重要的瓶颈。",
      "shortSummary": "Google DeepMind的AlphaEvolve系统利用大型语言模型（LLM）作为研究伙伴，通过迭代进化代码，在理论计算机科学领域取得了显著进展。该系统通过“提升”技术，将有限结构演化为更强的通用定理。具体而言，它改进了MAX-4-CUT问题的近似不可行性界限至0.987，并发现了具有更大割的Ramanujan图，从而收紧了稀疏随机图平均情况硬度的界限。所有AI发现的结构都经过严格计算验证，确保了数学结果的绝对正确性，展现了AI在数学发现中的巨大潜力。",
      "translated_title": "AI作为研究伙伴：利用AlphaEvolve推进理论计算机科学",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-1-Lifting.width-1250.png",
          "alt": "Lifting: Morphing finite structures using AI, while keeping the interface to the broader proof intact.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-2-Gadget.width-1250.png",
          "alt": "Drawing of a graph representing the gadget found by AlphaEvolve for the reduction to MAX-4-CUT.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-3-4RegGraph.width-1250.png",
          "alt": "Drawing of a 4-regular Ramanujan graph with large 2-cut found by AlphaEvolve.",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"c47sg\">Recently, large language models (LLMs) have demonstrated surprising capabilities in <a href=\"https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/\" target=\"_blank\" rel=\"noopener noreferrer\">competitive mathematics</a> and <a href=\"https://deepmind.google/discover/blog/gemini-achieves-gold-level-performance-at-the-international-collegiate-programming-contest-world-finals/\" target=\"_blank\" rel=\"noopener noreferrer\">competitive programming</a>, demonstrating world-leading performance across both of these fields. However, their successes in mathematical discovery — proving novel theorems or uncovering new combinatorial structures — have been relatively few (with some notable exceptions [<a href=\"https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/\" target=\"_blank\" rel=\"noopener noreferrer\">1</a>, <a href=\"https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/\" target=\"_blank\" rel=\"noopener noreferrer\">2</a>, <a href=\"https://arxiv.org/pdf/2505.20219#page=8.61\" target=\"_blank\" rel=\"noopener noreferrer\">3</a>]). Since mathematics and theoretical computer science demand absolute correctness<footnote id=\"94fb5494-00a3-46e9-8265-43676afdf080\">[94fb54]</footnote>, any AI-based method that makes mathematical discovery must either have a proof of correctness that can be confirmed computationally (without any human involvement), or have a domain-expert human in the loop to certify correctness.</p><p data-block-key=\"ebg0u\">In our recent paper, “<a href=\"https://arxiv.org/abs/2509.18057\" target=\"_blank\" rel=\"noopener noreferrer\">Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory</a>”, we demonstrate how an LLM-powered coding agent can help discover new mathematical structures that push the boundaries of our understanding of <a href=\"https://en.wikipedia.org/wiki/Computational_complexity_theory\" target=\"_blank\" rel=\"noopener noreferrer\">complexity theory</a> (a sub-field of theoretical computer science). Our work utilizes <a href=\"https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/\" target=\"_blank\" rel=\"noopener noreferrer\">AlphaEvolve</a>, a system developed at Google DeepMind that uses LLMs to iteratively evolve code. By employing a feedback loop, AlphaEvolve began with populations of code snippets, evaluated the structures produced by the code snippets, and used an LLM to morph the most successful snippets toward better solutions. This approach led to new results in two distinct areas of complexity theory: 1) improving the state-of-the-art for the limit on our ability to approximate the outcome (i.e., the \"inapproximability\") of the <a href=\"https://en.wikipedia.org/wiki/Maximum_cut\" target=\"_blank\" rel=\"noopener noreferrer\">maximum cut problem</a> for 4 slices (which we define as the <a href=\"https://arxiv.org/pdf/2509.18057#page=8\" target=\"_blank\" rel=\"noopener noreferrer\">MAX-4-CUT problem</a>), and 2) tightening the bounds on the <a href=\"https://arxiv.org/pdf/2509.18057#page=6\" target=\"_blank\" rel=\"noopener noreferrer\">average-case hardness of certifying properties of random graphs</a>.</p><p data-block-key=\"1oij\">AI-assisted mathematical research can operate in the following modes:</p><ol><li data-block-key=\"7g1t3\">A person invokes an LLM to summarize the literature, to chart a research plan towards new theorems, or to directly generate chunks of (or entire) proofs.</li><li data-block-key=\"30u56\">A person uses AI-derived tools, such as AlphaEvolve, to generate better proof elements.</li></ol><p data-block-key=\"duvp5\">Our work falls in the second category, where we obtain better proof elements using AlphaEvolve that can be automatically verified by a computer program.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The power of lifting: From finite constructions to universal statements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c47sg\">A fundamental challenge in using AI for theoretical computer science research lies in the universal nature of the problems studied. An AI system might find a solution to a specific instance of a problem — say, the <a href=\"https://en.wikipedia.org/wiki/Travelling_salesman_problem\" target=\"_blank\" rel=\"noopener noreferrer\">optimal route for a traveling salesman</a> visiting 50 specific cities. However, computer scientists often seek theorems that hold true universally for <i>all</i> problem instances and sizes (denoted as ∀n).</p><p data-block-key=\"2fqu\">How can we use AlphaEvolve to prove a universal statement? The answer lies in a technique known as \"lifting\" (see image below). If a proof is viewed as a long string, then one can take a chunk of the proof (corresponding to a certain finite structure), and evolve it to support a stronger universal statement, while keeping the interface to the rest of the proof intact. The advantage of this approach is that to certify overall correctness, one needs to only certify the correctness of the finite structure that has been evolved.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-1-Lifting.width-1250.png\" alt=\"Lifting: Morphing finite structures using AI, while keeping the interface to the broader proof intact.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-1-Lifting.width-1250.png\" alt=\"Lifting: Morphing finite structures using AI, while keeping the interface to the broader proof intact.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"g7wnv\">Lifting: Morphing finite structures using AI, while keeping the interface to the broader proof intact.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"c47sg\">In complexity theory, researchers often use established proof frameworks that rely on the existence of specific, highly optimized finite structures. If a better structure can be found, the entire proof framework \"lifts\" this improvement to a better universal result.</p><p data-block-key=\"f1cfa\">A key example of this is a \"<a href=\"https://people.csail.mit.edu/madhu/papers/1996/gadgets-journ.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">gadget reduction</a>.\" To prove that a target problem is computationally hard (intractable), researchers try to map a known intractable source problem to it, hence demonstrating that the target problem is at least as hard as the source problem. A gadget is a recipe for locally transforming a small piece of the source problem into a piece of the target problem. These gadgets are finite structures, and finding the optimal gadget is a painstaking process often done by hand.</p><p data-block-key=\"6na9u\">By tasking AlphaEvolve with finding better gadgets, we were able to discover structures far more complex than those previously known. These finite discoveries, when plugged into the existing mathematical frameworks, immediately yield new universal theorems in complexity theory.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">New theorems in complexity theory</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c47sg\">We applied this methodology to the MAX-k-CUT problem. Given a <a href=\"https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)\" target=\"_blank\" rel=\"noopener noreferrer\">graph</a> (a network of nodes and edges), the goal is to partition the nodes into <i>k</i> distinct sets such that the number of edges crossing between different sets is maximized. This is a classic intractable (<a href=\"https://en.wikipedia.org/wiki/NP-hardness\" target=\"_blank\" rel=\"noopener noreferrer\">NP-hard</a>) problem, meaning we do not expect to find efficient algorithms that solve it exactly. Therefore, we focused on <i>approximation algorithms</i> — those that efficiently find solutions guaranteed to be close to the optimum.</p><p data-block-key=\"62kuh\">The crucial question is: what is the limit of approximation?</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">MAX-4-CUT: A new state of the art</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c47sg\">For MAX-4-CUT (partitioning into four sets), the previous best-known result proved that it is NP-hard to approximate the solution within a factor of <a href=\"https://arxiv.org/pdf/2509.18057#page=8\" target=\"_blank\" rel=\"noopener noreferrer\">0.9883</a>. AlphaEvolve was deployed to search for a new gadget reduction to MAX-4-CUT.</p><p data-block-key=\"9bab8\">The system discovered an intricate gadget involving 19 variables (nodes) with a complex weighting scheme (some connections having up to 1429 times the weight of others). This discovery established a new inapproximability bound of 0.987.</p><p data-block-key=\"1m1ig\">This improvement may seem incremental, but in the mature field of hardness of approximation, such advances often require significant new techniques or combinatorial insights.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-2-Gadget.width-1250.png\" alt=\"Drawing of a graph representing the gadget found by AlphaEvolve for the reduction to MAX-4-CUT.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-2-Gadget.width-1250.png\" alt=\"Drawing of a graph representing the gadget found by AlphaEvolve for the reduction to MAX-4-CUT.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"g7wnv\">Gadget found by AlphaEvolve for the reduction to MAX-4-CUT.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Average-case hardness and Ramanujan graphs</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c47sg\">We also explored the hardness of problems <i>on average</i>, rather than in the worst case. Specifically, we studied the difficulty of certifying bounds on the MAX-2-CUT (as well as <a href=\"https://en.wikipedia.org/wiki/Independent_set_(graph_theory)\" target=\"_blank\" rel=\"noopener noreferrer\">maximum independent set</a>) of sparse random graphs<footnote id=\"3d54ef4b-a9db-4307-9987-e308f9f08c3b\">[3d54ef]</footnote>. <a href=\"https://arxiv.org/abs/2404.17012\" target=\"_blank\" rel=\"noopener noreferrer\">Recent work</a> connected this problem to the existence of specific <a href=\"https://en.wikipedia.org/wiki/Ramanujan_graph\" target=\"_blank\" rel=\"noopener noreferrer\">Ramanujan graphs</a> — deterministic graphs that “look” like sparse random graphs. They conjectured that the existence of Ramanujan graphs with unnaturally large cuts implies it is computationally hard to certify the MAX-2-CUT of a random graph.</p><p data-block-key=\"3nhje\">Prior work used computer assistance to find such graphs on up to 10 nodes. Improving their results requires finding more extremal Ramanujan graphs on many more nodes, which are exceedingly difficult to find and verify. AlphaEvolve successfully navigated this vast search space, discovering Ramanujan graphs with even larger cuts on as many as 163 nodes.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-3-4RegGraph.width-1250.png\" alt=\"Drawing of a 4-regular Ramanujan graph with large 2-cut found by AlphaEvolve.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-3-4RegGraph.width-1250.png\" alt=\"Drawing of a 4-regular Ramanujan graph with large 2-cut found by AlphaEvolve.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"g7wnv\">A 4-regular Ramanujan graph with large 2-cut found by AlphaEvolve.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"c47sg\">These discoveries significantly improved the lower bounds for average-case hardness. Furthermore, combined with new algorithmic progress (non-AI based), we were able to nearly settle the computational hardness of these questions, matching the upper and lower bounds to within the third decimal place.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The crucial role of verified correctness</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c47sg\">A critical distinction of this work is that the results come with proofs of correctness.</p><p data-block-key=\"8afn7\">When an LLM is prompted to generate a mathematical proof directly, it often produces a proof sketch or an argument that requires substantial human intervention to verify and complete. Hallucinations or subtle errors can render the output useless. As mentioned earlier, the standard for correctness in math is absolute.</p><p data-block-key=\"48i4d\">In contrast, the approach taken here uses AI to discover a <i>structure</i> within the proof, not the proof itself. The validity of the final theorem relies on two components: the correctness of the lifting framework, and the verification of the discovered structure. While the frameworks are sound, verifying the structures discovered by AlphaEvolve is computationally intensive.</p><p data-block-key=\"5d7lt\">Remarkably, AlphaEvolve achieved a 10,000x speedup in the verification process by implementing sophisticated <a href=\"https://en.wikipedia.org/wiki/Branch_and_bound\" target=\"_blank\" rel=\"noopener noreferrer\">branch-and-bound</a> strategies and system-level optimizations. This massive speedup was the key enabler for the research, allowing the system to explore much larger and more complex gadgets.</p><p data-block-key=\"fpuet\">Crucially, the final gadgets discovered were still verified using the original, brute-force algorithm, ensuring the absolute correctness of the theorems.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The future of AI-assisted theory</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c47sg\">While these initial research findings are far from conclusive, they suggest that AI is poised to become a helpful collaborator in mathematical discovery. We have observed the models in AlphaEvolve generate intricate mathematical objects that at times exhibit nascent reasoning capabilities. However, as we transition into an era where proofs may increasingly be attributed to AI, the crucial task of verification is set to become a significant bottleneck.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c47sg\"><i>We would like to thank Adam Zsolt Wagner, Swarat Chaudhuri, Pasin Manurangsi and Sushant Sachdeva for helping us during various stages of the project.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "个人健康代理的剖析 (原标题: The anatomy of a personal health agent)",
      "link": "https://research.google/blog/the-anatomy-of-a-personal-health-agent/",
      "pubDate": "Mon, 29 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-29T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 个人健康代理 (PHA) 框架：个性化健康支持的未来\n\n随着大型语言模型 (LLM) 和可穿戴设备数据的快速发展，为个人健康旅程提供支持迎来了变革性机遇。然而，由于个人健康需求的多样性和复杂性，单一系统难以有效应对。为此，研究人员提出了**个人健康代理 (Personal Health Agent, PHA)**，这是一个全面的研究框架，旨在通过多模态数据推理，提供个性化、循证的健康指导。\n\n### PHA 的多代理架构\n\nPHA 采用多代理架构，将个人健康和健康支持分解为三个核心角色，每个角色由一个专业的子代理处理：\n\n1.  **数据科学 (DS) 代理**：负责分析个人时间序列数据（如可穿戴设备数据和血液生物标志物），提供情境化的数值洞察。它通过两阶段数据科学模块增强基础模型，能够解释模糊的用户查询，并将其转化为稳健的统计分析计划，然后生成并执行代码以得出有效的数据驱动答案。\n2.  **领域专家 (DE) 代理**：作为可靠的健康和健康知识来源。它通过多步骤推理框架和工具箱（包括访问 NCBI 等权威来源）增强基础模型，确保信息准确可信，并能根据用户的特定情况（如既往病史）定制信息。\n3.  **健康教练 (HC) 代理**：旨在通过多轮对话支持用户设定目标并促进持久的行为改变。它采用受心理学策略（如动机性访谈）启发的模块化架构，以实现更自然有效的互动。\n\n### 用户中心设计\n\n为了构建一个真正满足多样化需求的代理，研究团队采用了用户中心设计流程。他们综合了来自在线健康论坛、500 多名用户的调查数据以及与设计和工程专家研讨会的见解，识别出人们在四个关键领域需要支持：理解一般健康主题、解释个人数据、获取可操作的健康建议以及评估症状。这促使 PHA 系统被设计成类似于人类专家团队的协作模式。\n\n![用户中心流程](https://storage.googleapis.com/gweb-research2023-media/images/PHA2_Process.width-1250.png)\n*一个用户中心流程，用于识别关键用户旅程。*\n\n### 综合评估框架\n\n为了验证 PHA 系统，研究人员开发了一个全面的多层次评估框架。他们首先针对每个子代理的独特核心能力与最先进的 LLM 基础模型进行了基准测试，然后评估了完全集成后的 PHA 的整体效能。评估涉及自动化和广泛的人工评估，涵盖 10 项基准任务，投入了超过 1,100 小时来自最终用户和健康专家的努力，以评估其在真实、多模态对话中的表现。\n\n![综合评估描述](https://storage.googleapis.com/gweb-research2023-media/images/PHA3_Table.width-1250.png)\n*对单个子代理和最终个人健康代理 (PHA) 系统进行综合评估的描述。*\n\n#### 子代理评估结果：\n\n*   **数据科学代理 (DS Agent)**：在分析计划质量方面显著优于基础模型（75.6% 对 53.7%），并且在生成准确、可执行的代码方面更可靠。\n    ![DS代理评估结果](https://storage.googleapis.com/gweb-research2023-media/images/PHA4_DSAgent.width-1250.png)\n    *DS代理：由人类数据科学家和自动评估器评估的DS代理和基础模型生成的六个维度数据分析计划的评估结果。*\n\n*   **领域专家代理 (DE Agent)**：在所有基准测试中始终优于基础模型。临床医生认为其多模态健康数据摘要在临床上更具相关性和实用性，最终用户认为其响应更具个性化和可信度。\n    ![DE代理评估结果](https://storage.googleapis.com/gweb-research2023-media/images/PHA5_DEAgent.width-1250.png)\n    *DE代理：由临床专家评估的DE代理和基础模型在七个临床维度上的多模态推理评估结果。*\n\n*   **健康教练代理 (HC Agent)**：在最终用户和健康教练专家的评估中，其能力显著优于基线模型，尤其在对话体验、目标导向有效性和动机支持方面表现出色。\n    ![HC代理评估结果](https://storage.googleapis.com/gweb-research2023-media/images/PHA6_HCAgent.width-1250.png)\n    *HC代理：由人类健康和教练专家评估的HC代理和基础模型在六个维度上的教练体验评估结果。*\n\n### 协作式 PHA：一个协调的团队\n\nPHA 框架通过智能协调器将这三个专业代理整合为一个有凝聚力的团队。当用户提出查询时，协调器会分析用户需求，动态分配“主”代理和“支持”代理，并促进协作、反思和记忆更新的迭代工作流，以综合生成一个全面响应。这种协作方法被证明显著优于其各部分的总和。\n\n在评估代理综合个人健康数据以帮助用户回答健康问题和实现个人健康目标的能力时，最终用户和健康专家都更倾向于 PHA，而非单一代理系统或简单的并行多代理基线。PHA 在大多数情况下被评为最佳整体系统，这强调了模仿人类专家团队协作结构对于提供真正有益支持的关键价值。\n\n![PHA评估结果1](https://storage.googleapis.com/gweb-research2023-media/images/PHA8_Results1body.width-1250.png)\n*PHA：由人类专家评估的PHA和其他基线生成的响应的评估结果。*\n\n![PHA评估结果2](https://storage.googleapis.com/gweb-research2023-media/images/PHA9_Results2.width-1250.png)\n*PHA：由人类专家评估的PHA和其他基线生成的响应的排名结果。*\n\n### 智能个人健康代理的未来\n\n这项研究为设计下一代个人健康 AI 提供了一个经过验证的概念蓝图，倡导从单一模型转向模块化、协作式系统，使其更值得信赖、更具连贯性和实用性。\n\n**重要提示**：此工作概述了一个用于研究目的的概念框架，不应被视为对任何当前正在开发或向公众提供的特定产品、服务或功能的描述。任何实际应用都将经过单独的设计、验证和审查流程。",
      "shortSummary": "个人健康代理（PHA）是一个基于大型语言模型和可穿戴设备数据的研究框架，旨在提供个性化、循证的健康指导。它采用多代理架构，由数据科学、领域专家和健康教练三个专业子代理组成，通过智能协调器协同工作。PHA 经过全面的用户中心设计和评估，结果表明其在处理复杂健康需求方面显著优于单一系统和并行多代理基线，为未来模块化、协作式个人健康 AI 奠定了基础。此框架目前仅用于研究目的。",
      "translated_title": "个人健康代理的剖析",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PHA2_Process.width-1250.png",
          "alt": "PHA2_Process",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PHA3_Table.width-1250.png",
          "alt": "PHA3_Table",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PHA4_DSAgent.width-1250.png",
          "alt": "PHA4_DSAgent",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PHA5_DEAgent.width-1250.png",
          "alt": "PHA5_DEAgent",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PHA6_HCAgent.width-1250.png",
          "alt": "PHA6_HCAgent",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"1sp1g\">The rapid advancement of large language models (LLMs), combined with <a href=\"https://research.google/blog/sensorlm-learning-the-language-of-wearable-sensors/\">data from wearable devices</a>, presents a transformative opportunity to empower people on their personal health journeys. However, health needs vary from individual to individual. Answering a specific query, such as, \"On average, how many hours have I been sleeping this last month?\" requires different skills than an open-ended question like, \"What can I do to improve my sleep quality?\" A single system can struggle to address this complexity.</p><p data-block-key=\"8dbs6\">To meet this challenge, we adopt a human-centered process and propose the <a href=\"https://arxiv.org/abs/2508.20148\" target=\"_blank\" rel=\"noopener noreferrer\">Personal Health Agent</a> (PHA). This agent is a comprehensive research framework that can reason about multimodal data to provide personalized, evidence-based guidance. Using a multi-agent architecture, PHA deconstructs personal health and wellness support into three core roles (data science, domain expert, and health coach), each handled by a specialist sub-agent. To evaluate each sub-agent and the multi-agent system, we leveraged <a href=\"https://arxiv.org/abs/2505.03784\" target=\"_blank\" rel=\"noopener noreferrer\">a real-world dataset</a> from an IRB-reviewed study where ~1200 users provided informed consent to share their wearables data from Fitbit, a health questionnaire, and blood test results. We conducted automated and human evaluations across 10 benchmark tasks, involving more than 7,000 annotations and 1,100 hours of effort from health experts and end-users. Our work represents the most comprehensive evaluation of a health agent to date and establishes a strong foundation towards the futuristic vision of a personal health agent accessible to everyone.</p><p data-block-key=\"fdbrn\">This work outlines a conceptual framework for research purposes, and should not be considered a description of any specific product, service, or feature currently in development or available to the public. Any real-world application would be subject to a separate design, validation, and review process.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/PHA1_Illustration.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"7rp20\"><i>An illustration of the internal functions of Personal Health Agent (PHA) that enable it to support personal health needs.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"3xfzh\">User-centered design for personal health needs</h2><p data-block-key=\"44e8q\">To build an agent that truly meets these diverse needs, we started with a user-centered design process. We synthesized insights from over 1,300 real-world health queries from online sources, such as health forums, survey data from more than 500 users, and a workshop with design and engineering experts. This research revealed four critical areas where people need support: understanding general health topics, interpreting their personal data, getting actionable wellness advice, and assessing symptoms. This insight led us to design the PHA system that resembles human expert teams, including data scientists, domain experts, and personal health coaches.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA2_Process.width-1250.png\" alt=\"PHA2_Process\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA2_Process.width-1250.png\" alt=\"PHA2_Process\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"enxez\"><i>A user-centered process to identify critical user journeys.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"v6xwt\">Evaluation of our proposed system</h2><p data-block-key=\"690u2\">To validate our system, we developed a holistic, multi-level evaluation framework. We first benchmarked each individual sub-agent on their unique core capabilities against the state-of-the-art LLM model as the base model, and then assessed the fully integrated PHA’s overall efficacy. The table below shows our comprehensive evaluation, which involved both automated and extensive human evaluations across 10 benchmark tasks, incorporating over 1,100 hours of effort from both end-users and health experts to assess performance in realistic, multi-modal conversations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA3_Table.width-1250.png\" alt=\"PHA3_Table\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA3_Table.width-1250.png\" alt=\"PHA3_Table\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"enxez\"><i>Description of our comprehensive evaluation of individual sub-agents and the final Personal Health Agent (PHA) system.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"v6xwt\">The data science agent: Personal data analyst</h2><p data-block-key=\"7nd9l\">The first specialist is the data science (DS) agent, which analyzes personal time-series data from wearables plus health data, such as blood biomarkers, to provide contextualized numerical insights. The DS agent builds on top of a base model (e.g., Gemini) and is enhanced by a two-stage data science module: Stage 1) interpret underspecified and ambiguous user queries (e.g., “Am I getting more fit recently?”), and Stage 2) translate them into robust statistical analysis plans. It then generates and executes code to produce a statistically valid, data-driven answer.</p><p data-block-key=\"eeeci\">We developed two auto-evaluation benchmarks for each stage of the DS agent's workflow. For the first stage, analysis planning, we used an auto-evaluator trained on 354 query-analysis plans curated by 10 expert data scientists. Based on a detailed rubric assessing dimensions like data sufficiency, statistical validity, and alignment with the user's query, our evaluations showed that the DS agent significantly outperforms the base model in creating high-quality analysis plans (achieving a 75.6% score vs. 53.7% for the baseline). For the second stage, code generation, the agent’s output was benchmarked against 173 rigorous unit tests written by data scientists. This confirmed the agent is more reliable at generating accurate, executable code used to derive insights from time-series wearable data.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA4_DSAgent.width-1250.png\" alt=\"PHA4_DSAgent\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA4_DSAgent.width-1250.png\" alt=\"PHA4_DSAgent\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"enxez\"><i>DS agent: Results of evaluating data analysis plan generated by the DS agent and the base model across six dimensions, as evaluated by human data scientist and auto raters.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"v6xwt\">The domain expert agent: Grounded, trustworthy knowledge</h2><p data-block-key=\"a5n8f\">Next is the domain expert (DE) agent, which functions as a reliable source of health and wellness knowledge. In a high-stakes domain like health and wellbeing, ensuring information is accurate and trustworthy is critical. The DE agent enhances a base model by using a multi-step reasoning framework and a toolbox that includes access to authoritative sources, such as the <a href=\"https://pmc.ncbi.nlm.nih.gov/tools/developers/\" target=\"_blank\" rel=\"noopener noreferrer\">National Center for Biotechnology Information</a> (NCBI) database, to ground its responses in <i>verifiable facts</i>. It excels at tailoring information to a user’s specific profile, such as pre-existing conditions. We developed two auto-evaluation benchmarks to test the DE agent’s medical knowledge (one evaluating our agent’s performance on board certification and coaching exam questions, and one for providing accurate differential diagnosis). We further developed two human-evaluation benchmarks (one for clinicians, and one for consumers) to measure the DE agent’s capability on personalization and multi-modal reasoning. Our DE Agent consistently outperforms the base model across all benchmarks. For instance, clinicians rated the DE agent's summaries of multimodal health data as significantly more clinically relevant and useful, and end-users found its responses to be substantially more personalized and trustworthy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA5_DEAgent.width-1250.png\" alt=\"PHA5_DEAgent\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA5_DEAgent.width-1250.png\" alt=\"PHA5_DEAgent\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"enxez\"><i>DE agent: Results of evaluating multi-modal reasoning of the DE agent and the base model across seven clinical dimensions, as evaluated by clinical experts.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"v6xwt\">The health coach agent: Guiding behavior change</h2><p data-block-key=\"c2d6v\">The third specialist is the health coach (HC) agent, which is designed to support users in setting goals and fostering lasting behavioral change through multi-turn conversations. Effective coaching requires a delicate balance between gathering information and providing actionable advice. The HC agent employs a modular architecture inspired by proven psychological strategies (e.g., <i>motivational interviewing</i>) to navigate this dynamic, leading to more natural and effective interactions. We benchmarked the HC agent’s performance in two human-evaluation setups, one with end-users and the other with health coaching experts, evaluating our model’s ability across several key areas. For the end-user evaluation, we focused on conversational experience, goal-oriented effectiveness, and motivational support. For the expert evaluation, we assessed adherence to professional coaching principles, recommendation quality, and agent credibility. Both evaluation aspects indicate that the HC agent is significantly more capable than the baseline<i>,</i> underscoring a key insight from our research: for coaching agents, users prioritize core competency and the ability to provide actionable guidance.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA6_HCAgent.width-1250.png\" alt=\"PHA6_HCAgent\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA6_HCAgent.width-1250.png\" alt=\"PHA6_HCAgent\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"enxez\"><i>HC agent: Results of evaluating coaching experience of the HC agent and base model on six dimensions, evaluated by human health and coaching experts.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"v6xwt\">The Personal Health Agent (PHA): A collaborative team</h2><p data-block-key=\"94ato\">While each agent is powerful alone, the true potential is realized when they collaborate. The Personal Health Agent (PHA) framework integrates these three specialists into a cohesive team managed by an intelligent orchestrator. When a user poses a query, the orchestrator analyzes the user's need, dynamically assigns a \"main\" agent and \"supporting\" agents, and facilitates an iterative workflow of collaboration, reflection, and memory updates to synthesize a single, comprehensive response.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/PHA7_Animation.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"enxez\"><i>A technical breakdown of the DS, DE, and HC agents, with orchestration into the Personal Health Agent (PHA).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"v6xwt\">This collaborative approach proved to significantly outperform the sum of its parts. In extensive evaluations of rubrics assessing agents' capability in synthesizing personal health data to help users answer their health and wellness queries, as well as achieving personal health goals, both end-users and health experts preferred the PHA over (i) a powerful <a href=\"https://arxiv.org/pdf/2406.06464\" target=\"_blank\" rel=\"noopener noreferrer\">single-agent system</a> that also builds on a base model that uses tools to achieve three roles within a single agent setup, and (ii) a parallel multi-agent baseline that includes the same DS, DE, and HC agents, but simply calls all three agents and synthesizes their results without dynamic orchestration. Both end-users and experts ranked PHA as the best overall system in the majority of cases. This provides a strong example of how the value of emulating the collaborative structure of human expert teams is key to providing truly helpful support.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA8_Results1body.width-1250.png\" alt=\"PHA8_Results1body\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA8_Results1body.width-1250.png\" alt=\"PHA8_Results1body\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"u5asb\"><i>PHA: Results of evaluating responses generated by the PHA and other baselines, evaluated by human experts.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA9_Results2.width-1250.png\" alt=\"PHA9_Results2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA9_Results2.width-1250.png\" alt=\"PHA9_Results2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"enxez\"><i>PHA: Results of ranking responses generated by the PHA and other baselines, evaluated by human experts.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"v6xwt\">The future of intelligent personal health agents</h2><p data-block-key=\"78bb4\">Creating AI systems that can interpret complex health and wellness data and provide actionable wellness advice has been a longstanding challenge in the field. Our research provides a validated conceptual blueprint for designing the next generation of personal health AI, advocating a shift away from monolithic models toward modular, collaborative systems that are more trustworthy, coherent, and helpful.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "迈向更好的健康对话：基于Gemini的“寻路”AI代理研究洞察 (原标题: Towards better health conversations: Research insights on a “wayfinding” AI agent based on Gemini)",
      "link": "https://research.google/blog/towards-better-health-conversations-research-insights-on-a-wayfinding-ai-agent-based-on-gemini/",
      "pubDate": "Wed, 24 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-24T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 迈向更好的健康对话：基于Gemini的“寻路”AI代理研究洞察\n\n### 引言：在线健康信息导航的挑战\n\n获取清晰、相关且个性化的健康信息对患者至关重要，但在线健康信息世界往往令人困惑、不知所措且缺乏个性化。现有的大型语言模型（LLM）工具多为被动的“问答者”，提供对初始查询的单一全面答案，这与医生等专家通过提问来理解全貌并引导患者的方式不同。这种寻求上下文的对话方式对AI设计构成了重大挑战。\n\n### “寻路AI”：一种新的对话式方法\n\n本文介绍了基于Gemini的早期研究原型“寻路AI”（Wayfinding AI），旨在探索一种新方法。其核心理念是：通过主动提出澄清性问题，AI代理能更好地发现用户需求，引导他们清晰表达担忧，并提供更有帮助、更个性化的信息。研究团队通过四项混合方法用户体验研究，共163名参与者，迭代设计了一个用户认为比基线AI代理更有帮助、更相关、更符合其需求的代理。\n\n### 形成性用户体验洞察：在线查找健康信息的挑战\n\n*   **用户痛点**：通过访谈33名参与者发现，人们在在线查找健康信息时，往往难以清晰表达自己的健康问题，因为缺乏医学背景，难以判断哪些细节具有医学相关性。\n*   **用户偏好**：研究显示，当聊天机器人主动提出澄清性问题时，用户体验会显著改变。大多数参与者更喜欢“延迟回答”的方式（即AI先提问），而非立即给出全面答案。这种对话风格被认为更个性化、更安心。\n*   **效果**：澄清性问题不仅帮助AI提供更好的答案，也赋能用户，引导他们提供更相关的上下文。\n*   **挑战**：这种基于澄清性问题的方法的有效性高度依赖于执行质量——如果问题措辞不当、不相关或隐藏在冗长文本中容易被忽略，用户参与度就会下降。\n\n### “寻路AI”的设计原则\n\n基于上述洞察，“寻路AI”围绕三个核心原则设计，以创造更赋能的对话体验：\n\n1.  **主动对话引导**：在每一轮对话中，“寻路AI”最多提出三个有针对性的问题，旨在系统性地减少歧义，帮助用户更完整地表达其健康故事，并直接满足用户对更多上下文答案的需求。\n2.  **每轮提供“尽力而为”的答案**：考虑到某些健康问题可能无需澄清即可获得良好答案，“寻路AI”在每一轮对话中，都会根据目前共享的信息提供一个“尽力而为”的答案，同时强调如果用户能回答一个或多个后续问题，答案可以得到改进。这种方法在整个对话过程中为用户提供有用的信息，并提供选项以随着对话的进行接收越来越好的答案。\n3.  **透明推理**： “寻路AI”会解释用户的最新回答如何帮助完善了之前的答案，使AI的推理过程清晰易懂。\n\n为了确保澄清性问题在“尽力而为”的答案中不会被遗漏，界面设计采用了两栏布局：对话和澄清性问题出现在左栏，而“尽力而为”的答案和更详细的解释出现在右栏。这使得交互式对话与信息内容分离。\n\n### 随机用户研究评估\n\n为评估该代理的潜在实际影响，研究团队进行了一项随机用户研究，招募了130名21岁及以上、非医疗专业人士、有健康相关问题并愿意与AI互动的美国参与者。在随机组内设计中，每位参与者都与“寻路AI”和基线Gemini 2.5 Flash模型互动，探索他们的健康话题。参与者在与每个AI互动后，就帮助性、问题相关性、定制性、目标理解、易用性和获取有用信息效率等六个维度评估了体验满意度。\n\n![研究设计图示](https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder2_StudyDesign.width-1250.png)\n*图示：本研究的设计。*\n\n### 研究结果：通过目标理解和定制对话提供有益且相关的信息\n\n研究结果表明，尽管“寻路AI”采用了不太常见的两栏界面，用户仍在其多个重要维度上偏爱“寻路AI”的方法。用户更喜欢“寻路AI”的帮助性、相关性、理解其目标的能力以及为特定需求定制对话的能力。这些发现表明，“寻路AI”主动提问的行为成功地为用户创造了更个性化、更有帮助的体验，而没有在用户体验中引入不必要的摩擦。\n\n![用户对基线AI和寻路AI的偏好比较](https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder3_Results.width-1250.png)\n*图示：用户在多个评估维度上对基线AI和“寻路AI”的偏好，包括代理的帮助性、响应的相关性、对话对用户的定制性、对用户目标的理解、易用性、对话效率以及未来健康信息需求的使用意愿。*\n\n此外，参与者与“寻路AI”的对话明显更长，尤其是在试图理解症状原因时。对于这些话题，“寻路AI”的对话平均有4.96轮，而基线AI为3.29轮。他们向每个AI提供的提示模式也不同。\n\n![Sankey图，展示基线AI和寻路AI的对话流程](https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder4_HeroSankey.width-1250.png)\n*图示：Sankey图，展示基线AI和“寻路AI”的对话流程。每个垂直条显示了前5轮对话中用户提示类型的细分。蓝色条表示参与者对澄清性问题的回应——这在“寻路AI”中更为常见。*\n\n### 结论\n\n在线查找正确的健康信息如同迷宫。“寻路AI”通过设计成个性化和主动的对话伙伴，并在结构化界面中提出有针对性的问题，证明了其能提供比传统问答体验更受用户青睐的体验，从而帮助人们获取更有帮助、更相关和更个性化的信息。用户研究结果有力地证明，这种以人为本的对话式方法是AI在健康领域未来发展的一个有前景的方向，有助于人们更好地管理自己的健康旅程。",
      "shortSummary": "在线查找个性化健康信息充满挑战。谷歌研究团队基于Gemini开发了“寻路AI”，它通过主动提出澄清性问题，像医生一样引导用户，而非被动回答。多项用户研究（163名参与者）表明，用户更喜欢这种对话式AI，认为它更有帮助、更相关、更个性化，能更好地理解用户目标，并促成更深入的健康对话。这表明以人为本的对话式AI是健康领域AI的未来方向。",
      "translated_title": "迈向更好的健康对话：基于Gemini的“寻路”AI代理研究洞察",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder2_StudyDesign.width-1250.png",
          "alt": "Wayfinder2_StudyDesign",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder3_Results.width-1250.png",
          "alt": "Wayfinder3_Results",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder4_HeroSankey.width-1250.png",
          "alt": "Wayfinder4_HeroSankey",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3lmno\">The ability to find clear, relevant, and personalized health information is a cornerstone of empowerment for medical patients. Yet, navigating the world of online health information is often a confusing, overwhelming, and impersonal experience. We are met with a flood of generic information that does not account for our unique context, and it can be difficult to know what details are relevant.</p><p data-block-key=\"2t1lv\">Large language models (LLMs) have the potential to make this information more accessible and tailored. However, many AI tools today act as passive \"question-answerers\" — they provide a single, comprehensive answer to an initial query. But this isn't how an expert, like a doctor, helps someone navigate a complex topic. A health professional doesn't just provide a lecture; they ask clarifying questions to understand the full picture, discover a person's goals, and guide them through the information maze. Though this <i>context-seeking</i> is critical, it's a significant design challenge for AI.</p><p data-block-key=\"5nqat\">In “<a href=\"https://storage.googleapis.com/research-media/wayfinding-ai.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Towards Better Health Conversations: The Benefits of Context-Seeking</a>”, we describe how we designed and tested our “Wayfinding AI”, an early-stage research prototype, based on Gemini, that explores a new approach. Our fundamental thesis is that by proactively asking clarifying questions, an AI agent can better discover a user's needs, guide them in articulating their concerns, and provide more helpful, tailored information. In a series of four mixed-method user experience studies with a total of 163 participants, we examined how people interact with AI for their health questions, and we iteratively designed an agent that users found to be significantly more helpful, relevant, and tailored to their needs than a baseline AI agent.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"3lmno\">Formative user experience insights: Challenges in finding health information online</h2><p data-block-key=\"f889p\">To better understand the hurdles people face, we interviewed 33 participants about their experiences finding health information online. A key theme quickly emerged: people often struggle to articulate their health concerns. As one participant described, their process was to \"...just kind of like throw all the words in there and then I'm just gonna see what comes back.\" It may be that without a clinical background, it’s difficult to know which details are medically relevant.</p><p data-block-key=\"ev7la\">The people we interviewed were then able to use research prototypes of different chatbots. (The chat histories were not logged.) These participants made up a diverse group and asked health questions on a wide range of topics (e.g., rib pain, vertigo, consistent and unexplained weight gain, tinnitus and surgery; more details <a href=\"https://storage.googleapis.com/research-media/wayfinding-ai.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">in the paper</a>). Our studies revealed that when a chatbot proactively asks clarifying questions, the experience changes dramatically. The majority of participants preferred a \"deferred-answer\" approach — where the AI asks questions first — over one that gives a comprehensive answer immediately. This conversational style was perceived as more personal and reassuring. As one person noted, \"It feels more like the way it would work if you talk to a doctor... it does make me feel a little more confident that it wants to know more before jumping right into an answer.\" These clarifying questions not only help the AI provide better answers, but also empower users, guiding them to provide more relevant context. We found similar patterns in <a href=\"https://dl.acm.org/doi/abs/10.1145/3613905.3651891\" target=\"_blank\" rel=\"noopener noreferrer\">prior work on AI for dermatology</a>.</p><p data-block-key=\"b2dcv\">However, the effectiveness of this clarifying question–based approach depends heavily on the execution — engagement drops if questions are poorly formulated, irrelevant, or buried within long paragraphs of text where they are easily missed.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"3lmno\">Designing a Wayfinding AI to empower people through personal and proactive conversations</h2><p data-block-key=\"8tbaa\">Informed by these insights, we designed our Wayfinding AI around three core principles to create a more empowering conversational experience:</p><ol><li data-block-key=\"4kmek\"><i>Proactive conversational guidance:</i> At each turn, the Wayfinding AI asks up to three targeted questions designed to systematically reduce ambiguity. This helps users articulate their health story more completely and directly incorporates users’ desire for more contextualized answers.<br><br></li><li data-block-key=\"cp0mo\"><i>Best-effort answers at each turn:</i> Because some health-related questions may not require clarification to get a good answer, the Wayfinding AI provides a \"best-effort\" answer at every conversational turn, based on the information shared so far, while emphasizing that the answer can be improved if the user can answer one or more of the follow-up questions. This approach gives the user helpful information throughout the conversation, while providing the option to further receive increasingly better answers as the conversation progresses.<br><br></li><li data-block-key=\"2rorl\"><i>Transparent reasoning:</i> The Wayfinding AI explains how the user's latest answers have helped refine the previous answer. This makes the AI's reasoning process clear and understandable.</li></ol><p data-block-key=\"81tqi\">To ensure clarifying questions are never missed within the longer answers in the “best-effort” answers section, we designed an interface with a two-column layout. The conversation and clarifying questions appear in the left column, while best-effort answers and more detailed explanations appear in the right. This separates the interactive conversation from the informational content.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Wayfinder1_UserInteraction.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d7rd2\"><i>Example of a user starting to interact with our Wayfinding AI prototype interface, including both the familiar multi-turn chat interface on the left, and a “best information so far” panel on the right. This two-panel interface separates the context-seeking stream from the more detailed information provision piece, enabling users to dive into the information only when they feel all relevant information has been relayed.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"65t26\">Evaluating our Wayfinding AI through a randomized user study</h2><p data-block-key=\"7b52c\">To evaluate the potential real-world impact of this agent, we conducted a randomized user study with 130 US-based participants recruited via a third party platform. All participants were 21 years and older, were not health care professionals, and had a health-related question for which they were willing to interact with an AI. To ensure a broad range of health topics, we imposed very few restrictions on which topic would be eligible for the study (details on excluded inquiries are provided in the paper). In a randomized <a href=\"https://www.nngroup.com/articles/between-within-subjects/\" target=\"_blank\" rel=\"noopener noreferrer\">within-subjects design</a>, each participant interacted with both our Wayfinding AI and a baseline Gemini 2.5 Flash model to explore their health topic. After providing informed consent and answering standard demographic questions, participants were instructed to have a conversation spending at least 3 minutes on their question; and then to resume the survey. After interacting with each AI, participants answered questions about their satisfaction with the experience along 6 dimensions: helpfulness, relevance of questions asked, tailoring to their situation, goal understanding, ease of use, and efficiency of getting useful information. They were able to provide open feedback about what they learned, and also had the option to upload their conversation with the AI. Sharing the conversation was not required to complete the survey. At the end of the study, participants were prompted to explicitly compare the two AIs and indicate which they would prefer in terms of each of the six dimensions above. They were also asked, <i>\"For a future topic, would you prefer the first or the second AI?\"</i> The order of AI exposure (Baseline AI first vs. Wayfinding AI first) was randomized across participants. Throughout the study, participants were instructed to not provide any identifying information about themselves.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder2_StudyDesign.width-1250.png\" alt=\"Wayfinder2_StudyDesign\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder2_StudyDesign.width-1250.png\" alt=\"Wayfinder2_StudyDesign\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"9mag7\"><i>Illustration of our study design.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"65t26\">Helpful and relevant information through goal understanding and tailored conversations</h2><p data-block-key=\"7mdek\">As shown below, the results of the study demonstrated that users preferred the Wayfinding AI's approach across several important dimensions, despite its less-familiar two-column interface. Users favored Wayfinding AI for its helpfulness, relevance, ability to understand their goal, and for tailoring the conversation to their specific needs. These findings suggest that the proactive, question-asking behavior of Wayfinding AI successfully created a more personalized and helpful experience for users without introducing undue friction in the user experience.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder3_Results.width-1250.png\" alt=\"Wayfinder3_Results\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder3_Results.width-1250.png\" alt=\"Wayfinder3_Results\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"9mag7\"><i>User preferences between a baseline and our Wayfinding AI along multiple evaluation axes, including helpfulness of the agent, relevance of its responses, tailoring of the conversation to the user, understanding the user’s goal, ease of use, efficiency of the conversation and willingness to use each for a future health information need.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"65t26\">Beyond simply preferring their conversations with the Wayfinding AI, participants had noticeably <i>different</i> conversations. Conversations were longer with the Wayfinding AI, in particular when participants were trying to understand the cause of their symptoms. For those topics, conversations with the Wayfinding AI had 4.96 turns on average, compared to 3.29 for the baseline AI. And the pattern of prompts they provided to each AI looked different across conversations:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder4_HeroSankey.width-1250.png\" alt=\"Wayfinder4_HeroSankey\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder4_HeroSankey.width-1250.png\" alt=\"Wayfinder4_HeroSankey\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"9mag7\"><i>Sankey diagram illustrating the flow of conversations with the baseline AI and the Wayfinding AI. Each of the vertical bars shows the breakdown of the types of user prompts, across the first 5 conversation turns. The blue bars indicate participants responding to clarifying questions — much more common for the Wayfinding AI.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"65t26\">Conclusion</h2><p data-block-key=\"9n52g\">Finding the right health information online can feel like navigating a maze. While AI has the potential to be a powerful guide, our research shows that its success hinges on its ability to move beyond being a passive question-answerer and become an active conversational partner.</p><p data-block-key=\"6i6kb\">By designing our Wayfinding AI to be personal and proactive, we demonstrated how asking targeted questions in a well-structured interface can power an experience that users prefer over a more classical, question-answering experience, and thus enable people to obtain more helpful, relevant, and tailored information. The results from our user studies provide strong evidence that this human-centered, conversational approach is a promising direction for the future of AI in health, helping people navigate their health journeys.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"65t26\">Acknowledgements</h2><p data-block-key=\"4bk1o\"><i>The research described here is joint work across Google Research, Google Health, and partnering teams. We would like to thank Yuexing Hao, Abbi Ward, Amy Wang, Beverly Freeman, Serena Zhan, Diego Ardila, Jimmy Li, I-Ching Lee, Anna Iurchenko, Siyi Kou, Kartikeya Badola, Jimmy Hu, Bhawesh Kumar, Keith Johnson, Supriya Vijay, Justin Krogue, Avinatan Hassidim, Yossi Matias, Dale Webster, Sunny Virmani, Yun Liu, Quang Duong, Fereshteh Mahvar, Laura Vardoulakis, Tiffany Guo, and Meredith Ringel Morris for contributing or reviewing this work. We would also like to thank the participants who contributed to these studies.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    }
  ],
  "lastUpdated": "2025-10-18T10:26:40.175Z"
}