{
  "sourceUrl": "https://rsshub.rssforever.com/google/research",
  "title": "Google Research Blog",
  "description": "Google Research Blog - Powered by RSSHub",
  "link": "https://research.google/blog",
  "items": [
    {
      "title": "å¯éªŒè¯çš„é‡å­ä¼˜åŠ¿ (åŸæ ‡é¢˜: A verifiable quantum advantage)",
      "link": "https://research.google/blog/a-verifiable-quantum-advantage/",
      "pubDate": "Tue, 21 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-21T16:00:00.000Z",
      "creator": "Google",
      "summary": "## å¯éªŒè¯çš„é‡å­ä¼˜åŠ¿ï¼šé‡å­å›å£°ç®—æ³•ä¸OTOC\n\n### å¼•è¨€ï¼šæ··æ²Œç³»ç»Ÿä¸é‡å­è®¡ç®—çš„æŒ‘æˆ˜\n\nè‡ªç„¶ç•Œä¸­å……æ»¡äº†æ··æ²Œç°è±¡ï¼Œå…¶ç‰¹ç‚¹æ˜¯ç³»ç»Ÿå¯¹å¾®å°æ‰°åŠ¨å…·æœ‰é«˜åº¦æ•æ„Ÿæ€§ã€‚æ— è®ºæ˜¯å®è§‚ä¸–ç•Œï¼ˆå¦‚å¤©æ°”æ¨¡å¼ã€äººå£åŠ¨æ€ï¼‰è¿˜æ˜¯é‡å­ç³»ç»Ÿï¼ˆå¦‚åŸå­æ ¸ç£åŒ–ã€é«˜æ¸©è¶…å¯¼ä½“ä¸­çš„ç”µå­æµï¼‰ï¼Œæ··æ²Œéƒ½æ™®éå­˜åœ¨ã€‚æ¨¡æ‹Ÿé‡å­æ··æ²Œç³»ç»Ÿå¯¹ç»å…¸è®¡ç®—è€Œè¨€æå…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè®¡ç®—æˆæœ¬å‘ˆæŒ‡æ•°çº§å¢é•¿ï¼Œè¿™ä½¿å¾—é‡å­è®¡ç®—æœºæˆä¸ºå®ç°é‡å­ä¼˜åŠ¿çš„ç†æƒ³å¹³å°ã€‚è™½ç„¶2019å¹´æ›¾é€šè¿‡å¯¹é«˜åº¦æ··æ²Œé‡å­æ€çš„æ¯”ç‰¹ä¸²é‡‡æ ·å±•ç¤ºäº†è¶…è¶Šç»å…¸çš„é‡å­è®¡ç®—ï¼Œä½†ç”±äºæ¯”ç‰¹ä¸²åœ¨å¤§å‹é‡å­ç³»ç»Ÿä¸­ä¸é‡å¤å‡ºç°ï¼Œå…¶å®ç”¨æ€§æœ‰é™ã€‚\n\n### é‡å­å›å£°ç®—æ³•ä¸æ—¶åºå…³è”å™¨ï¼ˆOTOCï¼‰\n\nåœ¨ã€Šè‡ªç„¶ã€‹æ‚å¿—å°é¢æ–‡ç« â€œåœ¨é‡å­éå†è¾¹ç¼˜è§‚å¯Ÿåˆ°å»ºè®¾æ€§å¹²æ¶‰â€ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»å¹¶å®éªŒæ¼”ç¤ºäº†ä¸€ç§åä¸ºâ€œé‡å­å›å£°â€çš„é‡å­ç®—æ³•ã€‚è¯¥ç®—æ³•çš„æ ¸å¿ƒæ˜¯æµ‹é‡ä¸€ä¸ªé‡å­å¯è§‚æµ‹é‡çš„æœŸæœ›å€¼ï¼Œå³â€œæ—¶åºå…³è”å™¨â€ï¼ˆOut-of-Time-Order Correlator, OTOCï¼‰ã€‚OTOCåŠå…¶é«˜é˜¶æ¨å¹¿æ˜¯ä¸€ç±»æè¿°é‡å­åŠ¨åŠ›å­¦å¦‚ä½•å˜å¾—æ··æ²Œçš„æ–°å‹å¯è§‚æµ‹é‡ã€‚ä¸æ¯”ç‰¹ä¸²ä¸åŒï¼Œç”µæµã€é€Ÿåº¦ã€ç£åŒ–å¼ºåº¦å’Œå¯†åº¦ç­‰é‡å­æœŸæœ›å€¼æ˜¯å¯éªŒè¯çš„è®¡ç®—ç»“æœï¼Œåœ¨ä¸åŒçš„é‡å­è®¡ç®—æœºä¸Šè¿è¡Œæ—¶ä¿æŒä¸å˜ã€‚æœŸæœ›å€¼çš„å¹¿æ³›ç›¸å…³æ€§åŠå…¶å¯éªŒè¯æ€§ï¼Œä¸ºä½¿ç”¨OTOCè§£å†³ç»å…¸è®¡ç®—æœºæ— æ³•è§£å†³çš„å®é™…é—®é¢˜æŒ‡æ˜äº†é“è·¯ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨Willowé‡å­èŠ¯ç‰‡ä¸Šè¿è¡Œé‡å­å›å£°ç®—æ³•ï¼Œå¯¹äºä¸€ç»„åŸºå‡†é‡å­ç”µè·¯è€Œè¨€ï¼Œå·²ç»è¾¾åˆ°äº†è¶…è¶Šç»å…¸çš„èŒƒç•´ã€‚\n\n### OTOCçš„å®éªŒå®ç°ä¸å¤šä½“å¹²æ¶‰\n\nåœ¨å®è·µä¸­ï¼ŒOTOCä»£è¡¨äº†ä¸€ç³»åˆ—é‡å­æ“ä½œç»“æŸåå•ä¸ªé‡å­æ¯”ç‰¹çš„çŠ¶æ€ã€‚åœ¨Willowä¸Šè¿è¡Œé‡å­å›å£°çš„å®éªŒä¸­ï¼Œå…±æœ‰103ä¸ªé‡å­æ¯”ç‰¹ç»å†äº†éšæœºé‡å­ç”µè·¯å½¢å¼çš„â€œæ­£å‘â€ï¼ˆUï¼‰å’Œâ€œåå‘â€ï¼ˆUâ€ ï¼‰æ¼”åŒ–ã€‚å¯¹æ‰€æœ‰é‡å­æ¯”ç‰¹ç›¸äº’ç‹¬ç«‹çš„çŠ¶æ€æ–½åŠ æ­£å‘æ¼”åŒ–ï¼Œä¼šä½¿ç³»ç»Ÿè¿›å…¥ä¸€ä¸ªé«˜åº¦æ··æ²Œä¸”æ‰€æœ‰é‡å­æ¯”ç‰¹ä¹‹é—´éƒ½å­˜åœ¨é‡å­å…³è”çš„çŠ¶æ€ã€‚åœ¨ä¸¤æ¬¡æ—¶é—´æ¼”åŒ–ä¹‹é—´ï¼Œå¯¹ä¸€ä¸ªé‡å­æ¯”ç‰¹æ–½åŠ ä¸€ä¸ªæ‰°åŠ¨ï¼ˆå•é‡å­æ¯”ç‰¹æ“ä½œBï¼‰ã€‚æ¥ç€æ˜¯å¦ä¸€ä¸ªæ¢æµ‹ï¼ˆå•é‡å­æ¯”ç‰¹æ“ä½œMï¼‰ã€‚é‡å¤æ­¤è¿‡ç¨‹ä¸€åˆ°ä¸¤æ¬¡ï¼Œå³å¯å¾—åˆ°ä¸€é˜¶æˆ–äºŒé˜¶OTOCã€‚åœ¨æ²¡æœ‰Bçš„æƒ…å†µä¸‹ï¼Œæ­£å‘å’Œåå‘æ¼”åŒ–ä¼šå°†ç³»ç»Ÿæ¢å¤åˆ°åˆå§‹çš„ç‹¬ç«‹é‡å­æ¯”ç‰¹çŠ¶æ€ã€‚è€ŒåŒ…å«æ‰°åŠ¨Båˆ™ä¼šå¼•å‘â€œè´è¶æ•ˆåº”â€ï¼šç»è¿‡è¿™ç§å—æ‰°åŠ¨çš„æ­£å‘å’Œåå‘æ¼”åŒ–åï¼Œæ•´ä¸ªç³»ç»Ÿæœ€ç»ˆä¼šè¿›å…¥ä¸€ä¸ªä¸åˆå§‹çŠ¶æ€æˆªç„¶ä¸åŒçš„ã€æ‰€æœ‰é‡å­æ¯”ç‰¹ä¹‹é—´éƒ½å­˜åœ¨é‡å­å…³è”çš„æ··æ²ŒçŠ¶æ€ã€‚\n\næˆ‘ä»¬ä»å®éªŒä¸­è·å¾—çš„ä¸€ä¸ªå…³é”®è§è§£æ˜¯ï¼Œé«˜é˜¶OTOCsè¡¨ç°å‡ºå¤æ‚çš„é‡å­å¹²æ¶‰æ•ˆåº”ï¼Œç±»ä¼¼äºä¼ ç»Ÿå¹²æ¶‰ä»ªã€‚è¿™è¢«ç§°ä¸ºå¤šä½“å¹²æ¶‰ï¼Œæ„å‘³ç€è®¸å¤šç²’å­çš„é‡å­æ€ç›¸äº’å¹²æ¶‰ï¼Œå°±åƒæ°´æ³¢å¯èƒ½ç›¸äº’å¹²æ¶‰ä¸€æ ·ï¼Œå¯¼è‡´å¤æ‚çš„æ•´ä½“æ•ˆåº”ã€‚åœ¨è¿™é‡Œï¼Œæ‰°åŠ¨Bå’ŒMå……å½“äº†ä¿®æ”¹ç³»ç»Ÿè½¨è¿¹çš„ä¸å®Œç¾é•œå­ã€‚ç”±äºâ€œå¾€è¿”â€æ¼”åŒ–æ¬¡æ•°çš„å¢åŠ ï¼Œé«˜é˜¶OTOCså¯¹æ‰°åŠ¨å˜å¾—æ›´åŠ æ•æ„Ÿï¼Œè½¨è¿¹åœ¨Bå’ŒMä¹‹é—´åå¼¹ã€‚å½“æ»¡è¶³å…±æŒ¯æ¡ä»¶ï¼ˆå³æ¼”åŒ–Uâ€ æ˜¯Uçš„ç²¾ç¡®é€†è¿ç®—ï¼‰æ—¶ï¼Œå¹²æ¶‰æ˜¯å»ºè®¾æ€§çš„ï¼Œå®ƒä¼šæ”¾å¤§æ··æ²Œæ€ä¸­å­˜åœ¨çš„é‡å­å…³è”å­é›†ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œè¿™ç§å¹²æ¶‰æµ‹é‡æ­ç¤ºäº†æ¼”åŒ–Uå¦‚ä½•åœ¨æ–½åŠ æ“ä½œBå’ŒMçš„ä¸¤ä¸ªé‡å­æ¯”ç‰¹ä¹‹é—´äº§ç”Ÿå…³è”ã€‚å®ƒå¯ä»¥ç”¨ä½œè¡¨å¾Uæ¼”åŒ–çš„æ•æ„Ÿå·¥å…·ã€‚\n\n![OTOC-1](https://storage.googleapis.com/gweb-research2023-media/images/OTOC-1.width-1250.png)\n\nå·¦å›¾ï¼šæµ‹é‡ä¸åŒé˜¶æ•°kçš„OTOCçš„é‡å­ç”µè·¯ã€‚é‡å­æ¯”ç‰¹ä»åŸºæ€å¼€å§‹ï¼Œå…¶ä¸­ä¸€ä¸ªé‡å­æ¯”ç‰¹å¤„äº|ğœ“MâŒªçŠ¶æ€ã€‚é‡å­å¤„ç†å™¨å®ç°å¤æ‚çš„è®¸å¤šä½“æ¼”åŒ–ï¼ˆUï¼‰ï¼ŒåŒ…æ‹¬åº”ç”¨äºäºŒç»´ç½‘æ ¼ä¸Šç›¸é‚»é‡å­æ¯”ç‰¹çš„å•é‡å­æ¯”ç‰¹å’ŒåŒé‡å­æ¯”ç‰¹æ“ä½œã€‚åœ¨ç”¨é—¨Bæ‰°åŠ¨ä¸€ä¸ªé‡å­æ¯”ç‰¹åï¼Œæ¼”åŒ–è¢«åè½¬ï¼ˆUâ€ ï¼‰ï¼Œç„¶åå¯¹æœ€åˆå‡†å¤‡çš„é‡å­æ¯”ç‰¹|ğœ“MâŒªè¿›è¡Œæ¢æµ‹æ“ä½œMã€‚è¿™é‡å¤kæ¬¡ï¼Œç„¶åæµ‹é‡å¦ä¸€ä¸ªé‡å­æ¯”ç‰¹Mã€‚å³å›¾ï¼šä¸åŒé˜¶æ•°OTOCä½œä¸ºå¹²æ¶‰ä»ªçš„æ¦‚å¿µè¡¨ç¤ºã€‚\n\n### é‡å­ä¼˜åŠ¿çš„å…³é”®ï¼šä¿¡å·æ”¾å¤§ä¸è®¡ç®—é¸¿æ²Ÿ\n\nå¤šä½“å¹²æ¶‰çš„å¹²æ¶‰æ€§è´¨å¸¦æ¥äº†ä¸¤ä¸ªå¯¹å®ç°é‡å­ä¼˜åŠ¿è‡³å…³é‡è¦çš„ç»“æœã€‚\n\n1.  **ä¿¡å·æ”¾å¤§ä¸æ•ˆç‡æå‡**\n\n    é¦–å…ˆï¼Œæ­£å‘å’Œåå‘æ¼”åŒ–éƒ¨åˆ†é€†è½¬äº†æ··æ²Œæ•ˆåº”ï¼Œå¹¶æ”¾å¤§äº†æœ€ç»ˆæµ‹é‡çš„é‡å­ä¿¡å·ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°äº†è¿™ç§æ”¾å¤§åœ¨OTOCä¿¡å·ä¸­çš„ç‰¹å¾ã€‚æ›´å…·ä½“åœ°è¯´ï¼ŒOTOCä¿¡å·å¹…åº¦ï¼ˆç”±éšæœºç”µè·¯é›†åˆä¸­OTOCå€¼åˆ†å¸ƒçš„å®½åº¦è¡¨å¾ï¼‰éšæ—¶é—´å‘ˆè´Ÿå¹‚å¾‹è¡°å‡ï¼Œè€Œæ²¡æœ‰åå‘æ¼”åŒ–çš„é‡å­ä¿¡å·åˆ™å‘ˆæŒ‡æ•°è¡°å‡ã€‚OTOCçš„ç¼“æ…¢å¹‚å¾‹è¡°å‡è¡¨æ˜ï¼Œåœ¨é‡å­è®¡ç®—æœºä¸Šæµ‹é‡è¿™äº›é‡æ¯”ç»å…¸æ¨¡æ‹Ÿæ•ˆç‡æ˜¾è‘—æ›´é«˜ï¼Œå› ä¸ºç»å…¸æ¨¡æ‹Ÿçš„æˆæœ¬éšæ—¶é—´å‘ˆæŒ‡æ•°å¢é•¿ã€‚\n\n    ![OTOC-2](https://storage.googleapis.com/gweb-research2023-media/images/OTOC-2.width-1250.png)\n\n    å·¦å›¾ï¼šæ²¡æœ‰æ—¶é—´åæ¼”ï¼ˆç°è‰²ï¼‰å’Œæœ‰æ—¶é—´åæ¼”ï¼ˆå“çº¢è‰²ã€è“è‰²ã€ç»¿è‰²ï¼‰æµ‹é‡çš„ä¿¡å·éšæ—¶é—´å˜åŒ–çš„å€¼ã€‚çºµè½´æ˜¾ç¤ºäº†ç›¸å…³å‡½æ•°C(1)ä»¥åŠä¸€é˜¶/äºŒé˜¶OTOCs C(2)å’ŒC(4)åœ¨éšæœºç”µè·¯ä¸Šçš„æ ‡å‡†å·®ã€‚å³å›¾ï¼šåœ¨Willowè®¾å¤‡ä¸Šæµ‹é‡çš„ä¸€ç»„äºŒé˜¶OTOCå€¼ï¼Œä¼°è®¡åœ¨Frontierè¶…çº§è®¡ç®—æœºä¸Šæ¨¡æ‹Ÿæ¯ä¸ªæ•°æ®ç‚¹éœ€è¦3.2å¹´ã€‚æ¨ªè½´æ ‡è®°äº†éšæœºç”µè·¯çš„å®ä¾‹ã€‚æœ¬æ¬¡å®éªŒä½¿ç”¨äº†105ä¸ªå¯ç”¨é‡å­æ¯”ç‰¹ä¸­çš„65ä¸ªã€‚\n\n2.  **ç»å…¸è®¡ç®—çš„å¤æ‚æ€§éšœç¢**\n\n    å¤šä½“å¹²æ¶‰çš„ç¬¬äºŒä¸ªç»“æœæ˜¯ç»å…¸å¤æ‚æ€§ã€‚é‡å­è®¡ç®—çš„æ ¸å¿ƒä»»åŠ¡æ˜¯è¯†åˆ«é‡å­è®¡ç®—æœºå’Œç»å…¸è®¡ç®—æœºåœ¨ç‰¹å®šè®¡ç®—ä»»åŠ¡ä¸Šçš„è®¡ç®—æˆæœ¬å·®è·ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤ç§æ–¹å¼è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼š(1) é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒç›¸ç»“åˆï¼Œæ­ç¤ºäº†å·²çŸ¥ç»å…¸ç®—æ³•åœ¨å®ç°ä¸æˆ‘ä»¬åœ¨Willowä¸Šè¿›è¡Œçš„OTOCè®¡ç®—ç›¸åŒç»“æœæ–¹é¢çš„æ ¹æœ¬éšœç¢ï¼›(2) é€šè¿‡ç›´æ¥å®ç°å’Œæˆæœ¬ä¼°ç®—ï¼Œæµ‹è¯•äº†ä¹ç§ç›¸å…³ç»å…¸æ¨¡æ‹Ÿç®—æ³•çš„æ€§èƒ½ã€‚\n\n    åœ¨ç¬¬ä¸€ç§æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å‘ç°é‡å­å¹²æ¶‰æ˜¯ç»å…¸è®¡ç®—çš„éšœç¢ã€‚é‡å­åŠ›å­¦çš„ä¸€ä¸ªæ˜¾è‘—ç‰¹å¾æ˜¯ï¼Œé¢„æµ‹å®éªŒç»“æœéœ€è¦åˆ†ææ¦‚ç‡å¹…ï¼Œè€Œä¸æ˜¯åƒç»å…¸åŠ›å­¦é‚£æ ·åˆ†ææ¦‚ç‡ã€‚ä¸€ä¸ªè‘—åçš„ä¾‹å­æ˜¯å…‰çš„çº ç¼ ï¼Œå®ƒè¡¨ç°ä¸ºå…‰å­ï¼ˆå…‰çš„åŸºå…ƒç²’å­ï¼‰ä¹‹é—´æŒç»­å¾ˆé•¿è·ç¦»çš„é‡å­å…³è”ï¼Œæˆ–è¶…å¯¼ç”µè·¯ä¸­çš„å®è§‚é‡å­éš§ç©¿ç°è±¡ã€‚æˆ‘ä»¬äºŒé˜¶OTOCæ•°æ®ä¸­çš„å¹²æ¶‰ï¼ˆå³ç»è¿‡ä¸¤æ¬¡åå‘å’Œæ­£å‘ç”µè·¯å¾ªç¯çš„OTOCï¼‰æ­ç¤ºäº†æ¦‚ç‡å’Œæ¦‚ç‡å¹…ä¹‹é—´çš„ç±»ä¼¼åŒºåˆ«ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œæ¦‚ç‡æ˜¯éè´Ÿæ•°ï¼Œè€Œæ¦‚ç‡å¹…å¯ä»¥æ˜¯ä»»æ„ç¬¦å·ï¼Œå¹¶ç”±å¤æ•°æè¿°ã€‚æ€»è€Œè¨€ä¹‹ï¼Œè¿™äº›ç‰¹å¾æ„å‘³ç€å®ƒä»¬åŒ…å«çš„ä¿¡æ¯é›†åˆè¦å¤æ‚å¾—å¤šã€‚æˆ‘ä»¬çš„å®éªŒä¸æ˜¯ä¸€å¯¹å…‰å­æˆ–ä¸€ä¸ªè¶…å¯¼ç»“ï¼Œè€Œæ˜¯ç”±65ä¸ªé‡å­æ¯”ç‰¹çš„æŒ‡æ•°çº§å¤§ç©ºé—´ä¸­çš„æ¦‚ç‡å¹…æ¥æè¿°çš„ã€‚å¯¹è¿™æ ·ä¸€ä¸ªé‡å­åŠ›å­¦ç³»ç»Ÿè¿›è¡Œç²¾ç¡®æè¿°éœ€è¦å­˜å‚¨å’Œå¤„ç†å†…å­˜ä¸­2^65ä¸ªå¤æ•°ï¼Œè¿™è¶…å‡ºäº†è¶…çº§è®¡ç®—æœºçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç”µè·¯ä¸­çš„é‡å­æ··æ²Œç¡®ä¿äº†æ¯ä¸ªæ¦‚ç‡å¹…éƒ½åŒç­‰é‡è¦ï¼Œå› æ­¤ä½¿ç”¨å‹ç¼©æè¿°ç³»ç»Ÿçš„ç®—æ³•æ‰€éœ€çš„å†…å­˜å’Œå¤„ç†æ—¶é—´ä¹Ÿè¶…å‡ºäº†è¶…çº§è®¡ç®—æœºçš„èƒ½åŠ›ã€‚\n\n    æˆ‘ä»¬è¿›ä¸€æ­¥çš„ç†è®ºå’Œå®éªŒåˆ†æè¡¨æ˜ï¼Œè¦é€šè¿‡æ•°å€¼è®¡ç®—é¢„æµ‹æˆ‘ä»¬çš„å®éªŒæ•°æ®ï¼Œå¿…é¡»ä»”ç»†è€ƒè™‘æ¦‚ç‡å¹…çš„ç¬¦å·ã€‚è¿™ç»™ä¸€ç±»é«˜æ•ˆçš„ç»å…¸ç®—æ³•ï¼ˆé‡å­è’™ç‰¹å¡æ´›ï¼‰å¸¦æ¥äº†é‡å¤§éšœç¢ï¼Œè¿™äº›ç®—æ³•åœ¨æè¿°å¤§å‹é‡å­åŠ›å­¦ç©ºé—´ä¸­çš„é‡å­ç°è±¡ï¼ˆä¾‹å¦‚æ¶²æ°¦-4çš„è¶…æµæ€§ï¼‰æ–¹é¢å–å¾—äº†æˆåŠŸã€‚è¿™äº›ç®—æ³•ä¾èµ–äºæ¦‚ç‡æè¿°ï¼Œä½†æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•ä¼šå¯¼è‡´è®¡ç®—è¾“å‡ºä¸­å‡ºç°æ— æ³•æ§åˆ¶çš„è¯¯å·®ã€‚æˆ‘ä»¬å¯¹ä¾èµ–äºå‹ç¼©è¡¨ç¤ºå’Œé«˜æ•ˆé‡å­è’™ç‰¹å¡æ´›çš„ç®—æ³•çš„ç›´æ¥å®ç°è¯å®äº†é¢„æµ‹äºŒé˜¶OTOCæ•°æ®çš„ä¸å¯è¡Œæ€§ã€‚æˆ‘ä»¬åœ¨Willowä¸Šçš„å®éªŒå¤§çº¦è€—æ—¶2å°æ—¶ï¼Œè€Œç»å…¸è¶…çº§è®¡ç®—æœºä¼°è®¡éœ€è¦13,000å€çš„æ—¶é—´æ¥å®Œæˆè¿™é¡¹ä»»åŠ¡ã€‚è¿™ä¸€ç»“è®ºæ˜¯åœ¨ä¼°è®¡æŠ•å…¥10ä¸ªäººå¹´è¿›è¡Œç»å…¸çº¢é˜Ÿæµ‹è¯•æˆ‘ä»¬çš„é‡å­ç»“æœï¼Œå¹¶å®ç°äº†æ€»å…±ä¹ç§ç»å…¸æ¨¡æ‹Ÿç®—æ³•ä¹‹åå¾—å‡ºçš„ã€‚\n\n### OTOCç”µè·¯çš„å®é™…åº”ç”¨ï¼šå“ˆå¯†é¡¿é‡å­¦ä¹ \n\nåœ¨ç¡®ç«‹äº†OTOCè¶…è¶Šç»å…¸çš„å¤æ‚æ€§ä¹‹åï¼Œæˆ‘ä»¬å¼€å§‹æ¢ç´¢å¦‚ä½•å°†å…¶åº”ç”¨äºè§£å†³å…·æœ‰å®é™…æ„ä¹‰çš„ç°å®ä¸–ç•Œé—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å“ˆå¯†é¡¿é‡å­¦ä¹ æ–¹æ¡ˆï¼Œå³é‡å­è®¡ç®—æœºæ¨¡æ‹Ÿè‡ªç„¶ç•Œä¸­ç‰©ç†ç³»ç»Ÿï¼ˆå¦‚åˆ†å­ï¼‰çš„OTOCä¿¡å·ï¼Œè¿™äº›ç³»ç»Ÿçš„å‚æ•°å°šæœªå®Œå…¨å·²çŸ¥ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†é‡å­è®¡ç®—æœºçš„OTOCä¿¡å·ä¸æœ‰å…³ç‰©ç†ç³»ç»Ÿçš„çœŸå®ä¸–ç•Œæ•°æ®è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è§‚å¯Ÿå®ƒä»¬ä½•æ—¶æœ€å»åˆã€‚é€šè¿‡å¯»æ‰¾è¿™ç§å»åˆï¼Œæˆ‘ä»¬æ—¨åœ¨è·å¾—æ¯”å…¶ä»–æŠ€æœ¯æ›´ç²¾ç¡®çš„ç³»ç»Ÿå‚æ•°ä¼°è®¡ã€‚ä¸ºäº†ä½¿è¯¥æ–¹æ¡ˆå®ç”¨åŒ–ï¼Œæˆ‘ä»¬å¿…é¡»æ‰¾åˆ°è‡ªç„¶ç•Œä¸­å¯ä»¥æ‰§è¡Œæˆ‘ä»¬çš„é‡å­å›å£°ç®—æ³•çš„ç³»ç»Ÿï¼Œå¹¶åœ¨æˆ‘ä»¬çš„é‡å­ç¡¬ä»¶ä¸Šæ¨¡æ‹Ÿè¿™äº›ç³»ç»Ÿã€‚ä½œä¸ºå®ç°è¿™ä¸€ç›®æ ‡çš„ä¸€æ­¥ï¼Œåœ¨â€œé€šè¿‡å¤šä½“æ ¸è‡ªæ—‹å›å£°è¿›è¡Œåˆ†å­å‡ ä½•çš„é‡å­è®¡ç®—â€ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä½¿ç”¨æ ¸ç£å…±æŒ¯ï¼ˆNMRï¼‰å…‰è°±å­¦éªŒè¯äº†è¿™ä¸€æ¦‚å¿µã€‚åœ¨NMRä¸­ï¼Œäººä»¬åˆ©ç”¨æ ¸è‡ªæ—‹åœ¨å¼ºç£åœºä¸­çš„è¿›åŠ¨æ¥äº†è§£åˆ†å­å’Œææ–™çš„ç»“æ„ï¼Œä¾‹å¦‚äººä½“ä¸­çš„è›‹ç™½è´¨æˆ–æ‰‹æœºä¸­çš„ç”µæ± ç»„ä»¶ã€‚æ ¸è‡ªæ—‹éµå¾ªé‡å­åŠ›å­¦å®šå¾‹ï¼Œåœ¨æŸäº›æ¡ä»¶ä¸‹ï¼ˆå³åœ¨å›ºä½“æˆ–ç±»å›ºä½“ææ–™ä¸­ï¼‰å®ƒä»¬è¡¨ç°å‡ºä¸Šè¿°ç›¸åŒçš„é‡å­æ··æ²Œè¡Œä¸ºã€‚è¿™ä½¿å¾—å®ƒä»¬æˆä¸ºOTOCåè®®çš„å®Œç¾å€™é€‰è€…ã€‚\n\nåœ¨è¿™ä»½å°†æäº¤åŒè¡Œè¯„å®¡çš„é¢„å°æœ¬ä¸­ï¼Œæˆ‘ä»¬åœ¨åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡æ´¾æ©æ–¯ç£å…±æŒ¯ä¸­å¿ƒæµ‹é‡äº†æº¶è§£åœ¨æ¶²æ™¶ä¸­çš„ä¸¤ç§æœ‰æœºåˆ†å­çš„OTOCsã€‚ç„¶åï¼Œè¯¥å®éªŒåœ¨æˆ‘ä»¬çš„WillowèŠ¯ç‰‡ä¸Šè¿›è¡Œäº†æ¨¡æ‹Ÿï¼Œä»è€Œæ”¹è¿›äº†åˆ†å­ç»“æ„çš„æ¨¡å‹ã€‚ç”±äºæ¨¡æ‹ŸçœŸå®ä¸–ç•Œç³»ç»Ÿçš„å›ºæœ‰å¤æ‚æ€§ä»¥åŠæˆ‘ä»¬å½“å‰èŠ¯ç‰‡çš„æ€§èƒ½é™åˆ¶ï¼Œè¿™ä¸€åˆæ­¥æ¼”ç¤ºå°šæœªè¶…è¶Šç»å…¸ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç»“æœå±•ç¤ºäº†å¯¹åˆ†å­ç»†èŠ‚çš„æ•æ„Ÿæ€§ï¼Œæˆ‘ä»¬ç›¸ä¿¡è¿™æ¡é“è·¯å°†å¸¦æ¥é‡å­è®¡ç®—çš„é¦–æ‰¹æœ‰ç”¨åº”ç”¨ã€‚\n\n![OTOC-3](https://storage.googleapis.com/gweb-research2023-media/images/OTOC-3.width-1250.png)\n\né€šè¿‡é‡å­è®¡ç®—æœºæ”¹è¿›ç‰©ç†é‡å­ç³»ç»ŸçŸ¥è¯†çš„ç¤ºæ„å›¾ï¼Œç§°ä¸ºå“ˆå¯†é¡¿é‡å­¦ä¹ ã€‚\n\n### ç»“è®º\n\næˆ‘ä»¬è¿›è¡Œäº†é¦–æ¬¡é‡å­è®¡ç®—å®éªŒï¼Œæµ‹é‡äº†ä¸€ä¸ªæ—¢å¯ä»¥é€šè¿‡å¦ä¸€å°é‡å­è®¡ç®—æœºæˆ–è‡ªç„¶é‡å­ç³»ç»Ÿè¿›è¡ŒéªŒè¯ï¼Œåˆè¶…è¶Šäº†å·²çŸ¥ç»å…¸ç®—æ³•æ¨¡æ‹Ÿèƒ½åŠ›çš„é‡å­å¯è§‚æµ‹é‡ã€‚è¿™é¡¹å®éªŒå¾—ç›Šäºæˆ‘ä»¬æœ€è¿‘çš„ç¡¬ä»¶è¿›æ­¥ï¼Œä¸ºé‡å­è®¡ç®—æœºåœ¨æ¢æµ‹åˆ†å­ç­‰ç‰©ç†ç³»ç»Ÿçš„å¾®è§‚ç»“æ„æ–¹é¢çš„é¦–æ¬¡å®é™…åº”ç”¨é“ºå¹³äº†é“è·¯ã€‚\n\n### è‡´è°¢\n\nè¿™é¡¹å·¥ä½œæ¶‰åŠé‡å­AIå›¢é˜Ÿçš„è®¸å¤šæˆå‘˜ï¼Œä»¥åŠGoogle DeepMindå’ŒåŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡ã€è¾¾ç‰¹èŒ…æ–¯å­¦é™¢ã€QSimulateå’ŒNVIDIAçš„å¤–éƒ¨åˆä½œè€…ã€‚",
      "shortSummary": "ç ”ç©¶äººå‘˜æå‡ºå¹¶å®éªŒéªŒè¯äº†â€œé‡å­å›å£°â€ç®—æ³•ï¼Œé€šè¿‡æµ‹é‡å¯éªŒè¯çš„â€œæ—¶åºå…³è”å™¨â€ï¼ˆOTOCï¼‰æœŸæœ›å€¼ï¼Œå®ç°äº†è¶…è¶Šç»å…¸è®¡ç®—çš„é‡å­ä¼˜åŠ¿ã€‚OTOCèƒ½æè¿°é‡å­æ··æ²Œï¼Œå…¶ç»“æœåœ¨Willowé‡å­èŠ¯ç‰‡ä¸Šæ¯”è¶…çº§è®¡ç®—æœºå¿«13,000å€ã€‚è¯¥ç®—æ³•åˆ©ç”¨å¤šä½“å¹²æ¶‰æ”¾å¤§é‡å­ä¿¡å·ï¼Œå¹¶å› é‡å­æ¦‚ç‡å¹…çš„å¤æ‚æ€§è€Œä½¿ç»å…¸æ¨¡æ‹Ÿé¢ä¸´æ ¹æœ¬éšœç¢ã€‚è¿™é¡¹çªç ´ä¸ºé‡å­è®¡ç®—æœºåœ¨å“ˆå¯†é¡¿é‡å­¦ä¹ ç­‰å®é™…åº”ç”¨ä¸­æ¢æµ‹åˆ†å­å¾®è§‚ç»“æ„é“ºå¹³äº†é“è·¯ï¼Œæœ‰æœ›æ”¹è¿›åˆ†å­ç»“æ„æ¨¡å‹ã€‚",
      "translated_title": "å¯éªŒè¯çš„é‡å­ä¼˜åŠ¿",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/OTOC-1.width-1250.png",
          "alt": "OTOC-1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/OTOC-2.width-1250.png",
          "alt": "OTOC-2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/OTOC-3.width-1250.png",
          "alt": "OTOC-3",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"9rtfc\">Nature is brimming with chaos, a phenomenon characterized by the high sensitivity of a system toward small perturbations. In the macroscopic world, notable examples of chaotic systems include weather patterns, wherein a small change in initial conditions leads to vastly different outcomes over time (often dubbed â€œthe <a href=\"https://en.wikipedia.org/wiki/Butterfly_effect\" target=\"_blank\" rel=\"noopener noreferrer\">butterfly effect</a>â€), and population dynamics, where small shifts in local populations may eventually affect the entire ecosystem. Chaos is similarly abundant in quantum systems, with examples including the <a href=\"https://en.wikipedia.org/wiki/Residual_dipolar_coupling\" target=\"_blank\" rel=\"noopener noreferrer\">dynamics of magnetization of atomic nuclei</a> when subjected to a time-varying magnetic field, and the <a href=\"https://en.wikipedia.org/wiki/Fermi_liquid_theory#Non-Fermi_liquids\" target=\"_blank\" rel=\"noopener noreferrer\">flow of electrons in high-temperature superconductors</a>.</p><p data-block-key=\"50aui\">Simulating quantum-chaotic systems is challenging for classical computation due to exponentially scaling computational cost, making quantum computers ideal for achieving quantum advantage. In 2019, we demonstrated the first <a href=\"https://research.google/blog/quantum-supremacy-using-a-programmable-superconducting-processor/\">beyond-classical quantum computation</a> by sampling bitstrings from a highly chaotic quantum state of <a href=\"https://en.wikipedia.org/wiki/Qubit\" target=\"_blank\" rel=\"noopener noreferrer\">qubits</a>. However, this <a href=\"https://research.google/blog/validating-random-circuit-sampling-as-a-benchmark-for-measuring-quantum-progress/\">random circuit sampling</a> approach has limited practical utility since the same bitstring never appears twice in a large quantum system, restricting its ability to reveal useful information.</p><p data-block-key=\"e7hrb\">In â€œ<a href=\"https://doi.org/10.1038/s41586-025-09526-6\" target=\"_blank\" rel=\"noopener noreferrer\">Observation of constructive interference at the edge of quantum ergodicity</a>â€, featured on the cover of <a href=\"https://www.nature.com/articles/s41586-025-09526-6\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature</i></a>, we introduce and experimentally demonstrate a quantum algorithm which we call Quantum Echoes. The heart of the algorithm is measuring the expectation value of a quantum <a href=\"https://en.wikipedia.org/wiki/Observable#:~:text=In%20physics%2C%20an%20observable%20is,of%20the%20space%20in%20question.\" target=\"_blank\" rel=\"noopener noreferrer\">observable</a>, called the <a href=\"http://www.scholarpedia.org/article/Out-of-time-order_correlations_and_quantum_chaos\" target=\"_blank\" rel=\"noopener noreferrer\">out-of-time-order correlator</a> (OTOC). OTOC and its higher order generalizations are a new family of observables that describe how quantum dynamics become chaotic. Unlike bitstrings, quantum expectation values, e.g., current, velocity, magnetization and density, are verifiable computational outcomes that remain the same when run on different quantum computers. The wide relevance of expectation values combined with their verifiability indicates a direct path toward using OTOCs to solve real-world problems using quantum computers, which are not possible to solve on classical computers. Remarkably, we show that running the Quantum Echoes algorithm on the <a href=\"https://blog.google/technology/research/google-willow-quantum-chip/\" target=\"_blank\" rel=\"noopener noreferrer\">Willow</a> quantum chip is already in the beyond-classical regime for a set of benchmarking quantum circuits.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Out-of-time-order correlator</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9rtfc\">In practice, OTOC represents the state of a single qubit at the end of a series of quantum operations. In our experiments running Quantum Echoes on <a href=\"https://blog.google/technology/research/google-willow-quantum-chip/\" target=\"_blank\" rel=\"noopener noreferrer\">Willow</a>, a total of 103 qubits underwent both â€œforwardâ€ (<i>U</i>) and â€œbackwardâ€ (<i>U</i><sup class=\"superscript\">â€ </sup>) evolutions in the form of random quantum circuits. A forward evolution applied to a state where all qubits are independent from each other brings the system to a highly chaotic state with quantum correlations across all qubits. A perturbation, a one-qubit operation <i>B</i>, is applied to a qubit in between the two time evolutions. This circuit is followed by another probe, a one-qubit operation <i>M</i>. Repeating this process once or twice leads to an OTOC of first or second order. In absence of <i>B</i> the forward and backward evolution returns the system to the initial state, where all qubits are independent. Inclusion of the perturbation <i>B</i> sets off a butterfly effect: after such perturbed forward and backward evolution, the whole system ends in a chaotic state with quantum correlations across all qubits that is very different from the initial state.</p><p data-block-key=\"dh297\">A crucial insight we obtained from our experiments is that higher-order OTOCs exhibit complex quantum interference effects analogous to a traditional <a href=\"https://en.wikipedia.org/wiki/Interferometry\" target=\"_blank\" rel=\"noopener noreferrer\">interferometer</a>. This is known as many-body interference, meaning the quantum states of many particles interfere with each other, much like waves of water might interfere, leading to complex overall effects. Here the perturbations, <i>B</i> and <i>M</i>, act as imperfect mirrors that modify the systemâ€™s trajectories. Higher order OTOCs become more sensitive to the perturbation due to increasing number of â€œround tripâ€ evolutions, where the trajectories bounce off of <i>B</i> and <i>M</i>. When a <a href=\"https://en.wikipedia.org/wiki/Resonance\" target=\"_blank\" rel=\"noopener noreferrer\">resonance condition</a> is satisfied, which corresponds to evolution <i>U</i><sup class=\"superscript\">â€ </sup> being the exact inverse of <i>U</i>, the interference is constructive and it amplifies the subset of quantum correlations from the totality of those present in the chaotic state. More specifically, this interferometry reveals how the evolution <i>U</i> generates correlations between the two qubits where operations <i>B</i> and <i>M</i> were applied. It can be used as a sensitive instrument to characterize the evolution of <i>U</i>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/OTOC-1.width-1250.png\" alt=\"OTOC-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/OTOC-1.width-1250.png\" alt=\"OTOC-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"pqe83\"><b><i>Left</i></b><i>: Quantum circuit measuring OTOCs of different orders, k. Qubits are initiated in the ground state, with one qubit in the state denoted by |ğœ“MâŒª. A complex many-body evolution (U) is implemented by the quantum processor, consisting of one- &amp; two-qubit operations applied to neighboring qubits on a two-dimensional grid. The evolution is reversed (U</i><i><sup class=\"superscript\">â€ </sup></i><i>) after perturbing one qubit with gate B, followed by a probe operation M on the initially prepared qubit |ğœ“MâŒª. This repeats k times before measuring another qubit M.</i> <b><i>Right</i></b><i>: Conceptual representation of OTOCs of different order as interferometers.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"9rtfc\">The interference nature of the OTOC leads to two consequences crucial for attaining quantum advantage. First, the forward and backward evolutions partially reverse the effects of chaos and amplify the quantum signal measured at the end. We observed the signature of this amplification in OTOC signals. More specifically, OTOC signal magnitude, characterized by the width of the distribution of OTOC values over the ensemble of random circuits, scales as a negative <a href=\"https://en.wikipedia.org/wiki/Power_law\" target=\"_blank\" rel=\"noopener noreferrer\">power</a> of time, whereas quantum signals measured without back evolutions decay exponentially. The slow power law decay of OTOCs suggests that measuring these quantities on a quantum computer is significantly more efficient than classical simulations, where costs increase exponentially over time.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/OTOC-2.width-1250.png\" alt=\"OTOC-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/OTOC-2.width-1250.png\" alt=\"OTOC-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"pqe83\"><b><i>Left</i></b><i>: Time-dependent values of signals measured without time inversion (</i><b><i>gray</i></b><i>) and with time inversion (</i><b><i>magenta</i></b><i>,</i> <b><i>blue</i></b><i>,</i> <b><i>green</i></b><i>). The vertical axis shows standard deviation over random circuits for correlation function, C</i><i><sup class=\"superscript\">(1)</sup></i><i>, and the first/second order OTOCs, C</i><i><sup class=\"superscript\">(2)</sup></i> <i>and C</i><i><sup class=\"superscript\">(4)</sup></i><i>.</i> <b><i>Right</i></b><i>: A set of 2nd order OTOC values measured on a Willow device that are estimated to require 3.2 years to simulate each data point on the</i> <a href=\"https://en.wikipedia.org/wiki/Frontier_(supercomputer)\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Frontier supercomputer</i></a><i>. The horizontal axis labels instances of random circuits. A total of 65 qubits out of the 105 available qubits are used in this experiment.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The computational gap between quantum and classical processors</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9rtfc\">The second consequence of many-body interference is classical complexity. A central task for quantum computing is to identify the computational cost gap between quantum and classical computers on specific computational tasks. We approached this in two ways: (1) through a combination of theoretical analysis and experiments, we revealed the fundamental obstacles to known classical algorithms in achieving the same outcome as our OTOC calculations on Willow, and (2) we tested the performance of nine relevant classical simulation algorithms by direct implementation and cost estimation.</p><p data-block-key=\"bngck\">In the first approach we identified that quantum interference is an obstacle for classical computation. A distinct characteristic of quantum mechanics is that predicting an outcome of an experiment requires analyzing <a href=\"https://en.wikipedia.org/wiki/Probability_amplitude\" target=\"_blank\" rel=\"noopener noreferrer\">probability amplitudes</a> rather than probabilities as in classical mechanics. A well known example is the entanglement of light that manifests in quantum correlations between photons, elementary particles of light, that persist over long distances (<a href=\"https://www.nobelprize.org/prizes/physics/2022/summary/\" target=\"_blank\" rel=\"noopener noreferrer\">2022 Physics Nobel Laureates</a>) or macroscopic quantum tunneling phenomena in superconducting circuits (<a href=\"https://blog.google/inside-google/company-announcements/googler-michel-devoret-awarded-the-nobel-prize-in-physics/\" target=\"_blank\" rel=\"noopener noreferrer\">2025 Physics Nobel Laureates</a>).</p><p data-block-key=\"86a3m\">The interference in our second order OTOC data (i.e., an OTOC that runs through the backward and forward circuit loop twice) reveals a similar distinction between probabilities and probability amplitudes. Crucially, probabilities are non-negative numbers, whereas probability amplitudes can be of an arbitrary sign and are described by complex numbers. Taken together, these features mean they contain a much more complex collection of information. Instead of a pair of photons or a single superconducting junction, our experiment is described by probability amplitudes across an exponentially large space of 65 qubits. An exact description of such a quantum mechanical system requires storing and processing 2<sup class=\"superscript\">65</sup> complex numbers in memory, which is beyond the capacity of supercomputers. Moreover, quantum chaos in our circuits ensures that every amplitude is equally important, and therefore algorithms using a compressed description of the system require memory and processing time beyond the capacity of supercomputers.</p><p data-block-key=\"4e84g\">Our further theoretical and experimental analysis revealed that carefully accounting for the signs of the probability amplitudes is necessary to predict our experimental data by a numerical calculation. This presents a significant barrier for a class of efficient classical algorithms, <a href=\"https://en.wikipedia.org/wiki/Quantum_Monte_Carlo\" target=\"_blank\" rel=\"noopener noreferrer\">quantum Monte Carlo</a>, that have been successful at describing quantum phenomena in a large quantum mechanical space (e.g., <a href=\"https://en.wikipedia.org/wiki/Superfluid_helium-4\" target=\"_blank\" rel=\"noopener noreferrer\">superfluidity of liquid Helium-4</a>). These algorithms rely on description in terms of probabilities, yet our analysis demonstrates that such approaches would result in an uncontrollable error in the computation output.</p><p data-block-key=\"br31a\">Our direct implementation of algorithms relying on both compressed representation and efficient quantum Monte Carlo confirmed the impossibility of predicting second-order OTOC data. Our experiments on Willow took approximately 2 hours, a task estimated to require 13,000 times longer on a classical supercomputer. This conclusion was reached after an estimated 10 person years spent in classical red teaming of our quantum result, implementing a total of nine classical simulation algorithms as a result.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Practical application of OTOC circuits</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9rtfc\">Having established the beyond-classical complexity of OTOCs, we began exploring how they could be applied to solving real-world problems of practical interest. To this end, we proposed <a href=\"https://en.wikipedia.org/wiki/Inverse_problem\" target=\"_blank\" rel=\"noopener noreferrer\">Hamiltonian learning</a>, a scheme where the quantum computer simulates OTOC signals from a physical system in nature, such as molecules, whose system parameters are not fully known. Then, we compare the quantum computer OTOC signals against real-world data about the physical system and observe when they best agree. By looking for this agreement, we aim to obtain a more precise estimate of system parameters than what is possible through other techniques.</p><p data-block-key=\"3dgr4\">To make this scheme practical, we have to find systems in nature that can perform our Quantum Echoes algorithm, and simulate these systems on our quantum hardware. As a step toward this goal, in \"<a href=\"https://quantumai.google/static/site-assets/downloads/quantum-computation-molecular-geometry-via-nuclear-spin-echoes.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Quantum computation of molecular geometry via many-body nuclear spin echoes</a>â€, we show that we tested this concept using <a href=\"https://en.wikipedia.org/wiki/Nuclear_magnetic_resonance\" target=\"_blank\" rel=\"noopener noreferrer\">nuclear magnetic resonance</a> (NMR) spectroscopy. In NMR, one uses the precession of nuclear spins in a large magnetic field to learn the structure of molecules and materials, like the proteins in your body or the battery components in your phone. Nuclear spins obey the laws of quantum mechanics, and under certain conditions (namely in solids or solid-like materials) they demonstrate the same quantum-chaotic behavior described above. This makes them a perfect candidate for the OTOC protocol.<br></p><p data-block-key=\"dfme5\">In this pre-print, which will be submitted for peer review, we measured OTOCs on two organic molecules dissolved in liquid crystal at the <a href=\"https://pmrc.berkeley.edu/\" target=\"_blank\" rel=\"noopener noreferrer\">Pines Magnetic Resonance Center</a> at UC Berkeley. This experiment was then simulated on our Willow chip, resulting in improved models of the molecular structure. Due to the inherent complexity of simulating real-world systems and performance limits of our current chip, this initial demonstration is not yet beyond classical. However, our results demonstrate sensitivity to molecular details and we're confident that this path will lead to some of the first useful applications of quantum computation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/OTOC-3.width-1250.png\" alt=\"OTOC-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/OTOC-3.width-1250.png\" alt=\"OTOC-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"pqe83\"><i>A schematic for refining knowledge of a physical quantum system through the quantum computer, known as Hamiltonian learning.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9rtfc\">We have performed the first quantum computing experiment measuring a quantum observable that is both verifiable through another quantum computer or a natural quantum system, and beyond the simulation capacity of known classical algorithms. This experiment was made possible by our recent <a href=\"https://blog.google/technology/research/quantum-hardware-verifiable-advantage/\" target=\"_blank\" rel=\"noopener noreferrer\">hardware advancement</a>, and paves the way toward the first real-world application of quantum computers in probing the microscopic structures of physical systems such as molecules.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9rtfc\"><i>This work involved many members of the Quantum AI team, along with Google DeepMind and external collaborators at</i> <a href=\"https://www.berkeley.edu/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>UC Berkeley</i></a><i>,</i> <a href=\"https://home.dartmouth.edu/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Dartmouth College</i></a><i>,</i> <a href=\"https://qsimulate.com/home\" target=\"_blank\" rel=\"noopener noreferrer\"><i>QSimulate</i></a><i>, and</i> <a href=\"https://www.nvidia.com/en-us/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>NVIDIA</i></a><i>.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "ä¸€å¼ å›¾ç‰‡èƒœè¿‡åƒè¨€ï¼ˆéšç§ï¼‰ï¼šè¿è´¯åˆæˆç›¸å†Œçš„åˆ†å±‚ç”Ÿæˆ (åŸæ ‡é¢˜: A picture's worth a thousand (private) words: Hierarchical generation of coherent synthetic photo albums)",
      "link": "https://research.google/blog/a-pictures-worth-a-thousand-private-words-hierarchical-generation-of-coherent-synthetic-photo-albums/",
      "pubDate": "Sun, 19 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-19T16:00:00.000Z",
      "creator": "Google",
      "summary": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ›æ–°æ–¹æ³•ï¼Œç”¨äºç§å¯†åœ°ç”Ÿæˆè¿è´¯çš„åˆæˆç›¸å†Œï¼Œä»¥æ»¡è¶³ç°ä»£AIå¯¹æ—¢ä¿æŠ¤éšç§åˆå…·æœ‰ä¸°å¯Œç»“æ„å’Œä¸Šä¸‹æ–‡çš„å¤šæ¨¡æ€æ•°æ®çš„éœ€æ±‚ã€‚\n\n### å·®åˆ†éšç§ï¼ˆDPï¼‰ä¸ç”Ÿæˆå¼AIçš„æŒ‘æˆ˜\n\n*   **å·®åˆ†éšç§ï¼ˆDPï¼‰**ï¼šæä¾›ä¸¥æ ¼çš„æ•°å­¦ä¿è¯ï¼Œç¡®ä¿æ•°æ®é›†ä¸­æ•æ„Ÿçš„ä¸ªäººä¿¡æ¯å—åˆ°ä¿æŠ¤ï¼Œå³ä½¿æ•°æ®ç”¨äºåˆ†æã€‚\n*   **ä¼ ç»ŸDPçš„å¤æ‚æ€§**ï¼šè¿‘äºŒåå¹´æ¥ï¼Œç ”ç©¶äººå‘˜ä¸ºå„ç§æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ æ–¹æ³•å¼€å‘äº†DPç‰ˆæœ¬ï¼Œä½†è¿™è¦æ±‚ç»„ç»‡å¯¹æ¯ç§åˆ†ææŠ€æœ¯è¿›è¡Œç§æœ‰åŒ–ï¼Œè¿‡ç¨‹å¤æ‚ã€ç¹çä¸”æ˜“å‡ºé”™ã€‚\n*   **ç”Ÿæˆå¼AIçš„è§£å†³æ–¹æ¡ˆ**ï¼šåƒGeminiè¿™æ ·çš„ç”Ÿæˆå¼AIæ¨¡å‹æä¾›äº†ä¸€ä¸ªæ›´ç®€å•ã€é«˜æ•ˆçš„æ–¹æ¡ˆã€‚å®ƒä»¬ä¸éœ€å•ç‹¬ä¿®æ”¹æ¯ç§åˆ†ææ–¹æ³•ï¼Œè€Œæ˜¯åˆ›å»ºä¸€ä¸ªåŸå§‹æ•°æ®é›†çš„å•ä¸€ç§æœ‰åˆæˆç‰ˆæœ¬ã€‚\n    *   **åˆæˆæ•°æ®ç‰¹æ€§**ï¼šè¿™ç§åˆæˆæ•°æ®èåˆäº†å¸¸è§çš„æ•°æ®æ¨¡å¼ï¼Œä¸åŒ…å«ä»»ä½•ä¸ªä½“ç”¨æˆ·çš„ç‹¬ç‰¹ç»†èŠ‚ã€‚\n    *   **éšç§ä¸ä»£è¡¨æ€§**ï¼šé€šè¿‡ä½¿ç”¨å·®åˆ†éšç§è®­ç»ƒç®—æ³•ï¼ˆå¦‚DP-SGDï¼‰å¯¹ç”Ÿæˆæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç¡®ä¿åˆæˆæ•°æ®é›†æ—¢ç§å¯†åˆé«˜åº¦ä»£è¡¨çœŸå®æ•°æ®ã€‚\n    *   **ç®€åŒ–å·¥ä½œæµç¨‹**ï¼šä»»ä½•æ ‡å‡†çš„ã€éç§æœ‰åˆ†ææŠ€æœ¯æˆ–å»ºæ¨¡éƒ½å¯ä»¥åœ¨è¿™ä¸ªå®‰å…¨ï¼ˆä¸”é«˜åº¦ä»£è¡¨æ€§ï¼‰çš„æ›¿ä»£æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œä»è€Œç®€åŒ–å·¥ä½œæµç¨‹ã€‚\n*   **ç°æœ‰å·¥ä½œçš„å±€é™æ€§**ï¼šå¤§å¤šæ•°å…³äºç§æœ‰åˆæˆæ•°æ®ç”Ÿæˆçš„ç ”ç©¶éƒ½é›†ä¸­åœ¨ç®€å•çš„è¾“å‡ºï¼Œå¦‚çŸ­æ–‡æœ¬æˆ–å•ä¸ªå›¾åƒã€‚ç„¶è€Œï¼Œä½¿ç”¨å¤šæ¨¡æ€æ•°æ®ï¼ˆå›¾åƒã€è§†é¢‘ç­‰ï¼‰çš„ç°ä»£åº”ç”¨ä¾èµ–äºå¯¹å¤æ‚çœŸå®ä¸–ç•Œç³»ç»Ÿå’Œè¡Œä¸ºçš„å»ºæ¨¡ï¼Œè¿™æ˜¯ç®€å•ã€éç»“æ„åŒ–æ–‡æœ¬æ•°æ®æ— æ³•å……åˆ†æ•æ‰çš„ã€‚\n\n### æ–°æ–¹æ³•ï¼šè¿è´¯åˆæˆç›¸å†Œçš„åˆ†å±‚ç”Ÿæˆ\n\nä¸ºäº†è§£å†³å¯¹ä¸°å¯Œã€ç»“æ„åŒ–å›¾åƒæ•°æ®é›†çš„åˆæˆç‰ˆæœ¬éœ€æ±‚ï¼Œç ”ç©¶äººå‘˜å¼•å…¥äº†ä¸€ç§ç§å¯†ç”Ÿæˆåˆæˆç›¸å†Œçš„æ–°æ–¹æ³•ã€‚è¿™é¡¹ä»»åŠ¡å¸¦æ¥äº†è¶…è¶Šç”Ÿæˆå•ä¸ªå›¾åƒçš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯éœ€è¦åœ¨ä¸€ä¸ªé¡ºåºç›¸å†Œä¸­ä¿æŒå¤šä¸ªç…§ç‰‡çš„ä¸»é¢˜è¿è´¯æ€§å’Œè§’è‰²ä¸€è‡´æ€§ã€‚\n\nè¯¥æ–¹æ³•åŸºäºå°†å¤æ‚çš„å›¾åƒæ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œç„¶åå†è½¬æ¢å›æ¥ï¼Œå¹¶é‡‡ç”¨åˆ†å±‚ç”Ÿæˆç­–ç•¥ã€‚\n\n#### å·¥ä½œåŸç†\n\n1.  **ç”Ÿæˆç»“æ„åŒ–æ–‡æœ¬è¡¨ç¤º**ï¼š\n    *   ç”¨AIæ¨¡å‹ä¸ºç›¸å†Œä¸­çš„æ¯å¼ ç…§ç‰‡ç”Ÿæˆè¯¦ç»†çš„æ–‡æœ¬æè¿°ï¼ˆcaptionï¼‰ã€‚\n    *   ä½¿ç”¨AIæ¨¡å‹ä¸ºæ¯ä¸ªç›¸å†Œç”Ÿæˆæ–‡æœ¬æ‘˜è¦ï¼ˆsummaryï¼‰ã€‚\n2.  **ç§æœ‰å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰**ï¼š\n    *   ç§æœ‰å¾®è°ƒä¸€å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ä»¥ç”Ÿæˆç±»ä¼¼çš„ç»“æ„åŒ–è¡¨ç¤ºã€‚\n    *   ç¬¬ä¸€ä¸ªæ¨¡å‹è®­ç»ƒç”¨äºç”Ÿæˆç›¸å†Œæ‘˜è¦ã€‚\n    *   ç¬¬äºŒä¸ªæ¨¡å‹è®­ç»ƒç”¨äºåŸºäºç›¸å†Œæ‘˜è¦ç”Ÿæˆå•ä¸ªç…§ç‰‡æè¿°ã€‚\n3.  **åˆ†å±‚ç”Ÿæˆç»“æ„åŒ–è¡¨ç¤º**ï¼š\n    *   é¦–å…ˆç”Ÿæˆç›¸å†Œçš„æ‘˜è¦ã€‚\n    *   ç„¶åï¼Œä»¥è¯¥æ‘˜è¦ä¸ºä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆç›¸å†Œä¸­æ¯å¼ ç…§ç‰‡çš„è¯¦ç»†æ–‡æœ¬æè¿°ã€‚\n4.  **æ–‡æœ¬åˆ°å›¾åƒè½¬æ¢**ï¼š\n    *   å°†ç”Ÿæˆçš„ç»“æ„åŒ–è¡¨ç¤ºï¼ˆæ–‡æœ¬ï¼‰ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„AIæ¨¡å‹è½¬æ¢ä¸ºå›¾åƒé›†ã€‚\n\n![Illustration of our method for generating synthetic photo albums.](https://storage.googleapis.com/gweb-research2023-media/images/PGSD-1-Cartoon.width-1250.png)\n*å›¾ç¤ºï¼šåˆæˆç›¸å†Œç”Ÿæˆæ–¹æ³•æ¦‚è¿°ã€‚*\n\n#### ä¸­é—´æ–‡æœ¬è¡¨ç¤ºçš„ä¼˜åŠ¿\n\n*   **LLMçš„ä¼˜åŠ¿**ï¼šæ–‡æœ¬ç”Ÿæˆæ˜¯å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸»è¦ä¼˜åŠ¿ã€‚\n*   **éšç§å¢å¼º**ï¼šæ–‡æœ¬æ‘˜è¦æœ¬è´¨ä¸Šå¢å¼ºäº†éšç§ï¼Œå› ä¸ºç”¨æ–‡æœ¬æè¿°å›¾åƒæ˜¯ä¸€ä¸ªæœ‰æŸæ“ä½œï¼Œå³ä½¿æ²¡æœ‰å¯ç”¨å·®åˆ†éšç§ï¼Œåˆæˆç…§ç‰‡ä¹Ÿä¸å¤ªå¯èƒ½æ˜¯åŸå§‹ç…§ç‰‡çš„ç²¾ç¡®å‰¯æœ¬ã€‚\n*   **èµ„æºæ•ˆç‡**ï¼šç”Ÿæˆå›¾åƒçš„æˆæœ¬è¿œé«˜äºç”Ÿæˆæ–‡æœ¬ã€‚é€šè¿‡é¦–å…ˆç”Ÿæˆæ–‡æœ¬ï¼Œå¯ä»¥åœ¨èŠ±è´¹èµ„æºç”Ÿæˆå›¾åƒä¹‹å‰æ ¹æ®å†…å®¹ç­›é€‰ç›¸å†Œã€‚\n\n#### åˆ†å±‚ç”Ÿæˆç­–ç•¥çš„ä¼˜åŠ¿\n\n*   **å†…éƒ¨ä¸€è‡´æ€§**ï¼šç¡®ä¿ç›¸å†Œä¸­çš„ç…§ç‰‡å†…éƒ¨ä¸€è‡´ï¼Œå› ä¸ºç›¸å†Œä¸­çš„æ¯å¼ ç…§ç‰‡æè¿°éƒ½æ˜¯åœ¨ç›¸åŒçš„ç›¸å†Œæ‘˜è¦ä¸Šä¸‹æ–‡ä¸‹ç”Ÿæˆçš„ã€‚\n*   **è®¡ç®—èµ„æºèŠ‚çœ**ï¼šåˆ†ä¸¤æ­¥ï¼ˆå…ˆç›¸å†Œæ‘˜è¦ï¼Œåç…§ç‰‡æè¿°ï¼‰ç”Ÿæˆç»“æ„åŒ–è¡¨ç¤ºï¼Œç›¸å¯¹äºä¸€æ­¥ç”Ÿæˆæ‰€æœ‰è¡¨ç¤ºï¼Œæ˜¾è‘—èŠ‚çœäº†è®¡ç®—èµ„æºã€‚ç”±äºè®­ç»ƒæˆæœ¬éšä¸Šä¸‹æ–‡é•¿åº¦å‘ˆäºŒæ¬¡æ–¹å¢é•¿ï¼Œè®­ç»ƒä¸¤ä¸ªä¸Šä¸‹æ–‡è¾ƒçŸ­çš„æ¨¡å‹è¿œæ¯”è®­ç»ƒä¸€ä¸ªä¸Šä¸‹æ–‡è¾ƒé•¿çš„æ¨¡å‹æˆæœ¬ä½ã€‚\n\n#### æ–‡æœ¬ä½œä¸ºä¸­é—´ä½“çš„æ•ˆç”¨æ¼”ç¤ºï¼ˆéDPï¼‰\n\né€šè¿‡ä¸€ä¸ªç®€å•çš„æ¼”ç¤ºï¼ˆæœªå¯ç”¨å·®åˆ†éšç§ï¼Œä»¥ä¾¿è¿›è¡Œå¹¶æ’æ¯”è¾ƒï¼‰ï¼Œå±•ç¤ºäº†è¿™ç§æ–¹æ³•çš„å¼ºå¤§ä¹‹å¤„ã€‚ç ”ç©¶äººå‘˜æç¤ºGeminiç”¨æ•°ç™¾ä¸ªè¯æè¿°ä¸€å¼ å›¾åƒï¼Œç„¶åå°†æè¿°æ–‡æœ¬åé¦ˆç»™Geminiï¼Œæç¤ºå®ƒç”Ÿæˆä¸€å¼ ä¸æè¿°åŒ¹é…çš„å›¾åƒã€‚è¿™è¯´æ˜äº†æ–‡æœ¬ä½œä¸ºåˆæˆå›¾åƒç”Ÿæˆä¸­é—´ä½“çš„å®ç”¨æ€§ã€‚\n\n![Privately generated synthetic photo example](https://storage.googleapis.com/gweb-research2023-media/images/PGSD-2-Example.width-1250.png)\n*å›¾ç¤ºï¼šå·¦ä¾§ä¸ºåŸå§‹å›¾åƒï¼Œå³ä¾§ä¸ºæ ¹æ®Geminiå¯¹åŸå§‹å›¾åƒçš„æ–‡æœ¬æè¿°ç”Ÿæˆçš„åˆæˆå›¾åƒã€‚*\n\n### è¯„ä¼°ä¸ç»“æœ\n\nè¯¥æ–¹æ³•åœ¨YFCC100Mæ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œè¯¥æ•°æ®é›†åŒ…å«è¿‘1äº¿å¼ åœ¨çŸ¥è¯†å…±äº«è®¸å¯ä¸‹å‘å¸ƒçš„å›¾åƒã€‚é€šè¿‡å°†åŒä¸€ç”¨æˆ·åœ¨åŒä¸€å°æ—¶å†…æ‹æ‘„çš„ç…§ç‰‡åˆ†ç»„ï¼Œå½¢æˆäº†â€œç›¸å†Œâ€ã€‚ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºäº†è®­ç»ƒé›†ï¼Œå¹¶ç¡®ä¿æ¯ä¸ªç”¨æˆ·å¯¹ä»»ä½•è®­ç»ƒé›†çš„è´¡çŒ®ä¸è¶…è¿‡ä¸€ä¸ªç¤ºä¾‹ï¼Œä»¥ä¿è¯å·®åˆ†éšç§çš„æœ‰æ•ˆæ€§ã€‚\n\nç”Ÿæˆåˆæˆç›¸å†Œåï¼Œè¯„ä¼°äº†å®ƒä»¬ä¸åŸå§‹ç›¸å†Œçš„ç›¸ä¼¼ç¨‹åº¦ï¼š\n\n1.  **MAUVEåˆ†æ•°**ï¼šè®¡ç®—äº†åŸå§‹å’Œåˆæˆç»“æ„åŒ–è¡¨ç¤ºï¼ˆç›¸å†Œæ‘˜è¦å’Œç…§ç‰‡æè¿°ï¼‰ä¹‹é—´çš„MAUVEåˆ†æ•°ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç¥ç»åµŒå…¥çš„è¯­ä¹‰ç›¸ä¼¼æ€§åº¦é‡ã€‚\n    ![MAUVE scores between real and synthetic album summaries & captions](https://storage.googleapis.com/gweb-research2023-media/images/PGSD-3-Mauve.width-1250.png)\n    *å›¾ç¤ºï¼šçœŸå®ä¸åˆæˆç›¸å†Œæ‘˜è¦åŠç…§ç‰‡æè¿°ä¹‹é—´çš„MAUVEåˆ†æ•°ã€‚MAUVEåˆ†æ•°è¶Šé«˜è¡¨ç¤ºç›¸ä¼¼åº¦è¶Šå¤§ã€‚éšç§å‚æ•°Îµå€¼è¶Šé«˜æ„å‘³ç€éšç§çº¦æŸè¶Šå¼±ã€‚*\n2.  **å¸¸è§ä¸»é¢˜åˆ†æ**ï¼šè®¡ç®—äº†ç›¸å†Œæ‘˜è¦ä¸­æœ€å¸¸è§çš„ä¸»é¢˜ï¼Œå‘ç°çœŸå®æ•°æ®å’Œåˆæˆæ•°æ®ä¹‹é—´éå¸¸ç›¸ä¼¼ã€‚\n    ![Real album summaries vs synthetic album summaries](https://storage.com/gweb-research2023-media/images/PGSD-4-Topics.width-1250.png)\n    *å›¾ç¤ºï¼šçœŸå®ç›¸å†Œæ‘˜è¦ä¸åˆæˆç›¸å†Œæ‘˜è¦ä¸­æœ€å¸¸è§çš„ä¸»é¢˜å¯¹æ¯”ã€‚*\n3.  **ç›´æ¥è§†è§‰æ£€æŸ¥**ï¼šåˆæˆç›¸å†Œçš„ç›´æ¥è§†è§‰æ£€æŸ¥æ˜¾ç¤ºï¼Œæ¯ä¸ªç›¸å†Œé€šå¸¸éƒ½å›´ç»•ä¸€ä¸ªå…±åŒä¸»é¢˜ï¼Œå°±åƒçœŸå®çš„ç›¸å†Œä¸€æ ·ã€‚\n    ![Privately generated synthetic photo albums](https://storage.googleapis.com/gweb-research2023-media/images/PGSD-5-Albums.width-1250.png)\n    *å›¾ç¤ºï¼šä¸¤ä¸ªç§å¯†ç”Ÿæˆçš„åˆæˆç›¸å†Œç¤ºä¾‹ã€‚æ¯ä¸ªç›¸å†Œéƒ½ä¿æŒä¸€ä¸ªç‰¹å®šä¸»é¢˜ï¼ˆä¸Šï¼šé‡‡è‹¹æœä¹‹æ—…ï¼›ä¸‹ï¼šæƒ…ä¾£å‚è§‚è‰åœ°ï¼‰ã€‚*\n\n### ç»“è®º\n\nç°ä»£AIçš„æŒ‘æˆ˜éœ€è¦ç§å¯†ã€ç»“æ„åŒ–ä¸”ä¸Šä¸‹æ–‡ä¸°å¯Œçš„æ•°æ®ï¼Œè€Œç®€å•çš„éç»“æ„åŒ–æ•°æ®æ— æ³•æ»¡è¶³è¿™ä¸€éœ€æ±‚ã€‚é€šè¿‡å°†åˆ†å±‚ã€ä»¥æ–‡æœ¬ä¸ºä¸­é—´ä½“çš„æ–¹æ³•åº”ç”¨äºç”Ÿæˆè¿è´¯åˆæˆç›¸å†Œçš„è‰°å·¨ä»»åŠ¡ï¼Œç ”ç©¶äººå‘˜æˆåŠŸå±•ç¤ºäº†å°†åˆæˆæ•°æ®çš„å¥½å¤„æ‰©å±•åˆ°ç®€å•æ–‡æœ¬æˆ–å­¤ç«‹å›¾åƒä¹‹å¤–çš„é€”å¾„ã€‚\n\nè¿™ç§æ–¹æ³•ä¸ºéšç§ä¿æŠ¤AIåˆ›æ–°å¼€è¾Ÿäº†æ¿€åŠ¨äººå¿ƒçš„æ–°é€”å¾„ï¼Œæœ‰åŠ©äºè§£å†³å¯¹å¤§é‡é«˜è´¨é‡æ•°æ®éœ€æ±‚ä¸ä¿æŠ¤ç”¨æˆ·éšç§ä¹‹é—´é•¿æœŸå­˜åœ¨çš„çŸ›ç›¾ï¼Œä¸ºå…³é”®è¡Œä¸šæ›´å®‰å…¨ã€æ›´æ™®éçš„AIå‘å±•é“ºå¹³äº†é“è·¯ã€‚",
      "shortSummary": "è¯¥ç ”ç©¶æå‡ºä¸€ç§åˆ†å±‚ç”Ÿæˆè¿è´¯åˆæˆç›¸å†Œçš„æ–°æ–¹æ³•ï¼Œä»¥è§£å†³ç°ä»£AIå¯¹ç§å¯†ã€ç»“æ„åŒ–å¤šæ¨¡æ€æ•°æ®çš„éœ€æ±‚ã€‚ä¼ ç»Ÿå·®åˆ†éšç§ï¼ˆDPï¼‰å¤æ‚ï¼Œè€Œç”Ÿæˆå¼AIé€šè¿‡åˆ›å»ºç§æœ‰åˆæˆæ•°æ®é›†ç®€åŒ–äº†æµç¨‹ã€‚æ–°æ–¹æ³•å°†å›¾åƒæ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬æè¿°å’Œç›¸å†Œæ‘˜è¦ï¼Œåˆ©ç”¨ç§æœ‰å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–°çš„æ–‡æœ¬è¡¨ç¤ºï¼Œå†å°†å…¶è½¬æ¢ä¸ºå›¾åƒã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†ç›¸å†Œå†…ç…§ç‰‡çš„ä¸»é¢˜ä¸€è‡´æ€§å’Œéšç§ä¿æŠ¤ï¼ŒåŒæ—¶æé«˜äº†è®¡ç®—æ•ˆç‡ï¼Œä¸ºéšç§ä¿æŠ¤AIåˆ›æ–°å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚",
      "translated_title": "ä¸€å¼ å›¾ç‰‡èƒœè¿‡åƒè¨€ï¼ˆéšç§ï¼‰ï¼šè¿è´¯åˆæˆç›¸å†Œçš„åˆ†å±‚ç”Ÿæˆ",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PGSD-1-Cartoon.width-1250.png",
          "alt": "Illustration of our method for generating synthetic photo albums.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PGSD-2-Example.width-1250.png",
          "alt": "Privately generated synthetic photo example",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PGSD-3-Mauve.width-1250.png",
          "alt": "MAUVE scores between real and synthetic album summaries & captions",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PGSD-4-Topics.width-1250.png",
          "alt": "Real album summaries vs synthetic album summaries",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PGSD-5-Albums.width-1250.png",
          "alt": "Privately generated synthetic photo albums",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"e3q59\"><a href=\"https://en.wikipedia.org/wiki/Differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">Differential privacy</a> (DP) provides a powerful, mathematically rigorous assurance that sensitive individual information in a dataset remains protected, even when a dataset is used for analysis. Since DPâ€™s <a href=\"https://link.springer.com/chapter/10.1007/11681878_14\" target=\"_blank\" rel=\"noopener noreferrer\">inception nearly two decades ago</a>, researchers have developed differentially private versions of myriad data analysis and machine learning methods, ranging from calculating simple statistics to <a href=\"https://research.google/blog/protecting-users-with-differentially-private-synthetic-training-data/\">fine-tuning complex AI models</a>. However, the requirement for organizations to privatize every analytical technique can be complex, burdensome, and error-prone.</p><p data-block-key=\"dbfdq\">Generative AI models like <a href=\"https://gemini.google.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a> offer a simpler, more efficient solution. Instead of separately modifying every analysis method, they create a single private synthetic version of the original dataset. This synthetic data is an amalgamation of common data patterns, containing no unique details from any individual user. By using a differentially private training algorithm, such as <a href=\"https://arxiv.org/abs/1607.00133\" target=\"_blank\" rel=\"noopener noreferrer\">DP-SGD</a>, to fine-tune the generative model on the original dataset, we ensure the synthetic dataset is both private and highly representative of the real data. Any standard, non-private analytical technique or modeling can then be performed on this safe (and highly representative) substitute dataset, simplifying workflows. DP fine-tuning is a versatile tool that is particularly valuable for generating high-volume, controlled datasets in situations where access to high-quality, representative data is unavailable.</p><p data-block-key=\"fiueb\">Most published work on private synthetic data generation has focused on <a href=\"https://arxiv.org/pdf/2210.14348\" target=\"_blank\" rel=\"noopener noreferrer\">simple outputs</a> like short text passages or individual images, but modern applications using multi-modal data (images, video, etc.) rely on modeling complex, real-world systems and behaviors, which simple, unstructured text data cannot adequately capture.</p><p data-block-key=\"794po\">We introduce a new method for privately generating synthetic photo albums as a way to address this need for synthetic versions of rich, structured image-based datasets. This task presents unique challenges beyond generating individual images, specifically the need to maintain thematic coherence and character consistency across multiple photos within a sequential album. Our method is based on translating complex image data to text and back. Our results show that this process, with rigorous DP guarantees enabled, successfully preserves the high-level semantic information and thematic coherence in datasets necessary for effective analysis and modeling applications.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How (and why) our method works</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"e3q59\">Our method differs from most other approaches to generating private synthetic image data in two major respects: (1) we use an intermediate text representation and (2) we generate the data hierarchically.</p><p data-block-key=\"mmvg\">Hereâ€™s how it works:</p><ol><li data-block-key=\"13lt2\">We generate a structured text representation of each original album, replacing each photo in the album with an AI-generated detailed text caption, and also using an AI model to produce a text summary of each album.</li><li data-block-key=\"ddg4l\">We then privately fine-tune a pair of large language models to produce similar structured representations. The first model is trained to generate album summaries, and the second model is trained to generate individual photo captions based on an album summary.</li><li data-block-key=\"5o70l\">We use the models to generate structured representations of photo albums in a hierarchical manner. For each photo album, we first generate a summary of the album, and then using that summary as context, we generate a detailed text caption of each photo in the album.</li><li data-block-key=\"466tp\">The generated structured representations are then converted into sets of images using a text-to-image AI model.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-1-Cartoon.width-1250.png\" alt=\"Illustration of our method for generating synthetic photo albums.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-1-Cartoon.width-1250.png\" alt=\"Illustration of our method for generating synthetic photo albums.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cnnu1\"><i>Illustration of our method for generating synthetic photo albums.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"e3q59\">Generating text as an intermediate step towards generating images has a number of advantages. First, text generation is the main strength of a large language model. Second, text summarization is inherently privacy enhancing, since describing an image by text is a lossy operation, so synthetic photos are unlikely to be exact copies of the originals, even when differential privacy is not enabled. Finally, generating images is far more costly than generating text, so by first generating text, we can filter albums based on their content before expending resources to produce the images in which we are most interested.</p><p data-block-key=\"1lmil\">Our hierarchical generation strategy ensures that the photos in each album are internally consistent, since each photo caption in an album is generated with the same album summary as context. Also, generating the structured representations in two steps (first the album summaries, and then the photo captions) preserves significant computational resources relative to generating each representation in one shot. Since training cost scales quadratically with context length (due to <a href=\"https://arxiv.org/pdf/2209.04881\" target=\"_blank\" rel=\"noopener noreferrer\">self-attention</a>), training two models with shorter contexts is far less costly than training a single model with a long context.</p><p data-block-key=\"1si9j\">It may seem that describing images with words is too lossy an operation to preserve any interesting characteristics of the original images, but a simple demonstration (without differential privacy, to allow for side-by-side comparison) illustrates the power of this approach. In the figure below, we prompted Gemini to describe an image using several hundred words, and then fed the response text back to Gemini, prompting it to generate an image matching the description. While this circular series of transformations does not satisfy differential privacy, it does illustrate the utility of text as an intermediary for synthetic image generation. As the saying goes, a picture is worth a thousand words â€” and it seems that it is not worth much more than that!</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-2-Example.width-1250.png\" alt=\"Privately generated synthetic photo example\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-2-Example.width-1250.png\" alt=\"Privately generated synthetic photo example\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cnnu1\"><b><i>Left:</i></b><i> Original image.</i> <b><i>Right:</i></b><i> Synthetic image.</i></p><p data-block-key=\"c1mff\"><i>We asked Gemini to describe the original image in text, and then prompted Gemini to generate the synthetic image based on the text description.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"e3q59\">Concurrent work by <a href=\"https://arxiv.org/abs/2506.07555\" target=\"_blank\" rel=\"noopener noreferrer\">Wang <i>et al</i>.</a> showed how one can leverage text-based intermediaries to generate differentially private single images using <a href=\"https://arxiv.org/abs/2403.01749\" target=\"_blank\" rel=\"noopener noreferrer\">Private Evolution</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Evaluation and results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"e3q59\">We tested our method on the <a href=\"https://arxiv.org/abs/1503.01817\" target=\"_blank\" rel=\"noopener noreferrer\">YFCC100M</a> dataset, a repository containing nearly 100 million images that have been released under the Creative Commons license. We formed â€œalbumsâ€ from these images by grouping together photos taken by the same user within the same hour. We constructed training sets for the large language models described above, taking care that no user contributes more than one example to any training set (contribution bounding is necessary to ensure the validity of the differential privacy guarantee).</p><p data-block-key=\"biu12\">After applying our method to generate synthetic photo albums, we evaluated how well they resemble the original albums. First, we computed the <a href=\"https://arxiv.org/abs/2102.01454\" target=\"_blank\" rel=\"noopener noreferrer\">MAUVE score</a>, a neural embeddingâ€“based measure of semantic similarity, between the original and synthetic structured representations.</p><p data-block-key=\"eddr7\">The figure below shows the MAUVE scores between real and synthetic album summaries, as well as real and synthetic photo captions, both before and after fine-tuning.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-3-Mauve.width-1250.png\" alt=\"MAUVE scores between real and synthetic album summaries &amp; captions\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-3-Mauve.width-1250.png\" alt=\"MAUVE scores between real and synthetic album summaries &amp; captions\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cnnu1\"><b><i>Left:</i></b><i> MAUVE scores between real and synthetic album summaries.</i> <b><i>Right:</i></b><i> MAUVE scores between real and synthetic photo captions. Higher MAUVE scores indicate greater similarity. Higher values of the privacy parameter Îµ imply weaker privacy constraints.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"e3q59\">Next, we calculated the most common topics in the album summaries, shown in the table below, and found that they were very similar between real and synthetic data.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-4-Topics.width-1250.png\" alt=\"Real album summaries vs synthetic album summaries\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-4-Topics.width-1250.png\" alt=\"Real album summaries vs synthetic album summaries\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cnnu1\"><b><i>Left:</i></b><i> Most common topics in real album summaries.</i> <b><i>Right:</i></b><i> Most common topics in synthetic album summaries.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"e3q59\">Finally, direct visual examination of the synthetic photos albums shows that each album is typically centered on a common theme, just like real photo albums, as demonstrated by the examples in the figure below.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-5-Albums.width-1250.png\" alt=\"Privately generated synthetic photo albums\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-5-Albums.width-1250.png\" alt=\"Privately generated synthetic photo albums\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cnnu1\"><i>Two synthetically-generated photo albums. Each album maintains a specific theme (</i><b><i>top:</i></b><i> apple picking trip;</i> <b><i>bottom:</i></b><i> couple visits a meadow).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"e3q59\">The challenges of modern AI require data that is not only private, but also structurally and contextually rich, a need that simple, unstructured data canâ€™t meet. By applying our hierarchical, text-as-intermediate method to the demanding task of generating coherent synthetic photo albums, weâ€™ve successfully shown a pathway for extending the benefits of synthetic data beyond simple text or isolated images.</p><p data-block-key=\"85ng9\">This methodology opens exciting new avenues for privacy-preserving AI innovation. It helps resolve the persistent tension between the need for large, high-quality data and the imperative to protect user privacy, paving the way for safer and more generalized AI development across critical industries.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"e3q59\"><i>This work is the result of a collaboration between many people at Google Research, including (in alphabetical order by last name): Kareem Amin, Alex Bie, Rudrajit Das, Alessandro Epasto, Weiwei Kong, Alex Kurakin, Natalia Ponomareva, Monica Ribero, Jane Shapiro, Umar Syed, and Sergei Vassilvitskii.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "ä»…ç”¨å°‘é‡ç¤ºä¾‹æ•™Geminiè¯†åˆ«çˆ†å‘æ’æ˜Ÿ (åŸæ ‡é¢˜: Teaching Gemini to spot exploding stars with just a few examples)",
      "link": "https://research.google/blog/teaching-gemini-to-spot-exploding-stars-with-just-a-few-examples/",
      "pubDate": "Sun, 19 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-19T16:00:00.000Z",
      "creator": "Google",
      "summary": "## ä»…ç”¨å°‘é‡ç¤ºä¾‹æ•™Geminiè¯†åˆ«çˆ†å‘æ’æ˜Ÿ\n\n### å¼•è¨€ï¼šå¤©æ–‡æ•°æ®æŒ‘æˆ˜ä¸ä¼ ç»Ÿæ¨¡å‹å±€é™\nç°ä»£å¤©æ–‡å­¦æ˜¯ä¸€åœºå®‡å®™è§„æ¨¡çš„å¯»å®æ¸¸æˆã€‚å…¨çƒæœ›è¿œé•œæ¯æ™šæ‰«æå¤©ç©ºï¼Œå¯»æ‰¾åƒçˆ†å‘æ’æ˜Ÿï¼ˆè¶…æ–°æ˜Ÿï¼‰è¿™æ ·çŸ­æš‚çš„äº‹ä»¶ï¼Œè¿™äº›äº‹ä»¶èƒ½ä¸ºæˆ‘ä»¬æä¾›å…³äºå®‡å®™è¿ä½œçš„å…³é”®è§è§£ã€‚è¿™äº›å·¡å¤©ä¼šç”Ÿæˆæ•°ç™¾ä¸‡æ¡æ½œåœ¨å‘ç°çš„è­¦æŠ¥ï¼Œä½†å…¶ä¸­ç»å¤§å¤šæ•°å¹¶éçœŸæ­£çš„å®‡å®™äº‹ä»¶ï¼Œè€Œæ˜¯å«æ˜Ÿè½¨è¿¹ã€å®‡å®™å°„çº¿æ’å‡»æˆ–å…¶ä»–ä»ªå™¨ä¼ªå½±é€ æˆçš„â€œè™šå‡â€ä¿¡å·ã€‚\n\nå¤šå¹´æ¥ï¼Œå¤©æ–‡å­¦å®¶ä¸€ç›´ä½¿ç”¨ä¸“é—¨çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œæ¥ç­›é€‰è¿™äº›æ•°æ®ã€‚å°½ç®¡è¿™äº›æ¨¡å‹æœ‰æ•ˆï¼Œä½†å®ƒä»¬é€šå¸¸å……å½“â€œé»‘ç®±â€ï¼Œåªæä¾›ç®€å•çš„â€œçœŸå®â€æˆ–â€œè™šå‡â€æ ‡ç­¾ï¼Œè€Œæ²¡æœ‰è§£é‡Šã€‚è¿™è¿«ä½¿ç§‘å­¦å®¶è¦ä¹ˆç›²ç›®ä¿¡ä»»è¾“å‡ºï¼Œè¦ä¹ˆèŠ±è´¹æ— æ•°æ—¶é—´æ‰‹åŠ¨éªŒè¯å€™é€‰äº‹ä»¶â€”â€”éšç€ä¸‹ä¸€ä»£æœ›è¿œé•œï¼ˆå¦‚Vera C. Rubinå¤©æ–‡å°ï¼‰é¢„è®¡æ¯æ™šç”Ÿæˆ1000ä¸‡æ¡è­¦æŠ¥ï¼Œè¿™ä¸€ç“¶é¢ˆå°†å¾ˆå¿«å˜å¾—éš¾ä»¥é€¾è¶Šã€‚\n\n### åˆ›æ–°æ–¹æ³•ï¼šGeminiçš„å¤šæ¨¡æ€å°‘æ ·æœ¬å­¦ä¹ \nè¿™ä¸€æŒ‘æˆ˜ä¿ƒä½¿æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼šä¸€ä¸ªæ—¨åœ¨åŒæ—¶ç†è§£æ–‡æœ¬å’Œå›¾åƒçš„é€šç”¨å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¦ä¸ä»…ä¸è¿™äº›ä¸“ä¸šæ¨¡å‹çš„å‡†ç¡®æ€§ç›¸åŒ¹é…ï¼Œè¿˜èƒ½è§£é‡Šå®ƒæ‰€çœ‹åˆ°çš„å†…å®¹ï¼Ÿåœ¨ã€Šè‡ªç„¶å¤©æ–‡å­¦ã€‹ä¸Šå‘è¡¨çš„è®ºæ–‡â€œå¤§å‹è¯­è¨€æ¨¡å‹å¯¹ç¬æ€å›¾åƒåˆ†ç±»çš„æ–‡æœ¬è§£é‡Šâ€ä¸­ï¼Œæˆ‘ä»¬è¯æ˜ç­”æ¡ˆæ˜¯è‚¯å®šçš„ã€‚æˆ‘ä»¬å±•ç¤ºäº†Googleçš„Geminiæ¨¡å‹å¦‚ä½•èƒ½å¤Ÿè½¬å˜ä¸ºä¸€ä¸ªä¸“å®¶çº§çš„å¤©æ–‡å­¦åŠ©æ‰‹ï¼Œä»¥é«˜ç²¾åº¦åˆ†ç±»å®‡å®™äº‹ä»¶ï¼Œå¹¶ä¸”æœ€é‡è¦çš„æ˜¯ï¼Œç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šå…¶æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬é€šè¿‡å¯¹Geminié‡‡ç”¨å°‘æ ·æœ¬å­¦ä¹ æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæ¯ä¸ªå·¡å¤©ä»…æä¾›15ä¸ªå¸¦æ³¨é‡Šçš„ç¤ºä¾‹å’Œç®€æ´çš„æŒ‡ä»¤ï¼Œä»¥å‡†ç¡®åˆ†ç±»å’Œè§£é‡Šå®‡å®™äº‹ä»¶ã€‚\n\n**ä»å°‘é‡ç¤ºä¾‹ä¸­å­¦ä¹ çš„æ–°æ–¹æ³•**\næˆ‘ä»¬æ²¡æœ‰åœ¨æ•°ç™¾ä¸‡å¼ æ ‡è®°å›¾åƒä¸Šè®­ç»ƒä¸€ä¸ªä¸“é—¨æ¨¡å‹ï¼Œè€Œæ˜¯å¯¹ä¸€ä¸ªé€šç”¨æ¨¡å‹ä½¿ç”¨äº†å°‘æ ·æœ¬å­¦ä¹ æŠ€æœ¯ã€‚æˆ‘ä»¬ä¸ºGeminiæä¾›äº†ä¸‰ä¸ªä¸»è¦å¤©æ–‡å·¡å¤©ï¼ˆPan-STARRSã€MeerLICHTå’ŒATLASï¼‰çš„æ¯ä¸ªå·¡å¤©ä»…15ä¸ªå¸¦æ³¨é‡Šçš„ç¤ºä¾‹ã€‚æ¯ä¸ªç¤ºä¾‹åŒ…æ‹¬ä¸‰å¼ å°å›¾åƒï¼šä¸€å¼ ç¬æ€è­¦æŠ¥çš„æ–°å›¾åƒã€ä¸€å¼ æ¥è‡ªå…ˆå‰è§‚æµ‹çš„åŒä¸€ç‰‡å¤©ç©ºåŒºåŸŸçš„å‚è€ƒå›¾åƒï¼Œä»¥åŠä¸€å¼ çªå‡ºä¸¤è€…ä¹‹é—´å˜åŒ–çš„å·®åˆ†å›¾åƒã€‚é™¤äº†è¿™äº›å›¾åƒï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ç»„ç®€æ´çš„æŒ‡ä»¤ã€ä¸€ä»½ä¸“å®¶ç¼–å†™çš„ç®€çŸ­åˆ†ç±»è¯´æ˜ï¼Œä»¥åŠä¸€ä¸ªå…´è¶£è¯„åˆ†ï¼ˆä¾‹å¦‚ï¼Œå¯¹äºå¯èƒ½çš„è¶…æ–°æ˜Ÿä¸ºâ€œé«˜å…´è¶£â€ï¼Œå¯¹äºå˜æ˜Ÿä¸ºâ€œä½å…´è¶£â€ï¼Œå¯¹äºè™šå‡ä¿¡å·ä¸ºâ€œæ— å…´è¶£â€ï¼‰ï¼Œå¹¶é™„å¸¦è¯¥è¯„åˆ†çš„è§£é‡Šã€‚\n\nè¯¥æ¨¡å‹å¿…é¡»å­¦ä¼šä»å„ç§æœ›è¿œé•œä¸­åˆ†ç±»ç¬æ€äº‹ä»¶ï¼Œæ¯ä¸ªæœ›è¿œé•œéƒ½å…·æœ‰ä¸åŒçš„åˆ†è¾¨ç‡ã€åƒç´ å°ºåº¦å’Œç›¸æœºç‰¹æ€§ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒåŒä¸€å¤©ä½“åœ¨è¿™äº›å·¡å¤©ä¸­å¯èƒ½çœ‹èµ·æ¥å¤§ç›¸å¾„åº­ï¼Œä½†Geminièƒ½å¤Ÿä»æä¾›çš„å°‘é‡ç¤ºä¾‹ä¸­è¿›è¡Œæ³›åŒ–ã€‚\n\n**å›¾ç‰‡ 1: Geminiåœ¨ä¸åŒå·¡å¤©æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›**\n![ExplodingStars1_Surveys](https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars1_Surveys.width-1250.png)\n*æè¿°ï¼šGeminiæ¨¡å‹èƒ½å¤Ÿåœ¨å…·æœ‰ä¸åŒåƒç´ å°ºåº¦å’Œåˆ†è¾¨ç‡çš„å·¡å¤©æ•°æ®ä¸­è¿è¡Œã€‚åŒä¸€ç¬æ€äº‹ä»¶åœ¨Pan-STARRSï¼ˆé¡¶éƒ¨ï¼‰ã€MeerLICHTï¼ˆä¸­éƒ¨ï¼‰å’ŒATLASï¼ˆåº•éƒ¨ï¼‰ä¸‰ä¸ªä¸åŒå·¡å¤©ä¸­è§‚æµ‹åˆ°ã€‚æ¯è¡Œä»å·¦åˆ°å³ä¾æ¬¡æ˜¯æ–°å›¾åƒã€å‚è€ƒå›¾åƒå’Œå·®åˆ†å›¾åƒã€‚å›¾åƒå—çš„åƒç´ å¤§å°å‡ä¸º100Ã—100ï¼Œä½†ç”±äºå·¡å¤©ç‰¹å®šçš„åƒç´ å°ºåº¦ï¼ˆPan-STARRSï¼š0.25â€³/åƒç´ ï¼ŒMeerLICHTï¼š0.56â€³/åƒç´ ï¼ŒATLASï¼š1.8â€³/åƒç´ ï¼‰ï¼Œå…¶è§’å¤©ç©ºè¦†ç›–èŒƒå›´æœ‰æ‰€ä¸åŒã€‚*\n\n### å“è¶Šæˆæœï¼šé«˜ç²¾åº¦åˆ†ç±»ä¸å¯è§£é‡Šæ€§\nä»…å‡­è¿™äº›æœ€å°‘çš„è¾“å…¥æŒ‡å¯¼ï¼Œæˆ‘ä»¬è¦æ±‚Geminiåˆ†ç±»æ•°åƒä¸ªæ–°çš„è­¦æŠ¥ã€‚è¯¥æ¨¡å‹åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸­å¹³å‡è¾¾åˆ°äº†93%çš„å‡†ç¡®ç‡ï¼Œè¿™ä¸éœ€è¦å¤§é‡ç²¾å¿ƒç­–åˆ’è®­ç»ƒæ•°æ®é›†çš„ä¸“ä¸šCNNæ¨¡å‹ç›¸å½“ã€‚\n\nä½†ä¸ä¼ ç»Ÿåˆ†ç±»å™¨ä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬æç¤ºGeminiä¸ä»…è¾“å‡ºä¸€ä¸ªæ ‡ç­¾ï¼Œè¿˜ä¸ºæ¯ä¸ªå€™é€‰äº‹ä»¶ç”Ÿæˆï¼š\n*   æè¿°å…¶è§‚å¯Ÿåˆ°çš„ç‰¹å¾å’Œå†³ç­–é€»è¾‘çš„æ–‡æœ¬è§£é‡Šã€‚\n*   å¸®åŠ©å¤©æ–‡å­¦å®¶ä¼˜å…ˆå®‰æ’åç»­è§‚æµ‹çš„å…´è¶£è¯„åˆ†ã€‚\n\nè¿™ä½¿å¾—æ¨¡å‹ä»ä¸€ä¸ªé»‘ç®±è½¬å˜ä¸ºä¸€ä¸ªé€æ˜ã€äº¤äº’å¼çš„åˆä½œä¼™ä¼´ã€‚ç§‘å­¦å®¶å¯ä»¥é˜…è¯»è§£é‡Šä»¥ç†è§£æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œå»ºç«‹ä¿¡ä»»å¹¶å®ç°æ›´ç»†è‡´çš„å†³ç­–ã€‚\n\n**å›¾ç‰‡ 2: Geminiæä¾›å¯è¯»çš„ç¬æ€åˆ†ç±»å’Œåç»­ä¼˜å…ˆçº§**\n![ExplodingStars2_Example](https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars2_Example.width-1250.png)\n*æè¿°ï¼šGeminiæä¾›äººç±»å¯è¯»çš„ç¬æ€åˆ†ç±»å’Œåç»­ä¼˜å…ˆçº§ã€‚æ¯ä¸ªç¤ºä¾‹åŒ…æ‹¬å€™é€‰ç¬æ€çš„æ–°å›¾åƒã€å‚è€ƒå›¾åƒå’Œå·®åˆ†å›¾åƒï¼Œéšåæ˜¯Geminiçš„åˆ†ç±»ã€æ–‡æœ¬æè¿°å’Œåç»­å…´è¶£è¯„åˆ†ã€‚å›¾ä¸­æ‰€ç¤ºç¤ºä¾‹æ¥è‡ªMeerLICHTæ•°æ®é›†ã€‚*\n\n### å¯é æ€§è¯„ä¼°ä¸ä¸ç¡®å®šæ€§é‡åŒ–\næ„å»ºä¸€ä¸ªå¯é ç³»ç»Ÿçš„ä¸€ä¸ªå…³é”®æ­¥éª¤æ˜¯ç¡®ä¿å…¶è¾“å‡ºçš„è´¨é‡ã€‚æˆ‘ä»¬å¬é›†äº†ä¸€ä¸ªç”±12ä½ä¸“ä¸šå¤©æ–‡å­¦å®¶ç»„æˆçš„å°ç»„ï¼Œä»–ä»¬å®¡æŸ¥äº†200ä¸ªGeminiçš„åˆ†ç±»å’Œè§£é‡Šã€‚ä»–ä»¬ä½¿ç”¨ä¸€ä¸ªå•ä¸€çš„ã€é”šå®šçš„0-5è¿è´¯æ€§è¯„ä¼°æ ‡å‡†ï¼ˆ0 = å¹»è§‰ï¼Œ5 = å®Œå…¨è¿è´¯ï¼‰ï¼Œè¯¥æ ‡å‡†ä¸æ–‡æœ¬å¦‚ä½•ä¸æ–°/å‚è€ƒ/å·®åˆ†å›¾åƒåŒ¹é…ç›¸å…³è”ï¼Œå¹¶è¿›è¡Œäº†ä¸€ä¸ªç®€å•çš„â€œæ˜¯/å¯èƒ½/å¦â€æ£€æŸ¥ï¼Œä»¥ç¡®è®¤åç»­å…´è¶£è¯„åˆ†ä¸è§£é‡Šä¸€è‡´ã€‚ä»–ä»¬å°†æ¨¡å‹çš„æè¿°è¯„ä¸ºé«˜åº¦è¿è´¯å’Œæœ‰ç”¨ï¼Œè¯å®äº†ä¸ä¸“å®¶æ¨ç†çš„ä¸€è‡´æ€§ã€‚\n\nä½†ä¹Ÿè®¸æˆ‘ä»¬æœ€é‡è¦çš„å‘ç°æ˜¯Geminièƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°è‡ªèº«çš„ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬æç¤ºæ¨¡å‹ä¸ºå…¶è‡ªèº«çš„è§£é‡Šåˆ†é…ä¸€ä¸ªâ€œè¿è´¯æ€§è¯„åˆ†â€ã€‚æˆ‘ä»¬å‘ç°ä½è¿è´¯æ€§è¯„åˆ†æ˜¯åˆ†ç±»ä¸æ­£ç¡®çš„æœ‰åŠ›æŒ‡æ ‡ã€‚æ¢å¥è¯è¯´ï¼Œæ¨¡å‹æ“…é•¿å‘Šè¯‰æˆ‘ä»¬å®ƒä½•æ—¶å¯èƒ½å‡ºé”™ã€‚\n\n**å›¾ç‰‡ 3: Geminiçš„è¯„ä¼°ç»“æœ**\n![ExplodingStars3_Results](https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars3_Results.width-1250.png)\n*æè¿°ï¼šå·¦å›¾ï¼š12ä½å¤©æ–‡å­¦å®¶å¯¹200ä¸ªMeerLICHTç¬æ€äº‹ä»¶çš„å¹³å‡è¿è´¯æ€§è¯„åˆ†ï¼ŒæŒ‰å¹³å‡åˆ†æ’åºï¼ˆè“è‰²ï¼‰ã€‚å¤§å¤šæ•°ç¤ºä¾‹è·å¾—é«˜åˆ†ï¼ˆ4-5ï¼‰ï¼Œè¡¨æ˜ä¸ç”¨æˆ·æœŸæœ›é«˜åº¦ä¸€è‡´ã€‚æ’å›¾ï¼šæ¨¡å‹åˆ†é…çš„å…´è¶£è¯„åˆ†ä¸å…¶è‡ªèº«è§£é‡Šä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œå‡ ä¹æ‰€æœ‰æ¡ˆä¾‹éƒ½è¢«æ ‡è®°ä¸ºè‡ªæ´½ï¼ˆå³â€œæ˜¯â€ï¼‰ã€‚å³å›¾ï¼šæŒ‰Geminiåˆ†ç±»çš„æ­£ç¡®æ€§ï¼ˆTPså’ŒTNsä¸ºç»¿è‰²ï¼ŒFPså’ŒFNsä¸ºçº¢è‰²ï¼‰åˆ’åˆ†çš„å¹³å‡ç”¨æˆ·è¿è´¯æ€§è¯„åˆ†ã€‚æ­£ç¡®åˆ†ç±»çš„ç¤ºä¾‹å¾€å¾€æ¯”ä¸æ­£ç¡®åˆ†ç±»çš„ç¤ºä¾‹å…·æœ‰æ›´é«˜çš„è¿è´¯æ€§è¯„åˆ†ã€‚*\n\n### â€œäººæœºåä½œâ€å·¥ä½œæµçš„æœªæ¥\nè¿™ç§èƒ½åŠ›å¯¹äºæ„å»ºå¯é çš„â€œäººæœºåä½œâ€å·¥ä½œæµæ¥è¯´æ˜¯ä¸€ä¸ªæ¸¸æˆè§„åˆ™çš„æ”¹å˜è€…ã€‚é€šè¿‡è‡ªåŠ¨æ ‡è®°å…¶æœ€ä¸ç¡®å®šçš„æ¡ˆä¾‹ï¼Œç³»ç»Ÿå¯ä»¥å°†å¤©æ–‡å­¦å®¶çš„æ³¨æ„åŠ›é›†ä¸­åœ¨æœ€éœ€è¦çš„åœ°æ–¹ã€‚è¿™åˆ›é€ äº†ä¸€ä¸ªå¼ºå¤§çš„åé¦ˆå¾ªç¯ã€‚é€šè¿‡å®¡æŸ¥æ ‡è®°çš„æ¡ˆä¾‹å¹¶å°†å…¶ä¸­ä¸€äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¤ºä¾‹é‡æ–°æ·»åŠ åˆ°æç¤ºä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è¿…é€Ÿæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡è¿™ç§è¿­ä»£è¿‡ç¨‹ï¼Œæˆ‘ä»¬å°†æ¨¡å‹åœ¨MeerLICHTæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ä»çº¦93.4%æé«˜åˆ°çº¦96.7%ï¼Œå±•ç¤ºäº†ç³»ç»Ÿå¦‚ä½•ä¸äººç±»ä¸“å®¶åˆä½œå­¦ä¹ å’Œæ”¹è¿›ã€‚\n\n### ç§‘å­¦å‘ç°çš„æœªæ¥å±•æœ›\næˆ‘ä»¬ç›¸ä¿¡è¿™ç§æ–¹æ³•æ ‡å¿—ç€ç§‘å­¦å‘ç°æ–°æ—¶ä»£çš„åˆ°æ¥â€”â€”ä¸€ä¸ªç”±èƒ½å¤Ÿå¯¹å¤æ‚ç§‘å­¦æ•°æ®é›†è¿›è¡Œæ¨ç†å¹¶ç”¨è‡ªç„¶è¯­è¨€è§£é‡Šå…¶è¾“å‡ºçš„æ¨¡å‹æ‰€åŠ é€Ÿçš„æ—¶ä»£ã€‚ç”±äºè¿™ç§æ–¹æ³•åªéœ€è¦å°‘é‡ç¤ºä¾‹å’Œé€šä¿—æ˜“æ‡‚çš„æŒ‡ä»¤ï¼Œå®ƒæœ‰å¯èƒ½åœ¨è®¸å¤šä¸åŒé¢†åŸŸå¿«é€Ÿé€‚åº”æ–°çš„ç§‘å­¦ä»ªå™¨ã€å·¡å¤©å’Œç ”ç©¶ç›®æ ‡ã€‚æˆ‘ä»¬è®¾æƒ³è¿™é¡¹æŠ€æœ¯å°†æˆä¸ºç§‘å­¦é¢†åŸŸâ€œæ™ºèƒ½ä»£ç†åŠ©æ‰‹â€çš„åŸºç¡€ã€‚è¿™æ ·çš„ç³»ç»Ÿå¯ä»¥æ•´åˆå¤šä¸ªæ•°æ®æºï¼Œæ£€æŸ¥è‡ªèº«çš„ç½®ä¿¡åº¦ï¼Œè¯·æ±‚åç»­è§‚æµ‹ï¼Œå¹¶ä»…å°†æœ€æœ‰å‰æ™¯çš„å‘ç°ä¸ŠæŠ¥ç»™äººç±»ç§‘å­¦å®¶ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†ä¸€æ¡é€šå‘ä¸æˆ‘ä»¬å…±åŒå­¦ä¹ ã€è§£é‡Šå…¶æ¨ç†å¹¶èµ‹èƒ½ä»»ä½•é¢†åŸŸç ”ç©¶äººå‘˜ä¸“æ³¨äºæœ€é‡è¦äº‹æƒ…â€”â€”æå‡ºä¸‹ä¸€ä¸ªä¼Ÿå¤§é—®é¢˜â€”â€”çš„ç³»ç»Ÿçš„é“è·¯ã€‚",
      "shortSummary": "ä¸€é¡¹æ–°ç ”ç©¶å±•ç¤ºäº†Googleçš„Geminiæ¨¡å‹å¦‚ä½•åˆ©ç”¨å°‘æ ·æœ¬å­¦ä¹ ï¼Œä»…é€šè¿‡å°‘é‡ç¤ºä¾‹å°±èƒ½é«˜ç²¾åº¦è¯†åˆ«çˆ†å‘æ’æ˜Ÿç­‰å¤©æ–‡ç¬æ€äº‹ä»¶ã€‚è¯¥æ¨¡å‹ä¸ä»…è¾¾åˆ°93%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œä¸ä¼ ç»Ÿä¸“ä¸šæ¨¡å‹ç›¸å½“ï¼Œè¿˜èƒ½æä¾›è¯¦ç»†çš„æ–‡æœ¬è§£é‡Šå’Œå…´è¶£è¯„åˆ†ï¼Œå°†â€œé»‘ç®±â€åˆ†ç±»å™¨è½¬å˜ä¸ºé€æ˜çš„äº¤äº’å¼ä¼™ä¼´ã€‚Geminiè¿˜èƒ½è¯„ä¼°è‡ªèº«ä¸ç¡®å®šæ€§ï¼Œæœ‰æ•ˆæŒ‡å¯¼å¤©æ–‡å­¦å®¶å…³æ³¨å…³é”®æ¡ˆä¾‹ï¼Œå¹¶é€šè¿‡äººæœºåä½œå¾ªç¯å¿«é€Ÿæå‡æ€§èƒ½ï¼Œé¢„ç¤ºç€ç§‘å­¦å‘ç°çš„æ–°æ—¶ä»£ã€‚",
      "translated_title": "ä»…ç”¨å°‘é‡ç¤ºä¾‹æ•™Geminiè¯†åˆ«çˆ†å‘æ’æ˜Ÿ",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars1_Surveys.width-1250.png",
          "alt": "ExplodingStars1_Surveys",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars2_Example.width-1250.png",
          "alt": "ExplodingStars2_Example",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars3_Results.width-1250.png",
          "alt": "ExplodingStars3_Results",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"m40sy\">Modern astronomy is a treasure hunt on a cosmic scale. Every night, telescopes around the globe scan the skies, searching for fleeting events like exploding stars (<a href=\"https://en.wikipedia.org/wiki/Supernova\" target=\"_blank\" rel=\"noopener noreferrer\">supernovae</a>) that give us crucial insights into the workings of the universe. These surveys generate millions of alerts about potential discoveries, but thereâ€™s a catch: the vast majority are not real cosmic events but \"bogus\" signals from satellite trails, cosmic ray hits, or other instrumental artefacts.</p><p data-block-key=\"4lso\">For years, astronomers have used specialized machine learning models, like <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\" target=\"_blank\" rel=\"noopener noreferrer\">convolutional neural networks</a> (CNNs), to sift through this data. While effective, these models often act as â€œblack boxes,â€ providing a simple \"real\" or \"bogus\" label with no explanation. This forces scientists to either blindly trust the output or spend countless hours manually verifying candidates â€” a bottleneck that will soon become insurmountable with next-generation telescopes like the <a href=\"https://rubinobservatory.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Vera C. Rubin Observatory</a>, expected to generate <a href=\"https://www.scientificamerican.com/article/rubin-observatory-data-flood-will-let-the-universe-alert-astronomers-10/\" target=\"_blank\" rel=\"noopener noreferrer\">10 million alerts per night</a>.</p><p data-block-key=\"a4o3a\">This challenge led us to ask a fundamental question: could a general-purpose, multimodal model, designed to understand text and images together, not only match the accuracy of these specialized models but also <i>explain</i> what it sees? In our paper, â€œ<a href=\"https://www.nature.com/articles/s41550-025-02670-z\" target=\"_blank\" rel=\"noopener noreferrer\">Textual interpretation of transient image classifications from large language models</a>â€, published in <a href=\"https://www.nature.com/natastron/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature Astronomy</i></a>, we demonstrate that the answer is a resounding yes. We show how Googleâ€™s Gemini model can be transformed into an expert astronomy assistant that can classify cosmic events with high accuracy and, crucially, explain its reasoning in plain language. We accomplished this by employing few-shot learning with Gemini, providing it with just 15 annotated examples per survey and concise instructions to accurately classify and explain cosmic events.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"m40sy\">A new approach: Learning from a few examples</h2><p data-block-key=\"blu8p\">Instead of training a specialized model on millions of labeled images, we used a technique called <a href=\"https://www.promptingguide.ai/techniques/fewshot\" target=\"_blank\" rel=\"noopener noreferrer\">few-shot learning</a> on a general-purpose model. We gave Gemini just 15 annotated examples for each of three major astronomical surveys: <a href=\"https://en.wikipedia.org/wiki/Pan-STARRS\" target=\"_blank\" rel=\"noopener noreferrer\">Pan-STARRS</a>, <a href=\"https://science.uct.ac.za/meerlicht/meerlicht\" target=\"_blank\" rel=\"noopener noreferrer\">MeerLICHT</a>, and <a href=\"https://atlas.fallingstar.com/\" target=\"_blank\" rel=\"noopener noreferrer\">ATLAS</a>. Each example consisted of three small images: a <i>new</i> image of the transient alert, a <i>reference</i> image of the same patch of sky from a previous observation, and a <i>difference</i> image that highlights the change between the two. Alongside these images, we provided a concise set of instructions, a short expert-written note explaining the classification, and an interest score (e.g., â€œhigh interestâ€ for a likely supernova, â€œlow interestâ€ for a variable star, or â€œno interestâ€ for a bogus signal) along with an explanation of that score.</p><p data-block-key=\"c449\">The model had to learn to classify transients from a diverse set of telescopes, each with different resolutions, pixel scales, and camera characteristics. As shown below, the same celestial object can appear quite different across these surveys, but Gemini was able to generalize from the few examples provided.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars1_Surveys.width-1250.png\" alt=\"ExplodingStars1_Surveys\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars1_Surveys.width-1250.png\" alt=\"ExplodingStars1_Surveys\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"5qfdw\"><i>Gemini operates across surveys with diverse pixel scales and resolutions. The same transient is observed in three different surveys, with rows corresponding to Pan-STARRS (</i><b><i>top</i></b><i>), MeerLICHT (</i><b><i>middle</i></b><i>) and ATLAS (</i><b><i>bottom</i></b><i>). Each row includes, from</i> <b><i>left</i></b><i> to</i> <b><i>right</i></b><i>, a new image, a reference image and a difference image. The image stamps are all the same size in pixels (100â€‰Ã—â€‰100) but differ in angular sky coverage due to survey-specific pixel scales: Pan-STARRS (0.25\" per pixel), MeerLICHT (0.56\" per pixel) and ATLAS (1.8\" per pixel).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"m40sy\">Guided only by this minimal input, we asked Gemini to classify thousands of new alerts. The model achieved an average accuracy of 93% across the three datasets, which is on par with specialized CNNs that require massive, curated training datasets.</p><p data-block-key=\"f7r54\">But unlike a traditional classifier, we prompted Gemini not just to output a label but also to generate for every candidate:</p><ol><li data-block-key=\"e6h3c\">A <i>textual explanation</i> describing the features it observed and the logic behind its decision.</li><li data-block-key=\"fa8cn\">An <i>interest score</i> to help astronomers prioritize follow-up observations.</li></ol><p data-block-key=\"ao7h5\">This turns the model from a black box into a transparent, interactive partner. Scientists can read the explanation to understand the modelâ€™s reasoning, building trust and allowing for more nuanced decision-making.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars2_Example.width-1250.png\" alt=\"ExplodingStars2_Example\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars2_Example.width-1250.png\" alt=\"ExplodingStars2_Example\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"5qfdw\"><i>Gemini provides human-readable transient classifications and follow-up priorities. Each example consists of a new, reference and difference image for a candidate transient, followed by the Gemini classification, textual description and follow-up interest score. The examples shown in the figure are from the MeerLICHT dataset.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"m40sy\">Knowing when to ask for help</h2><p data-block-key=\"ftr31\">A critical step in building a reliable system is ensuring the quality of its output. We assembled a panel of 12 professional astronomers who reviewed 200 of Geminiâ€™s classifications and explanations. Using a single, anchored 0â€“5 coherence rubric (0 = hallucination, 5 = perfectly coherent) tied to how well the text matched the new/reference/difference images, plus a simple Yes/Maybe/No check that the follow-up interest score agreed with the explanation, they rated the modelâ€™s descriptions as highly coherent and useful, confirming alignment with expert reasoning.</p><p data-block-key=\"e74hk\">But perhaps our most important finding was that Gemini can effectively assess its own uncertainty. We prompted the model to assign a â€œcoherence scoreâ€ to its own explanations. We discovered that low-coherence scores were a powerful indicator of an incorrect classification. In other words, the model is good at telling us when itâ€™s likely to be wrong. The details:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars3_Results.width-1250.png\" alt=\"ExplodingStars3_Results\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars3_Results.width-1250.png\" alt=\"ExplodingStars3_Results\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"5qfdw\"><b><i>Left:</i></b> <i>Average coherence scores from 12 astronomers for 200 MeerLICHT transients, sorted by mean score (</i><b><i>blue</i></b><i>). Most examples received high values (4â€“5), indicating close alignment with user expectations.</i> <b><i>Inset:</i></b><i> The consistency between the interest score assigned by the model &amp; its own explanation, with nearly all cases marked as self-consistent (i.e., â€œYesâ€).</i> <b><i>Right</i></b><i>: Average user coherence scores, split by the correctness of the classification made by Gemini. Correctly classified examples (TPs &amp; TNs,</i> <b><i>green</i></b><i>) tend to have higher coherence scores than incorrect ones (FPs &amp; FNs,</i> <b><i>red</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"m40sy\">This capability is a game-changer for building reliable \"human-in-the-loop\" workflows. By automatically flagging its most uncertain cases, the system can focus astronomers' attention where it is most needed. This creates a powerful feedback loop. By reviewing the flagged cases and adding a few of these challenging examples back into the prompt, we can rapidly improve the modelâ€™s performance. Using this iterative process, we improved the model's accuracy on the MeerLICHT dataset from ~93.4% to ~96.7%, demonstrating how the system can learn and improve in partnership with human experts.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"1ht7a\">The future of scientific discovery</h2><p data-block-key=\"1vu00\">We believe this approach marks a step toward a new era of scientific discovery â€” one accelerated by models that can both reason over complex scientific datasets and explain their outputs in natural language., but by models that can reason, explain their output, and collaborate with researchers.</p><p data-block-key=\"1n1n2\">Because this method requires only a small set of examples and plain-language instructions, it can potentially be rapidly adapted for new scientific instruments, surveys, and research goals across many different fields. We envision this technology as a foundation for \"agentic assistants\" in science. Such systems could integrate multiple data sources, check their own confidence, request follow-up observations, and escalate only the most promising discoveries to human scientists.</p><p data-block-key=\"fklo3\">This work shows a path toward systems that learn with us, explain their reasoning, and empower researchers in any field to focus on what matters most: asking the next great question.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"1ht7a\">Acknowledgements</h2><p data-block-key=\"djkdj\"><i>This research was a collaborative effort. We extend our sincere thanks to our co-authors Steven Bloemen, Stephen J. Smartt, Paul J. Groot, Paul Vreeswijk, and Ken W. Smith.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "è§£å†³è™šæ‹Ÿæœºéš¾é¢˜ï¼šAI å¦‚ä½•ä¼˜åŒ–äº‘è®¡ç®— (åŸæ ‡é¢˜: Solving virtual machine puzzles: How AI is optimizing cloud computing)",
      "link": "https://research.google/blog/solving-virtual-machine-puzzles-how-ai-is-optimizing-cloud-computing/",
      "pubDate": "Thu, 16 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-16T16:00:00.000Z",
      "creator": "Google",
      "summary": "## è§£å†³è™šæ‹Ÿæœºéš¾é¢˜ï¼šAI å¦‚ä½•ä¼˜åŒ–äº‘è®¡ç®—\n\näº‘è®¡ç®—æ•°æ®ä¸­å¿ƒé¢ä¸´ç€ä¸€ä¸ªæŒç»­çš„æŒ‘æˆ˜ï¼šå¦‚ä½•é«˜æ•ˆåœ°åˆ†é…å¤„ç†ä»»åŠ¡ï¼ˆå³è™šæ‹Ÿæœºï¼ŒVMï¼‰ã€‚è¿™ç±»ä¼¼äºä¸€ä¸ªä¿„ç½—æ–¯æ–¹å—æ¸¸æˆï¼Œä½†â€œæ–¹å—â€çš„ç”Ÿå‘½å‘¨æœŸæœªçŸ¥ä¸”å˜åŒ–è¿…é€Ÿã€‚ä½æ•ˆçš„VMåˆ†é…ä¼šå¯¼è‡´â€œèµ„æºææµ…â€ï¼ˆæµªè´¹å®¹é‡ï¼‰å’Œâ€œç©ºé—²ä¸»æœºâ€ä¸è¶³ï¼ˆç³»ç»Ÿæ›´æ–°å’Œå¤§å‹VMéƒ¨ç½²æ‰€éœ€ï¼‰ã€‚ä¼ ç»Ÿçš„AIæ–¹æ³•é€šè¿‡é¢„æµ‹VMç”Ÿå‘½å‘¨æœŸæ¥è§£å†³è¿™ä¸€ç»å…¸çš„è£…ç®±é—®é¢˜ï¼Œä½†å•æ¬¡é¢„æµ‹çš„é”™è¯¯å¯èƒ½å¯¼è‡´æ•ˆç‡ä¸‹é™ã€‚\n\n### LAVAç³»ç»Ÿï¼šåŸºäºå­¦ä¹ åˆ†å¸ƒå’Œé€‚åº”è¯¯é¢„æµ‹çš„ç”Ÿå‘½å‘¨æœŸæ„ŸçŸ¥VMåˆ†é…\n\nä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œç ”ç©¶äººå‘˜å¼•å…¥äº†LAVAç³»ç»Ÿï¼Œè¯¥ç³»ç»ŸåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç®—æ³•ï¼š\n\n*   **éä¾µå…¥å¼ç”Ÿå‘½å‘¨æœŸæ„ŸçŸ¥è°ƒåº¦ï¼ˆNILASï¼‰**\n*   **ç”Ÿå‘½å‘¨æœŸæ„ŸçŸ¥VMåˆ†é…ï¼ˆLAVAï¼‰**\n*   **ç”Ÿå‘½å‘¨æœŸæ„ŸçŸ¥é‡æ–°è°ƒåº¦ï¼ˆLARSï¼‰**\n\nè¯¥ç³»ç»Ÿçš„æ ¸å¿ƒæ˜¯â€œæŒç»­å†é¢„æµ‹â€æœºåˆ¶ï¼Œå®ƒä¸ä¾èµ–äºVMåˆ›å»ºæ—¶çš„ä¸€æ¬¡æ€§ç”Ÿå‘½å‘¨æœŸé¢„æµ‹ï¼Œè€Œæ˜¯æŒç»­è‡ªåŠ¨æ›´æ–°VMçš„é¢„æœŸå‰©ä½™ç”Ÿå‘½å‘¨æœŸé¢„æµ‹ã€‚\n\n### VMçš„ç§˜å¯†ç”Ÿå‘½ï¼šå†é¢„æµ‹ä¸æ¦‚ç‡åˆ†å¸ƒ\n\nç ”ç©¶å‘ç°ï¼ŒVMçš„ç”Ÿå‘½å‘¨æœŸé€šå¸¸æ˜¯ä¸å¯é¢„æµ‹çš„ï¼Œå¹¶éµå¾ªé•¿å°¾åˆ†å¸ƒã€‚ä¾‹å¦‚ï¼Œç»å¤§å¤šæ•°VMï¼ˆ88%ï¼‰çš„ç”Ÿå‘½å‘¨æœŸä¸åˆ°ä¸€å°æ—¶ï¼Œä½†å®ƒä»¬ä»…æ¶ˆè€—æ€»èµ„æºçš„æå°éƒ¨åˆ†ï¼ˆ2%ï¼‰ã€‚ç›¸åï¼Œå°‘æ•°é•¿ç”Ÿå‘½å‘¨æœŸçš„VMå¯¹æ•´ä½“èµ„æºæ•ˆç‡å½±å“å·¨å¤§ã€‚\n\n![VMæ•°é‡å’Œè®¡ç®—ä»½é¢çš„ç›¸å¯¹è´¡çŒ®å›¾](https://storage.googleapis.com/gweb-research2023-media/images/LAVA-2-Contributions.width-1250.png)\n\n*VMç”Ÿå‘½å‘¨æœŸåˆ†å¸ƒï¼ˆå·¦ï¼‰ä¸èµ„æºæ¶ˆè€—ï¼ˆå³ï¼‰ã€‚æœ€çŸ­ä½œä¸šï¼ˆ0-10åˆ†é’Ÿï¼Œæ·±è“è‰²ï¼‰å æ•°é‡çš„53%ï¼Œä½†èµ„æºæ¶ˆè€—å¯å¿½ç•¥ä¸è®¡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè¿è¡Œæ—¶é—´æœ€é•¿çš„ä½œä¸šï¼ˆ>30å¤©ï¼Œæ©™è‰²ï¼‰å èµ„æºæ¶ˆè€—çš„18%ï¼Œä½†æ•°é‡å æ¯”å¯å¿½ç•¥ä¸è®¡ã€‚*\n\nä¸ºäº†åº”å¯¹è¿™ç§ä¸ç¡®å®šæ€§ï¼ŒLAVAç³»ç»Ÿè®¾è®¡äº†ä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é¢„æµ‹VMç”Ÿå‘½å‘¨æœŸçš„æ¦‚ç‡åˆ†å¸ƒï¼Œè€Œéå•ä¸€å¹³å‡å€¼ã€‚è¿™ç§æ–¹æ³•å—ç”Ÿå­˜åˆ†æå¯å‘ï¼Œèƒ½å¤Ÿæ•æ‰VMè¡Œä¸ºçš„å†…åœ¨ä¸ç¡®å®šæ€§ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œç³»ç»Ÿåˆ©ç”¨æ­¤åˆ†å¸ƒæŒç»­æ›´æ–°é¢„æµ‹ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªVMè¿è¡Œäº†äº”å¤©åï¼Œç³»ç»Ÿä¼šé‡æ–°è¯„ä¼°å…¶é¢„æœŸå‰©ä½™ç”Ÿå‘½å‘¨æœŸï¼Œéšç€VMè¿è¡Œæ—¶é—´çš„å¢åŠ ï¼Œé¢„æµ‹å‡†ç¡®æ€§ä¹Ÿéšä¹‹æé«˜ã€‚\n\n![VMç”Ÿå‘½å‘¨æœŸåˆ†å¸ƒå›¾](https://storage.googleapis.com/gweb-research2023-media/images/LAVA-1-Lifetimes.width-1250.png)\n\n*VMç”Ÿå‘½å‘¨æœŸåˆ†å¸ƒã€‚VMè°ƒåº¦æ—¶ï¼Œé¢„æœŸï¼ˆå¹³å‡ï¼‰ç”Ÿå‘½å‘¨æœŸä¸º0.2å¤©ã€‚è¿è¡Œ1å¤©åï¼Œé¢„æœŸå‰©ä½™ç”Ÿå‘½å‘¨æœŸä¸º4å¤©ã€‚è¿è¡Œ7å¤©åï¼Œé¢„æœŸå‰©ä½™ç”Ÿå‘½å‘¨æœŸä¸º10å¤©ã€‚*\n\n### æ–°å‹è°ƒåº¦ç®—æ³•\n\nåŸºäºè¿™ç§æ›´å¼ºå¤§çš„é¢„æµ‹æ¨¡å‹ï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†ä¸‰ç§æ–°é¢–çš„ç®—æ³•æ¥æ”¹è¿›VMåˆ†é…ï¼š\n\n1.  **éä¾µå…¥å¼ç”Ÿå‘½å‘¨æœŸæ„ŸçŸ¥è°ƒåº¦ï¼ˆNILASï¼‰**ï¼š\n    *   å°†ç”Ÿå‘½å‘¨æœŸé¢„æµ‹æ•´åˆåˆ°ç°æœ‰è¯„åˆ†å‡½æ•°ä¸­ï¼Œé€šè¿‡ä¼˜å…ˆé€‰æ‹©æ‰€æœ‰VMé¢„è®¡åœ¨ç›¸ä¼¼æ—¶é—´é€€å‡ºçš„ä¸»æœºï¼Œæ—¨åœ¨åˆ›å»ºæ›´å¤šç©ºé—²æœºå™¨ã€‚\n    *   ç”±äºä½¿ç”¨å†é¢„æµ‹ï¼Œå¯¹åˆå§‹é¢„æµ‹å‡†ç¡®æ€§ä¸é‚£ä¹ˆæ•æ„Ÿï¼Œèƒ½å¤Ÿçº æ­£é”™è¯¯ã€‚\n    *   å·²éƒ¨ç½²åœ¨Googleçš„å¤§è§„æ¨¡é›†ç¾¤ç®¡ç†å™¨Borgä¸Šï¼Œæ˜¾è‘—æ”¹å–„äº†VMåˆ†é…ã€‚\n\n2.  **ç”Ÿå‘½å‘¨æœŸæ„ŸçŸ¥VMåˆ†é…ï¼ˆLAVAï¼‰**ï¼š\n    *   å°†çŸ­ç”Ÿå‘½å‘¨æœŸçš„VMæ”¾ç½®åœ¨å¸¦æœ‰ä¸€ä¸ªæˆ–å¤šä¸ªé•¿ç”Ÿå‘½å‘¨æœŸVMçš„ä¸»æœºä¸Šï¼Œä»¥å¡«è¡¥èµ„æºç©ºç™½ã€‚\n    *   ç›®æ ‡æ˜¯è®©çŸ­ç”Ÿå‘½å‘¨æœŸçš„VMè¿…é€Ÿé€€å‡ºï¼Œè€Œä¸å»¶é•¿ä¸»æœºçš„æ•´ä½“ç”Ÿå‘½å‘¨æœŸã€‚\n    *   é€šè¿‡åœ¨VMè¶…å‡ºé¢„æœŸæˆªæ­¢æ—¥æœŸæ—¶å¢åŠ ä¸»æœºçš„é¢„æœŸç”Ÿå‘½å‘¨æœŸæ¥ä¸»åŠ¨é€‚åº”è¯¯é¢„æµ‹ã€‚\n    *   æ¨¡æ‹Ÿç»“æœè¡¨æ˜ï¼Œè¯¥ç­–ç•¥èƒ½æœ€å¤§é™åº¦åœ°å‡å°‘ç¢ç‰‡åŒ–å¹¶ç¡®ä¿ä¸»æœºæœ€ç»ˆè¢«é‡Šæ”¾ã€‚\n\n3.  **ç”Ÿå‘½å‘¨æœŸæ„ŸçŸ¥é‡æ–°è°ƒåº¦ï¼ˆLARSï¼‰**ï¼š\n    *   åœ¨ç¢ç‰‡æ•´ç†å’Œç»´æŠ¤æœŸé—´ï¼Œæ ¹æ®é¢„æµ‹çš„å‰©ä½™ç”Ÿå‘½å‘¨æœŸå¯¹ä¸»æœºä¸Šçš„VMè¿›è¡Œæ’åºï¼Œå¹¶ä¼˜å…ˆè¿ç§»ç”Ÿå‘½å‘¨æœŸæœ€é•¿çš„VMã€‚\n    *   çŸ­ç”Ÿå‘½å‘¨æœŸçš„VMåœ¨è¿ç§»å‰è‡ªç„¶é€€å‡ºã€‚\n    *   æ¨¡æ‹Ÿç»“æœæ˜¾ç¤ºï¼ŒLARSæœ‰æœ›å°†æ‰€éœ€çš„æ€»è¿ç§»æ¬¡æ•°å‡å°‘çº¦4.5%ã€‚\n\n### è§£å†³å¤§è§„æ¨¡éƒ¨ç½²çš„æŒ‘æˆ˜\n\nå¼€å‘å¼ºå¤§çš„æ¨¡å‹å’Œç®—æ³•åªæ˜¯è§£å†³æ–¹æ¡ˆçš„ä¸€éƒ¨åˆ†ã€‚åœ¨å¤§è§„æ¨¡ç¯å¢ƒä¸­å¯é åœ°è¿è¡Œå®ƒä»¬éœ€è¦é‡æ–°æ€è€ƒæ¨¡å‹éƒ¨ç½²æ–¹æ³•ï¼š\n\n*   **é¿å…å¾ªç¯ä¾èµ–**ï¼šä¼ ç»Ÿçš„å°†æœºå™¨å­¦ä¹ æ¨¡å‹éƒ¨ç½²åœ¨ä¸“ç”¨æ¨ç†æœåŠ¡å™¨ä¸Šçš„åšæ³•ä¼šåˆ›å»ºå¾ªç¯ä¾èµ–ï¼Œå› ä¸ºè¿™äº›æœåŠ¡å™¨æœ¬èº«è¿è¡Œåœ¨é›†ç¾¤è°ƒåº¦ç³»ç»Ÿä¸Šã€‚è§£å†³æ–¹æ¡ˆæ˜¯å°†æ¨¡å‹ç›´æ¥ç¼–è¯‘åˆ°Borgè°ƒåº¦å™¨äºŒè¿›åˆ¶æ–‡ä»¶ä¸­ï¼Œæ¶ˆé™¤äº†å¾ªç¯ä¾èµ–ï¼Œå¹¶ç¡®ä¿äº†æ¨¡å‹ä»¥ä¸è°ƒåº¦å™¨å…¶ä»–ä»£ç æ›´æ”¹ç›¸åŒçš„ä¸¥æ ¼æµç¨‹è¿›è¡Œæµ‹è¯•å’Œéƒ¨ç½²ã€‚è¿™è¿˜å¸¦æ¥äº†é¢å¤–çš„å¥½å¤„ï¼šæ¨¡å‹çš„ä¸­ä½å»¶è¿Ÿä»…ä¸º9å¾®ç§’ï¼ˆÂµsï¼‰ï¼Œæ¯”ä½¿ç”¨ç‹¬ç«‹æ¨¡å‹æœåŠ¡å™¨çš„æ–¹æ³•å¿«780å€ï¼Œè¿™å¯¹äºé¢‘ç¹çš„å†é¢„æµ‹å’Œæ€§èƒ½æ•æ„Ÿä»»åŠ¡è‡³å…³é‡è¦ã€‚\n*   **è§£å†³é¢„æµ‹ç“¶é¢ˆ**ï¼šå¯¹äºæœ€å¤§çš„åŒºåŸŸï¼Œæ‰€éœ€çš„é¢„æµ‹æ•°é‡å¯èƒ½æˆä¸ºç“¶é¢ˆã€‚é€šè¿‡å¼•å…¥ä¸»æœºç”Ÿå‘½å‘¨æœŸè¯„åˆ†ç¼“å­˜æ¥è§£å†³ï¼Œè¯¥ç¼“å­˜ä»…åœ¨VMæ·»åŠ åˆ°æˆ–ä»ä¸»æœºç§»é™¤æ—¶ï¼Œæˆ–å½“ä¸»æœºçš„é¢„æœŸç”Ÿå‘½å‘¨æœŸåˆ°æœŸæ—¶æ‰æ›´æ–°é¢„æµ‹ã€‚è¿™ç§ç¼“å­˜æœºåˆ¶ç¡®ä¿äº†é«˜æ€§èƒ½ï¼Œå¹¶å…è®¸ç³»ç»Ÿåœ¨æ•´ä¸ªé›†ç¾¤ä¸­éƒ¨ç½²ã€‚\n\n### æˆæœ\n\nNILASç®—æ³•è‡ª2024å¹´åˆä»¥æ¥å·²åœ¨Googleçš„ç”Ÿäº§æ•°æ®ä¸­å¿ƒè¿è¡Œï¼Œå–å¾—äº†æ˜¾è‘—æˆæœï¼š\n\n*   **ç©ºé—²ä¸»æœºå¢åŠ **ï¼šç”Ÿäº§è¯•ç‚¹å’Œå…¨é›†ç¾¤æ¨å¹¿æ˜¾ç¤ºï¼Œç©ºé—²ä¸»æœºå¢åŠ äº†2.3-9.2ä¸ªç™¾åˆ†ç‚¹ï¼ˆppï¼‰ã€‚è¿™ç›´æ¥ä¸æ•ˆç‡ç›¸å…³ï¼Œå› ä¸º1ä¸ªç™¾åˆ†ç‚¹çš„æ”¹è¿›é€šå¸¸ç›¸å½“äºèŠ‚çœé›†ç¾¤å®¹é‡çš„1%ã€‚\n*   **èµ„æºææµ…å‡å°‘**ï¼šåœ¨ä¸€äº›è¯•ç‚¹å®éªŒä¸­ï¼ŒNILASå°†CPUææµ…å‡å°‘äº†çº¦3%ï¼Œå†…å­˜ææµ…å‡å°‘äº†2%ã€‚è¿™æ„å‘³ç€ä¸»æœºæ›´å¤šçš„èµ„æºå¯ä¾›æ–°VMä½¿ç”¨ã€‚\n*   **LAVAæ¨¡æ‹Ÿ**ï¼šè¡¨æ˜å®ƒå°†åœ¨NILASçš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥æé«˜çº¦0.4ä¸ªç™¾åˆ†ç‚¹ã€‚\n*   **LARSæ¨¡æ‹Ÿ**ï¼šè¡¨æ˜å®ƒæœ‰å¯èƒ½å°†ç»´æŠ¤æ‰€éœ€çš„VMå®æ—¶è¿ç§»æ¬¡æ•°å‡å°‘4.5%ã€‚\n\n### ç»“è®º\n\nè¿™é¡¹å·¥ä½œæ˜¯è¿ˆå‘æœºå™¨å­¦ä¹ ç³»ç»Ÿæ—¥ç›Šä¼˜åŒ–æ•°æ®ä¸­å¿ƒç®¡ç†æœªæ¥çš„åŸºç¡€æ€§ä¸€æ­¥ã€‚æ‰€å¼€å‘çš„æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯å†é¢„æµ‹çš„ä½¿ç”¨ä»¥åŠæ¨¡å‹å’Œç³»ç»Ÿçš„ååŒè®¾è®¡ï¼Œå…·æœ‰æ™®é€‚æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ä¸ç‰ºç‰²å¯é æ€§æˆ–å»¶è¿Ÿçš„æƒ…å†µä¸‹ï¼Œå°†å…ˆè¿›çš„æœºå™¨å­¦ä¹ æŠ€æœ¯é›†æˆåˆ°ç³»ç»ŸåŸºç¡€è®¾æ–½å †æ ˆçš„æœ€åº•å±‚æ˜¯å¯è¡Œçš„ï¼ŒåŒæ—¶è¿˜èƒ½å®ç°æ˜¾è‘—çš„æ•ˆç‡æå‡ã€‚",
      "shortSummary": "Googleå¼€å‘äº†LAVAç³»ç»Ÿï¼Œåˆ©ç”¨AIä¼˜åŒ–äº‘è®¡ç®—ä¸­çš„è™šæ‹Ÿæœºï¼ˆVMï¼‰åˆ†é…ã€‚è¯¥ç³»ç»Ÿé€šè¿‡â€œæŒç»­å†é¢„æµ‹â€æŠ€æœ¯ï¼ŒåŠ¨æ€æ›´æ–°VMç”Ÿå‘½å‘¨æœŸé¢„æµ‹ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å•æ¬¡é¢„æµ‹ä¸å‡†ç¡®çš„é—®é¢˜ã€‚LAVAåŒ…å«NILASã€LAVAå’ŒLARSä¸‰ç§ç®—æ³•ï¼Œæ—¨åœ¨æé«˜èµ„æºåˆ©ç”¨ç‡ã€å‡å°‘èµ„æºææµ…å¹¶ä¼˜åŒ–ç»´æŠ¤ã€‚NILASå·²åœ¨Googleç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ï¼Œæ˜¾è‘—å¢åŠ äº†ç©ºé—²ä¸»æœºå¹¶å‡å°‘äº†èµ„æºææµ…ã€‚é€šè¿‡å°†æ¨¡å‹ç›´æ¥ç¼–è¯‘åˆ°è°ƒåº¦å™¨ä¸­ï¼Œè§£å†³äº†å¤§è§„æ¨¡éƒ¨ç½²çš„æŒ‘æˆ˜ï¼Œç¡®ä¿äº†ä½å»¶è¿Ÿå’Œé«˜å¯é æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºMLé©±åŠ¨çš„æ•°æ®ä¸­å¿ƒç®¡ç†å¥ å®šäº†åŸºç¡€ã€‚",
      "translated_title": "è§£å†³è™šæ‹Ÿæœºéš¾é¢˜ï¼šAI å¦‚ä½•ä¼˜åŒ–äº‘è®¡ç®—",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/LAVA-2-Contributions.width-1250.png",
          "alt": "Graph of relative contributions by number and compute fraction.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/LAVA-1-Lifetimes.width-1250.png",
          "alt": "Plot showing distribution of VM lifetimes",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"pt8nw\">Imagine a puzzle game similar to Tetris with pieces rapidly falling onto a stack. Some fit perfectly. Others donâ€™t. The goal is to pack the blocks as tightly and efficiently as possible. This game is a loose analogy to the challenge faced by cloud data centers several times every second as they try to allocate processing jobs (called virtual machines or VMs) as efficiently as possible. But in this case, the â€œpiecesâ€ (or VMs) appear and disappear, some with a lifespan of only minutes, and others, days. In spite of the initially unknown VM lifespans, we still want to fill as much of the physical servers as possible with these VMs for the sake of efficiency. If only we knew the approximate lifespan of a job, we could clearly allocate much better.</p><p data-block-key=\"483c4\">At the scale of large data centers, efficient resource use is especially critical for both economic and environmental reasons. Poor VM allocation can lead to \"resource stranding\", where a server's remaining resources are too small or unbalanced to host new VMs, effectively wasting capacity. Poor VM allocation also reduces the number of \"empty hosts\", which are essential for tasks like system updates and provisioning large, resource-intensive VMs.</p><p data-block-key=\"3s50u\">This classic <a href=\"https://en.wikipedia.org/wiki/Bin_packing_problem\" target=\"_blank\" rel=\"noopener noreferrer\">bin packing problem</a> is made more complex by this incomplete information about VM behavior. AI can help with this problem by using learned models to predict VM lifetimes. However, this often relies on a single prediction at the VM's creation. The challenge with this approach is that a single misprediction can tie up an entire host for an extended period, degrading efficiency.</p><p data-block-key=\"8v1nd\">In â€œ<a href=\"https://arxiv.org/abs/2412.09840v1\" target=\"_blank\" rel=\"noopener noreferrer\">LAVA: Lifetime-Aware VM Allocation with Learned Distributions and Adaptation to Mispredictions</a>â€, we introduce a trio of algorithms â€” non-invasive lifetime aware scoring (NILAS), lifetime-aware VM allocation (LAVA), and lifetime-aware rescheduling (LARS) â€” which are designed to solve the bin packing problem of efficiently fitting VMs onto physical servers. This system uses a process we call â€œcontinuous repredictionâ€, which means it doesnâ€™t rely on the initial, one-time guess of a VMâ€™s lifespan made at its creation. Instead, the model constantly and automatically updates its prediction for a VM's expected remaining lifetime as the VM continues to run.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The secret life of VMs: Repredictions and probability distributions</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">One of the key insights driving this research is the recognition that VM lifetimes are often unpredictable and follow a <a href=\"https://en.wikipedia.org/wiki/Long_tail\" target=\"_blank\" rel=\"noopener noreferrer\">long-tailed distribution</a>. For example, while the vast majority of VMs (88%) live for less than an hour, these short-lived VMs consume only a tiny fraction (2%) of the total resources. This means that the placement of a small number of long-lived VMs has a disproportionately large impact on overall resource efficiency.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/LAVA-2-Contributions.width-1250.png\" alt=\"Graph of relative contributions by number and compute fraction.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/LAVA-2-Contributions.width-1250.png\" alt=\"Graph of relative contributions by number and compute fraction.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"b4xcm\"><i>Distribution of VM lifetimes of scheduled VMs (</i><b><i>left</i></b><i>) vs. their resource consumption (</i><b><i>right</i></b><i>). Interestingly, the shortest jobs (0â€“10 min, dark blue), which account for 53% by number, take a negligible fraction of resources. In contrast, the longest running jobs (&gt;30 days, orange), which take considerable resources (18%), amount to a negligible fraction by number.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"pt8nw\">Instead of trying to predict a single, average lifetime, which can be misleading for VMs with bi-modal or highly varied lifespans, we designed an ML model that predicts a probability distribution for a VM's lifetime. This approach, inspired by <a href=\"https://en.wikipedia.org/wiki/Survival_analysis\" target=\"_blank\" rel=\"noopener noreferrer\">survival analysis</a>, allows the model to capture the inherent uncertainty of a VM's behavior.</p><p data-block-key=\"91kfi\">More importantly, our system uses this distribution to continuously update its predictions. We ask, â€œGiven a VM has been running for five days, what is its expected remaining lifetime?â€ As a VM continues to run, the system gains more information, and its lifetime prediction becomes more accurate. Our algorithms are specifically co-designed to leverage these repredictions, actively responding to mispredictions and improving the accuracy over time.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/LAVA-1-Lifetimes.width-1250.png\" alt=\"Plot showing distribution of VM lifetimes\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/LAVA-1-Lifetimes.width-1250.png\" alt=\"Plot showing distribution of VM lifetimes\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"b4xcm\"><i>Lifetime distribution of VM lifetimes. When the VM is scheduled, the expected (average) lifetime is 0.2 days. After it has run for 1 day, the expected remaining lifetime is 4 days. After 7 days, the expected remaining lifetime is 10 days.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">A new class of scheduling algorithms</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">With this new, more robust prediction model, we developed three novel algorithms to improve VM allocation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">1. Non-Invasive Lifetime Aware Scheduling (NILAS)</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">NILAS is a non-invasive algorithm that incorporates lifetime predictions into an existing scoring function. It ranks potential hosts for a new VM by considering the repredicted exit times of all existing VMs on that host. By prioritizing hosts where all VMs are expected to exit at a similar time, NILAS aims to create more empty machines. Our use of repredictions is less sensitive to prediction accuracy and allows NILAS to correct for errors. The NILAS algorithm has been deployed on our <a href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43438.pdf\">large-scale cluster manager</a>, Borg, where it significantly improves VM allocation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">2. Lifetime-Aware VM Allocation (LAVA)</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">LAVA is a more fundamental departure from existing scheduling mechanisms. While NILAS aims to pack VMs with similar lifetimes, LAVA does the opposite: it puts shorter-lived VMs on hosts with one or more long-lived VMs. The goal is to fill in resource gaps with short-lived VMs that are at least an order of magnitude shorter than the hostâ€™s anticipated lifespan, so that they exit quickly without extending the hostâ€™s overall lifespan. LAVA also actively adapts to mispredictions by increasing a hostâ€™s anticipated lifespan if a VM outlives its expected deadline. Simulations show that this strategy minimizes fragmentation and ensures that hosts are eventually freed up.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">3. Lifetime-Aware Rescheduling (LARS)</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">LARS uses our lifetime predictions to minimize VM disruptions during defragmentation and maintenance. When a host needs to be defragmented, LARS sorts the VMs on that host by their predicted remaining lifetime and migrates the longest-lived VMs first. Shorter-lived VMs exit naturally before migration. Simulations with LARS indicate it has the potential to reduce the total number of migrations required by around 4.5%.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Addressing the challenge of deployment at scale</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">Developing powerful models and algorithms is only one part of the solution. Getting them to work reliably at large scale required us to rethink our approach to model deployment.</p><p data-block-key=\"cj0j9\">A common practice is to serve machine learning models on dedicated inference servers. However, this would have created a <a href=\"https://en.wikipedia.org/wiki/Circular_dependency\" target=\"_blank\" rel=\"noopener noreferrer\">circular dependency</a>, as these servers would themselves run on our cluster scheduling system. A failure in the model serving layer could then cause a cascading failure in the scheduler itself, which is unacceptable for a mission-critical system.</p><p data-block-key=\"34gvd\">Our solution was to compile the model directly into the <a href=\"https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/\">Borg scheduler binary</a>. This approach eliminated the circular dependency and ensured that the model was tested and rolled out with the same rigorous process as any other code change to the scheduler. This also yielded an additional benefit: the model's median latency is just 9 microseconds (Âµs), which is 780 times faster than a comparable approach that uses separate model servers. This low latency is crucial for running repredictions frequently and for using the model in performance-sensitive tasks, like maintenance and defragmentation.</p><p data-block-key=\"dpkp\">We also found that for our largest zones, the number of required predictions could become a bottleneck. We addressed this by introducing a host lifetime score cache, which only updates predictions when a VM is added or removed from a host, or when a host's expected lifetime expires. This caching mechanism ensures high performance and allows us to deploy our system fleet-wide.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">Our NILAS algorithm has been running in Google's production data centers since early 2024. The results are clear and significant.</p><ul><li data-block-key=\"27kt6\"><i>Increased empty hosts:</i> Our production pilots and fleet-wide rollouts have shown an increase in empty hosts by 2.3â€“9.2 percentage points (pp). This metric directly correlates with efficiency, as a 1 pp improvement is typically equivalent to saving 1% of a cluster's capacity.</li><li data-block-key=\"arb0l\"><i>Reduced resource stranding:</i> In some pilot experiments, NILAS reduced CPU stranding by approximately 3% and memory stranding by 2%. This means more of a host's resources are available to be used by new VMs.</li></ul><p data-block-key=\"7qme9\">Simulations running LAVA suggest it will provide a further ~0.4 pp improvement over NILAS. Similarly, simulations with LARS indicate that it has the potential to reduce the number of VM live migrations needed for maintenance by 4.5%.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">We believe this work is a foundational step towards a future where data center management is increasingly optimized by machine learning systems. The techniques we developed, particularly the use of repredictions and the co-design of models and systems, are generalizable to other tasks. We have demonstrated that it is possible to integrate advanced machine learning techniques into the lowest layers of a systemâ€™s infrastructure stack without sacrificing reliability or latency, while still delivering significant efficiency gains.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\"><i>LAVA is a large collaborative project that spanned multiple teams across Google, including Google Cloud, Google DeepMind, Google Research, and SystemsResearch@Google. Key contributors include Jianheng Ling, Pratik Worah, Yawen Wang, Yunchuan Kong, Anshul Kapoor, Chunlei Wang, Clifford Stein, Diwakar Gupta, Jason Behmer, Logan A. Bush, Prakash Ramanan, Rajesh Kumar, Thomas Chestna, Yajing Liu, Ying Liu, Ye Zhao, Kathryn S. McKinley, Meeyoung Park, and Martin Maas.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "ä½¿ç”¨AIé€šè¿‡DeepSomaticè¯†åˆ«è‚¿ç˜¤ä¸­çš„åŸºå› å˜å¼‚ (åŸæ ‡é¢˜: Using AI to identify genetic variants in tumors with DeepSomatic)",
      "link": "https://research.google/blog/using-ai-to-identify-genetic-variants-in-tumors-with-deepsomatic/",
      "pubDate": "Wed, 15 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-15T16:00:00.000Z",
      "creator": "Google",
      "summary": "DeepSomaticæ˜¯Google Researchä¸åŠ å·å¤§å­¦åœ£å…‹é²æ–¯åŸºå› ç»„å­¦ç ”ç©¶æ‰€ç­‰åˆä½œå¼€å‘çš„ä¸€æ¬¾äººå·¥æ™ºèƒ½å·¥å…·ï¼Œæ—¨åœ¨æ›´å‡†ç¡®åœ°è¯†åˆ«è‚¿ç˜¤ç»†èƒä¸­çš„åŸºå› å˜å¼‚ã€‚è¿™é¡¹å·¥ä½œå·²å‘è¡¨åœ¨ã€Šè‡ªç„¶ç”Ÿç‰©æŠ€æœ¯ã€‹æ‚å¿—ä¸Šï¼Œæ˜¯Googleåˆ©ç”¨AIæ–¹æ³•ç†è§£å’Œæ²»ç–—ç™Œç—‡çš„å¹¿æ³›åŠªåŠ›çš„ä¸€éƒ¨åˆ†ï¼Œç›®æ ‡æ˜¯åŠ é€Ÿç™Œç—‡ç ”ç©¶å¹¶æ¨è¿›ç²¾å‡†åŒ»ç–—ã€‚\n\n### ç™Œç—‡ä¸åŸºå› å˜å¼‚çš„æŒ‘æˆ˜\n*   **ç™Œç—‡çš„æœ¬è´¨**ï¼šç™Œç—‡æ˜¯ä¸€ç§åŸºå› ç–¾ç—…ï¼Œç»†èƒåˆ†è£‚çš„é—ä¼ æ§åˆ¶å‡ºç°å¼‚å¸¸ã€‚è¯†åˆ«è‚¿ç˜¤ç»†èƒä¸­çš„åŸºå› çªå˜æ˜¯ç ”ç©¶ç™Œç—‡å’Œåˆ¶å®šæ²»ç–—æ–¹æ¡ˆçš„å…³é”®æ­¥éª¤ã€‚\n*   **ä½“ç»†èƒå˜å¼‚çš„å¤æ‚æ€§**ï¼šä¸DeepVariantï¼ˆGoogleæ—©æœŸç”¨äºè¯†åˆ«é—ä¼ æ€§ç”Ÿæ®–ç³»å˜å¼‚çš„å·¥å…·ï¼‰ä¸åŒï¼ŒDeepSomaticä¸“æ³¨äºè¯†åˆ«ç™Œç—‡ä¸­æ›´å¤æ‚çš„ä½“ç»†èƒå˜å¼‚ã€‚è¿™äº›å˜å¼‚æ˜¯åœ¨å‡ºç”Ÿåè·å¾—çš„ï¼Œå¯èƒ½ç”±ç¯å¢ƒæš´éœ²æˆ–DNAå¤åˆ¶é”™è¯¯å¼•èµ·ã€‚\n*   **è¯†åˆ«éš¾åº¦**ï¼šè¯†åˆ«ä½“ç»†èƒå˜å¼‚æ¯”è¯†åˆ«é—ä¼ å˜å¼‚æ›´å›°éš¾ï¼Œå› ä¸ºè‚¿ç˜¤ç»†èƒå¯èƒ½åŒ…å«ä¸åŒé¢‘ç‡çš„å¤šç§å˜å¼‚ï¼Œä¸”æµ‹åºé”™è¯¯ç‡å¯èƒ½é«˜äºæ ·æœ¬ä¸­ä½“ç»†èƒå˜å¼‚çš„å®é™…é¢‘ç‡ã€‚\n\n### DeepSomaticçš„å·¥ä½œåŸç†ä¸è®­ç»ƒ\n*   **æ ¸å¿ƒæŠ€æœ¯**ï¼šDeepSomaticæ˜¯ä¸€ä¸ªçµæ´»çš„æ¨¡å‹ï¼Œåˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¥è¯†åˆ«è‚¿ç˜¤å˜å¼‚ã€‚å®ƒé€‚ç”¨äºæ‰€æœ‰ä¸»è¦çš„æµ‹åºå¹³å°ï¼Œæ”¯æŒä¸åŒç±»å‹çš„æ ·æœ¬å¤„ç†ï¼Œå¹¶èƒ½å°†å…¶å­¦ä¹ èƒ½åŠ›æ‰©å±•åˆ°æœªåŒ…å«åœ¨è®­ç»ƒä¸­çš„ç™Œç—‡ç±»å‹ã€‚\n*   **æ•°æ®å¤„ç†æµç¨‹**ï¼š\n    1.  å°†åŸºå› æµ‹åºæ•°æ®ï¼ˆæ¥è‡ªè‚¿ç˜¤ç»†èƒå’Œéç™Œç»†èƒï¼‰è½¬æ¢ä¸ºä¸€ç»„å›¾åƒï¼Œè¿™äº›å›¾åƒä»£è¡¨æµ‹åºæ•°æ®ã€æŸ“è‰²ä½“æ¯”å¯¹ã€è¾“å‡ºè´¨é‡åŠå…¶ä»–å˜é‡ã€‚\n    2.  DeepSomaticçš„å·ç§¯ç¥ç»ç½‘ç»œå¤„ç†è¿™äº›å›¾åƒã€‚\n    3.  åŒºåˆ†å‚è€ƒåŸºå› ç»„ã€ä¸ªä½“ä¸­çš„éç™Œç”Ÿæ®–ç³»å˜å¼‚ä»¥åŠè‚¿ç˜¤ä¸­ç”±ç™Œç—‡å¼•èµ·çš„ä½“ç»†èƒå˜å¼‚ï¼ŒåŒæ—¶æ’é™¤æµ‹åºè¿‡ç¨‹ä¸­äº§ç”Ÿçš„å¾®å°é”™è¯¯ã€‚\n    4.  æœ€ç»ˆè¾“å‡ºä¸€ä»½ç™Œç—‡ç›¸å…³å˜å¼‚ï¼ˆçªå˜ï¼‰åˆ—è¡¨ã€‚\n\n    ![DeepSomaticæ¦‚è§ˆ](https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-1-Overview.width-1250.png)\n    *DeepSomaticæ£€æµ‹åŸºå› ç»„æ•°æ®ä¸­çš„ç™Œç—‡å˜å¼‚ã€‚é¦–å…ˆï¼Œå°†è‚¿ç˜¤ç»†èƒå’Œéç™Œç»†èƒçš„æµ‹åºæ•°æ®è½¬æ¢ä¸ºå›¾åƒã€‚DeepSomaticé€šè¿‡å…¶å·ç§¯ç¥ç»ç½‘ç»œå¤„ç†è¿™äº›å›¾åƒï¼Œä»¥åŒºåˆ†å‚è€ƒåŸºå› ç»„ã€ä¸ªä½“ä¸­çš„éç™Œç”Ÿæ®–ç³»å˜å¼‚ä»¥åŠè‚¿ç˜¤ä¸­ç”±ç™Œç—‡å¼•èµ·çš„ä½“ç»†èƒå˜å¼‚ï¼ŒåŒæ—¶æ’é™¤å¾®å°çš„æµ‹åºé”™è¯¯ã€‚ç»“æœæ˜¯ç™Œç—‡å¼•èµ·çš„å˜å¼‚æˆ–çªå˜åˆ—è¡¨ã€‚*\n\n*   **è®­ç»ƒæ•°æ®ï¼ˆCASTLEï¼‰**ï¼š\n    *   ä¸ºè®­ç»ƒå‡†ç¡®æ¨¡å‹ï¼Œç ”ç©¶å›¢é˜Ÿä¸UC Santa Cruzå’Œç¾å›½å›½å®¶ç™Œç—‡ç ”ç©¶æ‰€åˆä½œï¼Œåˆ›å»ºäº†ä¸€ä¸ªæ–°çš„é«˜è´¨é‡è®­ç»ƒå’Œè¯„ä¼°æ•°æ®é›†ã€‚\n    *   å¯¹æ¥è‡ªå››ä¸ªä¹³è…ºç™Œæ ·æœ¬å’Œä¸¤ä¸ªè‚ºç™Œæ ·æœ¬ï¼ˆç ”ç©¶ç»†èƒç³»ï¼‰çš„è‚¿ç˜¤ç»†èƒå’Œä¼´éšçš„æ­£å¸¸ç»†èƒè¿›è¡Œäº†æµ‹åºã€‚\n    *   ä½¿ç”¨IlluminaçŸ­è¯»é•¿æµ‹åºã€PacBioé•¿è¯»é•¿æµ‹åºå’ŒOxford Nanopore Technologyé•¿è¯»é•¿æµ‹åºä¸‰ç§é¢†å…ˆå¹³å°è¿›è¡Œå…¨åŸºå› ç»„æµ‹åºã€‚\n    *   ç»“åˆæ‰€æœ‰ä¸‰ä¸ªå¹³å°çš„è¾“å‡ºï¼Œä»¥æ¶ˆé™¤å¹³å°ç‰¹å¼‚æ€§é”™è¯¯ï¼Œåˆ›å»ºäº†ä¸€ä¸ªåä¸ºâ€œç™Œç—‡æ ‡å‡†é•¿è¯»é•¿è¯„ä¼°æ•°æ®é›†â€ï¼ˆCASTLEï¼‰çš„å•ä¸€ã€å‡†ç¡®çš„å‚è€ƒæ•°æ®é›†ã€‚\n\n    ![çªå˜ç‡å›¾](https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-2-Variants.width-1250.png)\n    *ç”¨äºè®­ç»ƒDeepSomaticçš„åŸºå‡†æ•°æ®é›†ã€‚æ¯ä¸ªæ¡å½¢å›¾æ˜¾ç¤ºäº†åœ¨å››ä¸ªä¹³è…ºç™Œæ ·æœ¬å’Œä¸¤ä¸ªè‚ºç™Œæ ·æœ¬ä¸­å‘ç°çš„çªå˜æ•°é‡ï¼Œé¢œè‰²ä»£è¡¨ä¸åŒç±»å‹çš„çªå˜ã€‚è‚ºç™Œæ˜¾ç¤ºå‡ºç”±ç¯å¢ƒæ¯’ç´ å¼•èµ·çš„ä¸€ç§æ˜¾è‘—çªå˜ç±»å‹ï¼ŒåŒ…æ‹¬å›¾ä¸­ç»¿è‰²çš„SBS4ã€‚ä½†å³ä½¿æ˜¯åŒä¸€ç§ç™Œç—‡ï¼Œå…¶çªå˜ç‰¹å¾ä¹Ÿæ˜¾ç¤ºå‡ºå·¨å¤§å·®å¼‚ã€‚è¿™äº›ä¸ªä½“å·®å¼‚å¯ä»¥é¢„æµ‹å…¶å¯¹æ²»ç–—çš„ååº”æ•ˆæœã€‚*\n\n*   **â€œä»…è‚¿ç˜¤â€æ¨¡å¼**ï¼šDeepSomaticè¿˜èƒ½å¤Ÿåœ¨æ²¡æœ‰éè‚¿ç˜¤åºåˆ—å¯ç”¨çš„â€œä»…è‚¿ç˜¤â€æ¨¡å¼ä¸‹è¯†åˆ«ä½“ç»†èƒå˜å¼‚ï¼Œä¾‹å¦‚åœ¨ç™½è¡€ç—…ç­‰è¡€ç™Œä¸­ï¼Œå¾ˆéš¾ä»è¡€æ¶²æ ·æœ¬ä¸­è·å–çº¯å‡€çš„æ­£å¸¸ç»†èƒã€‚\n\n### DeepSomaticçš„æ€§èƒ½è¡¨ç°\n*   **é«˜å‡†ç¡®æ€§**ï¼šDeepSomaticæ¨¡å‹åœ¨æ‰€æœ‰ä¸‰ä¸ªä¸»è¦æµ‹åºå¹³å°ä¸Šå‡ä¼˜äºå…¶ä»–ç°æœ‰æ–¹æ³•ï¼Œä»¥æ›´é«˜çš„å‡†ç¡®æ€§è¯†åˆ«å‡ºæ›´å¤šçš„è‚¿ç˜¤å˜å¼‚ã€‚\n*   **æ“…é•¿è¯†åˆ«æ’å…¥å’Œç¼ºå¤±ï¼ˆIndelsï¼‰**ï¼š\n    *   åœ¨Illuminaæµ‹åºæ•°æ®ä¸Šï¼ŒDeepSomaticåœ¨è¯†åˆ«Indelsæ–¹é¢çš„F1-scoreè¾¾åˆ°90%ï¼Œè€Œæ¬¡ä¼˜æ–¹æ³•ä¸º80%ã€‚\n    *   åœ¨Pacific Biosciencesæµ‹åºæ•°æ®ä¸Šï¼ŒDeepSomaticçš„F1-scoreè¶…è¿‡80%ï¼Œè€Œæ¬¡ä¼˜æ–¹æ³•ä¸åˆ°50%ã€‚\n\n    ![ä¹³è…ºç™Œå‡†ç¡®æ€§å›¾](https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-3-BreastCancer.width-1250.png)\n    *DeepSomaticï¼ˆç´«è‰²ï¼‰åœ¨ç ”ç©¶ä¸­å¹¿æ³›ä½¿ç”¨çš„ä¹³è…ºç™Œæ ·æœ¬ä¸Šçš„ç»“æœï¼Œä¸å…¶ä»–å·¥å…·è¿›è¡Œæ¯”è¾ƒã€‚æœ‰å‡ ç§è½¯ä»¶å·¥å…·å¯ä»¥è¯†åˆ«Illuminaæ•°æ®ä¸­çš„ç™Œç—‡å˜å¼‚ï¼Œè€Œå¯¹äºPacBioå’ŒOxford Nanopore Technologiesç”Ÿæˆçš„é•¿è¯»é•¿æµ‹åºæ•°æ®ï¼Œåªæœ‰ä¸€ç§æ›¿ä»£æ–¹æ¡ˆï¼ˆç²‰è‰²ï¼‰ã€‚F1-scoreè¡¡é‡å‘ç°çš„å˜å¼‚æ•°é‡å’Œå‡†ç¡®æ€§ã€‚DeepSomaticåœ¨è¯†åˆ«å•å­—æ¯åŸºå› ä»£ç å˜å¼‚ï¼ˆå•æ ¸è‹·é…¸å˜å¼‚ï¼‰æ–¹é¢è¡¨ç°ç•¥å¥½ï¼Œå¹¶åœ¨æ¶‰åŠIndelsçš„å˜å¼‚æ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹è¿›ã€‚*\n\n*   **å¤„ç†å¤æ‚æ ·æœ¬çš„èƒ½åŠ›**ï¼š\n    *   åœ¨ç¦å°”é©¬æ—å›ºå®šçŸ³èœ¡åŒ…åŸ‹ï¼ˆFFPEï¼‰æ ·æœ¬ï¼ˆä¸€ç§å¸¸è§çš„ç»„ç»‡ä¿å­˜æ–¹æ³•ï¼Œä¼šå¼•å…¥DNAæŸä¼¤ï¼‰ä¸Šï¼ŒDeepSomaticè¡¨ç°å‡ºè‰²ã€‚\n    *   åœ¨å…¨å¤–æ˜¾å­ç»„æµ‹åºï¼ˆWESï¼‰æ•°æ®ï¼ˆä¸€ç§æ›´ç»æµçš„æ–¹æ³•ï¼Œä»…å…³æ³¨åŸºå› ç»„ä¸­ç¼–ç è›‹ç™½è´¨çš„çº¦1%ï¼‰ä¸Šï¼ŒDeepSomaticä¹Ÿä¼˜äºå…¶ä»–å·¥å…·ã€‚\n    *   è¿™è¡¨æ˜DeepSomaticå¯ç”¨äºåˆ†æè´¨é‡è¾ƒä½æˆ–å†å²è‚¿ç˜¤æ ·æœ¬ï¼Œå¹¶é€‚ç”¨äºä»…è¿›è¡Œå¤–æ˜¾å­ç»„æµ‹åºçš„ä¸´åºŠæ•°æ®ã€‚\n\n    ![FFPEå’ŒWESå‡†ç¡®æ€§å›¾](https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-4-FFPE-WES.width-1250.png)\n    *DeepSomaticåœ¨ç»è¿‡æ›´å¤æ‚é¢„å¤„ç†æ­¥éª¤çš„æ ·æœ¬ä¸Šå…·æœ‰æ˜¾è‘—æ›´é«˜çš„å‡†ç¡®æ€§ï¼ŒåŒ…æ‹¬ï¼šç¦å°”é©¬æ—å›ºå®šçŸ³èœ¡åŒ…åŸ‹ï¼ˆFFPEï¼‰ï¼Œä¸€ç§ç”¨äºä¿å­˜ç»„ç»‡æ ·æœ¬çš„æ–¹æ³•ï¼ˆå·¦ï¼‰ï¼Œä»¥åŠå…¨å¤–æ˜¾å­ç»„æµ‹åºï¼ˆWESï¼‰ï¼Œä¸€ç§ä»…å¯¹åŸºå› ç»„ä¸­ç¼–ç è›‹ç™½è´¨çš„éƒ¨åˆ†è¿›è¡Œæµ‹åºçš„æ–¹æ³•ï¼ˆå³ï¼‰ã€‚ä¸­é—´éƒ¨åˆ†æ˜¾ç¤ºäº†ä¸€ä¸ªç»è¿‡FFPEä¿å­˜å¹¶ä½¿ç”¨å…¨å¤–æ˜¾å­ç»„æµ‹åºçš„æ ·æœ¬ã€‚*\n\n### æ¨å¹¿åˆ°å…¶ä»–ç™Œç—‡ç±»å‹\n*   **èƒ¶è´¨æ¯ç»†èƒç˜¤**ï¼šDeepSomaticæˆåŠŸè¯†åˆ«äº†è¿™ç§ä¾µè¢­æ€§è„‘ç™Œçš„å˜å¼‚ï¼Œå±•ç¤ºäº†å…¶å­¦ä¹ æ³›åŒ–èƒ½åŠ›ã€‚\n*   **å„¿ç«¥ç™½è¡€ç—…**ï¼šä¸å ªè¨æ–¯åŸå„¿ç«¥æ…ˆå–„åŒ»é™¢åˆä½œï¼ŒDeepSomaticåˆ†æäº†å…«ä¸ªå·²æµ‹åºçš„å„¿ç«¥ç™½è¡€ç—…æ ·æœ¬ï¼Œä¸ä»…è¯†åˆ«äº†å·²çŸ¥çš„å˜å¼‚ï¼Œè¿˜å‘ç°äº†10ä¸ªæ–°å˜å¼‚ï¼Œè¯æ˜äº†å…¶åœ¨â€œä»…è‚¿ç˜¤â€æ ·æœ¬ä¸Šçš„æœ‰æ•ˆæ€§ã€‚\n\n### æœªæ¥å±•æœ›\n*   ç ”ç©¶å®éªŒå®¤å’Œä¸´åºŠåŒ»ç”Ÿæœ‰æœ›å¼€å§‹ä½¿ç”¨DeepSomaticå·¥å…·ã€‚\n*   æ£€æµ‹å·²çŸ¥ç™Œç—‡å˜å¼‚æœ‰åŠ©äºé€‰æ‹©ç°æœ‰æ²»ç–—æ–¹æ¡ˆï¼ˆå¦‚åŒ–ç–—ã€å…ç–«ç–—æ³•ï¼‰ã€‚\n*   è¯†åˆ«æ–°ç™Œç—‡å˜å¼‚å¯èƒ½ä¿ƒæˆå…¨æ–°çš„ç–—æ³•ã€‚\n*   æœ€ç»ˆç›®æ ‡æ˜¯æ›´æ·±å…¥åœ°äº†è§£æ¯ä¸ªè‚¿ç˜¤ï¼Œå‘ç°å…¶é©±åŠ¨å› ç´ ï¼Œå¹¶ä¸ºæ‚£è€…æä¾›æœ€æœ‰æ•ˆçš„æ²»ç–—ã€‚",
      "shortSummary": "DeepSomaticæ˜¯Google Researchæ¨å‡ºçš„ä¸€æ¬¾AIå·¥å…·ï¼Œåˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œæ›´å‡†ç¡®åœ°è¯†åˆ«è‚¿ç˜¤ä¸­çš„ä½“ç»†èƒåŸºå› å˜å¼‚ã€‚å®ƒèƒ½å¤„ç†å¤šç§æµ‹åºå¹³å°æ•°æ®ï¼Œå¹¶æ”¯æŒè‚¿ç˜¤-æ­£å¸¸æ ·æœ¬é…å¯¹åŠä»…è‚¿ç˜¤æ ·æœ¬æ¨¡å¼ã€‚DeepSomaticåœ¨è¯†åˆ«æ’å…¥å’Œç¼ºå¤±ï¼ˆIndelsï¼‰æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶èƒ½æœ‰æ•ˆåˆ†æFFPEå’ŒWESç­‰å¤æ‚æ ·æœ¬ã€‚è¯¥å·¥å…·å·²å¼€æºï¼Œæ—¨åœ¨åŠ é€Ÿç™Œç—‡ç ”ç©¶ï¼Œæ¨åŠ¨ç²¾å‡†åŒ»ç–—ï¼Œå¸®åŠ©ä¸´åºŠåŒ»ç”Ÿé€‰æ‹©ç°æœ‰ç–—æ³•å¹¶å‘ç°æ½œåœ¨æ–°ç–—æ³•ã€‚",
      "translated_title": "ä½¿ç”¨AIé€šè¿‡DeepSomaticè¯†åˆ«è‚¿ç˜¤ä¸­çš„åŸºå› å˜å¼‚",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-1-Overview.width-1250.png",
          "alt": "Overview of DeepSomatic",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-2-Variants.width-1250.png",
          "alt": "Plot of mutation rates",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-3-BreastCancer.width-1250.png",
          "alt": "Plot of accuracy on breast cancer",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-4-FFPE-WES.width-1250.png",
          "alt": "Plot of accuracy on FFPE & WES",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ry233\">Cancer is fundamentally a genetic disease in which the genetic controls on cell division go awry. Many types of cancer exist, and each poses unique challenges as it can have distinct genetic underpinnings. A powerful way to study cancer, and a critical step toward creating a treatment plan, is to identify the genetic mutations in tumor cells. Indeed, clinicians will now often sequence the genomes of biopsied tumor cells to inform treatment plans that specifically disrupt how that cancer grows.</p><p data-block-key=\"ftn7v\">With partners at the University of California, Santa Cruz <a href=\"https://genomics.ucsc.edu/\" target=\"_blank\" rel=\"noopener noreferrer\">Genomics Institute</a> and other federal and academic researchers, our new paper, â€œ<a href=\"https://www.nature.com/articles/s41587-025-02839-x\" target=\"_blank\" rel=\"noopener noreferrer\">DeepSomatic: Accurate somatic small variant discovery for multiple sequencing technologies</a>â€ in <a href=\"https://www.nature.com/nbt/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature Biotechnology</i></a> presents a tool that leverages machine learning to identify genetic variants in tumor cells more accurately than current methods. DeepSomatic is a flexible model that uses convolutional neural networks to identify tumor variants. It works on data from all major sequencing platforms, for different types of sample processing, and can extend its learning to cancer types not included in training.</p><p data-block-key=\"8b6u7\">We have made both <a href=\"https://github.com/google/deepsomatic\" target=\"_blank\" rel=\"noopener noreferrer\">the tool</a> and the <a href=\"https://github.com/CASTLE-Panel/castle\" target=\"_blank\" rel=\"noopener noreferrer\">high-quality training dataset</a> we created openly available to the research community. This work is part of broader Google efforts to develop AI methods to understand cancer and help scientists treat cancer, including analyzing mammogram images for <a href=\"https://health.google/mammography/\" target=\"_blank\" rel=\"noopener noreferrer\">breast cancer screening</a>, CT scans for <a href=\"https://research.google/blog/computer-aided-diagnosis-for-lung-cancer-screening/\">lung cancer screening</a>, as well as a partnership aimed at using AI to <a href=\"https://blog.google/technology/health/google-ai-institute-womens-cancers/\" target=\"_blank\" rel=\"noopener noreferrer\">advance research on gynecological cancers</a>. Our hope is to speed cancer research and further the goal of precision medicine.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Genetic variation acquired after birth</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\">Genome sequencing is used in research and medical clinics to identify genetic variations between an individual and the <a href=\"https://www.genome.gov/genetics-glossary/Human-Genome-Reference-Sequence\" target=\"_blank\" rel=\"noopener noreferrer\">human reference genome</a>. Distinguishing between real variants and simple errors made during the sequencing process is challenging. Thatâ€™s why almost a decade ago Google Research introduced <a href=\"https://research.google/blog/deepvariant-highly-accurate-genomes-with-deep-neural-networks/\">DeepVariant</a> to identify inherited variants, also called <a href=\"https://www.genomicseducation.hee.nhs.uk/genotes/knowledge-hub/constitutional-germline-vs-somatic-tumour-variants/\" target=\"_blank\" rel=\"noopener noreferrer\">germline variants</a>, that came from parents and are found in all of the bodyâ€™s cells.</p><p data-block-key=\"c63af\">The genetics of cancer is more complex. Cancer is often driven by variants acquired after birth. Environmental exposure that damages DNA, such as UV light or chemical carcinogens, as well as random errors that occur during DNA replication, can cause cells in the body, known as somatic cells, to acquire new variants. Sometimes, these acquired variants change the normal behavior of cells, and can cause them to replicate when they shouldnâ€™t. This process drives the initial development of cancer, as well as its later progression to more fast-growing and invasive stages.</p><p data-block-key=\"1tabm\">Identifying variants specific to some of a personâ€™s somatic cells is much harder than identifying inherited variants. Tumor cells can contain a diverse set of acquired variants at different frequencies, and the error rate of sequencing can be higher than the rate a somatic variant is present in a sample.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Training DeepSomatic to spot genetic variation in tumor cells</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\">We developed <a href=\"https://github.com/google/deepsomatic\" target=\"_blank\" rel=\"noopener noreferrer\">DeepSomatic</a> to address these challenges and accurately identify somatic variants. In most clinical and research settings, cancer is studied by sequencing the tumor cells acquired through biopsy, as well as normal cells that are unaffected by the tumor growth and contain more typical inherited genetic variations. DeepSomatic is trained to identify variations observed in tumor cells that are not inherited variants. These types of variations can provide critical insights about which variations are driving the tumor growth. DeepSomatic is also able to identify somatic variation in tumor-only mode where a non-tumor sequence is not available, for example in a blood cancer like leukemia where it is hard to get only normal cells from a blood draw. The ability to extend to different types of use-cases that follow common ways clinicians and researchers study cancer makes DeepSomatic applicable to many research and clinical settings.</p><p data-block-key=\"ae6vl\">Like our earlier tool, <a href=\"https://research.google/blog/deepvariant-highly-accurate-genomes-with-deep-neural-networks/\">DeepVariant</a>, the DeepSomatic model works by first turning genetic sequencing data into a <a href=\"https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/\" target=\"_blank\" rel=\"noopener noreferrer\">set of images</a>. The images represent the sequencing data, alignment along the chromosome, the quality of the output, and other variables. DeepSomatic then uses its <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\" target=\"_blank\" rel=\"noopener noreferrer\">convolutional neural network</a> on data from tumor cells and non-cancerous cells to differentiate between the reference genome, the non-cancer germline variants in that individual, and the cancer-caused somatic variants in the tumor, while discarding variations caused by small errors acquired during the sequencing process. The result is a list of cancer-related variants, or mutations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-1-Overview.width-1250.png\" alt=\"Overview of DeepSomatic\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-1-Overview.width-1250.png\" alt=\"Overview of DeepSomatic\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"48u3i\">DeepSomatic detects cancer variants in genomic data. First, sequencing data from the tumor cells and non-cancerous cells are turned into an image. DeepSomatic passes these images through its convolutional neural network to differentiate between the reference genome, the non-cancer germline variants in that individual, and the cancer-caused somatic variants in the tumor, while discarding variations caused by small sequencing errors. The result is a list of cancer-caused variants, or mutations.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ry233\">Training accurate models that can identify genetic variation for different cancer types requires comprehensive, high-quality data and truth sets. For this work we created a new training and evaluation dataset for detecting variants in tumor cells. With our partners at UC Santa Cruz and the <a href=\"https://www.cancer.gov/\" target=\"_blank\" rel=\"noopener noreferrer\">National Cancer Institute</a>, we sequenced tumor cells and accompanying normal cells from four breast cancer samples and two lung cancer samples from research cell lines.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-2-Variants.width-1250.png\" alt=\"Plot of mutation rates\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-2-Variants.width-1250.png\" alt=\"Plot of mutation rates\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"48u3i\">Benchmark dataset used to train DeepSomatic. Each bar shows the number of mutations found in four breast cancer samples and two lung cancer samples, with color representing different types of mutations. Lung cancer displays a notable type of mutation caused by environmental toxins, including <a href=\"https://signal.mutationalsignatures.com/explore/referenceCancerSignature/63/prevalence\" target=\"_blank\" rel=\"noopener noreferrer\">SBS4</a> shown in green. But even the same type of cancer shows big differences in its mutational signature. These individual differences can predict how well it will respond to a treatment.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ry233\">To create an accurate training dataset, we did whole-genome sequencing of these six samples using three leading platforms: <a href=\"https://www.illumina.com/systems/sequencing-platforms.html\" target=\"_blank\" rel=\"noopener noreferrer\">Illuminaâ€™s short-read sequencing</a>, <a href=\"https://www.pacb.com/technology/hifi-sequencing/\" target=\"_blank\" rel=\"noopener noreferrer\">PacBioâ€™s long-read sequencing</a>, and <a href=\"https://nanoporetech.com/platform\" target=\"_blank\" rel=\"noopener noreferrer\">Oxford Nanopore Technologyâ€™s long-read sequencing</a>. Output from all three platforms was combined to remove platform-specific errors and create a single, accurate reference dataset we call the <a href=\"https://github.com/CASTLE-Panel/castle\" target=\"_blank\" rel=\"noopener noreferrer\">Cancer Standards Long-read Evaluation dataset</a> (CASTLE) for genetic diversity in tumor and normal cells.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Testing DeepSomaticâ€™s ability to spot cancer-related variants</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\">We trained DeepSomatic on three of the breast cancer genomes and the two lung cancer genomes in the CASTLE reference dataset. We then tested DeepSomaticâ€™s performance in several ways, including on the single breast cancer genome that was not included in its training data, and on chromosome 1 from each sample, which we also excluded from the training.</p><p data-block-key=\"1u39v\">Results show that DeepSomatic models developed for each of the three major sequencing platforms performed better than other methods, identifying more tumor variants with higher accuracy. The tools used for comparison on short-read sequencing data were <a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC4242521/\" target=\"_blank\" rel=\"noopener noreferrer\">SomaticSniper</a>, <a href=\"https://gatk.broadinstitute.org/hc/en-us/articles/360037593851-Mutect2\" target=\"_blank\" rel=\"noopener noreferrer\">MuTect2</a> and <a href=\"https://www.nature.com/articles/s41592-018-0051-x\" target=\"_blank\" rel=\"noopener noreferrer\">Strelka2</a> (with SomaticSniper specifically for single nucleotide variants, or SNVs). For long-read sequencing data we compared against <a href=\"https://www.biorxiv.org/content/10.1101/2023.08.17.553778v1\" target=\"_blank\" rel=\"noopener noreferrer\">ClairS</a>, a deep learning model trained on synthetic data.</p><p data-block-key=\"9ghlt\">In our tests DeepSomatic identified 329,011 somatic variants across the six reference cell lines and a seventh preserved sample. DeepSomatic does particularly well at identifying cancer variations that involve insertions and deletions (â€œIndelsâ€) of genetic code. For these types of variants, DeepSomatic substantially increased the <a href=\"https://en.wikipedia.org/wiki/F-score\" target=\"_blank\" rel=\"noopener noreferrer\">F1-score</a>, a balanced measure of how well the model finds true variants in a sample (recall) while not making false positives (precision). On Illumina sequencing data the next-best method scored 80% at identifying Indels, while DeepSomatic scored 90%. On Pacific Biosciences sequencing data, the next-best method scored less than 50% at identifying Indels, and DeepSomatic scored more than 80%.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-3-BreastCancer.width-1250.png\" alt=\"Plot of accuracy on breast cancer\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-3-BreastCancer.width-1250.png\" alt=\"Plot of accuracy on breast cancer\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"48u3i\">DeepSomatic results (<b>purple</b>) for a breast cancer sample widely used in research, compared to other tools. Several software tools identify cancer variants in Illuminaâ€™s data, while only a single alternative (<b>pink</b>) exists for the long-read sequencing data generated by PacBio and Oxford Nanopore Technologies. The F1-score measures how many variants are discovered and with what accuracy. DeepSomatic performs slightly better for single-letter variations in genetic code, known as single nucleotide variations, and shows major improvements for variations that involve Indels.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ry233\">The seventh sample was one of the previously used research cell lines of a breast cancer tumor that was preserved using <a href=\"https://en.wikipedia.org/wiki/Fixation_(histology)\" target=\"_blank\" rel=\"noopener noreferrer\">formalin-fixed-paraffin-embedded</a> (FFPE). This common preservation method introduces additional patterns of DNA damage that can complicate genetic analysis. This sample was also sequenced using whole <a href=\"https://en.wikipedia.org/wiki/Exome_sequencing\" target=\"_blank\" rel=\"noopener noreferrer\">exome sequencing</a> (WES), a more affordable method that focuses only on the roughly 1% of the genome that codes for proteins. When DeepSomatic was trained on these types of sample data and then tested on chromosome 1, which was reserved from training, it again outperformed other tools, suggesting it can be used to identify variants in lower-quality or historic tumor samples, potentially rescuing samples that have been harder to sequence, and working on clinical data where only the exome was sequenced.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-4-FFPE-WES.width-1250.png\" alt=\"Plot of accuracy on FFPE &amp; WES\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-4-FFPE-WES.width-1250.png\" alt=\"Plot of accuracy on FFPE &amp; WES\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"48u3i\">DeepSomatic has notably higher accuracy on samples prepared with more complicated pre-processing steps involving: <a href=\"https://en.wikipedia.org/wiki/Fixation_(histology)\" target=\"_blank\" rel=\"noopener noreferrer\">fixed formalin paraffin embedded</a> (FFPE), a method used to preserve tissue samples (<b>left</b>), and whole <a href=\"https://en.wikipedia.org/wiki/Exome_sequencing\" target=\"_blank\" rel=\"noopener noreferrer\">exome sequencing</a> (WES), a method to sequence only the parts of the genome that code for proteins (<b>right</b>). The middle section shows a sample that was preserved with FFPE and also sequenced using whole exome sequencing.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Applying DeepSomatic to other cancers</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\">To test DeepSomaticâ€™s performance on other types of cancers, we analyzed a single sample of <a href=\"https://www.mayoclinic.org/diseases-conditions/glioblastoma/symptoms-causes/syc-20569077\" target=\"_blank\" rel=\"noopener noreferrer\">glioblastoma</a>, an aggressive form of brain cancer that arises from a small number of variants. DeepSomatic was able to pinpoint those variants, showing that it can generalize its learning to apply it to a different cancer type.</p><p data-block-key=\"2ga4q\">We also worked with partners at <a href=\"https://www.childrensmercy.org/childrens-mercy-research-institute/about/\" target=\"_blank\" rel=\"noopener noreferrer\">Childrenâ€™s Mercy</a> in Kansas City to analyze eight previously sequenced samples of <a href=\"https://www.childrensmercy.org/departments-and-clinics/division-of-pediatric-hematology-oncology-and-blood-and-marrow-transplantation/cancer-center/leukemia-and-lymphoma-program/understanding-leukemia/\" target=\"_blank\" rel=\"noopener noreferrer\">pediatric leukemia</a>, a cancer of the white blood cells that is the most common childhood cancer. Leukemia exists in the bloodstream, so a â€œnormalâ€ non-cancer blood sample is not possible. Despite that challenge, DeepSomatic identified the previously known variants as well as 10 new ones, showing that it can work with a tumor-only sample.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Whatâ€™s next</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\">Our hope is that research labs and clinicians can begin to use this tool. Detecting known cancer variants could help choose between existing treatments, such as chemotherapy, immunotherapy or other methods. Identifying new cancer variants could potentially lead to brand-new therapies. We hope people can take these tools and learn more about each cancer tumor, find whatâ€™s driving it, and ultimately deliver the most effective treatments to patients.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\"><i>We thank all research participants whose participation in research programs and donation of cell lines made this work and other biomedical research possible. We thank our collaborators at UC Santa Cruz Genomics Institute, the National Cancer Institute, the Frederick National Laboratory for Cancer Research, Childrenâ€™s Mercy Hospital, and NYU. We thank Hannah Hickey for writing contributions. We thank Avinatan Hassidim, Katherine Chou, Lizzie Dorfman, and Yossi Matias for research leadership support. We thank Resham Parikh and Isha Mishra for communications support.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Coral NPUï¼šé¢å‘è¾¹ç¼˜AIçš„å…¨æ ˆå¹³å° (åŸæ ‡é¢˜: Coral NPU: A full-stack platform for Edge AI)",
      "link": "https://research.google/blog/coral-npu-a-full-stack-platform-for-edge-ai/",
      "pubDate": "Tue, 14 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-14T16:00:00.000Z",
      "creator": "Google",
      "summary": "# Coral NPUï¼šé¢å‘è¾¹ç¼˜AIçš„å…¨æ ˆå¹³å°\n\nç”Ÿæˆå¼AIæå¤§åœ°æ”¹å˜äº†æˆ‘ä»¬å¯¹æŠ€æœ¯çš„æœŸæœ›ï¼Œä½†æœªæ¥çš„æŠ€æœ¯é£è·ƒåœ¨äºå°†AIæ™ºèƒ½ç›´æ¥åµŒå…¥åˆ°æˆ‘ä»¬çš„ä¸ªäººç¯å¢ƒä¸­ï¼Œä½¿å…¶èƒ½åœ¨æˆ‘ä»¬ç©¿æˆ´å’Œæºå¸¦çš„è®¾å¤‡ä¸Šè¿è¡Œï¼Œå®ç°çœŸæ­£çš„è¾…åŠ©æ€§ã€ç§å¯†ä¸”å…¨å¤©å€™çš„ä½“éªŒã€‚ç„¶è€Œï¼Œè¿™å¸¦æ¥äº†ä¸‰å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼š\n\n*   **æ€§èƒ½å·®è·**ï¼šå¤æ‚çš„æœºå™¨å­¦ä¹ æ¨¡å‹éœ€è¦å¤§é‡è®¡ç®—ï¼Œè¿œè¶…è¾¹ç¼˜è®¾å¤‡çš„åŠŸè€—ã€æ•£çƒ­å’Œå†…å­˜é™åˆ¶ã€‚\n*   **ç¢ç‰‡åŒ–æˆæœ¬**ï¼šä¸ºå¤šæ ·åŒ–çš„ä¸“æœ‰å¤„ç†å™¨ç¼–è¯‘å’Œä¼˜åŒ–MLæ¨¡å‹æ—¢å›°éš¾åˆæ˜‚è´µï¼Œé˜»ç¢äº†è®¾å¤‡é—´æ€§èƒ½çš„ä¸€è‡´æ€§ã€‚\n*   **ç”¨æˆ·ä¿¡ä»»èµ¤å­—**ï¼šä¸ªäººAIå¿…é¡»ä¼˜å…ˆè€ƒè™‘ä¸ªäººæ•°æ®çš„éšç§å’Œå®‰å…¨ã€‚\n\nä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº† **Coral NPU**ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨æ ˆå¹³å°ï¼Œæ—¨åœ¨ä¸ºç¡¬ä»¶è®¾è®¡è€…å’ŒMLå¼€å‘è€…æä¾›æ„å»ºä¸‹ä¸€ä»£ç§å¯†ã€é«˜æ•ˆè¾¹ç¼˜AIè®¾å¤‡æ‰€éœ€çš„å·¥å…·ã€‚Coral NPUä¸Google Researchå’ŒGoogle DeepMindåˆä½œè®¾è®¡ï¼Œæ˜¯ä¸€ç§AIä¼˜å…ˆçš„ç¡¬ä»¶æ¶æ„ï¼Œæ—¨åœ¨å®ç°è¶…ä½åŠŸè€—ã€å§‹ç»ˆåœ¨çº¿çš„è¾¹ç¼˜AIã€‚å®ƒæä¾›ç»Ÿä¸€çš„å¼€å‘ä½“éªŒï¼Œç®€åŒ–äº†ç¯å¢ƒæ„ŸçŸ¥ç­‰åº”ç”¨çš„éƒ¨ç½²ï¼Œå¹¶ä¸“ä¸ºåœ¨å¯ç©¿æˆ´è®¾å¤‡ä¸Šå®ç°å…¨å¤©å€™AIè€Œè®¾è®¡ï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°å‡å°‘ç”µæ± ä½¿ç”¨ï¼Œå¹¶å¯é…ç½®ä»¥æ”¯æŒæ›´é«˜æ€§èƒ½çš„ç”¨ä¾‹ã€‚\n\n## Coral NPUï¼šAIä¼˜å…ˆçš„æ¶æ„\n\nä¸ºä½åŠŸè€—è¾¹ç¼˜è®¾å¤‡è¿›è¡Œå¼€å‘çš„å¼€å‘è€…é¢ä¸´ä¸€ä¸ªåŸºæœ¬æƒè¡¡ï¼šé€‰æ‹©é€šç”¨CPUè¿˜æ˜¯ä¸“ç”¨åŠ é€Ÿå™¨ã€‚é€šç”¨CPUæä¾›çµæ´»æ€§å’Œå¹¿æ³›çš„è½¯ä»¶æ”¯æŒï¼Œä½†ç¼ºä¹é’ˆå¯¹MLå·¥ä½œè´Ÿè½½çš„é¢†åŸŸç‰¹å®šæ¶æ„ï¼Œå¯¼è‡´æ€§èƒ½ä½ä¸‹ä¸”åŠŸè€—æ•ˆç‡ä¸é«˜ã€‚ç›¸åï¼Œä¸“ç”¨åŠ é€Ÿå™¨æä¾›é«˜MLæ•ˆç‡ï¼Œä½†ç¼ºä¹çµæ´»æ€§ï¼Œéš¾ä»¥ç¼–ç¨‹ï¼Œä¸é€‚åˆé€šç”¨ä»»åŠ¡ã€‚\n\nCoral NPUæ¶æ„é€šè¿‡é¢ è¦†ä¼ ç»Ÿçš„èŠ¯ç‰‡è®¾è®¡æ¥ç›´æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å®ƒä¼˜å…ˆè€ƒè™‘MLçŸ©é˜µå¼•æ“è€Œéæ ‡é‡è®¡ç®—ï¼Œä»èŠ¯ç‰‡å±‚é¢ä¼˜åŒ–AIæ¶æ„ï¼Œåˆ›å»ºäº†ä¸€ä¸ªä¸“ä¸ºæ›´é«˜æ•ˆçš„è®¾å¤‡ç«¯æ¨ç†è€Œæ„å»ºçš„å¹³å°ã€‚\n\nä½œä¸ºä¸€å¥—å®Œæ•´çš„å‚è€ƒç¥ç»ç½‘ç»œå¤„ç†å•å…ƒï¼ˆNPUï¼‰æ¶æ„ï¼ŒCoral NPUä¸ºä¸‹ä¸€ä»£èŠ‚èƒ½ã€MLä¼˜åŒ–ç³»ç»Ÿçº§èŠ¯ç‰‡ï¼ˆSoCï¼‰æä¾›äº†æ„å»ºæ¨¡å—ã€‚è¯¥æ¶æ„åŸºäºä¸€å¥—ç¬¦åˆRISC-V ISAçš„æ¶æ„IPå—ï¼Œæ—¨åœ¨å®ç°æœ€å°åŠŸè€—ï¼Œéå¸¸é€‚åˆå§‹ç»ˆåœ¨çº¿çš„ç¯å¢ƒæ„ŸçŸ¥ã€‚åŸºç¡€è®¾è®¡æä¾›512 GOPSï¼ˆæ¯ç§’åäº¿æ¬¡æ“ä½œï¼‰çš„æ€§èƒ½ï¼ŒåŒæ—¶ä»…æ¶ˆè€—å‡ æ¯«ç“¦ï¼Œä»è€Œä¸ºè¾¹ç¼˜è®¾å¤‡ã€å¬æˆ´è®¾å¤‡ã€ARçœ¼é•œå’Œæ™ºèƒ½æ‰‹è¡¨æä¾›å¼ºå¤§çš„è®¾å¤‡ç«¯AIèƒ½åŠ›ã€‚\n\n![Coral NPU ç”Ÿæ€ç³»ç»Ÿ](https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-1.width-1250.png)\n*Coral NPUç”Ÿæ€ç³»ç»Ÿçš„ç»Ÿä¸€è§†å›¾ï¼Œå±•ç¤ºäº†é¢å‘SoCè®¾è®¡è€…å’ŒMLå¼€å‘è€…çš„ç«¯åˆ°ç«¯å †æ ˆã€‚åŸºäºRISC-Vçš„å¼€æ”¾å’Œå¯æ‰©å±•æ¶æ„ä½¿SoCè®¾è®¡è€…èƒ½å¤Ÿçµæ´»ä¿®æ”¹åŸºç¡€è®¾è®¡ï¼Œæˆ–å°†å…¶ç”¨ä½œé¢„é…ç½®çš„NPUã€‚*\n\nCoral NPUæ¶æ„åŒ…æ‹¬ä»¥ä¸‹ç»„ä»¶ï¼š\n\n*   **æ ‡é‡æ ¸å¿ƒ**ï¼šä¸€ä¸ªè½»é‡çº§ã€Cè¯­è¨€å¯ç¼–ç¨‹çš„RISC-Vå‰ç«¯ï¼Œç®¡ç†æ•°æ®æµåˆ°åç«¯æ ¸å¿ƒï¼Œé‡‡ç”¨ç®€å•çš„â€œè¿è¡Œè‡³å®Œæˆâ€æ¨¡å‹ï¼Œå®ç°è¶…ä½åŠŸè€—å’Œä¼ ç»ŸCPUåŠŸèƒ½ã€‚\n*   **å‘é‡æ‰§è¡Œå•å…ƒ**ï¼šä¸€ä¸ªå¼ºå¤§çš„å•æŒ‡ä»¤å¤šæ•°æ®ï¼ˆSIMDï¼‰åå¤„ç†å™¨ï¼Œç¬¦åˆRISC-Vå‘é‡æŒ‡ä»¤é›†ï¼ˆRVVï¼‰v1.0ï¼Œèƒ½å¤ŸåŒæ—¶å¯¹å¤§å‹æ•°æ®é›†è¿›è¡Œæ“ä½œã€‚\n*   **çŸ©é˜µæ‰§è¡Œå•å…ƒ**ï¼šä¸€ä¸ªé«˜æ•ˆçš„é‡åŒ–å¤–ç§¯ä¹˜ç´¯åŠ ï¼ˆMACï¼‰å¼•æ“ï¼Œä¸“ä¸ºåŠ é€ŸåŸºæœ¬ç¥ç»ç½‘ç»œæ“ä½œè€Œæ„å»ºã€‚ï¼ˆæ³¨ï¼šè¯¥çŸ©é˜µæ‰§è¡Œå•å…ƒä»åœ¨å¼€å‘ä¸­ï¼Œå°†äºä»Šå¹´æ™šäº›æ—¶å€™åœ¨GitHubä¸Šå‘å¸ƒã€‚ï¼‰\n\n![Coral NPU æ¶æ„è½¬å˜](https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-2.width-1250.png)\n*å¯è§†åŒ–ä»ä¼ ç»Ÿè®¾è®¡åˆ°Coral NPUçš„æ¶æ„è½¬å˜ã€‚*\n\n## ç»Ÿä¸€çš„å¼€å‘è€…ä½“éªŒ\n\nCoral NPUæ¶æ„æ˜¯ä¸€ä¸ªç®€å•ã€Cè¯­è¨€å¯ç¼–ç¨‹çš„ç›®æ ‡ï¼Œå¯ä»¥ä¸IREEå’ŒTFLMç­‰ç°ä»£ç¼–è¯‘å™¨æ— ç¼é›†æˆã€‚è¿™ä½¿å¾—å®ƒèƒ½å¤Ÿè½»æ¾æ”¯æŒTensorFlowã€JAXå’ŒPyTorchç­‰MLæ¡†æ¶ã€‚\n\nCoral NPUåŒ…å«ä¸€ä¸ªå…¨é¢çš„è½¯ä»¶å·¥å…·é“¾ï¼ŒåŒ…æ‹¬é’ˆå¯¹TensorFlowçš„TFLMç¼–è¯‘å™¨ã€é€šç”¨MLIRç¼–è¯‘å™¨ã€Cç¼–è¯‘å™¨ã€è‡ªå®šä¹‰å†…æ ¸å’Œæ¨¡æ‹Ÿå™¨ç­‰ä¸“ä¸šè§£å†³æ–¹æ¡ˆã€‚è¿™ä¸ºå¼€å‘è€…æä¾›äº†çµæ´»çš„è·¯å¾„ã€‚ä¾‹å¦‚ï¼Œæ¥è‡ªJAXç­‰æ¡†æ¶çš„æ¨¡å‹é¦–å…ˆä½¿ç”¨StableHLOæ–¹è¨€å¯¼å…¥åˆ°MLIRæ ¼å¼ã€‚ç„¶åï¼Œè¿™ä¸ªä¸­é—´æ–‡ä»¶è¢«è¾“å…¥åˆ°IREEç¼–è¯‘å™¨ï¼Œè¯¥ç¼–è¯‘å™¨åº”ç”¨ä¸€ä¸ªç¡¬ä»¶ç‰¹å®šçš„æ’ä»¶æ¥è¯†åˆ«Coral NPUçš„æ¶æ„ã€‚ä¹‹åï¼Œç¼–è¯‘å™¨æ‰§è¡Œæ¸è¿›å¼é™ä½â€”â€”è¿™æ˜¯ä¸€ä¸ªå…³é”®çš„ä¼˜åŒ–æ­¥éª¤ï¼Œä»£ç é€šè¿‡ä¸€ç³»åˆ—æ–¹è¨€ç³»ç»Ÿåœ°è½¬æ¢ï¼Œä½¿å…¶æ›´æ¥è¿‘æœºå™¨çš„æœ¬åœ°è¯­è¨€ã€‚ä¼˜åŒ–åï¼Œå·¥å…·é“¾ç”Ÿæˆä¸€ä¸ªæœ€ç»ˆçš„ã€ç´§å‡‘çš„äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œå‡†å¤‡åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šé«˜æ•ˆæ‰§è¡Œã€‚è¿™å¥—è¡Œä¸šæ ‡å‡†çš„å¼€å‘å·¥å…·ç®€åŒ–äº†MLæ¨¡å‹çš„ç¼–ç¨‹ï¼Œå¹¶å¯ä»¥åœ¨å„ç§ç¡¬ä»¶ç›®æ ‡ä¸Šæä¾›ä¸€è‡´çš„ä½“éªŒã€‚\n\n![Coral NPU ç¼–è¯‘å™¨å·¥å…·é“¾](https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-3.width-1250.png)\n*Coral NPUç¼–è¯‘å™¨å·¥å…·é“¾ï¼Œå±•ç¤ºäº†ä»MLæ¨¡å‹åˆ›å»ºåˆ°ä¼˜åŒ–ã€ç¼–è¯‘å†åˆ°è®¾å¤‡ç«¯éƒ¨ç½²çš„å®Œæ•´æµç¨‹ã€‚*\n\nCoral NPUçš„ååŒè®¾è®¡è¿‡ç¨‹ä¾§é‡äºä¸¤ä¸ªå…³é”®é¢†åŸŸï¼šé¦–å…ˆï¼Œè¯¥æ¶æ„é«˜æ•ˆåŠ é€Ÿäº†å½“ä»Šè®¾å¤‡ç«¯è§†è§‰å’ŒéŸ³é¢‘åº”ç”¨ä¸­é¢†å…ˆçš„åŸºäºç¼–ç å™¨çš„æ¶æ„ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ­£ä¸Gemmaå›¢é˜Ÿç´§å¯†åˆä½œï¼Œä¼˜åŒ–Coral NPUä»¥æ”¯æŒå°å‹Transformeræ¨¡å‹ï¼Œç¡®ä¿åŠ é€Ÿå™¨æ¶æ„æ”¯æŒä¸‹ä¸€ä»£è¾¹ç¼˜ç”Ÿæˆå¼AIã€‚è¿™ç§åŒé‡å…³æ³¨æ„å‘³ç€Coral NPUæœ‰æœ›æˆä¸ºç¬¬ä¸€ä¸ªå¼€æ”¾ã€åŸºäºæ ‡å‡†ã€ä½åŠŸè€—çš„NPUï¼Œæ—¨åœ¨å°†LLMå¸¦åˆ°å¯ç©¿æˆ´è®¾å¤‡ä¸Šã€‚\n\n## ç›®æ ‡åº”ç”¨\n\nCoral NPUæ—¨åœ¨å®ç°è¶…ä½åŠŸè€—ã€å§‹ç»ˆåœ¨çº¿çš„è¾¹ç¼˜AIåº”ç”¨ï¼Œç‰¹åˆ«å…³æ³¨ç¯å¢ƒæ„ŸçŸ¥ç³»ç»Ÿã€‚å…¶ä¸»è¦ç›®æ ‡æ˜¯åœ¨å¯ç©¿æˆ´è®¾å¤‡ã€æ‰‹æœºå’Œç‰©è”ç½‘ï¼ˆIoTï¼‰è®¾å¤‡ä¸Šå®ç°å…¨å¤©å€™AIä½“éªŒï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°å‡å°‘ç”µæ± ä½¿ç”¨ã€‚\n\næ½œåœ¨ç”¨ä¾‹åŒ…æ‹¬ï¼š\n\n*   **æƒ…å¢ƒæ„ŸçŸ¥**ï¼šæ£€æµ‹ç”¨æˆ·æ´»åŠ¨ï¼ˆä¾‹å¦‚ï¼Œæ­¥è¡Œã€è·‘æ­¥ï¼‰ã€æ¥è¿‘åº¦æˆ–ç¯å¢ƒï¼ˆä¾‹å¦‚ï¼Œå®¤å†…/å®¤å¤–ã€ç§»åŠ¨ä¸­ï¼‰ï¼Œä»¥å¯ç”¨â€œè¯·å‹¿æ‰“æ‰°â€æ¨¡å¼æˆ–å…¶ä»–æƒ…å¢ƒæ„ŸçŸ¥åŠŸèƒ½ã€‚\n*   **éŸ³é¢‘å¤„ç†**ï¼šè¯­éŸ³å’Œè¯­éŸ³æ£€æµ‹ã€å…³é”®è¯è¯†åˆ«ã€å®æ—¶ç¿»è¯‘ã€è½¬å½•ä»¥åŠåŸºäºéŸ³é¢‘çš„è¾…åŠ©åŠŸèƒ½ã€‚\n*   **å›¾åƒå¤„ç†**ï¼šäººç‰©å’Œç‰©ä½“æ£€æµ‹ã€é¢éƒ¨è¯†åˆ«ã€æ‰‹åŠ¿è¯†åˆ«å’Œä½åŠŸè€—è§†è§‰æœç´¢ã€‚\n*   **ç”¨æˆ·äº¤äº’**ï¼šé€šè¿‡æ‰‹åŠ¿ã€éŸ³é¢‘æç¤ºæˆ–å…¶ä»–ä¼ æ„Ÿå™¨é©±åŠ¨è¾“å…¥å®ç°æ§åˆ¶ã€‚\n\n## ç¡¬ä»¶å¼ºåˆ¶éšç§\n\nCoral NPUçš„ä¸€ä¸ªæ ¸å¿ƒåŸåˆ™æ˜¯é€šè¿‡ç¡¬ä»¶å¼ºåˆ¶å®‰å…¨æ¥å»ºç«‹ç”¨æˆ·ä¿¡ä»»ã€‚æˆ‘ä»¬çš„æ¶æ„æ­£åœ¨è®¾è®¡ä¸­ï¼Œä»¥æ”¯æŒCHERIç­‰æ–°å…´æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æä¾›ç»†ç²’åº¦çš„å†…å­˜çº§å®‰å…¨å’Œå¯æ‰©å±•çš„è½¯ä»¶éš”ç¦»ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿå°†æ•æ„Ÿçš„AIæ¨¡å‹å’Œä¸ªäººæ•°æ®éš”ç¦»åœ¨ç¡¬ä»¶å¼ºåˆ¶çš„æ²™ç®±ä¸­ï¼Œä»è€Œå‡è½»åŸºäºå†…å­˜çš„æ”»å‡»ã€‚\n\n## æ„å»ºç”Ÿæ€ç³»ç»Ÿ\n\nå¼€æ”¾ç¡¬ä»¶é¡¹ç›®çš„æˆåŠŸä¾èµ–äºå¼ºå¤§çš„åˆä½œä¼™ä¼´å…³ç³»ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ­£ä¸Synapticsåˆä½œï¼ŒSynapticsæ˜¯æˆ‘ä»¬åœ¨åµŒå…¥å¼è®¡ç®—ã€æ— çº¿è¿æ¥å’Œç‰©è”ç½‘å¤šæ¨¡æ€ä¼ æ„Ÿé¢†åŸŸçš„é¦–ä¸ªæˆ˜ç•¥ç¡…åˆä½œä¼™ä¼´å’Œé¢†å¯¼è€…ã€‚Synapticsåœ¨å…¶æŠ€æœ¯æ—¥ä¸Šå®£å¸ƒäº†å…¶æ–°çš„Astraâ„¢ SL2610ç³»åˆ—AIåŸç”Ÿç‰©è”ç½‘å¤„ç†å™¨ã€‚è¯¥äº§å“çº¿é‡‡ç”¨äº†å…¶Torqâ„¢ NPUå­ç³»ç»Ÿï¼Œè¿™æ˜¯ä¸šç•Œé¦–ä¸ªCoral NPUæ¶æ„çš„ç”Ÿäº§å®ç°ã€‚è¯¥NPUçš„è®¾è®¡æ”¯æŒTransformeræ¨¡å‹å’ŒåŠ¨æ€æ“ä½œç¬¦ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿä¸ºæ¶ˆè´¹å’Œå·¥ä¸šç‰©è”ç½‘æ„å»ºé¢å‘æœªæ¥çš„è¾¹ç¼˜AIç³»ç»Ÿã€‚\n\næ­¤æ¬¡åˆä½œæ”¯æŒäº†æˆ‘ä»¬å¯¹ç»Ÿä¸€å¼€å‘è€…ä½“éªŒçš„æ‰¿è¯ºã€‚Synaptics Torqâ„¢è¾¹ç¼˜AIå¹³å°åŸºäºIREEå’ŒMLIRçš„å¼€æºç¼–è¯‘å™¨å’Œè¿è¡Œæ—¶æ„å»ºã€‚è¿™æ¬¡åˆä½œæ˜¯æœç€ä¸ºæ™ºèƒ½ã€æƒ…å¢ƒæ„ŸçŸ¥è®¾å¤‡æ„å»ºå…±äº«ã€å¼€æ”¾æ ‡å‡†è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚\n\n## è§£å†³è¾¹ç¼˜æ ¸å¿ƒå±æœº\n\né€šè¿‡Coral NPUï¼Œæˆ‘ä»¬æ­£åœ¨ä¸ºä¸ªäººAIçš„æœªæ¥æ„å»ºä¸€ä¸ªåŸºç¡€å±‚ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æä¾›ä¸€ä¸ªé€šç”¨ã€å¼€æºå’Œå®‰å…¨çš„å¹³å°ï¼Œä¾›è¡Œä¸šåœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæ„å»ºï¼Œä»è€ŒåŸ¹è‚²ä¸€ä¸ªå……æ»¡æ´»åŠ›çš„ç”Ÿæ€ç³»ç»Ÿã€‚è¿™ä½¿å¾—å¼€å‘è€…å’Œç¡…ä¾›åº”å•†èƒ½å¤Ÿè¶…è¶Šå½“ä»Šç¢ç‰‡åŒ–çš„æ ¼å±€ï¼Œåœ¨è¾¹ç¼˜è®¡ç®—çš„å…±äº«æ ‡å‡†ä¸Šè¿›è¡Œåä½œï¼Œä»è€ŒåŠ é€Ÿåˆ›æ–°ã€‚",
      "shortSummary": "Coral NPUæ˜¯ä¸€ä¸ªé¢å‘è¾¹ç¼˜AIçš„å…¨æ ˆå¹³å°ï¼Œæ—¨åœ¨è§£å†³å°†ç”Ÿæˆå¼AIå¼•å…¥ä¸ªäººè®¾å¤‡æ‰€é¢ä¸´çš„æ€§èƒ½ã€ç¢ç‰‡åŒ–å’Œéšç§æŒ‘æˆ˜ã€‚å®ƒæ˜¯ä¸€ç§AIä¼˜å…ˆã€è¶…ä½åŠŸè€—çš„NPUæ¶æ„ï¼ŒåŸºäºRISC-Vï¼Œä¸“ä¸ºå¯ç©¿æˆ´è®¾å¤‡å’Œç‰©è”ç½‘çš„å§‹ç»ˆåœ¨çº¿è¾¹ç¼˜AIè®¾è®¡ã€‚Coral NPUæä¾›ç»Ÿä¸€çš„å¼€å‘ä½“éªŒï¼Œæ”¯æŒä¸»æµMLæ¡†æ¶å’Œç¼–è¯‘å™¨ï¼Œå¯å®ç°ç¯å¢ƒæ„ŸçŸ¥ã€æƒ…å¢ƒè¯†åˆ«å’Œå®æ—¶å¤„ç†ç­‰åº”ç”¨ã€‚å®ƒé€šè¿‡ç¡¬ä»¶å¼ºåˆ¶å®‰å…¨ä¿éšœç”¨æˆ·éšç§ï¼Œå¹¶ä¸Synapticsåˆä½œï¼Œå·²å®ç°é¦–æ¬¡ç”Ÿäº§éƒ¨ç½²ï¼Œæ—¨åœ¨æ„å»ºä¸€ä¸ªå¼€æ”¾ã€å®‰å…¨çš„è¾¹ç¼˜AIç”Ÿæ€ç³»ç»Ÿã€‚",
      "translated_title": "Coral NPUï¼šé¢å‘è¾¹ç¼˜AIçš„å…¨æ ˆå¹³å°",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-1.width-1250.png",
          "alt": "Coral NPU-1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-2.width-1250.png",
          "alt": "Coral NPU-2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-3.width-1250.png",
          "alt": "Coral NPU-3",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"tf5oi\">Generative AI has fundamentally reshaped our expectations of technology. We've seen the power of large-scale cloud-based models to create, reason and assist in incredible ways. However, the next great technological leap isn't just about making cloud models bigger; it's about embedding their intelligence directly into our immediate, personal environment. For AI to be truly assistive â€” proactively helping us navigate our day, translating conversations in real-time, or understanding our physical context â€” it must run on the devices we wear and carry. This presents a core challenge: embedding ambient AI onto battery-constrained edge devices, freeing them from the cloud to enable truly private, all-day assistive experiences.</p><p data-block-key=\"1a1a\">To move from the cloud to personal devices, we must solve three critical problems:</p><ul><li data-block-key=\"eu56\"><i>The performance gap:</i> Complex, state-of-the-art machine learning (ML) models demand more compute, far exceeding the limited power, thermal, and memory budgets of an edge device.</li><li data-block-key=\"7buqm\"><i>The fragmentation tax:</i> Compiling and optimizing ML models for a diverse landscape of proprietary processors is difficult and costly, hindering consistent performance across devices.</li><li data-block-key=\"a3ap1\"><i>The user trust deficit:</i> To be truly helpful, personal AI must prioritize the privacy and security of personal data and context.</li></ul><p data-block-key=\"ejp6o\">Today we introduce <a href=\"https://developers.google.com/coral\" target=\"_blank\" rel=\"noopener noreferrer\">Coral NPU</a>, a full-stack platform that builds on our original work from <a href=\"https://gweb-coral-full.uc.r.appspot.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Coral</a> to provide hardware designers and ML developers with the tools needed to build the next generation of private, efficient edge AI devices. Co-designed in partnership with Google Research and Google DeepMind, Coral NPU is an AI-first hardware architecture built to enable the next generation of ultra-low-power, always-on edge AI. It offers a unified developer experience, making it easier to deploy applications like ambient sensing. It's specifically designed to enable all-day AI on wearable devices while minimizing battery usage and being configurable for higher performance use cases. Weâ€™ve released our <a href=\"https://developers.google.com/coral\" target=\"_blank\" rel=\"noopener noreferrer\">documentation and tools</a> so that developers and designers can start building today.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Coral NPU: An AI-first architecture</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tf5oi\">Developers building for low-power edge devices face a fundamental trade-off, choosing between general purpose CPUs and specialized accelerators. General-purpose CPUs offer crucial flexibility and broad software support but lack the domain-specific architecture for demanding ML workloads, making them less performant and power-inefficient. Conversely, specialized accelerators provide high ML efficiency but are inflexible, difficult to program, and ill-suited for general tasks.</p><p data-block-key=\"1fta9\">This hardware problem is magnified by a highly fragmented software ecosystem. With starkly different programming models for CPUs and ML blocks, developers are often forced to use proprietary compilers and complex command buffers. This creates a steep learning curve and makes it difficult to combine the unique strengths of different compute units. Consequently, the industry lacks a mature, low-power architecture that can easily and effectively support multiple ML development frameworks.</p><p data-block-key=\"a5efi\">The Coral NPU architecture directly addresses this by reversing traditional chip design. It prioritizes the ML matrix engine over scalar compute, optimizing architecture for AI from silicon up and creating a platform purpose-built for more efficient, on-device inference.</p><p data-block-key=\"43d11\">As a complete, reference <a href=\"https://en.wikipedia.org/wiki/Neural_processing_unit\" target=\"_blank\" rel=\"noopener noreferrer\">neural processing unit</a> (NPU) architecture, Coral NPU provides the building blocks for the next generation of energy-efficient, ML-optimized <a href=\"https://en.wikipedia.org/wiki/System_on_a_chip\" target=\"_blank\" rel=\"noopener noreferrer\">systems on chip</a> (SoCs). The architecture is based on a set of <a href=\"https://riscv.org/specifications/ratified/\" target=\"_blank\" rel=\"noopener noreferrer\">RISC-V ISA</a> compliant architectural IP blocks and is designed for minimal power consumption, making it ideal for always-on ambient sensing. The base design delivers performance in the 512 <a href=\"https://en.wikipedia.org/wiki/Floating_point_operations_per_second\" target=\"_blank\" rel=\"noopener noreferrer\">giga operations per second</a> (GOPS) range while consuming just a few milliwatts, thus enabling powerful on-device AI for edge devices, hearables, AR glasses, and smartwatches.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-1.width-1250.png\" alt=\"Coral NPU-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-1.width-1250.png\" alt=\"Coral NPU-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vajmb\"><i>A unified view of the Coral NPU ecosystem, showcasing end-to-end stack for SoC designers and ML developers.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-vertical-padding --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"tf5oi\">The open and extensible <a href=\"https://developers.google.com/coral/guides/architecture\" target=\"_blank\" rel=\"noopener noreferrer\">architecture</a> based on RISC-V gives SoC designers flexibility to modify the base design, or use it as a pre-configured NPU. The Coral NPU architecture includes the following components:</p><ul><li data-block-key=\"bclpo\"><i>A scalar core:</i> A lightweight, C-programmable RISC-V frontend that manages data flow to the back-end cores, using a simple \"run-to-completion\" model for ultra-low power consumption and traditional CPU functions.</li><li data-block-key=\"1u961\"><i>A vector execution unit:</i> A robust single instruction multiple data (<a href=\"https://www.sciencedirect.com/topics/computer-science/single-instruction-multiple-data\" target=\"_blank\" rel=\"noopener noreferrer\">SIMD</a>) co-processor compliant with the RISC-V Vector instruction set (RVV) v1.0, enabling simultaneous operations on large data sets.</li><li data-block-key=\"2ju85\"><i>A matrix execution unit:</i> A highly efficient quantized outer product <a href=\"https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation\" target=\"_blank\" rel=\"noopener noreferrer\">multiply-accumulate</a> (MAC) engine purpose-built to accelerate fundamental neural network operations. Note that the matrix execution unit is still under development and will be released on <a href=\"https://github.com/google-coral/coralnpu\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a> later this year.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-2.width-1250.png\" alt=\"Coral NPU-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-2.width-1250.png\" alt=\"Coral NPU-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vajmb\"><i>Visualizing the architectural shift from traditional design to the Coral NPU.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Unified developer experience</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tf5oi\">The Coral NPU architecture is a simple, C-programmable target that can seamlessly integrate with modern compilers like <a href=\"https://iree.dev/\" target=\"_blank\" rel=\"noopener noreferrer\">IREE</a> and <a href=\"https://github.com/tensorflow/tflite-micro\" target=\"_blank\" rel=\"noopener noreferrer\">TFLM</a>. This enables easy support for ML frameworks like <a href=\"https://www.tensorflow.org/\" target=\"_blank\" rel=\"noopener noreferrer\">TensorFlow</a>, <a href=\"https://docs.jax.dev/en/latest/\" target=\"_blank\" rel=\"noopener noreferrer\">JAX</a>, and <a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">PyTorch</a>.</p><p data-block-key=\"46n8n\">Coral NPU incorporates a comprehensive software toolchain, including specialized solutions like the TFLM compiler for TensorFlow, alongside a general-purpose <a href=\"https://mlir.llvm.org/\" target=\"_blank\" rel=\"noopener noreferrer\">MLIR</a> compiler, C compiler, custom kernels, and a simulator. This provides developers with flexible pathways. For example, a model from a framework like JAX is first imported into the MLIR format using the <a href=\"https://openxla.org/stablehlo\" target=\"_blank\" rel=\"noopener noreferrer\">StableHLO</a> dialect. This intermediate file is then fed into the IREE compiler, which applies a hardware-specific plug-in to recognize the Coral NPU's architecture. From there, the compiler performs progressive lowering â€” a critical optimization step where the code is systematically translated through a series of dialects, moving closer to the machine's native language. After optimization, the toolchain generates a final, compact binary file ready for efficient execution on the edge device. This suite of industry-standard developer tools helps simplify the programming of ML models and can allow for a consistent experience across various hardware targets.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-3.width-1250.png\" alt=\"Coral NPU-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-3.width-1250.png\" alt=\"Coral NPU-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vajmb\"><i>The Coral NPU compiler toolchain, illustrating the complete flow from ML model creation through optimization and compilation to on-device deployment.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"tf5oi\">Coral NPUâ€™s co-design process focuses on two key areas. First, the architecture efficiently accelerates the leading encoder-based architectures used in today's on-device vision and audio applications. Second, we are collaborating closely with the <a href=\"https://deepmind.google/models/gemma/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma</a> team to optimize Coral NPU for small transformer models, helping to ensure the accelerator architecture supports the next generation of generative AI at the edge.</p><p data-block-key=\"brs14\">This dual focus means Coral NPU is on track to be the first open, standards-based, low-power NPU designed to bring LLMs to wearables. For developers, this provides a single, validated path to deploy both current and future models with maximum performance at minimal power.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Target applications</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tf5oi\">Coral NPU is designed to enable ultra-low-power, always-on edge AI applications, particularly focused on ambient sensing systems. Its primary goal is to enable all day AI-experiences on wearables, mobile phones and <a href=\"https://en.wikipedia.org/wiki/Internet_of_things\" target=\"_blank\" rel=\"noopener noreferrer\">Internet of Things</a> (IoT) devices minimizing battery usage.</p><p data-block-key=\"54nt5\">Potential use cases include:</p><ul><li data-block-key=\"7himc\"><i>Contextual awareness:</i> Detecting user activity (e.g., walking, running), proximity, or environment (e.g., indoors/outdoors, on-the-go) to enable \"do-not-disturb\" modes or other context-aware features.</li><li data-block-key=\"ff339\"><i>Audio processing:</i> Voice and speech detection, keyword spotting, live translation, transcription, and audio-based accessibility features.</li><li data-block-key=\"ai106\"><i>Image processing:</i> Person and object detection, facial recognition, gesture recognition, and low-power visual search.</li><li data-block-key=\"9m2mn\"><i>User interaction:</i> Enabling control via hand gestures, audio cues, or other sensor-driven inputs.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Hardware-enforced privacy</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tf5oi\">A core principle of Coral NPU is building user trust through hardware-enforced security. Our architecture is being designed to support emerging technologies like <a href=\"https://www.cl.cam.ac.uk/research/security/ctsrd/cheri/\" target=\"_blank\" rel=\"noopener noreferrer\">CHERI</a>, which provides fine-grained memory-level safety and scalable software compartmentalization. With this approach, we hope to enable sensitive AI models and personal data to be isolated in a hardware-enforced sandbox, mitigating memory-based attacks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Building an ecosystem</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tf5oi\">Open hardware projects rely on strong partnerships to succeed. To that end, weâ€™re collaborating with <a href=\"https://www.synaptics.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Synaptics</a>, our first strategic silicon partner and a leader in embedded compute, wireless connectivity, and multimodal sensing for the IoT. Today, at their Tech Day, Synaptics announced their new <a href=\"https://www.synaptics.com/sl2610-press-release\" target=\"_blank\" rel=\"noopener noreferrer\">Astraâ„¢ SL2610</a> line of <a href=\"https://www.synaptics.com/sl2610-product-line\" target=\"_blank\" rel=\"noopener noreferrer\">AI-Native IoT Processors</a>. This product line features their <a href=\"https://www.synaptics.com/torq-github\" target=\"_blank\" rel=\"noopener noreferrer\">Torqâ„¢ NPU</a> subsystem, the industryâ€™s first production implementation of the Coral NPU architecture. The NPUâ€™s design is transformer-capable and supports dynamic operators, enabling developers to build future-ready Edge AI systems for consumer and industrial IoT.</p><p data-block-key=\"4r0e3\">This partnership supports our commitment to a unified developer experience. The Synaptics Torqâ„¢ Edge AI platform is built on an open-source compiler and runtime based on IREE and MLIR. This collaboration is a significant step toward building a shared, open standard for intelligent, context-aware devices.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Solving core crises of the Edge</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tf5oi\">With Coral NPU, we are building a foundational layer for the future of personal AI. Our goal is to foster a vibrant ecosystem by providing a common, open-source, and secure platform for the industry to build upon. This empowers developers and silicon vendors to move beyond today's fragmented landscape and collaborate on a shared standard for edge computing, enabling faster innovation. Learn more about <a href=\"https://developers.google.com/coral\" target=\"_blank\" rel=\"noopener noreferrer\">Coral NPU</a> and start building today.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9lzmb\"><i>We would like to thank the core contributors and leadership team for this work, particularly Billy Rutledge, Ben Laurie, Derek Chow, Michael Hoang, Naveen Dodda, Murali Vijayaraghavan, Gregory Kielian, Matthew Wilson, Bill Luan, Divya Pandya, Preeti Singh, Akib Uddin, Stefan Hall, Alex Van Damme, David Gao, Lun Dong, Julian Mullings-Black, Roman Lewkow, Shaked Flur, Yenkai Wang, Reid Tatge, Tim Harvey, Tor Jeremiassen, Isha Mishra, Kai Yick, Cindy Liu, Bangfei Pan, Ian Field, Srikanth Muroor, Jay Yagnik, Avinatan Hassidim, and Yossi Matias.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "XR Blocksï¼šåŠ é€ŸAI + XRåˆ›æ–° (åŸæ ‡é¢˜: XR Blocks: Accelerating AI + XR innovation)",
      "link": "https://research.google/blog/xr-blocks-accelerating-ai-xr-innovation/",
      "pubDate": "Wed, 08 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-08T16:00:00.000Z",
      "creator": "Google",
      "summary": "## XR Blocksï¼šåŠ é€ŸAI + XRåˆ›æ–°\n\n### å¼•è¨€ï¼šå¼¥åˆAIä¸XRä¹‹é—´çš„é¸¿æ²Ÿ\n\nå½“å‰ï¼Œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ä¸æ‰©å±•ç°å®ï¼ˆXRï¼‰çš„ç»“åˆæœ‰æœ›å¼€å¯æ²‰æµ¸å¼æ™ºèƒ½è®¡ç®—çš„æ–°èŒƒå¼ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ä¸ªé¢†åŸŸä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„ç”Ÿæ€ç³»ç»Ÿå·®è·ã€‚AIçš„ç ”å‘å¾—ç›ŠäºJAXã€PyTorchã€TensorFlowç­‰æˆç†Ÿæ¡†æ¶ä»¥åŠImageNetã€LMArenaç­‰åŸºå‡†æµ‹è¯•çš„åŠ é€Ÿï¼Œè€ŒAIé©±åŠ¨çš„XRäº¤äº’åŸå‹å¼€å‘ä»æ˜¯ä¸€ä¸ªé«˜æ‘©æ“¦è¿‡ç¨‹ï¼Œé€šå¸¸éœ€è¦å¼€å‘è€…æ‰‹åŠ¨é›†æˆæ„ŸçŸ¥ã€æ¸²æŸ“å’Œäº¤äº’ç­‰ä½çº§ç³»ç»Ÿã€‚\n\nä¸ºäº†å¼¥åˆè¿™ä¸€é¸¿æ²Ÿï¼Œæˆ‘ä»¬æ¨å‡ºäº† **XR Blocks**ï¼ˆåœ¨ACM UIST 2025ä¸Šå‘å¸ƒï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè·¨å¹³å°æ¡†æ¶ï¼Œæ—¨åœ¨åŠ é€Ÿä»¥äººä¸ºä¸­å¿ƒçš„AI + XRåˆ›æ–°ã€‚è¿™æ˜¯æˆ‘ä»¬ä¹‹å‰é’ˆå¯¹éXRç”¨ä¾‹ã€é€šè¿‡å¯è§†åŒ–ç¼–ç¨‹ç®€åŒ–æœºå™¨å­¦ä¹ ç®¡é“åŸå‹çš„Visual Blocks for MLç ”ç©¶çš„é‡å¤§è¿›å±•ã€‚\n\n### XR Blocksæ¡†æ¶æ¦‚è¿°\n\nXR Blocksæä¾›äº†ä¸€ä¸ªæ¨¡å—åŒ–æ¶æ„ï¼ŒåŒ…å«å³æ’å³ç”¨ç»„ä»¶ï¼Œç”¨äºAI + XRä¸­çš„æ ¸å¿ƒæŠ½è±¡ï¼šç”¨æˆ·ã€ä¸–ç•Œã€ç•Œé¢ã€AIå’Œä»£ç†ã€‚å…¶æ ¸å¿ƒä½¿å‘½æ˜¯åŠ é€Ÿæ„ŸçŸ¥å‹AI + XRåº”ç”¨çš„å¿«é€ŸåŸå‹å¼€å‘ã€‚è¯¥å·¥å…·åŒ…åŸºäºWebXRã€threejsã€LiteRTå’ŒGeminiç­‰æ˜“äºè®¿é—®çš„æŠ€æœ¯æ„å»ºï¼Œé™ä½äº†XRåˆ›ä½œè€…çš„å…¥é—¨é—¨æ§›ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç³»åˆ—å¼€æºæ¨¡æ¿ã€å®æ—¶æ¼”ç¤ºå’ŒGitHubä¸Šçš„æºä»£ç å±•ç¤ºäº†å…¶æ•ˆç”¨ï¼Œç›®æ ‡æ˜¯èµ‹èƒ½ç¤¾åŒºå¿«é€Ÿå°†æ¦‚å¿µè½¬åŒ–ä¸ºäº¤äº’å¼åŸå‹ã€‚æ›´å¤šèƒ½åŠ›æ¦‚è¿°å¯åœ¨æˆ‘ä»¬çš„æ–¹å‘æ€§è®ºæ–‡å’Œé¢„å‘Šè§†é¢‘ä¸­æ‰¾åˆ°ã€‚\n\n### è®¾è®¡åŸåˆ™\n\næˆ‘ä»¬çš„æ¶æ„å’ŒAPIè®¾è®¡é€‰æ‹©éµå¾ªä»¥ä¸‹ä¸‰ä¸ªåŸåˆ™ï¼š\n\n*   **æ‹¥æŠ±ç®€æ´æ€§å’Œå¯è¯»æ€§ï¼š** å—Pythonç¦…æ„çš„å¯å‘ï¼Œæˆ‘ä»¬ä¼˜å…ˆè€ƒè™‘æ¸…æ™°ã€äººç±»å¯è¯»çš„æŠ½è±¡ã€‚å¼€å‘è€…çš„è„šæœ¬åº”åƒå¯¹æ‰€éœ€ä½“éªŒçš„é«˜çº§æè¿°ã€‚ç®€å•çš„ä»»åŠ¡åº”æ˜“äºå®ç°ï¼Œå¤æ‚çš„é€»è¾‘åº”ä¿æŒæ˜ç¡®å’Œå¯ç†è§£ã€‚\n*   **ä¼˜å…ˆè€ƒè™‘åˆ›ä½œè€…ä½“éªŒï¼š** æˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯ä½¿æ™ºèƒ½å’Œæ„ŸçŸ¥å‹XRåº”ç”¨çš„åˆ›ä½œå°½å¯èƒ½æ— ç¼ã€‚æˆ‘ä»¬è®¤ä¸ºåˆ›ä½œè€…åº”ä¸“æ³¨äºç”¨æˆ·ä½“éªŒï¼Œè€Œä¸æ˜¯ä¼ æ„Ÿå™¨èåˆã€AIæ¨¡å‹é›†æˆæˆ–è·¨å¹³å°äº¤äº’é€»è¾‘ç­‰ä½çº§â€œç®¡é“â€å·¥ä½œã€‚\n*   **å®ç”¨ä¸»ä¹‰è€Œéå®Œæ•´æ€§ï¼š** é‰´äºAIå’ŒXRé¢†åŸŸå‘å±•è¿…é€Ÿï¼Œæˆ‘ä»¬éµå¾ªå®ç”¨ä¸»ä¹‰çš„è®¾è®¡ç†å¿µã€‚ä¸€ä¸ªè¯•å›¾åšåˆ°å®Œç¾ã€å…¨é¢å¤æ‚çš„æ¡†æ¶åœ¨å‘å¸ƒæ—¶å¯èƒ½å·²ç»è¿‡æ—¶ã€‚æˆ‘ä»¬å€¾å‘äºä¸€ä¸ªç®€å•ã€æ¨¡å—åŒ–ã€é€‚åº”æ€§å¼ºçš„æ¶æ„ï¼Œå®ƒå¯ä»¥åœ¨æ¡Œé¢æ¨¡æ‹Ÿå™¨å’ŒAndroid XRè®¾å¤‡ä¸Šè¿è¡Œï¼Œé€‚ç”¨äºå¹¿æ³›çš„åº”ç”¨ã€‚\n\n### æ¡†æ¶å·¥ä½œåŸç†\n\nXR Blocksæ¡†æ¶ä»Visual Blocks for MLå’ŒInstructPipeä¸­æ±²å–çµæ„Ÿï¼Œæä¾›äº†ä¸€ä¸ªé«˜å±‚çº§ã€ä»¥äººä¸ºä¸­å¿ƒçš„æŠ½è±¡å±‚ï¼Œå°†äº¤äº’çš„â€œæ„å›¾â€ï¼ˆScriptï¼Œä¸‹æ–‡è¯¦è¿°ï¼‰ä¸ä½çº§å®ç°çš„â€œæ–¹å¼â€åˆ†ç¦»ã€‚\n\n![XR Blocksæ¡†æ¶](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks1_Framework.width-1250.png)\n\nXR BlocksåŠ é€Ÿäº†æ¡Œé¢æ¨¡æ‹Ÿå™¨å’ŒAndroid XRè®¾å¤‡ä¸Šå®æ—¶AI + XRåº”ç”¨çš„åŸå‹å¼€å‘ã€‚ä¾‹å¦‚ï¼š\n*   **(a) XRçœŸå®æ„Ÿï¼š** åœ¨æ¨¡æ‹Ÿä¸­åŸå‹åŒ–æ·±åº¦æ„ŸçŸ¥ã€åŸºäºç‰©ç†çš„äº¤äº’ï¼Œå¹¶å°†ç›¸åŒçš„ä»£ç éƒ¨ç½²åˆ°çœŸå®ä¸–ç•Œçš„XRè®¾å¤‡ã€‚\n*   **(b) XRäº¤äº’ï¼š** å°†è‡ªå®šä¹‰æ‰‹åŠ¿æ¨¡å‹æ— ç¼é›†æˆåˆ°æ¡Œé¢æ¨¡æ‹Ÿå™¨å’Œè®¾å¤‡ä¸Šçš„XRéƒ¨ç½²ã€‚\n*   **(c) AI + XRé›†æˆï¼š** æ„å»ºæ™ºèƒ½ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åŠ©æ‰‹ï¼Œä¾‹å¦‚æä¾›ä¸»åŠ¨å»ºè®®å’Œä¸æ˜¾çœ¼äº¤äº’çš„Sensible AgentåŸå‹ã€‚\n\n### æŠ½è±¡å±‚ï¼šç°å®æ¨¡å‹ (Reality Model)\n\næˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”±é«˜çº§æŠ½è±¡ç»„æˆçš„æ–°â€œç°å®æ¨¡å‹â€ï¼Œä»¥æŒ‡å¯¼XR Blocksæ¡†æ¶çš„å®ç°ã€‚ä¸ä¸ºç«¯åˆ°ç«¯æ— ç›‘ç£è®­ç»ƒè®¾è®¡çš„â€œä¸–ç•Œæ¨¡å‹â€ä¸åŒï¼Œæˆ‘ä»¬çš„ç°å®æ¨¡å‹ç”±å¯æ›¿æ¢çš„XRäº¤äº’æ¨¡å—ç»„æˆã€‚æˆ‘ä»¬è®¾è®¡çš„æ ¸å¿ƒæ˜¯ **Script**ï¼Œå®ƒæ˜¯åº”ç”¨ç¨‹åºçš„å™äº‹å’Œé€»è¾‘ä¸­å¿ƒã€‚Scriptæ“ä½œå…­ä¸ªæ ¸å¿ƒåŸè¯­ï¼š\n\n*   **ç”¨æˆ·ä¸ç‰©ç†ä¸–ç•Œï¼š** æˆ‘ä»¬çš„æ¨¡å‹ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒï¼ŒåŒ…æ‹¬æ‰‹éƒ¨ã€å‡è§†å’Œè™šæ‹Ÿå½¢è±¡ã€‚ç‰©ç†ä¸–ç•Œå…è®¸ScriptæŸ¥è¯¢æ„ŸçŸ¥çš„ç°å®ï¼Œä¾‹å¦‚æ·±åº¦ã€ä¼°è®¡çš„å…‰ç…§æ¡ä»¶å’Œå¯¹è±¡ã€‚\n*   **è™šæ‹Ÿç•Œé¢ä¸ä¸Šä¸‹æ–‡ï¼š** è¯¥æ¨¡å‹é€šè¿‡è™šæ‹ŸUIå…ƒç´ å¢å¼ºæ··åˆç°å®ï¼Œä»2Dé¢æ¿åˆ°å®Œå…¨3Dèµ„äº§ã€‚æ„ŸçŸ¥ç®¡é“åˆ†æç¯å¢ƒã€æ´»åŠ¨å’Œäº¤äº’å†å²çš„ä¸Šä¸‹æ–‡ã€‚ä¸€ä¸ªç¤ºä¾‹åº”ç”¨å¯åœ¨Sensible Agentä¸­æ‰¾åˆ°ã€‚\n*   **æ™ºèƒ½ä¸ç¤¾äº¤å®ä½“ï¼š** æˆ‘ä»¬å°†AIé©±åŠ¨çš„ä»£ç†å’Œè¿œç¨‹äººç±»ä¼™ä¼´è§†ä¸ºæ¨¡å‹ä¸­çš„ä¸»è¦å®ä½“ã€‚è¿™ä½¿å¾—åœ¨DialogLabä¸­å®ç°æ··åˆäººæœºå¯¹è¯ä¸­çš„åŠ¨æ€ç¾¤ç»„å¯¹è¯ã€‚\n\n![XR Blocksæ¡†æ¶çš„ç°å®æ¨¡å‹æ¦‚å¿µå›¾](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks2_RealityModel.width-1250.png)\n\n### å®ç°ï¼šæ ¸å¿ƒå¼•æ“ (Core Engine)\n\nè¿™ä¸ªç°å®æ¨¡å‹ç”±XR Blocksçš„æ¨¡å—åŒ–æ ¸å¿ƒå¼•æ“å®ç°ï¼Œè¯¥å¼•æ“æä¾›é«˜çº§APIï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿåˆ©ç”¨ä»¥ä¸‹å­ç³»ç»Ÿï¼Œè€Œæ— éœ€æŒæ¡å…¶åº•å±‚å®ç°ï¼š\n\n*   **æ„ŸçŸ¥ä¸è¾“å…¥ç®¡é“ï¼š** æ‘„åƒå¤´ã€æ·±åº¦å’Œå£°éŸ³æ¨¡å—æŒç»­ä¸ºç°å®æ¨¡å‹æä¾›å¹¶æ›´æ–°ç‰©ç†ç°å®çš„è¡¨ç¤ºã€‚è¾“å…¥æ¨¡å—æ ‡å‡†åŒ–æ¥è‡ªå„ç§è®¾å¤‡çš„ç”¨æˆ·æ“ä½œï¼Œä¸ºXR Blocksæä¾›è§£é‡Šçš„åŸå§‹æ•°æ®ã€‚\n*   **AIä½œä¸ºæ ¸å¿ƒå·¥å…·ï¼š** `ai`æ¨¡å—å……å½“ä¸­å¤®ç¥ç»ç³»ç»Ÿï¼Œæä¾›ç®€å•è€Œå¼ºå¤§çš„å‡½æ•°ï¼ˆ`.query`ã€`.runModel`ï¼‰ï¼Œä½¿å¤§å‹æ¨¡å‹æˆä¸ºå¯è®¿é—®çš„å·¥å…·ã€‚\n*   **ä½“éªŒä¸å¯è§†åŒ–å·¥å…·åŒ…ï¼š** ä¸ºäº†å®ç°å¿«é€Ÿåˆ›å»ºï¼Œè¯¥å·¥å…·åŒ…æä¾›äº†ä¸€ä¸ªå¸¸ç”¨åŠŸèƒ½åº“ã€‚`ux`æ¨¡å—æä¾›å¯é‡ç”¨çš„äº¤äº’è¡Œä¸ºï¼Œå¦‚`.selectable`å’Œ`.draggable`ï¼Œè€Œ`ui`å’Œ`effect`æ¨¡å—å¤„ç†ç•Œé¢æ¸²æŸ“å’Œå¤æ‚è§†è§‰æ•ˆæœï¼Œå¦‚é®æŒ¡ã€‚\n\n![XR Blocksæ ¸å¿ƒå¼•æ“çš„æ¨¡å—åŒ–æ¶æ„](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks3_Architecture.width-1250.png)\n\né€šè¿‡å°†æŠ½è±¡çš„ç°å®æ¨¡å‹ä¸å…·ä½“çš„æ ¸å¿ƒå¼•æ“åˆ†ç¦»ï¼ŒXR Blockså®ç°äº†å¼ºå¤§çš„æ–°åˆ›æ„å·¥ä½œæµã€‚ç›®æ ‡æ˜¯è®©åˆ›ä½œè€…æ›´å¿«åœ°ä»é«˜å±‚çº§ã€ä»¥äººä¸ºä¸­å¿ƒçš„æƒ³æ³•è½¬å‘äº¤äº’å¼åŸå‹ã€‚æˆ‘ä»¬è®¾æƒ³æœªæ¥ï¼Œä»»ä½•å£°æ˜æ€§æç¤ºï¼Œä¾‹å¦‚â€œå½“ç”¨æˆ·æä½ä¸€ä¸ªç‰©ä½“æ—¶ï¼Œä»£ç†åº”è¯¥ä¸ºå®ƒç”Ÿæˆä¸€é¦–è¯—â€ï¼Œéƒ½å¯ä»¥ç›´æ¥è½¬åŒ–ä¸ºXR Blocksä¸­çš„é«˜çº§æŒ‡ä»¤ï¼š\n\n![XR Blocksä¸­çš„é«˜å±‚çº§æŒ‡ä»¤](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks4_Instructions.width-1250.png)\n\nå› æ­¤ï¼Œåˆ›ä½œè€…çš„æç¤ºä¸å†æ˜¯ä¼ªä»£ç ï¼Œè€Œæ˜¯å®ç°é€»è¾‘çš„ç›´æ¥æ€»ç»“ã€‚æˆ‘ä»¬è®¾æƒ³è¿™ä¸ªæ¡†æ¶èƒ½æ›´æ— ç¼åœ°å°†ç”¨æˆ·æ„å›¾è½¬åŒ–ä¸ºç³»ç»Ÿçº§æ‰§è¡Œæµï¼Œç»„åˆæ¥è‡ªè¾“å…¥ã€å£°éŸ³ã€AIã€ä¸–ç•Œã€UIå’Œä»£ç†æ¨¡å—çš„åŠŸèƒ½ï¼Œä»¥é€šè¿‡ç”¨æˆ·äº¤äº’ç”Ÿæˆæ–°å…´çš„æ™ºèƒ½è¡Œä¸ºã€‚\n\n![XR Blocksçš„äº¤äº’è¯­æ³•](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks5_Interaction.width-1250.png)\n\n### åº”ç”¨åœºæ™¯\n\næˆ‘ä»¬æä¾›äº†ä¸€å¥—äº¤äº’å¼åº”ç”¨ç¨‹åºï¼Œä»¥å±•ç¤ºXR Blocksæ¡†æ¶çš„è¡¨è¾¾èƒ½åŠ›å’Œçµæ´»æ€§ã€‚è¿™äº›ç¤ºä¾‹å±•ç¤ºäº†æˆ‘ä»¬çš„æ¡†æ¶å¦‚ä½•å®ç°ä»¥å‰è¿‡äºå¤æ‚å’Œæ˜‚è´µè€Œæ— æ³•æ„å»ºçš„å¤æ‚ä½“éªŒçš„å¿«é€ŸåŸå‹å¼€å‘ï¼Œä»è€Œä¿ƒè¿›åˆ›å»ºé€¼çœŸã€äº¤äº’å¼å’Œæ™ºèƒ½çš„æ··åˆç°å®ä¸–ç•Œï¼š\n\n![XR Blocksçš„åº”ç”¨](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks_Applications.width-1250.png)\n\nXR Blocksçš„åº”ç”¨åŒ…æ‹¬ï¼š\n1.  **XRçœŸå®æ„Ÿï¼š** æ·±åº¦æ„ŸçŸ¥å’ŒåŸºäºç‰©ç†çš„çƒæ± å’Œæ³¼æ°´æ¸¸æˆï¼›å‡ ä½•æ„ŸçŸ¥é˜´å½±ã€å¸¦é®æŒ¡çš„3Dé«˜æ–¯æ³¼æº…å’Œå…‰ç…§ä¼°è®¡ã€‚\n2.  **XRäº¤äº’ï¼š** ç”±è‡ªå®šä¹‰MLæ¨¡å‹ã€åŠ¨æ€æ»‘åŠ¨è¯†åˆ«ã€ä¸ç‰©ç†ä¸–ç•Œçš„è§¦æ‘¸å’ŒæŠ“å–èµ‹èƒ½çš„æ²‰æµ¸å¼è¡¨æƒ…ç¬¦å·å’Œå‰ªåˆ€çŸ³å¤´å¸ƒæ¸¸æˆã€‚\n3.  **AI + XRï¼š** ä¸å¯¹è¯å¼AIã€XRå¯¹è±¡ã€XRä¸­çš„çœ¼é•œæ¨¡æ‹Ÿä»¥åŠç”¨çœŸå®ä¸–ç•Œæ‘„åƒå¤´ç”Ÿæˆè¯—æ­Œçš„é›†æˆã€‚\n\nå½“è¿™ä¸ªç°å®æ¨¡å‹ä¸ç”Ÿæˆå¼AIæ·±åº¦é›†æˆä»¥åˆ›å»ºåŠ¨æ€ã€ä¸ªæ€§åŒ–ç¯å¢ƒæ—¶ï¼Œæ¡†æ¶çš„çœŸæ­£åŠ›é‡å¾—ä»¥å®ç°ã€‚æˆ‘ä»¬é€šè¿‡æ„å»ºè¯¸å¦‚â€œå¢å¼ºå¯¹è±¡æ™ºèƒ½â€ï¼ˆXR-Objectsï¼‰ä¹‹ç±»çš„ç³»ç»Ÿæ¥è¯æ˜è¿™ä¸€ç‚¹ï¼Œè¯¥ç³»ç»Ÿèµ‹äºˆæ—¥å¸¸ç‰©ç†å¯¹è±¡äº¤äº’å¼æ•°å­—åŠŸèƒ½ï¼Œä¾‹å¦‚åŠ¨æ€è™šæ‹ŸæŒ‰é’®ã€‚XR Blocksä¹Ÿä½œä¸ºSensible Agentï¼ˆåœ¨ACM UIST 2025ä¸Šå‘å¸ƒï¼‰çš„åŸºç¡€ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºä¸»åŠ¨å’Œä¸æ˜¾çœ¼ARè¾…åŠ©çš„ç³»ç»Ÿã€‚æˆ‘ä»¬çš„æ¶æ„æä¾›äº†ä»£ç†çš„æ ¸å¿ƒæ„ŸçŸ¥å’Œäº¤äº’é€»è¾‘ï¼Œæä¾›äº†æˆ‘ä»¬ä¸»è¦ç›®æ ‡çš„ä¸€ä¸ªä¾‹å­ï¼šé€šè¿‡æä¾›å¼ºå¤§ã€é«˜å±‚çº§çš„å·¥å…·ï¼ŒXR Blocksèµ‹èƒ½äººæœºäº¤äº’ç ”ç©¶äººå‘˜ç»•è¿‡ä½çº§å®ç°ï¼Œç›´æ¥ä¸“æ³¨äºäººæœºåä½œçš„è®¤çŸ¥åŸç†ç­‰æ›´é«˜é˜¶çš„æŒ‘æˆ˜ã€‚SDKæ¼”ç¤ºè§†é¢‘å±•ç¤ºäº†XR Blocksä¸å¯¹è¯å¼AIçš„ç»“åˆã€Android XRä¸Šçš„ç‰©ç†ç¢°æ’ä¸æ·±åº¦æ„ŸçŸ¥ï¼Œä»¥åŠåœ¨è®¾å¤‡ä¸Šè¿è¡ŒLiteRTä¸è‡ªå®šä¹‰æ‰‹åŠ¿æ¨¡å‹ä»¥è§¦å‘XRåŠ¨ç”»ã€‚\n\n### ç»“è®ºä¸æœªæ¥å±•æœ›\n\nåˆ›å»ºæ™ºèƒ½XRä½“éªŒç›®å‰è¿‡äºç¢ç‰‡åŒ–ï¼Œåœ¨åˆ›ä½œè€…çš„æ„¿æ™¯å’Œå®ç°ä¹‹é—´è®¾ç½®äº†ä¸»è¦éšœç¢ã€‚æˆ‘ä»¬ä»‹ç»äº†XR Blocksï¼Œä¸€ä¸ªé€šè¿‡æä¾›é«˜å±‚çº§æŠ½è±¡å±‚æ¥æ¶ˆé™¤è¿™ç§å¤æ‚æ€§çš„æ¶æ„å’Œå·¥å…·åŒ…ï¼Œå®ƒå°†â€œæ„å›¾â€ï¼ˆwhatï¼‰ä¸â€œæ–¹å¼â€ï¼ˆhowï¼Œä½çº§å®ç°ï¼‰åˆ†ç¦»ï¼Œæå¤§åœ°åŠ é€Ÿäº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥åº”ç”¨çš„å¿«é€ŸåŸå‹å¼€å‘ã€‚è¿™æ˜¯è¿ˆå‘æœªæ¥ç¼–ç¨‹ã€è®¾è®¡å’Œå¯¹è¯ç•Œé™æ¶ˆå¤±çš„å¥ åŸºæ€§ä¸€æ­¥ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåƒç¼–å†™æ•…äº‹ä¸€æ ·æµç•…åœ°ç¼–å†™ç°å®ã€‚XR Blocksè¿œéå®Œç¾ï¼Œè¿™é¡¹å·¥ä½œæ˜¯æœ€åˆçš„æ„¿æ™¯æ–‡æ¡£ï¼Œæ—¨åœ¨é‚€è¯·æ›´å¤šåˆ›ä½œè€…åŠ å…¥æˆ‘ä»¬çš„æ—…ç¨‹ï¼ŒåŸºäºæˆ‘ä»¬ç›¸ä¿¡æ‹¥æœ‰æ­£ç¡®å·¥å…·é›†ï¼Œæ¯ä¸ªäººéƒ½å¯ä»¥é€šè¿‡AIé‡Šæ”¾å†…åœ¨åˆ›é€ åŠ›ã€‚\n\n### è‡´è°¢\n\nè¿™é¡¹å·¥ä½œæ˜¯Googleå¤šä¸ªå›¢é˜Ÿçš„è”åˆåä½œæˆæœï¼Œç”±David Liå’ŒRuofei Duï¼ˆåŒç­‰ä¸»è¦è´¡çŒ®ï¼‰ã€Nels Numanã€Xun Qianã€Yanhe Chenå’ŒZhongyi Zhouï¼ˆåŒç­‰æ¬¡è¦è´¡çŒ®ï¼ŒæŒ‰å­—æ¯é¡ºåºæ’åˆ—ï¼‰ä»¥åŠEvgenii Alekseevã€Geonsun Leeã€Alex Cooperã€Min Xiaã€Scott Chungã€Jeremy Nelsonã€Xiuxiu Yuanã€Jolica Diasã€Tim Bettridgeã€Benjamin Hershã€Michelle Huynhã€Konrad Piascikã€Ricardo Cabelloå’ŒDavid Kimç­‰ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆå…±åŒè´¡çŒ®ã€‚æˆ‘ä»¬æ„Ÿè°¢Mahdi Tayaraniã€Max Dzitsiukã€Patrick Hackettã€Seeyam Qiuã€Brian Collinsã€Steve Tohã€Eric Gonzalezã€NicolÃ¡s PeÃ±a Morenoã€Yi-Fei Liã€Ziyi Liuã€Jing Jinå¯¹æˆ‘ä»¬æ—©æœŸææ¡ˆå’ŒWebXRå®éªŒçš„åé¦ˆå’Œè®¨è®ºã€‚æˆ‘ä»¬æ„Ÿè°¢Max Spearã€Adarsh Kowdleå’ŒGuru Somadderçš„æ–¹å‘æ€§è´¡çŒ®å’Œå‘¨åˆ°è¯„å®¡ã€‚",
      "shortSummary": "XR Blocksæ˜¯ä¸€ä¸ªè·¨å¹³å°æ¡†æ¶ï¼Œæ—¨åœ¨å¼¥åˆäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å’Œæ‰©å±•ç°å®ï¼ˆXRï¼‰ç”Ÿæ€ç³»ç»Ÿä¹‹é—´çš„é¸¿æ²Ÿã€‚å®ƒé€šè¿‡æä¾›æ¨¡å—åŒ–æ¶æ„å’Œé«˜å±‚çº§æŠ½è±¡ï¼ŒåŠ é€Ÿæ„ŸçŸ¥å‹AI + XRåº”ç”¨çš„å¿«é€ŸåŸå‹å¼€å‘ã€‚è¯¥æ¡†æ¶åŸºäºWebXRç­‰æŠ€æœ¯ï¼Œç®€åŒ–äº†AIæ¨¡å‹é›†æˆã€ä¼ æ„Ÿå™¨èåˆå’Œè·¨å¹³å°äº¤äº’ç­‰å¤æ‚ä»»åŠ¡ï¼Œä½¿åˆ›ä½œè€…èƒ½å¤Ÿä¸“æ³¨äºç”¨æˆ·ä½“éªŒã€‚XR Blocksé€šè¿‡ç°å®æ¨¡å‹å’Œæ ¸å¿ƒå¼•æ“ï¼Œæ”¯æŒä»é«˜å±‚çº§æ„å›¾åˆ°äº¤äº’å¼åŸå‹çš„å¿«é€Ÿè½¬åŒ–ï¼Œå¹¶å·²é€šè¿‡å¤šç§åº”ç”¨åœºæ™¯å±•ç¤ºå…¶å¼ºå¤§åŠŸèƒ½ï¼Œæ—¨åœ¨èµ‹èƒ½ç¤¾åŒºé‡Šæ”¾AIåˆ›é€ åŠ›ã€‚",
      "translated_title": "XR Blocksï¼šåŠ é€ŸAI + XRåˆ›æ–°",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks1_Framework.width-1250.png",
          "alt": "XRBlocks1_Framework",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks2_RealityModel.width-1250.png",
          "alt": "XRBlocks2_RealityModel",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks3_Architecture.width-1250.png",
          "alt": "XRBlocks3_Architecture",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks4_Instructions.width-1250.png",
          "alt": "XRBlocks4_Instructions",
          "title": "",
          "position": 5
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks5_Interaction.width-1250.png",
          "alt": "XRBlocks5_Interaction",
          "title": "",
          "position": 6
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"x6cvv\">The combination of artificial intelligence (AI) and extended reality (XR) has the potential to unlock a new paradigm of immersive intelligent computing. However, a significant gap exists between the ecosystems of these two fields today. AI research and development is accelerated by mature frameworks like <a href=\"https://jax.readthedocs.io/en/latest/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">JAX</a>, <a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">PyTorch</a>, <a href=\"https://www.tensorflow.org/\" target=\"_blank\" rel=\"noopener noreferrer\">TensorFlow</a>, and benchmarks like <a href=\"https://www.image-net.org/\" target=\"_blank\" rel=\"noopener noreferrer\">ImageNet</a> and <a href=\"https://lmarena.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">LMArena</a>. Meanwhile, prototyping novel AI-driven XR interactions remains a high-friction process, often requiring practitioners to manually integrate disparate, low-level systems for perception, rendering, and interaction.</p><p data-block-key=\"5pqcb\">To bridge this gap, we introduce <a href=\"http://xrblocks.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">XR Blocks</a> (presented at <a href=\"https://uist.acm.org/2025/\" target=\"_blank\" rel=\"noopener noreferrer\">ACM UIST 2025</a>), a cross-platform framework designed to accelerate human-centered AI + XR innovation. This is a significant step from our prior research in <a href=\"https://research.google/blog/visual-blocks-for-ml-accelerating-machine-learning-prototyping-with-interactive-tools/\">Visual Blocks for ML</a>, which targets non-XR use cases and streamlines prototyping machine learning pipelines with visual programming. XR Blocks provides a modular architecture with plug-and-play components for core abstraction in AI + XR: <i>user</i>, <i>world</i>, <i>interface</i>, <i>AI</i>, and <i>agents</i>. Crucially, it is designed with the mission of accelerating rapid prototyping of perceptive AI + XR apps. Built upon accessible technologies (<a href=\"https://immersiveweb.dev/\" target=\"_blank\" rel=\"noopener noreferrer\">WebXR</a>, <a href=\"https://threejs.org/\" target=\"_blank\" rel=\"noopener noreferrer\">threejs</a>, <a href=\"https://ai.google.dev/edge/litert\" target=\"_blank\" rel=\"noopener noreferrer\">LiteRT</a>, <a href=\"https://gemini.google.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a>), our toolkit lowers the barrier to entry for XR creators. We demonstrate its utility through a set of open-source <a href=\"https://xrblocks.github.io/docs/templates/Basic/\" target=\"_blank\" rel=\"noopener noreferrer\">templates</a>, <a href=\"https://xrblocks.github.io/docs/samples/ModelViewer/\" target=\"_blank\" rel=\"noopener noreferrer\">live demos</a>, and <a href=\"https://github.com/google/xrblocks\" target=\"_blank\" rel=\"noopener noreferrer\">source code on GitHub</a>, with the goal of empowering the community to quickly move from concept to interactive prototype. You can find an overview of these capabilities in our <a href=\"https://arxiv.org/abs/2509.25504\" target=\"_blank\" rel=\"noopener noreferrer\">directional paper</a> and <a href=\"https://www.youtube.com/watch?v=75QJHTsAoB8\" target=\"_blank\" rel=\"noopener noreferrer\">teaser video</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"75QJHTsAoB8\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=75QJHTsAoB8\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kk7t9\"><i>Introductory video of XR Blocks.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xfvek\">Design principles</h2><p data-block-key=\"1ctfe\">Our architectural and API design choices are guided by three principles:</p><ul><li data-block-key=\"f851k\"><i>Embrace simplicity and readability:</i> Inspired by <a href=\"https://en.wikipedia.org/wiki/Zen_of_Python\" target=\"_blank\" rel=\"noopener noreferrer\">Python's Zen</a>, we prioritize clean, human-readable abstractions. A developer's script should read like a high-level description of the desired experience. Simple tasks should be simple to implement, and complex logic should remain explicit and understandable.</li><li data-block-key=\"43oej\"><i>Prioritize the creator experience</i>: Our primary goal is to make authoring intelligent and perceptive XR applications as seamless as possible. We believe that creators should focus on the user experience, not on the low-level â€œplumbingâ€ of sensor fusion, AI model integration, or cross-platform interaction logic.</li><li data-block-key=\"5dabh\"><i>Pragmatism over completeness</i>: We follow a design philosophy of pragmatism, since the fields of AI and XR are evolving quickly. A comprehensive, complex framework that attempts to be perfect will be obsolete upon release. We favor a simple, modular, and adaptable architecture that runs on both desktop and <a href=\"https://www.android.com/xr/\" target=\"_blank\" rel=\"noopener noreferrer\">Android XR</a> devices for a wide range of applications.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xfvek\">XR Blocks framework</h2><p data-block-key=\"6k5u2\">Drawing inspiration from <a href=\"https://visualblocks.withgoogle.com/#/\" target=\"_blank\" rel=\"noopener noreferrer\">Visual Blocks for ML</a> and <a href=\"https://research.google/blog/instructpipe-generating-visual-blocks-pipelines-with-human-instructions-and-llms/\">InstructPipe</a>, we designed the XR Blocks framework to provide a high-level, human-centered abstraction layer that separates the <i>what</i> of an interaction (denoted as <i>Script</i>, described more below) from the <i>how</i> of its low-level implementation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks1_Framework.width-1250.png\" alt=\"XRBlocks1_Framework\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks1_Framework.width-1250.png\" alt=\"XRBlocks1_Framework\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kk7t9\"><i>XR Blocks accelerates the prototyping of real-time AI + XR applications across desktop simulators and</i> <a href=\"http://android.com/xr\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Android XR</i></a><i> devices. Examples: (a) XR Realism: Prototype depth-aware, physics-based interactions in simulation and deploy the same code to real-world XR devices. (b) XR Interactions: Seamlessly integrate custom gesture models to desktop simulator and on-device XR deployment. (c) AI + XR Integration: Build intelligent, context-aware assistants, like the</i> <a href=\"https://research.google/blog/sensible-agent-a-framework-for-unobtrusive-interaction-with-proactive-ar-agents/\"><i>Sensible Agent</i></a><i> prototype that provides proactive suggestions with unobtrusive interactions.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"xfvek\">Abstractions</h3><p data-block-key=\"5j9q1\">We propose a new <i>Reality Model</i> composed of high-level abstractions to guide the implementation of the XR Blocks framework. Unlike the <a href=\"https://arxiv.org/abs/1803.10122\" target=\"_blank\" rel=\"noopener noreferrer\">World Model</a> designed for end-to-end unsupervised training, our <i>Reality Model</i> consists of replaceable modules for XR interaction. At the heart of our design is <i>Script</i>, the narrative and logical center of an application. <i>Script</i> operates on six first-class primitives (described and visualized below):</p><ul><li data-block-key=\"7hci8\"><i>User &amp; the physical world:</i> Our model is centered around the <i>User</i>, consisting of hands, gaze, and avatar. The physical <i>world</i> allows <i>Script</i> to query the perceived reality such as depth (<a href=\"https://xrblocks.github.io/docs/samples/DepthMap\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>), estimated lighting condition (<a href=\"https://xrblocks.github.io/docs/samples/Lighting\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>), and objects (<a href=\"https://xrblocks.github.io/docs/samples/Gemini-XRObject\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>).</li><li data-block-key=\"b8t27\"><i>Virtual interfaces &amp; context:</i> The model augments the blended reality with virtual UI elements, from 2D panels (<a href=\"https://xrblocks.github.io/docs/samples/UI\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>) to fully 3D assets (<a href=\"https://xrblocks.github.io/docs/samples/ModelViewer/\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>). The perception pipeline analyzes the context of environment, activities, and histories of interaction. An example application can be found in <a href=\"https://research.google/blog/sensible-agent-a-framework-for-unobtrusive-interaction-with-proactive-ar-agents/\">Sensible Agent</a> (discussed more below).</li><li data-block-key=\"e7sot\"><i>Intelligent &amp; Social Entities</i>: We treat AI-driven <i>agents</i> and remote human <i>peers</i> as primary entities within the model. This enables dynamic group conversations in hybrid human-AI conversations in <a href=\"https://dl.acm.org/doi/10.1145/3746059.3747696\" target=\"_blank\" rel=\"noopener noreferrer\">DialogLab</a>.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks2_RealityModel.width-1250.png\" alt=\"XRBlocks2_RealityModel\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks2_RealityModel.width-1250.png\" alt=\"XRBlocks2_RealityModel\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kk7t9\"><i>The conceptual Reality Model of the XR Blocks framework. At the center, Script contains the applicationâ€™s logic and operates on a unified model of first-class primitives including the user, the physical world, AI agents, and the application context.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"xfvek\">Implementation</h3><p data-block-key=\"c09j3\">This Reality Model is realized by XR Blocksâ€™s modular Core engine, which provides high-level APIs that enable developers to harness the following subsystems without needing to master the implementation:</p><ul><li data-block-key=\"e15ud\"><i>Perception &amp; input pipeline:</i> The <code>camera</code>, <code>depth</code>, and <code>sound</code> modules continuously feed and update the Reality Modelâ€™s representation of physical reality. The <code>input</code> module normalizes user actions from various devices, providing the raw data for XR Blocks to interpret.</li><li data-block-key=\"2r6v8\"><i>AI as a core utility:</i> The <code>ai</code> module acts as a central nervous system, providing simple yet powerful functions (<code>.query</code>, <code>.runModel</code>) that make large models an accessible utility.</li><li data-block-key=\"fahdq\"><i>Experience &amp; visualization toolkit:</i> To enable rapid creation, the toolkit provides a library of common affordances. The <code>ux</code> module offers reusable interaction behaviors like <code>.selectable</code> and <code>.draggable</code> (<a href=\"https://xrblocks.github.io/docs/samples/ModelViewer/\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>), while the <code>ui</code> and <code>effect</code> modules handle the rendering of interfaces and complex visual effects like occlusion (<a href=\"https://xrblocks.github.io/docs/samples/Occlusion\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>).</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks3_Architecture.width-1250.png\" alt=\"XRBlocks3_Architecture\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks3_Architecture.width-1250.png\" alt=\"XRBlocks3_Architecture\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kk7t9\"><i>The modular architecture of the XR Blocksâ€™s core engine, which consists of essential subsystems to realize the frameworkâ€™s high-level abstractions, spanning perception (</i><code><i>depth</i></code><i>,</i> <code><i>input</i></code><i>), AI integration (</i><code><i>ai</i></code><i>,</i> <code><i>agent</i></code><i>), and user experience (</i><code><i>ui</i></code><i>,</i> <code><i>ux</i></code><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xfvek\">By separating the abstract Reality Model from the concrete Core engine, XR Blocks enables a powerful new creative workflow. The goal is to allow creators to move from high level, human-centric ideas to interactive prototypes much more quickly. We envision a future where any declarative prompt, <i>â€œWhen the user pinches at an object, an agent should generate a poem of itâ€</i>, could be directly translated to high-level instructions in XR Blocks:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks4_Instructions.width-1250.png\" alt=\"XRBlocks4_Instructions\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks4_Instructions.width-1250.png\" alt=\"XRBlocks4_Instructions\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xfvek\">Hence, the creatorâ€™s prompt is no longer pseudocode but a direct summary of the implementation logic. We envision this framework to more seamlessly translate such user intent into a system-level execution flow, composing capabilities from the <code>input</code>, <code>sound</code>, <code>ai</code>, <code>world</code>, <code>ui</code>, and <code>agent</code> modules to generate an emergent, intelligent behavior with user interaction.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks5_Interaction.width-1250.png\" alt=\"XRBlocks5_Interaction\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks5_Interaction.width-1250.png\" alt=\"XRBlocks5_Interaction\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kk7t9\"><i>The Interaction Grammar of XR Blocks, which abstracts user input by distinguishing between two types of interaction. Explicit events are direct, low-level inputs (e.g., a touch or click), while implicit intents are higher-level interpretations (e.g., a gesture or voice command), allowing creators to build interaction against user intent.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"fbybx\">Application scenarios</h2><p data-block-key=\"5nimk\">We provide a suite of interactive applications to demonstrate the expressive power and flexibility of the XR Blocks framework. These examples showcase how our framework enables the rapid prototyping of sophisticated experiences that were previously too complex and costly to build, facilitating the creation of realistic, interactive, and intelligent mixed-reality worlds:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks_Applications.width-1250.png\" alt=\"XRBlocks_Applications\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks_Applications.width-1250.png\" alt=\"XRBlocks_Applications\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8ze35\"><i>Applications of XR Blocks. (1) XR Realism: Depth-aware and physics-based ball pit (</i><a href=\"https://xrblocks.github.io/docs/samples/Ballpit\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>) and splash games (</i><a href=\"https://xrblocks.github.io/docs/samples/Splash\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>); geometry-aware shadows (</i><a href=\"https://xrblocks.github.io/docs/samples/Occlusion\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>), 3D Gaussian splatting with occlusion, and lighting estimation. (2) XR Interaction: Immersive emoji (</i><a href=\"https://xrblocks.github.io/docs/samples/XR-Emoji\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>) and rock paper scissors game (</i><a href=\"https://xrblocks.github.io/docs/samples/RockPaperScissors\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>) empowered by custom ML models, dynamic swipe recognition, touch and grab with the physical world. (3) AI + XR: Integration with conversational AI (</i><a href=\"https://xrblocks.github.io/docs/samples/Gemini-Icebreakers\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>), XR objects (</i><a href=\"https://xrblocks.github.io/docs/samples/Gemini-XRObject\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>), glasses simulation in XR, and poem generation with a real-world camera.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fbybx\">The true power of the framework is realized when this Reality Model is deeply integrated with generative AI to create dynamic, personalized environments. We demonstrate this by building systems like Augmented Object Intelligence (<a href=\"https://research.google/blog/augmented-object-intelligence-with-xr-objects/\">XR-Objects</a>), which imbues everyday physical objects with interactive digital affordances, such as dynamic virtual buttons. XR Blocks also serves as the foundation for <a href=\"https://research.google/blog/sensible-agent-a-framework-for-unobtrusive-interaction-with-proactive-ar-agents/\">Sensible Agent</a> (published on ACM UIST 2025), a system for proactive and unobtrusive AR assistance. Our architecture provides the agent's core perception and interaction logic, providing an example of our primary goal: by providing robust, high-level tools, XR Blocks empowers Human-Computer Interaction researchers to bypass low-level implementation and focus directly on higher-order challenges like the cognitive principles of human-agent collaboration.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/XRBlocks6_Demos.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8ze35\"><i>Demonstrations of XR Blocks SDK. (1) Using XR Blocks with conversational AI to automatically generate and test user prompts. (2) Running physical collision with depth sensing on Android XR. (3) Running LiteRT on a device with a custom gesture model to trigger XR animation.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"fbybx\">Conclusion and future directions</h2><p data-block-key=\"c4i24\">Creating intelligent XR experiences is currently too fragmented, placing a major barrier between a creator's vision and its realization. We presented <a href=\"https://xrblocks.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">XR Blocks</a>, an architecture and toolkit that dissolves this complexity by providing a high-level abstraction layer that separates <i>what</i> (the intent) from the <i>how</i> (the low-level implementation), dramatically accelerating the prototyping of context-aware applications. This is a foundational step toward a future where the boundaries between programming, design, and conversation disappear, enabling us to script realities as fluidly as we script stories. XR Blocks is far from perfect, and this work serves as <a href=\"https://arxiv.org/abs/2509.25504\" target=\"_blank\" rel=\"noopener noreferrer\">an initial visionary document</a> to invite more creators to join our journey, based on our belief that <i>with the right set of tools, everyone can unleash their inner creativity with AI</i>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"fbybx\">Acknowledgements</h2><p data-block-key=\"ccr67\"><i>This work is a joint collaboration across multiple teams at Google. The following researchers and engineers contributed to this work: David Li and Ruofei Du (equal primary contributions), Nels Numan, Xun Qian, Yanhe Chen, and Zhongyi Zhou, (equal secondary contributions, sorted alphabetically), as well as Evgenii Alekseev, Geonsun Lee, Alex Cooper, Min Xia, Scott Chung, Jeremy Nelson, Xiuxiu Yuan, Jolica Dias, Tim Bettridge, Benjamin Hersh, Michelle Huynh, Konrad Piascik, Ricardo Cabello, and David Kim. We would like to thank Mahdi Tayarani, Max Dzitsiuk, Patrick Hackett, Seeyam Qiu, Brian Collins, Steve Toh, Eric Gonzalez, NicolÃ¡s PeÃ±a Moreno, Yi-Fei Li, Ziyi Liu, Jing Jin for their feedback and discussion on our early-stage proposal and WebXR experiments. We thank Max Spear, Adarsh Kowdle, and Guru Somadder for the directional contribution and thoughtful reviews.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "è¯­éŸ³åˆ°æ£€ç´¢ (S2R)ï¼šä¸€ç§æ–°çš„è¯­éŸ³æœç´¢æ–¹æ³• (åŸæ ‡é¢˜: â€‹â€‹Speech-to-Retrieval (S2R): A new approach to voice search)",
      "link": "https://research.google/blog/speech-to-retrieval-s2r-a-new-approach-to-voice-search/",
      "pubDate": "Mon, 06 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-06T16:00:00.000Z",
      "creator": "Google",
      "summary": "# è¯­éŸ³åˆ°æ£€ç´¢ (S2R)ï¼šä¸€ç§æ–°çš„è¯­éŸ³æœç´¢æ–¹æ³•\n\n## ä¼ ç»Ÿè¯­éŸ³æœç´¢çš„æŒ‘æˆ˜\n\n*   **çº§è”æ¨¡å‹ (Cascade Modeling)**ï¼šå½“å‰çš„è¯­éŸ³æœç´¢ç³»ç»Ÿé€šå¸¸é‡‡ç”¨çº§è”æ¨¡å‹ï¼Œå…ˆé€šè¿‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ« (ASR) å°†è¯­éŸ³è¾“å…¥è½¬æ¢ä¸ºæ–‡æœ¬æŸ¥è¯¢ï¼Œç„¶ååŸºäºæ–‡æœ¬è¿›è¡Œæœç´¢ã€‚\n*   **ASR é”™è¯¯çš„å½±å“**ï¼šè¿™ç§æ–¹æ³•çš„ä¸»è¦æŒ‘æˆ˜åœ¨äºï¼ŒASR é˜¶æ®µçš„å¾®å°é”™è¯¯å¯èƒ½æ˜¾è‘—æ”¹å˜æŸ¥è¯¢çš„å«ä¹‰ï¼Œå¯¼è‡´ä¸ç›¸å…³çš„æœç´¢ç»“æœã€‚\n    *   **ç¤ºä¾‹**ï¼šå°†â€œThe Scream paintingâ€ï¼ˆã€Šå‘å–Šã€‹ç”»ä½œï¼‰é”™è¯¯è¯†åˆ«ä¸ºâ€œscreen paintingâ€ï¼ˆå±å¹•ç”»ä½œï¼‰ï¼Œå°†è¿”å›å…³äºå±å¹•ç»˜ç”»æŠ€å·§çš„ç»“æœï¼Œè€Œéè’™å…‹æ°ä½œçš„ä¿¡æ¯ã€‚\n*   **é—®é¢˜æ ¹æº**ï¼šçº§è”æ¨¡å‹å­˜åœ¨ä¿¡æ¯ä¸¢å¤±å’Œé”™è¯¯ä¼ æ’­çš„é—®é¢˜ï¼ŒASR ç³»ç»Ÿä¸€æ—¦æ—©æœŸè¯¯è§£éŸ³é¢‘ï¼Œé”™è¯¯å°±ä¼šä¼ é€’ç»™æœç´¢ç³»ç»Ÿï¼Œä¸”æœç´¢ç³»ç»Ÿé€šå¸¸æ— æ³•çº æ­£ã€‚\n\n## å¼•å…¥è¯­éŸ³åˆ°æ£€ç´¢ (S2R)\n\n*   **æ ¸å¿ƒç†å¿µ**ï¼šS2R æ˜¯ä¸€ç§ç›´æ¥ä»è¯­éŸ³æŸ¥è¯¢ä¸­è§£é‡Šå’Œæ£€ç´¢ä¿¡æ¯çš„æŠ€æœ¯ï¼Œå®Œå…¨ç»•è¿‡äº†ä¸­é—´çš„ã€å¯èƒ½å­˜åœ¨ç¼ºé™·çš„æ–‡æœ¬è½¬å½•æ­¥éª¤ã€‚\n*   **èŒƒå¼è½¬å˜**ï¼šå®ƒä»£è¡¨äº†æœºå™¨å¤„ç†äººç±»è¯­éŸ³çš„æ ¹æœ¬æ€§æ¶æ„å’Œç†å¿µè½¬å˜ã€‚ä¼ ç»Ÿç³»ç»Ÿå…³æ³¨â€œè¯´äº†ä»€ä¹ˆè¯ï¼Ÿâ€ï¼Œè€Œ S2R æ—¨åœ¨å›ç­”æ›´å¼ºå¤§çš„é—®é¢˜ï¼šâ€œæ­£åœ¨å¯»æ±‚ä»€ä¹ˆä¿¡æ¯ï¼Ÿâ€\n*   **ç›®æ ‡**ï¼šå¼¥è¡¥å½“å‰è¯­éŸ³æœç´¢ä½“éªŒä¸­å­˜åœ¨çš„æ˜¾è‘—è´¨é‡å·®è·ã€‚\n\n## SVQ æ•°æ®é›†\n\n*   **å¼€æ”¾èµ„æº**ï¼šä½œä¸º Massive Sound Embedding Benchmark (MSEB) çš„ä¸€éƒ¨åˆ†ï¼Œè°·æ­Œå¼€æ”¾äº† Simple Voice Questions (SVQ) æ•°æ®é›†ã€‚\n*   **å†…å®¹**ï¼šè¯¥æ•°æ®é›†åŒ…å«ä»¥ 17 ç§ä¸åŒè¯­è¨€å’Œ 26 ä¸ªåŒºåŸŸå½•åˆ¶çš„çŸ­éŸ³é¢‘é—®é¢˜é›†åˆã€‚\n*   **ç”¨é€”**ï¼šç”¨äºè¯„ä¼° S2R çš„æ€§èƒ½æ½œåŠ›ã€‚\n\n## S2R æ½œåŠ›è¯„ä¼°å®éªŒ\n\n*   **å®éªŒç›®çš„**ï¼šæ¨¡æ‹Ÿç†æƒ³çš„ ASR æ€§èƒ½ï¼Œä»¥æ­ç¤ºå½“å‰ç³»ç»Ÿä¸ç†è®ºæœ€ä½³æ€§èƒ½ä¹‹é—´çš„å·®è·ã€‚\n*   **å®éªŒæ–¹æ³•**ï¼š\n    1.  æ”¶é›†ä»£è¡¨æ€§è¯­éŸ³æœç´¢æŸ¥è¯¢ã€‚\n    2.  ç”±äººå·¥æ ‡æ³¨å‘˜æ‰‹åŠ¨è½¬å½•è¿™äº›æŸ¥è¯¢ï¼Œåˆ›å»ºâ€œå®Œç¾ ASRâ€åœºæ™¯ï¼ˆå³â€œgroundtruthâ€ï¼‰ã€‚\n    3.  **å»ºç«‹ä¸¤ä¸ªæœç´¢ç³»ç»Ÿè¿›è¡Œæ¯”è¾ƒ**ï¼š\n        *   **çº§è” ASR (Cascade ASR)**ï¼šæ¨¡æ‹ŸçœŸå®ä¸–ç•Œè®¾ç½®ï¼Œè¯­éŸ³é€šè¿‡ ASR è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œç„¶åä¼ é€’ç»™æ£€ç´¢ç³»ç»Ÿã€‚\n        *   **çº§è”çœŸå€¼ (Cascade Groundtruth)**ï¼šæ¨¡æ‹Ÿâ€œå®Œç¾â€çº§è”æ¨¡å‹ï¼Œå°†æ— ç¼ºé™·çš„çœŸå€¼æ–‡æœ¬ç›´æ¥å‘é€ç»™ç›¸åŒçš„æ£€ç´¢ç³»ç»Ÿã€‚\n    4.  äººç±»è¯„ä¼°å‘˜æ¯”è¾ƒä¸¤ä¸ªç³»ç»Ÿçš„æœç´¢ç»“æœè´¨é‡ã€‚\n*   **è¯„ä¼°æŒ‡æ ‡**ï¼š\n    *   **ASR è´¨é‡**ï¼šè¯é”™è¯¯ç‡ (WER)ã€‚\n    *   **æœç´¢æ€§èƒ½**ï¼šå¹³å‡å€’æ•°æ’å (MRR)ï¼Œç”¨äºè¯„ä¼°å¯¹æŸ¥è¯¢åˆ—è¡¨å“åº”çš„æ­£ç¡®æ€§æ¦‚ç‡ã€‚\n*   **å…³é”®å‘ç°**ï¼š\n    *   **WER ä¸ MRR çš„å¤æ‚å…³ç³»**ï¼šè¾ƒä½çš„ WER å¹¶ä¸æ€»èƒ½å¯é åœ°å¸¦æ¥è¾ƒé«˜çš„ MRRï¼Œé”™è¯¯çš„å…·ä½“æ€§è´¨ï¼ˆè€Œéä»…ä»…å­˜åœ¨ï¼‰æ˜¯å…³é”®çš„ã€ä¾èµ–äºè¯­è¨€çš„å› ç´ ã€‚\n    *   **æ˜¾è‘—çš„æ€§èƒ½å·®è·**ï¼šåœ¨æ‰€æœ‰æµ‹è¯•è¯­è¨€ä¸­ï¼Œä¸¤ä¸ªç³»ç»Ÿä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„ MRR å·®å¼‚ã€‚è¿™è¡¨æ˜å½“å‰çº§è”è®¾è®¡ä¸å®Œç¾è¯­éŸ³è¯†åˆ«ç†è®ºä¸Šå¯èƒ½è¾¾åˆ°çš„æ€§èƒ½ä¹‹é—´å­˜åœ¨å·¨å¤§å·®è·ï¼Œä¸º S2R æ¨¡å‹æä¾›äº†å·¨å¤§çš„æ”¹è¿›æ½œåŠ›ã€‚\n\n![SVQæ•°æ®é›†ä¸­è¯­éŸ³æœç´¢è¯­è¨€çš„ASRæ¨¡å‹è¯é”™è¯¯ç‡](https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval3_WER.width-1250.png)\n*SVQæ•°æ®é›†ä¸­è¯­éŸ³æœç´¢è¯­è¨€çš„ASRæ¨¡å‹è¯é”™è¯¯ç‡*\n\n![å½“å‰çœŸå®ä¸–ç•Œï¼ˆâ€œçº§è” ASRâ€ï¼›è“è‰²ï¼‰æ¨¡å‹ä¸çœŸå€¼ï¼ˆå³å®Œç¾ï¼›â€œçº§è”çœŸå€¼â€ï¼›ç»¿è‰²ï¼‰çš„ MRR](https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval4_MRRCurrent.width-1250.png)\n*å½“å‰çœŸå®ä¸–ç•Œï¼ˆâ€œçº§è” ASRâ€ï¼›è“è‰²ï¼‰æ¨¡å‹ä¸çœŸå€¼ï¼ˆå³å®Œç¾ï¼›â€œçº§è”çœŸå€¼â€ï¼›ç»¿è‰²ï¼‰çš„ MRR*\n\n## S2R çš„æ¶æ„ï¼šä»å£°éŸ³åˆ°æ„ä¹‰\n\n*   **æ ¸å¿ƒæ¶æ„**ï¼šåŒç¼–ç å™¨ (dual-encoder) æ¶æ„ã€‚\n*   **ç»„ä»¶**ï¼š\n    *   **éŸ³é¢‘ç¼–ç å™¨**ï¼šå¤„ç†åŸå§‹éŸ³é¢‘æŸ¥è¯¢ï¼Œå°†å…¶è½¬æ¢ä¸ºæ•è·è¯­ä¹‰å«ä¹‰çš„ä¸°å¯Œå‘é‡è¡¨ç¤ºã€‚\n    *   **æ–‡æ¡£ç¼–ç å™¨**ï¼šå­¦ä¹ æ–‡æ¡£çš„ç±»ä¼¼å‘é‡è¡¨ç¤ºã€‚\n*   **è®­ç»ƒæœºåˆ¶**ï¼š\n    *   ä½¿ç”¨å¤§é‡é…å¯¹çš„éŸ³é¢‘æŸ¥è¯¢å’Œç›¸å…³æ–‡æ¡£æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚\n    *   ç³»ç»ŸåŒæ—¶è°ƒæ•´ä¸¤ä¸ªç¼–ç å™¨çš„å‚æ•°ã€‚\n    *   **è®­ç»ƒç›®æ ‡**ï¼šç¡®ä¿éŸ³é¢‘æŸ¥è¯¢çš„å‘é‡åœ¨è¡¨ç¤ºç©ºé—´ä¸­ä¸ç›¸åº”æ–‡æ¡£çš„å‘é‡åœ¨å‡ ä½•ä¸Šæ¥è¿‘ã€‚\n*   **ä¼˜åŠ¿**ï¼šè¯¥æ¶æ„ä½¿æ¨¡å‹èƒ½å¤Ÿç›´æ¥ä»éŸ³é¢‘ä¸­å­¦ä¹ æ›´æ¥è¿‘æ£€ç´¢æ‰€éœ€çš„æ ¸å¿ƒæ„å›¾ï¼Œä»è€Œç»•è¿‡çº§è”è®¾è®¡ä¸­è„†å¼±çš„é€å­—è½¬å½•ä¸­é—´æ­¥éª¤ã€‚\n\n![éŸ³é¢‘å’Œæ–‡æ¡£åµŒå…¥ä¹‹é—´ç›¸ä¼¼æ€§æŸå¤±çš„å·®å¼‚](https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval5_SimilarityLoss.width-1250.png)\n*éŸ³é¢‘å’Œæ–‡æ¡£åµŒå…¥ä¹‹é—´ç›¸ä¼¼æ€§æŸå¤±çš„å·®å¼‚*\n\n## S2R æ¨¡å‹çš„å·¥ä½œåŸç†\n\n1.  ç”¨æˆ·è¯´å‡ºæŸ¥è¯¢ã€‚\n2.  éŸ³é¢‘æµå¼ä¼ è¾“åˆ°é¢„è®­ç»ƒçš„éŸ³é¢‘ç¼–ç å™¨ï¼Œç”Ÿæˆä¸€ä¸ªæŸ¥è¯¢å‘é‡ï¼ˆéŸ³é¢‘åµŒå…¥ï¼‰ã€‚\n3.  è¯¥å‘é‡ç”¨äºé€šè¿‡å¤æ‚çš„æœç´¢æ’åè¿‡ç¨‹ï¼Œä»ç´¢å¼•ä¸­é«˜æ•ˆè¯†åˆ«ä¸€ç»„é«˜åº¦ç›¸å…³çš„å€™é€‰ç»“æœã€‚\n    *   **ç¤ºä¾‹**ï¼šç”¨æˆ·è¯­éŸ³è¯·æ±‚â€œThe Scream paintingâ€è¢«éŸ³é¢‘ç¼–ç å™¨è½¬æ¢ä¸ºéŸ³é¢‘åµŒå…¥ã€‚\n    *   è¯¥åµŒå…¥æ‰«æå¤§é‡æ–‡æ¡£ç´¢å¼•ï¼Œæµ®ç°å‡ºç›¸ä¼¼åº¦é«˜çš„åˆå§‹å€™é€‰ç»“æœï¼ˆå¦‚ç»´åŸºç™¾ç§‘é¡µé¢ã€è’™å…‹åšç‰©é¦†ç½‘ç«™ï¼‰ã€‚\n4.  æœç´¢æ’åç³»ç»Ÿç»“åˆåˆå§‹åˆ†æ•°å’Œæ•°ç™¾ä¸ªå…¶ä»–ä¿¡å·ï¼Œæ·±å…¥ç†è§£ç›¸å…³æ€§å’Œè´¨é‡ï¼Œåœ¨æçŸ­æ—¶é—´å†…ç¼–æ’æœ€ç»ˆæ’åï¼Œç¡®ä¿å‘ç”¨æˆ·å‘ˆç°æœ€æœ‰å¸®åŠ©å’Œæœ€å€¼å¾—ä¿¡èµ–çš„ä¿¡æ¯ã€‚\n\n## S2R è¯„ä¼°ç»“æœ\n\n*   åœ¨ SVQ æ•°æ®é›†ä¸Šå¯¹ S2R ç³»ç»Ÿè¿›è¡Œäº†è¯„ä¼°ã€‚\n*   **å…³é”®ç»“æœ**ï¼š\n    *   S2R æ¨¡å‹æ˜¾è‘—ä¼˜äºåŸºçº¿çº§è” ASR æ¨¡å‹ã€‚\n    *   å…¶æ€§èƒ½æ¥è¿‘ç”±çº§è”çœŸå€¼æ¨¡å‹è®¾å®šçš„ä¸Šé™ã€‚\n    *   å°½ç®¡ç»“æœä»¤äººé¼“èˆï¼Œä½†å‰©ä½™çš„å·®è·è¡¨æ˜ä»éœ€è¿›ä¸€æ­¥ç ”ç©¶ã€‚\n\n![å½“å‰çœŸå®ä¸–ç•Œï¼ˆâ€œçº§è” ASRâ€ï¼›è“è‰²ï¼‰æ¨¡å‹ä¸çœŸå€¼ï¼ˆå³å®Œç¾ï¼›â€œçº§è”çœŸå€¼â€ï¼›ç»¿è‰²ï¼‰ä»¥åŠ S2R æ¨¡å‹æ€§èƒ½ï¼ˆâ€œS2Râ€æ©™è‰²æ¡ï¼‰çš„ MRR](https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval7_Results.width-1250.png)\n*å½“å‰çœŸå®ä¸–ç•Œï¼ˆâ€œçº§è” ASRâ€ï¼›è“è‰²ï¼‰æ¨¡å‹ä¸çœŸå€¼ï¼ˆå³å®Œç¾ï¼›â€œçº§è”çœŸå€¼â€ï¼›ç»¿è‰²ï¼‰ä»¥åŠ S2R æ¨¡å‹æ€§èƒ½ï¼ˆâ€œS2Râ€æ©™è‰²æ¡ï¼‰çš„ MRR*\n\n## è¯­éŸ³æœç´¢çš„æ–°æ—¶ä»£\n\n*   **å·²æŠ•å…¥å®é™…åº”ç”¨**ï¼šS2R é©±åŠ¨çš„è¯­éŸ³æœç´¢å·²åœ¨å¤šç§è¯­è¨€ä¸­ä¸ºç”¨æˆ·æä¾›æœåŠ¡ï¼Œå®ç°äº†è¶…è¶Šä¼ ç»Ÿçº§è”ç³»ç»Ÿçš„æ˜¾è‘—å‡†ç¡®æ€§æå‡ã€‚\n*   **æ¨åŠ¨é¢†åŸŸå‘å±•**ï¼šè°·æ­Œå¼€æ”¾äº† SVQ æ•°æ®é›†ä½œä¸º MSEB çš„ä¸€éƒ¨åˆ†ï¼Œæ—¨åœ¨åŠ é€Ÿæ•´ä¸ªé¢†åŸŸçš„å‘å±•ã€‚\n*   **å‘¼ååˆä½œ**ï¼šé‚€è¯·å…¨çƒç ”ç©¶ç¤¾åŒºåˆ©ç”¨è¿™äº›æ•°æ®ï¼Œåœ¨å…¬å…±åŸºå‡†ä¸Šæµ‹è¯•æ–°æ–¹æ³•ï¼Œå…±åŒæ„å»ºä¸‹ä¸€ä»£çœŸæ­£æ™ºèƒ½çš„è¯­éŸ³ç•Œé¢ã€‚",
      "shortSummary": "ä¼ ç»Ÿè¯­éŸ³æœç´¢ä¾èµ–ASRå°†è¯­éŸ³è½¬æ–‡æœ¬ï¼Œæ˜“å› è¯†åˆ«é”™è¯¯å¯¼è‡´ç»“æœä¸å‡†ç¡®ã€‚è¯­éŸ³åˆ°æ£€ç´¢ï¼ˆS2Rï¼‰æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œå®ƒç›´æ¥ä»è¯­éŸ³æŸ¥è¯¢ä¸­ç†è§£å¹¶æ£€ç´¢ä¿¡æ¯ï¼Œç»•è¿‡æ–‡æœ¬è½¬å½•ã€‚S2Ré‡‡ç”¨åŒç¼–ç å™¨æ¶æ„ï¼Œå°†éŸ³é¢‘ç›´æ¥æ˜ å°„åˆ°è¯­ä¹‰æ„å›¾ã€‚å®éªŒè¡¨æ˜ï¼ŒS2Ræ˜¾è‘—ä¼˜äºä¼ ç»ŸASRç³»ç»Ÿï¼Œå¹¶æ¥è¿‘ç†è®ºæœ€ä½³æ€§èƒ½ã€‚è°·æ­Œå·²å°†S2RæŠ•å…¥å®é™…åº”ç”¨ï¼Œå¹¶åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­æå‡äº†è¯­éŸ³æœç´¢å‡†ç¡®æ€§ï¼ŒåŒæ—¶å¼€æ”¾äº†SVQæ•°æ®é›†ä»¥æ¨åŠ¨è¯¥é¢†åŸŸå‘å±•ã€‚",
      "translated_title": "è¯­éŸ³åˆ°æ£€ç´¢ (S2R)ï¼šä¸€ç§æ–°çš„è¯­éŸ³æœç´¢æ–¹æ³•",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval3_WER.width-1250.png",
          "alt": "SpeechToRetrieval3_WER",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval4_MRRCurrent.width-1250.png",
          "alt": "SpeechToRetrieval4_MRRCurrent",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval5_SimilarityLoss.width-1250.png",
          "alt": "SpeechToRetrieval5_SimilarityLoss",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval7_Results.width-1250.png",
          "alt": "SpeechToRetrieval7_Results",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"42rfj\">Voice-based web search has been around a long time and continues to be used by many people, with the underlying technology evolving rapidly to allow for expanded use cases. Googleâ€™s initial voice search solution used <a href=\"https://en.wikipedia.org/wiki/Speech_recognition\" target=\"_blank\" rel=\"noopener noreferrer\">automatic speech recognition</a> (ASR) to turn the voice input into a text query, and then searched for documents matching that text query. However, a challenge with this <a href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36340.pdf\">cascade modeling approach</a> is that any slight errors in the speech recognition phase can significantly alter the meaning of the query, producing the wrong results.</p><p data-block-key=\"9va6b\">For example, imagine someone does a voice-based web search for the famous painting, â€œ<a href=\"https://en.wikipedia.org/wiki/The_Scream\" target=\"_blank\" rel=\"noopener noreferrer\">The Scream</a>â€, by Edvard Munch. The search engine uses the typical approach of cascade modeling, first converting the voice query to text via ASR before passing the text to the search system. Ideally, the ASR transcribes the query perfectly. The search system then receives the correct text â€” â€œthe Scream paintingâ€ â€” and provides relevant results, like the paintingâ€™s history, its meaning, and where itâ€™s displayed. However, what if the ASR system mistakes the â€œmâ€ of â€œscreamâ€ for an â€œnâ€? It misinterprets the query as â€œscreen paintingâ€ and returns irrelevant results about screen painting techniques instead of details about Munch's masterpiece.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/SpeechToRetrieval2_Cascade.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"255p4\"><i>ASR accuracy is key for voice search. See what happens when a system correctly transcribes a query versus when it transcribes it incorrectly.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"42rfj\">To prevent such errors in web search systems, what if the system could map directly from speech to the desired retrieval intent, bypassing the textual transcription entirely?</p><p data-block-key=\"8hvip\">Enter Speech-to-Retrieval (S2R). At its core, S2R is a technology that directly interprets and retrieves information from a spoken query without the intermediate, and potentially flawed, step of having to create a perfect text transcript. It represents a fundamental architectural and philosophical shift in how machines process human speech. Where today's common voice search technologies are focused on the question, \"What words were said?\", S2R is designed to answer a more powerful question: \"What information is being sought?\" This post explores the substantial quality gap in current voice search experiences and demonstrates how the S2R model is poised to fill it. In addition, we are open-sourcing the <a href=\"https://huggingface.co/datasets/google/svq#:~:text=Simple%20Voice%20Questions%20(SVQ)%20is,languages%20under%20multiple%20audio%20conditions.\" target=\"_blank\" rel=\"noopener noreferrer\">Simple Voice Questions</a> (SVQ) dataset, a collection of short audio questions recorded in 17 different languages and 26 locales, which we used to evaluate the performance potential of S2R. The SVQ dataset is part of the new <a href=\"https://github.com/google-research/mseb\" target=\"_blank\" rel=\"noopener noreferrer\">Massive Sound Embedding Benchmark</a> benchmark.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"42rfj\">Evaluating the potential of S2R</h2><p data-block-key=\"e2hqh\">When a traditional ASR system converts audio into a single text string, it may lose contextual cues that could help disambiguate the meaning (i.e., information loss). If the system misinterprets the audio early on, that error is passed along to the search engine, which typically lacks the ability to correct it (i.e., error propagation). As a result, the final search result may not reflect the user's intent.</p><p data-block-key=\"fmodj\">To investigate this relationship, we conducted an experiment designed to simulate an ideal ASR performance. We began by collecting a representative set of test queries reflecting typical voice search traffic. Crucially, these queries were then manually transcribed by human annotators, effectively creating a \"perfect ASR\" scenario where the transcription is the absolute truth.</p><p data-block-key=\"esgbn\">We then established two distinct search systems for comparison (see chart below):</p><ul><li data-block-key=\"crdkj\">Cascade ASR represents a typical real-world setup, where speech is converted to text by an automatic speech recognition (ASR) system, and that text is then fed to a retrieval system.</li><li data-block-key=\"103o6\">Cascade groundtruth simulates a \"perfect\" cascade model by sending the flawless ground-truth text directly to the same retrieval system.</li></ul><p data-block-key=\"d2bvm\">The retrieved documents from both systems (cascade ASR and cascade groundtruth) were then presented to human evaluators, or \"raters\", alongside the original true query. The evaluators were tasked with comparing the search results from both systems, providing a subjective assessment of their respective quality.</p><p data-block-key=\"edlf6\">We use <a href=\"https://en.wikipedia.org/wiki/Word_error_rate\" target=\"_blank\" rel=\"noopener noreferrer\">word error rate</a> (WER) to measure the ASR quality and to measure the search performance, we use <a href=\"https://en.wikipedia.org/wiki/Mean_reciprocal_rank\" target=\"_blank\" rel=\"noopener noreferrer\">mean reciprocal rank</a> (MRR) â€” a statistical metric for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness and calculated as the average of the reciprocals of the rank of the first correct answer across all queries. The difference in MRR and WER between the real-world system and the groundtruth system reveals the potential performance gains <a href=\"https://vocal.media/education/the-rise-of-voice-search-and-its-impact-on-indian-businesses\" target=\"_blank\" rel=\"noopener noreferrer\">across some of the most commonly used voice search languages</a> in the SVQ dataset (shown below).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval3_WER.width-1250.png\" alt=\"SpeechToRetrieval3_WER\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval3_WER.width-1250.png\" alt=\"SpeechToRetrieval3_WER\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"255p4\"><i>The word error rate (WER) of the ASR model across voice search languages in the SVQ dataset.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval4_MRRCurrent.width-1250.png\" alt=\"SpeechToRetrieval4_MRRCurrent\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval4_MRRCurrent.width-1250.png\" alt=\"SpeechToRetrieval4_MRRCurrent\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"255p4\"><i>MRR of current real-world (â€œCascade ASRâ€; blue) models vs ground truth (i.e., perfect; â€œCascade Groundtruthâ€; green).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"42rfj\">The results of this comparison lead to two critical observations. First, and as can be seen by comparing both charts above, we found that a lower WER does not reliably lead to a higher MRR across different languages. The relationship is complex, suggesting that the impact of transcription errors on downstream tasks is not fully captured by the WER metric. The specific nature of an error â€” not just its existence â€” appears to be a critical, language-dependent factor. Second, and more importantly, thereâ€™s a significant MRR difference between the two systems across all tested languages. This reveals a substantial performance gap between current cascade designs and what is theoretically possible with perfect speech recognition. This gap represents the clear potential for S2R models to fundamentally improve voice search quality.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"h9kvr\">The architecture of S2R: From sound to meaning</h2><p data-block-key=\"aej9r\">At the heart of our S2R model is a dual-encoder architecture. This design features two specialized neural networks that learn from vast amounts of data to understand the relationship between speech and information. An audio encoder processes the raw audio of a query, converting it into a rich vector representation that captures its semantic meaning. In parallel, a document encoder learns a similar vector representation for documents.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval5_SimilarityLoss.width-1250.png\" alt=\"SpeechToRetrieval5_SimilarityLoss\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval5_SimilarityLoss.width-1250.png\" alt=\"SpeechToRetrieval5_SimilarityLoss\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"iex25\"><i>Difference in similarity loss between audio and document embedding.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"h9kvr\">The key to this model is how it is trained. Using a large dataset of paired audio queries and relevant documents, the system learns to adjust the parameters of both encoders simultaneously.</p><p data-block-key=\"3mkue\">The training objective ensures that the vector for an audio query is geometrically close to the vectors of its corresponding documents in the representation space. This architecture allows the model to learn something closer to the essential <i>intent</i> required for retrieval directly from the audio, bypassing the fragile intermediate step of transcribing every word, which is the principal weakness of the cascade design.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"h9kvr\">How the S2R model works</h2><p data-block-key=\"7b96s\">When a user speaks a query, the audio is streamed to the pre-trained audio encoder, which generates a query vector. This vector is then used to efficiently identify a highly relevant set of candidate results from our index through a complex search ranking process.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/SpeechToRetrieval6_Final.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"iex25\"><i>How S2R processes a spoken query.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"h9kvr\">The animation above illustrates how S2R understands and answers a spoken query. It starts with a user's voice request for â€œThe Scream paintingâ€. An audio encoder translates the sound into a rich <a href=\"https://dev.to/josethz00/audio-embeddings-understanding-the-basics-4pc1\" target=\"_blank\" rel=\"noopener noreferrer\">audio embedding</a>, a vector that represents the deep meaning of the query. This embedding is then used to scan a massive index of documents, surfacing initial candidates with high similarity scores, like the Wikipedia page for â€œThe Screamâ€ (0.8) and the Munch Museum website (0.7).</p><p data-block-key=\"3p31t\">But finding relevant documents is just the beginning. The crucial final step is orchestrated by the search ranking system. This powerful intelligence goes far beyond the initial scores, weaving them together with hundreds of other signals to deeply understand relevance and quality. It weighs all this information in a fraction of a second to choreograph the final ranking, ensuring the most helpful and trustworthy information is presented to the user.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"h9kvr\">Evaluating S2R</h2><p data-block-key=\"9ef7v\">We evaluated the S2R system described above on the SVQ dataset:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval7_Results.width-1250.png\" alt=\"SpeechToRetrieval7_Results\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval7_Results.width-1250.png\" alt=\"SpeechToRetrieval7_Results\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"iex25\"><i>MRR of current real-world (â€œCascade ASRâ€; blue) models vs ground truth (i.e., perfect; â€œCascade Groundtruthâ€; green) and the S2R model's performance (\"S2R\" orange bar).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"h9kvr\">The S2R model's performance (orange bar) shows two key results:</p><ul><li data-block-key=\"2hrqi\">It significantly outperforms the baseline cascade ASR model.</li><li data-block-key=\"elct4\">Its performance approaches the upper bound established by the cascade ground truth model.</li></ul><p data-block-key=\"a341o\">While promising, the remaining gap indicates that further research is required.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"h9kvr\">The new era for voice search is now live</h2><p data-block-key=\"bmjeq\">The move to S2R-powered voice search isnâ€™t a theoretical exercise; itâ€™s a live reality. In a close collaboration between Google Research and Search, these advanced models are now serving users in multiple languages, delivering a significant leap in accuracy beyond conventional cascade systems.</p><p data-block-key=\"cr6fk\">To help propel the entire field forward, we are also open-sourcing the <a href=\"https://huggingface.co/datasets/google/svq\" target=\"_blank\" rel=\"noopener noreferrer\">SVQ dataset</a> as part of the <a href=\"https://github.com/google-research/mseb\" target=\"_blank\" rel=\"noopener noreferrer\">Massive Sound Embedding Benchmark</a> (MSEB). We believe shared resources and transparent evaluation accelerates progress. In that spirit, we invite the global research community to use this data, test new approaches on public benchmarks, and join the effort to build the next generation of truly intelligent voice interfaces.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"h9kvr\">Acknowledgements</h2><p data-block-key=\"9o630\"><i>The authors sincerely thank all who contributed to this project, whose critical input made it possible. We are especially grateful to our colleagues Hawi Abraham, Cyril Allauzen, Tom Bagby, Karthik Kumar Bandi, Stefan Buettcher, Dave Dopson, Lucy Hadden, Georg Heigold, Sanjit Jhala, Shankar Kumar, Ji Ma, Eyal Mizrachi, Pandu Nayak, Pew Putthividhya, David Rybach, Jungshik Shin, Venkat Subramanian, Sundeep Tirumalareddy and Trystan Upstill. We also wish to acknowledge those who helped prepare this post: Mark Simborg for his extensive editing, Kimberly Schwede for the wonderful illustrations, and Mickey Wurts for his valuable assistance.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "å›¾åƒç”Ÿæˆä¸­çš„åä½œæ–¹æ³• (åŸæ ‡é¢˜: A collaborative approach to image generation)",
      "link": "https://research.google/blog/a-collaborative-approach-to-image-generation/",
      "pubDate": "Wed, 01 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-01T16:00:00.000Z",
      "creator": "Google",
      "summary": "åœ¨æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹æ—¥ç›Šå¼ºå¤§çš„ä»Šå¤©ï¼Œç”¨æˆ·ä»å¸¸é¢ä¸´ç”Ÿæˆç»“æœä¸å¿ƒä¸­æ‰€æƒ³ä¸ç¬¦çš„å›°å¢ƒï¼Œä¸”éš¾ä»¥é€šè¿‡åå¤ä¿®æ”¹æç¤ºè¯æ¥å¼¥åˆå·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶äººå‘˜æå‡ºäº† **PASTAï¼ˆåå¥½è‡ªé€‚åº”å’Œåºåˆ—æ–‡æœ¬åˆ°å›¾åƒä»£ç†ï¼‰**ï¼Œè¿™æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†ï¼Œæ—¨åœ¨é€šè¿‡ä¸ç”¨æˆ·åä½œï¼Œé€æ­¥å®Œå–„T2Iç”Ÿæˆç»“æœï¼Œä»è€Œæ¶ˆé™¤ç”¨æˆ·å¯¹è¯•é”™å¼æç¤ºè¯ä¼˜åŒ–çš„ä¾èµ–ã€‚\n\n### PASTA çš„æ ¸å¿ƒç†å¿µä¸ä¼˜åŠ¿\n\n*   **åä½œå¼å¯¹è¯**ï¼šå°†å›¾åƒç”Ÿæˆè¿‡ç¨‹è½¬å˜ä¸ºä¸ç”¨æˆ·çš„åä½œå¼å¯¹è¯ï¼Œè€Œéå•å‘çš„æç¤ºè¯è¾“å…¥ã€‚\n*   **æ¶ˆé™¤è¯•é”™**ï¼šç”¨æˆ·æ— éœ€åå¤ä¿®æ”¹æç¤ºè¯ï¼ŒPASTAä¼šæ ¹æ®ç”¨æˆ·åé¦ˆé€æ­¥è°ƒæ•´ã€‚\n*   **ç”¨æˆ·æ»¡æ„åº¦é«˜**ï¼šé€šè¿‡äººç±»è¯„ä¼°ï¼ŒPASTAç”Ÿæˆçš„å›¾åƒè¢«ç”¨æˆ·è¯„ä»·ä¸ºæ›´ä»¤äººæ»¡æ„ã€‚\n*   **æ•°æ®é©±åŠ¨**ï¼šåˆ›å»ºå¹¶å‘å¸ƒäº†ä¸€ä¸ªåŒ…å«7000å¤šæ¬¡äººç±»äº¤äº’çš„åºåˆ—åå¥½æ•°æ®é›†ã€‚\n\n### PASTA çš„å·¥ä½œåŸç†\n\nä¸ºäº†è®­ç»ƒPASTAæœ‰æ•ˆé€‚åº”ç”¨æˆ·çš„ä¸ªæ€§åŒ–åå¥½ï¼Œéœ€è¦å¤§é‡å¤šæ ·çš„äº¤äº’æ•°æ®ã€‚é‰´äºè·å–çœŸå®ç”¨æˆ·æ•°æ®çš„æŒ‘æˆ˜ï¼ˆå¦‚éšç§é—®é¢˜ï¼‰ï¼ŒPASTAé‡‡ç”¨äº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼š\n\n1.  **é«˜è´¨é‡åŸºç¡€æ•°æ®é›†æ”¶é›†**ï¼š\n    *   æ”¶é›†äº†è¶…è¿‡7000åè¯„ä¼°è€…ä¸PASTAçš„åºåˆ—äº¤äº’æ•°æ®ã€‚\n    *   è¿™äº›äº¤äº’åŒ…æ‹¬ç”±Gemini Flashå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ç”Ÿæˆçš„æç¤ºè¯æ‰©å±•ï¼Œä»¥åŠç”±Stable Diffusion XL (SDXL) T2Iæ¨¡å‹ç”Ÿæˆçš„ç›¸åº”å›¾åƒã€‚\n    *   è¿™äº›çœŸå®çš„åå¥½æ•°æ®è¢«ç”¨äºè®­ç»ƒç”¨æˆ·æ¨¡æ‹Ÿå™¨ã€‚\n\n2.  **ç”¨æˆ·æ¨¡æ‹Ÿå™¨è®­ç»ƒ**ï¼š\n    *   **ç”¨æˆ·æ¨¡å‹**ï¼šåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼š\n        *   **æ•ˆç”¨æ¨¡å‹**ï¼šé¢„æµ‹ç”¨æˆ·å¯¹ä»»ä½•å›¾åƒé›†çš„å–œå¥½ç¨‹åº¦ã€‚\n        *   **é€‰æ‹©æ¨¡å‹**ï¼šé¢„æµ‹ç”¨æˆ·åœ¨é¢å¯¹å¤šä¸ªå›¾åƒé›†æ—¶ä¼šé€‰æ‹©å“ªä¸€ä¸ªã€‚\n    *   **æ¨¡å‹æ„å»º**ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„CLIPç¼–ç å™¨ï¼Œå¹¶æ·»åŠ ç”¨æˆ·ç‰¹å®šç»„ä»¶ã€‚\n    *   **è®­ç»ƒç®—æ³•**ï¼šé‡‡ç”¨æœŸæœ›æœ€å¤§åŒ–ç®—æ³•ï¼ŒåŒæ—¶å­¦ä¹ ç”¨æˆ·åå¥½ç»†èŠ‚å¹¶å‘ç°æ½œåœ¨çš„â€œç”¨æˆ·ç±»å‹â€ï¼ˆå³å…·æœ‰ç›¸ä¼¼å“å‘³çš„é›†ç¾¤ï¼Œå¦‚åçˆ±åŠ¨ç‰©ã€é£æ™¯æˆ–æŠ½è±¡è‰ºæœ¯ï¼‰ã€‚\n    *   **æ•°æ®ç”Ÿæˆ**ï¼šè®­ç»ƒåçš„ç”¨æˆ·æ¨¡æ‹Ÿå™¨èƒ½å¤Ÿå¯¹ç”Ÿæˆçš„å›¾åƒæä¾›åé¦ˆå¹¶è¡¨è¾¾åå¥½ï¼Œä»è€Œç”Ÿæˆè¶…è¿‡30,000æ¡æ¨¡æ‹Ÿäº¤äº’è½¨è¿¹ï¼Œä¸ºPASTAä»£ç†çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæä¾›äº†å¤§è§„æ¨¡å—æ§ç¯å¢ƒã€‚\n\n    ![PASTA-1](https://storage.googleapis.com/gweb-research2023-media/images/PASTA-1.width-1250.png)\n    *å›¾ç¤ºï¼šç”¨æˆ·æ¨¡æ‹Ÿå™¨ä»åå¥½æ•°æ®ä¸­è¯†åˆ«å‡ºä¸åŒçš„ç”¨æˆ·ç±»å‹ã€‚æ¯ä¸€è¡Œå±•ç¤ºäº†æ–°å…´ç”¨æˆ·æ¡£æ¡ˆçš„é¡¶çº§å›¾åƒï¼Œæ­ç¤ºäº†å¯¹â€œåŠ¨ç‰©â€æˆ–â€œé£Ÿç‰©â€ç­‰ç±»åˆ«çš„æ¸…æ™°åå¥½ã€‚*\n\n### PASTA ä»£ç†çš„äº¤äº’æµç¨‹\n\nä¸€æ—¦PASTAè®­ç»ƒå¹¶éƒ¨ç½²ï¼Œç”¨æˆ·ä¸PASTAçš„äº¤äº’è¿‡ç¨‹å¦‚ä¸‹ï¼š\n\n1.  **åˆå§‹æç¤º**ï¼šç”¨æˆ·è¾“å…¥ä¸€ä¸ªåˆå§‹æç¤ºè¯ã€‚\n2.  **å€™é€‰ç”Ÿæˆ**ï¼šPASTAä½¿ç”¨å€™é€‰ç”Ÿæˆå™¨ï¼ˆä¸€ä¸ªå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼‰åˆ›å»ºä¸€ç»„å¤šæ ·åŒ–çš„æ½œåœ¨æç¤ºè¯æ‰©å±•ã€‚\n3.  **å€™é€‰é€‰æ‹©**ï¼šè®­ç»ƒæœ‰ç´ çš„RLä»£ç†ï¼ˆå€™é€‰é€‰æ‹©å™¨ï¼‰é€‰æ‹©æœ€ä½³çš„å››ç»„æ‰©å±•å»ºè®®ã€‚\n4.  **å›¾åƒç”Ÿæˆä¸å±•ç¤º**ï¼šæ ¹æ®é€‰å®šçš„æ‰©å±•ç”Ÿæˆç›¸åº”å›¾åƒï¼Œå¹¶å±•ç¤ºç»™ç”¨æˆ·ã€‚\n5.  **ç”¨æˆ·åé¦ˆ**ï¼šç”¨æˆ·é€‰æ‹©æœ€ç¬¦åˆå…¶æ„¿æ™¯çš„å›¾åƒï¼Œæ­¤é€‰æ‹©ä½œä¸ºåé¦ˆæŒ‡å¯¼PASTAçš„ä¸‹ä¸€ç»„å»ºè®®ã€‚\n6.  **è¿­ä»£ä¼˜åŒ–**ï¼šè¿™ç§åä½œå¼çš„æ¥å›äº¤äº’ä½¿æ¨¡å‹èƒ½å¤Ÿå³æ—¶å­¦ä¹ ç”¨æˆ·çš„åå¥½ï¼Œé€æ­¥å¼•å¯¼åˆ›ä½œè¿‡ç¨‹è¾¾åˆ°ç”¨æˆ·çš„ç†æƒ³ç›®æ ‡ã€‚\n\n### æ€§èƒ½è¯„ä¼°ä¸ç»“æœ\n\nç ”ç©¶äººå‘˜é€šè¿‡éšå¼Qå­¦ä¹ ï¼ˆIQLï¼‰è®­ç»ƒäº†PASTAï¼Œå¹¶è¯„ä¼°äº†ä¸åŒè®­ç»ƒæ•°æ®å¯¹æ€§èƒ½çš„å½±å“ã€‚ä»–ä»¬åˆ›å»ºäº†ä¸‰ä¸ªç‰ˆæœ¬çš„ä»£ç†ï¼šä»…ä½¿ç”¨çœŸå®å¿—æ„¿è€…æ•°æ®è®­ç»ƒã€ä»…ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è®­ç»ƒã€ä»¥åŠç»“åˆçœŸå®å’Œæ¨¡æ‹Ÿæ•°æ®é›†è®­ç»ƒã€‚è¿™äº›ä»£ç†ä¸ä¸€ä¸ªåŸºçº¿æ¨¡å‹ï¼ˆæœªç»è¿‡é¢å¤–è®­ç»ƒçš„åŸºç¡€Gemini Flashå’ŒSDXLæ¨¡å‹ï¼‰è¿›è¡Œäº†æ¯”è¾ƒï¼Œè¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬Pick-a-Picå‡†ç¡®æ€§ã€Spearmanç§©ç›¸å…³ã€é€‰æ‹©æ¨¡å‹å‡†ç¡®æ€§å’Œè·¨è½®æ¬¡å‡†ç¡®æ€§ã€‚\n\n**ä¸»è¦å‘ç°**ï¼š\n\n*   ä»…ä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒçš„PASTAæœªèƒ½è¶…è¶ŠåŸºçº¿æ¨¡å‹ã€‚\n*   ä»…ä½¿ç”¨çœŸå®äººç±»æ•°æ®è®­ç»ƒçš„PASTAè¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼Œä½†ä»æœªè¶…è¶ŠåŸºçº¿ã€‚\n*   **ç»“åˆçœŸå®å’Œæ¨¡æ‹Ÿæ•°æ®è®­ç»ƒçš„PASTAè¡¨ç°æœ€ä½³**ï¼Œè¿™è¯å®äº†ç”¨æˆ·æ¨¡æ‹Ÿå™¨æˆåŠŸæ•æ‰äº†äººç±»äº¤äº’çš„å…³é”®åŠ¨æ€ï¼Œå¹¶æä¾›äº†å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ‰€éœ€çš„è§„æ¨¡ã€‚\n*   åœ¨ç›´æ¥æ¯”è¾ƒä¸­ï¼Œ**85%çš„è¯„ä¼°è€…æ›´å–œæ¬¢PASTAç”Ÿæˆçš„æœ€ç»ˆå›¾åƒ**ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æŠ½è±¡æç¤ºè¯æ—¶ï¼ˆå¦‚â€œçˆ±çš„å›¾åƒâ€ã€â€œå¹¸ç¦çš„å›¾åƒâ€ï¼‰ï¼ŒPASTAèƒ½æ ¹æ®ä¸åŒç”¨æˆ·ç±»å‹ç”Ÿæˆæˆªç„¶ä¸åŒçš„ã€é«˜åº¦ä¸ªæ€§åŒ–çš„ç»“æœã€‚\n\n    ![PASTA-3](https://storage.googleapis.com/gweb-research2023-media/images/PASTA-3.width-1250.png)\n    *å›¾ç¤ºï¼šè®­ç»ƒåçš„ç”¨æˆ·æ¨¡å‹å‡†ç¡®æ€§æ€§èƒ½ï¼ˆyè½´ï¼‰ä¸è€ƒè™‘çš„ç”¨æˆ·ç±»å‹æ•°é‡ï¼ˆxè½´ï¼‰çš„å…³ç³»ã€‚ä¸Šæ’æ˜¾ç¤ºæ¨¡å‹åœ¨Pick-a-Picæµ‹è¯•é›†ä¸Šçš„å‡†ç¡®æ€§ï¼ˆå·¦ï¼‰å’Œåœ¨HPSæµ‹è¯•é›†ä¸Šçš„Spearmanç§©ç›¸å…³ï¼ˆå³ï¼‰ã€‚ä¸‹æ’æ˜¾ç¤ºæ¨¡å‹çš„é€‰æ‹©å‡†ç¡®æ€§ï¼ˆå·¦ï¼‰å’Œè·¨è½®æ¬¡åå¥½å‡†ç¡®æ€§ï¼ˆå³ï¼‰ï¼Œå‡åœ¨äººç±»è¯„ä¼°æµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚*\n\n    ![PASTA-4](https://storage.googleapis.com/gweb-research2023-media/images/PASTA-4.width-1250.png)\n    *å›¾ç¤ºï¼šä»ç›¸åŒçš„èµ·å§‹æç¤ºâ€œå¹¸ç¦çš„å›¾åƒâ€å¼€å§‹ï¼ŒPASTAä¸ºä¸¤ç§ä¸åŒçš„ç”¨æˆ·ç±»å‹ï¼ˆç”¨æˆ·ç±»å‹Aå’Œç”¨æˆ·ç±»å‹Bï¼‰ç”Ÿæˆäº†æˆªç„¶ä¸åŒçš„ç»“æœï¼Œå±•ç¤ºäº†å…¶é€‚åº”ä¸ªä½“ç‹¬ç‰¹åˆ›æ„é£æ ¼çš„èƒ½åŠ›ã€‚*\n\n### æœªæ¥å±•æœ›\n\nPASTAçš„ç ”ç©¶è¡¨æ˜ï¼Œç”Ÿæˆå¼AIçš„æœªæ¥å°†æ›´åŠ äº’åŠ¨ã€é€‚åº”åå¥½å’Œåä½œã€‚æ‰€å¼€å‘çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å¼ºå¤§çš„ç”¨æˆ·æ¨¡æ‹Ÿå™¨ï¼Œå¯åº”ç”¨äºè®¸å¤šå…¶ä»–ç”Ÿæˆä»»åŠ¡ï¼Œä»¥åˆ›å»ºæ›´ç¬¦åˆå’Œé€‚åº”äººç±»ç”¨æˆ·çš„AIã€‚ä¸ºäº†ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ï¼Œè¯¥å›¢é˜Ÿå·²å¼€æºäº†å…¶åºåˆ—è¯„ä¼°è€…æ•°æ®é›†å’Œæ¨¡æ‹Ÿç”¨æˆ·æ•°æ®ã€‚",
      "shortSummary": "PASTAï¼ˆåå¥½è‡ªé€‚åº”å’Œåºåˆ—æ–‡æœ¬åˆ°å›¾åƒä»£ç†ï¼‰æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ ä»£ç†ï¼Œæ—¨åœ¨é€šè¿‡åä½œå¯¹è¯è§£å†³æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹éš¾ä»¥æ•æ‰ç”¨æˆ·ç»†å¾®åˆ›æ„æ„å›¾çš„é—®é¢˜ã€‚å®ƒé€šè¿‡ç»“åˆçœŸå®äººç±»åé¦ˆå’Œå¤§è§„æ¨¡ç”¨æˆ·æ¨¡æ‹Ÿè¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ å¹¶é€‚åº”ç”¨æˆ·çš„ç‹¬ç‰¹åå¥½ã€‚PASTAèƒ½å¤Ÿé€æ­¥ä¼˜åŒ–å›¾åƒç”Ÿæˆç»“æœï¼Œç”¨æˆ·è¯„ä¼°æ˜¾ç¤ºå…¶ç”Ÿæˆçš„å›¾åƒæ¯”åŸºçº¿æ¨¡å‹æ›´ä»¤äººæ»¡æ„ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œç”Ÿæˆå¼AIçš„æœªæ¥å°†æ›´åŠ äº’åŠ¨ã€é€‚åº”åå¥½å’Œåä½œã€‚",
      "translated_title": "å›¾åƒç”Ÿæˆä¸­çš„åä½œæ–¹æ³•",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PASTA-1.width-1250.png",
          "alt": "PASTA-1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PASTA-3.width-1250.png",
          "alt": "PASTA-3",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PASTA-4.width-1250.png",
          "alt": "PASTA-4",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"cmj9x\">You have a perfect image in your mind. You enter a prompt, hit generate, and the result is close to what you were thinking, but not quite right. You try refining the prompt, adding more detail, but you can't seem to bridge the gap between your idea and the final image.</p><p data-block-key=\"bkhg8\">This is a common experience. While <a href=\"https://en.wikipedia.org/wiki/Text-to-image_model\" target=\"_blank\" rel=\"noopener noreferrer\">text-to-image</a> (T2I) models are incredibly powerful, they often struggle to capture the nuance and specificity of an individual's unique creative intent given just a single prompt. What if we could turn image generation into a collaborative conversation?</p><p data-block-key=\"34fn3\">In this post, we describe our research â€œ<a href=\"https://arxiv.org/abs/2412.10419\" target=\"_blank\" rel=\"noopener noreferrer\">Preference Adaptive and Sequential Text-to-image Agent</a>â€ (PASTA), a reinforcement learning (RL) agent that collaborates with users to progressively refine T2I results. This approach eliminates the need for users to rely on trial-and-error prompt refinement to reach a desirable image. Through human evaluations, we created a novel dataset of sequential preferences, which we then used to compare PASTA with a baseline state-of-the-art model. The results demonstrated that PASTA, trained with our mix of real and simulated data, consistently produced images that users rated as more satisfying. Weâ€™ve also released our <a href=\"https://www.kaggle.com/datasets/googleai/pasta-data\" target=\"_blank\" rel=\"noopener noreferrer\">foundational dataset</a> with a collection of over 7,000 human rater interactions with PASTA.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How PASTA works</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"cmj9x\">To effectively train an AI agent to adapt to a user's individual preferences, a large, diverse set of interaction data is needed. However, gathering this data from real users is challenging due to several factors, including user privacy. To address this, we trained PASTA using a two-stage strategy that combines real human feedback with large-scale user simulation.</p><p data-block-key=\"cnmkf\">First, we collected a <a href=\"https://www.kaggle.com/datasets/googleai/pasta-data\" target=\"_blank\" rel=\"noopener noreferrer\">high-quality foundational dataset</a> with over 7,000 raters' sequential interactions. These interactions included prompt expansions generated by a <a href=\"https://arxiv.org/pdf/2403.05530\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini Flash</a> large multimodal model and corresponding images generated by a <a href=\"https://arxiv.org/pdf/2307.01952\" target=\"_blank\" rel=\"noopener noreferrer\">Stable Diffusion XL</a> (SDXL) T2I model. This initial seed of authentic preference data was then used to train a user simulator, designed to generate additional data that replicate real human choices and preferences.</p><p data-block-key=\"f45gr\">At the heart of our method is a user model, comprising two key components: 1) a utility model that predicts the degree to which a user will like any set of images, and 2) a choice model that predicts which set of images they will select when presented with several sets. We constructed the user model using pre-trained <a href=\"https://arxiv.org/abs/2103.00020\" target=\"_blank\" rel=\"noopener noreferrer\">CLIP encoders</a> and added user-specific components. We trained the model using an <a href=\"https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm\" target=\"_blank\" rel=\"noopener noreferrer\">expectation-maximization</a> algorithm that allows us to simultaneously learn the specifics of user preferences while also discovering latent â€œuser types,â€ that is, clusters of users with similar tastes (e.g., tendencies to prefer images with animals, scenic views, or abstract art).</p><p data-block-key=\"bpgbn\">The trained user simulator can provide feedback and express preferences on generated images, and make selections from sets of proposed images. This allows us to generate over 30,000 simulated interaction trajectories.. Our approach does more than just create more data; it gives us a controlled environment in which to explore a vast range of user behaviors so we can train the PASTA agent to effectively collaborate with users.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PASTA-1.width-1250.png\" alt=\"PASTA-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PASTA-1.width-1250.png\" alt=\"PASTA-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"00pr5\"><i>Our user simulator learns to identify distinct user types from preference data. Each row shows the top-rated images for an emergent user profile, revealing clear preferences for categories like \"Animals\" or \"Food.\"</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"cmj9x\">With this robust, data-driven foundation, the PASTA agent is trained to effectively engage with arbitrary users to generate images that match their preferences. The agent itself is a <a href=\"https://arxiv.org/abs/1312.5602\" target=\"_blank\" rel=\"noopener noreferrer\">value-based reinforcement learning</a> model that learns to select the best \"slate\" of prompt expansions (i.e., elaborations of the current prompt used to generate subsequent images) to show the user at each turn. Its goal is to maximize the user's cumulative satisfaction over the entire interaction.</p><p data-block-key=\"da0lr\">Once PASTA is trained and deployed, a user initiates the engagement with an initial prompt. PASTA first uses a candidate generator (a large multimodal model) to create a diverse set of potential prompt expansions. Then, a candidate selector (our trained RL agent) selects the optimal slate of four such expansions, which are used to generate corresponding images to present to the user. The user selects the image that is closest to their vision, which provides feedback that guides PASTA's next set of suggestions. This collaborative back-and-forth allows the model to learn the user's preferences on the fly, steering the creative process toward their ideal goal with each step.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/PASTA-mp4.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"00pr5\"><i>Starting with a simple prompt for \"A white cat\", PASTA engages the user in a visually grounded dialogue. The user's selections (highlighted in blue) help the agent quickly learn their preference for a more fantastical and colorful style.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Putting PASTA to the test</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"cmj9x\">To evaluate our approach, we trained PASTA as a value-based reinforcement learning agent using <a href=\"https://arxiv.org/abs/2110.06169\" target=\"_blank\" rel=\"noopener noreferrer\">implicit Q-learning</a> (IQL). We specifically wanted to see how the use of different training data impacted performance. We created three versions of the agent: 1) trained only on the real volunteer-rater data, 2) trained only on the simulated data, and 3) trained on a combination of real and simulated datasets.</p><p data-block-key=\"p6im\">We then ran a series of human evaluations comparing these agents to a baseline model (i.e., base Gemini Flash and SDXL models with no additional training) across four metrics: accuracy over the <a href=\"https://proceedings.neurips.cc/paper_files/paper/2023/file/73aacd8b3b05b4b503d58310b523553c-Paper-Conference.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Pick-a-Pic</a> dataset, <a href=\"https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient\" target=\"_blank\" rel=\"noopener noreferrer\">Spearmanâ€™s rank correlation</a>, choice model accuracy, and cross turn accuracy. Pick-a-Pic accuracy and Spearman's rank correlation assess the model's ability to predict user preferences and rankings on existing, large-scale, single-turn datasets. Choice model accuracy and cross-turn accuracy measure the model's ability to predict which image a user will choose at a given turn and whether the selected images are an improvement over the previous turn, respectively.</p><p data-block-key=\"cp9o4\">The results demonstrated that training PASTA on synthetic data alone didn't beat the baseline and while the agent trained on real human data showed significant improvement, it also didnâ€™t outperform the baseline. However, the agent trained on the combination of both real and simulated data offered the best performance, confirming that our user simulation successfully captures key dynamics of human interaction while providing the scale needed for robust RL training.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PASTA-3.width-1250.png\" alt=\"PASTA-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PASTA-3.width-1250.png\" alt=\"PASTA-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"00pr5\"><i>The graphs above present the accuracy performance of a trained user model (y axis) as a function of the number of user types considered (x axis). The top row displays the modelâ€™s accuracy on the Pick-a-Pic test set (</i><b><i>left</i></b><i>) and its Spearmanâ€™s rank correlation on the</i> <a href=\"https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Human_Preference_Score_Better_Aligning_Text-to-Image_Models_with_Human_Preference_ICCV_2023_paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\"><i>HPS test</i></a><i> set (</i><b><i>right</i></b><i>). The bottom row shows the modelâ€™s choice accuracy (</i><b><i>left</i></b><i>) and cross-turn preference accuracy (</i><b><i>right</i></b><i>), both evaluated on our human-rated test set</i>.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"cmj9x\">When we asked raters to directly compare the final images from our best-performing agent against the baseline, 85% preferred PASTA's generated images. The difference is especially striking with abstract prompts. Starting with a simple idea like \"an image of love\", PASTA adapted to different user types to create a wide variety of results, from tender portraits to abstract, geometric art.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PASTA-4.width-1250.png\" alt=\"PASTA-4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PASTA-4.width-1250.png\" alt=\"PASTA-4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"00pr5\"><i>With the same starting prompt, \"An image of happiness\", PASTA produces dramatically different results for two distinct user types (User Type A and User Type B), showcasing its ability to adapt to an individual's unique creative style. For example, the result for Type A corresponds to a prompt like â€œAbstract happy faces, Art Deco inspired geometric shapes, muted jewel-toned background.â€</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">What's next?</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"cmj9x\">PASTA shows that the future of generative AI can be more interactive, preference adaptive, and collaborative. The methods we developed, particularly the use of robust user simulators, can be applied to many other generative tasks to create AI that better aligns and adapts to human users.</p><p data-block-key=\"1ads2\">To help spur further research, we have <a href=\"https://www.kaggle.com/datasets/googleai/pasta-data\" target=\"_blank\" rel=\"noopener noreferrer\">open-sourced</a> our sequential rater dataset and our simulated user data. We can't wait to see what the community builds with it.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"cmj9x\"><i>The author list is:</i> <i>Ofir Nabati,</i> <i>Guy Tennenholtz,</i> <i>ChihWei Hsu,</i> <i>Moonkyung Ryu,</i> <i>Deepak Ramachandran,</i> <i>Yinlam Chow,</i> <i>Xiang Li, and</i> <i>Craig Boutilier. Special thanks to Mark Simborg for his help crafting this blog post and Kimberly Schwede for creating the figures in this post.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Snapseed æ¨å‡ºäº¤äº’å¼è®¾å¤‡ç«¯åˆ†å‰²åŠŸèƒ½ (åŸæ ‡é¢˜: Introducing interactive on-device segmentation in Snapseed)",
      "link": "https://research.google/blog/introducing-interactive-on-device-segmentation-in-snapseed/",
      "pubDate": "Tue, 30 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-30T16:00:00.000Z",
      "creator": "Google",
      "summary": "Snapseed å¼•å…¥äº†åä¸ºâ€œå¯¹è±¡ç”»ç¬”â€ï¼ˆObject Brushï¼‰çš„æ–°åŠŸèƒ½ï¼Œæ—¨åœ¨ç®€åŒ–ç§»åŠ¨è®¾å¤‡ä¸Šçš„é€‰æ‹©æ€§å›¾åƒè°ƒæ•´ã€‚è¿™é¡¹åŠŸèƒ½é€šè¿‡å¼ºå¤§çš„è®¾å¤‡ç«¯äººå·¥æ™ºèƒ½æ¨¡å‹â€œäº¤äº’å¼åˆ†å‰²å™¨â€ï¼ˆInteractive Segmenterï¼‰å®ç°ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡ç®€å•çš„ç¬”è§¦æˆ–ç‚¹å‡»ï¼Œå¿«é€Ÿå‡†ç¡®åœ°é€‰æ‹©å¹¶ç¼–è¾‘ç…§ç‰‡ä¸­çš„ç‰¹å®šå¯¹è±¡ã€‚\n\n### æ ¸å¿ƒåŠŸèƒ½ä¸ç”¨æˆ·ä½“éªŒ\n\n*   **ç›´è§‚çš„å¯¹è±¡ç¼–è¾‘**ï¼šç”¨æˆ·åªéœ€åœ¨æƒ³è¦ç¼–è¾‘çš„å¯¹è±¡ä¸Šç»˜åˆ¶ä¸€ç¬”ï¼Œæ¨¡å‹ä¾¿ä¼šåœ¨20æ¯«ç§’å†…ç«‹å³æ£€æµ‹å¹¶é€‰æ‹©å®Œæ•´çš„å¯¹è±¡ï¼Œç”Ÿæˆç²¾ç¡®åŒ¹é…å…¶è¾¹ç•Œçš„æ©è†œã€‚è¿™ä½¿å¾—åœ¨ä¸å½±å“å›¾åƒå…¶ä»–éƒ¨åˆ†çš„æƒ…å†µä¸‹ï¼Œå¯¹å‰æ™¯ä¸»ä½“ã€å¤©ç©ºæˆ–ç‰¹å®šç‰©å“è¿›è¡Œäº®åº¦ã€é¢œè‰²ç­‰è°ƒæ•´å˜å¾—è½»è€Œæ˜“ä¸¾ã€‚\n*   **å®æ—¶åé¦ˆ**ï¼šæ¨¡å‹æä¾›å®æ—¶åé¦ˆï¼Œç”¨æˆ·å¯ä»¥å³æ—¶æ·»åŠ æˆ–åˆ é™¤é€‰åŒºï¼Œç›´åˆ°è¾¾åˆ°ç†æƒ³æ•ˆæœã€‚\n*   **è®¾å¤‡ç«¯è¿è¡Œ**ï¼šæ•´ä¸ªè¿‡ç¨‹å®Œå…¨åœ¨è®¾å¤‡ä¸Šè¿è¡Œï¼Œç”± MediaPipe å’Œ LiteRT çš„ GPU åŠ é€Ÿæä¾›æ”¯æŒï¼Œç¡®ä¿äº†å¿«é€Ÿæµç•…çš„ä½“éªŒã€‚\n\n### äº¤äº’å¼åˆ†å‰²å™¨æ¨¡å‹çš„æŠ€æœ¯åŸç†\n\nâ€œäº¤äº’å¼åˆ†å‰²å™¨â€æ—¨åœ¨æˆä¸ºä¸€ä¸ªé€šç”¨åˆ†å‰²æ¨¡å‹ï¼Œä¸é™äºç‰¹å®šç±»åˆ«çš„å¯¹è±¡æˆ–åœºæ™¯ã€‚å…¶å¼€å‘è¿‡ç¨‹æ¶‰åŠä»¥ä¸‹å…³é”®æ­¥éª¤ï¼š\n\n1.  **å¸ˆç”Ÿæ¨¡å‹è®­ç»ƒï¼ˆTeacher-Student Trainingï¼‰**ï¼š\n    *   **â€œäº¤äº’å¼åˆ†å‰²å™¨ï¼šæ•™å¸ˆâ€ï¼ˆInteractive Segmenter: Teacherï¼‰**ï¼šé¦–å…ˆï¼Œå›¢é˜Ÿä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒä¸”é«˜åº¦æ³›åŒ–çš„æ¨¡å‹ï¼Œé€šè¿‡å¯¹350å¤šç§ä¸åŒå¯¹è±¡ç±»åˆ«çš„çº¦30,000ä¸ªé«˜è´¨é‡åƒç´ çº§å›¾åƒæ©è†œè¿›è¡Œå¾®è°ƒï¼Œè®­ç»ƒå‡ºä¸€ä¸ªé«˜ç²¾åº¦çš„äº¤äº’å¼åˆ†å‰²æ¨¡å‹ã€‚è¯¥æ¨¡å‹è´¨é‡é«˜ï¼Œä½†ç”±äºé€Ÿåº¦å’Œå¤§å°é™åˆ¶ï¼Œä¸é€‚åˆè®¾å¤‡ç«¯ä½¿ç”¨ã€‚\n    *   **â€œäº¤äº’å¼åˆ†å‰²å™¨ï¼šè¾¹ç¼˜â€ï¼ˆInteractive Segmenter: Edgeï¼‰**ï¼šä¸ºäº†å®ç°è®¾å¤‡ç«¯åº”ç”¨ï¼Œå›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªæ›´å°ã€æ›´ä¸“ä¸šçš„æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰ä»â€œæ•™å¸ˆâ€æ¨¡å‹ä¸­å­¦ä¹ ï¼Œåˆ©ç”¨åŒ…å«200å¤šä¸‡å¼ å›¾åƒå’Œæ•°ç™¾ä¸ªä¸åŒç±»åˆ«çš„å¼±æ ‡æ³¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚åœ¨è’¸é¦è¿‡ç¨‹ä¸­ï¼Œâ€œæ•™å¸ˆâ€æ¨¡å‹ä¼šå®æ—¶ç”Ÿæˆé«˜è´¨é‡çš„çœŸå€¼æ©è†œï¼Œå¹¶æ¨¡æ‹Ÿç”¨æˆ·æç¤ºï¼ˆå¦‚å‰æ™¯æ¶‚é¸¦ã€èƒŒæ™¯æ¶‚é¸¦ã€ç‚¹å’Œæ¡†é€‰ï¼‰ï¼Œä»¥è®­ç»ƒâ€œè¾¹ç¼˜â€æ¨¡å‹ã€‚\n\n    ![Table comparing Interactive Segmenter: Edge and Interactive Segmenter: Teacher models using the IOU metric.](https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-3-IOU.width-1250.png)\n    *   **æç¤ºç”Ÿæˆ**ï¼šä¸ºäº†æ¨¡æ‹Ÿç”¨æˆ·é€‰æ‹©å¯¹è±¡ï¼Œæ¨¡å‹åœ¨è®­ç»ƒæ—¶ä¼šåœ¨çœŸå€¼æ©è†œå†…éƒ¨ç»˜åˆ¶éšæœºæ¶‚é¸¦ä½œä¸ºå‰æ™¯æç¤ºï¼Œåœ¨å¤–éƒ¨ç»˜åˆ¶éšæœºæ¶‚é¸¦ä½œä¸ºèƒŒæ™¯æç¤ºï¼Œå¹¶æ¨¡æ‹Ÿç‚¹å‡»å’Œå¥—ç´¢é€‰æ‹©ã€‚\n\n    ![Schematic of teacherâ€“student training for Interactive Segmenter.](https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-4a-Training.width-1250.png)\n\n2.  **é«˜ç²¾åº¦ä¸ä½å»¶è¿Ÿçš„å¹³è¡¡**ï¼š\n    *   ä¸ºäº†åœ¨åˆ†å‰²è´¨é‡å’Œå®æ—¶äº¤äº’å»¶è¿Ÿä¹‹é—´å–å¾—å¹³è¡¡ï¼Œæ¨¡å‹å°†å›¾åƒç†è§£å’Œæç¤ºç†è§£è§£è€¦ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„å­æ¨¡å‹ã€‚\n    *   **é‡é‡çº§å›¾åƒç¼–ç å™¨**ï¼šå¯¹æ¯å¼ å›¾åƒåªè¿è¡Œä¸€æ¬¡ï¼Œæå–ä¸°å¯Œçš„è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶åœ¨ç”¨æˆ·å¼€å§‹ä½¿ç”¨äº¤äº’å¼åˆ†å‰²æ—¶ç«‹å³è¿è¡Œï¼Œæœ‰æ•ˆéšè—äº†å»¶è¿Ÿã€‚\n    *   **è½»é‡çº§äº¤äº’å¼ç¼–ç å™¨-è§£ç å™¨**ï¼šåœ¨æ­¤é¢„è®¡ç®—çš„ç‰¹å¾ä¸Šè¿è¡Œï¼Œæ¥æ”¶ç”¨æˆ·çš„è§¦æ‘¸æç¤ºå¹¶åœ¨20æ¯«ç§’é¢„ç®—å†…ç”Ÿæˆæœ€ç»ˆåˆ†å‰²æ©è†œã€‚\n\n    ![Schematic of the Interactive Segmenter neural network architecture.](https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-5-Architecture.width-1250.png)\n    ![Table showing model inference latency when running Interactive Segmenter: Edge on-device.](https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-6-Latency.width-1250.png)\n\n3.  **å›¾åƒå°ºå¯¸æ©è†œä¸Šé‡‡æ ·**ï¼š\n    *   ä¸ºåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒä¸Šä¿æŒæœ€ä½³ç¼–è¾‘è´¨é‡ï¼Œæ¨¡å‹é¢„æµ‹768x768åˆ†è¾¨ç‡çš„æ©è†œï¼Œå¹¶é€šè¿‡é«˜æ•ˆçš„ GPU å®ç°è¾¹ç¼˜ä¿ç•™è”åˆåŒè¾¹ä¸Šé‡‡æ ·æ–¹æ³•ï¼Œå°†å…¶ä¸Šé‡‡æ ·è‡³å›¾åƒåŸå§‹åˆ†è¾¨ç‡ï¼ˆæœ€é«˜4Kï¼‰ã€‚\n    *   ä¸ºæé«˜å»¶è¿Ÿï¼Œä¸Šé‡‡æ ·ä»…åœ¨ç”¨æˆ·å®Œæˆæ‰‹åŠ¿ï¼ˆæŠ¬èµ·æ‰‹æŒ‡ï¼‰ååº”ç”¨ã€‚\n\n    ![Comparison of original Interactive Segmenter mask and upsampled mask.](https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-7-Upsampling.width-1250.png)\n\n### ç»“è®ºä¸å±•æœ›\n\nâ€œäº¤äº’å¼åˆ†å‰²å™¨â€çš„æ¨å‡ºï¼Œä½¿ Snapseed çš„å›¾åƒç¼–è¾‘å˜å¾—å‰æ‰€æœªæœ‰çš„ç®€å•å’Œå¼ºå¤§ã€‚å®ƒå°†ç®€å•çš„ç‚¹å‡»å’Œç¬”è§¦è½¬åŒ–ä¸ºç²¾ç¡®çš„é€‰æ‹©ï¼Œå¸®åŠ©ç”¨æˆ·è½»æ¾å®ç°ç¼–è¾‘æƒ³æ³•ã€‚è¿™é¡¹åº•å±‚æŠ€æœ¯ä¸ä»…å·²åº”ç”¨äº Chromebook Plus 14 çš„å›¾åº“åº”ç”¨ä¸­çš„ AI å›¾åƒç¼–è¾‘åŠŸèƒ½ï¼ŒGoogle è¿˜è®¡åˆ’å°†å…¶é›†æˆåˆ°æ›´å¤šå›¾åƒå’Œåˆ›æ„ç¼–è¾‘äº§å“ä¸­ã€‚",
      "shortSummary": "Snapseed æ¨å‡ºâ€œå¯¹è±¡ç”»ç¬”â€åŠŸèƒ½ï¼Œé€šè¿‡è®¾å¤‡ç«¯ AI æ¨¡å‹â€œäº¤äº’å¼åˆ†å‰²å™¨â€å®ç°å¿«é€Ÿã€ç›´è§‚çš„å¯¹è±¡é€‰æ‹©å’Œç¼–è¾‘ã€‚ç”¨æˆ·åªéœ€ç®€å•ç¬”è§¦ï¼Œæ¨¡å‹ä¾¿èƒ½åœ¨20æ¯«ç§’å†…ç²¾ç¡®åˆ†å‰²å›¾åƒä¸­çš„å¯¹è±¡ã€‚è¯¥æŠ€æœ¯é‡‡ç”¨å¸ˆç”Ÿæ¨¡å‹è’¸é¦è®­ç»ƒï¼Œå¹³è¡¡äº†åˆ†å‰²è´¨é‡ä¸å®æ—¶æ€§ï¼Œå¹¶æ”¯æŒé«˜åˆ†è¾¨ç‡æ©è†œã€‚è¿™é¡¹åˆ›æ–°ä½¿é«˜çº§ç…§ç‰‡ç¼–è¾‘æ›´æ˜“ç”¨ï¼Œå¹¶è®¡åˆ’é›†æˆåˆ°æ›´å¤š Google å›¾åƒå’Œåˆ›æ„ç¼–è¾‘äº§å“ä¸­ï¼ŒåŒ…æ‹¬å·²åº”ç”¨äº Chromebook Plus 14ã€‚",
      "translated_title": "Snapseed æ¨å‡ºäº¤äº’å¼è®¾å¤‡ç«¯åˆ†å‰²åŠŸèƒ½",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-3-IOU.width-1250.png",
          "alt": "Table comparing Interactive Segmenter: Edge and Interactive Segmenter: Teacher models using the IOU metric.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-4a-Training.width-1250.png",
          "alt": "Schematic of teacherâ€“student training for Interactive Segmenter.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-5-Architecture.width-1250.png",
          "alt": "Schematic of the Interactive Segmenter neural network architecture.",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-6-Latency.width-1250.png",
          "alt": "Table showing model inference latency when running Interactive Segmenter: Edge on-device.",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-7-Upsampling.width-1250.png",
          "alt": "Comparison of original Interactive Segmenter mask and upsampled mask.",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wunwg\">The key to elevating a good photo often lies in selective image adjustments: brightening a subject in the foreground, enhancing the sky, or making the color of a jacket pop. Yet, isolating specific elements with existing tools that offer subject, background, sky, or color-based selections has remained a frustrating and complex endeavor. This challenge has been particularly acute on mobile devices, where imprecise touch input and limited processing have made detailed selections and edits very difficult.</p><p data-block-key=\"978qd\">Now, we have made object-based image adjustments quick and easy. The new Object Brush in <a href=\"https://apps.apple.com/us/app/snapseed-photo-editor/id439438619\" target=\"_blank\" rel=\"noopener noreferrer\">Snapseed</a> on iOS, accessible in the \"Adjust\" tool, now lets you edit objects intuitively. It allows you to simply draw a stroke on the object you want to edit and then adjust how you want it to look, separate from the rest of the image. Give it a try as we roll this new capability out in the coming week!</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Snapseed-1-ObjBrush.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Selective editing using Snapseed's Object Brush.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Intuitive editing through interactive on-device segmentation</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">At its core, Object Brush is powered by our Interactive Segmenter, a powerful AI model that runs entirely on device. With a simple gesture â€” just a tap or tracing a quick line â€” you can choose an object or person in the frame. The model will then immediately detect and select the complete object or person, in less than 20ms. The model generates a mask for the object, which accurately matches its boundaries, whether it's a person, a pet, or the clouds in the sky. This real-time feedback lets you refine your selection on the fly, easily adding or subtracting areas until it's just right. This entire process is powered by <a href=\"https://ai.google.dev/edge/mediapipe/framework\" target=\"_blank\" rel=\"noopener noreferrer\">MediaPipe</a> and <a href=\"https://ai.google.dev/edge/litert/performance/gpu\" target=\"_blank\" rel=\"noopener noreferrer\">LiteRTâ€™s GPU acceleration</a> for a fast and seamless experience.</p><p data-block-key=\"c4tan\">This powerful fusion of a simple, intuitive user interface with an effective and efficient machine learning model makes advanced photo editing more accessible, enjoyable, and more precise than ever before, all running seamlessly on your own device.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Snapseed-0-Hero.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Use foreground prompts (green) to select parts of an image and background prompts (red) to refine the selection.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Training the Interactive Segmenter model</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">The Interactive Segmenter model is designed to be a universally capable segmentation model, not limited to any specific class of objects or scenes. To avoid having to annotate large amounts of data to cover all areas, we chose to follow the <a href=\"https://arxiv.org/abs/1912.11370\" target=\"_blank\" rel=\"noopener noreferrer\">Big Transfer</a> approach and use a general pre-trained image encoder for pseudo-annotation to complement small amounts of manually annotated images.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Teacher for Interactive Segmenter</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">We started with a pre-trained and highly-generalizable model, fine-tuned for interactive segmentation. We took samples for 350+ different object categories and asked annotators to precisely annotate object masks with pixel-perfect quality. Through this process, we obtained ~30,000 high-quality image masks for these categories. While insufficient for direct training of a small mobile model, large pre-trained models can successfully be fine-tuned on this data to predict high accuracy masks. Using this dataset we trained an interactive segmentation model, which we call â€œInteractive Segmenter: Teacherâ€.</p><p data-block-key=\"2dclt\">Interactive Segmenter: Teacher produces high-quality segmentation masks; however, its speed and size hinder its use in on-device scenarios. To overcome this challenge, we developed â€œInteractive Segmenter: Edgeâ€, a specialized model tailored for on-device use cases by leveraging the knowledge distilled from the original Interactive Segmenter: Teacher model.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Distillation</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">Since the on-device model is significantly smaller, it has limited generalization capabilities, and the 30,000 annotated images we used for fine-tuning aren't sufficient to train a new model. At the same time the small model size implies we wonâ€™t see significant gains from pre-training on different domains or tasks.</p><p data-block-key=\"bqvot\">For knowledge transfer from Interactive Segmenter: Teacher to Interactive Segmenter: Edge, we need millions of images and realistic prompts for a diverse range of object categories. So, we leveraged a large, weakly annotated dataset, which contains over 2 million images with masks across hundreds of different categories.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-3-IOU.width-1250.png\" alt=\"Table comparing Interactive Segmenter: Edge and Interactive Segmenter: Teacher models using the IOU metric.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-3-IOU.width-1250.png\" alt=\"Table comparing Interactive Segmenter: Edge and Interactive Segmenter: Teacher models using the IOU metric.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Interactive Segmenter: Edge yields a similar quality as Interactive Segmenter: Teacher for a given, fixed input prompt, as measured by the intersection over union (IOU) metric.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Prompt generation</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">The segmentation masks in the distillation dataset are not pixel-perfect, because they were generated through <a href=\"https://arxiv.org/pdf/1903.10830\" target=\"_blank\" rel=\"noopener noreferrer\">automated or semi-automated procedures</a>, and are not ideal for training high-quality segmenters. Nevertheless, they are suitable for creating realistic prompts for interactive segmentation. In this process, the ground truth mask is produced on-the-fly by Interactive Segmenter: Teacher, which acts as a teacher model in a process known as <a href=\"https://arxiv.org/abs/2106.05237\" target=\"_blank\" rel=\"noopener noreferrer\">knowledge distillation</a>. Importantly, both the teacher as well as the student model use the same prompts during training, ensuring consistency across models.</p><p data-block-key=\"6276u\">We attempt to simulate a user selecting objects in an image. We draw random scribbles within the (eroded) ground truth mask to get foreground prompts (i.e., what the user wants to select, shown in red in the image below) and random scribbles outside the ground truth mask to get background prompts (i.e., what the user explicitly does not want to select, shown in blue). We simulate tapping by drawing random points as well as random scribbles. Furthermore, to support lasso selection we also expose the model during training to box prompts around an object.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-4a-Training.width-1250.png\" alt=\"Schematic of teacherâ€“student training for Interactive Segmenter.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-4a-Training.width-1250.png\" alt=\"Schematic of teacherâ€“student training for Interactive Segmenter.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">By utilizing a teacher model we can train on data with low-quality ground truth annotations, reducing labeling costs without sacrificing model quality.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">High quality vs. low latency</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">A central challenge was reconciling the conflicting demands of segmentation quality versus real-time, interactive latency. To reach the right balance, we decouple image and prompt understanding into distinct sub-models. First, a powerful, heavyweight image encoder is run once per image to extract a rich set of semantic features. This image encoder can be run as soon as the userâ€™s intent to use interactive segmentation becomes apparent, thus effectively hiding the latency from the user. Second, a lightweight interactive encoder-decoder operates on these pre-computed features. This network takes the user's touch prompts and generates the final segmentation mask, executing well under our 20ms budget. This separation into two models allows Interactive Segmenter to harness the image understanding of a large model while delivering the instantaneous responsiveness of a small one.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-5-Architecture.width-1250.png\" alt=\"Schematic of the Interactive Segmenter neural network architecture.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-5-Architecture.width-1250.png\" alt=\"Schematic of the Interactive Segmenter neural network architecture.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Interactive Segmenter neural network architecture.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-6-Latency.width-1250.png\" alt=\"Table showing model inference latency when running Interactive Segmenter: Edge on-device.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-6-Latency.width-1250.png\" alt=\"Table showing model inference latency when running Interactive Segmenter: Edge on-device.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Model inference latency when running Interactive Segmenter: Edge on-device.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wunwg\">The final student models (encoder + super decoder) are quantized to 8 bits and both run on <a href=\"https://ai.google.dev/edge/litert/performance/gpu\" target=\"_blank\" rel=\"noopener noreferrer\">LiteRT's GPU acceleration</a> with decoder inference latencies of 7.4ms on an iPhone 16 Pro, enabling seamless and intuitive image editing.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Image-size mask upsampling</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">To preserve the best image editing quality on high-resolution images, we need high-resolution segmentation masks. To achieve this, we train our segmentation model to predict a mask in 768x768 resolution and further upsample it to image resolution (capped at 4k to have it fit within a single GPU buffer). We use an efficient GPU implementation of the <a href=\"https://dl.acm.org/doi/10.1145/1276377.1276497\" target=\"_blank\" rel=\"noopener noreferrer\">edge-preserving joint-bilateral upsampling method</a>. To improve latency, we only apply upsampling once a user completes a gesture by lifting their finger.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-7-Upsampling.width-1250.png\" alt=\"Comparison of original Interactive Segmenter mask and upsampled mask.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-7-Upsampling.width-1250.png\" alt=\"Comparison of original Interactive Segmenter mask and upsampled mask.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Original Interactive Segmenter mask (<b>left</b>) and upsampled mask (<b>right</b>).</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">With the new Interactive Segmenter in <a href=\"https://apps.apple.com/us/app/snapseed-photo-editor/id439438619\" target=\"_blank\" rel=\"noopener noreferrer\">Snapseed</a> image editing has become easier and more powerful than ever. Simple taps and strokes are translated into accurate selections, allowing users to translate their editing ideas into reality. Download Snapseed for iOS <a href=\"https://apps.apple.com/us/app/snapseed-photo-editor/id439438619\" target=\"_blank\" rel=\"noopener noreferrer\">here</a> and let your photos shine. Object Brush will be rolled out to more tools in Snapseed in the coming months. The underlying model powers a wide range of image editing and manipulation tasks and serves as a foundational technology for intuitive selective editing. It has also been shipped in the new <a href=\"https://blog.google/products/chromebooks/lenovo-chromebook-plus-14/\" target=\"_blank\" rel=\"noopener noreferrer\">Chromebook Plus 14</a> to power AI image editing in the Gallery app. Next, we plan to integrate it across more image and creative editing products at Google.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\"><i>Special thanks to all members who worked on the tech with us: Valentin Bazarevsky, Daniel Fenner, Lutz Justen, Ronald Wotzlaw, Tai-Yu Daniel Pan, Jason Chang, Matthew Harries, Giles Ochs, Jonathan Horsman, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Karthik Raveendran, Matsvei Zhdanovich, Mogan Shieh, Chris Parsons, Jianing Wei, and Matthias Grundmann.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    }
  ],
  "lastUpdated": "2025-10-23T10:32:46.635Z"
}