{
  "sourceUrl": "https://rsshub.rssforever.com/google/research",
  "title": "Google Research Blog",
  "description": "Google Research Blog - Powered by RSSHub",
  "link": "https://research.google/blog",
  "items": [
    {
      "title": "Scheduling in a changing world: Maximizing throughput with time-varying capacity",
      "link": "https://research.google/blog/scheduling-in-a-changing-world-maximizing-throughput-with-time-varying-capacity/",
      "pubDate": "Tue, 10 Feb 2026 16:00:00 GMT",
      "isoDate": "2026-02-10T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Scheduling in a changing world: Maximizing throughput with time-varying capacity",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/JobScheduling1_Example.width-1250.png",
          "alt": "JobScheduling1_Example",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/JobScheduling2_Add.width-800.gif",
          "alt": "JobScheduling2_Add",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/JobScheduling3_Replace.width-800.gif",
          "alt": "JobScheduling3_Replace",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/JobScheduling4_Interrupt.width-800.gif",
          "alt": "JobScheduling4_Interrupt",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/JobScheduling5_Discard.width-800.gif",
          "alt": "JobScheduling5_Discard",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"eh7qr\">In the world of algorithmic job scheduling, computing resources are often viewed as static: a server has a fixed number of CPUs, or a cluster has a constant number of available machines. However, the reality of modern large-scale cloud computing is far more dynamic. Resources fluctuate constantly due to hardware failure, maintenance cycles, or power limitations.</p><p data-block-key=\"ab8lf\">More significantly, in <a href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43438.pdf\">tiered scheduling systems</a>, high-priority tasks often claim resources on demand, leaving a time-varying amount of “leftover” capacity for lower-priority batch jobs. Imagine a restaurant where tables are reserved for VIPs at different times; scheduling regular customers on the remaining tables can become a complex puzzle.</p><p data-block-key=\"a8ei5\">When these low-priority jobs are <i>non-preemptive</i> — meaning they cannot be paused and resumed later — the stakes are high. If a job is interrupted because capacity drops, all progress is lost. The scheduler must decide: <i>Do we start this long job now, risking a future capacity drop? Or do we wait for a safer window, potentially missing the deadline?</i></p><p data-block-key=\"9tqkg\">In “<a href=\"https://dl.acm.org/doi/10.1145/3694906.3743338\" target=\"_blank\" rel=\"noopener noreferrer\">Non-preemptive Throughput Maximization under Time-varying Capacity</a>”, presented at <a href=\"https://spaa.acm.org/spaa-2025-main/\" target=\"_blank\" rel=\"noopener noreferrer\">SPAA 2025</a>, we initiate the study of maximizing throughput (total weight or number of successful jobs) in environments when the available capacity fluctuates over time. Our research provides the first constant-factor (i.e., the \"gap\" between the algorithm's answer and the optimal answer is guaranteed to be a fixed, stable number, regardless of how large the problem gets) <a href=\"https://en.wikipedia.org/wiki/Approximation_algorithm\" target=\"_blank\" rel=\"noopener noreferrer\">approximation algorithms</a> for several variants of this problem, offering a theoretical foundation for building more robust schedulers in volatile cloud environments.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"eh7qr\">Defining the scheduling problem</h2><p data-block-key=\"9rs46\">Our work focuses on designing a scheduling model that captures a number of key constraints. We consider a single machine (or cluster) with a capacity profile that varies over time. This profile dictates the maximum number of jobs that can run in parallel at any given moment. We are given a collection of potential jobs, each characterized by four key attributes:</p><ul><li data-block-key=\"8qj2k\"><i>Release time</i>: When the job becomes available to run</li><li data-block-key=\"9vrvm\"><i>Deadline</i>: A hard deadline by which the job must finish</li><li data-block-key=\"c65ss\"><i>Processing time</i>: The duration for which the machine must work on the job</li><li data-block-key=\"187b2\"><i>Weight</i>: The value gained if the job is successfully completed</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/JobScheduling1_Example.width-1250.png\" alt=\"JobScheduling1_Example\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/JobScheduling1_Example.width-1250.png\" alt=\"JobScheduling1_Example\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"xvfp2\"><i>An instance of throughput maximization problem consists of a set of jobs, each with its release time, deadline, duration and value, and a capacity profile for the machine. The capacity profile determines the maximum number of jobs that can be processed simultaneously at any given time.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"eh7qr\">The goal is to select a subset of jobs and schedule them so that each selected job runs continuously within its valid window. The critical constraint is that any time, the total number of running jobs must not exceed the current capacity. Our objective is to maximize the throughput, i.e., the total weight of all completed jobs.</p><p data-block-key=\"asq63\">We address this problem in two distinct environments:</p><ul><li data-block-key=\"d1ncq\"><i>Offline</i>: Where future job arrivals and capacity changes are known in advance.</li><li data-block-key=\"cq2rj\"><a href=\"https://en.wikipedia.org/wiki/Online_algorithm\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Online</i></a>: Where jobs arrive in real time, and the scheduler must make irrevocable decisions without knowledge of future arrivals. The capacity landscape is still known in advance.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"eh7qr\">Results for the offline setting</h2><p data-block-key=\"83s6i\">In the offline setting, where we can plan ahead, we find that simple strategies perform surprisingly well. Because finding the <i>optimal</i> schedule in this setting is considered “<a href=\"https://en.wikipedia.org/wiki/NP-hardness\" target=\"_blank\" rel=\"noopener noreferrer\">NP-hard</a>” (i.e., there is no known shortcut to find the <i>perfect</i> answer), we focus on algorithms with rigorous approximation guarantees. We analyze a myopic strategy, called Greedy, that iteratively schedules the job that would finish earliest and prove that this simple heuristic achieves a 1/2-approximation (in this context, often as good as it gets) when jobs have unit profits. This means that even in the worst-case scenario with adversarially chosen jobs and capacity profiles, our simple algorithm is guaranteed to schedule at least half of the optimal number of jobs. This matches the guarantees enjoyed by Greedy on simpler, unit capacity machines that can only do one task at a time. When different jobs can have different associated profits, we utilize a <a href=\"https://math.mit.edu/~goemans/PAPERS/book-ch4.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">primal-dual framework</a> to achieve a 1/4-approximation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"eh7qr\">Results for the online setting</h2><p data-block-key=\"d3q1l\">The real complexity lies in the online setting, where jobs arrive dynamically and the scheduler must make immediate, irrevocable decisions without knowing what jobs will arrive next. We quantified the performance of an online algorithm via its competitive ratio, which is the worst case comparison between the throughput of our online algorithm and the throughput of an optimal algorithm that is aware of all the jobs apriori.</p><p data-block-key=\"ciqlc\">The standard non-preemptive algorithms fail completely here as their competitive ratio approaches zero. This happens because a single bad decision of scheduling a long job can ruin the possibility of scheduling many future smaller jobs. In this example, if you imagine that each completed job brings equal weight, regardless of its length, completing many short jobs is much more profitable than completing one long job.</p><p data-block-key=\"19bti\">To make the online problem solvable and reflect real-world flexibility, we studied two models that allow an active job to be interrupted if a better opportunity arises (though only jobs restarted and later completed non-preemptively count as successful).</p><h3 data-block-key=\"8v9r7\">Interruption with restarts</h3><p data-block-key=\"fu5hg\">In this model, an online algorithm is allowed to interrupt a currently executing job. While the partial work already performed on the interrupted job is lost, the job itself remains in the system and can be retried.</p><p data-block-key=\"a0qst\">We found that the flexibility provided by allowing job restarts is highly beneficial. A variant of Greedy that iteratively schedules the job that finishes earliest continues to achieve a 1/2-competitive ratio, matching the result in the offline setting.</p><h3 data-block-key=\"6dpve\">Interruption without restarts</h3><p data-block-key=\"1ipnq\">In this stricter model, all work performed on the interrupted job is lost and the job itself is discarded forever. Unfortunately, we find that in this strict model, any online algorithm can encounter a sequence of jobs that forces it into decisions which prevent it from satisfying much more work in the future. Once again, the competitive ratio of all online algorithms approaches zero. Analyzing the above hard instances led us to focus on the practical scenario where all jobs share a common deadline (e.g., all data processing must finish by the nightly batch run). For such common deadline instances, we devise novel constant competitive algorithms. Our algorithm is very intuitive and we describe the algorithm here for the simple setting of a unit capacity profile, i.e., we can schedule a single job at any time.</p><p data-block-key=\"94ufg\">In this setting, our algorithm maintains a <i>tentative schedule</i> by assigning the jobs that have already arrived to disjoint time intervals. When a new job arrives, the algorithm modifies the tentative schedule by taking the first applicable action out of the following four actions:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"eh7qr\">1. <i>Add</i> the job to the tentative schedule by placing it in an empty interval</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-vertical-padding --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/JobScheduling2_Add.width-800.gif\" alt=\"JobScheduling2_Add\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/JobScheduling2_Add.width-800.gif\" alt=\"JobScheduling2_Add\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"eh7qr\">2. <i>Replace</i> another future job from the tentative schedule if the new job is significantly smaller</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/JobScheduling3_Replace.width-800.gif\" alt=\"JobScheduling3_Replace\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/JobScheduling3_Replace.width-800.gif\" alt=\"JobScheduling3_Replace\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"eh7qr\">3. <i>Interrupt</i> the currently executing job if the new job is smaller than the remaining time of the executing job</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/JobScheduling4_Interrupt.width-800.gif\" alt=\"JobScheduling4_Interrupt\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/JobScheduling4_Interrupt.width-800.gif\" alt=\"JobScheduling4_Interrupt\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"eh7qr\">4. <i>Discard</i> the newly arrived job</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/JobScheduling5_Discard.width-800.gif\" alt=\"JobScheduling5_Discard\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/JobScheduling5_Discard.width-800.gif\" alt=\"JobScheduling5_Discard\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"n771k\">Our main result shows that a generalization of this algorithm to arbitrary capacity profiles gives the first constant competitive ratio for this problem. Formally, we obtain a competitive ratio of 1/11. While guaranteeing to schedule only ~9% of the optimal number of jobs sounds like a weak guarantee, this is a worst-case guarantee that holds even in the most adversarial situations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"n771k\">Conclusion and future directions</h2><p data-block-key=\"4uisk\">As cloud infrastructure becomes more dynamic, the assumption of static capacity in scheduling algorithms no longer holds. This paper initiates the formal study of throughput maximization under time-varying capacity, bridging the gap between theoretical scheduling models and the fluctuating reality of data centers.</p><p data-block-key=\"6mc56\">While we establish strong constant factor approximations, there is still room for growth. The gap between our 1/11 competitive ratio for the online setting and the theoretical upper bound of 1/2 suggests that more efficient algorithms may exist. Exploring randomized algorithms or scenarios with imperfect knowledge of future capacity could yield even better results for real-world applications.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"n771k\">Acknowledgments</h2><p data-block-key=\"2f7vs\"><i>This represents joint work with Aniket Murhekar (University of Illinois at Urbana Champaign), Zoya Svitkina (Google Research), Erik Vee (Google Research), and Joshua Wang (Google Research).</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Beyond one-on-one: Authoring, simulating, and testing dynamic human-AI group conversations",
      "link": "https://research.google/blog/beyond-one-on-one-authoring-simulating-and-testing-dynamic-human-ai-group-conversations/",
      "pubDate": "Mon, 09 Feb 2026 16:00:00 GMT",
      "isoDate": "2026-02-09T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Beyond one-on-one: Authoring, simulating, and testing dynamic human-AI group conversations",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DialogLab-1.width-1250.jpg",
          "alt": "DialogLab-1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DialogLab-2.width-1250.jpg",
          "alt": "DialogLab-2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DialogLab-4.width-1250.png",
          "alt": "DialogLab-4",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3p374\">Conversational AI has fundamentally reshaped how we interact with technology. While one-on-one interactions with large language models (LLMs) have seen significant advances, they rarely capture the full complexity of human communication. Many real-world dialogues, including team meetings, family dinners, or classroom lessons, are inherently multi-party. These interactions involve fluid turn-taking, shifting roles, and dynamic interruptions.</p><p data-block-key=\"516mh\">For designers and developers, simulating natural and engaging multi-party conversations has historically required a trade-off: settle for the <i>rigidity</i> of scripted interaction or accept the <i>unpredictability</i> of purely generative models. To bridge this gap, we need tools that blend the structural predictability of a script with the spontaneous, improvisational nature of human conversation.</p><p data-block-key=\"bvfbq\">To address this need, we introduce <a href=\"https://dl.acm.org/doi/10.1145/3746059.3747696\" target=\"_blank\" rel=\"noopener noreferrer\">DialogLab</a>, presented at <a href=\"https://programs.sigchi.org/uist/2025/program/content/206895\" target=\"_blank\" rel=\"noopener noreferrer\">ACM UIST 2025</a>, an <a href=\"https://github.com/ecruhue/DialogLab\" target=\"_blank\" rel=\"noopener noreferrer\">open-source</a> prototyping framework designed to author, simulate, and test dynamic human-AI group conversations. DialogLab provides a unified interface to manage multi-party dialogue complexity, handling everything from defining agent personas to orchestrating complex turn-taking dynamics. Through integrating real-time improvisation with structured scripting, this framework enables developers to test conversations ranging from a structured Q&amp;A session to a free-flowing creative brainstorm. Our evaluations with 14 end users or domain experts validate that DialogLab supports efficient iteration and realistic, adaptable multi-party design for training and research.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DialogLab-1.width-1250.jpg\" alt=\"DialogLab-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DialogLab-1.width-1250.jpg\" alt=\"DialogLab-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"yb1dx\"><i>DialogLab is a research prototype that supports authoring, simulating, and testing dynamic human–AI group conversations. Designers can 1) configure group, party, snippet characteristics, 2) test with simulation and live interaction, and 3) gain insights with timeline view and post-hoc analytics.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">A framework for dynamic conversation</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3p374\">DialogLab decouples a conversation’s social setup — such as participants, roles, subgroups, and relationships — from its temporal progression. This separation enables creators to author complex dynamics via a streamlined three-stage workflow: author, test, verify.</p><p data-block-key=\"58obn\">At its core, the DialogLab framework defines conversations along two dimensions:</p><ul><li data-block-key=\"65a45\"><i>Group dynamics</i>: This covers the social setup of the interaction.<ul><li data-block-key=\"3ru87\">A <i>group</i> is the top-level container (e.g., a conference social event).</li><li data-block-key=\"8qt87\"><i>Parties</i> are sub-groups that have distinct roles (e.g., \"presenters\" and \"audience\").</li><li data-block-key=\"bupf1\"><i>Elements</i> are the individual participants (human or AI) and any shared content, like a presentation slide.</li></ul></li><li data-block-key=\"7agi8\"><i>Conversation flow dynamics</i>: This describes how the dialogue unfolds over time.<ul><li data-block-key=\"b78b8\">The flow is broken down into <i>snippets</i>, which represent distinct phases of the conversation. Each snippet has a defined set of participants, a sequence of conversational turns, and specific interaction styles (e.g., collaborative or argumentative). Creators can also define rules for interruptions and backchanneling to make the dialogue more realistic.</li></ul></li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DialogLab-2.width-1250.jpg\" alt=\"DialogLab-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DialogLab-2.width-1250.jpg\" alt=\"DialogLab-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"yb1dx\"><i>Our framework separates the social setup (roles, parties) from the temporal flow (snippets, turn-taking rules), allowing for modular conversation design.</i> <b><i>Left</i></b><i>: An example of authoring group dynamics for demo presenters and Q&amp;A audiences;</i> <b><i>Right</i></b><i>: An example of authoring conversation dynamics in three stages: opening, debate, consensus.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">The “author-test-verify” workflow for dynamic conversation</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3p374\">DialogLab guides creators through a structured author-test-verify workflow, supported by a visual interface designed for rapid iteration.</p><ol><li data-block-key=\"b9a\"><i>Authoring with visual tools</i>: The interface features a drag-and-drop canvas where users position avatars and content from libraries to build scenes. Inspector panels allow for granular configuration, from an avatar’s persona to the interaction patterns within a specific snippet. To accelerate the design process, DialogLab offers auto-generated conversation prompts that can be fine-tuned to meet specific narrative goals.</li><li data-block-key=\"5nmjf\"><i>Simulation with human-in-the-loop</i>: Testing is critical for multi-party interactions. DialogLab includes a live preview panel that displays the conversation transcript and a \"<i>human control</i>\" mode, where an <i>audit panel</i> suggests potential AI responses. The designer can edit, accept, or dismiss these suggestions, providing fine-grained control over the AI's contributions and allowing for rapid iterations.</li><li data-block-key=\"4ahjt\"><i>Verification and analytics</i>: To validate the interaction, the verification dashboard serves as a diagnostic tool. It visualizes conversation dynamics, allowing creators to quickly analyze turn-taking distributions and sentiment flows without parsing through lengthy raw transcripts.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/DialogLab-3.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n\n      </div>\n    \n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"6wr6h\"><i>Demonstration of the DialogLab prototype, which supports the authoring, simulating, and testing of dynamic human-AI group conversations.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Prototype evaluation</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3p374\">We evaluated DialogLab with 14 participants across game design, education, and social science research. Participants completed two tasks in DialogLab: designing an academic social event, and testing a group discussion with AI under three conditions:</p><ol><li data-block-key=\"6oth7\"><i>Human control</i>: When testing a conversation, the user can ask agents to “shift topic”, generate “new perspective”, “probe question”, or generate “emotional response”.</li><li data-block-key=\"d9nna\"><i>Autonomous</i>: The simulated agents proactively participate in the conversation based on pre-defined orders (random or one-by-one), while generating emotional responses and topic shifts automatically.</li><li data-block-key=\"om0t\"><i>Reactive</i>: the simulated human agent only responds when directly mentioned by other agents, simulating traditional human-AI turn-taking behaviors.</li></ol><p data-block-key=\"2seuj\">Participants rated each condition at a 5-point <a href=\"https://en.wikipedia.org/wiki/Likert_scale\" target=\"_blank\" rel=\"noopener noreferrer\">Likert scale</a>. Participants found the human control mode to be significantly more engaging, and generally more effective and realistic for simulating real-world conversations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DialogLab-4.width-1250.png\" alt=\"DialogLab-4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DialogLab-4.width-1250.png\" alt=\"DialogLab-4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"6cwdk\">Bar chart comparing Human Control, Autonomous, and Reactive systems across Ease of Use, Engagement, Effectiveness, and Realism.</p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3p374\">Participants’ feedback further highlighted the system's ability to balance automation with control:</p><ul><li data-block-key=\"citi2\"><i>Intuitive and engaging</i>: Most participants found DialogLab easy to use and the visual, drag-and-drop interface for setting up scenes and roles to be fun and efficient.</li><li data-block-key=\"7ssr0\"><i>Flexible and controllable</i>: Users appreciated the balance between auto-generated prompts and the ability to fine-tune conversation details. The system's ability to model different moderation strategies was also highlighted as a key strength.</li><li data-block-key=\"8o42q\"><i>Realistic simulation</i>: The human control mode was the clear favorite for testing, with users reporting that it gave them a greater sense of agency and immersion. It was rated as more engaging, effective, and realistic for simulating human behavior compared to fully autonomous or purely reactive agents.</li><li data-block-key=\"7qbon\"><i>Powerful verification</i>: The verification dashboard was seen as a valuable diagnostic tool for quickly analyzing conversation dynamics without having to read through lengthy transcripts.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Future directions</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3p374\">DialogLab is more than just a research prototype; it's a step toward a future where human-AI collaboration is richer and more nuanced. The potential applications are vast:</p><ul><li data-block-key=\"772ub\"><i>Education and skill development</i>: Students could practice public speaking in front of a simulated audience, or professionals could rehearse difficult conversations and interviews.</li><li data-block-key=\"5o9ph\"><i>Game design and storytelling</i>: Writers and game developers can create more believable and dynamic <a href=\"https://en.wikipedia.org/wiki/Non-player_character\" target=\"_blank\" rel=\"noopener noreferrer\">non-player characters</a> (NPCs) that interact with each other and the player in more natural ways.</li><li data-block-key=\"71gmh\"><i>Social science research</i>: DialogLab can be used as a controlled environment to study group dynamics, allowing researchers to test hypotheses about social interaction without the logistical challenges of recruiting large groups of people.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/DialogLab-mp4-2.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n\n      </div>\n    \n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"yb1dx\"><i>Example applications of DialogLab, including practicing conference Q&amp;A sessions, simulating debates, and creating game dialog design.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3p374\">Moving forward, we envision richer multimodal behaviors, such as non-verbal gestures and facial expressions, could be integrated into this framework, We could also explore the use of photorealistic avatars and 3D environments like <a href=\"https://research.google/blog/chatdirector-enhancing-video-conferencing-with-space-aware-scene-rendering-and-speech-driven-layout-transition/\">ChatDirector</a> to create even more immersive and realistic simulations in our open-source <a href=\"https://xrblocks.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">XR Blocks framework</a>. We hope this research will inspire continued innovation in the exciting and emerging field of human-AI group conversation dynamics.</p><p data-block-key=\"6hbno\"></p><p data-block-key=\"1gc1o\"><i>See</i> <a href=\"https://www.youtube.com/watch?v=U2Ag_Ktobzw\" target=\"_blank\" rel=\"noopener noreferrer\"><i>video demonstration</i></a><i> of DialogLab to learn more.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3p374\"><i>Key contributors to the project include Erzhen Hu, Yanhe Chen, Mingyi Li, Vrushank Phadnis, Pingmei Xu, Xun Qian, Alex Olwal, David Kim, Seongkook Heo, and Ruofei Du. We would like to extend our thanks to Adarsh Kowdle for providing feedback or assistance for the manuscript and the blog post. This project is partly sponsored by the</i> <a href=\"https://research.google/programs-and-events/phd-fellowship/recipients/?filtertab=2024\"><i>Google PhD fellowship</i></a><i>.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "How AI trained on birds is surfacing underwater mysteries",
      "link": "https://research.google/blog/how-ai-trained-on-birds-is-surfacing-underwater-mysteries/",
      "pubDate": "Sun, 08 Feb 2026 16:00:00 GMT",
      "isoDate": "2026-02-08T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "How AI trained on birds is surfacing underwater mysteries",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Perch_V2-1.width-1250.jpg",
          "alt": "Perch V2-1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Perch_V2-2.width-1250.png",
          "alt": "Perch V2-2",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"vfheo\">Underwater sound is critical for understanding the unseeable patterns of marine species and their environment. The ocean soundscape is full of mysterious noises and unfound discoveries. For example, the <a href=\"https://pubs.aip.org/asa/jasa/article/140/3/EL274/649641/A-complex-baleen-whale-call-recorded-in-the\" target=\"_blank\" rel=\"noopener noreferrer\">mysterious “biotwang” sound</a>, recently attributed to <a href=\"https://www.frontiersin.org/journals/marine-science/articles/10.3389/fmars.2024.1394695/full\" target=\"_blank\" rel=\"noopener noreferrer\">the elusive Bryde’s whales</a> by the U.S. <a href=\"https://www.noaa.gov/\" target=\"_blank\" rel=\"noopener noreferrer\">National Oceanic and Atmospheric Administration</a> (NOAA), illustrates the continuous challenge of new song types and species attributions being identified regularly.</p><p data-block-key=\"10cqa\">Google has a long history of collaborating with external scientists on using bioacoustics for monitoring and protecting whales, including our <a href=\"https://research.google/blog/acoustic-detection-of-humpback-whales-using-a-convolutional-neural-network/\">original research models to detect humpback whale classifications</a> and the <a href=\"https://research.google/blog/whistles-songs-boings-and-biotwangs-recognizing-whale-vocalizations-with-ai/\">release</a> of our <a href=\"https://www.kaggle.com/models/google/multispecies-whale\" target=\"_blank\" rel=\"noopener noreferrer\">multi-species whale model</a> in 2024. To keep up with this pace, Google’s approach to AI for bioacoustics is evolving to enable more efficient connections from new discoveries to scientific insights at scale. In August 2025, <a href=\"https://deepmind.google/\" target=\"_blank\" rel=\"noopener noreferrer\">Google DeepMind</a> released the latest <a href=\"https://deepmind.google/blog/how-ai-is-helping-advance-the-science-of-bioacoustics-to-save-endangered-species/\" target=\"_blank\" rel=\"noopener noreferrer\">Perch foundational bioacoustics model</a>, <a href=\"https://arxiv.org/abs/2508.04665\" target=\"_blank\" rel=\"noopener noreferrer\">Perch 2.0</a>, a bioacoustics foundation model trained primarily on birds and other terrestrial vocalizing animals. Surprisingly, despite including no underwater audio in training, Perch 2.0 performed well as an embedding model for transfer learning in marine validation tasks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Perch_V2-1.width-1250.jpg\" alt=\"Perch V2-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Perch_V2-1.width-1250.jpg\" alt=\"Perch V2-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"y3hqt\"><i>A beluga whale — the “canary” of the sea. (Credit: Lauren Harrell)</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"vfheo\">In our latest paper, “<a href=\"https://arxiv.org/abs/2512.03219\" target=\"_blank\" rel=\"noopener noreferrer\">Perch 2.0 transfers 'whale' to underwater tasks</a>”, a collaboration between Google Research and Google DeepMind presented at the <a href=\"https://neurips.cc/\" target=\"_blank\" rel=\"noopener noreferrer\">NeurIPS 2025</a> workshop on <a href=\"https://aiforanimalcomms.org/\" target=\"_blank\" rel=\"noopener noreferrer\">AI for Non-Human Animal Communications</a>, we deep dive into these results. We show how this bioacoustics foundation model, trained mostly on bird data, can be used to enable and scale insights for underwater marine ecosystems, particularly for classifying whale vocalizations. We are also sharing an end-to-end <a href=\"https://github.com/google-research/perch/blob/main/chirp/projects/whale_demo/agile_modeling_noaa_demo.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">tutorial in Google Colab</a> for our agile modeling workflow, demonstrating how to use Perch 2.0 to create a custom classifier for whale vocalizations using the <a href=\"https://www.ncei.noaa.gov/products/passive-acoustic-data\" target=\"_blank\" rel=\"noopener noreferrer\">NOAA NCEI Passive Acoustic Data Archive</a> through <a href=\"https://console.cloud.google.com/marketplace/details/noaa-public/passive_acoustic_monitoring?pli=1\" target=\"_blank\" rel=\"noopener noreferrer\">Google Cloud</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How bioacoustics classification works</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vfheo\">If a pre-trained classification model, such as our multi-species whale model, already has the necessary labels and works well on a researcher’s dataset, it can be used directly to produce scores and labels for their audio data. However, to create a new custom classifier for newly discovered sounds or to improve accuracy on new data, we can leverage <a href=\"https://www.nature.com/articles/s41598-023-49989-z\" target=\"_blank\" rel=\"noopener noreferrer\">transfer learning</a> instead of building a new model from scratch. This approach drastically reduces the amount of computation and experimentation needed to create a new custom classifier.</p><p data-block-key=\"81jh5\">In bioacoustics transfer learning, the pre-trained model (such as Perch 2.0) is used to produce embeddings for each window of audio. These embeddings reduce the large audio data into a much smaller array of features that serve as input for a simple classifier. To create a new custom model for any set of labeled audio data, we apply the pre-trained model to the audio data to get the embeddings, which are used as the input features for a logistic regression classifier. Instead of learning all of the parameters for a deep neural network, we now only need to learn new parameters for the last step of logistic regression, which is much more efficient for both the researcher’s time and computational resources.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Evaluation</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vfheo\">We evaluated Perch 2.0 using a few-shot linear probe on marine tasks, such as distinguishing different <a href=\"https://en.wikipedia.org/wiki/Baleen_whale\" target=\"_blank\" rel=\"noopener noreferrer\">baleen whale</a> species or different <a href=\"https://en.wikipedia.org/wiki/Orca\" target=\"_blank\" rel=\"noopener noreferrer\">killer whale</a> subpopulations. Its performance was compared against pre-trained models that are supported in our <a href=\"https://github.com/google-research/perch-hoplite\" target=\"_blank\" rel=\"noopener noreferrer\">Perch Hoplite</a> repository for agile modeling and transfer learning. They include <a href=\"https://www.kaggle.com/models/google/bird-vocalization-classifier/tensorFlow2/perch_v2/2\" target=\"_blank\" rel=\"noopener noreferrer\">Perch 2.0</a>, <a href=\"https://www.kaggle.com/models/google/bird-vocalization-classifier/tensorFlow2/bird-vocalization-classifier\" target=\"_blank\" rel=\"noopener noreferrer\">Perch 1.0</a>, <a href=\"https://www.kaggle.com/models/google/surfperch\" target=\"_blank\" rel=\"noopener noreferrer\">SurfPerch</a>, and the <a href=\"https://www.kaggle.com/models/google/multispecies-whale\" target=\"_blank\" rel=\"noopener noreferrer\">multispecies whale model</a>.</p><p data-block-key=\"5q62g\">For underwater data evaluation, we used three datasets: NOAA PIPAN, <a href=\"https://arxiv.org/abs/2404.16436\" target=\"_blank\" rel=\"noopener noreferrer\">ReefSet</a>, and <a href=\"https://www.nature.com/articles/s41597-025-05281-5\" target=\"_blank\" rel=\"noopener noreferrer\">DCLDE</a>.</p><ul><li data-block-key=\"ffthu\"><b>NOAA PIPAN</b>: An annotated subset of the <a href=\"https://www.ncei.noaa.gov/products/passive-acoustic-data\" target=\"_blank\" rel=\"noopener noreferrer\">NOAA NCEI Passive Acoustic Data Archive</a> from the NOAA Pacific Islands Fisheries Science Center recordings. It includes labels used in our prior whale models as well as new annotations for baleen species such as common minke whale, humpback whale, sei whale, blue whale, fin whale, and Bryde’s whale.</li><li data-block-key=\"73qhm\"><b>ReefSet:</b> Developed for <a href=\"https://blog.google/outreach-initiatives/arts-culture/a-new-ai-tool-to-help-monitor-coral-reef-health/\" target=\"_blank\" rel=\"noopener noreferrer\">SurfPerch</a> model training, this dataset leverages data annotations from the <a href=\"https://artsandculture.google.com/experiment/zgFx1tMqeIZyTw?e\" target=\"_blank\" rel=\"noopener noreferrer\">Google Arts and Culture project</a>: <a href=\"https://blog.google/outreach-initiatives/arts-culture/how-to-protect-coral-reefs/\" target=\"_blank\" rel=\"noopener noreferrer\">Calling in Our Corals</a>. It includes a mix of biological reef noises (croaks, crackles, growls), specific species/genera classes (e.g., damselfish, dolphins, and groupers), and anthropomorphic noise and wave classes.</li><li data-block-key=\"c9tev\"><b>DCLDE:</b> This dataset is evaluated using three different label sets:<ul><li data-block-key=\"2lt6v\">Species: For distinguishing between killer whales, humpbacks, abiotic sounds, and unknown underwater sounds (with some uncertainty in killer whale and humpbacks labels).</li><li data-block-key=\"789md\">Species Known Bio: For certain labels of killer whales and humpbacks.</li><li data-block-key=\"8n887\">Ecotype: For distinguishing between killer whale subpopulations (<a href=\"https://en.wikipedia.org/wiki/Ecotype\" target=\"_blank\" rel=\"noopener noreferrer\">ecotypes</a>), including Transient/Biggs, Northern Residents, Southern Residents, Southeastern Alaska killer whales, and offshore killer whales.</li></ul></li></ul><p data-block-key=\"58el0\">In this protocol, for a given target dataset with labeled data, we compute embeddings from each of the candidate models. We then select a fixed number of examples per class (4, 8, 16, or 32), and train a simple multi-class logistic regression model on top of the embeddings. We use the resulting classifier to compute the <a href=\"https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\" target=\"_blank\" rel=\"noopener noreferrer\">area under the receiver-operating characteristic curve</a> (AUC_ROC), where values closer to 1 indicate a stronger ability to distinguish between classes. This process simulates using a given pre-trained embedding model to create a custom classifier from a small number of labelled examples.</p><p data-block-key=\"eh6mr\">Our results show that more examples per class improve performance across all the models, except on ReefSet data, where performance is high even with only four examples per class for all models, except the multispecies whale model. Notably, Perch 2.0 is consistently either the top or second-best performing model for each dataset and sample size.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Perch_V2-2.width-1250.png\" alt=\"Perch V2-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Perch_V2-2.width-1250.png\" alt=\"Perch V2-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"y3hqt\"><i>Performance of trained models on marine datasets, varying the number (</i>k<i>) of training examples per-class. Higher values of AUC_ROC indicate improved classification performance.*Class “Bm” dropped for</i> k<i> = 16; **Classes ‘Bm’ and ‘Be’ dropped for</i> k<i> = 32 in NOAA PIPAN data.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"vfheo\">We also compare Perch 2.0 to embeddings from <a href=\"https://github.com/earthspecies/aves\" target=\"_blank\" rel=\"noopener noreferrer\">AVES-bird</a> and <a href=\"https://github.com/earthspecies/aves\" target=\"_blank\" rel=\"noopener noreferrer\">AVES-bio</a> (<a href=\"https://www.earthspecies.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Earth Species Project</a> transformer bioacoustics models trained on birds and biological sounds, respectively) and BirdNet v2.3 from <a href=\"https://www.birds.cornell.edu/ccb/\" target=\"_blank\" rel=\"noopener noreferrer\">K. Lisa Yang Center for Conservation Bioacoustics</a> at the<a href=\"https://www.birds.cornell.edu/home\" target=\"_blank\" rel=\"noopener noreferrer\"> Cornell Lab of Ornithology</a>. Perch 2.0 outperforms AVES-bird and AVES-bio on most underwater tasks, but there are other pre-trained models that also perform well that weren’t trained on underwater audio.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How does Perch 2.0 work so 'whale'?</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vfheo\">We offer a few possible reasons for this transfer performance from our model trained primarily on birds to underwater sounds. First, <a href=\"https://arxiv.org/abs/2001.08361\" target=\"_blank\" rel=\"noopener noreferrer\">prior research</a> shows that larger models with extensive training data generalize better, allowing our bioacoustics model to perform well even on downstream tasks classifying sounds for species and sounds not included in the training dataset. Additionally, the challenge of classifying similar bird calls (the “<a href=\"https://arxiv.org/abs/2508.04665\" target=\"_blank\" rel=\"noopener noreferrer\">bittern lesson</a>”) forces the model to learn detailed acoustic features that can then be informative for other bioacoustics tasks. For example, there are 14 species of doves in North America, each with their own subtly distinct “coo” sound. A model that extracts the features that can distinguish between each species-specific “coo” is likely to isolate features that can help separate other sound classes. Finally, feature transfer across different types of species could also be related to the sound production mechanism itself, where a variety of species — including birds and marine mammals — <a href=\"https://www.nature.com/articles/ncomms9978\" target=\"_blank\" rel=\"noopener noreferrer\">have evolved similar means of sound production</a>.</p><p data-block-key=\"aegi4\">A high performing model will have embeddings that are informative and linearly separable for the applied target classes. To visualize, we plot a summary of embeddings from each model using a procedure called <a href=\"https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding\" target=\"_blank\" rel=\"noopener noreferrer\">tSNE</a>, where different colors represent different classes. A highly informative model will show distinct clusters for each class, whereas the classes will be more intermixed in a less informative model (such as the Google multi-species whale model). While almost all models show some distinct clusters of points for Southern Resident killer whales (KW_SRKW) and Southern Alaskan Residents (KW_SAR), the resulting embeddings of sounds from Northern Resident killer whales (KW_NRKW), Transient killer whales (KW_TKW), and Offshore killer whales (KW_OKW) are intermingled in models such as AVES-bio, AVES-bird, and SurfPerch, but are more clearly distinguished in BirdNet v2.3 and Perch 2.0.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Perch_V2-3.width-1250.png\" alt=\"Perch V2-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Perch_V2-3.width-1250.png\" alt=\"Perch V2-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"y3hqt\"><i>tSNE plots of the embeddings from each model on the</i> <a href=\"https://www.nature.com/articles/s41597-025-05281-5\" target=\"_blank\" rel=\"noopener noreferrer\"><i>DCLDE 2026</i></a><i> Ecotype dataset, which contains five ecotype variants of the killer whale (orca) species. Plots were generated with</i> <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\" target=\"_blank\" rel=\"noopener noreferrer\"><i>sci-kit learn PCA</i></a><i> and</i> <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\" target=\"_blank\" rel=\"noopener noreferrer\"><i>tSNE</i></a><i> libraries, with embeddings first projected to 32 dimension vectors prior to tSNE being applied.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Looking ahead</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vfheo\">The Google DeepMind Perch team, in collaboration with Google Research and external partners, has pioneered an <a href=\"https://arxiv.org/abs/2505.03071\" target=\"_blank\" rel=\"noopener noreferrer\">agile modeling approach</a> for bioacoustics to create a custom classifier from a small number of labelled examples within a couple hours. In order to support both Google Research partners as well as the broader cetacean acoustics community, we have created an <a href=\"https://github.com/google-research/perch/blob/main/chirp/projects/whale_demo/agile_modeling_noaa_demo.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">end-to-end demo for working with the NOAA data</a> from the Passive Acoustic Archive dataset hosted on Google Cloud, updating <a href=\"https://neurips.cc/virtual/2023/76893\" target=\"_blank\" rel=\"noopener noreferrer\">our prior tutorials</a> using the more efficient Perch Hoplite databases for managing embeddings.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vfheo\"><i>The Perch Team, which developed the Perch 2.0 model and is part of Google DeepMind, includes Tom Denton, Bart van Merriënboer, Vincent Dumoulin, Jenny Hamer, Isabelle Simpson, Andrea Burns, and Lauren Harrell (Google Research). Special thanks to Ann Allen (NOAA Pacific Islands Fisheries Center) and Megan Wood (Saltwater Inc. in support of NOAA) for providing additional annotations used in the NOAA PIPAN dataset, Dan Morris (Google Research) and Matt Harvey (Google DeepMind).</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "How AI agents can redefine universal design to increase accessibility (原标题: How AI tools can redefine universal design to increase accessibility)",
      "link": "https://research.google/blog/how-ai-agents-can-redefine-universal-design-to-increase-accessibility/",
      "pubDate": "Wed, 04 Feb 2026 16:00:00 GMT",
      "isoDate": "2026-02-04T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "How AI agents can redefine universal design to increase accessibility",
      "images": [],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"awgus\">At Google, we believe in building for everyone and accessibility (A11y) is a key part of that. Our teams work with communities to build products with and for people with disabilities, incorporating accessibility from the beginning of the development process. Today, generative AI provides us with the opportunity to make our tools even more personal and adaptive.</p><p data-block-key=\"1q0jg\">People with disabilities make up 16% of the world’s population. With the adaptive capabilities of generative AI, we have an opportunity to better serve 1.3 billion people globally by adopting a <i>\"</i><a href=\"https://en.wikipedia.org/wiki/Nothing_about_us_without_us\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nothing About Us Without Us</i></a><i>\" approach</i> to our tech development. We believe technology should be as unique as the person using it. We’re creating a world where every interface shapes itself to your preferences, working in harmony with you, exactly as you are.</p><p data-block-key=\"5u1pk\">In this blog, we are proud to introduce <i>Natively Adaptive Interfaces</i> (NAI), a framework for creating more accessible applications through multimodal AI tools. With NAI, UI design can move beyond one-size-fits-all towards context-informed decisions. NAI replaces static navigation with dynamic, agent-driven modules, transforming digital architecture from a passive tool into an active collaborator.</p><p data-block-key=\"clmou\">Following rigorous prototyping to validate this framework, we have an emerging path toward <a href=\"https://en.wikipedia.org/wiki/Universal_design\" target=\"_blank\" rel=\"noopener noreferrer\">universal design</a>. Our goal is to create environments that are more inherently accessible to people with disabilities.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"awgus\">Community investments: Nothing About Us, Without Us</h2><p data-block-key=\"c2v5r\">Building on the long-standing advocacy principle of \"Nothing About Us, Without Us,\" we continue to integrate community-led co-design into our own development lifecycles.</p><p data-block-key=\"365qd\">By working with individuals from disability communities and engaging them as co-designers from the start, we can ensure their lived experiences and expertise are at the heart of the solutions being built. With support from <a href=\"http://google.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Google.org</a>, organizations like the <a href=\"https://www.rit.edu/ntid/\" target=\"_blank\" rel=\"noopener noreferrer\">Rochester Institute of Technology’s National Technical Institute for the Deaf (RIT/NTID)</a>, <a href=\"https://thearc.org/\" target=\"_blank\" rel=\"noopener noreferrer\">The Arc of the United States</a>, <a href=\"https://rnid.org.uk/\" target=\"_blank\" rel=\"noopener noreferrer\">RNID</a>, and <a href=\"https://teamgleason.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Team Gleason</a> are building adaptive AI tools that solve real-world friction points for their communities. These organizations recognize the transformative potential for impact of AI tools that are natively fluent in the diverse ways humanity communicates.</p><p data-block-key=\"3ur90\">Furthermore, this co-design approach drives economic empowerment and fosters employment opportunities within the disability community, ensuring that the people informing the technology are also rewarded for its success.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"awgus\">Our research direction: Designing for accessibility</h2><p data-block-key=\"e9u77\">In our early research, we found that a significant barrier to digital equity is the \"accessibility gap\", i.e., the delay between the release of a new feature and the creation of an assistive layer for it. To close this gap, we are shifting from reactive tools to agentic systems that are native to the interface.</p><h3 data-block-key=\"80c5q\">Research pillar: Using multi-system agents to improve accessibility</h3><p data-block-key=\"9ec98\">Multimodal AI tools provide one of the most promising paths to building accessible interfaces. In specific prototypes, such as our work with web readability, we’ve tested a model where a central Orchestrator acts as a strategic reading manager.</p><p data-block-key=\"dsc7m\">Instead of a user navigating a complex maze of menus, the Orchestrator maintains shared context — understanding the document and making it more accessible by delegating the tasks to expert sub-agents.</p><ul><li data-block-key=\"95ouj\"><i>The Summarization Agent:</i> It masters complex documents by breaking down information and delegating key tasks to expert sub-agents, making even the deepest insights clear and accessible.</li><li data-block-key=\"2i60q\"><i>The Settings agent:</i> Handles UI adjustments, such as scaling text, dynamically.</li></ul><p data-block-key=\"3qf1d\">By testing this modular approach,our research shows users can interact with systems more intuitively, ensuring that specialized tasks are always handled by the right expert without the user needing to hunt for the \"correct\" button.</p><h3 data-block-key=\"3ui92\">Toward multimodal fluency</h3><p data-block-key=\"1d1ah\">Our research also focuses on moving beyond basic text-to-speech toward multimodal fluency. By leveraging Gemini’s ability to process voice, vision, and text simultaneously, we’ve built prototypes that can turn live video into immediate, interactive audio descriptions.</p><p data-block-key=\"6fu7k\">This isn't just about describing a scene; it’s about situational awareness. In our co-design sessions, we’ve observed how allowing users to interactively query their environment — asking for specific visual details as they happen — can reduce cognitive load and transform a passive experience into an active, conversational exploration.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"awgus\">Proven prototypes: The \"vertex\" of human interaction</h2><p data-block-key=\"dggsu\">We validated this architecture through rigorous prototyping, aiming to solve complex interaction challenges with opportunities for improvement. In these \"vertex\" moments, our research showed that multimodal AI tools could accurately interpret and respond to the nuanced, specific needs of users.</p><ul><li data-block-key=\"bh892\"><a href=\"https://research.google/blog/streetreaderai-towards-making-street-view-accessible-via-context-aware-multimodal-ai/\"><i>StreetReaderAI</i></a><i>:</i> A virtual guide for blind and low-vision (BLV) users, navigating physical spaces can be a significant barrier to social participation. StreetReaderAI addresses this by employing two interactive AI subsystems: an AI Describer that constantly analyzes visual and geographic data, and an AI Chat that answers specific questions. Because the system maintains context, a user can walk past a landmark and later ask, \"Wait, where was that bus stop?\" The agent recalls the previous visual frame and provides precise guidance: \"The bus stop is behind you, approximately 12 meters away.\"</li><li data-block-key=\"3f4o5\"><a href=\"https://arxiv.org/abs/2602.04104\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Multimodal Agent Video Player (MAVP)</i></a><i>:</i> Passive listening standard Audio Descriptions (AD) provide a narrated track of visual elements, but they are often static. The MAVP prototype transforms video into an interactive, user-led dialogue. Built with Gemini models, MAVP allows users to verbally adjust descriptive detail in real-time or pause to ask questions like, \"What is the character wearing?\" The system uses a two-stage pipeline: it first generates a \"dense index\" of visual descriptions offline, then uses <a href=\"https://research.google/blog/deeper-insights-into-retrieval-augmented-generation-the-role-of-sufficient-context/\">retrieval-augmented generation</a> (RAG) to provide fast, high-accuracy responses during playback.</li><li data-block-key=\"e7j6g\"><i>Grammar Laboratory:</i> RIT/NTID, with support from Google.org, is building Grammar Laboratory, a bilingual (American Sign Language and English) AI-powered learning platform that provides tutoring and feedback on students’ English writing. It offers grammar instruction through multiple accessible formats, including: video explanations of English grammar rules delivered in ASL, captions in written English, spoken English narration, and written transcripts. Students interface with an adaptive AI tool that creates bespoke content and customizes their learning experience based on their interaction, ensuring that users can engage with the content in the format that best suits their language preferences and strengths. To highlight this impact, Grammar Laboratory was recently highlighted in a <a href=\"https://youtu.be/pdFiZMiDK9A\" target=\"_blank\" rel=\"noopener noreferrer\">film</a> produced for us by BBC StoryWorks Commercial Productions.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"awgus\">The curb-cut effect</h2><p data-block-key=\"f0o3\">Applications utilizing the NAI framework often experience a strong \"<a href=\"https://en.wikipedia.org/wiki/Curb_cut_effect\" target=\"_blank\" rel=\"noopener noreferrer\">curb-cut effect</a>\" — the phenomenon wherein features designed for extreme constraints benefit a much broader group. Just as sidewalk ramps were originally designed for wheelchair users but improved life for parents with strollers and travelers with luggage, AI tools built with the NAI framework create superior experiences for many. For example:</p><ul><li data-block-key=\"5itir\"><i>Universal utility:</i> Voice interfaces built for blind users can be incredibly useful for sighted users who are multitasking.</li><li data-block-key=\"7lvj\"><i>Synthesis tools:</i> Tools designed to support those with learning disabilities can help busy professionals parse information more quickly.</li><li data-block-key=\"atsnb\"><i>Personalized learning:</i> AI-powered tutors built for deaf and hard of hearing users can create custom learning journeys for all students.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"awgus\">Conclusion: The golden age of access</h2><p data-block-key=\"d8b4d\">We are entering a \"golden age\" of what is possible with AI for accessibility. With the adaptive power of multimodal AI, we have the opportunity to build user interfaces that adjust in real-time to the vast variety of human ability.</p><p data-block-key=\"usjv\">This era is about more than just using a device; it is about working directly with the communities who use these technologies. By building technology with and for the disability community, we can ignite a cycle of helpfulness that expands the horizon of what is possible by creating it.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"awgus\">Acknowledgements</h2><p data-block-key=\"285um\"><i>Our work is made possible through the generous support of</i> <a href=\"http://google.org/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Google.org</i></a><i>, whose commitment to our vision has been transformative. We are honored to work alongside dedicated teams from Google Research AI, Product For All (P4A),</i> <a href=\"https://www.bbcstudioworks.com/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>BBCWorks</i></a><i>,</i> <a href=\"https://www.rit.edu/ntid/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Rochester Institute of Technology’s National Technical Institute for the Deaf (RIT/NTID)</i></a><i>,</i> <a href=\"https://thearc.org/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>The Arc of the United States</i></a><i>,</i> <a href=\"https://rnid.org.uk/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>RNID</i></a><i>, and</i> <a href=\"https://teamgleason.org/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Team Gleason</i></a><i>.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "​Sequential Attention: Making AI models leaner and faster without sacrificing accuracy",
      "link": "https://research.google/blog/sequential-attention-making-ai-models-leaner-and-faster-without-sacrificing-accuracy/",
      "pubDate": "Tue, 03 Feb 2026 16:00:00 GMT",
      "isoDate": "2026-02-03T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "​Sequential Attention: Making AI models leaner and faster without sacrificing accuracy",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SequentialAttention2_FeatureSelection.width-800.gif",
          "alt": "SequentialAttention2_FeatureSelection",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SequentialAttention4_Results.width-1250.png",
          "alt": "SequentialAttention4_Results",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SequentialAttention3_MatrixSparsification.width-800.gif",
          "alt": "SequentialAttention3_MatrixSparsification",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3dhkr\">Feature selection is the process of identifying and retaining the most informative subset of input variables while discarding irrelevant or redundant noise. A fundamental challenge in both machine learning and deep learning, <a href=\"https://www.geeksforgeeks.org/machine-learning/feature-selection-techniques-in-machine-learning/\" target=\"_blank\" rel=\"noopener noreferrer\">feature selection</a> is <a href=\"https://en.wikipedia.org/wiki/NP_%28complexity%29\" target=\"_blank\" rel=\"noopener noreferrer\">NP-hard</a> (i.e., a problem that is mathematically \"impossible\" to solve perfectly and quickly for large groups of data), and as such, it remains a highly challenging area of research.</p><p data-block-key=\"3gp2m\">In modern deep neural networks, feature selection is further complicated by intricate non-linear feature interactions. A feature may appear statistically insignificant on its own but become critical when combined with others within the network's non-linear layers. Conversely, a feature’s contribution may appear significant in isolation, but made redundant when taking other features into account. The core challenge lies in identifying essential features for retention while effectively pruning redundancy within complex model architectures.</p><p data-block-key=\"3vv3j\">More broadly, many ML optimization tasks can be cast as <i>subset selection</i> problems, of which feature selection is a special case. For example, embedding <i>dimension tuning</i> can be viewed as selecting a subset of embedding chunks, and <i>weight pruning</i> as selecting a subset of entries from the weight matrix. Therefore devising a general solution for the subset selection problem that is applicable to modern deep learning tasks can be highly impactful for building the most efficient models.</p><p data-block-key=\"balrn\">Today, we explore our solution to the subset selection problem, called <a href=\"https://arxiv.org/abs/2209.14881\" target=\"_blank\" rel=\"noopener noreferrer\">Sequential Attention</a>. Sequential Attention uses a greedy selection mechanism to sequentially and adaptively select the best next component (like a layer, block, or feature) to add to the model. While adaptive greedy algorithms are known to provide strong guarantees for various subset selection problems, such as <a href=\"https://en.wikipedia.org/wiki/Submodular_set_function\" target=\"_blank\" rel=\"noopener noreferrer\">submodular optimization</a>, naïvely applying such algorithms would increase the training cost by many orders of magnitude. To tackle this scalability issue, we integrate selection directly into the model training process by performing selection within a single model training. This ensures that Sequential Attention can be applied to large scale ML models with minimal overhead without sacrificing accuracy or complexity. Here we will analyze how Sequential Attention works and show how it’s being used in real-world scenarios to optimize the structure of deep learning models.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"3dhkr\">How Sequential Attention works</h2><p data-block-key=\"afpt7\">Sequential Attention leverages the weighting power of the attention mechanism to build a subset step-by-step. In contrast to standard \"one-shot\" attention, in which all candidates are weighted simultaneously, Sequential Attention addresses the <a href=\"https://en.wikipedia.org/wiki/NP_(complexity)\" target=\"_blank\" rel=\"noopener noreferrer\">NP-hard</a> nature of subset selection by treating it as a sequential decision process. This is particularly effective for identifying high-order non-linear interactions often missed by \"<a href=\"https://www.geeksforgeeks.org/machine-learning/feature-selection-filter-methods/\" target=\"_blank\" rel=\"noopener noreferrer\">filter methods</a>”, which provide the simplest way to pick a subset by focusing only on the merits of each individual item.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SequentialAttention2_FeatureSelection.width-800.gif\" alt=\"SequentialAttention2_FeatureSelection\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SequentialAttention2_FeatureSelection.width-800.gif\" alt=\"SequentialAttention2_FeatureSelection\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"kktut\"><i>Feature Selection in neural networks: Selective pruning of input features to optimize performance. By \"switching off\" low-utility features, the model simplifies the learning task and reduces the risk of overfitting.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3dhkr\">The core idea is to maintain a set of <i>selected</i> candidates and use them as context to find the next most informative candidate to select. This is achieved through two primary methods: greedy selection, which allows the model to make a locally optimal decision around which element to include at each step, and importance evaluation, which uses “attention scores” (numerical values indicating the importance or relevance of different input parts) to quantify the importance of every candidate in addition to the currently selected candidates. Like the <a href=\"https://www.geeksforgeeks.org/artificial-intelligence/ml-attention-mechanism/\" target=\"_blank\" rel=\"noopener noreferrer\">attention mechanism</a>, Sequential Attention uses <a href=\"https://en.wikipedia.org/wiki/Softmax_function\" target=\"_blank\" rel=\"noopener noreferrer\">softmax</a> as an importance ranking of different components. Yet unlike the attention mechanism, it works sequentially as opposed to one shot, allowing the selection algorithm to adapt to previous selections — a crucial property for high-quality importance ranking.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"3dhkr\">Sequential Attention benefits</h2><p data-block-key=\"6j2le\">The primary benefits of Sequential Attention are:</p><ul><li data-block-key=\"9r532\"><i>Efficiency and accuracy</i>: By allowing parallel processing of the candidates (once the attention scores are calculated), they can be evaluated faster than in traditional sequential selection.</li><li data-block-key=\"dk9cl\"><i>Interpretability</i>: The attention scores themselves offer a powerful diagnostic tool. Researchers can inspect the attention scores to see exactly which parts of the input a model prioritized when making a specific decision or generating a specific token. This makes the model's internal reasoning more interpretable than that of a black-box model.</li><li data-block-key=\"c5jfe\"><i>Scalability</i>: The ability to efficiently handle a large number of candidates is crucial for large-scale feature selection for modern neural networks.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"3dhkr\">Sequential Attention in action</h2><h3 data-block-key=\"57p1k\">Feature selection</h3><p data-block-key=\"btm7j\">The standard feature selection method, i.e., greedy selection, is computationally expensive, as it requires re-training or re-evaluating the model for every potential feature at every step. In “<a href=\"https://arxiv.org/abs/2209.14881\" target=\"_blank\" rel=\"noopener noreferrer\">Sequential Attention for Feature Selection</a>”, we sought to replace this costly method with a much cheaper proxy: the model’s internal attention weights.</p><p data-block-key=\"b4b64\">At each step, the Sequential Attention algorithm calculates attention weights for all remaining, unselected features, and permanently adds the feature with the highest attention score (the one to which the model is \"paying the most attention\") to the subset. The algorithm then re-runs the selection process (the process of feeding input data through a neural network, layer by layer, from input to output, to generate a prediction) and re-calculates the attention weights for the remaining features. This recalculation naturally reflects the marginal gain (how much a feature contributes to performance, given the features already selected), allowing the model to effectively identify and avoid adding redundant features.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SequentialAttention4_Results.width-1250.png\" alt=\"SequentialAttention4_Results\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SequentialAttention4_Results.width-1250.png\" alt=\"SequentialAttention4_Results\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"hx2xe\"><i>Feature selection performance. Prediction accuracy of the proposed approach (orange) vs. baselines. Our method achieves competitive or leading results across proteomics, image, and activity recognition benchmarks, confirming its robustness.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3dhkr\">The Sequential Attention algorithm achieved state-of-the-art results across <a href=\"https://arxiv.org/abs/2209.14881\" target=\"_blank\" rel=\"noopener noreferrer\">several neural network benchmarks</a>. Notably, it drastically improved efficiency, enabling a fast, one-pass implementation of greedy selection without the need for expensive, explicit marginal gain calculations. The study also demonstrated that when applied to a simple <a href=\"https://en.wikipedia.org/wiki/Linear_regression\" target=\"_blank\" rel=\"noopener noreferrer\">linear regression</a> model, the Sequential Attention algorithm is mathematically equivalent to the established <a href=\"https://angms.science/doc/RM/OMP.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Orthogonal Matching Pursuit</a> (OMP) algorithm. This equivalence is critical because OMP comes with provable guarantees of reliability and performance.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"3dhkr\">Block sparsification</h3><p data-block-key=\"heh9\">Neural network <a href=\"https://en.wikipedia.org/wiki/Pruning_(artificial_neural_network)\" target=\"_blank\" rel=\"noopener noreferrer\">pruning</a> is essential for deploying large models efficiently because it reduces the model size by removing unnecessary weights. Prior research pursued two largely separate paths: differentiable pruning, which uses trainable parameters as proxies for importance, and combinatorial optimization, which uses algorithms to search for the best sparse structure.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SequentialAttention3_MatrixSparsification.width-800.gif\" alt=\"SequentialAttention3_MatrixSparsification\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SequentialAttention3_MatrixSparsification.width-800.gif\" alt=\"SequentialAttention3_MatrixSparsification\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"kktut\"><i>Matrix block sparsification: Identifying and zeroing out non-essential parameter blocks to optimize memory and speed. Unlike unstructured pruning, block-based sparsity leverages hardware acceleration for superior inference performance.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"078sl\">In “<a href=\"https://arxiv.org/abs/2402.17902\" target=\"_blank\" rel=\"noopener noreferrer\">SequentialAttention++ for Block Sparsification: Differentiable Pruning Meets Combinatorial Optimization</a>”, we sought to unite these two approaches into a coherent framework for structured neural network pruning that removes entire blocks or channels of weights to achieve real-world improvements on hardware accelerators like GPUs and TPUs.</p><p data-block-key=\"55ihf\">The resulting algorithm, SequentialAttention++, provides a new way to discover the most important blocks of weight matrices, and shows significant gains in model compression and efficiency without sacrificing accuracy in ML tasks, e.g., <a href=\"https://www.image-net.org/\" target=\"_blank\" rel=\"noopener noreferrer\">ImageNet</a> classification.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"3dhkr\">The future of sequential attention</h2><p data-block-key=\"61dcf\">As the increasing integration of AI models in science, engineering and business makes model efficiency more relevant than ever, model structure optimization is crucial for building highly effective yet efficient models. We have identified subset selection as a fundamental challenge related to model efficiency across various deep learning optimization tasks, and Sequential Attention has emerged as a pivotal technique for addressing these problems. Moving forward, we aim to extend the applications of subset selection to increasingly complex domains.</p><h3 data-block-key=\"9kjlc\">Feature engineering with real constraints</h3><p data-block-key=\"ag3bq\">Sequential Attention has demonstrated significant quality gains and efficiency savings in optimizing the feature embedding layer in large embedding models (LEMs) used in recommender systems. These models typically have a large number of heterogeneous features with large embedding tables, and so the tasks of feature selection/pruning, feature cross search and embedding dimension optimization are highly impactful. In the future, we would like to allow these feature engineering tasks to take real inference constraints into account, enabling fully automated, continual feature engineering.</p><h3 data-block-key=\"d77k3\">Large language model (LLM) pruning</h3><p data-block-key=\"a3oje\">The SequentialAttention++ paradigm is a promising direction for LLM pruning. By applying this framework we can enforce structured sparsity (e.g., block sparsity), prune redundant attention heads, embedding dimensions or entire transformer blocks, and significantly reduce model footprint and inference latency while preserving predictive performance.</p><h3 data-block-key=\"fbcac\">Drug discovery and genomics</h3><p data-block-key=\"3a5d9\">Feature selection is vital in the biological sciences. Sequential Attention can be adapted to efficiently extract influential genetic or chemical features from high-dimensional datasets, enhancing both the interpretability and accuracy of models in drug discovery and personalized medicine.</p><p data-block-key=\"6s7lm\">Current research focuses on scaling Sequential Attention to handle massive datasets and highly complex architectures more efficiently. Furthermore, ongoing efforts seek to identify superior pruned model structures and extend rigorous mathematical guarantees to real-world deep learning applications, solidifying the framework’s reliability across industries.</p><p data-block-key=\"3tru1\">Subset selection is a core problem central to multiple optimization tasks in deep learning, while Sequential Attention is a key technique to solve these problems. In the future, we will explore more applications of subset selection to solve more challenging problems in broader domains</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"3dhkr\">Conclusion</h2><p data-block-key=\"5p8o0\">Sequential Attention is an effective technique for multiple large-scale subset selection problems in deep learning and plays a key role in model architecture optimization. As these techniques evolve, they will solidify the future of machine learning, guaranteeing that powerful AI remains both accurate and accessible for years to come.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"3dhkr\">Acknowledgements</h2><p data-block-key=\"fipmc\"><i>We would like to express our gratitude to our research collaborators, Taisuke Yasuda, Lin Chen, Matthew Fahrbach, MohammadHossein Bateni, and Vahab Mirrokni, whose efforts have advanced the development of Sequential Attention</i><b><i>.</i></b> <i>This work builds upon fundamental research in differentiable subset selection and combinatorial optimization to create more efficient and accessible AI models.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Collaborating on a nationwide randomized study of AI in real-world virtual care",
      "link": "https://research.google/blog/collaborating-on-a-nationwide-randomized-study-of-ai-in-real-world-virtual-care/",
      "pubDate": "Mon, 02 Feb 2026 16:00:00 GMT",
      "isoDate": "2026-02-02T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Collaborating on a nationwide randomized study of AI in real-world virtual care",
      "images": [],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ze93b\">AI systems capable of clinical reasoning and dialogue have the potential to dramatically increase access to medical expertise and care while giving physicians back time with their patients where it truly matters. However, developing these technologies responsibly requires a rigorous, evidence-based approach. Over the past few years, our teams have explored the \"art of the possible\" through research systems that demonstrate clinician-level capabilities in simulated settings. While we have begun <a href=\"https://research.google/blog/advancing-amie-towards-specialist-care-and-real-world-validation/\">testing the safety and feasibility</a> of these systems in clinical settings, moving to the next stage of assessing these systems requires additional rigor and scale. It involves studying the utility and impact of AI in virtual care involving more patients across an array of geographies and conditions and with controlled comparisons.</p><p data-block-key=\"asvhc\">Today, we are announcing a significant step in that ongoing research journey: In partnership with <a href=\"https://includedhealth.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Included Health</a>, a leading US healthcare provider, we will be launching, pending Institutional Review Board (IRB) approval, a prospective consented nationwide randomized study to assess AI in a real-world virtual care setting. This new research will build upon our foundational research on the use of AI for diagnostic and management reasoning, personalized health insights and navigating health information.</p><p data-block-key=\"90j09\">This work represents a significant evolution in our research. Early studies published in <a href=\"http://nature.com/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature</i></a> first assessed our AI system’s <a href=\"https://www.nature.com/articles/s41586-025-08869-4\" target=\"_blank\" rel=\"noopener noreferrer\">diagnostic reasoning capabilities</a>, including its assistive effect for physicians. We then compared the system’s <a href=\"https://www.nature.com/articles/s41586-025-08866-7\" target=\"_blank\" rel=\"noopener noreferrer\">conversational diagnostic capabilities</a> to those of primary care physicians in <a href=\"https://research.google/blog/amie-a-research-ai-system-for-diagnostic-medical-reasoning-and-conversations/\">simulated settings with patient actors</a>. In addition to understanding capabilities, we also explored a physician-centered paradigm with <a href=\"https://research.google/blog/enabling-physician-centered-oversight-for-amie/\">asynchronous oversight of AI</a>. Our initial step toward testing conversational AI in real-world clinical settings was a <a href=\"https://research.google/blog/advancing-amie-towards-specialist-care-and-real-world-validation/\">single-center feasibility study</a> in partnership with <a href=\"https://bidmc.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Beth Israel Deaconess Medical Center</a>. The study’s goal was to demonstrate the system’s safety based on <a href=\"https://clinicaltrials.gov/study/NCT06911398#whatStudyMeasure\" target=\"_blank\" rel=\"noopener noreferrer\">outcome measures</a> like the number of interruptions by the safety supervisor in response to safety concerns. We have observed strong indications of safety in this initial study and look forward to sharing results when complete.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/ATS_Blog_Visual_1.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n\n      </div>\n    \n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"m8u0p\"><i>Google is partnering with Included Health, a leading US healthcare provider on a nationwide randomized study to assess AI in real-world virtual care settings.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Evaluation at scale: A nationwide study with Included Health</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ze93b\">Our new study will move beyond feasibility to use a <a href=\"https://en.wikipedia.org/wiki/Randomized_controlled_trial\" target=\"_blank\" rel=\"noopener noreferrer\">randomized controlled trial</a> setup with consented participants recruited nationwide. By gathering robust evidence at this scale, we aim to better understand the capabilities and limitations of our AI for managing patient interactions in real-world virtual care workflows compared to standard clinical practice, for real patients and concerns.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/ATS_Blog_Visual_2-final.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n\n      </div>\n    \n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"m8u0p\"><i>We take a responsible approach to characterizing the helpfulness and safety of conversational medical AI.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ze93b\">This phased approach to studying conversational AI in health ensures that as the stages of research proceed, more data becomes available about the patient and clinician experience, safety and usefulness of the AI system which will guide subsequent innovation responsibly. We believe that a responsible approach to conversational AI in health settings should adopt high standards of evidence generation, similar to other interventions in medicine. This is a crucial step towards ensuring that AI can be deployed safely in healthcare while building trust with patients and care teams.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Building on our existing foundation of rigorous research</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ze93b\">This study is informed by years of foundational research across Google, where we have systematically investigated the capabilities required for a helpful and safe medical AI.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Diagnostic and management reasoning</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ze93b\">We began by tackling the core challenge of the medical interview with <a href=\"https://research.google/blog/amie-a-research-ai-system-for-diagnostic-medical-reasoning-and-conversations/\">AMIE</a>. Our research with patient actors and synthetic clinical scenarios demonstrated that an AI system trained with simulated self-play could match or exceed primary care physicians in diagnostic accuracy and conversation quality during simulated consultations. We further advanced these capabilities to support <a href=\"https://research.google/blog/from-diagnosis-to-treatment-advancing-amie-for-longitudinal-disease-management/\">longitudinal disease management</a>, equipping the system to reason over clinical guidelines and patient history to plan investigations and treatments, as well as reasoning through <a href=\"https://research.google/blog/amie-gains-vision-a-research-ai-agent-for-multi-modal-diagnostic-dialogue/\">multimodal</a> evidence.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Personalized health insights</h3>\n            \n        \n        \n            <div class=\" component-intro__description --has-heading\">\n                <p data-block-key=\"ncr28\">Recognizing that health extends beyond the clinic, we also investigated how AI can reason over personal health data through our retrospective research on the <a href=\"https://research.google/blog/the-anatomy-of-a-personal-health-agent/\">Personal Health Agent</a> (PHA). This research explored how multimodal models could analyze sleep patterns and activity metrics from wearables to provide personalized coaching and insights. By using a collaborative multi-agent architecture, our PHA demonstrated how AI can act as a data scientist, domain expert, and health coach all in one, capabilities that are essential for understanding a patient's full health context. These insights also informed experiments in Fitbit Labs, such as the <a href=\"https://community.fitbit.com/t5/The-Pulse-Fitbit-Community-Blog/See-what-s-new-for-Fitbit-Labs/ba-p/5750673\" target=\"_blank\" rel=\"noopener noreferrer\">Symptom Checker and Medical Records Navigator</a> and <a href=\"https://community.fitbit.com/t5/The-Pulse-Fitbit-Community-Blog/Introducing-the-new-Plan-for-Care-Fitbit-Lab/ba-p/5796289\" target=\"_blank\" rel=\"noopener noreferrer\">Plan for Care</a>, which help us understand how users access personalized support when assessing symptoms at home and preparing for an upcoming doctor’s visit.</p>\n            </div>\n        \n    </div>\n\n\n\n    \n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Navigating health information</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ze93b\">To support people in their search for health information online, we demonstrated how a novel <a href=\"https://research.google/blog/towards-better-health-conversations-research-insights-on-a-wayfinding-ai-agent-based-on-gemini/\">“wayfinding” AI</a> agent helps people find better information through proactive conversational guidance, goal understanding, and tailored conversations. This stream of research has provided critical insights into how to structure AI interactions that are clear, helpful, and grounded in the practical realities of a health journey.</p><p data-block-key=\"f34ce\">These distinct threads of research — diagnostic and management reasoning, personal health insights, and navigating health information — have laid the groundwork for the AI system we are examining in this and future studies. By moving from demonstrating the art of the possible in the lab to studying AI systems at scale in the real world, we are taking a critical step toward making high-quality care accessible to everyone through frontier models of medical intelligence.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/ATS_Blog_Visual_3.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n\n      </div>\n    \n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"m8u0p\"><i>Google has conducted foundational research on conversational AI in various domains including diagnostic and management reasoning (AMIE), personalized health insights (PHA), and navigating health information (Wayfinding AI).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ze93b\">The upcoming launch of this nationwide randomized study, in partnership with Included Health, will mark a significant step forward in the assessment of our conversational AI in healthcare. By moving from simulated settings and small-scale feasibility studies to this large-scale, real-world nationwide randomized study, we are establishing a new, high standard of evidence generation for medical AI. Our goal is to rigorously understand how AI systems, drawing from foundational research like AMIE, PHA, and Wayfinding AI, can be safe and helpful in virtual care workflows for real patients and concerns. This phased, evidence-based approach is essential for ensuring that high-quality, AI-powered care can be developed safely and responsibly to increase access to medical expertise for everyone.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ze93b\"><i>We are grateful for the partnership with</i> <a href=\"https://includedhealth.com/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Included Health</i></a><i>. The study is a joint effort across many teams at Google, including</i> <a href=\"https://research.google/\"><i>Google Research</i></a><i>,</i> <a href=\"https://deepmind.google/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Google DeepMind</i></a><i>,</i> <a href=\"https://blog.google/products/platforms-devices/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Google Platforms and Devices</i></a><i>, and</i> <a href=\"https://health.google/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Google for Health</i></a><i>.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Towards a science of scaling agent systems: When and why agent systems work",
      "link": "https://research.google/blog/towards-a-science-of-scaling-agent-systems-when-and-why-agent-systems-work/",
      "pubDate": "Tue, 27 Jan 2026 16:00:00 GMT",
      "isoDate": "2026-01-27T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Towards a science of scaling agent systems: When and why agent systems work",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling1_Summary.width-1250.png",
          "alt": "AgentScaling2_Comparison",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling2_Comparison.width-1250.png",
          "alt": "AgentScaling2_Comparison",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling3_TaskPerformanceHERO.width-1250.png",
          "alt": "AgentScaling3_TaskPerformanceHERO",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling4_Reliability.width-1250.png",
          "alt": "AgentScaling4_Reliability",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"os8s1\">AI agents — systems capable of reasoning, planning, and acting — are becoming a common paradigm for real-world AI applications. From <a href=\"https://codeassist.google/\" target=\"_blank\" rel=\"noopener noreferrer\">coding assistants</a> to <a href=\"https://research.google/blog/the-anatomy-of-a-personal-health-agent/\">personal health coaches</a>, the industry is shifting from single-shot question answering to sustained, multi-step interactions. While researchers have long utilized established metrics to optimize the accuracy of traditional machine learning models, agents introduce a new layer of complexity. Unlike isolated predictions, agents must navigate sustained, multi-step interactions where a single error can cascade throughout a workflow. This shift compels us to look beyond standard accuracy and ask: How do we actually design these systems for optimal performance?</p><p data-block-key=\"9s91h\">Practitioners often rely on heuristics, such as the assumption that \"<a href=\"https://arxiv.org/abs/2402.05120\" target=\"_blank\" rel=\"noopener noreferrer\">more agents are better</a>\", believing that adding specialized agents will consistently improve results. For example, \"<a href=\"https://arxiv.org/abs/2402.05120\" target=\"_blank\" rel=\"noopener noreferrer\">More Agents Is All You Need</a>\" reported that LLM performance scales with agent count, while <a href=\"https://arxiv.org/abs/2406.07155\" target=\"_blank\" rel=\"noopener noreferrer\">collaborative scaling research</a> found that multi-agent collaboration \"...often surpasses each individual through collective reasoning.\"</p><p data-block-key=\"edjau\">In our new paper, “<a href=\"https://arxiv.org/abs/2512.08296\" target=\"_blank\" rel=\"noopener noreferrer\">Towards a Science of Scaling Agent Systems</a>”, we challenge this assumption. Through a large-scale controlled evaluation of 180 agent configurations, we derive the first quantitative scaling principles for agent systems, revealing that the \"more agents\" approach often hits a ceiling, and can even degrade performance if not aligned with the specific properties of the task.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"os8s1\">Defining \"agentic\" evaluation</h2><p data-block-key=\"1vpgq\">To understand how agents scale, we first defined what makes a task \"agentic\". Traditional static benchmarks measure a model's knowledge, but they don't capture the complexities of deployment. We argue that <i>agentic</i> tasks require three specific properties:</p><ol><li data-block-key=\"4lsio\"><i>Sustained multi-step interactions</i> with an external environment.</li><li data-block-key=\"7eqp0\"><i>Iterative information gathering</i> under partial observability.</li><li data-block-key=\"8vikn\"><i>Adaptive strategy refinement</i> based on environmental feedback.</li></ol><p data-block-key=\"5r5do\">We evaluated five canonical architectures: one single-agent system (SAS) and four multi-agent variants (independent, centralized, decentralized, and hybrid) across four diverse benchmarks, including <a href=\"https://www.vals.ai/benchmarks/finance_agent\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Finance-Agent</i></a> (financial reasoning), <a href=\"https://arxiv.org/pdf/2508.06600\" target=\"_blank\" rel=\"noopener noreferrer\"><i>BrowseComp-Plus</i></a> (web navigation), <a href=\"https://arxiv.org/abs/2412.21033\" target=\"_blank\" rel=\"noopener noreferrer\"><i>PlanCraft</i></a> (planning), and <a href=\"https://arxiv.org/abs/2405.00823\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Workbench</i></a> (tool use). The agent architectures are defined as follow:</p><ul><li data-block-key=\"crmfg\"><i>Single-Agent (SAS):</i> A solitary agent executing all reasoning and acting steps sequentially with a unified memory stream.</li><li data-block-key=\"6uuhv\"><i>Independent:</i> Multiple agents working in parallel on sub-tasks without communicating, aggregating results only at the end.</li><li data-block-key=\"bvudi\"><i>Centralized:</i> A \"hub-and-spoke\" model where a central orchestrator delegates tasks to workers and synthesizes their outputs.</li><li data-block-key=\"dp37a\"><i>Decentralized:</i> A peer-to-peer mesh where agents communicate directly with one another to share information and reach consensus.</li><li data-block-key=\"4s9bf\"><i>Hybrid:</i> A combination of hierarchical oversight and peer-to-peer coordination to balance central control with flexible execution.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling1_Summary.width-1250.png\" alt=\"AgentScaling2_Comparison\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling1_Summary.width-1250.png\" alt=\"AgentScaling2_Comparison\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"f1omg\"><i>Summary of the five canonical agent architectures evaluated in this study, including their computational complexity, communication overhead, and coordination mechanisms.</i> k<i> = max iterations per agent,</i> n<i> = number of agents,</i> r<i> = orchestrator rounds,</i> d<i> = debate rounds,</i> p<i> = peer communication rounds,</i> m<i> = average peer requests per round. Communication overhead counts inter-agent message exchanges. Independent offers maximal parallelization with minimal coordination. Decentralized uses sequential debate rounds. Hybrid combined orchestrator control with directed peer communication.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"os8s1\">Results: The myth of \"more agents\"</h2><p data-block-key=\"ff941\">To quantify the impact of model capabilities on agent performance, we evaluated our architectures across three leading model families: OpenAI GPT, Google Gemini, and Anthropic Claude. The results reveal a complex relationship between model capabilities and coordination strategy. As shown in the figure below, while performance generally trends upward with more capable models, multi-agent systems are not a universal solution — they can either significantly boost or unexpectedly degrade performance depending on the specific configuration.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling2_Comparison.width-1250.png\" alt=\"AgentScaling2_Comparison\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling2_Comparison.width-1250.png\" alt=\"AgentScaling2_Comparison\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"f1omg\"><i>Performance comparison across three major model families (OpenAI GPT, Google Gemini, Anthropic Claude) showing how different agent architectures scale with model intelligence, where multi-agent systems can either boost or degrade performance depending on the configuration.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"os8s1\">The results below compare the performance of the five architectures across different domains, such as web browsing and financial analysis. The box plots represent the accuracy distribution for each approach, while the percentages indicate the relative improvement (or decline) of multi-agent teams compared to the single-agent baseline. This data highlights that while adding agents can drive massive gains in parallelizable tasks, it can often lead to diminishing returns — or even performance drops — in more sequential workflows.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling3_TaskPerformanceHERO.width-1250.png\" alt=\"AgentScaling3_TaskPerformanceHERO\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling3_TaskPerformanceHERO.width-1250.png\" alt=\"AgentScaling3_TaskPerformanceHERO\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"f1omg\"><i>Task-specific performance showing that multi-agent coordination yields substantial gains on parallelizable tasks like Finance-Agent (+81%) while degrading performance on sequential tasks like PlanCraft (-70%).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"os8s1\">The alignment principle</h3><p data-block-key=\"44n34\">On parallelizable tasks like financial reasoning (e.g., distinct agents can simultaneously analyze revenue trends, cost structures, and market comparisons), centralized coordination improved performance by 80.9% over a single agent. The ability to decompose complex problems into sub-tasks allowed agents to work more effectively.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"os8s1\">The sequential penalty</h3><p data-block-key=\"4h7i0\">Conversely, on tasks requiring strict sequential reasoning (like planning in PlanCraft), every multi-agent variant we tested degraded performance by 39-70%. In these scenarios, the overhead of communication fragmented the reasoning process, leaving insufficient \"cognitive budget\" for the actual task.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"os8s1\">The tool-use bottleneck</h3><p data-block-key=\"7l2nm\">We identified a \"tool-coordination trade-off\". As tasks require more tools (e.g., a coding agent with access to 16+ tools), the \"tax\" of coordinating multiple agents increases disproportionately.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"os8s1\">Architecture as a safety feature</h2><p data-block-key=\"ffooe\">Perhaps most important for real-world deployment, we found a relationship between architecture and reliability. We measured <i>error amplification</i>, the rate at which a mistake by one agent propagates to the final result.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling4_Reliability.width-1250.png\" alt=\"AgentScaling4_Reliability\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling4_Reliability.width-1250.png\" alt=\"AgentScaling4_Reliability\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"f1omg\"><i>Comprehensive metrics across architectures reveal that centralized systems achieve the best balance between success rate and error containment, while independent multi-agent systems amplify errors by up to 17.2x.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"os8s1\">We found that independent multi-agent systems (agents working in parallel without talking) amplified errors by 17.2x. Without a mechanism to check each other's work, errors cascaded unchecked. Centralized systems (with an orchestrator) contained this amplification to just 4.4x. The orchestrator effectively acts as a \"validation bottleneck\", catching errors before they propagate.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"os8s1\">A predictive model for agent design</h2><p data-block-key=\"3kmqj\">Moving beyond retrospection, we developed a predictive model (<i>R</i>^2 = 0.513) that uses measurable task properties like tool count and decomposability to predict which architecture will perform best. This model correctly identifies the optimal coordination strategy for 87% of unseen task configurations.</p><p data-block-key=\"9vrri\">This suggests we are moving toward a new science of agent scaling. Instead of guessing whether to use a swarm of agents or a single powerful model, developers can now look at the properties of their task, specifically its sequential dependencies and tool density, to make principled engineering decisions.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"os8s1\">Conclusion</h2><p data-block-key=\"2bha4\">As foundational models like Gemini continue to advance, our research suggests that smarter models don't replace the need for multi-agent systems, they accelerate it, but only when the architecture is right. By moving from heuristics to quantitative principles, we can build the next generation of AI agents that are not just more numerous, but smarter, safer, and more efficient.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"os8s1\">Acknowledgements</h2><p data-block-key=\"eblfk\"><i>We would like to thank our co-authors and collaborators from Google Research, Google DeepMind, and academia for their contributions to this work.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "ATLAS: Practical scaling laws for multilingual models",
      "link": "https://research.google/blog/atlas-practical-scaling-laws-for-multilingual-models/",
      "pubDate": "Mon, 26 Jan 2026 16:00:00 GMT",
      "isoDate": "2026-01-26T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "ATLAS: Practical scaling laws for multilingual models",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-1.width-1250.png",
          "alt": "ATLAS-1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-2.width-1250.png",
          "alt": "ATLAS-2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-3.width-1250.png",
          "alt": "ATLAS-3",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-4.width-1250.png",
          "alt": "ATLAS-4",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"dni7s\">Over 50% of AI model users <a href=\"https://www.anthropic.com/research/anthropic-economic-index-september-2025-report\" target=\"_blank\" rel=\"noopener noreferrer\">speak non-English languages</a>, yet publicly accessible scaling laws are overwhelmingly focused on the English language. This imbalance creates a critical gap in public research, leaving model builders, tasked with serving billions of international and multilingual users, without data-driven guidance for key development decisions about efficiency, quality, and cost when building for non-English languages or with specific language mixtures.</p><p data-block-key=\"79l5e\">In “<a href=\"https://arxiv.org/pdf/2510.22037\" target=\"_blank\" rel=\"noopener noreferrer\">ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality</a>”, to be presented at <a href=\"https://iclr.cc/\" target=\"_blank\" rel=\"noopener noreferrer\">ICLR 2026</a>, we aim to address this gap. We present the largest public multilingual pre-training study to date, spanning 774 training runs across 10M–8B parameter models. It includes data spanning 400+ languages and evaluations in 48 languages. As a result of this study, we estimate the synergies between 1,400 pairs of languages, and introduce adaptive transfer scaling laws (ATLAS) for building multilingual models that enable practitioners to efficiently balance the mix of languages in training data with model size.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">ATLAS: A single scaling law that adapts to multilingual mixtures</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"dni7s\">ATLAS is a simple, practical approach to determining optimal model size, data volume, and language mixtures for training. Unlike traditional scaling laws that focus on monolingual settings, ATLAS provides these recommendations for more complex, multilingual environments. It specifically optimizes performance on a target language (e.g., Catalan) by leveraging data from multiple different languages. ATLAS extends these traditional scaling law principles through three components:</p><ul><li data-block-key=\"3v48v\">A cross-lingual transfer matrix used to identify which languages are best to train together</li><li data-block-key=\"cq19c\">A scaling law that provides guidance on efficiently expanding model size and data as the number of supported languages increases</li><li data-block-key=\"8283i\">Rules for deciding when to pre-train a model from scratch versus fine-tuning from a multilingual checkpoint</li></ul><p data-block-key=\"7dfi6\">ATLAS accomplishes this by training on hundreds of multilingual experiments (using the <a href=\"https://arxiv.org/pdf/2309.04662\" target=\"_blank\" rel=\"noopener noreferrer\">MADLAD-400</a> corpus with over 750 runs across 400+ languages) and accounting for three distinct data sources: 1) the target language, 2) similar transfer languages according to empirical analysis (e.g., Catalan might include Latin languages like Spanish, Portuguese, and Italian), and 3) all other languages. This novel approach enables the law to learn how much each source actually helps or hinders the target language, a capability prior laws did not support.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Evaluation</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"dni7s\">We used the MADLAD-400 dataset to evaluate how well ATLAS predicts a model’s performance on new model sizes, varying amounts of training data<i>,</i> or new language mixtures<i>.</i> To do this, we measure performance using a <a href=\"https://arxiv.org/pdf/2407.13623\" target=\"_blank\" rel=\"noopener noreferrer\">vocabulary-insensitive loss</a> across over 750 independent runs in monolingual, bilingual, and massively multilingual settings. Our evaluations show that ATLAS consistently outperforms prior work.</p><p data-block-key=\"4iauh\">For six languages — English (EN), French (FR), Russian (RU), Chinese (ZH), Hindi (HI), and Swahili (SW) — we analyzed how ATLAS predicted the optimal model size (<i>N</i>) and data size (<i>D</i>) should be scaled. When we compared these optimal scaling trajectories across languages, we made two observations. The curves look strikingly similar, but training with a multilingual vocabulary or fully multilingual data comes with a compute-efficiency tax — especially for English. Low-resource languages show upward bends as they run out of data, and the model struggles to learn from data repetition. ATLAS explicitly models these effects.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-1.width-1250.png\" alt=\"ATLAS-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-1.width-1250.png\" alt=\"ATLAS-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"fdspk\"><i>These charts show the optimal scaling trajectories (model size (</i><b><i>N</i></b><i>) and data size (</i><b><i>D</i></b><i>) determined by ATLAS for each language and model type. The lines represent three configurations:</i> <b><i>Solid</i></b><i> (monolingual vocab/data),</i> <b><i>Dashed</i></b><i> (multilingual vocab/monolingual data), and</i> <b><i>Dotted</i></b><i> (multilingual vocab/multilingual data). The dotted lines are consistently highest, indicating that training with a full multilingual setup requires slightly more compute for the same quality.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">The cross-lingual transfer map</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"dni7s\">Next, we measured language-to-language synergies and interference at scale, producing a matrix that quantifies how much training on language <i>A</i> helps (or hurts) language <i>B</i>. Our results show very intuitive results: Norwegian is helped primarily by Swedish and German, Malay by Indonesian, and Arabic by Hebrew. English, French, and Spanish are the most widely helpful languages with which to train, likely due to the inherent quality, heterogeneity, and quantity of text in these languages found on the web.</p><p data-block-key=\"9h723\">The analysis shows that the biggest predictor of positive transfer is sharing a script and/or language family (e.g., Latin script), statistically significant with p &lt; .001. English helps many, but not all, languages; and transfer isn’t always symmetric (<i>A</i> can help <i>B</i> more than <i>B</i> helps <i>A</i>). These measurements turn “hunches” into data-driven language mix choices.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-2.width-1250.png\" alt=\"ATLAS-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-2.width-1250.png\" alt=\"ATLAS-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"fdspk\"><i>This heatmap shows the cross-lingual transfer matrix, quantifying language-to-language synergies and inference.</i> <b><i>Red</i></b><i> indicates that a language helps and</i> <b><i>blue</i></b><i> indicates it hurts. Boxes highlight each target language’s top-5 helpers. Languages that share the same writing system (e.g., Latin script) are notably more synergistic.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Decoding the “curse of multilinguality” with clear scaling rules</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"dni7s\">The “curse of multilinguality” is a phenomenon where models trained on multiple languages see a decrease in performance with each new language due to limited model capacity. We formalize this problem with a scaling law that considers not just model size (<i>N</i>), and quantity of training data (<i>D</i>), but the number of languages in that data (<i>K</i>). Fitting this law to many experiments, we found that while adding languages brings a mild capacity tax, there is a high-degree of positive transfer. This means if we want to train a model to support twice as many languages (2·<i>K</i>) then we should increase model size by 1.18x, and total data by 1.66x. This equates to 83% of data in each of the 2K languages. Although there is less data per language, the positive synergies from learning on all of them means the capacity constraints that cause degradation to the performance are offset.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-3.width-1250.png\" alt=\"ATLAS-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-3.width-1250.png\" alt=\"ATLAS-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"fdspk\"><i>These curves show how much to increase model size</i> N<i> or data size</i> D<i>, when we scale from</i> K<i> to</i> rK<i> training languages. For instance, the blue solid-line curve shows all the possibilities for how much to increase</i> N<i> and/or</i> D<i> to achieve the same performance with 2</i>K<i> as with</i> K<i> languages. The dotted purple line shows the most computationally efficient way to increase</i> N<i> and</i> D<i>, as we increase</i> K<i>.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">When to pre-train vs. fine-tune a multilingual checkpoint</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"dni7s\">For ten languages, we compare two paths to get the best performing model: (a) pre-train from scratch on the target language or (b) fine-tune from a strong multilingual “<a href=\"https://arxiv.org/pdf/2304.09151\" target=\"_blank\" rel=\"noopener noreferrer\">Unimax</a>” checkpoint. Option (b) is likely to have the best performance with minimal additional compute, as the model is already pretty strong across languages. However, if the model can be trained for much longer, then option (a) can often yield better long-term results. Our goal is to find the crossover point between the two training curves, based on how much compute the model builder has to spend.</p><p data-block-key=\"51vic\">Our results show that fine-tuning wins early, but pre-training overtakes once you can afford enough tokens. In our runs, the crossover typically occurs between ~144B and 283B tokens (language-dependent) for models with 2B parameters. Next, we plotted the crossover point as a function of model size. This gives a concrete, budget-aware rule of thumb: if your token and compute budget is below the crossover point for your model size, start from a multilingual checkpoint; otherwise, pre-training from scratch will usually finish ahead. Note that exact thresholds depend on the base model and mixture.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-4.width-1250.png\" alt=\"ATLAS-4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-4.width-1250.png\" alt=\"ATLAS-4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"fdspk\"><i>As model size increases, the amount of compute (or training tokens since</i> C<i>=6</i>ND<i>) needed to hit the crossover point (where pre-training from scratch is better than fine-tuning) increases. We estimate a function for the crossover point based on model size. You can see how this estimated rule (</i><b><i>black line</i></b><i>) approximately fits the cross-over points found for the 10 plotted languages.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Try it yourself</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"dni7s\">By moving beyond English-centric scaling, ATLAS provides a roadmap for global model developers. It can be directly applied to scale language models beyond English by helping developers:</p><ul><li data-block-key=\"ci7bv\"><b>Planning to train a new multilingual or non-English model?</b> Use Figure 1 or Table C.1 from the <a href=\"https://arxiv.org/pdf/2510.22037\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a> to get a sense of the potential scaling laws based on vocabulary or training choices.</li><li data-block-key=\"6s6v1\"><b>Choosing a new training mix?</b> Consult the transfer matrix (Figure 2) to pick source languages that empirically help your targets — especially those sharing the same script/family.</li><li data-block-key=\"h95j\"><b>Training a new model with more languages?</b> Consult Section 5 to determine how to most efficiently expand your model size and data size to mitigate the effects of the curse of multilinguality.</li><li data-block-key=\"6rdle\"><b>Compute-constrained?</b> Consult Section 6 to decide if you should fine-tune a multilingual model or pre-train from scratch.</li></ul><p data-block-key=\"4mju\">We hope this work enables a new generation of multilingual models, serving billions of non-English speakers.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"8yqmj\"><i>We thank Luke Zettlemoyer, Catherine Arnett and Stella Biderman for helpful discussions on the paper. We thank Biao Zhang and Xavier Garcia for the technical discussions and feedback on early directions.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Introducing GIST: The next stage in smart sampling",
      "link": "https://research.google/blog/introducing-gist-the-next-stage-in-smart-sampling/",
      "pubDate": "Thu, 22 Jan 2026 16:00:00 GMT",
      "isoDate": "2026-01-22T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Introducing GIST: The next stage in smart sampling",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GIST-1a-Example.width-1250.png",
          "alt": "Diagram titled \"How GIST applies greedy selection while ensuring minimum diversity\" showing three purple circles with interconnected, numbered data nodes.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GIST-2-Performance.width-1250.png",
          "alt": "Line graph showing GIST methods achieving higher top-1 accuracy than random, margin, and k-center across cardinality constraint k.",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"vjebi\">Modern machine learning (ML) has unlocked unprecedented performance at the cost of having to process increasingly massive and complex datasets. From large language models (LLMs) to computer vision systems, there’s a common challenge: handling a massive amount of data that’s expensive to process.</p><p data-block-key=\"1n1iu\">This necessitates <i>subset selection</i> — the task of choosing a smaller, representative group of data points from the full dataset for the typical training task (not the fine-tuning). The question is then: how can we be sure this subset contains enough information to train an accurate model?</p><p data-block-key=\"ab14i\">At NeurIPS 2025, we introduced <a href=\"https://arxiv.org/abs/2405.18754\" target=\"_blank\" rel=\"noopener noreferrer\">Greedy Independent Set Thresholding</a> (GIST), a novel algorithm that helps solve this issue by balancing data “diversity” (ensuring the selected data is not redundant) and data “utility“ (data that is relevant and useful for the task). GIST not only outperforms state-of-the-art benchmarks tasks, such as image classification, but it does so with a mathematical guarantee about its solution quality.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"The conflict: Why smart sampling is hard\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The conflict: Why smart sampling is hard</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vjebi\">When selecting a subset of data, researchers must balance two often conflicting objectives: diversity and utility. Enforcing data diversity ensures the selected points aren’t redundant. Utility measures the overall usefulness or informational value of the selected subset.</p><p data-block-key=\"8o83b\">For diversity, we focus on maximizing the minimum distance (typically in embedding space) between any two selected data points, also known as <a href=\"https://pdfs.semanticscholar.org/f2ca/f56ff3270860daa8e60f50219588e8c62712.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">max-min diversity</a>. If you choose two data points that are very similar (e.g., two almost identical pictures of a golden retriever), your diversity is low. Max-min diversity forces you to select points that are all as far apart from each other as possible, minimizing redundancy and ensuring broad coverage of the data landscape. For utility, we focus on the class of <a href=\"https://en.wikipedia.org/wiki/Submodular_set_function\" target=\"_blank\" rel=\"noopener noreferrer\">monotone submodular functions</a>, which aim to maximize the total unique information covered by the subset.</p><p data-block-key=\"7k5ea\">The difficulty lies in combining these two goals. A pure max-min strategy might select diverse but ultimately irrelevant data points, while a pure utility strategy might select a tight, highly relevant cluster of redundant points. Finding a subset that is both maximally spread out and maximally informative is a complex combinatorial problem that is known to be <a href=\"https://en.wikipedia.org/wiki/NP-hardness\" target=\"_blank\" rel=\"noopener noreferrer\">NP-hard</a>, meaning that no algorithm can find the best solution efficiently, especially for massive datasets.</p><p data-block-key=\"2qj21\">This inherent conflict requires a clever <a href=\"https://en.wikipedia.org/wiki/Approximation_algorithm\" target=\"_blank\" rel=\"noopener noreferrer\">approximation</a> strategy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"How GIST works\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How GIST works</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vjebi\">Since finding a perfect subset is impractical, the goal shifts to finding an algorithm with a provable approximation guarantee — a mathematical safety net that assures the solution is always close to the true optimum. This is where GIST provides its groundbreaking solution.</p><p data-block-key=\"80lc8\">GIST breaks the diversity–utility challenge into a series of simpler, but related, optimization problems:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"1. Thresholding the diversity component\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">1. Thresholding the diversity component</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vjebi\">GIST starts by temporarily isolating the diversity component. Instead of trying to maximize the minimum distance between all points (the hard part), GIST tackles a simpler question: \"For a certain fixed minimum distance, what is the best subset of data we can select?\"</p><p data-block-key=\"foqdv\">By fixing the minimum required distance, GIST processes the data using a graph where two points are connected only if their distance is <i>less</i> than that specified distance. In this graph, any two connected points are considered too similar to be in the final subset.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GIST-1a-Example.width-1250.png\" alt=\"Diagram titled &quot;How GIST applies greedy selection while ensuring minimum diversity&quot; showing three purple circles with interconnected, numbered data nodes.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GIST-1a-Example.width-1250.png\" alt=\"Diagram titled &quot;How GIST applies greedy selection while ensuring minimum diversity&quot; showing three purple circles with interconnected, numbered data nodes.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"iy2vh\"><i>GIST looks for the point with the highest score that isn't already inside someone else's bubble. Then it picks the highest-scoring \"VIP\" data points (the dots with numbers) and draws a \"no-go zone\" around them to ensure the final selection is both high-quality and diverse. The higher the number, the more valuable that specific piece of data is for learning.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"2. Approximating the independent set\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">2. Approximating the independent set</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vjebi\">GIST then looks for the maximum-utility subset that can be chosen where no two points are connected in this graph: the classic <a href=\"https://en.wikipedia.org/wiki/Independent_set_(graph_theory)\" target=\"_blank\" rel=\"noopener noreferrer\">maximum independent set</a> problem. Imagine planning a dinner party where certain guests can’t sit together. Your goal is to invite the most interesting group of people possible, but you must follow one rule: no two people at the table can have a conflict. This is a massive puzzle because picking one guest might \"block\" you from inviting three other high-interest people. To find the best combination, you have to check an exponential number of groupings, which is why it is considered one of the hardest problems in computing.</p><p data-block-key=\"3mbmu\">Since the max independent set problem itself is NP-complete (meaning that it’s widely believed there does not exist an efficient algorithm to find the absolute “perfect” answer for a massive dataset) and admits no reasonable approximation algorithm, GIST uses a carefully constructed <a href=\"https://arxiv.org/abs/2309.14558\" target=\"_blank\" rel=\"noopener noreferrer\">bicriteria greedy algorithm</a> to approximate the solution efficiently. It iterates through many possible distance thresholds, solving the corresponding independent set problem and ultimately selecting the best solution found across all thresholds. For any minimum distance <i>d</i> achieved by the optimum solution, GIST achieves a comparable utility to the optimum utility at distance threshold <i>d</i>/2.</p><p data-block-key=\"1j8o3\">The bicriteria greedy algorithm acts like a systematic \"tuning knob\" that finds the right balance between data variety and value. Instead of guessing, it analyzes the actual distances between all data points to create a list of potential spacing rules. It then tests these rules one by one: for each rule, it \"greedily\" grabs the most valuable points it can find, provided they aren't too close to the points it has already picked. By running this process across every relevant distance and comparing the results, the algorithm identifies the specific \"sweet spot\" that captures the most useful information while ensuring the data is as spread out as possible.</p><p data-block-key=\"dkf7d\">By cleverly approximating a series of these maximum independent set problems, GIST manages to satisfy the utility goal while respecting the minimum diversity requirement.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Results\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    \n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Strong guarantees\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Strong guarantees</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vjebi\">Our theoretical results are the most significant findings: GIST is the first algorithm to provide a strong, provable guarantee for this diversity-utility tradeoff. The GIST algorithm is guaranteed to find a data subset whose value is at least <i>half</i> the value of the absolute optimal solution (see <a href=\"https://arxiv.org/abs/2405.18754\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a> for details). This strong guarantee provides practitioners with a necessary mathematical safety net, ensuring that the algorithm is making an efficient trade-off between maximizing utility while ensuring diversification. Further, we prove that it is NP-hard to find a subset with more than a 0.56 fraction of the optimal value.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Real-world impact\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Real-world impact</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vjebi\">We also evaluated GIST against state-of-the-art benchmarks in various ML applications, focusing on scenarios where diversity and utility are both essential:</p><ul><li data-block-key=\"7jutq\"><i>Random:</i> A simple and lightweight approach that promotes diversity in many settings and provides good solutions.</li><li data-block-key=\"f3udk\"><i>Margin:</i> Picks data points that the model is currently \"uncertain\" about and is not incentivized to output a diverse set of training examples.</li><li data-block-key=\"3r0ed\">k<i>-center:</i> Selects a subset of data points so that every single point in the original, massive dataset is as \"close\" as possible to one of the selected representatives. Instead of looking for the most important or interesting points, it tries to eliminate \"blind spots.\"</li><li data-block-key=\"9bv3a\"><i>Submod:</i> Chooses \"important\" points (utility) while also making sure they aren't \"too similar\" (diversity). However, it uses a slightly older mathematical way of defining diversity that can sometimes be inconsistent when the dataset becomes large.</li></ul><p data-block-key=\"d5qrs\">We then combined GIST with the other strategies to see if it could make them better.</p><ul><li data-block-key=\"1o96u\"><i>GIST-margin:</i> This takes the \"picking the hard cases\" strategy and forces it to follow GIST's strict diversity rules. It says, \"Pick the most confusing examples, but I forbid you from picking two confusing examples that are too similar to each other.\"</li><li data-block-key=\"fpdvi\"><i>GIST-submod:</i> Submod uses the GIST framework to handle the diversity part more rigorously than the original <i>submod</i> approach could.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Data sampling for image classification\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h4 class=\"\">Data sampling for image classification</h4>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vjebi\">In experiments using a <a href=\"https://www.geeksforgeeks.org/deep-learning/residual-networks-resnet-deep-learning/\" target=\"_blank\" rel=\"noopener noreferrer\">ResNet-56 model</a> on datasets like <a href=\"https://www.image-net.org/\" target=\"_blank\" rel=\"noopener noreferrer\">ImageNet</a>, GIST demonstrated significant advantages in single-shot subset selection. Single-shot data downsampling reduces the volume of a dataset — typically images, signals, or high-dimensional data — in one step while retaining critical information. Unlike iterative or multi-stage processes, this approach maximizes speed and efficiency and is often used to decrease computational burden or to optimize rendering performance in graphics-related tasks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GIST-2-Performance.width-1250.png\" alt=\"Line graph showing GIST methods achieving higher top-1 accuracy than random, margin, and k-center across cardinality constraint k.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GIST-2-Performance.width-1250.png\" alt=\"Line graph showing GIST methods achieving higher top-1 accuracy than random, margin, and k-center across cardinality constraint k.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"iy2vh\"><a href=\"https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Top-1 classification accuracy</i></a><i> (%) of GIST on</i> <a href=\"https://www.image-net.org/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>ImageNet</i></a><i> for different single-shot data downsampling algorithms. The cardinality constraint limits the number of items picked. If, for example, there is a pool of 1.3 million images, a cardinality constraint of 10% (k=10%) means that the algorithm is strictly forbidden from picking more than 130,000 images to train the model.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"vjebi\">This is critical for data-intensive tasks where we need to select the most informative and diverse images <i>once</i> before beginning costly model training. GIST consistently selected subsets that led to higher model accuracy compared to previous methods, proving its improved ability to balance coverage and non-redundancy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Running time\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h4 class=\"\">Running time</h4>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vjebi\">Despite the complexity of the underlying problem, the actual subset selection step of GIST is incredibly fast: its running time is often negligible compared to the hours or even days required for training the final ML model. This speed makes GIST practical for integration into large-scale training pipelines with billions of data points.</p><p data-block-key=\"61lba\">We also observed the value of the max-min diversity approach with the YouTube Home ranking team, which employed a similar principle to enhance the diversity of video recommendations and consequently improved long-term user value.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Conclusion: A foundation for scalable AI\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion: A foundation for scalable AI</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vjebi\">The challenge of combining competing optimization goals — such as maximizing total utility while maintaining maximum diversity — has been a long-standing hurdle in computational science. The GIST algorithm successfully solves this fundamental trade-off for data selection by providing a single, highly efficient framework.</p><p data-block-key=\"1cnhh\">By breaking the challenging diversity–utility tradeoff problem into a sequence of simpler, approximable tasks, GIST provides the ML community with a provably effective tool. This research establishes a strong foundation for the next generation of scalable AI systems, guaranteeing that as data continues to grow, we can still train models on subsets that are both maximally informative and minimally redundant. The gist of it is, GIST ensures we are sampling data intelligently.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Acknowledgements\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vjebi\"><i>We thank Sara Ahmadian from Google Research; Srikumar Ramalingam, Giulia DeSalvo, and Gui Citovsky from Google DeepMind; and Cheenar Banerjee, Cristos Goodrow, Fabio Soldo, and Su-Lin Wu from YouTube who contributed to this work. Matthew Fahrbach, Morteza Zadimoghaddam and Srikumar Ramalingam contributed equally in this research project.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Small models, big results: Achieving superior intent extraction through decomposition",
      "link": "https://research.google/blog/small-models-big-results-achieving-superior-intent-extraction-through-decomposition/",
      "pubDate": "Wed, 21 Jan 2026 16:00:00 GMT",
      "isoDate": "2026-01-21T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Small models, big results: Achieving superior intent extraction through decomposition",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-1-Workflow.width-1250.jpg",
          "alt": "A diagram titled that illustrates a user's intent extraction workflow. The user selects from a grid of travel options. Below, text labels describe the screen context, the specific user action, and a speculation on the user's travel goals.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-2-Stage2.width-1250.jpg",
          "alt": "Flowchart showing user action summaries entering a \"Model finetuning\" stage to produce a \"Cleaned gold\" intent statement.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-3a-FactCoverage.width-1250.jpg",
          "alt": "A side-by-side comparison of \"Reference\" and \"Predicted\" flight booking facts using checkmarks and red X's to evaluate extraction accuracy.",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-4a-ErrorPropagation.width-1250.jpg",
          "alt": "Two flowcharts showing Recall and Precision analysis to track where facts are missed or hallucinated during the extraction process.",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-5-Performance.width-1250.png",
          "alt": "Two bar charts comparing the BiFact F1 scores of Gemini 1.5 and Qwen2 models across Android and Web trajectories.",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"rymbe\">As AI technologies advance, truly helpful agents will become capable of better anticipating user needs. For experiences on mobile devices to be truly helpful, the underlying models need to understand what the user is doing (or trying to do) when users interact with them. Once current and previous tasks are understood, the model has more context to predict potential next actions. For example, if a user previously searched for music festivals across Europe and is now looking for a flight to London, the agent could offer to find festivals in London on those specific dates.</p><p data-block-key=\"30f8p\">Large multimodal LLMs are already quite good at understanding user intent from a user interface (UI) trajectory. But using LLMs for this task would typically require sending information to a server, which can be slow, costly, and carries the potential risk of exposing sensitive information.</p><p data-block-key=\"4q9jq\">Our recent paper “<a href=\"https://aclanthology.org/2025.emnlp-main.949/\" target=\"_blank\" rel=\"noopener noreferrer\">Small Models, Big Results: Achieving Superior Intent Extraction Through Decomposition</a>”, presented at <a href=\"https://2025.emnlp.org/\" target=\"_blank\" rel=\"noopener noreferrer\">EMNLP 2025</a>, addresses the question of how to use <i>small</i> multimodal LLMs (MLLMs) to understand sequences of user interactions on the web and on mobile devices all on device. By separating user intent understanding into two stages, first summarizing each screen separately and then extracting an intent from the sequence of generated summaries, we make the task more tractable for small models. We also formalize metrics for evaluation of model performance and show that our approach yields results comparable to much larger models, illustrating its potential for on-device applications. This work builds on <a href=\"https://dl.acm.org/doi/10.1145/3701716.3717525\" target=\"_blank\" rel=\"noopener noreferrer\">previous</a> <a href=\"https://arxiv.org/abs/2502.13149\" target=\"_blank\" rel=\"noopener noreferrer\">work</a> from our team on user intent understanding.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Details\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Details</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rymbe\">We introduce a decomposed workflow for user intent understanding from user interactions. At inference time the model performs two main steps. In the first step each individual interaction on a single screen and UI element is summarized independently. Next, those summaries are used as a series of events to predict the general intent of the entire UI trajectory.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Individual screen summaries\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Individual screen summaries</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rymbe\">At the first stage, every individual interaction is summarized by a small multimodal LLM.</p><p data-block-key=\"dmccv\">Given the a sliding window of three screens (previous, current, next), the following questions are asked:</p><ol><li data-block-key=\"3shdl\">What is the relevant screen context? Give a short list of salient details on the current screen.</li><li data-block-key=\"3uuvr\">What did the user just do? Provide a list of actions that the user took in this interaction.</li><li data-block-key=\"f3ud5\">Speculate. What is the user trying to accomplish with this interaction?</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-1-Workflow.width-1250.jpg\" alt=\"A diagram titled that illustrates a user's intent extraction workflow. The user selects from a grid of travel options. Below, text labels describe the screen context, the specific user action, and a speculation on the user's travel goals.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-1-Workflow.width-1250.jpg\" alt=\"A diagram titled that illustrates a user's intent extraction workflow. The user selects from a grid of travel options. Below, text labels describe the screen context, the specific user action, and a speculation on the user's travel goals.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"psr0n\"><i>First stage of the decomposed workflow. For each screenshot, action pair, we look at the surrounding screens, and ask questions about the screen context, the user action, and speculation about what the user is trying to do. At the bottom, we show a potential LLM-generated summary answering the three questions. This summary will serve as an input to the second stage of the decomposed workflow.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Intent extraction from summaries\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Intent extraction from summaries</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rymbe\">In this stage, a fine-tuned small model is used to extract a single sentence from the screen summaries.</p><p data-block-key=\"174rm\">We find that the following techniques are helpful.</p><ul><li data-block-key=\"9v2ab\"><i>Fine-tuning:</i> Giving examples of what a “good” intent statement looks like helps the model focus on the important parts of the summaries and drop the non-useful ones. We use publicly available automation datasets for training data, since they have good examples that pair intent with sequences of actions.</li><li data-block-key=\"9sibq\"><i>Label Preparation:</i> Because the summaries may be missing information, if we train with the full intents, we inadvertently teach the model to fill in details that aren’t present (i.e., to hallucinate). To avoid this, we first remove any information that doesn’t appear in the summaries from the training intents (using a separate LLM call).</li><li data-block-key=\"8ea57\"><i>Dropping speculations:</i> Giving the model a specified place to output its speculations on what the user is trying to do helps create a more complete step summary in stage one, but can confuse the intent extractor in stage two. So we do not use the speculations during the second stage. While this may seem counterintuitive — asking for speculations in the first stage only to drop them in the second — we find this helps improve performance.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-2-Stage2.width-1250.jpg\" alt=\"Flowchart showing user action summaries entering a &quot;Model finetuning&quot; stage to produce a &quot;Cleaned gold&quot; intent statement.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-2-Stage2.width-1250.jpg\" alt=\"Flowchart showing user action summaries entering a &quot;Model finetuning&quot; stage to produce a &quot;Cleaned gold&quot; intent statement.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"psr0n\"><i>The second stage of the decomposed workflow uses a fine-tuned model that takes the summaries generated in the first stage as inputs and outputs a concise intent statement. During this stage we drop all speculation from the summaries and clean the labels during training so that they don’t encourage hallucinations.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Evaluation approach\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Evaluation approach</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rymbe\">We use the <a href=\"https://arxiv.org/abs/2502.13149\" target=\"_blank\" rel=\"noopener noreferrer\">Bi-Fact approach</a> to evaluate the quality of a predicted intent against a reference intent. With this approach, we use a separate LLM call to split the reference and predicted intents into details of the intent that cannot be broken down further, which we call “atomic facts”. For example, “a one-way flight” would be an atomic fact, while “a flight from London to Kigali” would be two. We then count the number of reference facts that are entailed by the predicted intent and the number of predicted facts that are entailed by the reference intent. This enables us to know the <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\" target=\"_blank\" rel=\"noopener noreferrer\">precision</a> (how many of the predicted facts are correct) and <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\" target=\"_blank\" rel=\"noopener noreferrer\">recall</a> (how many of the true facts we correctly predicted) of our method and to calculate the <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall#F-measure\" target=\"_blank\" rel=\"noopener noreferrer\">F1 score</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-3a-FactCoverage.width-1250.jpg\" alt=\"A side-by-side comparison of &quot;Reference&quot; and &quot;Predicted&quot; flight booking facts using checkmarks and red X's to evaluate extraction accuracy.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-3a-FactCoverage.width-1250.jpg\" alt=\"A side-by-side comparison of &quot;Reference&quot; and &quot;Predicted&quot; flight booking facts using checkmarks and red X's to evaluate extraction accuracy.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"psr0n\"><i>Fact Coverage Analysis. Evaluating if reference facts were successfully captured in the predicted intent (</i><b><i>left</i></b><i>), and if predicted facts are supported by the reference intent (</i><b><i>right</i></b><i>).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"rymbe\">Working with atomic facts also helps to track how the different stages of the decomposed approach contribute to errors. Below we show how we analyze the flow of facts through the system to track missed details and hallucinations at each stage.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-4a-ErrorPropagation.width-1250.jpg\" alt=\"Two flowcharts showing Recall and Precision analysis to track where facts are missed or hallucinated during the extraction process.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-4a-ErrorPropagation.width-1250.jpg\" alt=\"Two flowcharts showing Recall and Precision analysis to track where facts are missed or hallucinated during the extraction process.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"psr0n\"><i>Propagation error analysis of recall and precision across both stages of our model.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Key results\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Key results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rymbe\">The decomposed approach of summarizing each screen separately and then extracting an intent from the sequence of generated summaries is helpful when using small models. We compare it against standard approaches, including <a href=\"https://en.wikipedia.org/wiki/Prompt_engineering#Chain-of-thought\" target=\"_blank\" rel=\"noopener noreferrer\">chain of thought prompting</a> (CoT) and end-to-end fine-tuning (E2E), and find that it outperforms both. This result holds true when we tested on both mobile device and web trajectories and for <a href=\"https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a> and <a href=\"https://huggingface.co/Qwen/Qwen2-VL-7B\" target=\"_blank\" rel=\"noopener noreferrer\">Qwen2</a> base models. We even find that applying the decomposed approach with the <a href=\"https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini 1.5 Flash 8B</a> model achieves comparable results to using <a href=\"https://arxiv.org/abs/2403.05530\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini 1.5 Pro</a> at a fraction of the cost and speed. See the <a href=\"https://aclanthology.org/2025.emnlp-main.949/\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a> for additional experiments.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-5-Performance.width-1250.png\" alt=\"Two bar charts comparing the BiFact F1 scores of Gemini 1.5 and Qwen2 models across Android and Web trajectories.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-5-Performance.width-1250.png\" alt=\"Two bar charts comparing the BiFact F1 scores of Gemini 1.5 and Qwen2 models across Android and Web trajectories.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"psr0n\"><i>Bi-Fact F1 scores comparing the decomposed method against two natural baselines: chain-of-thought prompting (CoT) and end-to-end fine-tuning (E2E). In all settings, our decomposed method outperforms baselines, and on the mobile device dataset, it is comparable to the performance of the large Gemini Pro model.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Conclusions &amp; future directions\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusions &amp; future directions</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rymbe\">We have shown that a decomposed approach to trajectory summarization can be helpful for intent understanding with small models. Ultimately, as models improve in performance and mobile devices acquire more processing power, we hope that on-device intent understanding can become a building block for many assistive features on mobile devices going forward.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Acknowledgements\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rymbe\"><i>Thank you to our paper coauthors: Noam Kahlon, Joel Oren, Omri Berkovitch, Sapir Caduri, Ido Dagan, and Anatoly Efros.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    }
  ],
  "lastUpdated": "2026-02-17T10:57:39.304Z"
}