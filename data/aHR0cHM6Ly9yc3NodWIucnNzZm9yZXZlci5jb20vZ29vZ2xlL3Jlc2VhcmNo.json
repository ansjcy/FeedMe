{
  "sourceUrl": "https://rsshub.rssforever.com/google/research",
  "title": "Google Research Blog",
  "description": "Google Research Blog - Powered by RSSHub",
  "link": "https://research.google/blog",
  "items": [
    {
      "title": "Towards a science of scaling agent systems: When and why agent systems work",
      "link": "https://research.google/blog/towards-a-science-of-scaling-agent-systems-when-and-why-agent-systems-work/",
      "pubDate": "Tue, 27 Jan 2026 16:00:00 GMT",
      "isoDate": "2026-01-27T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Towards a science of scaling agent systems: When and why agent systems work",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling1_Summary.width-1250.png",
          "alt": "AgentScaling2_Comparison",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling2_Comparison.width-1250.png",
          "alt": "AgentScaling2_Comparison",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling3_TaskPerformanceHERO.width-1250.png",
          "alt": "AgentScaling3_TaskPerformanceHERO",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling4_Reliability.width-1250.png",
          "alt": "AgentScaling4_Reliability",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"os8s1\">AI agents — systems capable of reasoning, planning, and acting — are becoming a common paradigm for real-world AI applications. From <a href=\"https://codeassist.google/\" target=\"_blank\" rel=\"noopener noreferrer\">coding assistants</a> to <a href=\"https://research.google/blog/the-anatomy-of-a-personal-health-agent/\">personal health coaches</a>, the industry is shifting from single-shot question answering to sustained, multi-step interactions. While researchers have long utilized established metrics to optimize the accuracy of traditional machine learning models, agents introduce a new layer of complexity. Unlike isolated predictions, agents must navigate sustained, multi-step interactions where a single error can cascade throughout a workflow. This shift compels us to look beyond standard accuracy and ask: How do we actually design these systems for optimal performance?</p><p data-block-key=\"9s91h\">Practitioners often rely on heuristics, such as the assumption that \"<a href=\"https://arxiv.org/abs/2402.05120\" target=\"_blank\" rel=\"noopener noreferrer\">more agents are better</a>\", believing that adding specialized agents will consistently improve results. For example, \"<a href=\"https://arxiv.org/abs/2402.05120\" target=\"_blank\" rel=\"noopener noreferrer\">More Agents Is All You Need</a>\" reported that LLM performance scales with agent count, while <a href=\"https://arxiv.org/abs/2406.07155\" target=\"_blank\" rel=\"noopener noreferrer\">collaborative scaling research</a> found that multi-agent collaboration \"...often surpasses each individual through collective reasoning.\"</p><p data-block-key=\"edjau\">In our new paper, “<a href=\"https://arxiv.org/abs/2512.08296\" target=\"_blank\" rel=\"noopener noreferrer\">Towards a Science of Scaling Agent Systems</a>”, we challenge this assumption. Through a large-scale controlled evaluation of 180 agent configurations, we derive the first quantitative scaling principles for agent systems, revealing that the \"more agents\" approach often hits a ceiling, and can even degrade performance if not aligned with the specific properties of the task.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"os8s1\">Defining \"agentic\" evaluation</h2><p data-block-key=\"1vpgq\">To understand how agents scale, we first defined what makes a task \"agentic\". Traditional static benchmarks measure a model's knowledge, but they don't capture the complexities of deployment. We argue that <i>agentic</i> tasks require three specific properties:</p><ol><li data-block-key=\"4lsio\"><i>Sustained multi-step interactions</i> with an external environment.</li><li data-block-key=\"7eqp0\"><i>Iterative information gathering</i> under partial observability.</li><li data-block-key=\"8vikn\"><i>Adaptive strategy refinement</i> based on environmental feedback.</li></ol><p data-block-key=\"5r5do\">We evaluated five canonical architectures: one single-agent system (SAS) and four multi-agent variants (independent, centralized, decentralized, and hybrid) across four diverse benchmarks, including <a href=\"https://www.vals.ai/benchmarks/finance_agent\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Finance-Agent</i></a> (financial reasoning), <a href=\"https://arxiv.org/pdf/2508.06600\" target=\"_blank\" rel=\"noopener noreferrer\"><i>BrowseComp-Plus</i></a> (web navigation), <a href=\"https://arxiv.org/abs/2412.21033\" target=\"_blank\" rel=\"noopener noreferrer\"><i>PlanCraft</i></a> (planning), and <a href=\"https://arxiv.org/abs/2405.00823\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Workbench</i></a> (tool use). The agent architectures are defined as follow:</p><ul><li data-block-key=\"crmfg\"><i>Single-Agent (SAS):</i> A solitary agent executing all reasoning and acting steps sequentially with a unified memory stream.</li><li data-block-key=\"6uuhv\"><i>Independent:</i> Multiple agents working in parallel on sub-tasks without communicating, aggregating results only at the end.</li><li data-block-key=\"bvudi\"><i>Centralized:</i> A \"hub-and-spoke\" model where a central orchestrator delegates tasks to workers and synthesizes their outputs.</li><li data-block-key=\"dp37a\"><i>Decentralized:</i> A peer-to-peer mesh where agents communicate directly with one another to share information and reach consensus.</li><li data-block-key=\"4s9bf\"><i>Hybrid:</i> A combination of hierarchical oversight and peer-to-peer coordination to balance central control with flexible execution.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling1_Summary.width-1250.png\" alt=\"AgentScaling2_Comparison\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling1_Summary.width-1250.png\" alt=\"AgentScaling2_Comparison\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"f1omg\"><i>Summary of the five canonical agent architectures evaluated in this study, including their computational complexity, communication overhead, and coordination mechanisms.</i> k<i> = max iterations per agent,</i> n<i> = number of agents,</i> r<i> = orchestrator rounds,</i> d<i> = debate rounds,</i> p<i> = peer communication rounds,</i> m<i> = average peer requests per round. Communication overhead counts inter-agent message exchanges. Independent offers maximal parallelization with minimal coordination. Decentralized uses sequential debate rounds. Hybrid combined orchestrator control with directed peer communication.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"os8s1\">Results: The myth of \"more agents\"</h2><p data-block-key=\"ff941\">To quantify the impact of model capabilities on agent performance, we evaluated our architectures across three leading model families: OpenAI GPT, Google Gemini, and Anthropic Claude. The results reveal a complex relationship between model capabilities and coordination strategy. As shown in the figure below, while performance generally trends upward with more capable models, multi-agent systems are not a universal solution — they can either significantly boost or unexpectedly degrade performance depending on the specific configuration.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling2_Comparison.width-1250.png\" alt=\"AgentScaling2_Comparison\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling2_Comparison.width-1250.png\" alt=\"AgentScaling2_Comparison\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"f1omg\"><i>Performance comparison across three major model families (OpenAI GPT, Google Gemini, Anthropic Claude) showing how different agent architectures scale with model intelligence, where multi-agent systems can either boost or degrade performance depending on the configuration.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"os8s1\">The results below compare the performance of the five architectures across different domains, such as web browsing and financial analysis. The box plots represent the accuracy distribution for each approach, while the percentages indicate the relative improvement (or decline) of multi-agent teams compared to the single-agent baseline. This data highlights that while adding agents can drive massive gains in parallelizable tasks, it can often lead to diminishing returns — or even performance drops — in more sequential workflows.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling3_TaskPerformanceHERO.width-1250.png\" alt=\"AgentScaling3_TaskPerformanceHERO\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling3_TaskPerformanceHERO.width-1250.png\" alt=\"AgentScaling3_TaskPerformanceHERO\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"f1omg\"><i>Task-specific performance showing that multi-agent coordination yields substantial gains on parallelizable tasks like Finance-Agent (+81%) while degrading performance on sequential tasks like PlanCraft (-70%).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"os8s1\">The alignment principle</h3><p data-block-key=\"44n34\">On parallelizable tasks like financial reasoning (e.g., distinct agents can simultaneously analyze revenue trends, cost structures, and market comparisons), centralized coordination improved performance by 80.9% over a single agent. The ability to decompose complex problems into sub-tasks allowed agents to work more effectively.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"os8s1\">The sequential penalty</h3><p data-block-key=\"4h7i0\">Conversely, on tasks requiring strict sequential reasoning (like planning in PlanCraft), every multi-agent variant we tested degraded performance by 39-70%. In these scenarios, the overhead of communication fragmented the reasoning process, leaving insufficient \"cognitive budget\" for the actual task.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"os8s1\">The tool-use bottleneck</h3><p data-block-key=\"7l2nm\">We identified a \"tool-coordination trade-off\". As tasks require more tools (e.g., a coding agent with access to 16+ tools), the \"tax\" of coordinating multiple agents increases disproportionately.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"os8s1\">Architecture as a safety feature</h2><p data-block-key=\"ffooe\">Perhaps most important for real-world deployment, we found a relationship between architecture and reliability. We measured <i>error amplification</i>, the rate at which a mistake by one agent propagates to the final result.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling4_Reliability.width-1250.png\" alt=\"AgentScaling4_Reliability\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AgentScaling4_Reliability.width-1250.png\" alt=\"AgentScaling4_Reliability\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"f1omg\"><i>Comprehensive metrics across architectures reveal that centralized systems achieve the best balance between success rate and error containment, while independent multi-agent systems amplify errors by up to 17.2x.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"os8s1\">We found that independent multi-agent systems (agents working in parallel without talking) amplified errors by 17.2x. Without a mechanism to check each other's work, errors cascaded unchecked. Centralized systems (with an orchestrator) contained this amplification to just 4.4x. The orchestrator effectively acts as a \"validation bottleneck\", catching errors before they propagate.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"os8s1\">A predictive model for agent design</h2><p data-block-key=\"3kmqj\">Moving beyond retrospection, we developed a predictive model (<i>R</i>^2 = 0.513) that uses measurable task properties like tool count and decomposability to predict which architecture will perform best. This model correctly identifies the optimal coordination strategy for 87% of unseen task configurations.</p><p data-block-key=\"9vrri\">This suggests we are moving toward a new science of agent scaling. Instead of guessing whether to use a swarm of agents or a single powerful model, developers can now look at the properties of their task, specifically its sequential dependencies and tool density, to make principled engineering decisions.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"os8s1\">Conclusion</h2><p data-block-key=\"2bha4\">As foundational models like Gemini continue to advance, our research suggests that smarter models don't replace the need for multi-agent systems, they accelerate it, but only when the architecture is right. By moving from heuristics to quantitative principles, we can build the next generation of AI agents that are not just more numerous, but smarter, safer, and more efficient.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"os8s1\">Acknowledgements</h2><p data-block-key=\"eblfk\"><i>We would like to thank our co-authors and collaborators from Google Research, Google DeepMind, and academia for their contributions to this work.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "ATLAS: Practical scaling laws for multilingual models",
      "link": "https://research.google/blog/atlas-practical-scaling-laws-for-multilingual-models/",
      "pubDate": "Mon, 26 Jan 2026 16:00:00 GMT",
      "isoDate": "2026-01-26T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "ATLAS: Practical scaling laws for multilingual models",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-1.width-1250.png",
          "alt": "ATLAS-1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-2.width-1250.png",
          "alt": "ATLAS-2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-3.width-1250.png",
          "alt": "ATLAS-3",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-4.width-1250.png",
          "alt": "ATLAS-4",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"dni7s\">Over 50% of AI model users <a href=\"https://www.anthropic.com/research/anthropic-economic-index-september-2025-report\" target=\"_blank\" rel=\"noopener noreferrer\">speak non-English languages</a>, yet publicly accessible scaling laws are overwhelmingly focused on the English language. This imbalance creates a critical gap in public research, leaving model builders, tasked with serving billions of international and multilingual users, without data-driven guidance for key development decisions about efficiency, quality, and cost when building for non-English languages or with specific language mixtures.</p><p data-block-key=\"79l5e\">In “<a href=\"https://arxiv.org/pdf/2510.22037\" target=\"_blank\" rel=\"noopener noreferrer\">ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality</a>”, to be presented at <a href=\"https://iclr.cc/\" target=\"_blank\" rel=\"noopener noreferrer\">ICLR 2026</a>, we aim to address this gap. We present the largest public multilingual pre-training study to date, spanning 774 training runs across 10M–8B parameter models. It includes data spanning 400+ languages and evaluations in 48 languages. As a result of this study, we estimate the synergies between 1,400 pairs of languages, and introduce adaptive transfer scaling laws (ATLAS) for building multilingual models that enable practitioners to efficiently balance the mix of languages in training data with model size.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">ATLAS: A single scaling law that adapts to multilingual mixtures</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"dni7s\">ATLAS is a simple, practical approach to determining optimal model size, data volume, and language mixtures for training. Unlike traditional scaling laws that focus on monolingual settings, ATLAS provides these recommendations for more complex, multilingual environments. It specifically optimizes performance on a target language (e.g., Catalan) by leveraging data from multiple different languages. ATLAS extends these traditional scaling law principles through three components:</p><ul><li data-block-key=\"3v48v\">A cross-lingual transfer matrix used to identify which languages are best to train together</li><li data-block-key=\"cq19c\">A scaling law that provides guidance on efficiently expanding model size and data as the number of supported languages increases</li><li data-block-key=\"8283i\">Rules for deciding when to pre-train a model from scratch versus fine-tuning from a multilingual checkpoint</li></ul><p data-block-key=\"7dfi6\">ATLAS accomplishes this by training on hundreds of multilingual experiments (using the <a href=\"https://arxiv.org/pdf/2309.04662\" target=\"_blank\" rel=\"noopener noreferrer\">MADLAD-400</a> corpus with over 750 runs across 400+ languages) and accounting for three distinct data sources: 1) the target language, 2) similar transfer languages according to empirical analysis (e.g., Catalan might include Latin languages like Spanish, Portuguese, and Italian), and 3) all other languages. This novel approach enables the law to learn how much each source actually helps or hinders the target language, a capability prior laws did not support.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Evaluation</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"dni7s\">We used the MADLAD-400 dataset to evaluate how well ATLAS predicts a model’s performance on new model sizes, varying amounts of training data<i>,</i> or new language mixtures<i>.</i> To do this, we measure performance using a <a href=\"https://arxiv.org/pdf/2407.13623\" target=\"_blank\" rel=\"noopener noreferrer\">vocabulary-insensitive loss</a> across over 750 independent runs in monolingual, bilingual, and massively multilingual settings. Our evaluations show that ATLAS consistently outperforms prior work.</p><p data-block-key=\"4iauh\">For six languages — English (EN), French (FR), Russian (RU), Chinese (ZH), Hindi (HI), and Swahili (SW) — we analyzed how ATLAS predicted the optimal model size (<i>N</i>) and data size (<i>D</i>) should be scaled. When we compared these optimal scaling trajectories across languages, we made two observations. The curves look strikingly similar, but training with a multilingual vocabulary or fully multilingual data comes with a compute-efficiency tax — especially for English. Low-resource languages show upward bends as they run out of data, and the model struggles to learn from data repetition. ATLAS explicitly models these effects.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-1.width-1250.png\" alt=\"ATLAS-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-1.width-1250.png\" alt=\"ATLAS-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"fdspk\"><i>These charts show the optimal scaling trajectories (model size (</i><b><i>N</i></b><i>) and data size (</i><b><i>D</i></b><i>) determined by ATLAS for each language and model type. The lines represent three configurations:</i> <b><i>Solid</i></b><i> (monolingual vocab/data),</i> <b><i>Dashed</i></b><i> (multilingual vocab/monolingual data), and</i> <b><i>Dotted</i></b><i> (multilingual vocab/multilingual data). The dotted lines are consistently highest, indicating that training with a full multilingual setup requires slightly more compute for the same quality.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">The cross-lingual transfer map</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"dni7s\">Next, we measured language-to-language synergies and interference at scale, producing a matrix that quantifies how much training on language <i>A</i> helps (or hurts) language <i>B</i>. Our results show very intuitive results: Norwegian is helped primarily by Swedish and German, Malay by Indonesian, and Arabic by Hebrew. English, French, and Spanish are the most widely helpful languages with which to train, likely due to the inherent quality, heterogeneity, and quantity of text in these languages found on the web.</p><p data-block-key=\"9h723\">The analysis shows that the biggest predictor of positive transfer is sharing a script and/or language family (e.g., Latin script), statistically significant with p &lt; .001. English helps many, but not all, languages; and transfer isn’t always symmetric (<i>A</i> can help <i>B</i> more than <i>B</i> helps <i>A</i>). These measurements turn “hunches” into data-driven language mix choices.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-2.width-1250.png\" alt=\"ATLAS-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-2.width-1250.png\" alt=\"ATLAS-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"fdspk\"><i>This heatmap shows the cross-lingual transfer matrix, quantifying language-to-language synergies and inference.</i> <b><i>Red</i></b><i> indicates that a language helps and</i> <b><i>blue</i></b><i> indicates it hurts. Boxes highlight each target language’s top-5 helpers. Languages that share the same writing system (e.g., Latin script) are notably more synergistic.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Decoding the “curse of multilinguality” with clear scaling rules</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"dni7s\">The “curse of multilinguality” is a phenomenon where models trained on multiple languages see a decrease in performance with each new language due to limited model capacity. We formalize this problem with a scaling law that considers not just model size (<i>N</i>), and quantity of training data (<i>D</i>), but the number of languages in that data (<i>K</i>). Fitting this law to many experiments, we found that while adding languages brings a mild capacity tax, there is a high-degree of positive transfer. This means if we want to train a model to support twice as many languages (2·<i>K</i>) then we should increase model size by 1.18x, and total data by 1.66x. This equates to 83% of data in each of the 2K languages. Although there is less data per language, the positive synergies from learning on all of them means the capacity constraints that cause degradation to the performance are offset.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-3.width-1250.png\" alt=\"ATLAS-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-3.width-1250.png\" alt=\"ATLAS-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"fdspk\"><i>These curves show how much to increase model size</i> N<i> or data size</i> D<i>, when we scale from</i> K<i> to</i> rK<i> training languages. For instance, the blue solid-line curve shows all the possibilities for how much to increase</i> N<i> and/or</i> D<i> to achieve the same performance with 2</i>K<i> as with</i> K<i> languages. The dotted purple line shows the most computationally efficient way to increase</i> N<i> and</i> D<i>, as we increase</i> K<i>.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">When to pre-train vs. fine-tune a multilingual checkpoint</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"dni7s\">For ten languages, we compare two paths to get the best performing model: (a) pre-train from scratch on the target language or (b) fine-tune from a strong multilingual “<a href=\"https://arxiv.org/pdf/2304.09151\" target=\"_blank\" rel=\"noopener noreferrer\">Unimax</a>” checkpoint. Option (b) is likely to have the best performance with minimal additional compute, as the model is already pretty strong across languages. However, if the model can be trained for much longer, then option (a) can often yield better long-term results. Our goal is to find the crossover point between the two training curves, based on how much compute the model builder has to spend.</p><p data-block-key=\"51vic\">Our results show that fine-tuning wins early, but pre-training overtakes once you can afford enough tokens. In our runs, the crossover typically occurs between ~144B and 283B tokens (language-dependent) for models with 2B parameters. Next, we plotted the crossover point as a function of model size. This gives a concrete, budget-aware rule of thumb: if your token and compute budget is below the crossover point for your model size, start from a multilingual checkpoint; otherwise, pre-training from scratch will usually finish ahead. Note that exact thresholds depend on the base model and mixture.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-4.width-1250.png\" alt=\"ATLAS-4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ATLAS-4.width-1250.png\" alt=\"ATLAS-4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"fdspk\"><i>As model size increases, the amount of compute (or training tokens since</i> C<i>=6</i>ND<i>) needed to hit the crossover point (where pre-training from scratch is better than fine-tuning) increases. We estimate a function for the crossover point based on model size. You can see how this estimated rule (</i><b><i>black line</i></b><i>) approximately fits the cross-over points found for the 10 plotted languages.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Try it yourself</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"dni7s\">By moving beyond English-centric scaling, ATLAS provides a roadmap for global model developers. It can be directly applied to scale language models beyond English by helping developers:</p><ul><li data-block-key=\"ci7bv\"><b>Planning to train a new multilingual or non-English model?</b> Use Figure 1 or Table C.1 from the <a href=\"https://arxiv.org/pdf/2510.22037\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a> to get a sense of the potential scaling laws based on vocabulary or training choices.</li><li data-block-key=\"6s6v1\"><b>Choosing a new training mix?</b> Consult the transfer matrix (Figure 2) to pick source languages that empirically help your targets — especially those sharing the same script/family.</li><li data-block-key=\"h95j\"><b>Training a new model with more languages?</b> Consult Section 5 to determine how to most efficiently expand your model size and data size to mitigate the effects of the curse of multilinguality.</li><li data-block-key=\"6rdle\"><b>Compute-constrained?</b> Consult Section 6 to decide if you should fine-tune a multilingual model or pre-train from scratch.</li></ul><p data-block-key=\"4mju\">We hope this work enables a new generation of multilingual models, serving billions of non-English speakers.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"8yqmj\"><i>We thank Luke Zettlemoyer, Catherine Arnett and Stella Biderman for helpful discussions on the paper. We thank Biao Zhang and Xavier Garcia for the technical discussions and feedback on early directions.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Introducing GIST: The next stage in smart sampling",
      "link": "https://research.google/blog/introducing-gist-the-next-stage-in-smart-sampling/",
      "pubDate": "Thu, 22 Jan 2026 16:00:00 GMT",
      "isoDate": "2026-01-22T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Introducing GIST: The next stage in smart sampling",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GIST-1a-Example.width-1250.png",
          "alt": "Diagram titled \"How GIST applies greedy selection while ensuring minimum diversity\" showing three purple circles with interconnected, numbered data nodes.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GIST-2-Performance.width-1250.png",
          "alt": "Line graph showing GIST methods achieving higher top-1 accuracy than random, margin, and k-center across cardinality constraint k.",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"vjebi\">Modern machine learning (ML) has unlocked unprecedented performance at the cost of having to process increasingly massive and complex datasets. From large language models (LLMs) to computer vision systems, there’s a common challenge: handling a massive amount of data that’s expensive to process.</p><p data-block-key=\"1n1iu\">This necessitates <i>subset selection</i> — the task of choosing a smaller, representative group of data points from the full dataset for the typical training task (not the fine-tuning). The question is then: how can we be sure this subset contains enough information to train an accurate model?</p><p data-block-key=\"ab14i\">At NeurIPS 2025, we introduced <a href=\"https://arxiv.org/abs/2405.18754\" target=\"_blank\" rel=\"noopener noreferrer\">Greedy Independent Set Thresholding</a> (GIST), a novel algorithm that helps solve this issue by balancing data “diversity” (ensuring the selected data is not redundant) and data “utility“ (data that is relevant and useful for the task). GIST not only outperforms state-of-the-art benchmarks tasks, such as image classification, but it does so with a mathematical guarantee about its solution quality.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"The conflict: Why smart sampling is hard\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The conflict: Why smart sampling is hard</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vjebi\">When selecting a subset of data, researchers must balance two often conflicting objectives: diversity and utility. Enforcing data diversity ensures the selected points aren’t redundant. Utility measures the overall usefulness or informational value of the selected subset.</p><p data-block-key=\"8o83b\">For diversity, we focus on maximizing the minimum distance (typically in embedding space) between any two selected data points, also known as <a href=\"https://pdfs.semanticscholar.org/f2ca/f56ff3270860daa8e60f50219588e8c62712.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">max-min diversity</a>. If you choose two data points that are very similar (e.g., two almost identical pictures of a golden retriever), your diversity is low. Max-min diversity forces you to select points that are all as far apart from each other as possible, minimizing redundancy and ensuring broad coverage of the data landscape. For utility, we focus on the class of <a href=\"https://en.wikipedia.org/wiki/Submodular_set_function\" target=\"_blank\" rel=\"noopener noreferrer\">monotone submodular functions</a>, which aim to maximize the total unique information covered by the subset.</p><p data-block-key=\"7k5ea\">The difficulty lies in combining these two goals. A pure max-min strategy might select diverse but ultimately irrelevant data points, while a pure utility strategy might select a tight, highly relevant cluster of redundant points. Finding a subset that is both maximally spread out and maximally informative is a complex combinatorial problem that is known to be <a href=\"https://en.wikipedia.org/wiki/NP-hardness\" target=\"_blank\" rel=\"noopener noreferrer\">NP-hard</a>, meaning that no algorithm can find the best solution efficiently, especially for massive datasets.</p><p data-block-key=\"2qj21\">This inherent conflict requires a clever <a href=\"https://en.wikipedia.org/wiki/Approximation_algorithm\" target=\"_blank\" rel=\"noopener noreferrer\">approximation</a> strategy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"How GIST works\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How GIST works</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vjebi\">Since finding a perfect subset is impractical, the goal shifts to finding an algorithm with a provable approximation guarantee — a mathematical safety net that assures the solution is always close to the true optimum. This is where GIST provides its groundbreaking solution.</p><p data-block-key=\"80lc8\">GIST breaks the diversity–utility challenge into a series of simpler, but related, optimization problems:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"1. Thresholding the diversity component\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">1. Thresholding the diversity component</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vjebi\">GIST starts by temporarily isolating the diversity component. Instead of trying to maximize the minimum distance between all points (the hard part), GIST tackles a simpler question: \"For a certain fixed minimum distance, what is the best subset of data we can select?\"</p><p data-block-key=\"foqdv\">By fixing the minimum required distance, GIST processes the data using a graph where two points are connected only if their distance is <i>less</i> than that specified distance. In this graph, any two connected points are considered too similar to be in the final subset.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GIST-1a-Example.width-1250.png\" alt=\"Diagram titled &quot;How GIST applies greedy selection while ensuring minimum diversity&quot; showing three purple circles with interconnected, numbered data nodes.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GIST-1a-Example.width-1250.png\" alt=\"Diagram titled &quot;How GIST applies greedy selection while ensuring minimum diversity&quot; showing three purple circles with interconnected, numbered data nodes.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"iy2vh\"><i>GIST looks for the point with the highest score that isn't already inside someone else's bubble. Then it picks the highest-scoring \"VIP\" data points (the dots with numbers) and draws a \"no-go zone\" around them to ensure the final selection is both high-quality and diverse. The higher the number, the more valuable that specific piece of data is for learning.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"2. Approximating the independent set\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">2. Approximating the independent set</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vjebi\">GIST then looks for the maximum-utility subset that can be chosen where no two points are connected in this graph: the classic <a href=\"https://en.wikipedia.org/wiki/Independent_set_(graph_theory)\" target=\"_blank\" rel=\"noopener noreferrer\">maximum independent set</a> problem. Imagine planning a dinner party where certain guests can’t sit together. Your goal is to invite the most interesting group of people possible, but you must follow one rule: no two people at the table can have a conflict. This is a massive puzzle because picking one guest might \"block\" you from inviting three other high-interest people. To find the best combination, you have to check an exponential number of groupings, which is why it is considered one of the hardest problems in computing.</p><p data-block-key=\"3mbmu\">Since the max independent set problem itself is NP-complete (meaning that it’s widely believed there does not exist an efficient algorithm to find the absolute “perfect” answer for a massive dataset) and admits no reasonable approximation algorithm, GIST uses a carefully constructed <a href=\"https://arxiv.org/abs/2309.14558\" target=\"_blank\" rel=\"noopener noreferrer\">bicriteria greedy algorithm</a> to approximate the solution efficiently. It iterates through many possible distance thresholds, solving the corresponding independent set problem and ultimately selecting the best solution found across all thresholds. For any minimum distance <i>d</i> achieved by the optimum solution, GIST achieves a comparable utility to the optimum utility at distance threshold <i>d</i>/2.</p><p data-block-key=\"1j8o3\">The bicriteria greedy algorithm acts like a systematic \"tuning knob\" that finds the right balance between data variety and value. Instead of guessing, it analyzes the actual distances between all data points to create a list of potential spacing rules. It then tests these rules one by one: for each rule, it \"greedily\" grabs the most valuable points it can find, provided they aren't too close to the points it has already picked. By running this process across every relevant distance and comparing the results, the algorithm identifies the specific \"sweet spot\" that captures the most useful information while ensuring the data is as spread out as possible.</p><p data-block-key=\"dkf7d\">By cleverly approximating a series of these maximum independent set problems, GIST manages to satisfy the utility goal while respecting the minimum diversity requirement.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Results\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    \n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Strong guarantees\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Strong guarantees</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vjebi\">Our theoretical results are the most significant findings: GIST is the first algorithm to provide a strong, provable guarantee for this diversity-utility tradeoff. The GIST algorithm is guaranteed to find a data subset whose value is at least <i>half</i> the value of the absolute optimal solution (see <a href=\"https://arxiv.org/abs/2405.18754\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a> for details). This strong guarantee provides practitioners with a necessary mathematical safety net, ensuring that the algorithm is making an efficient trade-off between maximizing utility while ensuring diversification. Further, we prove that it is NP-hard to find a subset with more than a 0.56 fraction of the optimal value.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Real-world impact\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Real-world impact</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vjebi\">We also evaluated GIST against state-of-the-art benchmarks in various ML applications, focusing on scenarios where diversity and utility are both essential:</p><ul><li data-block-key=\"7jutq\"><i>Random:</i> A simple and lightweight approach that promotes diversity in many settings and provides good solutions.</li><li data-block-key=\"f3udk\"><i>Margin:</i> Picks data points that the model is currently \"uncertain\" about and is not incentivized to output a diverse set of training examples.</li><li data-block-key=\"3r0ed\">k<i>-center:</i> Selects a subset of data points so that every single point in the original, massive dataset is as \"close\" as possible to one of the selected representatives. Instead of looking for the most important or interesting points, it tries to eliminate \"blind spots.\"</li><li data-block-key=\"9bv3a\"><i>Submod:</i> Chooses \"important\" points (utility) while also making sure they aren't \"too similar\" (diversity). However, it uses a slightly older mathematical way of defining diversity that can sometimes be inconsistent when the dataset becomes large.</li></ul><p data-block-key=\"d5qrs\">We then combined GIST with the other strategies to see if it could make them better.</p><ul><li data-block-key=\"1o96u\"><i>GIST-margin:</i> This takes the \"picking the hard cases\" strategy and forces it to follow GIST's strict diversity rules. It says, \"Pick the most confusing examples, but I forbid you from picking two confusing examples that are too similar to each other.\"</li><li data-block-key=\"fpdvi\"><i>GIST-submod:</i> Submod uses the GIST framework to handle the diversity part more rigorously than the original <i>submod</i> approach could.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Data sampling for image classification\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h4 class=\"\">Data sampling for image classification</h4>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vjebi\">In experiments using a <a href=\"https://www.geeksforgeeks.org/deep-learning/residual-networks-resnet-deep-learning/\" target=\"_blank\" rel=\"noopener noreferrer\">ResNet-56 model</a> on datasets like <a href=\"https://www.image-net.org/\" target=\"_blank\" rel=\"noopener noreferrer\">ImageNet</a>, GIST demonstrated significant advantages in single-shot subset selection. Single-shot data downsampling reduces the volume of a dataset — typically images, signals, or high-dimensional data — in one step while retaining critical information. Unlike iterative or multi-stage processes, this approach maximizes speed and efficiency and is often used to decrease computational burden or to optimize rendering performance in graphics-related tasks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GIST-2-Performance.width-1250.png\" alt=\"Line graph showing GIST methods achieving higher top-1 accuracy than random, margin, and k-center across cardinality constraint k.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GIST-2-Performance.width-1250.png\" alt=\"Line graph showing GIST methods achieving higher top-1 accuracy than random, margin, and k-center across cardinality constraint k.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"iy2vh\"><a href=\"https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Top-1 classification accuracy</i></a><i> (%) of GIST on</i> <a href=\"https://www.image-net.org/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>ImageNet</i></a><i> for different single-shot data downsampling algorithms. The cardinality constraint limits the number of items picked. If, for example, there is a pool of 1.3 million images, a cardinality constraint of 10% (k=10%) means that the algorithm is strictly forbidden from picking more than 130,000 images to train the model.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"vjebi\">This is critical for data-intensive tasks where we need to select the most informative and diverse images <i>once</i> before beginning costly model training. GIST consistently selected subsets that led to higher model accuracy compared to previous methods, proving its improved ability to balance coverage and non-redundancy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Running time\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h4 class=\"\">Running time</h4>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vjebi\">Despite the complexity of the underlying problem, the actual subset selection step of GIST is incredibly fast: its running time is often negligible compared to the hours or even days required for training the final ML model. This speed makes GIST practical for integration into large-scale training pipelines with billions of data points.</p><p data-block-key=\"61lba\">We also observed the value of the max-min diversity approach with the YouTube Home ranking team, which employed a similar principle to enhance the diversity of video recommendations and consequently improved long-term user value.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Conclusion: A foundation for scalable AI\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion: A foundation for scalable AI</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vjebi\">The challenge of combining competing optimization goals — such as maximizing total utility while maintaining maximum diversity — has been a long-standing hurdle in computational science. The GIST algorithm successfully solves this fundamental trade-off for data selection by providing a single, highly efficient framework.</p><p data-block-key=\"1cnhh\">By breaking the challenging diversity–utility tradeoff problem into a sequence of simpler, approximable tasks, GIST provides the ML community with a provably effective tool. This research establishes a strong foundation for the next generation of scalable AI systems, guaranteeing that as data continues to grow, we can still train models on subsets that are both maximally informative and minimally redundant. The gist of it is, GIST ensures we are sampling data intelligently.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Acknowledgements\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"vjebi\"><i>We thank Sara Ahmadian from Google Research; Srikumar Ramalingam, Giulia DeSalvo, and Gui Citovsky from Google DeepMind; and Cheenar Banerjee, Cristos Goodrow, Fabio Soldo, and Su-Lin Wu from YouTube who contributed to this work. Matthew Fahrbach, Morteza Zadimoghaddam and Srikumar Ramalingam contributed equally in this research project.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Small models, big results: Achieving superior intent extraction through decomposition",
      "link": "https://research.google/blog/small-models-big-results-achieving-superior-intent-extraction-through-decomposition/",
      "pubDate": "Wed, 21 Jan 2026 16:00:00 GMT",
      "isoDate": "2026-01-21T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Small models, big results: Achieving superior intent extraction through decomposition",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-1-Workflow.width-1250.jpg",
          "alt": "A diagram titled that illustrates a user's intent extraction workflow. The user selects from a grid of travel options. Below, text labels describe the screen context, the specific user action, and a speculation on the user's travel goals.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-2-Stage2.width-1250.jpg",
          "alt": "Flowchart showing user action summaries entering a \"Model finetuning\" stage to produce a \"Cleaned gold\" intent statement.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-3a-FactCoverage.width-1250.jpg",
          "alt": "A side-by-side comparison of \"Reference\" and \"Predicted\" flight booking facts using checkmarks and red X's to evaluate extraction accuracy.",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-4a-ErrorPropagation.width-1250.jpg",
          "alt": "Two flowcharts showing Recall and Precision analysis to track where facts are missed or hallucinated during the extraction process.",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-5-Performance.width-1250.png",
          "alt": "Two bar charts comparing the BiFact F1 scores of Gemini 1.5 and Qwen2 models across Android and Web trajectories.",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"rymbe\">As AI technologies advance, truly helpful agents will become capable of better anticipating user needs. For experiences on mobile devices to be truly helpful, the underlying models need to understand what the user is doing (or trying to do) when users interact with them. Once current and previous tasks are understood, the model has more context to predict potential next actions. For example, if a user previously searched for music festivals across Europe and is now looking for a flight to London, the agent could offer to find festivals in London on those specific dates.</p><p data-block-key=\"30f8p\">Large multimodal LLMs are already quite good at understanding user intent from a user interface (UI) trajectory. But using LLMs for this task would typically require sending information to a server, which can be slow, costly, and carries the potential risk of exposing sensitive information.</p><p data-block-key=\"4q9jq\">Our recent paper “<a href=\"https://aclanthology.org/2025.emnlp-main.949/\" target=\"_blank\" rel=\"noopener noreferrer\">Small Models, Big Results: Achieving Superior Intent Extraction Through Decomposition</a>”, presented at <a href=\"https://2025.emnlp.org/\" target=\"_blank\" rel=\"noopener noreferrer\">EMNLP 2025</a>, addresses the question of how to use <i>small</i> multimodal LLMs (MLLMs) to understand sequences of user interactions on the web and on mobile devices all on device. By separating user intent understanding into two stages, first summarizing each screen separately and then extracting an intent from the sequence of generated summaries, we make the task more tractable for small models. We also formalize metrics for evaluation of model performance and show that our approach yields results comparable to much larger models, illustrating its potential for on-device applications. This work builds on <a href=\"https://dl.acm.org/doi/10.1145/3701716.3717525\" target=\"_blank\" rel=\"noopener noreferrer\">previous</a> <a href=\"https://arxiv.org/abs/2502.13149\" target=\"_blank\" rel=\"noopener noreferrer\">work</a> from our team on user intent understanding.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Details\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Details</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rymbe\">We introduce a decomposed workflow for user intent understanding from user interactions. At inference time the model performs two main steps. In the first step each individual interaction on a single screen and UI element is summarized independently. Next, those summaries are used as a series of events to predict the general intent of the entire UI trajectory.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Individual screen summaries\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Individual screen summaries</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rymbe\">At the first stage, every individual interaction is summarized by a small multimodal LLM.</p><p data-block-key=\"dmccv\">Given the a sliding window of three screens (previous, current, next), the following questions are asked:</p><ol><li data-block-key=\"3shdl\">What is the relevant screen context? Give a short list of salient details on the current screen.</li><li data-block-key=\"3uuvr\">What did the user just do? Provide a list of actions that the user took in this interaction.</li><li data-block-key=\"f3ud5\">Speculate. What is the user trying to accomplish with this interaction?</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-1-Workflow.width-1250.jpg\" alt=\"A diagram titled that illustrates a user's intent extraction workflow. The user selects from a grid of travel options. Below, text labels describe the screen context, the specific user action, and a speculation on the user's travel goals.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-1-Workflow.width-1250.jpg\" alt=\"A diagram titled that illustrates a user's intent extraction workflow. The user selects from a grid of travel options. Below, text labels describe the screen context, the specific user action, and a speculation on the user's travel goals.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"psr0n\"><i>First stage of the decomposed workflow. For each screenshot, action pair, we look at the surrounding screens, and ask questions about the screen context, the user action, and speculation about what the user is trying to do. At the bottom, we show a potential LLM-generated summary answering the three questions. This summary will serve as an input to the second stage of the decomposed workflow.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Intent extraction from summaries\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Intent extraction from summaries</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rymbe\">In this stage, a fine-tuned small model is used to extract a single sentence from the screen summaries.</p><p data-block-key=\"174rm\">We find that the following techniques are helpful.</p><ul><li data-block-key=\"9v2ab\"><i>Fine-tuning:</i> Giving examples of what a “good” intent statement looks like helps the model focus on the important parts of the summaries and drop the non-useful ones. We use publicly available automation datasets for training data, since they have good examples that pair intent with sequences of actions.</li><li data-block-key=\"9sibq\"><i>Label Preparation:</i> Because the summaries may be missing information, if we train with the full intents, we inadvertently teach the model to fill in details that aren’t present (i.e., to hallucinate). To avoid this, we first remove any information that doesn’t appear in the summaries from the training intents (using a separate LLM call).</li><li data-block-key=\"8ea57\"><i>Dropping speculations:</i> Giving the model a specified place to output its speculations on what the user is trying to do helps create a more complete step summary in stage one, but can confuse the intent extractor in stage two. So we do not use the speculations during the second stage. While this may seem counterintuitive — asking for speculations in the first stage only to drop them in the second — we find this helps improve performance.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-2-Stage2.width-1250.jpg\" alt=\"Flowchart showing user action summaries entering a &quot;Model finetuning&quot; stage to produce a &quot;Cleaned gold&quot; intent statement.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-2-Stage2.width-1250.jpg\" alt=\"Flowchart showing user action summaries entering a &quot;Model finetuning&quot; stage to produce a &quot;Cleaned gold&quot; intent statement.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"psr0n\"><i>The second stage of the decomposed workflow uses a fine-tuned model that takes the summaries generated in the first stage as inputs and outputs a concise intent statement. During this stage we drop all speculation from the summaries and clean the labels during training so that they don’t encourage hallucinations.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Evaluation approach\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Evaluation approach</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rymbe\">We use the <a href=\"https://arxiv.org/abs/2502.13149\" target=\"_blank\" rel=\"noopener noreferrer\">Bi-Fact approach</a> to evaluate the quality of a predicted intent against a reference intent. With this approach, we use a separate LLM call to split the reference and predicted intents into details of the intent that cannot be broken down further, which we call “atomic facts”. For example, “a one-way flight” would be an atomic fact, while “a flight from London to Kigali” would be two. We then count the number of reference facts that are entailed by the predicted intent and the number of predicted facts that are entailed by the reference intent. This enables us to know the <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\" target=\"_blank\" rel=\"noopener noreferrer\">precision</a> (how many of the predicted facts are correct) and <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\" target=\"_blank\" rel=\"noopener noreferrer\">recall</a> (how many of the true facts we correctly predicted) of our method and to calculate the <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall#F-measure\" target=\"_blank\" rel=\"noopener noreferrer\">F1 score</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-3a-FactCoverage.width-1250.jpg\" alt=\"A side-by-side comparison of &quot;Reference&quot; and &quot;Predicted&quot; flight booking facts using checkmarks and red X's to evaluate extraction accuracy.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-3a-FactCoverage.width-1250.jpg\" alt=\"A side-by-side comparison of &quot;Reference&quot; and &quot;Predicted&quot; flight booking facts using checkmarks and red X's to evaluate extraction accuracy.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"psr0n\"><i>Fact Coverage Analysis. Evaluating if reference facts were successfully captured in the predicted intent (</i><b><i>left</i></b><i>), and if predicted facts are supported by the reference intent (</i><b><i>right</i></b><i>).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"rymbe\">Working with atomic facts also helps to track how the different stages of the decomposed approach contribute to errors. Below we show how we analyze the flow of facts through the system to track missed details and hallucinations at each stage.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-4a-ErrorPropagation.width-1250.jpg\" alt=\"Two flowcharts showing Recall and Precision analysis to track where facts are missed or hallucinated during the extraction process.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-4a-ErrorPropagation.width-1250.jpg\" alt=\"Two flowcharts showing Recall and Precision analysis to track where facts are missed or hallucinated during the extraction process.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"psr0n\"><i>Propagation error analysis of recall and precision across both stages of our model.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Key results\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Key results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rymbe\">The decomposed approach of summarizing each screen separately and then extracting an intent from the sequence of generated summaries is helpful when using small models. We compare it against standard approaches, including <a href=\"https://en.wikipedia.org/wiki/Prompt_engineering#Chain-of-thought\" target=\"_blank\" rel=\"noopener noreferrer\">chain of thought prompting</a> (CoT) and end-to-end fine-tuning (E2E), and find that it outperforms both. This result holds true when we tested on both mobile device and web trajectories and for <a href=\"https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a> and <a href=\"https://huggingface.co/Qwen/Qwen2-VL-7B\" target=\"_blank\" rel=\"noopener noreferrer\">Qwen2</a> base models. We even find that applying the decomposed approach with the <a href=\"https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini 1.5 Flash 8B</a> model achieves comparable results to using <a href=\"https://arxiv.org/abs/2403.05530\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini 1.5 Pro</a> at a fraction of the cost and speed. See the <a href=\"https://aclanthology.org/2025.emnlp-main.949/\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a> for additional experiments.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-5-Performance.width-1250.png\" alt=\"Two bar charts comparing the BiFact F1 scores of Gemini 1.5 and Qwen2 models across Android and Web trajectories.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/IntentExtraction-5-Performance.width-1250.png\" alt=\"Two bar charts comparing the BiFact F1 scores of Gemini 1.5 and Qwen2 models across Android and Web trajectories.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"psr0n\"><i>Bi-Fact F1 scores comparing the decomposed method against two natural baselines: chain-of-thought prompting (CoT) and end-to-end fine-tuning (E2E). In all settings, our decomposed method outperforms baselines, and on the mobile device dataset, it is comparable to the performance of the large Gemini Pro model.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Conclusions &amp; future directions\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusions &amp; future directions</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rymbe\">We have shown that a decomposed approach to trajectory summarization can be helpful for intent understanding with small models. Ultimately, as models improve in performance and mobile devices acquire more processing power, we hope that on-device intent understanding can become a building block for many assistive features on mobile devices going forward.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Acknowledgements\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rymbe\"><i>Thank you to our paper coauthors: Noam Kahlon, Joel Oren, Omri Berkovitch, Sapir Caduri, Ido Dagan, and Anatoly Efros.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Unlocking health insights: Estimating advanced walking metrics with smartwatches",
      "link": "https://research.google/blog/unlocking-health-insights-estimating-advanced-walking-metrics-with-smartwatches/",
      "pubDate": "Wed, 14 Jan 2026 16:00:00 GMT",
      "isoDate": "2026-01-14T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Unlocking health insights: Estimating advanced walking metrics with smartwatches",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GaitMetrics-1-Performance.width-1250.png",
          "alt": "Box and swarm plots comparing Watch vs. Phone Mean Absolute Percentage Error (MAPE) for eight bilateral and unilateral gait metrics.",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"gyndb\">Gait metrics — measures like walking speed, step length, and double support time (i.e., the proportion of gait cycle when both feet are on the ground) — are <a href=\"https://pubmed.ncbi.nlm.nih.gov/31074770/\" target=\"_blank\" rel=\"noopener noreferrer\">known to be vital biomarkers</a> for assessing a person’s overall health, risk of falling, and progression of neurological or musculoskeletal conditions. Analyzing how a person walks, known as gait analysis, offers valuable, non-invasive insights into general well-being, injuries, and health concerns.</p><p data-block-key=\"fpr91\">Historically, measuring gait required expensive, specialized laboratory equipment, making continuous tracking impractical. While smartphones now offer a portable alternative using their embedded inertial measurement units (IMUs), they demand precise placement — such as a thigh pocket or belt — for the most accurate results. In contrast, smartwatches are worn on the wrist in a fixed location. This provides a much more practical and consistent platform for continuous tracking, even expanding the tracking window to phone-less scenarios like walking around the house.</p><p data-block-key=\"24bkt\">Despite this crucial logistical advantage, smartwatches have historically lagged behind smartphones in comprehensive gait metric evaluation. In our work, \"<a href=\"https://dl.acm.org/doi/10.1145/3715071.3750401\" target=\"_blank\" rel=\"noopener noreferrer\">Smartwatch-Based Walking Metrics Estimation</a>\", we sought to bridge this gap. We demonstrated that consumer smartwatches are a highly viable, accurate, and reliable platform for estimating a comprehensive suite of spatio-temporal gait metrics, with performance comparable to smartphone-based methods.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"A deep learning approach for the wrist\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">A deep learning approach for the wrist</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"gyndb\">To achieve this, we developed a multi-output (i.e., multi-head) deep learning model built on a <a href=\"https://arxiv.org/abs/1611.05267\" target=\"_blank\" rel=\"noopener noreferrer\">temporal convolutional network</a> (TCN) architecture identical for both smartwatch and smartphone data. This multi-head model is a key differentiator from prior TCN-based approaches, which often only provide temporal events (like contact points) that require drift-prone integration for spatial metrics like step length and gait speed. Our model, in contrast, directly estimates all spatio-temporal gait metrics.</p><p data-block-key=\"1u0ai\">Our model takes two key inputs, <i>user height</i> (a single scalar demographic input) and <i>raw IMU signals</i>, which include 3-axis accelerometer and 3-axis gyroscope data from a single on-wrist Pixel Watch at 50 Hz sampling frequency. The model architecture extracts embeddings from the IMU sensor input, which are then concatenated with the demographic data before the final prediction layers. The multi-head model output then directly estimates a comprehensive suite of measures, including <i>bilateral metrics</i>, one head each for gait speed and double support time, and <i>unilateral metrics</i>, two heads each (one for left and one for right foot) for step length, swing time, and stance time. Definitions for each of these metrics are defined as:</p><ul><li data-block-key=\"dq867\"><i>Gait speed:</i> Distance an individual travels divided by the time taken (in cm/s)</li><li data-block-key=\"chroa\"><i>Double support time:</i> Proportion of gait cycle when both feet are on the ground (in %)</li><li data-block-key=\"2lsv\"><i>Step length (unilateral):</i> Distance from the initial contact of one foot to the initial contact of the other foot (in cm)</li><li data-block-key=\"7omu9\"><i>Swing time (unilateral):</i> Duration within the gait cycle when the foot is not in contact with the ground (in ms)</li><li data-block-key=\"d5jn9\"><i>Stance time (unilateral):</i> Duration within the gait cycle when the foot is in contact with the ground (in ms)</li></ul><p data-block-key=\"2q3q1\">For data segmentation, we used 5-second windows with a 1-second overlap. We utilized <a href=\"https://en.wikipedia.org/wiki/Mean_absolute_percentage_error\" target=\"_blank\" rel=\"noopener noreferrer\">mean absolute percentage error</a> (MAPE) for the loss function, which uniquely optimizes for the relative accuracy across all multi-unit outputs (e.g., step length in cm, double support time in ms).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Rigorous validation in a large-scale study\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Rigorous validation in a large-scale study</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"gyndb\">To rigorously evaluate the model, we conducted a large-scale validation study featuring a large cohort of 246 participants and approximately 70,000 walking segments. Participants were screened to be over 18, not using assistive devices, and without balance- or gait-affecting conditions. Data was collected from two international cohorts: Google in Mountain View, California and Kyoto University in Japan.</p><p data-block-key=\"845gh\">For the reference (ground truth) measurements, we used a lab-grade <a href=\"https://protokinetics.com/zeno-walkway/\" target=\"_blank\" rel=\"noopener noreferrer\">Zeno Gait Walkway</a> system. Participants were outfitted with a Pixel Watch 1 on each wrist and four Pixel 6 phones placed in the front pocket, back pocket, backpack, and a cross-body bag.</p><p data-block-key=\"1jmfp\">The study protocol included a diverse range of walking patterns to ensure comprehensive evaluation:</p><ul><li data-block-key=\"9jr8g\"><i>Six minute walk test (6MWT):</i> Complete loops along the track at a self-paced speed</li><li data-block-key=\"7ikb\"><i>Fast pace walking:</i> Walk at a comfortably fast but steady pace</li><li data-block-key=\"abomk\"><i>Mild and moderate asymmetry:</i> Walk self-paced while wearing a hinged knee brace locked into specific flexion/extension angles</li></ul><p data-block-key=\"4qi77\">The smartwatch model was trained using data from both wrist-worn devices, while the smartphone model's testing phases exclusively utilized data from front and back pocket phone placements, given their expected prevalence and highest accuracy. We employed a <a href=\"https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation\" target=\"_blank\" rel=\"noopener noreferrer\">five-fold cross-validation strategy</a> to maximize the test cohort and prevent data leakage by assigning all data from a single participant to a single split.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Key findings\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Key findings</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"gyndb\">The results collectively demonstrated the accuracy, correlation, and reliability of the smartwatch-based method, showing comparable performance to smartphone estimates, despite the smartphone model being trained with approximately two times more data segments.</p><ol><li data-block-key=\"4601c\"><i>Strong validity and excellent reliability:</i> Across the 70,000 walking segments, smartwatch estimates demonstrated strong validity (<a href=\"https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\" target=\"_blank\" rel=\"noopener noreferrer\">Pearson</a> <i>r</i> &gt;0.80) and excellent reliability (<a href=\"https://en.wikipedia.org/wiki/Intraclass_correlation\" target=\"_blank\" rel=\"noopener noreferrer\">intraclass correlation coefficient</a> (ICC) &gt;0.80) for most metrics, including gait speed, step length, swing time, and stance time. Double support time showed moderately lower but acceptable ICCs (0.56−0.60) with narrow 95% confidence intervals, underscoring reliability.</li><li data-block-key=\"c778a\"><i>Comparable to Smartphone Estimates</i><b><i>:</i></b> A quantitative comparison of the MAPE (see below) and mean absolute error (MAE) showed non-significant differences (<i>p</i> &gt;0.05) between the Pixel Watch and Pixel phone across all measured gait metrics. This establishes the smartwatch as a viable and highly comparable platform for accurate gait analysis. Both the smartwatch and smartphone models significantly outperformed a naïve estimator, which merely predicted the mean of the training samples.</li><li data-block-key=\"3ujkk\"><i>The role of user height:</i> An ablation study confirmed that providing user height significantly improved the smartwatch's accuracy for estimating gait speed and step length, which highlights the distinct importance of user height for wrist-based gait metric estimation.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GaitMetrics-1-Performance.width-1250.png\" alt=\"Box and swarm plots comparing Watch vs. Phone Mean Absolute Percentage Error (MAPE) for eight bilateral and unilateral gait metrics.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GaitMetrics-1-Performance.width-1250.png\" alt=\"Box and swarm plots comparing Watch vs. Phone Mean Absolute Percentage Error (MAPE) for eight bilateral and unilateral gait metrics.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"zxovi\"><i>Gait parameter accuracy reflecting the mean absolute percentage error (MAPE) for Pixel Smartwatch (Watch) and Pixel Smartphone (Phone) (N=246 participants). Boxes indicate the interquartile range (Q1–Q3), whiskers show 1st–99th percentiles.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Impact and future directions\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Impact and future directions</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"gyndb\">These findings are a major step in establishing the ubiquitous on-wrist smartwatch as a foundational technology for accurate and reliable gait-based health tracking. By bringing comprehensive gait analysis out of the lab and onto the wrist, we can enable:</p><ul><li data-block-key=\"c17d4\"><i>Continuous and accessible tracking:</i> Longitudinal monitoring of gait metrics outside traditional clinical and laboratory settings</li><li data-block-key=\"aj74d\"><i>Early detection and prevention:</i> Greater potential for the early detection of disease, fall risk, and personalized rehabilitation planning.</li></ul><p data-block-key=\"7vvpk\">The smartwatch offers a practical and consistent platform for health tracking that overcomes the placement issues associated with smartphones. Our continued work will explore refining and expanding the suite of metrics to maximize the utility of smartwatches in proactive health tracking and recommendations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Acknowledgements\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"gyndb\"><i>The following researchers contributed to this work: Amir B. Farjadian, Shun Liao</i><footnote id=\"e3e42868-dd29-4890-8ad1-d9583a1e4153\">[e3e428]</footnote><i>, Alicia Y. Kokoszka, Kyle DeHolton, Jonathan Hsu, Jonathan Wang, Lawrence Cai, Mark Malhotra, Shwetak Patel, Anupam Pathak, Ming-Zher Poh.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Hard-braking events as indicators of road segment crash risk",
      "link": "https://research.google/blog/hard-braking-events-as-indicators-of-road-segment-crash-risk/",
      "pubDate": "Mon, 12 Jan 2026 16:00:00 GMT",
      "isoDate": "2026-01-12T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Hard-braking events as indicators of road segment crash risk",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/HBEs-1-RoadSegs.width-1250.png",
          "alt": "Line graph titled \"Crash Segments\" showing the number of road segments with crashes from 2016 to 2025.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/HBEs-2-CrashRates.width-1250.png",
          "alt": "Comparison graphs for California and Virginia showing the correlation between HBE rates and crash rates by road type.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/HBEs-3-Merge.width-1250.png",
          "alt": "Street view and aerial map of the Highway 101 and 880 merge with arrows and crash warning icons indicating an intersection with high crash rate.",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"tj11f\">Traffic safety evaluation has traditionally relied on police-reported crash statistics, often considered the \"gold standard\" because they directly correlate with fatalities, injuries, and property damage. However, relying on historical crash data for predictive modeling presents significant challenges, because such data is inherently a \"lagging\" indicator. Also, crashes are statistically rare events on arterial and local roads, so it can take years to accumulate sufficient data to establish a valid safety profile for a specific <a href=\"https://highways.dot.gov/safety/other/older-road-user/handbook-designing-roadways-aging-population/chapter-4-roadway\" target=\"_blank\" rel=\"noopener noreferrer\">road segment</a>. This sparsity paired with inconsistent reporting standards across regions complicates the development of robust risk prediction models. Proactive safety assessment requires \"leading\" measures: proxies for crash risk that correlate with safety outcomes but occur more frequently than crashes.</p><p data-block-key=\"72kqv\">In \"<a href=\"https://arxiv.org/abs/2601.06327\" target=\"_blank\" rel=\"noopener noreferrer\">From Lagging to Leading: Validating Hard Braking Events as High-Density Indicators of Segment Crash Risk</a>\", we evaluate the efficacy of hard-braking events (HBEs) as a scalable surrogate for crash risk. An HBE is an instance where a vehicle’s forward deceleration exceeds a specific threshold (-3m/s²), which we interpret as an evasive maneuver. HBEs facilitate network-wide analysis because they are sourced from connected vehicle data, unlike proximity-based surrogates like time-to-collision that frequently necessitate the use of fixed sensors. We established a statistically significant positive correlation between the rates of crashes (of any severity level) and HBE frequency by combining public crash data from <a href=\"http://virginiaroads.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Virginia</a> and <a href=\"https://data.ca.gov/dataset/ccrs\" target=\"_blank\" rel=\"noopener noreferrer\">California</a> with anonymized, aggregated HBE information from the <a href=\"https://www.android.com/intl/en_us/auto/\" target=\"_blank\" rel=\"noopener noreferrer\">Android Auto platform</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Data density</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tj11f\">To validate the utility of this metric, we analyzed 10 years of public crash data alongside aggregated HBE measurements. The immediate advantage of HBEs is the density of the signal. Our analysis of road segments in California and Virginia revealed that the number of segments with observed HBEs was 18 times greater than those with reported crashes. While crash data is notoriously sparse — requiring years to observe a single event on some local roads — HBEs provide a continuous stream of data, effectively filling the gaps in the safety map.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/HBEs-1-RoadSegs.width-1250.png\" alt=\"Line graph titled &quot;Crash Segments&quot; showing the number of road segments with crashes from 2016 to 2025.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/HBEs-1-RoadSegs.width-1250.png\" alt=\"Line graph titled &quot;Crash Segments&quot; showing the number of road segments with crashes from 2016 to 2025.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"gm9lm\"><i>HBEs are observed on 18x more road segments compared to reported crashes.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Statistical validation</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tj11f\">The core objective was to determine if a high frequency of HBEs causally links to a high rate of crashes. We employed <a href=\"https://en.wikipedia.org/wiki/Poisson_regression\" target=\"_blank\" rel=\"noopener noreferrer\">negative binomial</a> (NB) regression models, a standard approach in the <a href=\"https://highways.dot.gov/safety/data-analysis-tools/highway-safety-manual\" target=\"_blank\" rel=\"noopener noreferrer\">Highway Safety Manual</a> (HSM), to account for a higher degree of variance than is typically found in crash data.</p><p data-block-key=\"c5n3k\">Our model structure controlled for various confounding factors, including:</p><ul><li data-block-key=\"3bkib\"><b>Exposure:</b> Traffic volume and segment length.</li><li data-block-key=\"ikh1\"><b>Infrastructure:</b> Road type (local, arterial, highway), slope, and cumulative turning angle.</li><li data-block-key=\"31eq0\"><b>Dynamics:</b> Presence of ramps and change in the number of lanes.</li></ul><p data-block-key=\"91h23\">The results demonstrated a statistically significant association between HBE rates and crash rates across both states. Road segments with higher frequencies of hard braking consistently exhibited higher crash rates, a relationship that holds true across different road types, from local arterials to <a href=\"https://en.wikipedia.org/wiki/Controlled-access_highway\" target=\"_blank\" rel=\"noopener noreferrer\">controlled-access highways</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/HBEs-2-CrashRates.width-1250.png\" alt=\"Comparison graphs for California and Virginia showing the correlation between HBE rates and crash rates by road type.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/HBEs-2-CrashRates.width-1250.png\" alt=\"Comparison graphs for California and Virginia showing the correlation between HBE rates and crash rates by road type.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"gm9lm\"><i>Crash Rate vs. HBE rate for different types of roads in California and Virginia.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"tj11f\">The regression analysis also quantified the impact of specific infrastructure elements. For instance, the presence of a ramp on a road segment was positively associated with crash risk in both states, likely due to the weaving maneuvers required for merging.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Case study: High-risk merge identification</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tj11f\">To visualize the practical application of this metric, we examined a <a href=\"https://www.google.com/maps/place/37%C2%B021&amp;#x27;50.9%22N+121%C2%B054&amp;#x27;06.1%22W/@37.3641552,-121.9019895,310m/data=!3m1!1e3!4m4!3m3!8m2!3d37.364128!4d-121.901694?entry=ttu&amp;g_ep=EgoyMDI1MTIwOS4wIKXMDSoKLDEwMDc5MjA2N0gBUAM%3D\" target=\"_blank\" rel=\"noopener noreferrer\">freeway merge segment</a> in California connecting Highway 101 and Highway 880. Historical data indicates this segment has an HBE rate approximately 70 times higher than the average California freeway, and averaging a crash every six weeks for a decade.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/HBEs-3-Merge.width-1250.png\" alt=\"Street view and aerial map of the Highway 101 and 880 merge with arrows and crash warning icons indicating an intersection with high crash rate.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/HBEs-3-Merge.width-1250.png\" alt=\"Street view and aerial map of the Highway 101 and 880 merge with arrows and crash warning icons indicating an intersection with high crash rate.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"gm9lm\"><i>Freeway merge segment in California Bay Area with one crash every six weeks and a 70x higher than average HBE rate.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"tj11f\">When analyzing the connected vehicle data for this location, we found that it ranked in the top 1% of all road segments for HBE frequency. The HBE signal successfully flagged this outlier without relying on the decade of crash reports it took to statistically confirm the risk. This alignment validates HBEs as a reliable proxy capable of identifying high-risk locations even in the absence of long-term collision history.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Real-world application</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tj11f\">Validating HBEs as a reliable proxy for crash risk transforms a raw sensor metric into a trusted safety tool for road management. This validation supports the use of connected vehicle data for network-wide traffic safety assessment, offering enhanced spatial and temporal granularity. While these results indicate utility for road segment risk determination, they do not draw conclusions about location-independent driving behavior risk.</p><p data-block-key=\"24ug6\">The <a href=\"https://research.google/blog/introducing-mobility-ai-advancing-urban-transportation/\">Mobility AI</a> team at Google Research is working with <a href=\"https://mapsplatform.google.com/resources/blog/roads-management-insights-is-now-available-build-safer-smarter-and-more-efficient-road-networks/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Maps Platform</a> to externalize these HBE datasets as a part of <a href=\"https://mapsplatform.google.com/resources/blog/roads-management-insights-is-now-available-build-safer-smarter-and-more-efficient-road-networks/\" target=\"_blank\" rel=\"noopener noreferrer\">Roads Management Insights</a> offering. By integrating these high-density signals, transportation agencies can access aggregated, anonymized data that is substantially more fresh and that covers a wider breadth of the road network compared to traditional crash statistics. This allows for the identification of high-risk locations using leading indicators rather than relying solely on lagging and sparse collision records.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Future work</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tj11f\">While this study confirms that HBEs are a robust leading indicator of crash risk, there are opportunities to further refine this signal. We are currently investigating mechanisms to spatially cluster homogenous road segments to reduce data sparsity even further. Addressing these limitations will enable the transition from risk identification to targeted engineering, where high-density data informs specific infrastructure interventions ranging from signal timing adjustments and improved signage to the geometric redesign of high-risk merge lanes.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tj11f\"><i>This work was a collaborative effort involving researchers from Google and Virginia Tech. We thank our co-authors Shantanu Shahane, Shoshana Vasserman, Carolina Osorio, Yi-fan Chen, Ivan Kuznetsov, Kristin White, Justyna Swiatkowska, and Feng Guo. We also appreciate the contributions of Aurora Cheung, Andrew Stober, Reymund Dumlao, and Nick Kan in translating this research into practical applications.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Next generation medical image interpretation with MedGemma 1.5 and medical speech to text with MedASR",
      "link": "https://research.google/blog/next-generation-medical-image-interpretation-with-medgemma-15-and-medical-speech-to-text-with-medasr/",
      "pubDate": "Mon, 12 Jan 2026 16:00:00 GMT",
      "isoDate": "2026-01-12T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Next generation medical image interpretation with MedGemma 1.5 and medical speech to text with MedASR",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MedGemma15-0-Hero.width-1250.png",
          "alt": "MedGemma workflow showing steps from use case definition and model selection to scaling on Google Cloud.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MedGemma15-2-Collection.width-1250.png",
          "alt": "MedGemma Collection infographic showing multimodal AI models for 2D imaging, text, speech, and advanced radiology.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MedGemma15-3-CT.width-1250.png",
          "alt": "Diagram of MedGemma 1.5 analyzing abdominal CT slices, followed by an AI response and a radiologist’s evaluation.",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MedGemma15-4-Imaging.width-1250.png",
          "alt": "Bar chart comparing MedGemma 1 and 1.5, showing performance improvements across various medical imaging and report metrics.",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MedGemma15-5-CXR.width-1250.png",
          "alt": "Diagram of MedGemma 1.5 analyzing chest X-rays, followed by an AI response and a radiologist’s evaluation.",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"2ajpj\">The adoption of artificial intelligence in healthcare is accelerating dramatically, with the healthcare industry adopting AI at <a href=\"https://menlovc.com/perspective/2025-the-state-of-ai-in-healthcare/\" target=\"_blank\" rel=\"noopener noreferrer\">twice the rate of the broader economy</a>. In support of this transformation, last year Google published the <a href=\"https://research.google/blog/medgemma-our-most-capable-open-models-for-health-ai-development/\">MedGemma collection</a> of open medical generative AI models through our <a href=\"http://goo.gle/hai-def\" target=\"_blank\" rel=\"noopener noreferrer\">Health AI Developer Foundations</a> (HAI-DEF) program. HAI-DEF models like MedGemma are intended as starting points for developers to evaluate and adapt to their medical use cases, and they can be easily scaled on <a href=\"https://console.cloud.google.com/vertex-ai/model-garden?inv=1&amp;invt=Ab2Ldw&amp;pageState=(%22galleryStateKey%22:(%22f%22:(%22g%22:%5B%22goals%22%5D,%22o%22:%5B%22Health%20%26%20Life%20Sciences%22%5D),%22s%22:%22%22))\" target=\"_blank\" rel=\"noopener noreferrer\">Google Cloud through Vertex AI</a>. The response to the MedGemma release has been incredible, with millions of downloads and hundreds of <a href=\"https://huggingface.co/models?other=or:base_model:finetune:google/medgemma-4b-it,base_model:finetune:google/medgemma-4b-pt,base_model:finetune:google/medgemma-27b-it,base_model:finetune:google/medgemma-27b-text-it\" target=\"_blank\" rel=\"noopener noreferrer\">community-built variants</a> published on Hugging Face.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma15-0-Hero.width-1250.png\" alt=\"MedGemma workflow showing steps from use case definition and model selection to scaling on Google Cloud.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma15-0-Hero.width-1250.png\" alt=\"MedGemma workflow showing steps from use case definition and model selection to scaling on Google Cloud.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"wh76r\"><i>Flow chart describing the intended use of MedGemma as a developer tool.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"2ajpj\">Today, we’re building on that momentum by releasing <a href=\"https://huggingface.co/google/medgemma-1.5-4b-it\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma 1.5 4B</a> and launching the <a href=\"https://www.kaggle.com/competitions/med-gemma-impact-challenge\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma Impact Challenge</a> hackathon on <a href=\"https://www.kaggle.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Kaggle</a>. Guided by direct feedback from the community, this model update enables developers to more effectively adapt MedGemma for applications that involve several medical imaging modalities:</p><ul><li data-block-key=\"80k78\"><i>High-dimensional medical imaging:</i> Computed tomography (CT), magnetic resonance imaging (MRI), and histopathology</li><li data-block-key=\"1kea5\"><i>Longitudinal medical imaging:</i> Chest X-ray time series review</li><li data-block-key=\"cs0ka\"><i>Anatomical localization:</i> Localization of anatomical features in chest X-rays</li><li data-block-key=\"7ivr9\"><i>Medical document understanding:</i> Extracting structured data from medical lab reports</li></ul><p data-block-key=\"3j477\">MedGemma 1.5 4B also improves accuracy on core capabilities for text, medical records and 2D images over MedGemma 1 4B. We are publishing the updated 4B model size today to provide an ideal compute-efficient starting point for developers that is small enough to run offline, and developers can continue to use our <a href=\"https://huggingface.co/google/medgemma-27b-it\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma 1 27B parameter model</a> for more complex text-based applications. Full details of the MedGemma 1.5 4B model and performance benchmarks appear in the <a href=\"https://developers.google.com/health-ai-developer-foundations/medgemma/model-card\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma 1.5 model card</a>.</p><p data-block-key=\"1rqne\">We also recently released <a href=\"https://developers.google.com/health-ai-developer-foundations/medasr/\" target=\"_blank\" rel=\"noopener noreferrer\">MedASR</a> (on <a href=\"https://huggingface.co/google/medasr\" target=\"_blank\" rel=\"noopener noreferrer\">Hugging Face</a> and <a href=\"https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/medasr\" target=\"_blank\" rel=\"noopener noreferrer\">Vertex AI</a>), a new open automated speech recognition (ASR) model that was fine-tuned for medical dictation. The initial release of MedASR enables developers to convert medical speech to text and seamlessly pairs with MedGemma for advanced reasoning tasks.</p><p data-block-key=\"505hu\">MedGemma 1.5, MedASR and all other HAI-DEF models, like the <a href=\"https://developers.google.com/health-ai-developer-foundations/medsiglip\" target=\"_blank\" rel=\"noopener noreferrer\">MedSigLIP image encoder</a>, remain free for research and commercial use and can be downloaded from <a href=\"https://huggingface.co/collections/google/health-ai-developer-foundations-hai-def\" target=\"_blank\" rel=\"noopener noreferrer\">Hugging Face</a> or trained and adapted into scalable applications in the cloud on <a href=\"https://console.cloud.google.com/vertex-ai/model-garden?inv=1&amp;invt=Ab2Ldw&amp;pageState=(%22galleryStateKey%22:(%22f%22:(%22g%22:%5B%22goals%22%5D,%22o%22:%5B%22Health%20%26%20Life%20Sciences%22%5D),%22s%22:%22%22))\" target=\"_blank\" rel=\"noopener noreferrer\">Vertex AI</a><a href=\"https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4\" target=\"_blank\" rel=\"noopener noreferrer\">.</a></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma15-2-Collection.width-1250.png\" alt=\"MedGemma Collection infographic showing multimodal AI models for 2D imaging, text, speech, and advanced radiology.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma15-2-Collection.width-1250.png\" alt=\"MedGemma Collection infographic showing multimodal AI models for 2D imaging, text, speech, and advanced radiology.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"wh76r\"><i>Summary of the MedGemma collection of models and their capabilities.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"The MedGemma Impact Challenge\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The MedGemma Impact Challenge</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"2ajpj\">We want to encourage developers to explore additional creative and impactful uses of MedGemma models to transform healthcare. To this end, we are excited to announce the <a href=\"https://www.kaggle.com/competitions/med-gemma-impact-challenge/overview\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma Impact Challenge</a>, a Kaggle-hosted hackathon with $100,000 in prizes. This hackathon is open to all developers and offers an opportunity to build on MedGemma and HAI-DEF to showcase the potential of AI in healthcare and life sciences. We look forward to what you all will build!</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Improved performance for medical imaging\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Improved performance for medical imaging use cases</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"2ajpj\">MedGemma was designed from the ground up as a multimodal model, reflecting the multimodal nature of medicine. MedGemma 1 included support for interpreting two-dimensional medical images, including chest X-rays, dermatology images, fundus images and histopathology patches.</p><p data-block-key=\"7rogg\">With MedGemma 1.5, we are expanding support for high-dimensional medical imaging, starting with three-dimensional volume representations of <a href=\"https://en.wikipedia.org/wiki/CT_scan\" target=\"_blank\" rel=\"noopener noreferrer\">CT imaging</a> and <a href=\"https://en.wikipedia.org/wiki/Magnetic_resonance_imaging\" target=\"_blank\" rel=\"noopener noreferrer\">MRI</a>, as well as whole-slide <a href=\"https://en.wikipedia.org/wiki/Digital_pathology\" target=\"_blank\" rel=\"noopener noreferrer\">histopathology imaging</a>. Developers can create applications in which multiple slices (for CT or MRI) or multiple patches (for histopathology) are provided as input along with a prompt that describes the task.</p><p data-block-key=\"b8dq3\">On internal benchmarks, the baseline absolute accuracy of MedGemma 1.5 improved by 3% over MedGemma 1 (61% vs. 58%) on classification of disease-related CT findings and by 14% (65% vs. 51%) on classification of disease-related MRI findings, averaged over findings. Additionally, on an internal diverse benchmark of histopathology slides and associated findings, the fidelity of MedGemma 1.5’s predictions, based on <a href=\"https://en.wikipedia.org/wiki/ROUGE_(metric)\" target=\"_blank\" rel=\"noopener noreferrer\">ROUGE-L</a> score on cases with exactly one histopathology slide, improved by 0.47 over MedGemma 1 (0.49 vs. 0.02), matching the 0.498 score achieved by the task-specific <a href=\"https://www.modernpathology.org/article/S0893-3952(25)00184-X/fulltext\" target=\"_blank\" rel=\"noopener noreferrer\">PolyPath model</a>.</p><p data-block-key=\"a6o0m\">This new high-dimensional support is the natural evolution of <a href=\"https://research.google/blog/taking-medical-imaging-embeddings-3d/\">CT foundation</a>, our previous API-based tool for generation of CT embeddings. To our knowledge, MedGemma 1.5 is the first public release of an open multimodal large language model that can interpret high-dimensional medical data while also retaining the ability to interpret general 2D data and text. Although these capabilities are in their early stages and remain imperfect, developers will achieve improved results by fine-tuning MedGemma models on their own data, and we hope to continually improve MedGemma models over time. We’ve released tutorial notebooks that illustrate how to use this high dimensional image capability for CT (<a href=\"https://github.com/Google-Health/medgemma/blob/main/notebooks/high_dimensional_ct_hugging_face.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">Hugging Face</a>, <a href=\"https://github.com/Google-Health/medgemma/blob/main/notebooks/high_dimensional_ct_model_garden.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">Model Garden</a>) and histopathology (<a href=\"https://github.com/Google-Health/medgemma/blob/main/notebooks/high_dimensional_pathology_hugging_face.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">Hugging Face</a>, <a href=\"https://github.com/Google-Health/medgemma/blob/main/notebooks/high_dimensional_pathology_model_garden.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">Model Garden</a>).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma15-3-CT.width-1250.png\" alt=\"Diagram of MedGemma 1.5 analyzing abdominal CT slices, followed by an AI response and a radiologist’s evaluation.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma15-3-CT.width-1250.png\" alt=\"Diagram of MedGemma 1.5 analyzing abdominal CT slices, followed by an AI response and a radiologist’s evaluation.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"wh76r\"><i>Example showing how MedGemma 1.5 4B can be used to interpret a CT volume along with commentary by a board-certified thoracic radiologist on the quality of the output. Note that MedGemma is not intended to be used without appropriate validation, adaptation and/or making meaningful modification by developers for their specific use case.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"2ajpj\">MedGemma 1.5 4B baseline performance also improves significantly over MedGemma 1 4B on several other forms of medical image interpretation:</p><ul><li data-block-key=\"8crrl\"><i>Anatomical localization:</i> Localization of anatomical features in chest X-rays; improvement of 35% <a href=\"https://en.wikipedia.org/wiki/Jaccard_index\" target=\"_blank\" rel=\"noopener noreferrer\">intersection over union</a> on the <a href=\"https://physionet.org/content/chest-imagenome/1.0.0/\" target=\"_blank\" rel=\"noopener noreferrer\">Chest ImaGenome</a> benchmark (38% vs. 3%). See our <a href=\"https://github.com/Google-Health/medgemma/blob/main/notebooks/cxr_anatomy_localization_with_hugging_face.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">tutorial notebook on anatomical localization</a>.</li><li data-block-key=\"8euem\"><i>Longitudinal medical imaging:</i> Chest X-ray time series review; improvement of 5% macro accuracy on the <a href=\"https://physionet.org/content/ms-cxr-t/1.0.0/\" target=\"_blank\" rel=\"noopener noreferrer\">MS-CXR-T</a> benchmark (66% vs. 61%). See the example below and our <a href=\"https://github.com/Google-Health/medgemma/blob/main/notebooks/cxr_longitudinal_comparison_with_hugging_face.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">tutorial notebook on longitudinal medical imaging</a>.</li><li data-block-key=\"2g05u\"><i>Medical image interpretation:</i> Our internal single-image benchmarks for CXR, dermatology, histopathology and ophthalmology; improvement of 3% (62% vs. 59%).</li><li data-block-key=\"8en64\"><i>Lab report extraction:</i> Extracting structured data from medical lab reports (lab type, value, units); improvement of 18% retrieval macro F1 on an internal benchmark of lab reports (78% vs. 60%).</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma15-4-Imaging.width-1250.png\" alt=\"Bar chart comparing MedGemma 1 and 1.5, showing performance improvements across various medical imaging and report metrics.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma15-4-Imaging.width-1250.png\" alt=\"Bar chart comparing MedGemma 1 and 1.5, showing performance improvements across various medical imaging and report metrics.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"wh76r\"><i>MedGemma 1.5 4B improves support for medical imaging, exceeding the performance of MedGemma 1 4B on high-dimensional image interpretation, anatomy localization and longitudinal disease assessment in chest X-rays, general medical image interpretation and extraction of content from medical laboratory reports.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma15-5-CXR.width-1250.png\" alt=\"Diagram of MedGemma 1.5 analyzing chest X-rays, followed by an AI response and a radiologist’s evaluation.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma15-5-CXR.width-1250.png\" alt=\"Diagram of MedGemma 1.5 analyzing chest X-rays, followed by an AI response and a radiologist’s evaluation.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"wh76r\"><i>Example showing how MedGemma 1.5 4B can be used to interpret a longitudinal pair of chest X-rays along with commentary by a board-certified thoracic radiologist on the quality of the output. Note that MedGemma is not intended to be used without appropriate validation, adaptation and/or making meaningful modification by developers for their specific use case.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"2ajpj\">Additionally, MedGemma applications that are deployed on <a href=\"https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/medgemma\" target=\"_blank\" rel=\"noopener noreferrer\">Google Cloud</a> now include <a href=\"https://cloud.google.com/blog/topics/developers-practitioners/integrating-medgemma-into-clinical-workflows-just-got-easier?e=0\" target=\"_blank\" rel=\"noopener noreferrer\">full DICOM support</a>, making it even easier to adapt MedGemma for medical imaging applications.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Improvements to text functionality\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Improvements to text functionality</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"2ajpj\">Beyond the improved support for medical images, we’ve worked hard to improve the baseline medical text functionality of MedGemma. Through the addition of new training datasets and training techniques, MedGemma 1.5 4B improves over MedGemma 1 4B by 5% on <a href=\"https://arxiv.org/abs/2009.13081\" target=\"_blank\" rel=\"noopener noreferrer\">MedQA</a> (69% vs. 64%) and by 22% on text-based EHR question-answering with <a href=\"https://arxiv.org/abs/2507.05201\" target=\"_blank\" rel=\"noopener noreferrer\">EHRQA</a> (90% vs. 68%).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma15-6-Text.width-1250.png\" alt=\"Bar chart showing MedGemma 1.5 4B outperforming version 1 in MedQA and EHRQA medical text accuracy scores.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma15-6-Text.width-1250.png\" alt=\"Bar chart showing MedGemma 1.5 4B outperforming version 1 in MedQA and EHRQA medical text accuracy scores.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"wh76r\"><i>MedGemma 1.5 4B improves at text-based tasks over MedGemma 1 4B, including on medical reasoning (MedQA) and electronic health record information retrieval (EHRQA).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"MedASR: An open model for medical automated s\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">MedASR: An open model for medical automated speech recognition</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"2ajpj\">While text is currently the primary interface for large language models, verbal communication remains crucial in many aspects of healthcare, including medical dictation and live conversations between patients and providers. Speech also provides a more natural way to interact with a language model.</p><p data-block-key=\"1cq1m\">To support these use cases that require the model to be familiar with healthcare’s specialized vocabulary, we developed the <a href=\"https://developers.google.com/health-ai-developer-foundations/medasr\" target=\"_blank\" rel=\"noopener noreferrer\">MedASR</a> speech to text model to transcribe speech from the medical domain. MedASR can be used both to transcribe medical dictation and as a natural method of generating prompts for MedGemma. We compared MedASR’s performance to <a href=\"https://huggingface.co/openai/whisper-large-v3\" target=\"_blank\" rel=\"noopener noreferrer\">Whisper large-v3</a>, a generalist ASR model, and found that MedASR had 58% fewer errors on <a href=\"https://physionet.org/content/egd-cxr/1.0.0/\" target=\"_blank\" rel=\"noopener noreferrer\">chest X-ray dictations</a> (5.2% vs. 12.5% word error rate, <a href=\"https://en.wikipedia.org/wiki/Word_error_rate\" target=\"_blank\" rel=\"noopener noreferrer\">WER</a>) and 82% fewer errors on an internal medical dictation benchmark with diverse specialties and speakers (5.2% vs. 28.2% WER). We’ve released a collection of <a href=\"https://github.com/Google-Health/medasr/tree/main/notebooks\" target=\"_blank\" rel=\"noopener noreferrer\">tutorial notebooks</a> to help developers create and adapt their own systems that combine the audio understanding of MedASR with the clinical reasoning of MedGemma 1.5. Learn more in the <a href=\"https://developers.google.com/health-ai-developer-foundations/medasr/model-card\" target=\"_blank\" rel=\"noopener noreferrer\">MedASR model card</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma15-7-MedASR.width-1250.png\" alt=\"Flowcharts showing MedASR converting spoken dictation to transcripts and spoken prompts into answers via MedGemma 1.5.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma15-7-MedASR.width-1250.png\" alt=\"Flowcharts showing MedASR converting spoken dictation to transcripts and spoken prompts into answers via MedGemma 1.5.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"wh76r\"><i>MedASR can be used either for transcribing medical dictation (</i><b><i>top</i></b><i>) or to dictate prompts for MedGemma (</i><b><i>bottom</i></b><i>).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"How developers are using MedGemma\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How developers are using MedGemma</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"2ajpj\">We’re seeing health tech startups and developers around the world leverage MedGemma to accelerate their research and product development across a broad range of use cases and settings.</p><p data-block-key=\"5mn2u\">As one example, <a href=\"https://qmed.asia/\" target=\"_blank\" rel=\"noopener noreferrer\">Qmed Asia</a> has adapted MedGemma for use into <a href=\"https://cpg.qmed.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">askCPG,</a> a conversational interface to Malaysia’s 150+ clinical practice guidelines. According to the <a href=\"https://mymahtas.moh.gov.my/\" target=\"_blank\" rel=\"noopener noreferrer\">Ministry of Health Malaysia</a>, the conversational interface has made navigating the Malaysian Clinical Practice Guidelines more practical in day-to-day clinical decision support, and the multimodal medical image extension with MedGemma has been especially well-received in pilot deployments.</p><p data-block-key=\"b8gmq\">Additionally, Taiwan’s National Health Insurance Administration has applied MedGemma for evaluating preoperative assessments for lung cancer surgery. By extracting key data from over 30,000 pathology reports and unstructured data using MedGemma, they performed statistical analyses to assess the preoperative medical condition of patients. This effort aims to inform policy decisions that improve decision-making for surgical resection in order to improve patient outcomes.</p><p data-block-key=\"66v07\">MedGemma has also been <a href=\"https://scholar.google.com/scholar?cites=15997809831302344826&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en\" target=\"_blank\" rel=\"noopener noreferrer\">cited extensively</a> in medical AI research articles since its release earlier this year, comparing favorably to other models as a base model for <a href=\"https://arxiv.org/abs/2507.03152\" target=\"_blank\" rel=\"noopener noreferrer\">understanding medical text</a>, <a href=\"https://static1.squarespace.com/static/59d5ac1780bd5ef9c396eda6/t/689b83f87336b643e181ecec/1755022328956/97_camera_ready+-+Jaesik+Kim.pdf#page=10.36\" target=\"_blank\" rel=\"noopener noreferrer\">multidisciplinary team decision making</a>, <a href=\"https://arxiv.org/abs/2508.09225\" target=\"_blank\" rel=\"noopener noreferrer\">mammography reporting</a>, and other clinical scenarios.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Get started\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Get started</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"2ajpj\">You can access all variants of MedGemma via <a href=\"https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4\" target=\"_blank\" rel=\"noopener noreferrer\">the Hugging Face collection</a> or <a href=\"https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/medgemma\" target=\"_blank\" rel=\"noopener noreferrer\">Vertex AI on Google Cloud</a>. MedASR is currently available on <a href=\"https://huggingface.co/google/medasr\" target=\"_blank\" rel=\"noopener noreferrer\">Hugging Face</a> and <a href=\"https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/medasr\" target=\"_blank\" rel=\"noopener noreferrer\">Vertex AI</a>. To showcase your ideas for the next generation of medical AI applications, check out the <a href=\"https://www.kaggle.com/competitions/med-gemma-impact-challenge/overview\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma Impact Challenge</a>.</p><p data-block-key=\"9pn2k\">Visit our <a href=\"https://github.com/Google-Health/medgemma/tree/main/notebooks\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma GitHub repository</a> to explore our expanded collection of tutorials. These include our existing tutorials on running inference and <a href=\"https://arxiv.org/abs/2106.09685\" target=\"_blank\" rel=\"noopener noreferrer\">LoRA</a>-based supervised fine-tuning and a new <a href=\"https://github.com/Google-Health/medgemma/blob/main/notebooks/reinforcement_learning_with_hugging_face.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\">tutorial on reinforcement learning</a>, a tuning method that is particularly effective at learning complex tasks without compromising existing model abilities.</p><p data-block-key=\"58tpr\">Visit the <a href=\"http://goo.gle/hai-def\" target=\"_blank\" rel=\"noopener noreferrer\">HAI-DEF site</a> for resources on MedGemma 1.5 and other Health AI Developer Foundations models. To stay up to date, please sign up for our <a href=\"https://services.google.com/fb/forms/hai-def-news/\" target=\"_blank\" rel=\"noopener noreferrer\">newsletter</a>. For technical support, please use the <a href=\"https://discuss.ai.google.dev/c/hai-def\" target=\"_blank\" rel=\"noopener noreferrer\">HAI-DEF forum</a>.</p><p data-block-key=\"fo75k\">We’re very excited by what the community will build with these new models and welcome your <a href=\"https://services.google.com/fb/forms/hai-def-feedback/\" target=\"_blank\" rel=\"noopener noreferrer\">feedback</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma15-8-Choices.width-1250.png\" alt=\"This table compares models MedGemma 1.5 4B, MedSigLIP, and MedASR, categorizing them by typical use cases, such as image interpretation or speech dictation, and ranks their compute requirements from low to high.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma15-8-Choices.width-1250.png\" alt=\"This table compares models MedGemma 1.5 4B, MedSigLIP, and MedASR, categorizing them by typical use cases, such as image interpretation or speech dictation, and ranks their compute requirements from low to high.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"wh76r\"><i>This table summarizes model features and can help you understand which model is ideal for your use case.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Note on datasets\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Note on datasets</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"2ajpj\">Models were trained and evaluated on a mix of public and private de-identified datasets. Google and its partners utilize datasets that have been rigorously anonymized or de-identified to ensure the protection of individual research participants and patient privacy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Disclaimer\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Disclaimer</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"2ajpj\">HAI-DEF models, including MedGemma and MedASR, are intended to be used as a starting point that enables efficient development of downstream healthcare applications involving medical text and images. HAI-DEF models are not intended to be used without appropriate validation, adaptation and/or making meaningful modification by developers for their specific use case. The outputs generated by these models are not intended to directly inform clinical diagnosis, patient management decisions, treatment recommendations, or any other direct clinical practice applications. Performance benchmarks reported here highlight baseline capabilities and are not intended to imply that MedGemma is safe to use in any given medical application. Inaccurate model outputs beyond those shown here are possible. All model outputs should be considered preliminary and require independent verification, clinical correlation, and further investigation through established research and development methodologies. Please refer to <a href=\"https://developers.google.com/health-ai-developer-foundations/terms\" target=\"_blank\" rel=\"noopener noreferrer\">terms of use</a> and <a href=\"https://developers.google.com/health-ai-developer-foundations/prohibited-use-policy\" target=\"_blank\" rel=\"noopener noreferrer\">prohibited use policy</a> for more details.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Acknowledgments\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"2ajpj\"><i>MedGemma, the MedGemma Impact Challenge, and MedASR are the products of a collaboration between teams at Google. We thank the many people who contributed to this work, including the engineering and cross-functional members of the Health AI, Gemma, and Kaggle teams, as well as our sponsors in Google Research and Google DeepMind.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Dynamic surface codes open new avenues for quantum error correction",
      "link": "https://research.google/blog/dynamic-surface-codes-open-new-avenues-for-quantum-error-correction/",
      "pubDate": "Mon, 12 Jan 2026 16:00:00 GMT",
      "isoDate": "2026-01-12T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Dynamic surface codes open new avenues for quantum error correction",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DynamicSC1_Example.width-1250.png",
          "alt": "DynamicSC1_Example",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DynamicSC2_DetectingRegionsHERO.width-1250.png",
          "alt": "DynamicSC2_DetectingRegionsHERO",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DynamicSC3_Hex.width-1250.png",
          "alt": "DynamicSC3_Hex",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DynamicSC4_Walking.width-1250.png",
          "alt": "DynamicSC4_Walking",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DynamicSC5_iSWAP.width-1250.png",
          "alt": "DynamicSC5_iSWAP",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"eqcg2\">Quantum error correction (QEC) is crucial for reaching the ultra low error rate necessary for <a href=\"https://research.google/blog/a-new-quantum-algorithm-for-classical-mechanics-with-an-exponential-speedup/\">useful quantum algorithms</a>. At Google Quantum AI, our quantum processors use physical qubits constructed from small superconducting circuits, which are <a href=\"https://research.google/blog/overcoming-leakage-on-error-corrected-quantum-processors/\">susceptible to noise</a>. QEC allows us to combine numerous physical qubits into <a href=\"https://research.google/blog/making-quantum-error-correction-work/\">logical qubits</a>, which are robust to noise.</p><p data-block-key=\"3ne1h\">In December 2024, we <a href=\"https://research.google/blog/making-quantum-error-correction-work/\">announced</a> that operation of error correction on our Willow quantum processor was <i>below threshold</i>, signifying that the logical qubit's robustness to errors exponentially increases as more physical qubits are added. This demonstration utilized a <a href=\"https://research.google/blog/suppressing-quantum-errors-by-scaling-a-surface-code-logical-qubit/\">surface code</a> for high-performance quantum error-correction. During the operation of this surface code, we employed a <i>static</i> circuit, i.e., a single consistent set of underlying physical operations was executed repeatedly to measure and correct errors. These static circuits, while useful for realizing QEC on a device with full yield, limit the ability to avoid \"dropouts\" — qubits or couplers that fail.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --small\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DynamicSC1_Example.width-1250.png\" alt=\"DynamicSC1_Example\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DynamicSC1_Example.width-1250.png\" alt=\"DynamicSC1_Example\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"g6tz4\"><i>A distance-5 surface code. Each circle represents a</i> data<i> qubit, each diamond a</i> measure<i> qubit, and the qubits are connected on a square lattice.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"eqcg2\">Today in “<a href=\"https://www.nature.com/articles/s41567-025-03070-w\" target=\"_blank\" rel=\"noopener noreferrer\">Demonstration of dynamic surface codes”,</a> recently published in <a href=\"https://www.nature.com/nphys/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature Physics</i></a>, we are excited to report the experimental demonstration of a surface code operating with <i>dynamic</i> circuits. Unlike their static counterparts, these dynamic circuits detect errors by alternating between different circuit constructions, which provides greater flexibility in our choice of the types of gates, connectivity, and correlated error suppression. Using dynamic circuits allows us to sidestep some of the big challenges that superconducting qubits face, like <a href=\"https://research.google/blog/overcoming-leakage-on-error-corrected-quantum-processors/\">leakage</a> out of the computational subspace, hardware layout constraints, and qubit dropouts.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"eqcg2\">Physical errors are triangulated using detecting regions</h2><p data-block-key=\"70a4q\">The core principle of QEC is to flag physical errors while not destabilizing the underlying logical quantum information. QEC circuits contain measurements that can localize physical errors to a \"detecting region\", containing a few qubits over a few QEC cycles. In other words, when an error is flagged, the detecting region specifies where and when that error could have occured. By combining many overlapping detecting regions, we can narrow down the location of physical errors and prevent any impact on the logical quantum information. In standard surface code circuits, these detecting regions form a square tiling.</p><p data-block-key=\"b6tqq\">Error correction circuits deform these detecting regions in spacetime. In the standard code, the detecting region tiling always returns to its starting point. In dynamic codes, the tiling of detecting regions changes each cycle. As expanded upon below, we demonstrated three new circuits featuring this periodic re-tiling of the detecting regions: <i>hexagonal</i>, <i>walking</i>, and <i>iSWAP</i>. Each of these three circuits solves a unique challenge in QEC: hexagonal circuits reduce the number of couplers, walking circuits limit non-computation errors, and iSWAP circuits allow use of non-standard two-qubit entangling gates. Together, these demonstrations open the door to a variety of dynamic circuits, including those that avoid dropouts.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DynamicSC2_DetectingRegionsHERO.width-1250.png\" alt=\"DynamicSC2_DetectingRegionsHERO\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DynamicSC2_DetectingRegionsHERO.width-1250.png\" alt=\"DynamicSC2_DetectingRegionsHERO\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"g6tz4\"><i>The red and blue shaded regions represent detecting regions at the end of a QEC cycle. While a standard circuit uses a static end-cycle tiling, our dynamic circuits each alternate between two distinct tilings.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"eqcg2\">Quantum error correction on a hexagonal lattice</h2><p data-block-key=\"fe6oi\">In our <a href=\"https://www.nature.com/articles/s41586-024-08449-y\" target=\"_blank\" rel=\"noopener noreferrer\">Willow</a> architecture, each physical qubit is connected to its four nearest neighbors, forming a <i>square</i> lattice. This arrangement of connections allows gates between the neighboring qubits, but also introduces design constraints, such as the overhead of extra wires needed to control couplers between qubits. Realizing error correction instead on a <i>hexagonal</i> lattice would permit each qubit to connect with only three neighbors instead of four, thereby simplifying the design and fabrication process of these large chips and enhancing hardware performance.</p><p data-block-key=\"6cpj1\">To achieve error correction with only three couplers per qubit, we make use of dynamic circuits that feature two distinct types of error correction cycles. Both cycle types leverage three couplers per qubit, with one coupler utilized twice within the cycle. The result is a quantum error correction circuit with dynamic, overlapping detecting regions that can still be used to triangulate errors, but only requiring three couplers per qubit.</p><p data-block-key=\"csttu\">We evaluated this three-coupler error correction circuit on our Willow processor, which has square connectivity. To measure the hexagonal code, we turned off all the unused couplers, to simulate the performance of hexagonal connectivity. We found that as the code's distance scales from 3 to 5, the logical error rate improves by a factor of 2.15, matching the performance of a traditional static circuit operating on the same hardware that we presented in our <a href=\"https://www.nature.com/articles/s41586-024-08449-y\" target=\"_blank\" rel=\"noopener noreferrer\">milestone experiment</a> last year.</p><p data-block-key=\"5dnut\">Our findings demonstrate the feasibility of constructing a hexagonal qubit lattice for quantum error correction, a design space we thoroughly investigated in simulation. By adopting a hexagonal lattice, we can significantly reduce the complexity of our optimization algorithms for selecting qubit and gate frequencies. This simplification leads to a 15% improvement in the simulated error suppression factor, showcasing the novel capabilities unlocked by designing a processor with three couplers per qubit, rather than four.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DynamicSC3_Hex.width-1250.png\" alt=\"DynamicSC3_Hex\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DynamicSC3_Hex.width-1250.png\" alt=\"DynamicSC3_Hex\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"g6tz4\"><b><i>Left:</i></b><i> Error correction performance of the hexagonal dynamic circuit on our Willow processor for distance-3 (red) and distance-5 (blue) codes.</i> <b><i>Right:</i></b><i> Simulated comparison between a hexagonal lattice (blue) and a square lattice (red) for quantum error correction.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"eqcg2\">Exchanging the roles of data and measure qubits</h2><p data-block-key=\"deteg\">While a qubit is fundamentally defined by its two quantum states, |0⟩ and |1⟩, our physical superconducting circuits possess additional, higher-energy states not utilized in computation or error correction. When a qubit escapes into these higher-energy states — a phenomenon known as <a href=\"https://research.google/blog/overcoming-leakage-on-error-corrected-quantum-processors/\"><i>leakage</i></a> — it can induce correlated errors, diminishing the efficacy of quantum error correction. We can remove this leakage on measure qubits using a leakage reset technique, and we’ve shown it can be removed on data qubits using a special sequence of gates called data-qubit-leakage-removal (DQLR). However, such DQLR introduces new gates into the circuit, adding complexity and additional sources of possible errors.</p><p data-block-key=\"3ub0r\">Using dynamic circuits, we can instead realize error correction with a circuit that periodically swaps the role of “data” and “measure” qubits. In this way, the simple leakage reset applied to measure qubits can now be applied to all qubits, without any additional gates in the QEC cycle. We call this periodic swapping of qubits a <i>walking circuit</i>, since it allows a logical qubit to move across the device, wiggling back-and-forth in the process.</p><p data-block-key=\"d8mvi\">Below we show how our walking circuit can reduce additional correlated errors caused by leakage. The plotted points represent how detectors separated by many cycles are correlated, with lower values signaling fewer correlated errors. In our standard circuit, these correlations remain for up to 40 cycles. By using walking (green), we significantly reduce these correlations by over an order of magnitude. Moreover, this level of suppression matches our standard surface code, using the DQLR technique.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --small\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DynamicSC4_Walking.width-1250.png\" alt=\"DynamicSC4_Walking\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DynamicSC4_Walking.width-1250.png\" alt=\"DynamicSC4_Walking\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"g6tz4\"><i>Time-correlated errors measured in different error correction circuits (lower is better).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"eqcg2\">Make way for the iSWAP gate</h2><p data-block-key=\"b5rf7\">In traditional circuits for error correction of the surface code, the <a href=\"https://en.wikipedia.org/wiki/Quantum_logic_gate\" target=\"_blank\" rel=\"noopener noreferrer\">controlled-Z (CZ) gate</a> is used to entangle data and measure qubits. However, in <a href=\"https://research.google/blog/a-verifiable-quantum-advantage/\">our recent demonstration of a verifiable quantum advantage</a>, we’ve used a different type of quantum gate, called the iSWAP gate. This iSWAP gate swaps the qubit states while also performing a controlled-Z (CZ) operation. Unlike the CZ gate, the iSWAP gate does not rely on non-computational states for its realization. As a result, the iSWAP gate generates fewer correlated errors caused by leakage.</p><p data-block-key=\"bfdr7\">The superior properties of the iSWAP gate raises the question: can the iSWAP gate be used for quantum error correction instead of the CZ gate? As shown in <a href=\"https://arxiv.org/pdf/2302.02192\" target=\"_blank\" rel=\"noopener noreferrer\">our prior theory work</a>, the iSWAP gate can be used in a dynamic circuit to realize error correction of the surface code. We demonstrate such a circuit on our Willow superconducting processor, achieving a strong error suppression factor of 1.56. This performance is slightly below that of a standard circuit using CZ gates as our device was designed and optimized for CZ gate error correction, but our demonstration of this iSWAP code confirms the viability of the iSWAP gate for error correction, paving the way for future device designs optimized for this gate.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DynamicSC5_iSWAP.width-1250.png\" alt=\"DynamicSC5_iSWAP\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DynamicSC5_iSWAP.width-1250.png\" alt=\"DynamicSC5_iSWAP\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"g6tz4\"><i>Error correction performance of the iSWAP code on our Willow processor.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"eqcg2\">What’s next</h2><p data-block-key=\"4bi98\">Our error correction demonstrations prove that dynamic circuits are a viable approach to fault tolerance. By relaxing the constraints on connectivity and expanding viable gate sets, dynamic circuits open new avenues for co-designing quantum hardware and error correction protocols. A notable advantage of these dynamic circuits is their capacity to circumvent \"dropouts\" within our quantum error-correcting codes, a phenomenon where certain qubits or couplers experience failure.</p><p data-block-key=\"3f0su\">These demonstrations, alongside our recent<a href=\"https://research.google/blog/a-colorful-quantum-future/\"> color code operation on the Willow architecture</a>, firmly establish the viability of error correction beyond the conventional static surface code model. Our results push us closer to our <a href=\"https://quantumai.google/roadmap\" target=\"_blank\" rel=\"noopener noreferrer\">next milestone</a>: a long-lived logical qubit with an error rate of less than one error per million cycles of error correction.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"eqcg2\">Acknowledgments</h2><p data-block-key=\"2bgtf\"><i>The primary contributors to this work are Alec Eickbusch, Matt McEwen, and Alexis Morvan.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "NeuralGCM harnesses AI to better simulate long-range global precipitation",
      "link": "https://research.google/blog/neuralgcm-harnesses-ai-to-better-simulate-long-range-global-precipitation/",
      "pubDate": "Sun, 11 Jan 2026 16:00:00 GMT",
      "isoDate": "2026-01-11T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "NeuralGCM harnesses AI to better simulate long-range global precipitation",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/NeuralGCM-precipitation-1-final.width-1250.jpg",
          "alt": "NeuralGCM-precipitation-1-final",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/NeuralGCM-precipitation-2.width-1250.png",
          "alt": "NeuralGCM-precipitation-2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/NeuralGCM-precipitation-3.width-1250.png",
          "alt": "NeuralGCM-precipitation-3",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/NeuralGCM-precipitation-4-final.width-1250.jpg",
          "alt": "NeuralGCM-precipitation-4-final",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/NeuralGCM-precipitation-5.width-1250.png",
          "alt": "NeuralGCM-precipitation-5",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3jfih\">Precipitation remains one of the trickiest tasks for global-scale weather and climate models. That’s because exactly where, when and how much precipitation will fall depends on a series of events happening at scales that are typically below the model resolution. Simulating precipitation is especially challenging for extreme events and over long periods of time. Whether it’s farmers knowing which day to plant seeds to optimize their harvest, or city planners knowing how to prepare for a 100-year storm, precipitation forecasts are some of the most relevant for humans.</p><p data-block-key=\"43eaq\">Last year, we introduced our open-sourced hybrid atmospheric model <a href=\"https://research.google/blog/fast-accurate-climate-modeling-with-neuralgcm/\">NeuralGCM</a>, which combines machine learning (ML) and physics to run fast, efficient and accurate global atmospheric simulations. In the 2024 <a href=\"https://www.nature.com/articles/s41586-024-07744-y\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>, NeuralGCM generated more accurate 2–15 day weather forecasts and reproduced historical temperatures over four decades with greater precision than traditional atmospheric models, marking a significant step towards developing more accessible climate models.</p><p data-block-key=\"52c34\">Now in “<a href=\"https://www.science.org/doi/10.1126/sciadv.adv6891\" target=\"_blank\" rel=\"noopener noreferrer\">Neural general circulation models for modeling precipitation</a>”, published in <a href=\"https://www.science.org/journal/sciadv\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Science Advances</i></a>, we describe how NeuralGCM was trained on satellite-based precipitation observations to achieve improved simulations of precipitation. Notably, at the current resolution of 280 km, we see improvements against a leading operational model for medium-range weather forecasting (up to 15 days) and against atmospheric models used for <a href=\"https://www.gfdl.noaa.gov/multidecadal-climate-changes/\" target=\"_blank\" rel=\"noopener noreferrer\">multi-decadal</a> climate simulations. We find NeuralGCM more accurately reproduces average precipitation, precipitation extremes — with major improvements for the top 0.1% of rainfall — and the <a href=\"https://svs.gsfc.nasa.gov/13346/\" target=\"_blank\" rel=\"noopener noreferrer\">daily weather cycle</a>.</p><p data-block-key=\"blrp3\">NeuralGCM is part of a broader effort to advance weather and climate science, within <a href=\"https://ai.google/earth-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Earth AI</a>. As a hybrid model that combines physics and AI to tackle longer-range questions, it complements other AI-only weather models like the recently introduced <a href=\"https://blog.google/technology/google-deepmind/weathernext-2/\" target=\"_blank\" rel=\"noopener noreferrer\">WeatherNext 2</a> update from our Google colleagues.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/NeuralGCM-precipitation-1-final.width-1250.jpg\" alt=\"NeuralGCM-precipitation-1-final\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/NeuralGCM-precipitation-1-final.width-1250.jpg\" alt=\"NeuralGCM-precipitation-1-final\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"cbdb7\"><i>NeuralGCM uses a hybrid framework that combines a traditional fluid dynamics solver (</i><b><i>gray sphere</i></b><i>) for large-scale processes with AI neural networks (</i><b><i>cartoon box</i></b><i> and</i> <b><i>umbrella</i></b><i>) for small-scale physics, like clouds, radiation and precipitation.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Clouds’ diversity and fleeting nature pose challenges</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3jfih\">To simulate precipitation, we must go to its source: clouds. Clouds can exist at scales smaller than 100 meters, the size of an athletic field — far below the <a href=\"https://www.ecmwf.int/en/newsletter/147/meteorology/new-model-cycle-brings-higher-resolution\" target=\"_blank\" rel=\"noopener noreferrer\">kilometers</a>-scale resolution of global weather models, or the <a href=\"https://www.climatechangeinaustralia.gov.au/en/learning-support/climate-models/resolution/\" target=\"_blank\" rel=\"noopener noreferrer\">tens-of-kilometers</a>–scale resolution of global climate models. Clouds come in different types, change quickly, and the intricate physics happening at even smaller scales can generate water droplets or ice crystals. All this complexity is impossible for large-scale models to resolve or calculate.</p><p data-block-key=\"8fgjb\">To account for the effect of small-scale atmospheric processes like cloud formation on the climate, models use approximations, called <a href=\"https://en.wikipedia.org/wiki/Parametrization_(atmospheric_modeling)\" target=\"_blank\" rel=\"noopener noreferrer\">parameterizations</a>, which are based on other variables. Rather than depending on these parameterizations, NeuralGCM uses a neural network to learn the effects of such small-scale events directly from existing weather data.</p><p data-block-key=\"24bjo\">We improved the representation of precipitation in this version of our model by training the ML portion of NeuralGCM directly on satellite-based precipitation observations. The <a href=\"https://research.google/blog/fast-accurate-climate-modeling-with-neuralgcm/\">initial offering of NeuralGCM</a> was, like most ML weather models, trained on recreations of previous atmospheric conditions, i.e., <a href=\"https://www.ecmwf.int/en/about/media-centre/focus/2023/fact-sheet-reanalysis\" target=\"_blank\" rel=\"noopener noreferrer\">reanalyses</a>, that combine physics-based models with observations to fill in gaps in observational data. But the physics of clouds is so complex that even reanalyses struggle to get precipitation right. Training on output from reanalyses means reproducing their weaknesses, for example, on precipitation extremes and the daily cycle.</p><p data-block-key=\"1tnmn\">Instead, we trained the precipitation part of NeuralGCM directly on <a href=\"https://gpm.nasa.gov/data/imerg\" target=\"_blank\" rel=\"noopener noreferrer\">NASA satellite-based</a> precipitation observations spanning from 2001 to 2018. NeuralGCM’s <a href=\"https://github.com/neuralgcm/dinosaur\" target=\"_blank\" rel=\"noopener noreferrer\">differential dynamical core</a> infrastructure allowed us to train it on satellite observations. Previous hybrid models that combine physics and AI could only use output from high-fidelity simulations or reanalysis data. By training the AI component of NeuralGCM directly on high-quality satellite observations instead of relying on reanalyses, we are effectively finding a better, machine-learned parameterization for precipitation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/NeuralGCM-precipitation-2.width-1250.png\" alt=\"NeuralGCM-precipitation-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/NeuralGCM-precipitation-2.width-1250.png\" alt=\"NeuralGCM-precipitation-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"cbdb7\"><i>Global weather and climate models divide Earth into a grid of boxes many kilometers across. But precipitation depends on smaller scales: water molecules that aggregate around a particle, forming droplets, which condense inside clouds and eventually precipitate out. Traditional models estimate the impact of small-scale processes with equations based on other variables (parameterizations). NeuralGCM instead uses those other variables with an ML model trained directly on satellite precipitation observations to better simulate global precipitation.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Forecasting precipitation over the next 15 days</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3jfih\">We evaluated NeuralGCM’s performance on two-week forecasts using <a href=\"https://sites.research.google/gr/weatherbench/\">WeatherBench 2</a>, comparing it against a leading physics-based model from the <a href=\"https://www.ecmwf.int/en/forecasts/documentation-and-support/medium-range-forecasts\" target=\"_blank\" rel=\"noopener noreferrer\">European Centre for Medium-range Weather Forecasts</a> (ECMWF). In tests using forecasts starting at noon and midnight on each day in 2020 (data was not used during training), NeuralGCM significantly outperformed the ECMWF model at low resolution across most averaged measures of precipitation. This included both 24-hour and 6-hour accumulated precipitation for all 15 forecast days. The advantage remained significant over land masses, which are critical for humans and ecosystems.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/NeuralGCM-precipitation-3.width-1250.png\" alt=\"NeuralGCM-precipitation-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/NeuralGCM-precipitation-3.width-1250.png\" alt=\"NeuralGCM-precipitation-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"cbdb7\"><i>The graph on the left compares 24-hour accumulated precipitation forecasts from NeuralGCM (</i><b><i>blue line</i></b><i>) and the ECMWF medium-range forecast (</i><b><i>orange line</i></b><i>). Performance was measured against</i> <a href=\"https://gpm.nasa.gov/data/imerg\" target=\"_blank\" rel=\"noopener noreferrer\"><i>IMERG</i></a><i> satellite observations using the</i> <a href=\"https://www.ecmwf.int/en/newsletter/166/news/new-tool-understand-changes-ensemble-forecast-skill\" target=\"_blank\" rel=\"noopener noreferrer\"><i>continuous ranked probability score</i></a><i>, a measure of forecast performance. NeuralGCM ranks lower, which is better, across all 15 forecast days. The maps on the right show the spatial distribution of that score two days into the forecast.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3jfih\">Knowing when and where precipitation is going to fall helps communities manage flooding and drought, use irrigation resources most efficiently, plan events and protect public safety. While NeuralGCM’s current resolution of 280 kilometers is too coarse for operational forecasts, these results suggest there’s potential to leverage this technique at smaller scales to improve the tools used for operational forecasts.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Precipitation patterns over years to decades</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3jfih\">Over longer timescales, from years to decades, understanding average precipitation patterns can help with flood control, crop planning, and managing drinking water supplies. Given NeuralGCM’s 280 kilometer resolution, our current focus extends to larger scales of geography and time. When comparing multi-year runs of NeuralGCM against leading global atmospheric models used to study climate, NeuralGCM had an average mean error of less than half a millimeter per day. This represents a 40% average error reduction compared to the leading tools used in the latest <a href=\"https://www.ipcc.ch/\" target=\"_blank\" rel=\"noopener noreferrer\">Intergovernmental Panel on Climate Change</a> report, with an even bigger improvement over land.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/NeuralGCM-precipitation-4-final.width-1250.jpg\" alt=\"NeuralGCM-precipitation-4-final\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/NeuralGCM-precipitation-4-final.width-1250.jpg\" alt=\"NeuralGCM-precipitation-4-final\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"cbdb7\"><i>The</i> <i>box plot (</i><b><i>top</i></b><i>) shows the mean absolute error in precipitation for traditional models in the</i> <a href=\"https://en.wikipedia.org/wiki/Atmospheric_Model_Intercomparison_Project\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Atmospheric Model Intercomparison Project</i></a><i> compared to NeuralGCM trained on satellite precipitation observations. The errors are measured in millimeters per day over land from 2002 to 2014 compared to</i> <a href=\"https://gpm.nasa.gov/data/imerg\" target=\"_blank\" rel=\"noopener noreferrer\"><i>IMERG</i></a><i> satellite precipitation observations. NeuralGCM had an average error (</i><b><i>orange line</i></b><i>) of 0.3 millimeters per day over land, less than half that of the leading tools. The maps show the spatial distribution of precipitation bias for NeuralGCM and the</i> <a href=\"https://www.cesm.ucar.edu/models/cesm2\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Community Earth System Model 2</i></a><i>.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3jfih\">NeuralGCM also showed major improvements for extreme precipitation, the top 0.1% of rainfall at a given location. Extreme events are some of the hardest to reproduce because fewer previous examples exist, and the parameterizations that physics-based models use to estimate precipitation often overrepresent light events while underrepresenting heavy events — known as the <a href=\"https://gmd.copernicus.org/articles/17/4689/2024/\" target=\"_blank\" rel=\"noopener noreferrer\">drizzle problem</a>. NeuralGCM more accurately captured the intensity of precipitation events, especially of heavy precipitation, in simulations from 2002 to 2014. Capturing these most extreme, damaging precipitation events is an important step for applications ranging from climate science to public safety.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/NeuralGCM-precipitation-5.width-1250.png\" alt=\"NeuralGCM-precipitation-5\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/NeuralGCM-precipitation-5.width-1250.png\" alt=\"NeuralGCM-precipitation-5\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"cbdb7\"><i>The</i><b><i> top</i></b> <i>graph shows tropical precipitation rates up to more than 160 mm/day from 2002 to 2014. NeuralGCM (</i><b><i>blue</i></b><i>), trained on satellite precipitation observations, closely matches the observed frequency of rain events from</i> <a href=\"https://gpm.nasa.gov/data/imerg\" target=\"_blank\" rel=\"noopener noreferrer\"><i>IMERG</i></a><i> (dotted line). The</i> <a href=\"https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels?tab=overview\" target=\"_blank\" rel=\"noopener noreferrer\"><i>ERA5</i></a><i> reanalysis (</i><b><i>orange</i></b><i>) and the</i> <a href=\"https://cmc.ipsl.fr/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>IPSL</i></a><i> model (</i><b><i>green</i></b><i>) overestimate light events and underestimate heavy precipitation. Maps show precipitation bias for the heaviest rainfall day of the year. The map colors show that NeuralGCM is less likely to underestimate (</i><b><i>pink</i></b><i>) or overestimate (</i><b><i>green</i></b><i>) heavy precipitation compared to the IPSL model.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3jfih\">Finally, we also looked at how precipitation falls during a single day of the long-range climate simulations. The Amazon rainforest, for example, has a very strong daily cycle. In summer you can expect heavy rains in the afternoon. While today’s climate models tend to have rain fall several hours earlier than in the real world, NeuralGCM more accurately reproduces the timing and amount of peak daily precipitation, particularly over land and in summer. The focus is on precipitation over land, where the diurnal cycle is stronger and where traditional models struggle, and in summer, a standard climate science evaluation period because of its marked diurnal cycle. Capturing when precipitation falls during the day matters at a large scale for ecosystems, weather systems and hydrology.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/NeuralGCM-precipitation-6.width-1250.png\" alt=\"NeuralGCM-precipitation-6\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/NeuralGCM-precipitation-6.width-1250.png\" alt=\"NeuralGCM-precipitation-6\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"cbdb7\"><i>NeuralGCM does better than other tools at reproducing the time of day of peak precipitation, represented here by the map colors. IMERG satellite observations and NeuralGCM have similar colors for the timing of peak precipitation, often late in the day. The</i> <a href=\"https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels?tab=overview\" target=\"_blank\" rel=\"noopener noreferrer\"><i>ERA5</i></a><i> reanalysis and NOAA’s</i> <a href=\"https://www.gfdl.noaa.gov/shield/#X-SHiELD\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Geophysical Fluid Dynamics Laboratory</i></a><i> model have lighter colors, meaning peak precipitation happens earlier in the day in those models, around midday or early afternoon.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Looking ahead</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3jfih\">We believe this is a step forward for large-scale precipitation forecasts and simulations, and we already have early support in the real world. A partnership between the <a href=\"https://www.uchicago.edu/en\" target=\"_blank\" rel=\"noopener noreferrer\">University of Chicago</a> and the <a href=\"https://en.wikipedia.org/wiki/Ministry_of_Agriculture_and_Farmers%27_Welfare\" target=\"_blank\" rel=\"noopener noreferrer\">Indian Ministry of Agriculture and Farmers’ Welfare</a> used NeuralGCM for a <a href=\"https://blog.google/technology/research/indian-farmers-monsoon-prediction/\" target=\"_blank\" rel=\"noopener noreferrer\">pilot program using AI-based forecasts to predict the onset of the monsoon season</a>. The team <a href=\"https://humancenteredforecasts.climate.uchicago.edu/news/artificial-intelligence-is-helping-indian-farmers-adapt-to-climate-change-forecast-accurately-predicting-an-unusual-monsoon-season-reached-38-million/\" target=\"_blank\" rel=\"noopener noreferrer\">selected NeuralGCM and one other model</a> after rigorous testing, and built a forecasting tool that was <a href=\"https://www.wsj.com/articles/farmers-in-india-are-tracking-monsoon-season-with-the-help-of-ai-487b5387?\" target=\"_blank\" rel=\"noopener noreferrer\">deployed for the first time</a> this past summer.</p><p data-block-key=\"9t1b\">Since introducing NeuralGCM we have made everything available as <a href=\"https://github.com/neuralgcm/neuralgcm\" target=\"_blank\" rel=\"noopener noreferrer\">open-source code</a> on which we hope people can build. This <a href=\"https://neuralgcm.readthedocs.io/en/latest/checkpoints.html\" target=\"_blank\" rel=\"noopener noreferrer\">precipitation model</a> is also being openly released to the extended community. Ultimately our hope is that these efforts will bring us one step closer to accurate long-term projections of future precipitation, especially under climate change.</p><p data-block-key=\"6f3rn\">To learn more about geospatial platforms and AI work at Google, check out <a href=\"http://earth.google.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Earth</a>, <a href=\"https://earthengine.google.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Earth Engine</a>, <a href=\"https://deepmind.google/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/\" target=\"_blank\" rel=\"noopener noreferrer\">AlphaEarth Foundations</a>, and <a href=\"https://ai.google/earth-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Earth AI</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3jfih\"><i>We would like to acknowledge the NeuralGCM Team (Dmitrii Kochkov, Ian Langmore and Stephan Hoyer). Thanks also to Hannah Hickey and Elise Kleeman for assistance with this blog post, and to Lizzie Dorfman, Michael Brenner and John Platt for their support and leadership.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Google Research 2025: Bolder breakthroughs, bigger impact",
      "link": "https://research.google/blog/google-research-2025-bolder-breakthroughs-bigger-impact/",
      "pubDate": "Wed, 17 Dec 2025 16:00:00 GMT",
      "isoDate": "2025-12-17T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Google Research 2025: Bolder breakthroughs, bigger impact",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EOY-0d-Hero.width-1250.png",
          "alt": "A timeline of Google Research’s 2025 moments",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EOY-3-FACTS.width-1250.png",
          "alt": "A data table ranks 15 AI models by their \"FACTS Score,\" showing Gemini 3 Pro in first place with a score of 68.8.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EOY-8-NLMGenomics.width-1250.png",
          "alt": "Two Google Research banners featuring microscopic cells and a DNA strand ask questions about linking genetics to health and genome sequencing.",
          "title": "",
          "position": 5
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EOY-10-FireSat.width-1250.png",
          "alt": "A satellite map of the Oregon-California border featuring a thermal infrared inset pinpointing a small fire near Medford.",
          "title": "",
          "position": 7
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EOY-12-MobilityAI.width-1250.png",
          "alt": "A \"Traffic Simulation API\" dashboard displays a map of Seattle with purple-shaded road segments analyzing the impact of lane closures.",
          "title": "",
          "position": 8
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qt6m0\">Google Research teams have invested over the years in advancing research and technology in a diverse range of strategic areas. We are working across time horizons, from bold moonshots and curiosity-driven transformative research where we explore the art of the possible, to innovation and applied research with accelerated impact. The <a href=\"https://research.google/blog/accelerating-the-magic-cycle-of-research-breakthroughs-and-real-world-applications/\">Magic Cycle of research</a> is accelerating — we’re driving research breakthroughs and translating them into real-world solutions, with impact on products, science and society, in close collaboration with many teams across Google and global partners.</p><p data-block-key=\"fc6rj\">This was quite a year! Our foundational AI breakthroughs helped make generative models more efficient, factual, multilingual, and multi-cultural, and we introduced generative UI. We advanced new architectures and algorithmic research and pioneered AI tools and agentic models that help accelerate scientific discovery. We achieved quantum breakthroughs that bring us closer to real-world applications of quantum computing; advanced research on Earth sciences to enable a level of planetary understanding never before possible; drove forward scientific domains including genomics, biology and neuroscience; and made headway on societal priorities like climate resilience, health and education.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-0-HeroRevised.width-1250.jpg\" alt=\"A timeline of Google Research’s 2025 moments\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-0-HeroRevised.width-1250.jpg\" alt=\"A timeline of Google Research’s 2025 moments\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"un5jf\"><i>A look back at some of Google Research's 2025 moments realized in collaboration with many teams across Google. This image was created with Nano Banana.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Factual, multilingual, multi-cultural GenAI\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Advancing generative models to be more efficient, factual, multilingual and multi-cultural</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">To help fuel this era of rapid innovation, we’re investing in efficiency, making Google products more cost and energy efficient, and setting the bar for the industry. We continue to develop new approaches based on <a href=\"https://research.google/blog/looking-back-at-speculative-decoding/\">speculative decoding</a>, such as <a href=\"https://arxiv.org/abs/2403.10444\" target=\"_blank\" rel=\"noopener noreferrer\">block verification</a>, to further accelerate efficiency gains. At the other end of the infrastructure stack, <a href=\"https://research.google/blog/solving-virtual-machine-puzzles-how-ai-is-optimizing-cloud-computing/\">LAVA</a> is a new scheduling algorithm that continuously re-predicts the lifespans of tasks on virtual machines. It is designed to optimize resource efficiency in large cloud data centers, without sacrificing reliability.</p><p data-block-key=\"birtg\">Equally critical, our pioneering research on LLM factuality, dating back to 2021, helps make <a href=\"https://blog.google/products/gemini/gemini-3/#gemini-3\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini 3</a> our most capable and factual LLM yet. It achieves state-of-the-art performance on public factuality benchmarks like <a href=\"https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified\" target=\"_blank\" rel=\"noopener noreferrer\">SimpleQA Verified</a> and the new <a href=\"https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/\" target=\"_blank\" rel=\"noopener noreferrer\">FACTS benchmark suite</a> that we released with Google DeepMind and Kaggle. Users can be confident that products such as the Gemini app, <a href=\"https://blog.google/products/search/generative-ai-google-search-may-2024/\" target=\"_blank\" rel=\"noopener noreferrer\">AI Overviews</a> and <a href=\"https://blog.google/products/search/google-search-ai-mode-update/#ai-mode-search\" target=\"_blank\" rel=\"noopener noreferrer\">AI Mode</a> in Search, and Vertex AI all provide outputs grounded in world knowledge. This year we studied how LLMs <a href=\"https://arxiv.org/abs/2505.24858\" target=\"_blank\" rel=\"noopener noreferrer\">convey uncertainty</a>; presented a framework for assessing whether LLMs <a href=\"https://arxiv.org/abs/2503.15299\" target=\"_blank\" rel=\"noopener noreferrer\">encode more factual knowledge</a> in their parameters than they express in their outputs; presented a multilingual dataset that evaluates cross-lingual knowledge, called <a href=\"https://research.google/blog/eclektic-a-novel-benchmark-for-evaluating-cross-lingual-knowledge-transfer-in-llms/\">ECLeKTic</a>; and more.</p><p data-block-key=\"tc29\">We also explored the role of <a href=\"https://research.google/blog/deeper-insights-into-retrieval-augmented-generation-the-role-of-sufficient-context/\">sufficient context</a> in <a href=\"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\" target=\"_blank\" rel=\"noopener noreferrer\">retrieval augmented generation</a> systems, which enhance LLMs by providing them with relevant external context. We <a href=\"https://arxiv.org/abs/2411.06037\" target=\"_blank\" rel=\"noopener noreferrer\">demonstrated</a> that it is possible to know when an LLM has enough information to provide a correct answer to a question. This work supported the launch of the <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/retrieval-and-ranking#llm_reranker\" target=\"_blank\" rel=\"noopener noreferrer\">LLM Re-Ranker</a> in the <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/rag-overview\" target=\"_blank\" rel=\"noopener noreferrer\">Vertex AI RAG Engine</a>, leading to better retrieval metrics and system accuracy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"FACTS\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-3-FACTS.width-1250.png\" alt=\"A data table ranks 15 AI models by their &quot;FACTS Score,&quot; showing Gemini 3 Pro in first place with a score of 68.8.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-3-FACTS.width-1250.png\" alt=\"A data table ranks 15 AI models by their &quot;FACTS Score,&quot; showing Gemini 3 Pro in first place with a score of 68.8.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"mr25z\"><i>We evaluated leading LLMs on the</i> <a href=\"https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>FACTS Benchmark Suite</i></a><i>, which includes four different factuality benchmarks. The table above lists 15 leading models and their overall FACTS score. Gemini 3 Pro leads in overall performance.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qt6m0\">With the rise of multimodal content, we’ve expanded our work on factuality to images, audio, video, 3D environments and LLM-generated applications. This work helps to improve the quality of Google’s video and image model families, including <a href=\"https://deepmind.google/models/veo/\" target=\"_blank\" rel=\"noopener noreferrer\">Veo</a>, <a href=\"https://blog.google/products/gemini/gemini-app-updates-io-2025/#imagen-4-veo-3\" target=\"_blank\" rel=\"noopener noreferrer\">Imagen</a> and <a href=\"https://blog.google/products/gemini/updated-image-editing-model/\" target=\"_blank\" rel=\"noopener noreferrer\">Nano Banana</a>. It is a great example of the cycle of research and how we’re continuously adapting to real user needs. Our latest research includes making <a href=\"https://arxiv.org/abs/2504.17502\" target=\"_blank\" rel=\"noopener noreferrer\">text-to-image generation</a> and <a href=\"https://arxiv.org/pdf/2506.07631\" target=\"_blank\" rel=\"noopener noreferrer\">image captions</a> more accurate, and creating <a href=\"https://arxiv.org/abs/2505.22657\" target=\"_blank\" rel=\"noopener noreferrer\">3DMem-Bench</a> for evaluating an agent’s ability to reason over long-term memory in 3D.</p><p data-block-key=\"f4fhj\">Our long-running multilinguality research helped <a href=\"https://deepmind.google/models/gemma/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma</a> expand to over 140 languages, making it today’s best multilingual open model. We’re also augmenting our models with socio-cultural intelligence, attuning them to diverse user needs and global contexts. We introduced <a href=\"https://arxiv.org/abs/2510.06124\" target=\"_blank\" rel=\"noopener noreferrer\">TUNA</a>, a comprehensive taxonomy of user needs and actions, launched a community-based data <a href=\"https://research.google/blog/amplify-initiative-localized-data-for-globalized-ai/\">collection platform</a> to target under-represented languages and geographies, and developed new methods to ground models in <a href=\"https://arxiv.org/pdf/2502.13497\" target=\"_blank\" rel=\"noopener noreferrer\">diverse cultural knowledge</a> and datasets. This research helps to ensure that Google models can connect with users globally in responsible and culturally-aware ways.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Interactive interfaces with generative UI\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Introducing interactive interfaces with generative UI</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">In a world where users expect more engaging and visual experiences, we introduced a novel implementation of <a href=\"https://research.google/blog/generative-ui-a-rich-custom-visual-interactive-user-experience-for-any-prompt/\">generative UI</a> in Gemini 3. This powerful capability enables AI models to dynamically create immersive visual experiences and interactive interfaces, such as web pages, games, tools and apps, in response to a prompt. Our research comes to life in AI Mode on <a href=\"http://blog.google/products/search/gemini-3-search-ai-mode\" target=\"_blank\" rel=\"noopener noreferrer\">Google Search</a>, and in experiments such as dynamic view, in the <a href=\"https://blog.google/products/gemini/gemini-3-gemini-app\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini app</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"GenUI Van Gogh demo\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"JALZmOVlR7s\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=JALZmOVlR7s\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"mr25z\"><i>Example of generative UI capabilities in dynamic view in the Gemini app. This is based on the prompt, “</i>Create a Van Gogh gallery with life context for each piece.<i>” More examples can be found</i> <a href=\"https://generativeui.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>here</i></a><i>.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"Generative UI RNA questions in Search\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/EOY-5-GenUI.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n\n      </div>\n    \n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"mr25z\"><i>Example of generative UI capabilities in AI Mode in Google Search. This is based on the prompt, “</i>Show me how rna polymerase works. what are the stages of transcription and how is it different in prokaryotic and eukaryotic cells.<i>”</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Quantum computing\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Quantum computing: The next frontier</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">Our strategic investment in quantum computing is poised to accelerate the next frontier of computing and scientific discovery. In the 1980s, Clarke, Devoret, and Martinis laid the foundations for superconducting qubits, which led to their recognition as <a href=\"https://blog.google/inside-google/company-announcements/googler-michel-devoret-awarded-the-nobel-prize-in-physics/\" target=\"_blank\" rel=\"noopener noreferrer\">2025 Physics Nobel Laureates</a>. The 40-year <a href=\"https://quantumai.google/roadmap\" target=\"_blank\" rel=\"noopener noreferrer\">journey</a> since has yielded the nascent quantum computing industry and led to breakthroughs like our recently announced <a href=\"https://research.google/blog/a-verifiable-quantum-advantage/\">verifiable quantum advantage</a>, published on the cover of <a href=\"https://www.nature.com/articles/s41586-025-09526-6\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature</i></a>. This work describes our “<a href=\"https://blog.google/technology/research/quantum-echoes-willow-verifiable-quantum-advantage/\" target=\"_blank\" rel=\"noopener noreferrer\">Quantum Echoes</a>” algorithm, which runs on our <a href=\"https://research.google/blog/making-quantum-error-correction-work/\">Willow chip</a> 13,000 times faster than the best classical algorithm on one of the world’s fastest supercomputers. It offers a new way to explain interactions between atoms in a molecule observed <a href=\"https://arxiv.org/abs/2510.19550\" target=\"_blank\" rel=\"noopener noreferrer\">using nuclear magnetic resonance spectroscopy</a>. It brings us closer to <a href=\"https://blog.google/technology/research/useful-quantum-computing-applications/\" target=\"_blank\" rel=\"noopener noreferrer\">real-world applications</a> of quantum computing, such as advancing drug design and helping to make fusion energy a reality.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"Circuits to Chandeliers: A Quantum History\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"lpzR_rPbrac\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=lpzR_rPbrac\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Accelerating scientific discovery\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Accelerating scientific discovery</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">AI-powered models and platforms are fundamentally changing <i>how</i> science is conducted. We released <a href=\"https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/\">AI co-scientist</a>, a collaboration across Google Research, Cloud AI and Google DeepMind. This multi-agent <a href=\"https://arxiv.org/abs/2502.18864\" target=\"_blank\" rel=\"noopener noreferrer\">AI system</a> helps scientists generate novel hypotheses. We also shared our <a href=\"https://research.google/blog/accelerating-scientific-discovery-with-ai-powered-empirical-software/\">AI-powered empirical software</a> system, a Gemini-backed coding agent to help scientists write expert-level empirical software to evaluate and iterate on hypotheses. These tools accelerate the very process of making scientific discoveries. They open the door to a future where every scientist in a lab has a team of AI assistants simultaneously investigating thousands of potential solutions to the scientific challenges that motivate their research. Already at Stanford, our AI co-scientist has helped <a href=\"https://advanced.onlinelibrary.wiley.com/doi/pdf/10.1002/advs.202508751\" target=\"_blank\" rel=\"noopener noreferrer\">identify drugs</a> that could be repurposed to treat liver fibrosis. At Imperial College London, researchers working on antimicrobial resistance <a href=\"https://www.imperial.ac.uk/news/261293/googles-ai-co-scientist-could-enhance-research/\" target=\"_blank\" rel=\"noopener noreferrer\">found</a> that it produced the same hypothesis in days that their team took years to develop.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"An overview of the AI co-scientist\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/EOY-7-AICoScientist.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n\n      </div>\n    \n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"mr25z\"><i>An overview of the AI co-scientist. It uses a coalition of specialized agents who iteratively generate, evaluate, and refine hypotheses.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Biology, genomics, and neuroscience\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Advancing science — from biology to genomics to neuroscience</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"xa7ab\">We continue to advance core scientific research. <a href=\"https://research.google/blog/using-ai-to-identify-genetic-variants-in-tumors-with-deepsomatic/\">DeepSomatic</a> and <a href=\"https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/\" target=\"_blank\" rel=\"noopener noreferrer\">C2S-Scale</a> join the AI-powered fight against cancer and are paving the way for brand-new therapies. Published in <a href=\"https://www.nature.com/articles/s41587-025-02839-x\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature Biotechnology</i></a><i>,</i> DeepSomatic is an <a href=\"https://github.com/google/deepsomatic\" target=\"_blank\" rel=\"noopener noreferrer\">open-source</a> tool that builds on <a href=\"https://blog.google/technology/research/ten-years-google-genomics/\" target=\"_blank\" rel=\"noopener noreferrer\">10 years of genomics</a> research at Google and helps scientists and doctors identify genetic variants in cancer cells. Our partners at <a href=\"https://www.childrensmercy.org/childrens-mercy-research-institute/about/\" target=\"_blank\" rel=\"noopener noreferrer\">Children’s Mercy</a> are using it to understand <a href=\"https://www.medrxiv.org/content/10.1101/2024.11.05.24316078v1\" target=\"_blank\" rel=\"noopener noreferrer\">how and why a particular form of cancer</a> affects a patient in order to develop personalized cures. C2S-Scale, which we released in collaboration with Google DeepMind and Yale, is a 27 billion parameter foundation model for single-cell analysis that made headlines for generating a <a href=\"https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/\" target=\"_blank\" rel=\"noopener noreferrer\">novel hypothesis</a> about cancer cellular behavior.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"NotebookLM summaries of our genomics research\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-8-NLMGenomics.width-1250.png\" alt=\"Two Google Research banners featuring microscopic cells and a DNA strand ask questions about linking genetics to health and genome sequencing.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-8-NLMGenomics.width-1250.png\" alt=\"Two Google Research banners featuring microscopic cells and a DNA strand ask questions about linking genetics to health and genome sequencing.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"mr25z\"><i>Our public NotebookLMs make our decade of genomics research more accessible and allow people to dive deeper into topics that interest them. Explore \"</i><a href=\"https://notebooklm.google.com/notebook/31c20c44-8c94-4f81-a2b8-a020a761d122\" target=\"_blank\" rel=\"noopener noreferrer\"><i>How do scientists link genetics to health?</i></a><i>\" and “</i><a href=\"https://notebooklm.google.com/notebook/a09e40ad-d41f-43af-a3ca-5fc82bd459e5\" target=\"_blank\" rel=\"noopener noreferrer\"><i>How can scientists know what's in your genome?</i></a><i>”</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qt6m0\">Turning to neuroscience, we <a href=\"https://www.nature.com/articles/s41586-025-08985-1\" target=\"_blank\" rel=\"noopener noreferrer\">published in <i>Nature</i></a> the first-ever method for using commonly available light microscopes to comprehensively map all the neurons and their connections in a block of brain tissue. Working with the <a href=\"https://ista.ac.at/en/home/\" target=\"_blank\" rel=\"noopener noreferrer\">Institute of Science and Technology Austria</a>, we applied our suite of image analysis and ML tools for <a href=\"https://sites.research.google/neural-mapping/\">connectomics</a>, leveraging over a decade of contributions we’ve made to this scientific field to understand the workings of the brain. We hope the method, called <a href=\"https://research.google/blog/a-new-light-on-neural-connections/\">LICONN</a>, will enable more labs around the world to pursue connectomics studies.</p><p data-block-key=\"bp5mb\">We also <a href=\"https://zapbench-release.storage.googleapis.com/landing.html\" target=\"_blank\" rel=\"noopener noreferrer\">open-sourced</a> the <a href=\"https://research.google/blog/improving-brain-models-with-zapbench/\">Zebrafish Activity Prediction Benchmark</a> (ZAPBench) in collaboration with <a href=\"https://www.janelia.org/\" target=\"_blank\" rel=\"noopener noreferrer\">HHMI Janelia</a> and <a href=\"https://lichtmanlab.fas.harvard.edu/\" target=\"_blank\" rel=\"noopener noreferrer\">Harvard</a>. With recordings of more than 70,000 neurons from the larval zebrafish brain, it will enable scientists to <a href=\"https://arxiv.org/abs/2503.02618\" target=\"_blank\" rel=\"noopener noreferrer\">investigate the relationship</a> between the structural wiring and dynamic neural activity across an entire vertebrate brain for the first time.</p><p data-block-key=\"8r98j\">Plus, we demonstrated how LLMs can help us understand the human brain. In a <a href=\"https://www.nature.com/articles/s41593-022-01026-4\" target=\"_blank\" rel=\"noopener noreferrer\">series</a> of <a href=\"https://www.nature.com/articles/s41467-024-46631-y\" target=\"_blank\" rel=\"noopener noreferrer\">studies</a> conducted over five years with <a href=\"https://hassonlab.princeton.edu/\" target=\"_blank\" rel=\"noopener noreferrer\">Princeton University</a>, <a href=\"https://nyulangone.org/locations/comprehensive-epilepsy-center\" target=\"_blank\" rel=\"noopener noreferrer\">NYU</a>, and <a href=\"https://www.deepcognitionlab.com/\" target=\"_blank\" rel=\"noopener noreferrer\">HUJI</a>, we explored connections in the ways the human brain and deep language models <a href=\"https://research.google/blog/deciphering-language-processing-in-the-human-brain-through-llm-representations/\">process natural language</a>. We discovered <a href=\"https://www.nature.com/articles/s41562-025-02105-9\" target=\"_blank\" rel=\"noopener noreferrer\">remarkable alignment</a> between the neural activity in the speech and language areas of the human brain and the speech and language embeddings of a Transformer-based speech-to-text model, and showed how the <a href=\"https://www.nature.com/articles/s41467-025-65518-0.epdf?sharing_token=XqLw7uzRcb-8uuak6fH8BdRgN0jAjWel9jnR3ZoTv0PSBTWDWtB7_YH7achJWXIc1XVbsxtqM_vvMoZPLZuQIoT45xjjIkxPMOnKorePnnoS9QL1SRZ58ZI65TPosI2w5PF8h5ZazOVlXhxgrnGSxw1GguylV5BBtKxLUyCAfLM%3D\" target=\"_blank\" rel=\"noopener noreferrer\">temporal structure</a> of language processing in the brain corresponds to the layered hierarchy of deep language models. Our research indicates that language representation in deep learning models could offer a novel framework for understanding the brain’s neural code; it also paves the way for innovative approaches to creating artificial neural networks with better information processing capabilities.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Planetary intelligence and crisis resilience\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Enabling planetary intelligence and crisis resilience</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\"><a href=\"https://blog.google/innovation-and-ai/technology/research/new-updates-and-more-access-to-google-earth-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Earth AI</a> is Google’s family of geospatial AI models and reasoning agents that we <a href=\"https://blog.google/innovation-and-ai/products/google-earth-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">announced</a> recently. It provides users with actionable insights, grounded in real-world understanding, enabling planetary intelligence and <a href=\"https://research.google/blog/how-ai-is-helping-us-build-a-more-resilient-planet/\">climate resilience</a>. Developed in collaboration with teams across Google, Earth AI builds on our years of modeling the world, paired with Gemini’s advanced reasoning, to offer an unprecedented level of understanding about our planet. It <a href=\"https://research.google/blog/google-earth-ai-unlocking-geospatial-insights-with-foundation-models-and-cross-modal-reasoning/\">brings together</a> many of Google’s geospatial models and technologies such as <a href=\"https://research.google/blog/google-earth-ai-unlocking-geospatial-insights-with-foundation-models-and-cross-modal-reasoning/\">remote sensing imagery</a>, <a href=\"https://research.google/blog/fast-accurate-climate-modeling-with-neuralgcm/\">weather</a>, <a href=\"https://blog.google/products/maps/google-maps-apis-environment-sustainability/\" target=\"_blank\" rel=\"noopener noreferrer\">air quality</a>, <a href=\"https://sites.research.google/gr/floodforecasting/\">floods</a>, <a href=\"https://research.google/blog/insights-into-population-dynamics-a-foundation-model-for-geospatial-inference/\">population dynamics</a>, <a href=\"https://deepmind.google/discover/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/\" target=\"_blank\" rel=\"noopener noreferrer\">AlphaEarth Foundations</a>, <a href=\"https://research.google/blog/introducing-mobility-ai-advancing-urban-transportation/\">mobility</a>, <a href=\"https://blog.google/technology/research/open-buildings-ai-powered-maps-for-a-changing-world/\" target=\"_blank\" rel=\"noopener noreferrer\">maps</a> and more. Thanks to Gemini’s reasoning power, Earth AI can synthesize vast datasets about the planet to generate insights in minutes that would previously take years of research. Earth AI offerings are available in <a href=\"https://mapsplatform.google.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Maps Platform</a>, <a href=\"https://blog.google/technology/research/new-updates-and-more-access-to-google-earth-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Earth and to Trusted Testers via Google Cloud</a>, and are already being used by partners, helping cities, enterprises and nonprofits with critical tasks from <a href=\"https://www.youtube.com/watch?v=ZxmB8Z5i1Ls&amp;list=PL95lT3XlM14ROFtYnlBDYZbKipH3JLaC7&amp;index=3\" target=\"_blank\" rel=\"noopener noreferrer\">urban planning</a> to <a href=\"https://www.youtube.com/watch?v=8-macH8ozr4&amp;list=PL95lT3XlM14ROFtYnlBDYZbKipH3JLaC7&amp;index=5\" target=\"_blank\" rel=\"noopener noreferrer\">disaster response</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"Google Earth AI\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"UZ4RaLGDXI4\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=UZ4RaLGDXI4\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qt6m0\">We’ve also made significant strides with the climate models that feed our AI capabilities for understanding the Earth, helping communities to prepare for and respond to severe weather and natural disasters. This year, in collaboration with the <a href=\"https://www.earthfirealliance.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Earth Fire Alliance</a>, the <a href=\"https://www.moore.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Moore Foundation</a> and <a href=\"https://www.muonspace.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Muon Space</a>, we launched the first satellite in the <a href=\"https://sites.research.google/gr/wildfires/firesat/\">FireSat</a> constellation. Named as one of <a href=\"https://time.com/collections/best-inventions-2025/7318230/firesat/\" target=\"_blank\" rel=\"noopener noreferrer\">TIME magazine’s best inventions</a> of 2025, FireSat uses AI to provide critical near–real-time insights for first responders. It has already <a href=\"https://blog.google/technology/research/first-firesat-images/\" target=\"_blank\" rel=\"noopener noreferrer\">detected</a> small wildfires not caught by other space-based systems, and when fully operational with over 50 satellites, it will be able to detect a classroom-sized wildfire anywhere on Earth.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"FireSat\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-10-FireSat.width-1250.png\" alt=\"A satellite map of the Oregon-California border featuring a thermal infrared inset pinpointing a small fire near Medford.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-10-FireSat.width-1250.png\" alt=\"A satellite map of the Oregon-California border featuring a thermal infrared inset pinpointing a small fire near Medford.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"mr25z\"><i>FireSat is equipped with a custom mid-wave infrared (MWIR) sensor that detected a small, relatively cool roadside fire near Medford, Oregon that was not detected by other space-based systems. It is seen here overlaid on a Google Earth basemap. Credit: Muon Space and Earth Fire Alliance.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qt6m0\">We also <a href=\"https://blog.google/technology/research/new-updates-and-more-access-to-google-earth-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">expanded</a> our flood forecasting models to cover over 2 billion people in 150 countries for the most significant riverine flood events, helping communities stay safe and informed. We partnered with our colleagues at Google DeepMind to debut an experimental model for <a href=\"https://deepmind.google/blog/how-were-supporting-better-tropical-cyclone-prediction-with-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">cyclone predictions</a> using stochastic neural networks that's helping weather agencies predict a cyclone’s path up to 15 days in advance. Moreover, we collaborated with Google DeepMind to launch <a href=\"https://deepmind.google/science/weathernext/\" target=\"_blank\" rel=\"noopener noreferrer\">WeatherNext 2</a>, which delivers our most accurate, mid-range AI weather forecasts to date. It’s now available to users of Search, Gemini and Pixel Weather as well as to developers on Google Maps and Google Cloud.</p><p data-block-key=\"d5150\">At the start of the year, we expanded <a href=\"https://blog.google/intl/en-africa/products/explore-get-answers/nowcasting-on-search-is-bringing-ai-powered-weather-forecasts-to-users-across-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">Nowcasting on Search to Africa</a>, bringing highly precise, short-term precipitation forecasts to users across the continent for the first time. We have since made this available for users worldwide. Powered by our <a href=\"https://arxiv.org/abs/2510.13050\" target=\"_blank\" rel=\"noopener noreferrer\">MetNet</a> model, it represents the first AI weather model on Search to operate at a global scale. In India, the University of Chicago and the Indian Ministry of Agriculture and Farmers’ Welfare used Google’s <a href=\"https://blog.google/technology/research/indian-farmers-monsoon-prediction/\" target=\"_blank\" rel=\"noopener noreferrer\">NeuralGCM model</a> to send longer-range monsoon forecasts to 38 million farmers, helping them make critical decisions about what to plant and when.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Health AI\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Advancing Health AI</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">As we make scientific breakthroughs with the potential to significantly reform healthcare, we’re working with partners and healthcare professionals to bring new capabilities responsibly to people around the world. <a href=\"https://research.google/blog/amie-a-research-ai-system-for-diagnostic-medical-reasoning-and-conversations/\">AMIE</a> is our conversational medical agent developed together with Google DeepMind and published in <a href=\"https://www.nature.com/articles/s41586-025-08866-7\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature</i></a>. It can now reason through <a href=\"https://research.google/blog/amie-gains-vision-a-research-ai-agent-for-multi-modal-diagnostic-dialogue/\">multimodal</a> evidence and support longitudinal <a href=\"https://research.google/blog/from-diagnosis-to-treatment-advancing-amie-for-longitudinal-disease-management/\">disease management</a> as well as or better than primary care physicians under simulated settings with professional patient actors. We’re exploring how this research could enable a physician-centered model with <a href=\"https://research.google/blog/enabling-physician-centered-oversight-for-amie/\">asynchronous oversight</a> of AMIE. We also launched <a href=\"https://community.fitbit.com/t5/The-Pulse-Fitbit-Community-Blog/Introducing-the-new-Plan-for-Care-Fitbit-Lab/ba-p/5796289\" target=\"_blank\" rel=\"noopener noreferrer\">Plan for Care Lab</a>, Fitbit’s latest experimental capability, to a select number of opt-in users. It’s designed to help users access personalized support when assessing symptoms at home and preparing for an upcoming doctor’s visit. In addition, <a href=\"https://research.google/blog/medgemma-our-most-capable-open-models-for-health-ai-development/\">MedGemma</a>, Google's most capable open model for multimodal medical comprehension, is available as part of our <a href=\"https://developers.google.com/health-ai-developer-foundations\" target=\"_blank\" rel=\"noopener noreferrer\">Health AI Developer Foundations</a> (HAI-DEF). It can support tasks such as classification, report generation, or interpreting complex electronic health records, making it useful for medical research and product development. Since launch, MedGemma and HAI-DEF have &gt;2M downloads. Plus, our <a href=\"https://developers.google.com/open-health-stack\" target=\"_blank\" rel=\"noopener noreferrer\">Open Health Stack</a> was recognized at the <a href=\"https://www.weforum.org/stories/2025/01/social-innovation-has-moved-from-the-margins-to-the-mainstream/\" target=\"_blank\" rel=\"noopener noreferrer\">World Economic Forum</a> for helping to address inequities in health access. It provides the building blocks for developers to create next-gen, data-driven healthcare apps for use in low-resource settings.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/EOY-10.5-MMAMIE-1.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n\n      </div>\n    \n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"nmr52\"><i>Multimodal AMIE is a diagnostic conversational AI that can intelligently request, interpret and reason about visual medical information during a clinical diagnostic conversation.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Learning and education\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Advancing learning and education</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">Gemini is now infused with <a href=\"https://blog.google/outreach-initiatives/education/google-learnlm-gemini-generative-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">LearnLM</a>, Google’s family of models fine-tuned for learning, <a href=\"https://storage.googleapis.com/deepmind-media/LearnLM/LearnLM_paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">announced</a> last year. We launched <a href=\"https://research.google/blog/learn-your-way-reimagining-textbooks-with-generative-ai/\">Learn Your Way</a> on <a href=\"https://learnyourway.withgoogle.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Labs</a>, powered by LearnLM’s foundational capabilities. It explores the future of textbooks by generating multiple engaging representations of the source material. It transforms static textbooks into active learning experiences that are tailored for every student, with interactive quizzes that enable real-time assessment, feedback, and content personalization. In our<a href=\"https://arxiv.org/abs/2509.13348\" target=\"_blank\" rel=\"noopener noreferrer\"> efficacy study</a>, students using it scored 11 percentage points higher on retention tests. We also piloted our LearnLM model for <a href=\"https://blog.google/intl/en-africa/company-news/outreach-and-initiatives/5-ways-were-bringing-ai-innovations-to-people-across-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">answer assessment</a> with thousands of high school students in Ghana. Plus, we explored the intersection of education and health through a learner-centric approach quantifying the benefits of LearnLM in <a href=\"https://research.google/blog/how-googles-ai-can-help-transform-health-professions-education/\">medical education</a> settings.</p><p data-block-key=\"bo45o\">This research brings us closer to realizing a future where AI makes learning more effective for everyone. In collaboration with teams across Google, we published “<a href=\"https://services.google.com/fh/files/misc/future_of_learning.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">AI and the Future of Learning</a>”, sharing our approach, grounded in learning science, to responsibly enable AI for learning. We’re creating personalized teaching experiences, empowering educators, and working to address challenges such as critical thinking and equal access.</p><p data-block-key=\"chemf\">In parallel, our AI Literacy efforts aim to inspire the next generation of innovators. <a href=\"https://research.google/ai-quests/intl/en_gb\">AI Quests</a>, <a href=\"https://blog.google/outreach-initiatives/education/ai-quests/\" target=\"_blank\" rel=\"noopener noreferrer\">launched</a> with the <a href=\"https://acceleratelearning.stanford.edu/\" target=\"_blank\" rel=\"noopener noreferrer\">Stanford Accelerator for Learning</a>, allows students to step into the shoes of Google researchers and use AI to solve challenges like flood forecasting and detecting eye disease. During <a href=\"https://blog.google/outreach-initiatives/education/next-gen-computer-science-innovators/\" target=\"_blank\" rel=\"noopener noreferrer\">Computer Science Education Week</a>, hundreds of Googler volunteers brought these quests to classrooms around the world.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/EOY-11-LearnYourWay.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n\n      </div>\n    \n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"mr25z\"><i>Learn Your Way explores how GenAI can transform educational materials into more effective, engaging, learner-driven experiences. It generates multiple representations of the source material, tailored for each student.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"ML foundations and algorithmic research\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Advancing ML foundations and algorithmic research</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">Our broad foundational ML and algorithmic research is the bedrock for groundbreaking advances across domains. This work provides the essential frameworks that power products and services, and underpins the development of next-generation models and intelligent systems. We improved voice search, for example, with our new <a href=\"https://research.google/blog/speech-to-retrieval-s2r-a-new-approach-to-voice-search/\">Speech-to-Retrieval</a> engine, which directly interprets and retrieves information from a spoken query without having to convert it first to text. And our state-of-the-art <a href=\"https://research.google/blog/rich-human-feedback-for-text-to-image-generation/\">predictive modeling</a> of rich human feedback improved text-to-image generation quality in products, including <a href=\"https://developers.googleblog.com/en/imagen-3-arrives-in-the-gemini-api/\" target=\"_blank\" rel=\"noopener noreferrer\">Imagen3</a>, <a href=\"https://blog.google/products/ads-commerce/new-creative-updates-advertisers-generate-lifestyle/\" target=\"_blank\" rel=\"noopener noreferrer\">creative generation</a> and <a href=\"https://blog.google/products/ads-commerce/google-ai-ads-creative/\" target=\"_blank\" rel=\"noopener noreferrer\">editing</a> in Google Ads, and <a href=\"https://blog.google/products/shopping/google-shopping-ai-mode-virtual-try-on-update/\" target=\"_blank\" rel=\"noopener noreferrer\">virtual try on</a> for shopping. We also extended this research to improve video generation quality in the <a href=\"https://blog.google/products/google-cloud/sphere-wizard-of-oz/\" target=\"_blank\" rel=\"noopener noreferrer\">Wizard of Oz film launch</a> at <a href=\"https://www.thesphere.com/shows/wizard-of-oz-experience\" target=\"_blank\" rel=\"noopener noreferrer\">Sphere</a> in Las Vegas.</p><p data-block-key=\"f7the\">The impact of our algorithmic research extends well beyond Google products. Our <a href=\"https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/\">TimesFM</a> model, which <a href=\"https://cloud.google.com/blog/products/data-analytics/timesfm-models-in-bigquery-and-alloydb\" target=\"_blank\" rel=\"noopener noreferrer\">helps businesses</a> with time-series forecasting, now has hundreds of millions of queries per month in <a href=\"https://cloud.google.com/bigquery\" target=\"_blank\" rel=\"noopener noreferrer\">BigQuery</a> and <a href=\"https://cloud.google.com/products/alloydb\" target=\"_blank\" rel=\"noopener noreferrer\">AlloyDB</a>. We introduced a novel approach using <a href=\"https://research.google/blog/time-series-foundation-models-can-be-few-shot-learners/\">in-context fine-tuning</a>, which teaches the model how to learn from multiple examples at inference time to further enhance its performance. Our <a href=\"https://research.google/blog/introducing-mobility-ai-advancing-urban-transportation/\">Mobility AI</a> model leverages our two decades of innovation in maps and transportation to provide transportation agencies with powerful tools for data-driven policymaking and traffic management. It can understand traffic and parking patterns, simulate systems to allow engineers to test different scenarios, and identify effective solutions for transportation networks. This complements our consumer-facing breakthroughs in Google Maps and Search, such as specialized models for <a href=\"https://research.google/blog/how-we-created-hov-specific-etas-in-google-maps/\">calculating ETAs</a> and <a href=\"https://research.google/blog/optimizing-llm-based-trip-planning/\">optimizing trip planning</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"https://storage.googleapis.com/gweb-research2023-media/original_images/EOY-12-MobilityAI.png\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"nSN-j-bjUGA\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=nSN-j-bjUGA\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"mr25z\"><i>Our Mobility AI Traffic Simulation API is built for modeling complex, city-scale traffic scenarios. The tool provides high-fidelity simulations of road closures, helping to de-risk major infrastructure investments and validate emergency response plans. It was launched in Seattle, Denver, Boston, Philadelphia and Orlando. Click on the image above for a video demonstration.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qt6m0\">Additionally, we’ve explored a range of topics in economics and computation from pricing dynamics in <a href=\"https://arxiv.org/abs/2502.20346\" target=\"_blank\" rel=\"noopener noreferrer\">modular marketplaces</a> and in <a href=\"https://arxiv.org/abs/2503.10910\" target=\"_blank\" rel=\"noopener noreferrer\">procurement auctions</a>, to <a href=\"https://dl.acm.org/doi/10.1145/3736252.3742578\" target=\"_blank\" rel=\"noopener noreferrer\">data-driven mechanism design</a> and <a href=\"https://dl.acm.org/doi/10.1145/3696410.3714881\" target=\"_blank\" rel=\"noopener noreferrer\">various</a> <a href=\"https://dl.acm.org/doi/10.1145/3736252.3742545\" target=\"_blank\" rel=\"noopener noreferrer\">approaches</a> to optimize ad auctions. We also studied swap regret and <a href=\"https://arxiv.org/abs/2502.20229\" target=\"_blank\" rel=\"noopener noreferrer\">correlated equilibria in games</a>.</p><p data-block-key=\"6ninf\">As AI becomes increasingly integrated into our daily lives, building it with privacy at its core is critical for users and industries. To this end, we’ve developed and published <a href=\"https://research.google/blog/fine-tuning-llms-with-user-level-differential-privacy/\">novel</a> <a href=\"https://research.google/blog/beyond-billion-parameter-burdens-unlocking-data-synthesis-with-a-conditional-generator/\">algorithms</a> for <a href=\"https://research.google/blog/synthetic-and-federated-privacy-preserving-domain-adaptation-with-llms-for-mobile-applications/\">private</a> learning and <a href=\"https://research.google/blog/securing-private-data-at-scale-with-differentially-private-partition-selection/\">private</a> <a href=\"https://research.google/blog/toward-provably-private-insights-into-ai-use/\">analytics</a>, and open sourced robust software tools to enable <a href=\"https://research.google/blog/discovering-new-words-with-confidential-federated-analytics/\">external verifiability</a>. For example, we introduced <a href=\"https://research.google/blog/parfait-enabling-private-ai-with-research-tools/\">Parfait</a>, a new GitHub organization for businesses and open-source projects. It has supported Google deployments of federated learning and analytics from <a href=\"https://support.google.com/gboard/answer/12373137?hl=en#zippy=%2Cfederated-learning\" target=\"_blank\" rel=\"noopener noreferrer\">Gboard</a> to <a href=\"https://arxiv.org/abs/2412.07962\" target=\"_blank\" rel=\"noopener noreferrer\">Google Maps</a>. We also announced <a href=\"https://research.google/blog/differentially-private-machine-learning-at-scale-with-jax-privacy/\">Jax Privacy 1.0</a>, a library for ML with differential privacy, which we used to train <a href=\"https://research.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/\">VaultGemma</a>, the largest and most capable open model trained from scratch with differential privacy, with weights available on <a href=\"https://huggingface.co/google/vaultgemma-1b\" target=\"_blank\" rel=\"noopener noreferrer\">Hugging Face</a> and <a href=\"https://www.kaggle.com/models/google/vaultgemma\" target=\"_blank\" rel=\"noopener noreferrer\">Kaggle</a>. By leveling up our privacy capabilities, we offer much stronger protections to businesses and users</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Novel architectures\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Introducing novel architectures</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">Our foundational ML research introduces advanced approaches to enable new opportunities. <a href=\"https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/\">Nested Learning</a> is a new ML paradigm that represents a leap forward in our understanding of deep learning. It treats model architecture and optimization as a single system that contains several, smaller, nested optimization problems. By unifying these elements, it solves the problem of catastrophic forgetting, when LLMs become forgetful and less capable at old tasks after learning new tasks. This research could help us build the next generation of more capable, self-improving AI. Meanwhile, our <a href=\"https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/\">Titans architecture and the MIRAS framework</a> mark a significant advancement in sequence modelling. They allow AI models to work much faster and handle massive contexts by employing deep neural networks that learn to memorize as data comes in, improving AI’s long-term memory.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"MIRAS Framework\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --medium\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-13-MIRAS.width-800.gif\" alt=\"A diagram titled &quot;MIRAS Framework&quot; illustrates how input tokens are processed through associative memory, featuring a sliding focus on individual data blocks.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-13-MIRAS.width-800.gif\" alt=\"A diagram titled &quot;MIRAS Framework&quot; illustrates how input tokens are processed through associative memory, featuring a sliding focus on individual data blocks.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n        <div class=\"\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          glue-grid__col--span-12-md\n          caption\n          --center\n        \">\n          <p data-block-key=\"mr25z\"><i>In the MIRAS framework, we aim to learn an associative memory, mapping between keys and values. For each token, the memory module internally optimizes its inner attentional bias while using its retention gate to make sure that it does not deviate from its past state.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qt6m0\">We also introduced <a href=\"https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/\">MUVERA</a>, a novel retrieval algorithm that reduces complex multi-vector retrieval back to single-vector maximum inner product search, achieving state-of-the-art performance with significantly improved efficiency. It creates new possibilities for information retrieval for use in applications such as recommendation systems and natural language processing. And our progress on <a href=\"https://research.google/blog/graph-foundation-models-for-relational-data/\">graph foundational models</a> pushes the frontiers of graph learning. While most graph neural networks are fixed to a specific graph on which the model has been trained, we developed graph foundational models capable of generalizing to arbitrary tables, features and tasks. This opens up new avenues for model reuse.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Collaboration and open research\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Collaborating with the research ecosystem</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">We partner with the academic community, industry leaders, governments and scientific institutes around the world. We also continue to engage the ecosystem through our Research@ events from <a href=\"https://research.google/blog/accelerating-the-magic-cycle-of-research-breakthroughs-and-real-world-applications/\">Mountain View</a> to <a href=\"https://www.youtube.com/watch?v=sikTOH-0J_c\" target=\"_blank\" rel=\"noopener noreferrer\">Tokyo</a>, <a href=\"https://blog.google/intl/en-au/company-news/technology/research-sydney-charting-new-ai-frontiers-alongside-the-research-ecosystem-in-australia/\" target=\"_blank\" rel=\"noopener noreferrer\">Sydney</a> and <a href=\"https://blog.google/technology/research/ai-collaboration-poland-2025/\" target=\"_blank\" rel=\"noopener noreferrer\">Poland</a>, and we support hundreds of PhD students in Google’s <a href=\"https://blog.google/outreach-initiatives/google-org/phd-fellowship-program-2025/\" target=\"_blank\" rel=\"noopener noreferrer\">Fellowship Program</a>.</p><p data-block-key=\"abe8b\">As a global team, we continue to expand our footprint beyond our major hubs. Having solidified our research <a href=\"https://blog.google/intl/en-africa/company-news/outreach-and-initiatives/supporting-the-future-of-ai-research-in-africa-and-globally/\" target=\"_blank\" rel=\"noopener noreferrer\">investment</a> and <a href=\"https://blog.google/intl/en-africa/company-news/outreach-and-initiatives/5-ways-were-bringing-ai-innovations-to-people-across-africa/#:~:text=Jul%2024%2C%202025,Mail\" target=\"_blank\" rel=\"noopener noreferrer\">innovation</a> in Africa (Accra and Nairobi) and our presence in Australia, we are now preparing to inaugurate a new Google Research hub in Singapore in 2026.</p><p data-block-key=\"j9dl\">We share our work through publications, conferences, academic talks, benchmarks, datasets and open-source releases. We’ve <a href=\"https://research.google/conferences-and-events/?&amp;year=2025\">sponsored and hosted workshops at conferences</a>, most recently at <a href=\"https://research.google/conferences-and-events/google-at-neurips-2025/\">NeurIPS</a>. We recently introduced an <a href=\"https://research.google/blog/gemini-provides-automated-feedback-for-theoretical-computer-scientists-at-stoc-2026/\">experimental program</a> that provided automated feedback to scientists before they submit their conference papers for peer review, helping them to rigorously verify their work and accelerate research workflows. Plus, we launched <a href=\"https://notebooklm.google.com/notebook/24d50377-8c14-4851-bcc2-b2d67b039041\" target=\"_blank\" rel=\"noopener noreferrer\">Google Research Featured Notebooks</a> in collaboration with NotebookLM, to make research more accessible to a broader community.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"AI as an amplifier of human ingenuity\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">AI as an amplifier of human ingenuity</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">This is a golden age for research. Never before have technical breakthroughs and scientific progress so quickly materialized into impactful, real-world solutions, which, in turn, bring to the fore new data and questions that inspire new avenues of foundational research. This <a href=\"https://research.google/blog/accelerating-the-magic-cycle-of-research-breakthroughs-and-real-world-applications/\">magic cycle</a> is accelerating significantly, propelled by more powerful models, new agentic tools that support scientific discovery, and open platforms and tools.</p><p data-block-key=\"77o7f\">Together with our Google colleagues and partners, we’re advancing research and technologies that aim to be helpful in diverse areas. Our research, grounded in a rigorous dedication to safety and trust, serves to unlock human potential — whether that’s to help a scientist accelerate their research, or a student learn more effectively and master new concepts, or to empower a doctor, developer or teacher.</p><p data-block-key=\"atf0n\">It is truly an exciting time to be in research. We’re able to leverage the full stack of Google AI infrastructure, models, platforms, and world-class talent, and contribute to products used by billions. We will keep building on our legacy, asking the biggest questions of today, and aiming to enable the solutions of tomorrow. We’ll keep advancing AI in a bold and responsible way, for the benefit of society, to help enhance human capacity and make AI an amplifier of human ingenuity.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n  \n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"Accelerating the Speed and Scope of Discovery\">\n\n    \n\n\n\n\n    <div class=\"\n      glue-grid\n      \n        --remove-gap\n        --full\n      \n    \">\n      \n        <div class=\"\n          dynamic_media__item\n          glue-grid__col\n          glue-grid__col--span-4-sm\n          \n            glue-grid__col--span-12-md\n          \n          \n            glue-grid__col--span-12-lg\n          \n        \">\n          <div class=\"\">\n            \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"VqSrYdDM0Zw\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=VqSrYdDM0Zw\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n          </div>\n        </div>\n      \n\n      \n    </div>\n  </div>\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Acknowledgements\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n            <div class=\" component-intro__description --has-heading\">\n                <p data-block-key=\"2nd4e\"><i>With thanks to everyone in Google Research, and many collaborators, who have contributed to this blog and the work represented here.</i></p>\n            </div>\n        \n    </div>\n\n\n\n    \n</div>\n\n    </div>\n</section>\n\n                    \n                "
    }
  ],
  "lastUpdated": "2026-02-01T10:38:22.070Z"
}