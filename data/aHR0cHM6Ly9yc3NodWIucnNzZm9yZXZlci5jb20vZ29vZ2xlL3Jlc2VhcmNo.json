{
  "sourceUrl": "https://rsshub.rssforever.com/google/research",
  "title": "Google Research Blog",
  "description": "Google Research Blog - Powered by RSSHub",
  "link": "https://research.google/blog",
  "items": [
    {
      "title": "使用用户级差分隐私微调大型语言模型 (原标题: Fine-tuning LLMs with user-level differential privacy)",
      "link": "https://research.google/blog/fine-tuning-llms-with-user-level-differential-privacy/",
      "pubDate": "Thu, 22 May 2025 16:00:00 GMT",
      "isoDate": "2025-05-22T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 使用用户级差分隐私微调大型语言模型\n\n## 引言与背景\n*   **模型微调与隐私挑战**：现代机器学习（ML）模型常需针对特定领域数据进行微调以优化性能，然而这些数据往往涉及隐私敏感信息。差分隐私（DP）通过在训练过程中注入噪声，严格保证训练模型尊重训练数据的隐私。\n*   **个体示例级DP的局限性**：大多数DP研究关注个体示例的隐私（示例级DP）。但如果一个用户贡献了大量示例，攻击者即使无法了解单个示例，也可能推断出该用户的某些信息。\n*   **用户级DP的重要性**：用户级DP是一种更强的隐私形式，它保证攻击者无法通过模型了解用户是否包含在训练数据集中。这更符合当今社会的数据所有权模式，并常用于联邦学习中，因为分布式设备（如手机）通常包含由单个用户拥有的多个示例，需要更严格的隐私保障。\n    *   ![用户级DP概述](https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-1-Overview.width-1250.png)\n    *   用户级DP确保无法判断某个用户的所有数据是否被包含在训练中，而不仅仅是其数据的一部分。\n*   **用户级DP的挑战**：与示例级DP相比，用户级DP的学习难度更高，需要注入更多的噪声，且模型越大，问题越严重。\n\n## 在数据中心扩展用户级DP到大型语言模型（LLMs）\n*   **研究目标**：论文《在固定计算预算下使用用户级差分隐私进行学习》旨在将用户级DP扩展到在数据中心训练的大型语言模型（LLMs）。\n*   **数据中心训练的优势**：与联邦学习相比，数据中心训练更灵活，可以查询单个示例或整个用户，并且可以在每个训练轮次选择要查询的用户。\n*   **研究核心问题**：如何利用数据中心训练的这种灵活性来获得更好的训练结果？\n*   **聚焦LLM微调**：由于DP训练计算成本高昂，不适用于整个LLM的训练，且微调更可能涉及私有领域特定数据，因此研究重点放在LLM的DP微调上。目标是确定最佳算法并利用数据中心灵活性进一步优化它们，因为即使是微小的噪声减少也能带来显著的质量提升。\n\n## 使用用户级隐私训练模型\n*   **DP-SGD原理**：随机梯度下降（SGD）通过将训练数据分成小批次，计算每个示例的“梯度”并应用于模型。DP-SGD通过向梯度添加随机噪声来实现DP，从而使模型获得不完美但保护隐私的信息。\n    *   ![DP-SGD步骤可视化](https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-2a-Visualization.width-800.gif)\n    *   DP-SGD非常适合实现示例级DP，因为它直接限制了每个示例对模型的影响。\n*   **实现用户级DP的两种方法**：\n    1.  **预处理**：首先限制每个用户对训练数据集贡献的示例数量（“贡献上限”）。\n    2.  **两种采样方法**：\n        *   **示例级采样（ELS）**：直接应用DP-SGD，但通过添加更多噪声将示例级DP保证转换为用户级DP。它随机采样示例来形成批次。\n        *   **用户级采样（ULS）**：随机采样用户，然后将所有来自这些采样用户的示例组成批次。ULS在数据中心环境中类似于联邦学习（但在实际联邦学习中没有随机采样）。\n    *   ![用户贡献上限示例](https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-3a-Example.width-1250.png)\n    *   ![ELS与ULS批次形成对比](https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-4a-ELSvULS.width-800.gif)\n    *   **关键参数**：两种算法都有一个关键参数需要优化——预处理中使用的“贡献上限”。\n\n## 针对LLMs的算法优化\n*   **ELS的噪声优化**：发现以往工作添加的噪声比实际需要的多几个数量级。通过新的证明，可以在保持相同隐私保证的前提下显著减少噪声，从而提高模型质量。\n*   **ELS和ULS的贡献上限优化**：\n    *   “默认”选择是使用每个用户都满足的贡献上限（即不进行预处理），但这会导致为保护大量数据贡献者而添加大量噪声。\n    *   设置较小的贡献上限可以减少所需噪声，但代价是丢弃大量数据。\n    *   由于LLM训练成本高昂，无法尝试训练多个模型来选择最佳贡献上限，因此需要一种在训练前有效选择贡献上限的策略。\n    *   **ELS策略**：经过大规模实验，发现将贡献上限设置为每个用户持有的示例数量的中位数是一种有效策略。\n    *   **ULS策略**：提供一个总噪声随贡献上限变化的预测函数，选择使该预测最小化的贡献上限是一种有效策略。\n\n## 实验结果\n*   **ELS噪声减少**：与以往工作相比，新分析证明所需的噪声水平呈指数级降低（隐私保证ε仅呈近似线性衰减，而非指数衰减）。\n    *   ![贡献上限与隐私保证](https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-5-ContributionBound.width-1250.png)\n*   **ELS与ULS对比**：在语言模型微调任务中，使用相同总示例数进行训练，对3.5亿参数的Transformer模型在StackOverflow和CC-News数据集上进行微调。\n    *   ![CCNews和StackOverflow上的ELS、ULS和无微调对比](https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-6a-ELSvULS.width-1250.png)\n    *   ![CCNews和StackOverflow上的ELS、ULS和无微调对比](https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-6b-ELSvULS.width-1250.png)\n    *   **结果**：在大多数情况下，ULS是更好的算法。例外情况（至少对于CC-News）是需要更高隐私或计算量较少的情况。值得注意的是，由于这些优化，尽管有严格的隐私要求，两种方法都比预训练模型表现更好。\n\n## 结论\n*   **主要贡献**：优化了ELS和ULS两种DP-SGD变体在数据中心实现用户级DP的性能。\n*   **具体优化**：为ELS提供了新的隐私保证，并为ELS和ULS开发了无需多次训练即可设置贡献上限的启发式方法。这些优化为重要参数提供了有充分依据的选择。\n*   **可行性与优势**：尽管用户级DP是严格的隐私定义，且使用DP训练大型模型面临挑战，但实验证明，通过这些优化，使用用户级DP微调LLMs是可行且优于仅使用预训练模型的。\n*   **实际意义**：这项工作使模型训练者能够对敏感数据集进行模型微调，同时为用户提供强大的隐私保护。",
      "shortSummary": "该研究专注于使用用户级差分隐私（DP）微调大型语言模型（LLMs），以解决敏感数据隐私问题。文章优化了两种DP-SGD变体：示例级采样（ELS）和用户级采样（ULS），使其适用于数据中心训练。通过为ELS提供新的隐私保证和为两种方法开发贡献上限的启发式选择策略，显著减少了所需噪声并提高了模型质量。实验表明，ULS通常表现更优，且经过优化的DP微调方法在严格隐私要求下，仍能超越预训练模型，实现对敏感数据的有效模型训练。",
      "translated_title": "使用用户级差分隐私微调大型语言模型",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-1-Overview.width-1250.png",
          "alt": "UserLvlDP-1-Overview",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-2a-Visualization.width-800.gif",
          "alt": "UserLvlDP-2a-Visualization",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-3a-Example.width-1250.png",
          "alt": "UserLvlDP-3a-Example",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-4a-ELSvULS.width-800.gif",
          "alt": "UserLvlDP-4a-ELSvULS",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-5-ContributionBound.width-1250.png",
          "alt": "UserLvlDP-5-ContributionBound",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ifmk7\">The machine learning community has consistently found that while modern machine learning (ML) models are powerful, they <a href=\"https://arxiv.org/pdf/2103.08493\" target=\"_blank\" rel=\"noopener noreferrer\">often need to be fine-tuned on domain-specific data</a> to maximize performance. This can be problematic or even impossible, as informative data is often privacy-sensitive. Differential privacy (DP) allows us to train ML models while rigorously guaranteeing that the learned model respects the privacy of its training data, by injecting noise into the training process.</p><p data-block-key=\"eauhh\">Most work on DP focuses on the privacy of individual examples (i.e., <a href=\"https://desfontain.es/blog/differential-privacy-in-more-detail.html\" target=\"_blank\" rel=\"noopener noreferrer\">example-level DP</a>). This has drawbacks. If a particular user has many examples training the model, attackers may be able to learn something about that user even if they can’t learn about their individual examples.</p><p data-block-key=\"8jgo8\"><a href=\"https://arxiv.org/abs/1710.06963\" target=\"_blank\" rel=\"noopener noreferrer\">User-level DP</a> is a stronger form of privacy that guarantees that an attacker who uses a model can’t learn things about the user, like whether or not the user’s data is included in the training dataset. User-level DP better reflects how data is actually owned in today’s society. It’s used frequently in <a href=\"https://research.google/blog/federated-learning-collaborative-machine-learning-without-centralized-training-data/\">federated learning</a> to train models across distributed devices like cell phones. Those devices often have many examples, all owned by a single user, and require more stringent privacy guarantees.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-1-Overview.width-1250.png\" alt=\"UserLvlDP-1-Overview\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-1-Overview.width-1250.png\" alt=\"UserLvlDP-1-Overview\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"37bs3\"><i>User-level DP makes it so you can’t tell if</i> <b><i>all</i></b><i> of someone’s data was included in training or not, rather than just one piece of their data.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ifmk7\">Learning with user-level DP is strictly harder than example-level DP, and requires adding significantly more noise. This is a problem that gets worse as the model gets larger!</p><p data-block-key=\"2tkl7\">In “<a href=\"https://arxiv.org/abs/2407.07737\" target=\"_blank\" rel=\"noopener noreferrer\">Learning with User-Level Differential Privacy Under Fixed Compute Budgets</a>”, we set out to scale user-level DP to large language models (LLMs) trained in the datacenter. Datacenter training is much more flexible than federated learning. In federated learning, one can only perform queries on users and not on individual examples, and it is not possible to choose which users are available to query. In datacenter training, one can query both individual examples and whole users, and it is possible to choose which ones to query every round. Our central question is, how can we use this increased flexibility to achieve better training results?</p><p data-block-key=\"630i4\">Rather than training the full LLM with DP, we focus on LLM DP fine-tuning as DP requires more computation, which might be unaffordable for full LLM training, and fine-tuning is more likely to require private domain-specific data. We determine which algorithms worked best and how to use the flexibility of datacenter training to further optimize them. This optimization is important for LLMs as even small reductions in noise can result in significant quality gains. We also show that even with the added flexibility in the datacenter, our proposed training strategy looks more like an algorithm for federated learning.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Training models with user-level privacy</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ifmk7\"><a href=\"https://en.wikipedia.org/wiki/Stochastic_gradient_descent\" target=\"_blank\" rel=\"noopener noreferrer\">Stochastic gradient descent</a> (SGD) is a common model training algorithm that randomly divides training data into small batches, computes model updates, called “gradients”, for each example in the batch, and applies them to the model. To train with DP, we modify this slightly by adding random noise to the gradients, essentially combining DP with SGD in a process referred to as DP-SGD. The noise makes it so the model gets imperfect information about the examples during training, which is good for privacy! But since we are giving the model imperfect information, its ability to learn from the examples is necessarily weaker than if we gave it perfect information. However, DP is a prerequisite for using private data, and imperfect information about the private data is better than no information at all.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-2a-Visualization.width-800.gif\" alt=\"UserLvlDP-2a-Visualization\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-2a-Visualization.width-800.gif\" alt=\"UserLvlDP-2a-Visualization\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"37bs3\"><i>Visualizing the steps in (DP-)SGD.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ifmk7\">DP-SGD is great for achieving example-level DP because it directly limits how much each example affects the model. It’s been <a href=\"https://research.google/blog/bridging-the-gap-in-differentially-private-model-training/\">extensively</a> <a href=\"https://research.google/blog/differential-privacy-accounting-by-connecting-the-dots/\">studied</a> at Google, including use cases such as <a href=\"https://research.google/blog/private-ads-prediction-with-dp-sgd/\">ads modelling</a> and <a href=\"https://research.google/blog/protecting-users-with-differentially-private-synthetic-training-data/\">synthetic data generation</a>.</p><p data-block-key=\"did4l\">How do we change DP-SGD if we want user-level DP? We need to make sure to limit the effect each user has on the model.</p><p data-block-key=\"7c1mm\">There are two ways to achieve this. For both, we first pre-process the dataset so that each user only contributes a bounded number of examples to the training dataset. Then we either:</p><ol><li data-block-key=\"dp1dq\">Apply DP-SGD as-is. By adding more noise than before, we can turn our example-level DP guarantee into user-level DP (this is the hard part!).</li><li data-block-key=\"d00is\">Instead of sampling random examples in DP-SGD to form batches, sample random users, and then take all examples from the sampled users to form the batch.</li></ol><p data-block-key=\"eskpp\">The big difference between these methods is in the data we sample. The first samples random examples, so we call it “Example-Level Sampling” (ELS). The second samples random users, so we call it “User-Level Sampling” (ULS). ULS looks a lot like federated learning in the datacenter (and is actually used in real federated learning settings, but without random sampling). Note that because of the random sampling step, both algorithms are not feasible in federated learning because devices are not necessarily available for every round of training!</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-3a-Example.width-1250.png\" alt=\"UserLvlDP-3a-Example\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-3a-Example.width-1250.png\" alt=\"UserLvlDP-3a-Example\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"37bs3\"><i>An example dataset before and after we bound the contribution of each user to at most 3.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-4a-ELSvULS.width-800.gif\" alt=\"UserLvlDP-4a-ELSvULS\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-4a-ELSvULS.width-800.gif\" alt=\"UserLvlDP-4a-ELSvULS\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"37bs3\"><i>A visual comparison of how ELS and ULS form batches.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ifmk7\">Both ELS and ULS have a key parameter to optimize: the bound on the number of examples each user can contribute to the dataset, which we call the “contribution bound”, that we use in pre-processing. As we will discuss later, this parameter needs to be carefully chosen to optimize performance.</p><p data-block-key=\"f4umd\">Which of these algorithms works better, especially at scale? It’s not obvious, and it isn’t something that we found an answer to in the literature. That’s what we set out to find.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Making these algorithms work for LLMs</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ifmk7\">If we run these algorithms “out-of-the-box” for LLMs, things go badly. So, we came up with optimizations to the algorithms that fix the key issues with running them “out-of-the-box”.</p><p data-block-key=\"5g4e3\">For ELS, we had to go from example-level DP guarantees to user-level DP guarantees. We found that <a href=\"https://privacytools.seas.harvard.edu/sites/g/files/omnuum6656/files/privacytools/files/manuscript_2016.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">previous work</a> was adding orders of magnitude more noise than was actually necessary. We were able to prove that we can add significantly less noise, making the model much better while retaining the same privacy guarantees.</p><p data-block-key=\"7hcld\">For both ELS and ULS, we had to figure out how to optimize the contribution bound. A “default” choice is to choose a contribution bound that every user already satisfies; that is, we don’t do any pre-processing. However, some users may contribute a large amount of data, and we will need to add large amounts of noise to provide privacy to these users. Setting a smaller contribution bound reduces the amount of noise we need to add, but the cost is having to discard a lot of data. Because LLM training runs are expensive, we can’t afford to try training a bunch of models with different contribution bounds and pick the best one — we need an effective strategy to pick the contribution bound <i>before</i> we start training.</p><p data-block-key=\"a8hkp\">After lengthy experimentation at scale, for ELS we found that setting the contribution bound to be the median number of examples held by each user was an effective strategy. For ULS, we give a prediction for the total noise added as a function of the contribution bound, and found that choosing the contribution bound minimizing this prediction was an effective strategy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ifmk7\">We first compared the amount of noise we proved was necessary for ELS to the noise <a href=\"https://salil.seas.harvard.edu/sites/g/files/omnuum4266/files/salil/files/the_complexity_of_differential_privacy.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">past work</a> suggested was necessary. The noise levels we determined by our analyses reflected an exponential reduction in the noise needed:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-5-ContributionBound.width-1250.png\" alt=\"UserLvlDP-5-ContributionBound\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-5-ContributionBound.width-1250.png\" alt=\"UserLvlDP-5-ContributionBound\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"37bs3\"><i>Black-box calculations suggest that privacy guarantees (ε) decay exponentially in the contribution bound. Our new bound shows the privacy guarantee only decays near-linearly in the contribution bound!</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ifmk7\">We next compared ELS and ULS, both using our optimizations, on language model fine-tuning tasks, where each algorithm saw the same total number of examples per round of training. We fine-tuned a 350 million parameter transformer model on the <a href=\"https://arxiv.org/abs/2307.09619\" target=\"_blank\" rel=\"noopener noreferrer\">StackOverflow and CC-News datasets</a>, two standard research datasets for studying user-level DP.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-6a-ELSvULS.width-1250.png\" alt=\"UserLvlDP-6a-ELSvULS\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-6a-ELSvULS.width-1250.png\" alt=\"UserLvlDP-6a-ELSvULS\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-6b-ELSvULS.width-1250.png\" alt=\"UserLvlDP-6b-ELSvULS\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-6b-ELSvULS.width-1250.png\" alt=\"UserLvlDP-6b-ELSvULS\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"37bs3\"><i>Comparing ELS, ULS, and no fine-tuning on CCNews and StackOverflow.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ifmk7\">We found that in most cases, ULS is the better algorithm. The exception (at least for CC-News) was in cases where we wanted more privacy or don’t use much compute. Notably, in part thanks to our optimizations, both methods performed better than the pre-trained model, despite the strict privacy requirement.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ifmk7\">In this work, we optimized the performance of ELS and ULS, two variants of DP-SGD that achieve user-level DP and are enabled by training in the datacenter. We optimized ELS by giving new privacy guarantees for it and by developing an heuristic for setting the contribution bound without needing to do multiple training runs. We similarly optimized ULS by developing an heuristic for setting the contribution bound, again without needing to do training runs. Our optimizations provide well-justified choices for important parameters that previously were chosen in an <i>ad hoc</i> manner.</p><p data-block-key=\"cgs4i\">Despite user-level DP being a strict privacy definition and the challenges of training large models with DP, our experiments demonstrated that fine-tuning LLMs with user-level DP is both feasible and advantageous over sticking to pre-trained models thanks to our optimizations. Our work thus enables model trainers to fine-tune their models to sensitive datasets while still providing strong protections to their users.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Google 研究在 Google I/O 2025 (原标题: Google Research at Google I/O 2025)",
      "link": "https://research.google/blog/google-research-at-google-io-2025/",
      "pubDate": "Wed, 21 May 2025 16:00:00 GMT",
      "isoDate": "2025-05-21T16:00:00.000Z",
      "creator": "Google",
      "summary": "# Google 研究在 Google I/O 2025\n\n在每年的 Google I/O 大会上，Google 都会分享其最先进的技术，展示它们如何提供帮助、带来新体验，以及开发者和社区如何利用它们进行创新。许多这些新技术源于 Google Research 多年的工作，通常与其他团队合作，基于人工智能及其他计算机科学领域的多次突破。今年的 I/O 强调了将研究变为现实的影响。正如 Sundar Pichai 所说：“所有这些进展意味着我们正处于 AI 平台转变的新阶段。数十年的研究现在正在全球范围内成为人们、企业和社区的现实。”\n\n除了 Google Research 对 Gemini 和 I/O 舞台上重点介绍的生成式 AI 产品的贡献外，以下是今年的一些亮点，它们都利用了 Google Research 多年来的努力，以实现研究的“魔力循环”。\n\n## 医疗健康领域的 AI 进展：MedGemma 和 AMIE\n\n自 2022 年首次推出 Med-PaLM 以来，Google 研究团队持续推进 AI，以使医疗健康更易于获取和有效。\n\n*   **MedGemma**：\n    *   Google 在 I/O 大会上宣布了 MedGemma，这是其在多模态医学文本和图像理解方面最强大的开放模型。\n    *   它基于 Gemma 3，旨在作为开发者构建健康应用（如分析放射影像或总结临床数据）的起点。\n    *   其小尺寸使其能高效地针对特定需求进行微调。\n    *   在 MedQA 基准测试中，其在临床知识和推理任务上的基线性能与大得多的模型相似。\n    *   MedGemma 是开放的，可在开发者首选环境（包括 Google Cloud Platform 或本地）中运行。\n    *   MedGemma 4B 和 27B（仅文本）模型现已作为健康 AI 开发者基础（HAI-DEF）的一部分，在 HuggingFace 和 Vertex Model Garden 上提供。\n    *   ![MedGemma 的基线性能](https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-2-CostPerformance.width-1250.png)\n        *   *MedGemma 在临床知识和推理任务上的基线性能与大得多的模型相似。*\n*   **AMIE**：\n    *   与 Google DeepMind 合作开发的 AMIE 也在 I/O 上被重点提及。\n    *   AMIE 是一种用于医疗诊断对话的研究型 AI 代理。\n    *   新的多模态版本能够智能地解释和推理视觉医学信息，帮助临床医生实现更准确的诊断。\n\n## 学习领域的 AI 进展：LearnLM\n\n近两年来，Google Research 和 Google 内部团队一直与教育专家合作开发 LearnLM，这是一个针对学习进行微调的模型家族。\n\n*   **LearnLM 在 Gemini 2.5 中可用**：\n    *   在 I/O 上宣布，LearnLM 将直接在 Gemini 2.5 中提供，使其成为全球领先的学习模型。\n    *   最新技术报告显示，Gemini 2.5 Pro 在学习科学原理方面优于其他模型，是教育工作者的首选。\n    *   它具有先进的 STEM 推理、多模态理解、测验和评估能力等。\n    *   ![LearnLM 的教学法](https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-3-Pedagogy.width-1250.png)\n*   **新的 Gemini 测验体验**：\n    *   Google Research 团队帮助设计和优化了 Gemini 中的新测验体验。\n    *   学生（18 岁以上）可以要求 Gemini 根据他们的课堂笔记或课程文档创建自定义测验，并提供关于正确和错误答案的反馈和解释。\n*   **LearnLM 提示指南与合作**：\n    *   探索 LearnLM 提示指南，以最大化 Gemini 的教学价值，例如，要求它扮演生物老师或调整文本难度。\n    *   Google 正在与合作伙伴合作，将 LearnLM 模型的强大功能引入教育环境，例如与 Kayma 合作在加纳高中试点自动评估。\n\n## Gemma 的多语言和效率：让模型普惠可用\n\n作为 Google 使世界信息普遍可访问使命的一部分，Google 正在推进多语言研究，以确保 LLM 在不同语言中产生可靠输出。\n\n*   **Gemma3 的多语言能力**：\n    *   两个月前，Google 推出了 Gemma3，其研究帮助 Gemma 扩展到 140 多种语言，使其成为当今最佳的多语言开放模型。\n*   **Gemma3n 的效率**：\n    *   在 I/O 上宣布，这些功能现已通过 Gemmaverse 的最新成员 Gemma3n 提供，该模型只需 2GB 内存即可运行，专为设备上应用而构建。\n    *   效率方面的努力使 Gemma3n 模型能够降低延迟并更节能。\n*   **ECLeKTic 基准**：\n    *   Google Research 最近推出了 ECLeKTic，这是一个用于评估 LLM 中跨语言知识迁移的新型基准。\n\n## 高效且有根据的模型：助力搜索中的 AI 模式\n\n随着 LLM 规模的扩大和需求的增加，提高模型效率同时保持甚至提升其质量，是实现这些高性能模型普惠化的关键。\n\n*   **效率突破**：\n    *   Google Research 在效率方面取得了突破，例如在推测解码和级联方面的工作，这些已成为行业标准。\n*   **事实一致性与准确性**：\n    *   Google 发布了关于事实一致性技术和评估的研究，并通过双重检查和 FACTS Grounding 排行榜（与 Google DeepMind 和 Kaggle 合作发布）设定了事实性和依据性的标准。\n*   **AI 模式**：\n    *   Google 将其研究贡献给了搜索中的 AI 模式，以显著改善用户体验。\n    *   在 I/O 上宣布的 AI 模式是 Google 最强大的 AI 搜索，具有高级推理能力。\n    *   它正在向美国所有用户推出，允许人们通过后续问题和相关网站链接进行更深入的研究。\n    *   效率工作使模型运行更可靠，输出更快；事实性研究改进了 AI 模式的网页搜索方式，确保提供的答案高度准确并基于多个来源。\n\n## 多模态事实性：助力 Imagen4、Gemini 2.5 和 Vids 中的 AI 头像\n\n随着多模态内容的普及，Google 的事实性团队正在推进多模态事实性研究，以确保 Google 产品的高准确性标准。\n\n*   **Imagen4**：改进了 Gemini 应用中 Imagen4 的质量，该模型可提供逼真的视觉效果。\n*   **Vids 中的 AI 头像**：帮助评估模型和图像字幕的质量，使用户能够在几秒钟内创建带有 AI 头像的视频内容。\n*   **Gemini 2.5**：显著增强了 Gemini 2.5 模型的视频理解能力，特别是高运动理解，使其更能评估健康和健身领域的人体运动。\n\n## Sparkify：将任何问题转化为动画视频\n\nGoogle 团队支持推出了新的 Labs 实验 Sparkify。\n\n*   它结合了 Gemini、MusicLM、AudioLM 和 Veo 的强大功能。\n*   允许用户将任何问题或想法转化为选择设计风格的短小引人入胜的动画视频。\n*   该项目建立在底层模型及其事实性之上。\n\n## FireSat：实现更早地检测小型野火\n\n作为 Google 长期以来帮助减少野火破坏性影响的努力的一部分，Google Research 与 Earth Fire Alliance、Moore Foundation 和 Muon Space 合作开发了 FireSat。\n\n*   **FireSat 卫星星座**：\n    *   FireSat 是一个卫星星座，旨在实现更早、更准确的全球野火检测。\n    *   它使用高分辨率多光谱卫星图像和 AI 为急救人员提供近乎实时的洞察，并允许科学家和机器学习专家研究火灾蔓延。\n    *   3 月份，星座中的 50 多颗卫星中的第一颗已发射。\n*   **扩展现有工作**：\n    *   这项工作扩展了 Google 的野火边界跟踪（在搜索和地图中提供关键信息）和合成 Firebench 数据集（在 Google Cloud Platform 上发布以推进该领域的科学研究）。\n\n## 量子 AI：真实世界应用的切实潜力\n\n在 Dialogues 舞台上，WAYE 创始人 Sinead Bovell 和 Google 量子硬件团队高级总监 Julian Kelly 讨论了量子计算的前景以及仍需克服的工程和科学挑战。\n\n*   **量子 AI 进展**：\n    *   Julian 强调了 Google Research 量子 AI 团队的最新进展，包括 Willow 芯片和量子纠错等领域的进展。\n    *   经典计算机无法完成的计算可以在量子芯片上在几分钟内完成，为未来各种真实世界应用铺平道路。\n    *   彻底改变药物发现和能源效率等领域的潜力正变得越来越切实。\n*   **互动体验**：\n    *   Google 还为 I/O 现场的参与者创建了互动量子 AI 游戏体验：量子迷宫跑者。\n\n## AI 联合科学家：加速科学发现\n\nGoogle 的 AI 联合科学家（在 I/O 上提及，与 Google DeepMind 合作开发）是一个基于 Gemini 的多智能体系统。\n\n*   **功能与应用**：\n    *   它能够合成信息并执行复杂的推理任务。\n    *   它被设计为科学家的协作工具，帮助他们创建新颖的假设和研究提案，并加速生物医学发现。\n    *   它在急性髓系白血病的药物再利用和肝纤维化新治疗靶点的假设提出等领域展现了潜力。\n*   **其他科学研究加速努力**：\n    *   **地理空间推理**：旨在推进公共卫生、城市规划、综合业务规划、气候科学等。\n    *   **神经科学**：\n        *   LICONN：首个使用常用光学显微镜全面绘制脑组织中神经元及其连接的方法。\n        *   斑马鱼活动预测基准（ZAPBench）：首次允许研究人员调查整个脊椎动物大脑中结构连接和动态神经活动之间的关系。\n    *   **基因组学**：\n        *   REGLE：一种无监督深度学习模型，帮助研究人员发现与遗传变异的关联。\n        *   开源新的 DeepVariant 模型：作为个性化泛基因组参考合作的一部分，可将分析不同祖先基因组时的错误减少 30%。\n\n## 结论\n\n本文重点介绍的研究代表了 Google Research 团队正在进行的一些工作，这些团队正在推动各个领域的突破并将其变为现实。在这个研究的黄金时代，研究与现实世界应用之间的“魔力循环”越来越快，范围也越来越广，I/O 是展示这如何对人类、企业、科学和社会产生更大影响的绝佳机会。",
      "shortSummary": "Google I/O 2025 展示了 Google Research 的最新 AI 突破，将研究变为现实。亮点包括：医疗健康领域的 MedGemma 和 AMIE；教育领域的 LearnLM，使 Gemini 成为领先学习模型；Gemma 模型的跨语言和高效率；助力搜索中 AI 模式的改进；多模态事实性提升 Imagen4 和 Gemini 2.5；创意工具 Sparkify；用于早期野火检测的 FireSat 卫星；量子 AI 的实际应用潜力；以及通过 AI 联合科学家加速科学发现，涵盖地理空间、神经科学和基因组学等领域。",
      "translated_title": "Google 研究在 Google I/O 2025",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-2-CostPerformance.width-1250.png",
          "alt": "GRatIO25-2-CostPerformance",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-3-Pedagogy.width-1250.png",
          "alt": "GRatIO25-3-Pedagogy",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wrbp3\">Each year at Google I/O, we share some of Google’s most advanced technologies. We show how they can be helpful and provide new experiences, and how developers and other communities can use them to innovate. Many of these new technologies emerged from years of work within Google Research, many in collaboration with other teams, building on multiple successive breakthroughs in AI and other areas of computer science. This year’s I/O highlights the impact of bringing <a href=\"https://blog.google/technology/ai/io-2025-keynote/\" target=\"_blank\" rel=\"noopener noreferrer\">research to reality</a>. As Sundar put it: “What all this progress means is that we’re in a new phase of the AI platform shift. Where decades of research are now becoming reality for people, businesses and communities all over the world.”</p><p data-block-key=\"dbi8g\">In addition to Google Research’s contributions to Gemini and the generative AI products highlighted on the I/O stage, here are some of our favorites this year, tapping into years-long efforts from Google Research to realize the <a href=\"https://blog.google/technology/research/what-is-google-research/\" target=\"_blank\" rel=\"noopener noreferrer\">magic cycle</a> of research.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">MedGemma and AMIE: Advancing healthcare with AI</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">Since we first introduced <a href=\"https://www.nature.com/articles/s41586-023-06291-2\" target=\"_blank\" rel=\"noopener noreferrer\">Med-PaLM</a> in 2022, followed by <a href=\"https://www.nature.com/articles/s41591-024-03423-7\" target=\"_blank\" rel=\"noopener noreferrer\">Med-PaLM2</a> and <a href=\"https://research.google/blog/advancing-medical-ai-with-med-gemini/\">Med-Gemini</a>, our research teams have been continuously advancing AI to make healthcare more accessible and effective. At I/O, we announced <a href=\"https://blog.google/technology/developers/google-ai-developer-updates-io-2025/\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma</a>, Google’s most capable open model for multimodal medical text and image comprehension. It has the potential to speed up the development of new healthcare products.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wrbp3\">MedGemma is based on <a href=\"https://blog.google/technology/developers/gemma-3/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma 3</a> and designed to be a starting point for developers building health applications, such as analyzing radiology images or summarizing clinical data. Its small size makes it efficient for fine-tuning for specific needs, and when evaluated on the <a href=\"https://arxiv.org/pdf/2009.13081\" target=\"_blank\" rel=\"noopener noreferrer\">MedQA benchmark</a>, its baseline performance on clinical knowledge and reasoning tasks is similar to that of much larger models. Since MedGemma is open, it can be run in the developer’s preferred environment, including on the Google Cloud Platform or locally. Both MedGemma 4B and the 27B text-only models are <a href=\"https://developers.google.com/health-ai-developer-foundations/medgemma\" target=\"_blank\" rel=\"noopener noreferrer\">now available</a> on HuggingFace and Vertex Model Garden as part of our <a href=\"http://goo.gle/hai-def\" target=\"_blank\" rel=\"noopener noreferrer\">Health AI Developer Foundations</a> (HAI-DEF).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-2-CostPerformance.width-1250.png\" alt=\"GRatIO25-2-CostPerformance\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-2-CostPerformance.width-1250.png\" alt=\"GRatIO25-2-CostPerformance\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"xpbds\"><i>MedGemma’s baseline performance on clinical knowledge and reasoning tasks is similar to that of much larger models.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wrbp3\">MedGemma follows our recent announcement about AMIE, developed in collaboration with Google DeepMind, that was also highlighted at I/O. <a href=\"https://research.google/blog/amie-a-research-ai-system-for-diagnostic-medical-reasoning-and-conversations/\">AMIE</a> is a research <a href=\"https://www.nature.com/articles/s41586-025-08869-4\" target=\"_blank\" rel=\"noopener noreferrer\">AI agent</a> for medical <a href=\"https://www.nature.com/articles/s41586-025-08866-7\" target=\"_blank\" rel=\"noopener noreferrer\">diagnostic conversations</a>. The new <a href=\"https://research.google/blog/amie-gains-vision-a-research-ai-agent-for-multi-modal-diagnostic-dialogue/\">multimodal version</a> can intelligently interpret and reason about visual medical information, helping clinicians towards more accurate diagnoses.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">LearnLM: Making Gemini the world’s leading model for learning</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">For nearly two years, our teams at Google Research and across Google have been collaborating with educational experts on <a href=\"https://storage.googleapis.com/deepmind-media/LearnLM/LearnLM_paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">LearnLM</a>, a family of fine-tuned models for learning. At I/O we announced that LearnLM will now be <a href=\"https://cloud.google.com/solutions/learnlm?e=48754805&amp;hl=en\" target=\"_blank\" rel=\"noopener noreferrer\">available</a> directly in Gemini 2.5, making it the world’s <a href=\"https://blog.google/outreach-initiatives/education/google-gemini-learnlm-update/\" target=\"_blank\" rel=\"noopener noreferrer\">leading model for learning</a>. Our latest <a href=\"https://storage.googleapis.com/deepmind-media/LearnLM/learnLM_may25.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">tech report</a> demonstrates that Gemini 2.5 Pro outperforms alternative models on learning science principles and is the preferred choice for educators. It has advanced STEM reasoning, multimodal understanding, quizzing and assessment capabilities, and much more.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-3-Pedagogy.width-1250.png\" alt=\"GRatIO25-3-Pedagogy\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-3-Pedagogy.width-1250.png\" alt=\"GRatIO25-3-Pedagogy\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wrbp3\">We also launched a new <a href=\"https://blog.google/products/gemini/gemini-app-updates-io-2025/#quizzes\" target=\"_blank\" rel=\"noopener noreferrer\">quiz experience in Gemini</a>, which our Research team helped to design and optimize for learning. Students (ages 18+) can ask Gemini to create custom quizzes to help them study any topic, based on their class notes or course documents, and it will offer feedback and explanations about right and wrong answers.</p><p data-block-key=\"enldb\">Explore our <a href=\"https://services.google.com/fh/files/misc/learnlm_prompt_guide.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">LearnLM prompt guide</a> to maximize the pedagogical value of Gemini, for example, by asking it to act as a biology teacher or to adjust the difficulty level of text for a particular school grade.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        <div class=\"glue-ambient-video \">\n            <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/GRatIO25-4-Releveling.mp4\" type=\"video/mp4\">\n            </video>\n            <div class=\"glue-ambient-video__button glue-ambient-video__button--paused\" aria-label=\"Video Play/pause\">\n                <div class=\"glue-ambient-video__tooltip\">\n                  <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                  <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                </div>\n                <div class=\"glue-ambient-video__icon\">\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                </div>\n              </div>\n        </div>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wrbp3\">As well as infusing pedagogy into Google products, we’re working with partners to bring the powerful capabilities of our LearnLM models to educational settings. Along with <a href=\"https://kayma.com/kayma-english.html\" target=\"_blank\" rel=\"noopener noreferrer\">Kayma</a>, we piloted the <a href=\"http://milgo.io/en\" target=\"_blank\" rel=\"noopener noreferrer\">automatic assessment</a> of both short and long-form content with thousands of students and educators in high schools in Ghana, and we’re working to scale to more students and countries.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Multilinguality and Efficiency in Gemma: Making our models accessible and useful for everyone</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">As part of Google’s mission to make the world’s information universally accessible, we are advancing research into multilinguality to ensure that LLMs produce reliable outputs in different languages and are truly useful for everyone around the world. Two months ago, Google introduced <a href=\"https://blog.google/technology/developers/gemma-3/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma3</a>, and our research helped Gemma expand to over 140 languages, making it today’s best multilingual open model. At I/O, we announced that these capabilities are now available with the latest addition to the Gemmaverse, Gemma3n, a model that can run on as little as two gigabytes of RAM and is built for on-device applications. Our efforts around efficiency enable the Gemma3n model to reduce latency and be more energy consumption friendly.</p><p data-block-key=\"48b9m\">To help developers build and improve multilingual models, Google Research recently introduced <a href=\"https://research.google/blog/eclektic-a-novel-benchmark-for-evaluating-cross-lingual-knowledge-transfer-in-llms/\">ECLeKTic</a>, a novel benchmark for evaluating cross-lingual knowledge transfer in LLMs.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Efficient and grounded models: Contributing to AI Mode in Search</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">As LLMs grow larger and demand increases, our ability to improve model efficiency while maintaining and even elevating their quality determines our success in democratizing access to these high-performing models. Google Research has made breakthroughs in efficiency that have become industry standards, for example, our work on <a href=\"https://research.google/blog/looking-back-at-speculative-decoding/\">speculative decoding</a> and <a href=\"https://arxiv.org/pdf/2307.02764\" target=\"_blank\" rel=\"noopener noreferrer\">cascades</a>.</p><p data-block-key=\"d9btd\">We have published research on factual consistency <a href=\"https://arxiv.org/abs/2306.00186\" target=\"_blank\" rel=\"noopener noreferrer\">techniques</a> and <a href=\"https://arxiv.org/abs/2204.04991\" target=\"_blank\" rel=\"noopener noreferrer\">evaluations</a>, and set the bar on factuality and grounding with features like <a href=\"https://blog.google/products/gemini/google-gemini-new-features-july-2024/\" target=\"_blank\" rel=\"noopener noreferrer\">double-check</a> and the <a href=\"https://storage.googleapis.com/deepmind-media/FACTS/FACTS_grounding_paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">FACTS Grounding leaderboard</a>, released in collaboration with Google DeepMind and Kaggle. Now, we have contributed our research to <a href=\"https://blog.google/products/search/google-search-ai-mode-update/#ai-mode-search\" target=\"_blank\" rel=\"noopener noreferrer\">AI Mode</a>, to meaningfully improve the experience for users.</p><p data-block-key=\"3kd8n\">Announced at I/O, AI Mode is Google’s most powerful AI search yet with advanced reasoning capabilities. It is rolling out to all users in the U.S., allowing people to conduct deeper research with follow-up questions and links to relevant sites. Our work on efficiency enables the models to run more reliably and serve quicker outputs, and our factuality research has improved the way AI Mode searches the web, helping to ensure that the answers provided are highly accurate and grounded in multiple sources with relevant links.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Multimodal factuality: Contributing to Imagen4, Gemini 2.5, and AI Avatars in Vids</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">As multimodal content becomes ubiquitous, our factuality team is advancing research around multimodal factuality to ensure high accuracy standards across Google products. We improved the quality of <a href=\"https://blog.google/products/gemini/gemini-app-updates-io-2025/#imagen-4-veo-3\" target=\"_blank\" rel=\"noopener noreferrer\">Imagen4</a> on the Gemini app, the latest image model announced at I/O, which can deliver visuals with lifelike detail. For <a href=\"https://www.youtube.com/watch?v=X2G3utAQuAU\" target=\"_blank\" rel=\"noopener noreferrer\">AI avatars</a> in Vids, a new feature that enables users to create video content with their chosen AI avatars in a matter of seconds, we helped to evaluate the quality of the model and image captions. We also delivered significant enhancements to the video understanding capabilities of Gemini 2.5 models, specifically targeting high motion understanding so that Gemini is more capable of assessing human motion across health and fitness domains.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Sparkify: Turning any question into an animated video</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">Our teams helped support the <a href=\"https://blog.google/outreach-initiatives/education/google-gemini-learnlm-update/\" target=\"_blank\" rel=\"noopener noreferrer\">launch</a> of the new Labs experiment, <a href=\"https://sparkify.withgoogle.com/explore\" target=\"_blank\" rel=\"noopener noreferrer\">Sparkify</a>. Bringing together the power of Gemini, <a href=\"https://musiclm.com/\" target=\"_blank\" rel=\"noopener noreferrer\">MusicLM</a>, <a href=\"https://research.google/blog/audiolm-a-language-modeling-approach-to-audio-generation/\">AudioLM</a>, and <a href=\"https://deepmind.google/models/veo/\" target=\"_blank\" rel=\"noopener noreferrer\">Veo</a>, Sparkify allows users to turn any question or idea into a short and engaging animated video in the design style of their choice. The project builds on the underlying models and their factuality. <a href=\"http://sparkify.withgoogle.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Sign up for the waitlist</a> for a chance to try it out.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme- --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n    <div data-gt-id=\"media\" data-gt-component-name=\"\">\n        \n\n        \n\n\n\n\n        \n            \n                \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"lstWBbY-Iqk\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=lstWBbY-Iqk\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n                \n            \n        \n    </div>\n\n\n\n    </div>\n</section>\n\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">FireSat: Enabling the detection of smaller wildfires earlier</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">As part of our long-established <a href=\"https://sites.research.google/gr/wildfires/\">efforts</a> to help reduce the devastating impacts of wildfires, Google Research has partnered with the <a href=\"https://www.earthfirealliance.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Earth Fire Alliance</a>, the <a href=\"https://www.moore.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Moore Foundation</a> and <a href=\"https://www.muonspace.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Muon Space</a> to develop <a href=\"https://sites.research.google/gr/wildfires/firesat/\">FireSat</a>. FireSat is a constellation of satellites built for earlier and more accurate global wildfire detection. It uses high-res multispectral <a href=\"https://blog.google/outreach-initiatives/sustainability/google-ai-wildfire-detection/\" target=\"_blank\" rel=\"noopener noreferrer\">satellite imagery and AI</a> to provide near real-time insights for first responders, and to allow scientists and ML experts to study fire propagation. In March, we launched the first of over 50 satellites in the constellation. This work expands upon our <a href=\"https://blog.google/outreach-initiatives/sustainability/google-wildfire-boundary-maps-europe-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">wildfire boundary tracking</a>, which makes critical information available in Search and Maps, and the synthetic <a href=\"https://arxiv.org/abs/2406.08589\" target=\"_blank\" rel=\"noopener noreferrer\">Firebench dataset</a>, which we released on the <a href=\"https://github.com/google-research/firebench\" target=\"_blank\" rel=\"noopener noreferrer\">Google Cloud Platform</a> to advance scientific research in the field.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        <div class=\"glue-ambient-video \">\n            <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/GRatIO25-6-FireSat.mp4\" type=\"video/mp4\">\n            </video>\n            <div class=\"glue-ambient-video__button glue-ambient-video__button--paused\" aria-label=\"Video Play/pause\">\n                <div class=\"glue-ambient-video__tooltip\">\n                  <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                  <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                </div>\n                <div class=\"glue-ambient-video__icon\">\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                </div>\n              </div>\n        </div>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"xpbds\"><i>FireSat is the first satellite constellation for the early detection of wildfires in high resolution imagery.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Quantum AI: Tangible potential for real-world applications</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">On the Dialogues Stage, Sinead Bovell, founder of WAYE, and Julian Kelly, Senior Director, from our Quantum Hardware team, discussed the promise of quantum computing and the engineering and scientific challenges that still need to be overcome. Julian highlighted the recent advances from Google Research's <a href=\"https://quantumai.google/\" target=\"_blank\" rel=\"noopener noreferrer\">Quantum AI</a> team, including our <a href=\"https://blog.google/technology/research/google-willow-quantum-chip/\" target=\"_blank\" rel=\"noopener noreferrer\">Willow chip</a> and progress in areas like <a href=\"https://research.google/blog/making-quantum-error-correction-work/\">quantum error correction</a>. Computations that are beyond reach for classical computers can be completed on a quantum chip in minutes, paving the way for various <a href=\"https://blog.google/technology/research/google-quantum-computer-real-world-applications/\" target=\"_blank\" rel=\"noopener noreferrer\">real-world applications</a> in the future. The potential to revolutionize fields like drug discovery and energy efficiency is becoming increasingly tangible.</p><p data-block-key=\"b55a5\">We also created an interactive Quantum AI game experience for attendees on the ground at I/O: the Quantum Maze Runner. Players had to race against the clock to complete the maze, and then see how the quantum computer would solve it.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">AI Co-Scientist: Accelerating scientific discovery</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">Our <a href=\"https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/\">AI co-scientist,</a> mentioned at I/O and developed in collaboration with Google DeepMind, is a multi-agent system based on Gemini that can synthesize information and perform complex reasoning tasks. It is designed as a collaborative tool for scientists, to aid them in creating novel hypotheses and research proposals, and to help accelerate biomedical discoveries. It has <a href=\"https://arxiv.org/abs/2502.18864\" target=\"_blank\" rel=\"noopener noreferrer\">demonstrated</a> potential in areas such as drug repurposing for acute myeloid leukemia and proposing hypotheses for novel treatment targets for liver fibrosis.</p><p data-block-key=\"biv0h\">It is one of our many efforts to <a href=\"https://blog.google/technology/research/google-research-scientific-discovery/\" target=\"_blank\" rel=\"noopener noreferrer\">accelerate scientific research</a> across the wider ecosystem. Our new <a href=\"https://research.google/blog/geospatial-reasoning-unlocking-insights-with-generative-ai-and-multiple-foundation-models/\">Geospatial Reasoning</a> initiative aims to advance public health, urban planning, integrated business planning, climate science and more. We’re also advancing neuroscience, with our recent publication on <a href=\"https://www.nature.com/articles/s41586-025-08985-1\" target=\"_blank\" rel=\"noopener noreferrer\">LICONN,</a> the first-ever method for using commonly available light microscopes to comprehensively map neurons and their connections in brain tissue, and with the release of the <a href=\"https://research.google/blog/improving-brain-models-with-zapbench/\">Zebrafish Activity Prediction Benchmark</a> (ZAPBench), which allows researchers to <a href=\"https://arxiv.org/abs/2503.02618\" target=\"_blank\" rel=\"noopener noreferrer\">investigate the relationship</a> between the structural wiring and dynamic neural activity across an entire vertebrate brain for the first time. We’re also advancing research into genomics to help diagnose rare diseases; <a href=\"https://www.nature.com/articles/s41588-024-01831-6\" target=\"_blank\" rel=\"noopener noreferrer\">REGLE</a> is an unsupervised deep learning model that helps researchers discover associations with genetic variants. And we open sourced new <a href=\"https://github.com/google/deepvariant/blob/r1.8/docs/pangenome-aware-wgs-vg-case-study.md\" target=\"_blank\" rel=\"noopener noreferrer\">DeepVariant models</a> as part of a collaboration on <a href=\"https://www.nature.com/articles/s41592-024-02407-2\" target=\"_blank\" rel=\"noopener noreferrer\">Personalized Pangenome References</a>, which can reduce errors by 30% when analyzing genomes of diverse ancestries.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">The research highlighted here represents some of the ongoing work done by the Google Research teams who are driving breakthroughs across a variety of fields and bringing them to reality. In this <a href=\"https://blog.google/technology/research/what-is-google-research/\" target=\"_blank\" rel=\"noopener noreferrer\">golden age of research</a>, the “magic cycle” between research and real-world application is increasingly faster and broader in scope, and I/O was a great opportunity to showcase how this leads to greater impact on people, businesses, science and society.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\"><i>With thanks to the many teams and collaborators who have contributed to this blog and the work represented here.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "深入理解检索增强生成：充足上下文的作用 (原标题: Deeper insights into retrieval augmented generation: The role of sufficient context)",
      "link": "https://research.google/blog/deeper-insights-into-retrieval-augmented-generation-the-role-of-sufficient-context/",
      "pubDate": "Tue, 13 May 2025 16:00:00 GMT",
      "isoDate": "2025-05-13T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 深入理解检索增强生成：充足上下文的作用\n\n### 引言：RAG的挑战与“充足上下文”的提出\n\n检索增强生成（RAG）系统通过向大型语言模型（LLM）提供相关的外部上下文来增强其能力，例如在问答任务中整合来自网页、文档库或知识图谱的信息。理想情况下，LLM应给出正确答案，或在信息不足时回应“我不知道”。然而，RAG系统面临的主要挑战是可能产生幻觉（即不正确的信息），误导用户。以往的研究多关注上下文的“相关性”，但本文认为更关键的是上下文是否提供了“足够的信息”来回答问题。\n\n在ICLR 2025发表的论文《充足上下文：检索增强生成系统的新视角》中，研究团队深入探讨了RAG系统中的“充足上下文”概念。他们证明了可以判断LLM何时拥有足够信息来提供正确答案，并量化了上下文充足性对事实准确性的影响，从而分析RAG系统成功或失败的因素。\n\n此外，这些研究成果已被应用于Vertex AI RAG引擎中的LLM Re-Ranker功能，该功能允许用户根据查询相关性重新排序检索到的片段，从而提升检索指标（如nDCG）和RAG系统准确性。\n\n![SufficientContext2_RAG](https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext2_RAG.width-1250.png)\n*RAG系统中，LLM利用检索到的上下文对输入问题提供响应。* \n\n### 核心概念：充足上下文的定义\n\n研究将上下文定义为“充足”：如果它包含提供查询明确答案所需的所有必要信息。反之，如果上下文缺乏必要信息、不完整、不确定或包含矛盾信息，则定义为“不充足”。\n\n**示例：**\n*   **输入查询：** “页面未找到”的错误代码以404房间命名，该房间曾存储错误消息的中央数据库，位于哪个著名实验室？\n*   **充足上下文：** “页面未找到”错误，通常显示为404代码，以欧洲核子研究组织（CERN）的404房间命名。这是存储包括页面未找到错误在内的中央错误消息数据库的房间。\n*   **不充足上下文：** 404错误或“页面未找到”错误表示Web服务器找不到请求的页面。这可能由于URL中的拼写错误、页面被移动或删除，或网站的临时问题等各种原因。\n\n第二个上下文虽然与用户查询高度相关，但并未回答问题，因此被视为不充足。\n\n### 开发充足上下文自动评估器\n\n为了量化上下文充足性，研究团队首先开发了一个基于LLM的自动评估器（“autorater”），用于评估查询-上下文对。为了验证自动评估器的有效性，他们首先请人类专家分析了115个问答-上下文示例，以确定上下文是否足以回答问题，这成为了“黄金标准”。随后，LLM对相同的问答和上下文进行评估，输出“true”（充足）或“false”（不充足）。\n\n为优化模型的任务解决能力，研究团队通过思维链提示和提供单样本示例等策略改进了提示词。结果显示，使用优化后的提示词，模型能够以极高的准确率（至少93%）对充足上下文进行分类。表现最佳的方法是未经微调的Gemini 1.5 Pro。与依赖真实答案的方法（如TRUE-NLI和“Contains GT”）相比，Gemini表现更优，这可能归因于其卓越的语言理解能力。\n\n![SufficientContext3_AutoRater](https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext3_AutoRater.width-1250.png)\n*我们的充足上下文自动评估方法。我们使用一个经过提示的LLM来评估由输入查询和检索到的上下文组成的示例。模型输出一个二元真/假标签，表示充足与不充足上下文。*\n\n![SufficientContext4_Accuracy](https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext4_Accuracy.width-1250.png)\n*分类充足上下文的准确率，衡量各种自动方法与人工标注标签之间的一致性。*\n\n### RAG系统的关键洞察\n\n利用充足上下文自动评估器，研究团队分析了各种LLM和数据集的性能，得出以下关键发现：\n\n*   **最先进的大型模型：** 如Gemini、GPT和Claude，在提供充足上下文时通常能很好地回答查询，但缺乏识别并在上下文不充足时避免生成错误答案的能力。\n*   **小型开源模型：** 分析揭示，即使上下文足以正确回答问题，开源模型仍存在较高的幻觉和拒绝回答率。\n*   **不充足上下文的价值：** 有时模型在上下文被评为不充足时也能生成正确答案。这表明不充足上下文仍可能有用，例如弥补模型知识空白或澄清查询歧义。\n*   **可操作的建议：** 基于这些发现，研究团队提出了改进RAG系统的建议，包括在生成前添加充足性检查、检索更多上下文或重新排序、以及根据置信度和上下文信号调整拒绝回答阈值。\n\n### 充足上下文背后的研究：数据集分析\n\n研究分析表明，许多标准基准数据集包含大量不充足上下文的实例，例如FreshQA、HotPotQA和MuSiQue。上下文充足实例比例较高的数据集（如FreshQA）往往是那些上下文来源于人工整理的支持文档的数据集。\n\n![SufficientContext5_Sufficiency](https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext5_Sufficiency.width-1250.png)\n*比较三个数据集（x轴）中包含充足上下文的示例百分比（y轴）。*\n\n### 悖论：添加上下文反而增加幻觉\n\n一个令人惊讶的发现是，尽管RAG通常能提高整体性能，但它却矛盾地降低了模型在适当情况下拒绝回答的能力。引入额外上下文似乎增加了模型的信心，导致其更倾向于产生幻觉而非拒绝回答。\n\n为了理解这一现象，研究团队使用Gemini评估每个模型响应，并将其与可能的真实答案集进行比较。他们将每个响应分类为“正确”、“幻觉”（即不正确答案）或“拒绝回答”（例如，说“我不知道”）。通过这种方法，他们发现，例如，Gemma在没有上下文时错误回答问题的比例为10.2%，而在使用不充足上下文时，这一比例上升到66.1%。\n\n![SufficientContext6_Analysis](https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext6_Analysis.width-1250.png)\n*在四种不同RAG设置下对三个LLM的详细分析。*\n\n### 选择性生成以减少幻觉\n\n作为另一项贡献，研究团队开发了一个“选择性生成”框架，该框架利用充足上下文信息来指导拒绝回答。他们考虑了以下指标：选择性准确率（模型在回答的问题中正确答案的比例）和覆盖率（回答问题的比例）。\n\n选择性生成方法结合了充足上下文信号和模型自评置信度分数，以做出何时拒绝回答的明智决策。这比简单地在上下文不充足时拒绝回答更为精细，因为模型有时即使在有限上下文下也能提供正确答案。研究团队利用这些信号训练了一个逻辑回归模型来预测幻觉，然后设置了一个覆盖率-准确率权衡阈值，决定模型何时应拒绝回答。\n\n该方法使用两个主要信号进行拒绝回答：\n\n1.  **自评置信度：** 采用两种策略：P(True)（多次采样答案并提示模型标记每个样本为正确或不正确）和P(Correct)（用于查询成本较高的模型，获取模型响应及其估计的正确概率）。\n2.  **充足上下文信号：** 使用来自自动评估器模型（FLAMe）的二元标签来指示上下文是否充足。关键在于，确定充足上下文标签不需要真实答案，因此可以在回答问题时使用此信号。\n\n结果表明，与仅使用模型置信度相比，这种方法在选择性准确率-覆盖率权衡方面表现更优。通过使用充足上下文标签，模型在回答问题上的准确率有时可提高高达10%。\n\n![SufficientContext8_ResultsFinal](https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext8_ResultsFinal.width-1250.png)\n*选择性准确率（即在回答问题中的准确率）与覆盖率（即回答问题的比例）。*\n\n### 结论\n\n这项工作通过引入和利用“充足上下文”的概念，为分析和增强RAG系统提供了新的见解和有价值的工具。研究表明，RAG系统中的幻觉可能源于上下文不充足，并且选择性生成可以有效缓解这一问题。未来的工作将分析不同检索方法如何影响上下文充足性，并探索如何利用检索质量信号来改进模型后训练。",
      "shortSummary": "该研究深入探讨了检索增强生成（RAG）系统中大型语言模型（LLM）的幻觉问题，指出其根源在于上下文的“充足性”而非仅仅“相关性”。文章定义了“充足上下文”，并开发了一个基于LLM的自动评估器，能以高准确率判断上下文是否充足。研究发现，即使添加上下文，LLM仍可能因上下文不充足而产生更多幻觉。为解决此问题，提出了“选择性生成”框架，结合上下文充足信号和模型置信度，有效减少了幻觉并提高了RAG系统的准确性。",
      "translated_title": "深入理解检索增强生成：充足上下文的作用",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext2_RAG.width-1250.png",
          "alt": "SufficientContext2_RAG",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext3_AutoRater.width-1250.png",
          "alt": "SufficientContext3_AutoRater",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext4_Accuracy.width-1250.png",
          "alt": "SufficientContext4_Accuracy",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext5_Sufficiency.width-1250.png",
          "alt": "SufficientContext5_Sufficiency",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext6_Analysis.width-1250.png",
          "alt": "SufficientContext6_Analysis",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"766t5\"><a href=\"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\" target=\"_blank\" rel=\"noopener noreferrer\">Retrieval augmented generation</a> (RAG) enhances large language models (LLMs) by providing them with relevant external context. For example, when using a RAG system for a question-answer (QA) task, the LLM receives a context that may be a combination of information from multiple sources, such as public webpages, private document corpora, or knowledge graphs. Ideally, the LLM either produces the correct answer or responds with “I don’t know” if certain key information is lacking.</p><p data-block-key=\"brfo6\">A main challenge with RAG systems is that they may mislead the user with <i>hallucinated</i> (and therefore incorrect) information. Another challenge is that most prior work only considers how <i>relevant</i> the context is to the user query. But we believe that the context’s relevance alone is the wrong thing to measure — we really want to know whether it provides enough information for the LLM to answer the question or not.</p><p data-block-key=\"dou1v\">In “<a href=\"https://arxiv.org/abs/2411.06037\" target=\"_blank\" rel=\"noopener noreferrer\">Sufficient Context: A New Lens on Retrieval Augmented Generation Systems</a>”, which appeared at <a href=\"https://iclr.cc/\" target=\"_blank\" rel=\"noopener noreferrer\">ICLR 2025</a>, we study the idea of \"sufficient context” in RAG systems. We show that it’s possible to know when an LLM has enough information to provide a correct answer to a question. We study the role that context (or lack thereof) plays in factual accuracy, and develop a way to quantify context sufficiency for LLMs. Our approach allows us to investigate the factors that influence the performance of RAG systems and to analyze when and why they succeed or fail.</p><p data-block-key=\"9jr23\">Moreover, we have used these ideas to launch the <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/retrieval-and-ranking#llm_reranker\" target=\"_blank\" rel=\"noopener noreferrer\">LLM Re-Ranker</a> in the <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/rag-overview\" target=\"_blank\" rel=\"noopener noreferrer\">Vertex AI RAG Engine</a>. Our feature allows users to re-rank retrieved snippets based on their relevance to the query, leading to better retrieval metrics (e.g., <a href=\"https://en.wikipedia.org/wiki/Discounted_cumulative_gain\" target=\"_blank\" rel=\"noopener noreferrer\">nDCG</a>) and better RAG system accuracy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext2_RAG.width-1250.png\" alt=\"SufficientContext2_RAG\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext2_RAG.width-1250.png\" alt=\"SufficientContext2_RAG\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"bzxw5\"><i>In a RAG system, an LLM uses retrieved context to provide a response to the input question.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"766t5\">Main conceptual contribution: Sufficient context</h2><p data-block-key=\"feoif\">We define context as “sufficient” if it contains all the necessary information to provide a definitive answer to the query and “insufficient” if it lacks the necessary information, is incomplete, inconclusive, or contains contradictory information. For example:</p><p data-block-key=\"6vl6g\"><i>Input query:</i> The error code for “Page Not Found” is named after room 404, which had stored the central database of error messages in what famous laboratory?</p><ul><li data-block-key=\"shs1\"><i>Sufficient context:</i> The “Page Not Found” error, often displayed as a 404 code, is named after Room 404 at CERN, the European Organization for Nuclear Research. This was the room where the central database of error messages was stored, including the one for a page not being found.</li><li data-block-key=\"6i6tu\"><i>Insufficient context:</i> A 404 error, or “Page Not Found” error, indicates that the web server cannot find the requested page. This can happen due to various reasons, including typos in the URL, a page being moved or deleted, or temporary issues with the website.</li></ul><p data-block-key=\"6n82k\">The second context is very relevant to the user’s query, but it does not answer the question, and hence it is insufficient.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"766t5\">Developing a sufficient context autorater</h2><p data-block-key=\"6d4og\">With this definition, we first develop an LLM-based automatic rater (“autorater”) that evaluates query-context pairs. To evaluate the autorater, we first had human experts analyze 115 question-and-context examples to determine if the context was sufficient to answer the question. This became the “gold standard” to which we compared the LLM’s judgements. Then, we had the LLM evaluate the same questions and contexts, where it outputs either “true” for sufficient context or “false” for insufficient context.</p><p data-block-key=\"9qrhu\">To optimize the model’s ability to solve this task, we also improved the prompt with various <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-design-strategies\" target=\"_blank\" rel=\"noopener noreferrer\">prompting strategies</a>, such as chain-of-thought prompting and providing a 1-shot example. We then measured classification performance based on how often the LLM’s true/false labels matched the gold standard labels.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext3_AutoRater.width-1250.png\" alt=\"SufficientContext3_AutoRater\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext3_AutoRater.width-1250.png\" alt=\"SufficientContext3_AutoRater\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"bzxw5\"><i>Our automatic rating method (autorater) for sufficient context. We use a prompted LLM to rate examples consisting of an input query and the retrieved context. The model outputs a binary true/false label that represents sufficient vs. insufficient context.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"766t5\">Using our optimized prompt, we show that we can classify sufficient context with very high accuracy (at least 93% of the time). It also turns out that the best performing method we tried is a prompted Gemini 1.5 Pro, without any fine-tuning. As baselines, we show that <a href=\"https://arxiv.org/abs/2407.10817\" target=\"_blank\" rel=\"noopener noreferrer\">FLAMe</a> (fine-tuned PaLM 24B) is slightly worse than Gemini but could be a more computationally efficient alternative. We also compare our approach to methods that rely on ground truth answers, such as <a href=\"https://github.com/google-research/true\" target=\"_blank\" rel=\"noopener noreferrer\">TRUE-NLI</a> (a fine-tuned entailment model), and a method that checks if the ground truth answer appears in the context (\"Contains GT\"). Gemini outperforms the alternatives, likely because it has better language understanding abilities.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext4_Accuracy.width-1250.png\" alt=\"SufficientContext4_Accuracy\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext4_Accuracy.width-1250.png\" alt=\"SufficientContext4_Accuracy\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"bzxw5\"><i>Accuracy of classifying sufficient context, where we measure the agreement between various automatic methods to the human-annotated labels.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"766t5\">This autorater enables us to scalably label instances and analyze model responses based on sufficient vs. insufficient context.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"766t5\">Key insights into RAG systems</h2><p data-block-key=\"cus6j\">Using our sufficient context autorater, we analyzed the performance of various LLMs and datasets, leading to several key findings:</p><ul><li data-block-key=\"8fv0s\"><i>State-of-the-art large models:</i> Models like <a href=\"https://ai.google.dev/gemini-api/docs/models\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a>, <a href=\"https://platform.openai.com/docs/models\" target=\"_blank\" rel=\"noopener noreferrer\">GPT</a>, and <a href=\"https://claude.ai/login?returnTo=%2F%3F\" target=\"_blank\" rel=\"noopener noreferrer\">Claude</a> generally excel at answering queries when provided with sufficient context but lack the ability to recognize and avoid generating incorrect answers when the provided context is insufficient.</li><li data-block-key=\"2bmoi\"><i>Smaller open-source models:</i> Our analysis uncovers the specific issue that open-source models have a high rate of hallucination and abstention even when the context is sufficient to answer the question correctly.</li><li data-block-key=\"4batf\"><i>Insufficient context</i>: Sometimes models generate correct answers when we rate the context as insufficient. This highlights the fact that insufficient context can still be useful. For example, it can bridge gaps in the model's knowledge or clarify ambiguities in the query.</li></ul><p data-block-key=\"5johu\"><i>Actionable insights:</i> Based on our findings, we now have recommendations for how to improve RAG systems. For example, it could be beneficial to (i) add a sufficiency check before generation, (ii) retrieve more context or re-rank the retrieved contexts, or (iii) tune an abstention threshold with confidence and context signals.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"766t5\">Diving into the research behind sufficient context</h2><p data-block-key=\"d09lm\">Our analysis reveals that several standard benchmark datasets contain many instances with insufficient context. We consider three datasets: <a href=\"https://github.com/freshllms/freshqa\" target=\"_blank\" rel=\"noopener noreferrer\">FreshQA</a>, <a href=\"https://hotpotqa.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">HotPotQA</a>, and <a href=\"https://paperswithcode.com/dataset/musique-ans\" target=\"_blank\" rel=\"noopener noreferrer\">MuSiQue</a>. Datasets with a higher percentage of sufficient context instances, such as FreshQA, tend to be those where the context is derived from human-curated supporting documents.</p><p data-block-key=\"79cha\"><a href=\"https://docs.google.com/spreadsheets/d/1pR1ETI3cX9_295HhfPfRUOHNlEYE5aJhbNFqN7DJ5Xk/edit?gid=334049794#gid=334049794\" target=\"_blank\" rel=\"noopener noreferrer\">Example 182 from FreshQA</a> is the question <i>“How many food allergens with mandatory labeling are there in the United States?”</i> This is a tricky question because the answer is nine, after <a href=\"https://www.fda.gov/food/food-allergies/faster-act-sesame-ninth-major-food-allergen\" target=\"_blank\" rel=\"noopener noreferrer\">sesame was added in 2023</a>. The supporting document in the dataset is the Wikipedia article on <a href=\"https://en.wikipedia.org/wiki/Food_allergy#Regulation_of_labelling\" target=\"_blank\" rel=\"noopener noreferrer\">Food Allergies</a>, where the table under “Regulation of labeling” provides the list of the nine allergens with mandatory labeling.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext5_Sufficiency.width-1250.png\" alt=\"SufficientContext5_Sufficiency\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext5_Sufficiency.width-1250.png\" alt=\"SufficientContext5_Sufficiency\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"bzxw5\"><i>Comparing the percentage of examples with sufficient context (y-axis) across three datasets (x-axis).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"766t5\">Adding context leads to more hallucinations</h2><p data-block-key=\"4h26e\">A surprising observation is that while RAG generally improves overall performance, it paradoxically reduces the model's ability to abstain from answering when appropriate. The introduction of additional context seems to increase the model's confidence, leading to a higher propensity for hallucination rather than abstention.</p><p data-block-key=\"79br5\">To understand this, we use Gemini to rate each model response by comparing it to the set of possible ground truth answers. We classify each response as “Correct”, “Hallucinate” (i.e., incorrect answer), or ”Abstain” (e.g., saying “I don’t know”). With this approach we find, for example, that Gemma goes from providing incorrect answers on 10.2% of questions with no context up to 66.1% when using insufficient context.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext6_Analysis.width-1250.png\" alt=\"SufficientContext6_Analysis\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext6_Analysis.width-1250.png\" alt=\"SufficientContext6_Analysis\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"bzxw5\"><i>Detailed analysis of three LLMs in four different RAG settings.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"766t5\">Selective generation to reduce hallucinations</h2><p data-block-key=\"fm6ak\">As another contribution, we develop a “selective generation” framework that leverages sufficient context information to guide abstention. We consider the following metrics: <i>selective accuracy</i> measures the model’s fraction of correct answers among the ones it answers, and <i>coverage</i> is the fraction of answered questions (non-abstentions).</p><p data-block-key=\"bbb7\">Our selective generation approach combines the sufficient context signal with the model's self-rated confidence scores to make informed decisions about when to abstain. This is more nuanced than simply abstaining whenever the context is insufficient, as models can sometimes provide correct answers even with limited context. We use these signals to train a logistic regression model to predict hallucinations. We then set a coverage-accuracy trade-off threshold, determining when the model should abstain from answering.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        <div class=\"glue-ambient-video \">\n            <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/SufficientContext7_PipelineFinal.mp4\" type=\"video/mp4\">\n            </video>\n            <div class=\"glue-ambient-video__button glue-ambient-video__button--paused\" aria-label=\"Video Play/pause\">\n                <div class=\"glue-ambient-video__tooltip\">\n                  <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                  <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                </div>\n                <div class=\"glue-ambient-video__icon\">\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                </div>\n              </div>\n        </div>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"bzxw5\"><i>The pipeline for our selective generation method to reduce hallucinations.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"766t5\">We use two main signals for abstention:</p><ul><li data-block-key=\"8faa0\"><i>Self-rated confidence:</i> We use two strategies: <i>P</i>(True) and <i>P</i>(Correct). <i>P</i>(True) involves sampling answers multiple times and prompting the model to label each sample as correct or incorrect. <i>P</i>(Correct) is used for models where extensive querying is expensive; it involves obtaining the model's response and its estimated probability of correctness.</li><li data-block-key=\"av834\"><i>Sufficient context signal:</i> We use the binary label from an autorater model (FLAMe) to indicate whether the context is sufficient. Crucially, we do not need the ground truth answer to determine the sufficient context label, and so we can use this signal when answering the question.</li></ul><p data-block-key=\"aaf07\">Our results demonstrate that this approach leads to a better selective accuracy-coverage trade-off compared to using model confidence alone. By using the sufficient context label, we can increase the accuracy on the questions that the model answers, sometimes by up to 10% of the time.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext8_ResultsFinal.width-1250.png\" alt=\"SufficientContext8_ResultsFinal\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SufficientContext8_ResultsFinal.width-1250.png\" alt=\"SufficientContext8_ResultsFinal\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"bzxw5\"><i>Selective accuracy (i.e., accuracy on the fraction of answered questions) vs. coverage (i.e., fraction of answered questions).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"766t5\">Conclusion</h2><p data-block-key=\"7f3r9\">Our work provides new insights into the behavior of LLMs within RAG systems. By introducing and utilizing the concept of sufficient context, we have developed a valuable tool for analyzing and enhancing these systems. We showed that hallucinations in RAG systems may be due to insufficient context, and we demonstrated that selective generation can mitigate this issue. Future work will analyze how different retrieval methods influence context sufficiency and explore how signals about retrieval quality can be used to improve model post-training.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"766t5\">Acknowledgments</h2><p data-block-key=\"1gba9\"><i>This work is in collaboration with Hailey Joren (lead student author), Jianyi Zhang, Chun-Sung Ferng, and Ankur Taly. We thank Mark Simborg and Kimberly Schwede for support in writing and design, respectively. We also thank Alyshia Olsen for help in designing the graphics.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "信任图上的差分隐私 (原标题: Differential privacy on trust graphs)",
      "link": "https://research.google/blog/differential-privacy-on-trust-graphs/",
      "pubDate": "Mon, 12 May 2025 16:00:00 GMT",
      "isoDate": "2025-05-12T16:00:00.000Z",
      "creator": "Google",
      "summary": "### 信任图上的差分隐私 (Trust Graph Differential Privacy, TGDP)\n\n本文介绍了信任图差分隐私（TGDP），这是一种新的分布式差分隐私（DP）模型，旨在更精细地控制参与者的信任假设，以适应现实世界中用户对数据共享的不同信任级别。\n\n#### 差分隐私（DP）概述\n\n*   **定义**：DP是一种数学上严谨且广泛研究的隐私框架，确保随机算法的输出在单个用户数据发生变化时仍具有统计上的不可区分性。\n*   **主要模型**：\n    *   **中心模型**：一个受信任的策展人访问原始数据并生成差分隐私输出。\n    *   **局部模型**：用户设备发送的所有消息本身就是差分隐私的，无需受信任的策展人。\n*   **局限性**：局部模型虽然信任要求最低，但通常会导致比中心模型显著更高的效用损失。现有DP模型通常采用二元信任假设，无法反映现实中用户基于关系的不同信任程度。\n\n#### 信任图差分隐私（TGDP）\n\n*   **动机**：现实世界中，用户对不同关系的人有不同的信任级别（例如，与家人分享位置数据，但不与陌生人分享）。这种不对称性促使需要超越二元信任假设的隐私框架。\n*   **模型构建**：\n    *   使用**信任图**来建模关系，其中顶点代表用户，连接的顶点表示相互信任。\n    *   **隐私保证**：TGDP确保用户（或其受信任邻居）与任何不受信任的用户之间交换的消息分布在统计上不可区分，即使该用户的输入数据发生变化。\n    *   **示例**：\n        ![DPTrustGraph1_ExampleHero](https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph1_ExampleHero.width-1250.png)\n        *信任图与位置共享示例。Alice与Bob共享位置，Bob与Carol和Dave共享，Carol和Dave与Eve共享。根据TGDP定义，Carol、Dave和Eve即使汇集所有数据，也无法识别Alice的信息。*\n    *   **插值作用**：TGDP自然地介于中心模型和局部模型之间：\n        *   **中心模型**：等同于星形图上的TGDP，中心是受信任的策展人，连接所有其他用户。\n        *   **局部模型**：等同于完全不连通图上的TGDP，其中没有用户信任其他用户。\n        ![DPTrustGraph2_Comparison](https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph2_Comparison.width-1250.png)\n        *左图：对应于中心DP模型的“星形”信任图，所有用户信任一个中心策展人。右图：对应于局部DP模型的完全不连通信任图，没有用户信任其他用户。*\n*   **精度量化**：\n    *   已知中心DP算法比局部DP算法更准确。TGDP允许在更一般的信任关系下量化算法精度。\n    *   通过一个简单的**聚合任务**来量化精度：估计所有用户私有数据值（实数）的总和，目标是最小化误差（均方误差）。\n\n#### TGDP算法与误差界限\n\n本文为满足TGDP的聚合算法提供了误差的上限和下限。\n\n1.  **基于支配集的算法（上限）**：\n    *   **支配集（Dominating Set）**：图顶点的一个子集T，使得T中每个不在T中的顶点都与T中至少一个顶点相邻。\n    *   **示例**：\n        ![DPTrustGraph3_DomSet](https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph3_DomSet.width-1250.png)\n        *信任图中支配集的示意图。Alice和Eve（蓝色高亮）是支配集T的一部分。*\n    *   **算法步骤**：\n        1.  找到信任图的支配集T。\n        2.  每个用户将其数据发送给T中的一个受信任邻居。\n        3.  T中的每个用户广播其接收到的所有数据之和加上适当的拉普拉斯噪声。\n        4.  最终估计是所有带噪声广播的总和。\n    *   **误差**：该算法的误差不大于支配集大小的函数。最小化误差需要找到最小支配集（其大小称为支配数）。\n    *   **改进**：论文中还提出了一种基于线性规划的更好算法和更紧的上限。\n\n2.  **通过填充数（Packing Number）的下限**：\n    *   **填充数**：图中一组顶点（最大尺寸）的集合，它们之间不共享任何邻居。\n    *   **误差下限**：论文证明，任何算法的误差都不可能小于填充数的某个常数倍。\n    *   **上下限之间的差距**：通常，支配数总是大于或等于填充数，但两者之间可能存在差距。\n        ![DPTrustGraph4_Graph](https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph4_Graph.width-1250.png)\n        *支配数为4，填充数为1的图。*\n        这种差距意味着本文所示的误差上限和下限之间存在差距，弥合这一差距是TGDP算法理论中的一个开放问题。\n\n#### 信任图DP在机器学习中的应用\n\n*   上述支配集协议可以很容易地适应聚合向量而非实数。\n*   向量聚合是联邦分析和联邦学习中的关键任务，可用于计算跨设备聚合的梯度。\n*   通过将TGDP应用于向量聚合，可以确保最终的机器学习模型满足TGDP。\n\n#### 结论\n\nTGDP模型为分布式差分隐私提供了一种新范式，允许更细粒度地控制信任假设，并自然地插值于中心DP和局部DP之间。它有助于更普遍地理解信任关系与DP算法精度之间的联系。该定义在平台向多样化信任模型发展（如医疗数据共享、社交平台个人数据共享）的背景下具有实际意义。",
      "shortSummary": "本文提出信任图差分隐私（TGDP），一种新的分布式DP模型，旨在适应现实世界中用户间细粒度的信任关系。TGDP介于传统中心化和局部DP模型之间，通过信任图建模隐私。研究通过聚合任务量化精度，提出了基于支配集的算法和基于填充数的误差下限，并讨论了其在联邦学习中的应用。该模型有助于理解信任与DP算法精度间的关系，并弥补现有模型的不足。",
      "translated_title": "信任图上的差分隐私",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph1_ExampleHero.width-1250.png",
          "alt": "DPTrustGraph1_ExampleHero",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph2_Comparison.width-1250.png",
          "alt": "DPTrustGraph2_Comparison",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph3_DomSet.width-1250.png",
          "alt": "DPTrustGraph3_DomSet",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph4_Graph.width-1250.png",
          "alt": "DPTrustGraph4_Graph",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xalxs\"><a href=\"https://en.wikipedia.org/wiki/Differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">Differential privacy</a> (DP) is a mathematically rigorous and widely studied privacy framework that ensures the output of a randomized algorithm remains statistically indistinguishable even if the data of a single user changes. This framework has been extensively studied in both theory and practice, with many applications in analytics and machine learning (e.g., <a href=\"https://research.google/blog/deep-learning-with-label-differential-privacy/\">1</a>, <a href=\"https://research.google/blog/differential-privacy-accounting-by-connecting-the-dots/\">2</a>, <a href=\"https://research.google/blog/advances-in-private-training-for-production-on-device-language-models/\">3</a>, <a href=\"https://research.google/blog/protecting-users-with-differentially-private-synthetic-training-data/\">4</a>, <a href=\"https://research.google/blog/improving-gboard-language-models-via-private-federated-analytics/\">5</a>, <a href=\"https://research.google/blog/bridging-the-gap-in-differentially-private-model-training/\">6</a>, <a href=\"https://research.google/blog/generating-synthetic-data-with-differentially-private-llm-inference/\">7</a>).</p><p data-block-key=\"616kf\">The two main models of DP are the <i>central model</i> and the <i>local model</i>. In the <a href=\"https://en.wikipedia.org/wiki/Differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">central model</a><i>,</i> a trusted curator has access to raw data and is responsible for producing an output that is differentially private. The <a href=\"https://en.wikipedia.org/wiki/Local_differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">local model</a> requires that all messages sent from a user’s device are themselves differentially private, removing the need for a trusted curator. While the local model is appealing due to its minimal trust requirements, it often comes with significantly higher utility degradation compared to the central model.</p><p data-block-key=\"eme60\">In real-world data-sharing scenarios, users often place varying levels of trust in others, depending on their relationships. For instance, someone might feel comfortable sharing their location data with family or close friends but would hesitate to allow strangers to access the same information. This asymmetry aligns with philosophical views of privacy as control over personal information, where individuals specify with whom they are willing to share their data. Such nuanced privacy preferences highlight the need for frameworks that go beyond the binary trust assumptions of existing differentially private models, accommodating more realistic trust dynamics in privacy-preserving systems.</p><p data-block-key=\"82np2\">In “<a href=\"https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.ITCS.2025.53\" target=\"_blank\" rel=\"noopener noreferrer\">Differential Privacy on Trust Graphs</a>”, published at the <i>Innovations in Theoretical Computer Science Conference (ITCS 2025)</i>, we use a trust graph to model relationships, where the vertices represent users, and connected vertices trust each other (see below). We explore how to apply DP to these trust graphs, ensuring that the privacy guarantee applies to messages shared between a user (or their trusted neighbors) and everyone else they do not trust. In particular, the distribution of messages exchanged by each user <i>u</i> or one of their neighbors with any other user not trusted by <i>u</i> should be statistically indistinguishable if the input held by <i>u</i> changes, which we call trust graph DP (TGDP).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph1_ExampleHero.width-1250.png\" alt=\"DPTrustGraph1_ExampleHero\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph1_ExampleHero.width-1250.png\" alt=\"DPTrustGraph1_ExampleHero\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"fckq9\"><i>Example of a trust graph with location sharing. Alice shares her location with Bob, who shares his location with Carol and Dave. Carol and Dave share their location with Eve. Under the definition of trust graph DP that we introduce, it is required that Carol, Dave, and Eve cannot identify Alice’s information, even if they pooled together all of their data and any data shared with them.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xalxs\">Trust graph DP</h2><p data-block-key=\"bcd7l\">TGDP interpolates between the central and local models in a natural way. The central model is equivalent to TGDP over a <a href=\"https://en.wikipedia.org/wiki/Star_(graph_theory)\" target=\"_blank\" rel=\"noopener noreferrer\">star graph</a>, with the trusted central data curator at the center connected to all other users (the top figure, below). The local model, on the other hand, is equivalent to TGDP over a fully unconnected graph, in which no users trust any other users (the bottom figure, below).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph2_Comparison.width-1250.png\" alt=\"DPTrustGraph2_Comparison\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph2_Comparison.width-1250.png\" alt=\"DPTrustGraph2_Comparison\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"fckq9\"><b><i>Left</i></b><i>: Example of a “star” trust graph that corresponds with the central model of DP, in which all users trust a central curator.</i> <b><i>Right</i></b><i>: Example of a fully unconnected trust graph that corresponds with the local model of DP, in which no users trust any other users.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xalxs\">It is known that algorithms that must satisfy central DP can be more <i>accurate</i> than algorithms that must satisfy local DP, since there is more flexibility to share data under central DP. With TGDP, we can quantify algorithm accuracy under more general trust relationships in between.</p><p data-block-key=\"2ka66\">We quantify accuracy specifically through a simple <i>aggregation</i> task, which is a building block to more complicated machine learning tasks. Suppose each user has a private data value, <b><i>x</i></b><b><i><sub class=\"subscript\">i</sub></i></b>, which is a real number bounded within some range. The goal of the aggregation task is to devise an algorithm that can estimate the sum of all users’ values, <b><i>Σ</i></b><b><i><sub class=\"subscript\">i</sub></i></b><b><i> x</i></b><b><i><sub class=\"subscript\">i</sub></i></b>, with as low error as possible (measured as <a href=\"https://en.wikipedia.org/wiki/Mean_squared_error\" target=\"_blank\" rel=\"noopener noreferrer\">mean-squared error</a>).</p><p data-block-key=\"edtag\">In the next sections, we provide both upper and lower bounds on the error of algorithms for aggregation that satisfy TGDP. These bounds quantify the limits of TGDP algorithms: one can do at least as well as the upper bound, and no TGDP algorithm can do better than the lower bound.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xalxs\">An algorithm based on dominating set</h2><p data-block-key=\"a2ko5\">We provide an algorithm for aggregation that satisfies TGDP. Both the algorithm and the upper bound rely on a <a href=\"https://en.wikipedia.org/wiki/Dominating_set\" target=\"_blank\" rel=\"noopener noreferrer\"><i>dominating set</i></a> of the trust graph.</p><p data-block-key=\"abo2n\">A dominating set <i>T</i> is a subset of graph vertices such that every vertex not in <i>T</i> is adjacent to at least one vertex in <i>T</i>. For example, the figure below shows a dominating set in blue.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph3_DomSet.width-1250.png\" alt=\"DPTrustGraph3_DomSet\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph3_DomSet.width-1250.png\" alt=\"DPTrustGraph3_DomSet\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"fckq9\"><i>Illustration of a dominating set in a trust graph. Alice and Eve, highlighted in blue, are part of the dominating set</i> T<i>.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xalxs\">By definition of a dominating set, every user trusts at least one user in the set. This allows us to build an algorithm by letting each user send their raw data to their trusted neighbor in the set, after which the trusted neighbor can aggregate all data it receives and add an appropriate noise to achieve differential privacy. More specifically, our <i>dominating set algorithm</i> works as follows<b>:</b></p><ol><li data-block-key=\"1h2ka\">Find a dominating set <i>T</i> of the trust graph.</li><li data-block-key=\"63pj4\">Each user sends their data to a neighbor in the dominating set <i>T</i>.</li><li data-block-key=\"8jrrj\">Each user in the dominating set <i>T</i> broadcasts the sum of all numbers it receives plus <a href=\"https://en.wikipedia.org/wiki/Additive_noise_differential_privacy_mechanisms#The_Laplace_Mechanism\" target=\"_blank\" rel=\"noopener noreferrer\">Laplace noise</a>.</li><li data-block-key=\"5e01l\">The estimate is the sum of all noisy broadcasts.</li></ol><p data-block-key=\"5qc97\">The error of this algorithm is no greater than a function of the size of the dominating set <i>T</i>. This error is minimized for the smallest possible dominating set, or the minimum dominating set. The size of the minimum dominating set in a graph <i>G</i> is called its <i>domination number</i>.</p><p data-block-key=\"181lm\">Thus, if we are able to find the minimum dominating set in step 1 above, then the dominating set algorithm can have error at most a constant times the domination number.</p><p data-block-key=\"drkfm\">A natural question is whether the dominating set algorithm is the <i>best</i> algorithm. It turns out that we can actually do better using linear programming, and we give a better algorithm and upper bound in <a href=\"https://arxiv.org/abs/2410.12045\" target=\"_blank\" rel=\"noopener noreferrer\">the paper</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xalxs\">A lower bound via packing number</h2><p data-block-key=\"f6leu\">More generally, we need a lower bound to characterize what the <i>best possible</i> error could be. It turns out that there’s a fairly intuitive lower bound that depends on the <i>packing number</i> of the graph: the maximum size of a set of vertices that share no neighbors.</p><p data-block-key=\"52vjb\">For example, for the graph in the first figure, the packing number is 2, since Alice and Eve do not share any neighbors. For the star graph, the packing number is 1.</p><p data-block-key=\"bmeu0\">While the dominating set algorithm in the previous section had error upper bounded by the domination number, we prove <a href=\"https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.ITCS.2025.53\" target=\"_blank\" rel=\"noopener noreferrer\">in the paper</a> that <i>no</i> algorithm can possibly have error smaller than some constant times the packing number.</p><p data-block-key=\"fb8ks\">In general, the domination number is always greater than or equal to the packing number (see e.g., <a href=\"https://arxiv.org/abs/2503.05562v1\" target=\"_blank\" rel=\"noopener noreferrer\">this paper</a>), but there can be a gap. For example, below is a graph with a domination number of 4 and a packing number of 1.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph4_Graph.width-1250.png\" alt=\"DPTrustGraph4_Graph\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DPTrustGraph4_Graph.width-1250.png\" alt=\"DPTrustGraph4_Graph\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"fckq9\"><i>Graph with a gap between the domination number (4) and the packing number (1).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xalxs\">The gap between the domination number and the packing number implies that there is a gap between the upper bound and lower bound on error that we’ve shown here (in the full paper we give an even better upper bound using a linear programming based algorithm, but a gap is still there). Closing this gap between the upper and lower bounds is an open problem in the theory of TGDP algorithms.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xalxs\">ML with trust graph DP</h2><p data-block-key=\"9hudo\">The above dominating set protocol can be easily adapted to aggregate <i>vectors</i> instead of real-numbers. Vector aggregation is a <a href=\"https://research.google/blog/federated-analytics-collaborative-data-science-without-data-collection/\">key task in federated analytics and federated learning</a>; for ML applications, vector aggregation can be used to compute the aggregated gradients across different devices. When performing this step with differential privacy, we also get that the final model satisfies differential privacy. This has been the main paradigm for achieving differential privacy in federated learning. Our approach naturally fits into this framework: by using vector aggregation with TGDP, we immediately get that the final ML model satisfies TGDP.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xalxs\">Conclusion</h2><p data-block-key=\"78vrt\">In this work, we introduce trust graph DP (TGDP), a new model for distributed DP which allows a finer-grained control of the trust assumption of each participant. Our TGDP model interpolates between central DP and local DP, allowing us to reason more generally about how trust relationships relate to algorithm accuracy in DP. We provided an algorithm and a lower bound for the aggregation problem. As mentioned above, an interesting open theoretical question is to close the gap between our upper and lower bounds. Our definition may be of practical interest as platforms move towards varied models of trust, from data sharing initiatives in healthcare, to individual data sharing choices on social platforms.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xalxs\">Acknowledgments</h2><p data-block-key=\"1s979\"><i>This blog post is based on joint work with Badih Ghazi and Ravi Kumar. We also thank them for providing us with helpful feedback during the preparation of this blog post.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "利用生成式AI将3D可购物产品带到线上 (原标题: Bringing 3D shoppable products online with generative AI)",
      "link": "https://research.google/blog/bringing-3d-shoppable-products-online-with-generative-ai/",
      "pubDate": "Sun, 11 May 2025 16:00:00 GMT",
      "isoDate": "2025-05-11T16:00:00.000Z",
      "creator": "Google",
      "summary": "### 引言：在线购物的挑战与生成式AI的解决方案\n\n每天有数十亿人进行在线购物，但屏幕上的体验难以复制实体店中“亲手挑选和检查”产品的直观感受。为商家大规模创建高质量的3D产品可视化工具，通常成本高昂且耗时。为解决这一问题，Google开发了新的生成式AI技术，仅需最少三张产品图片即可生成高质量、可购物的3D产品可视化。这项最新进展基于Google先进的视频生成模型Veo，目前已在Google购物上实现了多种产品类别的交互式3D视图。\n\n### 第一代：神经辐射场（NeRFs）\n\n*   **时间：** 2022年\n*   **目标：** Google研究人员开始利用神经辐射场（NeRF）技术，从五张或更多产品图片中学习产品的3D表示，以渲染新颖视图，例如360°旋转。\n*   **挑战：** 该方法面临输入信号噪声（如不准确的相机姿态）和稀疏输入视图带来的模糊性问题。在重建凉鞋和高跟鞋等具有薄结构和复杂几何形状的产品时，这些挑战尤为突出。\n*   **技术细节：** 该方法结合了多种3D技术，包括NOCS用于XYZ预测、CamP用于相机优化，以及Zip-NeRF用于从稀疏视图集进行最先进的新颖视图合成。\n\n![第一代NeRF方法](https://storage.googleapis.com/gweb-research2023-media/images/GenAI3d2_FirstGen.width-1250.png)\n\n### 第二代：基于视图条件扩散先验的规模化\n\n*   **时间：** 2023年\n*   **目标：** 引入了第二代方法，使用视图条件扩散先验来解决第一代方法的局限性。这意味着模型可以根据一张鞋的顶部图片，预测其正面或任何其他视角的样貌，即使只有有限的视角照片。\n*   **原理：** 实践中，该方法采用DreamFusion中首次提出的分数蒸馏采样（SDS）的变体。通过渲染3D模型并与视图条件扩散模型生成的图像进行比较，计算出一个分数来指导3D模型的优化，从而提高其质量和真实感。\n*   **成果：** 这一第二代方法带来了显著的规模化优势，使得Google能够为Google购物上日常浏览的大量鞋类产品（包括凉鞋、高跟鞋、靴子等）生成3D表示。\n\n![第二代视图条件扩散模型](https://storage.googleapis.com/gweb-research2023-media/images/3dGenAI3_SecondGen.width-1250.png)\n\n### 第三代：利用Veo实现泛化\n\n*   **基础：** 最新的突破建立在Google最先进的视频生成模型Veo之上。Veo的关键优势在于其能够生成捕捉光线、材质、纹理和几何形状之间复杂相互作用的视频。\n*   **训练：** 为了微调Veo以将产品图片转换为一致的360°视频，Google首先整理了数百万高质量的3D合成资产数据集。然后，从各种相机角度和光照条件下渲染这些3D资产，并创建了图像-视频配对数据集，监督Veo根据一张或多张图片生成360°旋转视频。\n*   **成果：** 这种方法有效地泛化到家具、服装、电子产品等多样化产品类别。Veo不仅能够生成符合现有产品图片的新颖视图，还能捕捉复杂的光照和材质交互（如闪亮表面），这对于前两代方法来说是具有挑战性的。此外，第三代方法避免了从稀疏的以物体为中心的图片中估计精确相机姿态的需求，简化了问题并提高了可靠性。\n*   **输入要求：** 尽管Veo在仅有一张图片时也能生成逼真的3D物体表示，但它需要对未见视图的细节进行“幻觉”。实践证明，仅需三张捕捉物体大部分表面的图片，就足以显著提高3D图像的质量并减少幻觉。\n\n### 结论与展望\n\n在过去几年中，3D生成式AI取得了巨大进步，从NeRF到视图条件扩散模型，再到现在的Veo。每项技术都在使在线购物更具触感和互动性方面发挥了关键作用。展望未来，Google将继续推动这一领域的发展，使在线购物对用户而言越来越愉快、信息丰富且引人入胜。",
      "shortSummary": "Google正利用生成式AI革新在线购物体验。通过从第一代NeRFs到第二代视图条件扩散模型，再到最新基于Veo的第三代技术，Google现在仅需最少三张产品图片，即可生成高质量、可购物的3D产品可视化。这项技术已广泛应用于Google购物，覆盖鞋类、家具等多种商品，旨在弥补线上购物缺乏“亲身体验”的不足，让用户能更直观地检查产品，提升购物的沉浸感和互动性。",
      "translated_title": "利用生成式AI将3D可购物产品带到线上",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GenAI3d2_FirstGen.width-1250.png",
          "alt": "GenAI3d2_FirstGen",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/3dGenAI3_SecondGen.width-1250.png",
          "alt": "3dGenAI3_SecondGen",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ssx28\">Every day, billions of people shop online, hoping to replicate the best parts of in-store shopping. Seeing something that catches your eye, picking it up and inspecting it for yourself can be a key part of how we connect with products. But capturing the intuitive, hands-on nature of the store experience is nuanced and can be challenging to replicate on a screen. We know that technology can help bridge the gap, bringing key details to your fingertips with a quick scroll. But these online tools can be costly and time consuming for businesses to create at scale.</p><p data-block-key=\"crqem\">To address this, we developed new generative AI techniques to create high quality and shoppable 3D product visualizations from as few as three product images. Today, we're excited to share the latest advancement, powered by Google’s state-of-the-art video generation model, <a href=\"https://deepmind.google/technologies/veo/veo-2/\" target=\"_blank\" rel=\"noopener noreferrer\">Veo</a>. This technology is already enabling the generation of interactive 3D views for a wide range of product categories on Google Shopping</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme- --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n    <div data-gt-id=\"media\" data-gt-component-name=\"\">\n        \n\n        \n\n\n\n\n        \n            \n            <div>\n                <div class=\"glue-ambient-video \">\n                    <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\" preload=\"auto\">\n                        <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/product_collage_borderless.mp4\" type=\"video/mp4\">\n                    </video>\n                    <div class=\"glue-ambient-video__button glue-ambient-video__button--paused\" aria-label=\"Video Play/pause\">\n                        <div class=\"glue-ambient-video__tooltip\">\n                            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                        </div>\n                        <div class=\"glue-ambient-video__icon\">\n                            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                        </div>\n                    </div>\n                </div>\n                \n                    <div class=\"caption --center\"><p data-block-key=\"hjiit\"><i>Examples of 3D product visualizations generated from photos.</i></p></div>\n                \n            </div>\n            \n        \n    </div>\n\n\n\n    </div>\n</section>\n\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"ssx28\">First generation: Neural Radiance Fields (NeRFs)</h2><p data-block-key=\"b8169\">In 2022, researchers from across Google came together to develop technologies to make product visualization more immersive. The initial efforts focused on using <a href=\"https://arxiv.org/abs/2003.08934\" target=\"_blank\" rel=\"noopener noreferrer\">Neural Radiance Fields (NeRF)</a> to learn a 3D representation of products to render novel views (i.e., novel view synthesis), like 360° spins, from five or more images of the product. This required solving many sub-problems, including selecting the most informative images, removing unwanted backgrounds, predicting 3D priors, estimating camera positions from a sparse set of object-centric images, and optimizing a 3D representation of the product.</p><p data-block-key=\"51aqd\">That same year, we <a href=\"https://blog.google/products/shopping/search-on-2022-shopping/\" target=\"_blank\" rel=\"noopener noreferrer\">announced</a> this breakthrough and launched the first milestone, interactive 360° visualizations of shoes on Google Search. While this technology was promising, it suffered from noisy input signals (e.g., inaccurate camera poses) and ambiguity from sparse input views. This challenge became apparent when attempting to reconstruct sandals and heels, whose thin structures and more complex geometry was tricky to reconstruct from just a handful of images.</p><p data-block-key=\"evm86\">This led us to wonder: could the recent advancements in generative diffusion models help us improve the learned 3D representation?</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GenAI3d2_FirstGen.width-1250.png\" alt=\"GenAI3d2_FirstGen\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GenAI3d2_FirstGen.width-1250.png\" alt=\"GenAI3d2_FirstGen\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ubcz9\"><i>Our first-generation approach used neural radiance fields (</i><a href=\"https://arxiv.org/abs/2003.08934\" target=\"_blank\" rel=\"noopener noreferrer\"><i>NeRF</i></a><i>) to render novel views, combining several 3D techniques like</i> <a href=\"https://geometry.stanford.edu/projects/NOCS_CVPR2019/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>NOCS</i></a><i> for XYZ prediction,</i> <a href=\"https://camp-nerf.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>CamP</i></a> <i>for camera optimization, and</i> <a href=\"https://jonbarron.info/zipnerf/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Zip-NeRF</i></a><i> for state-of-the-art novel view synthesis from a sparse set of views.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"ssx28\">Second generation: Scaling with a view-conditioned diffusion prior</h2><p data-block-key=\"c3e50\">In 2023, we introduced a second-generation approach which used a view-conditioned diffusion prior to address the limitations of the first approach. Being <i>view-conditioned</i> means that you can give it an image of the top of a shoe and ask the model <i>“what does the front of this shoe look like?”</i> In this way, we can use the<i> view-conditioned diffusion model</i> to help predict what the shoe looks like from any viewpoint, even if we only have photos of limited viewpoints.</p><p data-block-key=\"23khb\">In practice, we employ a variant of <i>score distillation sampling</i> (SDS), first proposed in <a href=\"https://dreamfusion3d.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">DreamFusion</a>. During training, we render the 3D model from a random camera view. We then use the view-conditioned diffusion model and the available posed images to generate a target from the same camera view. Finally, we calculate a score by comparing the rendered image and the generated target. This score directly informs the optimization process, refining the 3D model's parameters and enhancing its quality and realism.</p><p data-block-key=\"58lgn\">This second-generation approach led to significant scaling advantages enabling us to generate 3D representations for many of the shoes viewed daily on Google Shopping. Today, you can find interactive 360° visualizations for sandals, heels, boots, and other footwear categories when you shop on Google, the majority of which are created by this technology!</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/3dGenAI3_SecondGen.width-1250.png\" alt=\"3dGenAI3_SecondGen\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/3dGenAI3_SecondGen.width-1250.png\" alt=\"3dGenAI3_SecondGen\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ubcz9\"><i>The second-generation approach used a view-conditioned diffusion model based on the</i> <a href=\"https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_TryOnDiffusion_A_Tale_of_Two_UNets_CVPR_2023_paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\"><i>TryOn</i></a><i> architecture. The diffusion model acts as a learned prior using score distillation sampling proposed in</i> <a href=\"https://dreamfusion3d.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>DreamFusion</i></a><i> to improve the quality and fidelity of novel views.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"ssx28\">Third generation: Generalizing with Veo</h2><p data-block-key=\"6svfv\">Our latest breakthrough builds on <a href=\"https://deepmind.google/technologies/veo/veo-1/\" target=\"_blank\" rel=\"noopener noreferrer\">Veo</a>, Google's state-of-the-art video generation. A key strength of Veo is its ability to generate videos that capture complex interactions between light, material, texture, and geometry. Its powerful diffusion-based architecture and its ability to be finetuned on a variety of multi-modal tasks enable it to excel at novel view synthesis.</p><p data-block-key=\"aftc9\">To finetune Veo to transform product images into a consistent 360° video, we first curated a dataset of millions of high quality, 3D synthetic assets. We then rendered the 3D assets from various camera angles and lighting conditions. Finally, we created a dataset of paired images and videos and supervised Veo to generate 360° spins conditioned on one or more images.</p><p data-block-key=\"2p7s5\">We discovered that this approach generalized effectively across a diverse set of product categories, including furniture, apparel, electronics and more. Veo was not only able to generate novel views that adhered to the available product images, but it was also able to capture complex lighting and material interactions (i.e., shiny surfaces), something which was challenging for the first- and second-generation approaches.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        <div class=\"glue-ambient-video \">\n            <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/3dGenAI4_ThirdGen.mp4\" type=\"video/mp4\">\n            </video>\n            <div class=\"glue-ambient-video__button glue-ambient-video__button--paused\" aria-label=\"Video Play/pause\">\n                <div class=\"glue-ambient-video__tooltip\">\n                  <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                  <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                </div>\n                <div class=\"glue-ambient-video__icon\">\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                </div>\n              </div>\n        </div>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ubcz9\"><i>The third-generation approach builds on Veo to generate 360° spins from one or more product images.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ssx28\">Furthermore, this third-generation approach avoided the need to estimate precise camera poses from a sparse set of object-centric product images, simplifying the problem and increasing reliability. The fine-tuned Veo approach is powerful — with one image, you can generate a realistic, 3D representation of the object. But like any generative 3D technology, Veo will need to hallucinate details from unseen views, for example, the back of the object when only a view of the front is available. As the number of input images increases, so does Veo's ability to generate high fidelity and high quality novel views. In practice, we found that as few as three images capturing most object surfaces are sufficient to improve the quality of the 3D images and reduce hallucinations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"ssx28\">Conclusion and future outlook</h2><p data-block-key=\"8c9rs\">Over the last several years, there has been tremendous progress in 3D generative AI, from NeRF to view-conditioned diffusion models and now Veo. Each technology has played a key part in making online shopping feel more tangible and interactive. Looking ahead, we are excited to continue to push boundaries in this space and to help make shopping online increasingly delightful, informative, and engaging for our users.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"ssx28\">Acknowledgements</h2><p data-block-key=\"dq34d\"><i>This work was made possible by Philipp Henzler, Matthew Burruss, Matthew Levine, Laurie Zhang, Ke Yu, Chung-Yi Weng, Jason Y. Zhang, Changchang Wu, Ira Kemelmacher-Shlizerman, Carlos Hernandez, Keunhong Park, and Ricardo Martin-Brualla. We thank Aleksander Holynski, Ben Poole, Jon Barron, Pratul Srinivasan, Howard Zhou, Federico Tombari, and many more from Google Labs, Google DeepMind, and Google Shopping.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "神经连接研究的新突破 (原标题: A new light on neural connections)",
      "link": "https://research.google/blog/a-new-light-on-neural-connections/",
      "pubDate": "Tue, 06 May 2025 16:00:00 GMT",
      "isoDate": "2025-05-06T16:00:00.000Z",
      "creator": "Google",
      "summary": "# LICONN：神经连接研究的新突破\n\n## 引言：显微镜技术与神经科学的挑战\n自17世纪安东尼·范·列文虎克首次用自制光学显微镜观察到微生物以来，光学显微镜已成为生命科学研究的基石，广泛应用于细胞、器官和组织的识别以及疾病诊断。然而，在神经科学的“连接组学”（connectomics）领域，光学显微镜一直未能发挥作用。连接组学旨在全面绘制大脑区域内的所有神经元及其连接，此前主要依赖于电子显微镜（EM）技术，尽管EM能提供超高分辨率的结构信息，但其设备昂贵、操作复杂，限制了大多数神经科学实验室的普及使用。\n\n## LICONN：基于光学显微镜的连接组学新方法\n为了克服电子显微镜的局限性，Google与奥地利科学技术研究所（ISTA）的合作团队在《自然》杂志上发表了一项突破性研究，报告了首个利用光学显微镜全面绘制小鼠脑组织中所有神经元及其连接的方法——**LICONN**（light microscopy-based connectomics，基于光学显微镜的连接组学）。\n\nLICONN通过整合和定制多项成熟技术，形成了一个独特的实验流程。其核心创新在于ISTA团队开发的一种协议：在保持结构完整性的前提下，物理性地膨胀脑组织，同时对所有蛋白质进行化学标记，以提供追踪神经元和检测突触等细胞结构所需的图像对比度。\n\n## LICONN的优势与能力\n*   **可及性高：** 相较于昂贵且操作复杂的电子显微镜，LICONN基于光学显微镜，成本更低，更易于普及，使得更多实验室能够进行连接组学研究。\n*   **多模态信息获取：** LICONN能够同时测量组织样本中的结构信息和分子信息，这为理解大脑功能提供了前所未有的新机会。光学显微镜能够捕捉不同波长的光，从而可视化各种荧光标记物，揭示蛋白质、神经递质和其他分子的分布，这些分子对于理解神经元和回路的功能至关重要。LICONN以前所未有的精度和便捷性，将这些分子标记与神经通路详细的结构图谱对齐，有助于深入理解大脑如何产生认知、感知和行为。\n\n## LICONN的工作原理：组织膨胀显微技术\n为了解决光学显微镜分辨率有限的问题，ISTA团队将重点放在了样本本身。他们采用了**膨胀显微技术**，该技术利用水凝胶（与尿布吸水材料相同）吸收水分并膨胀的原理。\n\nLICONN的组织膨胀协议是现有膨胀显微技术的重大改进：\n1.  将微小的脑组织块切成50微米厚的切片。\n2.  用三种不同的水凝胶依次处理每个切片。其中两种水凝胶在组织内形成独特交织的聚合物网络，每种使组织膨胀四倍；第三种水凝胶用于稳定这些网络。\n3.  最终，组织在每个方向上膨胀约16倍，相当于将一块方糖膨胀成一盒纸巾的大小。\n\n为了使神经元可见，研究人员用绿色荧光染料孵育脑组织切片，有效“标记”细胞内的所有蛋白质。在某些样本中，他们还使用了其他染料来标记特定的蛋白质、神经递质或其他对确定神经元细胞类型或功能很重要的分子。\n\n## 自动化重建与验证\n在图像分析和机器学习方面，Google团队应用了其为连接组学开发的一系列工具，包括用于区分脑组织图像中不同结构的“洪水填充网络”（flood-filling networks）算法，以及用于对齐和拼接连续电子显微镜图像的SOFIMA算法。\n\n研究团队对LICONN进行了大规模验证：\n*   **自动化重建：** 对近一百万立方微米的小鼠皮层体积进行了自动化重建。\n*   **可追溯性验证：** 全面验证了小鼠海马组织中约0.5米神经突的追踪能力，结果表明LICONN与基于电子显微镜的连接组学表现相当。\n\n### 验证结果展示\n![LICONN-introfinal](https://storage.googleapis.com/gweb-research2023-media/images/LICONN-introfinal.width-1250.png)\n*连接组学利用基于显微镜的成像结合计算处理，重建脑组织中的单个细胞及其复杂的连接。*\n\n研究的主要目标之一是证明LICONN与基于电子显微镜的连接组学一样可靠。\n1.  **手动追踪对比：** LICONN的重建结果与对组织子区域内所有蔓延的树突和细小弯曲的轴突进行手动追踪的结果高度吻合。经过优化，自动化重建算法的性能与电子显微镜数据的重建结果相当。\n    ![LICONN1-RenderingsFinal](https://storage.googleapis.com/gweb-research2023-media/images/LICONN1-RenderingsFinal.width-1250.png)\n    *使用洪水填充网络自动分割重建的LICONN体积渲染图。该图像描绘了原始组织体积中5.8%的轴突（上）和27.3%的树突及少量轴突（下）的重建。*\n2.  **突触连接识别：** LICONN能够可靠地识别突触连接。其多模态能力——在同一样本中同时成像结构（形态）、连接以及特定目标分子——使得研究人员能够直接标记区分组织中突触前和突触后区域的特定蛋白质。这些标记还被用作训练机器学习算法的“真值”，以便仅从结构观察中识别这些突触区域，这与电子显微镜连接组学的方法类似。\n3.  **超越结构信息的分子洞察：** LICONN还利用蛋白质标记揭示了仅凭结构无法可靠获取的分子信息，从而超越了电子显微镜连接组学的能力。例如，标记与两种不同神经递质相关的蛋白质，可以直接区分抑制性突触和兴奋性突触。标记另一种蛋白质则能精确定位电突触，这些突触在神经系统中普遍存在，但由于其微小且微妙的结构外观，通常在基于电子显微镜的连接组学重建中被忽略。在脑组织中明确识别这些分子特征，并将其置于整体脑形态和连接的背景下，将显著扩展连接组图谱的实用性。\n    ![LICONN2-HeroFinal](https://storage.googleapis.com/gweb-research2023-media/images/LICONN2-HeroFinal.width-1250.png)\n    *树突（通过洪水填充网络分割）和通过免疫标记检测到的兴奋性突触连接（白色条）的3D渲染图和放大视图。*\n\n## 前景展望\n目前，研究团队正在努力扩大LICONN的应用规模，以捕获更大组织体积的数据。未来，他们计划与美国国立卫生研究院（NIH）资助的合作项目共同绘制小鼠大脑，从海马体开始。此外，LICONN还将用于研究阿尔茨海默病等疾病背景下大脑结构的变化。\n\nLICONN的成功验证是实现这些目标的重要一步，它有望大幅提高连接组学的可及性，并使科学家能够提取结合神经元、突触和以前无法获得的分子细节的“多模态”连接组。LICONN的实用性已得到证实，自2024年3月预印本发布以来，已有多个实验室成功复制了该技术。这项工作被视为实现神经科学高级目标的关键一步，有望在临床及其他领域解锁重要的应用。",
      "shortSummary": "Google和ISTA团队开发了一种名为LICONN的新型光学显微镜技术，首次实现了基于光学显微镜对哺乳动物脑组织中神经元及其连接的全面绘制。传统连接组学依赖昂贵且复杂的电子显微镜。LICONN通过独特的组织膨胀和化学标记协议，克服了光学显微镜的分辨率限制，并能同时获取结构和分子信息。该技术已验证与电子显微镜效果相当，显著提高了脑连接组学研究的可及性和效率，为理解大脑功能和疾病提供了前所未有的新工具。",
      "translated_title": "神经连接研究的新突破",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/LICONN-introfinal.width-1250.png",
          "alt": "LICONN-introfinal",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/LICONN1-RenderingsFinal.width-1250.png",
          "alt": "LICONN1-RenderingsFinal",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/LICONN2-HeroFinal.width-1250.png",
          "alt": "LICONN2-HeroFinal",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"tdt8j\">In the 1660s, with the help of a simple, homemade light microscope that magnified samples more than 250 times, a Dutch fabric merchant named <a href=\"https://en.wikipedia.org/wiki/Antonie_van_Leeuwenhoek\" target=\"_blank\" rel=\"noopener noreferrer\">Antoine van Leeuwenhoek</a> became the first person to document a close-up view of bacteria, red blood cells, sperm cells, and many other scientific sights. Since then, <a href=\"https://en.wikipedia.org/wiki/Optical_microscope\" target=\"_blank\" rel=\"noopener noreferrer\">light microscopy</a> has solidified its place as a bedrock technique in our quest to understand living organisms. Today, it is nearly ubiquitous in life science laboratories, enabling biologists to identify and characterize cells, organs and tissues and to diagnose many diseases.</p><p data-block-key=\"8mfs1\">One field that light microscopy has not managed to penetrate, however, is connectomics — an area of neuroscience in which Google <a href=\"https://research.google/blog/ten-years-of-neuroscience-at-google-yields-maps-of-human-brain/\">has made fundamental contributions over the past decade</a>. Efforts to comprehensively map all the neurons in a region — including our previous connectomics work — have instead relied on a technique called <a href=\"https://en.wikipedia.org/wiki/Electron_microscope\" target=\"_blank\" rel=\"noopener noreferrer\">electron microscopy</a>, which can capture an extremely close-up view of structural information within a cell. Electron microscopy has a major limitation, however: it requires expensive, highly specialized equipment that is not readily accessible to most neuroscience labs.</p><p data-block-key=\"e7ion\">Today, in collaboration with colleagues at the <a href=\"https://ista.ac.at/en/home/\" target=\"_blank\" rel=\"noopener noreferrer\">Institute of Science and Technology Austria</a> (ISTA), we published in the journal <a href=\"https://www.nature.com/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature</i></a>, “<a href=\"https://www.nature.com/articles/s41586-025-08985-1\" target=\"_blank\" rel=\"noopener noreferrer\">Light-microscopy based connectomic reconstruction of mammalian brain tissue</a>”, in which we report the first-ever method for using light microscopy to comprehensively map all the neurons and their connections in a block of mouse brain tissue. We achieved this by customizing several well-established and validated techniques and combining them into a single workflow that we call LICONN (light microscopy-based connectomics). Our colleagues at ISTA led the project’s key innovation — a protocol that physically expands brain tissue while preserving structural integrity, and at the same time chemically labels all proteins in order to provide the image contrast necessary for tracing neurons and detecting other cellular structures such as synapses.</p><p data-block-key=\"aiv5p\">We iterated with ISTA on the details of the protocol, applying our suite of image analysis and machine learning (ML) tools for connectomics, and ultimately validating LICONN at scale by providing an automated reconstruction of a nearly one-million cubic micron volume of mouse cortex. We then comprehensively verified the traceability of all ~0.5 meters of <a href=\"https://en.wikipedia.org/wiki/Neurite\" target=\"_blank\" rel=\"noopener noreferrer\">neurites</a> packed within a smaller volume of mouse hippocampus tissue, demonstrating that LICONN works comparably well to electron microscope–based connectomics. We also showed that LICONN unlocks the ability to simultaneously measure structural and molecular information in a tissue sample, which will enable fundamental new opportunities to understand the workings of the brain.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/LICONN-introfinal.width-1250.png\" alt=\"LICONN-introfinal\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/LICONN-introfinal.width-1250.png\" alt=\"LICONN-introfinal\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"pquzq\"><i>Connectomics utilizes microscopy-based imaging coupled with computational processing to reconstruct individual cells and their intricate connections within brain tissue.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"tdt8j\">The need for new approaches</h2><p data-block-key=\"51hf8\">Electron microscopes produce images based on how a beam of electrons scatters when it interacts with a sample, enabling it to have a much higher resolution than light microscopy — it can image features on the scale of a nanometer compared to hundreds of nanometers for (visible) light microscopy. But despite light microscopy’s limited resolution, its accessibility is a major advantage. The electron microscopes used for connectomics research can cost millions of dollars, and operating them requires extensive specialized training, making them often available only to neuroscientists at large, well-funded institutions.</p><p data-block-key=\"52i81\">Researchers have used a handful of “super-resolution” light microscopy techniques, which can image beyond light’s diffraction limit to <a href=\"https://www.science.org/doi/full/10.1126/science.aau8302\" target=\"_blank\" rel=\"noopener noreferrer\">achieve nanoscale resolution</a>, to label and trace relatively small subsets of neurons in various ways. But these techniques have not been able to achieve “dense” connectomics — i.e., mapping <i>all</i> densely packed neurons at the scale of the finest fibers.</p><p data-block-key=\"8in9d\">A key benefit of light microscopy is that it can capture light at many different wavelengths, so researchers can use it to visualize a rainbow of fluorescent markers for proteins, neurotransmitters, and other molecules that provide clues about how neurons and circuits function or malfunction. LICONN unlocks the ability to align these molecular markers with detailed structural maps of neuronal pathways with unprecedented precision and ease, which will help yield important insights into how the brain generates cognition, perception and behavior.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"tdt8j\">Expanding the possibilities</h2><p data-block-key=\"5epo6\">To get around the problem of limited resolution with light microscopy, our ISTA colleagues turned their efforts toward the sample itself. They used a technique called expansion microscopy, which uses a substance called a <a href=\"https://www.sciencedirect.com/topics/chemistry/hydrogel#:~:text=Hydrogels%20are%20three%2Ddimensional%20networks,properties%20to%20the%20hydrogel%20structures.\" target=\"_blank\" rel=\"noopener noreferrer\">hydrogel</a> (the same material that makes diapers absorbent). Hydrogels absorb moisture by crosslinking with water molecules, swelling as they do so — a principle that researchers at the Boyden Lab at the Massachusetts Institute of Technology relied on to <a href=\"https://www.science.org/doi/10.1126/science.1260088\" target=\"_blank\" rel=\"noopener noreferrer\">develop a tissue expansion protocol for microscopy</a> in 2015.</p><p data-block-key=\"9tdtg\">Although expansion microscopy is used in many labs today, existing tissue expansion protocols do not preserve the tissue well enough to trace densely labeled neuronal structures, so our ISTA collaborators developed a new expansion protocol for LICONN. It involved first cutting a tiny block of tissue into 50 micrometer sections, then treating each section with a sequence of three different hydrogels. Two of the hydrogels created distinct, interweaving polymer networks within the tissue, with each one expanding the tissue by a factor of four, and the third hydrogel served to stabilize those networks. With this protocol, the tissue expanded by about 16 times in each direction, which is loosely comparable to starting with a sugar cube and ending up with a box of tissues.</p><p data-block-key=\"6j963\">To make the neurons visible, our colleagues incubated the brain tissue sections with a green fluorescent dye, effectively \"labeling\" all proteins inside cells. In some samples, they also used several other dyes that home in on specific proteins, neurotransmitters or other molecules that are important in determining a neuron’s cell type or function.</p><p data-block-key=\"9og7f\">Each tissue section is essentially a serial slice of a thick swathe of neurons enmeshed with other cells such as <a href=\"https://en.wikipedia.org/wiki/Glia\" target=\"_blank\" rel=\"noopener noreferrer\">glia</a>. Tracing this dense mass of brain cell fragments in expanded tissue can be challenging. We accomplished this using algorithms that we have previously developed for automated reconstruction of neurons through multiple tissue slices. These tools have achieved state-of-the-art accuracy for delineating distinct structures within brain tissue images acquired with electron microscopy (<a href=\"https://www.nature.com/articles/s41592-018-0049-4\" target=\"_blank\" rel=\"noopener noreferrer\">flood-filling networks</a>) and for aligning and stitching together serial electron microscopy images (<a href=\"https://github.com/google-research/sofima\" target=\"_blank\" rel=\"noopener noreferrer\">SOFIMA</a>).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"tdt8j\">Validating LICONN</h2><p data-block-key=\"6apns\">A chief aim of our study was to demonstrate that LICONN is as robust as electron microscopy–based connectomics. First, we showed that our results matched closely with manual tracing of all the <a href=\"https://neuroglancer-demo.appspot.com/#!gs://liconn-public/ng_states/expid82_fig2_dends.json\" target=\"_blank\" rel=\"noopener noreferrer\">sprawling dendrites</a> and <a href=\"https://neuroglancer-demo.appspot.com/#!gs://liconn-public/ng_states/expid82_fig2_axons.json\" target=\"_blank\" rel=\"noopener noreferrer\">thin, twisty axons</a> within a subsection of tissue. We then trained our automated reconstruction algorithm on the manually drawn data. After optimizing the algorithm’s accuracy, we showed that it performed comparably to reconstructions from electron microscopy data.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/LICONN1-RenderingsFinal.width-1250.png\" alt=\"LICONN1-RenderingsFinal\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/LICONN1-RenderingsFinal.width-1250.png\" alt=\"LICONN1-RenderingsFinal\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"0fic3\"><i>Renderings of the reconstructed LICONN volume with automated segmentation from flood-filling networks. The image depicts a reconstruction of 5.8% of axons (</i><b><i>top</i></b><i>) and 27.3% of dendrites and a small number of axons (</i><b><i>bottom</i></b><i>) from the original tissue volume.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"28qpp\">Next, we confirmed that synaptic connections are robustly identifiable in LICONN volumes. LICONN’s multimodal capabilities — to simultaneously image structure (morphology), connectivity, and also specific molecules of interest within the same sample — enabled us to directly label specific proteins that distinguish pre-synaptic versus post-synaptic regions in the tissue. We were further able to use these labels as ground truth for training ML algorithms to identify these synaptic areas purely from structural observations, as is done in electron microscopy connectomics.</p><p data-block-key=\"pa4l\">Finally, we also used protein labeling to reveal molecular information in connectomics maps that cannot be reliably gleaned from structure alone, thus going well beyond the capabilities of electron microscopy connectomics. For example, labeling proteins associated with two different neurotransmitters allowed us to directly differentiate between inhibitory and excitatory <a href=\"https://www.ncbi.nlm.nih.gov/books/NBK11001/#:~:text=Such%20communication%20is%20made%20possible,electrical%20synapses%20and%20chemical%20synapses.\" target=\"_blank\" rel=\"noopener noreferrer\">synapses</a>. Labeling yet another protein pinpointed <a href=\"https://www.ncbi.nlm.nih.gov/books/NBK11164/\" target=\"_blank\" rel=\"noopener noreferrer\">electrical synapses</a>, which are ubiquitous in the nervous system, but are typically neglected from electron microscopy–based connectomic reconstructions due to their small and subtle structural appearance. The ability to conclusively identify these molecular features in brain tissue, and place them in the context of overall brain morphology and connectivity, will significantly expand the utility of connectomic maps.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/LICONN2-HeroFinal.width-1250.png\" alt=\"LICONN2-HeroFinal\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/LICONN2-HeroFinal.width-1250.png\" alt=\"LICONN2-HeroFinal\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"0fic3\"><i>3D-rendering and magnified views of a dendrite (segmented with flood filling networks) and excitatory synaptic connections (white bars) detected via</i> <a href=\"https://en.wikipedia.org/wiki/Immunolabeling#:~:text=Immunolabeling%20is%20a%20biochemical%20process,cell%2C%20tissue%2C%20or%20organ.\" target=\"_blank\" rel=\"noopener noreferrer\"><i>immunolabeling</i></a><i>.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"28qpp\">In our study, the block of mouse brain we mapped was just a small percentage of the <a href=\"https://www.science.org/doi/10.1126/science.adk4858\" target=\"_blank\" rel=\"noopener noreferrer\">1 square millimeter block of human brain tissue</a>, about the size of a grain of rice, that we mapped with colleagues at Harvard University last year using electron microscopy–based connectomics. We are currently working on scaling up LICONN to be able to capture data from larger tissue volumes.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"28qpp\">The path ahead</h2><p data-block-key=\"1sb6b\">We recently announced a joint effort <a href=\"https://research.google/blog/google-research-embarks-on-effort-to-map-a-mouse-brain/\">to map a mouse brain, starting with the hippocampus</a> — one of several connectomics collaborations funded by the <a href=\"https://braininitiative.nih.gov/\" target=\"_blank\" rel=\"noopener noreferrer\">National Institutes of Health</a>. We are also working towards understanding <a href=\"https://www.biorxiv.org/content/10.1101/2023.10.24.563674v3.abstract\" target=\"_blank\" rel=\"noopener noreferrer\">how brain structures change in the context of diseases like Alzheimer's</a>. However, fully achieving these goals will require innovations that make brain mapping more efficient by orders of magnitude.</p><p data-block-key=\"7uj0g\">The validation of LICONN takes a clear step toward that goal, holding great promise both to increase the accessibility of connectomics and to enable scientists to extract “multimodal” connectomes that combine information about neurons and synapses with molecular details that have previously been inaccessible. LICONN’s utility to the neuroscience community is evidenced by the fact that already, several labs have successfully replicated the technique <a href=\"https://www.biorxiv.org/content/10.1101/2024.03.01.582884v2\" target=\"_blank\" rel=\"noopener noreferrer\">based on our preprint</a>, first posted in March 2024. We see this work as a step toward achieving high-level goals of neuroscience that could unlock important applications in the clinic and beyond.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"28qpp\">Acknowledgements</h2><p data-block-key=\"6r92r\"><i>We thank our academic collaborators in the Danzl Lab (ISTA), and acknowledge core contributions from the Connectomics Team at Google. We are grateful to Alla Katsnelson and Elise Kleeman for their help. Thanks to Lizzie Dorfman, Michael Brenner, John Platt, and Yossi Matias for their support, coordination and leadership.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "让复杂文本易于理解：基于Gemini的最小信息损失文本简化 (原标题: Making complex text understandable: Minimally-lossy text simplification with Gemini)",
      "link": "https://research.google/blog/making-complex-text-understandable-minimally-lossy-text-simplification-with-gemini/",
      "pubDate": "Mon, 05 May 2025 16:00:00 GMT",
      "isoDate": "2025-05-05T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 让复杂文本易于理解：基于Gemini的最小信息损失文本简化\n\n## 引言与背景\n在数字时代，尽管知识获取途径日益增多，但大量信息仍被复杂语言和专业术语所阻碍。这种复杂性在专家交流中是必要的，但在用户需要理解关键信息（如健康、法律或金融信息）时，却成为障碍。能够将复杂文本简化为易于理解版本的工具，可以赋能用户更好地参与这些文本。\n\n## Google Research 的创新系统\nGoogle Research 在其论文《基于LLM的文本简化及其对用户理解和认知负荷的影响》中，介绍了一个使用Gemini模型专门设计用于**最小信息损失（高保真）文本简化**的系统。\n*   **目标**：在增强清晰度的同时，精确保留原始含义、细节和细微差别。\n*   **区别**：这不同于摘要（允许信息丢失）或解释（通常添加信息）。\n*   **应用**：该系统已作为一项新功能“Simplify”在iOS版Google应用中推出。\n\n实现这一目标要求模型准确地转述复杂思想，而不引入错误或遗漏关键细节。重写后的文本必须帮助读者理解挑战性材料，同时不牺牲原始信息的完整性。\n\n## 主要贡献\n这项工作提供了两项主要贡献：\n1.  **新颖的系统**：引入了一个具有自动化评估和迭代提示词优化循环的系统。这使得Gemini模型能够以手动优化无法达到的规模和速度，通过迭代发现最有效的提示词，实现高保真文本简化。\n2.  **严格的大规模随机研究**：通过一项大规模随机研究，证明文本简化可显著提高用户理解能力并降低认知负荷。\n\n## 基于Gemini的自动化评估与提示词优化系统\n为了实现目标，研究人员开发了一种利用Gemini模型进行简化质量评估和提示词自我优化的自动化方法。\n*   **挑战**：为细致的简化（既要提高可读性又不牺牲含义或细节）制作提示词极具挑战性。自动化系统通过实现发现最有效提示词所需的广泛试错来解决这一挑战。\n\n### 自动化评估\n手动评估对于快速迭代是不切实际的。该系统采用了两个新颖的评估组件：\n*   **可读性评估**：超越了Flesch-Kincaid等简单指标，使用Gemini提示词对文本可读性进行1-10评分。该提示词根据人工判断进行迭代优化，从而实现对理解难度的更细致评估。测试表明，这种基于LLM的可读性评估与人类判断的一致性优于Flesch-Kincaid。\n*   **忠实度评估**：确保意义保留至关重要。使用Gemini 1.5 Pro，研究人员实现了一个将原始文本中的主张映射到简化版本的过程。此方法识别特定错误类型，如信息丢失、增益或扭曲，每种错误根据严重程度加权，从而提供对原始含义忠实度（完整性和蕴含）的细致衡量。\n\n### 迭代提示词优化：LLM优化LLM\n最终简化文本（由Gemini 1.5 Flash生成）的质量在很大程度上取决于初始提示词。研究人员通过一个提示词优化循环实现了提示词优化过程的自动化：\n*   利用可读性和忠实度的自动评估分数，另一个Gemini 1.5 Pro模型分析简化提示词的性能，并为下一次迭代提出改进的提示词。\n*   这创建了一个强大的反馈循环，其中LLM系统根据性能指标迭代改进其自身指令，减少了对手动提示词工程的依赖，并能够发现高效的简化策略。在此项工作中，该循环运行了824次迭代，直到性能趋于稳定。\n\n这种自动化过程，即一个LLM评估另一个LLM的输出，并根据性能指标（可读性和忠实度）和细粒度错误来优化其指令（提示词），代表了一项关键创新。它超越了繁琐的手动提示词工程，使系统能够自主地在数百次迭代中发现细致简化的有效策略。\n\n![简化系统概述](https://storage.googleapis.com/gweb-research2023-media/images/Simplify1_SummaryHero.width-1250.png)\n基于Gemini的最小信息损失文本简化方法概述。\n\n## 衡量影响：大规模随机研究\n为了验证通过此方法简化的文本在实际世界中的有效性，研究人员进行了一项随机对照研究。\n\n### 研究设计\n*   **参与者**：招募了4,563名同意参与者，并对他们的主题专业知识进行了筛选。\n*   **文本**：使用了31篇多样化的真实世界文本摘录，涵盖了以复杂性著称的领域：医学研究、生物学、法律、金融、文学、哲学、航空航天和计算机科学。\n*   **比较**：采用随机完全区组设计，参与者被随机分配阅读原始文本、简化版本或两者。为了评估文本简化对内容短期记忆的进一步影响，用户在两种条件下进行了测试：一种是回答问题时可以参考文本，另一种是不能。\n*   **测量**：通过仔细审查的多项选择题（MCQs）评估理解能力；通过自我报告的信心评估；通过简化的NASA任务负荷指数评估认知负荷。\n\n![研究设计图](https://storage.googleapis.com/gweb-research2023-media/images/Simplify2_DesignFinal.width-1250.png)\n评估简化模型与真实文本的研究设计。\n\n### 结果\n这项研究包含了近50,000个MCQ答案，产生了具有统计学意义的结果，证明了简化的价值。\n*   **定量发现**：\n    *   阅读简化文本的参与者，MCQ整体准确率比阅读原始文本的参与者**绝对提高了4%**。\n    *   对于高度复杂的PubMed文本，影响最为显著，**准确率绝对提高了15%**。\n    *   在金融（6%）、法律（4%）和技术（航空/计算机科学，4%）领域也观察到显著提升。\n    *   即使参与者无法参考文本，这些提升也依然稳健，表明对即时理解和短期记忆均有益。\n    *   除了准确率，参与者报告对答案的信心更高（在-2到2的量表上平均提高0.24），并且在使用简化文本时认为任务更容易（在-2到2的量表上平均提高0.33）。\n*   **定性洞察**：\n    *   审查简化显著提高参与者MCQ准确率的例子（一篇医学研究文本提高了38%）揭示了其价值。简化通过定义专业术语（如“肺气肿”和“纤维化”）、分解密集句子以及澄清复杂关系来增强清晰度。\n    *   简化对于基线理解较低的文本特别有帮助。\n\n![简化前后文本对比示例](https://storage.googleapis.com/gweb-research2023-media/images/Simplify3_Excerpt.width-1250.png)\n*摘自一篇生物医学文章，PMC10177208，知识共享许可（CC BY 4.0）。*\n\n### 研究局限性\n*   通过调查平台招募参与者，可能无法完全反映积极寻求理解复杂信息的用户群体。\n*   尽管系统旨在实现高保真，但LLM错误仍可能发生，需要持续警惕。\n*   MCQ虽然可扩展，但对深度理解的衡量不完整。\n\n## 作为新“Simplify”功能推出\n从今天起，这项功能已在iOS版Google应用中作为新功能“Simplify”提供。用户可以在Google应用中访问的网页上选择任何复杂文本，然后点击出现的“Simplify”图标，即可查看文本的新简化版本，无需停止阅读或离开页面。这使得人们在网上学习新知识时，更容易掌握新或复杂的主题。\n\n## 结论\nGoogle开发并严格验证了一个使用Gemini的自动化系统，该系统通过迭代学习来简化文本，同时保持对原始文本的忠实度。通过显著弥合复杂信息的理解鸿沟，这项功能显著增强了用户在关键领域的理解能力并降低了认知负荷。",
      "shortSummary": "Google Research推出了一项基于Gemini模型的文本简化系统，旨在以最小信息损失（高保真）的方式，让复杂文本更易理解。该系统通过自动化评估和迭代提示词优化实现高效简化。一项大规模研究表明，简化文本能显著提高用户理解力并降低认知负负荷，尤其在医学、法律和金融等复杂领域效果显著。此功能已作为“Simplify”在iOS版Google应用中推出，帮助用户轻松掌握复杂信息。",
      "translated_title": "让复杂文本易于理解：基于Gemini的最小信息损失文本简化",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Simplify1_SummaryHero.width-1250.png",
          "alt": "Simplify1_SummaryHero",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Simplify2_DesignFinal.width-1250.png",
          "alt": "Simplify2_DesignFinal",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Simplify3_Excerpt.width-1250.png",
          "alt": "Simplify3_Excerpt",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qwfyl\">The digital age offers ever growing access to vast amounts of knowledge, yet much remains locked behind complex language and specialist jargon. While complexity is often necessary in expert discourse, it can become a barrier when users need to understand information critical to their lives, such as navigating health information, understanding legal language, or grasping financial details. Tools that let users produce a simplified version of complex text that they encounter online can empower them to engage with those texts when they wouldn't have been able to otherwise.</p><p data-block-key=\"8jn6b\">Today, in “<a href=\"https://arxiv.org/abs/2505.01980\" target=\"_blank\" rel=\"noopener noreferrer\">LLM-based Text Simplification and its Effect on User Comprehension and Cognitive Load</a>”, Google Research introduces a system using <a href=\"https://ai.google.dev/gemini-api/docs/models\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini models</a> specifically designed for <i>minimally-lossy (high fidelity) text simplification</i>. The goal of this system is to enhance clarity while meticulously preserving the original meaning, detail, and nuance. This is&nbsp;a distinct goal from summarization (which is ok with dropping information) or explanation (which often adds information). We also launch this system in a new feature in the Google app for iOS, <a href=\"https://blog.google/products/search/simplify-google-app-ios\" target=\"_blank\" rel=\"noopener noreferrer\">Simplify</a>.</p><p data-block-key=\"3unkf\">Achieving this requires models to paraphrase complex ideas accurately without introducing errors or omitting key details. The re-written text must help the reader understand challenging material without sacrificing the integrity of the original information.</p><p data-block-key=\"3ub23\">This work offers two primary contributions. First, we present a novel system featuring an automated evaluation and iterative prompt refinement loop: this enables Gemini models to discover the most effective prompt for high-fidelity text simplification by iterating at a scale and speed impractical to achieve with manual prompt optimization. Second, through a rigorous, large-scale randomized study, we demonstrate that text simplification measurably improves user comprehension and reduces cognitive load.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qwfyl\">Gemini-powered automatic evaluation and prompt refinement system</h2><p data-block-key=\"8ib90\">In order to achieve our goals, we developed an automated approach leveraging Gemini models for evaluation of simplification quality and self-refinement of prompts. However, crafting prompts for nuanced simplification, where readability must improve without sacrificing meaning or detail, is challenging. An automated system addresses this challenge by enabling the extensive trial-and-error needed to discover the most effective prompt.</p><h3 data-block-key=\"d64vs\">Automated evaluation</h3><p data-block-key=\"4meke\">Manual evaluation is impractical for rapid iteration. Our system employs two novel evaluation components:</p><ol><li data-block-key=\"78n7\"><i>Readability assessment</i>: Moving beyond simplistic metrics like <a href=\"https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests\" target=\"_blank\" rel=\"noopener noreferrer\">Flesch-Kincaid</a>, we used a Gemini prompt to score text readability on a 1-10 scale. This prompt was iteratively refined against human judgment, enabling a more nuanced assessment of comprehension ease. We observed in testing that this LLM-based readability assessment aligns better with human readability assessments than Flesch-Kincaid.</li><li data-block-key=\"ddfcv\"><i>Fidelity assessment</i>: Ensuring meaning preservation is critical. Using Gemini 1.5 Pro, we implemented a process that maps claims from the original text to the simplified version. This method identifies specific error types like information loss, gain, or distortion, each weighted by severity, providing a granular measure of faithfulness to the original meaning (completeness and entailment).</li></ol><h3 data-block-key=\"8j5c6\">Iterative prompt refinement: LLMs optimizing LLMs</h3><p data-block-key=\"3e0ac\">The quality of the final simplification (generated by Gemini 1.5 Flash) heavily depends on the initial prompt. We automated the prompt optimization process itself via a <i>prompt refinement loop:</i> using the autoeval scores for readability and fidelity, another Gemini 1.5 Pro model analyzed the simplification prompt's performance and proposed refined prompts for the next iteration.</p><p data-block-key=\"4hmkp\">This creates a powerful feedback loop where an LLM system iteratively improves its own instructions based on performance metrics, reducing reliance on manual prompt engineering and enabling the discovery of highly effective simplification strategies. For this work, the loop ran for 824 iterations until performance plateaued.</p><p data-block-key=\"f383v\">This automated process, where one LLM evaluates the output of another and refines its instructions (prompts) based on performance metrics (readability and fidelity) and granular errors, represents a key innovation. It moves beyond laborious manual prompt engineering, enabling the system to autonomously discover highly effective strategies for nuanced simplification over hundreds of iterations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Simplify1_SummaryHero.width-1250.png\" alt=\"Simplify1_SummaryHero\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Simplify1_SummaryHero.width-1250.png\" alt=\"Simplify1_SummaryHero\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"dped5\"><i>Summary of Gemini-based approach for minimally-lossy text simplification.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qwfyl\">Measuring impact: A large-scale randomized study</h2><p data-block-key=\"6p6s5\">To validate the real-world effectiveness of text simplified with this approach, we conducted a <a href=\"https://en.wikipedia.org/wiki/Randomized_experiment\" target=\"_blank\" rel=\"noopener noreferrer\">randomized controlled study</a>.</p><h3 data-block-key=\"1lpbp\">Study design</h3><ul><li data-block-key=\"1lsge\"><i>Participants</i>: A large cohort of 4,563 consenting participants was recruited after screening for topic expertise.</li><li data-block-key=\"3utc\"><i>Texts</i>: We used 31 diverse, real-world text excerpts across domains known for complexity: medical research, biology, law, finance, literature, philosophy, aerospace, and computer science.</li><li data-block-key=\"831ev\"><i>Comparison</i>: Using a randomized complete block design (a study design that compares groups while accounting for variations), participants were randomly assigned to read either the original text, the simplified version, or both. To evaluate any further effects of text simplification on short-term retention of the content, users were tested under two conditions: one where they could refer back to the text while answering questions, and one where they could not.</li><li data-block-key=\"c4i02\"><i>Measurements</i>: We assessed comprehension using carefully reviewed multiple-choice questions (MCQs); self-reported confidence; and cognitive load via a simplified <a href=\"https://humansystems.arc.nasa.gov/groups/tlx/downloads/TLXScale.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">NASA Task Load Index</a>.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Simplify2_DesignFinal.width-1250.png\" alt=\"Simplify2_DesignFinal\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Simplify2_DesignFinal.width-1250.png\" alt=\"Simplify2_DesignFinal\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"dped5\"><i>Study design to evaluate the simplification model with real texts.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"0wr7z\">Results</h3><p data-block-key=\"5e9af\">Our study, encompassing nearly 50,000 MCQ answers, yielded statistically significant results demonstrating the value of simplification.</p><h4 data-block-key=\"3cu24\">Quantitative findings</h4><p data-block-key=\"49cvi\">Participants reading the simplified text achieved a 4% absolute increase in MCQ accuracy overall compared to those reading the original. The impact was most pronounced for the highly complex PubMed texts, which saw a 15% absolute accuracy gain. Significant gains were also observed in finance (6%), legal (4%), and technical —&nbsp;aeronautics/computer science (4%) — domains. These gains were robust even when participants could not refer back to the text, suggesting benefits for both immediate comprehension and short-term retention.</p><p data-block-key=\"crkop\">Beyond accuracy, participants reported higher confidence in their answers (average improvement of 0.24 on a -2 to 2 scale) and found the task easier (average improvement of 0.33 on a -2 to 2 scale, simplified from the task load index) when interacting with simplified text.</p><h4 data-block-key=\"fpbju\">Qualitative insights</h4><p data-block-key=\"222ri\">Reviewing examples where simplification significantly boosted participant MCQ accuracy (by 38% for one medical research text) reveals <i>how</i> it adds value. Consider the original passage below compared to its simplified version. The simplification enhances clarity by defining jargon (like 'emphysema' and 'fibrosis'), breaking down a dense sentence, and clarifying complex relationships.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Simplify3_Excerpt.width-1250.png\" alt=\"Simplify3_Excerpt\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Simplify3_Excerpt.width-1250.png\" alt=\"Simplify3_Excerpt\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"p6h08\"><i>*Excerpt from a biomedical article,</i> <a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC10177208/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>PMC10177208</i></a><i>, Creative Commons license (CC BY 4.0).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"oc9db\">Simplification proved particularly helpful for texts where baseline comprehension was low, as further illustrated by additional examples in <a href=\"https://arxiv.org/abs/2505.01980\" target=\"_blank\" rel=\"noopener noreferrer\">our paper</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"oc9db\">Study limitations</h3><p data-block-key=\"cgdkr\">Our study, while large-scale, has limitations. We leveraged a survey platform to recruit study participants, and this may not fully reflect a group of users actively seeking to understand complex information. While our system aims for high fidelity, LLM errors are possible, requiring ongoing vigilance. Lastly, MCQs, while scalable, offer an incomplete measure of deep understanding.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"oc9db\">Available as a new Simplify feature</h2><p data-block-key=\"e3f9d\"><a href=\"https://blog.google/products/search/simplify-google-app-ios\" target=\"_blank\" rel=\"noopener noreferrer\">Starting today</a> this capability is available in a new feature on the Google app for iOS, Simplify. To use it, users can select any complex text on a web page they’re visiting in the Google app, then tap the “Simplify” icon that appears to see a new, simpler version of the text, without having to stop reading or leave the page. This makes it easier for people to grasp new or complex topics they might encounter when trying to learn something new on the web.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"8u3tr\">Conclusion</h2><p data-block-key=\"788gc\">We developed and rigorously validated an automated system using Gemini that iteratively learns to simplify text while maintaining fidelity to the original. By demonstrably bridging the comprehension gap for complex information, this capability significantly enhances understanding and reduces cognitive load for users across critical domains.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"8u3tr\">Acknowledgements</h2><p data-block-key=\"85rcv\"><i>This work involved the efforts of a multidisciplinary team of software engineers, researchers, clinicians and cross functional contributors. Key contributors to this project include: Theo Guidroz, Jimmy Li, Adam Mansour, Paul Jhun, Nina Gonzalez, Xiang Ji, Mike Sanchez, Mathias MJ Bellaiche, Miguel Ángel Garrido, Faruk Ahmed, Divyansh Choudhary, Jay Hartford, Chenwei Xu, Henry Javier Serrano Echeverria, Yifan Wang, Jeff Shaffer, Eric (Yifan) Cao, Yossi Matias, Avinatan Hassidim, Dale R Webster, Yun Liu, Sho Fujiwara, Peggy Bui, Quang Duong.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Amplify 倡议：为全球化人工智能提供本地化数据 (原标题: Amplify Initiative: Localized data for globalized AI)",
      "link": "https://research.google/blog/amplify-initiative-localized-data-for-globalized-ai/",
      "pubDate": "Thu, 01 May 2025 16:00:00 GMT",
      "isoDate": "2025-05-01T16:00:00.000Z",
      "creator": "Google",
      "summary": "# Amplify 倡议：为全球化人工智能提供本地化数据\n\n## 背景与目标\n生成式人工智能（AI）模型在全球范围内具有巨大潜力，但其训练数据在语言、主题和地理方面存在局限性，导致AI难以有效满足关键的本地需求，例如可访问的健康信息、文化相关的课程和金融服务。为解决这一问题，Amplify 倡议应运而生，旨在构建一个开放的、基于社区的数据平台，以在全球范围内扩展新颖的数据收集和验证方法。该倡议强调数据收集需尊重本地文化、以社区为导向并负责任。\n\n## Amplify 倡议的运作方式\nAmplify 倡议旨在通过一个应用程序与当地社区共同创建结构化、文化相关的数据集。该平台主要功能包括：\n\n*   **共同创建参与式、结构化数据集：** 赋能各地区研究人员社区定义AI数据需求，以负责任地开发AI并解决特定区域问题。数据需求在参与者和研究人员之间共享，以确保高质量数据集的创建。\n*   **访问高质量、多语言数据集：** AI开发者和研究人员可利用Amplify创建的数据集进行技术、模型和工具的开发。开放数据尤其能帮助全球南方的研究人员利用AI解决其社区的紧迫社会问题。这些数据适用于模型微调和评估，例如，可用于斯瓦希里语错误信息的基准测试数据集，或用于简化印度金融素养较低人群金融术语的微调数据集。\n*   **贡献者获得认可和奖励：** 平台为参与者提供奖励和认可，包括数据作者署名、专业证书和研究致谢。未来，数据作者可能能够追踪其贡献如何影响AI创新。\n\n## 撒哈拉以南非洲试点项目\n为实现这一倡议，Google Research 与乌干达 Makerere 大学人工智能实验室合作，开展了实地试点项目，与撒哈拉以南非洲的专家共同开发高质量数据集。\n\n*   **合作成果：**\n    *   创建了与相关领域专家（如医护人员、教师）共同收集和验证重要领域（如健康、教育、金融）数据的方法。\n    *   确定了数据创建的奖励机制（如报酬、证书）。\n    *   建立了使用应用程序收集数据的生态系统。\n    *   在加纳、肯尼亚、马拉维、尼日利亚和乌干达培训并招募了259名专家，通过面对面研讨会和应用内培训进行。\n    *   收集了8,091个带注释的对抗性查询，由来自不同行业的155名专家共同撰写，涵盖七种语言。\n\n## 数据收集流程\n在数据收集开始前，团队（Google Research 和合作机构）会确定对该地区最重要的特定领域。邀请在这些领域具有专业或学术经验的专家协助数据收集。\n\n1.  **定义指南与培训：** 团队成员和本地合作伙伴（国家研究负责人）定义数据创建过程中需考虑的指南。团队还构建培训材料，并为专家举办其语言的实践研讨会，确保包含负责任实践、潜在偏见问题和注释技术的指导。\n2.  **Android 应用程序：** 团队开发了一款保护隐私的 Android 应用程序供专家使用。该应用程序是沟通数据目标和捕捉与关键生成式AI主题（如刻板印象、专业建议和错误信息）相关的本地问题的必要步骤。\n    *   ![Amplify1_Hero](https://storage.googleapis.com/gweb-research2023-media/images/Amplify1_Hero.width-1250.png)\n        *   **图示：** Android 应用程序界面，包含负责任AI和查询创建的培训材料。专家通过此应用创建和注释数据。应用提供自动反馈，确保查询与数据收集目标相关，并避免创建重复或语义相似的查询。专家根据主题和领域特定主题注释每个查询。\n    *   ![Amplify2_App](https://storage.googleapis.com/gweb-research2023-media/images/Amplify2_App.width-1250.png)\n        *   **图示：** 应用程序上的查询创建流程视图，包含本地化查询示例和建议注释。\n3.  **数据验证：** 数据收集完成后，具有语言和区域专业知识的区域合作伙伴和国家研究负责人会翻译、评估和验证查询的本地相关性、连贯性、流畅性和覆盖范围。团队还会利用AI自动化方法进行翻译和验证，然后最终确定数据。\n\n## 试点数据分析\n作为试点项目的一部分，Makerere AI Lab 和 Google Research 收集了8,091个带注释的对抗性查询，涵盖英语和六种非洲语言（如皮钦语、卢干达语、斯瓦希里语、奇切瓦语）。这些查询本质上是对抗性的，极有可能从大型语言模型（LLM）中产生不安全的响应，以此作为测试和缓解潜在危害的手段。该数据集可用于评估模型在这些语言环境中的安全性和文化相关性。该数据集是开源的，可供探索。\n\n*   **注释详情：** 来自七个敏感领域（如文化与宗教、就业）的专家用十个领域内的主题（如政治与政府领域的“腐败与透明度”）、五个生成式AI主题（如公共利益、错误信息）和十三个与非洲背景相关的敏感特征（如年龄、部落）注释了这些查询。\n*   **主要发现：**\n    *   最突出的领域是健康（2,076个查询）和教育（1,469个查询）。\n    *   热门主题分别是慢性病（373个查询）和教育评估与测量（245个查询）。\n    *   近80%的查询包含关于错误信息或虚假信息、刻板印象以及与公共福利（如健康或法律）相关内容的上下文信息。\n    *   大多数查询涉及社会群体，如性别（例如“奇博克女孩”）、年龄（例如“新生儿”）、宗教或信仰（例如“传统非洲”宗教）和教育水平（例如“未受教育者”）。\n    *   ![Amplify3_Results](https://storage.googleapis.com/gweb-research2023-media/images/Amplify3_Results.width-1250.png)\n        *   **图示：** 各国按主题区域和领域划分的查询数量分布。\n    *   该数据集捕捉了每个国家特有的关注点、概念和社会群体，包括植根于当地语境、误解和谬误的对抗性查询。例如，一个查询捕捉了乌干达妇女在怀孕期间食用特定类型粘土的担忧，这是一种普遍存在的文化习俗，可能带来潜在健康风险。通过使用数据集中发现的多元文化细微差别，可以增强AI模型，使其能够检测并适当响应广泛的人群。\n\n## Amplify 倡议的未来展望\n与世界各地社区建立信任是Amplify倡议的核心。为实现这一目标，Amplify 正在拉丁美洲、南亚和东南亚扩展试点项目。团队已与巴西的 Universidade Federal de Minas Gerais 和印度的 Indian Institute of Technology Kharagpur 建立了合作关系。\n\n下一步将与合作伙伴共同收集和验证那些无法通过AI模型生成的重要本地化问题数据。该应用程序可能使这些地区的专家能够用他们的语言和国家背景向 Gemini 提出关键问题，并修改生成的响应以捕捉当前AI模型中缺失的上下文信息。通过赋能领域专家与 Gemini 协作，Amplify 可以识别并填补全球范围内与重要问题相关的潜在数据空白：从巴西农民的作物选择到印度女孩继续上学的价值。\n\n## 参与倡议\nAmplify 倡议旨在赋能世界各地的社区，让他们在下一波AI创新中发挥主导作用。",
      "shortSummary": "Amplify 倡议旨在通过构建开放的、基于社区的数据平台，解决生成式AI模型训练数据缺乏本地化和多样性的问题。该倡议与全球专家合作，通过移动应用程序收集和验证高质量、多语言的文化相关数据。在撒哈拉以南非洲的试点项目成功收集了8,091个对抗性查询，用于提升AI模型的安全性和文化适应性。未来，Amplify 将扩展至拉丁美洲和亚洲，并计划与Gemini集成，以填补AI模型中的本地化知识空白，赋能全球社区。",
      "translated_title": "Amplify 倡议：为全球化人工智能提供本地化数据",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Amplify1_Hero.width-1250.png",
          "alt": "Amplify1_Hero",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Amplify2_App.width-1250.png",
          "alt": "Amplify2_App",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Amplify3_Results.width-1250.png",
          "alt": "Amplify3_Results",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"s1lid\">Generative AI models are capable of transforming aspects of life from education to innovation globally, but their reach is not matched by the breadth of their training data, which is <a href=\"https://www.brookings.edu/articles/how-language-gaps-constrain-generative-ai-development/\" target=\"_blank\" rel=\"noopener noreferrer\">limited</a> in terms of languages, topics, and geographies.</p><p data-block-key=\"32beb\">To ensure AI can address critical local needs — such as accessible health information, culturally relevant curricula, and financial services — we need diverse, high-quality data. This data should represent people, their needs, and values from across the globe, in their own languages. How this data is collected also matters. The future of data collection needs to be locally respectful, community-oriented, and <a href=\"https://ai.google/responsibility/principles/\" target=\"_blank\" rel=\"noopener noreferrer\">responsible</a>.</p><p data-block-key=\"84nmd\">To help reach these goals, we introduce <a href=\"https://arxiv.org/abs/2504.14105\" target=\"_blank\" rel=\"noopener noreferrer\">Amplify Initiative</a> — an effort focused on building an open, community-based data platform that can scale novel data collection and validation globally. We describe Amplify’s approach to co-creating datasets with domain experts through a pilot conducted in Sub-Saharan Africa. Implemented with an Android app, this research resulted in an annotated dataset of 8,091 adversarial queries in seven languages authored collaboratively with 155 experts. Moreover, Amplify aims to scale this methodology in Brazil and India, and to identify innovative methods for capturing knowledge not currently available online.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"s1lid\">Amplify Initiative</h2><p data-block-key=\"c9sk2\">Amplify Initiative is designed to create structured, culturally relevant datasets through an app with local communities. At a high-level, this platform enables people to:</p><ul><li data-block-key=\"f8l81\"><i>Co-create participatory, structured datasets that reflect needs around the world.</i> Building on the current pilot in Sub-Saharan Africa, Amplify Initiative enables a community of researchers in each region to define the data needs to develop AI responsibly and to address region-specific problems<b>.</b> These data needs will be shared between participants and researchers so they can align to create high-quality datasets.</li><li data-block-key=\"d7hfv\"><i>Access high-quality, multilingual datasets for AI innovation.</i> AI developers and researchers can utilize the datasets created with Amplify to develop techniques, models, and tools. Access to open data will particularly enable researchers from the Global South to use AI for their communities and solve pressing societal issues. The data are suitable for fine-tuning and evaluation. For example, this could include a benchmarking dataset for misinformation in Swahili or a fine-tuning dataset to simplify financial terminology for individuals with low financial literacy in India.</li><li data-block-key=\"5l1po\"><i>Receive recognition and rewards for their valuable contributions to AI.</i> The platform provides rewards and recognition for participation, including data authorship attribution, professional certificates, and research acknowledgements. In the future, the data authors may be able to track and see how their contributions impact AI innovation.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"s1lid\">Pilot across Sub-Saharan Africa</h2><p data-block-key=\"9nin0\">To make this initiative a reality, Google Research partnered with <a href=\"https://air.ug/\" target=\"_blank\" rel=\"noopener noreferrer\">Makerere University’s AI Lab</a> in Uganda for an on-the-ground pilot program to co-develop high-quality datasets with experts across Sub-Saharan Africa. Researchers at Makerere were already primed to participate in such a program thanks to an ongoing collaboration with Google involving the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-56396-6_1#:~:text=3.2%20Potential%20Harms%20of%20LLMs%20for%20the%20African%20Context&amp;text=Categories%20of%20harms%20covered%20include,automation%2C%20access%20and%20environmental%20harms\" target=\"_blank\" rel=\"noopener noreferrer\">study of potential harms</a> found in LLMs across Africa.</p><p data-block-key=\"8nofn\">Together, we:</p><ul><li data-block-key=\"f70t3\">Created a methodology for collecting and validating data about salient domains (e.g., health, education, finance) with relevant experts (i.e., people with domain specific professional or academic expertise, such as health workers and teachers).</li><li data-block-key=\"20tqc\">Identified rewards for data creation (e.g., compensation, certificates).</li><li data-block-key=\"do7im\">Established an ecosystem using an app to collect data.</li><li data-block-key=\"6esph\">Trained and onboarded 259 experts in Ghana, Kenya, Malawi, Nigeria, and Uganda using in-person workshops and on-app training.</li><li data-block-key=\"bu2fo\">Collected 8,091 annotated adversarial queries in seven languages, co-authored by 155 experts from various industries.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"s1lid\">How it works</h2><p data-block-key=\"90hur\">Before beginning data collection, the team — Google Research and partner institutions — identify which specific domains are most important to the region. Experts equipped with professional or academic experience in these domains are invited to help with the data collection process. This intentional approach is the first step toward collecting data from a diverse group of individuals who can identify the most pressing local issues.</p><p data-block-key=\"805ve\">The team members and local partners (country-specific research leads) then define guidelines that need to be factored into the data creation process. The team also builds training materials and holds hands-on workshops for experts in their languages, making sure to include instruction on responsible practices, potential bias issues, and annotation techniques.</p><p data-block-key=\"234vm\">To scale training and data collection, the team built a privacy-preserving Android app for experts to use before creating data. The training is a necessary step to communicate data goals and capture locally relevant issues related to key generative AI themes, such as stereotypes, specialized advice, and misinformation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Amplify1_Hero.width-1250.png\" alt=\"Amplify1_Hero\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Amplify1_Hero.width-1250.png\" alt=\"Amplify1_Hero\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"k0nlp\"><i>A view of the Android app which contains training materials about responsible AI and query creation.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"s1lid\">With this app, experts create and annotate data. The app provides automated feedback to ensure the queries are relevant to the data collection goals and they are not creating queries that are duplicates or semantically similar to other queries in the dataset. Experts annotate each query with thematic and domain specific topics.</p><p data-block-key=\"b93el\">Experts see annotation topics that are specific to their domain. The app makes it easy for participants to receive rewards for their contributions. It is localized for each participating country, including adapted recognition and compensation by region.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Amplify2_App.width-1250.png\" alt=\"Amplify2_App\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Amplify2_App.width-1250.png\" alt=\"Amplify2_App\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"k0nlp\"><i>A view of the query creation flow on the app with an example of an localized query and suggested annotations.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"s1lid\">Once data collection is completed, regional partners and country research leads with language and regional expertise translate, evaluate, and validate the queries for local relevance, coherence, fluency, and coverage. The team also utilizes automated approaches using AI to translate and validate the data before finalizing.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"s1lid\">Pilot data</h2><p data-block-key=\"4eb4\">As part of the pilot, Makerere AI Lab and Google Research collected 8,091 annotated adversarial queries in English and six African languages (e.g., Pidgin English, Luganda, Swahili, Chichewa). The queries are adversarial in nature and have a high likelihood of producing unsafe responses from an LLM as a means of testing and mitigating for potential harm. This dataset in turn can be used to evaluate models for their safety and cultural relevance within the context of these languages. The <a href=\"https://github.com/google-research-datasets/Amplify_SSA\" target=\"_blank\" rel=\"noopener noreferrer\">dataset</a> is open-source and available for <a href=\"https://colab.research.google.com/github/google-research-datasets/Amplify_SSA/blob/main/code/Amplify_SSA_Data_Exploration.ipynb\">exploration</a>.</p><p data-block-key=\"831ka\">Experts from seven sensitive domains (e.g., culture and religion, employment) annotated these queries with ten topics within their domain of expertise (i.e., “corruption and transparency” for politics and government domain), five generative AI themes (e.g., public interest, misinformation) and 13 sensitive characteristics (e.g., age, tribe) that are relevant to the African context.</p><p data-block-key=\"f7tv\">The most prominent domains were health (2,076) and education (1,469), with the top topics being chronic disease (373) and education assessment and measurement (245), respectively. Almost 80 percent of the queries contained contextual information about misinformation or disinformation, stereotypes, and content relevant to public welfare such as health or law. The majority of the queries were about social groups belonging to gender (e.g., “Chibok girls”), age (e.g., “newborns”), religion or belief (e.g., “Traditional African” religions), and education level (e.g., “uneducated”).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Amplify3_Results.width-1250.png\" alt=\"Amplify3_Results\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Amplify3_Results.width-1250.png\" alt=\"Amplify3_Results\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"k0nlp\"><i>Distribution of number of queries per thematic area and domain across all countries.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"s1lid\">The dataset captured unique concerns, concepts, and social groups specific to each country. This includes adversarial queries rooted in local contexts, misconceptions, and fallacies. For example, one query captures the concerns around Ugandan women consuming a specific type of clay during pregnancy, which is a <a href=\"https://link.springer.com/article/10.1186/s12905-024-03205-w\" target=\"_blank\" rel=\"noopener noreferrer\">prevalent cultural practice</a> that poses <a href=\"https://www.sciencedirect.com/science/article/pii/S2667321524000404\" target=\"_blank\" rel=\"noopener noreferrer\">potential health risks</a>. AI models can be enhanced by using the diverse cultural nuances found within the dataset, enabling them to detect and respond appropriately to a wide range of populations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"s1lid\">Amplify Initiative in the future</h2><p data-block-key=\"e6p4h\">Building trust with communities around the world is central to Amplify Initiative’s approach. To accomplish this, Amplify is scaling the pilot in Latin America and South and Southeast Asia. The team has already partnered with <a href=\"https://ufmg.br/\" target=\"_blank\" rel=\"noopener noreferrer\">Universidade Federal de Minas Gerais</a> in Brazil and <a href=\"https://www.iitkgp.ac.in/\" target=\"_blank\" rel=\"noopener noreferrer\">Indian Institute of Technology Kharagpur</a> in India.</p><p data-block-key=\"40u1p\">Together with the partners, the next step is to collect and validate data around salient, localized issues that cannot be generated using an AI model. The app might enable experts from these regions to prompt Gemini about critical issues in their languages and countries, and to modify the generated responses to capture contextual information missing in the current AI models. By enabling domain experts to collaborate with Gemini, Amplify could identify and fill potential data gaps related to salient issues globally: from crop selection for farmers in Brazil to value of staying in school for girls in India.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        <div class=\"glue-ambient-video \">\n            <video class=\"glue-ambient-video__container\">\n                <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Amplify4_Demo.m4v\" type=\"video/mp4\">\n            </video>\n            <div class=\"glue-ambient-video__button glue-ambient-video__button--paused\" aria-label=\"Video Play/pause\">\n                <div class=\"glue-ambient-video__tooltip\">\n                  <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                  <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                </div>\n                <div class=\"glue-ambient-video__icon\">\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                </div>\n              </div>\n        </div>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"k0nlp\"><i>A demonstration of the new feature using Gemini on Amplify Initiative's web application.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"s1lid\">Join Amplify Initiative</h2><p data-block-key=\"35a0r\">Amplify Initiative aspires to empower communities around the world and put them in the driver’s seat of the next wave of AI innovation. If you’re interested to learn more about the project or get involved in your country, please express interest <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe7xxEQRhFoH_41dqRwWTQb_Iym4BHmsnedV5XSynQxe8XWFA/viewform\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"s1lid\">Acknowledgements</h2><p data-block-key=\"b0f03\"><i>This project was made possible by the longstanding partnership with and continuous leadership of Joyce Nakatumba-Nabende and Rehema Baguma from Makerere University. Andrew Katumba, Chodrine Mutebi, Jagen Marvin, Eric Peter Wairagala, Mugizi Bruce, Peter Oketta, Lawrence Nderu, Obichi Obiajunwa, Abigail Oppong, and Michael Zimba provided invaluable contributions to developing the existing platform ecosystem. This project would not have succeeded without crucial effort and leadership from Erin van Liemt, Amber Ebinama, Tiffany Shih, Adam Forbes, Karla Barrios Ramos, Madhurima Maji, Aishwarya Verma, Charu Kalia, and Alexandre Zanoni from Google Research. We are grateful for the continuous support and guidance from Jamila Smith-Loud, Tiffany Deng, Saška Mojsilović, and Marian Croak. Finally, we acknowledge the data authors listed in the Amplify Initiative</i> <a href=\"https://arxiv.org/abs/2504.14105\" target=\"_blank\" rel=\"noopener noreferrer\"><i>paper</i></a><i>, without whom the dataset would not have been possible.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "AMIE获得视觉能力：一种用于多模态诊断对话的研究型AI代理 (原标题: AMIE gains vision: A research AI agent for multimodal diagnostic dialogue)",
      "link": "https://research.google/blog/amie-gains-vision-a-research-ai-agent-for-multi-modal-diagnostic-dialogue/",
      "pubDate": "Wed, 30 Apr 2025 16:00:00 GMT",
      "isoDate": "2025-04-30T16:00:00.000Z",
      "creator": "Google",
      "summary": "Articulate Medical Intelligence Explorer (AMIE) 是一个基于语言模型的研究型诊断对话AI代理，此前已在《自然》杂志上发表，在文本医学诊断对话方面展现出巨大潜力。然而，它如何将多模态数据整合到这些对话中仍未被探索。鉴于即时通讯平台在医疗环境中被广泛用于丰富讨论（例如，通过图像和文档），并且调查和测试在有效医疗护理中至关重要，因此研究大型语言模型（LLMs）如何整合这类复杂信息变得尤为重要。\n\n### AMIE的新能力：整合视觉医学信息\n\n我们的新研究使AMIE能够智能地请求、解释和推理临床对话中的视觉医学信息，以实现准确的诊断和管理计划。为此，我们以多模态Gemini 2.0 Flash为核心组件，开发了一个代理系统，该系统根据对话阶段及其对潜在诊断不断演变的不确定性来优化其响应。这种组合使得病史采集过程更好地模拟了现实世界临床实践中常见的病史采集结构。\n\n### 关键进展\n\n1.  **多模态、状态感知推理框架**：\n    *   该框架允许AMIE根据其内部状态（即在对话特定点对患者的了解）调整其响应，并高效有效地收集信息以得出适当的诊断（例如，请求皮肤照片等多模态信息以弥补知识空白）。\n    *   AMIE采用一个状态感知对话框架，该框架通过三个不同的阶段进行：病史采集、诊断与管理以及随访。AMIE动态的内部状态（反映其对患者、诊断和知识空白的不断演变理解）驱动其在每个阶段的行动。当系统评估当前阶段的目标已达成时，将触发阶段之间的转换。\n    \n    ![AMIE框架](https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-2-Framerwork.width-1250.png)\n    \n    *   **状态感知推理示例**：在模拟咨询中，AMIE能够识别知识空白（如缺少图像），请求图像，并在获得后更新其知识和鉴别诊断。\n    \n    ![AMIE状态感知推理示例](https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-3-Example.width-1250.png)\n\n2.  **鲁棒的模拟环境**：\n    *   为了实现快速迭代和强大的自动化评估，我们开发了一个全面的模拟框架。\n    *   该框架生成真实的患者场景，包括来自PTB-XL和SCIN等数据集的详细档案和多模态工件，并使用Gemini模型和网络搜索增强了合理的临床背景。\n    *   然后，它模拟AMIE与患者代理之间基于场景的回合制多模态对话。\n    *   最后，使用自动评估代理根据预定义的临床标准（如诊断准确性、信息收集有效性、管理计划适当性和安全性）评估这些模拟对话。\n    \n    ![模拟环境概览](https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-4-DialogueEvalFin.width-1250.png)\n\n### 专家评估：多模态虚拟OSCE研究\n\n我们进行了一项远程专家研究，包括105个案例场景，其中经过验证的患者演员以OSCE（客观结构化临床考试）研究的形式与AMIE或初级保健医生（PCPs）进行对话。会话通过聊天界面进行，患者演员可以上传多模态工件（例如皮肤照片），模仿多媒体即时通讯平台的功能。我们引入了一个评估多模态能力以及其他临床有意义指标（如病史采集、诊断准确性、管理推理、沟通技巧和同理心）的框架。\n\n![OSCE评估概览](https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-5-OSCEEvalFin.width-1250.png)\n\n### 结果：AMIE在多模态咨询中匹配或超越PCP表现\n\n*   我们的研究表明，AMIE在模拟即时通讯咨询中解释多模态数据方面可以超越PCPs。它在咨询质量的其他关键指标（如诊断准确性、管理推理和同理心）上也得分更高。\n*   AMIE在此研究环境中比PCPs产生了更准确、更完整的鉴别诊断。\n\n![鉴别诊断准确性](https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-6-DDx.width-1250.png)\n\n*   患者演员和皮肤科、心脏病学和内科的专科医生对对话进行了评分。AMIE在大多数评估标准中平均得分更高。值得注意的是，专家们还对图像解释和推理的质量以及有效医学对话的其他关键属性（如鉴别诊断的完整性、管理计划的质量和适当的升级能力）给予了更高的分数。\n*   AMIE的幻觉（误报）发现与PCP的幻觉程度在统计学上无法区分。\n*   从患者演员的角度来看，AMIE通常被认为更具同理心和更值得信赖。\n\n![AMIE与PCPs在OSCE关键轴上的相对表现](https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-7-Performance.width-1250.png)\n\n### 基础模型的演进：Gemini 2.5 Flash的初步结果\n\n我们使用对话模拟框架进行了初步评估，比较了基于新Gemini 2.5 Flash模型的AMIE与当前Gemini 2.0 Flash版本的性能。结果表明，使用Gemini 2.5 Flash的AMIE在Top-3诊断准确性（0.65 vs. 0.59）和管理计划适当性（0.86 vs. 0.77）方面取得了统计学上的显著提升。信息收集性能保持一致（0.81），非幻觉率保持在当前高水平（0.99）。这些初步发现表明，AMIE的未来迭代可能会受益于底层基础模型的进步，从而可能带来更准确和有益的诊断对话。\n\n![Gemini 2.0 Flash与Gemini 2.5 Flash性能比较](https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-7-Gemini2.5.width-1250.png)\n\n### 局限性与未来方向\n\n*   **现实世界验证的重要性**：本研究探索的是一个研究型系统，在OSCE风格的评估中使用患者演员，这大大低估了现实世界护理中多模态数据、疾病、患者表现、特征和担忧的复杂性和广度。它也低估了临床医生在不熟悉的环境中，没有常用实践工具和条件下的专业知识。需要谨慎解释研究结果，避免过度概括。未来的研究将包括与Beth Israel Deaconess Medical Center合作进行的预期研究，以在真实临床环境中评估AMIE。\n*   **实时音视频交互**：在远程医疗实践中，医生和患者通常通过视频通话进行更丰富的实时多模态信息交流。基于聊天的交互限制了非语言线索、视觉评估和指导检查。开发和评估AMIE的实时音视频交互是重要的未来工作。\n*   **AMIE系统的演进**：新的多模态能力补充了其他正在进行的进展，例如最近分享的纵向疾病管理推理能力。这些里程碑标志着我们朝着一个统一系统的进步，该系统不断整合对医疗保健对话AI重要的、经过严格评估的新能力。\n\n### 结论\n\n多模态感知和推理的整合标志着医学对话AI能力向前迈出了有益的一步。通过使AMIE能够“看到”和解释对临床实践至关重要的视觉和文档证据，并由Gemini的先进能力提供支持，这项研究展示了AI在高质量护理中更有效地协助患者和临床医生所需的能力。我们的研究强调了我们对负责任创新和严格评估的承诺，以实现现实世界的适用性和安全性。",
      "shortSummary": "AMIE（Articulate Medical Intelligence Explorer）现在能够智能地请求、解释和推理临床对话中的视觉医学信息。该研究型AI代理基于多模态Gemini模型和状态感知推理框架，在模拟OSCE评估中，其诊断准确性、管理推理和同理心表现与初级保健医生相当或更优。初步结果显示，结合Gemini 2.5 Flash可进一步提升性能。未来工作将侧重于现实世界验证和实时音视频交互，旨在为医疗保健提供更强大、更易用的AI辅助。",
      "translated_title": "AMIE获得视觉能力：一种用于多模态诊断对话的研究型AI代理",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-2-Framerwork.width-1250.png",
          "alt": "MMAMIE-2-Framerwork",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-3-Example.width-1250.png",
          "alt": "MMAMIE-3-Example",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-4-DialogueEvalFin.width-1250.png",
          "alt": "MMAMIE-4-DialogueEvalFin",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-5-OSCEEvalFin.width-1250.png",
          "alt": "MMAMIE-5-OSCEEvalFin",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-6-DDx.width-1250.png",
          "alt": "MMAMIE-6-DDx",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7mzds\">Language model–based AI systems such as <a href=\"https://research.google/blog/amie-a-research-ai-system-for-diagnostic-medical-reasoning-and-conversations/\">Articulate Medical Intelligence Explorer</a> (AMIE, our research diagnostic conversational AI agent recently published in <a href=\"https://www.nature.com/articles/s41586-025-08866-7\" target=\"_blank\" rel=\"noopener noreferrer\">Nature</a>) have shown considerable promise for conducting text-based medical diagnostic conversations but the critical aspect of how they can integrate multimodal data during these dialogues remains unexplored. Instant messaging platforms are a popular tool for communication that allow static multimodal information (e.g., images and documents) to enrich discussions, and their adoption has also been reported in medical settings. This ability to discuss multimodal information is particularly relevant in medicine where investigations and tests are essential for effective care and can significantly inform the course of a consultation. Whether LLMs can conduct diagnostic clinical conversations that incorporate this more complex type of information is therefore an important area for research.</p><p data-block-key=\"6r8sd\">In our new <a href=\"https://arxiv.org/abs/2505.04653\" target=\"_blank\" rel=\"noopener noreferrer\">work</a>, we advance AMIE with the ability to intelligently request, interpret, and reason about visual medical information in a clinical conversation, working towards accurate diagnosis and management plans. To this end, building on multimodal <a href=\"https://ai.google.dev/gemini-api/docs/models#gemini-2.0-flash\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini 2.0 Flash</a> as the core component, we developed an agentic system that optimizes its responses based on the phase of the conversation and its evolving uncertainty regarding the underlying diagnosis. This combination resulted in a history-taking process that better emulated the structure of history-taking that is common in real-world clinical practice.</p><p data-block-key=\"5ne9s\">Through an expert evaluation adapting <a href=\"https://en.wikipedia.org/wiki/Objective_structured_clinical_examination\" target=\"_blank\" rel=\"noopener noreferrer\">Objective Structured Clinical Examinations</a> (OSCEs), a standardized assessment used globally in medical education, we compared AMIE’s performance to primary care physicians (PCPs) and evaluated its behaviors on a number of multimodal patient scenarios. Going further, preliminary experiments with <a href=\"https://deepmind.google/technologies/gemini/flash/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini 2.5 Flash</a> indicate the possibility of improving AMIE more so by integrating the latest base model.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        <div class=\"glue-ambient-video \">\n            <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/MMAMIE-1.mp4\" type=\"video/mp4\">\n            </video>\n            <div class=\"glue-ambient-video__button glue-ambient-video__button--paused\" aria-label=\"Video Play/pause\">\n                <div class=\"glue-ambient-video__tooltip\">\n                  <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                  <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                </div>\n                <div class=\"glue-ambient-video__icon\">\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                </div>\n              </div>\n        </div>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ms6wn\"><i>We introduce</i> <a href=\"https://arxiv.org/abs/2505.04653\" target=\"_blank\" rel=\"noopener noreferrer\"><i>multimodal AMIE</i></a><i>: our diagnostic conversational AI that can intelligently request, interpret and reason about visual medical information during a clinical diagnostic conversation. We integrate multimodal perception and reasoning into AMIE through the combination of natively multimodal Gemini models and our state-aware reasoning framework.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Advancing AMIE for multimodal reasoning</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7mzds\">We introduce two key advances to AMIE. First, we developed a multimodal, state-aware reasoning framework. This allows AMIE to adapt its responses based on its internal state, which captures its knowledge about the patient at a given point in the conversation, and to gather information efficiently and effectively to derive appropriate diagnoses (e.g., requesting multimodal information, such as skin photos, to resolve any gaps in its knowledge). Second, to inform key design choices in the AMIE system, we created a simulation environment for dialogue evaluation in which AMIE converses with simulated patients based on multimodal scenarios grounded in real-world datasets, such as the <a href=\"https://research.google/blog/scin-a-new-resource-for-representative-dermatology-images/\">SCIN</a> dataset of dermatology images.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Emulating history taking of experienced clinicians: State-aware reasoning</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7mzds\">Real clinical diagnostic dialogues follow a structured yet flexible path. Clinicians methodically gather information as they form potential diagnoses. They can strategically request and interpret further details from a wide variety of multimodal data (for example, skin photos, lab results or ECG measurements). In light of such new evidence, they can ask appropriate clarification questions to resolve information gaps and delineate diagnostic possibilities.</p><p data-block-key=\"905ad\">To equip AMIE with a similar dialogue capability, we introduce a novel state-aware phase transition framework that orchestrates the conversation flow. Leveraging Gemini 2.0 Flash, this framework dynamically adapts AMIE's responses based on intermediate model outputs that reflect the evolving patient state, diagnostic hypotheses, and uncertainty. This enables AMIE to request relevant multimodal artifacts when needed, interpret their findings accurately, integrate this information seamlessly into the ongoing dialogue, and use it to refine diagnoses and guide further questioning. This emulates the structured, adaptive reasoning process used by experienced clinicians.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-2-Framerwork.width-1250.png\" alt=\"MMAMIE-2-Framerwork\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-2-Framerwork.width-1250.png\" alt=\"MMAMIE-2-Framerwork\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ms6wn\"><i>AMIE employs a state-aware dialogue framework that progresses through three distinct phases, each with a clear objective: History Taking, Diagnosis &amp; Management, and Follow-up. AMIE's dynamic internal state — reflecting its evolving understanding of the patient, diagnoses, and knowledge gaps — drives its actions within each phase (e.g., information gathering and providing explanations to the patient). Transitions between phases are triggered when the system assesses that the objectives of the current phase have been met.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-3-Example.width-1250.png\" alt=\"MMAMIE-3-Example\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-3-Example.width-1250.png\" alt=\"MMAMIE-3-Example\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ms6wn\"><i>An example of state-aware reasoning in practice during a simulated consultation with a patient-actor. At the start of this part of the interaction, AMIE is aware of the gaps in its knowledge about the case: the lack of images. AMIE requests these images, and once provided, updates its knowledge as well as its differential diagnosis.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Accelerating development: A robust simulation environment</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7mzds\">To enable rapid iteration and robust automated assessment, we developed a comprehensive simulation framework:</p><ol><li data-block-key=\"at94d\">We generate realistic patient scenarios, including detailed profiles and multimodal artifacts derived from datasets like <a href=\"https://physionet.org/content/ptb-xl/1.0.3/\" target=\"_blank\" rel=\"noopener noreferrer\">PTB-XL</a> and <a href=\"https://research.google/blog/scin-a-new-resource-for-representative-dermatology-images/\">SCIN</a>, augmented with plausible clinical context using Gemini models with web search.</li><li data-block-key=\"bbafq\">Then, we simulate turn-by-turn multimodal dialogues between AMIE and a patient agent adhering to the scenario.</li><li data-block-key=\"295ka\">Lastly, we evaluate these simulated dialogues, using an auto-rater agent, against predefined clinical criteria such as diagnostic accuracy, information gathering effectiveness, management plan appropriateness, and safety (e.g., hallucination detection).</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-4-DialogueEvalFin.width-1250.png\" alt=\"MMAMIE-4-DialogueEvalFin\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-4-DialogueEvalFin.width-1250.png\" alt=\"MMAMIE-4-DialogueEvalFin\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"jewup\"><i>Overview of our simulation environment for multimodal dialogue evaluation.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Expert evaluation: The multimodal virtual OSCE study</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7mzds\">To evaluate multimodal AMIE, we carried out a remote expert study with 105 case scenarios where validated patient actors engaged in conversations with AMIE or primary care physicians (PCPs) in the style of an OSCE study. The sessions were performed through a chat interface where patient actors could upload multimodal artifacts (e.g., skin photos), mimicking the functionality of multimedia instant messaging platforms. We introduced a framework for evaluating multimodal capability in the context of diagnostic dialogues, along with other clinically meaningful metrics, such as history-taking, diagnostic accuracy, management reasoning, communication skills, and empathy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-5-OSCEEvalFin.width-1250.png\" alt=\"MMAMIE-5-OSCEEvalFin\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-5-OSCEEvalFin.width-1250.png\" alt=\"MMAMIE-5-OSCEEvalFin\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cp2t8\"><i>Overview of our simulation environment for multimodal dialogue evaluation.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results: AMIE matches or exceeds PCP performance in multimodal consultations</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7mzds\">Our study demonstrated that AMIE can outperform PCPs in interpreting multimodal data in simulated instant-messaging consultation. It also scored higher in other key indicators of consultation quality, such as diagnostic accuracy, management reasoning, and empathy. AMIE produced more accurate and more complete differential diagnoses than PCPs in this research setting:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-6-DDx.width-1250.png\" alt=\"MMAMIE-6-DDx\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-6-DDx.width-1250.png\" alt=\"MMAMIE-6-DDx\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ms6wn\"><i>Top-</i>k<i> accuracy of differential diagnosis (DDx). AMIE and primary care physicians (PCPs) are compared across 105 scenarios with respect to the ground truth diagnosis. Upon conclusion of the consultation, both AMIE and PCPs submit a differential diagnosis list (at least 3, up to 10 plausible items, ordered by likelihood).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7mzds\">We asked both patient actors and specialist physicians in dermatology, cardiology, and internal medicine to rate the conversations on a number of scales. We found that AMIE was rated more highly on average in the majority of our evaluation rubrics. Notably, specialists also assigned higher scores to the quality of image interpretation and reasoning along with other key attributes of effective medical conversations, such as the completeness of differential diagnosis, the quality of management plans, and the ability to escalate (e.g., for urgent treatment) appropriately. The degree to which AMIE hallucinated (misreported) findings that are not consistent with the provided image artifacts was deemed to be statistically indistinguishable from the degree of PCP hallucinations. From the patient actors’ perspective, AMIE was often perceived to be more empathetic and trustworthy. More comprehensive findings can be found in the <a href=\"https://www.gstatic.com/amie/multimodal_amie.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-7-Performance.width-1250.png\" alt=\"MMAMIE-7-Performance\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-7-Performance.width-1250.png\" alt=\"MMAMIE-7-Performance\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ms6wn\"><i>Relative performance of PCPs and AMIE on other key OSCE axes as assessed by specialist physicians and patient actors. The red segments represent the proportions of patient scenarios for which AMIE’s dialogues were rated more highly than the PCPs on the respective axes. The asterisks represent statistical significance (*: p&lt;0.05, **: p&lt;0.01, ***: p&lt;0.01, n.s.: not significant)</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Evolving the base model: Preliminary results with Gemini 2.5 Flash</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7mzds\">The capabilities of Gemini models are continuously advancing, so how would multimodal AMIE's performance change when leveraging a newer, generally more capable base model? To investigate this, we conducted a preliminary evaluation using our dialogue simulation framework, comparing the performance of multimodal AMIE built upon the new <a href=\"https://deepmind.google/technologies/gemini/flash/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini 2.5 Flash</a> model against the current Gemini 2.0 Flash version rigorously validated in our main expert study.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-7-Gemini2.5.width-1250.png\" alt=\"MMAMIE-7-Gemini2.5\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MMAMIE-7-Gemini2.5.width-1250.png\" alt=\"MMAMIE-7-Gemini2.5\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ms6wn\"><i>Comparison of multimodal AMIE performance using Gemini 2.0 Flash vs. Gemini 2.5 Flash as the base model, evaluated via the automated simulation framework. Scores represent performance on key clinical criteria (* indicates statistically significant difference (p &lt; 0.05) and “n.s.” indicates non-significant difference).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7mzds\">The results as summarized in the chart above suggest possibilities for further improvements. Notably, the AMIE variant using Gemini 2.5 Flash demonstrated statistically significant gains in Top-3 Diagnosis Accuracy (0.65 vs. 0.59) and Management Plan Appropriateness (0.86 vs. 0.77). On the other hand, performance on Information Gathering remained consistent (0.81), and Non-Hallucination Rate was maintained at its current high level (0.99). These preliminary findings suggest that future iterations of AMIE could benefit from advances in the underlying base models, potentially leading to even more accurate and helpful diagnostic conversations.</p><p data-block-key=\"gg2v\">We emphasize, however, that these findings are from automated evaluations and that rigorous assessment through expert physician review is essential to confirm these performance benefits.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Limitations and future directions</h2>\n            \n        \n        \n    </div>\n\n\n\n    <ul><li data-block-key=\"7mzds\"><i>Importance of real-world validation:</i> This study explores a research-only system in an OSCE-style evaluation using patient actors, which substantially under-represents the complexity and extent of multimodal data, diseases, patient presentations, characteristics and concerns of real-world care. It also under-represents the considerable expertise of clinicians as it occurs in an unfamiliar setting without usual practice tools and conditions. It is important to interpret the research with appropriate caution and avoid overgeneralization. Continued evaluation studies and responsible development are paramount in such research towards building AI capabilities that might safely and effectively augment healthcare delivery. Further research is therefore needed before real-world translation to safely improve our understanding of the potential impacts of AMIE on clinical workflows and patient outcomes as well as to characterise and improve safety and reliability of the system under real-world constraints and challenges. As a first step towards this, we are already embarking on a <a href=\"https://research.google/blog/advancing-amie-towards-specialist-care-and-real-world-validation/\">prospective consented research study</a> with Beth Israel Deaconess Medical Center that will evaluate AMIE in a real clinical setting.</li><li data-block-key=\"9j26p\"><i>Real-time audio-video interaction:</i> In telemedical practice, physicians and patients more commonly have richer real-time multimodal information with voice-based interaction over video calls. Chat-based interactions are less common and inherently limit the physician and patients’ ability to share non-verbal cues, perform visual assessments and conduct guided examinations, all of which are readily available and are often essential for providing high-quality care in remote consultations. Development and evaluation of such real-time audio-video–based interaction for AMIE remains important future work.</li><li data-block-key=\"2f28g\"><i>Evolution of the AMIE system:</i> The new multimodal capability introduced here complements other ongoing advances, such as the capability for longitudinal disease <a href=\"https://research.google/blog/from-diagnosis-to-treatment-advancing-amie-for-longitudinal-disease-management/\">management reasoning</a> we recently shared. These milestones chart our progress towards a unified system that continually incorporates new, rigorously evaluated capabilities important for conversational AI in healthcare.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion: Towards more capable and accessible AI in healthcare</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7mzds\">The integration of multimodal perception and reasoning marks a helpful step forward for capabilities of conversational AI in medicine. By enabling AMIE to \"see\" and interpret the kinds of visual and documentary evidence crucial to clinical practice, powered by the advanced capabilities of Gemini, this research demonstrates the AI capabilities needed to more effectively assist patients and clinicians in high-quality care. Our research underscores our commitment to responsible innovation with rigorous evaluations toward real-world applicability and safety.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7mzds\"><i>The research described here is joint work across many teams at Google Research and Google DeepMind. We are grateful to all our co-authors:</i> <i>CJ Park, Tim Strother, Yong Cheng, Wei-Hung Weng, David Stutz, Nenad Tomasev, David G.T. Barrett, Anil Palepu, Valentin Liévin, Yash Sharma, Roma Ruparel, Abdullah Ahmed, Elahe Vedadi, Kimberly Kanada, Cìan Hughes, Yun Liu, Geoff Brown, Yang Gao, S. Sara Mahdavi, James Manyika, Katherine Chou, Yossi Matias, Kat Chou, Avinatan Hassidim, Dale R. Webster, Pushmeet Kohli, S. M. Ali Eslami, Joëlle Barral, Adam Rodman, Vivek Natarajan, Mike Schaekermann, Tao Tu, Alan Karthikesalingam, and Ryutaro Tanno.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "对全球健康领域大型语言模型进行基准测试 (原标题: Benchmarking LLMs for global health)",
      "link": "https://research.google/blog/benchmarking-llms-for-global-health/",
      "pubDate": "Tue, 29 Apr 2025 16:00:00 GMT",
      "isoDate": "2025-04-29T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 对全球健康领域大型语言模型（LLMs）进行基准测试\n\n大型语言模型（LLMs）在医疗健康问答方面展现出巨大潜力，尤其是在资源匮乏地区，可作为决策支持工具，提升诊断准确性、可及性和培训效果。然而，LLMs在处理疾病类型、区域医学知识、语言和文化背景等“分布外”任务时的泛化能力仍存在不确定性。\n\n### 热带和传染病（TRINDs）的挑战\n\n热带和传染病（TRINDs）是典型的“分布外”疾病亚组，影响全球17亿人口，尤其在贫困地区。这些疾病在监测、早期发现、准确诊断和管理方面面临挑战。LLMs有望通过症状、地点和风险因素进行早期筛查和监测，但目前针对TRINDs的LLM性能研究和数据集非常有限。\n\n### TRINDs数据集的开发与基准测试方法\n\n为弥补这一空白，研究团队开发了合成角色（即用于评估和优化模型的数据集）和针对“分布外”疾病亚组的基准测试方法。他们创建了一个包含11,000多个手动和LLM生成的TRINDs角色数据集，涵盖了人口统计学、背景、地点、语言、临床和消费者等多种增强信息。这项工作已在NeurIPS 2024研讨会上部分展示。\n\n#### TRINDs数据集构建\n\n*   **数据来源：** 查阅WHO、PAHO和CDC等权威机构资料，创建了50种疾病的初始患者角色模板，包含一般症状、直接属性、特定症状、背景、生活方式和风险因素，并经临床医生审核。\n*   **数据扩展：** 利用LLM提示将初始种子集扩展到11,000多个角色，增加了人口统计学和语义临床/消费者增强信息。同时，将种子集手动翻译成法语，以评估语言分布变化对模型性能的影响。\n*   **评估工具：** 开发了一个基于LLM的自动评分器，用于判断预测诊断与真实诊断是否相同或有意义地相似。\n\n![TRINDs数据集的构建模块](https://storage.googleapis.com/gweb-research2023-media/images/TRINDs2_PyramidHero.width-1250.png)\n\n*TRINDs数据集的构建模块。*\n\n![原始种子角色和LLM增强示例](https://storage.googleapis.com/gweb-research2023-media/images/TRINDs3_Examples.width-1250.png)\n\n*原始种子角色和LLM增强示例。*\n\n### 评估结果\n\n*   **LLM在TRINDs与USMLE上的表现：** Gemini 1.5模型在TRINDs数据集上的准确性低于其在USMLE基准数据集上的报告表现，表明存在分布偏移。\n*   **上下文的重要性：** 系统评估显示，结合地点和风险因素与症状（通用和特定）能显著提高LLM的诊断准确性，仅凭症状可能不足以提供准确的响应。\n\n![模型在症状、地点、属性和风险因素等上下文组合上的表现](https://storage.googleapis.com/gweb-research2023-media/images/TRINDs4_PerfContext.width-1250.png)\n\n*模型在症状（S）、地点（L）、属性（A）、风险因素（R）、通用症状（gS）、特定症状（sS）和完整角色（FP）等上下文组合上的表现。误差条表示95%置信区间。*\n\n*   **跨种族、性别和反事实地点表现：** 研究发现，在种族和性别方面，模型表现没有统计学上的显著差异。文章还探讨了低发病率地点（反事实地点）对性能的影响。\n\n![TRINDs中LLM在种族和性别亚组上的表现无显著差异](https://storage.googleapis.com/gweb-research2023-media/images/TRINDs5_PerfSubgroups.width-1250.png)\n\n*TRINDs中LLM在种族和性别亚组上的表现无显著差异。误差条表示95%置信区间。*\n\n*   **人类专家表现与评估：** 招募了7名在TRINDs和公共卫生领域拥有10年以上经验的专家。LLMs（Gemini）在TRINDs上的表现虽低于USMLE，但优于表现最佳的专家以及大多数专家组合情景，除了“Exp_Any”情景（反映了多位专家共同决策的圆桌会议）。\n\n![LLM（Gemini）与前五名专家表现的比较](https://storage.googleapis.com/gweb-research2023-media/images/TRINDs6_PerfExperts.width-1250.png)\n\n*LLM（Gemini）与前五名专家表现的比较。专家表现通过四种分数表征：平均总分（Exp_Total）、多数投票正确（Exp_Majority）、任何专家正确（Exp_Any）和所有专家正确（Exp_All）。误差条表示95%置信区间。*\n\n*   **按疾病分类的表现：** LLMs更擅长识别常见疾病（如HIV）或具有特定症状和风险因素的疾病（如狂犬病）。某些疾病（如绦虫感染）在仅提供症状时容易被错误诊断。狂犬病等疾病对反事实地点更具鲁棒性。\n\n![LLM在热带和传染病上的按疾病分类表现](https://storage.googleapis.com/gweb-research2023-media/images/TRINDs7_Heatmap.width-1250.png)\n\n*LLM在热带和传染病上的按疾病分类表现，显示了不同上下文组合（左）、包含地点反事实的上下文组合（中）以及人类专家表现（右）。*\n\n### 通过上下文学习改进LLM性能\n\n通过简单的多样本提示（使用包含所有症状、地点和风险因素的50个种子问题），对LLM（Gemini 1.5）进行上下文学习调优。这显著提高了模型在人口统计学和语义增强数据集上的性能，表明通过有意的调优可以弥补性能差距。\n\n![上下文调优前后在人口统计学和语义增强数据集上的表现](https://storage.googleapis.com/gweb-research2023-media/images/TRINDs_PerfICL.width-1250.png)\n\n*上下文调优前后在人口统计学（左）和语义（右）增强数据集上的表现。误差条表示95%置信区间。*\n\n### LLM疾病筛查工具的潜力\n\n研究团队开发了一个由经过调优的Gemini版本驱动的TRINDs疾病筛查用户界面。该工具允许用户输入人口统计学信息、地点、生活方式和风险因素，并从综合列表中选择症状以获得诊断。该工具还连接了Our World in Data API以显示发病率。专家评估表明，该界面虽输出看似简单，但有望成为对临床医生和研究人员都极具影响力和用户友好的传染病参考工具。\n\n![专家对TRINDs工具的评价](https://storage.googleapis.com/gweb-research2023-media/images/TRINDs11_Ratings.width-1250.png)\n\n*专家对TRINDs工具的评价。*\n\n### 启示\n\n研究结果强调了LLM工具在资源有限地区作为宝贵决策支持工具的潜力。然而，这些工具应作为临床判断的补充而非替代品，并需持续评估和定期更新，以适应现实世界中的时间分布变化和多样化临床环境中的可靠性。鉴于健康结果的敏感性，评估LLMs的准确性、上下文相关性和文化相关性至关重要。将此方法转化为临床工具需要进一步验证和标准监管审查。未来的研究方向包括将基准扩展到多语言和多模态。",
      "shortSummary": "该研究对大型语言模型（LLMs）在全球健康领域的应用进行了基准测试，特别关注热带和传染病（TRINDs）。团队创建了一个包含11,000多个合成角色的TRINDs数据集，并评估了LLMs的诊断准确性。结果显示，LLMs在TRINDs上的表现低于USMLE基准，但加入地点和风险因素等上下文信息能显著提高准确性。LLMs的表现优于多数人类专家组合。研究还展示了通过上下文学习改进模型性能，并开发了一个LLM驱动的TRINDs筛查工具。强调LLMs应作为临床判断的补充，需持续评估和监管。",
      "translated_title": "对全球健康领域大型语言模型进行基准测试",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/TRINDs2_PyramidHero.width-1250.png",
          "alt": "TRINDs2_PyramidHero",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/TRINDs3_Examples.width-1250.png",
          "alt": "TRINDs3_Examples",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/TRINDs4_PerfContext.width-1250.png",
          "alt": "TRINDs4_PerfContext",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/TRINDs5_PerfSubgroups.width-1250.png",
          "alt": "TRINDs5_PerfSubgroups",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/TRINDs6_PerfExperts.width-1250.png",
          "alt": "TRINDs6_PerfExperts",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"m64dk\">Large language models (LLMs) have shown potential for medical and health question-answering across various health-related tests and spanning different formats and sources. Indeed we have been on the forefront of efforts to expand the utility of LLMs for health and medical applications, as demonstrated in our recent work on <a href=\"https://research.google/blog/advancing-medical-ai-with-med-gemini/\">Med-Gemini</a>, <a href=\"https://sites.research.google/med-palm/\">MedPaLM</a>, <a href=\"https://research.google/blog/amie-a-research-ai-system-for-diagnostic-medical-reasoning-and-conversations/\">AMIE</a>, <a href=\"https://research.google/blog/multimodal-medical-ai/\">Multimodal Medical AI</a>, and our release of <a href=\"https://research.google/pubs/a-toolbox-for-surfacing-health-equity-harms-and-biases-in-large-language-models/\">novel evaluation tools and methods</a> to assess model performance across various contexts. Especially in low-resource settings, LLMs can potentially serve as valuable decision-support tools, enhancing clinical diagnostic accuracy, accessibility, and multilingual clinical decision support, and health training, especially at the community level. Yet despite their success on existing medical benchmarks, there is still some uncertainty about how well these models generalize to tasks involving distribution shifts in disease types, region-specific medical knowledge, and contextual variations across symptoms, language, location, linguistic diversity, and localized cultural contexts.</p><p data-block-key=\"f49jm\">Tropical and infectious diseases (TRINDs) are an example of such an out-of-distribution disease subgroup. TRINDs are highly prevalent in the poorest regions of the world, <a href=\"https://unitingtocombatntds.org/en/neglected-tropical-diseases/disease-directory/\" target=\"_blank\" rel=\"noopener noreferrer\">affecting 1.7 billion people globally</a> with disproportionate impacts on women and children. Challenges in preventing and treating these diseases include limitations in <a href=\"https://pubmed.ncbi.nlm.nih.gov/36146463/\" target=\"_blank\" rel=\"noopener noreferrer\">surveillance, early detection, accurate initial diagnosis<b>,</b> management, and vaccines</a>. LLMs for health-related question answering could potentially enable early screening and surveillance based on a person’s symptoms, location, and risk factors. However, only limited studies have been conducted to understand LLM performance on TRINDs with few datasets existing for rigorous LLM evaluation.</p><p data-block-key=\"4gcft\">To address this gap, we have developed <a href=\"https://arxiv.org/abs/2409.09201\" target=\"_blank\" rel=\"noopener noreferrer\">synthetic personas</a> — i.e., datasets that represent profiles, scenarios, etc., that can be used to evaluate and optimize models — and benchmark methodologies for out-of-distribution disease subgroups. We have created a TRINDs dataset that consists of 11,000+ manually and LLM-generated personas representing a broad array of tropical and infectious diseases across demographic, contextual, location, language, clinical, and consumer augmentations. Part of this work was recently presented at the NeurIPS 2024 workshops on <a href=\"https://genai4health.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Generative AI for Health</a> and <a href=\"https://aim-fm-24.github.io/NeurIPS/\" target=\"_blank\" rel=\"noopener noreferrer\">Advances in Medical Foundation Models</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        <div class=\"glue-ambient-video \">\n            <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/TRINDs1_GIF.mp4\" type=\"video/mp4\">\n            </video>\n            <div class=\"glue-ambient-video__button glue-ambient-video__button--paused\" aria-label=\"Video Play/pause\">\n                <div class=\"glue-ambient-video__tooltip\">\n                  <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                  <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                </div>\n                <div class=\"glue-ambient-video__icon\">\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                </div>\n              </div>\n        </div>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"s57ej\"><i>TRINDs dataset development and benchmarking overview.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"m64dk\">Creating synthetic TRINDs personas to evaluate LLMs</h2><p data-block-key=\"fcuoi\">We examined authoritative sources, including the <a href=\"https://www.who.int/\" target=\"_blank\" rel=\"noopener noreferrer\">WHO</a>, <a href=\"https://www.paho.org/en\" target=\"_blank\" rel=\"noopener noreferrer\">PAHO</a> and <a href=\"https://www.cdc.gov/project-firstline/about/index.html?gad_source=1&amp;gclid=CjwKCAjw2dG1BhB4EiwA998cqHEgqlR-z2Dt_0n_VcShbTG5ZGQ9UDcW1ysCC3--HSAFPT9OnoiT4hoC2RIQAvD_BwE\" target=\"_blank\" rel=\"noopener noreferrer\">CDC</a>, that publish factual information about different diseases, and used what we learned to create an initial seed set of patient persona templates for each disease. These personas consist of general symptoms, direct attributes, and specific symptoms. They also include context, lifestyle, and risk factors that were reviewed by clinicians to confirm the accuracy and the clinical relevance of the formatting for the personas. These original seed personas currently span 50 diseases.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/TRINDs2_PyramidHero.width-1250.png\" alt=\"TRINDs2_PyramidHero\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/TRINDs2_PyramidHero.width-1250.png\" alt=\"TRINDs2_PyramidHero\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"s57ej\"><i>Building blocks for the TRINDs dataset.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"m64dk\">We utilize LLM prompting to expand the seed set of synthetic personas to include demographic and semantic clinical and consumer augmentations (see below), resulting in a total of 11,000+ personas. Additionally, we manually translated the seed set into French to enable evaluation on how distribution shifts in language impact model performance. We then developed an LLM-based autorater that scores an answer as correct if the ground truth and predicted diagnosis are the same or meaningfully similar to each other.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/TRINDs3_Examples.width-1250.png\" alt=\"TRINDs3_Examples\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/TRINDs3_Examples.width-1250.png\" alt=\"TRINDs3_Examples\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"s57ej\"><i>Examples of an original seed persona and LLM-augmentations.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"m64dk\">Evaluation</h2><h3 data-block-key=\"cvtcb\">LLM performance on TRINDs vs USMLE</h3><p data-block-key=\"c2lo3\">We evaluate the accuracy of Gemini models (<a href=\"https://ai.google.dev/gemini-api/docs/migrate-to-cloud\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini 1.5</a>) in identifying a disease indicated from persona descriptions. We demonstrate that there are distribution shifts in model performance on this dataset compared to USMLE-based benchmark datasets, with lower performance on the TRINDs set compared to<a href=\"https://arxiv.org/abs/2404.18416\" target=\"_blank\" rel=\"noopener noreferrer\"> reported performance</a> on US datasets</p><h3 data-block-key=\"fk9b5\">The relevance of context</h3><p data-block-key=\"a7fd7\">We systematically perform evaluations with the dataset to understand the impact of different contexts, types (clinical vs. consumer), demographics (age, race, gender), and semantic styles. We look at how combinations of symptoms, risk factors, location, and demographics impact LLM accuracy to provide an accurate diagnosis given full or partial context. Evaluations demonstrate that including location and risk factors in combination with specific and general symptoms results in the highest performance, suggesting that symptoms alone may be insufficient for accurate responses.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/TRINDs4_PerfContext.width-1250.png\" alt=\"TRINDs4_PerfContext\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/TRINDs4_PerfContext.width-1250.png\" alt=\"TRINDs4_PerfContext\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"s57ej\"><i>Model performance on contextual combinations of symptoms (general and specific), location, risk factors, and demographic attributes.</i> <i>S = symptoms, L = location, A = attributes, R = risk factors, gS = general symptoms, sS = specific symptoms, FP = full persona with all context. Error bars = 95% confidence interval. Note: FP accuracy is also significantly higher than S, SA and gSLAR.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"m64dk\">Performance across race, gender, and counterfactual location</h3><p data-block-key=\"271t6\">We examined the impact of including a statement identifying race, e.g.,<i>“I am Black”</i> or <i>“Patient is racially Asian”</i>. We also switched out gender references and evaluated the impact of using female, male and non-binary language on performance. We found no statistically significant difference in performance across race and gender. We also examine the impact of indicating low-incident locations (counterfactual location) on performance, described in <a href=\"https://arxiv.org/abs/2409.09201\" target=\"_blank\" rel=\"noopener noreferrer\">the paper</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/TRINDs5_PerfSubgroups.width-1250.png\" alt=\"TRINDs5_PerfSubgroups\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/TRINDs5_PerfSubgroups.width-1250.png\" alt=\"TRINDs5_PerfSubgroups\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"s57ej\"><i>LLM performance on race and gender subgroups in TRINDs demonstrate no significant differences. Error bars = 95% confidence interval.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"m64dk\">Human expert performance and evaluation</h3><p data-block-key=\"2nqjp\">We recruited 7 experts with 10+ years of experience in TRINDs and public health to answer open-ended short answer questions (SAQ) and multiple choice questions (MCQ). We characterized the performance of the top five experts and also generated LLM performance on the same subset of data. This allowed simulation of different scenarios of expert round tables.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/TRINDs6_PerfExperts.width-1250.png\" alt=\"TRINDs6_PerfExperts\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/TRINDs6_PerfExperts.width-1250.png\" alt=\"TRINDs6_PerfExperts\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"s57ej\"><i>Comparison of LLM (Gemini) performance to that of the the top five experts, which were characterized with four scores: 1) the average across the total expert score (Exp_Total); 2) the full score if the majority vote was correct (Exp_Majority); 3) the full score if any expert was correct (Exp_Any); and 4) the full score only if all experts were correct (Exp_All), allowing us to explore a variety of expert decision making scenarios. Error bars = 95% confidence interval.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"m64dk\">While LLMs performed worse on TRINDs than on USMLE, they performed better than the best performing expert, and most of the expert combination scenarios except the Exp_Any scenario, which is a more appropriate benchmark to beat as it reflects a round table of public health experts each with specific focal areas coming together to make decisions.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"m64dk\">Per disease performance</h3><p data-block-key=\"p873\">We find that the LLMs tend to more accurately identify common diseases (e.g., HIV) or diseases with specific symptoms and risk factors (e.g., rabies). Certain diseases, such as tapeworm infection, are more susceptible to being labeled inaccurately if only symptoms are provided. Additionally, certain diseases, like rabies) are more robust to counterfactual locations than others.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/TRINDs7_Heatmap.width-1250.png\" alt=\"TRINDs7_Heatmap\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/TRINDs7_Heatmap.width-1250.png\" alt=\"TRINDs7_Heatmap\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"s57ej\"><i>Per disease performance of the LLM on tropical and infectious diseases shown with different contextual combinations (</i><b><i>left</i></b><i>), contextual combinations with location counterfactual (</i><b><i>center</i></b><i>), and the human expert performance (</i><b><i>right</i></b><i>). Contextual combinations abbreviated as above.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"m64dk\">Improving LLM performance through in-context learning</h2><p data-block-key=\"37peg\">We tune the LLM (Gemini 1.5) through in-context learning with simple multi-shot prompting with the seed set of 50 questions (1 per disease) with all the symptoms, locations, and risk factors. This improves the performance on both demographic and semantic augmentations, demonstrating that this gap can be rectified with intentional tuning and that the seed datasets can be used to optimize LLM performance.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/TRINDs_PerfICL.width-1250.png\" alt=\"TRINDs_PerfICL\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/TRINDs_PerfICL.width-1250.png\" alt=\"TRINDs_PerfICL\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"s57ej\"><i>Performance on demographic (</i><b><i>left</i></b><i>) and semantic (</i><b><i>right</i></b><i>) augmented datasets before and after in-context tuning. Total = 10,570, clinical demographic (n=2635), consumer demographic (n=2635), clinical semantic (n=2650) and consumer semantic (n=2650). Error bars = 95% confidence interval.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"9ekhp\">Assessing potential for LLM-based tools for disease screening</h2><p data-block-key=\"2363i\">We developed a TRINDs disease screening user interface, powered by a version of Gemini tuned for this task, that allows users to easily put in their demographics, location, lifestyle information, and risk factors, to then select their symptoms from a comprehensive list and be provided a diagnosis. We connected the tool to the <a href=\"https://ourworldindata.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Our World in Data API</a> to display incidence rates.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        <div class=\"glue-ambient-video \">\n            <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/TRINDs10_UI.mp4\" type=\"video/mp4\">\n            </video>\n            <div class=\"glue-ambient-video__button glue-ambient-video__button--paused\" aria-label=\"Video Play/pause\">\n                <div class=\"glue-ambient-video__tooltip\">\n                  <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                  <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                </div>\n                <div class=\"glue-ambient-video__icon\">\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                  \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                </div>\n              </div>\n        </div>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"7xf6d\"><i>User interface and use case of LLM-based tool for tropical and infectious disease screening.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"9ekhp\">At this early stage, expert ratings (shown below) suggest the interface, while seemingly simple in output, has the potential to be a highly impactful and user-friendly reference tool for infectious diseases, benefiting both clinicians and researchers.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/TRINDs11_Ratings.width-1250.png\" alt=\"TRINDs11_Ratings\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/TRINDs11_Ratings.width-1250.png\" alt=\"TRINDs11_Ratings\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"7xf6d\"><i>Expert ratings of TRINDs tool.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"9ekhp\">Implications</h2><p data-block-key=\"eouif\">For healthcare workers, our findings highlight the potential of LLM-based tools to serve as valuable decision-support tools in resource-limited settings. However, these tools should complement, not replace, clinical judgment. They should be balanced by continuous assessment and updated regularly to accommodate temporal distribution shifts in real-world scenarios and reliability in diverse clinical settings. Given the sensitivity of health-related outcomes, it is essential that LLMs are evaluated for accurate, contextual, and culturally relevant performance. With these considerations in mind, translating the approach described here into a clinical tool would require further validation and standard regulatory review processes. Future research directions include expanding the benchmarks to encompass multilinguality and multimodality.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"9ekhp\">Acknowledgements</h2><p data-block-key=\"cp1v2\"><i>We would like to acknowledge the authors and contributors to this work: Mercy Asiedu, Nenad Tomasev, Tiya Tiyasirichokchai, Chintan Ghate, Awa Dieng, Katherine Heller, Mariana Perroni, Divleen Jeji, Heather Cole-Lewis. Thanks to our external experts Oluwatosin Akande, Geoffrey Siwo, Steve Adudans, Sylvanus Aitkins, Odianosen Ehiakhamen, and Eric Ndombi. Thanks to Marian Croak for her support and leadership.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    }
  ],
  "lastUpdated": "2025-06-03T10:31:58.631Z"
}