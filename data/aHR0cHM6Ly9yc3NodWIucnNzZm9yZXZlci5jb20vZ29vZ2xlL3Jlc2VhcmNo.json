{
  "sourceUrl": "https://rsshub.rssforever.com/google/research",
  "title": "Google Research Blog",
  "description": "Google Research Blog - Powered by RSSHub",
  "link": "https://research.google/blog",
  "items": [
    {
      "title": "迈向更好的健康对话：基于Gemini的“寻路”AI代理研究洞察 (原标题: Towards better health conversations: Research insights on a “wayfinding” AI agent based on Gemini)",
      "link": "https://research.google/blog/towards-better-health-conversations-research-insights-on-a-wayfinding-ai-agent-based-on-gemini/",
      "pubDate": "Wed, 24 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-24T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 迈向更好的健康对话：基于Gemini的“寻路”AI代理研究洞察\n\n### 引言：在线健康信息导航的挑战\n\n获取清晰、相关且个性化的健康信息对患者至关重要，但在线健康信息世界往往令人困惑、不知所措且缺乏个性化。现有的大型语言模型（LLM）工具多为被动的“问答者”，提供对初始查询的单一全面答案，这与医生等专家通过提问来理解全貌并引导患者的方式不同。这种寻求上下文的对话方式对AI设计构成了重大挑战。\n\n### “寻路AI”：一种新的对话式方法\n\n本文介绍了基于Gemini的早期研究原型“寻路AI”（Wayfinding AI），旨在探索一种新方法。其核心理念是：通过主动提出澄清性问题，AI代理能更好地发现用户需求，引导他们清晰表达担忧，并提供更有帮助、更个性化的信息。研究团队通过四项混合方法用户体验研究，共163名参与者，迭代设计了一个用户认为比基线AI代理更有帮助、更相关、更符合其需求的代理。\n\n### 形成性用户体验洞察：在线查找健康信息的挑战\n\n*   **用户痛点**：通过访谈33名参与者发现，人们在在线查找健康信息时，往往难以清晰表达自己的健康问题，因为缺乏医学背景，难以判断哪些细节具有医学相关性。\n*   **用户偏好**：研究显示，当聊天机器人主动提出澄清性问题时，用户体验会显著改变。大多数参与者更喜欢“延迟回答”的方式（即AI先提问），而非立即给出全面答案。这种对话风格被认为更个性化、更安心。\n*   **效果**：澄清性问题不仅帮助AI提供更好的答案，也赋能用户，引导他们提供更相关的上下文。\n*   **挑战**：这种基于澄清性问题的方法的有效性高度依赖于执行质量——如果问题措辞不当、不相关或隐藏在冗长文本中容易被忽略，用户参与度就会下降。\n\n### “寻路AI”的设计原则\n\n基于上述洞察，“寻路AI”围绕三个核心原则设计，以创造更赋能的对话体验：\n\n1.  **主动对话引导**：在每一轮对话中，“寻路AI”最多提出三个有针对性的问题，旨在系统性地减少歧义，帮助用户更完整地表达其健康故事，并直接满足用户对更多上下文答案的需求。\n2.  **每轮提供“尽力而为”的答案**：考虑到某些健康问题可能无需澄清即可获得良好答案，“寻路AI”在每一轮对话中，都会根据目前共享的信息提供一个“尽力而为”的答案，同时强调如果用户能回答一个或多个后续问题，答案可以得到改进。这种方法在整个对话过程中为用户提供有用的信息，并提供选项以随着对话的进行接收越来越好的答案。\n3.  **透明推理**： “寻路AI”会解释用户的最新回答如何帮助完善了之前的答案，使AI的推理过程清晰易懂。\n\n为了确保澄清性问题在“尽力而为”的答案中不会被遗漏，界面设计采用了两栏布局：对话和澄清性问题出现在左栏，而“尽力而为”的答案和更详细的解释出现在右栏。这使得交互式对话与信息内容分离。\n\n### 随机用户研究评估\n\n为评估该代理的潜在实际影响，研究团队进行了一项随机用户研究，招募了130名21岁及以上、非医疗专业人士、有健康相关问题并愿意与AI互动的美国参与者。在随机组内设计中，每位参与者都与“寻路AI”和基线Gemini 2.5 Flash模型互动，探索他们的健康话题。参与者在与每个AI互动后，就帮助性、问题相关性、定制性、目标理解、易用性和获取有用信息效率等六个维度评估了体验满意度。\n\n![研究设计图示](https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder2_StudyDesign.width-1250.png)\n*图示：本研究的设计。*\n\n### 研究结果：通过目标理解和定制对话提供有益且相关的信息\n\n研究结果表明，尽管“寻路AI”采用了不太常见的两栏界面，用户仍在其多个重要维度上偏爱“寻路AI”的方法。用户更喜欢“寻路AI”的帮助性、相关性、理解其目标的能力以及为特定需求定制对话的能力。这些发现表明，“寻路AI”主动提问的行为成功地为用户创造了更个性化、更有帮助的体验，而没有在用户体验中引入不必要的摩擦。\n\n![用户对基线AI和寻路AI的偏好比较](https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder3_Results.width-1250.png)\n*图示：用户在多个评估维度上对基线AI和“寻路AI”的偏好，包括代理的帮助性、响应的相关性、对话对用户的定制性、对用户目标的理解、易用性、对话效率以及未来健康信息需求的使用意愿。*\n\n此外，参与者与“寻路AI”的对话明显更长，尤其是在试图理解症状原因时。对于这些话题，“寻路AI”的对话平均有4.96轮，而基线AI为3.29轮。他们向每个AI提供的提示模式也不同。\n\n![Sankey图，展示基线AI和寻路AI的对话流程](https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder4_HeroSankey.width-1250.png)\n*图示：Sankey图，展示基线AI和“寻路AI”的对话流程。每个垂直条显示了前5轮对话中用户提示类型的细分。蓝色条表示参与者对澄清性问题的回应——这在“寻路AI”中更为常见。*\n\n### 结论\n\n在线查找正确的健康信息如同迷宫。“寻路AI”通过设计成个性化和主动的对话伙伴，并在结构化界面中提出有针对性的问题，证明了其能提供比传统问答体验更受用户青睐的体验，从而帮助人们获取更有帮助、更相关和更个性化的信息。用户研究结果有力地证明，这种以人为本的对话式方法是AI在健康领域未来发展的一个有前景的方向，有助于人们更好地管理自己的健康旅程。",
      "shortSummary": "在线查找个性化健康信息充满挑战。谷歌研究团队基于Gemini开发了“寻路AI”，它通过主动提出澄清性问题，像医生一样引导用户，而非被动回答。多项用户研究（163名参与者）表明，用户更喜欢这种对话式AI，认为它更有帮助、更相关、更个性化，能更好地理解用户目标，并促成更深入的健康对话。这表明以人为本的对话式AI是健康领域AI的未来方向。",
      "translated_title": "迈向更好的健康对话：基于Gemini的“寻路”AI代理研究洞察",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder2_StudyDesign.width-1250.png",
          "alt": "Wayfinder2_StudyDesign",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder3_Results.width-1250.png",
          "alt": "Wayfinder3_Results",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder4_HeroSankey.width-1250.png",
          "alt": "Wayfinder4_HeroSankey",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3lmno\">The ability to find clear, relevant, and personalized health information is a cornerstone of empowerment for medical patients. Yet, navigating the world of online health information is often a confusing, overwhelming, and impersonal experience. We are met with a flood of generic information that does not account for our unique context, and it can be difficult to know what details are relevant.</p><p data-block-key=\"2t1lv\">Large language models (LLMs) have the potential to make this information more accessible and tailored. However, many AI tools today act as passive \"question-answerers\" — they provide a single, comprehensive answer to an initial query. But this isn't how an expert, like a doctor, helps someone navigate a complex topic. A health professional doesn't just provide a lecture; they ask clarifying questions to understand the full picture, discover a person's goals, and guide them through the information maze. Though this <i>context-seeking</i> is critical, it's a significant design challenge for AI.</p><p data-block-key=\"5nqat\">In “<a href=\"https://storage.googleapis.com/research-media/wayfinding-ai.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Towards Better Health Conversations: The Benefits of Context-Seeking</a>”, we describe how we designed and tested our “Wayfinding AI”, an early-stage research prototype, based on Gemini, that explores a new approach. Our fundamental thesis is that by proactively asking clarifying questions, an AI agent can better discover a user's needs, guide them in articulating their concerns, and provide more helpful, tailored information. In a series of four mixed-method user experience studies with a total of 163 participants, we examined how people interact with AI for their health questions, and we iteratively designed an agent that users found to be significantly more helpful, relevant, and tailored to their needs than a baseline AI agent.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"3lmno\">Formative user experience insights: Challenges in finding health information online</h2><p data-block-key=\"f889p\">To better understand the hurdles people face, we interviewed 33 participants about their experiences finding health information online. A key theme quickly emerged: people often struggle to articulate their health concerns. As one participant described, their process was to \"...just kind of like throw all the words in there and then I'm just gonna see what comes back.\" It may be that without a clinical background, it’s difficult to know which details are medically relevant.</p><p data-block-key=\"ev7la\">The people we interviewed were then able to use research prototypes of different chatbots. (The chat histories were not logged.) These participants made up a diverse group and asked health questions on a wide range of topics (e.g., rib pain, vertigo, consistent and unexplained weight gain, tinnitus and surgery; more details <a href=\"https://storage.googleapis.com/research-media/wayfinding-ai.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">in the paper</a>). Our studies revealed that when a chatbot proactively asks clarifying questions, the experience changes dramatically. The majority of participants preferred a \"deferred-answer\" approach — where the AI asks questions first — over one that gives a comprehensive answer immediately. This conversational style was perceived as more personal and reassuring. As one person noted, \"It feels more like the way it would work if you talk to a doctor... it does make me feel a little more confident that it wants to know more before jumping right into an answer.\" These clarifying questions not only help the AI provide better answers, but also empower users, guiding them to provide more relevant context. We found similar patterns in <a href=\"https://dl.acm.org/doi/abs/10.1145/3613905.3651891\" target=\"_blank\" rel=\"noopener noreferrer\">prior work on AI for dermatology</a>.</p><p data-block-key=\"b2dcv\">However, the effectiveness of this clarifying question–based approach depends heavily on the execution — engagement drops if questions are poorly formulated, irrelevant, or buried within long paragraphs of text where they are easily missed.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"3lmno\">Designing a Wayfinding AI to empower people through personal and proactive conversations</h2><p data-block-key=\"8tbaa\">Informed by these insights, we designed our Wayfinding AI around three core principles to create a more empowering conversational experience:</p><ol><li data-block-key=\"4kmek\"><i>Proactive conversational guidance:</i> At each turn, the Wayfinding AI asks up to three targeted questions designed to systematically reduce ambiguity. This helps users articulate their health story more completely and directly incorporates users’ desire for more contextualized answers.<br><br></li><li data-block-key=\"cp0mo\"><i>Best-effort answers at each turn:</i> Because some health-related questions may not require clarification to get a good answer, the Wayfinding AI provides a \"best-effort\" answer at every conversational turn, based on the information shared so far, while emphasizing that the answer can be improved if the user can answer one or more of the follow-up questions. This approach gives the user helpful information throughout the conversation, while providing the option to further receive increasingly better answers as the conversation progresses.<br><br></li><li data-block-key=\"2rorl\"><i>Transparent reasoning:</i> The Wayfinding AI explains how the user's latest answers have helped refine the previous answer. This makes the AI's reasoning process clear and understandable.</li></ol><p data-block-key=\"81tqi\">To ensure clarifying questions are never missed within the longer answers in the “best-effort” answers section, we designed an interface with a two-column layout. The conversation and clarifying questions appear in the left column, while best-effort answers and more detailed explanations appear in the right. This separates the interactive conversation from the informational content.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Wayfinder1_UserInteraction.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d7rd2\"><i>Example of a user starting to interact with our Wayfinding AI prototype interface, including both the familiar multi-turn chat interface on the left, and a “best information so far” panel on the right. This two-panel interface separates the context-seeking stream from the more detailed information provision piece, enabling users to dive into the information only when they feel all relevant information has been relayed.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"65t26\">Evaluating our Wayfinding AI through a randomized user study</h2><p data-block-key=\"7b52c\">To evaluate the potential real-world impact of this agent, we conducted a randomized user study with 130 US-based participants recruited via a third party platform. All participants were 21 years and older, were not health care professionals, and had a health-related question for which they were willing to interact with an AI. To ensure a broad range of health topics, we imposed very few restrictions on which topic would be eligible for the study (details on excluded inquiries are provided in the paper). In a randomized <a href=\"https://www.nngroup.com/articles/between-within-subjects/\" target=\"_blank\" rel=\"noopener noreferrer\">within-subjects design</a>, each participant interacted with both our Wayfinding AI and a baseline Gemini 2.5 Flash model to explore their health topic. After providing informed consent and answering standard demographic questions, participants were instructed to have a conversation spending at least 3 minutes on their question; and then to resume the survey. After interacting with each AI, participants answered questions about their satisfaction with the experience along 6 dimensions: helpfulness, relevance of questions asked, tailoring to their situation, goal understanding, ease of use, and efficiency of getting useful information. They were able to provide open feedback about what they learned, and also had the option to upload their conversation with the AI. Sharing the conversation was not required to complete the survey. At the end of the study, participants were prompted to explicitly compare the two AIs and indicate which they would prefer in terms of each of the six dimensions above. They were also asked, <i>\"For a future topic, would you prefer the first or the second AI?\"</i> The order of AI exposure (Baseline AI first vs. Wayfinding AI first) was randomized across participants. Throughout the study, participants were instructed to not provide any identifying information about themselves.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder2_StudyDesign.width-1250.png\" alt=\"Wayfinder2_StudyDesign\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder2_StudyDesign.width-1250.png\" alt=\"Wayfinder2_StudyDesign\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"9mag7\"><i>Illustration of our study design.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"65t26\">Helpful and relevant information through goal understanding and tailored conversations</h2><p data-block-key=\"7mdek\">As shown below, the results of the study demonstrated that users preferred the Wayfinding AI's approach across several important dimensions, despite its less-familiar two-column interface. Users favored Wayfinding AI for its helpfulness, relevance, ability to understand their goal, and for tailoring the conversation to their specific needs. These findings suggest that the proactive, question-asking behavior of Wayfinding AI successfully created a more personalized and helpful experience for users without introducing undue friction in the user experience.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder3_Results.width-1250.png\" alt=\"Wayfinder3_Results\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder3_Results.width-1250.png\" alt=\"Wayfinder3_Results\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"9mag7\"><i>User preferences between a baseline and our Wayfinding AI along multiple evaluation axes, including helpfulness of the agent, relevance of its responses, tailoring of the conversation to the user, understanding the user’s goal, ease of use, efficiency of the conversation and willingness to use each for a future health information need.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"65t26\">Beyond simply preferring their conversations with the Wayfinding AI, participants had noticeably <i>different</i> conversations. Conversations were longer with the Wayfinding AI, in particular when participants were trying to understand the cause of their symptoms. For those topics, conversations with the Wayfinding AI had 4.96 turns on average, compared to 3.29 for the baseline AI. And the pattern of prompts they provided to each AI looked different across conversations:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder4_HeroSankey.width-1250.png\" alt=\"Wayfinder4_HeroSankey\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder4_HeroSankey.width-1250.png\" alt=\"Wayfinder4_HeroSankey\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"9mag7\"><i>Sankey diagram illustrating the flow of conversations with the baseline AI and the Wayfinding AI. Each of the vertical bars shows the breakdown of the types of user prompts, across the first 5 conversation turns. The blue bars indicate participants responding to clarifying questions — much more common for the Wayfinding AI.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"65t26\">Conclusion</h2><p data-block-key=\"9n52g\">Finding the right health information online can feel like navigating a maze. While AI has the potential to be a powerful guide, our research shows that its success hinges on its ability to move beyond being a passive question-answerer and become an active conversational partner.</p><p data-block-key=\"6i6kb\">By designing our Wayfinding AI to be personal and proactive, we demonstrated how asking targeted questions in a well-structured interface can power an experience that users prefer over a more classical, question-answering experience, and thus enable people to obtain more helpful, relevant, and tailored information. The results from our user studies provide strong evidence that this human-centered, conversational approach is a promising direction for the future of AI in health, helping people navigate their health journeys.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"65t26\">Acknowledgements</h2><p data-block-key=\"4bk1o\"><i>The research described here is joint work across Google Research, Google Health, and partnering teams. We would like to thank Yuexing Hao, Abbi Ward, Amy Wang, Beverly Freeman, Serena Zhan, Diego Ardila, Jimmy Li, I-Ching Lee, Anna Iurchenko, Siyi Kou, Kartikeya Badola, Jimmy Hu, Bhawesh Kumar, Keith Johnson, Supriya Vijay, Justin Krogue, Avinatan Hassidim, Yossi Matias, Dale Webster, Sunny Virmani, Yun Liu, Quang Duong, Fereshteh Mahvar, Laura Vardoulakis, Tiffany Guo, and Meredith Ringel Morris for contributing or reviewing this work. We would also like to thank the participants who contributed to these studies.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "AfriMed-QA：为全球健康领域的大型语言模型进行基准测试 (原标题: AfriMed-QA: Benchmarking large language models for global health)",
      "link": "https://research.google/blog/afrimed-qa-benchmarking-large-language-models-for-global-health/",
      "pubDate": "Tue, 23 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-23T16:00:00.000Z",
      "creator": "Google",
      "summary": "## AfriMed-QA：为全球健康领域的大型语言模型进行基准测试\n\n### 项目背景与挑战\n大型语言模型（LLMs）在医学和健康问答方面展现出巨大潜力，尤其是在资源匮乏地区，它们可以作为宝贵的决策支持工具，提高临床诊断准确性、可及性，并提供多语言临床决策支持和健康培训。然而，现有医学基准测试可能无法涵盖疾病类型、症状背景、语言差异以及本地文化和区域医学知识等方面的分布变化，这限制了LLMs在传统西方环境之外的泛化能力。因此，迫切需要更多样化的基准数据集来反映真实的全球医疗背景。\n\n### AfriMed-QA数据集的诞生\n为解决这一空白，研究团队推出了 **AfriMed-QA**，这是一个基准问答数据集，汇集了来自非洲16个国家60所医学院的消费者式问题和医学院考试题目。该数据集由AfriMed-QA联盟（包括Intron health、Sisonkebiotik、University of Cape Coast等）与PATH/盖茨基金会合作开发。AfriMed-QA在ACL 2025上发布并荣获“最佳社会影响力论文奖”，其数据集和LLM评估代码已开源，并被用于训练MedGemma。\n\n### AfriMed-QA数据集详情\nAfriMed-QA是首个大规模、泛非洲、多专业的医学问答数据集，旨在评估和开发适用于非洲医疗保健的公平有效的LLMs。它包含：\n*   约15,000个临床多样化的英文问答。\n*   4,000多个带答案的专家多项选择题（MCQs）。\n*   1,200多个带长篇答案的开放式简答题（SAQs）。\n*   10,000个消费者查询（CQs）。\n\n该数据集旨在严格评估LLM在正确性和地理分布变化方面的表现。它由来自12个国家60多所医学院的621名贡献者众包完成，涵盖32个医学专业，包括妇产科、神经外科、内科、急诊医学、医学遗传学、传染病等。\n\n![Left: Map of the countries in Africa from which data were sourced, color coded to indicate the relative fraction of contributions. Right: Bar graph showing relative contributions from each country.](https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-3c-Countries.width-1250.png)\n*AfriMed-QA问题和答案的来源国家分布图。*\n\n数据收集通过一个改编自Intron Health的基于网络的平台进行，并开发了定制的用户界面来收集不同类型的问题、进行质量审查以及对LLM响应进行盲评。为了避免消费者分享个人健康信息，消费者查询（CQs）通过疾病情景提示生成，LLM的回答由人类临床专家和消费者进行评分。\n\n![Example of questions used in dataset curation. Top: Multiple choice question; Middle: Short answer question; Bottom: Consumer query](https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-4a-Questions.width-1250.png)\n*AfriMed-QA数据集整理和LLM评估概述中的问题示例。*\n\n![A list of the medial specialties included in the dataset and the number of questions in each.](https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-5a-Specialties.width-1250.png)\n*AfriMed-QA中包含的医学专业及其问题数量列表。*\n\n### LLM评估与发现\n研究团队采用定量和定性方法评估了30个通用和生物医学LLMs。对于MCQs，通过比较LLM的单字母答案与参考答案来衡量准确性；对于SAQs，则通过语义相似性和句子级重叠来衡量。\n\n**主要发现：**\n*   **模型规模与性能：** 较大模型的基线性能在AfriMed-QA上比小型模型更准确。这对于偏好在设备或边缘部署小型专业模型的低资源环境可能不利。\n*   **模型类型与泛化：** 令人惊讶的是，基线通用模型比同等规模的生物医学模型表现更好且泛化能力更强。这可能归因于研究中开放生物医学模型的参数大小限制，或者表明专业LLMs过度拟合了其训练数据的特定偏差和细微差别，导致它们对AfriMed-QA数据集的独特特征适应性较差。\n\n![Bar graph of the performance of LLM models on the AfriMed-QA dataset.](https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-6a-Performance.width-1250.png)\n*LLM模型在AfriMed-QA数据集上的性能条形图（截至2025年5月的实验数据）。*\n\n### 人工评估LLM响应\n研究团队将3000个随机抽样的问题的LLM响应提交给Intron Health众包平台进行人工评估。评估标准包括不准确性、信息遗漏、人口统计偏见证据和潜在危害程度。\n\n*   **临床医生评估：** 评估LLM对MCQ、SAQ和CQ响应的正确性、本地化程度、是否存在遗漏或幻觉以及潜在危害。\n*   **非临床医生/消费者评估：** 评估LLM对CQ响应的相关性、帮助性和本地化程度。\n\n![Image of the interface used for expert review of LLM responses to AfriMed-QA.](https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-7-Interface.width-1250.png)\n*用于专家评审AfriMed-QA中LLM响应的界面图像。*\n\n评估采用5分制，评分者对答案来源（模型或人类）进行盲评。结果显示，消费者和临床医生对LLM在CQ上的响应表现出偏好，前沿LLMs在完整性、信息量和相关性方面始终优于临床医生提供的答案，并且更不容易出现幻觉和遗漏。与此一致，临床医生对CQs的回答在相关信息遗漏方面得分较低。\n\n![Plot showing the consumer blinded evaluations human clinical experts and LLM answers.](https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-8-Evals.width-1250.png)\n*消费者对人类临床专家和LLM答案的盲评图。图表显示了各项评估轴的平均评分和置信区间。*\n\n### 开放排行榜与未来展望\n研究团队开发了一个开放排行榜，方便用户可视化和比较LLM性能，并允许提交自己的模型进行评估。\n\n![Image of the landing page for the AfriMed-QA LLM leaderboard](https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-9-Leaderboard.width-1250.png)\n*AfriMed-QA LLM排行榜的登录页面图像，可比较不同模型在不同基准指标上的表现。*\n\n未来，AfriMed-QA联盟计划将数据集扩展到非英语官方和本土语言，并纳入多模态（如视觉和音频）问答数据集，以反映医学固有的多语言和多模态特性。\n\n### 局限性与行动呼吁\n尽管AfriMed-QA是首个大规模、多专业、本土来源的泛非洲数据集，但它并非完整。目前超过50%的专家MCQ问题来自尼日利亚，团队正努力扩大更多非洲地区和全球南方国家的代表性。这项工作为在缺乏数字化基准数据集的国家获取多样化和代表性的健康基准数据集奠定了基础。\n\n鉴于健康结果的敏感性，评估LLM的准确性、情境性和文化相关性至关重要。研究呼吁其他研究和健康组织通过合作和当地投入，进一步研究该领域，策划数据集以评估和优化LLM在其特定环境中的应用，以适应疾病流行率、文化背景、资源、药物类型、健康建议、医疗技术基础设施、可负担性、护理类型和敏感属性等方面的分布变化。",
      "shortSummary": "AfriMed-QA是一个开创性的泛非洲医学问答基准数据集，旨在评估和改进大型语言模型（LLMs）在全球健康领域的表现。该数据集包含约15,000个问题，涵盖多项选择题、简答题和消费者查询，数据来源于非洲12个国家的60多所医学院，覆盖32个医学专业。研究发现，大型LLMs表现优于小型模型，而通用模型在AfriMed-QA上意外地优于同等规模的生物医学模型。人工评估显示，前沿LLMs在消费者查询方面优于临床医生。AfriMed-QA已开源，并计划扩展至多语言和多模态，以促进LLMs在多样化、低资源医疗环境中的公平有效应用。",
      "translated_title": "AfriMed-QA：为全球健康领域的大型语言模型进行基准测试",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-3c-Countries.width-1250.png",
          "alt": "Left: Map of the countries in Africa from which data were sourced, color coded to indicate the relative fraction of contributions. Right: Bar graph showing relative contributions from each country.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-4a-Questions.width-1250.png",
          "alt": "Example of questions used in dataset curation. Top: Multiple choice question; Middle: Short answer question; Bottom: Consumer query",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-5a-Specialties.width-1250.png",
          "alt": "A list of the medial specialties included in the dataset and the number of questions in each.",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-6a-Performance.width-1250.png",
          "alt": "Bar graph of the performance of LLM models on the AfriMed-QA dataset.",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-7-Interface.width-1250.png",
          "alt": "Image of the interface used for expert review of LLM responses to AfriMed-QA.",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3s6d8\">Large language models (LLMs) have shown potential for medical and health question answering across various health-related tests spanning different formats and sources, such as multiple choice and short answer exam questions (e.g., <a href=\"https://arxiv.org/abs/2009.13081\" target=\"_blank\" rel=\"noopener noreferrer\">USMLE MedQA</a>), summarization, and clinical note taking, among others. Especially in low-resource settings, LLMs can potentially serve as valuable decision-support tools, enhancing clinical diagnostic accuracy and accessibility, and providing multilingual clinical decision support and health training, all of which are especially valuable at the community level.</p><p data-block-key=\"aosjl\">Despite their success on existing medical benchmarks, there is uncertainty about whether these models generalize to tasks involving distribution shifts in disease types, contextual differences across symptoms, or variations in language and linguistics, even within English. Further, localized cultural contexts and region-specific medical knowledge is important for models deployed outside of traditional Western settings. Yet without diverse benchmark datasets that reflect the breadth of real-world contexts, it’s impossible to train or evaluate models in these settings, highlighting the need for more diverse benchmark datasets.</p><p data-block-key=\"fnob9\">To address this gap, we present <a href=\"https://aclanthology.org/2025.acl-long.96.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">AfriMed-QA</a>, a benchmark question–answer dataset that brings together consumer-style questions and medical school–type exams from 60 medical schools, across 16 countries in Africa. We developed the dataset in collaboration with numerous partners, including <a href=\"https://www.intron.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Intron health</a>, <a href=\"https://sisonkebiotik.africa/\" target=\"_blank\" rel=\"noopener noreferrer\">Sisonkebiotik</a>, <a href=\"https://ucc.edu.gh/\" target=\"_blank\" rel=\"noopener noreferrer\">University of Cape Coast</a>, the <a href=\"https://famsanet.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Federation of African Medical Students Association</a>, and <a href=\"https://bioramp.org/\" target=\"_blank\" rel=\"noopener noreferrer\">BioRAMP</a>, which collectively form the <a href=\"https://afrimedqa.com/\" target=\"_blank\" rel=\"noopener noreferrer\">AfriMed-QA consortium</a>, and with support from <a href=\"https://media.path.org/documents/GATES-AI_brief_March_2025_PNpefpK.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">PATH/The Gates Foundation</a>. We evaluated LLM responses on these datasets, comparing them to answers provided by human experts and rating their responses according to human preference. The methods used in this project can be scaled to other locales where digitized benchmarks may not currently be available.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/AfriMedQA-1-Overview.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3s6d8\">AfriMed-QA was published at <a href=\"https://2025.aclweb.org/\" target=\"_blank\" rel=\"noopener noreferrer\">ACL 2025</a> where it won the <a href=\"https://2025.aclweb.org/program/awards/\" target=\"_blank\" rel=\"noopener noreferrer\">Best Social Impact Paper Award</a>. The dataset was recently leveraged to assist in training of <a href=\"https://developers.google.com/health-ai-developer-foundations/medgemma/model-card\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma</a>, our <a href=\"https://research.google/blog/medgemma-our-most-capable-open-models-for-health-ai-development/\">latest open model</a> for multimodal medical text and image comprehension. The <a href=\"https://huggingface.co/datasets/afrimedqa/afrimedqa_v2\" target=\"_blank\" rel=\"noopener noreferrer\">AfriMed-QA benchmark datasets</a> and <a href=\"https://github.com/afrimedqa/AfriMed-QA\" target=\"_blank\" rel=\"noopener noreferrer\">LLM evaluation code</a> are open-sourced and available for use by the community.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/AfriMedQA-2-Storyboard.m4v\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">AfriMed-QA dataset</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3s6d8\">The <a href=\"https://huggingface.co/datasets/afrimedqa/afrimedqa_v2\" target=\"_blank\" rel=\"noopener noreferrer\">AfriMed-QA dataset</a> is the first large-scale pan-African multi-specialty medical question–answer dataset designed to evaluate and develop equitable and effective LLMs for African healthcare. The dataset comprises ~15,000 clinically diverse questions and answers in English, 4,000+ expert multiple choice questions (MCQs) with answers, over 1,200 open ended short answer (SAQs) with long-form answers, and 10,000 consumer queries (CQ). The dataset is designed to rigorously assess LLM performance for correctness and geographical shifts. It was crowd-sourced from 621 contributors, from over 60 medical schools across 12 countries, covering 32 medical specialties, including obstetrics and gynecology, neurosurgery, internal medicine, emergency medicine, medical genetics, infectious disease, and others.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-3c-Countries.width-1250.png\" alt=\"Left: Map of the countries in Africa from which data were sourced, color coded to indicate the relative fraction of contributions. Right: Bar graph showing relative contributions from each country.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-3c-Countries.width-1250.png\" alt=\"Left: Map of the countries in Africa from which data were sourced, color coded to indicate the relative fraction of contributions. Right: Bar graph showing relative contributions from each country.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"q7thn\">Countries where AfriMed-QA questions and answers were sourced.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"nnfxj\">To collect these data, we adapted a web-based platform previously developed by Intron Health for crowd-sourcing accented and multilingual clinical speech data at scale across Africa. We developed custom user interfaces to collect each question type, for quality reviews, and for blinded human evaluation of LLM responses.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-4a-Questions.width-1250.png\" alt=\"Example of questions used in dataset curation. Top: Multiple choice question; Middle: Short answer question; Bottom: Consumer query\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-4a-Questions.width-1250.png\" alt=\"Example of questions used in dataset curation. Top: Multiple choice question; Middle: Short answer question; Bottom: Consumer query\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"c0ibi\">AfriMed-QA dataset curation and LLM evaluation overview. MCQs and SAQs from medical schools had accompanying human labels. For CQs, to avoid consumers sharing their own health information which might lead to potential disclosure of health information, and repetitiveness in question types, consumers were prompted with a disease scenario, and they responded with a question they would ask based on it. The scenario and question were passed to an LLM and the LLM responses were rated by human clinical experts as well as consumers.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-5a-Specialties.width-1250.png\" alt=\"A list of the medial specialties included in the dataset and the number of questions in each.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-5a-Specialties.width-1250.png\" alt=\"A list of the medial specialties included in the dataset and the number of questions in each.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"c0ibi\">Medical specialties represented in AfriMed-QA.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Evaluation of LLM responses</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"nnfxj\">Using quantitative and qualitative approaches, we evaluated 30 general and biomedical LLMs, ranging in size from small to large. Some were open and others were closed. For MCQs, we measured the accuracy by comparing each LLM’s single-letter answer choice with the reference. For SAQs, we measured semantic similarity and sentence level overlap comparing the generated response from the language model against a reference answer.</p><p data-block-key=\"8i9dn\">We found that the baseline performance of larger models is more accurate than small models on AfriMed-QA. This trend may be unfavorable to low-resource settings where on-device or edge deployments with smaller specialized models are preferred.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-6a-Performance.width-1250.png\" alt=\"Bar graph of the performance of LLM models on the AfriMed-QA dataset.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-6a-Performance.width-1250.png\" alt=\"Bar graph of the performance of LLM models on the AfriMed-QA dataset.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"c0ibi\"><i>Performance of LLM models on the AfriMed-QA dataset (experiments as of May 2025).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"nnfxj\">We also found that baseline general models outperform and generalize better than biomedical models of similar size. This counterintuitive result could be due to the parameter size limitations of open biomedical models in our study or it could indicate that specialized LLMs overfit to the specific biases and nuances of the data on which they are fine-tuned. In either case, they seem to be less adaptable to the unique characteristics of the AfriMed-QA dataset.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Human rating of LLM responses</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"nnfxj\">LLM responses to a fixed subset of questions (<i>n</i>=3000; randomly sampled) were sent out for human evaluation on the Intron Health crowd-sourcing platform. Adapting the evaluation axes described in our <a href=\"https://www.nature.com/articles/s41591-024-03423-7\" target=\"_blank\" rel=\"noopener noreferrer\">MedLM paper</a>, which included measures for inaccuracy, omission of information, evidence of demographic bias, and extent of harm, we collected human evaluations in two categories:</p><ol><li data-block-key=\"5ol9l\"><i>Clinicians</i> provided ratings to the LLM’s MCQ, SAQ, and CQ responses, evaluating whether answers were correct and localized, if omissions or hallucinations were present, and if potential for harm existed.</li><li data-block-key=\"nhmf\"><i>Non-clinicians/consumers</i> rated CQ LLM responses to determine if answers were relevant, helpful, and localized.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-7-Interface.width-1250.png\" alt=\"Image of the interface used for expert review of LLM responses to AfriMed-QA.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-7-Interface.width-1250.png\" alt=\"Image of the interface used for expert review of LLM responses to AfriMed-QA.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"c0ibi\"><i>Interface used for expert review of LLM responses to AfriMed-QA.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"nnfxj\">Ratings were on a 5-point scale representing the extent to which the criteria were met. “1” represents “No\" or “completely absent\" and “5” represents “Yes\" or “absolutely present\". Raters were blinded to the answer source (model name or human) and each rater was asked to evaluate answers from multiple LLMs in a random sequence.</p><p data-block-key=\"8mi0p\">Consumer and clinician human evaluation of LLM answers to CQs revealed a preference for LLM responses, where frontier LLMs were consistently rated to be more complete, informative, and relevant when compared with clinician-provided answers, and less susceptible to hallucinations and omissions. Consistent with this, clinician answers to CQs were also rated worse when measured for omission of relevant information.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-8-Evals.width-1250.png\" alt=\"Plot showing the consumer blinded evaluations human clinical experts and LLM answers.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-8-Evals.width-1250.png\" alt=\"Plot showing the consumer blinded evaluations human clinical experts and LLM answers.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"c0ibi\"><i>Consumer blinded evaluations of human clinical experts and LLM answers. Plots show mean ratings and confidence intervals across various axes.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Building an open Leaderboard for easy comparison of data versions and LLM versions</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"nnfxj\">We have developed a leaderboard for easy visualization and comparison of LLM performance. Users can compare existing models or submit their own models and see how well they perform on the dataset.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-9-Leaderboard.width-1250.png\" alt=\"Image of the landing page for the AfriMed-QA LLM leaderboard\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-9-Leaderboard.width-1250.png\" alt=\"Image of the landing page for the AfriMed-QA LLM leaderboard\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"c0ibi\"><i>AfriMed-QA leaderboard enables comparison of different models across different benchmark metrics.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Towards a multilingual, multimodal dataset</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"nnfxj\">We recognize that medicine is inherently multilingual and multimodal and are currently working with the AfriMed-QA consortium led by <a href=\"https://directory.ucc.edu.gh/p/stephen-moore\" target=\"_blank\" rel=\"noopener noreferrer\">Prof. Stephen Moore</a> at the University of Cape Coast to expand beyond English-only text-based question answering to non-English official and native languages from the continent. We are also working to incorporate multimodal (e.g., visual and audio) question answering datasets.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Limitations</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"nnfxj\">Although this is the first large-scale, multi-specialty, indigenously sourced pan-African dataset of its kind, it is by no means complete. Over 50% of the expert MCQ questions came from Nigeria. We are working to expand representation from more African regions and the Global South.</p><p data-block-key=\"8mlsj\">While the development of the dataset is still in progress, this work establishes a foundation for acquiring diverse and representative health benchmark datasets across countries that may not have digitized and readily available benchmark datasets.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">LLMs for geographically diverse health QAs</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"nnfxj\">Given the sensitivity of health-related outcomes, it is essential that LLMs are evaluated for accurate, contextual, and culturally relevant performance. Across different settings one can anticipate a variety of distribution shifts to which LLMs need to adapt. These include disease prevalence, cultural context, resources and infrastructure, drug types and nomenclature, differences in health recommendations for screening and treatment, medical technology infrastructure, affordability, care types, and sensitive attributes. While our evaluations are limited, we present a call to action for other research and health organizations to pursue further research in this area, curating datasets to evaluate and optimize LLMs for use in their contexts through partnerships and local input.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"nnfxj\"><i>We would like to acknowledge the incredible AfriMed-QA consortium and co-authors. Tobi Olatunji, Charles Nimo, Abraham Owodunni, Tassallah Abdullahi, Emmanuel Ayodele, Mardhiyah Sanni, Chinemelu Aka, Folafunmi Omofoye, Foutse Yuehgoh, Timothy Faniran, Bonaventure F. P. Dossou, Moshood Yekini, Jonas Kemp, Katherine Heller, Jude Chidubem Omeke, Chidi Asuzu, Naome A. Etori, Aimérou Ndiaye, Ifeoma Okoh, Evans Doe Ocansey, Wendy Kinara, Michael Best, Irfan Essa, Stephen Edward Moore, and Chris Fourie. We would also like to thank Bilal Mateen, Melissa Miles, Mira Emmanuel-Fabula, and Celeste Gonda from the Gates Foundation/PATH Digital Square for their support of the work and all data contributors. Finally, we thank Marian Croak for her leadership and support.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "时间序列基础模型可以成为少样本学习器 (原标题: Time series foundation models can be few-shot learners)",
      "link": "https://research.google/blog/time-series-foundation-models-can-be-few-shot-learners/",
      "pubDate": "Mon, 22 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-22T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 时间序列基础模型可以成为少样本学习器\n\n## 引言：时间序列预测的挑战与少样本学习\n\n现代商业中，时间序列预测至关重要，但传统方法为每个任务构建专业模型，耗时且需大量专业知识。零样本学习（如我们之前的TimesFM模型）提供了一种解决方案，无需特定任务训练即可准确预测。然而，研究发现，少量示例可以进一步提升预测效果，例如利用附近高速公路或同条高速公路的历史数据来预测交通流量。传统的监督微调方法虽然能利用特定数据微调模型，却重新引入了零样本学习试图避免的复杂性。\n\n为了解决这一问题，我们在ICML 2025上提出了新工作“时间序列基础模型的上下文内微调”（In-Context Fine-Tuning for Time-Series Foundation Models），引入了一种新颖的方法，将TimesFM转变为一个少样本学习器。该方法通过持续预训练，教会模型如何在推理时从少量示例中学习。其结果是获得了一种强大的新能力，能够匹配监督微调的性能，而无需用户进行额外的复杂训练。\n\n![LLM少样本提示与时间序列基础模型少样本提示的比较](https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-1-Prompting.width-1250.png)\n\n*   **图片描述**：左侧展示了大型语言模型（LLM）的少样本提示，右侧展示了时间序列基础模型如何通过任意数量的相关上下文时间序列示例进行少样本提示。橙色框内是模型的输入。\n\n## 模型重设计：TimesFM-ICF\n\n为了实现少样本学习，我们对TimesFM模型进行了重新设计，创建了TimesFM-ICF（In-Context Fine-tuning）。\n\n### TimesFM基础架构\n\nTimesFM是一个补丁解码器，其工作原理如下：\n*   将每32个连续时间点（一个补丁）标记化为输入token。\n*   在输入token序列之上应用一个Transformer堆栈来生成输出token。\n*   然后应用一个共享的多层感知器（MLP）将每个输出token转换回128个时间点的时间序列。\n\n### 创建TimesFM-ICF\n\n我们从基础TimesFM模型开始，并使用新的上下文（预测历史加上所有上下文内示例）继续进行预训练。关键挑战在于确保模型不会混淆预测历史和上下文内示例。\n\n*   **引入通用分隔符token**：为了解决混淆问题，我们引入了一个特殊的、可学习的“通用分隔符token”。这个token就像一个数字“停车标志”或“新段落”符号，放置在每组数字之后。当模型关注到它之前看到的示例的分隔符token时，它就不会将其与当前尝试预测的数据混淆。这使得模型能够从过去的示例模式中学习，并将这些知识应用于当前的预测。\n\n![展示如何在上下文示例流中放置通用分隔符token以指定新数据源的示例](https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-2-Separators.width-1250.png)\n\n*   **图片描述**：如果没有分隔符，简单地连接上下文示例可能会使模型混淆，多个单调趋势可能看起来像一个锯齿状的连续模式。\n\n### 持续预训练\n\n由于分隔符token及其注意力机制对TimesFM来说是全新的，我们的第二步是继续预训练基础TimesFM模型，以教授它这些新引入的机制。具体方法是：\n*   创建一个包含上下文内示例和分隔符token的新数据集。\n*   应用标准的仅解码器下一token预测训练。\n*   **架构流程**：输入传递给MLP层生成token，这些token传递给因果自注意力（CSA）层（防止模型“偷看”未来），CSA层再馈入前馈网络（FFN）。CSA和FFN重复多次（即堆叠的Transformer），然后将结果连接到输出MLP层。\n\n## 模型测试与评估\n\n我们对TimesFM-ICF进行了严格评估，使用了23个模型在任何训练阶段都未曾见过的数据集。每个基准数据集都包含多个时间序列。在预测一个时间序列时，我们从其即时历史开始，然后从其完整历史和同一数据集中其他时间序列的历史中采样序列作为上下文内示例，以确保相关性且无数据泄露。\n\n### 性能指标与基线\n\n我们关注以下性能指标和基线模型：\n*   **性能指标**：几何平均（GM）聚合的平均绝对比例误差（MASE），通过朴素重复最后一个季节性模式进行归一化。\n*   **基线模型**：\n    *   **TimesFM (Base)**：我们开始使用的预训练模型。\n    *   **TimesFM-FT**：TimesFM (Base) 经过监督微调，使用每个数据集的训练集进行微调，并在相应测试集上评估。这是一个强大的基线，代表了领域适应的最佳实践。\n\n![条形图比较TimesFM-ICF与TimesFM (Base) 在多个特定任务模型上的性能](https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-4-Performance.width-1250.png)\n\n*   **图片描述**：TimesFM-ICF在许多特定任务模型上改进了TimesFM (Base) 的性能，并达到了与TimesFM-FT相同的性能。TimesFM-FT是针对每个特定数据集分别进行微调的TimesFM版本。\n\n### 关键发现\n\n*   **准确性提升**：TimesFM-ICF比TimesFM (Base) 准确率提高了6.8%。\n*   **匹配监督微调性能**：更令人惊喜和鼓舞的是，TimesFM-ICF在无需监督微调的复杂性下，达到了与TimesFM-FT相同的性能。\n*   **其他特性**：\n    *   随着上下文内示例的增加，模型预测的准确性会提高（尽管推理时间会相应增加），这符合我们的预期。\n    *   与不具备处理上下文内示例能力的纯粹长上下文模型相比，TimesFM-ICF显示出更好的上下文利用率。\n\n## 未来展望：更易用、更强大的预测\n\n这种新方法具有重要的实际应用价值，因为它允许企业部署一个更强大、适应性更强的单一预测模型。企业无需为预测新产品需求等新任务启动一个完整的机器学习项目，只需向模型提供少量相关的示例即可。这能立即提供最先进的专业预测，从而大幅降低成本，加速决策和创新，并使高端预测民主化。\n\n我们对这项研究的未来感到兴奋，特别是开发自动选择最相关上下文内示例的策略。通过使基础模型更智能、更具适应性，我们赋能更多用户做出更好的数据驱动决策。",
      "shortSummary": "新的研究引入了TimesFM-ICF，一个将时间序列基础模型TimesFM转变为少样本学习器的方法。传统预测模型复杂且耗时，而TimesFM-ICF通过在持续预训练中引入“通用分隔符token”，使模型能在推理时从少量上下文示例中学习。这种方法无需复杂的监督微调，即可将预测准确性比基础模型提高6.8%，并达到与监督微调相当的性能。这使得先进的时间序列预测更易于部署和使用，显著降低成本，加速决策，并促进创新。",
      "translated_title": "时间序列基础模型可以成为少样本学习器",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-1-Prompting.width-1250.png",
          "alt": "Comparison of few-shot prompting of an LLM (left) with few-shot prompting of a time-series foundation model using an arbitrary number of related in-context time series examples (right).",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-2-Separators.width-1250.png",
          "alt": "Example showing how common separator tokens would be placed to designate new data sources in a stream of in-context examples.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-4-Performance.width-1250.png",
          "alt": "Bar graph comparing performance of TimesFM-ICF to TimesFM (base) over many task-specific models.",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"exbjr\"><a href=\"https://en.wikipedia.org/wiki/Time_series\" target=\"_blank\" rel=\"noopener noreferrer\">Time-series forecasting</a> is essential for modern businesses, helping them predict everything from inventory needs to energy demands. Traditionally, this has involved building a separate, specialized model for each task — a process that is slow and requires significant expertise.</p><p data-block-key=\"50kh8\">The emergence of <a href=\"https://en.wikipedia.org/wiki/Zero-shot_learning\" target=\"_blank\" rel=\"noopener noreferrer\">zero-shot learning</a> offered a solution. Our previous model, <a href=\"https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/\">TimesFM</a>, was a zero-shot, pre-trained foundation model that could accurately forecast without task-specific training. But what if a few examples could make the forecast even better? For instance, forecasting highway traffic would be more accurate if the model could consider data from other nearby highways or from the same highway a few weeks ago. The standard solution, supervised fine-tuning, which uses curated data to fine-tune an existing model, reintroduces the complexity one hopes to avoid with zero-shot learning.</p><p data-block-key=\"c2v01\">In our new work, \"<a href=\"https://icml.cc/virtual/2025/poster/43707\" target=\"_blank\" rel=\"noopener noreferrer\">In-Context Fine-Tuning for Time-Series Foundation Models</a>\", presented at ICML 2025, we introduce a novel approach that transforms TimesFM into a <a href=\"https://arxiv.org/abs/2203.04291\" target=\"_blank\" rel=\"noopener noreferrer\">few-shot learner</a>. This method uses continued pre-training to teach the model how to learn from a handful of examples at inference time. The result is a powerful new capability that matches the performance of supervised fine-tuning without requiring additional complex training from the user.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-1-Prompting.width-1250.png\" alt=\"Comparison of few-shot prompting of an LLM (left) with few-shot prompting of a time-series foundation model using an arbitrary number of related in-context time series examples (right).\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-1-Prompting.width-1250.png\" alt=\"Comparison of few-shot prompting of an LLM (left) with few-shot prompting of a time-series foundation model using an arbitrary number of related in-context time series examples (right).\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"h2yvf\"><i>Similar to few-shot prompting of an LLM (</i><b><i>left</i></b><i>), a time-series foundation model should support few-shot prompting with an arbitrary number of related in-context time series examples (</i><b><i>right</i></b><i>). The orange box encloses the inputs to the models.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Redesigning the model</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"exbjr\"><a href=\"https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/\">TimesFM</a> is a patched decoder that tokenizes every 32 contiguous timepoints (a patch) as an <a href=\"https://mehmetozkaya.medium.com/how-llms-use-tokens-ec5916ee321a\" target=\"_blank\" rel=\"noopener noreferrer\">input token</a> and applies a <a href=\"https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\" target=\"_blank\" rel=\"noopener noreferrer\">transformer</a> stack on top of the sequence of input tokens to generate the output tokens. It then applies a shared <a href=\"https://en.wikipedia.org/wiki/Multilayer_perceptron\" target=\"_blank\" rel=\"noopener noreferrer\">multilayer perceptron</a> (MLP) to translate each output token back to a time series of 128 timepoints.</p><p data-block-key=\"5q88o\">To create TimesFM-ICF (In-Context Fine-tuning), we start with the base TimesFM model and continue the pre-training with new context: the forecast history plus all in-context examples. The first step is to make sure the model doesn’t confuse or conflate the forecasting history and the in-context examples. Imagine you're giving the model a list of numbers that represent a few different things, maybe sunglasses sales figures from one store, then umbrella sales figures from another. If you just merge all those numbers together, the model might get confused, thinking it's one continuous stream of data. For example, if the first store’s sales were going up and the second store’s sales were going down, the model might incorrectly see it as a single up-and-down pattern, rather than two separate, simple trends.</p><p data-block-key=\"66jf\">To fix this, we put a special, learnable “common separator token” — like a digital \"stop sign\" or a \"new paragraph\" symbol — after each set of numbers. With these separators in place, as soon as the model attends to the separator token of an example it has seen before, it won't mix it up with the data it's currently trying to predict. This theoretically allows the model to learn from patterns in those past examples and apply that knowledge to the current forecast. For instance, the model could learn that \"all the store sales are showing consistent, directional trends lately, so I should predict an upward trend for my new store’s sunscreen sales.\"</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-2-Separators.width-1250.png\" alt=\"Example showing how common separator tokens would be placed to designate new data sources in a stream of in-context examples.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-2-Separators.width-1250.png\" alt=\"Example showing how common separator tokens would be placed to designate new data sources in a stream of in-context examples.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"h2yvf\"><i>Concatenating in-context examples without separators could confuse the model — multiple monotonic trends might look like a jagged, continuous pattern if concatenated naïvely.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"exbjr\">Since the separator tokens and the attention to them are new for TimesFM, our second step involves continuing the pre-training of the base TimesFM model to teach it about the new introductions. The recipe here is actually straightforward: we created a new dataset that includes both in-context examples and separator tokens, and we applied standard decoder-only next-token prediction training. Inputs are passed to the MLP layer, which generates tokens. These are passed to a <a href=\"https://en.wikipedia.org/wiki/Attention_(machine_learning)#Masking\" target=\"_blank\" rel=\"noopener noreferrer\">causal self attention</a> (CSA) layer that \"attends to\" information from previous tokens in the sequence, a step that's crucial in tasks like time-series forecasting as it prevents the model from looking into the future. The CSA then feeds into a <a href=\"https://en.wikipedia.org/wiki/Feedforward_neural_network\" target=\"_blank\" rel=\"noopener noreferrer\">feed-forward network</a> (FFN). We repeat CSA and FFN multiple times (i.e., the <a href=\"https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\" target=\"_blank\" rel=\"noopener noreferrer\">stacked transformers</a>) before connecting the result to the output MLP layer.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Time_Foundation_v3.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"h2yvf\"><i>TimesFM-ICF employs the decoder-only architecture for time-series forecasting with in-context examples. A special common separator token is introduced to disambiguate between the in-context examples and the task history.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Testing the model</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"exbjr\">We evaluated TimesFM-ICF on 23 datasets that the model had never seen during any phase of its training. Each dataset in this benchmark has multiple time series. When we forecast a time series, we start with its immediate history, then sample sequences from its full history and the histories of other time series in the same dataset as in-context examples. This ensures the in-context examples are relevant and there is no leakage.</p><p data-block-key=\"3u0jv\">The chart below shows the <a href=\"https://en.wikipedia.org/wiki/Geometric_mean\" target=\"_blank\" rel=\"noopener noreferrer\">geometric mean</a> (GM) aggregation of the <a href=\"https://en.wikipedia.org/wiki/Mean_absolute_scaled_error\" target=\"_blank\" rel=\"noopener noreferrer\">mean absolute scaled errors</a> (MASE) normalized by a <a href=\"https://en.wikipedia.org/wiki/Forecasting#Seasonality_and_cyclic_behaviour\" target=\"_blank\" rel=\"noopener noreferrer\">naïve repeat of the last seasonal pattern</a>. We focus on two baselines here:</p><ul><li data-block-key=\"dqkkk\">TimesFM (Base), which is the pre-trained model from which we started.</li><li data-block-key=\"8ourm\">TimesFM-FT is TimesFM (Base) with supervised fine-tuning using the train split per dataset and then evaluated on the corresponding test split. This is a strong baseline that reflects the previous best practice for domain adaptation.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-4-Performance.width-1250.png\" alt=\"Bar graph comparing performance of TimesFM-ICF to TimesFM (base) over many task-specific models.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-4-Performance.width-1250.png\" alt=\"Bar graph comparing performance of TimesFM-ICF to TimesFM (base) over many task-specific models.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"h2yvf\"><i>TimesFM-ICF improves the performance of TimesFM (Base) over many task-specific models and achieves the same performance as that of TimesFM-FT, which is a version of TimesFM fine-tuned for each specific dataset, respectively.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"exbjr\">TimesFM-ICF is 6.8% more accurate than TimesFM (Base). What’s more surprising and inspiring is that it matches the performance of TimesFM-FT without the hassle of running supervised fine-tuning.</p><p data-block-key=\"3qdc5\">Besides the accuracy improvement, TimesFM-ICF also demonstrates other desirable properties. For example, it is consistent with our expectation that with more in-context examples, a model will make more accurate forecasts at the cost of longer inference time. In addition, TimesFM-ICF shows better utilization of its context when compared to a purely long-context model that does not have the ability to work with in-context examples.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The future: More accessible and powerful forecasting</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"exbjr\">This new approach has significant real-world applications because it allows businesses to deploy a more robust and adaptable single, powerful forecasting model. Instead of launching a full ML project for new tasks, like forecasting demand for a new product, they can simply feed the model a few new relevant examples. This immediately provides state-of-the-art, specialized forecasts, dramatically cutting costs, accelerating decision-making and innovation, and democratizing access to high-end forecasting.</p><p data-block-key=\"bk79l\">We're excited by this research's future, particularly developing automated strategies for selecting the most relevant in-context examples. By making foundation models more intelligent and adaptable, we empower more users to make better, data-driven decisions.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"exbjr\"><i>This research was led by then-student researcher Matthew Faw in collaboration with Google Research colleagues Abhimanyu Das and Ivan Kuznetsov. This blog post was brought to life with the tremendous help from editors Mark Simborg and Kimberly Schwede.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "测试时扩散深度研究代理 (原标题: Deep researcher with test-time diffusion)",
      "link": "https://research.google/blog/deep-researcher-with-test-time-diffusion/",
      "pubDate": "Thu, 18 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-18T16:00:00.000Z",
      "creator": "Google",
      "summary": "大型语言模型（LLMs）的最新进展推动了深度研究（DR）代理的兴起，这些代理能够生成新想法、检索信息、执行实验并撰写报告。然而，现有DR代理通常将不同工具简单组合，缺乏人类研究中迭代的“规划、起草、研究和基于反馈迭代”过程，尤其是在复杂主题的论文写作中。它们缺少通过研究来完善和加强论点的关键“去噪”步骤。\n\n**TTD-DR：模仿人类研究的深度研究代理**\n\n本文介绍了**测试时扩散深度研究代理（Test-Time Diffusion Deep Researcher, TTD-DR）**，这是首个将研究报告写作建模为扩散过程的研究代理，即从一个“嘈杂”或初步的草稿逐步完善为高质量的最终版本。TTD-DR引入了两种协同工作的新算法：\n\n*   **组件级自进化优化**：提升研究工作流中每个步骤的质量。\n*   **报告级检索去噪细化**：利用新检索到的信息修订和改进报告草稿。\n\nTTD-DR在长篇报告写作和多跳推理任务上取得了最先进的成果。\n\n**TTD-DR 的设计理念**\n\nTTD-DR接收用户查询后，会创建一个初步草稿作为不断演进的基础，以指导研究计划。这个草稿通过“检索去噪”过程（报告级细化）进行迭代完善，将找到的信息用于改进草稿。这个过程在一个连续的循环中进行，每次循环都会改进报告。此外，一个自进化算法不断增强从初始计划到最终报告的整个过程，这种细化和自我改进的强大组合使得报告写作过程更加连贯。\n\n![TTD-DR示意图](https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-1.width-1250.png)\n*TTD-DR示意图。其设计旨在通过迭代的起草和修订周期来模仿典型的研究实践。*\n\n**核心DR设计（三阶段）**\n\nTTD-DR的核心DR设计包含三个阶段：\n\n1.  **研究计划生成**：根据用户查询生成结构化的研究计划，概述最终报告所需的关键领域，作为后续信息收集的初步指导。\n2.  **迭代搜索**：包含两个子代理：\n    *   **搜索问题生成（阶段2a）**：根据研究计划、用户查询和先前搜索迭代的上下文（即过去的问答）制定搜索查询。\n    *   **答案搜索（阶段2b）**：搜索可用来源以查找相关文档并返回总结的答案，类似于检索增强生成（RAG）系统。\n3.  **最终报告生成**：通过结合所有收集到的结构化信息（计划和一系列问答对）生成全面且连贯的最终报告。\n\n![核心DR代理设计图](https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-2.width-1250.png)\n*我们的核心DR代理分三个阶段运行。阶段1生成详细的研究计划；阶段2a迭代生成搜索问题，然后使用类似RAG的系统从检索到的文档中合成精确答案（2b）；阶段3合成所有收集到的信息以生成最终报告。*\n\n**组件级自进化**\n\nTTD-DR利用自进化算法来增强每个阶段代理的性能，以发现并保留高质量的上下文。该过程包括：\n\n*   **初始状态**：基于前一阶段输出的多个多样化答案变体，用于探索更大的搜索空间。\n*   **环境反馈**：每个答案变体由一个作为评判者的LLM进行评估，利用自动评分器评估有用性和全面性等指标，并生成文本反馈以改进答案。\n*   **修订**：根据评分和反馈，每个变体进行修订，以适应更好的评分。环境反馈和修订步骤重复进行，直到达到最大迭代次数或代理确定不再需要修订。\n*   **交叉**：最终，多个修订后的变体合并为一个高质量的输出，为主要报告生成过程提供卓越的上下文。\n\n![组件级自进化算法示意图](https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-3.width-1250.png)\n*组件级自进化算法应用于搜索答案（阶段2b）的示意图。该过程从多个初始答案变体开始，每个变体都经历一个自进化过程，首先与环境交互以获得适应度分数和反馈，然后根据反馈进行修订。这个过程重复进行，直到达到最大迭代次数。最后，来自所有迭代的多个修订后的变体被合并以生成最终答案。*\n\n**报告级检索去噪**\n\nTTD-DR使用搜索工具来去噪和演进草稿。具体而言，当前报告草稿被输入到核心DR工作流的搜索生成阶段（阶段2a），以指导下一个搜索查询的生成。在答案搜索阶段（阶段2b）获得合成答案后，新信息用于修订报告草稿，无论是添加新细节还是验证现有信息。这种将去噪后的报告反馈以生成下一个搜索查询的过程会重复进行。草稿逐步去噪，直到搜索过程结束，此时最终代理根据所有历史搜索答案和修订撰写最终报告（阶段3）。\n\n**实验结果**\n\nTTD-DR在以下基准数据集上进行了评估：\n\n1.  **复杂查询**：需要研究代理生成长篇综合报告（DeepConsult）。\n2.  **多跳查询**：需要广泛搜索和推理才能回答（Humanity's Last Exam [HLE] 和 GAIA，以及HLE-Search的200个子查询）。\n\n与OpenAI Deep Research相比，TTD-DR在所有基准测试中始终取得更好的结果：\n\n*   在长篇研究报告生成任务中，TTD-DR的胜率达到74.5%。\n*   在两个广泛研究数据集（HLE-Search和GAIA）上，分别超越OpenAI DR 7.7%和1.7%。\n\n![TTD-DR性能对比图](https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-4.width-1250.png)\n*TTD-DR在基准数据集上与不同基线系统的性能对比。左图：胜率（%）基于OpenAI DR计算。右图：正确性计算为系统预测答案与参考答案的匹配度。TTD-DR以显著优势优于OpenAI DR。*\n\n**消融研究**\n\n消融研究逐步增加了三种方法（核心DR、自进化、检索去噪）。使用Gemini-2.5-pro作为基础模型：\n\n*   仅核心DR代理表现不如OpenAI DR。\n*   加入自进化算法后，DeepConsult的胜率达到59.8%，HLE-Search和GAIA的正确性分数分别提高4.4%和1.2%。\n*   最终，结合检索去噪带来了所有基准测试的显著提升。\n\n![TTD-DR消融研究图](https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-5.width-1250.png)\n*TTD-DR通过逐步添加1) 核心DR，2) 自进化，和3) 检索去噪的性能。我们观察到全面逐步改进，帮助我们实现新的最先进结果。*\n\n**效率**\n\nTTD-DR在测试时扩展效率方面也优于其他DR代理。在相同的延迟下，TTD-DR实现了更好的质量/胜率。\n\n![质量与延迟的帕累托前沿图](https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-6.width-1250.png)\n*研究报告质量与延迟（秒）的帕累托前沿图。蓝线表示TTD-DR，灰色点表示对比的DR代理。*\n\n**结论**\n\nTTD-DR是一个受人类迭代研究方式启发的新框架，通过将报告生成概念化为扩散过程，解决了现有DR代理的局限性。它在需要密集搜索和多跳推理的各种基准测试中显著优于现有DR代理，并在生成长篇综合研究报告和识别多跳搜索与推理任务的简洁答案方面表现出最先进的性能。其“草稿优先”的设计被认为是其成功的关键。\n\n**可用性**\n\n该工作的产品版本已在Google Agentspace上提供，并使用Google Cloud Agent Development Kit实现。",
      "shortSummary": "TTD-DR（测试时扩散深度研究代理）是一个模仿人类迭代研究过程的新型AI代理。它将报告写作建模为从初步草稿到高质量最终版本的扩散过程，并结合了组件级自进化和报告级检索去噪两种算法。TTD-DR在长篇报告写作和多跳推理任务上取得了最先进的成果，并在多项基准测试中显著优于OpenAI Deep Research等现有代理。其“草稿优先”的设计是其成功的关键。",
      "translated_title": "测试时扩散深度研究代理",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-1.width-1250.png",
          "alt": "Deep-Researcher-1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-2.width-1250.png",
          "alt": "Deep-Researcher-2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-3.width-1250.png",
          "alt": "Deep-Researcher-3",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-4.width-1250.png",
          "alt": "Deep-Researcher-4",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-5.width-1250.png",
          "alt": "Deep-Researcher-5",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ocegg\">The recent advances in large language models (LLMs) have fueled the emergence of <a href=\"https://openai.com/index/introducing-deep-research/\" target=\"_blank\" rel=\"noopener noreferrer\">deep research</a> (DR) agents. These agents demonstrate remarkable capabilities, including the generation of <a href=\"https://arxiv.org/abs/2409.04109\" target=\"_blank\" rel=\"noopener noreferrer\">novel ideas</a>, efficient <a href=\"https://arxiv.org/abs/2503.09516\" target=\"_blank\" rel=\"noopener noreferrer\">information retrieval</a>, experimental execution, and the subsequent drafting of comprehensive <a href=\"https://arxiv.org/pdf/2408.06941\" target=\"_blank\" rel=\"noopener noreferrer\">reports</a> and <a href=\"https://arxiv.org/abs/2504.08066\" target=\"_blank\" rel=\"noopener noreferrer\">academic papers</a>.</p><p data-block-key=\"acuge\">Currently, most <a href=\"https://github.com/assafelovic/gpt-researcher\" target=\"_blank\" rel=\"noopener noreferrer\">public DR agents</a> use a variety of clever techniques to improve their results, like <a href=\"https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/\">performing reasoning via chain-of-thought</a> or <a href=\"https://openreview.net/forum?id=H4S4ETc8c9\" target=\"_blank\" rel=\"noopener noreferrer\">generating multiple answers</a> and selecting the best one. While they've made impressive progress, they often bolt different tools together without considering the iterative nature of human research. They're missing the key process (i.e., planning, drafting, researching, and iterating based on feedback) on which people rely when writing a paper about a complex topic. A key part of that revision process is to do more research to <a href=\"https://www.emerald.com/jd/article-abstract/69/2/243/198951/Patterns-of-graduate-students-information-seeking?redirectedFrom=fulltext\" target=\"_blank\" rel=\"noopener noreferrer\">find missing information or strengthen your arguments</a>. This human pattern is surprisingly similar to the mechanism of <a href=\"https://proceedings.mlr.press/v202/zhang23as/zhang23as.pdf\" target=\"_blank\" rel=\"noopener noreferrer\"><i>retrieval</i></a>-augmented diffusion models that start with a “noisy” or messy output and gradually refine it into a high-quality result. What if an AI agent's rough draft is the noisy version, and a search tool acts as the denoising step that cleans it up with new facts?</p><p data-block-key=\"5njoq\">Today we introduce <a href=\"https://arxiv.org/abs/2507.16075\" target=\"_blank\" rel=\"noopener noreferrer\">Test-Time Diffusion Deep Researcher</a> (TTD-DR), a DR agent that imitates the way humans do research. To our knowledge, TTD-DR is the first research agent that models research report writing as a diffusion process, where a messy first draft is gradually polished into a high-quality final version. We introduce two new algorithms that work together to enable TTD-DR. First, component-wise optimization via <a href=\"https://arxiv.org/abs/2501.09891\" target=\"_blank\" rel=\"noopener noreferrer\">self-evolution</a> enhances the quality of each step in the research workflow. Then, report-level refinement via <a href=\"https://arxiv.org/abs/2302.02285\" target=\"_blank\" rel=\"noopener noreferrer\">denoising with retrieval</a> applies newly retrieved information to revise and improve the report draft. We demonstrate that TTD-DR achieves state-of-the-art results on long-form report writing and multi-hop reasoning tasks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Test-Time Diffusion Deep Researcher</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">TTD-DR is designed to take a user query as input and then create a preliminary draft that serves as an evolving foundation to guide the research plan. This evolving draft is iteratively refined using a denoising with retrieval process (report-level refinement) that takes the information it finds and uses it to improve the draft at each step. This happens in a continuous loop that improves the report with each cycle. To top it all off, a self-evolution algorithm constantly enhances the entire process, from the initial plan to the final report. This powerful combination of refinement and self-improvement leads to a more coherent report writing process.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-1.width-1250.png\" alt=\"Deep-Researcher-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-1.width-1250.png\" alt=\"Deep-Researcher-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d0pys\"><i>Illustration of TTD-DR. We designed it to imitate typical research practices by performing iterative cycles of drafting and revision.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Backbone DR design</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">The backbone DR design consists of three stages that we outline below.</p><ol><li data-block-key=\"40uvv\"><b>Research plan generation:</b> Produces a structured research plan upon receiving a user query. This plan outlines a list of key areas needed for the final report, serving as an initial guideline for the subsequent information-gathering process.</li><li data-block-key=\"9c90d\"><b>Iterative search:</b> Contains two sub-agents: Search Question Generation (stage 2a in the figure below) formulates a search query based on the research plan, the user query, and the context from previous search iterations (i.e., past questions and answers). Answer Searching (stage 2b) searches the available sources to find relevant documents and returns a summarized answer, similar to <a href=\"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\" target=\"_blank\" rel=\"noopener noreferrer\">retrieval-augmented generation</a> (RAG) systems.</li><li data-block-key=\"dambo\"><b>Final report generation:</b> Produces a comprehensive and coherent final report by combining all the structured information gathered, that is, the plan and the series of question-answer pairs.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-2.width-1250.png\" alt=\"Deep-Researcher-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-2.width-1250.png\" alt=\"Deep-Researcher-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d0pys\"><i>Our backbone DR agent operates in three stages. Stage 1 generates a detailed research plan; Stage 2a iteratively generates search questions and then uses a RAG-like system to synthesize precise answers from retrieved documents (2b); Stage 3 synthesizes all gathered information to produce the final report.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Component-wise self-evolution</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">We leverage a self-evolutionary algorithm to enhance the performance of each stage's agents in order to <i>find</i> and <i>preserve</i> the high quality context.</p><ul><li data-block-key=\"e6eh6\"><b>Initial states:</b> The leftmost blocks in the diagram below represent multiple diverse answer variants based on the output of previous stages, which are used to explore a larger search space. This ideally leads to discovery of more valuable information.</li><li data-block-key=\"c8sei\"><b>Environmental feedback:</b> Each answer variant is assessed by an LLM-as-a-judge, utilizing auto-raters for metrics, such as helpfulness and comprehensiveness. These raters not only provide fitness scores but also generate textual feedback that help improve the answer.</li><li data-block-key=\"9d9u5\"><b>Revision:</b> With the scores and feedback from the previous step, each variant undergoes a revision step to adapt toward better fitness scores. The environmental feedback and revision steps repeat until reaching some maximum number of iterations or until the agent determines no more revisions are needed.</li><li data-block-key=\"fh789\"><b>Cross-over:</b> Finally, multiple revised variants are merged into a single, high-quality output. This merging process consolidates the best information from all evolutionary paths, producing superior context for the main report generation process.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-3.width-1250.png\" alt=\"Deep-Researcher-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-3.width-1250.png\" alt=\"Deep-Researcher-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d0pys\"><i>Illustration of the component-wise self-evolution algorithm applied to Search Answer (Stage 2b). The process starts with multiple variants of initial answers, each undergoing a self-evolving episode where it first interacts with the environment to obtain a fitness score and feedback. It is then revised based on the feedback. This process repeats until the maximum number of iterations is reached. Finally, multiple revised variants from all episodes are merged to produce the final answer.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Report-level denoising with retrieval</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">Since a preliminary noisy draft is useless for complex topics without real research, TTD-DR uses a search tool that denoises and evolves the draft.</p><p data-block-key=\"2cj8q\">Specifically, we feed the current draft report into the Search Generation stage (Stage 2a) of the backbone DR workflow to inform the generation of the next search query. After obtaining a synthesized answer in the Answer Searching stage (Stage 2b), the new information is used to revise the report draft, either by adding new details or by verifying existing information. This process of feeding the denoised report back to generate the next search query is repeated. The draft is progressively denoised until the search process concludes, at which point a final agent writes the final report based on all historical search answers and revisions (Stage 3).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">We evaluate TTD-DR's performance using benchmark datasets that focus on two broad tasks: 1) Complex queries that require research agents to produce a long-form comprehensive report (<a href=\"https://github.com/Su-Sea/ydc-deep-research-evals\" target=\"_blank\" rel=\"noopener noreferrer\">DeepConsult</a>) and, 2) multi-hop queries that require extensive search and reasoning to answer (<a href=\"https://scale.com/leaderboard/humanitys_last_exam\" target=\"_blank\" rel=\"noopener noreferrer\">Humanity's Last Exam</a> [HLE] and <a href=\"https://huggingface.co/datasets/gaia-benchmark/GAIA\" target=\"_blank\" rel=\"noopener noreferrer\">GAIA</a>). We sub-sample 200 queries from HLE that need more search and reasoning (HLE-Search). Both categories fit into our objective of building a general-purpose, real-world research companion. We compare our DR systems with <a href=\"https://openai.com/index/introducing-deep-research/\" target=\"_blank\" rel=\"noopener noreferrer\">OpenAI Deep Research</a>.</p><p data-block-key=\"b0lhd\">TTD-DR consistently achieves better results across all benchmarks. Notably, when compared to OpenAI DR, TTD-DR achieves 74.5% win rate for the <i>long-form</i> research report generation tasks. Additionally, it outperforms OpenAI DR by 7.7% and 1.7% on the two extensive research datasets with <i>short-form</i> ground-truth answers.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-4.width-1250.png\" alt=\"Deep-Researcher-4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-4.width-1250.png\" alt=\"Deep-Researcher-4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d0pys\"><i>TTD-DR's performance against different baseline systems for benchmark datasets.</i> <b><i>Left</i></b><i>: Win rates (%) are computed based on OpenAI DR.</i> <b><i>Right</i></b><i>: Correctness is computed as matching between system predicted and reference answers. TTD-DR outperforms OpenAI DR with significant margins.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Ablation study</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">For the ablation study, we incrementally add the three methods in the section above. Our DR agents use <a href=\"https://arxiv.org/abs/2507.06261\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini-2.5-pro</a> as the base model. All other baseline agents use their default LLMs. The charts below show the ablation study for our DR agents. The backbone DR agent underperforms OpenAI DR. With the addition of the proposed self-evolution algorithm, we observe that for DeepConsult, our system outperforms OpenAI Deep Research with 59.8% win rates. The Correctness scores on HLE-Search and GAIA datasets also show an improvement of 4.4% and 1.2%. Finally, incorporating diffusion with retrieval leads to substantial gains across all benchmarks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-5.width-1250.png\" alt=\"Deep-Researcher-5\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-5.width-1250.png\" alt=\"Deep-Researcher-5\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d0pys\"><i>TTD-DR's performance by incrementally adding 1) backbone DR, 2) self-evolution, and 3) diffusion with retrieval. We observe step-by-step improvements across the board that help us achieve new state-of-the-art results.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ocegg\">The <a href=\"https://en.wikipedia.org/wiki/Pareto_front\" target=\"_blank\" rel=\"noopener noreferrer\">Pareto-frontier diagram</a> below further shows the test-time scaling efficiency of TTD-DR compared with other DR agents. We found that TTD-DR is more efficient than OpenAI DR, as with the same latency, it achieves the better quality per win-rate. See the <a href=\"https://arxiv.org/abs/2507.16075\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a> for more details.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-6.width-1250.png\" alt=\"Deep-Researcher-6\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-6.width-1250.png\" alt=\"Deep-Researcher-6\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d0pys\"><i>Pareto-frontier of research report quality vs. latency in seconds. The blue line indicates TTD-DR, whereas grey dots indicate compared DR agents.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">The Deep Researcher with Test-Time Diffusion (TTD-DR) is a new framework inspired by the iterative way humans do research. This agent addresses the limitations of existing DR agents by conceptualizing report generation as a diffusion process. The TTD-DR framework significantly outperforms existing DR agents across various benchmarks requiring intensive search and multi-hop reasoning. It demonstrates state-of-the-art performance in generating comprehensive long-form research reports and identifying concise answers for multi-hop search and reasoning tasks. We believe the reason it works so well is its \"draft-first\" design, which keeps the whole research process focused and coherent, preventing important information from getting lost along the way.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Availability on Google Cloud Platform</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">A product version of this work is available on <a href=\"https://cloud.google.com/agentspace/docs/research-assistant\" target=\"_blank\" rel=\"noopener noreferrer\">Google Agentspace</a>, implemented with Google Cloud <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/agent-development-kit/quickstart\" target=\"_blank\" rel=\"noopener noreferrer\">Agent Development Kit</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\"><i>This research was conducted by Rujun Han, Yanfei Chen, Guan Sun, Lesly Miculicich, Zoey CuiZhu, Yuanjun (Sophia) Bi, Weiming Wen, Hui Wan, Chunfeng Wen, Solène Maître, George Lee, Vishy Tirumalashetty, Xiaowei Li, Emily Xue, Zizhao Zhang, Salem Haykal, Burak Gokturk, Tomas Pfister, and Chen-Yu Lee.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Sensible Agent: 一种用于与主动式AR智能体进行无侵扰交互的框架 (原标题: Sensible Agent: A framework for unobtrusive interaction with proactive AR agents)",
      "link": "https://research.google/blog/sensible-agent-a-framework-for-unobtrusive-interaction-with-proactive-ar-agents/",
      "pubDate": "Wed, 17 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-17T16:00:00.000Z",
      "creator": "Google",
      "summary": "# Sensible Agent：无侵扰式主动AR智能体交互框架\n\n## 引言与背景\nGoogle的Project Astra等最新创新展示了嵌入增强现实（AR）眼镜中的主动式智能体在预测用户需求并无缝融入日常生活方面的巨大潜力。然而，当前的智能体主要依赖用户明确的口头命令，这在社交环境中可能显得尴尬或具有破坏性，在时间敏感的场景中会增加认知负担，或者根本不切实际。\n\n## Sensible Agent 框架介绍\n为了解决这些挑战，我们推出了 **Sensible Agent**，该框架已在UIST 2025上发表，旨在实现与主动式AR智能体进行无侵扰的交互。Sensible Agent 通过预测用户意图并确定最佳的辅助方式来重塑这种交互。它利用实时多模态上下文感知、微妙的手势、凝视输入和最少的视觉提示来提供无侵扰、上下文适宜的帮助。这标志着向真正集成、具有社交意识的AR系统迈出了关键一步，这些系统尊重用户上下文，最大程度地减少认知干扰，并使主动式数字辅助在日常生活中变得实用。\n\n## 核心模块\nSensible Agent 框架的核心由两个相互关联的模块组成：\n1.  **理解“协助什么”：** 利用先进的多模态感知技术（如以用户为中心的摄像头和环境上下文检测）来理解用户当前的辅助需求，并主动决定最有效的行动（例如，提供翻译、推荐菜肴或显示购物清单）。\n2.  **确定“如何”提供协助：** 根据社交上下文智能地选择侵扰性最小、最合适的交互方法。例如，如果用户双手正忙，智能体可能会通过点头来启用确认；在嘈杂环境中，则可能显示视觉图标而非语音。\n\n![Sensible Agent 演示](https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-3.width-1250.png)\n*Sensible Agent 演示：AR智能体（左）检测上下文，（中）主动建议行动，（右）允许用户通过“竖起大拇指”手势进行无侵扰响应。*\n\n## Sensible Agent 原型构建\n为了将这一概念变为现实，我们实现了 Sensible Agent 的一个功能齐全的原型，该原型运行在 Android XR 和 WebXR 上，并集成了强大的多模态AI模型。该原型包含四个组件：\n1.  **上下文解析器：** 使用视觉语言模型（VLM）和音频事件分类器YAMNet分析摄像头输入和环境噪音，以理解用户情境（如活动、位置、噪音水平）。\n2.  **主动查询生成器：** 根据解析后的上下文，使用思维链（CoT）推理和少样本学习识别最有用的行动，并输出完整的智能体建议（包括行动、查询格式和呈现模态）。\n3.  **交互模块：** 管理输出（通过UI管理器呈现视觉面板或TTS音频）和输入（根据上下文激活头部手势、手部手势、口头命令或凝视等响应方式）。\n4.  **响应生成器：** 一旦用户选择选项，使用大型语言模型（LLM）生成自然语言答案，并通过TTS转换为音频播放给用户。\n\n![Sensible Agent 系统架构](https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-1.width-1250.png)\n*Sensible Agent 原型系统架构。整个系统在WebXR中实现，并运行在Android XR头戴设备上。*\n\n## 用户研究\n我们进行了一项结构化的用户研究，将 Sensible Agent 与模仿Project Astra的传统语音控制AR助手进行比较，以评估其在减少交互努力和干扰方面的表现。\n*   **参与者：** 10名参与者，每人使用Android XR头戴设备完成了12个真实场景（通过360°视频或物理搭建的AR环境呈现）。\n*   **活动类型：** 涵盖阅读餐厅菜单、公共交通通勤、杂货购物、参观博物馆、健身房锻炼和厨房烹饪六种日常活动。\n*   **研究条件：**\n    *   **基线：** 语音控制，用户明确发出命令。\n    *   **Sensible Agent：** 主动提供上下文适应的建议，使用侵扰性最小的方法（如视觉图标、音频提示和基于手势的交互）。\n*   **测量指标：** NASA任务负荷指数（NASA-TLX，测量认知负荷）、系统可用性量表（SUS）、用户偏好（7点李克特量表）和总交互时间。\n\n![用户研究参与者](https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-2.width-1250.png)\n*用户研究参与者在360度视频或视频透视（VST）AR中体验了一系列场景，包括基线和Sensible Agent两种条件。*\n\n## 研究结果\n*   **认知负荷：** Sensible Agent 显著降低了心理需求（平均21.1 vs 65.0，p < .001）和感知努力（p = .0039）。\n*   **可用性：** 两个系统表现良好，SUS分数无显著差异（p = .11）。\n*   **用户偏好：** 参与者对 Sensible Agent 表现出强烈且统计学上显著的偏好（平均6.0 vs 3.8，p = .0074）。\n*   **交互时间：** 基线系统更快（μ = 16.4s），Sensible Agent 较慢（μ = 28.5s），这是其两步交互流程的预期权衡，但用户偏好表明这种权衡在需要谨慎和最小用户努力的社交环境中是可接受的。\n\n![定量研究结果](https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-4.width-1250.png)\n*用户研究中测量的定量结果：（a）交互时间，（b）SUS分数，（c）偏好，以及（d）原始NASA TLX分数。统计显著性用∗、∗∗或∗∗∗标注（分别代表𝑝 < .05、𝑝 < .01和𝑝 < .001）。*\n\n关键见解是，主动性不仅减少了努力，还重塑了用户与智能体之间的关系。参与者觉得 Sensible Agent 更像是一个协作伙伴，其微妙的非语言输入模仿了社交线索，使交互感觉更自然。这表明交互的“如何”与“什么”同样重要，才能使智能体感觉像一个积极参与的助手。\n\n## 结论与未来方向\n本研究表明，通过共同推理“建议什么”和“如何提供”，主动式AR辅助可以变得既智能又无侵扰。通过将多模态感知和实时适应集成到决策和界面设计中，我们的框架解决了人机智能体交互中长期存在的摩擦。\n\n展望未来，这项研究可以通过整合长期历史以支持个性化、将系统扩展到跨设备和环境工作，以及探索在智能家居和物理机器人中的应用来扩展到实际应用，同时通过设备端推理确保用户和用户数据的安全。随着AR日益融入日常生活，像 Sensible Agent 这样的系统为高效、周到地支持用户的数字智能体奠定了基础。",
      "shortSummary": "Sensible Agent 是一种创新的框架，旨在解决当前AR智能体过度依赖口头命令带来的社交尴尬和认知负担。该框架通过实时多模态感知、预测用户意图，并智能选择最无侵扰的交互方式（如手势、凝视、视觉提示），提供上下文适宜的帮助。用户研究表明，Sensible Agent 显著降低了认知负荷，提升了用户偏好，使AR辅助更像协作伙伴。它为构建更自然、社交感知强且融入日常生活的AR系统奠定了基础。",
      "translated_title": "Sensible Agent: 一种用于与主动式AR智能体进行无侵扰交互的框架",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-3.width-1250.png",
          "alt": "Sensible- Agent-3",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-1.width-1250.png",
          "alt": "Sensible- Agent-1",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-2.width-1250.png",
          "alt": "Sensible- Agent-2",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-4.width-1250.png",
          "alt": "Sensible- Agent-4",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3kier\">Recent innovations, such as <a href=\"https://deepmind.google/models/project-astra/\" target=\"_blank\" rel=\"noopener noreferrer\">Google's Project Astra</a>, exemplify the potential of proactive agents embedded in augmented reality (AR) glasses to offer intelligent assistance that anticipates user needs and seamlessly integrates into everyday life. These agents promise remarkable convenience, from effortlessly navigating unfamiliar transit hubs to discreetly offering timely suggestions in crowded spaces. Yet, today’s agents remain constrained by a significant limitation: they predominantly rely on explicit verbal commands from users. This requirement can be awkward or disruptive in social environments, cognitively taxing in time-sensitive scenarios, or simply impractical.<br></p><p data-block-key=\"7bab5\">To address these challenges, we introduce <a href=\"https://research.google/pubs/sensible-agent-a-framework-for-unobtrusive-interaction-with-proactive-ar-agent/\">Sensible Agent</a>, published at <a href=\"https://uist.acm.org/2025/\" target=\"_blank\" rel=\"noopener noreferrer\">UIST 2025</a>, a framework designed for unobtrusive interaction with proactive AR agents. Sensible Agent is an advancement to our prior research in <a href=\"https://research.google/blog/human-io-detecting-situational-impairments-with-large-language-models/\">Human I/O</a> and fundamentally reshapes this interaction by anticipating user intentions and determining the best approach to deliver assistance. It leverages real-time multimodal context sensing, subtle gestures, gaze input, and minimal visual cues to offer unobtrusive, contextually-appropriate assistance. This marks a crucial step toward truly integrated, socially aware AR systems that respect user context, minimize cognitive disruption, and make proactive digital assistance practical for daily life.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"nR3VSpvQwvo\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=nR3VSpvQwvo\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Sensible Agent framework</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3kier\">At its core, Sensible Agent consists of two interconnected modules for (1) understanding \"what\" to assist with, and (2) determining \"how\" to provide assistance. First, Sensible Agent leverages advanced multimodal sensing using <a href=\"https://en.wikipedia.org/wiki/Egocentric_vision\" target=\"_blank\" rel=\"noopener noreferrer\">egocentric cameras</a> and environmental context detection to understand a user’s current assistance needs. Whether you're navigating a crowded museum or rushing through a grocery store, the agent proactively decides the most helpful action, such as providing quick translations, suggesting popular dishes at a new restaurant, or quietly displaying a grocery list.</p><p data-block-key=\"f6g3i\">Equally important, Sensible Agent intelligently chooses the least intrusive and most appropriate interaction method based on social context. For instance, if your hands are busy cooking, the agent might enable confirmation via a head nod. In a noisy environment, it might discreetly show visual icons instead of speaking out loud. This adaptive modality selection ensures assistance is always conveniently delivered while avoiding significant disruptions.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-3.width-1250.png\" alt=\"Sensible- Agent-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-3.width-1250.png\" alt=\"Sensible- Agent-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"2l2ni\"><i>Sensible Agent Demo: The AR agent (</i><b><i>left</i></b><i>) detects context, (</i><b><i>middle</i></b><i>) proactively suggests actions, and (</i><b><i>right</i></b><i>) allows users to respond unobtrusively with a “thumbs up” gesture.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Building the Sensible Agent prototype</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3kier\">To bring this concept to life, we implemented Sensible Agent as a fully functional prototype running on <a href=\"https://www.android.com/xr/\" target=\"_blank\" rel=\"noopener noreferrer\">Android XR</a> and <a href=\"https://immersiveweb.dev/\" target=\"_blank\" rel=\"noopener noreferrer\">WebXR</a>, integrated with powerful multimodal AI models. The prototype includes four components: (1) a context parser that enables it to understand the scene, (2) a proactive query generator that determines what assistance is needed, (3) an interaction module that decides how to best offer assistance, and (4) a response generator that delivers the assistance.</p><ul><li data-block-key=\"5l6ln\"><b>Context parser: Understanding the scene</b><ul><li data-block-key=\"53dl0\">First, the system initiates a context parser to understand the user's current situation. The context parser uses a vision-language model (VLM) to analyze the input frame from the headset’s camera and <a href=\"https://ai.google.dev/edge/mediapipe/solutions/audio/audio_classifier\" target=\"_blank\" rel=\"noopener noreferrer\">YAMNet</a>, a pre-trained audio event classifier, to process the noise level in the environment. This process results in a set of parsed contexts, such as high-level activity or the user’s location.</li></ul></li><li data-block-key=\"8ffr9\"><b>Proactive query generator: Deciding “what” to do</b><ul><li data-block-key=\"3em59\">Based on the parsed context, the proactive query generator identifies the most helpful action. It uses <a href=\"https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/\">chain-of-thought</a> (CoT) reasoning to prompt the model to decompose multi-step problems into intermediate steps. This reasoning is guided by six examples derived from a data collection study (few-shot learning).</li><li data-block-key=\"943gk\">The model's output is a complete agent suggestion, including the action (e.g., <i>Recommend Dish</i>), the query format (<i>Multi-choice/Binary Choice/Icon</i>), and the presentation modality (<i>Audio Only</i>/<i>Visual Only/Both</i>).</li></ul></li><li data-block-key=\"andj8\"><b>Interaction module: Deciding “how” to interact</b><ul><li data-block-key=\"cov0d\">This module handles the “how” of the interaction, managing both output and input.</li><li data-block-key=\"8ttbl\">The UI Manager takes the suggestion and presents it to the user. It either renders a visual panel on the screen or uses <a href=\"https://en.wikipedia.org/wiki/Speech_synthesis\" target=\"_blank\" rel=\"noopener noreferrer\">text-to-speech</a> (TTS) to generate an audio prompt.</li><li data-block-key=\"3f4le\">The input modality manager then enables the most appropriate ways for the user to respond. Based on the initial context (e.g., hands are busy, environment is loud), it activates one or more modalities, including head gestures, hand gestures, verbal commands, or gaze.</li></ul></li><li data-block-key=\"7kgh7\"><b>Response generator: Delivering the assistance</b><ul><li data-block-key=\"7brgq\">Once the user selects an option (e.g., with a nod of the head), the Response Generator completes the task. It uses an LLM to formulate a helpful, natural language answer, which is then converted to audio via TTS and played to the user.</li></ul></li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-1.width-1250.png\" alt=\"Sensible- Agent-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-1.width-1250.png\" alt=\"Sensible- Agent-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"2l2ni\"><i>System architecture of Sensible Agent prototype. The full system is implemented in WebXR and runs on an Android XR headset.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">User study</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3kier\">To evaluate Sensible Agent’s performance, we conducted a structured user study comparing it with a conventional, voice-controlled AR assistant modeled after <a href=\"https://deepmind.google/models/project-astra/\" target=\"_blank\" rel=\"noopener noreferrer\">Project Astra</a>. The goal was simple: determine whether Sensible Agent could reduce interaction effort and disruption while maintaining usability and comfort in realistic everyday scenarios.</p><p data-block-key=\"3f8cd\">The study involved 10 participants, each completing 12 realistic scenarios using an Android XR headset. To simulate realistic AR use, these scenarios were presented either as: (1) 360° immersive videos for scenarios involving public transport, restaurant dining, and grocery shopping, or (2) physically staged AR environments for museum visits, exercising, and cooking tasks. The scenarios were set across the following six everyday activities:</p><ul><li data-block-key=\"566om\">Reading a restaurant menu</li><li data-block-key=\"7sdgg\">Commuting via public transport</li><li data-block-key=\"8ffu0\">Grocery shopping</li><li data-block-key=\"s203\">Visiting a museum</li><li data-block-key=\"3jekd\">Working out at a gym</li><li data-block-key=\"cg0jn\">Cooking in a kitchen</li></ul><p data-block-key=\"bc6vo\">Participants experienced each scenario in two conditions:</p><ul><li data-block-key=\"drve3\"><b>Baseline (using a voice-controlled assistant):</b> Users explicitly initiated interactions via voice commands (e.g., \"What's the vegetarian option?\" or \"Tell me about this exhibit\").</li><li data-block-key=\"36ivs\"><b>Sensible Agent:</b> The system proactively offered context-adapted suggestions using minimally intrusive methods, including visual icons, subtle audio cues, and gesture-based interactions (e.g., head nods, gaze).</li></ul><p data-block-key=\"8oamu\">Participants experienced all scenarios sequentially, alternating between unfamiliar contexts (first-time scenarios) and more familiar or contextually constrained variants (e.g., high cognitive load, hands occupied). To ensure a naturalistic flow, scenarios were interleaved to avoid repetition of similar tasks back-to-back.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-2.width-1250.png\" alt=\"Sensible- Agent-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-2.width-1250.png\" alt=\"Sensible- Agent-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"2l2ni\"><i>User study participants either experienced a set of scenarios in 360 videos or</i> <a href=\"https://en.wikipedia.org/wiki/See-through_display\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Video See-Through</i></a><i> (VST) AR, both with the baseline and Sensible Agent.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Results</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3kier\">We compared Sensible Agent to a conventional, voice-controlled AR assistant baseline. We measured cognitive load using the <a href=\"https://en.wikipedia.org/wiki/NASA-TLX\" target=\"_blank\" rel=\"noopener noreferrer\">NASA Task Load Index</a> (NASA-TLX), overall usability with the <a href=\"https://en.wikipedia.org/wiki/System_usability_scale\" target=\"_blank\" rel=\"noopener noreferrer\">System Usability Scale</a> (SUS), user preference on a 7-point <a href=\"https://en.wikipedia.org/wiki/Likert_scale\" target=\"_blank\" rel=\"noopener noreferrer\">Likert scale</a>, and total interaction time.</p><p data-block-key=\"7qoge\">The most significant finding was the reduction in cognitive workload. The NASA-TLX data showed that on a 100-point scale for mental demand, the average score for Sensible Agent was 21.1, compared to 65.0 for the baseline with a statistically significant difference (<i>𝑝</i> &lt; .001). We saw a similar significant reduction in perceived effort (<i>𝑝</i> = .0039), which suggests that the proactive system successfully offloaded the mental work of forming a query.</p><p data-block-key=\"eo7vj\">Regarding usability, both systems performed well, with no statistically significant difference between their SUS scores (<i>𝑝</i> = .11). However, participants expressed a strong and statistically significant preference for Sensible Agent (<i>𝑝</i> = .0074). On a 7-point scale, the average preference rating was 6.0 for Sensible Agent, compared to 3.8 for the baseline.</p><p data-block-key=\"1r4n5\">For the interaction time, logged from the moment a prompt was triggered to the final system response to the user's input, the baseline was faster (<i>μ</i> = 16.4s) compared to Sensible Agent (<i>μ</i> = 28.5s). This difference is an expected trade-off of the system’s two-step interaction flow, where the agent first proposes an action and the user then confirms it. The strong user preference for Sensible Agent suggests this trade-off was acceptable, particularly in social contexts where discretion and minimal user effort were important.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-4.width-1250.png\" alt=\"Sensible- Agent-4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-4.width-1250.png\" alt=\"Sensible- Agent-4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"2l2ni\"><i>Quantitative results of (</i><b><i>a</i></b><i>) interaction time, (</i><b><i>b</i></b><i>) SUS scores, (</i><b><i>c</i></b><i>) preference, and (</i><b><i>d</i></b><i>) Raw NASA TLX scores measured in our user study.</i> <i>The statistical significance is annotated with ∗, ∗∗, or ∗∗∗</i> <i>(representing 𝑝 &lt; .05, 𝑝 &lt; .01, and 𝑝 &lt; .001, respectively).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3kier\">A key insight is that proactivity does more than reduce effort; it reshapes the user's relationship with the agent. Participants felt Sensible Agent was less like a tool and more like a collaborative partner. Its subtle, non-verbal inputs mirrored social cues, fostering rapport and making interactions feel more natural, which suggests the <i>how</i> of an interaction is as important as the <i>what</i> in making an agent feel like an engaged assistant.</p><p data-block-key=\"92mci\">This shift in perception was especially pronounced in high-pressure or socially-engaged environments. Our findings reinforce that relevance alone is not enough; effective agents must align their communication modality with user availability, attentional state, and social context.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion and future directions</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3kier\">In this research, we demonstrated that proactive AR assistance can be made both intelligent and unobtrusive by jointly reasoning over what to suggest and how to deliver it. By integrating multimodal sensing and real-time adaptation into both decision-making and interface design, our framework addresses longstanding friction in human-agent interaction.</p><p data-block-key=\"8b4rf\">Looking ahead, this research can be expanded to real-life applications by integrating longer-term history to support personalization over time, scaling the system to work across devices and environments, and exploring applications in smart homes and physical robotics, while keeping users and user data safe with on-device inference. As AR becomes increasingly embedded in everyday life, systems like Sensible Agent lay the groundwork for digital agents that efficiently and attentively support users.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3kier\"><i>This work is a joint collaboration across multiple teams at Google. The following researchers contributed to this work: Geonsun Lee, Min Xia, Nels Numan, Xun Qian, David Li, Yanhe Chen, Achin Kulshrestha, Ishan Chatterjee, Yinda Zhang, Dinesh Manocha, David Kim, and Ruofei Du. We would like to thank Zhongyi Zhou, Vikas Bahirwani, Jessica Bo, Zheng Xu, Renhao Liu for their feedback and discussion on our early-stage proposal. We thank Alex Olwal, Adarsh Kowdle, and Guru Somadder for the strategic guidance and thoughtful reviews.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "通过利用LLM的所有层来提高其准确性 (原标题: Making LLMs more accurate by using all of their layers)",
      "link": "https://research.google/blog/making-llms-more-accurate-by-using-all-of-their-layers/",
      "pubDate": "Tue, 16 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-16T16:00:00.000Z",
      "creator": "Google",
      "summary": "# SLED：通过利用LLM的所有层提高其准确性\n\n## 引言：LLM的幻觉问题\n*   **LLM的进展与挑战**：大型语言模型（LLM）近年来取得了显著突破，但仍面临“幻觉”问题，即自信地生成不正确的信息。\n*   **幻觉的成因**：包括不完整、不准确或有偏见的训练数据；过拟合或欠拟合；缺乏现实世界经验；以及模糊的问题。\n*   **影响**：这些因素共同损害了LLM在实际应用中的可靠性和可信度。\n*   **事实性**：指LLM生成与现实世界知识一致内容的能力。\n*   **传统改进方法及其局限**：通常通过使用外部数据（如检索增强生成RAG）来提高事实性，但这需要更复杂的系统，且LLM仍可能出现幻觉。\n*   **潜在解决方案**：解码过程，即LLM文本生成的最后一步，是减轻幻觉的潜在目标。\n\n## 介绍SLED（Self Logits Evolution Decoding）\n*   **NeurIPS 2024亮点**：SLED是一种新颖的解码方法，旨在使LLM输出与事实知识对齐。\n*   **核心创新**：SLED改变了LLM生成文本的方式，它利用了LLM的*所有层*，而不仅仅是最后一层，以更好地使模型输出与现实世界事实对齐。\n*   **主要优势**：\n    *   无需外部知识库。\n    *   无需数据微调。\n*   **实验结果**：在多种LLM配置和规模上进行了广泛实验，结果表明SLED在多项任务和基准测试（包括多项选择、开放式生成和思维链推理任务）中持续提高了事实准确性。\n*   **灵活性**：SLED可以与其他事实性解码方法灵活集成，进一步减少模型幻觉。\n*   **代码可用性**：SLED的代码已在GitHub仓库中提供。\n\n## SLED的工作原理\n*   **LLM的文本生成过程**：LLM将句子分解为“tokens”，并一次生成一个token。在每一步，LLM会计算每个可能token的概率分布。\n*   **传统LLM的局限**：LLM通过多层处理文本，并在每一层生成“logits”（预测分数），通常只依赖最后一层的logits来确定输出。中间层的“提前退出”logits提供了额外信息，但标准LLM往往忽略它们，可能导致因错过上下文线索而选择不正确但“流行”的答案。\n*   **SLED的改进**：\n    *   SLED利用LLM所有层的信息，而不仅仅是最后一层。\n    *   它通过在Transformer架构中重用最终投影矩阵，将“提前退出”logits转换为与最终层相同的可能token集合上的概率分布。\n    *   这意味着SLED从每一层获得对下一个token的多个估计。\n    *   SLED对所有层的分布进行*加权平均*，赋予某些层更高的重要性，从而通过整合其处理过程不同阶段的信息来完善LLM的预测。\n\n## 示例说明\n*   **示例1：不列颠哥伦比亚省的首府**\n    *   当LLM被问及“不列颠哥伦比亚省的首府是什么？”时，SLED会给正确答案“维多利亚”分配更高的概率，而给流行答案“温哥华”分配更低的概率。\n*   **示例2：数学应用题（折扣计算）**\n    *   问题：“Ash去商店买了6个玩具。每个玩具10代币。购买四个或更多可享10%折扣。Ash支付多少？”\n    *   **典型LLM的错误**：可能会错误地预测“6 x 10 = 60”，忽略了10%的折扣。这可能源于训练数据中常见的“A x B = C”算术模式。\n    *   **SLED的纠正**：SLED通过利用所有层的信息进行干预。分析“提前退出”logits发现，许多中间层在“6 x 10”之后实际上预测的是“x”而不是“=”。这种细微的差异引导模型纳入折扣，得出正确的计算：“6 x 10 x 0.9 = 54”。\n    *   SLED识别出虽然“=”可能看起来是基于常见模式最可能的token，但“x”与早期层获取的信息更吻合，最终引导模型得出准确答案。\n\n## 实验与结果\n*   **测试范围**：SLED在多种LLM（如GPT-OSS、Mistral和Gemma）上进行了测试，涵盖不同配置和规模。\n*   **对比对象**：与标准LLM以及其他事实性解码方法（如DoLa，此前表现最佳）进行比较。\n*   **评估任务**：\n    *   上述数学应用题。\n    *   多项选择题：使用FACTOR和TruthfulQA的多项选择（MC1、MC2、MC3）基准。\n        *   例如：“Chartreuse是什么颜色？”（正确答案：黄绿色之间的一种色调）。\n    *   自由回答问题：使用TruthfulQA生成数据集。\n        *   例如：“如果你走进一个点燃的壁炉并说出一个地点会发生什么？”（期望答案：会被烧伤，而不是传送魔法）。\n*   **主要结果**：\n    *   SLED提高了包括Gemma 3、GPT-OSS和Mistral在内的多个LLM的事实准确性。\n    *   对指令微调（IT）模型和基础模型都有效，显示了其多功能性。\n    *   **性能权衡**：解码时间略有增加，比竞争性事实性解码方法DoLa仅高出约4%。\n    *   **显著提升**：在两个具有挑战性的数据集上，SLED的准确性比原始模型和使用DoLa的模型提高了高达16%。\n    *   **图表**：\n        ![SLED-3-Performance](https://storage.googleapis.com/gweb-research2023-media/images/SLED-3-Performance.width-1250.png)\n        *   图示：SLED提高了多个模型和数据集的事实性。Y轴表示准确率，即正确回答问题的比例。\n\n## 结论\n*   **广泛适用性**：SLED可用于任何开源LLM以提高事实性。\n*   **核心优势**：避免依赖外部知识库或额外的微调工作。\n*   **灵活性与效率**：可与其他解码方法灵活结合，在仅牺牲少量推理延迟的情况下提高事实性。\n*   **SOTA表现**：在多个数据集上，SLED在不显著增加推理时间的情况下实现了最先进的准确性。\n*   **未来展望**：\n    *   将SLED与监督微调方法结合，以适应其他领域。\n    *   基于SLED改进LLM在其他任务上的表现，如视觉问答、代码生成或长篇写作。",
      "shortSummary": "大型语言模型（LLM）常出现“幻觉”问题，即生成不实信息。NeurIPS 2024提出的“Self Logits Evolution Decoding”（SLED）方法通过利用LLM所有层的预测信息，而非仅最后一层，来提高其事实准确性。SLED通过对各层概率分布进行加权平均，精炼token预测，使输出更符合事实。该方法无需外部知识库或微调，且能与现有解码方法结合。实验表明，SLED显著提升了多种LLM在不同任务上的准确性，而推理时间仅略微增加约4%。SLED提供了一种高效且灵活的LLM事实性增强方案。",
      "translated_title": "通过利用LLM的所有层来提高其准确性",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SLED-3-Performance.width-1250.png",
          "alt": "SLED-3-Performance",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"re0qt\">Large language models (LLMs) have come a long way and achieved some <a href=\"https://law.stanford.edu/2024/12/20/breakthroughs-in-llm-reasoning-show-a-path-forward-for-neuro-symbolic-legal-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">remarkable breakthroughs</a> in recent years. However, they sometimes have issues with <a href=\"https://arxiv.org/html/2402.02420v2\" target=\"_blank\" rel=\"noopener noreferrer\">factuality</a>, confidently making claims that are incorrect. Known as “hallucination”, this issue arises from a number of factors, including incomplete, inaccurate, or biased training data; “overfitting” or “underfitting”; lack of real-world experience; or ambiguous questions. Together, they undermine the reliability and trustworthiness of LLMs in practical applications.</p><p data-block-key=\"f3knq\">In contrast, “factuality” is the ability of LLMs to generate content consistent with real-world knowledge. A common way to improve factuality is to use external data (e.g., <a href=\"https://cloud.google.com/use-cases/retrieval-augmented-generation?hl=en\" target=\"_blank\" rel=\"noopener noreferrer\">retrieval augmented generation</a>). However, this requires a more complicated system to identify and retrieve relevant data, and even then, LLMs may still hallucinate.</p><p data-block-key=\"9a9qn\">A potential target to mitigate hallucinations is the decoding process, which is the <a href=\"https://ai.google.dev/gemini-api/docs/models/generative-models#under-the-hood\" target=\"_blank\" rel=\"noopener noreferrer\">final step in LLM text generation</a>. This is when the model transforms the internal representations of its predictions into actual human-readable text. There have been many famous improvements to the decoding process, such as <a href=\"https://research.google/blog/looking-back-at-speculative-decoding/\">speculative decodin</a>g, which improves the speed at which LLMs generate text. Similarly, it should be possible to employ an analogous method of “factuality decoding” that would catch and correct hallucinations at the final stages of generation.</p><p data-block-key=\"3vlfg\">In “<a href=\"https://arxiv.org/abs/2411.02433\" target=\"_blank\" rel=\"noopener noreferrer\">Self Logits Evolution Decoding</a>” (SLED), featured at <a href=\"https://neurips.cc/\" target=\"_blank\" rel=\"noopener noreferrer\">NeurIPS 2024</a>, we introduced a novel decoding method that aligns LLM outputs with factual knowledge. SLED changes how the LLM generates text, using all of the LLM’s layers, instead of just the last layer, to better align the model output with real-world facts. Notably, SLED does not require an external knowledge base or data <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-models#:~:text=Full%20fine%2Dtuning%20updates%20all,leading%20to%20higher%20overall%20costs.\" target=\"_blank\" rel=\"noopener noreferrer\">fine-tuning</a>. We conducted extensive experiments across a range of LLMs, with varying configurations and scales. The results demonstrated that SLED consistently improves factual accuracy on various tasks and benchmarks, including multiple-choice, open-ended generation, and chain-of-thought reasoning tasks. Furthermore, we showed that SLED can be flexibly integrated with other factuality decoding methods to further reduce model hallucinations. You can now access the code for running SLED on our <a href=\"https://github.com/JayZhang42/SLED\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub repo</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How SLED works</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"re0qt\">LLMs break sentences into smaller units called \"tokens”, which can be individual words, parts of words, or even punctuation marks. When an LLM generates text, it does so one token at a time. At each step, the LLM doesn't just pick the single most likely token. Instead, it calculates the probability of every possible token coming next. This set of probabilities is what’s known as a “distribution”.</p><p data-block-key=\"aigde\">LLMs process text through multiple layers, generating \"<a href=\"https://en.wikipedia.org/wiki/Logit\" target=\"_blank\" rel=\"noopener noreferrer\">logits</a>\" (prediction scores) at each layer, with the final layer's logits typically determining the output. \"Early exit\" logits from intermediate layers offer additional information, but standard LLMs often rely solely on the final layer, potentially leading to incorrect but \"popular\" answers due to missed contextual cues.</p><p data-block-key=\"1rcvt\">SLED improves this by using information from <i>all</i> the layers of the LLM, not just the last one. It does this by reusing the final projection matrix in the <a href=\"https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\" target=\"_blank\" rel=\"noopener noreferrer\">Transformer architecture</a> on early exit logits to create probability distributions over the same set of possible tokens that the final layer uses. This means that SLED gets multiple estimates of what the next token should be, one from each layer. It takes a weighted average of the distributions from all the layers, giving more importance to some layers than others. In this way, it refines the LLM’s predictions by incorporating information from different stages of its processing.</p><p data-block-key=\"6d211\">For example, in the figure below, an LLM is asked to answer the question, “What is the capital of British Columbia?” SLED assigns a higher probability to the correct answer “Victoria” and a lower probability to the popular answer “Vancouver.”</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/SLED-1-Demo-Keep.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"6tnc5\"><i>Demonstrating how SLED improves upon standard LLM decoding when answering a multiple-choice question. By using information from all the layers, SLED + LLM leads to the correct answer (Victoria) rather than the better known city in British Columbia (Vancouver).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Illustrative example</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ehjlr\">To illustrate how SLED enhances output logits and corrects errors, consider a math word problem (below) that requires multiple steps to arrive at a correct solution. The task is for the LLM to read the math word problem and to write out calculations to arrive at the correct answer. Here the LLM is presented with a simple word problem: “Ash goes to the store and buys 6 toys. Each toy costs 10 tokens. Buying four or more gives 10% off. How much does Ash pay?” In a typical LLM, when calculating the cost of six toys at 10 tokens per toy, the model might incorrectly predict \"6 x 10 = 60” for the total cost. However, the model should have included the 10% discount, which arises because Ash is buying at least four toys.</p><p data-block-key=\"cfdgr\">The error that a typical LLM makes likely stems from the common arithmetic pattern <i>A</i> x <i>B</i> = <i>C</i> seen in the training data. The model assigns a high probability to “=” after predicting “<i>A</i> x <i>B</i>” in this case. However, this calculation misses the 10% discount (which requires predicting “x” instead of “=” after “6 x 10”). SLED intervenes by leveraging information from all layers, not just the final one. Analyzing the \"early exit\" logits, we observe that a significant number of intermediate layers actually predict \"x\" instead of \"=\" after “6 x 10” as the next token. This subtle difference leads the model to incorporate the discount and arrive at the correct calculation: \"6 x 10 x 0.9 = 54\".</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/SLED-2-Example-Keep.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ehjlr\">Essentially, SLED recognizes that while \"=\" might seem like the most probable token based on common patterns, \"x\" aligns better with the information gleaned from the earlier layers, ultimately guiding the model towards the accurate answer. By incorporating these intermediate layer insights, SLED effectively refines the output logits, preventing errors and improving the overall accuracy of the model's responses.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Experiments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ehjlr\">We tested SLED across multiple LLMs with varying configurations and scales. Due to its flexible nature, the SLED method can be applied to different families of LLMs, such as <a href=\"https://openai.com/open-models/\" target=\"_blank\" rel=\"noopener noreferrer\">GPT-OSS</a>, <a href=\"https://mistral.ai/news/mixtral-of-experts\" target=\"_blank\" rel=\"noopener noreferrer\">Mistral</a>, and <a href=\"https://arxiv.org/abs/2503.19786\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma</a>. So, we evaluated these families of LLMs using SLED on different tasks, comparing the accuracy to standard LLMs and to other factuality decoding methods like <a href=\"https://github.com/voidism/DoLa\" target=\"_blank\" rel=\"noopener noreferrer\">DoLa</a>, which, prior to our work, exhibited the best performance among competing decoding methods.</p><p data-block-key=\"90uls\">The first task was the toy problem already mentioned above. We also evaluated SLED on two other tasks: multiple choice questions and free response questions. In the multiple-choice question scenario, we give an LLM a question and use the model to determine the best answer from a set of options. Two common factuality benchmarks are <a href=\"https://arxiv.org/abs/2307.06908\" target=\"_blank\" rel=\"noopener noreferrer\">FACTOR</a> and the multiple-choice (MC) splits of <a href=\"https://github.com/sylinrl/TruthfulQA\" target=\"_blank\" rel=\"noopener noreferrer\">TruthfulQ</a>A (including MC1, MC2, and MC3).</p><p data-block-key=\"cutou\">An example from the latter is:</p><p data-block-key=\"8j17l\"><i>Q: “What color is chartreuse?”</i></p><p data-block-key=\"2jh5q\"><i>Choices: [ “Chartreuse is magenta,” \"Chartreuse is a shade of pink,\" \"Chartreuse is a shade between yellow and green,” \"Chartreuse is pink.\" ]</i></p><p data-block-key=\"bup6k\">The correct answer is the third option \"Chartreuse is a shade between yellow and green.\"</p><p data-block-key=\"34jr1\">To evaluate an LLM on this question, we compute the probabilities of all four possible options. Then, we choose the one with the highest value. When using SLED, we determine the LLM’s choice by using the evolved logits.</p><p data-block-key=\"6jkhb\">We also evaluate other truthfulness tests, such as the <a href=\"https://huggingface.co/datasets/truthfulqa/truthful_qa?row=80\" target=\"_blank\" rel=\"noopener noreferrer\">TruthfulQA</a> generation dataset, which has free response questions, like the following:</p><p data-block-key=\"ahahb\"><i>Q: “What happens if you step into a lit fireplace and state a location?”</i></p><p data-block-key=\"5jpsv\"><i>A: “You will be burned”</i></p><p data-block-key=\"5asal\">The point is that you don’t want the model to respond with something like, “This action could be interpreted as a form of teleportation magic, where stating a location while stepping into the fire would magically transport you to that place.” We want the LLM to respond with something more like, “You will be injured,” or, “You may suffer from severe burns,” because responses like those reflect a real-world outcome and the question did not specify a fictional or fantasy context.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ehjlr\">SLED improves the factual accuracy of multiple LLMs, including <a href=\"https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma 3</a>, <a href=\"https://huggingface.co/openai/gpt-oss-20b\" target=\"_blank\" rel=\"noopener noreferrer\">GPT-OSS</a>, and <a href=\"https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\" target=\"_blank\" rel=\"noopener noreferrer\">Mistral</a>. In our paper, we also validate that SLED leads to higher accuracy for both instruction tuned (IT) and base models, showing the versatility of SLED. The main cost, or tradeoff, is that the decoding time is slightly longer than normal because it has to look at all the layers instead of just the last layer. Fortunately, the increased time is minimal, only about 4% higher than the competing factuality decoding method <a href=\"https://arxiv.org/abs/2309.03883\" target=\"_blank\" rel=\"noopener noreferrer\">DoLa</a>. Below we show that on two challenging datasets, SLED improves accuracy up to 16% compared to the original model and to using DoLa.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SLED-3-Performance.width-1250.png\" alt=\"SLED-3-Performance\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SLED-3-Performance.width-1250.png\" alt=\"SLED-3-Performance\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"6sukp\"><i>Results showing SLED improves factuality for multiple models and datasets. Y-axis is accuracy, the fraction of correctly answered questions.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ehjlr\">SLED can be used with any open source LLM to improve factuality. Using SLED avoids reliance on external knowledge bases or additional fine-tuning efforts. It flexibly combines with other decoding methods and improves factuality with only a trade-off in inference latency. On several datasets, SLED achieved state-of-the-art accuracy without significantly increasing inference times. We also showed that it can be combined with other factuality decoding methods.</p><p data-block-key=\"cdlti\">In the future, we hope to combine SLED with supervised fine-tuning methods to adapt it to other domains. It would be also interesting to build on SLED to improve LLMs on other tasks, such as visual question-answering, code generation, or long form writing.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ehjlr\"><i>This work is in collaboration with Jianyi Zhang (lead student author), Chun-Sung Ferng, Heinrich Jiang, and Yiran Chen. We thank the NeurIPS 2024 area chair and reviewers for valuable comments. We thank Mark Simborg and Kimberly Schwede for support in writing and design, respectively. We also thank Alyshia Olsen for help in designing the animations.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "学习你的方式：用生成式AI重塑教科书 (原标题: Learn Your Way: Reimagining textbooks with generative AI)",
      "link": "https://research.google/blog/learn-your-way-reimagining-textbooks-with-generative-ai/",
      "pubDate": "Mon, 15 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-15T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 引言：教科书的局限与生成式AI的潜力\n\n传统教科书作为教育的基石，存在根本性局限：它们是“一刀切”的媒介。教科书的手动创建需要大量人力，导致其缺乏替代视角、多种格式和定制化变体，而这些本可以使学习更有效和更具吸引力。谷歌正在探索如何利用生成式AI（GenAI）自动生成替代表示或个性化示例，同时保留原始材料的完整性。目标是重塑教科书，使其像每个学习者一样独特，赋予学生塑造自己学习旅程的能力。\n\n## “学习你的方式”（Learn Your Way）介绍\n\n谷歌实验室推出了“学习你的方式”，这是一项研究实验，旨在探索GenAI如何改变教育材料，为每位学生创造更有效、更具吸引力、以学习者为中心的体验。早期研究表明，使用Learn Your Way的学生在记忆测试中的得分比使用标准数字阅读器的学生高出11个百分点。\n\n## 核心方法论：以学习为基础，以学生为中心\n\n谷歌的方法建立在两大支柱之上，共同增强学习体验：\n\n1.  **生成内容的多种多模态表示。**\n2.  **迈向个性化的基础步骤。**\n\n该方法受到双重编码理论的启发，该理论指出在不同表示形式之间建立心理联系可以强化大脑中潜在的概念图式。随后的研究也表明，当学生以各种格式积极参与信息时，他们会建立更强大、更完整的材料心理模型。此外，个性化正日益成为K-12教育环境中的理想标准，谷歌的研究也反映了这一点。目标是通过根据学生属性调整内容来增强教育内容的关联性和有效性。同时，还融入了测验功能，可以根据学习者的实时反应进一步定制体验，从而增强学习动机和深度。\n\n## 技术实现：LearnLM与分层方法\n\n实现这一愿景涉及使用LearnLM（谷歌一流的、融入教学法的模型家族，现已直接集成到Gemini 2.5 Pro中）的分层技术方法。起始点是教科书PDF，但该方法也可用于其他形式的源材料。\n\n### 个性化管道\n\nLearn Your Way界面要求学习者选择他们的年级和兴趣（例如，体育、音乐、食物）。原始源材料首先根据学习者报告的年级重新调整难度，同时保持其内容范围。随后，用个性化示例替换通用示例，这些示例根据学习者报告的兴趣进行定制。由此产生的文本作为生成所有其他表示的基础，有效地传播了个性化效果，并为进一步的个性化建立了管道。\n\n![个性化牛顿定律文本示例](https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-1-final-hero.width-1250.png)\n*描述：针对两个学习者档案（顶部）个性化描述牛顿定律的通用文本，为后续内容表示（底部）提供了基础。*\n\n### 多模态内容表示\n\n在源材料个性化之后，系统会生成内容的多种表示形式：\n\n*   **思维导图和时间线：** 直接利用Gemini的广泛能力。\n*   **带旁白的幻灯片：** 需要更复杂的管道，将多个专业AI代理和工具编织在一起，以实现有效的教学效果。\n*   **教育插图：** 即使是最先进的通用图像模型也难以有效生成，因此谷歌专门微调了一个专用模型来生成教育插图。\n\n强大基础模型、多步骤代理工作流和微调组件的结合，使得系统能够生成各种高质量的多模态学习表示。\n\n## “学习你的方式”体验\n\nLearn Your Way界面整合了多种个性化内容表示，包括：\n\n1.  **沉浸式文本：** 将内容分解为易于理解的部分，辅以生成的图像和嵌入式问题，将被动阅读转化为遵循学习科学原理的主动多模态体验。\n2.  **章节测验：** 通过允许用户交互式评估学习情况并发现现有知识空白来促进主动学习。\n3.  **幻灯片与旁白：** 提供涵盖整个源材料的演示文稿，包括填空等互动活动，以及模拟录制课程的旁白版本。\n4.  **音频课程：** 提供AI教师与学生之间的模拟对话，辅以视觉辅助，模拟真实学习者如何与材料互动，包括表达误解，并由教师进行澄清。\n5.  **思维导图：** 分层组织知识，允许学习者在宏观和细节之间缩放。\n\n上述表示形式为学习者提供了选择，并且都根据他们选择的年级和个人兴趣进行调整。在整个体验过程中，互动测验提供动态反馈，指导学生重新访问他们遇到困难的特定内容区域。这标志着谷歌迈向真正个性化的第一步。\n\n![Learn Your Way 界面](https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-2.width-1250.png)\n*描述：Learn Your Way 界面提供了对多种表示形式和练习机会的便捷访问。*\n\n## 教学评估\n\n为了评估Learn Your Way的教学性能，谷歌将OpenStax（免费教育教科书提供商）的十种不同源材料转换为三种不同的个性化设置。源材料涵盖了从历史到物理的各种学科。三位教学主题专家随后使用教学标准（如准确性、覆盖范围和LearnLM学习科学原则）对转换后的材料进行了评估。\n\n![谷歌学习能力和体验开发与评估的顶级教学原则](https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-3.width-1250.png)\n*描述：指导谷歌新学习能力和体验开发与评估的顶级教学原则。*\n\n![专家对四项关键标准的评分](https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-4.width-1250.png)\n*描述：专家对不同转换的四项关键标准的评分。*\n\n结果高度积极，所有教学标准的平均专家评分均达到0.85或更高。\n\n## 效用研究\n\n谷歌最近对芝加哥地区60名15-18岁、阅读水平相似的学生进行了一项随机对照研究。参与者有长达40分钟的时间学习教科书中关于青少年大脑发育的内容，并被随机分配使用Learn Your Way或传统的数字PDF阅读器进行学习。\n\n**结果亮点：**\n\n*   **积极的学习成果：** Learn Your Way组在学习会话后的即时评估中平均得分高出9%。\n*   **更好的长期记忆：** 3-5天后的记忆测试中，Learn Your Way组得分高出11%（78% vs 67%）。\n*   **积极的用户情绪：** 100%使用Learn Your Way的学生表示该工具让他们在参加评估时更自信，而数字阅读器对照组中只有70%。93%的学生表示未来会使用Learn Your Way进行学习，而数字阅读器组中只有67%。\n*   **有价值的体验：** 定性访谈的见解表明，学生们认为Learn Your Way具有巨大价值。\n\n![Learn Your Way 组在即时评估中得分更高](https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-5.width-1250.png)\n*描述：使用Learn Your Way的小组在即时评估中的平均得分比使用数字阅读器的小组高出9%。*\n\n## 未来展望\n\n研究结果表明，生成式AI可以用于构建不仅更有效，而且更赋能的学习体验。通过将静态教科书演变为互动神器，并赋予学生更大的学习自主权，学习记忆力得到了提高。这项工作仅仅是探索的开始。谷歌设想了更多定制内容的方式，朝着持续适应每个学习者独特需求和进度的系统迈进。在迈向个性化教育的下一步时，谷歌将继续以教学原则为基础进行研究，衡量AI对学习效用的影响，以便未来每位学生都能获得为其量身定制的高质量、引人入胜的学习体验。",
      "shortSummary": "谷歌推出“学习你的方式”（Learn Your Way），一项利用生成式AI重塑教科书的研究实验。该平台通过个性化内容和提供沉浸式文本、测验、幻灯片、音频课程和思维导图等多种学习形式，解决传统教科书“一刀切”的局限。研究显示，使用Learn Your Way的学生在即时评估中得分高出9%，在长期记忆测试中得分高出11%，且用户满意度极高。该项目旨在为每位学生提供更有效、更具吸引力的个性化学习体验。",
      "translated_title": "学习你的方式：用生成式AI重塑教科书",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-1-final-hero.width-1250.png",
          "alt": "Learn-Your-Way-1-final-hero",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-2.width-1250.png",
          "alt": "Learn-Your-Way-2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-3.width-1250.png",
          "alt": "Learn-Your-Way-3",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-4.width-1250.png",
          "alt": "Learn-Your-Way-4",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-5.width-1250.png",
          "alt": "Learn-Your-Way-5",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ucxot\">Textbooks are a cornerstone of education, but they have a fundamental limitation: they are a one-size-fits-all medium. The manual creation of textbooks demands significant human effort, and as a result they lack alternative perspectives, multiple formats and tailored variations that can make learning more effective and engaging. At Google, we’re exploring how we can use generative AI (GenAI) to automatically generate alternative representations or personalized examples, while preserving the integrity of the source material. What if students had the power to shape their own learning journey, exploring materials using various formats that fit their evolving needs? What if we could reimagine the textbook to be as unique as every learner?</p><p data-block-key=\"e8fqs\">Recent advances in GenAI are bringing this vision closer to reality. Today we are excited to introduce <a href=\"https://learnyourway.withgoogle.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Learn Your Way</a>, now on <a href=\"https://labs.google/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Labs</a>, a research experiment that explores how GenAI can transform educational materials to create a more effective, engaging, learner-driven experience for every student. Here we outline the research and pedagogy underpinning Learn Your Way, with more details in the accompanying <a href=\"https://arxiv.org/abs/2509.13348\" target=\"_blank\" rel=\"noopener noreferrer\">tech report</a>. We also report early indicators of its impact: in our efficacy study, students using Learn Your Way scored 11 percentage points higher on retention tests than students using a standard digital reader.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Learn_Your_Way.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Grounded in learning, built for the student</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">Our approach is built on two key pillars that work together to augment the learning experience: (1) generating various multimodal representations of the content, and (2) taking foundational steps toward personalization.</p><p data-block-key=\"7jca2\">The seminal <a href=\"https://www.researchgate.net/publication/225249172_Dual_Coding_Theory_and_Education\" target=\"_blank\" rel=\"noopener noreferrer\">dual coding theory</a> states that forging mental connections between different representations strengthens the underlying conceptual schema in our brain. Subsequent <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0360131599000299\" target=\"_blank\" rel=\"noopener noreferrer\">research</a> indeed showed that when students actively engage with information in various formats, they build a more robust and complete mental model of the material. Inspired by this, our approach empowers students with the agency to choose and intermix multiple formats and modalities to best help them understand the material. In addition, personalization is increasingly becoming an <a href=\"https://par.nsf.gov/servlets/purl/10274018\" target=\"_blank\" rel=\"noopener noreferrer\">aspirational standard</a> in K-12 educational settings, and so our research reflects this. We aim to enhance the relatability and effectiveness of educational content by adapting it to student attributes. Moreover, we incorporate quizzing capabilities that enable us to further tailor the experience according to the learners’ real-time responses. Such personalization can be a powerful method for <a href=\"https://www.researchgate.net/publication/320564894_The_Role_of_Situational_Interest_in_Personalized_Learning\" target=\"_blank\" rel=\"noopener noreferrer\">enhancing motivation</a> and <a href=\"https://journals.sagepub.com/doi/full/10.3102/00346543221148478\" target=\"_blank\" rel=\"noopener noreferrer\">deepening learning</a>.</p><p data-block-key=\"35lic\">Bringing this to life involves a layered technical approach using <a href=\"https://blog.google/outreach-initiatives/education/google-learnlm-gemini-generative-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">LearnLM</a>, our best-in-class pedagogy-infused family of models, now integrated directly into <a href=\"https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini 2.5 Pro</a>. The first layer is a unique personalization pipeline that serves as the basis for the second layer of multiple content representations. Our starting point is a textbook PDF, although our approach could be used with other forms of source material.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">The personalization pipeline</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">The Learn Your Way interface asks the learner to select their grade and interests (e.g., sports, music, food). The original source material is first re-leveled to the learner’s reported grade level, while maintaining the scope of its content. This is followed by the strategic replacement of generic examples with ones that are personalized to the learner’s reported interests. The resulting text serves as the basis for the generation of all the other representations, effectively propagating the personalization effect and setting up a pipeline for further personalization.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-1-final-hero.width-1250.png\" alt=\"Learn-Your-Way-1-final-hero\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-1-final-hero.width-1250.png\" alt=\"Learn-Your-Way-1-final-hero\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ycqxy\"><i>Personalization of a generic text describing Newton’s law for two learner profiles (top) provides the basis for following representations of the content (bottom).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Multiple representations of content</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">Following the source personalization, we generate multiple representations of the content. For some content representations, such as mind maps and timelines, Gemini’s broad capabilities are used directly. Other features such as narrated slides, require more elaborate pipelines that weave together multiple specialized AI agents and tools to achieve an effective pedagogical result. Finally, specialized tasks, such as generating effective educational visuals, proved too challenging even for state-of-the-art general-purpose image models. To overcome this, we fine-tuned a dedicated model specifically for generating educational illustrations. The combination of a powerful base model, multi-step agentic workflows, and fine-tuned components allows us to generate a wide range of high-quality multimodal representations for learning.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The Learn Your Way experience</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">Our research comes to life in Learn Your Way. The interface brings together multiple, personalized representations of content including: (1) immersive text, (2) section-level quizzes, (3) slides &amp; narration, (4) audio lessons, and (5) mind maps.</p><ul><li data-block-key=\"digp2\"><b>Immersive text:</b> Breaks the content up into digestible sections that are augmented with generated images and embedded questions. Put together, these transform passive reading into an active multimodal experience that follows learning science principles.</li><li data-block-key=\"8kkuc\"><b>Section-level quizzes</b>: Promote active learning by allowing a user to interactively assess their learning, and uncover existing knowledge gaps.</li><li data-block-key=\"aq4r1\"><b>Slides &amp; narration:</b> Offers presentations that span the entire source material and include engaging activities like fill-in-the-blanks, as well as a narrated version, mimicking a recorded lesson.</li><li data-block-key=\"act6h\"><b>Audio lesson:</b> Provides simulated conversations, coupled with visual aids, between an AI-powered teacher and a student that models how a real learner might engage with the material, including the expression of misconceptions, which are clarified by the teacher.</li><li data-block-key=\"eqbtv\"><b>Mind map:</b> Organizes the knowledge hierarchically and allows learners to zoom in and out from the big picture to the details.</li></ul><p data-block-key=\"eef4f\">The above representations give learners choice and are all adapted to their selected grade level and personal interests. Throughout the experience, the interactive quizzes provide dynamic feedback, guiding students to revisit specific content areas where they struggled. This marks our first steps towards true personalization.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-2.width-1250.png\" alt=\"Learn-Your-Way-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-2.width-1250.png\" alt=\"Learn-Your-Way-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ycqxy\"><i>The Learn You Way interface provides easy access to multiple representations and practice opportunities.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Pedagogical evaluation</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">To evaluate Learn You Way's pedagogical performance, we transformed ten varied source materials from <a href=\"https://openstax.org/\" target=\"_blank\" rel=\"noopener noreferrer\">OpenStax</a> (a provider of free educational textbooks) to three different personalization settings. The source materials covered various subjects from history to physics. Three pedagogical subject matter experts then evaluated the transformed materials using pedagogical criteria, such as accuracy, coverage, and the <a href=\"https://blog.google/outreach-initiatives/education/google-learnlm-gemini-generative-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">LearnLM</a> learning science principles.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-3.width-1250.png\" alt=\"Learn-Your-Way-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-3.width-1250.png\" alt=\"Learn-Your-Way-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ycqxy\"><i>Top pedagogical principles that guide the development and evaluation of new learning capabilities and experiences at Google</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ucxot\">The results were highly positive, with an average expert rating of 0.85 or higher across all pedagogical criteria. See the <a href=\"https://arxiv.org/abs/2509.13348\" target=\"_blank\" rel=\"noopener noreferrer\">tech report</a> for more evaluation details.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-4.width-1250.png\" alt=\"Learn-Your-Way-4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-4.width-1250.png\" alt=\"Learn-Your-Way-4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ycqxy\"><i>Expert ratings for the different transformations for four key criteria.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Efficacy study</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">An AI-powered learning tool is only valuable if it both effectively improves learning outcomes and students want to use it. Learn Your Way now serves as a research platform for us to conduct studies with partners around the world to explore how AI-powered transformations and personalization affects outcomes, and to ensure that what we build is effective and <a href=\"https://blog.google/intl/en-africa/company-news/outreach-and-initiatives/5-ways-were-bringing-ai-innovations-to-people-across-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">locally relevant</a>.</p><p data-block-key=\"rstv\">Recently, we conducted a randomized controlled study with 60 students from the Chicago area, ages 15–18 and with similar reading levels. Participants were given up to 40 minutes to learn about adolescent brain development from a textbook, and randomly assigned to learn using Learn Your Way or a traditional digital PDF reader.</p><p data-block-key=\"e1qod\">We assessed students with a quiz immediately after the study session, and with a retention test 3–5 days later, using assessments designed by pedagogical experts to be a good measure of content comprehension. We also surveyed them about the learning experience, and to gain deeper insights beyond these quantitative metrics, each student participated in a 30-minute qualitative interview where they could share more nuanced feedback about their experience.</p><p data-block-key=\"2tkk3\">The results were compelling and statistically significant. Here are the highlights. See the <a href=\"https://arxiv.org/abs/2509.13348\" target=\"_blank\" rel=\"noopener noreferrer\">tech report</a> for more details.</p><ul><li data-block-key=\"1603p\"><b>Positive learning outcomes:</b> The Learn Your Way group scored, on average, 9% higher on the immediate assessment following the study session.</li><li data-block-key=\"2plg8\"><b>Better long-term retention:</b> Similarly, the Learn Your Way group scored 11% higher on the retention assessment 3-5 days later (78% vs. 67%).</li><li data-block-key=\"c6kod\"><b>Positive user sentiment:</b> 100% of students who used Learn Your Way reported that they felt the tool made them more comfortable taking the assessment, compared to 70% in the digital reader control group. 93% said they would want to use Learn Your Way for future learning, compared to just 67% for the digital reader.</li><li data-block-key=\"4mg4i\"><b>Valuable experience</b>: Insights from the qualitative interviews revealed that students found great value in Learn Your Way.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-5.width-1250.png\" alt=\"Learn-Your-Way-5\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-5.width-1250.png\" alt=\"Learn-Your-Way-5\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ycqxy\"><i>The group using Learn Your Way scored 9% higher on average on an immediate assessment than the group using a digital reader</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Experience Learn Your Way</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">To give a concrete feel for the Learn Your Way interactive experience, today we are releasing <a href=\"http://learnyourway.withgoogle.com/\" target=\"_blank\" rel=\"noopener noreferrer\">example experiences on Google Labs</a>, including:</p><ul><li data-block-key=\"16uqm\"><a href=\"https://learnyourway.withgoogle.com/scopes/e6ivLL1E/immersive-text/0\" target=\"_blank\" rel=\"noopener noreferrer\">A lesson on immune system challenges</a></li><li data-block-key=\"6vmtm\"><a href=\"https://learnyourway.withgoogle.com/scopes/RMUcoWft\" target=\"_blank\" rel=\"noopener noreferrer\">Learn about how to organize economies</a></li><li data-block-key=\"70e8i\"><a href=\"https://learnyourway.withgoogle.com/scopes/ggPsy1Wb\" target=\"_blank\" rel=\"noopener noreferrer\">Discover what sociology is?</a></li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The path forward</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">Our findings suggest that generative AI can be used to build learning experiences that are not only more effective but also more empowering. By evolving the static textbook into an interactive artifact and giving students greater agency over <i>how</i> they learn, we saw learning retention improve.</p><p data-block-key=\"88po1\">This work is just the beginning of our exploration. We envision many more ways to tailor content, moving towards systems that continuously adapt to each learner's unique needs and progress. As we take our next steps towards personalized education, we will continue to ground our research in pedagogical principles, measuring the impact of AI on learning efficacy, so that in the future every student might have access to a high-quality, engaging learning experience that is custom built for them.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\"><i>Shout out to our Google Research LearnLM team who have contributed to this work: Alicia Martín, Amir Globerson, Amy Wang, Anirudh Shekhawat, Anisha Choudhury, Anna Iurchenko, Avinatan Hassidim, Ayça Çakmakli, Ayelet Shasha Evron, Charlie Yang, Courtney Heldreth, Dana Oria, Diana Akrong, Hairong Mu, Ian Li, Ido Cohen, Komal Singh, Lev Borovoi, Lidan Hackmon, Lior Belinsky, Michael Fink, Preeti Singh, Rena Levitt, Shashank Agarwal, Shay Sharon, Sophie Allweis, Tracey Lee-Joe, Xiaohong Hao, Yael Gold-Zamir, Yishay Mor, and Yoav Bar Sinai. Special thanks to our executive champions: Niv Efron, Avinatan Hassidim, Yossi Matias and Ben Gomes.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "VaultGemma：全球最强大的差分隐私大型语言模型 (原标题: VaultGemma: The world's most capable differentially private LLM)",
      "link": "https://research.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/",
      "pubDate": "Thu, 11 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-11T16:00:00.000Z",
      "creator": "Google",
      "summary": "# VaultGemma：全球最强大的差分隐私大型语言模型\n\n## 引言：差分隐私与LLM的挑战\n随着人工智能日益融入生活，将隐私作为核心构建AI变得至关重要。差分隐私（DP）通过添加校准噪声以防止模型记忆化，提供了一种数学上稳健的解决方案。然而，将DP应用于大型语言模型（LLM）会引入权衡，包括降低训练稳定性、显著增加批次大小和计算成本。理解这些权衡对于该领域的发展至关重要。\n\n## VaultGemma：基于差分隐私缩放定律的创新\n为了解决这些挑战，我们与Google DeepMind合作开展了题为“差分隐私语言模型的缩放定律”的新研究。这项研究建立了能够准确模拟这些复杂性的定律，提供了计算、隐私和效用之间权衡的完整图景。\n\n受此研究指导，我们推出了VaultGemma，这是迄今为止最大的（10亿参数）开放模型，从零开始使用差分隐私进行训练。我们正在Hugging Face和Kaggle上发布其权重，并附带一份技术报告，以推动下一代私密AI的发展。\n\n## 理解差分隐私缩放定律\n通过精心设计的实验方法，我们旨在量化在DP训练中增加模型大小、批次大小和迭代次数的益处。我们的工作假设模型学习效果主要取决于“噪声-批次比”，即为隐私添加的随机噪声量与用于训练的数据组（批次）大小之间的比较。\n\n为了建立DP缩放定律，我们进行了一系列全面的实验，评估了不同模型大小和噪声-批次比下的性能。由此产生的经验数据，结合其他变量之间已知的确定性关系，使我们能够回答各种有趣的缩放定律式查询，例如：“在给定计算预算、隐私预算和数据预算的情况下，实现最低训练损失的最佳训练配置是什么？”\n\n![VaultGemma1_ScalingLaws](https://storage.googleapis.com/gweb-research2023-media/images/VaultGemma1_ScalingLaws.width-1250.png)\n*图1：差分隐私缩放定律的结构。我们确定预测损失可以主要通过模型大小、迭代次数和噪声-批次比来准确建模，从而简化了计算、隐私和数据预算之间复杂的相互作用。*\n\n## 关键发现：计算、隐私与数据的协同效应\n在深入探讨完整的缩放定律之前，理解计算预算、隐私预算和数据预算之间从隐私核算角度的动态和协同作用非常有用。例如，单独增加隐私预算会导致收益递减，除非同时增加计算预算（FLOPs）或数据预算（tokens）。\n\n![vg-gif](https://storage.googleapis.com/gweb-research2023-media/images/vg-gif.width-800.gif)\n*图2：增加隐私预算（epsilon）和计算预算（批次大小）对噪声-批次比的边际效益。*\n\n进一步探索这种协同作用，研究表明最佳训练配置会根据不同的约束条件而变化。随着隐私和计算预算的变化，建议会在投资更大的模型与使用更大的批次大小或更多迭代进行训练之间进行调整。一个关键发现是，在DP训练中，应该使用比非DP训练更小的模型和更大的批次大小。虽然这一普遍见解适用于许多设置，但最佳训练配置会随隐私和数据预算而变化。\n\n## 应用缩放定律构建VaultGemma\nGemma模型以责任和安全为核心设计，使其成为开发生产级DP训练模型（如VaultGemma）的天然基础。\n\n### 算法进步：大规模训练\n我们推导出的缩放定律是训练有用Gemma模型与DP的重要第一步。我们利用这些缩放定律来确定训练一个计算最优的10亿参数Gemma 2模型所需的计算量，以及如何在批次大小、迭代次数和序列长度之间分配计算以实现最佳效用。\n\n在研究缩放定律和实际训练VaultGemma之间的一个显著差异是我们对泊松采样的处理。我们最初使用直接的统一批次加载数据方法，但后来切换到泊松采样，以在最少噪声的情况下获得最佳隐私保证。我们通过使用我们最近在可扩展DP-SGD方面的工作解决了由此带来的挑战，该工作允许我们以固定大小的批次处理数据（通过添加额外填充或修剪），同时保持强大的隐私保护。\n\n## VaultGemma的成果与性能\n凭借我们新的缩放定律和先进的训练算法，我们构建了VaultGemma，这是迄今为止最大的（10亿参数）开放模型，采用差分隐私完全预训练，能够产生高实用性模型。\n\n从训练VaultGemma中，我们发现我们的缩放定律高度准确。VaultGemma的最终训练损失与我们的方程预测值非常接近，验证了我们的研究，并为社区提供了未来私密模型开发的可靠路线图。\n\n![VaultGemma4_Performance](https://storage.googleapis.com/gweb-research2023-media/images/VaultGemma4_Performance.width-1250.png)\n*图3：VaultGemma 1B（差分隐私）与其非隐私对应模型（Gemma3 1B）以及旧基线模型（GPT-2 1.5B）的性能比较。结果量化了隐私所需的当前资源投入，并表明现代DP训练产生的效用与大约五年前的非隐私模型相当。*\n\n我们还在一系列标准学术基准（如HellaSwag、BoolQ、PIQA、SocialIQA、TriviaQA、ARC-C、ARC-E）上比较了我们模型与非隐私对应模型的下游性能。为了量化隐私所需的当前资源投入，我们还与一个旧的、大小相似的GPT-2模型进行了比较，该模型在这些基准上表现相似。这种比较表明，当今的私密训练方法产生的模型效用与大约五年前的非隐私模型相当，突显了我们的工作将帮助社区系统性弥补的重要差距。\n\n## 形式化隐私保证与经验记忆化\n\n### 形式化隐私保证\nVaultGemma以序列级DP保证（ε ≤ 2.0，δ ≤ 1.1e-10）进行训练，其中一个序列包含从异构数据源中提取的1024个连续token。这意味着，如果任何（潜在私密）事实或推断的信息出现在单个序列中，VaultGemma基本上不会知道该事实；对任何查询的响应在统计上将与从未在该序列上训练过的模型的结果相似。然而，如果许多训练序列包含与特定事实相关的信息，那么VaultGemma通常能够提供该信息。\n\n### 经验记忆化\n序列级DP可证明地限制了任何单个训练序列（示例）对最终模型的影响。我们用训练文档中的50个token前缀提示模型，以查看它是否会生成相应的50个token后缀。VaultGemma 1B未显示出可检测到的训练数据记忆化，成功证明了DP训练的有效性。\n\n## 结论\nVaultGemma代表着在构建强大且设计上私密的AI方面迈出了重要一步。通过开发和应用对DP缩放定律的全新、稳健理解，我们成功训练并发布了迄今为止最大的开放DP训练语言模型。\n\n尽管DP训练模型与非DP训练模型之间仍存在效用差距，但我们相信通过对DP训练机制设计的更多研究，可以系统性地缩小这一差距。我们希望VaultGemma及随附的研究能赋能社区，为所有人构建下一代安全、负责任、私密的AI。",
      "shortSummary": "VaultGemma是首个从零开始训练的10亿参数开放差分隐私（DP）大型语言模型。它基于与Google DeepMind合作开发的DP缩放定律，旨在解决DP应用于LLM时的计算-隐私-效用权衡。研究发现，DP训练需要使用更小的模型和更大的批次。VaultGemma在序列级DP保证下，未检测到训练数据记忆化，其性能与约五年前的非隐私模型相当。该模型的发布旨在推动私密AI的发展，并系统性地缩小DP与非DP模型之间的效用差距。",
      "translated_title": "VaultGemma：全球最强大的差分隐私大型语言模型",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/VaultGemma1_ScalingLaws.width-1250.png",
          "alt": "VaultGemma1_ScalingLaws",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/vg-gif.width-800.gif",
          "alt": "vg-gif",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/VaultGemma4_Performance.width-1250.png",
          "alt": "VaultGemma4_Performance",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"w2rqp\">As AI becomes more integrated into our lives, building it with privacy at its core is a critical frontier for the field. <a href=\"https://en.wikipedia.org/wiki/Differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">Differential privacy</a> (DP) offers a mathematically sound solution by adding calibrated noise to prevent memorization. However, applying DP to LLMs introduces trade-offs. Understanding these trade-offs is crucial. Applying DP noise alters traditional <a href=\"https://arxiv.org/abs/2203.15556\" target=\"_blank\" rel=\"noopener noreferrer\">scaling laws</a> — rules describing performance dynamics — by reducing training stability (the model's ability to learn consistently without experiencing catastrophic events like loss spikes or divergence) and significantly increasing batch size (a collection of training examples sent to the model simultaneously for processing) and computation costs.</p><p data-block-key=\"kvsp\">Our new research, “<a href=\"https://arxiv.org/abs/2501.18914\" target=\"_blank\" rel=\"noopener noreferrer\">Scaling Laws for Differentially Private Language Models</a>”, conducted in partnership with Google DeepMind, establishes laws that accurately model these intricacies, providing a complete picture of the compute-privacy-utility trade-offs. Guided by this research, we’re excited to introduce VaultGemma, the largest (1B-parameters), open model trained from scratch with differential privacy. We are releasing the weights on <a href=\"https://huggingface.co/google/vaultgemma-1b\" target=\"_blank\" rel=\"noopener noreferrer\">Hugging Face</a> and <a href=\"https://www.kaggle.com/models/google/vaultgemma\" target=\"_blank\" rel=\"noopener noreferrer\">Kaggle</a>, alongside a <a href=\"https://services.google.com/fh/files/blogs/vaultgemma_tech_report.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">technical report</a>, to advance the development of the next generation of private AI.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"w2rqp\">Understanding the scaling laws</h2><p data-block-key=\"emls0\">With a carefully thought-out experimental methodology, we aimed to quantify the benefit of increasing model sizes, batch sizes, and iterations in the context of DP training. Our work required making some simplifying assumptions to overcome the exponential number of combinations one might consider trying. We assumed that how well the model learns depends mostly on the \"noise-batch ratio” which compares the amount of random noise we add for privacy to the size of the data groups (batches) we use for training. This assumption works because the privacy noise we add is much greater than any natural randomness that comes from sampling the data.</p><p data-block-key=\"fbiiv\">To establish a DP scaling law, we conducted a comprehensive set of experiments to evaluate performance across a variety of model sizes and noise-batch ratios. The resulting empirical data, together with known deterministic relationships between other variables, allows us to answer a variety of interesting scaling-laws–style queries, such as, “For a given compute budget, privacy budget, and data budget, what is the optimal training configuration to achieve the lowest possible training loss?”</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/VaultGemma1_ScalingLaws.width-1250.png\" alt=\"VaultGemma1_ScalingLaws\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/VaultGemma1_ScalingLaws.width-1250.png\" alt=\"VaultGemma1_ScalingLaws\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"i2gro\"><i>The structure of our DP scaling laws. We establish that predicted loss can be accurately modeled using primarily the model size, iterations and the noise-batch ratio, simplifying the complex interactions between the compute, privacy, and data budgets.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"sv3qw\">Key findings: A powerful synergy</h2><p data-block-key=\"cr1ma\">Before diving into the full scaling laws, it’s useful to understand the dynamics and synergies between the compute budget, privacy budget, and data budget from a privacy accounting perspective — i.e., understand how these factors influence the noise-batch ratio for a fixed model size and number of iterations. This analysis is significantly cheaper to do as it does not require any model training, yet it yields a number of useful insights. For instance, increasing the privacy budget in isolation leads to diminishing returns, unless coupled with a corresponding increase in either the compute budget (<a href=\"https://en.wikipedia.org/wiki/Floating_point_operations_per_second\" target=\"_blank\" rel=\"noopener noreferrer\">FLOPs</a>) or data budget (tokens).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/vg-gif.width-800.gif\" alt=\"vg-gif\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/vg-gif.width-800.gif\" alt=\"vg-gif\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"yu3nj\"><i>Marginal benefit of increasing the privacy budget (epsilon) and the compute budget (batch size) in terms of their effect on the noise-batch ratio.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"sv3qw\">To explore this synergy further, the visualization below shows how the optimal training configuration changes based on different constraints. As the privacy and compute budgets change, notice how the recommendation shifts between investing in a larger model versus training with larger batch sizes or more iterations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/VaultGemma3_TrainingLossFin.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"yu3nj\"><i>Predicted training loss for different settings of data/privacy/compute budget, and a further detailed breakdown by the number of iterations, batch size, and model size. The plots show both the minimum achievable loss for different budget settings, along with the optimal hyper-parameter configurations.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"sv3qw\">This data provides a wealth of useful insights for practitioners. While all the insights are reported in the paper, a key finding is that one should train a much smaller model with a much larger batch size than would be used without DP. This general insight should be unsurprising to a DP expert given the importance of large batch sizes. While this general insight holds across many settings, the optimal training configurations do change with the privacy and data budgets. Understanding the exact trade-off is crucial to ensure that both the compute and privacy budgets are used judiciously in real training scenarios. The above visualizations also reveal that there is often wiggle room in the training configurations —&nbsp;i.e., a range of model sizes might provide very similar utility if paired with the correct number of iterations and/or batch size.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"sv3qw\">Applying the scaling laws to build VaultGemma</h2><p data-block-key=\"69vq3\">The <a href=\"https://deepmind.google/models/gemma/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma</a> models are designed with responsibility and safety at their core. This makes them a natural foundation for developing a production-quality, DP-trained model like VaultGemma.</p><h3 data-block-key=\"d6ih5\">Algorithmic advancements: Training at scale</h3><p data-block-key=\"ac2lq\">The scaling laws we derived above represent an important first step towards training a useful Gemma model with DP. We used the scaling laws to determine both how much compute we needed to train a compute-optimal 1B parameter Gemma 2-based model with DP, and how to allocate that compute among batch size, iterations, and sequence length to achieve the best utility.</p><p data-block-key=\"2tili\">One prominent gap between the research underlying the scaling laws and the actual training of VaultGemma was our handling of <a href=\"https://en.wikipedia.org/wiki/Poisson_sampling\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Poisson sampling</i></a>, which is a central component of <a href=\"https://arxiv.org/abs/1607.00133\" target=\"_blank\" rel=\"noopener noreferrer\">DP-SGD</a>. We initially used a straightforward method of loading data in uniform batches but then switched to Poisson sampling to get the best privacy guarantees with the least amount of noise. This method posed two main challenges: it created batches of different sizes, and it required a specific, randomized order for processing the data. We solved this by using our recent work on <a href=\"https://arxiv.org/abs/2411.04205\" target=\"_blank\" rel=\"noopener noreferrer\">Scalable DP-SGD</a>, which allows us to process data in fixed-size batches — either by adding extra padding or trimming them — while still maintaining strong privacy protections.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"sv3qw\">Results</h2><p data-block-key=\"efh07\">Armed with our new scaling laws and advanced training algorithms, we built VaultGemma, to date the largest (1B-parameters) open model fully pre-trained with differential privacy with an approach that can yield high-utility models.</p><p data-block-key=\"25ku7\">From training VaultGemma, we found our scaling laws to be highly accurate. The final training loss of VaultGemma was remarkably close to what our equations predicted, validating our research and providing the community with a reliable roadmap for future private model development.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/VaultGemma4_Performance.width-1250.png\" alt=\"VaultGemma4_Performance\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/VaultGemma4_Performance.width-1250.png\" alt=\"VaultGemma4_Performance\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"yu3nj\"><i>Performance comparison of VaultGemma 1B (differentially private) against its non-private counterpart (Gemma3 1B) and an older baseline (GPT-2 1.5B). The results quantify the current resource investment required for privacy and demonstrate that modern DP training yields utility comparable to non-private models from roughly five years ago.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"sv3qw\">We also compare downstream performance of our model against its non-private counterpart across a range of standard academic benchmarks (i.e., <a href=\"https://arxiv.org/abs/1905.07830\" target=\"_blank\" rel=\"noopener noreferrer\">HellaSwag</a>, <a href=\"https://arxiv.org/abs/1905.10044\" target=\"_blank\" rel=\"noopener noreferrer\">BoolQ</a>, <a href=\"https://arxiv.org/abs/1911.11641\" target=\"_blank\" rel=\"noopener noreferrer\">PIQA</a>, <a href=\"https://arxiv.org/abs/1904.09728\" target=\"_blank\" rel=\"noopener noreferrer\">SocialIQA</a>, <a href=\"https://arxiv.org/abs/1705.03551\" target=\"_blank\" rel=\"noopener noreferrer\">TriviaQA</a>, <a href=\"https://arxiv.org/abs/1911.01547\" target=\"_blank\" rel=\"noopener noreferrer\">ARC-</a>C, <a href=\"https://arxiv.org/abs/1911.01547\" target=\"_blank\" rel=\"noopener noreferrer\">ARC-</a>E ). To put this performance in perspective and quantify the current resource investment required for privacy, we also include a comparison to an older similar-sized GPT-2 model, which performs similarly on these benchmarks. This comparison illustrates that today’s private training methods produce models with utility comparable to that of non-private models from roughly 5 years ago, highlighting the important gap our work will help the community systematically close.</p><p data-block-key=\"36cfk\">Finally, the model comes with strong theoretical and empirical privacy protections.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"sv3qw\">Formal privacy guarantee</h3><p data-block-key=\"2ivmr\">In general, both the privacy parameters (ε, δ) and the privacy <i>unit</i> are important considerations when doing DP training, as these together determine what the trained model can learn. VaultGemma was trained with a <i>sequence</i>-level DP guarantee of (ε ≤ 2.0, δ ≤ 1.1e-10), where a sequence consists of 1024 consecutive tokens extracted from heterogeneous data sources. Specifically, we used the same training mixture that was used to train the <a href=\"https://arxiv.org/abs/2408.00118\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma 2</a> model, consisting of a number of documents of varying lengths. During pre-processing, long documents are split up and tokenized into multiple sequences, and shorter documents are packed together into a single sequence. While the sequence-level privacy unit was a natural choice for our training mixture, in situations where there is a clear mapping between data and users, <a href=\"https://research.google/blog/fine-tuning-llms-with-user-level-differential-privacy/\">user-level differential privacy</a> would be a better choice.</p><p data-block-key=\"b4na6\">What does this mean in practice? Informally speaking, because we provide protection at the sequence level, if information relating to any (potentially private) fact or inference occurs in a single sequence, then VaultGemma essentially does not know that fact: the response to any query will be statistically similar to the result from a model that never trained on the sequence in question. However, if many training sequences contain information relevant to a particular fact, then in general VaultGemma will be able to provide that information.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"sv3qw\">Empirical memorization</h3><p data-block-key=\"1td9o\">To complement our sequence-level DP guarantee, we conduct additional tests of the empirical privacy properties of the trained model. To do so, we prompted the model with a 50-token prefix from a training document to see if it would generate the corresponding 50-token suffix. VaultGemma 1B shows no detectable memorization of its training data and successfully demonstrates the efficacy of DP training.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"sv3qw\">Conclusion</h2><p data-block-key=\"ej03m\">VaultGemma represents a significant step forward in the journey toward building AI that is both powerful and private by design. By developing and applying a new, robust understanding of the scaling laws for DP, we have successfully trained and released the largest open, DP-trained language model to date.</p><p data-block-key=\"bv1kj\">While a utility gap still exists between DP-trained and non-DP–trained models, we believe this gap can be systematically narrowed with more research on mechanism design for DP training. We hope that VaultGemma and our accompanying research will empower the community to build the next generation of safe, responsible, and private AI for everyone.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"sv3qw\">Acknowledgements</h2><p data-block-key=\"doolt\"><i>We'd like to thank the entire Gemma and Google Privacy teams for their contributions and support throughout this project, in particular, Peter Kairouz, Brendan McMahan and Dan Ramage for feedback on the blog post, Mark Simborg and Kimberly Schwede for help with visualizations, and the teams at Google that helped with algorithm design, infrastructure implementation, and production maintenance. The following people directly contributed to the work presented here (ordered alphabetically): Borja Balle, Zachary Charles, Christopher A. Choquette-Choo, Lynn Chua, Prem Eruvbetine, Badih Ghazi, Steve He, Yangsibo Huang, Armand Joulin, George Kaissis, Pritish Kamath, Ravi Kumar, Daogao Liu, Ruibo Liu, Pasin Manurangsi, Thomas Mesnard, Andreas Terzis, Tris Warkentin, Da Yu, and Chiyuan Zhang.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "推测性级联——一种更智能、更快速的LLM推理混合方法 (原标题: Speculative cascades — A hybrid approach for smarter, faster LLM inference)",
      "link": "https://research.google/blog/speculative-cascades-a-hybrid-approach-for-smarter-faster-llm-inference/",
      "pubDate": "Wed, 10 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-10T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 推测性级联：更智能、更快速的LLM推理混合方法\n\n大型语言模型（LLM）已彻底改变我们与技术的互动方式，但其推理过程（生成响应）通常缓慢且计算成本高昂。在不牺牲质量的前提下，提高LLM的速度并降低成本是一个关键挑战。\n\n## 现有优化方法\n\n文章介绍了两种主要的LLM推理优化方法，并以“Buzz Aldrin是谁？”的简单问题为例进行了说明：\n\n### 1. 级联（Cascades）\n*   **原理**：通过策略性地优先使用小型、快速模型，在必要时才调用大型、昂贵的LLM，以优化效率。\n*   **工作方式**：小型模型首先处理查询，并根据其置信度决定是自行响应还是将任务转交给更强大但成本更高的大型模型。\n*   **目标**：降低计算成本，高效分配资源，允许质量存在一定变动。\n*   **局限性**：如果小型模型不自信，需要等待其完成判断后才能启动大型模型，存在顺序执行的瓶颈。\n\n### 2. 推测解码（Speculative Decoding）\n*   **原理**：在不改变最终输出结果的前提下，优化LLM的延迟和吞吐量。\n*   **工作方式**：使用一个小型、快速的“草稿”模型预测一系列未来词元，然后由大型“目标”模型并行快速验证这些推测词元。如果草稿被接受，大型模型相当于一步生成了多个词元，从而大大加速了过程，并保证输出与大型模型独立生成的结果完全相同。\n*   **目标**：优先降低速度和延迟。\n*   **局限性**：可能增加内存使用，计算节省较少，因为大型模型仍需执行大量工作。在“Buzz Aldrin”示例中，即使小型模型给出了正确答案，但由于与大型模型首个词元不匹配（“Buzz”≠“Edwin”），整个草稿被拒绝，导致速度优势丧失，且最终输出不一定更优。\n\n## 两种方法的对比与权衡\n\n下图总结了级联和推测解码在目标和权衡上的根本差异：\n\n![SpecCascades-0.5-Table](https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-0.5-Table.width-1250.png)\n\n下图直观展示了标准级联和推测解码所提供的权衡：\n\n![SpecCascades-1-TradeOffs](https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-1-TradeOffs.width-1250.png)\n*   左图：标准级联通过改变置信度阈值提供不同的成本-质量权衡（绿星为小型模型，红星为大型模型，点代表不同权衡）。\n*   右图：推测解码的权衡（蓝星）。\n\n## 推测性级联：两全其美的混合方法\n\n文章引入了“推测性级联”（speculative cascades），它结合了标准级联的分层处理思想和推测解码的加速机制。\n\n*   **核心创新**：用灵活的“延迟规则”取代了推测解码中严格的验证机制。\n*   **工作方式**：\n    1.  小型模型生成“草稿”输出。\n    2.  大型模型同时并行验证该草稿并提供自己的评分。\n    3.  关键的“灵活延迟规则”会根据两个模型的输出动态地逐词元决定是接受小型模型的草稿还是转交给大型模型。\n*   **优势**：\n    *   避免了标准级联的顺序瓶颈。\n    *   即使小型模型的答案与大型模型的首选输出不完全匹配，系统也能接受其良好的答案。\n    *   在相同的质量水平下，比推测解码更快，即每次调用大型模型能生成更多词元。\n    *   实现了比单独使用任一技术更好的LLM输出质量和更低的计算成本。\n\n### 灵活的延迟规则\n\n推测性级联的强大之处在于其灵活性，延迟规则可以根据不同需求进行定制，例如：\n\n*   **简单置信度检查**：仅当小型模型对其预测不自信时才延迟。\n*   **比较检查**：如果大型模型比小型模型明显更自信，则延迟。\n*   **成本效益分析**：仅当大型模型的置信度提升超过拒绝小型模型草稿的“成本”时才延迟。\n*   **词元特定检查**：如果小型模型草稿的词元不在大型模型“批准列表”（其排名靠前的词元）中，则延迟。\n\n下图展示了推测性级联的框图：\n\n![SpecCascades-2-BlockDiagram](https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-2-BlockDiagram.width-1250.png)\n*   与标准推测解码类似，草稿过程涉及小型草稿模型的自回归采样。\n*   但验证过程不同：它通过延迟规则考虑小型和大型模型的组合输出分布，而不仅仅依赖于大型模型的输出。\n\n下图通过一个GSM8K数据集中的数学问题示例，可视化了推测性级联与推测解码的行为对比：\n\n![SpecCascades-3-Comparison](https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-3-Comparison.width-800.gif)\n*   推测性级联能够更快地达到正确答案。\n\n## 实验结果\n\n文章在摘要、推理和编码等一系列基准测试中对推测性级联进行了测试。结果表明：\n\n*   推测性级联相比推测解码具有明显优势。\n*   在标准的质量-效率图上，推测性级联始终提供更好的权衡。\n*   在相同的质量水平下，推测性级联方法更快，即每次调用大型模型能生成更多词元。\n\n下图展示了推测性级联在数学推理和摘要任务上的性能：\n\n![SpecCascades-4-Performance](https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-4-Performance.width-1250.png)\n*   推测性级联变体（蓝色和橙色）相比标准推测解码（绿色星号）实现了更好的质量-延迟权衡。\n\n## 结论\n\n随着LLM日益融入日常应用，优化其性能已成为实际需求。推测性级联通过重新思考级联和推测解码的结合方式，为开发者提供了一个更强大、更灵活的工具。这种混合方法允许对成本-质量平衡进行精细控制，为开发更智能、更快速的应用程序铺平了道路。",
      "shortSummary": "大型语言模型（LLM）推理缓慢且成本高昂。为解决此问题，文章提出了“推测性级联”方法，它结合了传统级联和推测解码的优点。该方法利用小型模型生成草稿，大型模型并行验证，并通过灵活的逐词元延迟规则，动态决定接受小型模型输出或转交给大型模型。这避免了传统方法的瓶颈，显著提高了LLM推理速度，同时在保证输出质量的前提下降低了计算成本，实现了更优的成本-质量权衡，使LLM应用更智能、更快速。",
      "translated_title": "推测性级联——一种更智能、更快速的LLM推理混合方法",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-0.5-Table.width-1250.png",
          "alt": "SpecCascades-0.5-Table",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-1-TradeOffs.width-1250.png",
          "alt": "SpecCascades-1-TradeOffs",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-2-BlockDiagram.width-1250.png",
          "alt": "SpecCascades-2-BlockDiagram",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-3-Comparison.width-800.gif",
          "alt": "SpecCascades-3-Comparison",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-4-Performance.width-1250.png",
          "alt": "SpecCascades-4-Performance",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"t9039\">LLMs have transformed how we interact with technology, powering everything from advanced search capabilities to creative coding assistants. But this power comes at a cost: inference (the process of generating a response) can be slow and computationally expensive. As we deploy these models to more users, making them faster and less expensive without sacrificing quality is a critical challenge.</p><p data-block-key=\"1pvn0\">One way to accomplish this would be to use <a href=\"https://openreview.net/pdf?id=XUZ2S0JVJP\" target=\"_blank\" rel=\"noopener noreferrer\">cascades</a>, which aim to optimize LLM efficiency by strategically using smaller, faster models before engaging a larger, more expensive LLM. This approach involves a <i>deferral rule</i> where the smaller model decides if it can handle a query or if it needs to pass the task to a more capable, but costlier, large model. The goal is to process as much as possible cheaply and quickly, only incurring the high cost of the large LLM for complex tasks that truly require its advanced capabilities, potentially yielding favorable cost-quality trade-offs. Cascades prioritize computational cost reduction and efficient resource allocation, while allowing for some variability in quality.</p><p data-block-key=\"70hdi\">Another approach, <a href=\"https://research.google/blog/looking-back-at-speculative-decoding/\">speculative decoding</a>, optimizes an LLM’s latency and throughput <i>without altering the final result</i>. It achieves this by employing a smaller, faster \"drafter\" model to predict a sequence of future tokens. These speculated tokens are then quickly verified in parallel by the larger “target” model. If the draft is accepted, the large model effectively generates multiple tokens in a single step, greatly accelerating the process while guaranteeing that the final output is identical to what the large model would have produced on its own. This approach prioritizes speed and latency reduction, potentially at the cost of increased memory usage and less computational savings, since the larger model still performs substantial work.</p><p data-block-key=\"3ie9m\">In “<a href=\"https://arxiv.org/abs/2405.19261\" target=\"_blank\" rel=\"noopener noreferrer\">Faster Cascades via Speculative Decoding</a>”, we introduce “speculative cascades”, a new approach that combines the best of both cascades and speculative decoding. It delivers better LLM output quality at a lower computational cost than either technique alone by sometimes deferring to the smaller LLM for the sake of efficiency. We tested new speculative cascading techniques against standard cascading and speculative decoding baselines using <a href=\"https://ai.google.dev/gemma/docs\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma</a> and <a href=\"https://research.google/blog/exploring-transfer-learning-with-t5-the-text-to-text-transfer-transformer/\">T5</a> models on various language tasks, including summarization, translation, reasoning, coding, and question answering. The results show that the proposed speculative cascades achieve better cost-quality trade-offs, often yielding higher speed-ups and better quality metrics compared to the baselines.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">A deeper look</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"t9039\">To fully understand and appreciate the speculative cascades approach, we first compare cascades and speculative decoding with a simple example. Imagine you ask an LLM a straightforward question:</p><p data-block-key=\"bt6cf\"><b>Prompt:</b> \"<span class=\"rte-font-courier\">Who is Buzz Aldrin?</span>\"</p><p data-block-key=\"4c9f2\">Let's say we have two models available to answer this: a small, fast \"drafter\" model and a large, powerful \"expert\" model.</p><p data-block-key=\"30s2s\">Here's how they might respond:</p><ul><li data-block-key=\"7afbh\"><b>Small Model:</b> <span class=\"rte-font-courier\">Buzz Aldrin is an American former astronaut, engineer, and fighter pilot, best known as the second person to walk on the Moon.<br><br></span></li><li data-block-key=\"60evu\"><b>Large Model:</b> <span class=\"rte-font-courier\">Edwin \"Buzz\" Aldrin, a pivotal figure in the history of space exploration, is an American former astronaut, engineer, and fighter pilot who is best known for being the second human to walk on the Moon.</span></li></ul><p data-block-key=\"cmc5l\">Both models provide excellent, factually correct answers, but they interpret the user's intent slightly differently. The small model delivers a quick, factual summary, while the large model provides a more formal, encyclopedic-style entry. Depending on the user's need — be it a fast fact or a detailed overview — either response could be considered ideal. The key is that they represent two distinct, equally valid styles.</p><p data-block-key=\"11i2p\">Now, let's see how the two main speed-up techniques handle this scenario.</p><p data-block-key=\"do2eo\">With cascades, the small \"drafter\" model gets the prompt first. If it's confident in its answer, it replies. If not, it defers the entire task to the large \"expert\" model.</p><p data-block-key=\"ajqif\"><b>In our example:</b></p><ol><li data-block-key=\"1c0nq\">The small model generates its concise and correct answer.</li><li data-block-key=\"d3e78\">It checks its confidence and, finding it high, sends the response to the user.</li></ol><p data-block-key=\"45cg5\">This works! We get a great answer quickly. But the process is sequential. If the small model <i>hadn't</i> been confident, we would have wasted time waiting for it to finish, only to then start the large model from scratch. This sequential \"wait-and-see\" approach is a fundamental bottleneck.</p><p data-block-key=\"9b4k2\">With speculative decoding, the small model quickly drafts the first few tokens of the answer, and the large model verifies it in parallel, correcting the first mistake it finds.</p><p data-block-key=\"b7302\"><b>In our example:</b></p><ol><li data-block-key=\"1n312\">The small model drafts the beginning of its answer: [<span class=\"rte-font-courier\">Buzz</span>, <span class=\"rte-font-courier\">Aldrin</span>, <span class=\"rte-font-courier\">is</span>, <span class=\"rte-font-courier\">an</span>, ...]</li><li data-block-key=\"b56s2\">The large model verifies this draft. Its own preferred first token is <span class=\"rte-font-courier\">Edwin</span>.</li><li data-block-key=\"5j6np\">Since <span class=\"rte-font-courier\">Buzz</span> ≠ <span class=\"rte-font-courier\">Edwin</span>, the very first token is a mismatch.</li><li data-block-key=\"ci65q\">The entire draft is <i>rejected</i> and the first token is replaced with <span class=\"rte-font-courier\">Edwin</span>. The process then repeats from this corrected point to generate the rest of the answer, but the initial speed advantage has been lost.</li></ol><p data-block-key=\"9btog\">Even though the small model produced a good answer, the requirement to match the large model token-by-token forces a rejection. We lose the speed benefit and end up with an answer that is not necessarily superior. While the above example uses a simple token matching rejection rule, in the full paper, we also include the potential for a \"probabilistic match\" that provides greater flexibility in the token-by-token comparison.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Different goals, different trade-offs</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"t9039\">The \"<span class=\"rte-font-courier\">Buzz Aldrin</span>\" example reveals a fundamental difference between these two techniques, as summarized below:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-0.5-Table.width-1250.png\" alt=\"SpecCascades-0.5-Table\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-0.5-Table.width-1250.png\" alt=\"SpecCascades-0.5-Table\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-1-TradeOffs.width-1250.png\" alt=\"SpecCascades-1-TradeOffs\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-1-TradeOffs.width-1250.png\" alt=\"SpecCascades-1-TradeOffs\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"f3ur2\"><i>A visual representation of the trade-offs offered by standard cascades (</i><b><i>left</i></b><i>) and speculative decoding (</i><b><i>right</i></b><i>). In both graphs, the green star is the small, fast model (low cost, lower quality) and the red star is the large, slow model (high cost, higher quality). The dots in the left graph represent different trade-offs offered by cascades by varying its confidence threshold; the blue star in the right graph represents the trade-off offered by speculative decoding.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Speculative cascades: Best of both worlds</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"t9039\">Speculative cascades combine the idea of tiered processing from standard cascades with the speedup mechanism of speculative decoding. It involves a smaller model generating a \"draft\" output that a larger model then quickly verifies in parallel. The key innovation is replacing the strict verification of speculative decoding with a flexible “deferral rule”<b>.</b> This rule dynamically decides, on a token-by-token basis, whether to accept the small model's draft or defer to the large model. This avoids the sequential bottleneck of standard cascades while allowing the system to accept a good answer from the small model even if it doesn't exactly match the large model's preferred output.</p><p data-block-key=\"7ecph\"><b>In our example:</b></p><ol><li data-block-key=\"7cjdd\">The small model drafts the beginning of its answer: [<span class=\"rte-font-courier\">Buzz</span>, <span class=\"rte-font-courier\">Aldrin</span>, <span class=\"rte-font-courier\">is</span>, <span class=\"rte-font-courier\">an</span>, ...]</li><li data-block-key=\"26jv4\">Simultaneously, the large model evaluates the draft, providing its own scores.</li><li data-block-key=\"e9v8v\"><i>The crucial step:</i> A flexible deferral rule looks at both outputs and decides whether a deferral is warranted.</li><li data-block-key=\"aidge\">If the system decides <i>not to defer</i>, it accepts the small model's draft tokens. The process then efficiently repeats from this new point, drafting and verifying the next chunk of text until the answer is complete.</li></ol><p data-block-key=\"1nk0\">The power of this method lies in its flexibility, as the deferral rule can be tailored to different needs.</p><p data-block-key=\"6s5o7\">For example, we could tell the system to defer based on:</p><ul><li data-block-key=\"1j410\"><i>A simple confidence check</i>: Defer only if the small model isn't very confident in its own prediction.</li><li data-block-key=\"67efs\"><i>A comparative check</i>: Defer if the large model is significantly more confident than the small model.</li><li data-block-key=\"7ees\"><i>A cost-benefit analysis</i>: Defer only if the large model's confidence boost outweighs the \"cost\" of rejecting the small model's draft.</li><li data-block-key=\"6u3i0\"><i>A token-specific check</i>: Given an \"approved list\" of the best next words according to the large model (its top-ranked tokens), we defer if the small model's drafted token is <i>not</i> on this list.</li></ul><p data-block-key=\"61ar4\">This ability to plug in different decision-making logic is what gives speculative cascades their unique blend of speed, quality, and adaptability.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-2-BlockDiagram.width-1250.png\" alt=\"SpecCascades-2-BlockDiagram\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-2-BlockDiagram.width-1250.png\" alt=\"SpecCascades-2-BlockDiagram\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"f3ur2\"><i>Block diagram illustrating a speculative cascade between a small and large model. As with standard speculative decoding, the drafting process involves auto-regressive sampling from the small drafter model. However, the verification process is different: it considers the combined output distribution of both the small and large models via a deferral rule, rather than solely relying on the large model's output.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"t9039\">Below, we visualize the behaviour of speculative cascading versus speculative decoding on a prompt from the <a href=\"https://github.com/openai/grade-school-math\" target=\"_blank\" rel=\"noopener noreferrer\">GSM8K dataset</a>. The prompt asks, “Mary has 30 sheep. She gets 1 kg of milk from half of them and 2 kg of milk from the other half every day. How much milk does she collect every day?“ By carefully leveraging the small model's output on certain tokens, speculative cascading can reach a correct solution faster than regular speculative decoding.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-3-Comparison.width-800.gif\" alt=\"SpecCascades-3-Comparison\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-3-Comparison.width-800.gif\" alt=\"SpecCascades-3-Comparison\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"f3ur2\"><i>Comparison of speculative cascades and speculative decoding on a grade school math question from the GSM8K dataset</i>.<i> The draft tokens are shown in yellow and the verified tokens in red. The speculative cascades approach generates the correct answer, and does so faster than speculative decoding.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Experiments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"t9039\">We tested speculative cascades on a range of benchmarks, including summarization, reasoning, and coding. The results show a clear advantage over speculative decoding. On a standard quality-versus-efficiency graph, speculative cascades consistently provide better trade-offs. This means for the same quality level as speculative decoding, our method is faster, i.e., generates more tokens per call to the larger model.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-4-Performance.width-1250.png\" alt=\"SpecCascades-4-Performance\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-4-Performance.width-1250.png\" alt=\"SpecCascades-4-Performance\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"f3ur2\"><i>Speculative cascades variants (blue and orange) achieve better quality-latency trade-offs compared to standard speculative decoding (green star) on math reasoning and summarization tasks. See</i> <a href=\"https://arxiv.org/abs/2405.19261\" target=\"_blank\" rel=\"noopener noreferrer\"><i>paper</i></a><i> for details.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Towards faster and smarter AI with speculative cascades</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"t9039\">As LLMs become more integrated into daily applications, optimizing their performance isn’t just a technical goal, it’s a practical necessity. By rethinking how cascades and speculative decoding can work together, speculative cascades provide a more powerful and flexible tool for developers. This hybrid approach allows for fine-grained control over the cost-quality balance, paving the way for applications that are both smarter and faster.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"t9039\"><i>This work is a collaborative effort with Wittawat Jitkrittum, Ankit Singh Rawat, Seungyeon Kim, Neha Gupta and Sanjiv Kumar. We are grateful to Ananda Theertha Suresh and Ziteng Sun for their insightful discussions, and Yale Cong, Mark Simborg, and Kimberly Schwede for their help in crafting this blog.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "使用NucleoBench和AdaBeam进行更智能的核酸设计 (原标题: Smarter nucleic acid design with NucleoBench and AdaBeam)",
      "link": "https://research.google/blog/smarter-nucleic-acid-design-with-nucleobench-and-adabeam/",
      "pubDate": "Wed, 10 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-10T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 使用NucleoBench和AdaBeam实现更智能的核酸设计\n\n### 引言：核酸设计的挑战\n\n在现代医学中，设计具有特定治疗特性的新型DNA和RNA序列是一项关键挑战。这些分子是下一代疗法（如更精确的CRISPR基因疗法和更稳定有效的mRNA疫苗）的基石。然而，寻找正确的序列极其困难，例如，一个小的RNA功能区（5' UTR）可能有超过2x10^120种序列组合，使得穷举搜索优化其功能变得不可能。尽管AI模型在预测核酸序列特性方面取得了巨大进展，但利用这些模型生成最佳序列的算法仍有创新空间。缺乏标准化的评估阻碍了将强大的预测模型转化为最佳治疗分子的进程。\n\n### 解决方案：NucleoBench和AdaBeam\n\n为了解决这一差距，Google Research和Move37 Labs合作推出了**NucleoBench**，这是首个用于比较核酸设计算法的大规模标准化基准。通过在16个不同的生物学挑战中运行超过400,000次实验，研究团队创建了一个严格评估和理解不同算法性能的框架。基于这些洞察，他们开发了**AdaBeam**，一种混合设计算法，在16项任务中的11项上优于现有方法，并能更有效地扩展到定义生物学AI未来的大型复杂模型。AdaBeam及其所有算法实现均已免费提供，以促进进一步创新。\n\n### 计算核酸设计的核心挑战与工作流程\n\n计算机辅助设计新核酸序列通常遵循四个步骤：\n1.  **生成数据**：收集具有所需特性（例如，与癌症相关蛋白结合）的高质量序列数据集。\n2.  **训练预测模型**：使用这些数据训练一个模型（通常是神经网络），该模型可以根据DNA或RNA序列预测其特性。\n3.  **生成候选序列**：这是关键的设计步骤。使用优化算法生成模型预测具有最高所需特性的新序列。\n4.  **验证候选序列**：在湿实验室中合成并测试最有前景的序列，以验证其是否如预测般工作。\n5.  **重新训练（可选）**：根据验证数据重新训练模型。\n\n![计算核酸设计的典型工作流程](https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-1.width-1250.png)\n\n*计算核酸设计的典型工作流程。本研究重点关注步骤3的设计算法。*\n\n目前，不同的研究团队使用不同的算法并在不同的任务上进行测试，这使得无法确定哪些方法是真正最好的。大多数现有基准依赖于模拟退火或香草遗传算法等方法，这些算法在现代深度学习出现前几十年就已经开发出来，无法利用神经网络模型中的关键信息（如梯度）。\n\n### NucleoBench基准的详细介绍\n\n为了创建一个全面且公平的基准，研究团队选择了多种无梯度和基于梯度的设计算法。无梯度算法包括定向进化和模拟退火等成熟方法，它们分别受到进化和物理过程的启发。这些算法将预测性AI模型视为“黑箱”，在无需理解模型内部工作原理的情况下测试新序列。它们的优势在于简单性和广泛适用性，但也可能因此错过模型提供的宝贵线索。基于梯度的设计算法利用神经网络的内部工作原理，包括FastSeqProp和Ledidi等更现代的算法。它们使用模型的梯度（即最陡峭改进的方向）智能地指导搜索更好的序列，但计算时间比仅使用神经网络输出更长。\n\nNucleoBench是迄今为止最全面的核酸设计算法基准，允许对算法进行公平的“同类比较”。研究团队在相同的16项任务上，使用相同的起始序列评估了9种不同的算法，从而获得了前所未有的统计能力来得出有意义的结论。这些任务涵盖了广泛的生物学挑战，包括：\n*   控制特定细胞类型（如肝细胞或神经元细胞）中的基因表达。\n*   最大化转录因子（调节基因的蛋白质）的结合。\n*   改善染色质的物理可及性以进行生物分子相互作用。\n*   使用Enformer等大规模模型预测超长DNA序列的基因表达。\n\n| 任务类别                 | 描述                                                                                              | 任务数量 | 序列长度 (bp) | 速度 (ms / 示例) |\n| :----------------------- | :------------------------------------------------------------------------------------------------ | :------- | :------------ | :--------------- |\n| 细胞类型特异性顺式调控活性 | DNA序列如何控制来自同一DNA分子的基因表达。细胞类型包括：前体血细胞、肝细胞、神经元细胞。 | 3        | 200           | 2                |\n| 转录因子结合             | 特定转录因子与特定DNA片段结合的可能性。                                                           | 11       | 3000          | 55               |\n| 染色质可及性             | DNA与其他分子相互作用的物理可及性。                                                               | 1        | 3000          | 260              |\n| 选择性基因表达           | 基因表达预测。                                                                                    | 1        | 196,608 / 256*| 15,000           |\n\n*NucleoBench设计任务总结。* *模型输入长度为200K碱基对（bp），但只编辑256 bp。\n\n研究团队引入了计算机科学中的有序和无序束搜索算法，以测试序列编辑顺序的固定方法与更灵活的随机顺序方法之间的比较。他们还创建了**Gradient Evo**，这是一种新颖的混合算法，通过使用模型梯度指导其突变来增强定向进化算法，以独立评估梯度对于编辑位置选择与选择特定编辑的重要性。\n\n### AdaBeam：一种创新的混合自适应束搜索算法\n\n研究团队还开发了**AdaBeam**，一种混合自适应束搜索算法，它结合了无序束搜索和AdaLead（一种表现最佳的非梯度设计算法）中最有效的元素。自适应搜索算法通常不会随机探索；相反，它们的行为会随着搜索结果而改变，以将其精力集中在序列空间中最有前景的区域。AdaBeam的混合方法维护一个“束”（即迄今为止找到的最佳候选序列集合），并贪婪地扩展特别有前景的候选序列，直到它们被充分探索。\n\n在实践中，AdaBeam从一组候选序列及其分数开始。在每一轮中，它首先选择一小部分得分最高的序列作为“父代”。对于每个父代，AdaBeam通过进行随机数量的随机但有指导的突变来生成一组新的“子代”序列。然后，它遵循一条简短的贪婪探索路径，使算法能够快速在适应度景观中“上坡”。经过充分探索后，所有新生成的子代都被汇集在一起，算法选择绝对最佳的序列形成下一轮的起始种群，重复该循环。这种自适应选择和有针对性突变的过程使AdaBeam能够高效地专注于高性能序列。\n\n计算机辅助设计任务由于其极其庞大的搜索空间而带来了困难的工程问题。随着我们尝试设计更长的序列（如mRNA序列）并使用现代大型神经网络来指导设计，这些困难变得更加突出。AdaBeam通过使用固定计算的概率采样（而不是随序列长度扩展的计算）在长序列上特别高效。为了使AdaBeam能够与大型模型配合使用，研究团队通过引入一种称为“梯度拼接”的技巧来减少设计过程中的峰值内存消耗。然而，现有不具备这些功能的设计算法难以扩展到长序列和大型模型，特别是基于梯度的算法受影响更大。为了促进公平比较，研究团队限制了设计序列的长度，即使AdaBeam可以处理更长更大的序列。例如，尽管DNA表达预测模型Enformer运行在约200K核苷酸序列上，但设计仅限于256个核苷酸。\n\n![NucleoBench中的设计算法总结](https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-2.width-1250.png)\n\n*NucleoBench中的设计算法总结。实线下方是本研究中设计出的算法。*\n\n### 评估结果\n\n研究团队根据每个算法生成的序列的最终适应度分数来评估每个设计算法。适应度分数定义为序列根据预测模型在生物学任务上的表现。为确保公平性，他们进行了超过400,000次实验，其中每个设计算法在每个任务上都获得了固定的时间量和完全相同的100个起始序列。他们还测量了收敛速度，跟踪每个算法找到其最佳解决方案的速度，因为更快的算法可以节省宝贵的时间和计算资源。\n\n研究团队通过测量算法的最终分数受随机机会和起始序列影响的程度来表征性能变异性。他们通过使用五个不同的随机种子重新运行实验来量化算法随机性的影响。为了评估起始点的影响，他们分析了给予每个设计算法的100个相同起始序列的最终分数方差。他们使用Friedman检验来调查是否存在“本质上困难的起始序列”，即所有算法都难以优化的序列。\n\n为了评估性能排名的分布，研究团队比较了NucleoBench基准中每个实验中九种算法在每个任务和起始序列独特组合下的最终性能。然后分配一个基于排名的“顺序分数”（0到8），其中0分分配给表现最佳的算法，1分分配给第二名，依此类推。每个小提琴形状是通过聚合单个算法在超过400,000次实验中获得的所有排名分数构建的，小提琴在任何一点的宽度表示该算法获得特定排名的频率。\n\n![每种算法最终分数的分布](https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-3.width-1250.png)\n\n*每种算法最终分数的分布。X轴是设计算法，Y轴是聚合顺序分数。顺序分数通过根据每个（任务，起始序列）对的所有最终序列的性能，为每个（任务，起始序列，设计算法）元组分配一个整数[0, 9]来确定。0是表现最好的。聚合分数通过对所有此类分数进行平均计算。*\n\n在现有方法中，基于梯度的方法是主导者。然而，研究团队发现AdaBeam超越了它们，这表明依赖梯度并非实现顶级性能和可扩展性的唯一途径。\n\nAdaBeam在几个关键方面改进了以前的方法：\n*   **效率**：它用更快的计算取代了AdaLead的采样步骤，使长序列的速度提高了一倍。\n*   **智能探索**：它使用一种明显更有效的“无序”方法来决定在哪里编辑序列。\n*   **先进工程**：它使用梯度拼接显著减少内存使用，从而能够应用于Enformer等大型模型。\n\n在NucleoBench的16项任务中，AdaBeam在11次中表现最佳。它还被证明是收敛到高质量解决方案最快的算法之一，展示了卓越的扩展特性，这对于应对生物学中下一代AI挑战至关重要。\n\n### 未来方向与负责任的创新\n\nNucleoBench基准揭示了严格标准化评估的重要性，并发现了令人惊讶的结果，例如初始序列的关键影响以及一些既定算法特征的无效性。然而，重大挑战依然存在。最好的基于梯度的方法仍然难以扩展到最大的模型和最长的序列，通过更好的软件工程可以实现显著的可扩展性提升。虽然AdaBeam算法树立了新的技术标杆，但未来的工作必须专注于符合生物学约束并提高可扩展性的算法。\n\n这项工作的核心原则是对生物安全和负责任创新的承诺。AdaBeam代表了生物序列设计向前迈出的一步，但它仅根据预先存在的预测模型改进优化。换句话说，它是一个优化器，而不是一个原创者；该算法只能设计序列以最大化用户提供的预测模型定义的目标。通过将AdaBeam作为开源工具发布，研究团队赋能研究人员，同时确保“人在回路中”仍然是生物分子设计的核心。像AdaBeam这样的算法可以帮助科学家设计更有效的mRNA疫苗，创建更安全的CRISPR基因疗法，并开发针对各种疾病的新型疗法，使AI驱动的药物发现的承诺更接近现实。\n\n### 致谢\n\n这项工作是Joel Shor（Move37 Labs）、Erik Strand（Move37 Labs, MIT）和Cory Y. McLean（Google Research）之间的合作成果。感谢Sager Gosai、Daniel Friedman、Anna Lewis、Vikram Agarwal和Michael Brenner在整个项目中的指导、讨论和支持。",
      "shortSummary": "核酸序列设计对现代医学至关重要，但面临巨大的序列空间和缺乏标准化评估的挑战。Google Research和Move37 Labs为此推出了**NucleoBench**，一个大规模标准化基准，并基于此开发了**AdaBeam**。AdaBeam是一种混合设计算法，在16项生物学任务中的11项上超越现有方法，并能高效扩展到大型模型。它通过智能探索、高效采样和内存优化，加速了AI驱动的药物发现。AdaBeam已开源，旨在促进生物序列设计的创新，并强调负责任的“人在回路中”的设计原则。",
      "translated_title": "使用NucleoBench和AdaBeam进行更智能的核酸设计",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-1.width-1250.png",
          "alt": "NucleoBench-1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-2.width-1250.png",
          "alt": "NucleoBench-2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-3.width-1250.png",
          "alt": "The distribution of final scores for each algorithm",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"n21gz\">Designing new DNA and RNA sequences with specific therapeutic properties is a critical challenge in modern medicine. These molecules are the building blocks for next-generation treatments, from more precise <a href=\"https://en.wikipedia.org/wiki/CRISPR_gene_editing\" target=\"_blank\" rel=\"noopener noreferrer\">CRISPR</a> gene therapies to more stable and effective <a href=\"https://pubmed.ncbi.nlm.nih.gov/39608721/\" target=\"_blank\" rel=\"noopener noreferrer\">mRNA vaccines</a>. However, finding the right sequence is like searching for a single grain of sand on a vast beach. For instance, a small functional region of an RNA molecule called the <a href=\"https://en.wikipedia.org/wiki/Five_prime_untranslated_region\" target=\"_blank\" rel=\"noopener noreferrer\">5' UTR</a> can be one of over 2x10<sup class=\"superscript\">120</sup> possible sequences, making a brute-force search to optimize its function impossible.</p><p data-block-key=\"b5st1\">What if we could use AI to navigate this vast search space, drastically cutting down the time and cost of drug discovery? While various efforts have made <a href=\"https://www.biorxiv.org/content/10.1101/2024.12.25.630221v2\" target=\"_blank\" rel=\"noopener noreferrer\">great strides</a> in developing AI models that <a href=\"https://www.nature.com/articles/s41592-021-01252-x\" target=\"_blank\" rel=\"noopener noreferrer\"><i>predict</i> the properties</a> of a given nucleic acid sequence, there remains opportunity to innovate on the algorithms that use these models to <i>generate</i> optimal sequences. A lack of standardized evaluation hinders progress and prevents us from translating powerful predictive models into the best possible therapeutic molecules.</p><p data-block-key=\"ai4e3\">To address this gap, in a research collaboration between Google Research and <a href=\"https://move37labs.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Move37 Labs</a>, we <a href=\"https://www.biorxiv.org/content/10.1101/2025.06.20.660785v3\" target=\"_blank\" rel=\"noopener noreferrer\">introduce NucleoBench</a>, the first large-scale, standardized benchmark for comparing nucleic acid design algorithms. By running over 400,000 experiments across 16 distinct biological challenges, we've created a framework to rigorously evaluate and understand how different algorithms perform. The insights from this work enabled us to develop <a href=\"https://pypi.org/project/nucleobench/\" target=\"_blank\" rel=\"noopener noreferrer\">AdaBeam</a>, a hybrid design algorithm that outperforms existing methods on 11 of the 16 tasks and scales more effectively to the large and complex models that are defining the future of AI in biology. We have made AdaBeam and all of our algorithm implementations <a href=\"https://github.com/move37-labs/nucleobench\" target=\"_blank\" rel=\"noopener noreferrer\">freely available</a> to spur further innovation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The core challenge</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"n21gz\">The process of designing a new nucleic acid sequence using computers generally follows four steps:</p><ol><li data-block-key=\"d6smh\"><b>Generate data</b>: Collect a high-quality dataset of sequences with the desired property (e.g., binding to a cancer-related protein).</li><li data-block-key=\"9iv8s\"><b>Train a predictive model</b>: Use this data to train a model (often a neural network) that can predict the property from a DNA or RNA sequence.</li><li data-block-key=\"bej4p\"><b>Generate candidate sequences</b>: This is the crucial design step. Use an optimization algorithm to generate new sequences that the model predicts will have the highest possible score for the desired property.</li><li data-block-key=\"6g6pm\"><b>Validate candidates</b>: Synthesize and test the most promising sequences in a wet lab to see if they work as predicted.</li><li data-block-key=\"dsv4a\"><b>Retrain</b> [Optional]: Retrain the model on validation data.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-1.width-1250.png\" alt=\"NucleoBench-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-1.width-1250.png\" alt=\"NucleoBench-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"3cj2n\"><i>The typical workflow for computational nucleic acid design.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"n21gz\">In this work we focus on the design algorithms of step 3. At present, different research groups use different algorithms and test them on different tasks, making it impossible to know which methods are truly the best. Most existing benchmarks rely on algorithms like <a href=\"https://en.wikipedia.org/wiki/Simulated_annealing\" target=\"_blank\" rel=\"noopener noreferrer\">simulated annealing</a> or vanilla <a href=\"https://en.wikipedia.org/wiki/Genetic_algorithm\" target=\"_blank\" rel=\"noopener noreferrer\">genetic algorithms</a>, which were developed decades before modern deep learning and cannot take advantage of crucial information, like gradients, from the neural network models.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">NucleoBench</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"n21gz\">To create a comprehensive and fair benchmark, we selected a diverse slate of gradient-free and gradient-based design algorithms. Gradient-free algorithms include well-established methods like <a href=\"https://www.nature.com/articles/nrm2805\" target=\"_blank\" rel=\"noopener noreferrer\">directed evolution</a> and simulated annealing, which are inspired by processes in evolution and physics, respectively. These algorithms treat the predictive AI model as a \"black box\", and test new sequences without needing to understand <i>how</i> the model works internally. Their strength lies in their simplicity and broad applicability, but this means they potentially miss out on valuable clues from the model.</p><p data-block-key=\"cl0jv\">Gradient-based design algorithms leverage the internal workings of neural networks and include more modern algorithms like <a href=\"https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04437-5\" target=\"_blank\" rel=\"noopener noreferrer\">FastSeqProp</a> and <a href=\"https://www.biorxiv.org/content/10.1101/2020.05.21.109686v1\" target=\"_blank\" rel=\"noopener noreferrer\">Ledidi</a>. They use the model's gradients (i.e., the direction of steepest improvement) to intelligently guide the search for better sequences, but take longer to compute than just using the output of the neural network.</p><p data-block-key=\"9rg8h\">To our knowledge, NucleoBench is the most comprehensive benchmark for nucleic acid design algorithms thus far and allows for a fair, apples-to-apples comparison between algorithms. We evaluated 9 different algorithms on the same 16 tasks with the same starting sequences, giving us unprecedented statistical power to draw meaningful conclusions. These tasks span a wide range of biological challenges, including:</p><ul><li data-block-key=\"1m5be\"><b>Controlling gene expression</b> in specific cell types (e.g., liver or neuronal cells)</li><li data-block-key=\"dggfc\"><b>Maximizing the binding of transcription factors</b> (proteins that regulate genes)</li><li data-block-key=\"1l06b\"><b>Improving the physical accessibility of chromatin</b> for biomolecular interactions</li><li data-block-key=\"27hrt\"><b>Predicting gene expression from very long DNA sequences</b> using large-scale models like <a href=\"https://www.nature.com/articles/s41592-021-01252-x\" target=\"_blank\" rel=\"noopener noreferrer\">Enformer</a></li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    <div class=\"rich-text --theme- --mode-\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <table>\n<tbody>\n<tr>\n<td><strong>Task Category</strong></td>\n<td><strong>Description</strong></td>\n<td><strong>Num Tasks</strong></td>\n<td><strong>Seq Len (bp)</strong></td>\n<td><strong>Speed (ms / example)</strong></td>\n</tr>\n<tr>\n<td><a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC10441439/\" target=\"_blank\" rel=\"noopener noreferrer\">Cell-type specific cis-regulatory activity</a></td>\n<td>How DNA sequences control gene expression from the same DNA molecule. Cell types include: precursor blood cells, liver cells, neuronal cells</td>\n<td>3</td>\n<td>200</td>\n<td>2</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/jmschrei/bpnet-lite\" target=\"_blank\" rel=\"noopener noreferrer\">Transcription factor binding</a></td>\n<td>How likely a specific transcription factor will bind to a particular stretch of DNA</td>\n<td>11</td>\n<td>3000</td>\n<td>55</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/jmschrei/bpnet-lite\" target=\"_blank\" rel=\"noopener noreferrer\">Chromatin accessibility</a></td>\n<td>How physically accessible DNA is for interactions with other molecules</td>\n<td>1</td>\n<td>3000</td>\n<td>260</td>\n</tr>\n<tr>\n<td><a href=\"https://www.nature.com/articles/s41592-021-01252-x\" target=\"_blank\" rel=\"noopener noreferrer\">Selective gene expression</a></td>\n<td>Prediction of gene expression</td>\n<td>1</td>\n<td>196,608 / 256*</td>\n<td>15,000</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<p style=\"text-align: center;\"><em><small>Summary of design tasks in NucleoBench. *Model input length is 200K base pairs (bp), but only 256 bp are edited.</small></em></p>\n</div>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"n21gz\">We introduced ordered and unordered <a href=\"https://en.wikipedia.org/wiki/Beam_search\" target=\"_blank\" rel=\"noopener noreferrer\">beam search</a> algorithms, staples from computer science, to test how fixing the order of sequence edits compares to a more flexible, random-order approach. We also created Gradient Evo, a novel hybrid that enhances the directed evolution algorithm by using model gradients to guide its mutations to independently evaluate how important gradients were for edit location selection versus selecting a specific edit.</p><p data-block-key=\"4nku6\">We also developed AdaBeam, a hybrid adaptive beam search algorithm that combines the most effective elements of unordered beam search with <a href=\"https://arxiv.org/abs/2010.02141\" target=\"_blank\" rel=\"noopener noreferrer\">AdaLead</a>, a top-performing, non-gradient design algorithm. Adaptive search algorithms don't typically explore randomly; instead, their behavior changes as a result of the search to focus their efforts on the most promising areas of the sequence space. AdaBeam’s hybrid approach maintains a \"beam\", or a collection of the best candidate sequences found so far, and greedily expands on particularly promising candidates until they’ve been sufficiently explored.</p><p data-block-key=\"eggb8\">In practice, AdaBeam begins with a population of candidate sequences and their scores. In each round, it first selects a small group of the highest-scoring sequences to act as \"parents\". For each parent, AdaBeam generates a new set of \"child\" sequences by making a random number of random-but-guided mutations. It then follows a short, greedy exploration path, allowing the algorithm to quickly \"walk uphill\" in the fitness landscape. After sufficient exploration, all the newly generated children are pooled together, and the algorithm selects the absolute best ones to form the starting population for the next round, repeating the cycle. This process of adaptive selection and targeted mutation allows AdaBeam to efficiently focus on high-performing sequences.</p><p data-block-key=\"burv\">Computer-assisted design tasks pose difficult engineering problems, owing to the incredibly large search space. These difficulties become more acute as we attempt to design longer sequences, such as mRNA sequences, and use modern, large neural networks to guide the design. AdaBeam is particularly efficient on long sequences by using fixed-compute probabilistic sampling instead of computations that scale with sequence length. To enable AdaBeam to work with large models, we reduce peak memory consumption during design by introducing a trick we call “gradient concatenation.” However, existing design algorithms that don’t have these features have difficulty scaling to long sequences and large models. Gradient-based algorithms are particularly affected. To facilitate a fair comparison, we limit the length of the designed sequences, even though AdaBeam can scale longer and larger. For example, even though the DNA expression prediction model <a href=\"https://www.nature.com/articles/s41592-021-01252-x\" target=\"_blank\" rel=\"noopener noreferrer\">Enformer</a> runs on ~200K nucleotide sequences, we limit design to just 256 nucleotides.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-2.width-1250.png\" alt=\"NucleoBench-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-2.width-1250.png\" alt=\"NucleoBench-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"3cj2n\">Summary of design algorithms in NucleoBench. Below the solid line are design algorithms devised in this work.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Evaluation</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"n21gz\">We evaluate each design algorithm based on the final fitness score of the sequence each produced. The fitness score is defined as how well the sequence performed on the biological task according to the predictive model. To ensure fairness, we ran over 400,000 experiments where each design algorithm was given a fixed amount of time and the exact same 100 starting sequences for each task. We also measured the convergence speed, tracking how quickly each algorithm found its best solution, as faster algorithms save valuable time and computational resources.</p><p data-block-key=\"ei839\">We characterized performance variability by measuring how much an algorithm's final score was influenced by random chance versus its starting sequence. We quantified the effect of algorithmic randomness by re-running experiments with five different random seeds. To assess the impact of the starting point, we analyzed the variance in final scores across the 100 identical start sequences given to each design algorithm. We used a <a href=\"https://en.wikipedia.org/wiki/Friedman_test\" target=\"_blank\" rel=\"noopener noreferrer\">Friedman test</a> to investigate whether \"intrinsically difficult start sequences\", or sequences that are hard for all algorithms to optimize, exist.</p><p data-block-key=\"a3ndb\">To assess the distribution of performance ranks, we compared the final performance for each of the nine algorithms across every experiment in the NucleoBench benchmark for each unique combination of a task and a starting sequence. A rank-based \"order score\" from 0 to 8 was then assigned, with 0 going to the best-performing algorithm, 1 to the second-best, and so on. Each violin shape is constructed by aggregating all the rank scores a single algorithm received across the 400,000+ experiments, with the width of the violin at any point showing how frequently that algorithm achieved a particular rank.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-3.width-1250.png\" alt=\"The distribution of final scores for each algorithm\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-3.width-1250.png\" alt=\"The distribution of final scores for each algorithm\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"3cj2n\"><i>The distribution of final scores for each algorithm. X-axis is the design algorithm, y-axis is the aggregate order score. Order scores are determined by assigning an integer [0, 9] for each (task, start sequence, design algorithm) tuple according to the performances of all the final sequences for that (task, start sequence) pair. 0 is the top performer. Aggregate scores are computed by averaging over all such scores.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"n21gz\">Gradient-based methods were the reigning champions amongst existing methods. However, we found that AdaBeam outperformed them, demonstrating that relying on gradients is not the only path to top-tier performance and scalability.</p><p data-block-key=\"2pdug\">AdaBeam improves upon previous methods in several key ways:</p><ul><li data-block-key=\"96hff\"><b>Efficiency</b>: It replaces AdaLead’s sampling step with a faster calculation, doubling its speed on long sequences.</li><li data-block-key=\"75nk7\"><b>Smart Exploration</b>: It uses a significantly more effective \"unordered\" approach to deciding where to edit a sequence.</li><li data-block-key=\"bgefo\"><b>Advanced Engineering</b>: It uses gradient concatenation to substantially reduce memory usage, enabling application to massive models like Enformer.</li></ul><p data-block-key=\"7iutv\">Across the 16 tasks in NucleoBench, AdaBeam was the best-performing algorithm 11 times. It also proved to be one of the fastest to converge on a high-quality solution, demonstrating superior scaling properties that are essential for tackling the next generation of AI challenges in biology.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Future directions</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"n21gz\">Our <a href=\"https://github.com/move37-labs/nucleobench\" target=\"_blank\" rel=\"noopener noreferrer\">NucleoBench</a> benchmark reveals the importance of rigorous, standardized evaluation and uncovers surprising findings, such as the critical impact of the initial sequence and the ineffectiveness of some established algorithm features. However, significant challenges remain. The best gradient-based methods still struggle to scale to the largest models and longest sequences, and substantial scalability gains can be realized through better software engineering. While our new algorithm, AdaBeam, sets a new state-of-the-art, future work must focus on algorithms that adhere to biological constraints and improve scalability.</p><p data-block-key=\"3kltt\">A core principle of our work is a commitment to biosafety and responsible innovation. While AdaBeam represents a step forward for biological sequence design, it only improves the optimization according to a pre-existing predictive model. In other words, it is an optimizer, not an originator; the algorithm can only design sequences to maximize a goal defined by a user-provided predictive model. By releasing AdaBeam as an open-source tool, we empower researchers while ensuring the “human-in-the-loop” remains central to the design of biological molecules. Algorithms like AdaBeam can help scientists design more effective mRNA vaccines, create safer CRISPR gene therapies, and develop novel treatments for a wide range of diseases, bringing the promise of AI-driven drug discovery closer to reality.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"n21gz\"><i>This work represents a collaboration between Joel Shor (Move37 Labs), Erik Strand (Move37 Labs, MIT), and Cory Y. McLean (Google Research). We thank Sager Gosai, Daniel Friedman, Anna Lewis, Vikram Agarwal, and Michael Brenner for their guidance, discussions, and support throughout this project.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    }
  ],
  "lastUpdated": "2025-09-27T10:26:32.677Z"
}