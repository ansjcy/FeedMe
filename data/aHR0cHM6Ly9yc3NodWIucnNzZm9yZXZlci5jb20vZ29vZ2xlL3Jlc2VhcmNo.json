{
  "sourceUrl": "https://rsshub.rssforever.com/google/research",
  "title": "Google Research Blog",
  "description": "Google Research Blog - Powered by RSSHub",
  "link": "https://research.google/blog",
  "items": [
    {
      "title": "Snapseed 推出交互式设备端分割功能 (原标题: Introducing interactive on-device segmentation in Snapseed)",
      "link": "https://research.google/blog/introducing-interactive-on-device-segmentation-in-snapseed/",
      "pubDate": "Tue, 30 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-30T16:00:00.000Z",
      "creator": "Google",
      "summary": "Snapseed 引入了名为“对象画笔”（Object Brush）的新功能，旨在简化移动设备上的选择性图像调整。这项功能通过强大的设备端人工智能模型“交互式分割器”（Interactive Segmenter）实现，使用户能够通过简单的笔触或点击，快速准确地选择并编辑照片中的特定对象。\n\n### 核心功能与用户体验\n\n*   **直观的对象编辑**：用户只需在想要编辑的对象上绘制一笔，模型便会在20毫秒内立即检测并选择完整的对象，生成精确匹配其边界的掩膜。这使得在不影响图像其他部分的情况下，对前景主体、天空或特定物品进行亮度、颜色等调整变得轻而易举。\n*   **实时反馈**：模型提供实时反馈，用户可以即时添加或删除选区，直到达到理想效果。\n*   **设备端运行**：整个过程完全在设备上运行，由 MediaPipe 和 LiteRT 的 GPU 加速提供支持，确保了快速流畅的体验。\n\n### 交互式分割器模型的技术原理\n\n“交互式分割器”旨在成为一个通用分割模型，不限于特定类别的对象或场景。其开发过程涉及以下关键步骤：\n\n1.  **师生模型训练（Teacher-Student Training）**：\n    *   **“交互式分割器：教师”（Interactive Segmenter: Teacher）**：首先，团队使用一个预训练且高度泛化的模型，通过对350多种不同对象类别的约30,000个高质量像素级图像掩膜进行微调，训练出一个高精度的交互式分割模型。该模型质量高，但由于速度和大小限制，不适合设备端使用。\n    *   **“交互式分割器：边缘”（Interactive Segmenter: Edge）**：为了实现设备端应用，团队开发了一个更小、更专业的模型。该模型通过知识蒸馏（Knowledge Distillation）从“教师”模型中学习，利用包含200多万张图像和数百个不同类别的弱标注数据集进行训练。在蒸馏过程中，“教师”模型会实时生成高质量的真值掩膜，并模拟用户提示（如前景涂鸦、背景涂鸦、点和框选），以训练“边缘”模型。\n\n    ![Table comparing Interactive Segmenter: Edge and Interactive Segmenter: Teacher models using the IOU metric.](https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-3-IOU.width-1250.png)\n    *   **提示生成**：为了模拟用户选择对象，模型在训练时会在真值掩膜内部绘制随机涂鸦作为前景提示，在外部绘制随机涂鸦作为背景提示，并模拟点击和套索选择。\n\n    ![Schematic of teacher–student training for Interactive Segmenter.](https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-4a-Training.width-1250.png)\n\n2.  **高精度与低延迟的平衡**：\n    *   为了在分割质量和实时交互延迟之间取得平衡，模型将图像理解和提示理解解耦为两个独立的子模型。\n    *   **重量级图像编码器**：对每张图像只运行一次，提取丰富的语义特征，并在用户开始使用交互式分割时立即运行，有效隐藏了延迟。\n    *   **轻量级交互式编码器-解码器**：在此预计算的特征上运行，接收用户的触摸提示并在20毫秒预算内生成最终分割掩膜。\n\n    ![Schematic of the Interactive Segmenter neural network architecture.](https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-5-Architecture.width-1250.png)\n    ![Table showing model inference latency when running Interactive Segmenter: Edge on-device.](https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-6-Latency.width-1250.png)\n\n3.  **图像尺寸掩膜上采样**：\n    *   为在高分辨率图像上保持最佳编辑质量，模型预测768x768分辨率的掩膜，并通过高效的 GPU 实现边缘保留联合双边上采样方法，将其上采样至图像原始分辨率（最高4K）。\n    *   为提高延迟，上采样仅在用户完成手势（抬起手指）后应用。\n\n    ![Comparison of original Interactive Segmenter mask and upsampled mask.](https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-7-Upsampling.width-1250.png)\n\n### 结论与展望\n\n“交互式分割器”的推出，使 Snapseed 的图像编辑变得前所未有的简单和强大。它将简单的点击和笔触转化为精确的选择，帮助用户轻松实现编辑想法。这项底层技术不仅已应用于 Chromebook Plus 14 的图库应用中的 AI 图像编辑功能，Google 还计划将其集成到更多图像和创意编辑产品中。",
      "shortSummary": "Snapseed 推出“对象画笔”功能，通过设备端 AI 模型“交互式分割器”实现快速、直观的对象选择和编辑。用户只需简单笔触，模型便能在20毫秒内精确分割图像中的对象。该技术采用师生模型蒸馏训练，平衡了分割质量与实时性，并支持高分辨率掩膜。这项创新使高级照片编辑更易用，并计划集成到更多 Google 图像和创意编辑产品中，包括已应用于 Chromebook Plus 14。",
      "translated_title": "Snapseed 推出交互式设备端分割功能",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-3-IOU.width-1250.png",
          "alt": "Table comparing Interactive Segmenter: Edge and Interactive Segmenter: Teacher models using the IOU metric.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-4a-Training.width-1250.png",
          "alt": "Schematic of teacher–student training for Interactive Segmenter.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-5-Architecture.width-1250.png",
          "alt": "Schematic of the Interactive Segmenter neural network architecture.",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-6-Latency.width-1250.png",
          "alt": "Table showing model inference latency when running Interactive Segmenter: Edge on-device.",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-7-Upsampling.width-1250.png",
          "alt": "Comparison of original Interactive Segmenter mask and upsampled mask.",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wunwg\">The key to elevating a good photo often lies in selective image adjustments: brightening a subject in the foreground, enhancing the sky, or making the color of a jacket pop. Yet, isolating specific elements with existing tools that offer subject, background, sky, or color-based selections has remained a frustrating and complex endeavor. This challenge has been particularly acute on mobile devices, where imprecise touch input and limited processing have made detailed selections and edits very difficult.</p><p data-block-key=\"978qd\">Now, we have made object-based image adjustments quick and easy. The new Object Brush in <a href=\"https://apps.apple.com/us/app/snapseed-photo-editor/id439438619\" target=\"_blank\" rel=\"noopener noreferrer\">Snapseed</a> on iOS, accessible in the \"Adjust\" tool, now lets you edit objects intuitively. It allows you to simply draw a stroke on the object you want to edit and then adjust how you want it to look, separate from the rest of the image. Give it a try as we roll this new capability out in the coming week!</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Snapseed-1-ObjBrush.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Selective editing using Snapseed's Object Brush.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Intuitive editing through interactive on-device segmentation</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">At its core, Object Brush is powered by our Interactive Segmenter, a powerful AI model that runs entirely on device. With a simple gesture — just a tap or tracing a quick line — you can choose an object or person in the frame. The model will then immediately detect and select the complete object or person, in less than 20ms. The model generates a mask for the object, which accurately matches its boundaries, whether it's a person, a pet, or the clouds in the sky. This real-time feedback lets you refine your selection on the fly, easily adding or subtracting areas until it's just right. This entire process is powered by <a href=\"https://ai.google.dev/edge/mediapipe/framework\" target=\"_blank\" rel=\"noopener noreferrer\">MediaPipe</a> and <a href=\"https://ai.google.dev/edge/litert/performance/gpu\" target=\"_blank\" rel=\"noopener noreferrer\">LiteRT’s GPU acceleration</a> for a fast and seamless experience.</p><p data-block-key=\"c4tan\">This powerful fusion of a simple, intuitive user interface with an effective and efficient machine learning model makes advanced photo editing more accessible, enjoyable, and more precise than ever before, all running seamlessly on your own device.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Snapseed-0-Hero.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Use foreground prompts (green) to select parts of an image and background prompts (red) to refine the selection.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Training the Interactive Segmenter model</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">The Interactive Segmenter model is designed to be a universally capable segmentation model, not limited to any specific class of objects or scenes. To avoid having to annotate large amounts of data to cover all areas, we chose to follow the <a href=\"https://arxiv.org/abs/1912.11370\" target=\"_blank\" rel=\"noopener noreferrer\">Big Transfer</a> approach and use a general pre-trained image encoder for pseudo-annotation to complement small amounts of manually annotated images.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Teacher for Interactive Segmenter</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">We started with a pre-trained and highly-generalizable model, fine-tuned for interactive segmentation. We took samples for 350+ different object categories and asked annotators to precisely annotate object masks with pixel-perfect quality. Through this process, we obtained ~30,000 high-quality image masks for these categories. While insufficient for direct training of a small mobile model, large pre-trained models can successfully be fine-tuned on this data to predict high accuracy masks. Using this dataset we trained an interactive segmentation model, which we call “Interactive Segmenter: Teacher”.</p><p data-block-key=\"2dclt\">Interactive Segmenter: Teacher produces high-quality segmentation masks; however, its speed and size hinder its use in on-device scenarios. To overcome this challenge, we developed “Interactive Segmenter: Edge”, a specialized model tailored for on-device use cases by leveraging the knowledge distilled from the original Interactive Segmenter: Teacher model.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Distillation</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">Since the on-device model is significantly smaller, it has limited generalization capabilities, and the 30,000 annotated images we used for fine-tuning aren't sufficient to train a new model. At the same time the small model size implies we won’t see significant gains from pre-training on different domains or tasks.</p><p data-block-key=\"bqvot\">For knowledge transfer from Interactive Segmenter: Teacher to Interactive Segmenter: Edge, we need millions of images and realistic prompts for a diverse range of object categories. So, we leveraged a large, weakly annotated dataset, which contains over 2 million images with masks across hundreds of different categories.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-3-IOU.width-1250.png\" alt=\"Table comparing Interactive Segmenter: Edge and Interactive Segmenter: Teacher models using the IOU metric.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-3-IOU.width-1250.png\" alt=\"Table comparing Interactive Segmenter: Edge and Interactive Segmenter: Teacher models using the IOU metric.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Interactive Segmenter: Edge yields a similar quality as Interactive Segmenter: Teacher for a given, fixed input prompt, as measured by the intersection over union (IOU) metric.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Prompt generation</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">The segmentation masks in the distillation dataset are not pixel-perfect, because they were generated through <a href=\"https://arxiv.org/pdf/1903.10830\" target=\"_blank\" rel=\"noopener noreferrer\">automated or semi-automated procedures</a>, and are not ideal for training high-quality segmenters. Nevertheless, they are suitable for creating realistic prompts for interactive segmentation. In this process, the ground truth mask is produced on-the-fly by Interactive Segmenter: Teacher, which acts as a teacher model in a process known as <a href=\"https://arxiv.org/abs/2106.05237\" target=\"_blank\" rel=\"noopener noreferrer\">knowledge distillation</a>. Importantly, both the teacher as well as the student model use the same prompts during training, ensuring consistency across models.</p><p data-block-key=\"6276u\">We attempt to simulate a user selecting objects in an image. We draw random scribbles within the (eroded) ground truth mask to get foreground prompts (i.e., what the user wants to select, shown in red in the image below) and random scribbles outside the ground truth mask to get background prompts (i.e., what the user explicitly does not want to select, shown in blue). We simulate tapping by drawing random points as well as random scribbles. Furthermore, to support lasso selection we also expose the model during training to box prompts around an object.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-4a-Training.width-1250.png\" alt=\"Schematic of teacher–student training for Interactive Segmenter.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-4a-Training.width-1250.png\" alt=\"Schematic of teacher–student training for Interactive Segmenter.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">By utilizing a teacher model we can train on data with low-quality ground truth annotations, reducing labeling costs without sacrificing model quality.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">High quality vs. low latency</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">A central challenge was reconciling the conflicting demands of segmentation quality versus real-time, interactive latency. To reach the right balance, we decouple image and prompt understanding into distinct sub-models. First, a powerful, heavyweight image encoder is run once per image to extract a rich set of semantic features. This image encoder can be run as soon as the user’s intent to use interactive segmentation becomes apparent, thus effectively hiding the latency from the user. Second, a lightweight interactive encoder-decoder operates on these pre-computed features. This network takes the user's touch prompts and generates the final segmentation mask, executing well under our 20ms budget. This separation into two models allows Interactive Segmenter to harness the image understanding of a large model while delivering the instantaneous responsiveness of a small one.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-5-Architecture.width-1250.png\" alt=\"Schematic of the Interactive Segmenter neural network architecture.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-5-Architecture.width-1250.png\" alt=\"Schematic of the Interactive Segmenter neural network architecture.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Interactive Segmenter neural network architecture.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-6-Latency.width-1250.png\" alt=\"Table showing model inference latency when running Interactive Segmenter: Edge on-device.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-6-Latency.width-1250.png\" alt=\"Table showing model inference latency when running Interactive Segmenter: Edge on-device.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Model inference latency when running Interactive Segmenter: Edge on-device.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wunwg\">The final student models (encoder + super decoder) are quantized to 8 bits and both run on <a href=\"https://ai.google.dev/edge/litert/performance/gpu\" target=\"_blank\" rel=\"noopener noreferrer\">LiteRT's GPU acceleration</a> with decoder inference latencies of 7.4ms on an iPhone 16 Pro, enabling seamless and intuitive image editing.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Image-size mask upsampling</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">To preserve the best image editing quality on high-resolution images, we need high-resolution segmentation masks. To achieve this, we train our segmentation model to predict a mask in 768x768 resolution and further upsample it to image resolution (capped at 4k to have it fit within a single GPU buffer). We use an efficient GPU implementation of the <a href=\"https://dl.acm.org/doi/10.1145/1276377.1276497\" target=\"_blank\" rel=\"noopener noreferrer\">edge-preserving joint-bilateral upsampling method</a>. To improve latency, we only apply upsampling once a user completes a gesture by lifting their finger.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-7-Upsampling.width-1250.png\" alt=\"Comparison of original Interactive Segmenter mask and upsampled mask.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-7-Upsampling.width-1250.png\" alt=\"Comparison of original Interactive Segmenter mask and upsampled mask.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Original Interactive Segmenter mask (<b>left</b>) and upsampled mask (<b>right</b>).</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">With the new Interactive Segmenter in <a href=\"https://apps.apple.com/us/app/snapseed-photo-editor/id439438619\" target=\"_blank\" rel=\"noopener noreferrer\">Snapseed</a> image editing has become easier and more powerful than ever. Simple taps and strokes are translated into accurate selections, allowing users to translate their editing ideas into reality. Download Snapseed for iOS <a href=\"https://apps.apple.com/us/app/snapseed-photo-editor/id439438619\" target=\"_blank\" rel=\"noopener noreferrer\">here</a> and let your photos shine. Object Brush will be rolled out to more tools in Snapseed in the coming months. The underlying model powers a wide range of image editing and manipulation tasks and serves as a foundational technology for intuitive selective editing. It has also been shipped in the new <a href=\"https://blog.google/products/chromebooks/lenovo-chromebook-plus-14/\" target=\"_blank\" rel=\"noopener noreferrer\">Chromebook Plus 14</a> to power AI image editing in the Gallery app. Next, we plan to integrate it across more image and creative editing products at Google.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\"><i>Special thanks to all members who worked on the tech with us: Valentin Bazarevsky, Daniel Fenner, Lutz Justen, Ronald Wotzlaw, Tai-Yu Daniel Pan, Jason Chang, Matthew Harries, Giles Ochs, Jonathan Horsman, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Karthik Raveendran, Matsvei Zhdanovich, Mogan Shieh, Chris Parsons, Jianing Wei, and Matthias Grundmann.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "AI作为研究伙伴：利用AlphaEvolve推进理论计算机科学 (原标题: AI as a research partner: Advancing theoretical computer science with AlphaEvolve)",
      "link": "https://research.google/blog/ai-as-a-research-partner-advancing-theoretical-computer-science-with-alphaevolve/",
      "pubDate": "Mon, 29 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-29T16:00:00.000Z",
      "creator": "Google",
      "summary": "大型语言模型（LLMs）在竞技数学和编程中展现出卓越能力，但在数学发现（如证明新定理或揭示新组合结构）方面相对较少成功，因为数学和理论计算机科学要求绝对的正确性。任何AI驱动的数学发现方法都必须提供可计算验证的正确性证明，或由领域专家进行认证。\n\n## AlphaEvolve：LLM驱动的数学发现工具\n\nGoogle DeepMind的最新研究论文《组合结构强化生成：在复杂性理论中的应用》展示了LLM驱动的编码智能体如何帮助发现新的数学结构，从而推动复杂性理论（理论计算机科学的一个子领域）的理解边界。这项工作利用了AlphaEvolve系统，该系统通过以下方式迭代进化代码：\n\n*   **反馈循环：** AlphaEvolve从代码片段集合开始，评估这些代码片段生成的结构。\n*   **LLM驱动的优化：** 利用LLM将最成功的代码片段演变为更好的解决方案。\n\n这种方法在复杂性理论的两个不同领域取得了新成果：\n\n1.  改进了MAX-4-CUT问题（将节点划分为四个集合的最大割问题）近似不可行性（即我们近似结果能力的极限）的最新界限。\n2.  收紧了认证随机图性质的平均情况硬度界限。\n\n## AI辅助数学研究的模式\n\nAI辅助数学研究主要有两种模式：\n\n*   **LLM直接生成：** 人类调用LLM来总结文献、规划新定理的研究路线，或直接生成部分（或全部）证明。\n*   **AI工具辅助生成：** 人类使用AlphaEvolve等AI工具生成更好的证明元素。本文的工作属于第二类，AlphaEvolve生成的证明元素可以由计算机程序自动验证。\n\n## 提升的力量：从有限构造到普遍陈述\n\n将AI用于理论计算机科学研究的一个根本挑战在于所研究问题的普遍性。AI系统可能找到特定问题实例的解决方案（例如，50个特定城市的旅行推销员最优路线），但计算机科学家通常寻求对所有问题实例和大小都普遍成立的定理（表示为∀n）。\n\n为了使AlphaEvolve能够证明普遍陈述，研究采用了“提升”（lifting）技术。如果将证明视为一个长字符串，可以将证明中的一个片段（对应于某个有限结构）进行演化，以支持一个更强的普遍陈述，同时保持与证明其余部分的接口不变。这种方法的优势在于，要认证整体正确性，只需认证已演化的有限结构的正确性。\n\n![提升：使用AI对有限结构进行变形，同时保持与更广泛证明的接口不变。](https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-1-Lifting.width-1250.png)\n\n在复杂性理论中，研究人员经常使用依赖于特定高度优化有限结构的既定证明框架。如果能找到更好的结构，整个证明框架就会将这种改进“提升”为一个更好的普遍结果。一个关键例子是“小工具归约”（gadget reduction），通过将已知难解的源问题映射到目标问题来证明目标问题的计算难度。小工具是局部转换源问题片段到目标问题片段的方法，它们是有限结构，找到最优小工具通常是一个艰苦的手工过程。通过让AlphaEvolve寻找更好的小工具，研究人员发现了比以往已知更复杂的结构。这些有限的发现，当插入到现有数学框架中时，立即产生了复杂性理论中的新普遍定理。\n\n## 复杂性理论中的新定理\n\n### MAX-4-CUT：新的最先进成果\n\n研究人员将此方法应用于MAX-k-CUT问题。给定一个图，目标是将节点划分为k个不同的集合，使不同集合之间交叉的边数最大化。这是一个经典的难解（NP-hard）问题，因此研究重点是近似算法。关键问题是：近似的极限是什么？\n\n对于MAX-4-CUT（划分为四个集合），之前最佳结果证明其近似解的因子为0.9883是NP-hard。AlphaEvolve被部署来寻找MAX-4-CUT的新小工具归约。系统发现了一个涉及19个变量（节点）的复杂小工具，具有复杂的加权方案。这一发现确立了新的近似不可行性界限为0.987。尽管这一改进看似微小，但在近似硬度这一成熟领域，此类进展通常需要重大的新技术或组合洞察力。\n\n![AlphaEvolve为MAX-4-CUT归约发现的小工具的图示。](https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-2-Gadget.width-1250.png)\n\n### 平均情况硬度和Ramanujan图\n\n研究还探讨了问题在平均情况下的硬度，而非最坏情况。具体来说，研究了稀疏随机图的MAX-2-CUT（以及最大独立集）界限认证的难度。最近的工作将此问题与特定Ramanujan图的存在联系起来——这些确定性图“看起来”像稀疏随机图。他们推测，具有异常大割的Ramanujan图的存在意味着认证随机图的MAX-2-CUT在计算上是困难的。先前的工作使用计算机辅助找到了多达10个节点的此类图。改进他们的结果需要找到更多节点上更极端的Ramanujan图，这极其难以发现和验证。AlphaEvolve成功地在这一广阔的搜索空间中导航，发现了多达163个节点上具有更大割的Ramanujan图。\n\n![AlphaEvolve发现的具有大2-割的4-正则Ramanujan图。](https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-3-4RegGraph.width-1250.png)\n\n这些发现显著改善了平均情况硬度的下限。此外，结合新的算法进展（非AI驱动），研究人员能够将这些计算硬度问题几乎解决，将上限和下限匹配到小数点后第三位。\n\n## 验证正确性的关键作用\n\n这项工作的一个关键区别在于其结果附带了正确性证明。当LLM被直接提示生成数学证明时，它通常会产生一个需要大量人工干预才能验证和完成的证明草图或论证。幻觉或细微错误可能使输出无用。数学的正确性标准是绝对的。相比之下，这里采用的方法是利用AI发现证明中的结构，而不是证明本身。最终定理的有效性依赖于两个组成部分：提升框架的正确性，以及所发现结构的验证。虽然框架是健全的，但验证AlphaEvolve发现的结构在计算上是密集的。值得注意的是，AlphaEvolve通过实施复杂的剪枝（branch-and-bound）策略和系统级优化，将验证过程加速了10,000倍。这一巨大的加速是研究的关键推动因素，使得系统能够探索更大、更复杂的小工具。最重要的是，最终发现的小工具仍然使用原始的暴力算法进行验证，确保了定理的绝对正确性。\n\n## AI辅助理论的未来\n\n尽管这些初步研究结果远非结论性的，但它们表明AI有望成为数学发现中一个有益的合作者。研究人员观察到AlphaEvolve中的模型生成了复杂的数学对象，有时展现出初步的推理能力。然而，随着我们进入一个证明可能越来越多地归因于AI的时代，验证这项关键任务将成为一个重要的瓶颈。",
      "shortSummary": "Google DeepMind的AlphaEvolve系统利用大型语言模型（LLM）作为研究伙伴，通过迭代进化代码，在理论计算机科学领域取得了显著进展。该系统通过“提升”技术，将有限结构演化为更强的通用定理。具体而言，它改进了MAX-4-CUT问题的近似不可行性界限至0.987，并发现了具有更大割的Ramanujan图，从而收紧了稀疏随机图平均情况硬度的界限。所有AI发现的结构都经过严格计算验证，确保了数学结果的绝对正确性，展现了AI在数学发现中的巨大潜力。",
      "translated_title": "AI作为研究伙伴：利用AlphaEvolve推进理论计算机科学",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-1-Lifting.width-1250.png",
          "alt": "Lifting: Morphing finite structures using AI, while keeping the interface to the broader proof intact.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-2-Gadget.width-1250.png",
          "alt": "Drawing of a graph representing the gadget found by AlphaEvolve for the reduction to MAX-4-CUT.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-3-4RegGraph.width-1250.png",
          "alt": "Drawing of a 4-regular Ramanujan graph with large 2-cut found by AlphaEvolve.",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"c47sg\">Recently, large language models (LLMs) have demonstrated surprising capabilities in <a href=\"https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/\" target=\"_blank\" rel=\"noopener noreferrer\">competitive mathematics</a> and <a href=\"https://deepmind.google/discover/blog/gemini-achieves-gold-level-performance-at-the-international-collegiate-programming-contest-world-finals/\" target=\"_blank\" rel=\"noopener noreferrer\">competitive programming</a>, demonstrating world-leading performance across both of these fields. However, their successes in mathematical discovery — proving novel theorems or uncovering new combinatorial structures — have been relatively few (with some notable exceptions [<a href=\"https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/\" target=\"_blank\" rel=\"noopener noreferrer\">1</a>, <a href=\"https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/\" target=\"_blank\" rel=\"noopener noreferrer\">2</a>, <a href=\"https://arxiv.org/pdf/2505.20219#page=8.61\" target=\"_blank\" rel=\"noopener noreferrer\">3</a>]). Since mathematics and theoretical computer science demand absolute correctness<footnote id=\"94fb5494-00a3-46e9-8265-43676afdf080\">[94fb54]</footnote>, any AI-based method that makes mathematical discovery must either have a proof of correctness that can be confirmed computationally (without any human involvement), or have a domain-expert human in the loop to certify correctness.</p><p data-block-key=\"ebg0u\">In our recent paper, “<a href=\"https://arxiv.org/abs/2509.18057\" target=\"_blank\" rel=\"noopener noreferrer\">Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory</a>”, we demonstrate how an LLM-powered coding agent can help discover new mathematical structures that push the boundaries of our understanding of <a href=\"https://en.wikipedia.org/wiki/Computational_complexity_theory\" target=\"_blank\" rel=\"noopener noreferrer\">complexity theory</a> (a sub-field of theoretical computer science). Our work utilizes <a href=\"https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/\" target=\"_blank\" rel=\"noopener noreferrer\">AlphaEvolve</a>, a system developed at Google DeepMind that uses LLMs to iteratively evolve code. By employing a feedback loop, AlphaEvolve began with populations of code snippets, evaluated the structures produced by the code snippets, and used an LLM to morph the most successful snippets toward better solutions. This approach led to new results in two distinct areas of complexity theory: 1) improving the state-of-the-art for the limit on our ability to approximate the outcome (i.e., the \"inapproximability\") of the <a href=\"https://en.wikipedia.org/wiki/Maximum_cut\" target=\"_blank\" rel=\"noopener noreferrer\">maximum cut problem</a> for 4 slices (which we define as the <a href=\"https://arxiv.org/pdf/2509.18057#page=8\" target=\"_blank\" rel=\"noopener noreferrer\">MAX-4-CUT problem</a>), and 2) tightening the bounds on the <a href=\"https://arxiv.org/pdf/2509.18057#page=6\" target=\"_blank\" rel=\"noopener noreferrer\">average-case hardness of certifying properties of random graphs</a>.</p><p data-block-key=\"1oij\">AI-assisted mathematical research can operate in the following modes:</p><ol><li data-block-key=\"7g1t3\">A person invokes an LLM to summarize the literature, to chart a research plan towards new theorems, or to directly generate chunks of (or entire) proofs.</li><li data-block-key=\"30u56\">A person uses AI-derived tools, such as AlphaEvolve, to generate better proof elements.</li></ol><p data-block-key=\"duvp5\">Our work falls in the second category, where we obtain better proof elements using AlphaEvolve that can be automatically verified by a computer program.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The power of lifting: From finite constructions to universal statements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c47sg\">A fundamental challenge in using AI for theoretical computer science research lies in the universal nature of the problems studied. An AI system might find a solution to a specific instance of a problem — say, the <a href=\"https://en.wikipedia.org/wiki/Travelling_salesman_problem\" target=\"_blank\" rel=\"noopener noreferrer\">optimal route for a traveling salesman</a> visiting 50 specific cities. However, computer scientists often seek theorems that hold true universally for <i>all</i> problem instances and sizes (denoted as ∀n).</p><p data-block-key=\"2fqu\">How can we use AlphaEvolve to prove a universal statement? The answer lies in a technique known as \"lifting\" (see image below). If a proof is viewed as a long string, then one can take a chunk of the proof (corresponding to a certain finite structure), and evolve it to support a stronger universal statement, while keeping the interface to the rest of the proof intact. The advantage of this approach is that to certify overall correctness, one needs to only certify the correctness of the finite structure that has been evolved.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-1-Lifting.width-1250.png\" alt=\"Lifting: Morphing finite structures using AI, while keeping the interface to the broader proof intact.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-1-Lifting.width-1250.png\" alt=\"Lifting: Morphing finite structures using AI, while keeping the interface to the broader proof intact.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"g7wnv\">Lifting: Morphing finite structures using AI, while keeping the interface to the broader proof intact.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"c47sg\">In complexity theory, researchers often use established proof frameworks that rely on the existence of specific, highly optimized finite structures. If a better structure can be found, the entire proof framework \"lifts\" this improvement to a better universal result.</p><p data-block-key=\"f1cfa\">A key example of this is a \"<a href=\"https://people.csail.mit.edu/madhu/papers/1996/gadgets-journ.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">gadget reduction</a>.\" To prove that a target problem is computationally hard (intractable), researchers try to map a known intractable source problem to it, hence demonstrating that the target problem is at least as hard as the source problem. A gadget is a recipe for locally transforming a small piece of the source problem into a piece of the target problem. These gadgets are finite structures, and finding the optimal gadget is a painstaking process often done by hand.</p><p data-block-key=\"6na9u\">By tasking AlphaEvolve with finding better gadgets, we were able to discover structures far more complex than those previously known. These finite discoveries, when plugged into the existing mathematical frameworks, immediately yield new universal theorems in complexity theory.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">New theorems in complexity theory</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c47sg\">We applied this methodology to the MAX-k-CUT problem. Given a <a href=\"https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)\" target=\"_blank\" rel=\"noopener noreferrer\">graph</a> (a network of nodes and edges), the goal is to partition the nodes into <i>k</i> distinct sets such that the number of edges crossing between different sets is maximized. This is a classic intractable (<a href=\"https://en.wikipedia.org/wiki/NP-hardness\" target=\"_blank\" rel=\"noopener noreferrer\">NP-hard</a>) problem, meaning we do not expect to find efficient algorithms that solve it exactly. Therefore, we focused on <i>approximation algorithms</i> — those that efficiently find solutions guaranteed to be close to the optimum.</p><p data-block-key=\"62kuh\">The crucial question is: what is the limit of approximation?</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">MAX-4-CUT: A new state of the art</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c47sg\">For MAX-4-CUT (partitioning into four sets), the previous best-known result proved that it is NP-hard to approximate the solution within a factor of <a href=\"https://arxiv.org/pdf/2509.18057#page=8\" target=\"_blank\" rel=\"noopener noreferrer\">0.9883</a>. AlphaEvolve was deployed to search for a new gadget reduction to MAX-4-CUT.</p><p data-block-key=\"9bab8\">The system discovered an intricate gadget involving 19 variables (nodes) with a complex weighting scheme (some connections having up to 1429 times the weight of others). This discovery established a new inapproximability bound of 0.987.</p><p data-block-key=\"1m1ig\">This improvement may seem incremental, but in the mature field of hardness of approximation, such advances often require significant new techniques or combinatorial insights.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-2-Gadget.width-1250.png\" alt=\"Drawing of a graph representing the gadget found by AlphaEvolve for the reduction to MAX-4-CUT.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-2-Gadget.width-1250.png\" alt=\"Drawing of a graph representing the gadget found by AlphaEvolve for the reduction to MAX-4-CUT.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"g7wnv\">Gadget found by AlphaEvolve for the reduction to MAX-4-CUT.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Average-case hardness and Ramanujan graphs</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c47sg\">We also explored the hardness of problems <i>on average</i>, rather than in the worst case. Specifically, we studied the difficulty of certifying bounds on the MAX-2-CUT (as well as <a href=\"https://en.wikipedia.org/wiki/Independent_set_(graph_theory)\" target=\"_blank\" rel=\"noopener noreferrer\">maximum independent set</a>) of sparse random graphs<footnote id=\"3d54ef4b-a9db-4307-9987-e308f9f08c3b\">[3d54ef]</footnote>. <a href=\"https://arxiv.org/abs/2404.17012\" target=\"_blank\" rel=\"noopener noreferrer\">Recent work</a> connected this problem to the existence of specific <a href=\"https://en.wikipedia.org/wiki/Ramanujan_graph\" target=\"_blank\" rel=\"noopener noreferrer\">Ramanujan graphs</a> — deterministic graphs that “look” like sparse random graphs. They conjectured that the existence of Ramanujan graphs with unnaturally large cuts implies it is computationally hard to certify the MAX-2-CUT of a random graph.</p><p data-block-key=\"3nhje\">Prior work used computer assistance to find such graphs on up to 10 nodes. Improving their results requires finding more extremal Ramanujan graphs on many more nodes, which are exceedingly difficult to find and verify. AlphaEvolve successfully navigated this vast search space, discovering Ramanujan graphs with even larger cuts on as many as 163 nodes.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-3-4RegGraph.width-1250.png\" alt=\"Drawing of a 4-regular Ramanujan graph with large 2-cut found by AlphaEvolve.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-3-4RegGraph.width-1250.png\" alt=\"Drawing of a 4-regular Ramanujan graph with large 2-cut found by AlphaEvolve.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"g7wnv\">A 4-regular Ramanujan graph with large 2-cut found by AlphaEvolve.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"c47sg\">These discoveries significantly improved the lower bounds for average-case hardness. Furthermore, combined with new algorithmic progress (non-AI based), we were able to nearly settle the computational hardness of these questions, matching the upper and lower bounds to within the third decimal place.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The crucial role of verified correctness</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c47sg\">A critical distinction of this work is that the results come with proofs of correctness.</p><p data-block-key=\"8afn7\">When an LLM is prompted to generate a mathematical proof directly, it often produces a proof sketch or an argument that requires substantial human intervention to verify and complete. Hallucinations or subtle errors can render the output useless. As mentioned earlier, the standard for correctness in math is absolute.</p><p data-block-key=\"48i4d\">In contrast, the approach taken here uses AI to discover a <i>structure</i> within the proof, not the proof itself. The validity of the final theorem relies on two components: the correctness of the lifting framework, and the verification of the discovered structure. While the frameworks are sound, verifying the structures discovered by AlphaEvolve is computationally intensive.</p><p data-block-key=\"5d7lt\">Remarkably, AlphaEvolve achieved a 10,000x speedup in the verification process by implementing sophisticated <a href=\"https://en.wikipedia.org/wiki/Branch_and_bound\" target=\"_blank\" rel=\"noopener noreferrer\">branch-and-bound</a> strategies and system-level optimizations. This massive speedup was the key enabler for the research, allowing the system to explore much larger and more complex gadgets.</p><p data-block-key=\"fpuet\">Crucially, the final gadgets discovered were still verified using the original, brute-force algorithm, ensuring the absolute correctness of the theorems.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The future of AI-assisted theory</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c47sg\">While these initial research findings are far from conclusive, they suggest that AI is poised to become a helpful collaborator in mathematical discovery. We have observed the models in AlphaEvolve generate intricate mathematical objects that at times exhibit nascent reasoning capabilities. However, as we transition into an era where proofs may increasingly be attributed to AI, the crucial task of verification is set to become a significant bottleneck.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c47sg\"><i>We would like to thank Adam Zsolt Wagner, Swarat Chaudhuri, Pasin Manurangsi and Sushant Sachdeva for helping us during various stages of the project.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "个人健康代理的剖析 (原标题: The anatomy of a personal health agent)",
      "link": "https://research.google/blog/the-anatomy-of-a-personal-health-agent/",
      "pubDate": "Mon, 29 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-29T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 个人健康代理 (PHA) 框架：个性化健康支持的未来\n\n随着大型语言模型 (LLM) 和可穿戴设备数据的快速发展，为个人健康旅程提供支持迎来了变革性机遇。然而，由于个人健康需求的多样性和复杂性，单一系统难以有效应对。为此，研究人员提出了**个人健康代理 (Personal Health Agent, PHA)**，这是一个全面的研究框架，旨在通过多模态数据推理，提供个性化、循证的健康指导。\n\n### PHA 的多代理架构\n\nPHA 采用多代理架构，将个人健康和健康支持分解为三个核心角色，每个角色由一个专业的子代理处理：\n\n1.  **数据科学 (DS) 代理**：负责分析个人时间序列数据（如可穿戴设备数据和血液生物标志物），提供情境化的数值洞察。它通过两阶段数据科学模块增强基础模型，能够解释模糊的用户查询，并将其转化为稳健的统计分析计划，然后生成并执行代码以得出有效的数据驱动答案。\n2.  **领域专家 (DE) 代理**：作为可靠的健康和健康知识来源。它通过多步骤推理框架和工具箱（包括访问 NCBI 等权威来源）增强基础模型，确保信息准确可信，并能根据用户的特定情况（如既往病史）定制信息。\n3.  **健康教练 (HC) 代理**：旨在通过多轮对话支持用户设定目标并促进持久的行为改变。它采用受心理学策略（如动机性访谈）启发的模块化架构，以实现更自然有效的互动。\n\n### 用户中心设计\n\n为了构建一个真正满足多样化需求的代理，研究团队采用了用户中心设计流程。他们综合了来自在线健康论坛、500 多名用户的调查数据以及与设计和工程专家研讨会的见解，识别出人们在四个关键领域需要支持：理解一般健康主题、解释个人数据、获取可操作的健康建议以及评估症状。这促使 PHA 系统被设计成类似于人类专家团队的协作模式。\n\n![用户中心流程](https://storage.googleapis.com/gweb-research2023-media/images/PHA2_Process.width-1250.png)\n*一个用户中心流程，用于识别关键用户旅程。*\n\n### 综合评估框架\n\n为了验证 PHA 系统，研究人员开发了一个全面的多层次评估框架。他们首先针对每个子代理的独特核心能力与最先进的 LLM 基础模型进行了基准测试，然后评估了完全集成后的 PHA 的整体效能。评估涉及自动化和广泛的人工评估，涵盖 10 项基准任务，投入了超过 1,100 小时来自最终用户和健康专家的努力，以评估其在真实、多模态对话中的表现。\n\n![综合评估描述](https://storage.googleapis.com/gweb-research2023-media/images/PHA3_Table.width-1250.png)\n*对单个子代理和最终个人健康代理 (PHA) 系统进行综合评估的描述。*\n\n#### 子代理评估结果：\n\n*   **数据科学代理 (DS Agent)**：在分析计划质量方面显著优于基础模型（75.6% 对 53.7%），并且在生成准确、可执行的代码方面更可靠。\n    ![DS代理评估结果](https://storage.googleapis.com/gweb-research2023-media/images/PHA4_DSAgent.width-1250.png)\n    *DS代理：由人类数据科学家和自动评估器评估的DS代理和基础模型生成的六个维度数据分析计划的评估结果。*\n\n*   **领域专家代理 (DE Agent)**：在所有基准测试中始终优于基础模型。临床医生认为其多模态健康数据摘要在临床上更具相关性和实用性，最终用户认为其响应更具个性化和可信度。\n    ![DE代理评估结果](https://storage.googleapis.com/gweb-research2023-media/images/PHA5_DEAgent.width-1250.png)\n    *DE代理：由临床专家评估的DE代理和基础模型在七个临床维度上的多模态推理评估结果。*\n\n*   **健康教练代理 (HC Agent)**：在最终用户和健康教练专家的评估中，其能力显著优于基线模型，尤其在对话体验、目标导向有效性和动机支持方面表现出色。\n    ![HC代理评估结果](https://storage.googleapis.com/gweb-research2023-media/images/PHA6_HCAgent.width-1250.png)\n    *HC代理：由人类健康和教练专家评估的HC代理和基础模型在六个维度上的教练体验评估结果。*\n\n### 协作式 PHA：一个协调的团队\n\nPHA 框架通过智能协调器将这三个专业代理整合为一个有凝聚力的团队。当用户提出查询时，协调器会分析用户需求，动态分配“主”代理和“支持”代理，并促进协作、反思和记忆更新的迭代工作流，以综合生成一个全面响应。这种协作方法被证明显著优于其各部分的总和。\n\n在评估代理综合个人健康数据以帮助用户回答健康问题和实现个人健康目标的能力时，最终用户和健康专家都更倾向于 PHA，而非单一代理系统或简单的并行多代理基线。PHA 在大多数情况下被评为最佳整体系统，这强调了模仿人类专家团队协作结构对于提供真正有益支持的关键价值。\n\n![PHA评估结果1](https://storage.googleapis.com/gweb-research2023-media/images/PHA8_Results1body.width-1250.png)\n*PHA：由人类专家评估的PHA和其他基线生成的响应的评估结果。*\n\n![PHA评估结果2](https://storage.googleapis.com/gweb-research2023-media/images/PHA9_Results2.width-1250.png)\n*PHA：由人类专家评估的PHA和其他基线生成的响应的排名结果。*\n\n### 智能个人健康代理的未来\n\n这项研究为设计下一代个人健康 AI 提供了一个经过验证的概念蓝图，倡导从单一模型转向模块化、协作式系统，使其更值得信赖、更具连贯性和实用性。\n\n**重要提示**：此工作概述了一个用于研究目的的概念框架，不应被视为对任何当前正在开发或向公众提供的特定产品、服务或功能的描述。任何实际应用都将经过单独的设计、验证和审查流程。",
      "shortSummary": "个人健康代理（PHA）是一个基于大型语言模型和可穿戴设备数据的研究框架，旨在提供个性化、循证的健康指导。它采用多代理架构，由数据科学、领域专家和健康教练三个专业子代理组成，通过智能协调器协同工作。PHA 经过全面的用户中心设计和评估，结果表明其在处理复杂健康需求方面显著优于单一系统和并行多代理基线，为未来模块化、协作式个人健康 AI 奠定了基础。此框架目前仅用于研究目的。",
      "translated_title": "个人健康代理的剖析",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PHA2_Process.width-1250.png",
          "alt": "PHA2_Process",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PHA3_Table.width-1250.png",
          "alt": "PHA3_Table",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PHA4_DSAgent.width-1250.png",
          "alt": "PHA4_DSAgent",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PHA5_DEAgent.width-1250.png",
          "alt": "PHA5_DEAgent",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PHA6_HCAgent.width-1250.png",
          "alt": "PHA6_HCAgent",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"1sp1g\">The rapid advancement of large language models (LLMs), combined with <a href=\"https://research.google/blog/sensorlm-learning-the-language-of-wearable-sensors/\">data from wearable devices</a>, presents a transformative opportunity to empower people on their personal health journeys. However, health needs vary from individual to individual. Answering a specific query, such as, \"On average, how many hours have I been sleeping this last month?\" requires different skills than an open-ended question like, \"What can I do to improve my sleep quality?\" A single system can struggle to address this complexity.</p><p data-block-key=\"8dbs6\">To meet this challenge, we adopt a human-centered process and propose the <a href=\"https://arxiv.org/abs/2508.20148\" target=\"_blank\" rel=\"noopener noreferrer\">Personal Health Agent</a> (PHA). This agent is a comprehensive research framework that can reason about multimodal data to provide personalized, evidence-based guidance. Using a multi-agent architecture, PHA deconstructs personal health and wellness support into three core roles (data science, domain expert, and health coach), each handled by a specialist sub-agent. To evaluate each sub-agent and the multi-agent system, we leveraged <a href=\"https://arxiv.org/abs/2505.03784\" target=\"_blank\" rel=\"noopener noreferrer\">a real-world dataset</a> from an IRB-reviewed study where ~1200 users provided informed consent to share their wearables data from Fitbit, a health questionnaire, and blood test results. We conducted automated and human evaluations across 10 benchmark tasks, involving more than 7,000 annotations and 1,100 hours of effort from health experts and end-users. Our work represents the most comprehensive evaluation of a health agent to date and establishes a strong foundation towards the futuristic vision of a personal health agent accessible to everyone.</p><p data-block-key=\"fdbrn\">This work outlines a conceptual framework for research purposes, and should not be considered a description of any specific product, service, or feature currently in development or available to the public. Any real-world application would be subject to a separate design, validation, and review process.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/PHA1_Illustration.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"7rp20\"><i>An illustration of the internal functions of Personal Health Agent (PHA) that enable it to support personal health needs.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"3xfzh\">User-centered design for personal health needs</h2><p data-block-key=\"44e8q\">To build an agent that truly meets these diverse needs, we started with a user-centered design process. We synthesized insights from over 1,300 real-world health queries from online sources, such as health forums, survey data from more than 500 users, and a workshop with design and engineering experts. This research revealed four critical areas where people need support: understanding general health topics, interpreting their personal data, getting actionable wellness advice, and assessing symptoms. This insight led us to design the PHA system that resembles human expert teams, including data scientists, domain experts, and personal health coaches.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA2_Process.width-1250.png\" alt=\"PHA2_Process\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA2_Process.width-1250.png\" alt=\"PHA2_Process\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"enxez\"><i>A user-centered process to identify critical user journeys.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"v6xwt\">Evaluation of our proposed system</h2><p data-block-key=\"690u2\">To validate our system, we developed a holistic, multi-level evaluation framework. We first benchmarked each individual sub-agent on their unique core capabilities against the state-of-the-art LLM model as the base model, and then assessed the fully integrated PHA’s overall efficacy. The table below shows our comprehensive evaluation, which involved both automated and extensive human evaluations across 10 benchmark tasks, incorporating over 1,100 hours of effort from both end-users and health experts to assess performance in realistic, multi-modal conversations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA3_Table.width-1250.png\" alt=\"PHA3_Table\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA3_Table.width-1250.png\" alt=\"PHA3_Table\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"enxez\"><i>Description of our comprehensive evaluation of individual sub-agents and the final Personal Health Agent (PHA) system.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"v6xwt\">The data science agent: Personal data analyst</h2><p data-block-key=\"7nd9l\">The first specialist is the data science (DS) agent, which analyzes personal time-series data from wearables plus health data, such as blood biomarkers, to provide contextualized numerical insights. The DS agent builds on top of a base model (e.g., Gemini) and is enhanced by a two-stage data science module: Stage 1) interpret underspecified and ambiguous user queries (e.g., “Am I getting more fit recently?”), and Stage 2) translate them into robust statistical analysis plans. It then generates and executes code to produce a statistically valid, data-driven answer.</p><p data-block-key=\"eeeci\">We developed two auto-evaluation benchmarks for each stage of the DS agent's workflow. For the first stage, analysis planning, we used an auto-evaluator trained on 354 query-analysis plans curated by 10 expert data scientists. Based on a detailed rubric assessing dimensions like data sufficiency, statistical validity, and alignment with the user's query, our evaluations showed that the DS agent significantly outperforms the base model in creating high-quality analysis plans (achieving a 75.6% score vs. 53.7% for the baseline). For the second stage, code generation, the agent’s output was benchmarked against 173 rigorous unit tests written by data scientists. This confirmed the agent is more reliable at generating accurate, executable code used to derive insights from time-series wearable data.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA4_DSAgent.width-1250.png\" alt=\"PHA4_DSAgent\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA4_DSAgent.width-1250.png\" alt=\"PHA4_DSAgent\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"enxez\"><i>DS agent: Results of evaluating data analysis plan generated by the DS agent and the base model across six dimensions, as evaluated by human data scientist and auto raters.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"v6xwt\">The domain expert agent: Grounded, trustworthy knowledge</h2><p data-block-key=\"a5n8f\">Next is the domain expert (DE) agent, which functions as a reliable source of health and wellness knowledge. In a high-stakes domain like health and wellbeing, ensuring information is accurate and trustworthy is critical. The DE agent enhances a base model by using a multi-step reasoning framework and a toolbox that includes access to authoritative sources, such as the <a href=\"https://pmc.ncbi.nlm.nih.gov/tools/developers/\" target=\"_blank\" rel=\"noopener noreferrer\">National Center for Biotechnology Information</a> (NCBI) database, to ground its responses in <i>verifiable facts</i>. It excels at tailoring information to a user’s specific profile, such as pre-existing conditions. We developed two auto-evaluation benchmarks to test the DE agent’s medical knowledge (one evaluating our agent’s performance on board certification and coaching exam questions, and one for providing accurate differential diagnosis). We further developed two human-evaluation benchmarks (one for clinicians, and one for consumers) to measure the DE agent’s capability on personalization and multi-modal reasoning. Our DE Agent consistently outperforms the base model across all benchmarks. For instance, clinicians rated the DE agent's summaries of multimodal health data as significantly more clinically relevant and useful, and end-users found its responses to be substantially more personalized and trustworthy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA5_DEAgent.width-1250.png\" alt=\"PHA5_DEAgent\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA5_DEAgent.width-1250.png\" alt=\"PHA5_DEAgent\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"enxez\"><i>DE agent: Results of evaluating multi-modal reasoning of the DE agent and the base model across seven clinical dimensions, as evaluated by clinical experts.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"v6xwt\">The health coach agent: Guiding behavior change</h2><p data-block-key=\"c2d6v\">The third specialist is the health coach (HC) agent, which is designed to support users in setting goals and fostering lasting behavioral change through multi-turn conversations. Effective coaching requires a delicate balance between gathering information and providing actionable advice. The HC agent employs a modular architecture inspired by proven psychological strategies (e.g., <i>motivational interviewing</i>) to navigate this dynamic, leading to more natural and effective interactions. We benchmarked the HC agent’s performance in two human-evaluation setups, one with end-users and the other with health coaching experts, evaluating our model’s ability across several key areas. For the end-user evaluation, we focused on conversational experience, goal-oriented effectiveness, and motivational support. For the expert evaluation, we assessed adherence to professional coaching principles, recommendation quality, and agent credibility. Both evaluation aspects indicate that the HC agent is significantly more capable than the baseline<i>,</i> underscoring a key insight from our research: for coaching agents, users prioritize core competency and the ability to provide actionable guidance.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA6_HCAgent.width-1250.png\" alt=\"PHA6_HCAgent\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA6_HCAgent.width-1250.png\" alt=\"PHA6_HCAgent\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"enxez\"><i>HC agent: Results of evaluating coaching experience of the HC agent and base model on six dimensions, evaluated by human health and coaching experts.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"v6xwt\">The Personal Health Agent (PHA): A collaborative team</h2><p data-block-key=\"94ato\">While each agent is powerful alone, the true potential is realized when they collaborate. The Personal Health Agent (PHA) framework integrates these three specialists into a cohesive team managed by an intelligent orchestrator. When a user poses a query, the orchestrator analyzes the user's need, dynamically assigns a \"main\" agent and \"supporting\" agents, and facilitates an iterative workflow of collaboration, reflection, and memory updates to synthesize a single, comprehensive response.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/PHA7_Animation.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"enxez\"><i>A technical breakdown of the DS, DE, and HC agents, with orchestration into the Personal Health Agent (PHA).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"v6xwt\">This collaborative approach proved to significantly outperform the sum of its parts. In extensive evaluations of rubrics assessing agents' capability in synthesizing personal health data to help users answer their health and wellness queries, as well as achieving personal health goals, both end-users and health experts preferred the PHA over (i) a powerful <a href=\"https://arxiv.org/pdf/2406.06464\" target=\"_blank\" rel=\"noopener noreferrer\">single-agent system</a> that also builds on a base model that uses tools to achieve three roles within a single agent setup, and (ii) a parallel multi-agent baseline that includes the same DS, DE, and HC agents, but simply calls all three agents and synthesizes their results without dynamic orchestration. Both end-users and experts ranked PHA as the best overall system in the majority of cases. This provides a strong example of how the value of emulating the collaborative structure of human expert teams is key to providing truly helpful support.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA8_Results1body.width-1250.png\" alt=\"PHA8_Results1body\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA8_Results1body.width-1250.png\" alt=\"PHA8_Results1body\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"u5asb\"><i>PHA: Results of evaluating responses generated by the PHA and other baselines, evaluated by human experts.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA9_Results2.width-1250.png\" alt=\"PHA9_Results2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA9_Results2.width-1250.png\" alt=\"PHA9_Results2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"enxez\"><i>PHA: Results of ranking responses generated by the PHA and other baselines, evaluated by human experts.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"v6xwt\">The future of intelligent personal health agents</h2><p data-block-key=\"78bb4\">Creating AI systems that can interpret complex health and wellness data and provide actionable wellness advice has been a longstanding challenge in the field. Our research provides a validated conceptual blueprint for designing the next generation of personal health AI, advocating a shift away from monolithic models toward modular, collaborative systems that are more trustworthy, coherent, and helpful.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "迈向更好的健康对话：基于Gemini的“寻路”AI代理研究洞察 (原标题: Towards better health conversations: Research insights on a “wayfinding” AI agent based on Gemini)",
      "link": "https://research.google/blog/towards-better-health-conversations-research-insights-on-a-wayfinding-ai-agent-based-on-gemini/",
      "pubDate": "Wed, 24 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-24T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 迈向更好的健康对话：基于Gemini的“寻路”AI代理研究洞察\n\n### 引言：在线健康信息导航的挑战\n\n获取清晰、相关且个性化的健康信息对患者至关重要，但在线健康信息世界往往令人困惑、不知所措且缺乏个性化。现有的大型语言模型（LLM）工具多为被动的“问答者”，提供对初始查询的单一全面答案，这与医生等专家通过提问来理解全貌并引导患者的方式不同。这种寻求上下文的对话方式对AI设计构成了重大挑战。\n\n### “寻路AI”：一种新的对话式方法\n\n本文介绍了基于Gemini的早期研究原型“寻路AI”（Wayfinding AI），旨在探索一种新方法。其核心理念是：通过主动提出澄清性问题，AI代理能更好地发现用户需求，引导他们清晰表达担忧，并提供更有帮助、更个性化的信息。研究团队通过四项混合方法用户体验研究，共163名参与者，迭代设计了一个用户认为比基线AI代理更有帮助、更相关、更符合其需求的代理。\n\n### 形成性用户体验洞察：在线查找健康信息的挑战\n\n*   **用户痛点**：通过访谈33名参与者发现，人们在在线查找健康信息时，往往难以清晰表达自己的健康问题，因为缺乏医学背景，难以判断哪些细节具有医学相关性。\n*   **用户偏好**：研究显示，当聊天机器人主动提出澄清性问题时，用户体验会显著改变。大多数参与者更喜欢“延迟回答”的方式（即AI先提问），而非立即给出全面答案。这种对话风格被认为更个性化、更安心。\n*   **效果**：澄清性问题不仅帮助AI提供更好的答案，也赋能用户，引导他们提供更相关的上下文。\n*   **挑战**：这种基于澄清性问题的方法的有效性高度依赖于执行质量——如果问题措辞不当、不相关或隐藏在冗长文本中容易被忽略，用户参与度就会下降。\n\n### “寻路AI”的设计原则\n\n基于上述洞察，“寻路AI”围绕三个核心原则设计，以创造更赋能的对话体验：\n\n1.  **主动对话引导**：在每一轮对话中，“寻路AI”最多提出三个有针对性的问题，旨在系统性地减少歧义，帮助用户更完整地表达其健康故事，并直接满足用户对更多上下文答案的需求。\n2.  **每轮提供“尽力而为”的答案**：考虑到某些健康问题可能无需澄清即可获得良好答案，“寻路AI”在每一轮对话中，都会根据目前共享的信息提供一个“尽力而为”的答案，同时强调如果用户能回答一个或多个后续问题，答案可以得到改进。这种方法在整个对话过程中为用户提供有用的信息，并提供选项以随着对话的进行接收越来越好的答案。\n3.  **透明推理**： “寻路AI”会解释用户的最新回答如何帮助完善了之前的答案，使AI的推理过程清晰易懂。\n\n为了确保澄清性问题在“尽力而为”的答案中不会被遗漏，界面设计采用了两栏布局：对话和澄清性问题出现在左栏，而“尽力而为”的答案和更详细的解释出现在右栏。这使得交互式对话与信息内容分离。\n\n### 随机用户研究评估\n\n为评估该代理的潜在实际影响，研究团队进行了一项随机用户研究，招募了130名21岁及以上、非医疗专业人士、有健康相关问题并愿意与AI互动的美国参与者。在随机组内设计中，每位参与者都与“寻路AI”和基线Gemini 2.5 Flash模型互动，探索他们的健康话题。参与者在与每个AI互动后，就帮助性、问题相关性、定制性、目标理解、易用性和获取有用信息效率等六个维度评估了体验满意度。\n\n![研究设计图示](https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder2_StudyDesign.width-1250.png)\n*图示：本研究的设计。*\n\n### 研究结果：通过目标理解和定制对话提供有益且相关的信息\n\n研究结果表明，尽管“寻路AI”采用了不太常见的两栏界面，用户仍在其多个重要维度上偏爱“寻路AI”的方法。用户更喜欢“寻路AI”的帮助性、相关性、理解其目标的能力以及为特定需求定制对话的能力。这些发现表明，“寻路AI”主动提问的行为成功地为用户创造了更个性化、更有帮助的体验，而没有在用户体验中引入不必要的摩擦。\n\n![用户对基线AI和寻路AI的偏好比较](https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder3_Results.width-1250.png)\n*图示：用户在多个评估维度上对基线AI和“寻路AI”的偏好，包括代理的帮助性、响应的相关性、对话对用户的定制性、对用户目标的理解、易用性、对话效率以及未来健康信息需求的使用意愿。*\n\n此外，参与者与“寻路AI”的对话明显更长，尤其是在试图理解症状原因时。对于这些话题，“寻路AI”的对话平均有4.96轮，而基线AI为3.29轮。他们向每个AI提供的提示模式也不同。\n\n![Sankey图，展示基线AI和寻路AI的对话流程](https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder4_HeroSankey.width-1250.png)\n*图示：Sankey图，展示基线AI和“寻路AI”的对话流程。每个垂直条显示了前5轮对话中用户提示类型的细分。蓝色条表示参与者对澄清性问题的回应——这在“寻路AI”中更为常见。*\n\n### 结论\n\n在线查找正确的健康信息如同迷宫。“寻路AI”通过设计成个性化和主动的对话伙伴，并在结构化界面中提出有针对性的问题，证明了其能提供比传统问答体验更受用户青睐的体验，从而帮助人们获取更有帮助、更相关和更个性化的信息。用户研究结果有力地证明，这种以人为本的对话式方法是AI在健康领域未来发展的一个有前景的方向，有助于人们更好地管理自己的健康旅程。",
      "shortSummary": "在线查找个性化健康信息充满挑战。谷歌研究团队基于Gemini开发了“寻路AI”，它通过主动提出澄清性问题，像医生一样引导用户，而非被动回答。多项用户研究（163名参与者）表明，用户更喜欢这种对话式AI，认为它更有帮助、更相关、更个性化，能更好地理解用户目标，并促成更深入的健康对话。这表明以人为本的对话式AI是健康领域AI的未来方向。",
      "translated_title": "迈向更好的健康对话：基于Gemini的“寻路”AI代理研究洞察",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder2_StudyDesign.width-1250.png",
          "alt": "Wayfinder2_StudyDesign",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder3_Results.width-1250.png",
          "alt": "Wayfinder3_Results",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder4_HeroSankey.width-1250.png",
          "alt": "Wayfinder4_HeroSankey",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3lmno\">The ability to find clear, relevant, and personalized health information is a cornerstone of empowerment for medical patients. Yet, navigating the world of online health information is often a confusing, overwhelming, and impersonal experience. We are met with a flood of generic information that does not account for our unique context, and it can be difficult to know what details are relevant.</p><p data-block-key=\"2t1lv\">Large language models (LLMs) have the potential to make this information more accessible and tailored. However, many AI tools today act as passive \"question-answerers\" — they provide a single, comprehensive answer to an initial query. But this isn't how an expert, like a doctor, helps someone navigate a complex topic. A health professional doesn't just provide a lecture; they ask clarifying questions to understand the full picture, discover a person's goals, and guide them through the information maze. Though this <i>context-seeking</i> is critical, it's a significant design challenge for AI.</p><p data-block-key=\"5nqat\">In “<a href=\"https://storage.googleapis.com/research-media/wayfinding-ai.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Towards Better Health Conversations: The Benefits of Context-Seeking</a>”, we describe how we designed and tested our “Wayfinding AI”, an early-stage research prototype, based on Gemini, that explores a new approach. Our fundamental thesis is that by proactively asking clarifying questions, an AI agent can better discover a user's needs, guide them in articulating their concerns, and provide more helpful, tailored information. In a series of four mixed-method user experience studies with a total of 163 participants, we examined how people interact with AI for their health questions, and we iteratively designed an agent that users found to be significantly more helpful, relevant, and tailored to their needs than a baseline AI agent.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"3lmno\">Formative user experience insights: Challenges in finding health information online</h2><p data-block-key=\"f889p\">To better understand the hurdles people face, we interviewed 33 participants about their experiences finding health information online. A key theme quickly emerged: people often struggle to articulate their health concerns. As one participant described, their process was to \"...just kind of like throw all the words in there and then I'm just gonna see what comes back.\" It may be that without a clinical background, it’s difficult to know which details are medically relevant.</p><p data-block-key=\"ev7la\">The people we interviewed were then able to use research prototypes of different chatbots. (The chat histories were not logged.) These participants made up a diverse group and asked health questions on a wide range of topics (e.g., rib pain, vertigo, consistent and unexplained weight gain, tinnitus and surgery; more details <a href=\"https://storage.googleapis.com/research-media/wayfinding-ai.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">in the paper</a>). Our studies revealed that when a chatbot proactively asks clarifying questions, the experience changes dramatically. The majority of participants preferred a \"deferred-answer\" approach — where the AI asks questions first — over one that gives a comprehensive answer immediately. This conversational style was perceived as more personal and reassuring. As one person noted, \"It feels more like the way it would work if you talk to a doctor... it does make me feel a little more confident that it wants to know more before jumping right into an answer.\" These clarifying questions not only help the AI provide better answers, but also empower users, guiding them to provide more relevant context. We found similar patterns in <a href=\"https://dl.acm.org/doi/abs/10.1145/3613905.3651891\" target=\"_blank\" rel=\"noopener noreferrer\">prior work on AI for dermatology</a>.</p><p data-block-key=\"b2dcv\">However, the effectiveness of this clarifying question–based approach depends heavily on the execution — engagement drops if questions are poorly formulated, irrelevant, or buried within long paragraphs of text where they are easily missed.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"3lmno\">Designing a Wayfinding AI to empower people through personal and proactive conversations</h2><p data-block-key=\"8tbaa\">Informed by these insights, we designed our Wayfinding AI around three core principles to create a more empowering conversational experience:</p><ol><li data-block-key=\"4kmek\"><i>Proactive conversational guidance:</i> At each turn, the Wayfinding AI asks up to three targeted questions designed to systematically reduce ambiguity. This helps users articulate their health story more completely and directly incorporates users’ desire for more contextualized answers.<br><br></li><li data-block-key=\"cp0mo\"><i>Best-effort answers at each turn:</i> Because some health-related questions may not require clarification to get a good answer, the Wayfinding AI provides a \"best-effort\" answer at every conversational turn, based on the information shared so far, while emphasizing that the answer can be improved if the user can answer one or more of the follow-up questions. This approach gives the user helpful information throughout the conversation, while providing the option to further receive increasingly better answers as the conversation progresses.<br><br></li><li data-block-key=\"2rorl\"><i>Transparent reasoning:</i> The Wayfinding AI explains how the user's latest answers have helped refine the previous answer. This makes the AI's reasoning process clear and understandable.</li></ol><p data-block-key=\"81tqi\">To ensure clarifying questions are never missed within the longer answers in the “best-effort” answers section, we designed an interface with a two-column layout. The conversation and clarifying questions appear in the left column, while best-effort answers and more detailed explanations appear in the right. This separates the interactive conversation from the informational content.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Wayfinder1_UserInteraction.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d7rd2\"><i>Example of a user starting to interact with our Wayfinding AI prototype interface, including both the familiar multi-turn chat interface on the left, and a “best information so far” panel on the right. This two-panel interface separates the context-seeking stream from the more detailed information provision piece, enabling users to dive into the information only when they feel all relevant information has been relayed.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"65t26\">Evaluating our Wayfinding AI through a randomized user study</h2><p data-block-key=\"7b52c\">To evaluate the potential real-world impact of this agent, we conducted a randomized user study with 130 US-based participants recruited via a third party platform. All participants were 21 years and older, were not health care professionals, and had a health-related question for which they were willing to interact with an AI. To ensure a broad range of health topics, we imposed very few restrictions on which topic would be eligible for the study (details on excluded inquiries are provided in the paper). In a randomized <a href=\"https://www.nngroup.com/articles/between-within-subjects/\" target=\"_blank\" rel=\"noopener noreferrer\">within-subjects design</a>, each participant interacted with both our Wayfinding AI and a baseline Gemini 2.5 Flash model to explore their health topic. After providing informed consent and answering standard demographic questions, participants were instructed to have a conversation spending at least 3 minutes on their question; and then to resume the survey. After interacting with each AI, participants answered questions about their satisfaction with the experience along 6 dimensions: helpfulness, relevance of questions asked, tailoring to their situation, goal understanding, ease of use, and efficiency of getting useful information. They were able to provide open feedback about what they learned, and also had the option to upload their conversation with the AI. Sharing the conversation was not required to complete the survey. At the end of the study, participants were prompted to explicitly compare the two AIs and indicate which they would prefer in terms of each of the six dimensions above. They were also asked, <i>\"For a future topic, would you prefer the first or the second AI?\"</i> The order of AI exposure (Baseline AI first vs. Wayfinding AI first) was randomized across participants. Throughout the study, participants were instructed to not provide any identifying information about themselves.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder2_StudyDesign.width-1250.png\" alt=\"Wayfinder2_StudyDesign\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder2_StudyDesign.width-1250.png\" alt=\"Wayfinder2_StudyDesign\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"9mag7\"><i>Illustration of our study design.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"65t26\">Helpful and relevant information through goal understanding and tailored conversations</h2><p data-block-key=\"7mdek\">As shown below, the results of the study demonstrated that users preferred the Wayfinding AI's approach across several important dimensions, despite its less-familiar two-column interface. Users favored Wayfinding AI for its helpfulness, relevance, ability to understand their goal, and for tailoring the conversation to their specific needs. These findings suggest that the proactive, question-asking behavior of Wayfinding AI successfully created a more personalized and helpful experience for users without introducing undue friction in the user experience.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder3_Results.width-1250.png\" alt=\"Wayfinder3_Results\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder3_Results.width-1250.png\" alt=\"Wayfinder3_Results\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"9mag7\"><i>User preferences between a baseline and our Wayfinding AI along multiple evaluation axes, including helpfulness of the agent, relevance of its responses, tailoring of the conversation to the user, understanding the user’s goal, ease of use, efficiency of the conversation and willingness to use each for a future health information need.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"65t26\">Beyond simply preferring their conversations with the Wayfinding AI, participants had noticeably <i>different</i> conversations. Conversations were longer with the Wayfinding AI, in particular when participants were trying to understand the cause of their symptoms. For those topics, conversations with the Wayfinding AI had 4.96 turns on average, compared to 3.29 for the baseline AI. And the pattern of prompts they provided to each AI looked different across conversations:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder4_HeroSankey.width-1250.png\" alt=\"Wayfinder4_HeroSankey\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder4_HeroSankey.width-1250.png\" alt=\"Wayfinder4_HeroSankey\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"9mag7\"><i>Sankey diagram illustrating the flow of conversations with the baseline AI and the Wayfinding AI. Each of the vertical bars shows the breakdown of the types of user prompts, across the first 5 conversation turns. The blue bars indicate participants responding to clarifying questions — much more common for the Wayfinding AI.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"65t26\">Conclusion</h2><p data-block-key=\"9n52g\">Finding the right health information online can feel like navigating a maze. While AI has the potential to be a powerful guide, our research shows that its success hinges on its ability to move beyond being a passive question-answerer and become an active conversational partner.</p><p data-block-key=\"6i6kb\">By designing our Wayfinding AI to be personal and proactive, we demonstrated how asking targeted questions in a well-structured interface can power an experience that users prefer over a more classical, question-answering experience, and thus enable people to obtain more helpful, relevant, and tailored information. The results from our user studies provide strong evidence that this human-centered, conversational approach is a promising direction for the future of AI in health, helping people navigate their health journeys.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"65t26\">Acknowledgements</h2><p data-block-key=\"4bk1o\"><i>The research described here is joint work across Google Research, Google Health, and partnering teams. We would like to thank Yuexing Hao, Abbi Ward, Amy Wang, Beverly Freeman, Serena Zhan, Diego Ardila, Jimmy Li, I-Ching Lee, Anna Iurchenko, Siyi Kou, Kartikeya Badola, Jimmy Hu, Bhawesh Kumar, Keith Johnson, Supriya Vijay, Justin Krogue, Avinatan Hassidim, Yossi Matias, Dale Webster, Sunny Virmani, Yun Liu, Quang Duong, Fereshteh Mahvar, Laura Vardoulakis, Tiffany Guo, and Meredith Ringel Morris for contributing or reviewing this work. We would also like to thank the participants who contributed to these studies.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "AfriMed-QA：为全球健康领域的大型语言模型进行基准测试 (原标题: AfriMed-QA: Benchmarking large language models for global health)",
      "link": "https://research.google/blog/afrimed-qa-benchmarking-large-language-models-for-global-health/",
      "pubDate": "Tue, 23 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-23T16:00:00.000Z",
      "creator": "Google",
      "summary": "## AfriMed-QA：为全球健康领域的大型语言模型进行基准测试\n\n### 项目背景与挑战\n大型语言模型（LLMs）在医学和健康问答方面展现出巨大潜力，尤其是在资源匮乏地区，它们可以作为宝贵的决策支持工具，提高临床诊断准确性、可及性，并提供多语言临床决策支持和健康培训。然而，现有医学基准测试可能无法涵盖疾病类型、症状背景、语言差异以及本地文化和区域医学知识等方面的分布变化，这限制了LLMs在传统西方环境之外的泛化能力。因此，迫切需要更多样化的基准数据集来反映真实的全球医疗背景。\n\n### AfriMed-QA数据集的诞生\n为解决这一空白，研究团队推出了 **AfriMed-QA**，这是一个基准问答数据集，汇集了来自非洲16个国家60所医学院的消费者式问题和医学院考试题目。该数据集由AfriMed-QA联盟（包括Intron health、Sisonkebiotik、University of Cape Coast等）与PATH/盖茨基金会合作开发。AfriMed-QA在ACL 2025上发布并荣获“最佳社会影响力论文奖”，其数据集和LLM评估代码已开源，并被用于训练MedGemma。\n\n### AfriMed-QA数据集详情\nAfriMed-QA是首个大规模、泛非洲、多专业的医学问答数据集，旨在评估和开发适用于非洲医疗保健的公平有效的LLMs。它包含：\n*   约15,000个临床多样化的英文问答。\n*   4,000多个带答案的专家多项选择题（MCQs）。\n*   1,200多个带长篇答案的开放式简答题（SAQs）。\n*   10,000个消费者查询（CQs）。\n\n该数据集旨在严格评估LLM在正确性和地理分布变化方面的表现。它由来自12个国家60多所医学院的621名贡献者众包完成，涵盖32个医学专业，包括妇产科、神经外科、内科、急诊医学、医学遗传学、传染病等。\n\n![Left: Map of the countries in Africa from which data were sourced, color coded to indicate the relative fraction of contributions. Right: Bar graph showing relative contributions from each country.](https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-3c-Countries.width-1250.png)\n*AfriMed-QA问题和答案的来源国家分布图。*\n\n数据收集通过一个改编自Intron Health的基于网络的平台进行，并开发了定制的用户界面来收集不同类型的问题、进行质量审查以及对LLM响应进行盲评。为了避免消费者分享个人健康信息，消费者查询（CQs）通过疾病情景提示生成，LLM的回答由人类临床专家和消费者进行评分。\n\n![Example of questions used in dataset curation. Top: Multiple choice question; Middle: Short answer question; Bottom: Consumer query](https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-4a-Questions.width-1250.png)\n*AfriMed-QA数据集整理和LLM评估概述中的问题示例。*\n\n![A list of the medial specialties included in the dataset and the number of questions in each.](https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-5a-Specialties.width-1250.png)\n*AfriMed-QA中包含的医学专业及其问题数量列表。*\n\n### LLM评估与发现\n研究团队采用定量和定性方法评估了30个通用和生物医学LLMs。对于MCQs，通过比较LLM的单字母答案与参考答案来衡量准确性；对于SAQs，则通过语义相似性和句子级重叠来衡量。\n\n**主要发现：**\n*   **模型规模与性能：** 较大模型的基线性能在AfriMed-QA上比小型模型更准确。这对于偏好在设备或边缘部署小型专业模型的低资源环境可能不利。\n*   **模型类型与泛化：** 令人惊讶的是，基线通用模型比同等规模的生物医学模型表现更好且泛化能力更强。这可能归因于研究中开放生物医学模型的参数大小限制，或者表明专业LLMs过度拟合了其训练数据的特定偏差和细微差别，导致它们对AfriMed-QA数据集的独特特征适应性较差。\n\n![Bar graph of the performance of LLM models on the AfriMed-QA dataset.](https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-6a-Performance.width-1250.png)\n*LLM模型在AfriMed-QA数据集上的性能条形图（截至2025年5月的实验数据）。*\n\n### 人工评估LLM响应\n研究团队将3000个随机抽样的问题的LLM响应提交给Intron Health众包平台进行人工评估。评估标准包括不准确性、信息遗漏、人口统计偏见证据和潜在危害程度。\n\n*   **临床医生评估：** 评估LLM对MCQ、SAQ和CQ响应的正确性、本地化程度、是否存在遗漏或幻觉以及潜在危害。\n*   **非临床医生/消费者评估：** 评估LLM对CQ响应的相关性、帮助性和本地化程度。\n\n![Image of the interface used for expert review of LLM responses to AfriMed-QA.](https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-7-Interface.width-1250.png)\n*用于专家评审AfriMed-QA中LLM响应的界面图像。*\n\n评估采用5分制，评分者对答案来源（模型或人类）进行盲评。结果显示，消费者和临床医生对LLM在CQ上的响应表现出偏好，前沿LLMs在完整性、信息量和相关性方面始终优于临床医生提供的答案，并且更不容易出现幻觉和遗漏。与此一致，临床医生对CQs的回答在相关信息遗漏方面得分较低。\n\n![Plot showing the consumer blinded evaluations human clinical experts and LLM answers.](https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-8-Evals.width-1250.png)\n*消费者对人类临床专家和LLM答案的盲评图。图表显示了各项评估轴的平均评分和置信区间。*\n\n### 开放排行榜与未来展望\n研究团队开发了一个开放排行榜，方便用户可视化和比较LLM性能，并允许提交自己的模型进行评估。\n\n![Image of the landing page for the AfriMed-QA LLM leaderboard](https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-9-Leaderboard.width-1250.png)\n*AfriMed-QA LLM排行榜的登录页面图像，可比较不同模型在不同基准指标上的表现。*\n\n未来，AfriMed-QA联盟计划将数据集扩展到非英语官方和本土语言，并纳入多模态（如视觉和音频）问答数据集，以反映医学固有的多语言和多模态特性。\n\n### 局限性与行动呼吁\n尽管AfriMed-QA是首个大规模、多专业、本土来源的泛非洲数据集，但它并非完整。目前超过50%的专家MCQ问题来自尼日利亚，团队正努力扩大更多非洲地区和全球南方国家的代表性。这项工作为在缺乏数字化基准数据集的国家获取多样化和代表性的健康基准数据集奠定了基础。\n\n鉴于健康结果的敏感性，评估LLM的准确性、情境性和文化相关性至关重要。研究呼吁其他研究和健康组织通过合作和当地投入，进一步研究该领域，策划数据集以评估和优化LLM在其特定环境中的应用，以适应疾病流行率、文化背景、资源、药物类型、健康建议、医疗技术基础设施、可负担性、护理类型和敏感属性等方面的分布变化。",
      "shortSummary": "AfriMed-QA是一个开创性的泛非洲医学问答基准数据集，旨在评估和改进大型语言模型（LLMs）在全球健康领域的表现。该数据集包含约15,000个问题，涵盖多项选择题、简答题和消费者查询，数据来源于非洲12个国家的60多所医学院，覆盖32个医学专业。研究发现，大型LLMs表现优于小型模型，而通用模型在AfriMed-QA上意外地优于同等规模的生物医学模型。人工评估显示，前沿LLMs在消费者查询方面优于临床医生。AfriMed-QA已开源，并计划扩展至多语言和多模态，以促进LLMs在多样化、低资源医疗环境中的公平有效应用。",
      "translated_title": "AfriMed-QA：为全球健康领域的大型语言模型进行基准测试",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-3c-Countries.width-1250.png",
          "alt": "Left: Map of the countries in Africa from which data were sourced, color coded to indicate the relative fraction of contributions. Right: Bar graph showing relative contributions from each country.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-4a-Questions.width-1250.png",
          "alt": "Example of questions used in dataset curation. Top: Multiple choice question; Middle: Short answer question; Bottom: Consumer query",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-5a-Specialties.width-1250.png",
          "alt": "A list of the medial specialties included in the dataset and the number of questions in each.",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-6a-Performance.width-1250.png",
          "alt": "Bar graph of the performance of LLM models on the AfriMed-QA dataset.",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-7-Interface.width-1250.png",
          "alt": "Image of the interface used for expert review of LLM responses to AfriMed-QA.",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3s6d8\">Large language models (LLMs) have shown potential for medical and health question answering across various health-related tests spanning different formats and sources, such as multiple choice and short answer exam questions (e.g., <a href=\"https://arxiv.org/abs/2009.13081\" target=\"_blank\" rel=\"noopener noreferrer\">USMLE MedQA</a>), summarization, and clinical note taking, among others. Especially in low-resource settings, LLMs can potentially serve as valuable decision-support tools, enhancing clinical diagnostic accuracy and accessibility, and providing multilingual clinical decision support and health training, all of which are especially valuable at the community level.</p><p data-block-key=\"aosjl\">Despite their success on existing medical benchmarks, there is uncertainty about whether these models generalize to tasks involving distribution shifts in disease types, contextual differences across symptoms, or variations in language and linguistics, even within English. Further, localized cultural contexts and region-specific medical knowledge is important for models deployed outside of traditional Western settings. Yet without diverse benchmark datasets that reflect the breadth of real-world contexts, it’s impossible to train or evaluate models in these settings, highlighting the need for more diverse benchmark datasets.</p><p data-block-key=\"fnob9\">To address this gap, we present <a href=\"https://aclanthology.org/2025.acl-long.96.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">AfriMed-QA</a>, a benchmark question–answer dataset that brings together consumer-style questions and medical school–type exams from 60 medical schools, across 16 countries in Africa. We developed the dataset in collaboration with numerous partners, including <a href=\"https://www.intron.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Intron health</a>, <a href=\"https://sisonkebiotik.africa/\" target=\"_blank\" rel=\"noopener noreferrer\">Sisonkebiotik</a>, <a href=\"https://ucc.edu.gh/\" target=\"_blank\" rel=\"noopener noreferrer\">University of Cape Coast</a>, the <a href=\"https://famsanet.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Federation of African Medical Students Association</a>, and <a href=\"https://bioramp.org/\" target=\"_blank\" rel=\"noopener noreferrer\">BioRAMP</a>, which collectively form the <a href=\"https://afrimedqa.com/\" target=\"_blank\" rel=\"noopener noreferrer\">AfriMed-QA consortium</a>, and with support from <a href=\"https://media.path.org/documents/GATES-AI_brief_March_2025_PNpefpK.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">PATH/The Gates Foundation</a>. We evaluated LLM responses on these datasets, comparing them to answers provided by human experts and rating their responses according to human preference. The methods used in this project can be scaled to other locales where digitized benchmarks may not currently be available.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/AfriMedQA-1-Overview.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3s6d8\">AfriMed-QA was published at <a href=\"https://2025.aclweb.org/\" target=\"_blank\" rel=\"noopener noreferrer\">ACL 2025</a> where it won the <a href=\"https://2025.aclweb.org/program/awards/\" target=\"_blank\" rel=\"noopener noreferrer\">Best Social Impact Paper Award</a>. The dataset was recently leveraged to assist in training of <a href=\"https://developers.google.com/health-ai-developer-foundations/medgemma/model-card\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma</a>, our <a href=\"https://research.google/blog/medgemma-our-most-capable-open-models-for-health-ai-development/\">latest open model</a> for multimodal medical text and image comprehension. The <a href=\"https://huggingface.co/datasets/afrimedqa/afrimedqa_v2\" target=\"_blank\" rel=\"noopener noreferrer\">AfriMed-QA benchmark datasets</a> and <a href=\"https://github.com/afrimedqa/AfriMed-QA\" target=\"_blank\" rel=\"noopener noreferrer\">LLM evaluation code</a> are open-sourced and available for use by the community.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/AfriMedQA-2-Storyboard.m4v\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">AfriMed-QA dataset</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3s6d8\">The <a href=\"https://huggingface.co/datasets/afrimedqa/afrimedqa_v2\" target=\"_blank\" rel=\"noopener noreferrer\">AfriMed-QA dataset</a> is the first large-scale pan-African multi-specialty medical question–answer dataset designed to evaluate and develop equitable and effective LLMs for African healthcare. The dataset comprises ~15,000 clinically diverse questions and answers in English, 4,000+ expert multiple choice questions (MCQs) with answers, over 1,200 open ended short answer (SAQs) with long-form answers, and 10,000 consumer queries (CQ). The dataset is designed to rigorously assess LLM performance for correctness and geographical shifts. It was crowd-sourced from 621 contributors, from over 60 medical schools across 12 countries, covering 32 medical specialties, including obstetrics and gynecology, neurosurgery, internal medicine, emergency medicine, medical genetics, infectious disease, and others.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-3c-Countries.width-1250.png\" alt=\"Left: Map of the countries in Africa from which data were sourced, color coded to indicate the relative fraction of contributions. Right: Bar graph showing relative contributions from each country.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-3c-Countries.width-1250.png\" alt=\"Left: Map of the countries in Africa from which data were sourced, color coded to indicate the relative fraction of contributions. Right: Bar graph showing relative contributions from each country.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"q7thn\">Countries where AfriMed-QA questions and answers were sourced.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"nnfxj\">To collect these data, we adapted a web-based platform previously developed by Intron Health for crowd-sourcing accented and multilingual clinical speech data at scale across Africa. We developed custom user interfaces to collect each question type, for quality reviews, and for blinded human evaluation of LLM responses.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-4a-Questions.width-1250.png\" alt=\"Example of questions used in dataset curation. Top: Multiple choice question; Middle: Short answer question; Bottom: Consumer query\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-4a-Questions.width-1250.png\" alt=\"Example of questions used in dataset curation. Top: Multiple choice question; Middle: Short answer question; Bottom: Consumer query\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"c0ibi\">AfriMed-QA dataset curation and LLM evaluation overview. MCQs and SAQs from medical schools had accompanying human labels. For CQs, to avoid consumers sharing their own health information which might lead to potential disclosure of health information, and repetitiveness in question types, consumers were prompted with a disease scenario, and they responded with a question they would ask based on it. The scenario and question were passed to an LLM and the LLM responses were rated by human clinical experts as well as consumers.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-5a-Specialties.width-1250.png\" alt=\"A list of the medial specialties included in the dataset and the number of questions in each.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-5a-Specialties.width-1250.png\" alt=\"A list of the medial specialties included in the dataset and the number of questions in each.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"c0ibi\">Medical specialties represented in AfriMed-QA.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Evaluation of LLM responses</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"nnfxj\">Using quantitative and qualitative approaches, we evaluated 30 general and biomedical LLMs, ranging in size from small to large. Some were open and others were closed. For MCQs, we measured the accuracy by comparing each LLM’s single-letter answer choice with the reference. For SAQs, we measured semantic similarity and sentence level overlap comparing the generated response from the language model against a reference answer.</p><p data-block-key=\"8i9dn\">We found that the baseline performance of larger models is more accurate than small models on AfriMed-QA. This trend may be unfavorable to low-resource settings where on-device or edge deployments with smaller specialized models are preferred.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-6a-Performance.width-1250.png\" alt=\"Bar graph of the performance of LLM models on the AfriMed-QA dataset.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-6a-Performance.width-1250.png\" alt=\"Bar graph of the performance of LLM models on the AfriMed-QA dataset.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"c0ibi\"><i>Performance of LLM models on the AfriMed-QA dataset (experiments as of May 2025).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"nnfxj\">We also found that baseline general models outperform and generalize better than biomedical models of similar size. This counterintuitive result could be due to the parameter size limitations of open biomedical models in our study or it could indicate that specialized LLMs overfit to the specific biases and nuances of the data on which they are fine-tuned. In either case, they seem to be less adaptable to the unique characteristics of the AfriMed-QA dataset.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Human rating of LLM responses</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"nnfxj\">LLM responses to a fixed subset of questions (<i>n</i>=3000; randomly sampled) were sent out for human evaluation on the Intron Health crowd-sourcing platform. Adapting the evaluation axes described in our <a href=\"https://www.nature.com/articles/s41591-024-03423-7\" target=\"_blank\" rel=\"noopener noreferrer\">MedLM paper</a>, which included measures for inaccuracy, omission of information, evidence of demographic bias, and extent of harm, we collected human evaluations in two categories:</p><ol><li data-block-key=\"5ol9l\"><i>Clinicians</i> provided ratings to the LLM’s MCQ, SAQ, and CQ responses, evaluating whether answers were correct and localized, if omissions or hallucinations were present, and if potential for harm existed.</li><li data-block-key=\"nhmf\"><i>Non-clinicians/consumers</i> rated CQ LLM responses to determine if answers were relevant, helpful, and localized.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-7-Interface.width-1250.png\" alt=\"Image of the interface used for expert review of LLM responses to AfriMed-QA.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-7-Interface.width-1250.png\" alt=\"Image of the interface used for expert review of LLM responses to AfriMed-QA.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"c0ibi\"><i>Interface used for expert review of LLM responses to AfriMed-QA.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"nnfxj\">Ratings were on a 5-point scale representing the extent to which the criteria were met. “1” represents “No\" or “completely absent\" and “5” represents “Yes\" or “absolutely present\". Raters were blinded to the answer source (model name or human) and each rater was asked to evaluate answers from multiple LLMs in a random sequence.</p><p data-block-key=\"8mi0p\">Consumer and clinician human evaluation of LLM answers to CQs revealed a preference for LLM responses, where frontier LLMs were consistently rated to be more complete, informative, and relevant when compared with clinician-provided answers, and less susceptible to hallucinations and omissions. Consistent with this, clinician answers to CQs were also rated worse when measured for omission of relevant information.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-8-Evals.width-1250.png\" alt=\"Plot showing the consumer blinded evaluations human clinical experts and LLM answers.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-8-Evals.width-1250.png\" alt=\"Plot showing the consumer blinded evaluations human clinical experts and LLM answers.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"c0ibi\"><i>Consumer blinded evaluations of human clinical experts and LLM answers. Plots show mean ratings and confidence intervals across various axes.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Building an open Leaderboard for easy comparison of data versions and LLM versions</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"nnfxj\">We have developed a leaderboard for easy visualization and comparison of LLM performance. Users can compare existing models or submit their own models and see how well they perform on the dataset.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-9-Leaderboard.width-1250.png\" alt=\"Image of the landing page for the AfriMed-QA LLM leaderboard\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-9-Leaderboard.width-1250.png\" alt=\"Image of the landing page for the AfriMed-QA LLM leaderboard\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"c0ibi\"><i>AfriMed-QA leaderboard enables comparison of different models across different benchmark metrics.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Towards a multilingual, multimodal dataset</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"nnfxj\">We recognize that medicine is inherently multilingual and multimodal and are currently working with the AfriMed-QA consortium led by <a href=\"https://directory.ucc.edu.gh/p/stephen-moore\" target=\"_blank\" rel=\"noopener noreferrer\">Prof. Stephen Moore</a> at the University of Cape Coast to expand beyond English-only text-based question answering to non-English official and native languages from the continent. We are also working to incorporate multimodal (e.g., visual and audio) question answering datasets.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Limitations</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"nnfxj\">Although this is the first large-scale, multi-specialty, indigenously sourced pan-African dataset of its kind, it is by no means complete. Over 50% of the expert MCQ questions came from Nigeria. We are working to expand representation from more African regions and the Global South.</p><p data-block-key=\"8mlsj\">While the development of the dataset is still in progress, this work establishes a foundation for acquiring diverse and representative health benchmark datasets across countries that may not have digitized and readily available benchmark datasets.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">LLMs for geographically diverse health QAs</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"nnfxj\">Given the sensitivity of health-related outcomes, it is essential that LLMs are evaluated for accurate, contextual, and culturally relevant performance. Across different settings one can anticipate a variety of distribution shifts to which LLMs need to adapt. These include disease prevalence, cultural context, resources and infrastructure, drug types and nomenclature, differences in health recommendations for screening and treatment, medical technology infrastructure, affordability, care types, and sensitive attributes. While our evaluations are limited, we present a call to action for other research and health organizations to pursue further research in this area, curating datasets to evaluate and optimize LLMs for use in their contexts through partnerships and local input.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"nnfxj\"><i>We would like to acknowledge the incredible AfriMed-QA consortium and co-authors. Tobi Olatunji, Charles Nimo, Abraham Owodunni, Tassallah Abdullahi, Emmanuel Ayodele, Mardhiyah Sanni, Chinemelu Aka, Folafunmi Omofoye, Foutse Yuehgoh, Timothy Faniran, Bonaventure F. P. Dossou, Moshood Yekini, Jonas Kemp, Katherine Heller, Jude Chidubem Omeke, Chidi Asuzu, Naome A. Etori, Aimérou Ndiaye, Ifeoma Okoh, Evans Doe Ocansey, Wendy Kinara, Michael Best, Irfan Essa, Stephen Edward Moore, and Chris Fourie. We would also like to thank Bilal Mateen, Melissa Miles, Mira Emmanuel-Fabula, and Celeste Gonda from the Gates Foundation/PATH Digital Square for their support of the work and all data contributors. Finally, we thank Marian Croak for her leadership and support.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "时间序列基础模型可以成为少样本学习器 (原标题: Time series foundation models can be few-shot learners)",
      "link": "https://research.google/blog/time-series-foundation-models-can-be-few-shot-learners/",
      "pubDate": "Mon, 22 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-22T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 时间序列基础模型可以成为少样本学习器\n\n## 引言：时间序列预测的挑战与少样本学习\n\n现代商业中，时间序列预测至关重要，但传统方法为每个任务构建专业模型，耗时且需大量专业知识。零样本学习（如我们之前的TimesFM模型）提供了一种解决方案，无需特定任务训练即可准确预测。然而，研究发现，少量示例可以进一步提升预测效果，例如利用附近高速公路或同条高速公路的历史数据来预测交通流量。传统的监督微调方法虽然能利用特定数据微调模型，却重新引入了零样本学习试图避免的复杂性。\n\n为了解决这一问题，我们在ICML 2025上提出了新工作“时间序列基础模型的上下文内微调”（In-Context Fine-Tuning for Time-Series Foundation Models），引入了一种新颖的方法，将TimesFM转变为一个少样本学习器。该方法通过持续预训练，教会模型如何在推理时从少量示例中学习。其结果是获得了一种强大的新能力，能够匹配监督微调的性能，而无需用户进行额外的复杂训练。\n\n![LLM少样本提示与时间序列基础模型少样本提示的比较](https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-1-Prompting.width-1250.png)\n\n*   **图片描述**：左侧展示了大型语言模型（LLM）的少样本提示，右侧展示了时间序列基础模型如何通过任意数量的相关上下文时间序列示例进行少样本提示。橙色框内是模型的输入。\n\n## 模型重设计：TimesFM-ICF\n\n为了实现少样本学习，我们对TimesFM模型进行了重新设计，创建了TimesFM-ICF（In-Context Fine-tuning）。\n\n### TimesFM基础架构\n\nTimesFM是一个补丁解码器，其工作原理如下：\n*   将每32个连续时间点（一个补丁）标记化为输入token。\n*   在输入token序列之上应用一个Transformer堆栈来生成输出token。\n*   然后应用一个共享的多层感知器（MLP）将每个输出token转换回128个时间点的时间序列。\n\n### 创建TimesFM-ICF\n\n我们从基础TimesFM模型开始，并使用新的上下文（预测历史加上所有上下文内示例）继续进行预训练。关键挑战在于确保模型不会混淆预测历史和上下文内示例。\n\n*   **引入通用分隔符token**：为了解决混淆问题，我们引入了一个特殊的、可学习的“通用分隔符token”。这个token就像一个数字“停车标志”或“新段落”符号，放置在每组数字之后。当模型关注到它之前看到的示例的分隔符token时，它就不会将其与当前尝试预测的数据混淆。这使得模型能够从过去的示例模式中学习，并将这些知识应用于当前的预测。\n\n![展示如何在上下文示例流中放置通用分隔符token以指定新数据源的示例](https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-2-Separators.width-1250.png)\n\n*   **图片描述**：如果没有分隔符，简单地连接上下文示例可能会使模型混淆，多个单调趋势可能看起来像一个锯齿状的连续模式。\n\n### 持续预训练\n\n由于分隔符token及其注意力机制对TimesFM来说是全新的，我们的第二步是继续预训练基础TimesFM模型，以教授它这些新引入的机制。具体方法是：\n*   创建一个包含上下文内示例和分隔符token的新数据集。\n*   应用标准的仅解码器下一token预测训练。\n*   **架构流程**：输入传递给MLP层生成token，这些token传递给因果自注意力（CSA）层（防止模型“偷看”未来），CSA层再馈入前馈网络（FFN）。CSA和FFN重复多次（即堆叠的Transformer），然后将结果连接到输出MLP层。\n\n## 模型测试与评估\n\n我们对TimesFM-ICF进行了严格评估，使用了23个模型在任何训练阶段都未曾见过的数据集。每个基准数据集都包含多个时间序列。在预测一个时间序列时，我们从其即时历史开始，然后从其完整历史和同一数据集中其他时间序列的历史中采样序列作为上下文内示例，以确保相关性且无数据泄露。\n\n### 性能指标与基线\n\n我们关注以下性能指标和基线模型：\n*   **性能指标**：几何平均（GM）聚合的平均绝对比例误差（MASE），通过朴素重复最后一个季节性模式进行归一化。\n*   **基线模型**：\n    *   **TimesFM (Base)**：我们开始使用的预训练模型。\n    *   **TimesFM-FT**：TimesFM (Base) 经过监督微调，使用每个数据集的训练集进行微调，并在相应测试集上评估。这是一个强大的基线，代表了领域适应的最佳实践。\n\n![条形图比较TimesFM-ICF与TimesFM (Base) 在多个特定任务模型上的性能](https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-4-Performance.width-1250.png)\n\n*   **图片描述**：TimesFM-ICF在许多特定任务模型上改进了TimesFM (Base) 的性能，并达到了与TimesFM-FT相同的性能。TimesFM-FT是针对每个特定数据集分别进行微调的TimesFM版本。\n\n### 关键发现\n\n*   **准确性提升**：TimesFM-ICF比TimesFM (Base) 准确率提高了6.8%。\n*   **匹配监督微调性能**：更令人惊喜和鼓舞的是，TimesFM-ICF在无需监督微调的复杂性下，达到了与TimesFM-FT相同的性能。\n*   **其他特性**：\n    *   随着上下文内示例的增加，模型预测的准确性会提高（尽管推理时间会相应增加），这符合我们的预期。\n    *   与不具备处理上下文内示例能力的纯粹长上下文模型相比，TimesFM-ICF显示出更好的上下文利用率。\n\n## 未来展望：更易用、更强大的预测\n\n这种新方法具有重要的实际应用价值，因为它允许企业部署一个更强大、适应性更强的单一预测模型。企业无需为预测新产品需求等新任务启动一个完整的机器学习项目，只需向模型提供少量相关的示例即可。这能立即提供最先进的专业预测，从而大幅降低成本，加速决策和创新，并使高端预测民主化。\n\n我们对这项研究的未来感到兴奋，特别是开发自动选择最相关上下文内示例的策略。通过使基础模型更智能、更具适应性，我们赋能更多用户做出更好的数据驱动决策。",
      "shortSummary": "新的研究引入了TimesFM-ICF，一个将时间序列基础模型TimesFM转变为少样本学习器的方法。传统预测模型复杂且耗时，而TimesFM-ICF通过在持续预训练中引入“通用分隔符token”，使模型能在推理时从少量上下文示例中学习。这种方法无需复杂的监督微调，即可将预测准确性比基础模型提高6.8%，并达到与监督微调相当的性能。这使得先进的时间序列预测更易于部署和使用，显著降低成本，加速决策，并促进创新。",
      "translated_title": "时间序列基础模型可以成为少样本学习器",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-1-Prompting.width-1250.png",
          "alt": "Comparison of few-shot prompting of an LLM (left) with few-shot prompting of a time-series foundation model using an arbitrary number of related in-context time series examples (right).",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-2-Separators.width-1250.png",
          "alt": "Example showing how common separator tokens would be placed to designate new data sources in a stream of in-context examples.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-4-Performance.width-1250.png",
          "alt": "Bar graph comparing performance of TimesFM-ICF to TimesFM (base) over many task-specific models.",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"exbjr\"><a href=\"https://en.wikipedia.org/wiki/Time_series\" target=\"_blank\" rel=\"noopener noreferrer\">Time-series forecasting</a> is essential for modern businesses, helping them predict everything from inventory needs to energy demands. Traditionally, this has involved building a separate, specialized model for each task — a process that is slow and requires significant expertise.</p><p data-block-key=\"50kh8\">The emergence of <a href=\"https://en.wikipedia.org/wiki/Zero-shot_learning\" target=\"_blank\" rel=\"noopener noreferrer\">zero-shot learning</a> offered a solution. Our previous model, <a href=\"https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/\">TimesFM</a>, was a zero-shot, pre-trained foundation model that could accurately forecast without task-specific training. But what if a few examples could make the forecast even better? For instance, forecasting highway traffic would be more accurate if the model could consider data from other nearby highways or from the same highway a few weeks ago. The standard solution, supervised fine-tuning, which uses curated data to fine-tune an existing model, reintroduces the complexity one hopes to avoid with zero-shot learning.</p><p data-block-key=\"c2v01\">In our new work, \"<a href=\"https://icml.cc/virtual/2025/poster/43707\" target=\"_blank\" rel=\"noopener noreferrer\">In-Context Fine-Tuning for Time-Series Foundation Models</a>\", presented at ICML 2025, we introduce a novel approach that transforms TimesFM into a <a href=\"https://arxiv.org/abs/2203.04291\" target=\"_blank\" rel=\"noopener noreferrer\">few-shot learner</a>. This method uses continued pre-training to teach the model how to learn from a handful of examples at inference time. The result is a powerful new capability that matches the performance of supervised fine-tuning without requiring additional complex training from the user.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-1-Prompting.width-1250.png\" alt=\"Comparison of few-shot prompting of an LLM (left) with few-shot prompting of a time-series foundation model using an arbitrary number of related in-context time series examples (right).\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-1-Prompting.width-1250.png\" alt=\"Comparison of few-shot prompting of an LLM (left) with few-shot prompting of a time-series foundation model using an arbitrary number of related in-context time series examples (right).\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"h2yvf\"><i>Similar to few-shot prompting of an LLM (</i><b><i>left</i></b><i>), a time-series foundation model should support few-shot prompting with an arbitrary number of related in-context time series examples (</i><b><i>right</i></b><i>). The orange box encloses the inputs to the models.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Redesigning the model</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"exbjr\"><a href=\"https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/\">TimesFM</a> is a patched decoder that tokenizes every 32 contiguous timepoints (a patch) as an <a href=\"https://mehmetozkaya.medium.com/how-llms-use-tokens-ec5916ee321a\" target=\"_blank\" rel=\"noopener noreferrer\">input token</a> and applies a <a href=\"https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\" target=\"_blank\" rel=\"noopener noreferrer\">transformer</a> stack on top of the sequence of input tokens to generate the output tokens. It then applies a shared <a href=\"https://en.wikipedia.org/wiki/Multilayer_perceptron\" target=\"_blank\" rel=\"noopener noreferrer\">multilayer perceptron</a> (MLP) to translate each output token back to a time series of 128 timepoints.</p><p data-block-key=\"5q88o\">To create TimesFM-ICF (In-Context Fine-tuning), we start with the base TimesFM model and continue the pre-training with new context: the forecast history plus all in-context examples. The first step is to make sure the model doesn’t confuse or conflate the forecasting history and the in-context examples. Imagine you're giving the model a list of numbers that represent a few different things, maybe sunglasses sales figures from one store, then umbrella sales figures from another. If you just merge all those numbers together, the model might get confused, thinking it's one continuous stream of data. For example, if the first store’s sales were going up and the second store’s sales were going down, the model might incorrectly see it as a single up-and-down pattern, rather than two separate, simple trends.</p><p data-block-key=\"66jf\">To fix this, we put a special, learnable “common separator token” — like a digital \"stop sign\" or a \"new paragraph\" symbol — after each set of numbers. With these separators in place, as soon as the model attends to the separator token of an example it has seen before, it won't mix it up with the data it's currently trying to predict. This theoretically allows the model to learn from patterns in those past examples and apply that knowledge to the current forecast. For instance, the model could learn that \"all the store sales are showing consistent, directional trends lately, so I should predict an upward trend for my new store’s sunscreen sales.\"</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-2-Separators.width-1250.png\" alt=\"Example showing how common separator tokens would be placed to designate new data sources in a stream of in-context examples.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-2-Separators.width-1250.png\" alt=\"Example showing how common separator tokens would be placed to designate new data sources in a stream of in-context examples.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"h2yvf\"><i>Concatenating in-context examples without separators could confuse the model — multiple monotonic trends might look like a jagged, continuous pattern if concatenated naïvely.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"exbjr\">Since the separator tokens and the attention to them are new for TimesFM, our second step involves continuing the pre-training of the base TimesFM model to teach it about the new introductions. The recipe here is actually straightforward: we created a new dataset that includes both in-context examples and separator tokens, and we applied standard decoder-only next-token prediction training. Inputs are passed to the MLP layer, which generates tokens. These are passed to a <a href=\"https://en.wikipedia.org/wiki/Attention_(machine_learning)#Masking\" target=\"_blank\" rel=\"noopener noreferrer\">causal self attention</a> (CSA) layer that \"attends to\" information from previous tokens in the sequence, a step that's crucial in tasks like time-series forecasting as it prevents the model from looking into the future. The CSA then feeds into a <a href=\"https://en.wikipedia.org/wiki/Feedforward_neural_network\" target=\"_blank\" rel=\"noopener noreferrer\">feed-forward network</a> (FFN). We repeat CSA and FFN multiple times (i.e., the <a href=\"https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\" target=\"_blank\" rel=\"noopener noreferrer\">stacked transformers</a>) before connecting the result to the output MLP layer.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Time_Foundation_v3.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"h2yvf\"><i>TimesFM-ICF employs the decoder-only architecture for time-series forecasting with in-context examples. A special common separator token is introduced to disambiguate between the in-context examples and the task history.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Testing the model</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"exbjr\">We evaluated TimesFM-ICF on 23 datasets that the model had never seen during any phase of its training. Each dataset in this benchmark has multiple time series. When we forecast a time series, we start with its immediate history, then sample sequences from its full history and the histories of other time series in the same dataset as in-context examples. This ensures the in-context examples are relevant and there is no leakage.</p><p data-block-key=\"3u0jv\">The chart below shows the <a href=\"https://en.wikipedia.org/wiki/Geometric_mean\" target=\"_blank\" rel=\"noopener noreferrer\">geometric mean</a> (GM) aggregation of the <a href=\"https://en.wikipedia.org/wiki/Mean_absolute_scaled_error\" target=\"_blank\" rel=\"noopener noreferrer\">mean absolute scaled errors</a> (MASE) normalized by a <a href=\"https://en.wikipedia.org/wiki/Forecasting#Seasonality_and_cyclic_behaviour\" target=\"_blank\" rel=\"noopener noreferrer\">naïve repeat of the last seasonal pattern</a>. We focus on two baselines here:</p><ul><li data-block-key=\"dqkkk\">TimesFM (Base), which is the pre-trained model from which we started.</li><li data-block-key=\"8ourm\">TimesFM-FT is TimesFM (Base) with supervised fine-tuning using the train split per dataset and then evaluated on the corresponding test split. This is a strong baseline that reflects the previous best practice for domain adaptation.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-4-Performance.width-1250.png\" alt=\"Bar graph comparing performance of TimesFM-ICF to TimesFM (base) over many task-specific models.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-4-Performance.width-1250.png\" alt=\"Bar graph comparing performance of TimesFM-ICF to TimesFM (base) over many task-specific models.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"h2yvf\"><i>TimesFM-ICF improves the performance of TimesFM (Base) over many task-specific models and achieves the same performance as that of TimesFM-FT, which is a version of TimesFM fine-tuned for each specific dataset, respectively.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"exbjr\">TimesFM-ICF is 6.8% more accurate than TimesFM (Base). What’s more surprising and inspiring is that it matches the performance of TimesFM-FT without the hassle of running supervised fine-tuning.</p><p data-block-key=\"3qdc5\">Besides the accuracy improvement, TimesFM-ICF also demonstrates other desirable properties. For example, it is consistent with our expectation that with more in-context examples, a model will make more accurate forecasts at the cost of longer inference time. In addition, TimesFM-ICF shows better utilization of its context when compared to a purely long-context model that does not have the ability to work with in-context examples.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The future: More accessible and powerful forecasting</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"exbjr\">This new approach has significant real-world applications because it allows businesses to deploy a more robust and adaptable single, powerful forecasting model. Instead of launching a full ML project for new tasks, like forecasting demand for a new product, they can simply feed the model a few new relevant examples. This immediately provides state-of-the-art, specialized forecasts, dramatically cutting costs, accelerating decision-making and innovation, and democratizing access to high-end forecasting.</p><p data-block-key=\"bk79l\">We're excited by this research's future, particularly developing automated strategies for selecting the most relevant in-context examples. By making foundation models more intelligent and adaptable, we empower more users to make better, data-driven decisions.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"exbjr\"><i>This research was led by then-student researcher Matthew Faw in collaboration with Google Research colleagues Abhimanyu Das and Ivan Kuznetsov. This blog post was brought to life with the tremendous help from editors Mark Simborg and Kimberly Schwede.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "测试时扩散深度研究代理 (原标题: Deep researcher with test-time diffusion)",
      "link": "https://research.google/blog/deep-researcher-with-test-time-diffusion/",
      "pubDate": "Thu, 18 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-18T16:00:00.000Z",
      "creator": "Google",
      "summary": "大型语言模型（LLMs）的最新进展推动了深度研究（DR）代理的兴起，这些代理能够生成新想法、检索信息、执行实验并撰写报告。然而，现有DR代理通常将不同工具简单组合，缺乏人类研究中迭代的“规划、起草、研究和基于反馈迭代”过程，尤其是在复杂主题的论文写作中。它们缺少通过研究来完善和加强论点的关键“去噪”步骤。\n\n**TTD-DR：模仿人类研究的深度研究代理**\n\n本文介绍了**测试时扩散深度研究代理（Test-Time Diffusion Deep Researcher, TTD-DR）**，这是首个将研究报告写作建模为扩散过程的研究代理，即从一个“嘈杂”或初步的草稿逐步完善为高质量的最终版本。TTD-DR引入了两种协同工作的新算法：\n\n*   **组件级自进化优化**：提升研究工作流中每个步骤的质量。\n*   **报告级检索去噪细化**：利用新检索到的信息修订和改进报告草稿。\n\nTTD-DR在长篇报告写作和多跳推理任务上取得了最先进的成果。\n\n**TTD-DR 的设计理念**\n\nTTD-DR接收用户查询后，会创建一个初步草稿作为不断演进的基础，以指导研究计划。这个草稿通过“检索去噪”过程（报告级细化）进行迭代完善，将找到的信息用于改进草稿。这个过程在一个连续的循环中进行，每次循环都会改进报告。此外，一个自进化算法不断增强从初始计划到最终报告的整个过程，这种细化和自我改进的强大组合使得报告写作过程更加连贯。\n\n![TTD-DR示意图](https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-1.width-1250.png)\n*TTD-DR示意图。其设计旨在通过迭代的起草和修订周期来模仿典型的研究实践。*\n\n**核心DR设计（三阶段）**\n\nTTD-DR的核心DR设计包含三个阶段：\n\n1.  **研究计划生成**：根据用户查询生成结构化的研究计划，概述最终报告所需的关键领域，作为后续信息收集的初步指导。\n2.  **迭代搜索**：包含两个子代理：\n    *   **搜索问题生成（阶段2a）**：根据研究计划、用户查询和先前搜索迭代的上下文（即过去的问答）制定搜索查询。\n    *   **答案搜索（阶段2b）**：搜索可用来源以查找相关文档并返回总结的答案，类似于检索增强生成（RAG）系统。\n3.  **最终报告生成**：通过结合所有收集到的结构化信息（计划和一系列问答对）生成全面且连贯的最终报告。\n\n![核心DR代理设计图](https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-2.width-1250.png)\n*我们的核心DR代理分三个阶段运行。阶段1生成详细的研究计划；阶段2a迭代生成搜索问题，然后使用类似RAG的系统从检索到的文档中合成精确答案（2b）；阶段3合成所有收集到的信息以生成最终报告。*\n\n**组件级自进化**\n\nTTD-DR利用自进化算法来增强每个阶段代理的性能，以发现并保留高质量的上下文。该过程包括：\n\n*   **初始状态**：基于前一阶段输出的多个多样化答案变体，用于探索更大的搜索空间。\n*   **环境反馈**：每个答案变体由一个作为评判者的LLM进行评估，利用自动评分器评估有用性和全面性等指标，并生成文本反馈以改进答案。\n*   **修订**：根据评分和反馈，每个变体进行修订，以适应更好的评分。环境反馈和修订步骤重复进行，直到达到最大迭代次数或代理确定不再需要修订。\n*   **交叉**：最终，多个修订后的变体合并为一个高质量的输出，为主要报告生成过程提供卓越的上下文。\n\n![组件级自进化算法示意图](https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-3.width-1250.png)\n*组件级自进化算法应用于搜索答案（阶段2b）的示意图。该过程从多个初始答案变体开始，每个变体都经历一个自进化过程，首先与环境交互以获得适应度分数和反馈，然后根据反馈进行修订。这个过程重复进行，直到达到最大迭代次数。最后，来自所有迭代的多个修订后的变体被合并以生成最终答案。*\n\n**报告级检索去噪**\n\nTTD-DR使用搜索工具来去噪和演进草稿。具体而言，当前报告草稿被输入到核心DR工作流的搜索生成阶段（阶段2a），以指导下一个搜索查询的生成。在答案搜索阶段（阶段2b）获得合成答案后，新信息用于修订报告草稿，无论是添加新细节还是验证现有信息。这种将去噪后的报告反馈以生成下一个搜索查询的过程会重复进行。草稿逐步去噪，直到搜索过程结束，此时最终代理根据所有历史搜索答案和修订撰写最终报告（阶段3）。\n\n**实验结果**\n\nTTD-DR在以下基准数据集上进行了评估：\n\n1.  **复杂查询**：需要研究代理生成长篇综合报告（DeepConsult）。\n2.  **多跳查询**：需要广泛搜索和推理才能回答（Humanity's Last Exam [HLE] 和 GAIA，以及HLE-Search的200个子查询）。\n\n与OpenAI Deep Research相比，TTD-DR在所有基准测试中始终取得更好的结果：\n\n*   在长篇研究报告生成任务中，TTD-DR的胜率达到74.5%。\n*   在两个广泛研究数据集（HLE-Search和GAIA）上，分别超越OpenAI DR 7.7%和1.7%。\n\n![TTD-DR性能对比图](https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-4.width-1250.png)\n*TTD-DR在基准数据集上与不同基线系统的性能对比。左图：胜率（%）基于OpenAI DR计算。右图：正确性计算为系统预测答案与参考答案的匹配度。TTD-DR以显著优势优于OpenAI DR。*\n\n**消融研究**\n\n消融研究逐步增加了三种方法（核心DR、自进化、检索去噪）。使用Gemini-2.5-pro作为基础模型：\n\n*   仅核心DR代理表现不如OpenAI DR。\n*   加入自进化算法后，DeepConsult的胜率达到59.8%，HLE-Search和GAIA的正确性分数分别提高4.4%和1.2%。\n*   最终，结合检索去噪带来了所有基准测试的显著提升。\n\n![TTD-DR消融研究图](https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-5.width-1250.png)\n*TTD-DR通过逐步添加1) 核心DR，2) 自进化，和3) 检索去噪的性能。我们观察到全面逐步改进，帮助我们实现新的最先进结果。*\n\n**效率**\n\nTTD-DR在测试时扩展效率方面也优于其他DR代理。在相同的延迟下，TTD-DR实现了更好的质量/胜率。\n\n![质量与延迟的帕累托前沿图](https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-6.width-1250.png)\n*研究报告质量与延迟（秒）的帕累托前沿图。蓝线表示TTD-DR，灰色点表示对比的DR代理。*\n\n**结论**\n\nTTD-DR是一个受人类迭代研究方式启发的新框架，通过将报告生成概念化为扩散过程，解决了现有DR代理的局限性。它在需要密集搜索和多跳推理的各种基准测试中显著优于现有DR代理，并在生成长篇综合研究报告和识别多跳搜索与推理任务的简洁答案方面表现出最先进的性能。其“草稿优先”的设计被认为是其成功的关键。\n\n**可用性**\n\n该工作的产品版本已在Google Agentspace上提供，并使用Google Cloud Agent Development Kit实现。",
      "shortSummary": "TTD-DR（测试时扩散深度研究代理）是一个模仿人类迭代研究过程的新型AI代理。它将报告写作建模为从初步草稿到高质量最终版本的扩散过程，并结合了组件级自进化和报告级检索去噪两种算法。TTD-DR在长篇报告写作和多跳推理任务上取得了最先进的成果，并在多项基准测试中显著优于OpenAI Deep Research等现有代理。其“草稿优先”的设计是其成功的关键。",
      "translated_title": "测试时扩散深度研究代理",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-1.width-1250.png",
          "alt": "Deep-Researcher-1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-2.width-1250.png",
          "alt": "Deep-Researcher-2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-3.width-1250.png",
          "alt": "Deep-Researcher-3",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-4.width-1250.png",
          "alt": "Deep-Researcher-4",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-5.width-1250.png",
          "alt": "Deep-Researcher-5",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ocegg\">The recent advances in large language models (LLMs) have fueled the emergence of <a href=\"https://openai.com/index/introducing-deep-research/\" target=\"_blank\" rel=\"noopener noreferrer\">deep research</a> (DR) agents. These agents demonstrate remarkable capabilities, including the generation of <a href=\"https://arxiv.org/abs/2409.04109\" target=\"_blank\" rel=\"noopener noreferrer\">novel ideas</a>, efficient <a href=\"https://arxiv.org/abs/2503.09516\" target=\"_blank\" rel=\"noopener noreferrer\">information retrieval</a>, experimental execution, and the subsequent drafting of comprehensive <a href=\"https://arxiv.org/pdf/2408.06941\" target=\"_blank\" rel=\"noopener noreferrer\">reports</a> and <a href=\"https://arxiv.org/abs/2504.08066\" target=\"_blank\" rel=\"noopener noreferrer\">academic papers</a>.</p><p data-block-key=\"acuge\">Currently, most <a href=\"https://github.com/assafelovic/gpt-researcher\" target=\"_blank\" rel=\"noopener noreferrer\">public DR agents</a> use a variety of clever techniques to improve their results, like <a href=\"https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/\">performing reasoning via chain-of-thought</a> or <a href=\"https://openreview.net/forum?id=H4S4ETc8c9\" target=\"_blank\" rel=\"noopener noreferrer\">generating multiple answers</a> and selecting the best one. While they've made impressive progress, they often bolt different tools together without considering the iterative nature of human research. They're missing the key process (i.e., planning, drafting, researching, and iterating based on feedback) on which people rely when writing a paper about a complex topic. A key part of that revision process is to do more research to <a href=\"https://www.emerald.com/jd/article-abstract/69/2/243/198951/Patterns-of-graduate-students-information-seeking?redirectedFrom=fulltext\" target=\"_blank\" rel=\"noopener noreferrer\">find missing information or strengthen your arguments</a>. This human pattern is surprisingly similar to the mechanism of <a href=\"https://proceedings.mlr.press/v202/zhang23as/zhang23as.pdf\" target=\"_blank\" rel=\"noopener noreferrer\"><i>retrieval</i></a>-augmented diffusion models that start with a “noisy” or messy output and gradually refine it into a high-quality result. What if an AI agent's rough draft is the noisy version, and a search tool acts as the denoising step that cleans it up with new facts?</p><p data-block-key=\"5njoq\">Today we introduce <a href=\"https://arxiv.org/abs/2507.16075\" target=\"_blank\" rel=\"noopener noreferrer\">Test-Time Diffusion Deep Researcher</a> (TTD-DR), a DR agent that imitates the way humans do research. To our knowledge, TTD-DR is the first research agent that models research report writing as a diffusion process, where a messy first draft is gradually polished into a high-quality final version. We introduce two new algorithms that work together to enable TTD-DR. First, component-wise optimization via <a href=\"https://arxiv.org/abs/2501.09891\" target=\"_blank\" rel=\"noopener noreferrer\">self-evolution</a> enhances the quality of each step in the research workflow. Then, report-level refinement via <a href=\"https://arxiv.org/abs/2302.02285\" target=\"_blank\" rel=\"noopener noreferrer\">denoising with retrieval</a> applies newly retrieved information to revise and improve the report draft. We demonstrate that TTD-DR achieves state-of-the-art results on long-form report writing and multi-hop reasoning tasks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Test-Time Diffusion Deep Researcher</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">TTD-DR is designed to take a user query as input and then create a preliminary draft that serves as an evolving foundation to guide the research plan. This evolving draft is iteratively refined using a denoising with retrieval process (report-level refinement) that takes the information it finds and uses it to improve the draft at each step. This happens in a continuous loop that improves the report with each cycle. To top it all off, a self-evolution algorithm constantly enhances the entire process, from the initial plan to the final report. This powerful combination of refinement and self-improvement leads to a more coherent report writing process.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-1.width-1250.png\" alt=\"Deep-Researcher-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-1.width-1250.png\" alt=\"Deep-Researcher-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d0pys\"><i>Illustration of TTD-DR. We designed it to imitate typical research practices by performing iterative cycles of drafting and revision.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Backbone DR design</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">The backbone DR design consists of three stages that we outline below.</p><ol><li data-block-key=\"40uvv\"><b>Research plan generation:</b> Produces a structured research plan upon receiving a user query. This plan outlines a list of key areas needed for the final report, serving as an initial guideline for the subsequent information-gathering process.</li><li data-block-key=\"9c90d\"><b>Iterative search:</b> Contains two sub-agents: Search Question Generation (stage 2a in the figure below) formulates a search query based on the research plan, the user query, and the context from previous search iterations (i.e., past questions and answers). Answer Searching (stage 2b) searches the available sources to find relevant documents and returns a summarized answer, similar to <a href=\"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\" target=\"_blank\" rel=\"noopener noreferrer\">retrieval-augmented generation</a> (RAG) systems.</li><li data-block-key=\"dambo\"><b>Final report generation:</b> Produces a comprehensive and coherent final report by combining all the structured information gathered, that is, the plan and the series of question-answer pairs.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-2.width-1250.png\" alt=\"Deep-Researcher-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-2.width-1250.png\" alt=\"Deep-Researcher-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d0pys\"><i>Our backbone DR agent operates in three stages. Stage 1 generates a detailed research plan; Stage 2a iteratively generates search questions and then uses a RAG-like system to synthesize precise answers from retrieved documents (2b); Stage 3 synthesizes all gathered information to produce the final report.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Component-wise self-evolution</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">We leverage a self-evolutionary algorithm to enhance the performance of each stage's agents in order to <i>find</i> and <i>preserve</i> the high quality context.</p><ul><li data-block-key=\"e6eh6\"><b>Initial states:</b> The leftmost blocks in the diagram below represent multiple diverse answer variants based on the output of previous stages, which are used to explore a larger search space. This ideally leads to discovery of more valuable information.</li><li data-block-key=\"c8sei\"><b>Environmental feedback:</b> Each answer variant is assessed by an LLM-as-a-judge, utilizing auto-raters for metrics, such as helpfulness and comprehensiveness. These raters not only provide fitness scores but also generate textual feedback that help improve the answer.</li><li data-block-key=\"9d9u5\"><b>Revision:</b> With the scores and feedback from the previous step, each variant undergoes a revision step to adapt toward better fitness scores. The environmental feedback and revision steps repeat until reaching some maximum number of iterations or until the agent determines no more revisions are needed.</li><li data-block-key=\"fh789\"><b>Cross-over:</b> Finally, multiple revised variants are merged into a single, high-quality output. This merging process consolidates the best information from all evolutionary paths, producing superior context for the main report generation process.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-3.width-1250.png\" alt=\"Deep-Researcher-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-3.width-1250.png\" alt=\"Deep-Researcher-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d0pys\"><i>Illustration of the component-wise self-evolution algorithm applied to Search Answer (Stage 2b). The process starts with multiple variants of initial answers, each undergoing a self-evolving episode where it first interacts with the environment to obtain a fitness score and feedback. It is then revised based on the feedback. This process repeats until the maximum number of iterations is reached. Finally, multiple revised variants from all episodes are merged to produce the final answer.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Report-level denoising with retrieval</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">Since a preliminary noisy draft is useless for complex topics without real research, TTD-DR uses a search tool that denoises and evolves the draft.</p><p data-block-key=\"2cj8q\">Specifically, we feed the current draft report into the Search Generation stage (Stage 2a) of the backbone DR workflow to inform the generation of the next search query. After obtaining a synthesized answer in the Answer Searching stage (Stage 2b), the new information is used to revise the report draft, either by adding new details or by verifying existing information. This process of feeding the denoised report back to generate the next search query is repeated. The draft is progressively denoised until the search process concludes, at which point a final agent writes the final report based on all historical search answers and revisions (Stage 3).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">We evaluate TTD-DR's performance using benchmark datasets that focus on two broad tasks: 1) Complex queries that require research agents to produce a long-form comprehensive report (<a href=\"https://github.com/Su-Sea/ydc-deep-research-evals\" target=\"_blank\" rel=\"noopener noreferrer\">DeepConsult</a>) and, 2) multi-hop queries that require extensive search and reasoning to answer (<a href=\"https://scale.com/leaderboard/humanitys_last_exam\" target=\"_blank\" rel=\"noopener noreferrer\">Humanity's Last Exam</a> [HLE] and <a href=\"https://huggingface.co/datasets/gaia-benchmark/GAIA\" target=\"_blank\" rel=\"noopener noreferrer\">GAIA</a>). We sub-sample 200 queries from HLE that need more search and reasoning (HLE-Search). Both categories fit into our objective of building a general-purpose, real-world research companion. We compare our DR systems with <a href=\"https://openai.com/index/introducing-deep-research/\" target=\"_blank\" rel=\"noopener noreferrer\">OpenAI Deep Research</a>.</p><p data-block-key=\"b0lhd\">TTD-DR consistently achieves better results across all benchmarks. Notably, when compared to OpenAI DR, TTD-DR achieves 74.5% win rate for the <i>long-form</i> research report generation tasks. Additionally, it outperforms OpenAI DR by 7.7% and 1.7% on the two extensive research datasets with <i>short-form</i> ground-truth answers.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-4.width-1250.png\" alt=\"Deep-Researcher-4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-4.width-1250.png\" alt=\"Deep-Researcher-4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d0pys\"><i>TTD-DR's performance against different baseline systems for benchmark datasets.</i> <b><i>Left</i></b><i>: Win rates (%) are computed based on OpenAI DR.</i> <b><i>Right</i></b><i>: Correctness is computed as matching between system predicted and reference answers. TTD-DR outperforms OpenAI DR with significant margins.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Ablation study</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">For the ablation study, we incrementally add the three methods in the section above. Our DR agents use <a href=\"https://arxiv.org/abs/2507.06261\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini-2.5-pro</a> as the base model. All other baseline agents use their default LLMs. The charts below show the ablation study for our DR agents. The backbone DR agent underperforms OpenAI DR. With the addition of the proposed self-evolution algorithm, we observe that for DeepConsult, our system outperforms OpenAI Deep Research with 59.8% win rates. The Correctness scores on HLE-Search and GAIA datasets also show an improvement of 4.4% and 1.2%. Finally, incorporating diffusion with retrieval leads to substantial gains across all benchmarks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-5.width-1250.png\" alt=\"Deep-Researcher-5\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-5.width-1250.png\" alt=\"Deep-Researcher-5\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d0pys\"><i>TTD-DR's performance by incrementally adding 1) backbone DR, 2) self-evolution, and 3) diffusion with retrieval. We observe step-by-step improvements across the board that help us achieve new state-of-the-art results.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ocegg\">The <a href=\"https://en.wikipedia.org/wiki/Pareto_front\" target=\"_blank\" rel=\"noopener noreferrer\">Pareto-frontier diagram</a> below further shows the test-time scaling efficiency of TTD-DR compared with other DR agents. We found that TTD-DR is more efficient than OpenAI DR, as with the same latency, it achieves the better quality per win-rate. See the <a href=\"https://arxiv.org/abs/2507.16075\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a> for more details.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-6.width-1250.png\" alt=\"Deep-Researcher-6\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-6.width-1250.png\" alt=\"Deep-Researcher-6\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d0pys\"><i>Pareto-frontier of research report quality vs. latency in seconds. The blue line indicates TTD-DR, whereas grey dots indicate compared DR agents.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">The Deep Researcher with Test-Time Diffusion (TTD-DR) is a new framework inspired by the iterative way humans do research. This agent addresses the limitations of existing DR agents by conceptualizing report generation as a diffusion process. The TTD-DR framework significantly outperforms existing DR agents across various benchmarks requiring intensive search and multi-hop reasoning. It demonstrates state-of-the-art performance in generating comprehensive long-form research reports and identifying concise answers for multi-hop search and reasoning tasks. We believe the reason it works so well is its \"draft-first\" design, which keeps the whole research process focused and coherent, preventing important information from getting lost along the way.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Availability on Google Cloud Platform</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">A product version of this work is available on <a href=\"https://cloud.google.com/agentspace/docs/research-assistant\" target=\"_blank\" rel=\"noopener noreferrer\">Google Agentspace</a>, implemented with Google Cloud <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/agent-development-kit/quickstart\" target=\"_blank\" rel=\"noopener noreferrer\">Agent Development Kit</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\"><i>This research was conducted by Rujun Han, Yanfei Chen, Guan Sun, Lesly Miculicich, Zoey CuiZhu, Yuanjun (Sophia) Bi, Weiming Wen, Hui Wan, Chunfeng Wen, Solène Maître, George Lee, Vishy Tirumalashetty, Xiaowei Li, Emily Xue, Zizhao Zhang, Salem Haykal, Burak Gokturk, Tomas Pfister, and Chen-Yu Lee.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Sensible Agent: 一种用于与主动式AR智能体进行无侵扰交互的框架 (原标题: Sensible Agent: A framework for unobtrusive interaction with proactive AR agents)",
      "link": "https://research.google/blog/sensible-agent-a-framework-for-unobtrusive-interaction-with-proactive-ar-agents/",
      "pubDate": "Wed, 17 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-17T16:00:00.000Z",
      "creator": "Google",
      "summary": "# Sensible Agent：无侵扰式主动AR智能体交互框架\n\n## 引言与背景\nGoogle的Project Astra等最新创新展示了嵌入增强现实（AR）眼镜中的主动式智能体在预测用户需求并无缝融入日常生活方面的巨大潜力。然而，当前的智能体主要依赖用户明确的口头命令，这在社交环境中可能显得尴尬或具有破坏性，在时间敏感的场景中会增加认知负担，或者根本不切实际。\n\n## Sensible Agent 框架介绍\n为了解决这些挑战，我们推出了 **Sensible Agent**，该框架已在UIST 2025上发表，旨在实现与主动式AR智能体进行无侵扰的交互。Sensible Agent 通过预测用户意图并确定最佳的辅助方式来重塑这种交互。它利用实时多模态上下文感知、微妙的手势、凝视输入和最少的视觉提示来提供无侵扰、上下文适宜的帮助。这标志着向真正集成、具有社交意识的AR系统迈出了关键一步，这些系统尊重用户上下文，最大程度地减少认知干扰，并使主动式数字辅助在日常生活中变得实用。\n\n## 核心模块\nSensible Agent 框架的核心由两个相互关联的模块组成：\n1.  **理解“协助什么”：** 利用先进的多模态感知技术（如以用户为中心的摄像头和环境上下文检测）来理解用户当前的辅助需求，并主动决定最有效的行动（例如，提供翻译、推荐菜肴或显示购物清单）。\n2.  **确定“如何”提供协助：** 根据社交上下文智能地选择侵扰性最小、最合适的交互方法。例如，如果用户双手正忙，智能体可能会通过点头来启用确认；在嘈杂环境中，则可能显示视觉图标而非语音。\n\n![Sensible Agent 演示](https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-3.width-1250.png)\n*Sensible Agent 演示：AR智能体（左）检测上下文，（中）主动建议行动，（右）允许用户通过“竖起大拇指”手势进行无侵扰响应。*\n\n## Sensible Agent 原型构建\n为了将这一概念变为现实，我们实现了 Sensible Agent 的一个功能齐全的原型，该原型运行在 Android XR 和 WebXR 上，并集成了强大的多模态AI模型。该原型包含四个组件：\n1.  **上下文解析器：** 使用视觉语言模型（VLM）和音频事件分类器YAMNet分析摄像头输入和环境噪音，以理解用户情境（如活动、位置、噪音水平）。\n2.  **主动查询生成器：** 根据解析后的上下文，使用思维链（CoT）推理和少样本学习识别最有用的行动，并输出完整的智能体建议（包括行动、查询格式和呈现模态）。\n3.  **交互模块：** 管理输出（通过UI管理器呈现视觉面板或TTS音频）和输入（根据上下文激活头部手势、手部手势、口头命令或凝视等响应方式）。\n4.  **响应生成器：** 一旦用户选择选项，使用大型语言模型（LLM）生成自然语言答案，并通过TTS转换为音频播放给用户。\n\n![Sensible Agent 系统架构](https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-1.width-1250.png)\n*Sensible Agent 原型系统架构。整个系统在WebXR中实现，并运行在Android XR头戴设备上。*\n\n## 用户研究\n我们进行了一项结构化的用户研究，将 Sensible Agent 与模仿Project Astra的传统语音控制AR助手进行比较，以评估其在减少交互努力和干扰方面的表现。\n*   **参与者：** 10名参与者，每人使用Android XR头戴设备完成了12个真实场景（通过360°视频或物理搭建的AR环境呈现）。\n*   **活动类型：** 涵盖阅读餐厅菜单、公共交通通勤、杂货购物、参观博物馆、健身房锻炼和厨房烹饪六种日常活动。\n*   **研究条件：**\n    *   **基线：** 语音控制，用户明确发出命令。\n    *   **Sensible Agent：** 主动提供上下文适应的建议，使用侵扰性最小的方法（如视觉图标、音频提示和基于手势的交互）。\n*   **测量指标：** NASA任务负荷指数（NASA-TLX，测量认知负荷）、系统可用性量表（SUS）、用户偏好（7点李克特量表）和总交互时间。\n\n![用户研究参与者](https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-2.width-1250.png)\n*用户研究参与者在360度视频或视频透视（VST）AR中体验了一系列场景，包括基线和Sensible Agent两种条件。*\n\n## 研究结果\n*   **认知负荷：** Sensible Agent 显著降低了心理需求（平均21.1 vs 65.0，p < .001）和感知努力（p = .0039）。\n*   **可用性：** 两个系统表现良好，SUS分数无显著差异（p = .11）。\n*   **用户偏好：** 参与者对 Sensible Agent 表现出强烈且统计学上显著的偏好（平均6.0 vs 3.8，p = .0074）。\n*   **交互时间：** 基线系统更快（μ = 16.4s），Sensible Agent 较慢（μ = 28.5s），这是其两步交互流程的预期权衡，但用户偏好表明这种权衡在需要谨慎和最小用户努力的社交环境中是可接受的。\n\n![定量研究结果](https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-4.width-1250.png)\n*用户研究中测量的定量结果：（a）交互时间，（b）SUS分数，（c）偏好，以及（d）原始NASA TLX分数。统计显著性用∗、∗∗或∗∗∗标注（分别代表𝑝 < .05、𝑝 < .01和𝑝 < .001）。*\n\n关键见解是，主动性不仅减少了努力，还重塑了用户与智能体之间的关系。参与者觉得 Sensible Agent 更像是一个协作伙伴，其微妙的非语言输入模仿了社交线索，使交互感觉更自然。这表明交互的“如何”与“什么”同样重要，才能使智能体感觉像一个积极参与的助手。\n\n## 结论与未来方向\n本研究表明，通过共同推理“建议什么”和“如何提供”，主动式AR辅助可以变得既智能又无侵扰。通过将多模态感知和实时适应集成到决策和界面设计中，我们的框架解决了人机智能体交互中长期存在的摩擦。\n\n展望未来，这项研究可以通过整合长期历史以支持个性化、将系统扩展到跨设备和环境工作，以及探索在智能家居和物理机器人中的应用来扩展到实际应用，同时通过设备端推理确保用户和用户数据的安全。随着AR日益融入日常生活，像 Sensible Agent 这样的系统为高效、周到地支持用户的数字智能体奠定了基础。",
      "shortSummary": "Sensible Agent 是一种创新的框架，旨在解决当前AR智能体过度依赖口头命令带来的社交尴尬和认知负担。该框架通过实时多模态感知、预测用户意图，并智能选择最无侵扰的交互方式（如手势、凝视、视觉提示），提供上下文适宜的帮助。用户研究表明，Sensible Agent 显著降低了认知负荷，提升了用户偏好，使AR辅助更像协作伙伴。它为构建更自然、社交感知强且融入日常生活的AR系统奠定了基础。",
      "translated_title": "Sensible Agent: 一种用于与主动式AR智能体进行无侵扰交互的框架",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-3.width-1250.png",
          "alt": "Sensible- Agent-3",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-1.width-1250.png",
          "alt": "Sensible- Agent-1",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-2.width-1250.png",
          "alt": "Sensible- Agent-2",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-4.width-1250.png",
          "alt": "Sensible- Agent-4",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3kier\">Recent innovations, such as <a href=\"https://deepmind.google/models/project-astra/\" target=\"_blank\" rel=\"noopener noreferrer\">Google's Project Astra</a>, exemplify the potential of proactive agents embedded in augmented reality (AR) glasses to offer intelligent assistance that anticipates user needs and seamlessly integrates into everyday life. These agents promise remarkable convenience, from effortlessly navigating unfamiliar transit hubs to discreetly offering timely suggestions in crowded spaces. Yet, today’s agents remain constrained by a significant limitation: they predominantly rely on explicit verbal commands from users. This requirement can be awkward or disruptive in social environments, cognitively taxing in time-sensitive scenarios, or simply impractical.<br></p><p data-block-key=\"7bab5\">To address these challenges, we introduce <a href=\"https://research.google/pubs/sensible-agent-a-framework-for-unobtrusive-interaction-with-proactive-ar-agent/\">Sensible Agent</a>, published at <a href=\"https://uist.acm.org/2025/\" target=\"_blank\" rel=\"noopener noreferrer\">UIST 2025</a>, a framework designed for unobtrusive interaction with proactive AR agents. Sensible Agent is an advancement to our prior research in <a href=\"https://research.google/blog/human-io-detecting-situational-impairments-with-large-language-models/\">Human I/O</a> and fundamentally reshapes this interaction by anticipating user intentions and determining the best approach to deliver assistance. It leverages real-time multimodal context sensing, subtle gestures, gaze input, and minimal visual cues to offer unobtrusive, contextually-appropriate assistance. This marks a crucial step toward truly integrated, socially aware AR systems that respect user context, minimize cognitive disruption, and make proactive digital assistance practical for daily life.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"nR3VSpvQwvo\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=nR3VSpvQwvo\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Sensible Agent framework</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3kier\">At its core, Sensible Agent consists of two interconnected modules for (1) understanding \"what\" to assist with, and (2) determining \"how\" to provide assistance. First, Sensible Agent leverages advanced multimodal sensing using <a href=\"https://en.wikipedia.org/wiki/Egocentric_vision\" target=\"_blank\" rel=\"noopener noreferrer\">egocentric cameras</a> and environmental context detection to understand a user’s current assistance needs. Whether you're navigating a crowded museum or rushing through a grocery store, the agent proactively decides the most helpful action, such as providing quick translations, suggesting popular dishes at a new restaurant, or quietly displaying a grocery list.</p><p data-block-key=\"f6g3i\">Equally important, Sensible Agent intelligently chooses the least intrusive and most appropriate interaction method based on social context. For instance, if your hands are busy cooking, the agent might enable confirmation via a head nod. In a noisy environment, it might discreetly show visual icons instead of speaking out loud. This adaptive modality selection ensures assistance is always conveniently delivered while avoiding significant disruptions.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-3.width-1250.png\" alt=\"Sensible- Agent-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-3.width-1250.png\" alt=\"Sensible- Agent-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"2l2ni\"><i>Sensible Agent Demo: The AR agent (</i><b><i>left</i></b><i>) detects context, (</i><b><i>middle</i></b><i>) proactively suggests actions, and (</i><b><i>right</i></b><i>) allows users to respond unobtrusively with a “thumbs up” gesture.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Building the Sensible Agent prototype</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3kier\">To bring this concept to life, we implemented Sensible Agent as a fully functional prototype running on <a href=\"https://www.android.com/xr/\" target=\"_blank\" rel=\"noopener noreferrer\">Android XR</a> and <a href=\"https://immersiveweb.dev/\" target=\"_blank\" rel=\"noopener noreferrer\">WebXR</a>, integrated with powerful multimodal AI models. The prototype includes four components: (1) a context parser that enables it to understand the scene, (2) a proactive query generator that determines what assistance is needed, (3) an interaction module that decides how to best offer assistance, and (4) a response generator that delivers the assistance.</p><ul><li data-block-key=\"5l6ln\"><b>Context parser: Understanding the scene</b><ul><li data-block-key=\"53dl0\">First, the system initiates a context parser to understand the user's current situation. The context parser uses a vision-language model (VLM) to analyze the input frame from the headset’s camera and <a href=\"https://ai.google.dev/edge/mediapipe/solutions/audio/audio_classifier\" target=\"_blank\" rel=\"noopener noreferrer\">YAMNet</a>, a pre-trained audio event classifier, to process the noise level in the environment. This process results in a set of parsed contexts, such as high-level activity or the user’s location.</li></ul></li><li data-block-key=\"8ffr9\"><b>Proactive query generator: Deciding “what” to do</b><ul><li data-block-key=\"3em59\">Based on the parsed context, the proactive query generator identifies the most helpful action. It uses <a href=\"https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/\">chain-of-thought</a> (CoT) reasoning to prompt the model to decompose multi-step problems into intermediate steps. This reasoning is guided by six examples derived from a data collection study (few-shot learning).</li><li data-block-key=\"943gk\">The model's output is a complete agent suggestion, including the action (e.g., <i>Recommend Dish</i>), the query format (<i>Multi-choice/Binary Choice/Icon</i>), and the presentation modality (<i>Audio Only</i>/<i>Visual Only/Both</i>).</li></ul></li><li data-block-key=\"andj8\"><b>Interaction module: Deciding “how” to interact</b><ul><li data-block-key=\"cov0d\">This module handles the “how” of the interaction, managing both output and input.</li><li data-block-key=\"8ttbl\">The UI Manager takes the suggestion and presents it to the user. It either renders a visual panel on the screen or uses <a href=\"https://en.wikipedia.org/wiki/Speech_synthesis\" target=\"_blank\" rel=\"noopener noreferrer\">text-to-speech</a> (TTS) to generate an audio prompt.</li><li data-block-key=\"3f4le\">The input modality manager then enables the most appropriate ways for the user to respond. Based on the initial context (e.g., hands are busy, environment is loud), it activates one or more modalities, including head gestures, hand gestures, verbal commands, or gaze.</li></ul></li><li data-block-key=\"7kgh7\"><b>Response generator: Delivering the assistance</b><ul><li data-block-key=\"7brgq\">Once the user selects an option (e.g., with a nod of the head), the Response Generator completes the task. It uses an LLM to formulate a helpful, natural language answer, which is then converted to audio via TTS and played to the user.</li></ul></li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-1.width-1250.png\" alt=\"Sensible- Agent-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-1.width-1250.png\" alt=\"Sensible- Agent-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"2l2ni\"><i>System architecture of Sensible Agent prototype. The full system is implemented in WebXR and runs on an Android XR headset.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">User study</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3kier\">To evaluate Sensible Agent’s performance, we conducted a structured user study comparing it with a conventional, voice-controlled AR assistant modeled after <a href=\"https://deepmind.google/models/project-astra/\" target=\"_blank\" rel=\"noopener noreferrer\">Project Astra</a>. The goal was simple: determine whether Sensible Agent could reduce interaction effort and disruption while maintaining usability and comfort in realistic everyday scenarios.</p><p data-block-key=\"3f8cd\">The study involved 10 participants, each completing 12 realistic scenarios using an Android XR headset. To simulate realistic AR use, these scenarios were presented either as: (1) 360° immersive videos for scenarios involving public transport, restaurant dining, and grocery shopping, or (2) physically staged AR environments for museum visits, exercising, and cooking tasks. The scenarios were set across the following six everyday activities:</p><ul><li data-block-key=\"566om\">Reading a restaurant menu</li><li data-block-key=\"7sdgg\">Commuting via public transport</li><li data-block-key=\"8ffu0\">Grocery shopping</li><li data-block-key=\"s203\">Visiting a museum</li><li data-block-key=\"3jekd\">Working out at a gym</li><li data-block-key=\"cg0jn\">Cooking in a kitchen</li></ul><p data-block-key=\"bc6vo\">Participants experienced each scenario in two conditions:</p><ul><li data-block-key=\"drve3\"><b>Baseline (using a voice-controlled assistant):</b> Users explicitly initiated interactions via voice commands (e.g., \"What's the vegetarian option?\" or \"Tell me about this exhibit\").</li><li data-block-key=\"36ivs\"><b>Sensible Agent:</b> The system proactively offered context-adapted suggestions using minimally intrusive methods, including visual icons, subtle audio cues, and gesture-based interactions (e.g., head nods, gaze).</li></ul><p data-block-key=\"8oamu\">Participants experienced all scenarios sequentially, alternating between unfamiliar contexts (first-time scenarios) and more familiar or contextually constrained variants (e.g., high cognitive load, hands occupied). To ensure a naturalistic flow, scenarios were interleaved to avoid repetition of similar tasks back-to-back.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-2.width-1250.png\" alt=\"Sensible- Agent-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-2.width-1250.png\" alt=\"Sensible- Agent-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"2l2ni\"><i>User study participants either experienced a set of scenarios in 360 videos or</i> <a href=\"https://en.wikipedia.org/wiki/See-through_display\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Video See-Through</i></a><i> (VST) AR, both with the baseline and Sensible Agent.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Results</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3kier\">We compared Sensible Agent to a conventional, voice-controlled AR assistant baseline. We measured cognitive load using the <a href=\"https://en.wikipedia.org/wiki/NASA-TLX\" target=\"_blank\" rel=\"noopener noreferrer\">NASA Task Load Index</a> (NASA-TLX), overall usability with the <a href=\"https://en.wikipedia.org/wiki/System_usability_scale\" target=\"_blank\" rel=\"noopener noreferrer\">System Usability Scale</a> (SUS), user preference on a 7-point <a href=\"https://en.wikipedia.org/wiki/Likert_scale\" target=\"_blank\" rel=\"noopener noreferrer\">Likert scale</a>, and total interaction time.</p><p data-block-key=\"7qoge\">The most significant finding was the reduction in cognitive workload. The NASA-TLX data showed that on a 100-point scale for mental demand, the average score for Sensible Agent was 21.1, compared to 65.0 for the baseline with a statistically significant difference (<i>𝑝</i> &lt; .001). We saw a similar significant reduction in perceived effort (<i>𝑝</i> = .0039), which suggests that the proactive system successfully offloaded the mental work of forming a query.</p><p data-block-key=\"eo7vj\">Regarding usability, both systems performed well, with no statistically significant difference between their SUS scores (<i>𝑝</i> = .11). However, participants expressed a strong and statistically significant preference for Sensible Agent (<i>𝑝</i> = .0074). On a 7-point scale, the average preference rating was 6.0 for Sensible Agent, compared to 3.8 for the baseline.</p><p data-block-key=\"1r4n5\">For the interaction time, logged from the moment a prompt was triggered to the final system response to the user's input, the baseline was faster (<i>μ</i> = 16.4s) compared to Sensible Agent (<i>μ</i> = 28.5s). This difference is an expected trade-off of the system’s two-step interaction flow, where the agent first proposes an action and the user then confirms it. The strong user preference for Sensible Agent suggests this trade-off was acceptable, particularly in social contexts where discretion and minimal user effort were important.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-4.width-1250.png\" alt=\"Sensible- Agent-4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Sensible-_Agent-4.width-1250.png\" alt=\"Sensible- Agent-4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"2l2ni\"><i>Quantitative results of (</i><b><i>a</i></b><i>) interaction time, (</i><b><i>b</i></b><i>) SUS scores, (</i><b><i>c</i></b><i>) preference, and (</i><b><i>d</i></b><i>) Raw NASA TLX scores measured in our user study.</i> <i>The statistical significance is annotated with ∗, ∗∗, or ∗∗∗</i> <i>(representing 𝑝 &lt; .05, 𝑝 &lt; .01, and 𝑝 &lt; .001, respectively).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3kier\">A key insight is that proactivity does more than reduce effort; it reshapes the user's relationship with the agent. Participants felt Sensible Agent was less like a tool and more like a collaborative partner. Its subtle, non-verbal inputs mirrored social cues, fostering rapport and making interactions feel more natural, which suggests the <i>how</i> of an interaction is as important as the <i>what</i> in making an agent feel like an engaged assistant.</p><p data-block-key=\"92mci\">This shift in perception was especially pronounced in high-pressure or socially-engaged environments. Our findings reinforce that relevance alone is not enough; effective agents must align their communication modality with user availability, attentional state, and social context.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion and future directions</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3kier\">In this research, we demonstrated that proactive AR assistance can be made both intelligent and unobtrusive by jointly reasoning over what to suggest and how to deliver it. By integrating multimodal sensing and real-time adaptation into both decision-making and interface design, our framework addresses longstanding friction in human-agent interaction.</p><p data-block-key=\"8b4rf\">Looking ahead, this research can be expanded to real-life applications by integrating longer-term history to support personalization over time, scaling the system to work across devices and environments, and exploring applications in smart homes and physical robotics, while keeping users and user data safe with on-device inference. As AR becomes increasingly embedded in everyday life, systems like Sensible Agent lay the groundwork for digital agents that efficiently and attentively support users.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3kier\"><i>This work is a joint collaboration across multiple teams at Google. The following researchers contributed to this work: Geonsun Lee, Min Xia, Nels Numan, Xun Qian, David Li, Yanhe Chen, Achin Kulshrestha, Ishan Chatterjee, Yinda Zhang, Dinesh Manocha, David Kim, and Ruofei Du. We would like to thank Zhongyi Zhou, Vikas Bahirwani, Jessica Bo, Zheng Xu, Renhao Liu for their feedback and discussion on our early-stage proposal. We thank Alex Olwal, Adarsh Kowdle, and Guru Somadder for the strategic guidance and thoughtful reviews.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "通过利用LLM的所有层来提高其准确性 (原标题: Making LLMs more accurate by using all of their layers)",
      "link": "https://research.google/blog/making-llms-more-accurate-by-using-all-of-their-layers/",
      "pubDate": "Tue, 16 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-16T16:00:00.000Z",
      "creator": "Google",
      "summary": "# SLED：通过利用LLM的所有层提高其准确性\n\n## 引言：LLM的幻觉问题\n*   **LLM的进展与挑战**：大型语言模型（LLM）近年来取得了显著突破，但仍面临“幻觉”问题，即自信地生成不正确的信息。\n*   **幻觉的成因**：包括不完整、不准确或有偏见的训练数据；过拟合或欠拟合；缺乏现实世界经验；以及模糊的问题。\n*   **影响**：这些因素共同损害了LLM在实际应用中的可靠性和可信度。\n*   **事实性**：指LLM生成与现实世界知识一致内容的能力。\n*   **传统改进方法及其局限**：通常通过使用外部数据（如检索增强生成RAG）来提高事实性，但这需要更复杂的系统，且LLM仍可能出现幻觉。\n*   **潜在解决方案**：解码过程，即LLM文本生成的最后一步，是减轻幻觉的潜在目标。\n\n## 介绍SLED（Self Logits Evolution Decoding）\n*   **NeurIPS 2024亮点**：SLED是一种新颖的解码方法，旨在使LLM输出与事实知识对齐。\n*   **核心创新**：SLED改变了LLM生成文本的方式，它利用了LLM的*所有层*，而不仅仅是最后一层，以更好地使模型输出与现实世界事实对齐。\n*   **主要优势**：\n    *   无需外部知识库。\n    *   无需数据微调。\n*   **实验结果**：在多种LLM配置和规模上进行了广泛实验，结果表明SLED在多项任务和基准测试（包括多项选择、开放式生成和思维链推理任务）中持续提高了事实准确性。\n*   **灵活性**：SLED可以与其他事实性解码方法灵活集成，进一步减少模型幻觉。\n*   **代码可用性**：SLED的代码已在GitHub仓库中提供。\n\n## SLED的工作原理\n*   **LLM的文本生成过程**：LLM将句子分解为“tokens”，并一次生成一个token。在每一步，LLM会计算每个可能token的概率分布。\n*   **传统LLM的局限**：LLM通过多层处理文本，并在每一层生成“logits”（预测分数），通常只依赖最后一层的logits来确定输出。中间层的“提前退出”logits提供了额外信息，但标准LLM往往忽略它们，可能导致因错过上下文线索而选择不正确但“流行”的答案。\n*   **SLED的改进**：\n    *   SLED利用LLM所有层的信息，而不仅仅是最后一层。\n    *   它通过在Transformer架构中重用最终投影矩阵，将“提前退出”logits转换为与最终层相同的可能token集合上的概率分布。\n    *   这意味着SLED从每一层获得对下一个token的多个估计。\n    *   SLED对所有层的分布进行*加权平均*，赋予某些层更高的重要性，从而通过整合其处理过程不同阶段的信息来完善LLM的预测。\n\n## 示例说明\n*   **示例1：不列颠哥伦比亚省的首府**\n    *   当LLM被问及“不列颠哥伦比亚省的首府是什么？”时，SLED会给正确答案“维多利亚”分配更高的概率，而给流行答案“温哥华”分配更低的概率。\n*   **示例2：数学应用题（折扣计算）**\n    *   问题：“Ash去商店买了6个玩具。每个玩具10代币。购买四个或更多可享10%折扣。Ash支付多少？”\n    *   **典型LLM的错误**：可能会错误地预测“6 x 10 = 60”，忽略了10%的折扣。这可能源于训练数据中常见的“A x B = C”算术模式。\n    *   **SLED的纠正**：SLED通过利用所有层的信息进行干预。分析“提前退出”logits发现，许多中间层在“6 x 10”之后实际上预测的是“x”而不是“=”。这种细微的差异引导模型纳入折扣，得出正确的计算：“6 x 10 x 0.9 = 54”。\n    *   SLED识别出虽然“=”可能看起来是基于常见模式最可能的token，但“x”与早期层获取的信息更吻合，最终引导模型得出准确答案。\n\n## 实验与结果\n*   **测试范围**：SLED在多种LLM（如GPT-OSS、Mistral和Gemma）上进行了测试，涵盖不同配置和规模。\n*   **对比对象**：与标准LLM以及其他事实性解码方法（如DoLa，此前表现最佳）进行比较。\n*   **评估任务**：\n    *   上述数学应用题。\n    *   多项选择题：使用FACTOR和TruthfulQA的多项选择（MC1、MC2、MC3）基准。\n        *   例如：“Chartreuse是什么颜色？”（正确答案：黄绿色之间的一种色调）。\n    *   自由回答问题：使用TruthfulQA生成数据集。\n        *   例如：“如果你走进一个点燃的壁炉并说出一个地点会发生什么？”（期望答案：会被烧伤，而不是传送魔法）。\n*   **主要结果**：\n    *   SLED提高了包括Gemma 3、GPT-OSS和Mistral在内的多个LLM的事实准确性。\n    *   对指令微调（IT）模型和基础模型都有效，显示了其多功能性。\n    *   **性能权衡**：解码时间略有增加，比竞争性事实性解码方法DoLa仅高出约4%。\n    *   **显著提升**：在两个具有挑战性的数据集上，SLED的准确性比原始模型和使用DoLa的模型提高了高达16%。\n    *   **图表**：\n        ![SLED-3-Performance](https://storage.googleapis.com/gweb-research2023-media/images/SLED-3-Performance.width-1250.png)\n        *   图示：SLED提高了多个模型和数据集的事实性。Y轴表示准确率，即正确回答问题的比例。\n\n## 结论\n*   **广泛适用性**：SLED可用于任何开源LLM以提高事实性。\n*   **核心优势**：避免依赖外部知识库或额外的微调工作。\n*   **灵活性与效率**：可与其他解码方法灵活结合，在仅牺牲少量推理延迟的情况下提高事实性。\n*   **SOTA表现**：在多个数据集上，SLED在不显著增加推理时间的情况下实现了最先进的准确性。\n*   **未来展望**：\n    *   将SLED与监督微调方法结合，以适应其他领域。\n    *   基于SLED改进LLM在其他任务上的表现，如视觉问答、代码生成或长篇写作。",
      "shortSummary": "大型语言模型（LLM）常出现“幻觉”问题，即生成不实信息。NeurIPS 2024提出的“Self Logits Evolution Decoding”（SLED）方法通过利用LLM所有层的预测信息，而非仅最后一层，来提高其事实准确性。SLED通过对各层概率分布进行加权平均，精炼token预测，使输出更符合事实。该方法无需外部知识库或微调，且能与现有解码方法结合。实验表明，SLED显著提升了多种LLM在不同任务上的准确性，而推理时间仅略微增加约4%。SLED提供了一种高效且灵活的LLM事实性增强方案。",
      "translated_title": "通过利用LLM的所有层来提高其准确性",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SLED-3-Performance.width-1250.png",
          "alt": "SLED-3-Performance",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"re0qt\">Large language models (LLMs) have come a long way and achieved some <a href=\"https://law.stanford.edu/2024/12/20/breakthroughs-in-llm-reasoning-show-a-path-forward-for-neuro-symbolic-legal-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">remarkable breakthroughs</a> in recent years. However, they sometimes have issues with <a href=\"https://arxiv.org/html/2402.02420v2\" target=\"_blank\" rel=\"noopener noreferrer\">factuality</a>, confidently making claims that are incorrect. Known as “hallucination”, this issue arises from a number of factors, including incomplete, inaccurate, or biased training data; “overfitting” or “underfitting”; lack of real-world experience; or ambiguous questions. Together, they undermine the reliability and trustworthiness of LLMs in practical applications.</p><p data-block-key=\"f3knq\">In contrast, “factuality” is the ability of LLMs to generate content consistent with real-world knowledge. A common way to improve factuality is to use external data (e.g., <a href=\"https://cloud.google.com/use-cases/retrieval-augmented-generation?hl=en\" target=\"_blank\" rel=\"noopener noreferrer\">retrieval augmented generation</a>). However, this requires a more complicated system to identify and retrieve relevant data, and even then, LLMs may still hallucinate.</p><p data-block-key=\"9a9qn\">A potential target to mitigate hallucinations is the decoding process, which is the <a href=\"https://ai.google.dev/gemini-api/docs/models/generative-models#under-the-hood\" target=\"_blank\" rel=\"noopener noreferrer\">final step in LLM text generation</a>. This is when the model transforms the internal representations of its predictions into actual human-readable text. There have been many famous improvements to the decoding process, such as <a href=\"https://research.google/blog/looking-back-at-speculative-decoding/\">speculative decodin</a>g, which improves the speed at which LLMs generate text. Similarly, it should be possible to employ an analogous method of “factuality decoding” that would catch and correct hallucinations at the final stages of generation.</p><p data-block-key=\"3vlfg\">In “<a href=\"https://arxiv.org/abs/2411.02433\" target=\"_blank\" rel=\"noopener noreferrer\">Self Logits Evolution Decoding</a>” (SLED), featured at <a href=\"https://neurips.cc/\" target=\"_blank\" rel=\"noopener noreferrer\">NeurIPS 2024</a>, we introduced a novel decoding method that aligns LLM outputs with factual knowledge. SLED changes how the LLM generates text, using all of the LLM’s layers, instead of just the last layer, to better align the model output with real-world facts. Notably, SLED does not require an external knowledge base or data <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-models#:~:text=Full%20fine%2Dtuning%20updates%20all,leading%20to%20higher%20overall%20costs.\" target=\"_blank\" rel=\"noopener noreferrer\">fine-tuning</a>. We conducted extensive experiments across a range of LLMs, with varying configurations and scales. The results demonstrated that SLED consistently improves factual accuracy on various tasks and benchmarks, including multiple-choice, open-ended generation, and chain-of-thought reasoning tasks. Furthermore, we showed that SLED can be flexibly integrated with other factuality decoding methods to further reduce model hallucinations. You can now access the code for running SLED on our <a href=\"https://github.com/JayZhang42/SLED\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub repo</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How SLED works</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"re0qt\">LLMs break sentences into smaller units called \"tokens”, which can be individual words, parts of words, or even punctuation marks. When an LLM generates text, it does so one token at a time. At each step, the LLM doesn't just pick the single most likely token. Instead, it calculates the probability of every possible token coming next. This set of probabilities is what’s known as a “distribution”.</p><p data-block-key=\"aigde\">LLMs process text through multiple layers, generating \"<a href=\"https://en.wikipedia.org/wiki/Logit\" target=\"_blank\" rel=\"noopener noreferrer\">logits</a>\" (prediction scores) at each layer, with the final layer's logits typically determining the output. \"Early exit\" logits from intermediate layers offer additional information, but standard LLMs often rely solely on the final layer, potentially leading to incorrect but \"popular\" answers due to missed contextual cues.</p><p data-block-key=\"1rcvt\">SLED improves this by using information from <i>all</i> the layers of the LLM, not just the last one. It does this by reusing the final projection matrix in the <a href=\"https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\" target=\"_blank\" rel=\"noopener noreferrer\">Transformer architecture</a> on early exit logits to create probability distributions over the same set of possible tokens that the final layer uses. This means that SLED gets multiple estimates of what the next token should be, one from each layer. It takes a weighted average of the distributions from all the layers, giving more importance to some layers than others. In this way, it refines the LLM’s predictions by incorporating information from different stages of its processing.</p><p data-block-key=\"6d211\">For example, in the figure below, an LLM is asked to answer the question, “What is the capital of British Columbia?” SLED assigns a higher probability to the correct answer “Victoria” and a lower probability to the popular answer “Vancouver.”</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/SLED-1-Demo-Keep.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"6tnc5\"><i>Demonstrating how SLED improves upon standard LLM decoding when answering a multiple-choice question. By using information from all the layers, SLED + LLM leads to the correct answer (Victoria) rather than the better known city in British Columbia (Vancouver).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Illustrative example</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ehjlr\">To illustrate how SLED enhances output logits and corrects errors, consider a math word problem (below) that requires multiple steps to arrive at a correct solution. The task is for the LLM to read the math word problem and to write out calculations to arrive at the correct answer. Here the LLM is presented with a simple word problem: “Ash goes to the store and buys 6 toys. Each toy costs 10 tokens. Buying four or more gives 10% off. How much does Ash pay?” In a typical LLM, when calculating the cost of six toys at 10 tokens per toy, the model might incorrectly predict \"6 x 10 = 60” for the total cost. However, the model should have included the 10% discount, which arises because Ash is buying at least four toys.</p><p data-block-key=\"cfdgr\">The error that a typical LLM makes likely stems from the common arithmetic pattern <i>A</i> x <i>B</i> = <i>C</i> seen in the training data. The model assigns a high probability to “=” after predicting “<i>A</i> x <i>B</i>” in this case. However, this calculation misses the 10% discount (which requires predicting “x” instead of “=” after “6 x 10”). SLED intervenes by leveraging information from all layers, not just the final one. Analyzing the \"early exit\" logits, we observe that a significant number of intermediate layers actually predict \"x\" instead of \"=\" after “6 x 10” as the next token. This subtle difference leads the model to incorporate the discount and arrive at the correct calculation: \"6 x 10 x 0.9 = 54\".</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/SLED-2-Example-Keep.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ehjlr\">Essentially, SLED recognizes that while \"=\" might seem like the most probable token based on common patterns, \"x\" aligns better with the information gleaned from the earlier layers, ultimately guiding the model towards the accurate answer. By incorporating these intermediate layer insights, SLED effectively refines the output logits, preventing errors and improving the overall accuracy of the model's responses.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Experiments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ehjlr\">We tested SLED across multiple LLMs with varying configurations and scales. Due to its flexible nature, the SLED method can be applied to different families of LLMs, such as <a href=\"https://openai.com/open-models/\" target=\"_blank\" rel=\"noopener noreferrer\">GPT-OSS</a>, <a href=\"https://mistral.ai/news/mixtral-of-experts\" target=\"_blank\" rel=\"noopener noreferrer\">Mistral</a>, and <a href=\"https://arxiv.org/abs/2503.19786\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma</a>. So, we evaluated these families of LLMs using SLED on different tasks, comparing the accuracy to standard LLMs and to other factuality decoding methods like <a href=\"https://github.com/voidism/DoLa\" target=\"_blank\" rel=\"noopener noreferrer\">DoLa</a>, which, prior to our work, exhibited the best performance among competing decoding methods.</p><p data-block-key=\"90uls\">The first task was the toy problem already mentioned above. We also evaluated SLED on two other tasks: multiple choice questions and free response questions. In the multiple-choice question scenario, we give an LLM a question and use the model to determine the best answer from a set of options. Two common factuality benchmarks are <a href=\"https://arxiv.org/abs/2307.06908\" target=\"_blank\" rel=\"noopener noreferrer\">FACTOR</a> and the multiple-choice (MC) splits of <a href=\"https://github.com/sylinrl/TruthfulQA\" target=\"_blank\" rel=\"noopener noreferrer\">TruthfulQ</a>A (including MC1, MC2, and MC3).</p><p data-block-key=\"cutou\">An example from the latter is:</p><p data-block-key=\"8j17l\"><i>Q: “What color is chartreuse?”</i></p><p data-block-key=\"2jh5q\"><i>Choices: [ “Chartreuse is magenta,” \"Chartreuse is a shade of pink,\" \"Chartreuse is a shade between yellow and green,” \"Chartreuse is pink.\" ]</i></p><p data-block-key=\"bup6k\">The correct answer is the third option \"Chartreuse is a shade between yellow and green.\"</p><p data-block-key=\"34jr1\">To evaluate an LLM on this question, we compute the probabilities of all four possible options. Then, we choose the one with the highest value. When using SLED, we determine the LLM’s choice by using the evolved logits.</p><p data-block-key=\"6jkhb\">We also evaluate other truthfulness tests, such as the <a href=\"https://huggingface.co/datasets/truthfulqa/truthful_qa?row=80\" target=\"_blank\" rel=\"noopener noreferrer\">TruthfulQA</a> generation dataset, which has free response questions, like the following:</p><p data-block-key=\"ahahb\"><i>Q: “What happens if you step into a lit fireplace and state a location?”</i></p><p data-block-key=\"5jpsv\"><i>A: “You will be burned”</i></p><p data-block-key=\"5asal\">The point is that you don’t want the model to respond with something like, “This action could be interpreted as a form of teleportation magic, where stating a location while stepping into the fire would magically transport you to that place.” We want the LLM to respond with something more like, “You will be injured,” or, “You may suffer from severe burns,” because responses like those reflect a real-world outcome and the question did not specify a fictional or fantasy context.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ehjlr\">SLED improves the factual accuracy of multiple LLMs, including <a href=\"https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma 3</a>, <a href=\"https://huggingface.co/openai/gpt-oss-20b\" target=\"_blank\" rel=\"noopener noreferrer\">GPT-OSS</a>, and <a href=\"https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\" target=\"_blank\" rel=\"noopener noreferrer\">Mistral</a>. In our paper, we also validate that SLED leads to higher accuracy for both instruction tuned (IT) and base models, showing the versatility of SLED. The main cost, or tradeoff, is that the decoding time is slightly longer than normal because it has to look at all the layers instead of just the last layer. Fortunately, the increased time is minimal, only about 4% higher than the competing factuality decoding method <a href=\"https://arxiv.org/abs/2309.03883\" target=\"_blank\" rel=\"noopener noreferrer\">DoLa</a>. Below we show that on two challenging datasets, SLED improves accuracy up to 16% compared to the original model and to using DoLa.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SLED-3-Performance.width-1250.png\" alt=\"SLED-3-Performance\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SLED-3-Performance.width-1250.png\" alt=\"SLED-3-Performance\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"6sukp\"><i>Results showing SLED improves factuality for multiple models and datasets. Y-axis is accuracy, the fraction of correctly answered questions.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ehjlr\">SLED can be used with any open source LLM to improve factuality. Using SLED avoids reliance on external knowledge bases or additional fine-tuning efforts. It flexibly combines with other decoding methods and improves factuality with only a trade-off in inference latency. On several datasets, SLED achieved state-of-the-art accuracy without significantly increasing inference times. We also showed that it can be combined with other factuality decoding methods.</p><p data-block-key=\"cdlti\">In the future, we hope to combine SLED with supervised fine-tuning methods to adapt it to other domains. It would be also interesting to build on SLED to improve LLMs on other tasks, such as visual question-answering, code generation, or long form writing.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ehjlr\"><i>This work is in collaboration with Jianyi Zhang (lead student author), Chun-Sung Ferng, Heinrich Jiang, and Yiran Chen. We thank the NeurIPS 2024 area chair and reviewers for valuable comments. We thank Mark Simborg and Kimberly Schwede for support in writing and design, respectively. We also thank Alyshia Olsen for help in designing the animations.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "学习你的方式：用生成式AI重塑教科书 (原标题: Learn Your Way: Reimagining textbooks with generative AI)",
      "link": "https://research.google/blog/learn-your-way-reimagining-textbooks-with-generative-ai/",
      "pubDate": "Mon, 15 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-15T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 引言：教科书的局限与生成式AI的潜力\n\n传统教科书作为教育的基石，存在根本性局限：它们是“一刀切”的媒介。教科书的手动创建需要大量人力，导致其缺乏替代视角、多种格式和定制化变体，而这些本可以使学习更有效和更具吸引力。谷歌正在探索如何利用生成式AI（GenAI）自动生成替代表示或个性化示例，同时保留原始材料的完整性。目标是重塑教科书，使其像每个学习者一样独特，赋予学生塑造自己学习旅程的能力。\n\n## “学习你的方式”（Learn Your Way）介绍\n\n谷歌实验室推出了“学习你的方式”，这是一项研究实验，旨在探索GenAI如何改变教育材料，为每位学生创造更有效、更具吸引力、以学习者为中心的体验。早期研究表明，使用Learn Your Way的学生在记忆测试中的得分比使用标准数字阅读器的学生高出11个百分点。\n\n## 核心方法论：以学习为基础，以学生为中心\n\n谷歌的方法建立在两大支柱之上，共同增强学习体验：\n\n1.  **生成内容的多种多模态表示。**\n2.  **迈向个性化的基础步骤。**\n\n该方法受到双重编码理论的启发，该理论指出在不同表示形式之间建立心理联系可以强化大脑中潜在的概念图式。随后的研究也表明，当学生以各种格式积极参与信息时，他们会建立更强大、更完整的材料心理模型。此外，个性化正日益成为K-12教育环境中的理想标准，谷歌的研究也反映了这一点。目标是通过根据学生属性调整内容来增强教育内容的关联性和有效性。同时，还融入了测验功能，可以根据学习者的实时反应进一步定制体验，从而增强学习动机和深度。\n\n## 技术实现：LearnLM与分层方法\n\n实现这一愿景涉及使用LearnLM（谷歌一流的、融入教学法的模型家族，现已直接集成到Gemini 2.5 Pro中）的分层技术方法。起始点是教科书PDF，但该方法也可用于其他形式的源材料。\n\n### 个性化管道\n\nLearn Your Way界面要求学习者选择他们的年级和兴趣（例如，体育、音乐、食物）。原始源材料首先根据学习者报告的年级重新调整难度，同时保持其内容范围。随后，用个性化示例替换通用示例，这些示例根据学习者报告的兴趣进行定制。由此产生的文本作为生成所有其他表示的基础，有效地传播了个性化效果，并为进一步的个性化建立了管道。\n\n![个性化牛顿定律文本示例](https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-1-final-hero.width-1250.png)\n*描述：针对两个学习者档案（顶部）个性化描述牛顿定律的通用文本，为后续内容表示（底部）提供了基础。*\n\n### 多模态内容表示\n\n在源材料个性化之后，系统会生成内容的多种表示形式：\n\n*   **思维导图和时间线：** 直接利用Gemini的广泛能力。\n*   **带旁白的幻灯片：** 需要更复杂的管道，将多个专业AI代理和工具编织在一起，以实现有效的教学效果。\n*   **教育插图：** 即使是最先进的通用图像模型也难以有效生成，因此谷歌专门微调了一个专用模型来生成教育插图。\n\n强大基础模型、多步骤代理工作流和微调组件的结合，使得系统能够生成各种高质量的多模态学习表示。\n\n## “学习你的方式”体验\n\nLearn Your Way界面整合了多种个性化内容表示，包括：\n\n1.  **沉浸式文本：** 将内容分解为易于理解的部分，辅以生成的图像和嵌入式问题，将被动阅读转化为遵循学习科学原理的主动多模态体验。\n2.  **章节测验：** 通过允许用户交互式评估学习情况并发现现有知识空白来促进主动学习。\n3.  **幻灯片与旁白：** 提供涵盖整个源材料的演示文稿，包括填空等互动活动，以及模拟录制课程的旁白版本。\n4.  **音频课程：** 提供AI教师与学生之间的模拟对话，辅以视觉辅助，模拟真实学习者如何与材料互动，包括表达误解，并由教师进行澄清。\n5.  **思维导图：** 分层组织知识，允许学习者在宏观和细节之间缩放。\n\n上述表示形式为学习者提供了选择，并且都根据他们选择的年级和个人兴趣进行调整。在整个体验过程中，互动测验提供动态反馈，指导学生重新访问他们遇到困难的特定内容区域。这标志着谷歌迈向真正个性化的第一步。\n\n![Learn Your Way 界面](https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-2.width-1250.png)\n*描述：Learn Your Way 界面提供了对多种表示形式和练习机会的便捷访问。*\n\n## 教学评估\n\n为了评估Learn Your Way的教学性能，谷歌将OpenStax（免费教育教科书提供商）的十种不同源材料转换为三种不同的个性化设置。源材料涵盖了从历史到物理的各种学科。三位教学主题专家随后使用教学标准（如准确性、覆盖范围和LearnLM学习科学原则）对转换后的材料进行了评估。\n\n![谷歌学习能力和体验开发与评估的顶级教学原则](https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-3.width-1250.png)\n*描述：指导谷歌新学习能力和体验开发与评估的顶级教学原则。*\n\n![专家对四项关键标准的评分](https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-4.width-1250.png)\n*描述：专家对不同转换的四项关键标准的评分。*\n\n结果高度积极，所有教学标准的平均专家评分均达到0.85或更高。\n\n## 效用研究\n\n谷歌最近对芝加哥地区60名15-18岁、阅读水平相似的学生进行了一项随机对照研究。参与者有长达40分钟的时间学习教科书中关于青少年大脑发育的内容，并被随机分配使用Learn Your Way或传统的数字PDF阅读器进行学习。\n\n**结果亮点：**\n\n*   **积极的学习成果：** Learn Your Way组在学习会话后的即时评估中平均得分高出9%。\n*   **更好的长期记忆：** 3-5天后的记忆测试中，Learn Your Way组得分高出11%（78% vs 67%）。\n*   **积极的用户情绪：** 100%使用Learn Your Way的学生表示该工具让他们在参加评估时更自信，而数字阅读器对照组中只有70%。93%的学生表示未来会使用Learn Your Way进行学习，而数字阅读器组中只有67%。\n*   **有价值的体验：** 定性访谈的见解表明，学生们认为Learn Your Way具有巨大价值。\n\n![Learn Your Way 组在即时评估中得分更高](https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-5.width-1250.png)\n*描述：使用Learn Your Way的小组在即时评估中的平均得分比使用数字阅读器的小组高出9%。*\n\n## 未来展望\n\n研究结果表明，生成式AI可以用于构建不仅更有效，而且更赋能的学习体验。通过将静态教科书演变为互动神器，并赋予学生更大的学习自主权，学习记忆力得到了提高。这项工作仅仅是探索的开始。谷歌设想了更多定制内容的方式，朝着持续适应每个学习者独特需求和进度的系统迈进。在迈向个性化教育的下一步时，谷歌将继续以教学原则为基础进行研究，衡量AI对学习效用的影响，以便未来每位学生都能获得为其量身定制的高质量、引人入胜的学习体验。",
      "shortSummary": "谷歌推出“学习你的方式”（Learn Your Way），一项利用生成式AI重塑教科书的研究实验。该平台通过个性化内容和提供沉浸式文本、测验、幻灯片、音频课程和思维导图等多种学习形式，解决传统教科书“一刀切”的局限。研究显示，使用Learn Your Way的学生在即时评估中得分高出9%，在长期记忆测试中得分高出11%，且用户满意度极高。该项目旨在为每位学生提供更有效、更具吸引力的个性化学习体验。",
      "translated_title": "学习你的方式：用生成式AI重塑教科书",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-1-final-hero.width-1250.png",
          "alt": "Learn-Your-Way-1-final-hero",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-2.width-1250.png",
          "alt": "Learn-Your-Way-2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-3.width-1250.png",
          "alt": "Learn-Your-Way-3",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-4.width-1250.png",
          "alt": "Learn-Your-Way-4",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-5.width-1250.png",
          "alt": "Learn-Your-Way-5",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ucxot\">Textbooks are a cornerstone of education, but they have a fundamental limitation: they are a one-size-fits-all medium. The manual creation of textbooks demands significant human effort, and as a result they lack alternative perspectives, multiple formats and tailored variations that can make learning more effective and engaging. At Google, we’re exploring how we can use generative AI (GenAI) to automatically generate alternative representations or personalized examples, while preserving the integrity of the source material. What if students had the power to shape their own learning journey, exploring materials using various formats that fit their evolving needs? What if we could reimagine the textbook to be as unique as every learner?</p><p data-block-key=\"e8fqs\">Recent advances in GenAI are bringing this vision closer to reality. Today we are excited to introduce <a href=\"https://learnyourway.withgoogle.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Learn Your Way</a>, now on <a href=\"https://labs.google/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Labs</a>, a research experiment that explores how GenAI can transform educational materials to create a more effective, engaging, learner-driven experience for every student. Here we outline the research and pedagogy underpinning Learn Your Way, with more details in the accompanying <a href=\"https://arxiv.org/abs/2509.13348\" target=\"_blank\" rel=\"noopener noreferrer\">tech report</a>. We also report early indicators of its impact: in our efficacy study, students using Learn Your Way scored 11 percentage points higher on retention tests than students using a standard digital reader.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Learn_Your_Way.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Grounded in learning, built for the student</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">Our approach is built on two key pillars that work together to augment the learning experience: (1) generating various multimodal representations of the content, and (2) taking foundational steps toward personalization.</p><p data-block-key=\"7jca2\">The seminal <a href=\"https://www.researchgate.net/publication/225249172_Dual_Coding_Theory_and_Education\" target=\"_blank\" rel=\"noopener noreferrer\">dual coding theory</a> states that forging mental connections between different representations strengthens the underlying conceptual schema in our brain. Subsequent <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0360131599000299\" target=\"_blank\" rel=\"noopener noreferrer\">research</a> indeed showed that when students actively engage with information in various formats, they build a more robust and complete mental model of the material. Inspired by this, our approach empowers students with the agency to choose and intermix multiple formats and modalities to best help them understand the material. In addition, personalization is increasingly becoming an <a href=\"https://par.nsf.gov/servlets/purl/10274018\" target=\"_blank\" rel=\"noopener noreferrer\">aspirational standard</a> in K-12 educational settings, and so our research reflects this. We aim to enhance the relatability and effectiveness of educational content by adapting it to student attributes. Moreover, we incorporate quizzing capabilities that enable us to further tailor the experience according to the learners’ real-time responses. Such personalization can be a powerful method for <a href=\"https://www.researchgate.net/publication/320564894_The_Role_of_Situational_Interest_in_Personalized_Learning\" target=\"_blank\" rel=\"noopener noreferrer\">enhancing motivation</a> and <a href=\"https://journals.sagepub.com/doi/full/10.3102/00346543221148478\" target=\"_blank\" rel=\"noopener noreferrer\">deepening learning</a>.</p><p data-block-key=\"35lic\">Bringing this to life involves a layered technical approach using <a href=\"https://blog.google/outreach-initiatives/education/google-learnlm-gemini-generative-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">LearnLM</a>, our best-in-class pedagogy-infused family of models, now integrated directly into <a href=\"https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini 2.5 Pro</a>. The first layer is a unique personalization pipeline that serves as the basis for the second layer of multiple content representations. Our starting point is a textbook PDF, although our approach could be used with other forms of source material.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">The personalization pipeline</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">The Learn Your Way interface asks the learner to select their grade and interests (e.g., sports, music, food). The original source material is first re-leveled to the learner’s reported grade level, while maintaining the scope of its content. This is followed by the strategic replacement of generic examples with ones that are personalized to the learner’s reported interests. The resulting text serves as the basis for the generation of all the other representations, effectively propagating the personalization effect and setting up a pipeline for further personalization.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-1-final-hero.width-1250.png\" alt=\"Learn-Your-Way-1-final-hero\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-1-final-hero.width-1250.png\" alt=\"Learn-Your-Way-1-final-hero\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ycqxy\"><i>Personalization of a generic text describing Newton’s law for two learner profiles (top) provides the basis for following representations of the content (bottom).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Multiple representations of content</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">Following the source personalization, we generate multiple representations of the content. For some content representations, such as mind maps and timelines, Gemini’s broad capabilities are used directly. Other features such as narrated slides, require more elaborate pipelines that weave together multiple specialized AI agents and tools to achieve an effective pedagogical result. Finally, specialized tasks, such as generating effective educational visuals, proved too challenging even for state-of-the-art general-purpose image models. To overcome this, we fine-tuned a dedicated model specifically for generating educational illustrations. The combination of a powerful base model, multi-step agentic workflows, and fine-tuned components allows us to generate a wide range of high-quality multimodal representations for learning.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The Learn Your Way experience</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">Our research comes to life in Learn Your Way. The interface brings together multiple, personalized representations of content including: (1) immersive text, (2) section-level quizzes, (3) slides &amp; narration, (4) audio lessons, and (5) mind maps.</p><ul><li data-block-key=\"digp2\"><b>Immersive text:</b> Breaks the content up into digestible sections that are augmented with generated images and embedded questions. Put together, these transform passive reading into an active multimodal experience that follows learning science principles.</li><li data-block-key=\"8kkuc\"><b>Section-level quizzes</b>: Promote active learning by allowing a user to interactively assess their learning, and uncover existing knowledge gaps.</li><li data-block-key=\"aq4r1\"><b>Slides &amp; narration:</b> Offers presentations that span the entire source material and include engaging activities like fill-in-the-blanks, as well as a narrated version, mimicking a recorded lesson.</li><li data-block-key=\"act6h\"><b>Audio lesson:</b> Provides simulated conversations, coupled with visual aids, between an AI-powered teacher and a student that models how a real learner might engage with the material, including the expression of misconceptions, which are clarified by the teacher.</li><li data-block-key=\"eqbtv\"><b>Mind map:</b> Organizes the knowledge hierarchically and allows learners to zoom in and out from the big picture to the details.</li></ul><p data-block-key=\"eef4f\">The above representations give learners choice and are all adapted to their selected grade level and personal interests. Throughout the experience, the interactive quizzes provide dynamic feedback, guiding students to revisit specific content areas where they struggled. This marks our first steps towards true personalization.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-2.width-1250.png\" alt=\"Learn-Your-Way-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-2.width-1250.png\" alt=\"Learn-Your-Way-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ycqxy\"><i>The Learn You Way interface provides easy access to multiple representations and practice opportunities.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Pedagogical evaluation</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">To evaluate Learn You Way's pedagogical performance, we transformed ten varied source materials from <a href=\"https://openstax.org/\" target=\"_blank\" rel=\"noopener noreferrer\">OpenStax</a> (a provider of free educational textbooks) to three different personalization settings. The source materials covered various subjects from history to physics. Three pedagogical subject matter experts then evaluated the transformed materials using pedagogical criteria, such as accuracy, coverage, and the <a href=\"https://blog.google/outreach-initiatives/education/google-learnlm-gemini-generative-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">LearnLM</a> learning science principles.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-3.width-1250.png\" alt=\"Learn-Your-Way-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-3.width-1250.png\" alt=\"Learn-Your-Way-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ycqxy\"><i>Top pedagogical principles that guide the development and evaluation of new learning capabilities and experiences at Google</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ucxot\">The results were highly positive, with an average expert rating of 0.85 or higher across all pedagogical criteria. See the <a href=\"https://arxiv.org/abs/2509.13348\" target=\"_blank\" rel=\"noopener noreferrer\">tech report</a> for more evaluation details.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-4.width-1250.png\" alt=\"Learn-Your-Way-4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-4.width-1250.png\" alt=\"Learn-Your-Way-4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ycqxy\"><i>Expert ratings for the different transformations for four key criteria.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Efficacy study</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">An AI-powered learning tool is only valuable if it both effectively improves learning outcomes and students want to use it. Learn Your Way now serves as a research platform for us to conduct studies with partners around the world to explore how AI-powered transformations and personalization affects outcomes, and to ensure that what we build is effective and <a href=\"https://blog.google/intl/en-africa/company-news/outreach-and-initiatives/5-ways-were-bringing-ai-innovations-to-people-across-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">locally relevant</a>.</p><p data-block-key=\"rstv\">Recently, we conducted a randomized controlled study with 60 students from the Chicago area, ages 15–18 and with similar reading levels. Participants were given up to 40 minutes to learn about adolescent brain development from a textbook, and randomly assigned to learn using Learn Your Way or a traditional digital PDF reader.</p><p data-block-key=\"e1qod\">We assessed students with a quiz immediately after the study session, and with a retention test 3–5 days later, using assessments designed by pedagogical experts to be a good measure of content comprehension. We also surveyed them about the learning experience, and to gain deeper insights beyond these quantitative metrics, each student participated in a 30-minute qualitative interview where they could share more nuanced feedback about their experience.</p><p data-block-key=\"2tkk3\">The results were compelling and statistically significant. Here are the highlights. See the <a href=\"https://arxiv.org/abs/2509.13348\" target=\"_blank\" rel=\"noopener noreferrer\">tech report</a> for more details.</p><ul><li data-block-key=\"1603p\"><b>Positive learning outcomes:</b> The Learn Your Way group scored, on average, 9% higher on the immediate assessment following the study session.</li><li data-block-key=\"2plg8\"><b>Better long-term retention:</b> Similarly, the Learn Your Way group scored 11% higher on the retention assessment 3-5 days later (78% vs. 67%).</li><li data-block-key=\"c6kod\"><b>Positive user sentiment:</b> 100% of students who used Learn Your Way reported that they felt the tool made them more comfortable taking the assessment, compared to 70% in the digital reader control group. 93% said they would want to use Learn Your Way for future learning, compared to just 67% for the digital reader.</li><li data-block-key=\"4mg4i\"><b>Valuable experience</b>: Insights from the qualitative interviews revealed that students found great value in Learn Your Way.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-5.width-1250.png\" alt=\"Learn-Your-Way-5\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-5.width-1250.png\" alt=\"Learn-Your-Way-5\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ycqxy\"><i>The group using Learn Your Way scored 9% higher on average on an immediate assessment than the group using a digital reader</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Experience Learn Your Way</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">To give a concrete feel for the Learn Your Way interactive experience, today we are releasing <a href=\"http://learnyourway.withgoogle.com/\" target=\"_blank\" rel=\"noopener noreferrer\">example experiences on Google Labs</a>, including:</p><ul><li data-block-key=\"16uqm\"><a href=\"https://learnyourway.withgoogle.com/scopes/e6ivLL1E/immersive-text/0\" target=\"_blank\" rel=\"noopener noreferrer\">A lesson on immune system challenges</a></li><li data-block-key=\"6vmtm\"><a href=\"https://learnyourway.withgoogle.com/scopes/RMUcoWft\" target=\"_blank\" rel=\"noopener noreferrer\">Learn about how to organize economies</a></li><li data-block-key=\"70e8i\"><a href=\"https://learnyourway.withgoogle.com/scopes/ggPsy1Wb\" target=\"_blank\" rel=\"noopener noreferrer\">Discover what sociology is?</a></li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The path forward</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">Our findings suggest that generative AI can be used to build learning experiences that are not only more effective but also more empowering. By evolving the static textbook into an interactive artifact and giving students greater agency over <i>how</i> they learn, we saw learning retention improve.</p><p data-block-key=\"88po1\">This work is just the beginning of our exploration. We envision many more ways to tailor content, moving towards systems that continuously adapt to each learner's unique needs and progress. As we take our next steps towards personalized education, we will continue to ground our research in pedagogical principles, measuring the impact of AI on learning efficacy, so that in the future every student might have access to a high-quality, engaging learning experience that is custom built for them.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\"><i>Shout out to our Google Research LearnLM team who have contributed to this work: Alicia Martín, Amir Globerson, Amy Wang, Anirudh Shekhawat, Anisha Choudhury, Anna Iurchenko, Avinatan Hassidim, Ayça Çakmakli, Ayelet Shasha Evron, Charlie Yang, Courtney Heldreth, Dana Oria, Diana Akrong, Hairong Mu, Ian Li, Ido Cohen, Komal Singh, Lev Borovoi, Lidan Hackmon, Lior Belinsky, Michael Fink, Preeti Singh, Rena Levitt, Shashank Agarwal, Shay Sharon, Sophie Allweis, Tracey Lee-Joe, Xiaohong Hao, Yael Gold-Zamir, Yishay Mor, and Yoav Bar Sinai. Special thanks to our executive champions: Niv Efron, Avinatan Hassidim, Yossi Matias and Ben Gomes.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    }
  ],
  "lastUpdated": "2025-10-02T10:30:42.964Z"
}