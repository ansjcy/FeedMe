{
  "sourceUrl": "https://rsshub.rssforever.com/google/research",
  "title": "Google Research Blog",
  "description": "Google Research Blog - Powered by RSSHub",
  "link": "https://research.google/blog",
  "items": [
    {
      "title": "XR Blocks：加速AI + XR创新 (原标题: XR Blocks: Accelerating AI + XR innovation)",
      "link": "https://research.google/blog/xr-blocks-accelerating-ai-xr-innovation/",
      "pubDate": "Wed, 08 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-08T16:00:00.000Z",
      "creator": "Google",
      "summary": "## XR Blocks：加速AI + XR创新\n\n### 引言：弥合AI与XR之间的鸿沟\n\n当前，人工智能（AI）与扩展现实（XR）的结合有望开启沉浸式智能计算的新范式。然而，这两个领域之间存在显著的生态系统差距。AI的研发得益于JAX、PyTorch、TensorFlow等成熟框架以及ImageNet、LMArena等基准测试的加速，而AI驱动的XR交互原型开发仍是一个高摩擦过程，通常需要开发者手动集成感知、渲染和交互等低级系统。\n\n为了弥合这一鸿沟，我们推出了 **XR Blocks**（在ACM UIST 2025上发布），这是一个跨平台框架，旨在加速以人为中心的AI + XR创新。这是我们之前针对非XR用例、通过可视化编程简化机器学习管道原型的Visual Blocks for ML研究的重大进展。\n\n### XR Blocks框架概述\n\nXR Blocks提供了一个模块化架构，包含即插即用组件，用于AI + XR中的核心抽象：用户、世界、界面、AI和代理。其核心使命是加速感知型AI + XR应用的快速原型开发。该工具包基于WebXR、threejs、LiteRT和Gemini等易于访问的技术构建，降低了XR创作者的入门门槛。我们通过一系列开源模板、实时演示和GitHub上的源代码展示了其效用，目标是赋能社区快速将概念转化为交互式原型。更多能力概述可在我们的方向性论文和预告视频中找到。\n\n### 设计原则\n\n我们的架构和API设计选择遵循以下三个原则：\n\n*   **拥抱简洁性和可读性：** 受Python禅意的启发，我们优先考虑清晰、人类可读的抽象。开发者的脚本应像对所需体验的高级描述。简单的任务应易于实现，复杂的逻辑应保持明确和可理解。\n*   **优先考虑创作者体验：** 我们的主要目标是使智能和感知型XR应用的创作尽可能无缝。我们认为创作者应专注于用户体验，而不是传感器融合、AI模型集成或跨平台交互逻辑等低级“管道”工作。\n*   **实用主义而非完整性：** 鉴于AI和XR领域发展迅速，我们遵循实用主义的设计理念。一个试图做到完美、全面复杂的框架在发布时可能已经过时。我们倾向于一个简单、模块化、适应性强的架构，它可以在桌面模拟器和Android XR设备上运行，适用于广泛的应用。\n\n### 框架工作原理\n\nXR Blocks框架从Visual Blocks for ML和InstructPipe中汲取灵感，提供了一个高层级、以人为中心的抽象层，将交互的“意图”（Script，下文详述）与低级实现的“方式”分离。\n\n![XR Blocks框架](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks1_Framework.width-1250.png)\n\nXR Blocks加速了桌面模拟器和Android XR设备上实时AI + XR应用的原型开发。例如：\n*   **(a) XR真实感：** 在模拟中原型化深度感知、基于物理的交互，并将相同的代码部署到真实世界的XR设备。\n*   **(b) XR交互：** 将自定义手势模型无缝集成到桌面模拟器和设备上的XR部署。\n*   **(c) AI + XR集成：** 构建智能、上下文感知的助手，例如提供主动建议和不显眼交互的Sensible Agent原型。\n\n### 抽象层：现实模型 (Reality Model)\n\n我们提出了一个由高级抽象组成的新“现实模型”，以指导XR Blocks框架的实现。与为端到端无监督训练设计的“世界模型”不同，我们的现实模型由可替换的XR交互模块组成。我们设计的核心是 **Script**，它是应用程序的叙事和逻辑中心。Script操作六个核心原语：\n\n*   **用户与物理世界：** 我们的模型以用户为中心，包括手部、凝视和虚拟形象。物理世界允许Script查询感知的现实，例如深度、估计的光照条件和对象。\n*   **虚拟界面与上下文：** 该模型通过虚拟UI元素增强混合现实，从2D面板到完全3D资产。感知管道分析环境、活动和交互历史的上下文。一个示例应用可在Sensible Agent中找到。\n*   **智能与社交实体：** 我们将AI驱动的代理和远程人类伙伴视为模型中的主要实体。这使得在DialogLab中实现混合人机对话中的动态群组对话。\n\n![XR Blocks框架的现实模型概念图](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks2_RealityModel.width-1250.png)\n\n### 实现：核心引擎 (Core Engine)\n\n这个现实模型由XR Blocks的模块化核心引擎实现，该引擎提供高级API，使开发者能够利用以下子系统，而无需掌握其底层实现：\n\n*   **感知与输入管道：** 摄像头、深度和声音模块持续为现实模型提供并更新物理现实的表示。输入模块标准化来自各种设备的用户操作，为XR Blocks提供解释的原始数据。\n*   **AI作为核心工具：** `ai`模块充当中央神经系统，提供简单而强大的函数（`.query`、`.runModel`），使大型模型成为可访问的工具。\n*   **体验与可视化工具包：** 为了实现快速创建，该工具包提供了一个常用功能库。`ux`模块提供可重用的交互行为，如`.selectable`和`.draggable`，而`ui`和`effect`模块处理界面渲染和复杂视觉效果，如遮挡。\n\n![XR Blocks核心引擎的模块化架构](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks3_Architecture.width-1250.png)\n\n通过将抽象的现实模型与具体的核心引擎分离，XR Blocks实现了强大的新创意工作流。目标是让创作者更快地从高层级、以人为中心的想法转向交互式原型。我们设想未来，任何声明性提示，例如“当用户捏住一个物体时，代理应该为它生成一首诗”，都可以直接转化为XR Blocks中的高级指令：\n\n![XR Blocks中的高层级指令](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks4_Instructions.width-1250.png)\n\n因此，创作者的提示不再是伪代码，而是实现逻辑的直接总结。我们设想这个框架能更无缝地将用户意图转化为系统级执行流，组合来自输入、声音、AI、世界、UI和代理模块的功能，以通过用户交互生成新兴的智能行为。\n\n![XR Blocks的交互语法](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks5_Interaction.width-1250.png)\n\n### 应用场景\n\n我们提供了一套交互式应用程序，以展示XR Blocks框架的表达能力和灵活性。这些示例展示了我们的框架如何实现以前过于复杂和昂贵而无法构建的复杂体验的快速原型开发，从而促进创建逼真、交互式和智能的混合现实世界：\n\n![XR Blocks的应用](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks_Applications.width-1250.png)\n\nXR Blocks的应用包括：\n1.  **XR真实感：** 深度感知和基于物理的球池和泼水游戏；几何感知阴影、带遮挡的3D高斯泼溅和光照估计。\n2.  **XR交互：** 由自定义ML模型、动态滑动识别、与物理世界的触摸和抓取赋能的沉浸式表情符号和剪刀石头布游戏。\n3.  **AI + XR：** 与对话式AI、XR对象、XR中的眼镜模拟以及用真实世界摄像头生成诗歌的集成。\n\n当这个现实模型与生成式AI深度集成以创建动态、个性化环境时，框架的真正力量得以实现。我们通过构建诸如“增强对象智能”（XR-Objects）之类的系统来证明这一点，该系统赋予日常物理对象交互式数字功能，例如动态虚拟按钮。XR Blocks也作为Sensible Agent（在ACM UIST 2025上发布）的基础，这是一个用于主动和不显眼AR辅助的系统。我们的架构提供了代理的核心感知和交互逻辑，提供了我们主要目标的一个例子：通过提供强大、高层级的工具，XR Blocks赋能人机交互研究人员绕过低级实现，直接专注于人机协作的认知原理等更高阶的挑战。SDK演示视频展示了XR Blocks与对话式AI的结合、Android XR上的物理碰撞与深度感知，以及在设备上运行LiteRT与自定义手势模型以触发XR动画。\n\n### 结论与未来展望\n\n创建智能XR体验目前过于碎片化，在创作者的愿景和实现之间设置了主要障碍。我们介绍了XR Blocks，一个通过提供高层级抽象层来消除这种复杂性的架构和工具包，它将“意图”（what）与“方式”（how，低级实现）分离，极大地加速了上下文感知应用的快速原型开发。这是迈向未来编程、设计和对话界限消失的奠基性一步，使我们能够像编写故事一样流畅地编写现实。XR Blocks远非完美，这项工作是最初的愿景文档，旨在邀请更多创作者加入我们的旅程，基于我们相信拥有正确工具集，每个人都可以通过AI释放内在创造力。\n\n### 致谢\n\n这项工作是Google多个团队的联合协作成果，由David Li和Ruofei Du（同等主要贡献）、Nels Numan、Xun Qian、Yanhe Chen和Zhongyi Zhou（同等次要贡献，按字母顺序排列）以及Evgenii Alekseev、Geonsun Lee、Alex Cooper、Min Xia、Scott Chung、Jeremy Nelson、Xiuxiu Yuan、Jolica Dias、Tim Bettridge、Benjamin Hersh、Michelle Huynh、Konrad Piascik、Ricardo Cabello和David Kim等研究人员和工程师共同贡献。我们感谢Mahdi Tayarani、Max Dzitsiuk、Patrick Hackett、Seeyam Qiu、Brian Collins、Steve Toh、Eric Gonzalez、Nicolás Peña Moreno、Yi-Fei Li、Ziyi Liu、Jing Jin对我们早期提案和WebXR实验的反馈和讨论。我们感谢Max Spear、Adarsh Kowdle和Guru Somadder的方向性贡献和周到评审。",
      "shortSummary": "XR Blocks是一个跨平台框架，旨在弥合人工智能（AI）和扩展现实（XR）生态系统之间的鸿沟。它通过提供模块化架构和高层级抽象，加速感知型AI + XR应用的快速原型开发。该框架基于WebXR等技术，简化了AI模型集成、传感器融合和跨平台交互等复杂任务，使创作者能够专注于用户体验。XR Blocks通过现实模型和核心引擎，支持从高层级意图到交互式原型的快速转化，并已通过多种应用场景展示其强大功能，旨在赋能社区释放AI创造力。",
      "translated_title": "XR Blocks：加速AI + XR创新",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks1_Framework.width-1250.png",
          "alt": "XRBlocks1_Framework",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks2_RealityModel.width-1250.png",
          "alt": "XRBlocks2_RealityModel",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks3_Architecture.width-1250.png",
          "alt": "XRBlocks3_Architecture",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks4_Instructions.width-1250.png",
          "alt": "XRBlocks4_Instructions",
          "title": "",
          "position": 5
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks5_Interaction.width-1250.png",
          "alt": "XRBlocks5_Interaction",
          "title": "",
          "position": 6
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"x6cvv\">The combination of artificial intelligence (AI) and extended reality (XR) has the potential to unlock a new paradigm of immersive intelligent computing. However, a significant gap exists between the ecosystems of these two fields today. AI research and development is accelerated by mature frameworks like <a href=\"https://jax.readthedocs.io/en/latest/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">JAX</a>, <a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">PyTorch</a>, <a href=\"https://www.tensorflow.org/\" target=\"_blank\" rel=\"noopener noreferrer\">TensorFlow</a>, and benchmarks like <a href=\"https://www.image-net.org/\" target=\"_blank\" rel=\"noopener noreferrer\">ImageNet</a> and <a href=\"https://lmarena.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">LMArena</a>. Meanwhile, prototyping novel AI-driven XR interactions remains a high-friction process, often requiring practitioners to manually integrate disparate, low-level systems for perception, rendering, and interaction.</p><p data-block-key=\"5pqcb\">To bridge this gap, we introduce <a href=\"http://xrblocks.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">XR Blocks</a> (presented at <a href=\"https://uist.acm.org/2025/\" target=\"_blank\" rel=\"noopener noreferrer\">ACM UIST 2025</a>), a cross-platform framework designed to accelerate human-centered AI + XR innovation. This is a significant step from our prior research in <a href=\"https://research.google/blog/visual-blocks-for-ml-accelerating-machine-learning-prototyping-with-interactive-tools/\">Visual Blocks for ML</a>, which targets non-XR use cases and streamlines prototyping machine learning pipelines with visual programming. XR Blocks provides a modular architecture with plug-and-play components for core abstraction in AI + XR: <i>user</i>, <i>world</i>, <i>interface</i>, <i>AI</i>, and <i>agents</i>. Crucially, it is designed with the mission of accelerating rapid prototyping of perceptive AI + XR apps. Built upon accessible technologies (<a href=\"https://immersiveweb.dev/\" target=\"_blank\" rel=\"noopener noreferrer\">WebXR</a>, <a href=\"https://threejs.org/\" target=\"_blank\" rel=\"noopener noreferrer\">threejs</a>, <a href=\"https://ai.google.dev/edge/litert\" target=\"_blank\" rel=\"noopener noreferrer\">LiteRT</a>, <a href=\"https://gemini.google.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a>), our toolkit lowers the barrier to entry for XR creators. We demonstrate its utility through a set of open-source <a href=\"https://xrblocks.github.io/docs/templates/Basic/\" target=\"_blank\" rel=\"noopener noreferrer\">templates</a>, <a href=\"https://xrblocks.github.io/docs/samples/ModelViewer/\" target=\"_blank\" rel=\"noopener noreferrer\">live demos</a>, and <a href=\"https://github.com/google/xrblocks\" target=\"_blank\" rel=\"noopener noreferrer\">source code on GitHub</a>, with the goal of empowering the community to quickly move from concept to interactive prototype. You can find an overview of these capabilities in our <a href=\"https://arxiv.org/abs/2509.25504\" target=\"_blank\" rel=\"noopener noreferrer\">directional paper</a> and <a href=\"https://www.youtube.com/watch?v=75QJHTsAoB8\" target=\"_blank\" rel=\"noopener noreferrer\">teaser video</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"75QJHTsAoB8\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=75QJHTsAoB8\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kk7t9\"><i>Introductory video of XR Blocks.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xfvek\">Design principles</h2><p data-block-key=\"1ctfe\">Our architectural and API design choices are guided by three principles:</p><ul><li data-block-key=\"f851k\"><i>Embrace simplicity and readability:</i> Inspired by <a href=\"https://en.wikipedia.org/wiki/Zen_of_Python\" target=\"_blank\" rel=\"noopener noreferrer\">Python's Zen</a>, we prioritize clean, human-readable abstractions. A developer's script should read like a high-level description of the desired experience. Simple tasks should be simple to implement, and complex logic should remain explicit and understandable.</li><li data-block-key=\"43oej\"><i>Prioritize the creator experience</i>: Our primary goal is to make authoring intelligent and perceptive XR applications as seamless as possible. We believe that creators should focus on the user experience, not on the low-level “plumbing” of sensor fusion, AI model integration, or cross-platform interaction logic.</li><li data-block-key=\"5dabh\"><i>Pragmatism over completeness</i>: We follow a design philosophy of pragmatism, since the fields of AI and XR are evolving quickly. A comprehensive, complex framework that attempts to be perfect will be obsolete upon release. We favor a simple, modular, and adaptable architecture that runs on both desktop and <a href=\"https://www.android.com/xr/\" target=\"_blank\" rel=\"noopener noreferrer\">Android XR</a> devices for a wide range of applications.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xfvek\">XR Blocks framework</h2><p data-block-key=\"6k5u2\">Drawing inspiration from <a href=\"https://visualblocks.withgoogle.com/#/\" target=\"_blank\" rel=\"noopener noreferrer\">Visual Blocks for ML</a> and <a href=\"https://research.google/blog/instructpipe-generating-visual-blocks-pipelines-with-human-instructions-and-llms/\">InstructPipe</a>, we designed the XR Blocks framework to provide a high-level, human-centered abstraction layer that separates the <i>what</i> of an interaction (denoted as <i>Script</i>, described more below) from the <i>how</i> of its low-level implementation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks1_Framework.width-1250.png\" alt=\"XRBlocks1_Framework\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks1_Framework.width-1250.png\" alt=\"XRBlocks1_Framework\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kk7t9\"><i>XR Blocks accelerates the prototyping of real-time AI + XR applications across desktop simulators and</i> <a href=\"http://android.com/xr\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Android XR</i></a><i> devices. Examples: (a) XR Realism: Prototype depth-aware, physics-based interactions in simulation and deploy the same code to real-world XR devices. (b) XR Interactions: Seamlessly integrate custom gesture models to desktop simulator and on-device XR deployment. (c) AI + XR Integration: Build intelligent, context-aware assistants, like the</i> <a href=\"https://research.google/blog/sensible-agent-a-framework-for-unobtrusive-interaction-with-proactive-ar-agents/\"><i>Sensible Agent</i></a><i> prototype that provides proactive suggestions with unobtrusive interactions.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"xfvek\">Abstractions</h3><p data-block-key=\"5j9q1\">We propose a new <i>Reality Model</i> composed of high-level abstractions to guide the implementation of the XR Blocks framework. Unlike the <a href=\"https://arxiv.org/abs/1803.10122\" target=\"_blank\" rel=\"noopener noreferrer\">World Model</a> designed for end-to-end unsupervised training, our <i>Reality Model</i> consists of replaceable modules for XR interaction. At the heart of our design is <i>Script</i>, the narrative and logical center of an application. <i>Script</i> operates on six first-class primitives (described and visualized below):</p><ul><li data-block-key=\"7hci8\"><i>User &amp; the physical world:</i> Our model is centered around the <i>User</i>, consisting of hands, gaze, and avatar. The physical <i>world</i> allows <i>Script</i> to query the perceived reality such as depth (<a href=\"https://xrblocks.github.io/docs/samples/DepthMap\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>), estimated lighting condition (<a href=\"https://xrblocks.github.io/docs/samples/Lighting\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>), and objects (<a href=\"https://xrblocks.github.io/docs/samples/Gemini-XRObject\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>).</li><li data-block-key=\"b8t27\"><i>Virtual interfaces &amp; context:</i> The model augments the blended reality with virtual UI elements, from 2D panels (<a href=\"https://xrblocks.github.io/docs/samples/UI\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>) to fully 3D assets (<a href=\"https://xrblocks.github.io/docs/samples/ModelViewer/\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>). The perception pipeline analyzes the context of environment, activities, and histories of interaction. An example application can be found in <a href=\"https://research.google/blog/sensible-agent-a-framework-for-unobtrusive-interaction-with-proactive-ar-agents/\">Sensible Agent</a> (discussed more below).</li><li data-block-key=\"e7sot\"><i>Intelligent &amp; Social Entities</i>: We treat AI-driven <i>agents</i> and remote human <i>peers</i> as primary entities within the model. This enables dynamic group conversations in hybrid human-AI conversations in <a href=\"https://dl.acm.org/doi/10.1145/3746059.3747696\" target=\"_blank\" rel=\"noopener noreferrer\">DialogLab</a>.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks2_RealityModel.width-1250.png\" alt=\"XRBlocks2_RealityModel\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks2_RealityModel.width-1250.png\" alt=\"XRBlocks2_RealityModel\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kk7t9\"><i>The conceptual Reality Model of the XR Blocks framework. At the center, Script contains the application’s logic and operates on a unified model of first-class primitives including the user, the physical world, AI agents, and the application context.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"xfvek\">Implementation</h3><p data-block-key=\"c09j3\">This Reality Model is realized by XR Blocks’s modular Core engine, which provides high-level APIs that enable developers to harness the following subsystems without needing to master the implementation:</p><ul><li data-block-key=\"e15ud\"><i>Perception &amp; input pipeline:</i> The <code>camera</code>, <code>depth</code>, and <code>sound</code> modules continuously feed and update the Reality Model’s representation of physical reality. The <code>input</code> module normalizes user actions from various devices, providing the raw data for XR Blocks to interpret.</li><li data-block-key=\"2r6v8\"><i>AI as a core utility:</i> The <code>ai</code> module acts as a central nervous system, providing simple yet powerful functions (<code>.query</code>, <code>.runModel</code>) that make large models an accessible utility.</li><li data-block-key=\"fahdq\"><i>Experience &amp; visualization toolkit:</i> To enable rapid creation, the toolkit provides a library of common affordances. The <code>ux</code> module offers reusable interaction behaviors like <code>.selectable</code> and <code>.draggable</code> (<a href=\"https://xrblocks.github.io/docs/samples/ModelViewer/\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>), while the <code>ui</code> and <code>effect</code> modules handle the rendering of interfaces and complex visual effects like occlusion (<a href=\"https://xrblocks.github.io/docs/samples/Occlusion\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>).</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks3_Architecture.width-1250.png\" alt=\"XRBlocks3_Architecture\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks3_Architecture.width-1250.png\" alt=\"XRBlocks3_Architecture\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kk7t9\"><i>The modular architecture of the XR Blocks’s core engine, which consists of essential subsystems to realize the framework’s high-level abstractions, spanning perception (</i><code><i>depth</i></code><i>,</i> <code><i>input</i></code><i>), AI integration (</i><code><i>ai</i></code><i>,</i> <code><i>agent</i></code><i>), and user experience (</i><code><i>ui</i></code><i>,</i> <code><i>ux</i></code><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xfvek\">By separating the abstract Reality Model from the concrete Core engine, XR Blocks enables a powerful new creative workflow. The goal is to allow creators to move from high level, human-centric ideas to interactive prototypes much more quickly. We envision a future where any declarative prompt, <i>“When the user pinches at an object, an agent should generate a poem of it”</i>, could be directly translated to high-level instructions in XR Blocks:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks4_Instructions.width-1250.png\" alt=\"XRBlocks4_Instructions\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks4_Instructions.width-1250.png\" alt=\"XRBlocks4_Instructions\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xfvek\">Hence, the creator’s prompt is no longer pseudocode but a direct summary of the implementation logic. We envision this framework to more seamlessly translate such user intent into a system-level execution flow, composing capabilities from the <code>input</code>, <code>sound</code>, <code>ai</code>, <code>world</code>, <code>ui</code>, and <code>agent</code> modules to generate an emergent, intelligent behavior with user interaction.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks5_Interaction.width-1250.png\" alt=\"XRBlocks5_Interaction\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks5_Interaction.width-1250.png\" alt=\"XRBlocks5_Interaction\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kk7t9\"><i>The Interaction Grammar of XR Blocks, which abstracts user input by distinguishing between two types of interaction. Explicit events are direct, low-level inputs (e.g., a touch or click), while implicit intents are higher-level interpretations (e.g., a gesture or voice command), allowing creators to build interaction against user intent.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"fbybx\">Application scenarios</h2><p data-block-key=\"5nimk\">We provide a suite of interactive applications to demonstrate the expressive power and flexibility of the XR Blocks framework. These examples showcase how our framework enables the rapid prototyping of sophisticated experiences that were previously too complex and costly to build, facilitating the creation of realistic, interactive, and intelligent mixed-reality worlds:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks_Applications.width-1250.png\" alt=\"XRBlocks_Applications\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks_Applications.width-1250.png\" alt=\"XRBlocks_Applications\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8ze35\"><i>Applications of XR Blocks. (1) XR Realism: Depth-aware and physics-based ball pit (</i><a href=\"https://xrblocks.github.io/docs/samples/Ballpit\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>) and splash games (</i><a href=\"https://xrblocks.github.io/docs/samples/Splash\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>); geometry-aware shadows (</i><a href=\"https://xrblocks.github.io/docs/samples/Occlusion\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>), 3D Gaussian splatting with occlusion, and lighting estimation. (2) XR Interaction: Immersive emoji (</i><a href=\"https://xrblocks.github.io/docs/samples/XR-Emoji\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>) and rock paper scissors game (</i><a href=\"https://xrblocks.github.io/docs/samples/RockPaperScissors\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>) empowered by custom ML models, dynamic swipe recognition, touch and grab with the physical world. (3) AI + XR: Integration with conversational AI (</i><a href=\"https://xrblocks.github.io/docs/samples/Gemini-Icebreakers\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>), XR objects (</i><a href=\"https://xrblocks.github.io/docs/samples/Gemini-XRObject\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>), glasses simulation in XR, and poem generation with a real-world camera.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fbybx\">The true power of the framework is realized when this Reality Model is deeply integrated with generative AI to create dynamic, personalized environments. We demonstrate this by building systems like Augmented Object Intelligence (<a href=\"https://research.google/blog/augmented-object-intelligence-with-xr-objects/\">XR-Objects</a>), which imbues everyday physical objects with interactive digital affordances, such as dynamic virtual buttons. XR Blocks also serves as the foundation for <a href=\"https://research.google/blog/sensible-agent-a-framework-for-unobtrusive-interaction-with-proactive-ar-agents/\">Sensible Agent</a> (published on ACM UIST 2025), a system for proactive and unobtrusive AR assistance. Our architecture provides the agent's core perception and interaction logic, providing an example of our primary goal: by providing robust, high-level tools, XR Blocks empowers Human-Computer Interaction researchers to bypass low-level implementation and focus directly on higher-order challenges like the cognitive principles of human-agent collaboration.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/XRBlocks6_Demos.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8ze35\"><i>Demonstrations of XR Blocks SDK. (1) Using XR Blocks with conversational AI to automatically generate and test user prompts. (2) Running physical collision with depth sensing on Android XR. (3) Running LiteRT on a device with a custom gesture model to trigger XR animation.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"fbybx\">Conclusion and future directions</h2><p data-block-key=\"c4i24\">Creating intelligent XR experiences is currently too fragmented, placing a major barrier between a creator's vision and its realization. We presented <a href=\"https://xrblocks.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">XR Blocks</a>, an architecture and toolkit that dissolves this complexity by providing a high-level abstraction layer that separates <i>what</i> (the intent) from the <i>how</i> (the low-level implementation), dramatically accelerating the prototyping of context-aware applications. This is a foundational step toward a future where the boundaries between programming, design, and conversation disappear, enabling us to script realities as fluidly as we script stories. XR Blocks is far from perfect, and this work serves as <a href=\"https://arxiv.org/abs/2509.25504\" target=\"_blank\" rel=\"noopener noreferrer\">an initial visionary document</a> to invite more creators to join our journey, based on our belief that <i>with the right set of tools, everyone can unleash their inner creativity with AI</i>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"fbybx\">Acknowledgements</h2><p data-block-key=\"ccr67\"><i>This work is a joint collaboration across multiple teams at Google. The following researchers and engineers contributed to this work: David Li and Ruofei Du (equal primary contributions), Nels Numan, Xun Qian, Yanhe Chen, and Zhongyi Zhou, (equal secondary contributions, sorted alphabetically), as well as Evgenii Alekseev, Geonsun Lee, Alex Cooper, Min Xia, Scott Chung, Jeremy Nelson, Xiuxiu Yuan, Jolica Dias, Tim Bettridge, Benjamin Hersh, Michelle Huynh, Konrad Piascik, Ricardo Cabello, and David Kim. We would like to thank Mahdi Tayarani, Max Dzitsiuk, Patrick Hackett, Seeyam Qiu, Brian Collins, Steve Toh, Eric Gonzalez, Nicolás Peña Moreno, Yi-Fei Li, Ziyi Liu, Jing Jin for their feedback and discussion on our early-stage proposal and WebXR experiments. We thank Max Spear, Adarsh Kowdle, and Guru Somadder for the directional contribution and thoughtful reviews.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "语音到检索 (S2R)：一种新的语音搜索方法 (原标题: ​​Speech-to-Retrieval (S2R): A new approach to voice search)",
      "link": "https://research.google/blog/speech-to-retrieval-s2r-a-new-approach-to-voice-search/",
      "pubDate": "Mon, 06 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-06T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 语音到检索 (S2R)：一种新的语音搜索方法\n\n## 传统语音搜索的挑战\n\n*   **级联模型 (Cascade Modeling)**：当前的语音搜索系统通常采用级联模型，先通过自动语音识别 (ASR) 将语音输入转换为文本查询，然后基于文本进行搜索。\n*   **ASR 错误的影响**：这种方法的主要挑战在于，ASR 阶段的微小错误可能显著改变查询的含义，导致不相关的搜索结果。\n    *   **示例**：将“The Scream painting”（《呐喊》画作）错误识别为“screen painting”（屏幕画作），将返回关于屏幕绘画技巧的结果，而非蒙克杰作的信息。\n*   **问题根源**：级联模型存在信息丢失和错误传播的问题，ASR 系统一旦早期误解音频，错误就会传递给搜索系统，且搜索系统通常无法纠正。\n\n## 引入语音到检索 (S2R)\n\n*   **核心理念**：S2R 是一种直接从语音查询中解释和检索信息的技术，完全绕过了中间的、可能存在缺陷的文本转录步骤。\n*   **范式转变**：它代表了机器处理人类语音的根本性架构和理念转变。传统系统关注“说了什么词？”，而 S2R 旨在回答更强大的问题：“正在寻求什么信息？”\n*   **目标**：弥补当前语音搜索体验中存在的显著质量差距。\n\n## SVQ 数据集\n\n*   **开放资源**：作为 Massive Sound Embedding Benchmark (MSEB) 的一部分，谷歌开放了 Simple Voice Questions (SVQ) 数据集。\n*   **内容**：该数据集包含以 17 种不同语言和 26 个区域录制的短音频问题集合。\n*   **用途**：用于评估 S2R 的性能潜力。\n\n## S2R 潜力评估实验\n\n*   **实验目的**：模拟理想的 ASR 性能，以揭示当前系统与理论最佳性能之间的差距。\n*   **实验方法**：\n    1.  收集代表性语音搜索查询。\n    2.  由人工标注员手动转录这些查询，创建“完美 ASR”场景（即“groundtruth”）。\n    3.  **建立两个搜索系统进行比较**：\n        *   **级联 ASR (Cascade ASR)**：模拟真实世界设置，语音通过 ASR 转换为文本，然后传递给检索系统。\n        *   **级联真值 (Cascade Groundtruth)**：模拟“完美”级联模型，将无缺陷的真值文本直接发送给相同的检索系统。\n    4.  人类评估员比较两个系统的搜索结果质量。\n*   **评估指标**：\n    *   **ASR 质量**：词错误率 (WER)。\n    *   **搜索性能**：平均倒数排名 (MRR)，用于评估对查询列表响应的正确性概率。\n*   **关键发现**：\n    *   **WER 与 MRR 的复杂关系**：较低的 WER 并不总能可靠地带来较高的 MRR，错误的具体性质（而非仅仅存在）是关键的、依赖于语言的因素。\n    *   **显著的性能差距**：在所有测试语言中，两个系统之间存在显著的 MRR 差异。这表明当前级联设计与完美语音识别理论上可能达到的性能之间存在巨大差距，为 S2R 模型提供了巨大的改进潜力。\n\n![SVQ数据集中语音搜索语言的ASR模型词错误率](https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval3_WER.width-1250.png)\n*SVQ数据集中语音搜索语言的ASR模型词错误率*\n\n![当前真实世界（“级联 ASR”；蓝色）模型与真值（即完美；“级联真值”；绿色）的 MRR](https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval4_MRRCurrent.width-1250.png)\n*当前真实世界（“级联 ASR”；蓝色）模型与真值（即完美；“级联真值”；绿色）的 MRR*\n\n## S2R 的架构：从声音到意义\n\n*   **核心架构**：双编码器 (dual-encoder) 架构。\n*   **组件**：\n    *   **音频编码器**：处理原始音频查询，将其转换为捕获语义含义的丰富向量表示。\n    *   **文档编码器**：学习文档的类似向量表示。\n*   **训练机制**：\n    *   使用大量配对的音频查询和相关文档数据集进行训练。\n    *   系统同时调整两个编码器的参数。\n    *   **训练目标**：确保音频查询的向量在表示空间中与相应文档的向量在几何上接近。\n*   **优势**：该架构使模型能够直接从音频中学习更接近检索所需的核心意图，从而绕过级联设计中脆弱的逐字转录中间步骤。\n\n![音频和文档嵌入之间相似性损失的差异](https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval5_SimilarityLoss.width-1250.png)\n*音频和文档嵌入之间相似性损失的差异*\n\n## S2R 模型的工作原理\n\n1.  用户说出查询。\n2.  音频流式传输到预训练的音频编码器，生成一个查询向量（音频嵌入）。\n3.  该向量用于通过复杂的搜索排名过程，从索引中高效识别一组高度相关的候选结果。\n    *   **示例**：用户语音请求“The Scream painting”被音频编码器转换为音频嵌入。\n    *   该嵌入扫描大量文档索引，浮现出相似度高的初始候选结果（如维基百科页面、蒙克博物馆网站）。\n4.  搜索排名系统结合初始分数和数百个其他信号，深入理解相关性和质量，在极短时间内编排最终排名，确保向用户呈现最有帮助和最值得信赖的信息。\n\n## S2R 评估结果\n\n*   在 SVQ 数据集上对 S2R 系统进行了评估。\n*   **关键结果**：\n    *   S2R 模型显著优于基线级联 ASR 模型。\n    *   其性能接近由级联真值模型设定的上限。\n    *   尽管结果令人鼓舞，但剩余的差距表明仍需进一步研究。\n\n![当前真实世界（“级联 ASR”；蓝色）模型与真值（即完美；“级联真值”；绿色）以及 S2R 模型性能（“S2R”橙色条）的 MRR](https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval7_Results.width-1250.png)\n*当前真实世界（“级联 ASR”；蓝色）模型与真值（即完美；“级联真值”；绿色）以及 S2R 模型性能（“S2R”橙色条）的 MRR*\n\n## 语音搜索的新时代\n\n*   **已投入实际应用**：S2R 驱动的语音搜索已在多种语言中为用户提供服务，实现了超越传统级联系统的显著准确性提升。\n*   **推动领域发展**：谷歌开放了 SVQ 数据集作为 MSEB 的一部分，旨在加速整个领域的发展。\n*   **呼吁合作**：邀请全球研究社区利用这些数据，在公共基准上测试新方法，共同构建下一代真正智能的语音界面。",
      "shortSummary": "传统语音搜索依赖ASR将语音转文本，易因识别错误导致结果不准确。语音到检索（S2R）是一种新方法，它直接从语音查询中理解并检索信息，绕过文本转录。S2R采用双编码器架构，将音频直接映射到语义意图。实验表明，S2R显著优于传统ASR系统，并接近理论最佳性能。谷歌已将S2R投入实际应用，并在多语言环境中提升了语音搜索准确性，同时开放了SVQ数据集以推动该领域发展。",
      "translated_title": "语音到检索 (S2R)：一种新的语音搜索方法",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval3_WER.width-1250.png",
          "alt": "SpeechToRetrieval3_WER",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval4_MRRCurrent.width-1250.png",
          "alt": "SpeechToRetrieval4_MRRCurrent",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval5_SimilarityLoss.width-1250.png",
          "alt": "SpeechToRetrieval5_SimilarityLoss",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval7_Results.width-1250.png",
          "alt": "SpeechToRetrieval7_Results",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"42rfj\">Voice-based web search has been around a long time and continues to be used by many people, with the underlying technology evolving rapidly to allow for expanded use cases. Google’s initial voice search solution used <a href=\"https://en.wikipedia.org/wiki/Speech_recognition\" target=\"_blank\" rel=\"noopener noreferrer\">automatic speech recognition</a> (ASR) to turn the voice input into a text query, and then searched for documents matching that text query. However, a challenge with this <a href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36340.pdf\">cascade modeling approach</a> is that any slight errors in the speech recognition phase can significantly alter the meaning of the query, producing the wrong results.</p><p data-block-key=\"9va6b\">For example, imagine someone does a voice-based web search for the famous painting, “<a href=\"https://en.wikipedia.org/wiki/The_Scream\" target=\"_blank\" rel=\"noopener noreferrer\">The Scream</a>”, by Edvard Munch. The search engine uses the typical approach of cascade modeling, first converting the voice query to text via ASR before passing the text to the search system. Ideally, the ASR transcribes the query perfectly. The search system then receives the correct text — “the Scream painting” — and provides relevant results, like the painting’s history, its meaning, and where it’s displayed. However, what if the ASR system mistakes the “m” of “scream” for an “n”? It misinterprets the query as “screen painting” and returns irrelevant results about screen painting techniques instead of details about Munch's masterpiece.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/SpeechToRetrieval2_Cascade.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"255p4\"><i>ASR accuracy is key for voice search. See what happens when a system correctly transcribes a query versus when it transcribes it incorrectly.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"42rfj\">To prevent such errors in web search systems, what if the system could map directly from speech to the desired retrieval intent, bypassing the textual transcription entirely?</p><p data-block-key=\"8hvip\">Enter Speech-to-Retrieval (S2R). At its core, S2R is a technology that directly interprets and retrieves information from a spoken query without the intermediate, and potentially flawed, step of having to create a perfect text transcript. It represents a fundamental architectural and philosophical shift in how machines process human speech. Where today's common voice search technologies are focused on the question, \"What words were said?\", S2R is designed to answer a more powerful question: \"What information is being sought?\" This post explores the substantial quality gap in current voice search experiences and demonstrates how the S2R model is poised to fill it. In addition, we are open-sourcing the <a href=\"https://huggingface.co/datasets/google/svq#:~:text=Simple%20Voice%20Questions%20(SVQ)%20is,languages%20under%20multiple%20audio%20conditions.\" target=\"_blank\" rel=\"noopener noreferrer\">Simple Voice Questions</a> (SVQ) dataset, a collection of short audio questions recorded in 17 different languages and 26 locales, which we used to evaluate the performance potential of S2R. The SVQ dataset is part of the new <a href=\"https://github.com/google-research/mseb\" target=\"_blank\" rel=\"noopener noreferrer\">Massive Sound Embedding Benchmark</a> benchmark.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"42rfj\">Evaluating the potential of S2R</h2><p data-block-key=\"e2hqh\">When a traditional ASR system converts audio into a single text string, it may lose contextual cues that could help disambiguate the meaning (i.e., information loss). If the system misinterprets the audio early on, that error is passed along to the search engine, which typically lacks the ability to correct it (i.e., error propagation). As a result, the final search result may not reflect the user's intent.</p><p data-block-key=\"fmodj\">To investigate this relationship, we conducted an experiment designed to simulate an ideal ASR performance. We began by collecting a representative set of test queries reflecting typical voice search traffic. Crucially, these queries were then manually transcribed by human annotators, effectively creating a \"perfect ASR\" scenario where the transcription is the absolute truth.</p><p data-block-key=\"esgbn\">We then established two distinct search systems for comparison (see chart below):</p><ul><li data-block-key=\"crdkj\">Cascade ASR represents a typical real-world setup, where speech is converted to text by an automatic speech recognition (ASR) system, and that text is then fed to a retrieval system.</li><li data-block-key=\"103o6\">Cascade groundtruth simulates a \"perfect\" cascade model by sending the flawless ground-truth text directly to the same retrieval system.</li></ul><p data-block-key=\"d2bvm\">The retrieved documents from both systems (cascade ASR and cascade groundtruth) were then presented to human evaluators, or \"raters\", alongside the original true query. The evaluators were tasked with comparing the search results from both systems, providing a subjective assessment of their respective quality.</p><p data-block-key=\"edlf6\">We use <a href=\"https://en.wikipedia.org/wiki/Word_error_rate\" target=\"_blank\" rel=\"noopener noreferrer\">word error rate</a> (WER) to measure the ASR quality and to measure the search performance, we use <a href=\"https://en.wikipedia.org/wiki/Mean_reciprocal_rank\" target=\"_blank\" rel=\"noopener noreferrer\">mean reciprocal rank</a> (MRR) — a statistical metric for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness and calculated as the average of the reciprocals of the rank of the first correct answer across all queries. The difference in MRR and WER between the real-world system and the groundtruth system reveals the potential performance gains <a href=\"https://vocal.media/education/the-rise-of-voice-search-and-its-impact-on-indian-businesses\" target=\"_blank\" rel=\"noopener noreferrer\">across some of the most commonly used voice search languages</a> in the SVQ dataset (shown below).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval3_WER.width-1250.png\" alt=\"SpeechToRetrieval3_WER\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval3_WER.width-1250.png\" alt=\"SpeechToRetrieval3_WER\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"255p4\"><i>The word error rate (WER) of the ASR model across voice search languages in the SVQ dataset.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval4_MRRCurrent.width-1250.png\" alt=\"SpeechToRetrieval4_MRRCurrent\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval4_MRRCurrent.width-1250.png\" alt=\"SpeechToRetrieval4_MRRCurrent\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"255p4\"><i>MRR of current real-world (“Cascade ASR”; blue) models vs ground truth (i.e., perfect; “Cascade Groundtruth”; green).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"42rfj\">The results of this comparison lead to two critical observations. First, and as can be seen by comparing both charts above, we found that a lower WER does not reliably lead to a higher MRR across different languages. The relationship is complex, suggesting that the impact of transcription errors on downstream tasks is not fully captured by the WER metric. The specific nature of an error — not just its existence — appears to be a critical, language-dependent factor. Second, and more importantly, there’s a significant MRR difference between the two systems across all tested languages. This reveals a substantial performance gap between current cascade designs and what is theoretically possible with perfect speech recognition. This gap represents the clear potential for S2R models to fundamentally improve voice search quality.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"h9kvr\">The architecture of S2R: From sound to meaning</h2><p data-block-key=\"aej9r\">At the heart of our S2R model is a dual-encoder architecture. This design features two specialized neural networks that learn from vast amounts of data to understand the relationship between speech and information. An audio encoder processes the raw audio of a query, converting it into a rich vector representation that captures its semantic meaning. In parallel, a document encoder learns a similar vector representation for documents.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval5_SimilarityLoss.width-1250.png\" alt=\"SpeechToRetrieval5_SimilarityLoss\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval5_SimilarityLoss.width-1250.png\" alt=\"SpeechToRetrieval5_SimilarityLoss\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"iex25\"><i>Difference in similarity loss between audio and document embedding.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"h9kvr\">The key to this model is how it is trained. Using a large dataset of paired audio queries and relevant documents, the system learns to adjust the parameters of both encoders simultaneously.</p><p data-block-key=\"3mkue\">The training objective ensures that the vector for an audio query is geometrically close to the vectors of its corresponding documents in the representation space. This architecture allows the model to learn something closer to the essential <i>intent</i> required for retrieval directly from the audio, bypassing the fragile intermediate step of transcribing every word, which is the principal weakness of the cascade design.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"h9kvr\">How the S2R model works</h2><p data-block-key=\"7b96s\">When a user speaks a query, the audio is streamed to the pre-trained audio encoder, which generates a query vector. This vector is then used to efficiently identify a highly relevant set of candidate results from our index through a complex search ranking process.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/SpeechToRetrieval6_Final.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"iex25\"><i>How S2R processes a spoken query.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"h9kvr\">The animation above illustrates how S2R understands and answers a spoken query. It starts with a user's voice request for “The Scream painting”. An audio encoder translates the sound into a rich <a href=\"https://dev.to/josethz00/audio-embeddings-understanding-the-basics-4pc1\" target=\"_blank\" rel=\"noopener noreferrer\">audio embedding</a>, a vector that represents the deep meaning of the query. This embedding is then used to scan a massive index of documents, surfacing initial candidates with high similarity scores, like the Wikipedia page for “The Scream” (0.8) and the Munch Museum website (0.7).</p><p data-block-key=\"3p31t\">But finding relevant documents is just the beginning. The crucial final step is orchestrated by the search ranking system. This powerful intelligence goes far beyond the initial scores, weaving them together with hundreds of other signals to deeply understand relevance and quality. It weighs all this information in a fraction of a second to choreograph the final ranking, ensuring the most helpful and trustworthy information is presented to the user.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"h9kvr\">Evaluating S2R</h2><p data-block-key=\"9ef7v\">We evaluated the S2R system described above on the SVQ dataset:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval7_Results.width-1250.png\" alt=\"SpeechToRetrieval7_Results\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechToRetrieval7_Results.width-1250.png\" alt=\"SpeechToRetrieval7_Results\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"iex25\"><i>MRR of current real-world (“Cascade ASR”; blue) models vs ground truth (i.e., perfect; “Cascade Groundtruth”; green) and the S2R model's performance (\"S2R\" orange bar).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"h9kvr\">The S2R model's performance (orange bar) shows two key results:</p><ul><li data-block-key=\"2hrqi\">It significantly outperforms the baseline cascade ASR model.</li><li data-block-key=\"elct4\">Its performance approaches the upper bound established by the cascade ground truth model.</li></ul><p data-block-key=\"a341o\">While promising, the remaining gap indicates that further research is required.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"h9kvr\">The new era for voice search is now live</h2><p data-block-key=\"bmjeq\">The move to S2R-powered voice search isn’t a theoretical exercise; it’s a live reality. In a close collaboration between Google Research and Search, these advanced models are now serving users in multiple languages, delivering a significant leap in accuracy beyond conventional cascade systems.</p><p data-block-key=\"cr6fk\">To help propel the entire field forward, we are also open-sourcing the <a href=\"https://huggingface.co/datasets/google/svq\" target=\"_blank\" rel=\"noopener noreferrer\">SVQ dataset</a> as part of the <a href=\"https://github.com/google-research/mseb\" target=\"_blank\" rel=\"noopener noreferrer\">Massive Sound Embedding Benchmark</a> (MSEB). We believe shared resources and transparent evaluation accelerates progress. In that spirit, we invite the global research community to use this data, test new approaches on public benchmarks, and join the effort to build the next generation of truly intelligent voice interfaces.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"h9kvr\">Acknowledgements</h2><p data-block-key=\"9o630\"><i>The authors sincerely thank all who contributed to this project, whose critical input made it possible. We are especially grateful to our colleagues Hawi Abraham, Cyril Allauzen, Tom Bagby, Karthik Kumar Bandi, Stefan Buettcher, Dave Dopson, Lucy Hadden, Georg Heigold, Sanjit Jhala, Shankar Kumar, Ji Ma, Eyal Mizrachi, Pandu Nayak, Pew Putthividhya, David Rybach, Jungshik Shin, Venkat Subramanian, Sundeep Tirumalareddy and Trystan Upstill. We also wish to acknowledge those who helped prepare this post: Mark Simborg for his extensive editing, Kimberly Schwede for the wonderful illustrations, and Mickey Wurts for his valuable assistance.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "图像生成中的协作方法 (原标题: A collaborative approach to image generation)",
      "link": "https://research.google/blog/a-collaborative-approach-to-image-generation/",
      "pubDate": "Wed, 01 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-01T16:00:00.000Z",
      "creator": "Google",
      "summary": "在文本到图像（T2I）模型日益强大的今天，用户仍常面临生成结果与心中所想不符的困境，且难以通过反复修改提示词来弥合差距。为了解决这一问题，研究人员提出了 **PASTA（偏好自适应和序列文本到图像代理）**，这是一种强化学习（RL）代理，旨在通过与用户协作，逐步完善T2I生成结果，从而消除用户对试错式提示词优化的依赖。\n\n### PASTA 的核心理念与优势\n\n*   **协作式对话**：将图像生成过程转变为与用户的协作式对话，而非单向的提示词输入。\n*   **消除试错**：用户无需反复修改提示词，PASTA会根据用户反馈逐步调整。\n*   **用户满意度高**：通过人类评估，PASTA生成的图像被用户评价为更令人满意。\n*   **数据驱动**：创建并发布了一个包含7000多次人类交互的序列偏好数据集。\n\n### PASTA 的工作原理\n\n为了训练PASTA有效适应用户的个性化偏好，需要大量多样的交互数据。鉴于获取真实用户数据的挑战（如隐私问题），PASTA采用了两阶段训练策略：\n\n1.  **高质量基础数据集收集**：\n    *   收集了超过7000名评估者与PASTA的序列交互数据。\n    *   这些交互包括由Gemini Flash大型多模态模型生成的提示词扩展，以及由Stable Diffusion XL (SDXL) T2I模型生成的相应图像。\n    *   这些真实的偏好数据被用于训练用户模拟器。\n\n2.  **用户模拟器训练**：\n    *   **用户模型**：包含两个关键组件：\n        *   **效用模型**：预测用户对任何图像集的喜好程度。\n        *   **选择模型**：预测用户在面对多个图像集时会选择哪一个。\n    *   **模型构建**：使用预训练的CLIP编码器，并添加用户特定组件。\n    *   **训练算法**：采用期望最大化算法，同时学习用户偏好细节并发现潜在的“用户类型”（即具有相似品味的集群，如偏爱动物、风景或抽象艺术）。\n    *   **数据生成**：训练后的用户模拟器能够对生成的图像提供反馈并表达偏好，从而生成超过30,000条模拟交互轨迹，为PASTA代理的强化学习训练提供了大规模受控环境。\n\n    ![PASTA-1](https://storage.googleapis.com/gweb-research2023-media/images/PASTA-1.width-1250.png)\n    *图示：用户模拟器从偏好数据中识别出不同的用户类型。每一行展示了新兴用户档案的顶级图像，揭示了对“动物”或“食物”等类别的清晰偏好。*\n\n### PASTA 代理的交互流程\n\n一旦PASTA训练并部署，用户与PASTA的交互过程如下：\n\n1.  **初始提示**：用户输入一个初始提示词。\n2.  **候选生成**：PASTA使用候选生成器（一个大型多模态模型）创建一组多样化的潜在提示词扩展。\n3.  **候选选择**：训练有素的RL代理（候选选择器）选择最佳的四组扩展建议。\n4.  **图像生成与展示**：根据选定的扩展生成相应图像，并展示给用户。\n5.  **用户反馈**：用户选择最符合其愿景的图像，此选择作为反馈指导PASTA的下一组建议。\n6.  **迭代优化**：这种协作式的来回交互使模型能够即时学习用户的偏好，逐步引导创作过程达到用户的理想目标。\n\n### 性能评估与结果\n\n研究人员通过隐式Q学习（IQL）训练了PASTA，并评估了不同训练数据对性能的影响。他们创建了三个版本的代理：仅使用真实志愿者数据训练、仅使用模拟数据训练、以及结合真实和模拟数据集训练。这些代理与一个基线模型（未经过额外训练的基础Gemini Flash和SDXL模型）进行了比较，评估指标包括Pick-a-Pic准确性、Spearman秩相关、选择模型准确性和跨轮次准确性。\n\n**主要发现**：\n\n*   仅使用合成数据训练的PASTA未能超越基线模型。\n*   仅使用真实人类数据训练的PASTA表现出显著改进，但仍未超越基线。\n*   **结合真实和模拟数据训练的PASTA表现最佳**，这证实了用户模拟器成功捕捉了人类交互的关键动态，并提供了强化学习训练所需的规模。\n*   在直接比较中，**85%的评估者更喜欢PASTA生成的最终图像**，尤其是在处理抽象提示词时（如“爱的图像”、“幸福的图像”），PASTA能根据不同用户类型生成截然不同的、高度个性化的结果。\n\n    ![PASTA-3](https://storage.googleapis.com/gweb-research2023-media/images/PASTA-3.width-1250.png)\n    *图示：训练后的用户模型准确性性能（y轴）与考虑的用户类型数量（x轴）的关系。上排显示模型在Pick-a-Pic测试集上的准确性（左）和在HPS测试集上的Spearman秩相关（右）。下排显示模型的选择准确性（左）和跨轮次偏好准确性（右），均在人类评估测试集上进行评估。*\n\n    ![PASTA-4](https://storage.googleapis.com/gweb-research2023-media/images/PASTA-4.width-1250.png)\n    *图示：从相同的起始提示“幸福的图像”开始，PASTA为两种不同的用户类型（用户类型A和用户类型B）生成了截然不同的结果，展示了其适应个体独特创意风格的能力。*\n\n### 未来展望\n\nPASTA的研究表明，生成式AI的未来将更加互动、适应偏好和协作。所开发的方法，特别是强大的用户模拟器，可应用于许多其他生成任务，以创建更符合和适应人类用户的AI。为了促进进一步研究，该团队已开源了其序列评估者数据集和模拟用户数据。",
      "shortSummary": "PASTA（偏好自适应和序列文本到图像代理）是一种强化学习代理，旨在通过协作对话解决文本到图像模型难以捕捉用户细微创意意图的问题。它通过结合真实人类反馈和大规模用户模拟进行训练，学习并适应用户的独特偏好。PASTA能够逐步优化图像生成结果，用户评估显示其生成的图像比基线模型更令人满意。该研究表明，生成式AI的未来将更加互动、适应偏好和协作。",
      "translated_title": "图像生成中的协作方法",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PASTA-1.width-1250.png",
          "alt": "PASTA-1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PASTA-3.width-1250.png",
          "alt": "PASTA-3",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PASTA-4.width-1250.png",
          "alt": "PASTA-4",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"cmj9x\">You have a perfect image in your mind. You enter a prompt, hit generate, and the result is close to what you were thinking, but not quite right. You try refining the prompt, adding more detail, but you can't seem to bridge the gap between your idea and the final image.</p><p data-block-key=\"bkhg8\">This is a common experience. While <a href=\"https://en.wikipedia.org/wiki/Text-to-image_model\" target=\"_blank\" rel=\"noopener noreferrer\">text-to-image</a> (T2I) models are incredibly powerful, they often struggle to capture the nuance and specificity of an individual's unique creative intent given just a single prompt. What if we could turn image generation into a collaborative conversation?</p><p data-block-key=\"34fn3\">In this post, we describe our research “<a href=\"https://arxiv.org/abs/2412.10419\" target=\"_blank\" rel=\"noopener noreferrer\">Preference Adaptive and Sequential Text-to-image Agent</a>” (PASTA), a reinforcement learning (RL) agent that collaborates with users to progressively refine T2I results. This approach eliminates the need for users to rely on trial-and-error prompt refinement to reach a desirable image. Through human evaluations, we created a novel dataset of sequential preferences, which we then used to compare PASTA with a baseline state-of-the-art model. The results demonstrated that PASTA, trained with our mix of real and simulated data, consistently produced images that users rated as more satisfying. We’ve also released our <a href=\"https://www.kaggle.com/datasets/googleai/pasta-data\" target=\"_blank\" rel=\"noopener noreferrer\">foundational dataset</a> with a collection of over 7,000 human rater interactions with PASTA.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How PASTA works</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"cmj9x\">To effectively train an AI agent to adapt to a user's individual preferences, a large, diverse set of interaction data is needed. However, gathering this data from real users is challenging due to several factors, including user privacy. To address this, we trained PASTA using a two-stage strategy that combines real human feedback with large-scale user simulation.</p><p data-block-key=\"cnmkf\">First, we collected a <a href=\"https://www.kaggle.com/datasets/googleai/pasta-data\" target=\"_blank\" rel=\"noopener noreferrer\">high-quality foundational dataset</a> with over 7,000 raters' sequential interactions. These interactions included prompt expansions generated by a <a href=\"https://arxiv.org/pdf/2403.05530\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini Flash</a> large multimodal model and corresponding images generated by a <a href=\"https://arxiv.org/pdf/2307.01952\" target=\"_blank\" rel=\"noopener noreferrer\">Stable Diffusion XL</a> (SDXL) T2I model. This initial seed of authentic preference data was then used to train a user simulator, designed to generate additional data that replicate real human choices and preferences.</p><p data-block-key=\"f45gr\">At the heart of our method is a user model, comprising two key components: 1) a utility model that predicts the degree to which a user will like any set of images, and 2) a choice model that predicts which set of images they will select when presented with several sets. We constructed the user model using pre-trained <a href=\"https://arxiv.org/abs/2103.00020\" target=\"_blank\" rel=\"noopener noreferrer\">CLIP encoders</a> and added user-specific components. We trained the model using an <a href=\"https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm\" target=\"_blank\" rel=\"noopener noreferrer\">expectation-maximization</a> algorithm that allows us to simultaneously learn the specifics of user preferences while also discovering latent “user types,” that is, clusters of users with similar tastes (e.g., tendencies to prefer images with animals, scenic views, or abstract art).</p><p data-block-key=\"bpgbn\">The trained user simulator can provide feedback and express preferences on generated images, and make selections from sets of proposed images. This allows us to generate over 30,000 simulated interaction trajectories.. Our approach does more than just create more data; it gives us a controlled environment in which to explore a vast range of user behaviors so we can train the PASTA agent to effectively collaborate with users.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PASTA-1.width-1250.png\" alt=\"PASTA-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PASTA-1.width-1250.png\" alt=\"PASTA-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"00pr5\"><i>Our user simulator learns to identify distinct user types from preference data. Each row shows the top-rated images for an emergent user profile, revealing clear preferences for categories like \"Animals\" or \"Food.\"</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"cmj9x\">With this robust, data-driven foundation, the PASTA agent is trained to effectively engage with arbitrary users to generate images that match their preferences. The agent itself is a <a href=\"https://arxiv.org/abs/1312.5602\" target=\"_blank\" rel=\"noopener noreferrer\">value-based reinforcement learning</a> model that learns to select the best \"slate\" of prompt expansions (i.e., elaborations of the current prompt used to generate subsequent images) to show the user at each turn. Its goal is to maximize the user's cumulative satisfaction over the entire interaction.</p><p data-block-key=\"da0lr\">Once PASTA is trained and deployed, a user initiates the engagement with an initial prompt. PASTA first uses a candidate generator (a large multimodal model) to create a diverse set of potential prompt expansions. Then, a candidate selector (our trained RL agent) selects the optimal slate of four such expansions, which are used to generate corresponding images to present to the user. The user selects the image that is closest to their vision, which provides feedback that guides PASTA's next set of suggestions. This collaborative back-and-forth allows the model to learn the user's preferences on the fly, steering the creative process toward their ideal goal with each step.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/PASTA-mp4.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"00pr5\"><i>Starting with a simple prompt for \"A white cat\", PASTA engages the user in a visually grounded dialogue. The user's selections (highlighted in blue) help the agent quickly learn their preference for a more fantastical and colorful style.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Putting PASTA to the test</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"cmj9x\">To evaluate our approach, we trained PASTA as a value-based reinforcement learning agent using <a href=\"https://arxiv.org/abs/2110.06169\" target=\"_blank\" rel=\"noopener noreferrer\">implicit Q-learning</a> (IQL). We specifically wanted to see how the use of different training data impacted performance. We created three versions of the agent: 1) trained only on the real volunteer-rater data, 2) trained only on the simulated data, and 3) trained on a combination of real and simulated datasets.</p><p data-block-key=\"p6im\">We then ran a series of human evaluations comparing these agents to a baseline model (i.e., base Gemini Flash and SDXL models with no additional training) across four metrics: accuracy over the <a href=\"https://proceedings.neurips.cc/paper_files/paper/2023/file/73aacd8b3b05b4b503d58310b523553c-Paper-Conference.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Pick-a-Pic</a> dataset, <a href=\"https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient\" target=\"_blank\" rel=\"noopener noreferrer\">Spearman’s rank correlation</a>, choice model accuracy, and cross turn accuracy. Pick-a-Pic accuracy and Spearman's rank correlation assess the model's ability to predict user preferences and rankings on existing, large-scale, single-turn datasets. Choice model accuracy and cross-turn accuracy measure the model's ability to predict which image a user will choose at a given turn and whether the selected images are an improvement over the previous turn, respectively.</p><p data-block-key=\"cp9o4\">The results demonstrated that training PASTA on synthetic data alone didn't beat the baseline and while the agent trained on real human data showed significant improvement, it also didn’t outperform the baseline. However, the agent trained on the combination of both real and simulated data offered the best performance, confirming that our user simulation successfully captures key dynamics of human interaction while providing the scale needed for robust RL training.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PASTA-3.width-1250.png\" alt=\"PASTA-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PASTA-3.width-1250.png\" alt=\"PASTA-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"00pr5\"><i>The graphs above present the accuracy performance of a trained user model (y axis) as a function of the number of user types considered (x axis). The top row displays the model’s accuracy on the Pick-a-Pic test set (</i><b><i>left</i></b><i>) and its Spearman’s rank correlation on the</i> <a href=\"https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Human_Preference_Score_Better_Aligning_Text-to-Image_Models_with_Human_Preference_ICCV_2023_paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\"><i>HPS test</i></a><i> set (</i><b><i>right</i></b><i>). The bottom row shows the model’s choice accuracy (</i><b><i>left</i></b><i>) and cross-turn preference accuracy (</i><b><i>right</i></b><i>), both evaluated on our human-rated test set</i>.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"cmj9x\">When we asked raters to directly compare the final images from our best-performing agent against the baseline, 85% preferred PASTA's generated images. The difference is especially striking with abstract prompts. Starting with a simple idea like \"an image of love\", PASTA adapted to different user types to create a wide variety of results, from tender portraits to abstract, geometric art.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PASTA-4.width-1250.png\" alt=\"PASTA-4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PASTA-4.width-1250.png\" alt=\"PASTA-4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"00pr5\"><i>With the same starting prompt, \"An image of happiness\", PASTA produces dramatically different results for two distinct user types (User Type A and User Type B), showcasing its ability to adapt to an individual's unique creative style. For example, the result for Type A corresponds to a prompt like “Abstract happy faces, Art Deco inspired geometric shapes, muted jewel-toned background.”</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">What's next?</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"cmj9x\">PASTA shows that the future of generative AI can be more interactive, preference adaptive, and collaborative. The methods we developed, particularly the use of robust user simulators, can be applied to many other generative tasks to create AI that better aligns and adapts to human users.</p><p data-block-key=\"1ads2\">To help spur further research, we have <a href=\"https://www.kaggle.com/datasets/googleai/pasta-data\" target=\"_blank\" rel=\"noopener noreferrer\">open-sourced</a> our sequential rater dataset and our simulated user data. We can't wait to see what the community builds with it.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"cmj9x\"><i>The author list is:</i> <i>Ofir Nabati,</i> <i>Guy Tennenholtz,</i> <i>ChihWei Hsu,</i> <i>Moonkyung Ryu,</i> <i>Deepak Ramachandran,</i> <i>Yinlam Chow,</i> <i>Xiang Li, and</i> <i>Craig Boutilier. Special thanks to Mark Simborg for his help crafting this blog post and Kimberly Schwede for creating the figures in this post.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Snapseed 推出交互式设备端分割功能 (原标题: Introducing interactive on-device segmentation in Snapseed)",
      "link": "https://research.google/blog/introducing-interactive-on-device-segmentation-in-snapseed/",
      "pubDate": "Tue, 30 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-30T16:00:00.000Z",
      "creator": "Google",
      "summary": "Snapseed 引入了名为“对象画笔”（Object Brush）的新功能，旨在简化移动设备上的选择性图像调整。这项功能通过强大的设备端人工智能模型“交互式分割器”（Interactive Segmenter）实现，使用户能够通过简单的笔触或点击，快速准确地选择并编辑照片中的特定对象。\n\n### 核心功能与用户体验\n\n*   **直观的对象编辑**：用户只需在想要编辑的对象上绘制一笔，模型便会在20毫秒内立即检测并选择完整的对象，生成精确匹配其边界的掩膜。这使得在不影响图像其他部分的情况下，对前景主体、天空或特定物品进行亮度、颜色等调整变得轻而易举。\n*   **实时反馈**：模型提供实时反馈，用户可以即时添加或删除选区，直到达到理想效果。\n*   **设备端运行**：整个过程完全在设备上运行，由 MediaPipe 和 LiteRT 的 GPU 加速提供支持，确保了快速流畅的体验。\n\n### 交互式分割器模型的技术原理\n\n“交互式分割器”旨在成为一个通用分割模型，不限于特定类别的对象或场景。其开发过程涉及以下关键步骤：\n\n1.  **师生模型训练（Teacher-Student Training）**：\n    *   **“交互式分割器：教师”（Interactive Segmenter: Teacher）**：首先，团队使用一个预训练且高度泛化的模型，通过对350多种不同对象类别的约30,000个高质量像素级图像掩膜进行微调，训练出一个高精度的交互式分割模型。该模型质量高，但由于速度和大小限制，不适合设备端使用。\n    *   **“交互式分割器：边缘”（Interactive Segmenter: Edge）**：为了实现设备端应用，团队开发了一个更小、更专业的模型。该模型通过知识蒸馏（Knowledge Distillation）从“教师”模型中学习，利用包含200多万张图像和数百个不同类别的弱标注数据集进行训练。在蒸馏过程中，“教师”模型会实时生成高质量的真值掩膜，并模拟用户提示（如前景涂鸦、背景涂鸦、点和框选），以训练“边缘”模型。\n\n    ![Table comparing Interactive Segmenter: Edge and Interactive Segmenter: Teacher models using the IOU metric.](https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-3-IOU.width-1250.png)\n    *   **提示生成**：为了模拟用户选择对象，模型在训练时会在真值掩膜内部绘制随机涂鸦作为前景提示，在外部绘制随机涂鸦作为背景提示，并模拟点击和套索选择。\n\n    ![Schematic of teacher–student training for Interactive Segmenter.](https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-4a-Training.width-1250.png)\n\n2.  **高精度与低延迟的平衡**：\n    *   为了在分割质量和实时交互延迟之间取得平衡，模型将图像理解和提示理解解耦为两个独立的子模型。\n    *   **重量级图像编码器**：对每张图像只运行一次，提取丰富的语义特征，并在用户开始使用交互式分割时立即运行，有效隐藏了延迟。\n    *   **轻量级交互式编码器-解码器**：在此预计算的特征上运行，接收用户的触摸提示并在20毫秒预算内生成最终分割掩膜。\n\n    ![Schematic of the Interactive Segmenter neural network architecture.](https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-5-Architecture.width-1250.png)\n    ![Table showing model inference latency when running Interactive Segmenter: Edge on-device.](https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-6-Latency.width-1250.png)\n\n3.  **图像尺寸掩膜上采样**：\n    *   为在高分辨率图像上保持最佳编辑质量，模型预测768x768分辨率的掩膜，并通过高效的 GPU 实现边缘保留联合双边上采样方法，将其上采样至图像原始分辨率（最高4K）。\n    *   为提高延迟，上采样仅在用户完成手势（抬起手指）后应用。\n\n    ![Comparison of original Interactive Segmenter mask and upsampled mask.](https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-7-Upsampling.width-1250.png)\n\n### 结论与展望\n\n“交互式分割器”的推出，使 Snapseed 的图像编辑变得前所未有的简单和强大。它将简单的点击和笔触转化为精确的选择，帮助用户轻松实现编辑想法。这项底层技术不仅已应用于 Chromebook Plus 14 的图库应用中的 AI 图像编辑功能，Google 还计划将其集成到更多图像和创意编辑产品中。",
      "shortSummary": "Snapseed 推出“对象画笔”功能，通过设备端 AI 模型“交互式分割器”实现快速、直观的对象选择和编辑。用户只需简单笔触，模型便能在20毫秒内精确分割图像中的对象。该技术采用师生模型蒸馏训练，平衡了分割质量与实时性，并支持高分辨率掩膜。这项创新使高级照片编辑更易用，并计划集成到更多 Google 图像和创意编辑产品中，包括已应用于 Chromebook Plus 14。",
      "translated_title": "Snapseed 推出交互式设备端分割功能",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-3-IOU.width-1250.png",
          "alt": "Table comparing Interactive Segmenter: Edge and Interactive Segmenter: Teacher models using the IOU metric.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-4a-Training.width-1250.png",
          "alt": "Schematic of teacher–student training for Interactive Segmenter.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-5-Architecture.width-1250.png",
          "alt": "Schematic of the Interactive Segmenter neural network architecture.",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-6-Latency.width-1250.png",
          "alt": "Table showing model inference latency when running Interactive Segmenter: Edge on-device.",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-7-Upsampling.width-1250.png",
          "alt": "Comparison of original Interactive Segmenter mask and upsampled mask.",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wunwg\">The key to elevating a good photo often lies in selective image adjustments: brightening a subject in the foreground, enhancing the sky, or making the color of a jacket pop. Yet, isolating specific elements with existing tools that offer subject, background, sky, or color-based selections has remained a frustrating and complex endeavor. This challenge has been particularly acute on mobile devices, where imprecise touch input and limited processing have made detailed selections and edits very difficult.</p><p data-block-key=\"978qd\">Now, we have made object-based image adjustments quick and easy. The new Object Brush in <a href=\"https://apps.apple.com/us/app/snapseed-photo-editor/id439438619\" target=\"_blank\" rel=\"noopener noreferrer\">Snapseed</a> on iOS, accessible in the \"Adjust\" tool, now lets you edit objects intuitively. It allows you to simply draw a stroke on the object you want to edit and then adjust how you want it to look, separate from the rest of the image. Give it a try as we roll this new capability out in the coming week!</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Snapseed-1-ObjBrush.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Selective editing using Snapseed's Object Brush.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Intuitive editing through interactive on-device segmentation</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">At its core, Object Brush is powered by our Interactive Segmenter, a powerful AI model that runs entirely on device. With a simple gesture — just a tap or tracing a quick line — you can choose an object or person in the frame. The model will then immediately detect and select the complete object or person, in less than 20ms. The model generates a mask for the object, which accurately matches its boundaries, whether it's a person, a pet, or the clouds in the sky. This real-time feedback lets you refine your selection on the fly, easily adding or subtracting areas until it's just right. This entire process is powered by <a href=\"https://ai.google.dev/edge/mediapipe/framework\" target=\"_blank\" rel=\"noopener noreferrer\">MediaPipe</a> and <a href=\"https://ai.google.dev/edge/litert/performance/gpu\" target=\"_blank\" rel=\"noopener noreferrer\">LiteRT’s GPU acceleration</a> for a fast and seamless experience.</p><p data-block-key=\"c4tan\">This powerful fusion of a simple, intuitive user interface with an effective and efficient machine learning model makes advanced photo editing more accessible, enjoyable, and more precise than ever before, all running seamlessly on your own device.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Snapseed-0-Hero.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Use foreground prompts (green) to select parts of an image and background prompts (red) to refine the selection.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Training the Interactive Segmenter model</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">The Interactive Segmenter model is designed to be a universally capable segmentation model, not limited to any specific class of objects or scenes. To avoid having to annotate large amounts of data to cover all areas, we chose to follow the <a href=\"https://arxiv.org/abs/1912.11370\" target=\"_blank\" rel=\"noopener noreferrer\">Big Transfer</a> approach and use a general pre-trained image encoder for pseudo-annotation to complement small amounts of manually annotated images.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Teacher for Interactive Segmenter</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">We started with a pre-trained and highly-generalizable model, fine-tuned for interactive segmentation. We took samples for 350+ different object categories and asked annotators to precisely annotate object masks with pixel-perfect quality. Through this process, we obtained ~30,000 high-quality image masks for these categories. While insufficient for direct training of a small mobile model, large pre-trained models can successfully be fine-tuned on this data to predict high accuracy masks. Using this dataset we trained an interactive segmentation model, which we call “Interactive Segmenter: Teacher”.</p><p data-block-key=\"2dclt\">Interactive Segmenter: Teacher produces high-quality segmentation masks; however, its speed and size hinder its use in on-device scenarios. To overcome this challenge, we developed “Interactive Segmenter: Edge”, a specialized model tailored for on-device use cases by leveraging the knowledge distilled from the original Interactive Segmenter: Teacher model.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Distillation</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">Since the on-device model is significantly smaller, it has limited generalization capabilities, and the 30,000 annotated images we used for fine-tuning aren't sufficient to train a new model. At the same time the small model size implies we won’t see significant gains from pre-training on different domains or tasks.</p><p data-block-key=\"bqvot\">For knowledge transfer from Interactive Segmenter: Teacher to Interactive Segmenter: Edge, we need millions of images and realistic prompts for a diverse range of object categories. So, we leveraged a large, weakly annotated dataset, which contains over 2 million images with masks across hundreds of different categories.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-3-IOU.width-1250.png\" alt=\"Table comparing Interactive Segmenter: Edge and Interactive Segmenter: Teacher models using the IOU metric.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-3-IOU.width-1250.png\" alt=\"Table comparing Interactive Segmenter: Edge and Interactive Segmenter: Teacher models using the IOU metric.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Interactive Segmenter: Edge yields a similar quality as Interactive Segmenter: Teacher for a given, fixed input prompt, as measured by the intersection over union (IOU) metric.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Prompt generation</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">The segmentation masks in the distillation dataset are not pixel-perfect, because they were generated through <a href=\"https://arxiv.org/pdf/1903.10830\" target=\"_blank\" rel=\"noopener noreferrer\">automated or semi-automated procedures</a>, and are not ideal for training high-quality segmenters. Nevertheless, they are suitable for creating realistic prompts for interactive segmentation. In this process, the ground truth mask is produced on-the-fly by Interactive Segmenter: Teacher, which acts as a teacher model in a process known as <a href=\"https://arxiv.org/abs/2106.05237\" target=\"_blank\" rel=\"noopener noreferrer\">knowledge distillation</a>. Importantly, both the teacher as well as the student model use the same prompts during training, ensuring consistency across models.</p><p data-block-key=\"6276u\">We attempt to simulate a user selecting objects in an image. We draw random scribbles within the (eroded) ground truth mask to get foreground prompts (i.e., what the user wants to select, shown in red in the image below) and random scribbles outside the ground truth mask to get background prompts (i.e., what the user explicitly does not want to select, shown in blue). We simulate tapping by drawing random points as well as random scribbles. Furthermore, to support lasso selection we also expose the model during training to box prompts around an object.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-4a-Training.width-1250.png\" alt=\"Schematic of teacher–student training for Interactive Segmenter.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-4a-Training.width-1250.png\" alt=\"Schematic of teacher–student training for Interactive Segmenter.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">By utilizing a teacher model we can train on data with low-quality ground truth annotations, reducing labeling costs without sacrificing model quality.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">High quality vs. low latency</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">A central challenge was reconciling the conflicting demands of segmentation quality versus real-time, interactive latency. To reach the right balance, we decouple image and prompt understanding into distinct sub-models. First, a powerful, heavyweight image encoder is run once per image to extract a rich set of semantic features. This image encoder can be run as soon as the user’s intent to use interactive segmentation becomes apparent, thus effectively hiding the latency from the user. Second, a lightweight interactive encoder-decoder operates on these pre-computed features. This network takes the user's touch prompts and generates the final segmentation mask, executing well under our 20ms budget. This separation into two models allows Interactive Segmenter to harness the image understanding of a large model while delivering the instantaneous responsiveness of a small one.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-5-Architecture.width-1250.png\" alt=\"Schematic of the Interactive Segmenter neural network architecture.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-5-Architecture.width-1250.png\" alt=\"Schematic of the Interactive Segmenter neural network architecture.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Interactive Segmenter neural network architecture.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-6-Latency.width-1250.png\" alt=\"Table showing model inference latency when running Interactive Segmenter: Edge on-device.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-6-Latency.width-1250.png\" alt=\"Table showing model inference latency when running Interactive Segmenter: Edge on-device.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Model inference latency when running Interactive Segmenter: Edge on-device.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wunwg\">The final student models (encoder + super decoder) are quantized to 8 bits and both run on <a href=\"https://ai.google.dev/edge/litert/performance/gpu\" target=\"_blank\" rel=\"noopener noreferrer\">LiteRT's GPU acceleration</a> with decoder inference latencies of 7.4ms on an iPhone 16 Pro, enabling seamless and intuitive image editing.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Image-size mask upsampling</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">To preserve the best image editing quality on high-resolution images, we need high-resolution segmentation masks. To achieve this, we train our segmentation model to predict a mask in 768x768 resolution and further upsample it to image resolution (capped at 4k to have it fit within a single GPU buffer). We use an efficient GPU implementation of the <a href=\"https://dl.acm.org/doi/10.1145/1276377.1276497\" target=\"_blank\" rel=\"noopener noreferrer\">edge-preserving joint-bilateral upsampling method</a>. To improve latency, we only apply upsampling once a user completes a gesture by lifting their finger.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-7-Upsampling.width-1250.png\" alt=\"Comparison of original Interactive Segmenter mask and upsampled mask.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Snapseed-7-Upsampling.width-1250.png\" alt=\"Comparison of original Interactive Segmenter mask and upsampled mask.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"73ip4\">Original Interactive Segmenter mask (<b>left</b>) and upsampled mask (<b>right</b>).</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\">With the new Interactive Segmenter in <a href=\"https://apps.apple.com/us/app/snapseed-photo-editor/id439438619\" target=\"_blank\" rel=\"noopener noreferrer\">Snapseed</a> image editing has become easier and more powerful than ever. Simple taps and strokes are translated into accurate selections, allowing users to translate their editing ideas into reality. Download Snapseed for iOS <a href=\"https://apps.apple.com/us/app/snapseed-photo-editor/id439438619\" target=\"_blank\" rel=\"noopener noreferrer\">here</a> and let your photos shine. Object Brush will be rolled out to more tools in Snapseed in the coming months. The underlying model powers a wide range of image editing and manipulation tasks and serves as a foundational technology for intuitive selective editing. It has also been shipped in the new <a href=\"https://blog.google/products/chromebooks/lenovo-chromebook-plus-14/\" target=\"_blank\" rel=\"noopener noreferrer\">Chromebook Plus 14</a> to power AI image editing in the Gallery app. Next, we plan to integrate it across more image and creative editing products at Google.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wunwg\"><i>Special thanks to all members who worked on the tech with us: Valentin Bazarevsky, Daniel Fenner, Lutz Justen, Ronald Wotzlaw, Tai-Yu Daniel Pan, Jason Chang, Matthew Harries, Giles Ochs, Jonathan Horsman, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Karthik Raveendran, Matsvei Zhdanovich, Mogan Shieh, Chris Parsons, Jianing Wei, and Matthias Grundmann.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "AI作为研究伙伴：利用AlphaEvolve推进理论计算机科学 (原标题: AI as a research partner: Advancing theoretical computer science with AlphaEvolve)",
      "link": "https://research.google/blog/ai-as-a-research-partner-advancing-theoretical-computer-science-with-alphaevolve/",
      "pubDate": "Mon, 29 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-29T16:00:00.000Z",
      "creator": "Google",
      "summary": "大型语言模型（LLMs）在竞技数学和编程中展现出卓越能力，但在数学发现（如证明新定理或揭示新组合结构）方面相对较少成功，因为数学和理论计算机科学要求绝对的正确性。任何AI驱动的数学发现方法都必须提供可计算验证的正确性证明，或由领域专家进行认证。\n\n## AlphaEvolve：LLM驱动的数学发现工具\n\nGoogle DeepMind的最新研究论文《组合结构强化生成：在复杂性理论中的应用》展示了LLM驱动的编码智能体如何帮助发现新的数学结构，从而推动复杂性理论（理论计算机科学的一个子领域）的理解边界。这项工作利用了AlphaEvolve系统，该系统通过以下方式迭代进化代码：\n\n*   **反馈循环：** AlphaEvolve从代码片段集合开始，评估这些代码片段生成的结构。\n*   **LLM驱动的优化：** 利用LLM将最成功的代码片段演变为更好的解决方案。\n\n这种方法在复杂性理论的两个不同领域取得了新成果：\n\n1.  改进了MAX-4-CUT问题（将节点划分为四个集合的最大割问题）近似不可行性（即我们近似结果能力的极限）的最新界限。\n2.  收紧了认证随机图性质的平均情况硬度界限。\n\n## AI辅助数学研究的模式\n\nAI辅助数学研究主要有两种模式：\n\n*   **LLM直接生成：** 人类调用LLM来总结文献、规划新定理的研究路线，或直接生成部分（或全部）证明。\n*   **AI工具辅助生成：** 人类使用AlphaEvolve等AI工具生成更好的证明元素。本文的工作属于第二类，AlphaEvolve生成的证明元素可以由计算机程序自动验证。\n\n## 提升的力量：从有限构造到普遍陈述\n\n将AI用于理论计算机科学研究的一个根本挑战在于所研究问题的普遍性。AI系统可能找到特定问题实例的解决方案（例如，50个特定城市的旅行推销员最优路线），但计算机科学家通常寻求对所有问题实例和大小都普遍成立的定理（表示为∀n）。\n\n为了使AlphaEvolve能够证明普遍陈述，研究采用了“提升”（lifting）技术。如果将证明视为一个长字符串，可以将证明中的一个片段（对应于某个有限结构）进行演化，以支持一个更强的普遍陈述，同时保持与证明其余部分的接口不变。这种方法的优势在于，要认证整体正确性，只需认证已演化的有限结构的正确性。\n\n![提升：使用AI对有限结构进行变形，同时保持与更广泛证明的接口不变。](https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-1-Lifting.width-1250.png)\n\n在复杂性理论中，研究人员经常使用依赖于特定高度优化有限结构的既定证明框架。如果能找到更好的结构，整个证明框架就会将这种改进“提升”为一个更好的普遍结果。一个关键例子是“小工具归约”（gadget reduction），通过将已知难解的源问题映射到目标问题来证明目标问题的计算难度。小工具是局部转换源问题片段到目标问题片段的方法，它们是有限结构，找到最优小工具通常是一个艰苦的手工过程。通过让AlphaEvolve寻找更好的小工具，研究人员发现了比以往已知更复杂的结构。这些有限的发现，当插入到现有数学框架中时，立即产生了复杂性理论中的新普遍定理。\n\n## 复杂性理论中的新定理\n\n### MAX-4-CUT：新的最先进成果\n\n研究人员将此方法应用于MAX-k-CUT问题。给定一个图，目标是将节点划分为k个不同的集合，使不同集合之间交叉的边数最大化。这是一个经典的难解（NP-hard）问题，因此研究重点是近似算法。关键问题是：近似的极限是什么？\n\n对于MAX-4-CUT（划分为四个集合），之前最佳结果证明其近似解的因子为0.9883是NP-hard。AlphaEvolve被部署来寻找MAX-4-CUT的新小工具归约。系统发现了一个涉及19个变量（节点）的复杂小工具，具有复杂的加权方案。这一发现确立了新的近似不可行性界限为0.987。尽管这一改进看似微小，但在近似硬度这一成熟领域，此类进展通常需要重大的新技术或组合洞察力。\n\n![AlphaEvolve为MAX-4-CUT归约发现的小工具的图示。](https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-2-Gadget.width-1250.png)\n\n### 平均情况硬度和Ramanujan图\n\n研究还探讨了问题在平均情况下的硬度，而非最坏情况。具体来说，研究了稀疏随机图的MAX-2-CUT（以及最大独立集）界限认证的难度。最近的工作将此问题与特定Ramanujan图的存在联系起来——这些确定性图“看起来”像稀疏随机图。他们推测，具有异常大割的Ramanujan图的存在意味着认证随机图的MAX-2-CUT在计算上是困难的。先前的工作使用计算机辅助找到了多达10个节点的此类图。改进他们的结果需要找到更多节点上更极端的Ramanujan图，这极其难以发现和验证。AlphaEvolve成功地在这一广阔的搜索空间中导航，发现了多达163个节点上具有更大割的Ramanujan图。\n\n![AlphaEvolve发现的具有大2-割的4-正则Ramanujan图。](https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-3-4RegGraph.width-1250.png)\n\n这些发现显著改善了平均情况硬度的下限。此外，结合新的算法进展（非AI驱动），研究人员能够将这些计算硬度问题几乎解决，将上限和下限匹配到小数点后第三位。\n\n## 验证正确性的关键作用\n\n这项工作的一个关键区别在于其结果附带了正确性证明。当LLM被直接提示生成数学证明时，它通常会产生一个需要大量人工干预才能验证和完成的证明草图或论证。幻觉或细微错误可能使输出无用。数学的正确性标准是绝对的。相比之下，这里采用的方法是利用AI发现证明中的结构，而不是证明本身。最终定理的有效性依赖于两个组成部分：提升框架的正确性，以及所发现结构的验证。虽然框架是健全的，但验证AlphaEvolve发现的结构在计算上是密集的。值得注意的是，AlphaEvolve通过实施复杂的剪枝（branch-and-bound）策略和系统级优化，将验证过程加速了10,000倍。这一巨大的加速是研究的关键推动因素，使得系统能够探索更大、更复杂的小工具。最重要的是，最终发现的小工具仍然使用原始的暴力算法进行验证，确保了定理的绝对正确性。\n\n## AI辅助理论的未来\n\n尽管这些初步研究结果远非结论性的，但它们表明AI有望成为数学发现中一个有益的合作者。研究人员观察到AlphaEvolve中的模型生成了复杂的数学对象，有时展现出初步的推理能力。然而，随着我们进入一个证明可能越来越多地归因于AI的时代，验证这项关键任务将成为一个重要的瓶颈。",
      "shortSummary": "Google DeepMind的AlphaEvolve系统利用大型语言模型（LLM）作为研究伙伴，通过迭代进化代码，在理论计算机科学领域取得了显著进展。该系统通过“提升”技术，将有限结构演化为更强的通用定理。具体而言，它改进了MAX-4-CUT问题的近似不可行性界限至0.987，并发现了具有更大割的Ramanujan图，从而收紧了稀疏随机图平均情况硬度的界限。所有AI发现的结构都经过严格计算验证，确保了数学结果的绝对正确性，展现了AI在数学发现中的巨大潜力。",
      "translated_title": "AI作为研究伙伴：利用AlphaEvolve推进理论计算机科学",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-1-Lifting.width-1250.png",
          "alt": "Lifting: Morphing finite structures using AI, while keeping the interface to the broader proof intact.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-2-Gadget.width-1250.png",
          "alt": "Drawing of a graph representing the gadget found by AlphaEvolve for the reduction to MAX-4-CUT.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-3-4RegGraph.width-1250.png",
          "alt": "Drawing of a 4-regular Ramanujan graph with large 2-cut found by AlphaEvolve.",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"c47sg\">Recently, large language models (LLMs) have demonstrated surprising capabilities in <a href=\"https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/\" target=\"_blank\" rel=\"noopener noreferrer\">competitive mathematics</a> and <a href=\"https://deepmind.google/discover/blog/gemini-achieves-gold-level-performance-at-the-international-collegiate-programming-contest-world-finals/\" target=\"_blank\" rel=\"noopener noreferrer\">competitive programming</a>, demonstrating world-leading performance across both of these fields. However, their successes in mathematical discovery — proving novel theorems or uncovering new combinatorial structures — have been relatively few (with some notable exceptions [<a href=\"https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/\" target=\"_blank\" rel=\"noopener noreferrer\">1</a>, <a href=\"https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/\" target=\"_blank\" rel=\"noopener noreferrer\">2</a>, <a href=\"https://arxiv.org/pdf/2505.20219#page=8.61\" target=\"_blank\" rel=\"noopener noreferrer\">3</a>]). Since mathematics and theoretical computer science demand absolute correctness<footnote id=\"94fb5494-00a3-46e9-8265-43676afdf080\">[94fb54]</footnote>, any AI-based method that makes mathematical discovery must either have a proof of correctness that can be confirmed computationally (without any human involvement), or have a domain-expert human in the loop to certify correctness.</p><p data-block-key=\"ebg0u\">In our recent paper, “<a href=\"https://arxiv.org/abs/2509.18057\" target=\"_blank\" rel=\"noopener noreferrer\">Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory</a>”, we demonstrate how an LLM-powered coding agent can help discover new mathematical structures that push the boundaries of our understanding of <a href=\"https://en.wikipedia.org/wiki/Computational_complexity_theory\" target=\"_blank\" rel=\"noopener noreferrer\">complexity theory</a> (a sub-field of theoretical computer science). Our work utilizes <a href=\"https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/\" target=\"_blank\" rel=\"noopener noreferrer\">AlphaEvolve</a>, a system developed at Google DeepMind that uses LLMs to iteratively evolve code. By employing a feedback loop, AlphaEvolve began with populations of code snippets, evaluated the structures produced by the code snippets, and used an LLM to morph the most successful snippets toward better solutions. This approach led to new results in two distinct areas of complexity theory: 1) improving the state-of-the-art for the limit on our ability to approximate the outcome (i.e., the \"inapproximability\") of the <a href=\"https://en.wikipedia.org/wiki/Maximum_cut\" target=\"_blank\" rel=\"noopener noreferrer\">maximum cut problem</a> for 4 slices (which we define as the <a href=\"https://arxiv.org/pdf/2509.18057#page=8\" target=\"_blank\" rel=\"noopener noreferrer\">MAX-4-CUT problem</a>), and 2) tightening the bounds on the <a href=\"https://arxiv.org/pdf/2509.18057#page=6\" target=\"_blank\" rel=\"noopener noreferrer\">average-case hardness of certifying properties of random graphs</a>.</p><p data-block-key=\"1oij\">AI-assisted mathematical research can operate in the following modes:</p><ol><li data-block-key=\"7g1t3\">A person invokes an LLM to summarize the literature, to chart a research plan towards new theorems, or to directly generate chunks of (or entire) proofs.</li><li data-block-key=\"30u56\">A person uses AI-derived tools, such as AlphaEvolve, to generate better proof elements.</li></ol><p data-block-key=\"duvp5\">Our work falls in the second category, where we obtain better proof elements using AlphaEvolve that can be automatically verified by a computer program.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The power of lifting: From finite constructions to universal statements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c47sg\">A fundamental challenge in using AI for theoretical computer science research lies in the universal nature of the problems studied. An AI system might find a solution to a specific instance of a problem — say, the <a href=\"https://en.wikipedia.org/wiki/Travelling_salesman_problem\" target=\"_blank\" rel=\"noopener noreferrer\">optimal route for a traveling salesman</a> visiting 50 specific cities. However, computer scientists often seek theorems that hold true universally for <i>all</i> problem instances and sizes (denoted as ∀n).</p><p data-block-key=\"2fqu\">How can we use AlphaEvolve to prove a universal statement? The answer lies in a technique known as \"lifting\" (see image below). If a proof is viewed as a long string, then one can take a chunk of the proof (corresponding to a certain finite structure), and evolve it to support a stronger universal statement, while keeping the interface to the rest of the proof intact. The advantage of this approach is that to certify overall correctness, one needs to only certify the correctness of the finite structure that has been evolved.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-1-Lifting.width-1250.png\" alt=\"Lifting: Morphing finite structures using AI, while keeping the interface to the broader proof intact.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-1-Lifting.width-1250.png\" alt=\"Lifting: Morphing finite structures using AI, while keeping the interface to the broader proof intact.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"g7wnv\">Lifting: Morphing finite structures using AI, while keeping the interface to the broader proof intact.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"c47sg\">In complexity theory, researchers often use established proof frameworks that rely on the existence of specific, highly optimized finite structures. If a better structure can be found, the entire proof framework \"lifts\" this improvement to a better universal result.</p><p data-block-key=\"f1cfa\">A key example of this is a \"<a href=\"https://people.csail.mit.edu/madhu/papers/1996/gadgets-journ.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">gadget reduction</a>.\" To prove that a target problem is computationally hard (intractable), researchers try to map a known intractable source problem to it, hence demonstrating that the target problem is at least as hard as the source problem. A gadget is a recipe for locally transforming a small piece of the source problem into a piece of the target problem. These gadgets are finite structures, and finding the optimal gadget is a painstaking process often done by hand.</p><p data-block-key=\"6na9u\">By tasking AlphaEvolve with finding better gadgets, we were able to discover structures far more complex than those previously known. These finite discoveries, when plugged into the existing mathematical frameworks, immediately yield new universal theorems in complexity theory.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">New theorems in complexity theory</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c47sg\">We applied this methodology to the MAX-k-CUT problem. Given a <a href=\"https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)\" target=\"_blank\" rel=\"noopener noreferrer\">graph</a> (a network of nodes and edges), the goal is to partition the nodes into <i>k</i> distinct sets such that the number of edges crossing between different sets is maximized. This is a classic intractable (<a href=\"https://en.wikipedia.org/wiki/NP-hardness\" target=\"_blank\" rel=\"noopener noreferrer\">NP-hard</a>) problem, meaning we do not expect to find efficient algorithms that solve it exactly. Therefore, we focused on <i>approximation algorithms</i> — those that efficiently find solutions guaranteed to be close to the optimum.</p><p data-block-key=\"62kuh\">The crucial question is: what is the limit of approximation?</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">MAX-4-CUT: A new state of the art</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c47sg\">For MAX-4-CUT (partitioning into four sets), the previous best-known result proved that it is NP-hard to approximate the solution within a factor of <a href=\"https://arxiv.org/pdf/2509.18057#page=8\" target=\"_blank\" rel=\"noopener noreferrer\">0.9883</a>. AlphaEvolve was deployed to search for a new gadget reduction to MAX-4-CUT.</p><p data-block-key=\"9bab8\">The system discovered an intricate gadget involving 19 variables (nodes) with a complex weighting scheme (some connections having up to 1429 times the weight of others). This discovery established a new inapproximability bound of 0.987.</p><p data-block-key=\"1m1ig\">This improvement may seem incremental, but in the mature field of hardness of approximation, such advances often require significant new techniques or combinatorial insights.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-2-Gadget.width-1250.png\" alt=\"Drawing of a graph representing the gadget found by AlphaEvolve for the reduction to MAX-4-CUT.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-2-Gadget.width-1250.png\" alt=\"Drawing of a graph representing the gadget found by AlphaEvolve for the reduction to MAX-4-CUT.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"g7wnv\">Gadget found by AlphaEvolve for the reduction to MAX-4-CUT.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Average-case hardness and Ramanujan graphs</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c47sg\">We also explored the hardness of problems <i>on average</i>, rather than in the worst case. Specifically, we studied the difficulty of certifying bounds on the MAX-2-CUT (as well as <a href=\"https://en.wikipedia.org/wiki/Independent_set_(graph_theory)\" target=\"_blank\" rel=\"noopener noreferrer\">maximum independent set</a>) of sparse random graphs<footnote id=\"3d54ef4b-a9db-4307-9987-e308f9f08c3b\">[3d54ef]</footnote>. <a href=\"https://arxiv.org/abs/2404.17012\" target=\"_blank\" rel=\"noopener noreferrer\">Recent work</a> connected this problem to the existence of specific <a href=\"https://en.wikipedia.org/wiki/Ramanujan_graph\" target=\"_blank\" rel=\"noopener noreferrer\">Ramanujan graphs</a> — deterministic graphs that “look” like sparse random graphs. They conjectured that the existence of Ramanujan graphs with unnaturally large cuts implies it is computationally hard to certify the MAX-2-CUT of a random graph.</p><p data-block-key=\"3nhje\">Prior work used computer assistance to find such graphs on up to 10 nodes. Improving their results requires finding more extremal Ramanujan graphs on many more nodes, which are exceedingly difficult to find and verify. AlphaEvolve successfully navigated this vast search space, discovering Ramanujan graphs with even larger cuts on as many as 163 nodes.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-3-4RegGraph.width-1250.png\" alt=\"Drawing of a 4-regular Ramanujan graph with large 2-cut found by AlphaEvolve.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ReGeCS-3-4RegGraph.width-1250.png\" alt=\"Drawing of a 4-regular Ramanujan graph with large 2-cut found by AlphaEvolve.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"g7wnv\">A 4-regular Ramanujan graph with large 2-cut found by AlphaEvolve.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"c47sg\">These discoveries significantly improved the lower bounds for average-case hardness. Furthermore, combined with new algorithmic progress (non-AI based), we were able to nearly settle the computational hardness of these questions, matching the upper and lower bounds to within the third decimal place.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The crucial role of verified correctness</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c47sg\">A critical distinction of this work is that the results come with proofs of correctness.</p><p data-block-key=\"8afn7\">When an LLM is prompted to generate a mathematical proof directly, it often produces a proof sketch or an argument that requires substantial human intervention to verify and complete. Hallucinations or subtle errors can render the output useless. As mentioned earlier, the standard for correctness in math is absolute.</p><p data-block-key=\"48i4d\">In contrast, the approach taken here uses AI to discover a <i>structure</i> within the proof, not the proof itself. The validity of the final theorem relies on two components: the correctness of the lifting framework, and the verification of the discovered structure. While the frameworks are sound, verifying the structures discovered by AlphaEvolve is computationally intensive.</p><p data-block-key=\"5d7lt\">Remarkably, AlphaEvolve achieved a 10,000x speedup in the verification process by implementing sophisticated <a href=\"https://en.wikipedia.org/wiki/Branch_and_bound\" target=\"_blank\" rel=\"noopener noreferrer\">branch-and-bound</a> strategies and system-level optimizations. This massive speedup was the key enabler for the research, allowing the system to explore much larger and more complex gadgets.</p><p data-block-key=\"fpuet\">Crucially, the final gadgets discovered were still verified using the original, brute-force algorithm, ensuring the absolute correctness of the theorems.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The future of AI-assisted theory</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c47sg\">While these initial research findings are far from conclusive, they suggest that AI is poised to become a helpful collaborator in mathematical discovery. We have observed the models in AlphaEvolve generate intricate mathematical objects that at times exhibit nascent reasoning capabilities. However, as we transition into an era where proofs may increasingly be attributed to AI, the crucial task of verification is set to become a significant bottleneck.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c47sg\"><i>We would like to thank Adam Zsolt Wagner, Swarat Chaudhuri, Pasin Manurangsi and Sushant Sachdeva for helping us during various stages of the project.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "个人健康代理的剖析 (原标题: The anatomy of a personal health agent)",
      "link": "https://research.google/blog/the-anatomy-of-a-personal-health-agent/",
      "pubDate": "Mon, 29 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-29T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 个人健康代理 (PHA) 框架：个性化健康支持的未来\n\n随着大型语言模型 (LLM) 和可穿戴设备数据的快速发展，为个人健康旅程提供支持迎来了变革性机遇。然而，由于个人健康需求的多样性和复杂性，单一系统难以有效应对。为此，研究人员提出了**个人健康代理 (Personal Health Agent, PHA)**，这是一个全面的研究框架，旨在通过多模态数据推理，提供个性化、循证的健康指导。\n\n### PHA 的多代理架构\n\nPHA 采用多代理架构，将个人健康和健康支持分解为三个核心角色，每个角色由一个专业的子代理处理：\n\n1.  **数据科学 (DS) 代理**：负责分析个人时间序列数据（如可穿戴设备数据和血液生物标志物），提供情境化的数值洞察。它通过两阶段数据科学模块增强基础模型，能够解释模糊的用户查询，并将其转化为稳健的统计分析计划，然后生成并执行代码以得出有效的数据驱动答案。\n2.  **领域专家 (DE) 代理**：作为可靠的健康和健康知识来源。它通过多步骤推理框架和工具箱（包括访问 NCBI 等权威来源）增强基础模型，确保信息准确可信，并能根据用户的特定情况（如既往病史）定制信息。\n3.  **健康教练 (HC) 代理**：旨在通过多轮对话支持用户设定目标并促进持久的行为改变。它采用受心理学策略（如动机性访谈）启发的模块化架构，以实现更自然有效的互动。\n\n### 用户中心设计\n\n为了构建一个真正满足多样化需求的代理，研究团队采用了用户中心设计流程。他们综合了来自在线健康论坛、500 多名用户的调查数据以及与设计和工程专家研讨会的见解，识别出人们在四个关键领域需要支持：理解一般健康主题、解释个人数据、获取可操作的健康建议以及评估症状。这促使 PHA 系统被设计成类似于人类专家团队的协作模式。\n\n![用户中心流程](https://storage.googleapis.com/gweb-research2023-media/images/PHA2_Process.width-1250.png)\n*一个用户中心流程，用于识别关键用户旅程。*\n\n### 综合评估框架\n\n为了验证 PHA 系统，研究人员开发了一个全面的多层次评估框架。他们首先针对每个子代理的独特核心能力与最先进的 LLM 基础模型进行了基准测试，然后评估了完全集成后的 PHA 的整体效能。评估涉及自动化和广泛的人工评估，涵盖 10 项基准任务，投入了超过 1,100 小时来自最终用户和健康专家的努力，以评估其在真实、多模态对话中的表现。\n\n![综合评估描述](https://storage.googleapis.com/gweb-research2023-media/images/PHA3_Table.width-1250.png)\n*对单个子代理和最终个人健康代理 (PHA) 系统进行综合评估的描述。*\n\n#### 子代理评估结果：\n\n*   **数据科学代理 (DS Agent)**：在分析计划质量方面显著优于基础模型（75.6% 对 53.7%），并且在生成准确、可执行的代码方面更可靠。\n    ![DS代理评估结果](https://storage.googleapis.com/gweb-research2023-media/images/PHA4_DSAgent.width-1250.png)\n    *DS代理：由人类数据科学家和自动评估器评估的DS代理和基础模型生成的六个维度数据分析计划的评估结果。*\n\n*   **领域专家代理 (DE Agent)**：在所有基准测试中始终优于基础模型。临床医生认为其多模态健康数据摘要在临床上更具相关性和实用性，最终用户认为其响应更具个性化和可信度。\n    ![DE代理评估结果](https://storage.googleapis.com/gweb-research2023-media/images/PHA5_DEAgent.width-1250.png)\n    *DE代理：由临床专家评估的DE代理和基础模型在七个临床维度上的多模态推理评估结果。*\n\n*   **健康教练代理 (HC Agent)**：在最终用户和健康教练专家的评估中，其能力显著优于基线模型，尤其在对话体验、目标导向有效性和动机支持方面表现出色。\n    ![HC代理评估结果](https://storage.googleapis.com/gweb-research2023-media/images/PHA6_HCAgent.width-1250.png)\n    *HC代理：由人类健康和教练专家评估的HC代理和基础模型在六个维度上的教练体验评估结果。*\n\n### 协作式 PHA：一个协调的团队\n\nPHA 框架通过智能协调器将这三个专业代理整合为一个有凝聚力的团队。当用户提出查询时，协调器会分析用户需求，动态分配“主”代理和“支持”代理，并促进协作、反思和记忆更新的迭代工作流，以综合生成一个全面响应。这种协作方法被证明显著优于其各部分的总和。\n\n在评估代理综合个人健康数据以帮助用户回答健康问题和实现个人健康目标的能力时，最终用户和健康专家都更倾向于 PHA，而非单一代理系统或简单的并行多代理基线。PHA 在大多数情况下被评为最佳整体系统，这强调了模仿人类专家团队协作结构对于提供真正有益支持的关键价值。\n\n![PHA评估结果1](https://storage.googleapis.com/gweb-research2023-media/images/PHA8_Results1body.width-1250.png)\n*PHA：由人类专家评估的PHA和其他基线生成的响应的评估结果。*\n\n![PHA评估结果2](https://storage.googleapis.com/gweb-research2023-media/images/PHA9_Results2.width-1250.png)\n*PHA：由人类专家评估的PHA和其他基线生成的响应的排名结果。*\n\n### 智能个人健康代理的未来\n\n这项研究为设计下一代个人健康 AI 提供了一个经过验证的概念蓝图，倡导从单一模型转向模块化、协作式系统，使其更值得信赖、更具连贯性和实用性。\n\n**重要提示**：此工作概述了一个用于研究目的的概念框架，不应被视为对任何当前正在开发或向公众提供的特定产品、服务或功能的描述。任何实际应用都将经过单独的设计、验证和审查流程。",
      "shortSummary": "个人健康代理（PHA）是一个基于大型语言模型和可穿戴设备数据的研究框架，旨在提供个性化、循证的健康指导。它采用多代理架构，由数据科学、领域专家和健康教练三个专业子代理组成，通过智能协调器协同工作。PHA 经过全面的用户中心设计和评估，结果表明其在处理复杂健康需求方面显著优于单一系统和并行多代理基线，为未来模块化、协作式个人健康 AI 奠定了基础。此框架目前仅用于研究目的。",
      "translated_title": "个人健康代理的剖析",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PHA2_Process.width-1250.png",
          "alt": "PHA2_Process",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PHA3_Table.width-1250.png",
          "alt": "PHA3_Table",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PHA4_DSAgent.width-1250.png",
          "alt": "PHA4_DSAgent",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PHA5_DEAgent.width-1250.png",
          "alt": "PHA5_DEAgent",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PHA6_HCAgent.width-1250.png",
          "alt": "PHA6_HCAgent",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"1sp1g\">The rapid advancement of large language models (LLMs), combined with <a href=\"https://research.google/blog/sensorlm-learning-the-language-of-wearable-sensors/\">data from wearable devices</a>, presents a transformative opportunity to empower people on their personal health journeys. However, health needs vary from individual to individual. Answering a specific query, such as, \"On average, how many hours have I been sleeping this last month?\" requires different skills than an open-ended question like, \"What can I do to improve my sleep quality?\" A single system can struggle to address this complexity.</p><p data-block-key=\"8dbs6\">To meet this challenge, we adopt a human-centered process and propose the <a href=\"https://arxiv.org/abs/2508.20148\" target=\"_blank\" rel=\"noopener noreferrer\">Personal Health Agent</a> (PHA). This agent is a comprehensive research framework that can reason about multimodal data to provide personalized, evidence-based guidance. Using a multi-agent architecture, PHA deconstructs personal health and wellness support into three core roles (data science, domain expert, and health coach), each handled by a specialist sub-agent. To evaluate each sub-agent and the multi-agent system, we leveraged <a href=\"https://arxiv.org/abs/2505.03784\" target=\"_blank\" rel=\"noopener noreferrer\">a real-world dataset</a> from an IRB-reviewed study where ~1200 users provided informed consent to share their wearables data from Fitbit, a health questionnaire, and blood test results. We conducted automated and human evaluations across 10 benchmark tasks, involving more than 7,000 annotations and 1,100 hours of effort from health experts and end-users. Our work represents the most comprehensive evaluation of a health agent to date and establishes a strong foundation towards the futuristic vision of a personal health agent accessible to everyone.</p><p data-block-key=\"fdbrn\">This work outlines a conceptual framework for research purposes, and should not be considered a description of any specific product, service, or feature currently in development or available to the public. Any real-world application would be subject to a separate design, validation, and review process.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/PHA1_Illustration.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"7rp20\"><i>An illustration of the internal functions of Personal Health Agent (PHA) that enable it to support personal health needs.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"3xfzh\">User-centered design for personal health needs</h2><p data-block-key=\"44e8q\">To build an agent that truly meets these diverse needs, we started with a user-centered design process. We synthesized insights from over 1,300 real-world health queries from online sources, such as health forums, survey data from more than 500 users, and a workshop with design and engineering experts. This research revealed four critical areas where people need support: understanding general health topics, interpreting their personal data, getting actionable wellness advice, and assessing symptoms. This insight led us to design the PHA system that resembles human expert teams, including data scientists, domain experts, and personal health coaches.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA2_Process.width-1250.png\" alt=\"PHA2_Process\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA2_Process.width-1250.png\" alt=\"PHA2_Process\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"enxez\"><i>A user-centered process to identify critical user journeys.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"v6xwt\">Evaluation of our proposed system</h2><p data-block-key=\"690u2\">To validate our system, we developed a holistic, multi-level evaluation framework. We first benchmarked each individual sub-agent on their unique core capabilities against the state-of-the-art LLM model as the base model, and then assessed the fully integrated PHA’s overall efficacy. The table below shows our comprehensive evaluation, which involved both automated and extensive human evaluations across 10 benchmark tasks, incorporating over 1,100 hours of effort from both end-users and health experts to assess performance in realistic, multi-modal conversations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA3_Table.width-1250.png\" alt=\"PHA3_Table\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA3_Table.width-1250.png\" alt=\"PHA3_Table\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"enxez\"><i>Description of our comprehensive evaluation of individual sub-agents and the final Personal Health Agent (PHA) system.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"v6xwt\">The data science agent: Personal data analyst</h2><p data-block-key=\"7nd9l\">The first specialist is the data science (DS) agent, which analyzes personal time-series data from wearables plus health data, such as blood biomarkers, to provide contextualized numerical insights. The DS agent builds on top of a base model (e.g., Gemini) and is enhanced by a two-stage data science module: Stage 1) interpret underspecified and ambiguous user queries (e.g., “Am I getting more fit recently?”), and Stage 2) translate them into robust statistical analysis plans. It then generates and executes code to produce a statistically valid, data-driven answer.</p><p data-block-key=\"eeeci\">We developed two auto-evaluation benchmarks for each stage of the DS agent's workflow. For the first stage, analysis planning, we used an auto-evaluator trained on 354 query-analysis plans curated by 10 expert data scientists. Based on a detailed rubric assessing dimensions like data sufficiency, statistical validity, and alignment with the user's query, our evaluations showed that the DS agent significantly outperforms the base model in creating high-quality analysis plans (achieving a 75.6% score vs. 53.7% for the baseline). For the second stage, code generation, the agent’s output was benchmarked against 173 rigorous unit tests written by data scientists. This confirmed the agent is more reliable at generating accurate, executable code used to derive insights from time-series wearable data.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA4_DSAgent.width-1250.png\" alt=\"PHA4_DSAgent\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA4_DSAgent.width-1250.png\" alt=\"PHA4_DSAgent\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"enxez\"><i>DS agent: Results of evaluating data analysis plan generated by the DS agent and the base model across six dimensions, as evaluated by human data scientist and auto raters.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"v6xwt\">The domain expert agent: Grounded, trustworthy knowledge</h2><p data-block-key=\"a5n8f\">Next is the domain expert (DE) agent, which functions as a reliable source of health and wellness knowledge. In a high-stakes domain like health and wellbeing, ensuring information is accurate and trustworthy is critical. The DE agent enhances a base model by using a multi-step reasoning framework and a toolbox that includes access to authoritative sources, such as the <a href=\"https://pmc.ncbi.nlm.nih.gov/tools/developers/\" target=\"_blank\" rel=\"noopener noreferrer\">National Center for Biotechnology Information</a> (NCBI) database, to ground its responses in <i>verifiable facts</i>. It excels at tailoring information to a user’s specific profile, such as pre-existing conditions. We developed two auto-evaluation benchmarks to test the DE agent’s medical knowledge (one evaluating our agent’s performance on board certification and coaching exam questions, and one for providing accurate differential diagnosis). We further developed two human-evaluation benchmarks (one for clinicians, and one for consumers) to measure the DE agent’s capability on personalization and multi-modal reasoning. Our DE Agent consistently outperforms the base model across all benchmarks. For instance, clinicians rated the DE agent's summaries of multimodal health data as significantly more clinically relevant and useful, and end-users found its responses to be substantially more personalized and trustworthy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA5_DEAgent.width-1250.png\" alt=\"PHA5_DEAgent\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA5_DEAgent.width-1250.png\" alt=\"PHA5_DEAgent\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"enxez\"><i>DE agent: Results of evaluating multi-modal reasoning of the DE agent and the base model across seven clinical dimensions, as evaluated by clinical experts.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"v6xwt\">The health coach agent: Guiding behavior change</h2><p data-block-key=\"c2d6v\">The third specialist is the health coach (HC) agent, which is designed to support users in setting goals and fostering lasting behavioral change through multi-turn conversations. Effective coaching requires a delicate balance between gathering information and providing actionable advice. The HC agent employs a modular architecture inspired by proven psychological strategies (e.g., <i>motivational interviewing</i>) to navigate this dynamic, leading to more natural and effective interactions. We benchmarked the HC agent’s performance in two human-evaluation setups, one with end-users and the other with health coaching experts, evaluating our model’s ability across several key areas. For the end-user evaluation, we focused on conversational experience, goal-oriented effectiveness, and motivational support. For the expert evaluation, we assessed adherence to professional coaching principles, recommendation quality, and agent credibility. Both evaluation aspects indicate that the HC agent is significantly more capable than the baseline<i>,</i> underscoring a key insight from our research: for coaching agents, users prioritize core competency and the ability to provide actionable guidance.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA6_HCAgent.width-1250.png\" alt=\"PHA6_HCAgent\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA6_HCAgent.width-1250.png\" alt=\"PHA6_HCAgent\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"enxez\"><i>HC agent: Results of evaluating coaching experience of the HC agent and base model on six dimensions, evaluated by human health and coaching experts.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"v6xwt\">The Personal Health Agent (PHA): A collaborative team</h2><p data-block-key=\"94ato\">While each agent is powerful alone, the true potential is realized when they collaborate. The Personal Health Agent (PHA) framework integrates these three specialists into a cohesive team managed by an intelligent orchestrator. When a user poses a query, the orchestrator analyzes the user's need, dynamically assigns a \"main\" agent and \"supporting\" agents, and facilitates an iterative workflow of collaboration, reflection, and memory updates to synthesize a single, comprehensive response.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/PHA7_Animation.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"enxez\"><i>A technical breakdown of the DS, DE, and HC agents, with orchestration into the Personal Health Agent (PHA).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"v6xwt\">This collaborative approach proved to significantly outperform the sum of its parts. In extensive evaluations of rubrics assessing agents' capability in synthesizing personal health data to help users answer their health and wellness queries, as well as achieving personal health goals, both end-users and health experts preferred the PHA over (i) a powerful <a href=\"https://arxiv.org/pdf/2406.06464\" target=\"_blank\" rel=\"noopener noreferrer\">single-agent system</a> that also builds on a base model that uses tools to achieve three roles within a single agent setup, and (ii) a parallel multi-agent baseline that includes the same DS, DE, and HC agents, but simply calls all three agents and synthesizes their results without dynamic orchestration. Both end-users and experts ranked PHA as the best overall system in the majority of cases. This provides a strong example of how the value of emulating the collaborative structure of human expert teams is key to providing truly helpful support.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA8_Results1body.width-1250.png\" alt=\"PHA8_Results1body\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA8_Results1body.width-1250.png\" alt=\"PHA8_Results1body\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"u5asb\"><i>PHA: Results of evaluating responses generated by the PHA and other baselines, evaluated by human experts.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA9_Results2.width-1250.png\" alt=\"PHA9_Results2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PHA9_Results2.width-1250.png\" alt=\"PHA9_Results2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"enxez\"><i>PHA: Results of ranking responses generated by the PHA and other baselines, evaluated by human experts.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"v6xwt\">The future of intelligent personal health agents</h2><p data-block-key=\"78bb4\">Creating AI systems that can interpret complex health and wellness data and provide actionable wellness advice has been a longstanding challenge in the field. Our research provides a validated conceptual blueprint for designing the next generation of personal health AI, advocating a shift away from monolithic models toward modular, collaborative systems that are more trustworthy, coherent, and helpful.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "迈向更好的健康对话：基于Gemini的“寻路”AI代理研究洞察 (原标题: Towards better health conversations: Research insights on a “wayfinding” AI agent based on Gemini)",
      "link": "https://research.google/blog/towards-better-health-conversations-research-insights-on-a-wayfinding-ai-agent-based-on-gemini/",
      "pubDate": "Wed, 24 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-24T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 迈向更好的健康对话：基于Gemini的“寻路”AI代理研究洞察\n\n### 引言：在线健康信息导航的挑战\n\n获取清晰、相关且个性化的健康信息对患者至关重要，但在线健康信息世界往往令人困惑、不知所措且缺乏个性化。现有的大型语言模型（LLM）工具多为被动的“问答者”，提供对初始查询的单一全面答案，这与医生等专家通过提问来理解全貌并引导患者的方式不同。这种寻求上下文的对话方式对AI设计构成了重大挑战。\n\n### “寻路AI”：一种新的对话式方法\n\n本文介绍了基于Gemini的早期研究原型“寻路AI”（Wayfinding AI），旨在探索一种新方法。其核心理念是：通过主动提出澄清性问题，AI代理能更好地发现用户需求，引导他们清晰表达担忧，并提供更有帮助、更个性化的信息。研究团队通过四项混合方法用户体验研究，共163名参与者，迭代设计了一个用户认为比基线AI代理更有帮助、更相关、更符合其需求的代理。\n\n### 形成性用户体验洞察：在线查找健康信息的挑战\n\n*   **用户痛点**：通过访谈33名参与者发现，人们在在线查找健康信息时，往往难以清晰表达自己的健康问题，因为缺乏医学背景，难以判断哪些细节具有医学相关性。\n*   **用户偏好**：研究显示，当聊天机器人主动提出澄清性问题时，用户体验会显著改变。大多数参与者更喜欢“延迟回答”的方式（即AI先提问），而非立即给出全面答案。这种对话风格被认为更个性化、更安心。\n*   **效果**：澄清性问题不仅帮助AI提供更好的答案，也赋能用户，引导他们提供更相关的上下文。\n*   **挑战**：这种基于澄清性问题的方法的有效性高度依赖于执行质量——如果问题措辞不当、不相关或隐藏在冗长文本中容易被忽略，用户参与度就会下降。\n\n### “寻路AI”的设计原则\n\n基于上述洞察，“寻路AI”围绕三个核心原则设计，以创造更赋能的对话体验：\n\n1.  **主动对话引导**：在每一轮对话中，“寻路AI”最多提出三个有针对性的问题，旨在系统性地减少歧义，帮助用户更完整地表达其健康故事，并直接满足用户对更多上下文答案的需求。\n2.  **每轮提供“尽力而为”的答案**：考虑到某些健康问题可能无需澄清即可获得良好答案，“寻路AI”在每一轮对话中，都会根据目前共享的信息提供一个“尽力而为”的答案，同时强调如果用户能回答一个或多个后续问题，答案可以得到改进。这种方法在整个对话过程中为用户提供有用的信息，并提供选项以随着对话的进行接收越来越好的答案。\n3.  **透明推理**： “寻路AI”会解释用户的最新回答如何帮助完善了之前的答案，使AI的推理过程清晰易懂。\n\n为了确保澄清性问题在“尽力而为”的答案中不会被遗漏，界面设计采用了两栏布局：对话和澄清性问题出现在左栏，而“尽力而为”的答案和更详细的解释出现在右栏。这使得交互式对话与信息内容分离。\n\n### 随机用户研究评估\n\n为评估该代理的潜在实际影响，研究团队进行了一项随机用户研究，招募了130名21岁及以上、非医疗专业人士、有健康相关问题并愿意与AI互动的美国参与者。在随机组内设计中，每位参与者都与“寻路AI”和基线Gemini 2.5 Flash模型互动，探索他们的健康话题。参与者在与每个AI互动后，就帮助性、问题相关性、定制性、目标理解、易用性和获取有用信息效率等六个维度评估了体验满意度。\n\n![研究设计图示](https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder2_StudyDesign.width-1250.png)\n*图示：本研究的设计。*\n\n### 研究结果：通过目标理解和定制对话提供有益且相关的信息\n\n研究结果表明，尽管“寻路AI”采用了不太常见的两栏界面，用户仍在其多个重要维度上偏爱“寻路AI”的方法。用户更喜欢“寻路AI”的帮助性、相关性、理解其目标的能力以及为特定需求定制对话的能力。这些发现表明，“寻路AI”主动提问的行为成功地为用户创造了更个性化、更有帮助的体验，而没有在用户体验中引入不必要的摩擦。\n\n![用户对基线AI和寻路AI的偏好比较](https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder3_Results.width-1250.png)\n*图示：用户在多个评估维度上对基线AI和“寻路AI”的偏好，包括代理的帮助性、响应的相关性、对话对用户的定制性、对用户目标的理解、易用性、对话效率以及未来健康信息需求的使用意愿。*\n\n此外，参与者与“寻路AI”的对话明显更长，尤其是在试图理解症状原因时。对于这些话题，“寻路AI”的对话平均有4.96轮，而基线AI为3.29轮。他们向每个AI提供的提示模式也不同。\n\n![Sankey图，展示基线AI和寻路AI的对话流程](https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder4_HeroSankey.width-1250.png)\n*图示：Sankey图，展示基线AI和“寻路AI”的对话流程。每个垂直条显示了前5轮对话中用户提示类型的细分。蓝色条表示参与者对澄清性问题的回应——这在“寻路AI”中更为常见。*\n\n### 结论\n\n在线查找正确的健康信息如同迷宫。“寻路AI”通过设计成个性化和主动的对话伙伴，并在结构化界面中提出有针对性的问题，证明了其能提供比传统问答体验更受用户青睐的体验，从而帮助人们获取更有帮助、更相关和更个性化的信息。用户研究结果有力地证明，这种以人为本的对话式方法是AI在健康领域未来发展的一个有前景的方向，有助于人们更好地管理自己的健康旅程。",
      "shortSummary": "在线查找个性化健康信息充满挑战。谷歌研究团队基于Gemini开发了“寻路AI”，它通过主动提出澄清性问题，像医生一样引导用户，而非被动回答。多项用户研究（163名参与者）表明，用户更喜欢这种对话式AI，认为它更有帮助、更相关、更个性化，能更好地理解用户目标，并促成更深入的健康对话。这表明以人为本的对话式AI是健康领域AI的未来方向。",
      "translated_title": "迈向更好的健康对话：基于Gemini的“寻路”AI代理研究洞察",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder2_StudyDesign.width-1250.png",
          "alt": "Wayfinder2_StudyDesign",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder3_Results.width-1250.png",
          "alt": "Wayfinder3_Results",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder4_HeroSankey.width-1250.png",
          "alt": "Wayfinder4_HeroSankey",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3lmno\">The ability to find clear, relevant, and personalized health information is a cornerstone of empowerment for medical patients. Yet, navigating the world of online health information is often a confusing, overwhelming, and impersonal experience. We are met with a flood of generic information that does not account for our unique context, and it can be difficult to know what details are relevant.</p><p data-block-key=\"2t1lv\">Large language models (LLMs) have the potential to make this information more accessible and tailored. However, many AI tools today act as passive \"question-answerers\" — they provide a single, comprehensive answer to an initial query. But this isn't how an expert, like a doctor, helps someone navigate a complex topic. A health professional doesn't just provide a lecture; they ask clarifying questions to understand the full picture, discover a person's goals, and guide them through the information maze. Though this <i>context-seeking</i> is critical, it's a significant design challenge for AI.</p><p data-block-key=\"5nqat\">In “<a href=\"https://storage.googleapis.com/research-media/wayfinding-ai.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Towards Better Health Conversations: The Benefits of Context-Seeking</a>”, we describe how we designed and tested our “Wayfinding AI”, an early-stage research prototype, based on Gemini, that explores a new approach. Our fundamental thesis is that by proactively asking clarifying questions, an AI agent can better discover a user's needs, guide them in articulating their concerns, and provide more helpful, tailored information. In a series of four mixed-method user experience studies with a total of 163 participants, we examined how people interact with AI for their health questions, and we iteratively designed an agent that users found to be significantly more helpful, relevant, and tailored to their needs than a baseline AI agent.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"3lmno\">Formative user experience insights: Challenges in finding health information online</h2><p data-block-key=\"f889p\">To better understand the hurdles people face, we interviewed 33 participants about their experiences finding health information online. A key theme quickly emerged: people often struggle to articulate their health concerns. As one participant described, their process was to \"...just kind of like throw all the words in there and then I'm just gonna see what comes back.\" It may be that without a clinical background, it’s difficult to know which details are medically relevant.</p><p data-block-key=\"ev7la\">The people we interviewed were then able to use research prototypes of different chatbots. (The chat histories were not logged.) These participants made up a diverse group and asked health questions on a wide range of topics (e.g., rib pain, vertigo, consistent and unexplained weight gain, tinnitus and surgery; more details <a href=\"https://storage.googleapis.com/research-media/wayfinding-ai.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">in the paper</a>). Our studies revealed that when a chatbot proactively asks clarifying questions, the experience changes dramatically. The majority of participants preferred a \"deferred-answer\" approach — where the AI asks questions first — over one that gives a comprehensive answer immediately. This conversational style was perceived as more personal and reassuring. As one person noted, \"It feels more like the way it would work if you talk to a doctor... it does make me feel a little more confident that it wants to know more before jumping right into an answer.\" These clarifying questions not only help the AI provide better answers, but also empower users, guiding them to provide more relevant context. We found similar patterns in <a href=\"https://dl.acm.org/doi/abs/10.1145/3613905.3651891\" target=\"_blank\" rel=\"noopener noreferrer\">prior work on AI for dermatology</a>.</p><p data-block-key=\"b2dcv\">However, the effectiveness of this clarifying question–based approach depends heavily on the execution — engagement drops if questions are poorly formulated, irrelevant, or buried within long paragraphs of text where they are easily missed.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"3lmno\">Designing a Wayfinding AI to empower people through personal and proactive conversations</h2><p data-block-key=\"8tbaa\">Informed by these insights, we designed our Wayfinding AI around three core principles to create a more empowering conversational experience:</p><ol><li data-block-key=\"4kmek\"><i>Proactive conversational guidance:</i> At each turn, the Wayfinding AI asks up to three targeted questions designed to systematically reduce ambiguity. This helps users articulate their health story more completely and directly incorporates users’ desire for more contextualized answers.<br><br></li><li data-block-key=\"cp0mo\"><i>Best-effort answers at each turn:</i> Because some health-related questions may not require clarification to get a good answer, the Wayfinding AI provides a \"best-effort\" answer at every conversational turn, based on the information shared so far, while emphasizing that the answer can be improved if the user can answer one or more of the follow-up questions. This approach gives the user helpful information throughout the conversation, while providing the option to further receive increasingly better answers as the conversation progresses.<br><br></li><li data-block-key=\"2rorl\"><i>Transparent reasoning:</i> The Wayfinding AI explains how the user's latest answers have helped refine the previous answer. This makes the AI's reasoning process clear and understandable.</li></ol><p data-block-key=\"81tqi\">To ensure clarifying questions are never missed within the longer answers in the “best-effort” answers section, we designed an interface with a two-column layout. The conversation and clarifying questions appear in the left column, while best-effort answers and more detailed explanations appear in the right. This separates the interactive conversation from the informational content.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Wayfinder1_UserInteraction.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d7rd2\"><i>Example of a user starting to interact with our Wayfinding AI prototype interface, including both the familiar multi-turn chat interface on the left, and a “best information so far” panel on the right. This two-panel interface separates the context-seeking stream from the more detailed information provision piece, enabling users to dive into the information only when they feel all relevant information has been relayed.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"65t26\">Evaluating our Wayfinding AI through a randomized user study</h2><p data-block-key=\"7b52c\">To evaluate the potential real-world impact of this agent, we conducted a randomized user study with 130 US-based participants recruited via a third party platform. All participants were 21 years and older, were not health care professionals, and had a health-related question for which they were willing to interact with an AI. To ensure a broad range of health topics, we imposed very few restrictions on which topic would be eligible for the study (details on excluded inquiries are provided in the paper). In a randomized <a href=\"https://www.nngroup.com/articles/between-within-subjects/\" target=\"_blank\" rel=\"noopener noreferrer\">within-subjects design</a>, each participant interacted with both our Wayfinding AI and a baseline Gemini 2.5 Flash model to explore their health topic. After providing informed consent and answering standard demographic questions, participants were instructed to have a conversation spending at least 3 minutes on their question; and then to resume the survey. After interacting with each AI, participants answered questions about their satisfaction with the experience along 6 dimensions: helpfulness, relevance of questions asked, tailoring to their situation, goal understanding, ease of use, and efficiency of getting useful information. They were able to provide open feedback about what they learned, and also had the option to upload their conversation with the AI. Sharing the conversation was not required to complete the survey. At the end of the study, participants were prompted to explicitly compare the two AIs and indicate which they would prefer in terms of each of the six dimensions above. They were also asked, <i>\"For a future topic, would you prefer the first or the second AI?\"</i> The order of AI exposure (Baseline AI first vs. Wayfinding AI first) was randomized across participants. Throughout the study, participants were instructed to not provide any identifying information about themselves.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder2_StudyDesign.width-1250.png\" alt=\"Wayfinder2_StudyDesign\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder2_StudyDesign.width-1250.png\" alt=\"Wayfinder2_StudyDesign\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"9mag7\"><i>Illustration of our study design.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"65t26\">Helpful and relevant information through goal understanding and tailored conversations</h2><p data-block-key=\"7mdek\">As shown below, the results of the study demonstrated that users preferred the Wayfinding AI's approach across several important dimensions, despite its less-familiar two-column interface. Users favored Wayfinding AI for its helpfulness, relevance, ability to understand their goal, and for tailoring the conversation to their specific needs. These findings suggest that the proactive, question-asking behavior of Wayfinding AI successfully created a more personalized and helpful experience for users without introducing undue friction in the user experience.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder3_Results.width-1250.png\" alt=\"Wayfinder3_Results\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder3_Results.width-1250.png\" alt=\"Wayfinder3_Results\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"9mag7\"><i>User preferences between a baseline and our Wayfinding AI along multiple evaluation axes, including helpfulness of the agent, relevance of its responses, tailoring of the conversation to the user, understanding the user’s goal, ease of use, efficiency of the conversation and willingness to use each for a future health information need.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"65t26\">Beyond simply preferring their conversations with the Wayfinding AI, participants had noticeably <i>different</i> conversations. Conversations were longer with the Wayfinding AI, in particular when participants were trying to understand the cause of their symptoms. For those topics, conversations with the Wayfinding AI had 4.96 turns on average, compared to 3.29 for the baseline AI. And the pattern of prompts they provided to each AI looked different across conversations:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder4_HeroSankey.width-1250.png\" alt=\"Wayfinder4_HeroSankey\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Wayfinder4_HeroSankey.width-1250.png\" alt=\"Wayfinder4_HeroSankey\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"9mag7\"><i>Sankey diagram illustrating the flow of conversations with the baseline AI and the Wayfinding AI. Each of the vertical bars shows the breakdown of the types of user prompts, across the first 5 conversation turns. The blue bars indicate participants responding to clarifying questions — much more common for the Wayfinding AI.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"65t26\">Conclusion</h2><p data-block-key=\"9n52g\">Finding the right health information online can feel like navigating a maze. While AI has the potential to be a powerful guide, our research shows that its success hinges on its ability to move beyond being a passive question-answerer and become an active conversational partner.</p><p data-block-key=\"6i6kb\">By designing our Wayfinding AI to be personal and proactive, we demonstrated how asking targeted questions in a well-structured interface can power an experience that users prefer over a more classical, question-answering experience, and thus enable people to obtain more helpful, relevant, and tailored information. The results from our user studies provide strong evidence that this human-centered, conversational approach is a promising direction for the future of AI in health, helping people navigate their health journeys.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"65t26\">Acknowledgements</h2><p data-block-key=\"4bk1o\"><i>The research described here is joint work across Google Research, Google Health, and partnering teams. We would like to thank Yuexing Hao, Abbi Ward, Amy Wang, Beverly Freeman, Serena Zhan, Diego Ardila, Jimmy Li, I-Ching Lee, Anna Iurchenko, Siyi Kou, Kartikeya Badola, Jimmy Hu, Bhawesh Kumar, Keith Johnson, Supriya Vijay, Justin Krogue, Avinatan Hassidim, Yossi Matias, Dale Webster, Sunny Virmani, Yun Liu, Quang Duong, Fereshteh Mahvar, Laura Vardoulakis, Tiffany Guo, and Meredith Ringel Morris for contributing or reviewing this work. We would also like to thank the participants who contributed to these studies.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "AfriMed-QA：为全球健康领域的大型语言模型进行基准测试 (原标题: AfriMed-QA: Benchmarking large language models for global health)",
      "link": "https://research.google/blog/afrimed-qa-benchmarking-large-language-models-for-global-health/",
      "pubDate": "Tue, 23 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-23T16:00:00.000Z",
      "creator": "Google",
      "summary": "## AfriMed-QA：为全球健康领域的大型语言模型进行基准测试\n\n### 项目背景与挑战\n大型语言模型（LLMs）在医学和健康问答方面展现出巨大潜力，尤其是在资源匮乏地区，它们可以作为宝贵的决策支持工具，提高临床诊断准确性、可及性，并提供多语言临床决策支持和健康培训。然而，现有医学基准测试可能无法涵盖疾病类型、症状背景、语言差异以及本地文化和区域医学知识等方面的分布变化，这限制了LLMs在传统西方环境之外的泛化能力。因此，迫切需要更多样化的基准数据集来反映真实的全球医疗背景。\n\n### AfriMed-QA数据集的诞生\n为解决这一空白，研究团队推出了 **AfriMed-QA**，这是一个基准问答数据集，汇集了来自非洲16个国家60所医学院的消费者式问题和医学院考试题目。该数据集由AfriMed-QA联盟（包括Intron health、Sisonkebiotik、University of Cape Coast等）与PATH/盖茨基金会合作开发。AfriMed-QA在ACL 2025上发布并荣获“最佳社会影响力论文奖”，其数据集和LLM评估代码已开源，并被用于训练MedGemma。\n\n### AfriMed-QA数据集详情\nAfriMed-QA是首个大规模、泛非洲、多专业的医学问答数据集，旨在评估和开发适用于非洲医疗保健的公平有效的LLMs。它包含：\n*   约15,000个临床多样化的英文问答。\n*   4,000多个带答案的专家多项选择题（MCQs）。\n*   1,200多个带长篇答案的开放式简答题（SAQs）。\n*   10,000个消费者查询（CQs）。\n\n该数据集旨在严格评估LLM在正确性和地理分布变化方面的表现。它由来自12个国家60多所医学院的621名贡献者众包完成，涵盖32个医学专业，包括妇产科、神经外科、内科、急诊医学、医学遗传学、传染病等。\n\n![Left: Map of the countries in Africa from which data were sourced, color coded to indicate the relative fraction of contributions. Right: Bar graph showing relative contributions from each country.](https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-3c-Countries.width-1250.png)\n*AfriMed-QA问题和答案的来源国家分布图。*\n\n数据收集通过一个改编自Intron Health的基于网络的平台进行，并开发了定制的用户界面来收集不同类型的问题、进行质量审查以及对LLM响应进行盲评。为了避免消费者分享个人健康信息，消费者查询（CQs）通过疾病情景提示生成，LLM的回答由人类临床专家和消费者进行评分。\n\n![Example of questions used in dataset curation. Top: Multiple choice question; Middle: Short answer question; Bottom: Consumer query](https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-4a-Questions.width-1250.png)\n*AfriMed-QA数据集整理和LLM评估概述中的问题示例。*\n\n![A list of the medial specialties included in the dataset and the number of questions in each.](https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-5a-Specialties.width-1250.png)\n*AfriMed-QA中包含的医学专业及其问题数量列表。*\n\n### LLM评估与发现\n研究团队采用定量和定性方法评估了30个通用和生物医学LLMs。对于MCQs，通过比较LLM的单字母答案与参考答案来衡量准确性；对于SAQs，则通过语义相似性和句子级重叠来衡量。\n\n**主要发现：**\n*   **模型规模与性能：** 较大模型的基线性能在AfriMed-QA上比小型模型更准确。这对于偏好在设备或边缘部署小型专业模型的低资源环境可能不利。\n*   **模型类型与泛化：** 令人惊讶的是，基线通用模型比同等规模的生物医学模型表现更好且泛化能力更强。这可能归因于研究中开放生物医学模型的参数大小限制，或者表明专业LLMs过度拟合了其训练数据的特定偏差和细微差别，导致它们对AfriMed-QA数据集的独特特征适应性较差。\n\n![Bar graph of the performance of LLM models on the AfriMed-QA dataset.](https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-6a-Performance.width-1250.png)\n*LLM模型在AfriMed-QA数据集上的性能条形图（截至2025年5月的实验数据）。*\n\n### 人工评估LLM响应\n研究团队将3000个随机抽样的问题的LLM响应提交给Intron Health众包平台进行人工评估。评估标准包括不准确性、信息遗漏、人口统计偏见证据和潜在危害程度。\n\n*   **临床医生评估：** 评估LLM对MCQ、SAQ和CQ响应的正确性、本地化程度、是否存在遗漏或幻觉以及潜在危害。\n*   **非临床医生/消费者评估：** 评估LLM对CQ响应的相关性、帮助性和本地化程度。\n\n![Image of the interface used for expert review of LLM responses to AfriMed-QA.](https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-7-Interface.width-1250.png)\n*用于专家评审AfriMed-QA中LLM响应的界面图像。*\n\n评估采用5分制，评分者对答案来源（模型或人类）进行盲评。结果显示，消费者和临床医生对LLM在CQ上的响应表现出偏好，前沿LLMs在完整性、信息量和相关性方面始终优于临床医生提供的答案，并且更不容易出现幻觉和遗漏。与此一致，临床医生对CQs的回答在相关信息遗漏方面得分较低。\n\n![Plot showing the consumer blinded evaluations human clinical experts and LLM answers.](https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-8-Evals.width-1250.png)\n*消费者对人类临床专家和LLM答案的盲评图。图表显示了各项评估轴的平均评分和置信区间。*\n\n### 开放排行榜与未来展望\n研究团队开发了一个开放排行榜，方便用户可视化和比较LLM性能，并允许提交自己的模型进行评估。\n\n![Image of the landing page for the AfriMed-QA LLM leaderboard](https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-9-Leaderboard.width-1250.png)\n*AfriMed-QA LLM排行榜的登录页面图像，可比较不同模型在不同基准指标上的表现。*\n\n未来，AfriMed-QA联盟计划将数据集扩展到非英语官方和本土语言，并纳入多模态（如视觉和音频）问答数据集，以反映医学固有的多语言和多模态特性。\n\n### 局限性与行动呼吁\n尽管AfriMed-QA是首个大规模、多专业、本土来源的泛非洲数据集，但它并非完整。目前超过50%的专家MCQ问题来自尼日利亚，团队正努力扩大更多非洲地区和全球南方国家的代表性。这项工作为在缺乏数字化基准数据集的国家获取多样化和代表性的健康基准数据集奠定了基础。\n\n鉴于健康结果的敏感性，评估LLM的准确性、情境性和文化相关性至关重要。研究呼吁其他研究和健康组织通过合作和当地投入，进一步研究该领域，策划数据集以评估和优化LLM在其特定环境中的应用，以适应疾病流行率、文化背景、资源、药物类型、健康建议、医疗技术基础设施、可负担性、护理类型和敏感属性等方面的分布变化。",
      "shortSummary": "AfriMed-QA是一个开创性的泛非洲医学问答基准数据集，旨在评估和改进大型语言模型（LLMs）在全球健康领域的表现。该数据集包含约15,000个问题，涵盖多项选择题、简答题和消费者查询，数据来源于非洲12个国家的60多所医学院，覆盖32个医学专业。研究发现，大型LLMs表现优于小型模型，而通用模型在AfriMed-QA上意外地优于同等规模的生物医学模型。人工评估显示，前沿LLMs在消费者查询方面优于临床医生。AfriMed-QA已开源，并计划扩展至多语言和多模态，以促进LLMs在多样化、低资源医疗环境中的公平有效应用。",
      "translated_title": "AfriMed-QA：为全球健康领域的大型语言模型进行基准测试",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-3c-Countries.width-1250.png",
          "alt": "Left: Map of the countries in Africa from which data were sourced, color coded to indicate the relative fraction of contributions. Right: Bar graph showing relative contributions from each country.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-4a-Questions.width-1250.png",
          "alt": "Example of questions used in dataset curation. Top: Multiple choice question; Middle: Short answer question; Bottom: Consumer query",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-5a-Specialties.width-1250.png",
          "alt": "A list of the medial specialties included in the dataset and the number of questions in each.",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-6a-Performance.width-1250.png",
          "alt": "Bar graph of the performance of LLM models on the AfriMed-QA dataset.",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-7-Interface.width-1250.png",
          "alt": "Image of the interface used for expert review of LLM responses to AfriMed-QA.",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3s6d8\">Large language models (LLMs) have shown potential for medical and health question answering across various health-related tests spanning different formats and sources, such as multiple choice and short answer exam questions (e.g., <a href=\"https://arxiv.org/abs/2009.13081\" target=\"_blank\" rel=\"noopener noreferrer\">USMLE MedQA</a>), summarization, and clinical note taking, among others. Especially in low-resource settings, LLMs can potentially serve as valuable decision-support tools, enhancing clinical diagnostic accuracy and accessibility, and providing multilingual clinical decision support and health training, all of which are especially valuable at the community level.</p><p data-block-key=\"aosjl\">Despite their success on existing medical benchmarks, there is uncertainty about whether these models generalize to tasks involving distribution shifts in disease types, contextual differences across symptoms, or variations in language and linguistics, even within English. Further, localized cultural contexts and region-specific medical knowledge is important for models deployed outside of traditional Western settings. Yet without diverse benchmark datasets that reflect the breadth of real-world contexts, it’s impossible to train or evaluate models in these settings, highlighting the need for more diverse benchmark datasets.</p><p data-block-key=\"fnob9\">To address this gap, we present <a href=\"https://aclanthology.org/2025.acl-long.96.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">AfriMed-QA</a>, a benchmark question–answer dataset that brings together consumer-style questions and medical school–type exams from 60 medical schools, across 16 countries in Africa. We developed the dataset in collaboration with numerous partners, including <a href=\"https://www.intron.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Intron health</a>, <a href=\"https://sisonkebiotik.africa/\" target=\"_blank\" rel=\"noopener noreferrer\">Sisonkebiotik</a>, <a href=\"https://ucc.edu.gh/\" target=\"_blank\" rel=\"noopener noreferrer\">University of Cape Coast</a>, the <a href=\"https://famsanet.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Federation of African Medical Students Association</a>, and <a href=\"https://bioramp.org/\" target=\"_blank\" rel=\"noopener noreferrer\">BioRAMP</a>, which collectively form the <a href=\"https://afrimedqa.com/\" target=\"_blank\" rel=\"noopener noreferrer\">AfriMed-QA consortium</a>, and with support from <a href=\"https://media.path.org/documents/GATES-AI_brief_March_2025_PNpefpK.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">PATH/The Gates Foundation</a>. We evaluated LLM responses on these datasets, comparing them to answers provided by human experts and rating their responses according to human preference. The methods used in this project can be scaled to other locales where digitized benchmarks may not currently be available.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/AfriMedQA-1-Overview.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3s6d8\">AfriMed-QA was published at <a href=\"https://2025.aclweb.org/\" target=\"_blank\" rel=\"noopener noreferrer\">ACL 2025</a> where it won the <a href=\"https://2025.aclweb.org/program/awards/\" target=\"_blank\" rel=\"noopener noreferrer\">Best Social Impact Paper Award</a>. The dataset was recently leveraged to assist in training of <a href=\"https://developers.google.com/health-ai-developer-foundations/medgemma/model-card\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma</a>, our <a href=\"https://research.google/blog/medgemma-our-most-capable-open-models-for-health-ai-development/\">latest open model</a> for multimodal medical text and image comprehension. The <a href=\"https://huggingface.co/datasets/afrimedqa/afrimedqa_v2\" target=\"_blank\" rel=\"noopener noreferrer\">AfriMed-QA benchmark datasets</a> and <a href=\"https://github.com/afrimedqa/AfriMed-QA\" target=\"_blank\" rel=\"noopener noreferrer\">LLM evaluation code</a> are open-sourced and available for use by the community.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/AfriMedQA-2-Storyboard.m4v\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">AfriMed-QA dataset</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"3s6d8\">The <a href=\"https://huggingface.co/datasets/afrimedqa/afrimedqa_v2\" target=\"_blank\" rel=\"noopener noreferrer\">AfriMed-QA dataset</a> is the first large-scale pan-African multi-specialty medical question–answer dataset designed to evaluate and develop equitable and effective LLMs for African healthcare. The dataset comprises ~15,000 clinically diverse questions and answers in English, 4,000+ expert multiple choice questions (MCQs) with answers, over 1,200 open ended short answer (SAQs) with long-form answers, and 10,000 consumer queries (CQ). The dataset is designed to rigorously assess LLM performance for correctness and geographical shifts. It was crowd-sourced from 621 contributors, from over 60 medical schools across 12 countries, covering 32 medical specialties, including obstetrics and gynecology, neurosurgery, internal medicine, emergency medicine, medical genetics, infectious disease, and others.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-3c-Countries.width-1250.png\" alt=\"Left: Map of the countries in Africa from which data were sourced, color coded to indicate the relative fraction of contributions. Right: Bar graph showing relative contributions from each country.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-3c-Countries.width-1250.png\" alt=\"Left: Map of the countries in Africa from which data were sourced, color coded to indicate the relative fraction of contributions. Right: Bar graph showing relative contributions from each country.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"q7thn\">Countries where AfriMed-QA questions and answers were sourced.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"nnfxj\">To collect these data, we adapted a web-based platform previously developed by Intron Health for crowd-sourcing accented and multilingual clinical speech data at scale across Africa. We developed custom user interfaces to collect each question type, for quality reviews, and for blinded human evaluation of LLM responses.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-4a-Questions.width-1250.png\" alt=\"Example of questions used in dataset curation. Top: Multiple choice question; Middle: Short answer question; Bottom: Consumer query\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-4a-Questions.width-1250.png\" alt=\"Example of questions used in dataset curation. Top: Multiple choice question; Middle: Short answer question; Bottom: Consumer query\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"c0ibi\">AfriMed-QA dataset curation and LLM evaluation overview. MCQs and SAQs from medical schools had accompanying human labels. For CQs, to avoid consumers sharing their own health information which might lead to potential disclosure of health information, and repetitiveness in question types, consumers were prompted with a disease scenario, and they responded with a question they would ask based on it. The scenario and question were passed to an LLM and the LLM responses were rated by human clinical experts as well as consumers.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-5a-Specialties.width-1250.png\" alt=\"A list of the medial specialties included in the dataset and the number of questions in each.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-5a-Specialties.width-1250.png\" alt=\"A list of the medial specialties included in the dataset and the number of questions in each.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"c0ibi\">Medical specialties represented in AfriMed-QA.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Evaluation of LLM responses</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"nnfxj\">Using quantitative and qualitative approaches, we evaluated 30 general and biomedical LLMs, ranging in size from small to large. Some were open and others were closed. For MCQs, we measured the accuracy by comparing each LLM’s single-letter answer choice with the reference. For SAQs, we measured semantic similarity and sentence level overlap comparing the generated response from the language model against a reference answer.</p><p data-block-key=\"8i9dn\">We found that the baseline performance of larger models is more accurate than small models on AfriMed-QA. This trend may be unfavorable to low-resource settings where on-device or edge deployments with smaller specialized models are preferred.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-6a-Performance.width-1250.png\" alt=\"Bar graph of the performance of LLM models on the AfriMed-QA dataset.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-6a-Performance.width-1250.png\" alt=\"Bar graph of the performance of LLM models on the AfriMed-QA dataset.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"c0ibi\"><i>Performance of LLM models on the AfriMed-QA dataset (experiments as of May 2025).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"nnfxj\">We also found that baseline general models outperform and generalize better than biomedical models of similar size. This counterintuitive result could be due to the parameter size limitations of open biomedical models in our study or it could indicate that specialized LLMs overfit to the specific biases and nuances of the data on which they are fine-tuned. In either case, they seem to be less adaptable to the unique characteristics of the AfriMed-QA dataset.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Human rating of LLM responses</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"nnfxj\">LLM responses to a fixed subset of questions (<i>n</i>=3000; randomly sampled) were sent out for human evaluation on the Intron Health crowd-sourcing platform. Adapting the evaluation axes described in our <a href=\"https://www.nature.com/articles/s41591-024-03423-7\" target=\"_blank\" rel=\"noopener noreferrer\">MedLM paper</a>, which included measures for inaccuracy, omission of information, evidence of demographic bias, and extent of harm, we collected human evaluations in two categories:</p><ol><li data-block-key=\"5ol9l\"><i>Clinicians</i> provided ratings to the LLM’s MCQ, SAQ, and CQ responses, evaluating whether answers were correct and localized, if omissions or hallucinations were present, and if potential for harm existed.</li><li data-block-key=\"nhmf\"><i>Non-clinicians/consumers</i> rated CQ LLM responses to determine if answers were relevant, helpful, and localized.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-7-Interface.width-1250.png\" alt=\"Image of the interface used for expert review of LLM responses to AfriMed-QA.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-7-Interface.width-1250.png\" alt=\"Image of the interface used for expert review of LLM responses to AfriMed-QA.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"c0ibi\"><i>Interface used for expert review of LLM responses to AfriMed-QA.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"nnfxj\">Ratings were on a 5-point scale representing the extent to which the criteria were met. “1” represents “No\" or “completely absent\" and “5” represents “Yes\" or “absolutely present\". Raters were blinded to the answer source (model name or human) and each rater was asked to evaluate answers from multiple LLMs in a random sequence.</p><p data-block-key=\"8mi0p\">Consumer and clinician human evaluation of LLM answers to CQs revealed a preference for LLM responses, where frontier LLMs were consistently rated to be more complete, informative, and relevant when compared with clinician-provided answers, and less susceptible to hallucinations and omissions. Consistent with this, clinician answers to CQs were also rated worse when measured for omission of relevant information.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-8-Evals.width-1250.png\" alt=\"Plot showing the consumer blinded evaluations human clinical experts and LLM answers.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-8-Evals.width-1250.png\" alt=\"Plot showing the consumer blinded evaluations human clinical experts and LLM answers.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"c0ibi\"><i>Consumer blinded evaluations of human clinical experts and LLM answers. Plots show mean ratings and confidence intervals across various axes.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Building an open Leaderboard for easy comparison of data versions and LLM versions</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"nnfxj\">We have developed a leaderboard for easy visualization and comparison of LLM performance. Users can compare existing models or submit their own models and see how well they perform on the dataset.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-9-Leaderboard.width-1250.png\" alt=\"Image of the landing page for the AfriMed-QA LLM leaderboard\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AfriMedQA-9-Leaderboard.width-1250.png\" alt=\"Image of the landing page for the AfriMed-QA LLM leaderboard\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"c0ibi\"><i>AfriMed-QA leaderboard enables comparison of different models across different benchmark metrics.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Towards a multilingual, multimodal dataset</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"nnfxj\">We recognize that medicine is inherently multilingual and multimodal and are currently working with the AfriMed-QA consortium led by <a href=\"https://directory.ucc.edu.gh/p/stephen-moore\" target=\"_blank\" rel=\"noopener noreferrer\">Prof. Stephen Moore</a> at the University of Cape Coast to expand beyond English-only text-based question answering to non-English official and native languages from the continent. We are also working to incorporate multimodal (e.g., visual and audio) question answering datasets.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Limitations</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"nnfxj\">Although this is the first large-scale, multi-specialty, indigenously sourced pan-African dataset of its kind, it is by no means complete. Over 50% of the expert MCQ questions came from Nigeria. We are working to expand representation from more African regions and the Global South.</p><p data-block-key=\"8mlsj\">While the development of the dataset is still in progress, this work establishes a foundation for acquiring diverse and representative health benchmark datasets across countries that may not have digitized and readily available benchmark datasets.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">LLMs for geographically diverse health QAs</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"nnfxj\">Given the sensitivity of health-related outcomes, it is essential that LLMs are evaluated for accurate, contextual, and culturally relevant performance. Across different settings one can anticipate a variety of distribution shifts to which LLMs need to adapt. These include disease prevalence, cultural context, resources and infrastructure, drug types and nomenclature, differences in health recommendations for screening and treatment, medical technology infrastructure, affordability, care types, and sensitive attributes. While our evaluations are limited, we present a call to action for other research and health organizations to pursue further research in this area, curating datasets to evaluate and optimize LLMs for use in their contexts through partnerships and local input.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"nnfxj\"><i>We would like to acknowledge the incredible AfriMed-QA consortium and co-authors. Tobi Olatunji, Charles Nimo, Abraham Owodunni, Tassallah Abdullahi, Emmanuel Ayodele, Mardhiyah Sanni, Chinemelu Aka, Folafunmi Omofoye, Foutse Yuehgoh, Timothy Faniran, Bonaventure F. P. Dossou, Moshood Yekini, Jonas Kemp, Katherine Heller, Jude Chidubem Omeke, Chidi Asuzu, Naome A. Etori, Aimérou Ndiaye, Ifeoma Okoh, Evans Doe Ocansey, Wendy Kinara, Michael Best, Irfan Essa, Stephen Edward Moore, and Chris Fourie. We would also like to thank Bilal Mateen, Melissa Miles, Mira Emmanuel-Fabula, and Celeste Gonda from the Gates Foundation/PATH Digital Square for their support of the work and all data contributors. Finally, we thank Marian Croak for her leadership and support.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "时间序列基础模型可以成为少样本学习器 (原标题: Time series foundation models can be few-shot learners)",
      "link": "https://research.google/blog/time-series-foundation-models-can-be-few-shot-learners/",
      "pubDate": "Mon, 22 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-22T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 时间序列基础模型可以成为少样本学习器\n\n## 引言：时间序列预测的挑战与少样本学习\n\n现代商业中，时间序列预测至关重要，但传统方法为每个任务构建专业模型，耗时且需大量专业知识。零样本学习（如我们之前的TimesFM模型）提供了一种解决方案，无需特定任务训练即可准确预测。然而，研究发现，少量示例可以进一步提升预测效果，例如利用附近高速公路或同条高速公路的历史数据来预测交通流量。传统的监督微调方法虽然能利用特定数据微调模型，却重新引入了零样本学习试图避免的复杂性。\n\n为了解决这一问题，我们在ICML 2025上提出了新工作“时间序列基础模型的上下文内微调”（In-Context Fine-Tuning for Time-Series Foundation Models），引入了一种新颖的方法，将TimesFM转变为一个少样本学习器。该方法通过持续预训练，教会模型如何在推理时从少量示例中学习。其结果是获得了一种强大的新能力，能够匹配监督微调的性能，而无需用户进行额外的复杂训练。\n\n![LLM少样本提示与时间序列基础模型少样本提示的比较](https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-1-Prompting.width-1250.png)\n\n*   **图片描述**：左侧展示了大型语言模型（LLM）的少样本提示，右侧展示了时间序列基础模型如何通过任意数量的相关上下文时间序列示例进行少样本提示。橙色框内是模型的输入。\n\n## 模型重设计：TimesFM-ICF\n\n为了实现少样本学习，我们对TimesFM模型进行了重新设计，创建了TimesFM-ICF（In-Context Fine-tuning）。\n\n### TimesFM基础架构\n\nTimesFM是一个补丁解码器，其工作原理如下：\n*   将每32个连续时间点（一个补丁）标记化为输入token。\n*   在输入token序列之上应用一个Transformer堆栈来生成输出token。\n*   然后应用一个共享的多层感知器（MLP）将每个输出token转换回128个时间点的时间序列。\n\n### 创建TimesFM-ICF\n\n我们从基础TimesFM模型开始，并使用新的上下文（预测历史加上所有上下文内示例）继续进行预训练。关键挑战在于确保模型不会混淆预测历史和上下文内示例。\n\n*   **引入通用分隔符token**：为了解决混淆问题，我们引入了一个特殊的、可学习的“通用分隔符token”。这个token就像一个数字“停车标志”或“新段落”符号，放置在每组数字之后。当模型关注到它之前看到的示例的分隔符token时，它就不会将其与当前尝试预测的数据混淆。这使得模型能够从过去的示例模式中学习，并将这些知识应用于当前的预测。\n\n![展示如何在上下文示例流中放置通用分隔符token以指定新数据源的示例](https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-2-Separators.width-1250.png)\n\n*   **图片描述**：如果没有分隔符，简单地连接上下文示例可能会使模型混淆，多个单调趋势可能看起来像一个锯齿状的连续模式。\n\n### 持续预训练\n\n由于分隔符token及其注意力机制对TimesFM来说是全新的，我们的第二步是继续预训练基础TimesFM模型，以教授它这些新引入的机制。具体方法是：\n*   创建一个包含上下文内示例和分隔符token的新数据集。\n*   应用标准的仅解码器下一token预测训练。\n*   **架构流程**：输入传递给MLP层生成token，这些token传递给因果自注意力（CSA）层（防止模型“偷看”未来），CSA层再馈入前馈网络（FFN）。CSA和FFN重复多次（即堆叠的Transformer），然后将结果连接到输出MLP层。\n\n## 模型测试与评估\n\n我们对TimesFM-ICF进行了严格评估，使用了23个模型在任何训练阶段都未曾见过的数据集。每个基准数据集都包含多个时间序列。在预测一个时间序列时，我们从其即时历史开始，然后从其完整历史和同一数据集中其他时间序列的历史中采样序列作为上下文内示例，以确保相关性且无数据泄露。\n\n### 性能指标与基线\n\n我们关注以下性能指标和基线模型：\n*   **性能指标**：几何平均（GM）聚合的平均绝对比例误差（MASE），通过朴素重复最后一个季节性模式进行归一化。\n*   **基线模型**：\n    *   **TimesFM (Base)**：我们开始使用的预训练模型。\n    *   **TimesFM-FT**：TimesFM (Base) 经过监督微调，使用每个数据集的训练集进行微调，并在相应测试集上评估。这是一个强大的基线，代表了领域适应的最佳实践。\n\n![条形图比较TimesFM-ICF与TimesFM (Base) 在多个特定任务模型上的性能](https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-4-Performance.width-1250.png)\n\n*   **图片描述**：TimesFM-ICF在许多特定任务模型上改进了TimesFM (Base) 的性能，并达到了与TimesFM-FT相同的性能。TimesFM-FT是针对每个特定数据集分别进行微调的TimesFM版本。\n\n### 关键发现\n\n*   **准确性提升**：TimesFM-ICF比TimesFM (Base) 准确率提高了6.8%。\n*   **匹配监督微调性能**：更令人惊喜和鼓舞的是，TimesFM-ICF在无需监督微调的复杂性下，达到了与TimesFM-FT相同的性能。\n*   **其他特性**：\n    *   随着上下文内示例的增加，模型预测的准确性会提高（尽管推理时间会相应增加），这符合我们的预期。\n    *   与不具备处理上下文内示例能力的纯粹长上下文模型相比，TimesFM-ICF显示出更好的上下文利用率。\n\n## 未来展望：更易用、更强大的预测\n\n这种新方法具有重要的实际应用价值，因为它允许企业部署一个更强大、适应性更强的单一预测模型。企业无需为预测新产品需求等新任务启动一个完整的机器学习项目，只需向模型提供少量相关的示例即可。这能立即提供最先进的专业预测，从而大幅降低成本，加速决策和创新，并使高端预测民主化。\n\n我们对这项研究的未来感到兴奋，特别是开发自动选择最相关上下文内示例的策略。通过使基础模型更智能、更具适应性，我们赋能更多用户做出更好的数据驱动决策。",
      "shortSummary": "新的研究引入了TimesFM-ICF，一个将时间序列基础模型TimesFM转变为少样本学习器的方法。传统预测模型复杂且耗时，而TimesFM-ICF通过在持续预训练中引入“通用分隔符token”，使模型能在推理时从少量上下文示例中学习。这种方法无需复杂的监督微调，即可将预测准确性比基础模型提高6.8%，并达到与监督微调相当的性能。这使得先进的时间序列预测更易于部署和使用，显著降低成本，加速决策，并促进创新。",
      "translated_title": "时间序列基础模型可以成为少样本学习器",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-1-Prompting.width-1250.png",
          "alt": "Comparison of few-shot prompting of an LLM (left) with few-shot prompting of a time-series foundation model using an arbitrary number of related in-context time series examples (right).",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-2-Separators.width-1250.png",
          "alt": "Example showing how common separator tokens would be placed to designate new data sources in a stream of in-context examples.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-4-Performance.width-1250.png",
          "alt": "Bar graph comparing performance of TimesFM-ICF to TimesFM (base) over many task-specific models.",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"exbjr\"><a href=\"https://en.wikipedia.org/wiki/Time_series\" target=\"_blank\" rel=\"noopener noreferrer\">Time-series forecasting</a> is essential for modern businesses, helping them predict everything from inventory needs to energy demands. Traditionally, this has involved building a separate, specialized model for each task — a process that is slow and requires significant expertise.</p><p data-block-key=\"50kh8\">The emergence of <a href=\"https://en.wikipedia.org/wiki/Zero-shot_learning\" target=\"_blank\" rel=\"noopener noreferrer\">zero-shot learning</a> offered a solution. Our previous model, <a href=\"https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/\">TimesFM</a>, was a zero-shot, pre-trained foundation model that could accurately forecast without task-specific training. But what if a few examples could make the forecast even better? For instance, forecasting highway traffic would be more accurate if the model could consider data from other nearby highways or from the same highway a few weeks ago. The standard solution, supervised fine-tuning, which uses curated data to fine-tune an existing model, reintroduces the complexity one hopes to avoid with zero-shot learning.</p><p data-block-key=\"c2v01\">In our new work, \"<a href=\"https://icml.cc/virtual/2025/poster/43707\" target=\"_blank\" rel=\"noopener noreferrer\">In-Context Fine-Tuning for Time-Series Foundation Models</a>\", presented at ICML 2025, we introduce a novel approach that transforms TimesFM into a <a href=\"https://arxiv.org/abs/2203.04291\" target=\"_blank\" rel=\"noopener noreferrer\">few-shot learner</a>. This method uses continued pre-training to teach the model how to learn from a handful of examples at inference time. The result is a powerful new capability that matches the performance of supervised fine-tuning without requiring additional complex training from the user.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-1-Prompting.width-1250.png\" alt=\"Comparison of few-shot prompting of an LLM (left) with few-shot prompting of a time-series foundation model using an arbitrary number of related in-context time series examples (right).\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-1-Prompting.width-1250.png\" alt=\"Comparison of few-shot prompting of an LLM (left) with few-shot prompting of a time-series foundation model using an arbitrary number of related in-context time series examples (right).\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"h2yvf\"><i>Similar to few-shot prompting of an LLM (</i><b><i>left</i></b><i>), a time-series foundation model should support few-shot prompting with an arbitrary number of related in-context time series examples (</i><b><i>right</i></b><i>). The orange box encloses the inputs to the models.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Redesigning the model</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"exbjr\"><a href=\"https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/\">TimesFM</a> is a patched decoder that tokenizes every 32 contiguous timepoints (a patch) as an <a href=\"https://mehmetozkaya.medium.com/how-llms-use-tokens-ec5916ee321a\" target=\"_blank\" rel=\"noopener noreferrer\">input token</a> and applies a <a href=\"https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\" target=\"_blank\" rel=\"noopener noreferrer\">transformer</a> stack on top of the sequence of input tokens to generate the output tokens. It then applies a shared <a href=\"https://en.wikipedia.org/wiki/Multilayer_perceptron\" target=\"_blank\" rel=\"noopener noreferrer\">multilayer perceptron</a> (MLP) to translate each output token back to a time series of 128 timepoints.</p><p data-block-key=\"5q88o\">To create TimesFM-ICF (In-Context Fine-tuning), we start with the base TimesFM model and continue the pre-training with new context: the forecast history plus all in-context examples. The first step is to make sure the model doesn’t confuse or conflate the forecasting history and the in-context examples. Imagine you're giving the model a list of numbers that represent a few different things, maybe sunglasses sales figures from one store, then umbrella sales figures from another. If you just merge all those numbers together, the model might get confused, thinking it's one continuous stream of data. For example, if the first store’s sales were going up and the second store’s sales were going down, the model might incorrectly see it as a single up-and-down pattern, rather than two separate, simple trends.</p><p data-block-key=\"66jf\">To fix this, we put a special, learnable “common separator token” — like a digital \"stop sign\" or a \"new paragraph\" symbol — after each set of numbers. With these separators in place, as soon as the model attends to the separator token of an example it has seen before, it won't mix it up with the data it's currently trying to predict. This theoretically allows the model to learn from patterns in those past examples and apply that knowledge to the current forecast. For instance, the model could learn that \"all the store sales are showing consistent, directional trends lately, so I should predict an upward trend for my new store’s sunscreen sales.\"</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-2-Separators.width-1250.png\" alt=\"Example showing how common separator tokens would be placed to designate new data sources in a stream of in-context examples.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-2-Separators.width-1250.png\" alt=\"Example showing how common separator tokens would be placed to designate new data sources in a stream of in-context examples.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"h2yvf\"><i>Concatenating in-context examples without separators could confuse the model — multiple monotonic trends might look like a jagged, continuous pattern if concatenated naïvely.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"exbjr\">Since the separator tokens and the attention to them are new for TimesFM, our second step involves continuing the pre-training of the base TimesFM model to teach it about the new introductions. The recipe here is actually straightforward: we created a new dataset that includes both in-context examples and separator tokens, and we applied standard decoder-only next-token prediction training. Inputs are passed to the MLP layer, which generates tokens. These are passed to a <a href=\"https://en.wikipedia.org/wiki/Attention_(machine_learning)#Masking\" target=\"_blank\" rel=\"noopener noreferrer\">causal self attention</a> (CSA) layer that \"attends to\" information from previous tokens in the sequence, a step that's crucial in tasks like time-series forecasting as it prevents the model from looking into the future. The CSA then feeds into a <a href=\"https://en.wikipedia.org/wiki/Feedforward_neural_network\" target=\"_blank\" rel=\"noopener noreferrer\">feed-forward network</a> (FFN). We repeat CSA and FFN multiple times (i.e., the <a href=\"https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\" target=\"_blank\" rel=\"noopener noreferrer\">stacked transformers</a>) before connecting the result to the output MLP layer.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Time_Foundation_v3.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"h2yvf\"><i>TimesFM-ICF employs the decoder-only architecture for time-series forecasting with in-context examples. A special common separator token is introduced to disambiguate between the in-context examples and the task history.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Testing the model</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"exbjr\">We evaluated TimesFM-ICF on 23 datasets that the model had never seen during any phase of its training. Each dataset in this benchmark has multiple time series. When we forecast a time series, we start with its immediate history, then sample sequences from its full history and the histories of other time series in the same dataset as in-context examples. This ensures the in-context examples are relevant and there is no leakage.</p><p data-block-key=\"3u0jv\">The chart below shows the <a href=\"https://en.wikipedia.org/wiki/Geometric_mean\" target=\"_blank\" rel=\"noopener noreferrer\">geometric mean</a> (GM) aggregation of the <a href=\"https://en.wikipedia.org/wiki/Mean_absolute_scaled_error\" target=\"_blank\" rel=\"noopener noreferrer\">mean absolute scaled errors</a> (MASE) normalized by a <a href=\"https://en.wikipedia.org/wiki/Forecasting#Seasonality_and_cyclic_behaviour\" target=\"_blank\" rel=\"noopener noreferrer\">naïve repeat of the last seasonal pattern</a>. We focus on two baselines here:</p><ul><li data-block-key=\"dqkkk\">TimesFM (Base), which is the pre-trained model from which we started.</li><li data-block-key=\"8ourm\">TimesFM-FT is TimesFM (Base) with supervised fine-tuning using the train split per dataset and then evaluated on the corresponding test split. This is a strong baseline that reflects the previous best practice for domain adaptation.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-4-Performance.width-1250.png\" alt=\"Bar graph comparing performance of TimesFM-ICF to TimesFM (base) over many task-specific models.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/TimeSeriesFM-4-Performance.width-1250.png\" alt=\"Bar graph comparing performance of TimesFM-ICF to TimesFM (base) over many task-specific models.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"h2yvf\"><i>TimesFM-ICF improves the performance of TimesFM (Base) over many task-specific models and achieves the same performance as that of TimesFM-FT, which is a version of TimesFM fine-tuned for each specific dataset, respectively.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"exbjr\">TimesFM-ICF is 6.8% more accurate than TimesFM (Base). What’s more surprising and inspiring is that it matches the performance of TimesFM-FT without the hassle of running supervised fine-tuning.</p><p data-block-key=\"3qdc5\">Besides the accuracy improvement, TimesFM-ICF also demonstrates other desirable properties. For example, it is consistent with our expectation that with more in-context examples, a model will make more accurate forecasts at the cost of longer inference time. In addition, TimesFM-ICF shows better utilization of its context when compared to a purely long-context model that does not have the ability to work with in-context examples.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The future: More accessible and powerful forecasting</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"exbjr\">This new approach has significant real-world applications because it allows businesses to deploy a more robust and adaptable single, powerful forecasting model. Instead of launching a full ML project for new tasks, like forecasting demand for a new product, they can simply feed the model a few new relevant examples. This immediately provides state-of-the-art, specialized forecasts, dramatically cutting costs, accelerating decision-making and innovation, and democratizing access to high-end forecasting.</p><p data-block-key=\"bk79l\">We're excited by this research's future, particularly developing automated strategies for selecting the most relevant in-context examples. By making foundation models more intelligent and adaptable, we empower more users to make better, data-driven decisions.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"exbjr\"><i>This research was led by then-student researcher Matthew Faw in collaboration with Google Research colleagues Abhimanyu Das and Ivan Kuznetsov. This blog post was brought to life with the tremendous help from editors Mark Simborg and Kimberly Schwede.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "测试时扩散深度研究代理 (原标题: Deep researcher with test-time diffusion)",
      "link": "https://research.google/blog/deep-researcher-with-test-time-diffusion/",
      "pubDate": "Thu, 18 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-18T16:00:00.000Z",
      "creator": "Google",
      "summary": "大型语言模型（LLMs）的最新进展推动了深度研究（DR）代理的兴起，这些代理能够生成新想法、检索信息、执行实验并撰写报告。然而，现有DR代理通常将不同工具简单组合，缺乏人类研究中迭代的“规划、起草、研究和基于反馈迭代”过程，尤其是在复杂主题的论文写作中。它们缺少通过研究来完善和加强论点的关键“去噪”步骤。\n\n**TTD-DR：模仿人类研究的深度研究代理**\n\n本文介绍了**测试时扩散深度研究代理（Test-Time Diffusion Deep Researcher, TTD-DR）**，这是首个将研究报告写作建模为扩散过程的研究代理，即从一个“嘈杂”或初步的草稿逐步完善为高质量的最终版本。TTD-DR引入了两种协同工作的新算法：\n\n*   **组件级自进化优化**：提升研究工作流中每个步骤的质量。\n*   **报告级检索去噪细化**：利用新检索到的信息修订和改进报告草稿。\n\nTTD-DR在长篇报告写作和多跳推理任务上取得了最先进的成果。\n\n**TTD-DR 的设计理念**\n\nTTD-DR接收用户查询后，会创建一个初步草稿作为不断演进的基础，以指导研究计划。这个草稿通过“检索去噪”过程（报告级细化）进行迭代完善，将找到的信息用于改进草稿。这个过程在一个连续的循环中进行，每次循环都会改进报告。此外，一个自进化算法不断增强从初始计划到最终报告的整个过程，这种细化和自我改进的强大组合使得报告写作过程更加连贯。\n\n![TTD-DR示意图](https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-1.width-1250.png)\n*TTD-DR示意图。其设计旨在通过迭代的起草和修订周期来模仿典型的研究实践。*\n\n**核心DR设计（三阶段）**\n\nTTD-DR的核心DR设计包含三个阶段：\n\n1.  **研究计划生成**：根据用户查询生成结构化的研究计划，概述最终报告所需的关键领域，作为后续信息收集的初步指导。\n2.  **迭代搜索**：包含两个子代理：\n    *   **搜索问题生成（阶段2a）**：根据研究计划、用户查询和先前搜索迭代的上下文（即过去的问答）制定搜索查询。\n    *   **答案搜索（阶段2b）**：搜索可用来源以查找相关文档并返回总结的答案，类似于检索增强生成（RAG）系统。\n3.  **最终报告生成**：通过结合所有收集到的结构化信息（计划和一系列问答对）生成全面且连贯的最终报告。\n\n![核心DR代理设计图](https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-2.width-1250.png)\n*我们的核心DR代理分三个阶段运行。阶段1生成详细的研究计划；阶段2a迭代生成搜索问题，然后使用类似RAG的系统从检索到的文档中合成精确答案（2b）；阶段3合成所有收集到的信息以生成最终报告。*\n\n**组件级自进化**\n\nTTD-DR利用自进化算法来增强每个阶段代理的性能，以发现并保留高质量的上下文。该过程包括：\n\n*   **初始状态**：基于前一阶段输出的多个多样化答案变体，用于探索更大的搜索空间。\n*   **环境反馈**：每个答案变体由一个作为评判者的LLM进行评估，利用自动评分器评估有用性和全面性等指标，并生成文本反馈以改进答案。\n*   **修订**：根据评分和反馈，每个变体进行修订，以适应更好的评分。环境反馈和修订步骤重复进行，直到达到最大迭代次数或代理确定不再需要修订。\n*   **交叉**：最终，多个修订后的变体合并为一个高质量的输出，为主要报告生成过程提供卓越的上下文。\n\n![组件级自进化算法示意图](https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-3.width-1250.png)\n*组件级自进化算法应用于搜索答案（阶段2b）的示意图。该过程从多个初始答案变体开始，每个变体都经历一个自进化过程，首先与环境交互以获得适应度分数和反馈，然后根据反馈进行修订。这个过程重复进行，直到达到最大迭代次数。最后，来自所有迭代的多个修订后的变体被合并以生成最终答案。*\n\n**报告级检索去噪**\n\nTTD-DR使用搜索工具来去噪和演进草稿。具体而言，当前报告草稿被输入到核心DR工作流的搜索生成阶段（阶段2a），以指导下一个搜索查询的生成。在答案搜索阶段（阶段2b）获得合成答案后，新信息用于修订报告草稿，无论是添加新细节还是验证现有信息。这种将去噪后的报告反馈以生成下一个搜索查询的过程会重复进行。草稿逐步去噪，直到搜索过程结束，此时最终代理根据所有历史搜索答案和修订撰写最终报告（阶段3）。\n\n**实验结果**\n\nTTD-DR在以下基准数据集上进行了评估：\n\n1.  **复杂查询**：需要研究代理生成长篇综合报告（DeepConsult）。\n2.  **多跳查询**：需要广泛搜索和推理才能回答（Humanity's Last Exam [HLE] 和 GAIA，以及HLE-Search的200个子查询）。\n\n与OpenAI Deep Research相比，TTD-DR在所有基准测试中始终取得更好的结果：\n\n*   在长篇研究报告生成任务中，TTD-DR的胜率达到74.5%。\n*   在两个广泛研究数据集（HLE-Search和GAIA）上，分别超越OpenAI DR 7.7%和1.7%。\n\n![TTD-DR性能对比图](https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-4.width-1250.png)\n*TTD-DR在基准数据集上与不同基线系统的性能对比。左图：胜率（%）基于OpenAI DR计算。右图：正确性计算为系统预测答案与参考答案的匹配度。TTD-DR以显著优势优于OpenAI DR。*\n\n**消融研究**\n\n消融研究逐步增加了三种方法（核心DR、自进化、检索去噪）。使用Gemini-2.5-pro作为基础模型：\n\n*   仅核心DR代理表现不如OpenAI DR。\n*   加入自进化算法后，DeepConsult的胜率达到59.8%，HLE-Search和GAIA的正确性分数分别提高4.4%和1.2%。\n*   最终，结合检索去噪带来了所有基准测试的显著提升。\n\n![TTD-DR消融研究图](https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-5.width-1250.png)\n*TTD-DR通过逐步添加1) 核心DR，2) 自进化，和3) 检索去噪的性能。我们观察到全面逐步改进，帮助我们实现新的最先进结果。*\n\n**效率**\n\nTTD-DR在测试时扩展效率方面也优于其他DR代理。在相同的延迟下，TTD-DR实现了更好的质量/胜率。\n\n![质量与延迟的帕累托前沿图](https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-6.width-1250.png)\n*研究报告质量与延迟（秒）的帕累托前沿图。蓝线表示TTD-DR，灰色点表示对比的DR代理。*\n\n**结论**\n\nTTD-DR是一个受人类迭代研究方式启发的新框架，通过将报告生成概念化为扩散过程，解决了现有DR代理的局限性。它在需要密集搜索和多跳推理的各种基准测试中显著优于现有DR代理，并在生成长篇综合研究报告和识别多跳搜索与推理任务的简洁答案方面表现出最先进的性能。其“草稿优先”的设计被认为是其成功的关键。\n\n**可用性**\n\n该工作的产品版本已在Google Agentspace上提供，并使用Google Cloud Agent Development Kit实现。",
      "shortSummary": "TTD-DR（测试时扩散深度研究代理）是一个模仿人类迭代研究过程的新型AI代理。它将报告写作建模为从初步草稿到高质量最终版本的扩散过程，并结合了组件级自进化和报告级检索去噪两种算法。TTD-DR在长篇报告写作和多跳推理任务上取得了最先进的成果，并在多项基准测试中显著优于OpenAI Deep Research等现有代理。其“草稿优先”的设计是其成功的关键。",
      "translated_title": "测试时扩散深度研究代理",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-1.width-1250.png",
          "alt": "Deep-Researcher-1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-2.width-1250.png",
          "alt": "Deep-Researcher-2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-3.width-1250.png",
          "alt": "Deep-Researcher-3",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-4.width-1250.png",
          "alt": "Deep-Researcher-4",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-5.width-1250.png",
          "alt": "Deep-Researcher-5",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ocegg\">The recent advances in large language models (LLMs) have fueled the emergence of <a href=\"https://openai.com/index/introducing-deep-research/\" target=\"_blank\" rel=\"noopener noreferrer\">deep research</a> (DR) agents. These agents demonstrate remarkable capabilities, including the generation of <a href=\"https://arxiv.org/abs/2409.04109\" target=\"_blank\" rel=\"noopener noreferrer\">novel ideas</a>, efficient <a href=\"https://arxiv.org/abs/2503.09516\" target=\"_blank\" rel=\"noopener noreferrer\">information retrieval</a>, experimental execution, and the subsequent drafting of comprehensive <a href=\"https://arxiv.org/pdf/2408.06941\" target=\"_blank\" rel=\"noopener noreferrer\">reports</a> and <a href=\"https://arxiv.org/abs/2504.08066\" target=\"_blank\" rel=\"noopener noreferrer\">academic papers</a>.</p><p data-block-key=\"acuge\">Currently, most <a href=\"https://github.com/assafelovic/gpt-researcher\" target=\"_blank\" rel=\"noopener noreferrer\">public DR agents</a> use a variety of clever techniques to improve their results, like <a href=\"https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/\">performing reasoning via chain-of-thought</a> or <a href=\"https://openreview.net/forum?id=H4S4ETc8c9\" target=\"_blank\" rel=\"noopener noreferrer\">generating multiple answers</a> and selecting the best one. While they've made impressive progress, they often bolt different tools together without considering the iterative nature of human research. They're missing the key process (i.e., planning, drafting, researching, and iterating based on feedback) on which people rely when writing a paper about a complex topic. A key part of that revision process is to do more research to <a href=\"https://www.emerald.com/jd/article-abstract/69/2/243/198951/Patterns-of-graduate-students-information-seeking?redirectedFrom=fulltext\" target=\"_blank\" rel=\"noopener noreferrer\">find missing information or strengthen your arguments</a>. This human pattern is surprisingly similar to the mechanism of <a href=\"https://proceedings.mlr.press/v202/zhang23as/zhang23as.pdf\" target=\"_blank\" rel=\"noopener noreferrer\"><i>retrieval</i></a>-augmented diffusion models that start with a “noisy” or messy output and gradually refine it into a high-quality result. What if an AI agent's rough draft is the noisy version, and a search tool acts as the denoising step that cleans it up with new facts?</p><p data-block-key=\"5njoq\">Today we introduce <a href=\"https://arxiv.org/abs/2507.16075\" target=\"_blank\" rel=\"noopener noreferrer\">Test-Time Diffusion Deep Researcher</a> (TTD-DR), a DR agent that imitates the way humans do research. To our knowledge, TTD-DR is the first research agent that models research report writing as a diffusion process, where a messy first draft is gradually polished into a high-quality final version. We introduce two new algorithms that work together to enable TTD-DR. First, component-wise optimization via <a href=\"https://arxiv.org/abs/2501.09891\" target=\"_blank\" rel=\"noopener noreferrer\">self-evolution</a> enhances the quality of each step in the research workflow. Then, report-level refinement via <a href=\"https://arxiv.org/abs/2302.02285\" target=\"_blank\" rel=\"noopener noreferrer\">denoising with retrieval</a> applies newly retrieved information to revise and improve the report draft. We demonstrate that TTD-DR achieves state-of-the-art results on long-form report writing and multi-hop reasoning tasks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Test-Time Diffusion Deep Researcher</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">TTD-DR is designed to take a user query as input and then create a preliminary draft that serves as an evolving foundation to guide the research plan. This evolving draft is iteratively refined using a denoising with retrieval process (report-level refinement) that takes the information it finds and uses it to improve the draft at each step. This happens in a continuous loop that improves the report with each cycle. To top it all off, a self-evolution algorithm constantly enhances the entire process, from the initial plan to the final report. This powerful combination of refinement and self-improvement leads to a more coherent report writing process.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-1.width-1250.png\" alt=\"Deep-Researcher-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-1.width-1250.png\" alt=\"Deep-Researcher-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d0pys\"><i>Illustration of TTD-DR. We designed it to imitate typical research practices by performing iterative cycles of drafting and revision.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Backbone DR design</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">The backbone DR design consists of three stages that we outline below.</p><ol><li data-block-key=\"40uvv\"><b>Research plan generation:</b> Produces a structured research plan upon receiving a user query. This plan outlines a list of key areas needed for the final report, serving as an initial guideline for the subsequent information-gathering process.</li><li data-block-key=\"9c90d\"><b>Iterative search:</b> Contains two sub-agents: Search Question Generation (stage 2a in the figure below) formulates a search query based on the research plan, the user query, and the context from previous search iterations (i.e., past questions and answers). Answer Searching (stage 2b) searches the available sources to find relevant documents and returns a summarized answer, similar to <a href=\"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\" target=\"_blank\" rel=\"noopener noreferrer\">retrieval-augmented generation</a> (RAG) systems.</li><li data-block-key=\"dambo\"><b>Final report generation:</b> Produces a comprehensive and coherent final report by combining all the structured information gathered, that is, the plan and the series of question-answer pairs.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-2.width-1250.png\" alt=\"Deep-Researcher-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-2.width-1250.png\" alt=\"Deep-Researcher-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d0pys\"><i>Our backbone DR agent operates in three stages. Stage 1 generates a detailed research plan; Stage 2a iteratively generates search questions and then uses a RAG-like system to synthesize precise answers from retrieved documents (2b); Stage 3 synthesizes all gathered information to produce the final report.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Component-wise self-evolution</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">We leverage a self-evolutionary algorithm to enhance the performance of each stage's agents in order to <i>find</i> and <i>preserve</i> the high quality context.</p><ul><li data-block-key=\"e6eh6\"><b>Initial states:</b> The leftmost blocks in the diagram below represent multiple diverse answer variants based on the output of previous stages, which are used to explore a larger search space. This ideally leads to discovery of more valuable information.</li><li data-block-key=\"c8sei\"><b>Environmental feedback:</b> Each answer variant is assessed by an LLM-as-a-judge, utilizing auto-raters for metrics, such as helpfulness and comprehensiveness. These raters not only provide fitness scores but also generate textual feedback that help improve the answer.</li><li data-block-key=\"9d9u5\"><b>Revision:</b> With the scores and feedback from the previous step, each variant undergoes a revision step to adapt toward better fitness scores. The environmental feedback and revision steps repeat until reaching some maximum number of iterations or until the agent determines no more revisions are needed.</li><li data-block-key=\"fh789\"><b>Cross-over:</b> Finally, multiple revised variants are merged into a single, high-quality output. This merging process consolidates the best information from all evolutionary paths, producing superior context for the main report generation process.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-3.width-1250.png\" alt=\"Deep-Researcher-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-3.width-1250.png\" alt=\"Deep-Researcher-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d0pys\"><i>Illustration of the component-wise self-evolution algorithm applied to Search Answer (Stage 2b). The process starts with multiple variants of initial answers, each undergoing a self-evolving episode where it first interacts with the environment to obtain a fitness score and feedback. It is then revised based on the feedback. This process repeats until the maximum number of iterations is reached. Finally, multiple revised variants from all episodes are merged to produce the final answer.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Report-level denoising with retrieval</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">Since a preliminary noisy draft is useless for complex topics without real research, TTD-DR uses a search tool that denoises and evolves the draft.</p><p data-block-key=\"2cj8q\">Specifically, we feed the current draft report into the Search Generation stage (Stage 2a) of the backbone DR workflow to inform the generation of the next search query. After obtaining a synthesized answer in the Answer Searching stage (Stage 2b), the new information is used to revise the report draft, either by adding new details or by verifying existing information. This process of feeding the denoised report back to generate the next search query is repeated. The draft is progressively denoised until the search process concludes, at which point a final agent writes the final report based on all historical search answers and revisions (Stage 3).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">We evaluate TTD-DR's performance using benchmark datasets that focus on two broad tasks: 1) Complex queries that require research agents to produce a long-form comprehensive report (<a href=\"https://github.com/Su-Sea/ydc-deep-research-evals\" target=\"_blank\" rel=\"noopener noreferrer\">DeepConsult</a>) and, 2) multi-hop queries that require extensive search and reasoning to answer (<a href=\"https://scale.com/leaderboard/humanitys_last_exam\" target=\"_blank\" rel=\"noopener noreferrer\">Humanity's Last Exam</a> [HLE] and <a href=\"https://huggingface.co/datasets/gaia-benchmark/GAIA\" target=\"_blank\" rel=\"noopener noreferrer\">GAIA</a>). We sub-sample 200 queries from HLE that need more search and reasoning (HLE-Search). Both categories fit into our objective of building a general-purpose, real-world research companion. We compare our DR systems with <a href=\"https://openai.com/index/introducing-deep-research/\" target=\"_blank\" rel=\"noopener noreferrer\">OpenAI Deep Research</a>.</p><p data-block-key=\"b0lhd\">TTD-DR consistently achieves better results across all benchmarks. Notably, when compared to OpenAI DR, TTD-DR achieves 74.5% win rate for the <i>long-form</i> research report generation tasks. Additionally, it outperforms OpenAI DR by 7.7% and 1.7% on the two extensive research datasets with <i>short-form</i> ground-truth answers.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-4.width-1250.png\" alt=\"Deep-Researcher-4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-4.width-1250.png\" alt=\"Deep-Researcher-4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d0pys\"><i>TTD-DR's performance against different baseline systems for benchmark datasets.</i> <b><i>Left</i></b><i>: Win rates (%) are computed based on OpenAI DR.</i> <b><i>Right</i></b><i>: Correctness is computed as matching between system predicted and reference answers. TTD-DR outperforms OpenAI DR with significant margins.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Ablation study</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">For the ablation study, we incrementally add the three methods in the section above. Our DR agents use <a href=\"https://arxiv.org/abs/2507.06261\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini-2.5-pro</a> as the base model. All other baseline agents use their default LLMs. The charts below show the ablation study for our DR agents. The backbone DR agent underperforms OpenAI DR. With the addition of the proposed self-evolution algorithm, we observe that for DeepConsult, our system outperforms OpenAI Deep Research with 59.8% win rates. The Correctness scores on HLE-Search and GAIA datasets also show an improvement of 4.4% and 1.2%. Finally, incorporating diffusion with retrieval leads to substantial gains across all benchmarks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-5.width-1250.png\" alt=\"Deep-Researcher-5\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-5.width-1250.png\" alt=\"Deep-Researcher-5\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d0pys\"><i>TTD-DR's performance by incrementally adding 1) backbone DR, 2) self-evolution, and 3) diffusion with retrieval. We observe step-by-step improvements across the board that help us achieve new state-of-the-art results.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ocegg\">The <a href=\"https://en.wikipedia.org/wiki/Pareto_front\" target=\"_blank\" rel=\"noopener noreferrer\">Pareto-frontier diagram</a> below further shows the test-time scaling efficiency of TTD-DR compared with other DR agents. We found that TTD-DR is more efficient than OpenAI DR, as with the same latency, it achieves the better quality per win-rate. See the <a href=\"https://arxiv.org/abs/2507.16075\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a> for more details.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-6.width-1250.png\" alt=\"Deep-Researcher-6\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Deep-Researcher-6.width-1250.png\" alt=\"Deep-Researcher-6\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d0pys\"><i>Pareto-frontier of research report quality vs. latency in seconds. The blue line indicates TTD-DR, whereas grey dots indicate compared DR agents.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">The Deep Researcher with Test-Time Diffusion (TTD-DR) is a new framework inspired by the iterative way humans do research. This agent addresses the limitations of existing DR agents by conceptualizing report generation as a diffusion process. The TTD-DR framework significantly outperforms existing DR agents across various benchmarks requiring intensive search and multi-hop reasoning. It demonstrates state-of-the-art performance in generating comprehensive long-form research reports and identifying concise answers for multi-hop search and reasoning tasks. We believe the reason it works so well is its \"draft-first\" design, which keeps the whole research process focused and coherent, preventing important information from getting lost along the way.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Availability on Google Cloud Platform</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\">A product version of this work is available on <a href=\"https://cloud.google.com/agentspace/docs/research-assistant\" target=\"_blank\" rel=\"noopener noreferrer\">Google Agentspace</a>, implemented with Google Cloud <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/agent-development-kit/quickstart\" target=\"_blank\" rel=\"noopener noreferrer\">Agent Development Kit</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ocegg\"><i>This research was conducted by Rujun Han, Yanfei Chen, Guan Sun, Lesly Miculicich, Zoey CuiZhu, Yuanjun (Sophia) Bi, Weiming Wen, Hui Wan, Chunfeng Wen, Solène Maître, George Lee, Vishy Tirumalashetty, Xiaowei Li, Emily Xue, Zizhao Zhang, Salem Haykal, Burak Gokturk, Tomas Pfister, and Chen-Yu Lee.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    }
  ],
  "lastUpdated": "2025-10-15T10:32:09.673Z"
}