{
  "sourceUrl": "https://rsshub.rssforever.com/google/research",
  "title": "Google Research Blog",
  "description": "Google Research Blog - Powered by RSSHub",
  "link": "https://research.google/blog",
  "items": [
    {
      "title": "使用消费级超宽带雷达测量心率 (原标题: Measuring heart rate with consumer ultra-wideband radar)",
      "link": "https://research.google/blog/measuring-heart-rate-with-consumer-ultra-wideband-radar/",
      "pubDate": "Wed, 16 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-16T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 使用消费级超宽带雷达测量心率：谷歌的最新研究\n\n### 引言\n\n随着消费设备功能日益强大，内置的各种传感器可用于监测健康和福祉。谷歌此前在Nest Hub中推出的Soli雷达技术已用于分析睡眠模式。最近，谷歌进一步展示了Soli平台所基于的调频连续波（FMCW）雷达技术能够以完全非接触的方式追踪睡眠和冥想期间的生命体征，如心率和呼吸率。\n\n现在，谷歌在题为“UWB雷达心率监测：一种迁移学习方法”的新研究中，展示了已广泛应用于许多手机的超宽带（UWB）技术，也可用于基于雷达的心率测量。尽管UWB在安全车辆解锁和精确物品定位等功能中得到广泛应用，但其在雷达传感方面的潜力尚未被充分挖掘。这项研究展示了如何利用现有硬件进行生命体征监测，例如测量心率（HR）。\n\n### 消费设备上的雷达传感器\n\n在消费设备中，最有希望用于生命体征测量的雷达系统包括毫米波调频连续波（mm-wave FMCW）和脉冲式超宽带（IR-UWB）雷达系统。谷歌之前在Soli雷达平台上进行睡眠、运动和手势感应的进展使用了FMCW技术，这使得谷歌拥有大量针对这些任务（包括FMCW雷达心率监测）训练的现有数据集、研究和机器学习算法。\n\n同时，UWB作为一种多功能技术，其普及度不断提高，并越来越多地出现在许多当前手机型号和其他消费设备中，也提供了雷达功能。然而，UWB的雷达功能迄今为止在很大程度上尚未被开发，当前的UWB应用更多地倾向于非雷达用途，如定位和追踪、车辆解锁功能或数据传输。\n\n### 克服非接触式传感的挑战\n\n使用雷达以非接触方式检测心率极具挑战性，因为心跳引起的微小胸壁运动很容易被呼吸和身体大范围运动所掩盖。这时，雷达信号的独特特性就发挥了作用：\n\n*   **空间分辨率：** 雷达的3D空间分辨率利用距离和方向来聚焦测量，从而在人体躯干周围定义一个精确的“测量区域”。这使得雷达能够隔离来自胸部区域的反射，同时忽略静止的背景物体或该区域之外的运动。\n*   **时间分辨率：** 同时，其高时间分辨率能够足够快地（高达200Hz）采样信号，以捕捉心跳本身细微而快速的运动。\n\n研究人员开发了一种新方法，充分利用雷达信号这些独特的二维时空特性，以实现高精度的心率测量。\n\n### 弥合雷达类型之间的鸿沟：迁移学习\n\n研究人员探讨了是否可以将从FMCW雷达（拥有大量现有数据集和研究的优势）中学习到的特征迁移到UWB雷达。这两种雷达系统基于完全不同的物理原理运行：\n\n*   **FMCW：** 发射连续正弦波，其频率随时间线性增加，周期性地扫描一个频率范围。\n*   **UWB：** 发射持续时间在几百皮秒到几纳秒之间的极短脉冲。\n\n这项研究首次表明，学习到的特征可以在不同雷达类型之间进行迁移，用于生命体征测量。研究人员选择心率作为初始任务，因为它具有高潜在实用性和挑战性。\n\n![模型迁移的高层架构](https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-2-Architecture.width-1250.jpg)\n\n### 开发用于雷达心率测量的新型深度学习模型\n\n为了完成这项任务，研究人员开发了一个新颖的深度学习框架，旨在模拟雷达信号中复杂的时空关系以进行心率估计。该架构首先使用一个2D ResNet处理输入数据，其中一个轴代表时间，另一个轴代表空间测量。这个初始阶段旨在从胸壁运动产生的精细时空模式中提取特征。\n\n在此步骤之后，模型通过平均池化折叠空间维度。然后，将生成的特征集输入到1D ResNet中，该网络旨在专门沿时间维度分析信号。这第二个阶段从第一阶段提取的特征中识别出心跳特有的长程周期性模式。\n\n当使用FMCW数据集进行训练时，该模型在心率测量方面实现了0.85次/分钟（bpm）的平均绝对误差（MAE）。这一发现代表了在该数据集上比现有最先进结果的显著提升，将之前的错误率减半。\n\n![FMCW雷达数据上现有最先进模型与我们模型性能的比较](https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-3-Comparison.width-1250.png)\n\n![测试集上夜间会话性能的代表性示例](https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-4-Example.width-1250.png)\n\n### 将学习到的特征迁移到超宽带雷达\n\n随后，研究人员进行了一项研究，收集了UWB雷达数据，并以心电图（ECG）和光电容积描记图（PPG）数据作为心率的真实值。实验设置将UWB雷达传感器放置在用户通常握持手机的位置，即放在面前的桌子上或大腿上。与980小时的FMCW数据集相比，UWB雷达数据集要小得多，只有37.3小时。由于UWB雷达配置接近手机上可行的配置，带宽更低，其距离分辨率远低于FMCW数据集。\n\n为了确保模型针对UWB数据集的迁移进行优化，研究人员在进行额外的预处理步骤后重新训练了模型，以修改毫米波FMCW雷达数据，使其更接近目标IR-UWB数据，从而有效地降低其距离分辨率。然后，研究人员在IR-UWB数据集上对该模型进行了微调，实现了4.1 bpm的MAE和6.3%的平均绝对百分比误差（MAPE），比基线错误率降低了25%。UWB雷达性能的基线是5.4 bpm MAE和8.4% MAPE，这是通过从头开始在UWB数据集上训练的最佳模型实现的。通过迁移学习，UWB雷达能够满足消费技术协会（CTA）对消费设备心率测量标准的要求：精度达到5 bpm MAE和10% MAPE。\n\n![IR-UWB雷达数据上基线模型与迁移学习模型性能的比较](https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-5-IR-UWB.width-1250.png)\n\n![测试集中三名选定参与者在雷达放置在参与者面前的桌子上（a）和参与者大腿上（b）的会话中模型性能（蓝色）与真实值（橙色）的代表性示例](https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-6-Performance.width-1250.png)\n\n### 确保不同场景下的准确性\n\n为了确保模型的准确性和可靠性，研究人员分析了其在每个数据集中捕获的各种场景和用户条件下的性能。对于两种类型的雷达，研究人员发现心率测量性能在充分代表的情况下保持一致。例如，在收集夜间睡眠会话数据的FMCW雷达上，性能在各种睡眠姿势下甚至在人们在姿势之间移动时也能保持。对于UWB雷达，心率测量对于两种测试的设备相对于用户的位置（放在面前的桌子上或大腿上）都同样准确。有关此子组分析和其他结果的更多详细信息，请参阅完整的研\n\n### 宏观视角：日常健康监测\n\n心率测量对于一系列健康、健身和福祉应用都非常有用，它能提供对个体心血管状况和各种健康状况下生理反应的基本洞察。这项心率测量的演示可能成为使用移动设备测量心脏和大血管更复杂、更细微健康信号的一步。\n\n虽然健身手环和戒指等可穿戴设备已普及了健康和健身的连续监测，但使用消费级雷达传感器以非接触方式测量心率的能力，使得这项技术的益处能够惠及更广泛的智能手机用户。这项研究侧重于睡眠期间的心率（对于FMCW）以及雷达传感器放置在手机通常使用时所处位置的设置（UWB）。随着技术的发展，连续监测可以扩展到各种日常环境，无缝融入用户的日常活动。\n\n### 对未来设备的影响\n\n这项工作使我们更接近于实现使用消费设备进行非接触式心率测量，特别是随着超宽带（UWB）技术在手机中变得越来越普遍。尽管本研究未包含在真实世界环境中直接使用手机进行测试，但这项研究为未来的此类应用奠定了关键基础。\n\n这项工作的核心发现是，证明了在一个雷达类型（FMCW）上训练的模型可以成功地适应另一个雷达类型（UWB）来测量心率。这种迁移学习方法是向前迈出的重要一步。它为未来的研究和开发提供了一条更高效的途径，可以利用现有大型数据集的基础知识来开发新设备。这种方法无需为每种新硬件从头开始进行大量数据收集，从而实现了更精简的流程，加速了此类功能推向消费设备的时间线。",
      "shortSummary": "谷歌最新研究展示了如何利用手机中常见的超宽带（UWB）雷达进行非接触式心率测量。通过开发新的深度学习模型和创新性地将FMCW雷达数据学习到的特征迁移到UWB雷达，研究团队成功克服了非接触式心率测量的挑战，并达到了消费技术协会的精度标准。这项突破性工作为未来在消费级移动设备上集成更广泛的健康监测功能奠定了基础，有望让更多智能手机用户享受到便捷的健康监测服务。",
      "translated_title": "使用消费级超宽带雷达测量心率",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-2-Architecture.width-1250.jpg",
          "alt": "RadarUWB-2-Architecture",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-3-Comparison.width-1250.png",
          "alt": "RadarUWB-3-Comparison",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-4-Example.width-1250.png",
          "alt": "RadarUWB-4-Example",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-5-IR-UWB.width-1250.png",
          "alt": "RadarUWB-5-IR-UWB",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-6-Performance.width-1250.png",
          "alt": "RadarUWB-6-Performance",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ng5yj\">Consumer devices are becoming increasingly capable, featuring various sensors useful for monitoring fitness and wellbeing. A few years ago, we launched <a href=\"https://research.google/blog/contactless-sleep-sensing-in-nest-hub/\">sleep sensing in the Nest Hub</a>, which used radar technology, called Soli, to analyze sleep patterns<footnote id=\"da2d8402-1a4c-4976-8152-684e51081729\">[da2d84]</footnote> while the device is placed near the bedside. More recently, we showed that the frequency modulated continuous wave (FMCW) radar technology underlying the Soli radar platform can <a href=\"https://research.google/pubs/soli-enabled-non-contact-heart-rate-detection-for-sleep-and-meditation-tracking/\">track vital signs like heart rate and breathing rate during sleep and meditation</a> in a fully contactless manner.</p><p data-block-key=\"8k4g8\">Today, in “<a href=\"https://www.gstatic.com/hai/UWB_Radar_based_Heart_Rate_Monitoring_A_Transfer_Learning_Approach.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">UWB Radar-based heart rate monitoring: A transfer learning approach</a>”, we present new research showing that the ultra-wideband (UWB) technology, already common in many mobile phones, can be used for radar-based heart rate measurement. While UWB is widely adopted for features like secure vehicle unlocking and precise item location, its potential for radar sensing has been largely untapped. We demonstrate how this existing hardware can be leveraged for vital sign monitoring, such as measuring heart rate (HR).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n            <div class=\"glue-ambient-video \">\n                <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/RadarUWB-1b-Overview.mp4\" type=\"video/mp4\">\n                </video>\n\n                <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n                    <div class=\"glue-ambient-video__tooltip\">\n                    <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                    <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                    </div>\n                    <div class=\"glue-ambient-video__icon\">\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                    </div>\n                </div>\n            </div>\n        \n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"scntp\"><i>Detecting heart rate in a contactless manner with UWB radar, similar to that in current mobile phones, using a deep learning ML model.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Radar sensors available on consumer devices</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ng5yj\">The radar systems that have shown the most promise for vital sign measurement from consumer devices include <a href=\"https://www.everythingrf.com/community/what-is-a-fmcw-radar\" target=\"_blank\" rel=\"noopener noreferrer\">millimeter wave frequency-modulated continuous wave</a> (mm-wave FMCW) and <a href=\"https://www.everythingrf.com/community/what-is-impulse-radio-ultra-wideband-ir-uwb-radar-technology\" target=\"_blank\" rel=\"noopener noreferrer\">impulse-radio ultra-wideband</a> (IR-UWB) radar systems. Google’s previous advances on <a href=\"https://blog.google/products/google-nest/new-nest-hub-soli/\" target=\"_blank\" rel=\"noopener noreferrer\">sensing sleep</a>, <a href=\"https://research.google/blog/soli-radar-based-perception-and-interaction-in-pixel-4/\">motion and gestures</a> on the Soli radar platform used FMCW technology. This meant that we already had extensive datasets, studies, and machine learning algorithms trained for these tasks, <a href=\"https://research.google/pubs/soli-enabled-non-contact-heart-rate-detection-for-sleep-and-meditation-tracking/\">including for heart rate</a> monitoring using FMCW radar.</p><p data-block-key=\"78eoa\">Meanwhile UWB — a multipurpose technology that has grown in popularity and is <a href=\"https://www.firaconsortium.org/resource-hub/blog/the-present-and-future-of-uwb\" target=\"_blank\" rel=\"noopener noreferrer\">increasingly available on many current mobile phone models</a> and other consumer devices, also offers radar capabilities. The radar capabilities of UWB have been thus far largely untapped, with current UWB applications leaning more on non-radar uses like localization and tracking, vehicle unlock features, or data transfer.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Overcoming the challenge of contactless sensing</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ng5yj\">Detecting HR in a contactless manner with radar is challenging because the tiny movements of the chest wall caused by the heartbeat are easily obscured by the far larger movements from breathing and general body motion. This is where the distinct nature of the radar signal comes into play. Its spatial resolution works in three dimensions, using both distance and direction to focus its measurement. This allows the radar to define a precise “measurement zone” around a person's torso. As a result, it can isolate reflections coming from the chest area while ignoring stationary background objects or movements occurring outside this zone. Simultaneously, its high temporal resolution samples the signal fast enough (up to 200Hz) to capture the subtle, rapid motion of the heartbeat itself. We developed a new method that makes optimal use of these unique 2-dimensional spatio-temporal properties of the radar signal to achieve highly accurate heart rate measurement.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Bridging the gap between radar types</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ng5yj\">We investigated if we could transfer the features learned from FMCW radar — where we had the benefit of large existing datasets and studies — to the UWB radar. The two radar systems operate using completely different physical principles. Mm-wave FMCW transmits a continuous sinusoidal wave whose frequency increases linearly with time, periodically sweeping a frequency range, while UWB transmits very short pulses with duration on the order of a few hundred picoseconds to a few nanoseconds. Our study is the first to show that learned features can be transferred between radar types for vital sign measurement. We chose heart rate as an initial task, for both its high potential utility and level of challenge.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-2-Architecture.width-1250.jpg\" alt=\"RadarUWB-2-Architecture\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-2-Architecture.width-1250.jpg\" alt=\"RadarUWB-2-Architecture\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"scntp\"><i>High level architecture of model transfer.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Developing a new deep learning model for heart rate from radar</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ng5yj\">To accomplish this task, we developed a novel deep learning framework designed to model the complex spatial-temporal relationships in radar signals for HR estimation. The architecture first uses a 2D <a href=\"https://arxiv.org/abs/1512.03385\" target=\"_blank\" rel=\"noopener noreferrer\">ResNet</a> to process the input data, in which one axis represents time and the other represents the spatial measurements. This initial stage is designed to extract features from the fine-grained spatio-temporal patterns created by chest wall movements.</p><p data-block-key=\"5lvb7\">Following this step, the model collapses the spatial dimension via average pooling. The resulting feature set is then fed into a 1D ResNet, which is designed to analyze the signal exclusively along the temporal dimension. This second stage identifies the longer-range, periodic patterns characteristic of a heartbeat from the features extracted in the first stage.</p><p data-block-key=\"fs2i2\">When trained with our FMCW dataset, the model achieves a mean absolute error (MAE) of 0.85 beats per minute (bpm) for heart rate measurement. This finding represents a substantial gain over prior state-of-the-art results on this dataset, halving the previous error rate.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-3-Comparison.width-1250.png\" alt=\"RadarUWB-3-Comparison\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-3-Comparison.width-1250.png\" alt=\"RadarUWB-3-Comparison\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"scntp\"><i>Comparison of the previous state of the art and our model's performance on FMCW radar data, including MAE with 95% confidence interval and mean absolute precision error (MAPE).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-4-Example.width-1250.png\" alt=\"RadarUWB-4-Example\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-4-Example.width-1250.png\" alt=\"RadarUWB-4-Example\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"scntp\"><i>Representative example of overnight session performance on the test set. The top plot shows the model performance (blue) compared to the ground truth (orange). The middle plot shows the body position, and the bottom plot shows the estimated user distance from the radar.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Transferring learned features to ultra-wideband radar</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ng5yj\">We then ran a study that collected UWB radar data, along with electrocardiogram (ECG) and photoplethysmogram (PPG) data as our ground truth for heart rate, using a setup that placed the UWB radar sensor in positions where users typically hold their phone, i.e., on a table in front of them or on their lap. Compared to the FMCW dataset, which was 980 hours of data, the UWB radar dataset was much smaller, with 37.3 hours. As the UWB radar configuration was close to what is feasible on a mobile phone, with a much lower bandwidth, its range resolution was far lower than the FMCW dataset.</p><p data-block-key=\"9o9i4\">To ensure that our model was optimized to transfer to the UWB dataset, we retrained it after performing additional pre-processing steps to modify the mm-wave FMCW radar data to better resemble the target IR-UWB data, effectively lowering its range resolution. We then fine-tuned this model on the IR-UWB dataset, achieving an MAE of 4.1 bpm and mean absolute percentage error (MAPE) of 6.3%, a 25% reduction over the baseline error rate. Our baseline for performance on UWB radar was 5.4 bpm MAE and 8.4% MAPE, achieved by selecting the best model trained from scratch on our UWB dataset. With transfer learning, we enabled the UWB radar to meet the <a href=\"https://shop.cta.tech/collections/standards/products/cta-2065\" target=\"_blank\" rel=\"noopener noreferrer\">Consumer Technology Association standards</a> for heart rate measurement for consumer devices: an accuracy of up to 5 bpm MAE and 10% MAPE.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-5-IR-UWB.width-1250.png\" alt=\"RadarUWB-5-IR-UWB\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-5-IR-UWB.width-1250.png\" alt=\"RadarUWB-5-IR-UWB\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"scntp\"><i>Comparison of the baseline and transfer learning models performance on IR-UWB radar data, including MAE with 95% confidence interval and MAPE.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-6-Performance.width-1250.png\" alt=\"RadarUWB-6-Performance\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RadarUWB-6-Performance.width-1250.png\" alt=\"RadarUWB-6-Performance\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"scntp\"><i>Representative examples of the model performance (blue) compared to the ground truth (orange) for three selected participants from the test set for the session where the radar was located on the table in front of the participant (</i><b><i>a</i></b><i>) and at the participant's lap (</i><b><i>b</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Ensuring accuracy in different scenarios</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ng5yj\">To make sure our model is both accurate and reliable, we analyzed its performance across the various scenarios and user conditions captured in each dataset. For both types of radar, we found that performance on heart rate measurement is consistent in situations that were adequately represented. For example, on the FMCW radar, which collected data during overnight sleep sessions, the performance is maintained across various sleep positions and even when a person is moving between positions. For UWB radar, heart rate measurement is equally accurate for both tested device positions relative to the user — on a table in front of them or in their lap. For more details on this subgroup analysis and other results, see the <a href=\"https://www.gstatic.com/hai/UWB_Radar_based_Heart_Rate_Monitoring_A_Transfer_Learning_Approach.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">full research paper</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The big picture: Everyday health monitoring</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ng5yj\">Heart rate measurement is useful for a range of health, fitness, and wellness applications, offering fundamental insight into an individual's cardiovascular status and physiological responses across various health conditions. This demonstration of heart rate measurement could be a step towards using mobile devices to measure even more complex and subtle health signals from the heart and large blood vessels.</p><p data-block-key=\"367u5\">While wearable devices like fitness bands and rings have popularized continuous monitoring of health and fitness, the ability to measure heart rate in a contactless manner with consumer-device–grade radar sensors allows the benefits of this technology to reach a much wider audience of smartphone users. For this study, we focused on heart rate while sleeping (for FMCW) and on a setup where the radar sensor was in positions where the phone is usually held during use (UWB). As technology evolves, continuous monitoring could extend to various daily settings, integrating seamlessly into a user's routine activities.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">What this means for future devices</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ng5yj\">This work moves us closer to enabling contactless heart rate measurement using consumer devices, especially as ultra-wideband (UWB) technology becomes more prevalent in mobile phones. Although our study did not include direct testing using mobile phones in a real-world setting, this research establishes the crucial groundwork for such future applications.</p><p data-block-key=\"6s3q0\">A core finding of this work is the demonstration that a model trained on one type of radar (FMCW) can be successfully adapted for another (UWB) to measure heart rate. This transfer learning approach is a significant step forward. It suggests a more efficient path for future research and development, where the foundational knowledge from existing, large datasets can be leveraged for new devices. Instead of starting from scratch with extensive data collection for each new piece of hardware, this method allows for a more streamlined process, accelerating the timeline for bringing such features to consumer devices.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ng5yj\"><i>This research is a result of a collaborative effort between Google Research and Google Platforms &amp; Devices teams. We would like to thank our co-authors Sebastien Baur, Matthew Baugh, Mathias Bellaiche, Sharanya Srinivas, Octavio Ponce, Matthew Thompson, Pramod Rudrapatna, Michael Sanchez, Lawrence Cai, Tim Chico, Robert Storey, Emily Maz, Umesh Telang, Shravya Shetty, and Mayank Daswani for their significant contributions. We are also grateful to Abhijit Guha Roy, Michał Matuszak, Florence Thng, Yun Liu, and Shwetak Patel for their expert review. We also want to thank Tiya Tiyasirichokchai for designing the graphic for this post.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Android地震预警：一个全球性的早期预警系统 (原标题: Android Earthquake Alerts: A global system for early warning)",
      "link": "https://research.google/blog/android-earthquake-alerts-a-global-system-for-early-warning/",
      "pubDate": "Wed, 16 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-16T16:00:00.000Z",
      "creator": "Google",
      "summary": "### Android地震预警系统：一个全球性的早期预警方案\n\n地震对全球社区构成持续威胁。尽管我们对地震可能发生的地点有了更好的了解，但地震发生时仍会带来毁灭性后果。如果能在震动开始前提供几秒钟的宝贵预警，这些时间足以让人从梯子上下来、远离危险物品并寻找掩护。多年来，这一直是地震早期预警（EEW）系统的目标。然而，许多地震多发地区缺乏昂贵的地震网络。\n\n#### Android手机如何实现地震检测和预警\n\n在《科学》杂志发表的“使用Android手机进行全球地震检测和预警”一文中，我们展示了如何将全球Android智能手机网络转变为一个强大的、口袋大小的地震检测系统，以补充官方预警系统。\n\n*   **工作原理：**\n    *   Android手机中的加速度计（与横向翻转屏幕相同的传感器）可以检测地震引起的地面震动。\n    *   当一部静止的手机检测到地震的初始、传播速度更快的P波时，它会向地震检测服务器发送信号，并附带震动发生的大致位置。\n    *   系统随后迅速分析来自多部手机的数据，以确认地震正在发生，并估算其位置和震级。\n    *   目标是在地震传播速度较慢、破坏性更大的S波到达之前，尽可能多地向人们发出警告。\n*   **预警类型：**\n    *   **BeAware 警报：** 针对预计的轻微震动。\n    *   **TakeAction 警报：** 针对预计的更强震动，会占据手机屏幕并播放响亮的声音。\n*   **接收条件：** 用户必须开启Wi-Fi和/或蜂窝数据连接，并启用Android地震预警和位置设置。警报基于保护隐私的设备粗略位置发送。用户可以在设备设置中关闭地震警报。\n\n![EQdetection1_Map](https://storage.googleapis.com/gweb-research2023-media/images/EQdetection1_Map.width-1250.png)\n*图示：浅绿色区域显示了Android地震预警系统当前正在检测并发送警报的国家。红色区域表示发生强烈震动（MMI 5+）时发出警报的区域，黄色区域表示轻微震动（MMI 3-4）时发出警报的区域。灰色圆圈表示未发出警报的区域中的其他Android检测。Android还在加利福尼亚州、俄勒冈州和华盛顿州（深绿色）提供由ShakeAlert生成的警报。*\n\n#### 全球覆盖与显著影响\n\n*   **推广历程：** 2021年4月，系统开始在新西兰和希腊推出由Android检测生成的警报。到2023年底，该系统已在98个国家/地区启用。\n*   **检测成果：** 系统已检测到超过18,000次地震，包括M1.9的小型震颤到M7.8的重大地震。对于足以向人们发出警告的事件，系统已对2000多次地震发出了警报，总计向全球手机发送了7.9亿次警报。\n*   **EEW覆盖范围的巨大提升：** 2019年，全球只有约2.5亿人能够使用EEW系统。如今，主要得益于Android系统，这一数字已增至25亿，实现了约10倍的增长。\n\n![EQdetection2_Reach](https://storage.googleapis.com/gweb-research2023-media/images/EQdetection2_Reach.width-1250.png)\n*图示：随着Android系统的引入，EEW的全球覆盖范围显著扩大。*\n\n#### 震级估算的挑战与改进\n\n*   **挑战：** 实时估算地震震级是EEW系统中最棘手的部分之一。震级决定了震动传播的距离以及需要警报的人群。准确估算至关重要——低估可能无法警告处于危险中的人，高估则可能发出虚假警报，损害公众信任。挑战在于速度和准确性之间的权衡。地震开始的最初几秒提供的数据有限，但每延迟一秒发出警报，受影响区域的人们获得的预警时间就减少一秒。\n*   **持续改进：** 过去三年，系统持续改进了震级估算。首次震级估算的中位数绝对误差已从0.50降至0.25。与传统的地震网络相比，Android系统的准确性相似，在某些情况下甚至更好。\n\n![EQdetection3_Error](https://storage.googleapis.com/gweb-research2023-media/images/EQdetection3_Error.width-1250.png)\n*图示：过去三年Android地震预警系统地震震级误差的演变。对于一次地震，系统会随着新数据的收集进行多次检测。首次估算很重要，因为它提供了最长的预警时间，而最大震级估算则为最大区域生成警报。*\n\n#### 具体案例分析\n\n*   **2023年11月菲律宾6.7级地震：** 系统在地震开始后18.3秒发出了首次警报。震中附近受影响最严重的人们获得了长达15秒的预警，而较远但仍感受到中度震动的人们获得了长达一分钟的预警。总计近250万人收到警报。\n*   **2023年11月尼泊尔5.7级地震：** 首次警报在地震开始后15.6秒发出。经历中度至强烈震动的人们获得了10到60秒的预警时间。此次事件共发送了超过1000万次警报。\n\n![EQdetection4_Warning](https://storage.googleapis.com/gweb-research2023-media/images/EQdetection4_Warning.width-1250.png)\n*图示：这些图表显示了人们根据与震中的距离和所经历的震动强度获得的预警时间。*\n\n*   **2025年4月土耳其6.2级地震：** 首次警报在地震开始后8.0秒发出。经历中度至强烈震动的人们获得了几秒到20秒的预警时间。此次事件共发送了超过1600万次警报。\n\n#### 用户反馈：系统有效性的真实检验\n\n系统在警报中加入了简单的调查，反馈结果非常积极。在超过150万名受访者中，85%的人认为警报“非常有帮助”。\n\n*   **关键发现：**\n    *   即使没有感受到震动，人们也感谢预警。令人惊讶的是，79%收到警报但未感受到地震的人仍然认为警报非常有帮助。这表明人们重视了解其区域内的潜在危险。\n    *   正确的警报很重要。收到“TakeAction”警报的人中，报告感受到强烈震动的比例远高于收到“BeAware”警报的人。这表明系统在区分轻微震动和潜在破坏性震动方面做得很好。\n    *   人们正在采取行动。对于收到“TakeAction”警报的人来说，最常见的反应是“趴下、掩护、抓牢”。这是一个非常好的结果，表明这些警报正在促使人们采取正确的、挽救生命的行动。\n\n![EQdetection6_UserFeedback](https://storage.googleapis.com/gweb-research2023-media/images/EQdetection6_UserFeedback.width-1250.png)\n*图示：超过150万用户对地震警报体验的反馈快照。*\n\n#### 地震早期预警的未来\n\n最令人兴奋的是，该系统正在不断学习和改进。收集到的数据有助于更好地理解地震并建立更准确的预测模型。未来，该系统不仅可以提供预警，还可以向应急响应人员提供快速的震后信息，帮助他们迅速评估最需要帮助的区域。",
      "shortSummary": "Android地震预警系统利用全球Android手机的加速度计，构建了一个大规模的地震检测网络。该系统能检测地震P波，快速分析并估算震级和位置，在破坏性S波到达前向用户发送警报。自2021年推出以来，系统已覆盖近百个国家，检测了数万次地震，发送了数亿次警报，将全球地震预警覆盖人数从2.5亿提升至25亿。系统持续优化震级估算准确性，并通过用户反馈证实了其有效性，促使人们采取正确的避险行动。未来，它有望提供更准确的预警和震后信息。",
      "translated_title": "Android地震预警：一个全球性的早期预警系统",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EQdetection1_Map.width-1250.png",
          "alt": "EQdetection1_Map",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EQdetection2_Reach.width-1250.png",
          "alt": "EQdetection2_Reach",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EQdetection3_Error.width-1250.png",
          "alt": "EQdetection3_Error",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EQdetection4_Warning.width-1250.png",
          "alt": "EQdetection4_Warning",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EQdetection6_UserFeedback.width-1250.png",
          "alt": "EQdetection6_UserFeedback",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"rgp22\">Earthquakes are a constant threat to communities around the globe. While we’ve gotten good at knowing where they’re likely to strike, we still face devastating consequences when they do. What if we could give people a few precious seconds of warning before the shaking starts? Those seconds can be enough time to get off a ladder, move away from dangerous objects and take cover. For years, that’s been the goal of <a href=\"https://en.wikipedia.org/wiki/Earthquake_early_warning_system\" target=\"_blank\" rel=\"noopener noreferrer\">earthquake early warning</a> (EEW) systems. But the expensive seismic networks on which they rely just don’t exist in many of the world’s most earthquake-prone regions.</p><p data-block-key=\"6l06b\">In “<a href=\"https://www.science.org/doi/10.1126/science.ads4779\" target=\"_blank\" rel=\"noopener noreferrer\">Global earthquake detection and warning using Android phones</a>”, published in <a href=\"https://www.science.org/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Science</i></a>, we show how we've turned the global network of Android smartphones into a powerful, pocket-sized earthquake detection system to help supplement official warnings systems. Over the last four years, the <a href=\"https://crisisresponse.google/android-alerts/\" target=\"_blank\" rel=\"noopener noreferrer\">Android Earthquake Alerts system</a> has detected thousands of earthquakes and sent alerts to millions of people in nearly 100 countries, often giving them the crucial moments they need to take cover before the shaking arrives. Evaluation of thousands of earthquakes, analysis of specific earthquake examples and direct user feedback allows the system to continuously improve its performance in key areas, like magnitude estimation, making the alerts more effective over time.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"rgp22\">How it works: A global network of tiny seismometers</h2><p data-block-key=\"fempe\">The accelerometer in an Android phone, the same sensor that flips the screen when it’s turned sideways, can also detect the ground shaking from an earthquake. If a stationary phone detects the initial, faster-moving <a href=\"https://en.wikipedia.org/wiki/P_wave\" target=\"_blank\" rel=\"noopener noreferrer\">P-wave</a> of an earthquake, it sends a signal to our earthquake detection server, along with a coarse location of where the shaking occurred.</p><p data-block-key=\"17h5m\">The system then quickly analyzes data from many phones to confirm that an earthquake is happening and estimate its location and magnitude. The goal is to warn as many people as possible before the slower, more damaging <a href=\"https://en.wikipedia.org/wiki/S_wave\" target=\"_blank\" rel=\"noopener noreferrer\">S-wave</a> of an earthquake reaches them. The system sends out two types of alerts:</p><ul><li data-block-key=\"2fhu9\"><i>BeAware alerts</i> — for estimated light shaking.</li><li data-block-key=\"3nd9u\"><i>TakeAction alerts</i> — for estimated stronger shaking, which take over the phone's screen and play a loud sound.</li></ul><p data-block-key=\"dil0d\">To receive alerts, users must have Wi-Fi and/or cellular data connectivity, and both Android Earthquake Alerts and location settings enabled. Alerts are sent based on a privacy-preserving, coarse location of the device. Users who do not wish to receive these alerts can <a href=\"https://support.google.com/android/answer/9319337?hl=en#zippy=%2Cget-alerts-for-nearby-earthquakes\" target=\"_blank\" rel=\"noopener noreferrer\">turn off Earthquake Alerts in device settings</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EQdetection1_Map.width-1250.png\" alt=\"EQdetection1_Map\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EQdetection1_Map.width-1250.png\" alt=\"EQdetection1_Map\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"i9nm7\"><i>Light green areas show the countries where the Android Earthquake Alerts System is currently detecting and delivering alerts. The areas alerted in individual earthquakes are shown in red where there was strong shaking (</i><a href=\"https://en.wikipedia.org/wiki/Modified_Mercalli_intensity_scale\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Modified Mercalli Intensity</i></a><i> (MMI) 5+) and yellow for lighter shaking (MMI 3-4). The gray circles indicate other Android detections in regions where alerts were not issued. Android also delivers alerts generated by</i> <a href=\"https://www.shakealert.org/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>ShakeAlert</i></a><i> in California, Oregon and Washington (dark green).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"rgp22\">A global safety net</h2><p data-block-key=\"9f2ts\">In April 2021, we began rolling out alerts generated by Android detections, starting in New Zealand and Greece. By the end of 2023, the system was active in 98 countries.</p><p data-block-key=\"91eke\">The system has now detected over 18,000 earthquakes, from small tremors of <a href=\"https://en.wikipedia.org/wiki/Moment_magnitude_scale\" target=\"_blank\" rel=\"noopener noreferrer\">M1.9</a> to major quakes reaching M7.8. For the events significant enough to warn people, alerts were issued for over 2000 earthquakes, culminating in 790 million alerts being sent to phones worldwide.</p><p data-block-key=\"3jk2o\">The impact has been a ~10x change in the number of people with access to EEW systems. In 2019, only about 250 million people had access. Today, thanks in large part to the Android system, that number has increased to 2.5 billion.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EQdetection2_Reach.width-1250.png\" alt=\"EQdetection2_Reach\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EQdetection2_Reach.width-1250.png\" alt=\"EQdetection2_Reach\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"i9nm7\"><i>The global reach of EEW has expanded dramatically with the introduction of the Android system.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"rgp22\">The challenge of estimating an earthquake's power</h2><p data-block-key=\"bkme8\">One of the trickiest parts of an EEW system is estimating the <a href=\"https://en.wikipedia.org/wiki/Moment_magnitude_scale\" target=\"_blank\" rel=\"noopener noreferrer\">magnitude</a> of an earthquake in real-time. The magnitude tells us how big the earthquake is, which in turn determines how far the shaking will travel and who needs to be alerted. Getting this right is crucial — underestimate, and you might not warn people in danger; overestimate, and you risk sending out false alarms that erode public trust.</p><p data-block-key=\"86rp3\">The challenge lies in the trade-off between speed and accuracy. The first few seconds of an earthquake provide limited data, but every second you wait to issue an alert is a second less of warning for those in the path of the shaking.</p><p data-block-key=\"d0k8r\">Over the last three years, we've continuously improved our magnitude estimation. The median absolute error of our first magnitude estimate has dropped from 0.50 to just 0.25. When we compare our system to established, traditional seismic networks, our accuracy is similar, and in some cases, even better.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EQdetection3_Error.width-1250.png\" alt=\"EQdetection3_Error\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EQdetection3_Error.width-1250.png\" alt=\"EQdetection3_Error\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"i9nm7\"><i>Evolution of the Android Earthquake Alerts System’s earthquake magnitude error over the last three years. For one earthquake, the system does several detections as new data is gathered. The first estimate is important as it provides the maximum warning time, whereas the maximum magnitude estimate generates an alert for the largest area.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"rgp22\">Specific examples</h2><p data-block-key=\"2bpsu\">So, how well does it work in a real earthquake? Let's look at three examples.</p><p data-block-key=\"rp37\">During a magnitude 6.7 earthquake in the Philippines in November 2023, our system sent out the first alert just 18.3 seconds after the quake started. People closest to the epicenter, who experienced the most intense shaking, received up to 15 seconds of warning. Those farther away, who still felt moderate shaking, got up to a minute of warning. In total, nearly 2.5 million people were alerted.</p><p data-block-key=\"8fgml\">In a magnitude 5.7 earthquake in Nepal in November 2023, the first alert was issued 15.6 seconds after the earthquake began. People who experienced moderate to strong shaking had a warning time of 10 to 60 seconds. In this event, over 10 million alerts were delivered.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EQdetection4_Warning.width-1250.png\" alt=\"EQdetection4_Warning\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EQdetection4_Warning.width-1250.png\" alt=\"EQdetection4_Warning\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"i9nm7\"><i>These charts show the warning time people received based on how far they were from the epicenter and the intensity of the shaking they experienced.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"rgp22\">Moreover, in a magnitude 6.2 earthquake in Turkey in April 2025, the first alert was issued 8.0 seconds after the earthquake began. People who experienced moderate to strong shaking had a warning time of a few to 20 seconds. In this event, over 16 million alerts were delivered.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n            <div class=\"glue-ambient-video \">\n                <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/EQdetection5_ExampleFinal.mp4\" type=\"video/mp4\">\n                </video>\n\n                <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n                    <div class=\"glue-ambient-video__tooltip\">\n                    <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                    <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                    </div>\n                    <div class=\"glue-ambient-video__icon\">\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                    </div>\n                </div>\n            </div>\n        \n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ofl7l\"><i>Animation showing phones detecting shaking as the 6.2 earthquake in Turkey progressed. Yellow dots are phones that detect shaking. The yellow circle is the P-wave’s estimated location and the red circle is for the S-wave. Note that phones can detect shaking for reasons other than an earthquake which is a source of noise the system needs to handle. UTC time during the event is displayed in the upper left.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"bx736\">Learning from user feedback</h2><p data-block-key=\"jou3\">The true test of any alert system is if people find it helpful. We included a simple survey in our alerts, and the feedback has been overwhelmingly positive. Of the more than 1.5 million people who responded, 85% found the alerts to be \"very helpful.\"</p><p data-block-key=\"79tab\">Here are some of the key takeaways:</p><ul><li data-block-key=\"3sv1e\"><i>People appreciate the warning, even if they don't feel shaking.</i> A surprising 79% of people who received an alert but didn't feel the earthquake still found the alert to be very helpful. This tells us that people value being informed about potential hazards in their area.</li><li data-block-key=\"c4u75\"><i>The right alert matters.</i> A much higher percentage of people who received a TakeAction alert reported feeling strong shaking compared to those who got a BeAware alert. This shows that our system is doing a good job of distinguishing between light and potentially damaging shaking.</li><li data-block-key=\"9oe5f\"><i>People are taking action.</i> For those who received a TakeAction alert, the most common response was to \"Drop, Cover, and Hold On.\" This is a fantastic result and shows that these alerts are prompting people to take the correct, life-saving actions.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EQdetection6_UserFeedback.width-1250.png\" alt=\"EQdetection6_UserFeedback\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EQdetection6_UserFeedback.width-1250.png\" alt=\"EQdetection6_UserFeedback\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"xsckk\"><i>A snapshot of what over 1.5 million users told us about their experience with the earthquake alerts.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"bx736\">The future of earthquake early warning</h2><p data-block-key=\"6sjpo\">What's most exciting is that our system is constantly learning and improving. The data we collect is helping us to better understand earthquakes and build more accurate prediction models. In the future, this system could not only provide warnings but also deliver rapid post-earthquake information to emergency responders, helping them to quickly assess the areas most in need.</p><p data-block-key=\"f10ud\">We’re excited to continue to show how the devices in so many of our pockets can be used to create a more informed and safer world.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "用于关系数据的图基础模型 (原标题: Graph foundation models for relational data)",
      "link": "https://research.google/blog/graph-foundation-models-for-relational-data/",
      "pubDate": "Wed, 09 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-09T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 用于关系数据的图基础模型\n\n### 引言\n\n关系数据库是企业数据的主要形式，为谷歌及日常服务（如内容推荐、交通预测）提供支持。然而，大多数复杂应用涉及多张表（谷歌有些应用甚至需要维护数百张表），从这些表网络中提取有价值的信息并非易事。传统的表格机器学习（ML）方法（如决策树）难以充分利用这些关系模式的连接结构。尽管图神经网络（GNN）在图结构数据方面取得了进展，但它们通常固定于特定训练图，无法泛化到具有新节点、边类型、特征和标签的新图。目前，尚未出现一个能够学习关系数据中有意义表示并处理所有节点、链接和图级别预测任务的通用模型。\n\n### 目标\n\n本文探讨了设计一个单一模型的可能性，该模型不仅能在互联的关系表中表现出色，还能在无需额外训练的情况下，泛化到任意的表、特征和任务集。谷歌分享了在开发此类图基础模型（GFM）方面的最新进展，这些模型将图学习和表格机器学习的边界推向了标准基线之外。\n\n### 将关系表转换为图\n\n*   **核心理念**：利用表之间的连接结构是实现有效机器学习算法和更好下游性能的关键，即使表格特征数据（如价格、大小、类别）稀疏或嘈杂。\n*   **数据准备**：唯一的准备步骤是将表集合转换为一个单一的异构图。\n    *   **过程**：每个表成为一个独特的节点类型，表中的每一行成为一个节点。对于表中的每一行，其外键关系成为指向其他表中相应节点的带类型边，而其余列则被视为节点特征（通常为数值或分类值）。此外，还可以选择将时间信息保留为节点或边特征。\n    *   **挑战**：将关系表转换为图后，每个目标域都会产生具有不同节点类型、边类型、节点特征和节点标签的独立图。下一个挑战是创建一个单一的、可泛化的ML模型，该模型可以在一个图（一组表）上训练，并在结构和模式存在差异的任何未见图上执行推理。\n\n### 图基础模型 (GFM)\n\n*   **构建方法**：构建基础模型的典型方法是使用高容量神经网络（如Transformer）在大量多样化数据上进行训练。\n*   **GFM的独特挑战**：图缺乏通用的“标记化”机制。与语言和视觉模型不同，图无法通过预定义的词汇表或图像块进行统一编码。\n*   **解决方案**：这需要可迁移的方法来编码任意数据库模式（无论节点类别和边类型的数量如何）并处理节点特征。这包括为具有不同数量和类型的特征（例如，三个连续浮点特征或三十个分类特征）的节点推导出固定大小的表示。为了使模型能够泛化到任意表和节点类型（例如，在引文图上训练并在产品图上进行推理），不能依赖硬编码的节点类型嵌入表。同样，对于节点特征，模型需要能够从训练时的“长度”和“季节”等特征泛化到“价格”和“大小”等任意浮点和分类特征。\n*   **关键发现**：模型如果依赖“绝对”数据集特征（即硬编码的嵌入表或特定于给定特征分布的投影），则无法泛化。相反，捕捉特征在多样化任务中如何相互作用，才能带来更好的泛化能力。\n\n### 成果\n\n*   **谷歌规模应用**：在谷歌规模下，处理数十亿节点和边的图，JAX环境和可扩展的TPU基础设施发挥了关键作用。如此大的数据量有利于训练通用模型。\n*   **内部任务表现**：GFM在多个内部分类任务中进行了测试，例如广告中的垃圾邮件检测，这些任务涉及数十个大型且相互连接的关系表。\n*   **性能提升**：与最佳调优的单表基线相比，GFM带来了显著的性能提升。根据下游任务的不同，GFM在平均精度方面带来了3到40倍的增益，这表明关系表中的图结构为ML模型提供了至关重要的信号。\n\n**图片：GFM在关系数据上的表现提升**\n\n![GFM4RelationalData-4](https://storage.googleapis.com/gweb-research2023-media/images/GFM4RelationalData-4.width-1250.png)\n\n### 结论\n\n利用数据结构改进ML模型是一个日益重要的领域，在人工智能中具有广泛应用。将基础模型方法应用于图学习，开辟了模型重用的新途径，并显著改善了零样本和少样本泛化能力。通过进一步的扩展、多样化的训练数据收集以及对泛化更深入的理论理解，这些结果有望得到进一步提升。",
      "shortSummary": "关系数据库广泛应用，但传统机器学习难以利用其复杂连接。现有图神经网络（GNN）缺乏泛化能力。谷歌提出“图基础模型”（GFM），通过将关系表转换为异构图，并学习可泛化的图表示。GFM克服了图数据“标记化”挑战，通过捕捉特征交互实现泛化。在谷歌内部任务中，GFM比传统方法性能提升3-40倍，证明图结构为机器学习模型提供了关键信号，显著改善了零样本和少样本泛化能力。",
      "translated_title": "用于关系数据的图基础模型",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GFM4RelationalData-4.width-1250.png",
          "alt": "GFM4RelationalData-4",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"m73wf\">Relational databases constitute the main bulk of enterprise data formats and power many prediction services across Google as well as other services people use every day, like content recommendation or traffic prediction. Most non-trivial applications employ multiple tables — in fact, some elaborate applications at Google might require maintaining hundreds of tables — and extracting an actionable value from such networks of tables is rather non-trivial. Traditional tabular machine learning (ML) methods (like <a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning\" target=\"_blank\" rel=\"noopener noreferrer\">decision trees</a>) often struggle to fully leverage the connectivity structure of these relational schemas.</p><p data-block-key=\"lh3b\">On the other hand, recent advances in ML offer a suite of tools to build <a href=\"https://research.google/blog/graph-neural-networks-in-tensorflow/\">graph neural networks</a> (GNN) tailored for <a href=\"https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)\" target=\"_blank\" rel=\"noopener noreferrer\">graph</a>-structured data, where industry-relevant tasks can be framed as <a href=\"https://research.google/blog/graph-neural-networks-in-tensorflow/\">node classification</a> (or regression) or <a href=\"https://research.google/blog/insights-into-population-dynamics-a-foundation-model-for-geospatial-inference/\">graph-level predictions</a>. However, most GNNs are fixed to a particular graph on which the model has been trained and cannot generalize to novel graphs with new nodes, edge types, features, and node labels. For example, a model trained on a large 100M-node citation graph benchmark can’t be re-used for your own graph (e.g., transactions between users and products) since the feature and label spaces are vastly different, so you’ll have to re-train the same model from scratch on your own data. While some initial attempts have demonstrated the viability of the concept in specific <a href=\"https://arxiv.org/abs/2310.04562\" target=\"_blank\" rel=\"noopener noreferrer\">link prediction</a> and <a href=\"https://arxiv.org/abs/2405.20445\" target=\"_blank\" rel=\"noopener noreferrer\">node classification</a> tasks, there has yet to be a generalist model that can learn meaningful representations across relational data and tackle all node-, link-, and graph-level prediction tasks.</p><p data-block-key=\"b1clf\">Today, we explore the possibility of designing a single model that can excel on interconnected relational tables and at the same time generalize to any arbitrary set of tables, features, and tasks without additional training. We are excited to share our recent progress on developing such graph foundation models (GFM) that push the frontiers of graph learning and tabular ML well beyond standard baselines.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Relational tables as graphs</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"m73wf\">We argue that leveraging the connectivity structure between tables is key for effective ML algorithms and better downstream performance, even when tabular feature data (e.g., price, size, category) is sparse or noisy. To this end, the only data preparation step consists of transforming a collection of tables into a single heterogeneous graph.</p><p data-block-key=\"202mb\">The process is rather straightforward and can be executed at scale: each table becomes a unique node type and each row in a table becomes a node. For each row in a table, its <a href=\"https://en.wikipedia.org/wiki/Foreign_key\" target=\"_blank\" rel=\"noopener noreferrer\">foreign key</a> relations become typed edges to respective nodes from other tables while the rest of the columns are treated as node features (typically, with numerical or categorical values). Optionally, we can also keep temporal information as node or edge features.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n            <div class=\"glue-ambient-video \">\n                <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/GFM4RelationalData-2.mp4\" type=\"video/mp4\">\n                </video>\n\n                <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n                    <div class=\"glue-ambient-video__tooltip\">\n                    <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                    <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                    </div>\n                    <div class=\"glue-ambient-video__icon\">\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                    </div>\n                </div>\n            </div>\n        \n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8l8yd\"><i>Data preparation consists of transforming tables into a single graph, where each row of a table becomes a node of the respective node type, and foreign key columns become edges between the nodes. Connections between five tables shown become edges in the resulting graph.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"9c5o9\">Transforming relational tables into graphs for each target domain results in separate graphs with a different number of node types, edge types, node features, and node labels. The next challenge is to create a single generalizable ML model, which can be trained on one graph (a set of tables) and perform inference on any unseen graph despite the differences in the structure and schema.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Graph foundation models</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9c5o9\">A typical recipe for building foundation models is to use a high-capacity neural network (like a <a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" rel=\"noopener noreferrer\">Transformer</a>) trained on large amounts of diverse data. A unique challenge of GFMs is the lack of a common tokenization mechanism for graphs. In contrast, when applying a Transformer to language and vision models, every possible string can be represented via <a href=\"https://research.google/blog/a-fast-wordpiece-tokenization-system/\">tokens from a prepared vocabulary</a> or images and videos can be <a href=\"https://research.google/blog/improving-vision-transformer-efficiency-and-accuracy-by-learning-to-tokenize/\">encoded via image patches</a>, respectively.</p><p data-block-key=\"eke0d\">When applied to heterogeneous graphs made of relational data, this requires transferable methods for encoding arbitrary database schemas — regardless of the number of node (class) and edge types between them — and handling node features. This includes deriving a fixed-size representation for nodes with, for example, three continuous float features or thirty categorical features. Since we want a single model that can generalize to arbitrary tables and node types — for example, training on citation graphs and running inference on product graphs — we cannot rely on hard-coded embedding tables of node types. Similarly for node features, we want a model to generalize from training on features like “length” and “season” to arbitrary floats and categorical features like “price” and “size”.</p><p data-block-key=\"bmg6d\">Our key finding is that models trained on “absolute” dataset features, i.e., hard-coded embedding tables or projections specific to a given feature distribution, do not generalize, whereas capturing how features interact with each other in diverse tasks does lead to better generalization.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n            <div class=\"glue-ambient-video \">\n                <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/GFM4RelationalData-1.mp4\" type=\"video/mp4\">\n                </video>\n\n                <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n                    <div class=\"glue-ambient-video__tooltip\">\n                    <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                    <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                    </div>\n                    <div class=\"glue-ambient-video__icon\">\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                    </div>\n                </div>\n            </div>\n        \n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"w8ha8\">Similar to frontier language and vision models like <a href=\"https://deepmind.google/models/gemini/pro/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a>, a GFM is <i>a single model that learns transferable graph representations that can generalize to any new, previously unseen graph, including its schema, structure, and features.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9c5o9\">Operating at Google scale means processing graphs of billions of nodes and edges where our <a href=\"https://github.com/jax-ml/jax\" target=\"_blank\" rel=\"noopener noreferrer\">JAX</a> environment and scalable <a href=\"https://en.wikipedia.org/wiki/Tensor_Processing_Unit\" target=\"_blank\" rel=\"noopener noreferrer\">TPU</a> infrastructure particularly shines. Such data volumes are amenable for training generalist models, so we probed our GFM on several internal classification tasks like spam detection in ads, which involves dozens of large and connected relational tables. Typical tabular baselines, albeit scalable, do not consider connections between rows of different tables, and therefore miss context that might be useful for accurate predictions. Our experiments vividly demonstrate that gap.</p><p data-block-key=\"ebjse\">We observe a significant performance boost compared to the best tuned single-table baselines. Depending on the downstream task, GFM brings 3x – 40x gains in average <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\" target=\"_blank\" rel=\"noopener noreferrer\">precision</a>, which indicates that the graph structure in relational tables provides a crucial signal to be leveraged by ML models.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GFM4RelationalData-4.width-1250.png\" alt=\"GFM4RelationalData-4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GFM4RelationalData-4.width-1250.png\" alt=\"GFM4RelationalData-4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusions</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9c5o9\">Leveraging the structure of data to improve ML models is an area of growing importance, <a href=\"https://research.google/blog/talk-like-a-graph-encoding-graphs-for-large-language-models/\">with broad applications in artificial intelligence</a>. We’ve observed that adapting a foundation model approach to graph learning enables new avenues for model reuse, and substantially improved zero-shot and few-shot generalization. These results can be further improved by additional scaling and diverse training data collection together with a deeper theoretical understanding of generalization.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9c5o9\"><i>The following researchers contributed to this work: Michael Galkin, Brandon Mayer, Hamed Sadeghi, Mathieu Gillaume-Bert, Arjun Gopalan, Saurabh Nagrecha, Pramod Doguparty, Bryan Perozzi, Jonathan Halcrow, Silvio Lattanzi, Vahab Mirrokni, and the Google Research</i> <a href=\"https://research.google/teams/graph-mining/\"><i>Graph Mining team</i></a><i>. We would also like to thank Kimberly Schwede for creating the illustrations in this post.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "MedGemma：我们最强大的健康AI开发开源模型 (原标题: MedGemma: Our most capable open models for health AI development)",
      "link": "https://research.google/blog/medgemma-our-most-capable-open-models-for-health-ai-development/",
      "pubDate": "Tue, 08 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-08T16:00:00.000Z",
      "creator": "Google",
      "summary": "## MedGemma：我们最强大的健康AI开发开源模型\n\n### 引言：医疗AI的需求与HAI-DEF\n\n医疗保健领域正日益广泛地采用人工智能（AI）来改进工作流管理、患者沟通以及诊断和治疗支持。构建高性能、高效且保护隐私的AI系统至关重要。基于这些考量，我们构建并发布了**健康AI开发者基础（HAI-DEF）**。HAI-DEF是一系列轻量级开源模型，旨在为开发者提供强大的起点，以进行健康研究和应用开发。由于HAI-DEF模型是开源的，开发者可以完全控制隐私、基础设施和模型的修改。\n\n### MedGemma系列扩展与新模型发布\n\n2023年5月，我们通过**MedGemma**扩展了HAI-DEF系列。MedGemma是一系列基于Gemma 3的生成模型，旨在加速医疗保健和生命科学AI开发。今天，我们自豪地宣布该系列中的两款新模型：\n\n*   **MedGemma 27B 多模态：** 补充了此前发布的4B多模态和27B纯文本模型，新增了对复杂多模态和纵向电子健康记录解读的支持。\n*   **MedSigLIP：** 一款轻量级图像和文本编码器，用于分类、搜索及相关任务。MedSigLIP基于与4B和27B MedGemma模型相同的图像编码器。\n\nMedGemma和MedSigLIP是医疗研究和产品开发的强大起点。MedGemma适用于需要生成自由文本的医疗文本或图像任务，例如报告生成或视觉问答。MedSigLIP推荐用于涉及结构化输出（如分类或检索）的图像任务。所有上述模型都可以在单个GPU上运行，而MedGemma 4B和MedSigLIP甚至可以适应在移动硬件上运行。MedGemma和MedSigLIP开发的完整细节和评估可在MedGemma技术报告中找到。\n\n![MedGemma-1a-Overview](https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-1a-Overview.width-1250.png)\n\n### MedGemma：医疗领域的多模态生成模型\n\nMedGemma系列包括4B和27B两种尺寸的变体，两者现在都接受图像和文本输入并生成文本输出。\n\n*   **MedGemma 4B 多模态：** 在MedQA上得分64.4%，使其位列最佳超小型（<8B）开源模型之列。在一项非盲法研究中，81%的MedGemma 4B生成的胸部X光报告被美国认证放射科医生判断为准确性足以导致与原始放射科医生报告相似的患者管理。它还在医学图像分类任务上取得了与特定任务的SOTA模型相当的性能。\n\n*   **MedGemma 27B 文本和MedGemma 27B 多模态：** 根据内部和已发布的评估，MedGemma 27B模型在MedQA医学知识和推理基准测试中是表现最佳的小型开源模型（<50B）之一；其文本变体得分87.7%，与领先的开源模型DeepSeek R1仅差3分，但推理成本约为其十分之一。MedGemma 27B模型在包括电子健康记录数据检索和解读在内的各种基准测试中，与大型模型具有竞争力。\n\n![MedGemma-2a-MedQA](https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-2a-MedQA.width-1250.png)\n\n*在MedQA上，MedGemma 4B和27B是同等规模模型中表现最佳的。*\n\n![MedGemma-3b-CXR](https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-3b-CXR.width-1250.jpg)\n\n*根据美国心脏胸腔放射科医生的审查，我们发现81%的MedGemma胸部X光报告将导致与原始放射科医生报告相似的患者管理。*\n\n这些模型是通过训练医学优化的图像编码器（独立发布为MedSigLIP，下文描述），然后将Gemma 3模型的相应4B和27B版本在医疗数据上进行训练而开发的。在此过程中，我们注意保留了Gemma的通用（非医疗）能力。这使得MedGemma在混合医疗和非医疗信息的任务中表现良好，并保留了指令遵循和非英语语言能力。\n\n![MedGemma-4a-MedSigLIP](https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-4a-MedSigLIP.width-1250.png)\n\n这些模型的一个关键方面是其适应性。例如，在微调后，MedGemma 4B能够在胸部X光报告生成方面达到最先进的性能，RadGraph F1得分为30.3。开发者能够直接提高其目标应用程序性能的能力，突显了MedGemma作为构建医疗AI的开发者起点的价值。\n\n### MedSigLIP：医疗领域的专用图像编码器\n\nMedSigLIP是一款轻量级图像编码器，仅有4亿参数，采用Sigmoid损失语言图像预训练（SigLIP）架构。MedSigLIP通过使用多样化的医学影像数据（包括胸部X光、组织病理学切片、皮肤病学图像和眼底图像）进行微调，使其能够学习特定于这些模态的细微特征。重要的是，我们也确保MedSigLIP在原始SigLIP模型训练的自然图像上保持强大的性能，从而保持其多功能性。\n\nMedSigLIP旨在通过将医学图像和医学文本编码到共同的嵌入空间中，弥合两者之间的差距。与特定任务的视觉嵌入模型相比，MedSigLIP实现了相似或改进的分类性能，同时在医学影像领域更具通用性。\n\nMedSigLIP非常适合：\n\n*   **传统图像分类：** 构建高性能模型以分类医学图像。\n*   **零样本图像分类：** 无需特定训练示例即可通过比较图像嵌入与文本类别标签的嵌入来分类图像。\n*   **语义图像检索：** 从大型医学图像数据库中查找视觉或语义相似的图像。\n\n### 开源模型的优势\n\n由于MedGemma系列是开源的，模型可以被下载、在此基础上构建并进行微调，以支持开发者的特定需求。特别是在医疗领域，这种开放方法比基于API的模型具有几个明显的优势：\n\n*   **灵活性和隐私：** 模型可以在开发者偏好的环境中（包括Google Cloud Platform或本地）运行，解决隐私问题或机构政策。\n*   **定制化以实现高性能：** 模型可以进行微调和修改，以在目标任务和数据集上实现最佳性能。\n*   **可复现性和稳定性：** 由于模型以快照形式分发，其参数是固定的，不像API那样随时间意外变化。这种稳定性对于一致性和可复现性至关重要的医疗应用尤为关键。\n\n为了确保广泛的可访问性和易用性，我们的Hugging Face集合以流行的Hugging Face safetensors格式提供MedSigLIP和MedGemma。\n\n### 开发者应用案例\n\n研究人员和开发者一直在探索MedGemma模型，并发现这些模型能够很好地解决一些关键问题：\n\n*   美国马萨诸塞州DeepHealth的开发者正在探索MedSigLIP，以改进其胸部X光分诊和结节检测。\n*   台湾长庚纪念医院的研究人员指出，MedGemma与繁体中文医学文献配合良好，并能很好地回答医务人员的问题。\n*   印度古尔冈Tap Health的开发者评论了MedGemma卓越的医学基础，指出其在需要对临床背景敏感的任务（如总结病程记录或建议符合指南的提示）上的可靠性。\n\n我们很高兴能继续从开发者那里了解这些以及其他用例，因为他们正在使用MedGemma和MedSigLIP创建下一代健康AI工具。\n\n### 开始使用\n\n为了帮助开发者入门，我们在GitHub上提供了MedGemma和MedSigLIP的详细Jupyter Notebook，演示了如何在Hugging Face上创建MedSigLIP和MedGemma实例以进行推理和微调。当开发者准备好进行扩展时，MedGemma和MedSigLIP可以无缝部署到Vertex AI作为专用端点，我们还在GitHub中提供了如何在这些端点上运行推理的示例。我们还在HAI-DEF Hugging Face演示集合中添加了一个新演示，展示了如何将MedGemma构建到应用程序中，以简化患者就诊前的预访信息收集。\n\n![MedGemma-6-Summary](https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-6-Summary.width-1250.png)\n\n### 训练数据集说明与免责声明\n\n模型使用公共和私人去识别化数据集混合训练。Google及其合作伙伴利用经过严格匿名化或去识别化的数据集，以确保保护个人研究参与者和患者隐私。\n\n**免责声明：** MedGemma和MedSigLIP旨在作为高效开发下游医疗应用的起点，涉及医疗文本和图像。MedGemma和MedSigLIP不应在未经开发者适当验证、适应和/或进行有意义修改的情况下直接用于其特定用例。这些模型生成的输出不旨在直接用于临床诊断、患者管理决策、治疗建议或任何其他直接临床实践应用。性能基准突出显示了相关基准上的基线能力，但即使对于构成大量训练数据的图像和文本领域，也可能出现不准确的模型输出。所有模型输出都应被视为初步结果，需要通过既定的研究和开发方法进行独立验证、临床关联和进一步调查。",
      "shortSummary": "Google发布了MedGemma和MedSigLIP，进一步扩展其健康AI开发者基础（HAI-DEF）开源模型系列。MedGemma是基于Gemma 3的多模态生成模型，包含4B和27B版本，擅长医疗文本和图像生成，如报告生成。MedSigLIP是轻量级图像和文本编码器，适用于医疗图像分类和检索。这些模型旨在加速医疗AI开发，提供灵活性、隐私保护和定制化能力，并在MedQA等基准测试中展现出卓越性能，为开发者构建下一代健康AI工具提供强大起点。",
      "translated_title": "MedGemma：我们最强大的健康AI开发开源模型",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-1a-Overview.width-1250.png",
          "alt": "MedGemma-1a-Overview",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-2a-MedQA.width-1250.png",
          "alt": "MedGemma-2a-MedQA",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-3b-CXR.width-1250.jpg",
          "alt": "MedGemma-3b-CXR",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-4a-MedSigLIP.width-1250.png",
          "alt": "MedGemma-4a-MedSigLIP",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-6-Summary.width-1250.png",
          "alt": "MedGemma-6-Summary",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"vfkaa\">Healthcare is increasingly embracing AI to improve workflow management, patient communication, and diagnostic and treatment support. It’s critical that these AI-based systems are not only high-performing, but also efficient and privacy-preserving. It’s with these considerations in mind that we built and recently released <a href=\"http://goo.gle/hai-def\" target=\"_blank\" rel=\"noopener noreferrer\">Health AI Developer Foundations</a> (HAI-DEF). HAI-DEF is a collection of lightweight open models designed to offer developers robust starting points for their own health research and application development. Because HAI-DEF models are open, developers retain full control over privacy, infrastructure and modifications to the models. In <a href=\"https://research.google/blog/google-research-at-google-io-2025/\">May</a> of this year, we expanded the HAI-DEF collection with <a href=\"https://deepmind.google/models/gemma/medgemma/\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma</a>, a collection of generative models based on <a href=\"https://deepmind.google/models/gemma/gemma-3/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma 3</a> that are designed to accelerate healthcare and lifesciences AI development.</p><p data-block-key=\"57k5v\">Today, we’re proud to announce two new models in this collection. The first is MedGemma 27B Multimodal, which complements the previously-released 4B Multimodal and 27B text-only models by adding support for complex multimodal and longitudinal electronic health record interpretation. The second new model is MedSigLIP, a lightweight image and text encoder for classification, search, and related tasks. MedSigLIP is based on the same image encoder that powers the 4B and 27B MedGemma models.</p><p data-block-key=\"748kc\">MedGemma and MedSigLIP are strong starting points for medical research and product development. MedGemma is useful for medical text or imaging tasks that require generating free text, like report generation or visual question answering. MedSigLIP is recommended for imaging tasks that involve structured outputs like classification or retrieval. All of the above models can be run on a single GPU, and MedGemma 4B and MedSigLIP can even be adapted to run on mobile hardware.</p><p data-block-key=\"3rv2m\">Full details of MedGemma and MedSigLIP development and evaluation can be found in the <a href=\"https://arxiv.org/abs/2507.05201\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma technical report</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-1a-Overview.width-1250.png\" alt=\"MedGemma-1a-Overview\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-1a-Overview.width-1250.png\" alt=\"MedGemma-1a-Overview\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">MedGemma: A multimodal generative model for health</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ndfm1\">The MedGemma collection includes variants in 4B and 27B sizes, both of which now accept image and text inputs and produce text outputs.</p><ul><li data-block-key=\"fbsaf\"><b>MedGemma 4B Multimodal</b>: MedGemma 4B scores 64.4% on <a href=\"https://arxiv.org/abs/2009.13081\" target=\"_blank\" rel=\"noopener noreferrer\">MedQA</a>, which ranks it among the best very small (&lt;8B) open models. In an unblinded study, 81% of MedGemma 4B–generated chest X-ray reports were judged by a US board certified radiologist to be of sufficient accuracy to result in similar patient management compared to the original radiologist reports. It additionally achieves performance on medical image classification tasks that is competitive with task-specific state-of-the-art models.<br><br></li><li data-block-key=\"2qp0\"><b>MedGemma 27B Text</b> and<b> MedGemma 27B Multimodal</b>: Based on internal and published evaluations, the MedGemma 27B models are among the best performing small open models (&lt;50B) on the MedQA medical knowledge and reasoning benchmark; the text variant scores 87.7%, which is within 3 points of <a href=\"https://github.com/deepseek-ai/DeepSeek-R1\" target=\"_blank\" rel=\"noopener noreferrer\">DeepSeek R1</a>, a leading open model, but at approximately one tenth the inference cost. The MedGemma 27B models are competitive with larger models across a variety of benchmarks, including retrieval and interpretation of electronic health record data.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-2a-MedQA.width-1250.png\" alt=\"MedGemma-2a-MedQA\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-2a-MedQA.width-1250.png\" alt=\"MedGemma-2a-MedQA\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ou0n6\">On MedQA, MedGemma 4B and 27B are among the best performing models of their size. Note that in this plot, cost estimates are made based on <a href=\"http://legacy.lmarena.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">legacy.lmarena.ai</a> price analysis and <a href=\"http://together.ai/pricing\" target=\"_blank\" rel=\"noopener noreferrer\">together.ai/pricing</a>. For models not present on the leaderboard, we used price data from the models from which they were derived.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-3b-CXR.width-1250.jpg\" alt=\"MedGemma-3b-CXR\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-3b-CXR.width-1250.jpg\" alt=\"MedGemma-3b-CXR\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"h7d8g\">Based on review by a US board-certified cardiothoracic radiologist, we found that 81% of MedGemma chest X-ray reports would lead to similar patient management compared to the original radiologist reports.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ndfm1\">We developed these models by training a medically optimized image encoder (independently released as MedSigLIP, described below), followed by training the corresponding 4B and 27B versions of the <a href=\"https://huggingface.co/docs/transformers/main/en/model_doc/gemma3\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma 3 model</a> on medical data. We took care to retain the general (non-medical) capabilities of Gemma throughout this process. This allows MedGemma to perform well on tasks that mix medical and non-medical information and preserve instruction-following and capabilities in non-English languages.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-4a-MedSigLIP.width-1250.png\" alt=\"MedGemma-4a-MedSigLIP\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-4a-MedSigLIP.width-1250.png\" alt=\"MedGemma-4a-MedSigLIP\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ndfm1\">A key aspect of these models is their adaptability. For instance, after fine-tuning, MedGemma 4B is able to achieve state-of-the-art performance on chest X-ray report generation, with a <a href=\"https://arxiv.org/abs/2106.14463\" target=\"_blank\" rel=\"noopener noreferrer\">RadGraph F1</a> score of 30.3. The straightforward ability for developers to improve performance on their target applications highlights the value of MedGemma as a starting point for developers looking to build AI for healthcare.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">MedSigLIP: A specialized image encoder for healthcare</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ndfm1\">MedSigLIP is a lightweight image encoder of only 400M parameters that uses the <a href=\"https://arxiv.org/abs/2303.15343\" target=\"_blank\" rel=\"noopener noreferrer\">Sigmoid loss for Language Image Pre-training</a> (SigLIP) architecture. MedSigLIP was adapted from SigLIP via tuning with diverse medical imaging data, including chest X-rays, <a href=\"https://en.wikipedia.org/wiki/Histopathology\" target=\"_blank\" rel=\"noopener noreferrer\">histopathology</a> patches, dermatology images, and <a href=\"https://en.wikipedia.org/wiki/Fundus_photography\" target=\"_blank\" rel=\"noopener noreferrer\">fundus images</a>, allowing the model to learn nuanced features specific to these modalities. Importantly, we also took care to ensure that MedSigLIP retains strong performance on the natural images on which the original SigLIP model was trained, maintaining its versatility.</p><p data-block-key=\"8g76m\">MedSigLIP is designed to bridge the gap between medical images and medical text by encoding them into a common embedding space. MedSigLIP achieves similar or improved classification performance compared to task-specific vision embedding models while being far more versatile across medical imaging domains.</p><p data-block-key=\"673ca\">MedSigLIP is ideal for:</p><ul><li data-block-key=\"3ecjd\"><i>Traditional image classification:</i> Build performant models to classify medical images.<br><br></li><li data-block-key=\"8sio3\"><i>Zero-shot image classification:</i> Classify images without specific training examples by comparing image embeddings to the embeddings of textual class labels.<br><br></li><li data-block-key=\"9m877\"><i>Semantic image retrieval:</i> Find visually or semantically similar images from large medical image databases.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The power of open models</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ndfm1\">Because the MedGemma collection is open, the models can be downloaded, built upon, and fine-tuned to support developers’ specific needs. Particularly in the medical space, this open approach offers several distinct advantages over API-based models:</p><ul><li data-block-key=\"fqifp\"><i>Flexibility and privacy:</i> Models can be run on proprietary hardware in the developer’s preferred environment, including on Google Cloud Platform or locally, which can address privacy concerns or institutional policies.<br><br></li><li data-block-key=\"1j8r8\"><i>Customization for high performance:</i> Models can be fine-tuned and modified to achieve optimal performance on target tasks and datasets.<br><br></li><li data-block-key=\"b0rvo\"><i>Reproducibility and stability:</i> Because the models are distributed as snapshots, their parameters are frozen and unlike an API, will not change unexpectedly over time. This stability is particularly crucial for medical applications where consistency and reproducibility are paramount.</li></ul><p data-block-key=\"bbula\">To ensure broad accessibility and ease of use, our <a href=\"https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4\" target=\"_blank\" rel=\"noopener noreferrer\">Hugging Face collection</a> offers MedSigLIP and MedGemma in the popular <a href=\"https://huggingface.co/docs/safetensors/en/index\" target=\"_blank\" rel=\"noopener noreferrer\">Hugging Face safetensors</a> format.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">What developers are building with MedGemma &amp; MedSigLIP</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ndfm1\">Researchers and developers have been exploring the MedGemma models for their use cases and have found the models adept at solving some crucial problems. Developers at <a href=\"https://deephealth.com/\" target=\"_blank\" rel=\"noopener noreferrer\">DeepHealth</a> in Massachusetts, USA have been exploring MedSigLIP to improve their chest X-ray triaging and nodule detection. Researchers at <a href=\"https://www.cgmh.org.tw/eng\" target=\"_blank\" rel=\"noopener noreferrer\">Chang Gung Memorial Hospital</a> in Taiwan noted that MedGemma works well with traditional Chinese-language medical literature and can respond well to medical staff questions. Developers at <a href=\"https://tap.health/\" target=\"_blank\" rel=\"noopener noreferrer\">Tap Health</a> in Gurgaon, India, remarked on MedGemma’s superior medical grounding, noting its reliability on tasks that require sensitivity to clinical context, such as summarizing progress notes or suggesting guideline-aligned nudges.</p><p data-block-key=\"fisuk\">We’re excited to continue to learn about these and other use cases from developers as they create the next generation of Health AI tools with MedGemma and MedSigLIP.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n            <div class=\"glue-ambient-video \">\n                <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/MedGemma-0a-HeroVid.mp4\" type=\"video/mp4\">\n                </video>\n\n                <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n                    <div class=\"glue-ambient-video__tooltip\">\n                    <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                    <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                    </div>\n                    <div class=\"glue-ambient-video__icon\">\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                    </div>\n                </div>\n            </div>\n        \n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Get started and explore</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ndfm1\">To help developers get started, we’ve provided detailed notebooks on GitHub for <a href=\"https://github.com/google-health/medgemma\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma</a> and <a href=\"https://github.com/google-health/medsiglip\" target=\"_blank\" rel=\"noopener noreferrer\">MedSigLIP</a> that demonstrate how to create instances of MedSigLIP and MedGemma for both inference and fine-tuning on Hugging Face. When developers are ready to scale, MedGemma and MedSigLIP can be seamlessly deployed in <a href=\"https://cloud.google.com/vertex-ai\" target=\"_blank\" rel=\"noopener noreferrer\">Vertex AI</a> as dedicated endpoints, and we provide examples in GitHub of how to run inference on these endpoints. We’ve also added a <a href=\"https://huggingface.co/spaces/google/appoint-ready\" target=\"_blank\" rel=\"noopener noreferrer\">new demo</a> to our HAI-DEF Hugging Face <a href=\"https://huggingface.co/collections/google/hai-def-concept-apps-6837acfccce400abe6ec26c1\" target=\"_blank\" rel=\"noopener noreferrer\">demo collection</a> that shows how MedGemma can be built into an application to streamline pre-visit information gathering ahead of a patient appointment.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n            <div class=\"glue-ambient-video \">\n                <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/MedGemma-5a-Demos.mp4\" type=\"video/mp4\">\n                </video>\n\n                <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n                    <div class=\"glue-ambient-video__tooltip\">\n                    <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                    <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                    </div>\n                    <div class=\"glue-ambient-video__icon\">\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                    </div>\n                </div>\n            </div>\n        \n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"7t2ez\">This demo illustrates how MedGemma can be built into an application to streamline pre-visit information gathering ahead of a patient appointment. Code for the demo is available on <a href=\"https://huggingface.co/spaces/google/appoint-ready\" target=\"_blank\" rel=\"noopener noreferrer\">its Hugging Face site</a>.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xdhby\">Refer to the following table to understand which model from the MedGemma family is ideal for your use case.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-6-Summary.width-1250.png\" alt=\"MedGemma-6-Summary\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MedGemma-6-Summary.width-1250.png\" alt=\"MedGemma-6-Summary\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --left\">\n        <p data-block-key=\"zr19a\">* For pathology-specific applications that do not require language alignment, <a href=\"https://developers.google.com/health-ai-developer-foundations/path-foundation\" target=\"_blank\" rel=\"noopener noreferrer\">Path Foundation</a> provides high performance for data-efficient classification and lower compute requirements.</p><p data-block-key=\"45ch6\">** <a href=\"https://en.wikipedia.org/wiki/Fast_Healthcare_Interoperability_Resources\" target=\"_blank\" rel=\"noopener noreferrer\">Fast Healthcare Interoperability Resources</a> (FHIR) records are text-based, but have a unique structure. Electronic health record data was included in the training of the MedGemma 27B multimodal model only.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ndfm1\">Please visit <a href=\"https://goo.gle/hai-def\" target=\"_blank\" rel=\"noopener noreferrer\">the HAI-DEF site</a> for these resources and to learn more about the MedGemma collection and other Health AI Developer Foundations models. The <a href=\"https://discuss.ai.google.dev/c/hai-def/62\" target=\"_blank\" rel=\"noopener noreferrer\">HAI-DEF forum</a> is available for questions or feedback.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Note on training datasets</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ndfm1\">Models were trained on a mix of public and private de-identified datasets. Google and its partners utilize datasets that have been rigorously anonymized or de-identified to ensure the protection of individual research participants and patient privacy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Disclaimer</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ndfm1\">MedGemma and MedSigLIP are intended to be used as a starting point that enables efficient development of downstream healthcare applications involving medical text and images. MedGemma and MedSigLIP are not intended to be used without appropriate validation, adaptation and/or making meaningful modification by developers for their specific use case. The outputs generated by these models are not intended to directly inform clinical diagnosis, patient management decisions, treatment recommendations, or any other direct clinical practice applications. Performance benchmarks highlight baseline capabilities on relevant benchmarks, but even for image and text domains that constitute a substantial portion of training data, inaccurate model output is possible. All model outputs should be considered preliminary and require independent verification, clinical correlation, and further investigation through established research and development methodologies.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"yb79i\"><i>MedGemma is the product of a collaboration between Google Research and Google DeepMind. We thank the many people who contributed to this work, including the engineering and cross-functional members of the Google Health AI and Gemma teams, as well as our sponsors in Google Research and Google Deepmind.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "通过声音定位提升群组对话的可访问性 (原标题: Making group conversations more accessible with sound localization)",
      "link": "https://research.google/blog/making-group-conversations-more-accessible-with-sound-localization/",
      "pubDate": "Tue, 01 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-01T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 通过声音定位提升群组对话的可访问性：SpeechCompass\n\n### 引言：现有问题与挑战\n\n当前移动设备上的语音转文本（ASR）功能（如Live Transcribe）在听力与言语辅助、语言翻译、笔记记录和会议记录方面发挥着重要作用。然而，在多人对话中，现有的移动ASR应用通常会将所有转录的语音内容简单地拼接在一起，导致用户难以区分是谁在说话。这种限制增加了用户的认知负担，他们需要同时处理文本、识别说话者并参与对话。此外，依赖机器学习（ML）的现有解决方案在移动场景中设置复杂，例如，音视频语音分离需要说话者在摄像头可见范围内，而说话者嵌入方法则需要模型确定每个说话者独特的声纹。\n\n### SpeechCompass 方案概述\n\n在荣获CHI 2025最佳论文奖的“SpeechCompass: 通过多麦克风定位增强移动字幕的说话人分离和方向引导”研究中，我们探索了一种通过说话人分离（在ASR文本中区分说话者）和实时传入声音定位来增强移动字幕的方法。SpeechCompass通过为每个说话者提供颜色编码的视觉分离和方向指示器（箭头），帮助用户确定声音的来源方向，从而为群组对话创建用户友好的文本记录。\n\n![SpeechCompass-2](https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-2.width-1250.jpg)\n左图：现有移动转录应用将转录文本拼接在一起。右图：SpeechCompass指示传入语音的方向，在用户界面中通过颜色和方向指示器（如箭头）实现视觉分离的文本。\n\n### SpeechCompass 的优势\n\n与基于机器学习的单源说话人分离方法不同，SpeechCompass的多麦克风方法具有以下优势：\n\n*   **更低的计算和内存成本**：由于没有模型或权重，算法可以在内存和计算能力有限的小型微控制器上运行。\n*   **更低的延迟**：提取说话人嵌入需要足够的信息来聚类说话者，这可能导致延迟。\n*   **更好的隐私保护**：SpeechCompass假设不同的说话者物理上位于不同的位置，不需要视频或任何独特的个人身份信息（如说话者嵌入）。\n*   **语言无关性**：SpeechCompass关注音频波形之间的差异，不预设内容，适用于语音以外的声音。\n*   **即时重新配置**：通过移动手机，SpeechCompass可以即时重新配置。\n\n### 实现方式\n\n我们以两种形式实现了SpeechCompass：\n\n1.  **手机壳原型**：一个带有四个麦克风并连接到低功耗微控制器的手机壳，提供最佳麦克风放置以实现360度声音定位。\n2.  **现有手机软件**：针对带有两个或更多麦克风的现有手机（如Pixel手机）的软件实现，提供180度定位。\n\n在这两种实现中，手机用于语音识别，并通过移动应用程序可视化文本记录。\n\n![SpeechCompass-5](https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-5.width-1250.png)\n原型手机壳及其内部电子元件的实现。A) 带有安装原型壳的移动应用程序界面。B) 带有柔性PCB麦克风支架和主PCB的原型壳。C) 主PCB (STM32) 的顶视图和底视图。\n\n### 声音定位算法\n\n由于声音频率较低，在室内环境中容易反弹产生混响，使得音频（尤其是语音）难以精确本地化。为解决此问题，我们应用了一种基于到达时间差（TDOA）的定位算法。音频信号以略微不同的时间到达每个麦克风，因此算法通过互相关估计麦克风对之间的TDOA，以预测声音的到达角度。具体来说，我们使用广义互相关相位变换（GCC-PHAT）来提高噪声鲁棒性并加快计算速度。然后，我们应用统计估计（如核密度估计）来提高定位精度。使用两个全向麦克风总是存在“前后”混淆（即，阵列前方或后方的信号对麦克风阵列来说是相同的），因此只能实现180度定位。通过使用三个或更多麦克风可以解决此问题，从而实现360度定位。\n\n![SpeechCompass-1](https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-1.width-1250.png)\nSpeechCompass系统图，包括手机壳硬件和手机应用程序。\n\n### 用户界面可视化\n\n我们利用Android的语音转文本功能开发了一个移动应用程序，通过USB接收来自原型手机壳麦克风的定位数据，并增强语音文本记录。Android应用程序提供多种语音文本可视化样式来指示说话者方向：\n\n*   **彩色文本**：使用不同颜色的文本区分说话者。\n*   **方向符号**：文本框周围的箭头、圆形刻度盘和颜色高亮指示每个说话者的位置。\n*   **小地图**：一个类似雷达的小型显示屏显示当前说话者的位置。\n*   **边缘指示器**：屏幕边缘的视觉提示突出显示说话者方向。\n*   **不必要语音抑制**：用户可以点击屏幕两侧来抑制来自这些方向的语音。这可用于移除自己的语音，或从文本记录中移除不相关的附近对话，从而增强附近说话者的隐私。\n\n![SpeechCompass-3](https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-3.width-1250.png)\n各种可视化样式增强了语音文本记录。\n\n### 技术评估\n\n为评估SpeechCompass软件，我们将手机壳放置在旋转平台上，并用一个固定扬声器播放语音或噪音。平台以10度增量旋转，并测量每个角度的到达角。评估结果显示，SpeechCompass能够准确地定位声音方向，对于正常对话响度（60-65 dB），平均误差为11°-22°。其准确性大致与人类定位能力相当（例如，如果一个人被问及声音从他们身后何处传来，他们的答案通常会有高达20度的误差）。SpeechCompass系统在不同材料和不同环境噪音条件下均表现良好。\n\n![SpeechCompass-6](https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-6.width-1250.png)\n不同音频级别和声源角度下的定位误差。\n\n在说话人分离方面，我们使用说话人分离错误率（DER），这是衡量界面中颜色编码说话人分离正确性的标准指标。我们的测试表明，四麦克风配置始终优于三麦克风设置，在不同信噪比（SNR）条件下，相对DER改善了23%–35%。\n\n![SpeechCompass-7](https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-7.width-1250.jpg)\n在不同信噪比下，3麦克风和4麦克风配置的说话人分离错误率（DER）。\n\n### 用户评估与反馈\n\n为理解当前移动字幕技术的局限性，我们对263名字幕技术常用用户进行了在线调查。结果显示，现有解决方案存在一个显著的局限性——无法区分说话者，这使得它们在群组对话中难以使用。\n\n![SpeechCompass-8](https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-8.width-1250.jpg)\n移动字幕常用用户调查结果。\n\n其次，我们向八位移动语音转文本的常用用户展示了原型并收集了反馈。原型用于分离和可视化研究人员之间的对话。我们发现彩色文本和方向箭头是最受欢迎的可视化方法。所有参与者都认同方向引导对于群组对话的价值。\n\n![SpeechCompass-4](https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-4.width-1250.png)\n工作原型用户研究结果。A) 对不同可视化技术的偏好。B) 方向反馈对用户的价值。\n\n### 未来展望\n\n多麦克风定位技术在移动转录方面具有广泛的实际应用前景，例如在课堂环境中帮助学生更轻松地跟踪师生讨论，或在商务会议、采访和社交聚会中帮助用户跟踪多人对话中的说话者变化。SpeechCompass在群组对话的移动字幕方面取得了显著改进，未来有许多可能的开发方向：\n\n*   与智能眼镜、智能手表等更多可穿戴设备集成。\n*   通过机器学习方法增强噪声鲁棒性。\n*   进一步定制可视化偏好。\n*   进行长期研究以了解日常场景中的采用和行为。\n\n我们希望这项研究能激发持续创新，使沟通对每个人都更易于访问和包容。",
      "shortSummary": "SpeechCompass通过多麦克风声音定位技术，显著提升了群组对话的移动语音转文本体验。它解决了现有应用无法区分说话者的问题，通过颜色编码和方向指示器清晰地分离说话者，从而降低用户认知负担。该方案相比机器学习方法，具有更低的成本、延迟和更高的隐私性。技术评估和用户反馈均证实了其在声音定位和说话人分离方面的有效性，为群组沟通提供了更便捷、更具包容性的解决方案。",
      "translated_title": "通过声音定位提升群组对话的可访问性",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-2.width-1250.jpg",
          "alt": "SpeechCompass-2",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-5.width-1250.png",
          "alt": "SpeechCompass-5",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-1.width-1250.png",
          "alt": "SpeechCompass-1",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-3.width-1250.png",
          "alt": "SpeechCompass-3",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-6.width-1250.png",
          "alt": "SpeechCompass-6",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"k94rq\"><a href=\"https://research.google/blog/an-all-neural-on-device-speech-recognizer/\">Speech-to-text</a> capabilities on mobile devices, such as <a href=\"https://research.google/blog/real-time-continuous-transcription-with-live-transcribe/\">Live Transcribe</a>, have become invaluable for hearing and speech accessibility, language translation, note-taking, and meeting transcripts. However, when multiple people participate in a conversation, existing mobile automatic speech recognition (ASR) apps typically concatenate all transcribed speech together, making it difficult to follow who is saying what. This limitation creates cognitive overload for users who need to simultaneously process the transcript, identify speakers, and participate in the conversation. Solutions have been deployed, but are currently impractical to set up in mobile scenarios. For example, <a href=\"https://research.google/blog/looking-to-listen-audio-visual-speech-separation/\">audio-visual speech separation</a> requires speakers to be visible to a camera and <a href=\"https://research.google/blog/who-said-what-recorders-on-device-solution-for-labeling-speakers/\">speaker embedding</a> approaches require a model to determine and register the unique voiceprint of each speaker.</p><p data-block-key=\"4qb0t\">In “<a href=\"https://dl.acm.org/doi/pdf/10.1145/3706598.3713631\" target=\"_blank\" rel=\"noopener noreferrer\">SpeechCompass: Enhancing Mobile Captioning with Diarization and Directional Guidance via Multi-Microphone Localization</a>”, recipient of a <a href=\"https://programs.sigchi.org/chi/2025/program/content/189502\" target=\"_blank\" rel=\"noopener noreferrer\">Best Paper Award</a> at <a href=\"https://programs.sigchi.org/chi/2025\" target=\"_blank\" rel=\"noopener noreferrer\">CHI 2025</a>, we explore an approach that enhances mobile captioning with <a href=\"https://en.wikipedia.org/wiki/Speaker_diarisation\" target=\"_blank\" rel=\"noopener noreferrer\">speaker diarization</a> (separating speakers in an ASR transcript) and real-time localization of incoming sound. SpeechCompass creates user-friendly transcripts for group conversations by providing color-coded visual separation for each speaker and directional indicators (arrows) to help users determine the direction from which speech is coming. This multi-microphone approach lowers computational costs, reduces latency, and enhances privacy preservation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-2.width-1250.jpg\" alt=\"SpeechCompass-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-2.width-1250.jpg\" alt=\"SpeechCompass-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"zcbwu\"><b><i>Left</i></b><i>: Existing mobile transcription apps concatenate transcribed text together.</i> <b><i>Right</i></b><i>: SpeechCompass indicates the direction of incoming speech, allowing visually separated transcripts with colors and directional indicators (such as arrows) in the user interface.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Efficient real-time audio localization</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rtnsv\">We implement SpeechCompass in two different forms: as a phone case prototype with four microphones connected to a low-power microcontroller and as software for existing phones with two microphones. The phone case design provides optimal microphone placement to enable 360-degree sound localization. The software implementation offers only 180-degree localization on devices with two or more microphones, such as the Pixel phone. In both implementations, the phone is used for speech recognition and transcripts are visualized using a mobile application.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-5.width-1250.png\" alt=\"SpeechCompass-5\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-5.width-1250.png\" alt=\"SpeechCompass-5\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vzr66\"><i>Implementation of the prototype phone-case and its internal electronics.</i> <b><i>A</i></b><i>) Mobile application interface with a mounted prototype case.</i> <b><i>B</i></b><i>) Prototype case with a flexible</i> <a href=\"https://en.wikipedia.org/wiki/Printed_circuit_board\" target=\"_blank\" rel=\"noopener noreferrer\"><i>PCB</i></a><i> microphone mount and a main PCB.</i> <b><i>C</i></b><i>) Top and bottom view of the main PCB (</i><a href=\"https://en.wikipedia.org/wiki/STM32\" target=\"_blank\" rel=\"noopener noreferrer\"><i>STM32</i></a><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"rtnsv\">Because sound has low frequency, it bounces around indoor environments causing reverberations and making audio, especially speech, difficult to localize precisely. To address this challenge, we apply a localization algorithm based on <a href=\"https://en.wikipedia.org/wiki/Time_of_arrival\" target=\"_blank\" rel=\"noopener noreferrer\">time-difference of arrival</a> (TDOA). Audio signals arrive at each microphone at slightly different times, so the algorithm estimates the TDOA between microphone pairs with <a href=\"https://en.wikipedia.org/wiki/Cross-correlation\" target=\"_blank\" rel=\"noopener noreferrer\">cross-correlation</a> to predict the <a href=\"https://en.wikipedia.org/wiki/Angle_of_arrival\" target=\"_blank\" rel=\"noopener noreferrer\">angle of arrival</a> for the sound. Specifically, we use <a href=\"https://xavieranguera.com/phdthesis/node92.html\" target=\"_blank\" rel=\"noopener noreferrer\">Generalized Cross Correlation with Phase Transform</a> (GCC-PHAT) to improve noise robustness and increase compute speed. We then apply statistical estimations, such as <a href=\"https://en.wikipedia.org/wiki/Kernel_density_estimation\" target=\"_blank\" rel=\"noopener noreferrer\">kernel density estimation</a>, to improve localizer precision. The use of two omnidirectional microphones will always have “front–back” confusion (i.e., when the signals in front or in the back of the array appear identical to the <a href=\"https://en.wikipedia.org/wiki/Microphone_array\" target=\"_blank\" rel=\"noopener noreferrer\">microphone array</a>), thus allowing only 180 degree localization. This issue is solved by using three or more microphones, making 360 degree localization possible.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-1.width-1250.png\" alt=\"SpeechCompass-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-1.width-1250.png\" alt=\"SpeechCompass-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vzr66\"><i>SpeechCompass system diagram, including the phone case hardware and the phone application.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"rtnsv\">Unlike ML approaches to single-source speaker diarization, the SpeechCompass multi-microphone approach offers several advantages:</p><ul><li data-block-key=\"ag2i2\"><i>Lower computational and memory costs:</i> Since there is no model nor weights, the algorithm can run on small microcontrollers with limited memory and compute.</li><li data-block-key=\"17dpt\"><i>Reduced latency:</i> SpeechCompass does not rely on capturing distinguishing voice characteristics. Instead, it extracts directional information from basic sound properties, allowing it to operate in real-time with minimal lag.</li><li data-block-key=\"7p4fj\"><i>Greater privacy preservation:</i> SpeechCompass assumes that different speakers are physically in separate places and does not require video or any unique personally identifying information, like speaker embeddings (unique identity of individual’s voice).</li><li data-block-key=\"adsip\"><i>Language-agnostic operation:</i> SpeechCompass looks at differences between the audio waveforms, without prior assumptions about the content and works for sounds beyond speech.</li><li data-block-key=\"b3kk5\"><i>Instant reconfiguration</i>: SpeechCompass can be reconfigured instantly by moving the phone.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">User interface for visualizing speaker direction</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rtnsv\">We used Android’s speech-to-text <a href=\"https://developer.android.com/reference/android/speech/SpeechRecognizer\" target=\"_blank\" rel=\"noopener noreferrer\">capabilities</a> to develop a mobile application that augments speech transcripts with localization data sent from the prototype phone case’s microphones via <a href=\"https://en.wikipedia.org/wiki/USB#:~:text=Universal%20Serial%20Bus%20(USB)%20is,between%20many%20types%20of%20electronics.\" target=\"_blank\" rel=\"noopener noreferrer\">USB</a>. The Android application provides multiple visualization styles to indicate speaker direction:</p><ol><li data-block-key=\"711vq\"><i>Colored text</i>: Speakers are separated using different colored text.</li><li data-block-key=\"au4pl\"><i>Directional glyphs</i>: Arrows, dials in a circle and color highlights on the boxes around the text point to the location of each speaker.</li><li data-block-key=\"1fuub\"><i>Minimap</i>: A small radar-like display shows the current speaker's position.</li><li data-block-key=\"40qtr\"><i>Edge indicators</i>: Visual cues around the screen edges highlight speaker direction.</li><li data-block-key=\"1ji9e\"><i>Unwanted speech suppression</i>: The user can click on the sides of the screen to suppress speech coming from those directions. This can be used to remove their own speech. Irrelevant nearby conversation can be removed from the transcript, which enhances the privacy of nearby speakers.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-3.width-1250.png\" alt=\"SpeechCompass-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-3.width-1250.png\" alt=\"SpeechCompass-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vzr66\"><i>Various visualization styles augment speech transcripts.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Technical evaluation</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rtnsv\">To evaluate the SpeechCompass software, we placed a phone case on a rotating platform with a stationary speaker playing speech or noise. The platform was rotated at 10 degree increments and the angle of arrival was measured for each angle. Our evaluation shows that SpeechCompass can accurately localize sound direction with an average error of 11°–22° for normal conversational loudness (60–65 <a href=\"https://en.wikipedia.org/wiki/Sound_pressure#Sound_pressure_level\" target=\"_blank\" rel=\"noopener noreferrer\">dB</a>). The accuracy is roughly comparable to human localization abilities. For example, if a person were asked where sound was heard behind them, their answer would typically have up to 20 degrees error. The SpeechCompass system performs well across different materials and under varying ambient noise conditions, see the <a href=\"https://dl.acm.org/doi/pdf/10.1145/3706598.3713631\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a> for more details.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-6.width-1250.png\" alt=\"SpeechCompass-6\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-6.width-1250.png\" alt=\"SpeechCompass-6\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vzr66\"><i>Error in localization at different audio levels and source angles.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"rtnsv\">For diarization, we used <a href=\"https://pyannote.github.io/pyannote-metrics/reference.html#diarization\" target=\"_blank\" rel=\"noopener noreferrer\">diarization error rate</a> (DER), a standard metric for diarization that corresponds to correctness of color coded speaker diarization in the interface. Our tests showed the four-microphone configuration consistently outperformed the three-microphone setup, with relative DER improvements of 23%–35% across different <a href=\"https://en.wikipedia.org/wiki/Signal-to-noise_ratio\" target=\"_blank\" rel=\"noopener noreferrer\">signal-to-noise</a> (SNR) conditions.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-7.width-1250.jpg\" alt=\"SpeechCompass-7\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-7.width-1250.jpg\" alt=\"SpeechCompass-7\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vzr66\"><i>Diarization error rate (DER) with 3- and 4-microphone configurations at different signal-to-noise ratios.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">User evaluation and feedback</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rtnsv\">To understand the limitations of current mobile captioning technology, we conducted an online survey with 263 frequent users of captioning technology. The results show that current solutions struggle with a significant limitation — the inability to distinguish between speakers makes them challenging to use in group conversations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-8.width-1250.jpg\" alt=\"SpeechCompass-8\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-8.width-1250.jpg\" alt=\"SpeechCompass-8\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vzr66\"><i>The results of the survey with frequent users of mobile captioning.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"rtnsv\">Second, we demonstrated the prototype to eight frequent users of mobile speech-to-text and gathered feedback. The prototype was used to diarize and visualize a conversation between the researchers. We found that colored text and directional arrows were the most preferred visualization methods. All participants agreed on the value of directional guidance for group conversations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-4.width-1250.png\" alt=\"SpeechCompass-4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpeechCompass-4.width-1250.png\" alt=\"SpeechCompass-4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vzr66\"><i>Results of a user study with the working prototype.</i> <b><i>A</i></b><i>) Preferences for the different visualization techniques.</i> <b><i>B</i></b><i>) Value of the directional feedback to the users.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">What's next?</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rtnsv\">We imagine that multi-microphone localization for mobile transcription could have numerous practical applications. One example could be in the classroom setting<b>,</b> where students could more easily follow discussions between instructors and classmates. Similarly in business meetings, interviews or social gatherings, users could track speaker changes in multi-person conversations.</p><p data-block-key=\"65sir\">SpeechCompass demonstrates significant improvements for mobile captioning in group conversations, and there are numerous possible directions for additional development:</p><ul><li data-block-key=\"5m8pb\">Integration with additional wearable form factors like smart glasses and smartwatches</li><li data-block-key=\"25pn2\">Enhanced noise robustness through machine learning approaches</li><li data-block-key=\"e031l\">Further customization of visualization preferences</li><li data-block-key=\"ebktc\">Longitudinal studies to understand adoption and behavior in everyday scenarios</li></ul><p data-block-key=\"31ksu\">We hope that this research inspires continued innovation in making communication more accessible and inclusive for everyone.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"rtnsv\"><i>We thank Artem Dementyev, Alex Olwal, Mathieu Parvaix, Chiong Lai and Dimitri Kanevsky for their work on the SpeechCompass publication and research. Dmitrii Votintcev for ideas on prototypes and interaction designs. We are grateful to Pascal Getreuer, Richard Lyon, Alex Huang, Shao-Fu Shih, and Chet Gnegy for their help with algorithms. We also thank Shaun Kane, James Landay, Malcolm Slaney, and Meredith Morris for their feedback on this paper. We appreciate the contributions of Carson Lau for the phone case mechanical design and Ngan Nguyen for electronics assembly. Finally, we thank Mei Lu, Don Barnett, Ryan Geraghty, and Sanjay Batra for UX research and design.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "谷歌地图如何实现HOV专用预计到达时间 (原标题: How we created HOV-specific ETAs in Google Maps)",
      "link": "https://research.google/blog/how-we-created-hov-specific-etas-in-google-maps/",
      "pubDate": "Sun, 29 Jun 2025 16:00:00 GMT",
      "isoDate": "2025-06-29T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 谷歌地图HOV专用预计到达时间（ETA）的实现\n\n随着电动汽车、拼车和公共交通等可持续出行方式的普及，出行时间变得更加多样化，这主要得益于专用车道，如高载客量（HOV）车道。HOV车道在高峰时段通常比普通车道更快，例如在犹他州盐湖谷，HOV车道的平均速度比普通车道快约16%。为了提升通勤体验，准确的ETA预测和优化的路线规划至关重要。谷歌地图最近推出了一项新功能，允许驾驶员选择包含HOV车道的路线，并查看该路线的ETA。本文详细介绍了谷歌如何开发此功能，并创建了一个分类系统来区分HOV行程和非HOV行程，从而在谷歌地图中实现了HOV专用ETA。\n\n![谷歌地图中的HOV感知路线选项](https://storage.googleapis.com/gweb-research2023-media/images/HOV1_RouteOptions.width-1250.png)\n*谷歌地图中的HOV感知路线选项。*\n\n### 开发HOV专用ETA\n\n为了估算HOV出行时间，谷歌首先通过分析聚合和匿名的交通趋势来推断过去的HOV出行时间，然后利用这些推断的时间来专门训练其ETA预测模型。然而，识别HOV行程并非易事，尤其是在交通流量较小的情况下，HOV用户和非HOV用户的简单数据（如速度）可能相似。HOV出行模式具有一些独特的约束，例如基于位置、时间和特殊事件的可用性限制。\n\n为了解决这些问题，谷歌开发了一种无监督学习方法，在没有初始标签（HOV与非HOV）的情况下进行分类。他们对位于具有HOV可用性的单个路段上的行程部分执行分类任务，然后通过组合这些路段级别的分类来确定整体行程分类。\n\n### 路段级别分类\n\n对于每个单独的路段，谷歌在短时间窗口（例如15分钟）内处理该路段上来自不同行程的行程部分集合。目标是利用行程本身以及同一时间窗口内其他行程的信息来对这些行程部分进行分类。每个行程由旅行期间记录的多个观测值或“行程点”组成，这些点包括速度、距道路中心线的横向距离以及观测时间等信息。\n\n**速度信息是此分类中最突出的特征。** 当HOV车道上的出行时间与普通车道明显不同时，分类任务的价值更高。在这种情况下，通常会观察到双峰速度分布，即同时出现两种不同的交通模式。\n\n*   **场景A：** HOV速度明显快于普通车道。例如，下图显示了西雅图I5公路下午4:00至4:30之间收集的匿名聚合速度数据，其中较快的速度可能对应于使用HOV车道的车辆。\n\n    ![场景A：高峰时段HOV可用路段的速度分布，HOV速度远快于普通车道](https://storage.googleapis.com/gweb-research2023-media/images/HOV2_A.width-1250.png)\n    *场景A：高峰时段HOV可用路段的速度分布，HOV速度远快于普通车道。*\n\n*   **场景B：** HOV速度略快于普通车道，但差异不显著。例如，下图显示了同一路段在高峰时段的速度分布，但HOV速度仅略快。\n\n    ![场景B：高峰时段HOV可用路段的速度分布，HOV速度快于普通车道但差异不显著](https://storage.googleapis.com/gweb-research2023-media/images/HOV3_B.width-1250.png)\n    *场景B：高峰时段HOV可用路段的速度分布，HOV速度快于普通车道但差异不显著。*\n\n### 超越速度：结合估计的横向距离\n\n虽然速度是一个强信号，但谷歌也探索了其他因素来改进分类。估计的距道路中心线的横向距离，尽管由于GPS固有的不精确性而存在噪声，但与速度结合使用时证明是有用的。即使存在一些不精确性，距离信息也有助于突出车道特定的行为，尤其是在与相邻普通车道区分时。\n\n以下两图展示了之前讨论的两种速度分布。它们描绘了西雅图都会区一条五车道高速公路路段上的匿名高峰时段交通数据，其中两条HOV车道位于左侧。图中显示了该路段的速度和估计的距道路中心线的相对距离，数据点用绿色和蓝色编码，以指示它们是否落在HOV车道内。\n\n*   **场景A：** HOV车道速度明显高于普通车道，平均速度分别为65英里/小时和25英里/小时。\n\n    ![场景A：记录的速度与距道路中心线的横向距离](https://storage.googleapis.com/gweb-research2023-media/images/HOV4_ALatDist.width-1250.png)\n    *场景A — 左图：不同匿名行程在单个路段上记录的速度与距道路中心线的横向距离。右图：按数据是否位于HOV车道内划分的速度分布。*\n\n*   **场景B：** 速度差异较小，HOV车道平均速度为67英里/小时，普通车道为55英里/小时。\n\n    ![场景B：记录的速度与距道路中心线的横向距离](https://storage.googleapis.com/gweb-research2023-media/images/HOV5_BLatDist.width-1250.png)\n    *场景B — 左图：不同匿名行程在单个路段上记录的速度与距道路中心线的横向距离。右图：按数据是否位于HOV车道内划分的速度分布。*\n\n### 时间聚类和软分配\n\n聚类是为这些观测值生成初始标签的有效方法。谷歌的方法超越了基本聚类，加入了“时间”维度。在对行程观测值进行分类时，其他观测值的时间起着重要作用。虽然每个时间间隔内有足够的数据以确保统计可靠性很重要，但谷歌在处理过程中也优先考虑较新的数据点。因此，他们使用考虑事件时间的加权中位数方法。\n\n另一个有助于分类的因素是转向软聚类技术。他们不将每个数据点明确地分配给单个聚类（HOV或非HOV），而是计算每个点属于每个聚类的概率。这对于临界数据点特别有帮助。软聚类在聚合这些分类以对整个行程做出最终判断时也提供了更大的灵活性。\n\n### 最终聚合和分类\n\n一个行程跨越多个路段，谷歌通过聚合每个路段的分类结果来对每个行程进行分类。特别关注落在HOV合格路段内的路段。他们计算行程中可能在HOV车道上花费的比例，并将其作为最终分类的关键因素。\n\n为了进一步完善结果，谷歌实施了**专家混合（MoE）方法**。该框架使用多个分类器，每个分类器都为路段级别分类模型设置了不同的参数。最终的行程分类通过这些分类器之间的多数投票机制确定，从而产生更可靠的结果。\n\n### 评估与结果\n\n为了评估ETA的准确性，谷歌进行了一系列实验，将使用新HOV估算值计算的ETA与传统系统进行比较。他们将行程划分为不同长度的路段，并分析了每个路段的出行时间分布。对于每个路段，他们使用两个正态分布（一个代表普通车道出行，另一个代表HOV车道出行）来模拟行程出行时间的双峰分布。在此基础上，他们计算了每个行程的z分数，以评估其与任一分布的接近程度。这使得他们能够自信地将z分数落在高确定性阈值内的行程标记为HOV或非HOV，然后根据这些高置信度标签评估算法的性能。\n\n**结果显示：**\n*   使用此功能的驾驶员的整体ETA准确性提高了75%，使得HOV用户的准确性指标与不使用HOV车道的驾驶员相当。\n*   最终分类方法比最初仅比较出行速度的方法，ETA准确性提高了18%。\n\n### 结论\n\n通过结合车道位置分析、速度分析和专家混合方法进行行程分类，谷歌开发了一种强大的方法来解决标记HOV数据稀缺的问题。该框架提供了一种解释动态交通状况和解决交通建模关键挑战的新颖方式。除了HOV出行，类似原则还可以扩展到其他表现出类似使用模式的交通方式，例如在两轮车交通量大的地区，这些概念也可能适用于两轮车出行者。\n\n谷歌相信这种方法在推进交通数据分析领域具有巨大潜力，并对增强谷歌地图等实际应用具有实际意义。通过提高HOV车道利用的准确性和效率，该模型可以帮助用户规划更高效的路线，减少出行时间，并为更智能、更环保的通勤做出贡献。",
      "shortSummary": "谷歌地图现已推出HOV（高载客量）专用预计到达时间（ETA）功能，显著提升了拼车用户的导航准确性。该功能通过无监督学习方法开发，结合了速度和横向距离数据，并采用“专家混合”模型进行行程分类。此创新使HOV用户的ETA准确性提高了75%，达到与普通车道用户相当的水平，有助于更高效、更环保的出行规划。",
      "translated_title": "谷歌地图如何实现HOV专用预计到达时间",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/HOV1_RouteOptions.width-1250.png",
          "alt": "HOV1_RouteOptions",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/HOV2_A.width-1250.png",
          "alt": "HOV2_A",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/HOV3_B.width-1250.png",
          "alt": "HOV3_B",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/HOV4_ALatDist.width-1250.png",
          "alt": "HOV4_ALatDist",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/HOV5_BLatDist.width-1250.png",
          "alt": "HOV5_BLatDist",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"n5he5\">The shift to sustainable travel modes like electric vehicles (EVs), carpooling, and public transit, has made travel times more varied. This is largely due to the availability of dedicated lanes, such as carpool lanes, also called high-occupancy vehicle (HOV) lanes, which are reserved for vehicles with multiple passengers and are designed to move traffic more efficiently during peak hours. As a result, HOV lanes are typically faster than general lanes during rush hour. For example, in Utah’s Salt Lake Valley, <a href=\"https://www.library.nd.gov/statedocs/mpc/mpc05-17420091210.pdf?utm_source=chatgpt.com\" target=\"_blank\" rel=\"noopener noreferrer\">the average speed in HOV lanes was recorded</a> at 68.18 mph compared to 58.60 mph in general lanes, a difference of about 16%.</p><p data-block-key=\"dao86\">Accurate estimated time of arrival (ETA) predictions and optimized routing are key to improving the commuting experience. With precise ETAs, travelers can make better decisions, save time, and even contribute to reducing congestion and emissions. With this in mind, Google Maps recently introduced a feature that lets drivers select routes that include HOV lanes and see that route’s ETA. In this blog post, we explain how we developed this feature and developed a classification system to determine HOV trips from non-HOV trips, which led to the launch of HOV specific ETAs in Google Maps.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/HOV1_RouteOptions.width-1250.png\" alt=\"HOV1_RouteOptions\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/HOV1_RouteOptions.width-1250.png\" alt=\"HOV1_RouteOptions\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"dmrad\"><i>HOV-aware route options in Google Maps.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"n5he5\">Developing HOV-specific ETAs</h2><p data-block-key=\"f0rgp\">To estimate HOV travel times, we first infer past HOV travel times by analyzing aggregated and anonymized traffic trends. We then use these inferred times to train our ETA prediction models specifically for HOV lanes.</p><p data-block-key=\"1ot27\">However, identifying HOV trips isn't straightforward. Simple data, like speed, can be similar for both HOV and non-HOV users, especially when traffic is light. Yet HOV travel patterns also have several distinct and useful constraints, including limitations on availability based on location, time of day, and exceptional events.</p><p data-block-key=\"bv3k0\">To address these issues, we develop an <a href=\"https://cloud.google.com/discover/what-is-unsupervised-learning?hl=en\" target=\"_blank\" rel=\"noopener noreferrer\">unsupervised learning approach</a>, performing a classification without initial labels (HOV vs. non-HOV). We perform a classification task of the trip parts lying on individual <a href=\"https://support.google.com/mapcontentpartners/answer/160414?hl=en#:~:text=Use%20a%20segment%2Dbased%20representation,the%20end%20of%20the%20segment).\" target=\"_blank\" rel=\"noopener noreferrer\">road segments</a> that have HOV availability. The overall trip classification is then determined by combining these segment-level classifications.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"7j5a7\">Segment-level classification</h3><p data-block-key=\"1bel8\">For each individual segment, we process a collection of trip parts from different trips that lie on this segment in a short window of time, e.g., 15 minutes. Our goal is to classify these trip parts using the information from both the trip itself and also the other trips that happened in the same window of time. Each trip is composed of several observations, or “trip points”, recorded during the travel time. These points include information such as speed, lateral distance from center of the road, and the time of the observation.</p><p data-block-key=\"b5dds\">The most prominent feature in this classification is the speed information. In fact, our classification task is more valuable when travel times on HOV lanes differ from general lanes. In such scenarios, we often observe a bimodal speed distribution, two distinct traffic patterns emerging at the same time. For example, in the figure below, anonymized aggregated speed data collected between 4:00 to 4:30 pm on Seattle’s I5 shows this clearly, with faster speeds likely corresponding to vehicles using HOV lanes. We call this Scenario A.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/HOV2_A.width-1250.png\" alt=\"HOV2_A\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/HOV2_A.width-1250.png\" alt=\"HOV2_A\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"zx20y\"><i>Scenario A: Distribution of Speed on a segment with HOV availability during peak hours when HOV speeds are much faster than general lanes.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7j5a7\">By analyzing speed data, we can differentiate between HOV and non-HOV travelers when their travel times show a significant difference. However it is possible that there is a noticeable difference between travel times but it is not necessarily significant. For example, the following plot shows the speed distribution for the same segment during peak hours but when the HOV is traveling just a bit faster. We call this Scenario B.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/HOV3_B.width-1250.png\" alt=\"HOV3_B\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/HOV3_B.width-1250.png\" alt=\"HOV3_B\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"zx20y\"><i>Scenario B: Distribution of Speed on a segment with HOV availability during peak hours when HOV speeds are faster than general lanes but it is not significantly faster.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"7j5a7\">Beyond speed: Incorporating estimated lateral distance</h3><p data-block-key=\"5e83l\">While speed is a strong signal, we also explored other factors to improve classification. Estimated lateral distance to the center of the road, though inherently noisy due to the inherent imprecision of GPS, turned out to be useful when combined with speed. In fact, even with some imprecision, distance information helps highlight lane-specific behaviors, especially when distinguishing from adjacent general lanes.</p><p data-block-key=\"bv9ar\">The two figures below illustrate the two speed distributions discussed previously. They depict anonymized rush-hour traffic data in the Seattle metro area on a highway segment with five lanes, where the two HOV lanes are located on the left. Speed and estimated relative distance to the center of the road from this segment are shown, with data points color-coded green and blue to indicate whether they fall within an HOV lane. The first figure highlights data in Scenario A where HOV lanes experience significantly higher speeds compared to general lanes, with average speeds of 65 mph versus 25 mph.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/HOV4_ALatDist.width-1250.png\" alt=\"HOV4_ALatDist\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/HOV4_ALatDist.width-1250.png\" alt=\"HOV4_ALatDist\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"zx20y\"><i>Scenario A —</i> <b><i>Left:</i></b><i> Recorded speed versus lateral distance from the center of the road for different anonymized trips on an individual segment.</i> <b><i>Right:</i></b><i> The distribution of speeds separated by whether the data was located in an HOV lane or not.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7j5a7\">The second figure highlights the same data collected during the time that the the difference in speed is less (Scenario B) with average speeds of 67 mph versus 55 mph.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/HOV5_BLatDist.width-1250.png\" alt=\"HOV5_BLatDist\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/HOV5_BLatDist.width-1250.png\" alt=\"HOV5_BLatDist\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"zx20y\"><i>Scenario B —</i> <b><i>Left:</i></b><i> Recorded speed versus lateral distance from the center of the road for different anonymized trips on an individual segment.</i> <b><i>Right:</i></b><i> The distribution of speeds separated by whether the data was located in an HOV lane or not.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"7j5a7\">Temporal clustering and soft assignments</h3><p data-block-key=\"788tj\">As the figure above suggests, clustering can be an effective method for generating initial labels for these observations. However our approach goes beyond basic clustering by incorporating an additional dimension: time. When classifying a trip observation, the timing of other observations plays a significant role. While it is important to have enough data within each time interval for statistical reliability, we also prioritize more recent data points during processing. Therefore, we use <a href=\"https://en.wikipedia.org/wiki/Weighted_median\" target=\"_blank\" rel=\"noopener noreferrer\">weighted median</a> approaches that account for the timing of events.</p><p data-block-key=\"co5pr\">Another factor that helps our classification is a shift toward <a href=\"https://en.wikipedia.org/wiki/Fuzzy_clustering\" target=\"_blank\" rel=\"noopener noreferrer\">soft clustering</a> techniques. Rather than assigning each data point definitively to a single cluster (HOV or non-HOV), we calculate the probability of each point belonging to each cluster. This is especially helpful for borderline data points. Soft clustering also gives us more flexibility when aggregating these classifications to make a final determination for the entire trip.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"7j5a7\">Final aggregation and classification</h3><p data-block-key=\"5fi8l\">A trip spans multiple road segments, and we classify each trip by aggregating classification outcomes from each segment. Special attention is given to segments that fall within HOV-eligible stretches of the road. We compute the proportion of the trip that was likely spent in HOV lanes and use this as a key factor in the final classification.</p><p data-block-key=\"ci9hq\">To further refine our results, we implement a <a href=\"https://en.wikipedia.org/wiki/Mixture_of_experts\" target=\"_blank\" rel=\"noopener noreferrer\">mixture of experts</a> (MoE) approach. This framework uses multiple classifiers, each with different parameter settings for segment-level classification models. The final trip classification is then determined through a majority voting mechanism across these classifiers, resulting in more reliable outcomes.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"7j5a7\">Evaluation</h2><p data-block-key=\"9ffkm\">To evaluate our ETA accuracy, we conducted a series of experiments comparing the ETA calculated using our new HOV-based estimates with that of our legacy system. We partitioned trips across road stretches of varying lengths and analyzed the distribution of travel times for each stretch.</p><p data-block-key=\"d2r64\">For each stretch, we modeled the bimodal distribution of trips’ travel times using two normal distributions — one representing general lane travel and the other representing HOV-lane travel. Based on this, we computed the <a href=\"https://www.calculator.net/z-score-calculator.html\" target=\"_blank\" rel=\"noopener noreferrer\">z-score</a> for each trip to assess how closely it aligned with either distribution. This allowed us to confidently label trips as either HOV or not when their z-scores fell within high-certainty thresholds. We then evaluated our algorithm’s performance against these high-confidence labels.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"7j5a7\">Results</h2><p data-block-key=\"4vopd\">We now present HOV-specific ETAs. With the launch of this feature, we've improved overall ETA accuracy for drivers using this feature by 75%, making our accuracy metrics for HOV users comparable to those of drivers taking routes without HOV lanes. Our final classification method yielded an 18% improvement in ETA accuracy over the initial method, which only compares travel speeds.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"7j5a7\">Conclusion</h2><p data-block-key=\"8cljh\">By analyzing lane placement with speed analysis and applying a mixture-of-experts approach to trip classification, we developed a powerful method to address the scarcity of labeled HOV data. This framework offers a novel way to interpret dynamic traffic conditions and address key challenges in traffic modeling. Beyond HOV travel, similar principles can be extended to other transportation modes exhibiting similar usage patterns. For instance, in regions with significant two-wheeled traffic, these concepts could also be applicable to two-wheeled travelers.</p><p data-block-key=\"6pahg\">We believe this approach holds strong potential for advancing the field of traffic data analysis and has practical implications for enhancing real-world applications such as Google Maps. By improving the accuracy and efficiency of HOV lane utilization, our model could help users plan more efficient routes, reduce travel times, and contribute to smarter, greener commuting.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"7j5a7\">Acknowledgements</h2><p data-block-key=\"443mg\"><i>These technological advances were enabled by the tireless work of our collaborators in Google Maps: Daniel Delling, Amruta Gulanikar, Cameron Jones, Oliver Lange, Ramesh Namburi, Pooja Patel, Lorenzo Prelli, Stella Stylianidou, and Qian Zheng. Special thanks to Corinna Cortes, Sreenivas Gollapudi, Ravi Kumar, and Andrew Tomkins for their support during this project. We thank Cameron Jones, Sreenivas Gollapudi, and</i> <i>Ravi Kumar for their valuable contribution to this blog post.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "REGEN：用自然语言赋能个性化推荐 (原标题: REGEN: Empowering personalized recommendations with natural language)",
      "link": "https://research.google/blog/regen-empowering-personalized-recommendations-with-natural-language/",
      "pubDate": "Thu, 26 Jun 2025 16:00:00 GMT",
      "isoDate": "2025-06-26T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 引言：推荐系统的演变与挑战\n\n大型语言模型（LLMs）正在重塑推荐系统与用户的交互方式。传统的推荐流程侧重于根据用户过去的互动预测下一个可能喜欢的物品（如书籍、鞋子、办公用品）。然而，真正的目标远不止于此：我们希望系统能够与用户互动，理解他们的需求，通过自然语言反馈进行调整，并解释推荐的理由。目前，尚无现有数据集能够探索这些新能力。\n\n## REGEN数据集：赋能对话式推荐\n\n为了弥补这一空白，我们开发了**REGEN（Reviews Enhanced with GEnerative Narratives）**，这是一个新的基准数据集，它整合了物品推荐、由合成用户评论组成的自然语言特征以及包含购买原因和产品推荐的个性化叙述。REGEN并非从零开始，而是借助Gemini 1.5 Flash，通过合成缺失的对话元素，增强了广泛使用的亚马逊产品评论数据集。该数据集使我们能够探索和评估新的推荐架构，这些架构既能整合用户反馈（如FLARE），也能输出与推荐一致的自然语言（如LUMEN）。我们的结果表明，在REGEN数据集上训练的LLMs能够有效地生成推荐和上下文叙述，其性能可与最先进的推荐系统和语言模型相媲美。\n\n## 构建REGEN：核心组件\n\n现有用于训练对话式推荐系统的数据集往往未能捕捉真实世界对话的细微差别。它们可能侧重于序列物品预测、简短对话片段，或者缺乏明确的用户反馈。我们选择亚马逊产品评论数据集，是因为它特别适用于大型词汇表，这些词汇表可能对LLM来说是陌生的。\n\nREGEN通过两个关键组件丰富了亚马逊评论数据集：\n\n*   **用户评论（Critiques）**\n    *   评论是对话式推荐的一个关键方面，允许用户表达他们的偏好并引导系统。在REGEN中，评论的生成旨在引导推荐系统从当前物品转向一个相似的、期望的物品。例如，用户可能会评论一支“红色圆珠笔”，说“我更喜欢一支黑色的”。\n    *   为确保评论的相关性，我们仅为足够相似的相邻物品对生成评论，使用亚马逊评论提供的分层物品类别作为相似性的代理。Gemini 1.5 Flash模型为每对物品生成了几个评论选项，我们从中随机选择一个包含在数据集中。\n\n*   **叙述（Narratives）**\n    *   叙述提供了关于推荐物品的丰富上下文信息，增强了用户体验。REGEN包含多种叙述，例如：\n        *   **购买原因**：解释为什么某个物品可能适合用户。\n        *   **产品推荐**：描述突出物品的优点和特点。\n        *   **用户摘要**：用户偏好和购买历史的简洁概况。\n    *   这些叙述在上下文和长度上各不相同，为训练对话式推荐系统提供了丰富的数据集。\n\n## 实验设计：联合生成式对话推荐任务\n\n为了有效评估REGEN，我们不仅想测试模型是否能推荐正确的物品，还想看看它们是否能传达其推理过程，适应反馈，并生成符合用户需求的语言。因此，我们构建了一种新型任务：**联合生成式对话推荐**。这个想法简单而强大——给定购买历史，以及可选的自然语言评论（例如，“我需要更多存储空间的东西”），模型必须推荐下一个物品并生成关于它的上下文叙述。\n\n这项任务反映了用户在有机会用自己的语言表达偏好时，与推荐系统自然互动的方式。它也摆脱了分离建模，即推荐和语言生成分别处理。相反，我们将两者视为统一的、端到端目标的一部分。\n\n为了探索不同的建模方法，我们开发并实现了两种基线架构：\n\n*   **混合系统（Hybrid System）**\n    *   其中一个序列推荐器（FLARE）根据协同过滤和内容信号预测下一个物品。然后，该输出被输入到一个轻量级LLM（Gemma 2B），由其负责生成叙述。这种设置反映了生产系统中常见的架构，其中不同的组件专注于管道的不同阶段。\n\n*   **LUMEN（LLM-based Unified Multi-task Model with Critiques, Recommendations, and Narratives）**\n    *   LUMEN在一个单一的LLM内部完成所有任务。它经过端到端训练，以连贯的方式处理评论、生成推荐和产生叙述。在解码过程中，模型决定何时发出物品ID以及何时继续生成自然语言。我们修改了词汇表和嵌入层以支持两种类型的输出——物品标记和文本标记——这使得模型能够将物品推荐视为生成过程的另一个部分。\n\n这种双重方法——混合式与完全生成式——使我们能够衡量模块化和集成之间的权衡，并为衡量模型处理这种更全面的对话任务的能力提供了坚实的基础。\n\n## 实验结果与分析\n\n我们的实验表明，REGEN能够有意义地挑战和区分模型在推荐和生成任务上的表现。在亚马逊产品评论数据集的办公用品领域，我们观察到将用户评论纳入输入始终能提高两种架构的推荐指标。例如，FLARE混合模型在衡量所需物品出现在前10个预测结果中的频率（即Recall@10）的指标上，其已达到最先进的性能（0.124），当在办公用品数据集中包含评论时，该性能提高到0.1402，这是一个显著的提升，强调了语言引导细化的价值。\n\nLUMEN的性能具有竞争力，尽管在传统推荐指标上略低。考虑到在一次通过中联合生成物品和叙述的难度增加，这并不令人意外。然而，其真正的优势在于它能够保持物品和所生成文本之间的连贯性。与模块化管道不同，模块化管道中组件之间的脱节可能导致尴尬或通用的解释，LUMEN的叙述往往更自然地与用户的历史和评论上下文保持一致。\n\n在生成方面，我们使用BLEU、ROUGE和语义相似度评估了输出。混合模型在BLEU和ROUGE上通常得分更高，特别是对于产品推荐和购买原因，这可能是因为LLM被提供了正确的物品作为提示。相比之下，LUMEN的n-gram重叠略低，但保持了强大的语义对齐，特别是对于更多依赖长期用户行为而非特定物品的用户摘要（详见论文）。\n\n![FLARE-3a-Benchmarks](https://storage.googleapis.com/gweb-research2023-media/images/FLARE-3a-Benchmarks.width-1250.png)\n\n这些结果突出了一些有趣的动态。主要依赖于用户的叙述（如用户偏好摘要）对两种模型来说都更容易持续生成。但是当叙述与物品上下文紧密耦合时（如产品推荐），性能更多地取决于推荐的准确性。如果模型推荐了错误的物品，它可能会破坏整个叙述。这种影响在LUMEN中更为明显，因为物品和叙述是共同生成的，这使得它成为端到端对齐的更严格测试。\n\n我们还在更大的物品空间（服装领域，拥有超过370,000个独特物品，比其他任何产品类别大5-60倍）上评估了性能。据我们所知，没有其他人在如此大的服装数据集上进行评估，这是FLARE和REGEN的一个关键区别。即使在这种更复杂的设置中，混合系统也表现良好，并且在包含评论时，Recall@10再次出现明显提升（从0.1264到0.1355），验证了REGEN作为奖励细致、用户引导推理的基准设计。\n\n![FLARE-Recommendation](https://storage.googleapis.com/gweb-research2023-media/images/FLARE-Recommendation.width-1250.png)\n\n## 结论与展望\n\nREGEN提供了一个包含一致用户偏好、推荐和生成叙述的数据集，从而能够研究LLM在对话式推荐中的能力。我们使用LUMEN（一个基于LLM的联合推荐和叙述生成模型）以及序列推荐模型评估了REGEN，展示了其效用。我们相信REGEN是研究对话式推荐模型能力的基础资源，是迈向个性化多轮系统的关键一步。\n\nREGEN通过将语言作为基本元素整合，推进了对话式推荐，增强了推荐器解释和响应用户偏好的方式。这种方法促进了对多轮交互的研究，其中系统可以进行扩展对话，根据不断变化的用户反馈来完善推荐。\n\n该数据集还鼓励开发更复杂的模型和训练方法。它支持探索模型容量的扩展、利用先进的训练技术以及将方法适应于亚马逊评论之外的不同领域，例如旅行、教育和音乐。\n\n最终，REGEN为推荐系统设定了新的方向，强调理解和交互，为更直观、支持性和人性化的推荐体验铺平了道路。",
      "shortSummary": "REGEN是一个新的基准数据集，旨在推动基于大型语言模型（LLMs）的个性化对话式推荐系统。它通过合成用户评论和个性化叙述来增强现有亚马逊产品评论数据集，使模型能够理解自然语言反馈并生成解释性内容。实验表明，在REGEN上训练的LLMs（如LUMEN和混合模型）能有效生成推荐和上下文叙述，性能与现有最先进系统相当，尤其在整合用户反馈和保持推荐与解释连贯性方面表现出色。REGEN为开发更智能、更具交互性的推荐系统提供了关键资源。",
      "translated_title": "REGEN：用自然语言赋能个性化推荐",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/FLARE-3a-Benchmarks.width-1250.png",
          "alt": "FLARE-3a-Benchmarks",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/FLARE-Recommendation.width-1250.png",
          "alt": "FLARE-Recommendation",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7fyz5\">Large language models (LLMs) are reshaping how recommender systems interact with users. Traditional recommendation pipelines focus on predicting the next item a user might like — books, shoes, office supplies, etc. — based on past interactions. But the real goal goes further: we want systems that interact with users, understand their needs, adapt through natural language feedback, and explain why a recommendation makes sense. However, no datasets currently exist to explore these new capabilities.</p><p data-block-key=\"5h38j\">To address this gap we developed <a href=\"https://arxiv.org/pdf/2503.11924\" target=\"_blank\" rel=\"noopener noreferrer\">Reviews Enhanced with GEnerative Narratives</a> (REGEN), a new <a href=\"https://www.kaggle.com/datasets/googleai/regen-reviews-enhanced-with-generative-narratives\" target=\"_blank\" rel=\"noopener noreferrer\">benchmark dataset</a> that incorporates item recommendations, natural language features composed of synthetic user critiques, and personalized narratives comprising purchase reasons and product endorsements. Rather than start from scratch, we augmented the widely-used <a href=\"https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Product Reviews dataset</a> by synthesizing missing conversational elements with the help of <a href=\"https://ai.google.dev/gemini-api/docs/models#gemini-1.5-flash\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini 1.5 Flash</a>. This dataset allows us to explore and benchmark new recommender architectures that incorporate both user feedback (e.g., <a href=\"https://arxiv.org/pdf/2409.11699\" target=\"_blank\" rel=\"noopener noreferrer\">FLARE</a>) as well those that output natural language consistent with the recommendations (e.g., <a href=\"https://arxiv.org/pdf/2503.11924\" target=\"_blank\" rel=\"noopener noreferrer\">LUMEN</a>). Our results show that LLMs trained on our dataset effectively generate both recommendations and contextual narratives, achieving performance comparable to state-of-the-art recommenders and language models.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Building the REGEN dataset</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7fyz5\">Existing datasets for training conversational recommenders often fall short in capturing the nuances of real-world conversations. They may focus on sequential item prediction, short dialog snippets, or lack explicit user feedback. We chose the Amazon Product Reviews dataset because of its specific utility for large vocabularies, potentially unfamiliar to an LLM.</p><p data-block-key=\"bdcfu\">REGEN enriches the Amazon Reviews dataset with two key components:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Critiques</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7fyz5\">Critiques are a crucial aspect of conversational recommendation, allowing users to express their preferences and guide the system. In REGEN, critiques are generated to steer the recommender from a current item to a similar, desired item. For example, a user might critique a \"red ball-point pen\" by saying, \"I'd prefer a black one\".</p><p data-block-key=\"9cejc\">To ensure the relevance of critiques, we generate them only for adjacent item pairs that are sufficiently similar, using the Amazon Reviews–provided hierarchical item categories as a proxy for similarity. The Gemini 1.5 Flash model generates several critique options for each pair, from which we select one at random to include in the dataset.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Narratives</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7fyz5\">Narratives provide rich contextual information about recommended items, enhancing the user experience. REGEN includes diverse narratives, such as:</p><ul><li data-block-key=\"53ip4\"><i>Purchase reasons</i>: Explanations for why an item might be suitable for a user.</li><li data-block-key=\"80ko6\"><i>Product endorsements</i>: Descriptions highlighting the benefits and features of an item.</li><li data-block-key=\"50do9\"><i>User summaries</i>: Concise profiles of user preferences and purchase history.</li></ul><p data-block-key=\"5gbcr\">These narratives vary in contextualization and length, providing a rich dataset for training conversational recommenders.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Experiments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7fyz5\">To evaluate REGEN effectively, we didn’t just want to test if models could recommend the right item, we wanted to see if they could communicate their reasoning, adapt to feedback, and generate language that feels tailored to the user. So we framed a new kind of task: conversational recommendation that’s jointly generative. The idea is simple but powerful — given a provided purchase history, and optionally a natural language critique (e.g., “I need something with more storage”), a model must recommend the next item <i>and</i> generate a contextual narrative about it.</p><p data-block-key=\"53f82\">This task reflects how users naturally interact with recommendation systems when given the opportunity to express preferences in their own words. It also moves away from disjointed modeling, where recommendation and language generation are handled separately. Instead, we treat both as part of a unified, end-to-end objective.</p><p data-block-key=\"a1bi9\">To explore different modeling approaches, we developed and implemented two baseline architectures. The first is a hybrid system, where a sequential recommender (FLARE) predicts the next item based on collaborative filtering and content signals. That output is then fed into a lightweight LLM (<a href=\"https://huggingface.co/google/gemma-2b\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma 2B</a>), which is responsible for generating the narrative. This setup reflects a common architecture in production systems, where different components specialize in different stages of the pipeline.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n            <div class=\"glue-ambient-video \">\n                <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/FLARE-1-FLARE.mp4\" type=\"video/mp4\">\n                </video>\n\n                <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n                    <div class=\"glue-ambient-video__tooltip\">\n                    <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                    <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                    </div>\n                    <div class=\"glue-ambient-video__icon\">\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                    </div>\n                </div>\n            </div>\n        \n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7fyz5\">The second architecture is LUMEN (LLM-based Unified Multi-task Model with Critiques, Recommendations, and Narratives). LUMEN does everything inside a single LLM. It’s trained end-to-end to handle critiques, generate recommendations, and produce narratives in a coherent way. During decoding, the model decides when to emit an item ID and when to continue generating natural language. We modified the vocabulary and embedding layers to support both types of outputs — item tokens and text tokens — which allowed the model to treat item recommendation as just another part of the generative process.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n            <div class=\"glue-ambient-video \">\n                <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/FLARE-2b-LUMEN.mp4\" type=\"video/mp4\">\n                </video>\n\n                <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n                    <div class=\"glue-ambient-video__tooltip\">\n                    <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                    <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                    </div>\n                    <div class=\"glue-ambient-video__icon\">\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                    </div>\n                </div>\n            </div>\n        \n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7fyz5\">This dual approach — hybrid versus fully generative — lets us benchmark the trade-offs between modularity and integration, and provides a solid foundation for measuring how well models can tackle this more holistic conversational task.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7fyz5\">Our experiments show that REGEN can meaningfully challenge and differentiate models across both recommendation and generation tasks. In the Amazon Product Reviews dataset's Office domain, we observed that incorporating user critiques into the input consistently improved recommendation metrics across both architectures. For example, the FLARE hybrid model’s already state-of-the-art performance (0.124) on a metric which measures how often a desired item appears in the top 10 predicted results (known as Recall@10) increased to 0.1402 when critiques were included in the Office dataset, a notable bump that underscores the value of language-guided refinement.</p><p data-block-key=\"6nrlm\">LUMEN’s performance was competitive, albeit slightly lower on traditional recommendation metrics. That’s not surprising, given the increased difficulty of generating the item and narrative jointly in a single pass. However, its real strength lies in its ability to maintain coherence between the item and the text it produces. Unlike modular pipelines, where disconnects between components can lead to awkward or generic explanations, LUMEN’s narratives tend to align more naturally with the user’s history and critique context.</p><p data-block-key=\"4qjcj\">On the generation side, we evaluated outputs using <a href=\"https://en.wikipedia.org/wiki/BLEU\" target=\"_blank\" rel=\"noopener noreferrer\">BLEU</a>, <a href=\"https://en.wikipedia.org/wiki/ROUGE_(metric)\" target=\"_blank\" rel=\"noopener noreferrer\">ROUGE</a>, and <a href=\"https://en.wikipedia.org/wiki/Semantic_similarity\" target=\"_blank\" rel=\"noopener noreferrer\">semantic similarity</a>. The hybrid model generally scored higher on BLEU and ROUGE, especially for product endorsements and purchase reasons, likely because the LLM was given the correct item as a prompt. LUMEN, by contrast, had slightly lower <a href=\"https://en.wikipedia.org/wiki/N-gram\" target=\"_blank\" rel=\"noopener noreferrer\"><i>n</i>-gram</a> overlap but maintained strong semantic alignment, particularly for user summaries that relied more on long-term user behavior than on the specific item (see <a href=\"https://arxiv.org/pdf/2503.11924\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a> for details).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/FLARE-3a-Benchmarks.width-1250.png\" alt=\"FLARE-3a-Benchmarks\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/FLARE-3a-Benchmarks.width-1250.png\" alt=\"FLARE-3a-Benchmarks\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7fyz5\">These results highlight a few interesting dynamics. Narratives that depend primarily on the user, like summaries of their preferences, are easier for both models to generate consistently. But when the narrative is tightly coupled with the item context, like a product endorsement, performance hinges more on recommendation accuracy. If the model recommends the wrong item, it can throw off the entire narrative. This effect is more pronounced in LUMEN, where both the item and the narrative are co-generated, making it a stricter test of end-to-end alignment.</p><p data-block-key=\"dse3\">We also evaluated performance on a much larger item space using the Clothing domain, which has over 370,000 unique items (5x–60x larger than any other product category). No one else we’re aware of performs evaluations on this much larger Clothing dataset, a key distinction of FLARE and REGEN. Even in this more complex setting, the hybrid system held up well, and again we saw clear gains in Recall@10 from 0.1264 to 0.1355 when critiques were included, validating the design of REGEN as a benchmark that rewards nuanced, user-guided reasoning.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/FLARE-Recommendation.width-1250.png\" alt=\"FLARE-Recommendation\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/FLARE-Recommendation.width-1250.png\" alt=\"FLARE-Recommendation\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7fyz5\">REGEN provides a dataset with consistent user preferences, recommendations, and generated narratives, enabling the study of LLM capabilities in conversational recommendation. We evaluated REGEN using LUMEN, an LLM-based model for joint recommendation and narrative generation, demonstrating its utility, along with sequential recommender models. We believe REGEN serves as a fundamental resource for studying the capabilities of conversational recommender models, a crucial step towards personalized multi-turn systems.</p><p data-block-key=\"5n641\">REGEN advances conversational recommendation by integrating language as a fundamental element, enhancing how recommenders interpret and respond to user preferences. This approach fosters research into multi-turn interactions, where systems can engage in extended dialogues to refine recommendations based on evolving user feedback.</p><p data-block-key=\"b1ehv\">The dataset also encourages the development of more sophisticated models and training methodologies. It supports exploration into scaling model capacity, utilizing advanced training techniques, and adapting the methodology across different domains beyond Amazon reviews, such as travel, education, and music.</p><p data-block-key=\"ffahk\">Ultimately, REGEN sets a new direction for recommender systems, emphasizing comprehension and interaction, which paves the way for more intuitive, supportive, and human-like recommendation experiences.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7fyz5\"><i>We would like to thank our co-authors of the FLARE and REGEN papers without whom this work would not be possible: Liam Hebert from University of Waterloo and Kun Su, James Pine, Marialena Kyriakidi, Yuri Vasilevski, Raghavendra Vasudeva, Ambarish Jash, Sukhdeep Sodhi, Anushya Subbiah, from Google Research. Additionally, we are grateful for the support and guidance of our leadership Vikram Aggarwal, John Anderson, Dima Kuzmin, Emil Praun and Sarvjeet Singh. We also are grateful to Kimberly Schwede, Mark Simborg and the Google Research Blog editorial staff for helping us present our work to a larger audience. Finally we appreciate the authors of “</i><a href=\"https://cseweb.ucsd.edu/~jmcauley/pdfs/emnlp19a.pdf\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Justifying Recommendations Using Distantly-Labeled Reviews and Fine-Grained Aspects</i></a><i>” for releasing the Amazon Product Reviews dataset used in our work.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "MUVERA：使多向量检索像单向量搜索一样快 (原标题: MUVERA: Making multi-vector retrieval as fast as single-vector search)",
      "link": "https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/",
      "pubDate": "Tue, 24 Jun 2025 16:00:00 GMT",
      "isoDate": "2025-06-24T16:00:00.000Z",
      "creator": "Google",
      "summary": "## MUVERA：实现多向量检索的单向量速度\n\n### 引言：信息检索与嵌入模型\n\n现代信息检索（IR）的核心是神经嵌入模型。这些模型将每个数据点转换为一个“单向量嵌入”，使得语义相似的数据点被转换为数学上相似的向量，并通过最大内积搜索（MIPS）算法实现高效检索。然而，多向量模型（如ColBERT）的出现显著提升了IR任务的性能。与单向量不同，多向量模型使用一组嵌入来表示每个数据点，并利用更复杂的相似性函数（如Chamfer相似性）来捕捉数据点之间更丰富的关系。尽管这种方法提高了准确性，但由于嵌入数量的增加和相似性评分的复杂性，它带来了巨大的计算挑战，使得检索成本显著提高。\n\n### MUVERA：弥合效率鸿沟的解决方案\n\n在论文《MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encodings》中，我们提出了一种新颖的多向量检索算法MUVERA，旨在弥合单向量和多向量检索之间的效率差距。MUVERA通过构建查询和文档的**固定维度编码（FDEs）**，将复杂的多向量检索问题简化为单向量MIPS。FDEs是单向量，其内积可以近似多向量相似性。这种方法允许我们利用高度优化的MIPS算法来检索初始候选集，然后使用精确的多向量相似性进行重新排序，从而在不牺牲准确性的前提下实现高效的多向量检索。我们已在GitHub上提供了FDE构建算法的开源实现。\n\n### 多向量检索的挑战\n\n多向量模型通常为每个查询或文档生成多个嵌入（通常是每个词元一个）。查询和文档之间的相似性通常使用Chamfer匹配来计算，它衡量每个查询嵌入与最接近的文档嵌入之间的最大相似性，然后将这些相似性累加起来。Chamfer相似性提供了一种“整体”衡量，反映了查询的每个部分如何与文档的某个部分相关联。\n\n尽管多向量表示提供了改进的可解释性和泛化能力，但它们带来了显著的检索挑战：\n\n*   **嵌入量增加**：为每个词元生成嵌入会大大增加需要处理的嵌入数量。\n*   **复杂且计算密集型相似性评分**：Chamfer匹配是一种非线性操作，需要进行矩阵乘法，比单向量点积更昂贵。\n*   **缺乏高效的次线性搜索方法**：单向量检索受益于高度优化的算法（例如基于空间划分的算法），这些算法可以同时实现高准确性和次线性搜索时间。多向量相似性的复杂性阻碍了这些快速几何技术的直接应用，从而阻碍了大规模高效检索。\n\n传统的单向量MIPS算法无法直接应用于多向量检索，因为一个文档可能与单个查询词元具有高相似性，但整体上文档可能并不相关。这使得需要更复杂和计算密集型的检索方法。\n\n### MUVERA：固定维度编码的解决方案\n\nMUVERA通过将多向量相似性搜索简化为单向量MIPS，从而显著加快了复杂多向量数据的检索速度。MUVERA的核心思想是将整个多向量组压缩成一个更易于处理的单向量，即固定维度编码（FDE）。关键在于，比较这些简化的FDEs所得到的结果，与比较原始更复杂的多向量集所得到的结果非常接近。这使得我们可以使用为单向量设计的更快搜索方法。\n\nMUVERA的工作原理简化如下：\n\n1.  **FDE生成**：MUVERA采用映射将查询和文档的多向量集转换为FDEs。这些映射旨在以固定长度的向量捕获基本的相似性信息。\n2.  **基于MIPS的检索**：文档的FDEs使用标准的MIPS求解器进行索引。给定一个查询，计算其FDE，然后MIPS求解器高效地检索最相似的文档FDEs。\n3.  **重新排序**：通过MIPS检索到的初始候选集使用原始的Chamfer相似性进行重新排序，以提高准确性。\n\nMUVERA的一个关键优势是FDE转换是**数据无关**的。这意味着它不依赖于特定的数据集，使其对数据分布的变化具有鲁棒性，并适用于流式应用。此外，与模型生成的单向量不同，FDEs保证在指定误差范围内近似真实的Chamfer相似性。因此，在重新排序阶段之后，MUVERA保证能找到最相似的多向量表示。\n\n### 理论基础\n\nMUVERA的方法灵感来源于概率树嵌入技术，但我们将其应用于内积和Chamfer相似性。FDE生成的核心思想是将嵌入空间划分为多个区域。如果查询和文档中的相似向量落入同一区域，我们就可以有效地近似它们的相似性。由于我们事先不知道查询和文档向量之间的最佳匹配，我们使用随机分区方案。MUVERA还提供了理论保证，证明FDEs能强近似Chamfer相似性，这为使用单向量代理进行多向量检索提供了一种有原则的方法，并具有可证明的准确性。\n\n### 实验结果\n\n我们在BEIR基准测试中的多个信息检索数据集上评估了MUVERA。实验表明，与现有最先进的方法PLAID相比，MUVERA在显著降低延迟的同时，始终保持高检索准确性。\n\n主要发现包括：\n\n*   **召回率提高**：MUVERA优于单向量启发式方法（PLAID也采用的常见方法），在检索显著更少候选文档的情况下实现了更好的召回率。例如，FDEs只需检索5-20倍的候选文档即可达到固定的召回率。\n\n    ![MUVERA3_Recall](https://storage.googleapis.com/gweb-research2023-media/images/MUVERA3_Recall.width-1250.png)\n    不同维度的固定维度编码（FDE）与单向量启发式（SV）的召回率对比。请注意，10240维的FDEs与原始MV表示（用于SV启发式）具有几乎相同的表示大小，但在搜索中所需的比较次数显著减少（即使对于20k维的FDEs也是如此）。\n\n*   **延迟降低**：与高度优化的多向量检索系统PLAID相比，MUVERA在BEIR数据集上平均实现了10%更高的召回率，同时延迟显著降低了90%。\n\n    ![MUVERA5_ResultsFinal](https://storage.googleapis.com/gweb-research2023-media/images/MUVERA5_ResultsFinal.width-1250.png)\n    MUVERA与PLAID在BEIR基准测试上的表现。\n\n此外，我们发现MUVERA的FDEs可以使用乘积量化进行有效压缩，将内存占用减少32倍，而对检索质量的影响最小。这些结果突显了MUVERA显著加速多向量检索的潜力，使其在实际应用中更具可行性。\n\n### 结论\n\n我们提出了MUVERA，一种新颖高效的多向量检索算法，具有可证明的近似质量保证和良好的实际性能。通过将多向量搜索简化为单向量MIPS，MUVERA利用现有优化搜索技术，以显著提高的效率实现了最先进的性能。感兴趣的读者可以在GitHub上找到我们FDE构建算法的开源实现。\n\n我们的工作为高效多向量检索开辟了新途径，这对于搜索引擎、推荐系统和自然语言处理等各种应用至关重要。我们相信，对MUVERA的进一步研究和优化将带来更大的性能提升和多向量检索技术的更广泛采用。",
      "shortSummary": "MUVERA是一种创新的多向量检索算法，旨在解决多向量模型（如ColBERT）在提高准确性同时带来的计算开销。它通过将多向量数据转换为固定维度编码（FDEs），将复杂的多向量检索问题简化为高效的单向量MIPS。MUVERA利用MIPS进行快速初步检索，再通过精确的多向量相似性进行重新排序，从而在不牺牲准确性的前提下显著降低检索延迟（比PLAID降低90%）并提高召回率。该方法数据无关且具有理论保证，为实际应用中的高效多向量检索提供了可行方案。",
      "translated_title": "MUVERA：使多向量检索像单向量搜索一样快",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MUVERA3_Recall.width-1250.png",
          "alt": "MUVERA3_Recall",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MUVERA5_ResultsFinal.width-1250.png",
          "alt": "MUVERA5_ResultsFinal",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"0xkr2\">Neural <a href=\"https://developers.google.com/machine-learning/crash-course/embeddings\" target=\"_blank\" rel=\"noopener noreferrer\">embedding models</a> have become a cornerstone of modern <a href=\"https://en.wikipedia.org/wiki/Information_retrieval\" target=\"_blank\" rel=\"noopener noreferrer\">information retrieval</a> (IR). Given a query from a user (e.g., “How tall is Mt Everest?”), the goal of IR is to find information relevant to the query from a very large collection of data (e.g., the billions of documents, images, or videos on the Web). Embedding models transform each datapoint into a single-vector “embedding”, such that <i>semantically</i> similar datapoints are transformed into <i>mathematically</i> similar vectors. The embeddings are generally compared via the <a href=\"https://en.wikipedia.org/wiki/Inner_product_space#Euclidean_vector_space\" target=\"_blank\" rel=\"noopener noreferrer\">inner-product similarity</a>, enabling efficient retrieval through optimized <a href=\"https://en.wikipedia.org/wiki/Maximum_inner-product_search\" target=\"_blank\" rel=\"noopener noreferrer\">maximum inner product search</a> (MIPS) algorithms. However, recent advances, particularly the introduction of multi-vector models like <a href=\"https://www.answer.ai/posts/colbert-pooling.html\" target=\"_blank\" rel=\"noopener noreferrer\">ColBERT</a>, have demonstrated significantly improved performance in IR tasks.</p><p data-block-key=\"ef8b9\">Unlike single-vector embeddings, multi-vector models represent each data point with a <i>set</i> of embeddings, and leverage more sophisticated similarity functions that can capture richer relationships between datapoints. For example, the popular <a href=\"https://www.sciencedirect.com/topics/engineering/chamfer-matching\" target=\"_blank\" rel=\"noopener noreferrer\">Chamfer similarity measure</a> used in state-of-the-art multi-vector models captures when the information in one multi-vector embedding is contained within another multi-vector embedding. While this multi-vector approach boosts accuracy and enables retrieving more relevant documents, it introduces substantial computational challenges. In particular, the increased number of embeddings and the complexity of multi-vector similarity scoring make retrieval significantly more expensive.</p><p data-block-key=\"bkmgl\">In “<a href=\"https://arxiv.org/abs/2405.19504\" target=\"_blank\" rel=\"noopener noreferrer\">MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encodings</a>”, we introduce a novel multi-vector retrieval algorithm designed to bridge the efficiency gap between single- and multi-vector retrieval. We transform multi-vector retrieval into a simpler problem by constructing fixed dimensional encodings (FDEs) of queries and documents, which are single vectors whose inner product approximates multi-vector similarity, thus reducing complex multi-vector retrieval back to single-vector maximum inner product search (MIPS). This new approach allows us to leverage the highly-optimized MIPS algorithms to retrieve an initial set of candidates that can then be re-ranked with the exact multi-vector similarity, thereby enabling efficient multi-vector retrieval without sacrificing accuracy. We have provided an open-source implementation of our FDE construction algorithm on <a href=\"https://github.com/google/graph-mining/tree/main/sketching/point_cloud\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"0xkr2\">The challenge of multi-vector retrieval</h2><p data-block-key=\"6ktb8\">Multi-vector models generate multiple embeddings per query or document, often one embedding per token. One typically calculates the similarity between a query and a document using Chamfer matching, which measures the maximum similarity between each query embedding and the closest <a href=\"https://github.com/Yeema/ecommerce-review-score-classification/blob/master/resources/docs/TUTORIAL_5_DOCUMENT_EMBEDDINGS.md\" target=\"_blank\" rel=\"noopener noreferrer\">document embedding</a>, and then adds these similarities up across all query vectors (the standard method of computing multi-vector similarity). The Chamfer similarity, therefore, provides a \"holistic\" measure of how each part of the query relates to some part of the document.</p><p data-block-key=\"5s9if\">While multi-vector representations offer advantages like improved interpretability and generalization, they pose significant retrieval challenges:</p><ul><li data-block-key=\"1kcio\"><i>Increased embedding volume</i>: Generating embeddings per token drastically increases the number of embeddings to be processed.</li><li data-block-key=\"50lte\"><i>Complex and compute-intensive similarity scoring</i>: Chamfer matching is a non-linear operation requiring a matrix product, which is more expensive than a single vector <a href=\"https://en.wikipedia.org/wiki/Dot_product\" target=\"_blank\" rel=\"noopener noreferrer\">dot-product</a>.</li><li data-block-key=\"5mtsc\"><i>Lack of efficient sublinear search methods</i>: Single-vector retrieval benefits from highly optimized algorithms (e.g., based on <a href=\"https://en.wikipedia.org/wiki/Space_partitioning\" target=\"_blank\" rel=\"noopener noreferrer\">space partitioning</a>) that simultaneously achieve high accuracy and sublinear search times, avoiding exhaustive comparisons. The complex nature of multi-vector similarity prevents the direct application of these fast geometric techniques, hindering efficient retrieval at scale.</li></ul><p data-block-key=\"fsa1h\">Unfortunately, traditional single-vector MIPS algorithms cannot be directly applied to multi-vector retrieval — for example, a document might have a token with high similarity to a single query token, but overall, the document might not be very relevant. This problem necessitates more complex and computationally intensive retrieval methods.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"0xkr2\">MUVERA: A solution with fixed dimensional encodings</h2><p data-block-key=\"67rm1\">MUVERA offers an elegant solution by reducing multi-vector similarity search to single-vector MIPS to make retrieval over complex multi-vector data much faster. Imagine you have a large dataset of \"multi-vector sets\" (i.e., sets of vectors) where each set describes some datapoint, but searching through each of these sets is slow. MUVERA's trick is to take that whole group of multi-vectors and squeeze them into a single, easier-to-handle vector that we call a <i>fixed dimensional encoding</i> (FDE). A key part is that if you compare these simplified FDEs, their comparison closely matches what you'd get if you compared the original, more complex multi-vector sets. This lets us use much quicker search methods designed for single vectors.</p><p data-block-key=\"919ud\">Here's a simplified breakdown of how MUVERA works:</p><ol><li data-block-key=\"2be5p\"><i>FDE generation</i>: MUVERA employs mappings to convert query and document multi-vector sets into FDEs. These mappings are designed to capture the essential similarity information in a fixed-length vector.</li><li data-block-key=\"2t4u9\"><i>MIPS-based retrieval</i>: The FDEs of documents are indexed using a standard MIPS solver. Given a query, its FDE is computed, and the MIPS solver efficiently retrieves the most similar document FDEs.</li><li data-block-key=\"6taf6\"><i>Re-ranking</i>: The initial candidates retrieved by MIPS are re-ranked using the original Chamfer similarity for improved accuracy.</li></ol><p data-block-key=\"ed9lp\">A key advantage of MUVERA is that the FDE transformation is data-oblivious. This means it doesn't depend on the specific dataset, making it both robust to changes in data distribution and suitable for streaming applications. Additionally, unlike single-vectors produced by a model, FDE’s are guaranteed to approximate the true Chamfer similarity to within a specified error. Thus, after the re-ranking stage, MUVERA is guaranteed to find the most similar multi-vector representations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n            <div class=\"glue-ambient-video \">\n                <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/MUVERA1_Query.mp4\" type=\"video/mp4\">\n                </video>\n\n                <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n                    <div class=\"glue-ambient-video__tooltip\">\n                    <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                    <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                    </div>\n                    <div class=\"glue-ambient-video__icon\">\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                    </div>\n                </div>\n            </div>\n        \n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"6ie81\"><i>Illustration of the construction of query FDE's. Each token (shown as a word in this example) is mapped to a high-dimensional vector (2-D in the example for simplicity). The high-dimensional space is randomly partitioned by hyperplane cuts. Each piece of space is assigned a block of coordinates in the output FDE, which is set to the sum of the coordinates of the query vectors that land in that piece.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n            <div class=\"glue-ambient-video \">\n                <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/MUVERA2_Document.mp4\" type=\"video/mp4\">\n                </video>\n\n                <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n                    <div class=\"glue-ambient-video__tooltip\">\n                    <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                    <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                    </div>\n                    <div class=\"glue-ambient-video__icon\">\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                    </div>\n                </div>\n            </div>\n        \n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"6ie81\"><i>Illustration of the construction of document FDE's. The construction is the same as the query construction, except that the vectors falling in a given piece of the partitioned space are averaged together instead of summed, which accurately captures the asymmetric nature of the Chamfer similarity.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"0xkr2\">Theoretical foundations</h2><p data-block-key=\"d79eo\">Our approach is inspired by techniques used in <a href=\"https://arxiv.org/abs/2111.03528\" target=\"_blank\" rel=\"noopener noreferrer\">probabilistic tree embeddings</a>, a powerful tool in the theory of geometric algorithms. However, we adapt these techniques to work with inner products and Chamfer similarity.</p><p data-block-key=\"ebt62\">The core idea behind FDE generation is to partition the embedding space into sections (illustrated in the figure above). If similar vectors from a query and a document fall into the same section, we can approximate their similarity efficiently. However, since we don't know the optimal matching between query and document vectors beforehand, we use a randomized partitioning scheme.</p><p data-block-key=\"4jl3a\">We also provide theoretical guarantees for MUVERA, proving that FDEs offer a strong approximation of Chamfer similarity (you can read more in <a href=\"https://arxiv.org/abs/2405.19504\" target=\"_blank\" rel=\"noopener noreferrer\">the paper</a>). This is a significant result, as it provides a principled way to perform multi-vector retrieval using single-vector proxies with provable accuracy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"0xkr2\">Experimental results</h2><p data-block-key=\"9aa8i\">We evaluated MUVERA on several information retrieval datasets from the <a href=\"https://arxiv.org/abs/2104.08663\" target=\"_blank\" rel=\"noopener noreferrer\">BEIR</a> benchmarks. Our experiments demonstrate that MUVERA consistently achieves high retrieval accuracy with significantly reduced latency compared to the previous state-of-the-art method known as <a href=\"https://arxiv.org/pdf/2205.09707\" target=\"_blank\" rel=\"noopener noreferrer\">PLAID</a>.</p><p data-block-key=\"4h9g9\">Our key findings include:</p><p data-block-key=\"ji43\"><i>Improved recall:</i> MUVERA outperforms the single-vector heuristic, a common approach used in multi-vector retrieval (which PLAID also employs), achieving better recall while retrieving significantly fewer candidate documents (shown in the figure below). For instance, FDE’s retrieve 5–20x fewer candidates to achieve a fixed recall.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MUVERA3_Recall.width-1250.png\" alt=\"MUVERA3_Recall\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MUVERA3_Recall.width-1250.png\" alt=\"MUVERA3_Recall\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"6ie81\"><i>Recall of fixed dimensional encodings (FDE) of varying dimensions vs. a single-vector heuristic (SV). Note 10240-dimensional FDE’s have nearly the same representation size as the original MV representation (used in SV heuristic), while requiring significantly fewer comparisons in the search (true even for 20k-dimensional FDE’s).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"0xkr2\"><i>Reduced latency:</i> Compared to PLAID, a highly optimized multi-vector retrieval system based on the single-vector heuristic, MUVERA achieves an average of 10% higher recall with a remarkable 90% reduction in latency across the BEIR datasets (shown in the figure below).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MUVERA5_ResultsFinal.width-1250.png\" alt=\"MUVERA5_ResultsFinal\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MUVERA5_ResultsFinal.width-1250.png\" alt=\"MUVERA5_ResultsFinal\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"6ie81\"><i>MUVERA vs. PLAID over BEIR benchmarks.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"0xkr2\">Moreover, we found that MUVERA's FDEs can be <i>effectively compressed</i> using product quantization, reducing memory footprint by 32x with minimal impact on retrieval quality.</p><p data-block-key=\"avedq\">These results highlight MUVERA's potential to significantly accelerate multi-vector retrieval, making it more practical for real-world applications.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"0xkr2\">Conclusion</h2><p data-block-key=\"f5c08\">We have presented MUVERA, a novel and efficient multi-vector retrieval algorithm with provable guarantees on its approximation quality and good practical performance. By reducing multi-vector search to single-vector MIPS, MUVERA leverages existing optimized search techniques and achieves state-of-the-art performance with significantly improved efficiency. Interested readers can find an open-source implementation of our FDE construction algorithm on <a href=\"https://github.com/google/graph-mining/tree/main/sketching/point_cloud\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a>.</p><p data-block-key=\"dnb18\">Our work opens up new avenues for efficient multi-vector retrieval, which is crucial for various applications, including search engines, recommendation systems, and natural language processing. We believe that further research and optimization of MUVERA will lead to even greater performance gains and broader adoption of multi-vector retrieval techniques.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"0xkr2\">Acknowledgements</h2><p data-block-key=\"6n887\"><i>The work summarized in this blog post was done in collaboration with Majid Hadian, Jason Lee, and Vahab Mirrokni. Lastly, we thank Kimberly Schwede for their valuable help with making the animation in this blog post.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "从研究到气候韧性 (原标题: From research to climate resilience)",
      "link": "https://research.google/blog/how-ai-is-helping-us-build-a-more-resilient-planet/",
      "pubDate": "Mon, 23 Jun 2025 16:00:00 GMT",
      "isoDate": "2025-06-23T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 从研究到气候韧性：谷歌AI助力应对气候危机\n\n谷歌研究部门致力于探索“可能性艺术”，并将突破性研究转化为现实世界的影响。目前，谷歌正利用先进的AI技术和创新，增强气候韧性，应对野火、洪水、极端天气和气旋等紧迫的气候危机，旨在提供及时、可靠的威胁预测，帮助人们保持安全并建设社区韧性。\n\n![AIforthePlanet-0b-Hero](https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-0b-Hero.width-1250.png)\n\n## AI驱动的全球洪水预报\n\n*   **突破性进展**：几年前，准确可靠的洪水预测被认为是“不可能的挑战”。如今，谷歌研究部门开创性的全球水文AI模型（已发表在《自然》杂志）能够提前长达七天准确预测全球河流洪水。\n*   **覆盖范围**：预测结果可在谷歌的Flood Hub平台获取，覆盖全球100多个国家超过7亿人口，赋能政府和当地社区保护生命和生计。\n*   **数据扩展**：通过API和Flood Hub上的专家数据层，为研究人员和专家提供更广泛的覆盖。AI被用于分析历史数据并创建“虚拟水位计”，以弥补数据稀缺地区（覆盖150个国家）的不足。\n*   **全球合作**：与世界气象组织（WMO）以及捷克共和国、尼日利亚、乌拉圭和越南等国的气象部门合作，共同推动洪水预报的全球规模化。\n\n## 提高气旋预报的提前期和准确性\n\n*   **挑战与影响**：气旋（台风或飓风）在过去50年造成了1.4万亿美元的经济损失，提前预警至关重要，但预测其路径、强度和影响非常困难。\n*   **AI应用**：谷歌DeepMind和谷歌研究团队正在探索AI潜力，以改进气旋预测和准备工作。目前可提前15天生成多达50种可能情景，预测气旋的存在、路径、强度、大小和结构。\n*   **Weather Lab**：谷歌推出了交互式网站Weather Lab，分享正在进行的研究，并向专家和公众提供最新、最准确的天气模型。\n*   **合作伙伴**：与美国国家飓风中心（NHC）合作，NHC将在大西洋飓风季节利用这些实验模型，以期改进预报并提供更早、更准确的预警。\n\n![AIforthePlanet-2-Hurricanes](https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-2-Hurricanes.width-1250.png)\n\n## 短期预报（Nowcasting）：提升实时天气信息的可及性\n\n*   **解决痛点**：AI被应用于改进日常天气预报，尤其解决了非洲等传统基础设施有限地区可靠预报稀缺的挑战。\n*   **技术细节**：生成超本地、短期天气预测（即“短期预报”），提供全球降水预测，分辨率为5公里，每15分钟更新一次，可提前12小时。\n*   **核心模型**：这项变革性能力源于谷歌最先进的AI神经天气模型MetNet-3。\n*   **全球推广**：谷歌研究团队利用全球可用的卫星观测数据，通过谷歌搜索直接向非洲用户提供AI驱动的天气预报，弥补了地面雷达等传统基础设施的不足。\n*   **经济效益**：短期预报和其他天气模型可帮助农民应对不断变化的天气条件，潜在提高产量、减少浪费、降低运营成本并增强经济韧性。\n*   **合作研究**：与利兹大学和英国气象局的专家合作，探索模型在各种应用中的潜力，推动气象科学发展。\n\n![AIforthePlanet-3a-Nowcasting](https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-3a-Nowcasting.width-1250.png)\n\n## 利用AI理解和减轻日益增长的野火威胁\n\n*   **实时监测**：多年来，谷歌利用卫星图像和AI近乎实时地检测野火边界，并将关键信息分享给急救人员和受影响社区（已在27个国家/地区的谷歌搜索和地图上提供）。\n*   **FireSat卫星星座**：为根本性改变野火检测方式，谷歌推出了FireSat专用卫星星座。\n    *   首颗FireSat卫星已于3月发射，能检测小至5x5米教室大小的火灾（远超现有技术只能发现足球场大小火灾的能力）。\n    *   由50颗卫星组成的星座将每20分钟提供一次全球高分辨率图像，为消防部门提供快速行动的时间。\n    *   FireSat的全球视角也将帮助科学家更好地研究火灾蔓延。\n*   **多方合作**：该项目是谷歌研究、Google.org与地球火灾联盟、Muon Space、摩尔基金会以及野火管理机构等多方合作的成果。\n\n## 地理空间推理：开创性的地球洞察\n\n*   **新前沿**：除了预测和预报，谷歌正通过“地理空间推理”开创新的前沿，利用AI从地理空间数据中获取地球洞察。\n*   **框架功能**：地理空间推理框架将地球模型与生成式AI结合，加速地理空间问题解决。它允许用户以自然语言提问（例如，在自然灾害前应优先疏散哪些脆弱社区），并获得基于可靠地理空间数据的全面答案和可视化。\n*   **赋能专家**：通过谷歌云平台使复杂的行星分析更易于访问，赋能专家和开发者将谷歌先进的地理空间地球模型与其数据整合，解锁强大洞察。\n*   **行业应用**：自4月推出以来，已引起公共卫生、气候韧性、商业应用（如保险）等广泛领域的兴趣。\n*   **合作伙伴**：与Maxar、Sust-Global、Airbus、Planet Labs和Carto等合作伙伴合作，将潜力转化为实际解决方案。\n\n## 利用AI减少交通排放和改善空气质量\n\n*   **航空业**：AI正在使航空运输更可持续。\n    *   **凝结尾迹**：凝结尾迹占航空气候影响的三分之一。与美国航空等合作伙伴的早期实际演示表明，AI驱动的预测帮助飞行员将凝结尾迹减少了54%。\n    *   **Contrails API**：谷歌正通过Contrails API向航空业提供这些AI预测，并与EUROCONTROL等合作伙伴合作将其整合到飞行计划系统中。\n    *   **非营利组织**：Google.org正提供300万美元，与Breakthrough Energy共同资助一个新的非营利组织contrails.org，专注于将研究转化为航空业的实际工具。\n\n![AIforthePlanet-6-Contrails](https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-6-Contrails.width-1250.png)\n\n*   **地面交通**：与城市合作改善交通流量。\n    *   **Project Green Light**：利用AI和谷歌地图驾驶趋势数据，调整交通信号灯配时，以减少车辆燃油排放并改善空气质量。\n    *   **成效**：该项目已证明可将交叉路口停车次数减少高达30%，交叉路口排放量平均减少10%以上。\n\n## 结论\n\n谷歌研究部门在过去几年取得的突破性研究已对自然灾害防备产生了深远影响。这仅仅是可能性的开始。谷歌坚信，推进AI和科学研究将在提供及时、可靠的全球预测和建设更好的气候韧性方面发挥关键作用。",
      "shortSummary": "谷歌研究正利用AI和技术应对气候危机，构建全球气候韧性。其工作涵盖：通过AI模型实现提前7天的全球洪水预报；利用AI改进气旋路径和强度预测；通过MetNet-3模型提供超本地实时天气信息（短期预报）；利用卫星AI检测和FireSat星座提升野火预警能力；以及通过地理空间推理框架提供地球洞察。此外，AI还应用于减少航空凝结尾迹和优化城市交通信号，以降低排放、改善空气质量。这些努力旨在提供及时可靠的预测，帮助全球社区应对自然灾害。",
      "translated_title": "从研究到气候韧性",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-0b-Hero.width-1250.png",
          "alt": "AIforthePlanet-0b-Hero",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-2-Hurricanes.width-1250.png",
          "alt": "AIforthePlanet-2-Hurricanes",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-3a-Nowcasting.width-1250.png",
          "alt": "AIforthePlanet-3a-Nowcasting",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-6-Contrails.width-1250.png",
          "alt": "AIforthePlanet-6-Contrails",
          "title": "",
          "position": 7
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7b5wd\">At Google Research, we're driven by exploring the art of the possible. Our research impacts products, businesses, scientific discovery, and society. Today, the opportunity to help solve seemingly impossible problems with breakthrough research is greater than ever, as is the opportunity to translate this research into real-world impact.</p><p data-block-key=\"a9ohf\">I'm excited to share how we're advancing research and harnessing tech innovation to help build more resilience, tackling the urgent challenges of climate crises, such as wildfires, floods, extreme weather, and cyclones. Can we deliver timely, reliable predictions of these threats, helping people stay safe and communities build resilience?</p><p data-block-key=\"6ak23\">Our work with AI over the past years, advancing climate science and tackling these difficult problems, is already making a tangible difference. It clearly demonstrates AI’s potential to build towards better climate resilience.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-0b-Hero.width-1250.png\" alt=\"AIforthePlanet-0b-Hero\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-0b-Hero.width-1250.png\" alt=\"AIforthePlanet-0b-Hero\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">From impossible to global scale: AI-powered flood forecasting</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7b5wd\">A few years ago, providing accurate, reliable flood predictions was widely considered an impossible challenge. Today, Google Research’s <a href=\"https://blog.google/technology/ai/google-ai-global-flood-forecasting/\" target=\"_blank\" rel=\"noopener noreferrer\">groundbreaking</a> global hydrological AI model, <a href=\"https://www.nature.com/articles/s41586-024-07145-1\" target=\"_blank\" rel=\"noopener noreferrer\">published in <i>Nature</i></a>, enables us to accurately <a href=\"https://sites.research.google/gr/floodforecasting/\">forecast riverine floods</a> around the world up to seven days in advance. Our forecasts are available on Google’s <a href=\"https://sites.research.google/floods/l/0/0/3\">Flood Hub</a> platform and cover over 700 million people across more than 100 countries worldwide, empowering governments and local communities to protect lives and livelihoods.</p><p data-block-key=\"ftgqe\">We’re also providing researchers and experts with <a href=\"https://blog.google/technology/ai/expanding-flood-forecasting-coverage-helping-partners/\" target=\"_blank\" rel=\"noopener noreferrer\">expanded coverage</a> via an API as well as an expert data layer on Flood Hub. We’ve employed AI to analyze historical data and create “<a href=\"https://research.google/blog/a-flood-forecasting-ai-model-trained-and-evaluated-globally/\">virtual gauges</a>” for locations where data is scarce and physical gauges are not available. This significantly expands our coverage to data-scarce regions across 150 countries, addressing the need, raised by our partners, for greater information in these regions, which are particularly vulnerable. We’re also working with the <a href=\"https://wmo.int/media/news/wmo-embraces-private-sector-and-academia-ai\" target=\"_blank\" rel=\"noopener noreferrer\">WMO</a> and meteorological services in other countries, including the Czech Republic, Nigeria, Uruguay, and Vietnam, on efforts to scale flood forecasting globally, further enhancing community resilience worldwide.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"iZmltPAFhY0\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=iZmltPAFhY0\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Improving the lead time and accuracy of cyclone forecasts</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7b5wd\">Cyclones — also known as typhoons or hurricanes — devastate communities and endanger lives. In the last 50 years, they have caused <a href=\"https://wmo.int/topics/tropical-cyclone\" target=\"_blank\" rel=\"noopener noreferrer\">1.4 trillion dollars in economic losses</a>. Advanced warning could make a tremendous difference, but it is difficult to predict whether oceanic storms will become dangerous cyclones, where they might make landfall, and what the impact will be. For decades, scientists have relied on supercomputers to simulate the laws of physics to estimate the future path, intensity and size of these storms. Now Google DeepMind and Google Research teams are exploring AI’s potential to improve how we <a href=\"https://deepmind.google/discover/blog/weather-lab-cyclone-predictions-with-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">predict</a> and prepare for cyclones. We can currently predict existence, track, intensity, size, and structure, generating up to 50 possible scenarios as far as 15 days in advance. It’s still early days and we're working with experts around the world, including partners in academia, government agencies, and non-profit organizations, to refine and expand the impact of our work.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-2-Hurricanes.width-1250.png\" alt=\"AIforthePlanet-2-Hurricanes\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-2-Hurricanes.width-1250.png\" alt=\"AIforthePlanet-2-Hurricanes\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7b5wd\">Earlier this month we announced <a href=\"https://deepmind.google.com/science/weatherlab\" target=\"_blank\" rel=\"noopener noreferrer\">Weather Lab</a>, an interactive website where we are sharing ongoing research, making our newest and most accurate weather models available to experts and the public. We also announced a <a href=\"https://wmo.int/media/news/wmo-embraces-private-sector-and-academia-ai\" target=\"_blank\" rel=\"noopener noreferrer\">partnership</a> with the US <a href=\"https://www.nhc.noaa.gov/\" target=\"_blank\" rel=\"noopener noreferrer\">National Hurricane Center</a>, who will be leveraging these experimental models this summer throughout the Atlantic Hurricane Season. We hope this data can help improve NHC forecasts and provide earlier and more accurate warnings for hazards linked to tropical cyclones.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Nowcasting: Improving access to reliable, real-time weather information</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7b5wd\">We’re applying AI to improve forecasts for everyday weather, too. This addresses an acute challenge in regions with limited traditional infrastructure, including across Africa, where reliable forecasting is often scarce. We’re generating hyper-local, short-term weather predictions — known as <a href=\"https://blog.google/intl/en-africa/products/explore-get-answers/nowcasting-on-search-is-bringing-ai-powered-weather-forecasts-to-users-across-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">nowcasting</a> — and can now make available these global precipitation predictions with a 5km resolution, updated every 15 minutes, up to 12 hours ahead.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-3a-Nowcasting.width-1250.png\" alt=\"AIforthePlanet-3a-Nowcasting\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-3a-Nowcasting.width-1250.png\" alt=\"AIforthePlanet-3a-Nowcasting\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7vhvz\">This transformative capability stems from <a href=\"https://research.google/blog/metnet-3-a-state-of-the-art-neural-weather-model-available-in-google-products/\">MetNet-3</a>, our state-of-the-art AI neural weather model. To scale nowcasting globally, Google Research teams are leveraging globally available satellite observations in our models, allowing us to bring the power of AI-driven weather forecasts directly to people across Africa via Google Search. This innovative approach overcomes the critical gap where traditional weather forecasting infrastructure, like ground-based radar, is scarce. For users across Africa, this translates to more reliable, real-time weather information that improves their daily lives. The agricultural community tells us that nowcasting and other weather models can help farmers react to changing conditions, potentially improving yields, reducing waste, lowering operating costs, and enhancing economic resilience.</p><p data-block-key=\"t7d0\">We are collaborating with leading experts, including meteorologists at the University of Leeds and the <a href=\"https://www.metoffice.gov.uk/\" target=\"_blank\" rel=\"noopener noreferrer\">UK Met office</a>, exploring how they can use our models for various applications. Such partnerships with local scientific communities are crucial not only to continue to refine the accuracy of our forecasts, but also to advance the science of meteorology and improve weather predictions for everyone, everywhere.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Leveraging AI to understand and mitigate the growing threat of wildfires</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7b5wd\"><a href=\"https://sites.research.google/gr/wildfires/\">Wildfires</a> are increasing in frequency and intensity, putting lives, homes, and ecosystems at risk. For years we have been leveraging satellite imagery and AI to detect the boundaries of wildfires in near real time. We share this vital information with first responders and affected communities, enabling quicker and more effective responses. <a href=\"https://blog.google/outreach-initiatives/sustainability/google-wildfire-boundary-maps-europe-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">Wildfire boundary</a> information is available on Google Search and Maps in 27 countries.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"pmQlXLaHT_Y\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=pmQlXLaHT_Y\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7b5wd\">During this ongoing work, we recognized the need for more accurate information on earlier, smaller fires that can rapidly escalate. We set out to fundamentally change how we detect wildfires with a dedicated satellite constellation, <a href=\"https://sites.research.google/gr/wildfires/firesat/\">FireSat</a>. In March, the <a href=\"https://blog.google/feed/firesat-first-satellite-launch/\" target=\"_blank\" rel=\"noopener noreferrer\">first FireSat satellite</a> was launched, representing a major leap forward in our ability to mitigate the threat of wildfires worldwide. FireSat consists of satellites that can detect and track wildfires as small as a 5x5 meter classroom, a significant improvement over current technology that can only spot fires the size of a football field. A constellation of 50 satellites will provide high-resolution imagery updated globally every 20 minutes, giving fire authorities time for quick action. Additionally, FireSat’s unprecedented global view of fires will allow scientists to better study fire propagation. This leap is the result of a partnership between Google Research and Google.org with the <a href=\"https://www.earthfirealliance.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Earth Fire Alliance</a>, <a href=\"https://www.muonspace.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Muon Space</a>, the <a href=\"https://www.moore.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Moore Foundation</a>, wildfire authorities, and many others.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Geospatial Reasoning: Pioneering planetary insights</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7b5wd\">Beyond prediction and forecasting, we’re pioneering new frontiers by enabling insights about our planet with <a href=\"https://research.google/blog/geospatial-reasoning-unlocking-insights-with-generative-ai-and-multiple-foundation-models/\">Geospatial Reasoning</a>. Our long-standing investment in applying AI to geospatial data is evident in our extensive portfolio of work — from AI models that power weather forecasts and help predict floods and wildfires to remote sensing and analyzing population dynamics. The Geospatial Reasoning framework brings Earth models together with generative AI to accelerate geospatial problem solving. It enables people to ask questions in natural language about, for example, which vulnerable communities should be evacuated first before a natural disaster and to receive comprehensive answers and visualizations that are grounded in robust geospatial data. This can help provide critical insights to governments, local agencies and businesses, creating new opportunities for building community resilience.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"g9F-_tCakL8\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=g9F-_tCakL8\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7b5wd\">We’re removing cost and expertise barriers by making sophisticated planetary analysis on <a href=\"https://cloud.google.com/gcp\" target=\"_blank\" rel=\"noopener noreferrer\">Google Cloud Platform</a> more accessible, and empowering experts and developers to integrate Google’s advanced geospatial Earth models with their data to unlock powerful insights. Since the launch of Geospatial Reasoning in April, we've seen interest from a wide range of sectors eager to harness its potential, from public health to climate resilience to commercial applications, such as insurance. We are now collaborating with partners like <a href=\"https://www.maxar.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Maxar</a>, <a href=\"https://www.sustglobal.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Sust-Global</a>, <a href=\"https://www.airbus.com/en\" target=\"_blank\" rel=\"noopener noreferrer\">Airbus</a>, <a href=\"https://www.planet.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Planet Labs</a> and <a href=\"https://carto.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Carto</a> to translate the potential into tangible solutions.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Using AI to reduce transport-related emissions and improve air quality</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7b5wd\">AI is paving the way for making air and ground transportation more sustainable, with reduced carbon emission and improved air quality in our cities. One third of aviation’s climate impact are the result of <a href=\"https://sites.research.google/contrails/\">contrails</a>, the condensation trails sometimes seen behind airplanes. In early real-world demonstrations with partners like American Airlines, our AI-powered forecasts helped pilots reduce contrails by 54%. We’re now making these AI-powered forecasts available across the aviation industry through our <a href=\"https://developers.google.com/contrails\" target=\"_blank\" rel=\"noopener noreferrer\">Contrails API</a> and working with partners like <a href=\"https://www.eurocontrol.int/\" target=\"_blank\" rel=\"noopener noreferrer\">EUROCONTROL</a> to integrate the forecasts into their flight planning systems. <a href=\"http://google.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Google.org</a> is providing 3 million dollars to co-catalyze a new nonprofit with <a href=\"https://www.breakthroughenergy.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Breakthrough Energy</a>, called <a href=\"http://contrails.org/\" target=\"_blank\" rel=\"noopener noreferrer\">contrails.org</a>, which will focus on translating research into tangible tools for aviation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-6-Contrails.width-1250.png\" alt=\"AIforthePlanet-6-Contrails\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-6-Contrails.width-1250.png\" alt=\"AIforthePlanet-6-Contrails\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7b5wd\">For ground transportation, we’re teaming up with cities to improve traffic flow. <a href=\"https://sites.research.google/gr/greenlight/\">Project Green Light</a> uses AI and <a href=\"https://blog.google/outreach-initiatives/sustainability/google-ai-project-greenlight/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Maps driving trends</a> to propose adjustment to traffic light timing in order to reduce vehicle gas emissions, also resulting in improved air quality. Green Light has <a href=\"https://blog.google/outreach-initiatives/sustainability/project-green-light-boston-expansion/\" target=\"_blank\" rel=\"noopener noreferrer\">demonstrated</a> potential to reduce stops at intersections by up to 30% and reduce emissions at intersections by an average of over 10%.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7b5wd\">Our breakthrough research over the past years is already having a profound impact on natural disaster preparedness. Yet, this is just the beginning of what’s possible. We are confident that advancing AI and scientific research can play a key role in addressing the difficult problems of timely, reliable global predictions, working towards better climate resilience. I am incredibly optimistic that collectively we’ll ultimately get closer to a point where no one is surprised by a natural disaster coming their way.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "通过多模态AI与M-REGLE解锁丰富的遗传学见解 (原标题: Unlocking rich genetic insights through multimodal AI with M-REGLE)",
      "link": "https://research.google/blog/unlocking-rich-genetic-insights-through-multimodal-ai-with-m-regle/",
      "pubDate": "Sun, 22 Jun 2025 16:00:00 GMT",
      "isoDate": "2025-06-22T16:00:00.000Z",
      "creator": "Google",
      "summary": "# M-REGLE：通过多模态AI解锁丰富的遗传学见解\n\n## 引言：多模态健康数据的重要性\n\n当前，从尖端医疗技术到智能手表，各种设备正以前所未有的规模生成数据。电子健康记录、医学影像、诊断测试、基因组数据乃至智能手表的实时测量数据汇聚成海量信息，供研究人员和临床医生分析。这些多样化的数据流，即使在同一器官系统内，也常携带着独特且重叠的信号。例如，在心血管系统中，心电图（ECG）测量心脏的电活动，而光电容积描记图（PPG，智能手表常见）则追踪血容量变化。对这些模态进行协同分析，可以同时评估心脏的电系统及其泵血效率，从而提供更全面的心脏健康图景。将这些生理特征与来自大型国家级生物样本库的遗传信息相结合，有助于识别疾病的遗传基础。\n\n## 背景：从单模态到多模态的需求\n\n我们早期的工作REGLE在利用健康数据进行遗传发现方面取得了成功，但它被设计用于单一数据类型（即单模态设置）。另一种方法是分别分析每种模态，然后尝试将结果拼凑起来（我们称之为U-REGLE或单模态REGLE），但这可能不是最有效的方式。U-REGLE可能会错过不同模态之间微妙的共享信息。因此，我们假设联合建模这些互补数据流将增强重要的生物信号，减少噪声，并带来更强大的遗传发现。\n\n## M-REGLE的提出与优势\n\n我们近期在《美国人类遗传学杂志》上发表了论文“利用多模态AI改进心血管性状的遗传分析”，其中介绍了我们开发的多模态版REGLE，名为M-REGLE。M-REGLE允许同时分析多种类型的临床数据。与前身U-REGLE相比，M-REGLE产生更低的重建误差，识别出更多的遗传关联，并在预测心脏疾病方面优于风险评分。\n\n## M-REGLE工作原理\n\n### 核心理念\n\nM-REGLE的核心前提是，不同的临床模态，特别是那些与单一器官系统（如循环系统）相关的模态，编码着互补和重叠的信息。例如，在12导联心电图中，不同的导联放置在身体的不同位置。为了确定心脏病发作的位置或诊断心律失常，医生会分析特定导联的信息。M-REGLE的方法是在表征学习过程之前结合多种模态（如12导联ECG或一个导联加上PPG数据），提供了一种更准确的工具，在发现遗传关联、分析复杂生理数据和预测疾病方面表现更优。\n\n### 多步骤方法：联合学习\n\n为了有效地实现这一点，M-REGLE采用了一种稳健的多步骤联合学习方法：\n\n1.  **数据融合**：M-REGLE首先将多种模态（如12个不同的ECG导联或ECG与PPG波形）结合起来，而不是单独查看它们。\n2.  **特征学习**：它随后使用卷积变分自编码器（CVAE）从这些多数据流中学习一个压缩的、组合的“签名”（潜在因子）。CVAE旨在以较低维度、大部分不相关的表示形式捕获最基本的信息。它由编码器和解码器网络组成，编码器将ECG和PPG波形压缩为潜在因子，解码器网络则从创建的潜在因子重建波形。\n3.  **独立性确保**：为了确保学习到的因子真正独立，对这些CVAE生成的签名应用主成分分析（PCA）。\n4.  **遗传关联**：最后，通过全基因组关联研究（GWAS）找到计算出的独立因子与遗传数据之间的关联（显著相关性）。\n5.  **结果整合**：对这些个体GWAS的结果进行统计学组合，以精确定位与潜在生理系统相关的基因变异。\n\n![MREGLE-1-Overview](https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-1-Overview.width-1250.png)\n\n## M-REGLE的性能提升\n\n### 更好的学习表示\n\nM-REGLE在U-REGLE的基础上实现了进步，能够持续生成更好的数据“学习表示”。医学数据（如ECG）包含数百个独立数据点。在分析多种医学模态时，M-REGLE不是单独处理每种模态，而是捕获最重要的特征并将其浓缩为“潜在因子”。这种方法显著降低了重建误差，并且比单独从每种模态学习更好地捕获了原始波形的基本信息。对于12导联ECG，M-REGLE将重建误差降低了72.5%。\n\n### 可解释性洞察\n\n生成式AI的优势之一是其可解释性能力。在我们的研究中，我们使用M-REGLE嵌入来展示这些嵌入与ECG和PPG波形之间的联系，特别是改变单个嵌入坐标如何改变M-REGLE解码器重建的ECG和PPG波形。\n\n我们专注于识别能够最好地区分房颤（AFib）样本和非房颤样本的坐标。M-REGLE在位置4、6和10的嵌入被发现最具区分性。当我们改变第4个M-REGLE嵌入的值（从[-2, 2]）同时保持其余M-REGLE嵌入固定时，我们观察到重建的ECG导联I和PPG发生相应变化：ECG导联I的T波段幅度发生变化，PPG信号的双波切迹出现微小改变。双波切迹提供有关心血管功能和健康的宝贵信息。例如，不那么突出或缺失的双波切迹通常与动脉僵硬度增加相关。\n\n![MREGLE-2-EmbeddingEffect](https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-2-EmbeddingEffect.width-800.gif)\n\n### 增强的遗传学发现\n\nM-REGLE在识别心血管疾病的遗传关联方面也比U-REGLE有所改进。对于12导联ECG，M-REGLE比单模态方法多识别了19.3%的关联遗传位点（基因组区域）。对于ECG导联I + PPG，M-REGLE多发现了13.0%的位点。重要的是，这些发现的绝大多数（12导联ECG的35个中有24个，ECG导联I + PPG的12个中有11个）复制了GWAS目录中报告的ECG或PPG性状的已知遗传关联。M-REGLE还发现了几个以前未与这些性状关联的新位点，其中一些在其他数据库中显示出与心血管性状的联系。\n\n![MREGLE-3-Discoveries](https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-3-Discoveries.width-1250.png)\n\n### 改进的多基因风险评分\n\n多基因风险评分（PRS）量化个体患某种疾病的遗传风险。我们发现，使用M-REGLE（来自12导联ECG数据）识别的遗传变异开发的PRS在预测心脏疾病（最显著的是房颤AFib）方面显著优于U-REGLE。M-REGLE的PRS在识别高风险个体方面表现显著更好。这些AFib的PRS改进不仅在英国生物样本库中观察到，还在其他大型数据集（如印第安纳生物样本库、EPIC-Norfolk和英国女性心脏健康研究）中得到了独立验证。\n\n![MREGLE-4-Comparison](https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-4-Comparison.width-1250.png)\n\n## M-REGLE为何有效？\n\nM-REGLE的强大之处在于其处理信息的方式。通过从一开始就考虑多种模态，M-REGLE获得了三大优势：\n\n1.  **高效捕获共享信息**：它只需学习一次共享信息，而不是在每种模态中重复学习。\n2.  **增强独特和互补信号**：它能提升每种模态提供的独特和互补信号。\n3.  **减少噪声**：一种模态的信息可能有助于澄清或过滤掉另一种模态中的噪声。\n\n所有这些都导致了更清晰、更稳健的信号，从而实现强大的下游遗传分析。\n\n## 未来展望\n\n这项研究是利用日益丰富的多模态健康数据向前迈出的一步。M-REGLE提供了一种发现复杂疾病新遗传联系、提高疾病风险预测能力以及潜在识别新治疗靶点的方法。此外，随着智能可穿戴设备持续收集ECG和PPG等生理数据，M-REGLE等方法对于将健康数据转化为洞察力并最终改善健康结果至关重要。",
      "shortSummary": "M-REGLE是一种新型多模态AI模型，用于遗传分析。它通过联合分析心电图（ECG）和光电容积描记图（PPG）等多种临床数据，克服了传统单模态方法的局限性。M-REGLE能更有效地捕获共享信息、增强独特信号并减少噪声，从而生成更好的数据表示、识别更多与心血管疾病相关的遗传位点，并显著提高房颤等疾病的多基因风险评分预测准确性。这项研究为利用丰富的健康数据解锁遗传学见解、改善疾病预测和开发新疗法铺平了道路。",
      "translated_title": "通过多模态AI与M-REGLE解锁丰富的遗传学见解",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-1-Overview.width-1250.png",
          "alt": "MREGLE-1-Overview",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-2-EmbeddingEffect.width-800.gif",
          "alt": "MREGLE-2-EmbeddingEffect",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-3-Discoveries.width-1250.png",
          "alt": "MREGLE-3-Discoveries",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-4-Comparison.width-1250.png",
          "alt": "MREGLE-4-Comparison",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"8t3jr\">Everything from medical specialists with cutting-edge technology to simple smartwatches are generating data on an unprecedented scale. The aggregation of electronic health records, medical imaging, diagnostic tests, genomic data, and even real-time measurements from smartwatches creates a wealth of data for researchers and clinicians to analyze. These diverse data streams often carry unique and overlapping signals, even within the same organ system.</p><p data-block-key=\"13r16\">In the cardiovascular system, for example, an <a href=\"https://en.wikipedia.org/wiki/Electrocardiography\" target=\"_blank\" rel=\"noopener noreferrer\">electrocardiogram</a> (ECG) measures the heart's electrical activity, while a <a href=\"https://en.wikipedia.org/wiki/Photoplethysmogram\" target=\"_blank\" rel=\"noopener noreferrer\">photoplethysmogram</a> (PPG) — common in <a href=\"https://blog.google/products/fitbit/irregular-heart-rhythm-notifications/\" target=\"_blank\" rel=\"noopener noreferrer\">smartwatches</a> — tracks blood volume changes. The co-analysis of these modalities can simultaneously assess both the heart’s electrical system and its pumping efficiency, thus providing a more complete picture of heart health. Integrating these physiological signatures with genetic information from large nation-level biobanks could enable the identification of the genetic underpinnings of disease.</p><p data-block-key=\"5e6f4\">Our earlier work, <a href=\"https://research.google/blog/harnessing-hidden-genetic-information-in-clinical-data-with-regle/\">REGLE</a>, was successful for genetic discovery using health data, but it was designed for a single data type (i.e., the unimodal setting). Alternatively, analyzing each modality separately and then trying to piece together the findings later (what we refer to as <a href=\"https://www.nature.com/articles/s41588-024-01831-6\" target=\"_blank\" rel=\"noopener noreferrer\">U-REGLE</a> or Unimodal REGLE) also might not be the most efficient way. U-REGLE could miss subtle shared information between different modalities. Instead, we hypothesized that <i>jointly</i> modeling these complementary data streams would boost the important biological signals, reduce noise, and lead to more powerful genetic discoveries.</p><p data-block-key=\"1ggvs\">Here we present our recent paper, “<a href=\"https://www.cell.com/ajhg/fulltext/S0002-9297(25)00231-9\" target=\"_blank\" rel=\"noopener noreferrer\">Utilizing multimodal AI to improve genetic analyses of cardiovascular traits</a>”, which we published in the <a href=\"https://www.cell.com/ajhg/home\" target=\"_blank\" rel=\"noopener noreferrer\"><i>American Journal of Human Genetics</i></a>. We developed a multimodal version of REGLE, called M-REGLE, that allows the analysis of multiple types of clinical data together at once. M-REGLE produces lower reconstruction error, identifies more genetic associations, and outperforms risk scores in predicting cardiac disease compared to its predecessor, U-REGLE.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-1-Overview.width-1250.png\" alt=\"MREGLE-1-Overview\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-1-Overview.width-1250.png\" alt=\"MREGLE-1-Overview\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"nconb\"><i>M-REGLE overview steps compared to running our previous model REGLE on each modality separately (U-REGLE, or Unimodal REGLE).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The challenge: Seeing the whole picture</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"8t3jr\">The central premise of M-REGLE is that different clinical modalities, especially those pertaining to a single organ system (like the circulatory system), encode both complementary and overlapping information. In a 12-lead ECG, for example, the different leads are placed in distinct locations on the body. To determine the location of a heart attack, or diagnose arrhythmias, physicians analyze information from specific leads. M-REGLE’s approach, which combines multiple modalities (like the 12 leads of the ECG or one lead plus PPG data) before the representation learning process, offers a more accurate tool that is superior at finding genetic associations, analyzing complex physiological data, and predicting disease.</p><p data-block-key=\"bcdfm\">To effectively do this, M-REGLE employs a robust, multi-step approach that uses joint learning. Instead of looking at 12 different ECG leads or an ECG and a PPG waveform separately, M-REGLE first combines them. It then uses a <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\" target=\"_blank\" rel=\"noopener noreferrer\">convolutional</a> <a href=\"https://en.wikipedia.org/wiki/Variational_autoencoder\" target=\"_blank\" rel=\"noopener noreferrer\">variational autoencoder</a> (CVAE) to learn a compressed, combined \"signature\" (latent factors) from these multiple data streams. The CVAE is designed to capture the most essential information in a lower-dimensional, largely uncorrelated representation. It consists of encoder and decoder networks where the encoder compresses the ECG and PPG waveforms to latent factors and the decoder network reconstructs the waveforms from the created latent factors. To ensure the learned factors are truly independent, <a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\" target=\"_blank\" rel=\"noopener noreferrer\">principal component analysis</a> (PCA) is applied to these CVAE-generated signatures. Finally, we find associations (significant correlation) between computed independent factors and genetic data via <a href=\"https://en.wikipedia.org/wiki/Genome-wide_association_study\" target=\"_blank\" rel=\"noopener noreferrer\">genome-wide association studies</a> (GWAS). The results from these individual GWAS are statistically combined to pinpoint genetic variations associated with the underlying physiological system.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Better learned representations</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"8t3jr\">M-REGLE advances U-REGLE to consistently produce better \"learned representations\" of the data. Medical data, like an ECG, consists of hundreds of individual data points. When analyzing multiple medical modalities, instead of processing the modalities individually, M-REGLE captures the most important characteristics and condenses it into “latent factors.” This approach resulted in significantly lower reconstruction errors and did a better job of capturing the essential information from the original waveforms compared to learning from each modality separately. For 12-lead ECGs, M-REGLE reduced reconstruction error by 72.5%.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Interpretability sheds some light on embeddings</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"8t3jr\">One of the advantages of generative AI is its interpretability capability. In our study, we used M-REGLE embeddings to show the connection between these embeddings and ECG and PPG waveforms, specifically how altering individual embedding coordinates changes the reconstructed ECG and PPG waveforms from the M-REGLE decoder.</p><p data-block-key=\"aaj2k\">We focused on identifying coordinates that would best distinguish between samples with and without atrial fibrillation (AFib). M-REGLE embeddings at position 4, 6, and 10 were found to be the most distinctive. When we changed values in the 4th M-REGLE embedding from [-2, 2] while keeping the rest of M-REGLE embeddings fixed, we observed corresponding changes in the reconstructed ECG lead I and PPG: the <a href=\"https://en.wikipedia.org/wiki/T_wave\" target=\"_blank\" rel=\"noopener noreferrer\">T-wave</a> segment of ECG lead I changed in magnitude, and the dicrotic notch of the PPG signal showed a small alteration. The dicrotic notch provides valuable information about cardiovascular function and health. For example, a less prominent or absent dicrotic notch is often associated with increased arterial stiffness.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-2-EmbeddingEffect.width-800.gif\" alt=\"MREGLE-2-EmbeddingEffect\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-2-EmbeddingEffect.width-800.gif\" alt=\"MREGLE-2-EmbeddingEffect\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"nconb\"><i>The effect of varying the 4th M-REGLE embedding on the reconstructed ECG Lead I and PPG, which leads to a reduction in the magnitude of the T-wave segment of ECG lead I (</i><b><i>left</i></b><i>) and a change in prominence of the dichroic notch in the PPG (</i><b><i>right</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Enhanced genetic discovery</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"8t3jr\">M-REGLE also made improvements over U-REGLE in the identification of genetic associations with cardiovascular disease. For 12-lead ECGs, M-REGLE identified 19.3% more associated genetic loci (regions in the genome) than the unimodal approach. For ECG lead I + PPG, M-REGLE found 13.0% more loci. Importantly, a vast majority of these findings (24/35 for 12 lead ECG and 11/12 for ECG lead I + PPG) replicated known genetic associations for ECG or PPG traits as reported in the <a href=\"https://en.wikipedia.org/wiki/GWAS_catalog\" target=\"_blank\" rel=\"noopener noreferrer\">GWAS catalog</a>. M-REGLE also uncovered several new loci not previously associated with these traits, some of which showed links to cardiovascular traits in other databases.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-3-Discoveries.width-1250.png\" alt=\"MREGLE-3-Discoveries\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-3-Discoveries.width-1250.png\" alt=\"MREGLE-3-Discoveries\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"nconb\"><i>A 3-way Venn diagram of the GWAS catalog loci, loci discovered by M-REGLE (12-lead ECG) and loci discovered by U-REGLE. GWAS catalog indicates previously discovered loci.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Improved polygenic risk scores</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"8t3jr\">A <a href=\"https://en.wikipedia.org/wiki/Polygenic_score\" target=\"_blank\" rel=\"noopener noreferrer\">polygenic risk score</a> (PRS) quantifies an individual's genetic risk for a disease. We found that PRS developed using genetic variants identified by M-REGLE (from the 12-lead ECG data) significantly outperformed those from U-REGLE in predicting cardiac disease, most notably in atrial fibrillation (AFib). M-REGLE's PRS was significantly better at identifying individuals at risk. These PRS improvements for AFib were not just seen in the <a href=\"https://www.ukbiobank.ac.uk/\" target=\"_blank\" rel=\"noopener noreferrer\">UK Biobank</a>, but also independently validated in other large datasets, like the <a href=\"https://indianabiobank.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Indiana Biobank</a>, <a href=\"https://www.epic-norfolk.org.uk/\" target=\"_blank\" rel=\"noopener noreferrer\">EPIC-Norfolk</a>, and the <a href=\"https://www.ucl.ac.uk/british-womens-heart-health-study/\" target=\"_blank\" rel=\"noopener noreferrer\">British Women's Heart and Health Study</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-4-Comparison.width-1250.png\" alt=\"MREGLE-4-Comparison\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-4-Comparison.width-1250.png\" alt=\"MREGLE-4-Comparison\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"nconb\"><i>Comparison of M-REGLE PRSs for atrial fibrillation (AFib). Prevalence,</i> <a href=\"https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\" target=\"_blank\" rel=\"noopener noreferrer\"><i>AUC-ROC</i></a><i>, and</i> <a href=\"https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\" target=\"_blank\" rel=\"noopener noreferrer\"><i>AUC-PR</i></a><i> are computed (* indicates a statistically significant difference).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Why does M-REGLE work?</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"8t3jr\">The power of M-REGLE lies in how it handles information. By considering multiple modalities at the outset, M-REGLE gains three main advantages. First, it efficiently captures shared information, learning it once instead of multiple times across each modality. Second, it boosts the unique and complementary signals that each modality provides. Third, M-REGLE reduces noise, as information from one modality might help clarify or filter out noise in another. This all leads to a clearer, more robust signal for powerful downstream genetic analysis.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The future is multimodal</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"8t3jr\">This research is a step forward in leveraging the rich, multimodal health data becoming increasingly available. M-REGLE offers a way to uncover new genetic links to complex disease, improve our ability to predict disease risk, and potentially identify new targets for therapies. In addition, with the rise of smart wearables continuously collecting physiological data like ECG and PPG, methods like M-REGLE will be crucial for translating health data into insights and ultimately, better health outcomes.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"8t3jr\"><i>This work represents a collaborative achievement by many contributors and institutions. We sincerely thank our collaborators for their essential input: Yuchen Zhou, Justin Cosentino, Howard Yang, Andrew Carroll, Cory Y. McLean, Babak Behsaz (Google); Zachary R. McCaw (University of North Carolina); Tae-Hwi Schwantes-An, Dongbing Lai (Indiana University); Mahantesh I. Biradar, Robert Luben, Jorgen Engmann, Rui Providencia, Anthony P. Khawaja (University College London); Patricia B Munroe (Queen Mary University of London). Our thanks also go to Anastasiya Belyaeva for reviewing the manuscript, Greg Corrado, Shravya Shetty, and Michael Brenner for their support, and Monique Brouillette for her help in writing this blog post.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    }
  ],
  "lastUpdated": "2025-07-20T10:33:08.075Z"
}