{
  "sourceUrl": "https://rsshub.rssforever.com/google/research",
  "title": "Google Research Blog",
  "description": "Google Research Blog - Powered by RSSHub",
  "link": "https://research.google/blog",
  "items": [
    {
      "title": "A differentially private framework for gaining insights into AI chatbot use",
      "link": "https://research.google/blog/a-differentially-private-framework-for-gaining-insights-into-ai-chatbot-use/",
      "pubDate": "Tue, 09 Dec 2025 16:00:00 GMT",
      "isoDate": "2025-12-09T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "A differentially private framework for gaining insights into AI chatbot use",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_hero1.width-1250.jpg",
          "alt": "Gaining_insights_into_AI_chatbot_use_hero1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_hero.width-1250.png",
          "alt": "Gaining_insights_into_AI_chatbot_use_hero",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_2.width-1250.png",
          "alt": "Gaining_insights_into_AI_chatbot_use_2",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_3.width-1250.png",
          "alt": "Gaining_insights_into_AI_chatbot_use_3",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"k5pub\">Large language model (LLM) chatbots are used by hundreds of millions of people daily for tasks ranging from drafting emails and writing code to planning vacations and creating menus for cafes. Understanding these high-level use cases is incredibly valuable for platform providers looking to improve services or enforce safety policies. It also offers the public insights into how AI is shaping our world.</p><p data-block-key=\"ergf9\">But this raises a critical question: How can we gain valuable insights when the conversations themselves might contain private or sensitive information?</p><p data-block-key=\"d3h83\">Existing approaches, like the <a href=\"https://www.anthropic.com/research/clio\" target=\"_blank\" rel=\"noopener noreferrer\">CLIO</a> framework, attempt to solve this by using an LLM to summarize conversations while prompting it to strip out <a href=\"https://en.wikipedia.org/wiki/Personal_data\" target=\"_blank\" rel=\"noopener noreferrer\">personally identifiable information</a> (PII). While a good first step, this method relies on heuristic privacy protections. The resulting privacy guarantee is difficult to formalize and may not hold up as models evolve, making these systems difficult to maintain and audit. This limitation led us to ask if it is possible to achieve similar utility with formal, end-to-end privacy guarantees.</p><p data-block-key=\"7hpa6\">In our paper, \"<a href=\"https://arxiv.org/abs/2506.04681\" target=\"_blank\" rel=\"noopener noreferrer\">Urania: Differentially Private Insights into AI Use</a>,\" presented at <a href=\"https://colmweb.org/\" target=\"_blank\" rel=\"noopener noreferrer\">COLM 2025</a>, we introduce a new framework that generates insights from LLM chatbot interactions with rigorous <a href=\"https://en.wikipedia.org/wiki/Differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">differential privacy</a> (DP) guarantees. This framework uses a <a href=\"https://research.google/blog/practical-differentially-private-clustering/\">DP clustering algorithm</a> and keyword extraction method to ensure that no single conversation overly influences the result (i.e., the output summaries do not reveal information about any single individual's conversation). Here we explain the algorithm and demonstrate that this framework is indeed providing better privacy guarantees than prior solutions.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_hero1.width-1250.jpg\" alt=\"Gaining_insights_into_AI_chatbot_use_hero1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_hero1.width-1250.jpg\" alt=\"Gaining_insights_into_AI_chatbot_use_hero1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"qlcu8\"><i>Gemini-generated image showing schematically how the algorithm works for one cluster of conversations.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Privacy-preserving framework for insights mining</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"k5pub\">DP uses a privacy budget parameter, ε, to measure the maximum allowed influence of any single user's contributions to the final output of a model. Our framework is designed to rely on two key properties of DP:</p><ol><li data-block-key=\"410n7\"><i>Post-processing:</i> If <i>B</i> is an <i>ε</i>-DP algorithm and <i>A</i> is any non-DP algorithm, then running <i>A</i> on the output of <i>B</i> keeps things private at a <i>ε</i>-DP level.</li><li data-block-key=\"auflb\"><i>Composition:</i> If <i>A</i> and <i>B</i> are two separate <i>ε</i>-DP algorithms, running <i>A</i> on the <i>dataset</i> and the output of <i>B</i> still keeps the entire process private at a 2<i>ε</i>-DP level.</li></ol><p data-block-key=\"9pfmg\">This differentially private pipeline is designed to ensure end-to-end user data protection through the following stages:</p><ol><li data-block-key=\"feaob\"><b>DP clustering</b>: Conversations are first converted into numerical representations (embeddings). The framework then groups representations that are close to each other using a <a href=\"https://research.google/blog/practical-differentially-private-clustering/\">DP clustering algorithm</a>. This ensures that no single conversation overly influences the cluster centers.</li><li data-block-key=\"9393b\"><b>DP keyword extraction</b>: Keywords are extracted from each conversation. For every cluster, our approach computes a histogram of keywords, i.e., it counts the number of times each keyword appears in the cluster, using a DP histogram mechanism (e.g., [<a href=\"https://arxiv.org/abs/2006.03684\" target=\"_blank\" rel=\"noopener noreferrer\">1</a>, <a href=\"https://arxiv.org/abs/2301.01998\" target=\"_blank\" rel=\"noopener noreferrer\">2</a>]). We add noise to the histogram in order to mask the influence of individual conversations, ensuring that only keywords common to multiple users are selected, preventing unique or sensitive terms from being exposed. We explore three methods for creation of keywords for each conversation:<ol><li data-block-key=\"9ve1l\">LLM guided selection: We provide an LLM the conversation and ask it to create the top five most relevant keywords;</li><li data-block-key=\"fuvdn\">DP version of <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" target=\"_blank\" rel=\"noopener noreferrer\">TF-IDF</a>: We get all the words in the conversation and weight them proportionally to the number of times they appear in the text and inversely proportional to the number of times they appear in the corpus; and</li><li data-block-key=\"9r76v\">LLM guided approach where there is an initial list of keywords obtained from public data: Instead of asking the LLM to generate keywords for each conversation independently, we create a list of potential keywords and ask LLM to choose the top 5 most relevant keywords from the list.</li></ol></li><li data-block-key=\"71r3d\"><b>LLM summarization from keywords</b>: Finally, an LLM generates a high-level summary for each cluster using only the privately selected keywords. The LLM never sees the original conversations in the cluster, only the anonymized keywords. This post-processing property ensures the end-to-end privacy of the entire framework.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_hero.width-1250.png\" alt=\"Gaining_insights_into_AI_chatbot_use_hero\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_hero.width-1250.png\" alt=\"Gaining_insights_into_AI_chatbot_use_hero\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"hhudz\"><i>The framework’s data flow. Yellow nodes denote non-DP data, green nodes represent operations that are either DP or per conversation, light blue nodes denote private data, and dark blue nodes represent non-private operations.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"k5pub\">By integrating DP at its core, this framework’s privacy guarantees are mathematical, not heuristic. They don't depend on an LLM's ability to perfectly redact private data: in other words, even if keywords contain PII or some other sensitive data, the generated summaries are not going to contain this data. In more practical terms, this guarantee makes it impossible for the LLM to reveal sensitive data (e.g., due to prompt injection attacks).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Putting this framework to the test</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"k5pub\">To evaluate our framework’s utility (summary quality) and privacy (protection strength), we compared its performance against Simple-CLIO, a non-private baseline we created inspired by <a href=\"https://www.anthropic.com/research/clio\" target=\"_blank\" rel=\"noopener noreferrer\">CLIO</a>. The baseline follows a two-step process:</p><ol><li data-block-key=\"30t2i\">Conversations are converted into embeddings and clustered non-privately.</li><li data-block-key=\"25ftm\">For each cluster, a sample of conversations is fed to an LLM to generate a summary of those samples.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">The privacy-utility trade-off</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"k5pub\">As expected, we observed a trade-off: stronger privacy settings (lower values of the privacy parameter <i>ϵ</i>) led to a decrease in the granularity of the summaries. For instance, topic coverage dropped as the privacy budget tightened, because the DP clustering algorithm produced fewer and less precise clusters.</p><p data-block-key=\"cg6p6\">However, the results also held a surprise. In head-to-head comparisons, LLM evaluators often preferred the private summaries generated by our framework. In one evaluation, the DP-generated summaries were favored up to 70% of the time. This suggests that the constraints imposed by this DP pipeline — forcing summaries to be based on general, frequent keywords — can lead to outputs that are more concise and focused than those from an unconstrained, non-private approach.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Empirical privacy evaluation</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"k5pub\">To test the framework’s robustness, we ran a <a href=\"https://arxiv.org/abs/1610.05820\" target=\"_blank\" rel=\"noopener noreferrer\">membership inference-style attack</a> designed to identify whether a specific sensitive conversation was included in the dataset. The results were clear: the attack on the DP pipeline performed about as well as random guessing, achieving an <a href=\"https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\" target=\"_blank\" rel=\"noopener noreferrer\">area under the curve</a> (AUC) score of 0.53 (i.e., the integral of the <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\" rel=\"noopener noreferrer\"><i>ROC</i></a> curve). In contrast, the attack was more successful against the non-private pipeline, which had a higher AUC of 0.58, indicating greater information leakage. This experiment provides empirical evidence that our privacy framework offers significantly stronger protection against privacy leakage.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_2.width-1250.png\" alt=\"Gaining_insights_into_AI_chatbot_use_2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_2.width-1250.png\" alt=\"Gaining_insights_into_AI_chatbot_use_2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"hhudz\"><i>The</i> <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\" rel=\"noopener noreferrer\"><i>ROC</i></a><i> curve for DP pipeline shows performance close to random guessing (AUC = 0.53), demonstrating its robustness.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_3.width-1250.png\" alt=\"Gaining_insights_into_AI_chatbot_use_3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_3.width-1250.png\" alt=\"Gaining_insights_into_AI_chatbot_use_3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"hhudz\"><i>The</i> <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\" rel=\"noopener noreferrer\"><i>ROC</i></a><i> curve for the non-private pipeline is more vulnerable (AUC = 0.58).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Looking ahead</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"k5pub\">Our work is a first step toward building systems that can analyze large-scale text corpora with formal privacy guarantees. We've shown that it's possible to balance the need for meaningful insights with stringent user privacy.</p><p data-block-key=\"6t4ua\">Looking forward, we see several exciting avenues for future research. These include adapting the framework for online settings where new conversations are constantly added, exploring alternative DP mechanisms to further improve the utility-privacy trade-off, and adding support for multi-modal conversations (i.e., conversations involving images, videos, and audio).</p><p data-block-key=\"9t7eu\">As AI becomes more integrated into our daily lives, developing privacy-preserving methods for understanding its use is not just a technical challenge — it's a fundamental requirement for building trustworthy and responsible AI.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"k5pub\">Thanks to all project contributors, whose essential efforts were pivotal to its success. Special thanks to our colleagues: Yaniv Carmel, Edith Cohen, Rudrajit Das, Chris Dibak, Vadym Doroshenko, Alessandro Epasto, Prem Eruvbetine, Dem Gerolemou, Badih Ghazi, Miguel Guevara, Steve He, Peter Kairouz, Pritish Kamath, Nir Kerem, Ravi Kumar, Ethan Leeman, Pasin Manurangsi, Shlomi Pasternak, Mikhail Pravilov, Adam Sealfon, Yurii Sushko, Da Yu, Chiyuan Zhang.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Titans + MIRAS: Helping AI have long-term memory",
      "link": "https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/",
      "pubDate": "Wed, 03 Dec 2025 16:00:00 GMT",
      "isoDate": "2025-12-03T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Titans + MIRAS: Helping AI have long-term memory",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Titans-1-Overview.width-1250.png",
          "alt": "A diagram illustrating a neural architecture with three layers: Contextual Memory (learning), Core (in-context learning), and Persistent Memory (fixed weights).",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Titans-3-Performance1.width-1250.png",
          "alt": "Two line charts showing that LMM and MM models maintain lower perplexity than Mamba as sequence length increases across 360M and 760M parameter scales.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Titans-4-Performance2.width-1250.png",
          "alt": "Line graph showing Titans (MAC)-FT maintains improved accuracy over increasing sequence lengths compared to GPT-4, Mamba-FT, and other models.",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"il1w2\">The <a href=\"https://en.wikipedia.org/wiki/Transformer_(deep_learning)\" target=\"_blank\" rel=\"noopener noreferrer\">Transformer architecture</a> revolutionized <a href=\"https://medium.com/machine-learning-basics/sequence-modelling-b2cdf244c233\" target=\"_blank\" rel=\"noopener noreferrer\">sequence modeling</a> with its introduction of <a href=\"https://en.wikipedia.org/wiki/Attention_%28machine_learning%29\" target=\"_blank\" rel=\"noopener noreferrer\">attention</a>, a mechanism by which models look back at earlier inputs to prioritize relevant input data. However, computational cost increases drastically with sequence length, which limits the ability to scale Transformer-based models to extremely long contexts, such as those required for full-document understanding or genomic analysis.</p><p data-block-key=\"36kb5\">The research community explored various approaches for solutions, such as efficient linear <a href=\"https://www.d2l.ai/chapter_recurrent-modern/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">recurrent neural networks</a> (RNNs) and <a href=\"https://huggingface.co/blog/lbourdois/get-on-the-ssm-train\" target=\"_blank\" rel=\"noopener noreferrer\">state space models</a> (SSMs) like <a href=\"https://arxiv.org/pdf/2405.21060\" target=\"_blank\" rel=\"noopener noreferrer\">Mamba-2</a>. These models offer fast, linear scaling by compressing context into a fixed-size. However, this fixed-size compression cannot adequately capture the rich information in very long sequences.</p><p data-block-key=\"40m00\">In two new papers, <a href=\"https://arxiv.org/abs/2501.00663\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Titans</i></a> and <a href=\"https://arxiv.org/pdf/2504.13173\" target=\"_blank\" rel=\"noopener noreferrer\"><i>MIRAS</i></a>, we introduce an architecture and theoretical blueprint that combine the speed of RNNs with the accuracy of transformers. Titans is the specific architecture (the tool), and MIRAS is the theoretical framework (the blueprint) for generalizing these approaches. Together, they advance the concept of test-time memorization, the ability of an AI model to maintain long-term memory by incorporating more powerful “surprise” metrics (i.e., unexpected pieces of information) while the model is running and without dedicated offline retraining.</p><p data-block-key=\"eic3n\">The MIRAS framework, as demonstrated by Titans, introduces a meaningful shift toward real-time adaptation. Instead of compressing information into a static state, this architecture actively learns and updates its own parameters as data streams in. This crucial mechanism enables the model to incorporate new, specific details into its core knowledge instantly.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Titans: Learning new context on the fly\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Titans: Learning new context on the fly</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">An effective learning system requires distinct yet interconnected memory modules, mirroring the <a href=\"https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/\">human brain's separation of short-term and long-term memory</a>.</p><p data-block-key=\"dpe7v\">While attention mechanisms excel for precise, short-term memory, Titans introduces a novel neural <a href=\"https://arxiv.org/abs/2306.07174#:~:text=LongMem%20can:%20*%20Memorize%20long%20past%20context,Yan%20*%20Jianfeng%20Gao%20*%20Furu%20Wei\" target=\"_blank\" rel=\"noopener noreferrer\">long-term memory module</a>, that, unlike the fixed-size vector or matrix memory in traditional RNNs, acts as a deep neural network (specifically, a <a href=\"https://en.wikipedia.org/wiki/Multilayer_perceptron\" target=\"_blank\" rel=\"noopener noreferrer\">multi-layer perceptron</a>). This memory module provides significantly higher expressive power, allowing the model to summarize large volumes of information without losing important context. The model isn't simply taking notes; it's understanding and synthesizing the entire story.</p><p data-block-key=\"e95op\">Crucially, Titans doesn’t just passively store data. It actively learns <i>how</i> to recognize and retain important relationships and conceptual themes that connect tokens across the entire input. A key aspect of this ability is what we call the “surprise metric”. In human psychology, we know we quickly and easily forget routine, expected events but remember things that break the pattern — unexpected, surprising, or highly emotional events.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Titans-1-Overview.width-1250.png\" alt=\"A diagram illustrating a neural architecture with three layers: Contextual Memory (learning), Core (in-context learning), and Persistent Memory (fixed weights).\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Titans-1-Overview.width-1250.png\" alt=\"A diagram illustrating a neural architecture with three layers: Contextual Memory (learning), Core (in-context learning), and Persistent Memory (fixed weights).\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"759np\">Overview of the Titans (MAC) architecture. It uses a long-term memory to compress the past data and then incorporate the summary into the context and pass it to attention. Attention can then decide if it needs to attend to the summary of the past or not.</p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"il1w2\">In the context of Titans, the \"surprise metric\" is the model detecting a large difference between what it currently remembers and what the new input is telling it.</p><ul><li data-block-key=\"a9lns\"><i>Low surprise</i>: If the new word is \"cat\" and the model's memory state already expects an animal word, the gradient (surprise) is low. It can safely skip memorizing the word \"cat\" in its permanent long-term state.</li><li data-block-key=\"2t2sa\"><i>High surprise</i>: If the model's memory state is summarizing a serious financial report, and the new input is a picture of a banana peel (the unexpected event), the gradient (surprise) will be very high. This signals that the new input is important or anomalous, and it must be prioritized for permanent storage in the long-term memory module.</li></ul><p data-block-key=\"djj22\">The model uses this internal error signal (the gradient) as a mathematical equivalent of saying, \"This is unexpected and important!\" This allows the Titans architecture to selectively update its long-term memory only with the most novel and context-breaking information, keeping the overall process fast and efficient.</p><p data-block-key=\"dm2am\">Titans refines this mechanism by incorporating two critical elements:</p><ol><li data-block-key=\"bb101\"><i>Momentum</i>: The model considers both \"momentary surprise\" (the current input) and \"past surprise\" (the recent context flow). This ensures relevant subsequent information is also captured, even if those tokens are not individually surprising.</li><li data-block-key=\"b269a\"><i>Forgetting (weight decay)</i>: To manage the finite capacity of the memory when dealing with extremely long sequences, Titans employ an adaptive weight decay mechanism. This acts as a forgetting gate, allowing the model to discard information that is no longer needed.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"MIRAS: A unified view of sequence modeling\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">MIRAS: A unified view of sequence modeling</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">Every major breakthrough in sequence modeling — from modern transformers to the new, lightning-fast linear RNNs — is essentially the same thing under the hood: a highly complex <a href=\"https://www.geeksforgeeks.org/computer-organization-architecture/associative-memory/\" target=\"_blank\" rel=\"noopener noreferrer\">associative memory</a> module.</p><p data-block-key=\"d91su\">Accordingly, what makes MIRAS both unique and practical is the way it views AI modeling. Instead of seeing diverse architectures, it sees different methods of solving the same problem: efficiently combining new information with old memories without letting the essential concepts be forgotten<b>.</b></p><p data-block-key=\"7u78e\">MIRAS defines a sequence model through four key design choices:</p><ul><li data-block-key=\"abcem\"><i>Memory architecture</i>: The structure that stores information (e.g., a vector, matrix, or a deep multi-layer perceptron, like in Titans).</li><li data-block-key=\"5s0u1\"><i>Attentional bias</i>: The internal learning objective the model optimizes that determines what it prioritizes.</li><li data-block-key=\"4qd03\"><i>Retention gate</i>: The memory regularizer. MIRAS reinterprets \"forgetting mechanisms\" as specific forms of <a href=\"https://dev.to/nareshnishad/day-27-regularization-techniques-for-large-language-models-llms-4af3\" target=\"_blank\" rel=\"noopener noreferrer\">regularization</a> that balance new learning against retaining past knowledge.</li><li data-block-key=\"9savd\"><i>Memory algorithm</i>: The optimization algorithm used to update the memory.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/MIRAS_Framework_Animation.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"759np\">The MIRAS framework overview. In the MIRAS framework, we aim to learn an associative memory, mapping between keys and values. For each token, the memory module internally optimizes its inner attentional bias while using its retention gate to make sure that it does not deviate from its past state. The optimization process is done through gradient-based optimizer.</p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Transcending the mean squared error paradigm\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Transcending the mean squared error paradigm</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">Virtually all successful existing sequence models rely on <a href=\"https://en.wikipedia.org/wiki/Mean_squared_error\" target=\"_blank\" rel=\"noopener noreferrer\">mean squared error</a> (MSE) or <a href=\"https://medium.com/advanced-deep-learning/understanding-vector-similarity-b9c10f7506de\" target=\"_blank\" rel=\"noopener noreferrer\">dot-product similarity</a> for both their bias and retention. This reliance can make models sensitive to outliers and limit their expressive power.</p><p data-block-key=\"1cust\">MIRAS transcends this limitation by providing a generative framework to explore a more rich design space informed by the literature in optimization and statistics. This allows for the creation of novel architectures with <a href=\"https://en.wikipedia.org/wiki/Non-Euclidean_geometry\" target=\"_blank\" rel=\"noopener noreferrer\">non-Euclidean objectives</a> and regularization.</p><p data-block-key=\"40qp3\">Using MIRAS, we created three specific attention-free models:</p><ul><li data-block-key=\"fpeib\"><i>YAAD</i>: We designed this MIRAS variant to be less sensitive to major errors or \"outliers\" (like a single typo in a large document). It uses a gentler math penalty (<a href=\"https://en.wikipedia.org/wiki/Huber_loss\" target=\"_blank\" rel=\"noopener noreferrer\">Huber loss</a>) for mistakes, so it doesn't overreact to one-off issues. This makes the model more robust when the input data is messy or inconsistent.</li><li data-block-key=\"28vrl\"><i>MONETA</i>: This model explores the use of more complex and strict mathematical penalties (called <a href=\"https://en.wikipedia.org/wiki/Norm_(mathematics)\" target=\"_blank\" rel=\"noopener noreferrer\">generalized norms</a>). It investigates whether using these more disciplined rules for both what the model attends to and what it forgets can lead to a more powerful and stable long-term memory system overall.</li><li data-block-key=\"d4e49\"><i>MEMORA</i>: This model focuses on achieving the best possible memory stability by forcing its memory to act like a strict probability map. By using this constraint, it ensures that every time the memory state is updated, the changes are controlled and balanced. This guarantees a clean, stable process for integrating new information.Virtually all successful existing sequence models rely on <a href=\"https://en.wikipedia.org/wiki/Mean_squared_error\" target=\"_blank\" rel=\"noopener noreferrer\">mean squared error</a> (MSE) or <a href=\"https://medium.com/advanced-deep-learning/understanding-vector-similarity-b9c10f7506de\" target=\"_blank\" rel=\"noopener noreferrer\">dot-product similarity</a> for both their bias and retention. This reliance can make models sensitive to outliers and limit their expressive power.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Experiments and results\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Experiments and results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">We rigorously compared Titans along with MIRAS variants (YAAD, MONETA, MEMORA) against leading architectures, including <a href=\"https://arxiv.org/abs/2003.04974\" target=\"_blank\" rel=\"noopener noreferrer\">Transformer++</a>, <a href=\"https://arxiv.org/pdf/2405.21060\" target=\"_blank\" rel=\"noopener noreferrer\">Mamba-2</a>, and <a href=\"https://arxiv.org/pdf/2412.06464\" target=\"_blank\" rel=\"noopener noreferrer\">Gated DeltaNet</a>. We further validated versatility by testing Titans on genomic modeling (DNA) and time-series forecasting, proving the architecture generalizes effectively beyond text.</p><p data-block-key=\"a9v6c\">Across both standard language modeling datasets (<a href=\"https://c4model.com/\" target=\"_blank\" rel=\"noopener noreferrer\">C4</a>, <a href=\"https://huggingface.co/datasets/Salesforce/wikitext\" target=\"_blank\" rel=\"noopener noreferrer\">WikiTex</a>t) and <a href=\"https://medium.com/@hetzer2807/zero-shot-reasoning-unleashed-the-magic-of-large-language-models-4e877dfe470e\" target=\"_blank\" rel=\"noopener noreferrer\">zero-shot reasoning tasks</a> (<a href=\"https://arxiv.org/abs/1905.07830\" target=\"_blank\" rel=\"noopener noreferrer\">HellaSwag</a>, PIQA), our models consistently demonstrated higher accuracy and <a href=\"https://en.wikipedia.org/wiki/Perplexity\" target=\"_blank\" rel=\"noopener noreferrer\">perplexity</a> (a measure of how surprised an LLM is when looking at a piece of text).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"The power of deep memory\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">The power of deep memory</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">Ablation studies clearly show that the depth of the memory architecture is crucial. When comparing long-term memory modules of the same size but different depths, modules with deeper memories consistently achieve lower perplexity in language modeling. Furthermore, they exhibit better scaling properties, maintaining performance as the sequence length increases significantly.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Titans-3-Performance1.width-1250.png\" alt=\"Two line charts showing that LMM and MM models maintain lower perplexity than Mamba as sequence length increases across 360M and 760M parameter scales.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Titans-3-Performance1.width-1250.png\" alt=\"Two line charts showing that LMM and MM models maintain lower perplexity than Mamba as sequence length increases across 360M and 760M parameter scales.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"759np\"><i>The effect of memory depth on the perplexity across 360M and 760M parameter scales.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Language modeling and efficiency\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Language modeling and efficiency</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">In language modeling and commonsense reasoning tasks, Titans architectures outperform state-of-the-art linear recurrent models (such as Mamba-2 and Gated DeltaNet) and Transformer++ baselines of comparable sizes. The novel MIRAS variants (MONETA, YAAD, MEMORA) also achieve improved performance compared to these baselines, validating the benefit of exploring robust, non-MSE optimization mechanisms. Importantly, these models maintain efficient, parallelizable training and fast linear inference speeds.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Extreme long-context recall\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Extreme long-context recall</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">The most significant advantage of these new architectures is their ability to handle extremely long contexts. This is highlighted in the <a href=\"https://github.com/booydar/babilong\" target=\"_blank\" rel=\"noopener noreferrer\">BABILong benchmark</a>, a task requiring reasoning across facts distributed in extremely long documents. In this challenging setting, Titans outperforms all baselines, including extremely large models like GPT-4, despite having many fewer parameters. Titans further demonstrates the capability to scale effectively to context window sizes larger than 2 million tokens.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Titans-4-Performance2.width-1250.png\" alt=\"Line graph showing Titans (MAC)-FT maintains improved accuracy over increasing sequence lengths compared to GPT-4, Mamba-FT, and other models.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Titans-4-Performance2.width-1250.png\" alt=\"Line graph showing Titans (MAC)-FT maintains improved accuracy over increasing sequence lengths compared to GPT-4, Mamba-FT, and other models.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"759np\"><i>Performance of Titans on extreme long-context reasoning.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Conclusion\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">The introduction of Titans and the MIRAS framework marks a significant advancement in sequence modeling. By employing deep neural networks as memory modules that learn to memorize as data is coming in, these approaches overcome the limitations of fixed-size recurrent states. Furthermore, MIRAS provides a powerful theoretical unification, revealing the connection between online optimization, associative memory, and architectural design. By moving beyond the standard Euclidean paradigm, this research opens the door to a new generation of sequence models that combine the efficiency of RNNs with the expressive power needed for the era of long-context AI.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "From Waveforms to Wisdom: The New Benchmark for Auditory Intelligence",
      "link": "https://research.google/blog/from-waveforms-to-wisdom-the-new-benchmark-for-auditory-intelligence/",
      "pubDate": "Tue, 02 Dec 2025 16:00:00 GMT",
      "isoDate": "2025-12-02T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "From Waveforms to Wisdom: The New Benchmark for Auditory Intelligence",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/image2.width-1250.png",
          "alt": "Infographic titled Massive Sound Embedding Benchmark (MSEB) displaying icons for eight audio tasks, such as Retrieval, Classification, and Transcription.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/image1.width-1250.png",
          "alt": "Bar chart comparing performance metrics for Text and Sound inputs across \"SuperTasks\" like Retrieval, Reasoning, and Classification.",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"04uji\">Sound is a critical part of <a href=\"https://courses.lumenlearning.com/waymaker-psychology/chapter/multi-modal-perception/#:~:text=For%20example%2C%20if%20the%20middle,Right%20there\" target=\"_blank\" rel=\"noopener noreferrer\">multimodal perception</a>. For a system — be it a voice assistant, a next-generation security monitor, or an autonomous agent — to behave naturally, it must demonstrate a full spectrum of auditory capabilities. These capabilities include transcription, classification, retrieval, reasoning, segmentation, clustering, reranking, and reconstruction.</p><p data-block-key=\"erg92\">These diverse functions rely on transforming raw sound into an intermediate representation, or <a href=\"https://huggingface.co/spaces/hesamation/primer-llm-embedding\" target=\"_blank\" rel=\"noopener noreferrer\">embedding</a>. But research into improving the auditory capabilities of multimodal perception models has been fragmented, and there remain important unanswered questions: How do we compare performance across domains like human speech and bioacoustics? What is the <i>true</i> performance potential we are leaving on the table? And could a single, general-purpose sound embedding serve as the foundation for all these capabilities?</p><p data-block-key=\"bn8cg\">To investigate these queries and accelerate progress toward robust machine sound intelligence, we created the <a href=\"https://github.com/google-research/mseb/tree/main\" target=\"_blank\" rel=\"noopener noreferrer\">Massive Sound Embedding Benchmark</a> (MSEB), presented at <a href=\"https://neurips.cc/virtual/2025/loc/san-diego/poster/121597\" target=\"_blank\" rel=\"noopener noreferrer\">NeurIPS 2025</a>.</p><p data-block-key=\"932nn\">MSEB provides the necessary structure to answer these questions by:</p><ul><li data-block-key=\"brt1v\">Standardizing evaluation for a comprehensive suite of eight real-world capabilities that we believe every human-like intelligent system must possess.</li><li data-block-key=\"h3sm\">Providing an open and extensible framework that allows researchers to seamlessly integrate and evaluate any model type — from conventional downstream uni-modal models to cascade models to end-to-end multimodal embedding models.</li><li data-block-key=\"8e1pi\">Establishing clear performance goals to objectively highlight research opportunities beyond current state-of-the-art approaches.</li></ul><p data-block-key=\"fl1pu\">Our initial experiments confirm that current sound representations are far from universal, revealing substantial performance \"headroom” (i.e., maximum improvement possible) across all eight tasks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The three pillars of MSEB: A unified framework</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">MSEB is built on three foundational pillars designed to provide the community with the tools needed to build the next generation of sound understanding models.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">1. Diverse datasets for real-world scenarios</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">A benchmark is only as strong as its data. MSEB includes a curated collection of accessible datasets that better reflect our diverse global user community. The cornerstone of our benchmark is the <a href=\"https://huggingface.co/datasets/google/svq\" target=\"_blank\" rel=\"noopener noreferrer\">Simple Voice Questions</a> (SVQ) dataset, a new resource featuring 177,352 short, spoken queries across 26 locales and 17 languages. These recordings were captured in four distinct acoustic environments (clean, background speech, traffic noise, and media noise), and include rich metadata on speaker attributes and time-aligned salient terms. We collected and open-sourced this resource, available on <a href=\"https://huggingface.co/datasets/google/svq/viewer\" target=\"_blank\" rel=\"noopener noreferrer\">Hugging Face</a>.</p><p data-block-key=\"4t0bt\">MSEB also integrates high-quality, public datasets covering a variety of sound domains:</p><ul><li data-block-key=\"92lni\"><a href=\"https://huggingface.co/datasets/FBK-MT/Speech-MASSIVE\" target=\"_blank\" rel=\"noopener noreferrer\">Speech-MASSIVE</a>: For multilingual spoken language understanding and intent classification.</li><li data-block-key=\"c5qpu\"><a href=\"https://huggingface.co/datasets/Fhrozen/FSD50k\" target=\"_blank\" rel=\"noopener noreferrer\">FSD50K</a>: A large dataset for multi-label environmental sound event recognition (200 classes from the <a href=\"https://research.google.com/audioset/ontology/index.html\">AudioSet Ontology</a>).</li><li data-block-key=\"40c5u\"><a href=\"https://huggingface.co/datasets/DBD-research-group/BirdSet\" target=\"_blank\" rel=\"noopener noreferrer\">BirdSet</a>: A massive-scale benchmark for avian bioacoustics, including complex soundscape recordings.</li></ul><p data-block-key=\"4tacb\">We’re actively working on creating and adding more relevant and large-scale datasets to MSEB. We invite the community to share their suggestions and express interest in collaboration through our <a href=\"https://github.com/google-research/mseb/tree/main/mseb\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub repo</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">2. A comprehensive suite of eight core capabilities</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">The design of MSEB is built on the premise that the future of AI-based sound interaction is multimodal. Every task uses sound as the critical input, but also incorporates information from other modalities (like text context or knowledge bases) to simulate realistic scenarios.</p><p data-block-key=\"diln9\">MSEB is structured around eight core “super-tasks”, i.e., tasks that represent a capability vital for an intelligent system:</p><ul><li data-block-key=\"53jlg\"><i>Retrieval (voice search)</i>: Simulates voice search by finding relevant documents or passages in a knowledge base from a spoken query.</li><li data-block-key=\"csvqq\"><i>Reasoning (intelligent assistants)</i>: Tests the ability to find a precise answer within a given document or passage based on a spoken question.</li><li data-block-key=\"9hbci\"><i>Classification (monitoring/security)</i>: Categorizes sounds based on speaker attributes, user intent, recording environment, or specific sound events.</li><li data-block-key=\"4ksci\"><i>Transcription</i>: Converts the audio signal into a verbatim text representation (like automatic speech recognition, or ASR, for spoken languages).</li><li data-block-key=\"3a90f\"><i>Segmentation (indexing)</i>: Identifies the most important terms within a sound clip and localizes them with precise start and end times.</li><li data-block-key=\"3c6p0\"><i>Clustering (organization)</i>: Groups a collection of sound samples based on shared attributes (like speaker identity or environment) without relying on predefined labels.</li><li data-block-key=\"a3rmt\"><i>Reranking (hypothesis refinement)</i>: Reorders a list of ambiguous text hypotheses (e.g., ASR output) to better match the original spoken query.</li><li data-block-key=\"5c70t\"><i>Reconstruction (generative AI)</i>: Tests the quality of the embedding by measuring the fidelity with which the original audio waveform can be regenerated from it.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/image2.width-1250.png\" alt=\"Infographic titled Massive Sound Embedding Benchmark (MSEB) displaying icons for eight audio tasks, such as Retrieval, Classification, and Transcription.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/image2.width-1250.png\" alt=\"Infographic titled Massive Sound Embedding Benchmark (MSEB) displaying icons for eight audio tasks, such as Retrieval, Classification, and Transcription.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"i7px7\"><i>MSEB tasks range from information access (retrieval, reranking, reasoning), to fundamental core perception (classification, transcription, segmentation), to higher-level organization generation (clustering, reconstruction).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"04uji\">Future development is focused on practical, multimodal tasks in new domains, like music or combinations with images.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">3. A robust evaluation framework and headroom baselines</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">The primary goal of MSEB is to establish strong baselines and reveal the headroom in current AI models by evaluating them across two main task categories:</p><ul><li data-block-key=\"fe0k3\"><i>Semantic (e.g., voice search, reasoning)</i>: Do the models correctly understand the meaning and intent of the spoken words, even when the audio is noisy?</li><li data-block-key=\"e2afs\"><i>Acoustic (e.g., classification, clustering)</i>: Do the models accurately identify who is speaking or what the environmental sound is, regardless of meaning?</li></ul><p data-block-key=\"4ovqc\">The model-agnostic design of the MSEB library is built to evaluate a range of models — from cascade systems to novel end-to-end audio encoders — all within a standardized, comparative framework.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Comparison methodology</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">We used the MSEB framework to test the performance of current sound embedding models to see how close the models are to being truly intelligent and universal.</p><p data-block-key=\"7kl64\">For semantic tasks, the models were compared against the ground-truth text input. For non-semantic tasks, the models are compared against the best current dedicated solution to set a solid performance baseline that any new, general-purpose model must surpass.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Core limitations of existing sound representations</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">The results demonstrate that existing AI models have measurable flaws across all key sound-understanding capabilities, which demonstrates the need for an evaluation framework like MSEB.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/image1.width-1250.png\" alt=\"Bar chart comparing performance metrics for Text and Sound inputs across &quot;SuperTasks&quot; like Retrieval, Reasoning, and Classification.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/image1.width-1250.png\" alt=\"Bar chart comparing performance metrics for Text and Sound inputs across &quot;SuperTasks&quot; like Retrieval, Reasoning, and Classification.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"i7px7\"><i>MSEB’s evaluation of AI models across key tasks shows important deficiencies and room for improvement. The metrics used for comparison include the</i> <a href=\"https://en.wikipedia.org/wiki/Mean_reciprocal_rank\" target=\"_blank\" rel=\"noopener noreferrer\"><i>MRR</i></a><i>,</i> <a href=\"https://en.wikipedia.org/wiki/F-score\" target=\"_blank\" rel=\"noopener noreferrer\"><i>F1</i></a><i>,</i> <a href=\"https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)\" target=\"_blank\" rel=\"noopener noreferrer\"><i>mAP</i></a><i>,</i> <a href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision\" target=\"_blank\" rel=\"noopener noreferrer\"><i>ACC</i></a><i>,</i> <a href=\"https://en.wikipedia.org/wiki/Word_error_rate\" target=\"_blank\" rel=\"noopener noreferrer\"><i>WER</i></a><i>,</i> <a href=\"https://en.wikipedia.org/wiki/Discounted_cumulative_gain\" target=\"_blank\" rel=\"noopener noreferrer\"><i>NDCG</i></a><i>,</i> <a href=\"https://www.cs.columbia.edu/~amaxwell/pubs/clustering_metric_paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\"><i>VMeasure</i></a><i>, and</i> <a href=\"https://arxiv.org/abs/1812.08466\" target=\"_blank\" rel=\"noopener noreferrer\"><i>FAD</i></a><i>.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"04uji\">This evaluation reveals five major problems that currently limit the capability of sound-processing AI:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">1. Semantic bottlenecks</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">For tasks relying on language content (retrieval, reasoning, reranking), the ASR stage consistently and universally bottlenecks performance, resulting in loss of <a href=\"https://medium.com/@therealitydrift/semantic-fidelity-when-ai-gets-the-facts-right-but-the-meaning-wrong-8dd270b4017f\" target=\"_blank\" rel=\"noopener noreferrer\">semantic fidelity</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">2. Misaligned objectives</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">The standard practice in speech technology involves a cascade model: transcribing speech to text, and then relying on that text for all downstream tasks. This is fundamentally wrong because it forces optimization onto the wrong metric. The ASR component is solely trained to minimize word error rate, a goal that is severely misaligned with the needs of real-world applications — which often requires maximizing the relevance, accuracy, or reasoning capability of the output, independent of perfect transcription.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">3. Non-universality</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">Models exhibit a severe lack of reliability, meaning performance varies drastically by language. The systems only work well for major, common languages. When tested on less common languages, the transcription quality collapses, causing critical task failures in search, ranking, and segmentation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">4. Lack of robustness</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">The quality of sound reconstruction degrades sharply under noise. When background noise is introduced, the model’s ability to accurately interpret the original sound and environment struggles significantly. This establishes the most challenging benchmarks for the system, highlighting its difficulty in handling complex, general environmental sounds found in real-world settings (like a busy office or a noisy street).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">5. Over-complexity</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">For simple tasks that don't involve understanding meaning (like identifying who is speaking), complicated, pre-trained AI models are surprisingly no better than just using the raw representation of the sound waves. This often leads developers to waste their efforts on overly complex models when basic data works just as well.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">The results demonstrate a substantial performance gap in existing general sound-based approaches across all eight super-tasks. This widespread underperformance, relative to the maximum potential defined by these ceilings, underscores the critical need for more research into unified and robust sound representations that can close the gap in machine auditory intelligence.</p><p data-block-key=\"fcbrv\">We envision MSEB as a dynamic and growing platform for the entire sound processing community. We invite you to contribute to this effort by using MSEB to evaluate your own sound representation techniques, contributing new tasks and datasets to the benchmark to help it grow, and joining the collaborative effort to push the boundaries of what's possible in machine sound intelligence.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\"><i>This project was led by Ehsan Variani, Georg Heigold, Tom Bagby, and Cyril Allauzen. The authors sincerely thank all who contributed to this project, whose critical input made it possible. We are especially grateful to our colleagues Hawi Abraham, Shankar Kumar, Ji Ma, Michael Riley, Sunil Vemuri, and Travis Trekel. We also wish to acknowledge those who helped prepare this post: Mark Simborg for his extensive editing, Kimberly Schwede for the wonderful illustrations, and Mickey Wurts for his valuable assistance.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Reducing EV range anxiety: How a simple AI model predicts port availability",
      "link": "https://research.google/blog/reducing-ev-range-anxiety-how-a-simple-ai-model-predicts-port-availability/",
      "pubDate": "Thu, 20 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-20T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Reducing EV range anxiety: How a simple AI model predicts port availability",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-1-30minWeights.width-1250.png",
          "alt": "Plot of feature weights for each hour for the 30 minute horizon.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-2-60minWeights.width-1250.png",
          "alt": "Plot of feature weights for each hour for the 60 minute horizon.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-3-ErrorRates.width-1250.png",
          "alt": "Table comparing error rates on the availability of at least one free port (30 to 60-Minute Horizon).",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"mhje4\">The transition to electric vehicles (EVs) is accelerating globally, bringing with it the critical need for a reliable and robust charging infrastructure. While building out more physical charging stations is an important step, an equally important task is maximizing the efficiency of this infrastructure and minimizing \"range anxiety”, a term used to describe an EV driver’s fear of running out of battery before reaching their destination or the nearest available charging station. These concerns led us to design an <a href=\"https://research.google/blog/addressing-range-anxiety-with-smart-electric-vehicle-routing/\">approach for EV routing</a> that reduces range anxiety by integrating charging stations into the navigational route based on the battery level and destination.</p><p data-block-key=\"d0uuh\">This week <a href=\"https://blog.google/products/maps/holiday-gemini-tips-new-explore-tab/\" target=\"_blank\" rel=\"noopener noreferrer\">we announced</a> a new lightweight, highly efficient prediction model that can answer the core question, “<i>What is the probability that an EV charging port will be available at a specific station a certain number of minutes from now?</i>” We found that the most sophisticated model isn't always the best solution. By co-designing the model and the deployment infrastructure, we were able to create a highly effective prediction system based on a simple <a href=\"https://en.wikipedia.org/wiki/Linear_regression\" target=\"_blank\" rel=\"noopener noreferrer\">linear regression</a> approach. This model’s simplicity is its strength, allowing it to rely on easily accessible features while still achieving performance improvements over a strong baseline. Our work demonstrates that combining intuitive real-world logic with machine learning can deliver significant operational and user experience benefits.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Creating the model</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\">Our goal was to maximize predictive power while minimizing the feature set (i.e., the specific, measurable data points the model uses to make a prediction) to ensure speed and low-latency deployment. After testing various architectures, including a decision tree and a simple neural network, a straightforward linear regression model proved to be the most performant and robust for this specific task.</p><p data-block-key=\"73a8o\">We trained the model using real-time availability data from charging networks to calculate the true number of available charging ports within a certain number of minutes from the current observation time using criteria for model features and weights. We uniformly sampled ports from two distinct regions (CA and Germany). Larger stations were more likely to be included in the training set because they see more traffic than isolated ports and more closely reflect real-world usage.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Features</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\">The model uses the hour of the day as a key piece of information (a \"feature\"). It treats each hour (or hour range) separately. For example, \"9 AM\" is one feature, and \"5 PM\" is another.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Weights</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\">The \"weights\" are the specific numerical values that the linear regression algorithm learns during training. These numbers dictate how much each specific hour of the day affects the final prediction.</p><ul><li data-block-key=\"ed5n\">A positive weight means that during that hour (e.g., 7:00 AM), ports tend to get occupied (the occupancy is increasing).</li><li data-block-key=\"cleil\">A negative weight means that during that hour (e.g., 5:00 PM), ports tend to get freed up (the occupancy is decreasing).</li><li data-block-key=\"7cqut\">A zero or near-zero weight means that during that hour (e.g., 3:00 AM), there is little change in port status.</li></ul><p data-block-key=\"91vn8\">These “hour feature weights\" are the model's learned coefficients that quantify the predictable rate of EV port occupancy change for every hour of the day. Essentially, the model learns to express the difference between the current number of available ports and the future number of available ports as a function of the hour feature weights.</p><p data-block-key=\"eebfc\">The feature weights learned for each hour of the day are particularly insightful because they directly represent the rate at which port occupancy changes. As illustrated by the chart below, there are clear, predictable trends tied to driver schedules:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-1-30minWeights.width-1250.png\" alt=\"Plot of feature weights for each hour for the 30 minute horizon.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-1-30minWeights.width-1250.png\" alt=\"Plot of feature weights for each hour for the 30 minute horizon.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"ifwbm\"><i>Feature weights for each hour for the 30 minute horizon. They correspond to the rate at which port occupancy changes at each 30 minute bucket.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-2-60minWeights.width-1250.png\" alt=\"Plot of feature weights for each hour for the 60 minute horizon.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-2-60minWeights.width-1250.png\" alt=\"Plot of feature weights for each hour for the 60 minute horizon.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"ifwbm\"><i>Feature weights for each hour for the 60 minute horizon. They correspond to the rate at which port occupancy changes at each 60 minute bucket.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"mhje4\">Note that the model only differentiates from the current state when the change rate is significant (e.g., rush hour) or the station is large (more ports amplify the predicted change), which are intuitively the correct times to issue an updated prediction.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Experiments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\">Our evaluation was designed to be rigorous and representative of real-world usage. For both the 30-minute and 60-minute time horizons, we evaluated predictions on 100 randomly selected stations, sampling their occupancy status 48 times daily (every 30 minutes) for a full week.</p><p data-block-key=\"6khd7\">The model was benchmarked against a remarkably strong baseline: the \"Keep Current State\" approach. This baseline simply assumes that the number of available ports a certain number of minutes (<i>H</i>) in the future will be exactly the same as the current number.</p><p data-block-key=\"2lqm6\">While simple, this baseline is very hard to beat, especially over short horizons. For example, our data showed that on the US East Coast, never more than 10% of ports change their availability state within a 30-minute block. Since most of the time the state doesn't change, the simplest prediction — no change — is correct most of the time, making the task of adding predictive value extremely difficult.</p><p data-block-key=\"6taqm\">We focused on two key metrics to measure the model’s accuracy for predicting the exact number of free ports: mean squared error (MSE) and mean absolute error (MAE). A ratio of MSE/MAE ≥ 1 free port measures the accuracy of the most critical binary task for the user: “Will I find at least one free port (Yes/No)?”</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\">The evaluation confirmed that the linear regression model provides crucial gains over the strong \"Keep Current State\" baseline, primarily by correctly identifying the infrequent, yet vital, moments of high occupancy turnover.</p><p data-block-key=\"em9tt\">We sampled test instances from among stations with at least 6 ports with horizons of 30 to 60 minutes, a realistic set of cases for charging in urban environments. We evaluated the model for the task of predicting the availability of at least one port in a station. This evaluation focused on the station profile and time of day when the model would differentiate from the baseline, namely large stations at times of significant rates of change.</p><p data-block-key=\"2selt\">The table below presents the fraction of time in which we provide a wrong prediction (which is equivalent to the MAE for this problem) for the times of highest change (8am and 8pm).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-3-ErrorRates.width-1250.png\" alt=\"Table comparing error rates on the availability of at least one free port (30 to 60-Minute Horizon).\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-3-ErrorRates.width-1250.png\" alt=\"Table comparing error rates on the availability of at least one free port (30 to 60-Minute Horizon).\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"ifwbm\"><i>Comparison of error rates on the availability of at least one free port (30 to 60-Minute Horizon).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"mhje4\">In summary, deploying the regression model allows us to reduce the number of bad predictions by approximately 20% in morning peak times and by approximately 40% in evening peak times.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Regional differences</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\">Further examinations revealed that while the shape of the change rate curve (when ports fill vs. when they empty) is similar across regions, the magnitude of the change is distinct enough to warrant separate models. For instance, training the model separately for regions like California and Germany yielded better performance than pooling all data together, suggesting that it’s necessary to account for unique regional EV usage patterns.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\">We have successfully developed and deployed a lightweight, linear regression model that effectively predicts EV charging port availability. By focusing on simplicity, speed, and co-designing the model with the existing infrastructure, we bypassed the complexity and latency associated with more detailed, but often unscalable, approaches.</p><p data-block-key=\"bg92t\">The resulting model provides a crucial predictive advantage over a strong \"Keep Current State\" baseline, particularly during high-traffic periods. This capability translates directly into an improved user experience: reduced anxiety, smarter routing decisions, and a better overall experience that supports the continued growth of electric mobility. Future work will focus on extending the prediction horizons to provide even greater value for long-distance travel planning.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\"><i>We thank our collaborators Achir Ramadhan, Sreenivas Gollapudi, Shubham Gupta, Ilya Eyzerman, and Ivan Kuznetsov.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Real-time speech-to-speech translation",
      "link": "https://research.google/blog/real-time-speech-to-speech-translation/",
      "pubDate": "Tue, 18 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-18T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Real-time speech-to-speech translation",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST1.width-1250.png",
          "alt": "RTS2ST1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST2.width-1250.png",
          "alt": "RTS2ST2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST3.width-1250.png",
          "alt": "RTS2ST3",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST4.width-1250.png",
          "alt": "RTS2ST4",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST5.width-1250.png",
          "alt": "RTS2ST5",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"gi1wx\">Real-time communication is an integral part of both our professional and personal lives. When speaking to people remotely across language barriers, it can be difficult to truly connect by just relying on state-of-the-art translated captions, as they lack personality and real-time responsiveness essential for fluid conversation. The arrival of <a href=\"https://research.google/blog/introducing-translatotron-an-end-to-end-speech-to-speech-translation-model\">speech-to-speech translation</a> (S2ST) bridges this gap by directly generating translated audio, leading to more natural communication. Existing speech-to-speech translation systems often incur significant delays (4–5s), tend to accumulate errors, and typically lack personalization.</p><p data-block-key=\"325ls\">Today we describe an innovative end-to-end S2ST model that overcomes these limitations, enabling live translation in the original speaker's voice with only 2 second delay. The novel architecture leverages a streaming framework and, with training on time-synchronized data, significantly reduces the delay between the original input and the translated speech. To support a breadth of languages, we introduce a scalable time-synced data acquisition pipeline that allows us to gradually expand the system to include more languages. This technology has demonstrated its effectiveness through successful deployment in real-time sensitive use cases.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qrznl\">Cascaded S2ST</h2><p data-block-key=\"6p2r8\">Prior real-time speech-to-speech technologies employed a cascaded pipeline of individual processing blocks:</p><ol><li data-block-key=\"6s8ba\">Firstly, the source audio is transcribed to text using <a href=\"https://en.wikipedia.org/wiki/Speech_recognition\" target=\"_blank\" rel=\"noopener noreferrer\">automatic speech recognition</a> (ASR) AI models.</li><li data-block-key=\"fuunf\">Next, the transcribed text is translated word-for-word to the target language using <a href=\"https://en.wikipedia.org/wiki/Machine_translation\" target=\"_blank\" rel=\"noopener noreferrer\">automatic speech translation</a> (AST).</li><li data-block-key=\"9mjqi\">Finally, the translated text is converted back to audio using text-to-speech pipelines (TTS).</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST1_Cascade.width-1250.png\" alt=\"RTS2ST1_Cascade\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST1_Cascade.width-1250.png\" alt=\"RTS2ST1_Cascade\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"b1hy5\"><i>Schematic representation of classic, cascade-style speech-to-speech translation system.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qrznl\">Despite the high quality of the individual cascade components, achieving a seamless, real-time S2ST experience has been challenging due to three primary factors:</p><ol><li data-block-key=\"grol\">Significant delays of 4–5 seconds, forcing turn-based conversations.</li><li data-block-key=\"bbctr\">Accumulated errors at each stage of the translation process.</li><li data-block-key=\"ba4or\">A notable lack of personalization due to the general-purpose TTS technology.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qrznl\">A novel end-to-end, personalized S2ST</h2><p data-block-key=\"3e2vl\">To significantly advance S2ST, we created a scalable data acquisition pipeline and developed an end-to-end model that provides direct, real-time language translation with just a two-second delay:</p><ol><li data-block-key=\"fl20f\"><i>Scalable data acquisition pipeline</i>: We created a data processing pipeline to convert raw audio into a time-synchronized input/target dataset. This was achieved by integrating existing ASR and TTS technologies with precise alignment steps, ensuring translated audio best matches the input. Rigorous filtering and validation were employed to remove difficult-to-align examples.</li><li data-block-key=\"32p9v\"><i>Real-time speech-to-speech translation architecture</i>: We introduced an audio-specific streaming machine learning architecture to support training on this time-synchronized data. Building on the <a href=\"https://research.google/blog/audiolm-a-language-modeling-approach-to-audio-generation/\">AudioLM framework</a> and fundamental transformer blocks, this architecture is designed to handle continuous audio streams, allowing the model to decide when to output translations. It is also structured to manage hierarchical audio representations using the <a href=\"https://arxiv.org/abs/2508.05207\" target=\"_blank\" rel=\"noopener noreferrer\">SpectroStream</a> codec technology.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    <div class=\"rich-text --theme- --mode-\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p style=\"text-align: center;\"><video muted=\"\" controls=\"controls\" width=\"67%\" height=\"67%\"> <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/spanish_english_s2st_viz.mp4\" type=\"video/mp4\"> </video></p>\n<table class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\" cellspacing=\"0\" cellpadding=\"0\" align=\"center\">\n<tbody>\n<tr>\n<td class=\"tr-caption\" style=\"text-align: center;\">An example of our personalized S2ST applied to a Spanish original translated to English.</td>\n</tr>\n</tbody>\n</table>\n</div>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qrznl\">Scalable data acquisition pipeline</h2><p data-block-key=\"eo8vt\">For a given language pair, initial work starts with raw audio acquisition. We utilize a diverse set of audio sources, including data generated by TTS models. This audio undergoes a cleaning and filtering process to ensure it contains a single speaker of the source language and has an appropriate noise level. After initial data collection, an ASR step transcribes the source text. With both source audio and text available, <a href=\"https://www.danielpovey.com/files/htkbook.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">the forced alignment algorithm</a> generates alignment timesteps (audio-to-text mapping). Any audio segments where alignment fails are discarded.</p><p data-block-key=\"6hb40\">The remaining clips are machine translated from the source into the target language. Subsequently, a series of automated filters validate the translated output, ensuring accuracy and correspondence to the input text. Next, the original transcribed and translated texts are also aligned to generate corresponding timestamp annotations (text-to-translated text mapping).</p><p data-block-key=\"baqfg\">Using <a href=\"https://research.google/blog/audiolm-a-language-modeling-approach-to-audio-generation/\">a custom text-to-speech generation engine</a>, the translated text is converted into translated audio, preserving the voice characteristics from the original audio while producing natural-sounding output. The pipeline concludes with one more forced alignment step of the translated text and the generated speech (speech-to-text mapping).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST2_Pipeline.width-1250.png\" alt=\"RTS2ST2_Pipeline\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST2_Pipeline.width-1250.png\" alt=\"RTS2ST2_Pipeline\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"b1hy5\"><i>Streaming audio-to-audio translation dataset generation.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qrznl\">Utilizing the three generated alignments from the previous steps, the overlap between them is calculated, yielding alignment masks between the source and target audio. These alignment masks are then used to guide the loss computation during training.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-vertical-padding --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST3.width-1250.png\" alt=\"RTS2ST3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST3.width-1250.png\" alt=\"RTS2ST3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n\n  \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n    \n\n\n\n\n\n<div class=\"sound-player --theme-light --mode-standalone\" data-gt-id=\"sound_player\" data-gt-component-name=\"component name\">\n    <div class=\"sound-player__wrapper\">\n        \n            \n\n\n<div class=\"sound-player__column\">\n    \n\n\n\n\n    \n\n<audio controls=\"\">\n    \n    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/RealTimeS2ST_es_alignment.wav\" type=\"audio/wav\">\n    Your browser does not support the audio element.\n</audio>\n\n    \n\n    \n</div>\n\n        \n    </div>\n</div>\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-vertical-padding --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST4.width-1250.png\" alt=\"RTS2ST4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST4.width-1250.png\" alt=\"RTS2ST4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n\n  \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n    \n\n\n\n\n\n<div class=\"sound-player --theme-light --mode-standalone\" data-gt-id=\"sound_player\" data-gt-component-name=\"component name\">\n    <div class=\"sound-player__wrapper\">\n        \n            \n\n\n<div class=\"sound-player__column\">\n    \n\n\n\n\n    \n\n<audio controls=\"\">\n    \n    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/RealTimeS2ST_en_alignment.wav\" type=\"audio/wav\">\n    Your browser does not support the audio element.\n</audio>\n\n    \n        <div class=\"caption --center\">\n            <p data-block-key=\"80ok3\"><i>Text alignment of input and translated audio with corresponding overlaps.</i></p>\n        </div>\n    \n\n    \n</div>\n\n        \n    </div>\n</div>\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"gi1wx\">Invalid overlaps or translations that fail to meet delay requirements are filtered from the training dataset. The remaining aligned data is used to train the streaming S2ST model in chunks of up to 60 seconds. Various audio augmentation techniques are also applied during training, including sample rate reduction, reverberation, saturation, and denoising.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qrznl\">Real-time speech-to-speech translation architecture</h2><p data-block-key=\"38c83\">The end-to-end S2ST model leverages fundamental transformer blocks and consists of two main components:</p><ul><li data-block-key=\"5ra6j\"><i>Streaming encoder:</i> Summarizes source audio data based on the preceding 10 seconds of input.</li><li data-block-key=\"moc\"><i>Streaming decoder:</i> Predicts translated audio autoregressively, using the compressed encoder state and predictions from previous iterations.</li></ul><p data-block-key=\"2g26o\">A feature of these models is their representation of audio as a 2D set of tokens, known as <a href=\"https://arxiv.org/abs/2107.03312\" target=\"_blank\" rel=\"noopener noreferrer\">RVQ audio tokens</a>. As shown below, the X-axis represents time, while the Y-axis represents a set of tokens that describe the current audio segment. When summed, all tokens in a specific set can be readily converted into an audio stream using an ML codec. The number of tokens controls the audio quality for every segment, with more tokens yielding higher fidelity. The model predicts tokens sequentially, prioritizing those at the beginning. Typically, 16 tokens are sufficient for high-quality audio representation of a 100 ms chunk.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST5.width-1250.png\" alt=\"RTS2ST5\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST5.width-1250.png\" alt=\"RTS2ST5\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"b1hy5\"><i>Schematic representation of audio-to-audio streaming inference for S2ST.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qrznl\">The model outputs a single text token in addition to the audio tokens. This text token acts as an extra prior for audio generation and enables direct metric calculation (<a href=\"https://cloud.google.com/translate/docs/advanced/automl-evaluate#bleu\" target=\"_blank\" rel=\"noopener noreferrer\">BLEU</a>) without relying on proxy ASR systems.</p><p data-block-key=\"dh8dl\">During training, a per-token loss is applied to the model to ensure accurate translation. The model's prediction delay, or lookahead, can be adjusted by shifting ground truth tokens to the right, allowing for flexibility based on the target language's complexity. For real-time conversations, a standard 2-second delay is typically used, which is suitable for most languages. While a longer lookahead improves translation quality by providing more context, it negatively impacts the real-time communication experience.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RealTimeS2ST6_Ablation.width-1250.png\" alt=\"RealTimeS2ST6_Ablation\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RealTimeS2ST6_Ablation.width-1250.png\" alt=\"RealTimeS2ST6_Ablation\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"b1hy5\"><i>Ablation of lookahead and corresponding quality of translation for Spanish / English language pair (BLEU, higher is better).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qrznl\">In addition to the internal 2 second delay, the model's inference time contributes to the overall system latency. To minimize this and achieve real-time performance, we implemented several optimization techniques, including hybrid low-bit (<a href=\"https://blog.tensorflow.org/2024/04/faster-dynamically-quantized-inference-with-xnnpack.html\" target=\"_blank\" rel=\"noopener noreferrer\">int8 and int4</a>) quantization and optimized <a href=\"https://arxiv.org/abs/2207.12598\" target=\"_blank\" rel=\"noopener noreferrer\">CFG</a> precomputation.</p><p data-block-key=\"bv7i0\">The examples of translation for different language pairs with developed models with corresponding ground truth (taken from publicly available <a href=\"https://arxiv.org/abs/2201.03713\" target=\"_blank\" rel=\"noopener noreferrer\">CVSS dataset</a>) follow:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    <div class=\"rich-text --theme- --mode-\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <table style=\"border-collapse: collapse; width: 100%;\" border=\"0\">\n<tbody>\n<tr>\n<td style=\"width: 15%;\">\n<p><strong>Direction</strong></p>\n</td>\n<td style=\"width: 24.973%; text-align: center;\">\n<p><strong>Input audio</strong></p>\n</td>\n<td style=\"width: 24.973%; text-align: center;\">\n<p><strong>Translated audio</strong></p>\n</td>\n<td style=\"width: 34%; text-align: center;\">\n<p><strong>Ground truth</strong></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">Spanish to English</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/es_en_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/es_en_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">In coastal areas there is a larger accumulation of water molecules in the air.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">English to Spanish</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_es_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_es_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">Su portaaviones insignia, el Portaviones Formidable fue tocado por un kamikaze sin gravísimas consecuencias.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">German to English</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/de_en_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/de_en_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">The electrician used a piece of aluminum foil to splice the fuse.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\"> English to German</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_de_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_de_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">Margaret versucht mit allen Mitteln, die bevorstehende Katastrophe zu verhindern.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">Italian to English</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/it_en_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/it_en_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">By clicking on the right, on the bell icon, you can activate Pash notifications so that they are updated in real time.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">English to Italian</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_it_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_it_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">La voce popolare parla dei proprietari terrieri, dei mafiosi e dei rappresentanti del Partito conservatore e dei loro nomi, che sono noti a tutti.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">Portuguese to English</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/pt_en_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/pt_en_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">This text is made available under the respective license.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">English to Portuguese</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_pt_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_pt_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">Um homem de camisa azul observa algo projetado na frente dele.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">French to English</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/fr_en_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/fr_en_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">The volunteer firefighters are a full part of our civil security arsenal.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">English to French</span></p>\n</td>\n<td style=\"width: 24.973%;\">\n<p><span style=\"font-weight: 400;\">&nbsp;<audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_fr_source.wav\" controls=\"controls\"></audio></span></p>\n</td>\n<td style=\"width: 24.973%;\">\n<p><span style=\"font-weight: 400;\">&nbsp;<audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_fr_translation.wav\" controls=\"controls\"></audio></span></p>\n</td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">Ce gouvernement et cette majorité portent donc seuls la responsabilité de cette situation.</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n<table class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\" cellspacing=\"0\" cellpadding=\"0\" align=\"center\">\n<tbody>\n<tr>\n<td class=\"tr-caption\" style=\"text-align: center;\">Exemplar bidirectional translation generated by the trained models for English, Spanish, German, Italian, Portuguese, and French languages with corresponding ground truth.</td>\n</tr>\n</tbody>\n</table>\n</div>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qrznl\">Real-world applications</h2><p data-block-key=\"e3d53\">The new end-to-end S2ST technology has been launched in two key areas, highlighting the importance of real-time cross-language communication. It is now available in <a href=\"https://blog.google/products/workspace/google-meet-langauge-translation-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Meet</a> on servers, and as a built-in <a href=\"https://store.google.com/intl/en/ideas/articles/pixel-live-translate/\" target=\"_blank\" rel=\"noopener noreferrer\">on-device feature</a> for the new Pixel 10 devices. Although the products utilize different strategies for running the S2ST pipeline, they share training data and model architecture. The Pixel Voice Translate on-device feature also employs a cascade approach to maximize language coverage. To mitigate potential feature misuse, prior to each translation session, we inform the end-user that the translation is synthetically generated.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"hyXqcsWOONo\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=hyXqcsWOONo\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"5y2ft\"><i>The new end-to-end S2ST technology enables Google Meet speech translation feature.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"k3ba8\">The current end-to-end model delivers robust performance for five Latin-based language pairs (English to and from Spanish, German, French, Italian, Portuguese), enabling our initial product launches. We are also observing promising capabilities in other languages, such as Hindi, that we plan to develop further. Future enhancements will focus on improving the dynamism of the model's lookahead. This will enable the S2ST technology to seamlessly adjust to languages with word orders significantly different from English, facilitating more contextual rather than literal word-for-word translation.</p><p data-block-key=\"e0dg1\">We believe that this breakthrough in S2ST technology will revolutionize real-time, cross-language communication, turning a long-envisioned concept into reality.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"k3ba8\">Acknowledgements</h2><p data-block-key=\"f31cm\"><i>We are sincerely grateful to everyone who contributed to this project; their critical contributions were instrumental in making it a reality. We are particularly thankful to our colleagues Kevin Kilgour, Pen Li, Félix de Chaumont Quitry, Michael Dooley, Jeremy Thorpe, Mihajlo Velimirović, Alex Tudor, Christian Frank, Daniel Johansson, Hanna Silén, Christian Schuldt, Henrik Lundin, Esbjörn Dominique, Marcus Wirebrand, Daniel Kallander, Pablo Barrera González, Huib Kleinhout, Niklas Blum, Fredric Lindstrom, Esha Uboweja, Karthik Raveendran, Frédéric Rechtenstein, Xing Li, Queenie Zhang, Cheng Yang, Jason Fan, Matsvei Zhdanovich, Jianing Wei, and Matthias Grundmann.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Generative UI: A rich, custom, visual interactive user experience for any prompt",
      "link": "https://research.google/blog/generative-ui-a-rich-custom-visual-interactive-user-experience-for-any-prompt/",
      "pubDate": "Mon, 17 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-17T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Generative UI: A rich, custom, visual interactive user experience for any prompt",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GenUI-0b-Hero.width-1250.png",
          "alt": "A collage showing three different AI-generated user interfaces. They include a page for \"Tailored Fashion Advice\", a science education screen featuring 'Fractals', and a children's learning game interface with math practice options.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GenUI-4-Overview.width-1250.png",
          "alt": "Flowchart illustrating the process for generating a web page. The LLM takes input from the user prompt taking into account underlying system instructions, and utilizes Tools where needed. It outputs HTML/CSS/JS to the user's browser.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GenUI-0-Hero.width-1250.png",
          "alt": "Three teal-themed web design concepts displayed side-by-side: \"Clash of the Titans,\" \"Your Perfect Pizza Night,\" and \"Fabulous Flamingo Flair.\"",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"j0qlf\">Generative UI is a powerful capability in which an AI model generates not only content but an entire user experience. Today we introduce a novel implementation of generative UI, which dynamically creates immersive visual experiences and interactive interfaces — such as web pages, games, tools, and applications — that are automatically designed and fully customized in response to any question, instruction, or prompt. These prompts can be as simple as a single word, or as long as needed for detailed instructions. These new types of interfaces are markedly different from the static, predefined interfaces in which AI models typically render content.</p><p data-block-key=\"f5456\">In our new paper, “<a href=\"https://generativeui.github.io/static/pdfs/paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Generative UI: LLMs are Effective UI Generators</a>”, we describe the core principles that enabled our implementation of generative UI and demonstrate the effective viability of this new paradigm. Our evaluations indicate that, when ignoring generation speed, the interfaces from our generative UI implementations are strongly preferred by human raters compared to standard LLM outputs. This work represents a first step toward fully AI-generated user experiences, where users automatically get dynamic interfaces tailored to their needs, rather than having to select from an existing catalog of applications.</p><p data-block-key=\"9kn4n\">Our research on generative UI, also referred to as generative interfaces, comes to life today in the <a href=\"https://blog.google/products/gemini/gemini-3-gemini-app\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini app</a> through an experiment called dynamic view and in AI Mode in <a href=\"http://blog.google/products/search/gemini-3-search-ai-mode\" target=\"_blank\" rel=\"noopener noreferrer\">Google Search</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GenUI-0b-Hero.width-1250.png\" alt=\"A collage showing three different AI-generated user interfaces. They include a page for &quot;Tailored Fashion Advice&quot;, a science education screen featuring 'Fractals', and a children's learning game interface with math practice options.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GenUI-0b-Hero.width-1250.png\" alt=\"A collage showing three different AI-generated user interfaces. They include a page for &quot;Tailored Fashion Advice&quot;, a science education screen featuring 'Fractals', and a children's learning game interface with math practice options.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"do5os\"><i>Generative UI is useful for a range of applications. For any user question, need, or prompt, as simple as a single word or as complex as elaborate instructions, the model creates a fully custom interface.</i> <b><i>Left:</i></b> <a href=\"https://generativeui.github.io/static/demos/carousel.html?result=fashion-advisor\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Getting tailored fashion advice</i></a><i>.</i> <b><i>Middle:</i></b> <a href=\"https://generativeui.github.io/static/demos/carousel.html?result=fractal-explorer\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Learning about fractals</i></a><i>.</i> <b><i>Right:</i></b> <a href=\"https://generativeui.github.io/static/demos/carousel.html?result=basketball-math\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Teaching mathematics</i></a><i>.</i></p><p data-block-key=\"4ukk0\"><i>For more examples see the</i> <a href=\"https://generativeui.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>project page</i></a><i>.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Bringing generative UI to Google products</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"j0qlf\">Generative UI capabilities will be rolled out as two experiments in the <a href=\"https://blog.google/products/gemini/gemini-3-gemini-app\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini app</a>: dynamic view and visual layout. When using dynamic view, an experience built upon our generative UI implementation, Gemini designs and codes a fully customized interactive response for each prompt, using Gemini’s agentic coding capabilities. It customizes the experience with an understanding that explaining the microbiome to a 5 year old requires different content and a different set of features than explaining it to an adult, just as creating a gallery of social media posts for a business requires a completely different interface to generating a plan for an upcoming trip.</p><p data-block-key=\"cvpbe\">Dynamic view can be used for a wide range of scenarios, from learning about <a href=\"https://generativeui.github.io/static/demos/carousel.html?result=rolling-an-8\" target=\"_blank\" rel=\"noopener noreferrer\">probability</a> to helping in practical tasks like <a href=\"https://generativeui.github.io/static/demos/carousel.html?result=thanksgiving\" target=\"_blank\" rel=\"noopener noreferrer\">event planning</a> and getting <a href=\"https://generativeui.github.io/static/demos/carousel.html?result=fashion-advisor\" target=\"_blank\" rel=\"noopener noreferrer\">fashion advice</a>. The interfaces allow users to learn, play or explore interactively. Dynamic view, along with visual layout, are rolling out today. To help us learn about these experiments, users may initially see only one of them.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Dynamic_View_Van_Gogh_1920x1080.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"vu0ul\">Example of generative UI in dynamic view based on the prompt, “Create a Van Gogh gallery with life context for each piece”.</p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"j0qlf\">Generative UI experiences are also integrated into <a href=\"http://blog.google/products/search/gemini-3-search-ai-mode\" target=\"_blank\" rel=\"noopener noreferrer\">Google Search</a> starting with AI Mode, unlocking dynamic visual experiences with interactive tools and simulations that are generated specifically for a user’s question. Now, thanks to Gemini 3’s unparalleled multimodal understanding and powerful agentic coding capabilities, Gemini 3 in AI Mode can interpret the intent behind any prompt to instantly build bespoke generative user interfaces. By generating interactive tools and simulations on the fly, it creates a dynamic environment optimized for deep comprehension and task completion. Generative UI capabilities in AI Mode are available for Google AI Pro and Ultra subscribers in the U.S. starting today. Select \"Thinking\" from the model drop-down menu in AI Mode to try it out.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/AIM-CAPYBARA-RNA-1920x1080-Under20MB.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"3jqbs\">Example of AI Mode in Google Search with the prompt, “show me how rna polymerase works. what are the stages of transcription and how is it different in prokaryotic and eukaryotic cells”.</p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How the generative UI implementation works</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"j0qlf\">Our generative UI implementation, described in the <a href=\"https://generativeui.github.io/static/pdfs/paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>, uses Google’s Gemini 3 Pro model with three important additions:</p><ol><li data-block-key=\"appno\"><i>Tool access</i>: A server provides access to several key tools, like image generation and web search. This allows the results to be made accessible to the model to increase quality or sent directly to the user’s browser to improve efficiency.</li><li data-block-key=\"br80m\"><i>Carefully crafted system instructions</i>: The system is guided by detailed instructions that include the goal, planning, examples and technical specifications, including formatting, tool manuals, and tips for avoiding common errors.</li><li data-block-key=\"cqq2h\"><i>Post-processing</i>: The model’s outputs are passed through a set of post-processors to address potential common issues.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GenUI-4-Overview.width-1250.png\" alt=\"Flowchart illustrating the process for generating a web page. The LLM takes input from the user prompt taking into account underlying system instructions, and utilizes Tools where needed. It outputs HTML/CSS/JS to the user's browser.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GenUI-4-Overview.width-1250.png\" alt=\"Flowchart illustrating the process for generating a web page. The LLM takes input from the user prompt taking into account underlying system instructions, and utilizes Tools where needed. It outputs HTML/CSS/JS to the user's browser.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"do5os\"><i>A high-level system overview of the generative UI implementation.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"j0qlf\">For some products, it might be preferable to consistently see results in specific styles. Our implementation could be configured for these products so that all results, including generated assets, are created in a consistent style for all users. Without specific styling instructions, the generative UI will select a style automatically, or the user can influence styling in their prompt, as in the case of dynamic view in the Gemini app.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GenUI-0-Hero.width-1250.png\" alt=\"Three teal-themed web design concepts displayed side-by-side: &quot;Clash of the Titans,&quot; &quot;Your Perfect Pizza Night,&quot; and &quot;Fabulous Flamingo Flair.&quot;\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GenUI-0-Hero.width-1250.png\" alt=\"Three teal-themed web design concepts displayed side-by-side: &quot;Clash of the Titans,&quot; &quot;Your Perfect Pizza Night,&quot; and &quot;Fabulous Flamingo Flair.&quot;\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"do5os\"><i>Screenshots of generative UI results with consistent “Wizard Green” styling.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Generative UI outputs are strongly preferred over standard formats</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"j0qlf\">To facilitate consistent evaluations and comparisons of generative UI implementations, we created PAGEN, a dataset of human expert–made websites and will soon be releasing it to the research community.</p><p data-block-key=\"7bgiq\">To evaluate user preferences, we compared our new generative UI experience against various different formats: a website designed for a specific prompt by human-experts, the top Google Search result for the query, and baseline LLM outputs in raw text or the standard markdown formats.</p><p data-block-key=\"64aut\">The sites designed by human experts had the highest preference rates. These were followed closely by the results from our generative UI implementation, with a substantial gap from all other output methods. This evaluation did not take into account generation speed. We also show that the performance of generative UI strongly depends on the performance of the underlying model, and that our newest models perform substantially better. See more details in the <a href=\"https://generativeui.github.io/static/pdfs/paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Opportunities ahead</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"j0qlf\">We are still in the early days of generative UI, and important opportunities for improvement remain. For example, our current implementation can sometimes take a minute or more to generate results, and there are occasional inaccuracies in the outputs; these are areas of ongoing research. Generative UI is an example of the <a href=\"https://blog.google/technology/research/google-research-team-tackles-big-challenges-with-science/\" target=\"_blank\" rel=\"noopener noreferrer\">magic cycle of research</a>, where research breakthroughs lead to product innovation that opens up new opportunities for addressing user needs and in turn fuel further research. We see potential in extending generative UI to access a wider set of services, adapt to additional context and human feedback, and deliver increasingly more helpful visual and interactive interfaces. We are excited about the further opportunities ahead for generative UI.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "利用人工智能区分天然林与其他林木覆盖，助力无砍伐供应链 (原标题: Separating natural forests from other tree cover with AI for deforestation-free supply chains)",
      "link": "https://research.google/blog/separating-natural-forests-from-other-tree-cover-with-ai-for-deforestation-free-supply-chains/",
      "pubDate": "Wed, 12 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-12T16:00:00.000Z",
      "creator": "Google",
      "summary": "森林对地球至关重要，它们调节降雨、缓解洪水、储存碳并维持大部分陆地物种。然而，森林砍伐仍在以惊人的速度进行。在保护工作中，一个关键挑战是利用卫星数据区分拥有数百年历史的天然生态系统与新种植的森林或林木作物种植园。大多数现有地图仅显示“林木覆盖”，这是一种对任何木本植被的基本测量，导致“混淆视听”，将短期种植园的采伐与不可替代、生物多样性丰富的天然林的永久性损失混为一谈。\n\n### 区分天然林的重要性\n\n对天然林进行区分比以往任何时候都更加重要，原因如下：\n\n*   **新全球法规**：例如欧盟无砍伐产品法规（EUDR），该法规要求在欧盟销售的咖啡、可可、橡胶、木材和棕榈油等产品不得来自2020年12月31日之后被砍伐或退化的土地。其目标是保护原始森林和自然再生林等天然林。\n*   **政策需求**：这项政策催生了对2020年天然林可靠、高分辨率和全球一致地图的需求。\n*   **气候共识**：保护这些森林也是COP30的核心支柱，会议认识到它们在气候稳定和人类福祉中的关键作用。\n\n### “2020年世界天然林”地图\n\n为了满足这一需求，Google DeepMind与Google Research合作，并与世界资源研究所（WRI）和国际应用系统分析研究所（IIASA）协作，发布了《2020年世界天然林》这一新的地图和数据集，并发表在《自然科学数据》杂志上。该项目为森林砍伐和退化监测提供了关键基线。\n\n*   **核心特点**：这是首个全球一致的10米分辨率地图，能够将天然林与其他人造林木覆盖区分开来。\n*   **准确性**：经全球独立数据集验证，该地图达到了92.2%的同类最佳准确率。\n*   **用途**：我们希望这一公开可用的基线能帮助企业进行尽职调查，支持政府监测森林砍伐，并赋能保护组织将其努力集中于保护最重要的区域。\n\n![天然林与人工林](https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-1.width-1250.png)\n\n*   Gemini生成的图像显示天然林（左）与人工林（右）接壤。全球卫星模型难以区分它们，使保护生物多样性更丰富的天然林的工作复杂化。\n\n![2020年全球天然林分布](https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-2.width-1250.png)\n\n*   2020年全球天然林的范围（原始分辨率为10米）。\n\n### AI如何区分森林类型\n\n利用单一卫星图像区分天然林与复杂的农林复合系统或拥有50年历史的人工林非常困难。为了克服这一挑战，我们开发了一个AI模型，它像林业专家一样，观察一块土地一年内的变化，分割1280 x 1280米的区域，并估计其中每个10 x 10米像素是天然林的可能性。这使得模型能够根据周围环境而不是单一快照进行评估。\n\n*   **模型技术**：这种新颖的多模态时空视觉Transformer（MTSViT）模型分析季节性Sentinel-2卫星图像和地形数据（例如海拔和坡度），以及样本的地理坐标。\n*   **识别机制**：通过观察卫星图像随时间的变化，模型识别出独特的谱、时序和纹理特征（即用于识别不同森林类型的数据模式），从而区分复杂的天然林与均匀、快速生长的商业种植园以及其他土地利用和土地覆盖类型。\n\n### 地图生成流程\n\n为了构建《2020年世界天然林》地图，我们采样了全球超过120万个1280 x 1280米、10米分辨率的区域，创建了一个大规模、多源的训练数据集。我们利用这些数据训练MTSViT模型识别天然林和其他土地类型的复杂模式。然后，我们将训练好的MTSViT模型应用于地球上所有陆地，生成了一张无缝、全球一致的10米概率地图。为了严格验证地图，我们通过重新利用一个专注于2015年全球森林管理的独立数据集，并将其标签更新至2020年，创建了一个评估数据集。\n\n![天然林地图生成工作流程](https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-3.width-1250.png)\n\n*   天然林地图生成的端到端工作流程（标注数据生成、处理、模型训练、地图生成和验证步骤）。\n\n### 未来展望：森林理解的新愿景\n\n我们希望《2020年世界天然林》基线能成为政策制定者、审计师和寻求遵守EUDR等新无砍伐法规的公司宝贵资源。但森林并非静态。为了真正支持全球保护和可持续发展，我们需要区分更多类别的森林，并关键地了解它们如何随时间变化。这包括区分和定位关键森林类型：天然林（碳密集和生物多样性丰富的森林）、人工林、种植园和商业林木作物（如生态友好的咖啡和可可农林复合系统）。\n\n*   **新一代地图**：为了推进这项工作，我们正在开发一系列新的多年度全球森林类型地图，由下一代AI模型驱动。这些地图将把世界陆地分为六种不同类型：原始森林、自然再生林、人工林、种植园林、林木作物和其他土地覆盖。我们预计在2026年发布这些综合地图。\n*   **开放数据集**：为了鼓励更广泛的研究社区为此努力做出贡献，我们还发布了两个大规模基准数据集：\n    *   **Planted数据集**：一个全球、多传感器、长时间序列的集合，包含超过230万个时间序列分类示例。它专门设计用于帮助AI模型识别全球64种不同（物种或属）类型的人工林和林木作物。\n    *   **Forest Typology (ForTy) 基准**：提供了一个真正全球规模的数据集，包含20万个多源、多时序图像块，带有像素级标签，用于核心任务（天然林、人工林和林木作物）的语义分割。\n\n### 助力保护地球\n\n将气候和自然雄心转化为行动需要透明、可信和高分辨率的数据。我们致力于使这些工具尽可能易于获取。我们希望这些新的数据集和工具能帮助政府、企业和社区共同努力，实现其无砍伐目标，并保护我们赖以生存的关键生态系统。\n\n**致谢**：这项研究由Google Deepmind和Google Research与WRI和IIASA共同开发。",
      "shortSummary": "Google DeepMind等机构发布了“2020年世界天然林”地图，利用AI技术以10米分辨率在全球范围内区分天然林与其他人造林木覆盖。该地图准确率达92.2%，旨在为企业遵守欧盟无砍伐产品法规（EUDR）等新规提供关键基线数据。通过分析卫星图像和地形数据，AI模型能识别天然林的复杂模式。未来将推出更详细的森林类型地图和开放数据集，以促进研究，共同推动全球森林保护和可持续发展。",
      "translated_title": "利用人工智能区分天然林与其他林木覆盖，助力无砍伐供应链",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-1.width-1250.png",
          "alt": "Natural Forests-1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-2.width-1250.png",
          "alt": "Natural Forests-2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-3.width-1250.png",
          "alt": "Natural Forests-3",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"1h9b3\">Forests are vital for our planet as they regulate rainfall, mitigate floods, store and sequester carbon, and help sustain the majority of the <a href=\"https://zenodo.org/records/6417333\" target=\"_blank\" rel=\"noopener noreferrer\">planet’s land-based species</a>. Despite their importance, deforestation continues at an alarming rate. A key challenge in conservation efforts is differentiating centuries-old natural ecosystems from newly planted forests or tree crop plantations with satellite data. Most existing maps simply show \"tree cover,\" a basic measure of any woody vegetation, leading to an \"apples-to-oranges\" comparison. This conflates the harvesting of a short-term plantation with the permanent loss of an irreplaceable, biodiversity-rich natural forest.</p><p data-block-key=\"dg5d6\">The need for this distinction is more important than ever due to new global regulations, like the <a href=\"https://environment.ec.europa.eu/topics/forests/deforestation/regulation-deforestation-free-products_en\" target=\"_blank\" rel=\"noopener noreferrer\">European Union Regulation on Deforestation-free Products</a> (EUDR). This regulation mandates that products like coffee, cocoa, rubber, timber, and palm oil sold in the EU cannot come from land that was deforested or degraded after December 31, 2020, with the goal of protecting natural forests, like primary and naturally regenerating forests. This policy creates a need for a reliable, high-resolution, and globally-consistent map of natural forests as they existed in 2020. The protection of these forests is also a central pillar for <a href=\"https://unfccc.int/cop30\" target=\"_blank\" rel=\"noopener noreferrer\">COP30</a>, which recognizes their crucial role in climate stability and human well-being.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-1.width-1250.png\" alt=\"Natural Forests-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-1.width-1250.png\" alt=\"Natural Forests-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"2ra3h\"><i>Gemini generated image showing natural forest (</i><b><i>left</i></b><i>) bordering a planted forest (</i><b><i>right</i></b><i>). Global satellite-based models struggle to distinguish between them, complicating efforts to protect the more biodiversity-rich natural forest.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"1h9b3\">In an effort to help meet this need, together with <a href=\"https://deepmind.google/\" target=\"_blank\" rel=\"noopener noreferrer\">Google DeepMind</a>, we’re releasing <a href=\"https://plus.figshare.com/articles/dataset/Natural_forests_of_the_world_2020_-_probability_maps/28881731\" target=\"_blank\" rel=\"noopener noreferrer\">Natural Forests of the World 2020</a>, a new map and dataset, published in <a href=\"https://www.doi.org/10.1038/s41597-025-06097-z\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature Scientific Data</i></a>. This project stems from a collaboration with the <a href=\"https://www.wri.org/\" target=\"_blank\" rel=\"noopener noreferrer\">World Resources Institute</a> and the <a href=\"https://iiasa.ac.at/\" target=\"_blank\" rel=\"noopener noreferrer\">International Institute for Applied Systems Analysis</a>, and provides a critical baseline for deforestation and degradation monitoring. We provide the first globally consistent, <a href=\"https://developers.google.com/earth-engine/datasets/catalog/projects_nature-trace_assets_forest_typology_natural_forest_2020_v1_0_collection\" target=\"_blank\" rel=\"noopener noreferrer\">10-meter resolution map</a> that differentiates natural forests from other tree cover and achieves a best-in-class accuracy of 92.2% when validated against a <a href=\"https://figshare.com/articles/dataset/Natural_forests_of_the_world_2020_-_validation_dataset/30051517?file=57664081\" target=\"_blank\" rel=\"noopener noreferrer\">global independent dataset</a>. We hope that this publicly available baseline can help companies conduct due diligence, support governments in monitoring deforestation, and empower conservation groups to target their efforts to protect what matters most.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-2.width-1250.png\" alt=\"Natural Forests-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-2.width-1250.png\" alt=\"Natural Forests-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"2ra3h\"><i>The global extent of natural forests in 2020 (originally at 10-meter resolution).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How AI can separate the forest from the trees</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1h9b3\">Distinguishing a natural forest from a complex agroforestry system or a 50-year-old planted forest is difficult using a single satellite image. To overcome this, we developed an AI model that acts like a forester, observing a patch of land over the course of a year, segmenting a 1280 x 1280 meter patch and estimating the likelihood that each 10 x 10 meter pixel within it is a natural forest. This allows the model to make assessments based on the surrounding context, rather than a single snapshot. This novel <i>multi-modal temporal-spatial</i> <a href=\"https://en.wikipedia.org/wiki/Vision_transformer\" target=\"_blank\" rel=\"noopener noreferrer\"><i>vision transformer</i></a> (MTSViT) model analyzes seasonal <a href=\"https://sentinels.copernicus.eu/web/sentinel/copernicus/sentinel-2\" target=\"_blank\" rel=\"noopener noreferrer\">Sentinel-2</a> satellite imagery and topographical data (e.g., elevation and slope), along with the sample’s geographical coordinate. By observing satellite imagery over time, the model identifies distinct spectral, temporal, and texture signatures (i.e., data patterns used to recognize different forest types) that differentiate complex, natural forests from uniform, fast-growing commercial plantations and other land use and land cover.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/natural_forests_video_im_560p_lq.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"z7b76\">To build the Natural Forests of the World 2020 map, we sampled over 1.2 million global 1280 x 1280 meter patch locations at 10-meter resolution to create a massive, multi-source training dataset. We used this data to train the MTSViT model to recognize complex patterns of natural forests and other land types. We then applied the trained MTSViT model across all land on Earth, generating a seamless, globally consistent 10-meter probability map. To rigorously validate the map, we created an <a href=\"https://figshare.com/articles/dataset/Natural_forests_of_the_world_2020_-_validation_dataset/30051517?file=57664081\" target=\"_blank\" rel=\"noopener noreferrer\">evaluation dataset</a> by repurposing an <a href=\"https://www.nature.com/articles/s41597-022-01332-3\" target=\"_blank\" rel=\"noopener noreferrer\">independent dataset</a> focused on global forest management for 2015 and updating its labels to focus on natural forests for 2020. See more details in the <a href=\"https://doi.org/10.1038/s41597-025-06097-z\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-3.width-1250.png\" alt=\"Natural Forests-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-3.width-1250.png\" alt=\"Natural Forests-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"2ra3h\"><i>End-to-end workflow of the Natural Forests map generation (annotating data generation, processing, model training, map generation, and validation steps).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">What's next: A new vision for forest understanding</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1h9b3\">We hope that the Natural Forests of the World 2020 baseline proves to be a valuable resource for policymakers, auditors, and companies seeking to comply with new deforestation-free regulations such as the EUDR. But forests are not static. To truly support global conservation and sustainability, we need to distinguish between more classes of forest and, crucially, understand how they change over time. This involves differentiating between and locating key forest types: natural forests (carbon-dense and biodiversity-rich forests), planted forests, plantations, and commercial tree crops (such as ecosystem-friendly coffee and cocoa agroforestry systems).</p><p data-block-key=\"e8q12\">To advance this effort, we’re developing a new multi-year series of global forest type maps, powered by next-generation AI models. These maps will categorize the world's land into six distinct types: Primary Forest, Naturally Regenerating Forest, Planted Forest, Plantation Forest, Tree Crops, and Other Land Cover. We expect to release these comprehensive maps in 2026.</p><p data-block-key=\"2b9sn\">To encourage the broader research community to contribute to this effort, we have also released two large-scale benchmark datasets. These datasets are important for developing and rigorously testing the next generation of AI models designed to analyze the world’s forests. The <a href=\"https://arxiv.org/abs/2406.18554\" target=\"_blank\" rel=\"noopener noreferrer\">Planted dataset</a> is a global, multi-sensor long-temporal collection featuring over 2.3 million time-series classification examples. It is specifically designed to help AI models recognize 64 different (species or genera) types of planted forests and tree crops worldwide. The <a href=\"https://arxiv.org/abs/2505.01805\" target=\"_blank\" rel=\"noopener noreferrer\">Forest Typology</a> (ForTy) benchmark provides a truly global-scale dataset with 200,000 multi-source and multi-temporal image patches with per-pixel labels for semantic segmentation models. This resource is tailored for the core task of mapping the key classes: natural forest, planted forest, and tree crops.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Helping to protect our planet</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1h9b3\">Turning climate and nature ambitions into action requires transparent, trusted, and high-resolution data. We are committed to making these tools as accessible as possible. We hope these new datasets and tools will help governments, companies, and communities work together to meet their deforestation-free goals and protect the critical ecosystems on which we all depend.</p><p data-block-key=\"159ar\">Learn more about our AI and sustainability efforts by checking out <a href=\"https://blog.google/technology/research/new-updates-and-more-access-to-google-earth-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Earth AI</a>, <a href=\"https://cloud.google.com/blog/topics/sustainability/look-back-at-a-year-of-earth-engine-advancements\" target=\"_blank\" rel=\"noopener noreferrer\">Google Earth Engine</a>, and <a href=\"https://deepmind.google/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/\" target=\"_blank\" rel=\"noopener noreferrer\">AlphaEarth Foundations</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1h9b3\"><i>This research was co-developed by Google Deepmind and Google Research in collaboration with WRI and IIASA.</i></p><p data-block-key=\"1fca\"><i>We thank our collaborators at Google, World Resources Institute (WRI) / Global Forest Watch (GFW), and International Institute for Applied Systems Analysis (IIASA): Anton Raichuk, Charlotte Stanton, Dan Morris, Drew Purves, Elizabeth Goldman, Katelyn Tarrio, Keith Anderson, Maxim Neumann, Mélanie Rey, Michelle J. Sims, Myroslava Lesiv, Nicholas Clinton, Petra Poklukar, Radost Stanimirova, Sarah Carter, Steffen Fritz, Yuchang Jiang.</i></p><p data-block-key=\"ftoen\"><i>Special thanks to early map reviewers: Andrew Lister (United States Forest Service), Astrid Verheggen (Joint Research Centre), Clement Bourgoin (Joint Research Centre), Erin Glen (WRI), Frederic Achard (Joint Research Centre), Jonas Fridman (Swedish University of Agricultural Sciences), Jukka Meiteninen (VTT), Karen Saunders (World Wildlife Fund Canada), Louis Reymondin (Alliance Bioversity International - CIAT), Martin Herold (GFZ Helmholtz Centre for Geosciences), Olga Nepomshina (GFZ Helmholtz Centre for Geosciences), Peter Potapov (University of Maryland/WRI), Rene Colditz (Joint Research Centre), Thibaud Vantalon (Alliance Bioversity International - CIAT), and Viviana Zalles (WRI).</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "一种用于优化的新型量子工具包 (原标题: A new quantum toolkit for optimization)",
      "link": "https://research.google/blog/a-new-quantum-toolkit-for-optimization/",
      "pubDate": "Wed, 12 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-12T16:00:00.000Z",
      "creator": "Google",
      "summary": "优化问题无处不在，从设计更高效的航班路线到组织临床试验，但即使是最强大的经典超级计算机也难以找到最佳解决方案。量子计算能否在经典计算机力所不能及的优化问题上取得成功，是一个长达数十年的重大未解之谜。\n\n## 解码量子干涉（DQI）算法\n\nGoogle Quantum AI 的研究人员与斯坦福、麻省理工学院和加州理工学院的合作者在《自然》杂志上发表了一篇论文，介绍了一种名为“解码量子干涉”（Decoded Quantum Interferometry, DQI）的高效量子算法。DQI 利用量子力学的波状特性创建干涉模式，从而收敛到经典计算机难以找到的近似最优解。\n\n## 优化与解码的量子联系\n\nDQI 的关键在于，为了构建必要的干涉模式，必须解决另一个计算难题——解码问题。解码问题是指给定一个格点和空间中的一个点，需要找到离该点最近的格点元素。虽然二维方格的解码问题很简单，但在数百或数千维的某些格点上，它会变得非常困难。\n\n幸运的是，解码问题在过去几十年中得到了广泛研究，主要应用于数据存储或传输中的错误纠正。许多复杂而强大的算法已被设计出来，用于解决各种特殊结构格点的解码问题。研究人员发现，对于某些类型的优化问题，相关的解码问题具有正确的结构，可以被这些强大的解码算法解决。然而，只有通过量子计算的力量，这些解码算法才能被利用来解决优化问题。通过将 DQI 的量子干涉与这些复杂的解码算法相结合，足够大的量子计算机可以找到这些优化问题的近似解——这些解似乎超出了任何已知经典方法的范围。\n\n这一关于量子算法的数学发现为优化提供了加速，加深了我们对量子计算机最终用例的理解。当量子计算硬件足够先进时，研究人员可以使用 DQI 算法来解决经典上具有挑战性的优化问题。\n\n![DQI1_Animation](https://storage.googleapis.com/gweb-research2023-media/images/DQI1_Animation.width-800.gif)\n\n*图示：将崎岖成本函数景观上的优化问题转换为周期性格点的解码问题的形象化表示。*\n\n## 明确的量子优势：最优多项式交集（OPI）\n\n在这项工作中，DQI 在一个名为“最优多项式交集”（Optimal Polynomial Intersection, OPI）的问题上取得了最佳结果。OPI 问题是指给定一系列目标点，通过调整一个度数低于点数的低次多项式的系数，使其尽可能多地与这些目标点相交。这是数据科学中常见的任务，被称为多项式回归，其变体也出现在数字纠错和密码学中。尽管某些特殊情况下有复杂的算法，但在其他情况下，使用已知经典算法解决该问题仍然极其困难。\n\n使用 DQI，量子计算机可以将 OPI 转换为里德-所罗门码（广泛应用于 DVD 和二维码的编码家族）的解码问题。由于里德-所罗门码的解码算法已经非常成熟，因此使用 DQI 的量子计算机可以比已知经典算法找到更好的 OPI 问题近似最优解。例如，分析表明，某些 OPI 问题的例子可以通过量子计算机仅使用数百万次基本量子逻辑操作来解决，而使用最有效的已知经典算法在传统经典计算机上解决则需要超过 10^23 次基本操作。\n\n![DQI2_OPI](https://storage.googleapis.com/gweb-research2023-media/images/DQI2_OPI.width-1250.png)\n\n*图示：OPI 问题的插图：目标是找到一个低次多项式，使其与尽可能多的目标集合相交。图中显示的多项式 f 和 g 尽管非常不同，但都与三个目标集合相交，因此目标值相同。这种遥远解达到相同目标值的现象是使用传统计算机难以解决该问题的原因之一。*\n\n## 量子优势的来源\n\n优化问题和解码问题都属于 NP-hard 问题，这意味着即使有量子计算机的帮助，也无法高效地找到所有实例的精确解。DQI 将一个难题转换为另一个难题，其关键在于 NP-hard 描述的是给定问题中最难实例的难度。如果问题实例被限制具有某种附加结构，这可以使它们变得更容易。DQI 的承诺是，某些类型的结构可以使解码问题变得更容易，而不会使优化问题更容易用传统计算机解决。\n\n在 OPI 问题中，出现的格点具有代数结构；基向量的分量不是任意的，而是通过将一个数提升到连续更高的幂次获得的。这种代数结构反映在原始优化问题（OPI）和量子计算机可以将其转换成的解码问题（里德-所罗门解码）中。这种结构使得解码问题更容易，但据我们所知，并没有使优化问题对传统计算机更容易。在这种情况下，利用量子计算的力量将优化问题转换为解码问题的能力提供了优势。\n\n## 随机稀疏优化问题：一个诱人的挑战\n\n论文中还考虑了缺乏代数结构但基向量稀疏（即大部分为零）的更通用格点。相应的优化问题称为 max-k-XORSAT，如下图所示。格点的稀疏性体现在每个约束只涉及少数变量（最多 k 个）。在 max-k-XORSAT 中，约束多于变量，不可能全部满足。相反，目标是找到一个满足尽可能多约束的解。尽管听起来抽象，max-k-XORSAT 问题常被用作新优化算法的测试平台，并包含许多其他知名优化问题的特例，如 max-cut 和 QUBO。\n\n![DQI3_Example](https://storage.googleapis.com/gweb-research2023-media/images/DQI3_Example.width-1250.png)\n\n*图示：一个 k=2 的 max-k-XORSAT 问题示例，它有 4 个变量和 5 个约束。通过将 A、B、C、D 分配为 0 或 1，你能满足多少个约束？*\n\nDQI 可以将 max-k-XORSAT 转换为由稀疏矩阵定义的码的解码问题，这些码被称为低密度奇偶校验（LDPC）码。20世纪60年代发现稀疏性使解码问题变得更容易。然而，原始 max-k-XORSAT 问题的稀疏性也使其更容易通过模拟退火等算法在传统计算机上解决。因此，很难找到稀疏性恰到好处的 max-k-XORSAT 问题，使得解码器比我们比较的模拟退火算法更有帮助。论文中提出了一个稀疏性恰到好处的示例问题，DQI 似乎比模拟退火具有速度优势。然而，研究人员使用为该示例量身定制的专用算法在传统计算机上高效地解决了这个问题。因此，目前与 OPI 不同，我们还没有一个 max-k-XORSAT 问题的例子，既能被 DQI 解决，又不能被任何已知在传统计算机上运行的算法高效解决。\n\n由于稀疏优化问题具有广泛的实际应用，研究人员将继续探索 DQI 在稀疏优化问题上实现量子优势的方法。特别是，DQI 已经激发了对 LDPC 码解码的经典和量子算法的新研究方向。\n\n## 前景\n\nDQI 算法为开发量子优化算法提供了一个强大的新工具包。这种将优化问题转化为解码问题的方法为解决该领域长期存在的问题之一提供了一条新途径。研究人员期待看到 Google 和更广泛社区的科学家们将如何利用这些工具。",
      "shortSummary": "Google Quantum AI 推出了一种名为“解码量子干涉”（DQI）的新型量子算法，旨在解决传统计算机难以处理的优化问题。DQI 通过将优化问题转化为解码问题，并结合量子干涉与复杂的经典解码算法来实现。对于“最优多项式交集”（OPI）问题，DQI 展示了显著的量子加速。尽管在稀疏优化问题（如 max-k-XORSAT）上仍需进一步探索以证明其对所有已知经典算法的明确优势，DQI 仍代表着量子优化领域的一个强大新工具包。",
      "translated_title": "一种用于优化的新型量子工具包",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DQI1_Animation.width-800.gif",
          "alt": "DQI1_Animation",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DQI2_OPI.width-1250.png",
          "alt": "DQI2_OPI",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DQI3_Example.width-1250.png",
          "alt": "DQI3_Example",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"16fxo\">From designing more efficient airline routes to organizing clinical trials, optimization problems are everywhere. Yet for many real-world challenges, even our most powerful supercomputers struggle to find the best solution. This has led to a major, decades-long question in quantum computing: could quantum machines succeed on optimization problems where classical ones fall short? This has proven to be a very difficult mathematical question, which remains largely open. As the capabilities of quantum hardware <a href=\"https://research.google/blog/making-quantum-error-correction-work/\">undergo</a> <a href=\"https://research.google/blog/a-colorful-quantum-future/\">rapid</a> <a href=\"https://research.google/blog/a-new-hybrid-platform-for-quantum-simulation-of-magnetism/\">advancement</a>, such theoretical problems of working out the eventual commercial and scientific use cases of large-scale error-corrected quantum computers become only more urgent.</p><p data-block-key=\"5a78j\">In <a href=\"https://www.nature.com/articles/s41586-025-09527-5\" target=\"_blank\" rel=\"noopener noreferrer\">a recent Nature paper</a>, researchers from Google Quantum AI and collaborators from Stanford, MIT, and Caltech shed new light on this question. We introduce an efficient quantum algorithm — called Decoded Quantum Interferometry (DQI) — that uses the wavelike nature of quantum mechanics to create interference patterns that converge on near-optimal solutions that are incredibly difficult to find using classical computers.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"16fxo\">A quantum link between optimization and decoding</h2><p data-block-key=\"eflao\">There is a catch, however. To build the necessary interference patterns, one must solve another hard computational problem called decoding. In a decoding problem one is given a lattice and a point in space, and one needs to find the nearest lattice element to the point. For example, the corners of the squares on a chessboard form a two dimensional lattice. After dropping a grain of sand at a random location on a chessboard, the decoding problem would be to find the nearest corner. Although this problem is easy for a square lattice in two dimensions, it can become very difficult on some lattices in hundreds or thousands of dimensions.</p><p data-block-key=\"bhrt5\">Fortunately, decoding problems have been extremely well studied over the past several decades, mainly due to applications in correcting errors incurred during data storage or transmission. Many sophisticated and powerful algorithms have been devised to solve decoding problems for various specially structured lattices. We have discovered that for certain kinds of optimization problems, the related decoding problems have the right kind of structure to be solved by some of these powerful decoding algorithms. However, it is only through the power of quantum computing that these decoding algorithms can be leveraged to also solve optimization problems. By pairing the quantum interference of DQI with these sophisticated decoding algorithms, a sufficiently large quantum computer could find approximate solutions to these optimization problems — solutions that appear to be beyond the reach of any known classical method.</p><p data-block-key=\"c6d4f\">This mathematical discovery of a quantum algorithm that offers speedup for optimization improves our understanding of the eventual use cases for quantum computers. When quantum computing hardware is advanced enough, researchers can use the DQI algorithm to solve classically challenging optimization problems.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DQI1_Animation.width-800.gif\" alt=\"DQI1_Animation\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DQI1_Animation.width-800.gif\" alt=\"DQI1_Animation\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"bj89f\"><i>A figurative representation of the conversion of an optimization problem on a rugged cost function landscape into a decoding problem for a periodic lattice.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"16fxo\">A clear quantum win: Optimal polynomial intersection</h2><p data-block-key=\"bh66r\">In this work, our best result is for a problem that we call <a href=\"https://arxiv.org/abs/2408.08292\" target=\"_blank\" rel=\"noopener noreferrer\">optimal polynomial intersection</a> (OPI). In the OPI problem, one is given a list of target points and wishes to intersect as many as possible by tuning the coefficients of a polynomial whose degree is lower than the number of points. This is a common task in data science known as <a href=\"https://en.wikipedia.org/wiki/Polynomial_regression\" target=\"_blank\" rel=\"noopener noreferrer\">polynomial regression</a>. Variants of this problem have arisen in the context of <a href=\"https://www.youtube.com/watch?v=X8jsijhllIA\" target=\"_blank\" rel=\"noopener noreferrer\">digital error correction</a> as well as <a href=\"https://www.wisdom.weizmann.ac.il/~naor/PAPERS/ope.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">cryptography</a>. Consequently, sophisticated algorithms have been developed for solving it in certain special cases, but for other cases, the problem remains hopelessly difficult to solve using known algorithms with conventional classical computers.</p><p data-block-key=\"36hdr\">Using DQI, a quantum computer could convert this into a problem of decoding <a href=\"https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction\" target=\"_blank\" rel=\"noopener noreferrer\">Reed-Solomon codes</a> (a widely-used family of codes found in DVDs and QR codes). Very good decoding algorithms have been developed for decoding Reed-Solomon codes, and as a result, quantum computers using DQI can find better approximate optima to the OPI problem than can be found by known algorithms on classical computers. For example, <a href=\"https://arxiv.org/abs/2510.10967\" target=\"_blank\" rel=\"noopener noreferrer\">our analysis</a> shows that certain examples of the OPI problem could be solved by quantum computers using only on the order of a few million elementary quantum logic operations which would require over 10<sup class=\"superscript\">23</sup> (one hundred sextillion) elementary operations to solve on a conventional classical computer using the most efficient known classical algorithm.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DQI2_OPI.width-1250.png\" alt=\"DQI2_OPI\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DQI2_OPI.width-1250.png\" alt=\"DQI2_OPI\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"bj89f\"><i>An illustration of the OPI problem: One wishes to find a low degree polynomial intersecting as many as possible of the target sets. The polynomials</i> f<i> and</i> g<i> displayed here, despite being very different, each intersect three of the target sets, thus scoring the same value of the objective. This phenomenon of distant solutions achieving equal objective values is one reason why it is so hard to solve using conventional computers.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"16fxo\">Where does the quantum advantage come from?</h2><p data-block-key=\"38qmv\">Taking a step back, we can ask why converting optimization problems into decoding problems should ever be advantageous in the first place? By understanding this more deeply, one could hope to gain intuition to guide the search for additional optimization problems on which quantum computers may provide advantage.</p><p data-block-key=\"64okm\">Both the optimization problems that we start with and the decoding problems that we convert them into are something called <a href=\"https://jeffe.cs.illinois.edu/teaching/algorithms/book/12-nphard.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">NP-hard problems</a>. This suggests that it is impossible to efficiently find exact solutions to all instances of these problems, even with the help of quantum computers. By using quantum effects, DQI has converted one hard problem into another hard problem. How does this accomplish anything? The key is that the NP-hardness speaks to the difficulty of the <i>very hardest instances</i> of a given problem. If the problem instances are restricted to have some additional structure, <i>this can make them easier</i>. The promise of DQI is that certain kinds of structure may make the decoding problem much easier, without also making the optimization problem easier to solve using conventional computers.</p><p data-block-key=\"43cq0\">In the OPI problem, the lattice that arises is algebraically structured; the components of the basis vectors, instead of being arbitrary, are obtained by raising a number to successively higher powers. This algebraic structure is reflected in both the original optimization problem (OPI) and the decoding problem that quantum computers can convert it into (Reed-Solomon decoding). This structure makes the decoding problem much easier, but as far as we can tell does not make the optimization problem easier for conventional computers. In this circumstance, the ability to convert the optimization problem into the decoding problem, using the power of quantum computing, provides advantage.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"16fxo\">Random sparse optimization problems: A tantalizing challenge</h2><p data-block-key=\"4oejb\">In the paper, we also consider more generic lattices that lack algebraic structure but whose basis vectors are sparse, i.e., consisting mostly of zeros. The corresponding optimization problem is called <code>max-k-XORSAT</code> and is illustrated below. The sparsity of the lattice is reflected in the fact that each of the constraints involves only a few of the variables (at most <i>k</i>). In <code>max-k-XORSAT</code> there are more constraints than variables and it is impossible to satisfy all of them. Instead, one wishes to find a solution that satisfies as many constraints as possible. Though it may sound abstract, the <code>max-k-XORSAT</code> problem is commonly used as a testbed for new optimization algorithms and includes a number of other well-known optimization problems as special cases, such as <a href=\"https://en.wikipedia.org/wiki/Maximum_cut\" target=\"_blank\" rel=\"noopener noreferrer\">max-cut</a> and <a href=\"https://en.wikipedia.org/wiki/Qubo\" target=\"_blank\" rel=\"noopener noreferrer\">QUBO</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DQI3_Example.width-1250.png\" alt=\"DQI3_Example\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DQI3_Example.width-1250.png\" alt=\"DQI3_Example\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"bj89f\"><i>An example of a</i> <code><i>max-k-XORSAT</i></code><i> problem with</i> k<i> = 2, which has 4 variables and 5 constraints. By assigning each of A, B, C, D to 0 or 1, how many constraints can you satisfy?</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"16fxo\">DQI can convert <code>max-k-XORSAT</code> into a decoding problem for codes defined by sparse matrices. Such codes are called <a href=\"https://en.wikipedia.org/wiki/Low-density_parity-check_code\" target=\"_blank\" rel=\"noopener noreferrer\">low density parity check (LDPC) codes</a>. It was discovered in the 1960s that sparsity makes the decoding problem much easier. However, the sparsity of the original <code>max-k-XORSAT</code> problem also makes it easier to solve on conventional computers using an algorithm called <a href=\"https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=150774\" target=\"_blank\" rel=\"noopener noreferrer\">simulated annealing</a>. Thus, it is hard to find <code>max-k-XORSAT</code> problems that have just the right sparsity so that the decoder is helped more than the simulated annealing algorithm that we are comparing against. In the paper we present one example problem where the sparsity is just right so that DQI appears to have a speed advantage over simulated annealing. However, we managed to solve this efficiently on a conventional computer using a specialized algorithm that we tailored to our example. So, at present, unlike for OPI, we do not have an example of a <code>max-k-XORSAT</code> problem that both can be solved by DQI and cannot be solved efficiently by any known algorithm running on conventional computers.</p><p data-block-key=\"f6rea\">Since sparse optimization problems have widespread practical applications, we continue to search for ways that DQI might achieve quantum advantage on sparse optimization problems. In particular, DQI has motivated new lines of research into both classical and quantum algorithms for decoding LDPC codes.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"16fxo\">Prospects</h2><p data-block-key=\"dtn01\">The DQI algorithm provides a powerful new toolkit for developing quantum optimization algorithms. This approach of translating optimization problems into decoding problems offers a new way to address one of the longest-standing questions in the field. We are excited to see what researchers, both at Google and in the broader community, will build with these tools.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "使用 JAX-Privacy 实现大规模差分隐私机器学习 (原标题: Differentially private machine learning at scale with JAX-Privacy)",
      "link": "https://research.google/blog/differentially-private-machine-learning-at-scale-with-jax-privacy/",
      "pubDate": "Tue, 11 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-11T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 使用 JAX-Privacy 实现大规模差分隐私机器学习\n\n人工智能模型在改善生活和变革行业方面发挥着关键作用，其影响和准确性往往取决于所使用数据的质量。为了开发准确且具有代表性的人工智能模型，高质量的大型数据集至关重要，但同时必须以保护个人隐私的方式使用这些数据。JAX 和 JAX-Privacy 正是为了解决这一挑战而生。\n\n## JAX 简介\n\nJAX 是一个于2020年推出的高性能数值计算库，专为大规模机器学习而设计。其核心特性包括自动微分、即时编译以及跨多个加速器的无缝扩展，使其成为高效构建和训练复杂模型的理想平台。JAX 已成为推动人工智能边界的研究人员和工程师的基石，其生态系统包含Flax（简化神经网络架构实现）和Optax（实现最先进优化器）等领域特定库。\n\n## JAX-Privacy 简介\n\nJAX-Privacy 构建于 JAX 之上，是一个强大的工具包，用于构建和审计差分隐私（DP）模型。它使研究人员和开发人员能够快速高效地实现差分隐私算法，用于在大数据集上训练深度学习模型，并提供将隐私训练集成到现代分布式训练工作流所需的核心工具。JAX-Privacy 的最初版本于2022年推出，旨在帮助外部研究人员复现和验证谷歌在隐私训练方面的一些进展。此后，它已发展成为一个中心，谷歌内部的各个研究团队在此集成他们关于 DP 训练和审计算法的最新研究成果。\n\n### JAX-Privacy 1.0 发布\n\n谷歌自豪地宣布发布 JAX-Privacy 1.0 版本。该新版本集成了最新的研究进展，并重新设计以提高模块化，使研究人员和开发人员能够比以往更轻松地构建结合最先进 DP 算法和 JAX 提供可扩展性的 DP 训练管道。\n\n## JAX-Privacy 的必要性\n\n多年来，差分隐私（DP）一直被认为是量化和限制隐私泄露的黄金标准。DP 保证算法的输出几乎相同，无论数据集中是否包含单个个体（或示例）。\n\n尽管 DP 理论已经成熟，但其在大规模机器学习中的实际实现却充满挑战。最常见的方法——差分隐私随机梯度下降（DP-SGD）——需要定制的批处理程序、逐例梯度裁剪以及添加经过仔细校准的噪声。这个过程计算密集，难以正确高效地实现，尤其是在现代基础模型的规模下。\n\n![JAXPrivacy2_Overview](https://storage.googleapis.com/gweb-research2023-media/images/JAXPrivacy2_Overview.width-1250.png)\n\nJAX-Privacy 通过其用于梯度裁剪和相关噪声生成的原始构建块，实现了在分布式环境中高效工作，从而使研究人员和开发人员能够使用最先进的差分隐私算法，以可扩展和高效的方式在私有数据上训练和微调基础模型。现有框架虽有进步，但在可扩展性或灵活性方面往往不足。谷歌的工作一直致力于推动隐私机器学习的边界，从开创新的 DP 算法到开发复杂的审计技术。因此，需要一个能够跟上研究步伐的工具——一个不仅正确高效，而且从一开始就设计用于处理最先进模型的并行性和复杂性的库。JAX 的函数式范式和强大的转换（如 vmap 和 shard_map）提供了坚实的基础。通过在 JAX 上构建，谷歌创建了一个开箱即用的并行化库，支持跨多个加速器和超级计算机训练大规模模型。JAX-Privacy 是这项努力的结晶，一个经过时间考验的库，已为内部生产集成提供支持，现在正与更广泛的社区共享。\n\n## JAX-Privacy 的核心功能\n\nJAX-Privacy 通过提供一套精心设计的组件，简化了 DP 的复杂性：\n\n*   **核心构建块**：该库提供了基本 DP 原语的正确高效实现，包括逐例梯度裁剪、噪声添加和数据批次构建。这些组件使开发人员能够自信地构建 DP-SGD 和 DP-FTRL 等知名算法。\n*   **最先进的算法**：JAX-Privacy 不仅限于基础功能，还支持高级方法，如依赖于在迭代中注入相关噪声的 DP 矩阵分解，这已被证明可以提高性能。这使得研究人员可以轻松尝试尖端的隐私训练技术。\n*   **可扩展性**：所有组件都设计为与 JAX 的原生并行化功能无缝协作。这意味着您可以训练需要数据和模型并行化的大规模模型，而无需复杂的自定义代码，从而使大型模型的隐私训练成为现实。JAX-Privacy 还提供微批处理和填充等工具，用于无缝处理通常需要获得最佳隐私/效用权衡的巨大、可变大小的批次。\n*   **正确性和审计**：该库建立在谷歌最先进的 DP 审计库之上，确保噪声校准在数学上是正确的，并且尽可能紧密。这些关于隐私损失的正式界限可以通过量化经验隐私损失的指标来补充，从而提供训练管道隐私属性的更完整视图。用户可以轻松测试和开发自己的审计技术，例如谷歌在“差分隐私机器学习的严格审计”方面的获奖工作，该工作通过注入“金丝雀”（已知数据点）并在每个步骤计算审计指标来发挥作用。\n\nJAX-Privacy 实现了多种基础工具，用于裁剪、噪声添加、批次选择、审计和核算，这些工具可以以各种方式组合，以构建端到端的 DP 训练计划。\n\n## 从研究到实践：自信地微调大型语言模型（LLMs）\n\nJAX-Privacy 最令人兴奋的方面之一是其实际应用。该库旨在支持用于预训练和微调 LLM 的现代机器学习框架。一个显著的例子是谷歌最近在训练 VaultGemma（世界上最强大的差分隐私 LLM）时使用了 JAX-Privacy 构建块。\n\n通过此次开源发布，谷歌希望开发人员能够通过流行的 Keras 框架，仅用几行代码即可轻松微调大型模型。特别是，谷歌提供了用于微调 Gemma 系列模型（由 Google DeepMind 基于 Gemini 构建的开源模型集合）的完整功能示例。这些示例展示了如何将 JAX-Privacy 应用于对话摘要和合成数据生成等任务，表明该库即使在处理最先进的模型时也能提供最先进的结果。\n\n通过简化 DP 的集成，JAX-Privacy 赋能开发人员从头开始构建隐私保护应用程序，无论是为医疗保健应用微调聊天机器人，还是为个性化金融建议微调模型。它降低了隐私保护机器学习的入门门槛，使强大、负责任的人工智能更易于访问。\n\n## 展望未来\n\n谷歌很高兴与研究社区分享 JAX-Privacy。此次发布是多年不懈努力的成果，代表着对隐私保护机器学习领域的重大贡献。谷歌希望通过提供这些工具，能够开启新一波的研究和创新，造福所有人。谷歌将继续支持和开发该库，整合新的研究进展并响应社区的需求。鼓励用户查看 GitHub 上的存储库或 PIP 包，立即开始训练隐私保护机器学习模型。\n\n**致谢**：JAX-Privacy 包含来自 Leonard Berrada, Robert Stanforth, Brendan McMahan, Christopher A. Choquette-Choo, Galen Andrew, Mikhail Pravilov, Sahra Ghalebikesabi, Aneesh Pappu, Michael Reneer, Jamie Hayes, Vadym Doroshenko, Keith Rush, Dj Dvijotham, Zachary Charles, Peter Kairouz, Soham De, Samuel L. Smith, Judy Hanwen Shen 的贡献。",
      "shortSummary": "JAX-Privacy 1.0 是一个基于高性能 JAX 库的工具包，旨在实现大规模差分隐私（DP）机器学习。它解决了在大型模型中高效实施 DP 的挑战，提供核心构建块、先进算法、可扩展性和审计功能。JAX-Privacy 简化了 DP 训练管道的构建，支持在保护隐私的同时训练和微调大型语言模型（如 VaultGemma 和 Gemma 系列），从而降低了隐私保护机器学习的门槛，使负责任的 AI 更易于实现。",
      "translated_title": "使用 JAX-Privacy 实现大规模差分隐私机器学习",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/JAXPrivacy2_Overview.width-1250.png",
          "alt": "JAXPrivacy2_Overview",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xtj9j\">From personalized recommendations to scientific advances, AI models are helping to improve lives and transform industries. But the impact and accuracy of these AI models is often determined by the quality of data they use. Large, high-quality datasets are crucial for developing accurate and representative AI models, however, they must be used in ways that preserve individual privacy.</p><p data-block-key=\"b23gn\">That’s where <a href=\"https://docs.jax.dev/en/latest/\" target=\"_blank\" rel=\"noopener noreferrer\">JAX</a> and <a href=\"https://github.com/google-deepmind/jax_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">JAX-Privacy</a> come in. Introduced in 2020, JAX is a high-performance numerical computing library designed for large-scale machine learning (ML). Its core features — including <a href=\"https://en.wikipedia.org/wiki/Automatic_differentiation\" target=\"_blank\" rel=\"noopener noreferrer\">automatic differentiation</a>, <a href=\"https://en.wikipedia.org/wiki/Just-in-time_compilation\" target=\"_blank\" rel=\"noopener noreferrer\">just-in-time compilation</a>, and seamless scaling across multiple accelerators — make it an ideal platform for building and training complex models efficiently. JAX <a href=\"https://github.com/jax-ml/jax/network/dependents\" target=\"_blank\" rel=\"noopener noreferrer\">has become a cornerstone</a> for researchers and engineers pushing the boundaries of AI. Its surrounding ecosystem includes a robust set of domain-specific libraries, including <a href=\"https://flax.readthedocs.io/en/latest/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">Flax</a>, which simplifies the implementation of neural network architectures, and <a href=\"https://github.com/google-deepmind/optax\" target=\"_blank\" rel=\"noopener noreferrer\">Optax</a>, which implements state-of-the-art optimizers.</p><p data-block-key=\"fkr7i\">Built on JAX, JAX-Privacy is a robust toolkit for building and auditing differentially private models. It enables researchers and developers to quickly and efficiently implement <a href=\"https://en.wikipedia.org/wiki/Differential_privacyhttps://en.wikipedia.org/wiki/Differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">differentially private</a> (DP) algorithms for training deep learning models on large datasets, and provides the core tools needed to integrate private training into modern distributed training workflows. The original version of JAX-Privacy was introduced in 2022 to enable external researchers to reproduce and validate some of our <a href=\"https://github.com/google-deepmind/jax_privacy#reproducing-results\" target=\"_blank\" rel=\"noopener noreferrer\">advances on private training</a>. It has since evolved into a hub where research teams across Google integrate their novel research insights into DP training and auditing algorithms.</p><p data-block-key=\"857iv\">Today, we are proud to announce the release of <a href=\"https://github.com/google-deepmind/jax_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">JAX-Privacy 1.0</a>. Integrating our latest research advances and re-designed for modularity, this new version makes it easier than ever for researchers and developers to build DP training pipelines that combine state-of-the-art DP algorithms with the scalability provided by JAX.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xtj9j\">How we got here: The need for JAX-Privacy</h2><p data-block-key=\"d6p4s\">For years, researchers have turned to DP as the gold standard for quantifying and bounding privacy leakage. DP guarantees that the output of an algorithm is nearly the same whether or not a single individual (or example) is included in the dataset.</p><p data-block-key=\"efhk8\">While the theory of DP is well-established, its practical implementation in large-scale ML can be a challenge. The most common approach, <a href=\"https://arxiv.org/abs/1607.00133\" target=\"_blank\" rel=\"noopener noreferrer\">differentially private stochastic gradient descent</a> (DP-SGD), requires customized batching procedures, per-example gradient clipping, and the addition of carefully calibrated noise. This process is computationally intensive and can be difficult to implement correctly and efficiently, especially at the scale of modern foundation models.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/JAXPrivacy2_Overview.width-1250.png\" alt=\"JAXPrivacy2_Overview\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/JAXPrivacy2_Overview.width-1250.png\" alt=\"JAXPrivacy2_Overview\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"q4d8f\"><i>JAX-Privacy enables researchers and developers to train and fine-tune foundation models on private data using state-of-the-art differentially private algorithms in a scalable and efficient way thanks to its primitive building blocks for gradient clipping and correlated noise generation, both of which work effectively in distributed environments.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xtj9j\">Existing frameworks have made strides, but they often fall short in scalability or flexibility. Our work has consistently pushed the boundaries of private ML, from <a href=\"https://arxiv.org/abs/1607.00133\" target=\"_blank\" rel=\"noopener noreferrer\">pioneering new DP algorithms</a> to <a href=\"https://arxiv.org/abs/2302.07956\" target=\"_blank\" rel=\"noopener noreferrer\">developing sophisticated auditing techniques</a>. We needed a tool that could keep pace with our research — a library that was not only correct and efficient but also designed from the ground up to handle the parallelism and complexity of state-of-the-art models.</p><p data-block-key=\"1shto\">JAX's functional paradigm and powerful transformations, like <code>vmap</code> (for automatic vectorization) and <code>shard_map</code> (for single-program multiple-data parallelization), provided a strong foundation. By building on JAX, we could create a library that was parallelism-ready out-of-the-box, supporting the training of large-scale models across multiple accelerators and supercomputers. JAX-Privacy is the culmination of this effort, a time-tested library that has powered internal production integrations and is now being shared with the broader community.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xtj9j\">What JAX-Privacy delivers</h2><p data-block-key=\"ba6f3\">JAX-Privacy simplifies the complexities of DP by providing a suite of carefully engineered components:</p><ul><li data-block-key=\"1qgme\"><i>Core building blocks</i>: The library offers correct and efficient implementations of the fundamental DP primitives, including <a href=\"https://github.com/google-deepmind/jax_privacy/blob/main/jax_privacy/clipping.py\" target=\"_blank\" rel=\"noopener noreferrer\">per-example gradient clipping</a>, <a href=\"https://github.com/google-deepmind/jax_privacy/blob/main/jax_privacy/noise_addition.py\" target=\"_blank\" rel=\"noopener noreferrer\">noise addition</a>, and <a href=\"https://github.com/google-deepmind/jax_privacy/blob/main/jax_privacy/batch_selection.py\" target=\"_blank\" rel=\"noopener noreferrer\">data batch construction</a>. These components enable developers to build well-known algorithms like DP-SGD and <a href=\"https://arxiv.org/abs/2103.00039\" target=\"_blank\" rel=\"noopener noreferrer\">DP-FTRL</a> with confidence.</li><li data-block-key=\"7gq93\"><i>State-of-the-art algorithms</i>: JAX-Privacy goes beyond the basics, supporting advanced methods like <a href=\"https://arxiv.org/abs/2506.08201\" target=\"_blank\" rel=\"noopener noreferrer\">DP matrix factorization</a> that rely on injecting correlated noise across iterations, which have been shown to improve performance. This makes it easy for researchers to experiment with cutting-edge private training techniques.</li><li data-block-key=\"430g0\"><i>Scalability</i>: All components are designed to work seamlessly with JAX's native parallelism features. This means you can train large-scale models that require data and model parallelism without complex, custom code, making private training on large models a reality. JAX-Privacy also provides tools like micro-batching and padding for seamlessly handling massive, variable-sized batches that are typically needed to obtain the best privacy/utility trade-offs.</li><li data-block-key=\"7nhv3\"><i>Correctness and</i> <a href=\"https://github.com/google-deepmind/jax_privacy/blob/main/jax_privacy/auditing.py\" target=\"_blank\" rel=\"noopener noreferrer\"><i>auditing</i></a>: The library is built on Google's state-of-the-art <a href=\"https://github.com/google/differential-privacy/tree/main/python/dp_accounting\" target=\"_blank\" rel=\"noopener noreferrer\">DP accounting library</a>, ensuring the noise calibration is both mathematically correct and as tight as possible. These formal bounds on the privacy loss can be complemented with metrics that quantify the empirical privacy loss, providing a more complete view of the privacy properties of a training pipeline. Users can easily test and develop their own auditing techniques, like our award-winning work on \"<a href=\"https://www.usenix.org/conference/usenixsecurity23/presentation/nasr\" target=\"_blank\" rel=\"noopener noreferrer\">Tight Auditing of Differentially Private Machine Learning</a>\", which works by injecting \"canaries\" — known data points — and computing auditing metrics at each step.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/JAXPrivacy3_Animation.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"q4d8f\"><i>JAX-Privacy implements a variety of foundational tools for clipping, noise addition, batch selection, accounting, and auditing that can be combined in various ways to construct end-to-end DP training plans.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xtj9j\">From research to practice: Fine-tuning LLMs with confidence</h2><p data-block-key=\"9h6ls\">One of the most exciting aspects of JAX-Privacy is its practical application. The library is designed to support modern ML frameworks used for pre-training and fine-tuning LLMs. A notable example is our recent use of JAX-Privacy building blocks in the training of <a href=\"https://research.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/\">VaultGemma</a>, the world's most capable differentially private LLM.</p><p data-block-key=\"6fss3\">With this open-source release, we want to enable developers to easily fine-tune large models with just a few lines of code via the popular <a href=\"https://keras.io/examples/nlp/\" target=\"_blank\" rel=\"noopener noreferrer\">Keras</a> framework. In particular, we include <a href=\"https://github.com/google-deepmind/jax_privacy/tree/main/examples\" target=\"_blank\" rel=\"noopener noreferrer\">fully-functional examples</a> for fine-tuning models in the <a href=\"https://developers.googleblog.com/en/gemma-explained-overview-gemma-model-family-architectures/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma family</a>, a collection of open models built by Google DeepMind based on Gemini. These examples demonstrate how to apply JAX-Privacy to tasks like dialogue summarization and synthetic data generation, showing that this library can deliver state-of-the-art results even when working with the most advanced models.</p><p data-block-key=\"b2918\">By simplifying the integration of DP, JAX-Privacy empowers developers to build privacy-preserving applications from the ground up, whether they are fine-tuning a chatbot for a healthcare application or a model for personalized financial advice. It lowers the barrier to entry for privacy-preserving ML and makes powerful, responsible AI more accessible.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xtj9j\">Looking ahead</h2><p data-block-key=\"7gt08\">We are excited to share JAX-Privacy with the research community. This release is the result of years of dedicated effort and represents a significant contribution to the field of privacy-preserving ML. We hope that by providing these tools, we can enable a new wave of research and innovation that benefits everyone.</p><p data-block-key=\"7ne51\">We will continue to support and develop the library, incorporating new research advances and responding to the needs of the community. We look forward to seeing what you build using JAX-Privacy. Check out the <a href=\"https://github.com/google-deepmind/jax_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">repository on GitHub</a> or the <a href=\"https://pypi.org/project/jax-privacy/\" target=\"_blank\" rel=\"noopener noreferrer\">PIP package</a> to start training privacy-preserving ML models today.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xtj9j\">Acknowledgements</h2><p data-block-key=\"5vdv8\"><i>JAX-Privacy includes contributions from: Leonard Berrada, Robert Stanforth, Brendan McMahan, Christopher A. Choquette-Choo, Galen Andrew, Mikhail Pravilov, Sahra Ghalebikesabi, Aneesh Pappu, Michael Reneer, Jamie Hayes, Vadym Doroshenko, Keith Rush, Dj Dvijotham, Zachary Charles, Peter Kairouz, Soham De, Samuel L. Smith, Judy Hanwen Shen.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "介绍嵌套学习：一种用于持续学习的新机器学习范式 (原标题: Introducing Nested Learning: A new ML paradigm for continual learning)",
      "link": "https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/",
      "pubDate": "Thu, 06 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-06T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 介绍嵌套学习：一种用于持续学习的新机器学习范式\n\n### 引言与挑战\n\n机器学习（ML）在过去十年取得了显著进步，但**持续学习**——模型在不遗忘旧知识的情况下主动获取新知识和技能——仍然是一个核心挑战。大型语言模型（LLM）尤其面临“**灾难性遗忘**”（CF）问题，即学习新任务会牺牲对旧任务的熟练度。人脑通过**神经可塑性**实现持续学习，而当前LLM的知识受限于即时上下文或预训练信息。传统上，研究人员通过架构调整或优化规则来解决CF，但他们将模型架构和优化算法视为独立实体，阻碍了统一高效学习系统的实现。\n\n### 嵌套学习范式\n\n在NeurIPS 2025发表的论文《嵌套学习：深度学习架构的幻象》中，研究人员引入了**嵌套学习**，旨在弥合这一差距。嵌套学习将单个ML模型视为一个由相互连接、多层次学习问题组成的系统，这些问题同时进行优化。其核心观点是：模型架构和训练规则（优化算法）本质上是相同的概念，它们只是不同“级别”的优化，各自拥有独特的“上下文流”和更新速率。通过识别这种内在结构，嵌套学习为设计更强大的AI提供了新的维度，允许构建具有更深计算深度的学习组件，从而有助于解决灾难性遗忘等问题。\n\n该范式揭示，一个复杂的ML模型实际上是一组相互嵌套或并行运行的连贯、相互连接的优化问题，每个问题都有其独特的上下文流。这种视角暗示，现有深度学习方法通过压缩其内部上下文流来工作。\n\n### 关联记忆的视角\n\n文章通过关联记忆的概念来阐释嵌套学习：\n*   **训练过程**（特别是反向传播）可以建模为关联记忆，模型学习将给定数据点映射到其局部误差值，作为数据点“意外”程度的度量。\n*   **关键架构组件**，如Transformer中的注意力机制，也可以形式化为简单的关联记忆模块，学习序列中token之间的映射。\n\n### 人脑与嵌套学习的类比\n\n![Diagram comparing biological brain waves and neuroplasticity to the uniform structure and multi-frequency updates used in Nested Learning models.](https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-1a-Inspiration.width-1250.png)\n\n人脑中统一可重用的结构以及多时间尺度更新是人类持续学习的关键组成部分。嵌套学习允许大脑每个组件进行多时间尺度更新，并表明Transformer和记忆模块等知名架构实际上是具有不同频率更新的线性层。通过定义更新频率（即每个组件的权重调整频率），可以将这些相互连接的优化问题组织成“级别”，形成嵌套学习范式的核心。\n\n### 嵌套学习的应用\n\n嵌套学习的视角提供了改进现有算法和架构的原则性方法：\n*   **深度优化器**：将优化器视为关联记忆模块，通过将底层目标改为L2回归损失等标准损失指标，推导出新的动量公式，使其对不完善数据更具弹性。\n*   **连续记忆系统（CMS）**：将Transformer的短期记忆（序列模型）和长期记忆（前馈网络）概念扩展。记忆被视为一系列模块，每个模块以不同的特定频率更新，从而为持续学习创建更丰富、更有效的记忆系统。\n\n### Hope：一种自修改架构\n\n作为概念验证，研究人员利用嵌套学习原理设计了**Hope**，它是Titans架构的变体。Hope是一种自修改的循环架构，能够利用无限级别的上下文学习，并通过CMS模块扩展到更大的上下文窗口。它可以通过自指过程优化自身的记忆，创建一个具有无限循环学习级别的架构。\n\n### 实验与结果\n\n实验评估了深度优化器和Hope在语言建模、长上下文推理、持续学习和知识整合任务上的有效性。结果证实了嵌套学习、连续记忆系统设计和自修改Titans的强大能力：\n\n*   在多样化的语言建模和常识推理任务上，Hope架构展现出比现代循环模型和标准Transformer更低的困惑度和更高的准确性。\n\n![Bar chart that shows the Hope model outperforming Titans, Samba, and Transformer on both language modeling and common-sense reasoning performance metrics.](https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-1-Performance.width-1250.png)\n\n*   在长上下文“大海捞针”（NIAH）下游任务中，Hope展现出卓越的记忆管理能力，证明CMS能更高效地处理扩展信息序列。\n\n![Bar chart showing Hope and Titans models consistently outperforming TTT and Mamba2 across long-context tasks of three difficulty levels.](https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-2-LongContext.width-1250.png)\n\n### 结论\n\n嵌套学习范式通过将架构和优化视为一个连贯的嵌套优化问题系统，为深度学习设计开辟了新维度。由此产生的模型，如Hope架构，表明统一这些元素的原则性方法可以带来更具表现力、能力更强、效率更高的学习算法。该范式为弥合当前LLM的局限性与人脑卓越的持续学习能力之间的差距提供了坚实基础。",
      "shortSummary": "嵌套学习是一种新的机器学习范式，它将模型架构和优化算法统一为相互连接的多层次学习问题。该范式通过识别不同优化级别的“上下文流”和更新速率，旨在解决大型语言模型中的“灾难性遗忘”问题，并实现持续学习。概念验证模型“Hope”展示了在语言建模和长上下文记忆管理方面的卓越性能，为构建能够自我改进的AI提供了新途径，弥合了当前LLM与人脑持续学习能力之间的差距。",
      "translated_title": "介绍嵌套学习：一种用于持续学习的新机器学习范式",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-1a-Inspiration.width-1250.png",
          "alt": "Diagram comparing biological brain waves and neuroplasticity to the uniform structure and multi-frequency updates used in Nested Learning models.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-1-Performance.width-1250.png",
          "alt": "Bar chart that shows the Hope model outperforming Titans, Samba, and Transformer on both language modeling and common-sense reasoning performance metrics.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-2-LongContext.width-1250.png",
          "alt": "Bar chart showing Hope and Titans models consistently outperforming TTT and Mamba2 across long-context tasks of three difficulty levels.",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"1guni\">The last decade has seen incredible progress in machine learning (ML), primarily driven by powerful neural network architectures and the algorithms used to train them. However, despite the success of large language models (LLMs), a few fundamental challenges persist, especially around continual learning, the ability for a model to actively acquire new knowledge and skills over time without forgetting old ones.</p><p data-block-key=\"9an83\">When it comes to continual learning and self-improvement, the human brain is the gold standard. It adapts through neuroplasticity — the remarkable capacity to change its structure in response to new experiences, memories, and learning. Without this ability, a person is limited to immediate context (like <a href=\"https://en.wikipedia.org/wiki/Anterograde_amnesia\" target=\"_blank\" rel=\"noopener noreferrer\">anterograde amnesia</a>). We see a similar limitation in current LLMs: their knowledge is confined to either the immediate context of their input window or the static information that they learn during pre-training.</p><p data-block-key=\"fh441\">The simple approach, continually updating a model's parameters with new data, often leads to “<a href=\"https://en.wikipedia.org/wiki/Catastrophic_interference\" target=\"_blank\" rel=\"noopener noreferrer\">catastrophic forgetting</a>” (CF), where learning new tasks sacrifices proficiency on old tasks. Researchers traditionally combat CF through architectural tweaks or better optimization rules. However, for too long, we have treated the model's architecture (the network structure) and the optimization algorithm (the training rule) as two separate things, which prevents us from achieving a truly unified, efficient learning system.</p><p data-block-key=\"9ifb6\">In our paper, “<a href=\"http://abehrouz.github.io/files/NL.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Nested Learning: The Illusion of Deep Learning Architectures</a>”, published at <a href=\"https://neurips.cc/virtual/2025/poster/116123\" target=\"_blank\" rel=\"noopener noreferrer\">NeurIPS 2025</a>, we introduce Nested Learning, which bridges this gap. Nested Learning treats a single ML model not as one continuous process, but as a system of interconnected, multi-level learning problems that are optimized simultaneously. We argue that the model's architecture and the rules used to train it (i.e., the optimization algorithm) are fundamentally the same concepts; they are just different \"levels\" of optimization, each with its own internal flow of information (\"context flow\") and update rate. By recognizing this inherent structure, Nested Learning provides a new, previously invisible dimension for designing more capable AI, allowing us to build learning components with deeper computational depth, which ultimately helps solve issues like catastrophic forgetting.</p><p data-block-key=\"cp75j\">We test and validate Nested Learning through a proof-of-concept, self-modifying architecture that we call “Hope”, which achieves superior performance in language modeling and demonstrates better long-context memory management than existing state-of-the-art models.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The Nested Learning paradigm</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1guni\">Nested Learning reveals that a complex ML model is actually a set of coherent, interconnected optimization problems nested within each other or running in parallel. Each of these internal problems has its own <i>context flow</i> — its own distinct set of information from which it is trying to learn.</p><p data-block-key=\"aog9\">This perspective implies that existing deep learning methods work by essentially <i>compressing</i> their internal context flows. More importantly, Nested Learning reveals a new dimension for designing models, allowing us to build learning components with deeper computational depth.</p><p data-block-key=\"28ofj\">To illustrate this paradigm, we look at the concept of <a href=\"https://en.wikipedia.org/wiki/Associative_memory_(psychology)\" target=\"_blank\" rel=\"noopener noreferrer\">associative memory</a> — the ability to map and recall one thing based on another (like recalling a name when you see a face).</p><ul><li data-block-key=\"565p0\">We show that the training process itself, specifically the <a href=\"https://en.wikipedia.org/wiki/Backpropagation\" target=\"_blank\" rel=\"noopener noreferrer\">backpropagation</a> process, can be modeled as an associative memory. The model learns to map a given data point to the value of its local error, which serves as a measure of how \"surprising\" or unexpected that data point was.</li><li data-block-key=\"d9pfu\">Similarly, following previous studies (e.g., <a href=\"https://arxiv.org/pdf/2504.13173\" target=\"_blank\" rel=\"noopener noreferrer\">Miras</a>), key architectural components, such as the <a href=\"https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\" target=\"_blank\" rel=\"noopener noreferrer\">attention mechanism in transformers</a>, can also be formalized as simple associative memory modules that learn the mapping between tokens in a sequence.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-1a-Inspiration.width-1250.png\" alt=\"Diagram comparing biological brain waves and neuroplasticity to the uniform structure and multi-frequency updates used in Nested Learning models.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-1a-Inspiration.width-1250.png\" alt=\"Diagram comparing biological brain waves and neuroplasticity to the uniform structure and multi-frequency updates used in Nested Learning models.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"48tdd\">The uniform and reusable structure as well as multi-time–scale update in the brain are the key components of continual learning in humans. Nested Learning allows for multi-time–scale updates for each component of the brain, while showing that well-known architectures such as transformers and memory modules are in fact linear layers with different frequency updates.</p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"1guni\">By defining an update frequency rate, i.e., how often each component's weights are adjusted, we can order these interconnected optimization problems into \"levels.\" This ordered set forms the heart of the Nested Learning paradigm.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Putting Nested Learning to work</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1guni\">The Nested Learning perspective immediately gives us principled ways to improve existing algorithms and architectures:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Deep optimizers</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1guni\">Since Nested Learning views optimizers (e.g., momentum-based optimizers) as associative memory modules, it allows us to apply principles from associative memory perspective to them. We observed that many standard optimizers rely on simple <a href=\"https://math.stackexchange.com/questions/689022/how-does-the-dot-product-determine-similarity\" target=\"_blank\" rel=\"noopener noreferrer\">dot-product similarity</a> (a measure of how alike two vectors are by calculating the sum of the products of their corresponding components) whose update doesn't account for how different data samples relate to each other. By changing the underlying objective of the optimizer to a more standard loss metric, such as <a href=\"https://en.wikipedia.org/wiki/Ridge_regression\" target=\"_blank\" rel=\"noopener noreferrer\">L2 regression loss</a> (a common loss function in regression tasks that quantifies the error by summing the squares of the differences between predicted and true values), we derive new formulations for core concepts like momentum, making them more resilient to imperfect data.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Continuum memory systems</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1guni\">In a standard Transformer, the sequence model acts as a short-term memory, holding the immediate context, while the <a href=\"https://en.wikipedia.org/wiki/Feedforward_neural_network\" target=\"_blank\" rel=\"noopener noreferrer\">feedforward neural networks</a> act as long-term memory, storing pre-training knowledge. The Nested Learning paradigm extends this concept into what we call a “continuum memory system” (CMS), where memory is seen as a spectrum of modules, each updating at a different, specific frequency rate. This creates a much richer and more effective memory system for continual learning.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Hope: A self-modifying architecture with continuum memory</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1guni\">As a proof-of-concept, we used Nested Learning principles to design Hope, a variant of the <a href=\"https://arxiv.org/abs/2501.00663\" target=\"_blank\" rel=\"noopener noreferrer\">Titans</a> architecture. Titans architectures are long-term memory modules that prioritize memories based on how surprising they are. Despite their powerful memory management, they only have two levels of parameters update, resulting in a first-order in-context learning. Hope, however, is a self-modifying recurrent architecture that can take advantage of unbounded levels of in-context learning and also is augmented with CMS blocks to scale to larger context windows. It can essentially optimize its own memory through a <a href=\"https://people.idsia.ch/~juergen/selfref1992.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">self-referential process</a>, creating an architecture with infinite, looped learning levels.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Experiments</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1guni\">We conducted experiments to evaluate the effectiveness of our deep optimizers and the performance of Hope on language modeling, long-context reasoning, continual learning, and knowledge incorporation tasks. The full results are available in our <a href=\"http://abehrouz.github.io/files/NL.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Results</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1guni\">Our experiments confirm the power of Nested Learning, the design of continuum memory systems, and self-modifying Titans.</p><p data-block-key=\"ca57m\">On a diverse set of commonly used and public language modeling and common-sense reasoning tasks, the Hope architecture demonstrates lower perplexity and higher accuracy compared to modern recurrent models and standard transformers.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-1-Performance.width-1250.png\" alt=\"Bar chart that shows the Hope model outperforming Titans, Samba, and Transformer on both language modeling and common-sense reasoning performance metrics.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-1-Performance.width-1250.png\" alt=\"Bar chart that shows the Hope model outperforming Titans, Samba, and Transformer on both language modeling and common-sense reasoning performance metrics.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"48tdd\">Comparison of performance on language modeling (<a href=\"https://en.wikipedia.org/wiki/Perplexity\" target=\"_blank\" rel=\"noopener noreferrer\">perplexity</a>; left) and common-sense reasoning (accuracy; right) tasks between different architectures: Hope, Titans, <a href=\"https://arxiv.org/pdf/2406.07522\" target=\"_blank\" rel=\"noopener noreferrer\">Samba</a> and a baseline Transformer.</p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"1guni\">Hope showcases superior memory management in long-context Needle-In-Haystack (NIAH) downstream tasks, proving that the CMSs offer a more efficient and effective way to handle extended sequences of information.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-2-LongContext.width-1250.png\" alt=\"Bar chart showing Hope and Titans models consistently outperforming TTT and Mamba2 across long-context tasks of three difficulty levels.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/NestedLearning-2-LongContext.width-1250.png\" alt=\"Bar chart showing Hope and Titans models consistently outperforming TTT and Mamba2 across long-context tasks of three difficulty levels.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"48tdd\">Performance comparison on long-context tasks with different levels of difficulty between different architectures: Hope, Titans, <a href=\"https://arxiv.org/pdf/2407.04620\" target=\"_blank\" rel=\"noopener noreferrer\">TTT</a>, and <a href=\"https://arxiv.org/pdf/2405.21060\" target=\"_blank\" rel=\"noopener noreferrer\">Mamba2</a>. NIAH-PK, NIAH-H, and NIAH-W are needle-in-a-haystack tasks with pass-key, number, and word, respectively.</p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1guni\">The Nested Learning paradigm represents a step forward in our understanding of deep learning. By treating architecture and optimization as a single, coherent system of nested optimization problems, we unlock a new dimension for design, stacking multiple levels. The resulting models, like the Hope architecture, show that a principled approach to unifying these elements can lead to more expressive, capable, and efficient learning algorithms.</p><p data-block-key=\"ejnn7\">We believe the Nested Learning paradigm offers a robust foundation for closing the gap between the limited, forgetting nature of current LLMs and the remarkable continual learning abilities of the human brain. We are excited for the research community to explore this new dimension and help us build the next generation of self-improving AI.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1guni\"><i>This research was conducted by Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, and Vahab Mirrokni. We thank Praneeth Kacham and Corinna Cortes for reviewing the work and their valuable suggestions. We also thank Yuan Deng and Zeman Li. Finally, we thank Mark Simborg and Kimberly Schwede for their help in crafting this blog post.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    }
  ],
  "lastUpdated": "2025-12-11T10:36:21.558Z"
}