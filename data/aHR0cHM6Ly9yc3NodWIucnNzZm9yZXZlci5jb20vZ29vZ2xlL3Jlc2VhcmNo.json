{
  "sourceUrl": "https://rsshub.rssforever.com/google/research",
  "title": "Google Research Blog",
  "description": "Google Research Blog - Powered by RSSHub",
  "link": "https://research.google/blog",
  "items": [
    {
      "title": "StreetReaderAI：通过情境感知多模态AI使街景无障碍化 (原标题: StreetReaderAI: Towards making street view accessible via context-aware multimodal AI)",
      "link": "https://research.google/blog/streetreaderai-towards-making-street-view-accessible-via-context-aware-multimodal-ai/",
      "pubDate": "Tue, 28 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-28T16:00:00.000Z",
      "creator": "Google",
      "summary": "# StreetReaderAI：通过情境感知多模态AI使街景无障碍化\n\n## 1. 背景与问题\n*   当前主流地图服务的街景工具虽然革新了虚拟导航和探索方式，但对盲人及低视力用户而言，屏幕阅读器无法解读街景图像，也缺乏替代文本，导致其无法使用。\n*   多模态AI和图像理解技术为解决这一问题提供了机会，有望使拥有超过2200亿张图像、覆盖110多个国家和地区的Google街景对视障社区更具包容性，提供沉浸式视觉体验并开辟新的探索可能性。\n\n## 2. StreetReaderAI 介绍\n*   在UIST’25上发布的论文“StreetReaderAI：通过情境感知多模态AI使街景无障碍化”中，研究人员推出了StreetReaderAI。\n*   这是一个概念验证（proof-of-concept）的无障碍街景原型，利用情境感知、实时AI和无障碍导航控制。\n*   该项目由盲人和视力正常的可访问性研究人员团队迭代设计，借鉴了无障碍第一人称游戏和导航工具（如Shades of Doom、BlindSquare、SoundScape）的经验。\n\n## 3. 核心功能\n*   **实时AI生成描述：** 提供附近道路、交叉口和地点的实时AI生成描述。\n*   **动态对话：** 与多模态AI代理就场景和当地地理进行动态对话。\n*   **无障碍平移和移动：** 通过语音命令或键盘快捷键在全景图像之间进行无障碍平移和移动。\n\n## 4. 技术实现\n*   StreetReaderAI通过将地理信息源和用户当前视野输入到Gemini中，提供情境感知的街景场景描述。\n*   利用Gemini Live实现关于场景和当地地理特征的实时互动对话。\n\n## 5. 导航体验\n*   StreetReaderAI提供沉浸式的第一人称探索体验，类似于以音频为主要界面的视频游戏。\n*   支持键盘和语音无缝交互：\n    *   **视角调整：** 用户可以使用左右箭头键调整视角。AI会提供方向（如“当前朝向：北”或“东北”）的音频反馈，并指示是否可以向前移动以及是否正对着附近的地标或地点。\n    *   **虚拟移动：** 用户可以使用上/下箭头键进行“虚拟步行”。AI会描述移动距离和附近的地理信息。\n    *   **快速移动：** 用户还可以使用“跳跃”或“传送”功能快速移动到新位置。\n\n## 6. AI子系统：虚拟向导\nStreetReaderAI的核心是两个由Gemini支持的AI子系统：AI Describer（AI描述器）和AI Chat（AI聊天）。两者都接收静态提示、可选的用户配置文件以及关于用户当前位置的动态信息（如附近地点、道路信息和当前视野图像）。\n\n### 6.1 AI Describer (AI描述器)\n*   作为情境感知场景描述工具，它结合动态地理信息和当前街景图像分析，生成实时音频描述。\n*   **两种模式：**\n    *   “默认”模式：强调盲人行人的导航和安全。\n    *   “导游”模式：提供额外的旅游信息（如历史和建筑背景）。\n*   利用Gemini预测盲人或低视力旅行者可能感兴趣的、与当前场景和当地地理相关的后续问题。\n*   **AI描述器如何结合多模态数据支持情境感知场景描述的图示：**\n    ![AI Describer Diagram](https://storage.googleapis.com/gweb-research2023-media/images/StreetReaderAI-3.width-1250.png)\n\n### 6.2 AI Chat (AI聊天)\n*   在AI Describer的基础上，允许用户询问关于当前视图、过去视图和附近地理的问题。\n*   使用Google的多模态Live API，支持实时交互、功能调用，并临时保留单个会话中的所有交互记忆。\n*   系统会跟踪并发送每次平移或移动交互以及用户的当前视图和地理上下文。\n*   **强大的“记忆”功能：** 会话上下文窗口最大可达1,048,576个输入令牌（约相当于4000多张输入图像），使其能够记住用户的位置和上下文。例如，用户可以走过一个公交车站，转弯后问“等等，那个公交车站在哪里？”，AI代理能回忆起之前的上下文并回答。\n\n## 7. 用户测试与结果\n*   研究人员与11位盲人屏幕阅读器用户进行了面对面实验室研究，参与者学习并使用StreetReaderAI探索多个地点和评估潜在步行路线。\n*   **用户反馈：**\n    *   总体评价积极，有用性评分中位数为7（1-7级量表，平均6.4，标准差0.9）。\n    *   参与者强调了虚拟导航与AI的结合、AI Chat界面的无缝性以及所提供信息的价值。\n    *   被认为是对现有街景工具无障碍性不足的重大改进。\n    *   互动式AI聊天功能被描述为使关于街道和地点的对话既引人入胜又实用。\n*   **使用数据：**\n    *   参与者访问了350多个全景图，发出了1000多次AI请求。\n    *   AI Chat的使用频率是AI Describer的六倍，表明用户更倾向于个性化、对话式查询。\n*   **改进空间：** 参与者有时难以正确辨别方向、区分AI回答的真实性以及确定AI知识的局限性。\n\n## 8. AI Chat交互问题类型分析\n研究分析了917次AI Chat交互，并标记了23类问题。最常见的四种问题类型包括：\n1.  **空间定位 (27.0%)：** 关注物体的位置和距离，例如“公交车站离我有多远？”和“垃圾桶在长凳的哪一边？”\n2.  **物体存在 (26.5%)：** 查询关键特征（如人行道、障碍物、门）是否存在，例如“这里有斑马线吗？”\n3.  **一般描述 (18.4%)：** 请求当前视图的摘要，例如“我前面有什么？”\n4.  **物体/地点位置 (14.9%)：** 询问事物在哪里，例如“最近的交叉口在哪里？”或“你能帮我找到门吗？”\n\n## 9. StreetReaderAI 准确性\n在参与者向AI Chat提出的816个问题中：\n*   **正确回答：** 703个 (86.3%)\n*   **不正确：** 32个 (3.9%)\n*   **部分正确：** 26个 (3.2%)\n*   **AI拒绝回答：** 54个 (6.6%)\n在32个不正确回答中：\n*   20个 (62.5%) 是假阴性（例如，说自行车架不存在但实际存在）。\n*   12个 (37.5%) 是误识别（例如，将黄色减速带误认为斑马线）或由于AI Chat尚未在街景中看到目标而导致的错误。\n*   研究指出，需要在实验室环境之外探索StreetReaderAI的性能。\n\n## 10. 未来展望\nStreetReaderAI是使街景工具对所有人无障碍化的一个有前景的开端。未来的扩展机会包括：\n*   **迈向地理视觉代理：** 设想更自主的AI Chat代理，可以自行探索（例如，用户询问“这条路上的下一个公交车站在哪里？”，代理自动导航并报告）。\n*   **支持路线规划：** 支持完整的起点到终点路线规划（例如，代理可以“预先步行”路线，生成对盲人友好的摘要，指出潜在障碍）。\n*   **更丰富的音频界面：** 探索更丰富的非语言反馈，包括空间化音频和从图像本身合成的沉浸式3D音频景观。",
      "shortSummary": "StreetReaderAI是一个概念验证原型，旨在通过情境感知多模态AI使街景对盲人及低视力用户无障碍化。它提供实时AI描述、与AI代理的动态对话以及无障碍导航控制。该系统由AI Describer和AI Chat组成，后者具有强大的记忆功能。用户测试显示其有用性高，尤其偏爱AI Chat。尽管存在方向辨别和AI准确性挑战，StreetReaderAI在无障碍导航方面取得了显著进展，并为未来的自主地理视觉代理和路线规划奠定了基础。",
      "translated_title": "StreetReaderAI：通过情境感知多模态AI使街景无障碍化",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/StreetReaderAI-3.width-1250.png",
          "alt": "StreetReaderAI-3",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"mnsbf\">Interactive streetscape tools, available today in every major mapping service, have revolutionized how people virtually navigate and explore the world — from previewing routes and inspecting destinations to remotely visiting world-class tourist locations. But to date, screen readers have not been able to interpret street view imagery, and alt text is unavailable. We now have an opportunity to redefine this immersive streetscape experience to be inclusive for all with multimodal AI and image understanding. This could eventually allow a service like Google Street View, which has over 220 billion images spanning 110+ countries and territories, to be more accessible to people in the blind and low-vision community, offering an immersive visual experience and opening up new possibilities for exploration.</p><p data-block-key=\"a7psv\">In “<a href=\"https://arxiv.org/pdf/2508.08524\" target=\"_blank\" rel=\"noopener noreferrer\">StreetReaderAI: Making Street View Accessible Using Context-Aware Multimodal AI</a>”, presented at <a href=\"https://uist.acm.org/2025/\" target=\"_blank\" rel=\"noopener noreferrer\">UIST’25</a>, we introduce StreetReaderAI, a proof-of-concept accessible street view prototype that uses context-aware, real-time AI and accessible navigation controls. StreetReaderAI was designed iteratively by a team of blind and sighted accessibility researchers, drawing on previous work in accessible first-person gaming and navigation tools, such as <a href=\"https://www.gmagames.com/sod.html\" target=\"_blank\" rel=\"noopener noreferrer\">Shades of Doom</a>, <a href=\"https://www.blindsquare.com/\" target=\"_blank\" rel=\"noopener noreferrer\">BlindSquare</a>, and <a href=\"https://www.microsoft.com/en-us/research/product/soundscape/\" target=\"_blank\" rel=\"noopener noreferrer\">SoundScape</a>. Key capabilities include:</p><ul><li data-block-key=\"9pu1u\">Real-time AI-generated descriptions of nearby roads, intersections, and places.</li><li data-block-key=\"5qf3g\">Dynamic conversation with a multimodal AI agent about scenes and local geography.</li><li data-block-key=\"9oaqi\">Accessible panning and movement between panoramic images using voice commands or keyboard shortcuts.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/StreetReaderAI-AIDescriber.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"xrzw7\"><i>StreetReaderAI provides a context-aware description of the street view scene by inputting geographic information sources and the user’s current field-of-view into Gemini. For the full audio-video experience, including sound, please refer to this</i> <a href=\"https://youtu.be/cm9gd-502TI\" target=\"_blank\" rel=\"noopener noreferrer\"><i>YouTube video</i></a><i>.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/AIChat_BigBen-PrivacyRedacted.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"xrzw7\"><i>StreetReaderAI uses</i> <a href=\"https://ai.google.dev/gemini-api/docs/live\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Gemini Live</i></a><i> to provide a real-time, interactive conversation about the scene and local geographic features. For the full audio-video experience, including sound, please refer to this</i> <a href=\"https://youtu.be/unsNaq-ea3s\" target=\"_blank\" rel=\"noopener noreferrer\"><i>YouTube video</i></a><i>.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Navigating in StreetReaderAI</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">StreetReaderAI offers an immersive, first-person exploration experience, much like a video game where audio is the primary interface.</p><p data-block-key=\"f9d2t\">StreetReaderAI provides seamless navigation through both keyboard and voice interaction. Users can explore their surroundings using the left and right arrow keys to shift their view. As the user pans, StreetReaderAI shares audio feedback, voicing the current heading as a cardinal or intercardinal direction (<i>e.g.,</i> “<i>Now facing: North</i>” or “<i>Northeast</i>”). It also expresses whether the user can move forward and if they are currently facing a nearby landmark or place.</p><p data-block-key=\"9ccll\">To move, the user can take “virtual steps” using the up arrow or move backward with the down arrow. As a user moves through the virtual streetscape, StreetReaderAI describes how far the user traveled and key geographic information, such as nearby places. Users can also use “jump” or “teleport” features to quickly move to new locations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How StreetReaderAI serves as a virtual guide</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">The core of StreetReaderAI is its two underlying AI subsystems backed by Gemini: AI Describer and AI Chat. Both subsystems take in a static prompt and optional user profile as well as dynamic information about the user’s current location, such as nearby places, road information, and the current field-of-view image (i.e., what’s being shown in Street View).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">AI Describer</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">AI Describer functions as a context-aware scene description tool that combines dynamic geographic information about the user’s virtual location along with an analysis of the current Street View image to generate a real-time audio description.</p><p data-block-key=\"6gha8\">It has two modes: a “default<i>”</i> prompt emphasizing navigation and safety for blind pedestrians, and a “tour guide<i>”</i> prompt that provides additional tourism information (e.g., historic and architectural context). We also use Gemini to predict likely follow-up questions specific to the current scene and local geography that may be of interest to blind or low-vision travelers.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/StreetReaderAI-3.width-1250.png\" alt=\"StreetReaderAI-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/StreetReaderAI-3.width-1250.png\" alt=\"StreetReaderAI-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"xrzw7\"><i>A diagram of how AI Describer combines multimodal data to support context-aware scene descriptions.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">AI Chat</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">AI Chat builds on AI Describer but allows users to ask questions about their current view, past views, and nearby geography. The chat agent uses <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/live-api\" target=\"_blank\" rel=\"noopener noreferrer\">Google's Multimodal Live API</a>, which supports real-time interaction, function calling, and temporarily retains memory of all interactions within a single session. We track and send each pan or movement interaction along with the user's current view and geographic context (e.g., nearby places, current heading).</p><p data-block-key=\"6v4fs\">What makes AI Chat so powerful is its ability to hold a temporary “memory” of the user's session — the context window is set to a maximum of 1,048,576 input tokens, which is roughly equivalent to over 4k input images. Because AI Chat receives the user's view and location with every virtual step, it collects information about the user’s location and context. A user can virtually walk past a bus stop, turn a corner, and then ask, “<i>Wait, where was that bus stop?</i>” The agent can recall its previous context, analyze the current geographic input, and answer, “<i>The bus stop is behind you, approximately 12 meters away.</i>”</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Testing StreetReaderAI with blind users</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">To evaluate StreetReaderAI, we conducted an in-person lab study with eleven blind screen reader users. During the sessions, participants learned about StreetReaderAI and used it to explore multiple locations and evaluate potential walking routes to destinations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/StreetReaderAI-BusStopTask.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"jvl9c\"><i>A blind participant using StreetReaderAI to explore potential travel to a bus stop and inquire about bus stop features, such as the existence of benches and a shelter. For the full audio-video experience, including sound, please refer to this</i> <a href=\"https://youtu.be/rt7Dlqv0eoA\" target=\"_blank\" rel=\"noopener noreferrer\"><i>YouTube video</i></a><i>.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"mnsbf\">Overall, participants reacted positively to StreetReaderAI, rating the overall usefulness 6.4 (median=7; SD=0.9) on a <a href=\"https://en.wikipedia.org/wiki/Likert_scale\" target=\"_blank\" rel=\"noopener noreferrer\">Likert scale</a> from 1–7 (where 1 was ‘not at all useful’ and 7 was ‘very useful’), emphasizing the interplay between virtual navigation and AI, the seamlessness of the interactive AI Chat interface, and the value of information provided. Qualitative feedback from participants consistently highlighted StreetReaderAI's significant accessibility advancement for navigation, noting that existing street view tools lack this level of accessibility. The interactive AI chat feature was also described as making conversations about streets and places both engaging and helpful.</p><p data-block-key=\"evnc8\">During the study, participants visited over 350 panoramas and made over 1,000 AI requests. Interestingly, AI Chat was used six times more often than AI Describer, indicating a clear preference for personalized, conversational inquiries. While participants found value in StreetReaderAI and adeptly combined virtual world navigation with AI interactions, there is room for improvement: participants sometimes struggled with properly orienting themselves, distinguishing the veracity of AI responses, and determining the limits of AI knowledge.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/StreetReaderAI-PlaygroundTask.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"jvl9c\"><i>In one study task, participants were given the instruction, “Find out about an unfamiliar playground to plan a trip with your two young nieces.” This video clip illustrates the diversity of questions asked and the responsiveness of StreetReaderAI. For the full audio-video experience, including sound, please refer to this</i> <a href=\"https://youtu.be/Uxj5fSCp1Dg\" target=\"_blank\" rel=\"noopener noreferrer\"><i>YouTube video</i></a><i>.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Results</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">As the first study of an accessible street view system, our research also provides the first-ever analysis of the types of questions blind people ask about streetscape imagery. We analyzed all 917 AI Chat interactions and annotated each with up to three tags drawn from an emergent list of 23 question type categories. The four most common question types included:</p><ul><li data-block-key=\"c3cvo\"><b>Spatial orientation</b>: 27.0% of participants were most interested in the location and distance of objects, e.g., “<i>How far is the bus stop from where I'm standing?</i>” and <i>“Which side are the garbage cans next to the bench?”</i></li><li data-block-key=\"akthh\"><b>Object existence</b>: 26.5% of participants queried for the presence of key features like sidewalks, obstacles, and doors; <i>“Is there a crosswalk here?”</i></li><li data-block-key=\"e9jjp\"><b>General description</b>: 18.4% of participants started AI Chat by requesting a summary of the current view, often asking, “<i>What's in front of me?</i>”</li><li data-block-key=\"84eds\"><b>Object/place location</b>: 14.9% of participants asked <i>where</i> things were, such as, “<i>Where is the nearest intersection?</i>” or “<i>Can you help me find the door?</i>”</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">StreetReaderAI accuracy</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">Because StreetReaderAI relies so significantly on AI, a critical challenge is response accuracy. Of the 816 questions that participants asked AI Chat:</p><ul><li data-block-key=\"4i79q\">703 (86.3%) were correctly answered.</li><li data-block-key=\"e5j91\">32 (3.9%) were incorrect (3.9%).</li><li data-block-key=\"7tdgb\">The remaining were either: partially correct (26; 3.2%) or the AI refused to answer (54; 6.6%).</li></ul><p data-block-key=\"2rj55\">Of the 32 incorrect responses:</p><ul><li data-block-key=\"20m48\">20 (62.5%) were false negatives, <i>e.g.,</i> stating that a bike rack did not exist when it did.</li><li data-block-key=\"98cf5\">12 (37.5%) were misidentifications (<i>e.g.,</i> a yellow speed bump interpreted as a crosswalk) or misc errors due to AI Chat not yet seeing the target in street view.</li></ul><p data-block-key=\"61i5r\">More work is necessary to explore how StreetReaderAI performs in other contexts and beyond lab settings.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">What’s next?</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">StreetReaderAI is a promising first step toward making streetscape tools accessible to all. Our <a href=\"https://research.google/pubs/streetviewai-making-street-view-accessible-using-context-aware-multimodal-ai/\">study</a> highlights <i>what</i> information blind users desire from and ask about streetscape imagery and the potential for multimodal AI to answer their questions.</p><p data-block-key=\"bp879\">There are several other opportunities to expand on this work:</p><ul><li data-block-key=\"mbdd\"><b>Towards Geo-visual Agents:</b> We envision a more autonomous AI Chat agent that can explore on its own. For example, a user could ask, “<i>What’s the next bus stop down this road?</i>” and the agent could automatically navigate the Street View network, find the stop, analyze its features (benches, shelters), and report back.</li><li data-block-key=\"946n0\"><b>Supporting Route Planning:</b> Similarly, StreetReaderAI does not yet support full origin-to-destination routing. Imagine asking, “<i>What’s the walk like from the nearest subway station to the library?</i>” A future AI agent could “pre-walk” the route, analyzing every Street View image to generate a blind-friendly summary, noting potential obstacles, and identifying the exact location of the library’s door.</li><li data-block-key=\"987rq\"><b>Richer Audio Interface:</b> The primary output of StreetReaderAI is speech. We are also exploring richer, non-verbal feedback, including spatialized audio and fully immersive 3D audio soundscapes synthesized from the images themselves.</li></ul><p data-block-key=\"fokff\">Though a “proof-of-concept” research prototype, StreetReaderAI helps demonstrate the potential of making immersive streetscape environments accessible.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\"><i>This research was conducted by Jon E. Froehlich, Alexander J. Fiannaca, Nimer Jaber, Victor Tsaran, Shaun K. Kane, and Philip Nelson. We thank Project Astra and the Google Geo teams for their feedback as well as our participants. Diagram icons are from Noun Project, including: “</i><a href=\"https://thenounproject.com/icon/ai-prompt-7906362/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>prompt icon</i></a><i>” by Firdaus Faiz, “</i><a href=\"https://thenounproject.com/icon/html-7643378/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>command functions</i></a><i>” by Kawalan Icon, “</i><a href=\"https://thenounproject.com/icon/place-7892484/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>dynamic geo-context</i></a><i>” by Didik Darmanto, and “</i><a href=\"https://thenounproject.com/icon/ai-7865331/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>MLLM icon</i></a><i>” by Funtasticon.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "我们如何构建个人健康教练 (原标题: How we are building the personal health coach)",
      "link": "https://research.google/blog/how-we-are-building-the-personal-health-coach/",
      "pubDate": "Sun, 26 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-26T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 我们如何构建个人健康教练\n\n## 解决现有健康旅程的痛点\n\n文章指出，当前的健康和健身旅程存在碎片化、通用性和难以获取的问题。例如，初级保健医生可能会建议专科就诊或减肥以更好地管理糖尿病，但通常不会提供营养师或健身教练的联系方式，这使得用户需要自行连接这些服务。\n\n## 我们的愿景：AI 驱动的个人健康教练\n\n为了解决上述问题，我们的愿景是提供一个主动、个性化和适应性强的 AI 驱动的个人健康教练。该教练将无缝实现以下功能：\n\n*   **主动洞察**：提供关于睡眠、健身和健康的预见性洞察。\n*   **个性化指导**：基于行为科学、既定的健康和福祉原则，以及个人健康指标（如活动和其他生理数据）提供个性化指导。\n*   **个性化辅导**：通过可操作、适应性强的计划，帮助用户设定目标并养成可持续的习惯。\n\n## 推出与可用性\n\n从明天开始，我们将在接下来的一周内，为符合条件的美国 Fitbit Premium Android 用户推出健康教练的可选公开预览版，并很快扩展到 iOS 用户。选择参与的用户将被要求同意提供其 Fitbit 数据访问权限，以接收个性化洞察。\n\n## 技术基础与创新\n\n这项创新得益于 Gemini 模型的进步，以及 Fitbit 应用中全新的 AI 优先个人健康教练体验，以及 Fitbit、Google Research 和 Google DeepMind 在尖端研究方面的持续进展。构建一个健康和福祉产品需要时间和严谨性，我们以深思熟虑和迭代的方式进行，以科学为基础，并结合科学界和用户的反馈。\n\n### 引导 Gemini 进行健康辅导\n\n回答“我锻炼后睡眠会更好吗？”这类看似简单的问题，需要教练具备主动、个性化和适应性，这涉及多项技术创新：\n\n1.  **理解和数值推理生理时间序列数据**：教练需要理解并对睡眠和活动等生理时间序列数据进行数值推理，其能力类似于 PH-LLM 所展示的。对于此类问题，教练会验证近期数据的可用性，选择正确的指标，对比相关日期，根据个人基线和人群统计数据进行情境化分析，整合与教练的过往互动，并最终利用分析结果提供量身定制的答案和洞察。\n2.  **多智能体框架**：我们利用一个多智能体框架来协调专家子智能体，以提供清晰、一致和全面的支持，包括：\n    *   **对话智能体**：负责多轮对话、意图理解、智能体编排、上下文收集和响应生成。\n    *   **数据科学智能体**：迭代使用工具来获取、分析和总结相关数据（例如，睡眠和锻炼数据），并根据需要利用代码生成能力。\n    *   **领域专家智能体**：例如健身专家，分析用户数据以生成个性化健身计划，并根据进展和上下文变化进行调整。\n\n    ![个人健康教练的工作原理示意图](https://storage.googleapis.com/gweb-research2023-media/images/PH-coach-1.width-1250.png)\n    *个人健康教练的工作原理示意图。用户可以进行对话式提问。在后台，对话智能体处理对话、收集上下文并协调其他智能体。其他智能体包括专注于检索和分析相关数据的数据科学智能体，以及理解特定领域（如健身）的领域专家智能体。*\n\n3.  **对基础模型的精心引导**：尽管基础模型能力强大，但在健康和福祉领域，需要仔细引导才能使其发挥作用。我们开发了基于消费者健康和福祉需求的评估方法，以指导系统指令并改进 Gemini 协助用户核心能力。\n\n这些创新共同带来了更个性化和有益的指导。\n\n## 专家验证和用户迭代设计的重要性\n\n我们深知仅有技术卓越是不够的，可靠性和安全性至关重要。\n\n### 科学基础与专家反馈\n\n*   我们将教练建立在科学和成熟的辅导及健身框架之上。\n*   我们采用以人为本的设计，整合了专家和用户反馈，包括：\n    *   召集由顶尖专家组成的消费者健康咨询小组，为教练的开发提供反馈和指导。\n    *   通过专业健身教练的意见扩展运动科学基础，以纳入特定情境的方法。\n    *   开发新颖的方法与专家合作，在个人健康和福祉的细微领域促进共识。\n    *   在大型同意研究中（例如，来自 Fitbit Insights Explorer、睡眠和症状检查器实验室）积极征求了数千名用户的反馈。\n\n### 持续评估与改进\n\n我们使用 SHARP 评估框架（安全性、有用性、准确性、相关性和个性化）持续验证个人健康教练。这种多层次评估涉及超过 100 万次人工标注和超过 10 万小时的由通才和运动、睡眠、家庭医学、心脏病学、内分泌学、运动和行为科学等各个领域的专家进行的人工评估。这一过程通过自动评估器进一步扩展和规模化，以确保健康建议的科学准确性。该框架还整合了教练的实际表现，从而在最关键的领域实现持续改进。\n\n## 帮助我们构建您的教练\n\n加入公开预览版，并在应用内或通过我们的社区论坛分享您的反馈。您将帮助塑造教练，使其能为您做更多事情，并与您共同成长。",
      "shortSummary": "我们正在构建一个由AI驱动的个人健康教练，旨在解决当前健康旅程的碎片化问题。该教练基于Gemini模型和多智能体框架，提供主动、个性化和适应性的睡眠、健身及健康指导，帮助用户设定目标并养成习惯。它通过理解生理数据、专家系统和持续的用户反馈与科学验证（SHARP框架）来确保可靠性和准确性。目前已向符合条件的美国Fitbit Premium Android用户推出公开预览版。",
      "translated_title": "我们如何构建个人健康教练",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PH-coach-1.width-1250.png",
          "alt": "PH-coach-1",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fl9us\">Historically, health and fitness journeys have been fragmented, generic and inaccessible, whether within existing apps or through general health and fitness journeys outside of apps. For instance, a primary care provider might suggest seeing a specialist or losing weight for better diabetes management, but often without providing connections to a nutritionist or fitness coach. This leaves users with the burden of connecting these dots themselves.</p><p data-block-key=\"2u7lq\">Our vision is to address this by offering a proactive, personalized and adaptive AI-powered personal health coach. This coach will seamlessly enable:</p><ul><li data-block-key=\"b8to4\">Proactive insights on sleep, fitness and health</li><li data-block-key=\"9pvk4\">Personalized guidance grounded in behavioral science, established health and wellness principles and individual health metrics, such as activity and other physiological data</li><li data-block-key=\"1756a\">Personalized coaching to set goals and build sustainable habits through actionable, adaptive plans</li></ul><p data-block-key=\"1n2cb\">Starting tomorrow and over the next week, we are rolling out an optional <a href=\"https://blog.google/products/fitbit/personal-health-coach-public-preview/\" target=\"_blank\" rel=\"noopener noreferrer\">public preview of the health coach</a> for eligible US-based <a href=\"https://store.google.com/product/fitbit_premium?hl=en-US&amp;pli=1\" target=\"_blank\" rel=\"noopener noreferrer\">Fitbit Premium</a> Android users and expanding to iOS users soon. Users who opt in to participate will be presented the consent to provide access to their Fitbit data to receive personalized insights.</p><p data-block-key=\"a9d5t\">This innovation is powered by advances in <a href=\"https://ai.google.dev/gemini-api/docs/models\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini models</a> plus our new AI-first personal health coach experience on the Fitbit app, and our continuous progress in cutting-edge research across Fitbit, Google Research and Google DeepMind. Building from the ground up takes time and rigor, a commitment especially important for health and wellness. We are approaching this thoughtfully and iteratively, grounded in science, incorporating feedback from the scientific community and users. We will continue to be open about our work through publications and updates, and here we provide a quick peek into what went into building the personal health coach.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Steering Gemini for health coaching</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fl9us\">“Do I get better sleep after exercising?” sounds like a simple question, but answering it like a proactive, personalized and adaptive coach required several technical innovations.</p><p data-block-key=\"4rl0\">First, we need the coach to understand and do numerical reasoning on physiological time series data such as sleep and activity, using capabilities similar to those showcased by <a href=\"https://research.google/blog/advancing-personal-health-and-wellness-insights-with-ai/\">PH-LLM</a>. For questions like this, the coach verifies recent data availability, chooses the right metrics, contrasts relevant days, contextualizes results against personal baselines and population-level statistics, incorporates prior interactions with the coach, and finally uses the analysis to provide tailored answers and insights.</p><p data-block-key=\"f4inb\">Second, we utilize a <a href=\"https://research.google/blog/the-anatomy-of-a-personal-health-agent/\">multi-agent framework</a> that coordinates expert sub-agents to provide clear, consistent and holistic support, such as (1) a <a href=\"https://arxiv.org/pdf/2503.19328\" target=\"_blank\" rel=\"noopener noreferrer\">conversational agent</a> for multi-turn conversations, intent understanding, agent orchestration, context gathering and response generation; (2) a data science agent that iteratively uses tools to fetch, analyze, and summarize relevant data (e.g., sleep and workout data), <a href=\"https://arxiv.org/pdf/2406.06464\" target=\"_blank\" rel=\"noopener noreferrer\">leveraging code-generation capabilities as needed</a>; and (3) a domain expert, such as a fitness expert that analyzes user data to generate personalized fitness plans and adapt them as progress and context change.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PH-coach-1.width-1250.png\" alt=\"PH-coach-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PH-coach-1.width-1250.png\" alt=\"PH-coach-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"jj9h8\"><i>Schematic of how the personal health coach works. The user can ask questions conversationally. Behind the scenes, a conversation agent handles the conversation, gathers context, and orchestrates other agents. The other agents include a data science agent focusing on retrieving and analyzing relevant data, and a domain expert agent that understands specific fields such as fitness.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fl9us\">Third, while foundational models are incredibly capable, careful steer is required for it to be useful in the health and wellness context. We developed evaluations based on consumer health and wellness needs to inform the system instructions and improve upon Gemini’s core capabilities in assisting our users.</p><p data-block-key=\"einqj\"><a href=\"https://research.google/blog/the-anatomy-of-a-personal-health-agent/\">Working together</a>, these innovations result in more personalized and beneficial guidance.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The critical role of expert validation and iterative design with users</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fl9us\">We know that technical excellence alone isn’t enough, and reliability and safety are paramount.</p><p data-block-key=\"81mub\">First, we grounded the coach in scientific and well established <a href=\"https://services.google.com/fh/files/blogs/evolving_weekly_cardio_load.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">coaching and fitness frameworks</a>.</p><p data-block-key=\"7sbql\">We also used human-centered design to integrate expert and user feedback including:</p><ul><li data-block-key=\"a61ei\">Convening a <a href=\"https://store.google.com/intl/en/ideas/articles/google-consumer-health-advisory-panel/\" target=\"_blank\" rel=\"noopener noreferrer\">Consumer Health Advisory Panel</a> of leading experts to provide feedback and guidance for the development of the coach.</li><li data-block-key=\"86hoc\">Extending the sport science foundation with input with professional fitness coaches to incorporate context-specific approaches.</li><li data-block-key=\"3mvn\">Developing <a href=\"https://arxiv.org/pdf/2508.09349\" target=\"_blank\" rel=\"noopener noreferrer\">novel methods</a> to collaborate with experts, fostering consensus in nuanced areas of personal health and wellness.</li><li data-block-key=\"ih7j\">Actively soliciting feedback from thousands of users in large-scale consented research studies (e.g., from Fitbit Insights Explorer, Sleep and Symptom Checker <a href=\"https://support.google.com/fitbit/answer/14566053?hl=en#zippy=%2Cwhat-personalized-sleep-schedule-lab-is\" target=\"_blank\" rel=\"noopener noreferrer\">Labs</a>).</li></ul><p data-block-key=\"cq03p\">Finally, we continuously validate the personal health coach using dimensions of safety, helpfulness, accuracy, relevance, and personalization, collectively known as the <a href=\"https://services.google.com/fh/files/blogs/winslow_2025_sharp_framework.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">SHARP evaluation framework</a>. This multi-level assessment involves over 1 million human annotations and more than 100k hours of human evaluation by generalists and experts across various fields such as sports, sleep, family medicine, cardiology, endocrinology, exercise and behavioral science. This process is further extended and scaled with autoraters to ensure that wellness recommendations are scientifically accurate. The framework also incorporates the coach’s real-world performance, enabling ongoing improvements in the most critical areas.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Help us build your coach</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fl9us\">Join the <a href=\"https://blog.google/products/fitbit/personal-health-coach-public-preview/\" target=\"_blank\" rel=\"noopener noreferrer\">public preview</a> and share your feedback in the app or through our <a href=\"https://community.fitbit.com/t5/Personal-health-coach-public-preview/bd-p/PHAPP\" target=\"_blank\" rel=\"noopener noreferrer\">community forum</a>. You will help shape the coach, so it can do more for and with you.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Google Earth AI：利用基础模型和跨模态推理解锁地理空间洞察 (原标题: Google Earth AI: Unlocking geospatial insights with foundation models and cross-modal reasoning)",
      "link": "https://research.google/blog/google-earth-ai-unlocking-geospatial-insights-with-foundation-models-and-cross-modal-reasoning/",
      "pubDate": "Wed, 22 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-22T16:00:00.000Z",
      "creator": "Google",
      "summary": "Google Earth AI 旨在通过结合强大的基础模型和基于 Gemini 的地理空间推理代理，解决跨领域洞察的复杂挑战，实现行星尺度的复杂推理。该系统将复杂问题分解为多步骤计划，利用基础模型、海量数据存储和地理空间工具执行计划，并整合结果以提供全面的答案。\n\n### 最新创新\n\nGoogle Earth AI 引入了以下新功能：\n\n*   **新的图像和人口基础模型**：提供了详细的技术信息和评估，展示了最先进的性能。\n*   **地理空间推理代理演示**：展示了如何利用这些模型解决复杂的多步骤地理空间查询。\n\n### Earth AI 的构建模块\n\nEarth AI 的核心在于其先进的基础模型：\n\n*   **图像模型（遥感基础模型）**\n    *   **能力**：包括视觉-语言模型、开放词汇目标检测和适应性视觉骨干。\n    *   **自然语言查询**：用户可以使用自然语言提问，例如在风暴后的图像中“查找所有被洪水淹没的道路”，并获得快速准确的答案。\n    *   **性能**：模型在大量高分辨率航拍图像和文本描述语料库上进行训练，在多个公共地球观测基准上取得了最先进的结果。例如，在基于文本的图像搜索任务中平均改进超过16%，零样本新目标检测模型的基线准确率翻倍。\n    *   **图片**：\n        *   Earth AI 流程图，展示了其如何结合最先进的模型与地理空间推理代理来解决关键的全球挑战。\n        ![Flowchart of Earth AI](https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-1a-Overview.width-1250.png)\n        *   模型评估显示，遥感优化型 RS-OWL-ViT-v2 模型（“我们的”）在零样本设置下，相对于 OWL-ViT-v2 开放词汇检测模型，平均精度（AP50）有显著提高，并说明了 FLAME + RS-OWL-ViT-v2 组合方法（“我们的”）在新型类别少样本检测中相对于 SIoU 的优势。\n        ![EarthAI-2-RemoteSensingEvalFinal](https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-2-RemoteSensingEvalFinal.width-1250.png)\n\n*   **人口模型（人口动态基础模型）**\n    *   **目标**：理解人与地点之间复杂的相互作用。\n    *   **创新**：引入了两个关键创新：跨17个国家的全球一致嵌入，以及每月更新的嵌入，以捕捉人类活动不断变化的动态，这对于时间敏感的预测至关重要。\n    *   **效果**：在独立研究中表现出显著效果，例如，牛津大学研究人员发现，将这些嵌入整合到巴西登革热预测模型中，将12个月预测的长期 R²（衡量模型解释实际疾病率的指标）从0.456提高到0.656。\n    *   **图片**：\n        *   人口动态基础模型在17个国家/地区的评估；R² 分数（范围0-1，越高越好）按国家/地区预测人口密度、树木覆盖、夜间灯光和海拔。全球趋势与我们最初在美国展示的强大性能相符。\n        ![Earth AI Population Dynamics plot](https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-3-PopDynEval.width-1250.png)\n\n*   **环境模型**\n    *   **预测能力**：提供中程天气、季风、空气质量和河流洪水的最新预测。\n    *   **扩展**：最近已扩展到为全球提供降水临近预报，并为20亿人提供最严重的河流洪水预报。\n\n### 模型组合的预测能力增强\n\n研究发现，结合不同模型能够产生更强大的预测能力。这种协同方法可以更全面、准确地理解现实世界现象，并显著改善关键应用的预测。例如，通过融合人口动态基础模型中的社会经济特征嵌入和 AlphaEarth 基础模型中的景观特征，FEMA 国家风险指数对20种不同灾害的预测平均 R² 提高了11%，其中龙卷风风险预测提高了25%，河流洪水风险预测提高了17%。\n\n### 通过地理空间推理解决复杂问题\n\n由 Gemini 驱动的地理空间推理代理简化了 Earth AI 洞察的协调。该代理能够将复杂的自然语言查询分解为动态的多步骤计划，并调用“专家”子代理，利用 Earth AI 模型、Data Commons、Earth Engine 和地理空间特定工具来执行每个步骤。这种模块化的代理网络具有可扩展性和可定制性。\n\n**案例演示：识别易受风暴影响的人口**\n\n1.  调用环境模型识别可能受到飓风风力影响的特定地理区域。\n2.  查询 Data Commons 获取人口统计数据，以识别预测登陆区域内人口较多的县。\n3.  从 BigQuery 的公共数据集中检索相关县的官方边界。\n4.  对风区和官方县边界进行空间交叉。\n5.  利用人口动态基础模型和县级统计数据，动态训练模型以识别最脆弱的邮政编码。\n6.  使用遥感基础模型的目标检测功能，识别最脆弱邮政编码区域内的关键基础设施。\n\n**代理评估**\n\n*   **Q&A 基准测试**：地理空间推理代理在 Q&A 基准测试中取得了0.82的总体准确率，显著优于基线 Gemini 2.5 Pro (0.50) 和 Gemini 2.5 Flash (0.39) 代理。\n*   **图片**：\n    *   代理在 Q&A 基准测试中的表现可视化。地理空间推理代理在描述性和检索类别中比基线 Gemini 2.5 Pro 代理高出37%，在更复杂的分析和关系类别中高出124%，总分高出64%。\n    ![Earth AI performance on QA](https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-7-QAPerformance.width-1250.png)\n*   **危机响应案例研究**：论文通过案例研究展示了协调环境、遥感和人口动态等多样化洞察的益处。\n\n### 共同释放地球潜力\n\nEarth AI 代表着行星理解的根本性飞跃。多模态、基于推理的方法，建立在最先进的地理空间 AI 模型基础上，能够解锁单一分析无法实现的洞察。Google 致力于扩大访问权限，以帮助全球社区应对地球最紧迫的挑战。\n\n**实际应用案例**：\n\n*   **Bellwether (Google X)**：利用 Earth AI 预测风暴前的建筑物损坏，帮助保险客户更快支付索赔。\n*   **联合国全球脉动**：使用 Earth AI 图像模型评估自然灾害后的损失，实现快速危机响应。\n*   **GiveDirectly**：利用地理空间推理和洪水预报识别高风险社区并提供现金援助。\n*   **Google.org 资助合作伙伴**：如 Khushi Baby、Cooper/Smith 等，利用人口动态基础模型建模传染病并改善全球公共卫生行动。\n*   **新企业用户**：包括 Public Storage、CARTO 和 Visiona Space Technology。\n\nGoogle 鼓励组织表达对早期访问遥感基础模型（Vertex AI 中的图像模型）、人口动态基础模型和地理空间推理的兴趣。",
      "shortSummary": "Google Earth AI 结合强大的基础模型和基于 Gemini 的地理空间推理代理，旨在实现行星尺度的复杂地理空间洞察。它引入了新的图像和人口基础模型，并展示了其推理代理如何处理多步骤查询。通过整合遥感、人口动态和环境模型，Earth AI 显著提升了预测能力，例如在灾害风险评估和传染病建模方面。该平台已应用于预测风暴损害、评估灾后损失和提供灾害援助等领域，致力于帮助全球应对地球挑战，并邀请开发者和企业表达合作兴趣。",
      "translated_title": "Google Earth AI：利用基础模型和跨模态推理解锁地理空间洞察",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-1a-Overview.width-1250.png",
          "alt": "Flowchart of Earth AI",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-2-RemoteSensingEvalFinal.width-1250.png",
          "alt": "EarthAI-2-RemoteSensingEvalFinal",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-3-PopDynEval.width-1250.png",
          "alt": "Earth AI Population Dynamics plot",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-7-QAPerformance.width-1250.png",
          "alt": "Earth AI performance on QA",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"445ko\">For years, Google has developed AI models that enhance our understanding of the planet. These models help keep Google products fresh, for example, ensuring Maps is accurate by <a href=\"https://blog.google/technology/research/open-buildings-ai-powered-maps-for-a-changing-world/\" target=\"_blank\" rel=\"noopener noreferrer\">analyzing satellite images</a> and giving Search users the most up-to-date alerts about <a href=\"https://blog.google/intl/en-africa/products/explore-get-answers/nowcasting-on-search-is-bringing-ai-powered-weather-forecasts-to-users-across-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">weather</a> and <a href=\"https://blog.google/technology/ai/expanding-flood-forecasting-coverage-helping-partners/\" target=\"_blank\" rel=\"noopener noreferrer\">natural</a> <a href=\"https://blog.google/outreach-initiatives/sustainability/google-wildfire-boundary-maps-europe-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">disasters</a>.</p><p data-block-key=\"f8odl\">As individual models grow more powerful, we’ve learned that many real-world questions require the combination of insights <i>across</i> domains. Answering complex queries like, <i>\"Where is a hurricane likely to make landfall? Which communities are most vulnerable and how should they prepare?\"</i> requires reasoning about imagery, population and the environment.</p><p data-block-key=\"a9dl3\">Earlier this year, we introduced <a href=\"https://blog.google/technology/ai/google-earth-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Earth AI</a> to solve this core challenge. By pairing our family of powerful foundation models with a <a href=\"https://research.google/blog/geospatial-reasoning-unlocking-insights-with-generative-ai-and-multiple-foundation-models/\">geospatial reasoning</a> agent, which uses our latest Gemini models, it’s becoming possible to perform complex, real-world reasoning at planetary scale. The models provide detailed understanding of our planet, grounded in real-world data. The agent, in turn, acts as an intelligent orchestrator. It deconstructs a complex question into a multi-step plan; executes the plan by calling on these foundation models, querying vast datastores, and using geospatial tools; and finally fuses the results at each step into a holistic answer.</p><p data-block-key=\"pma0\">Today, we're <a href=\"http://goo.gle/earthai-techreport\" target=\"_blank\" rel=\"noopener noreferrer\">introducing new Earth AI innovations</a>:</p><ol><li data-block-key=\"5pdsv\">New Imagery and Population foundation models, along with technical details and evaluations showing state-of-the-art performance.</li><li data-block-key=\"22iue\">Demonstrations of our geospatial reasoning agent using these models to solve complex, multi-step geospatial queries.</li></ol><p data-block-key=\"22evo\">To learn more, we invite you to read our full technical paper, \"<a href=\"http://goo.gle/earthai-techreport\" target=\"_blank\" rel=\"noopener noreferrer\">Google Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning</a>\". You can also get involved by <a href=\"https://forms.gle/1DPfcuys2AU63HgZ8\" target=\"_blank\" rel=\"noopener noreferrer\">expressing interest</a> as we expand access to these new capabilities for developers and enterprises.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-1a-Overview.width-1250.png\" alt=\"Flowchart of Earth AI\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-1a-Overview.width-1250.png\" alt=\"Flowchart of Earth AI\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"hz039\">Earth AI unites state-of-the-art models with geospatial reasoning agents to address critical global challenges.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Building blocks of Earth AI: State-of-the-art foundation models</h2>\n            \n        \n        \n    </div>\n\n\n\n    \n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Imagery</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"445ko\">Our new Remote Sensing Foundations models simplify and accelerate satellite imagery analysis using three core capabilities: vision-language models, open-vocabulary object detection, and adaptable vision backbones. Users can ask natural language queries, like <i>\"find all flooded roads\"</i> in an image captured after a storm, and get rapid, accurate answers. Our models are trained on a large corpus of high-resolution overhead imagery, paired with text descriptions. They achieve state-of-the-art results on multiple public Earth observation benchmarks. For instance, we achieve &gt;16% average improvement on text-based image search tasks, while our zero-shot model for novel object detection more than doubles the baseline accuracy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-2-RemoteSensingEvalFinal.width-1250.png\" alt=\"EarthAI-2-RemoteSensingEvalFinal\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-2-RemoteSensingEvalFinal.width-1250.png\" alt=\"EarthAI-2-RemoteSensingEvalFinal\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"1oe99\">Model evaluation shows significant Average Precision (AP50) improvement of our Remote-Sensing optimized RS-OWL-ViT-v2 model (“Ours”) over the OWL-ViT-v2 open vocabulary detection model in a zero-shot setting and illustrates the advantage of the combined FLAME + RS-OWL-ViT-v2 approach (\"Ours\") over SIoU for few-shot detection on novel classes.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Population</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"445ko\">This area of research, which includes <a href=\"https://research.google/blog/introducing-mobility-ai-advancing-urban-transportation/\">Mobility AI</a> and <a href=\"https://research.google/blog/insights-into-population-dynamics-a-foundation-model-for-geospatial-inference/\">Population Dynamics Foundations</a>, aims to understand the complex interplay between people and places. Our latest research in Population Dynamics Foundations introduces two key innovations: globally-consistent embeddings across 17 countries and monthly updated embeddings that capture the changing dynamics of human activity, which are critical for time-sensitive predictions. Population Dynamics Foundations has shown remarkable effectiveness in independent studies; for example, researchers at the <a href=\"https://www.ox.ac.uk/\" target=\"_blank\" rel=\"noopener noreferrer\">University of Oxford</a> found that incorporating these embeddings into a forecasting model for <a href=\"https://en.wikipedia.org/wiki/Dengue_fever\" target=\"_blank\" rel=\"noopener noreferrer\">Dengue fever</a> in Brazil improved long-range <a href=\"https://en.wikipedia.org/wiki/Coefficient_of_determination\" target=\"_blank\" rel=\"noopener noreferrer\">R²</a> (a metric that measures how well a model explains the actual disease rates) from 0.456 to 0.656 for 12-month predictions.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-3-PopDynEval.width-1250.png\" alt=\"Earth AI Population Dynamics plot\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-3-PopDynEval.width-1250.png\" alt=\"Earth AI Population Dynamics plot\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"1oe99\">Evaluation of our Population Dynamics Foundations across 17 countries; R2 score (range is 0–1, higher is better) by country for predicting population density, tree cover, night time lights, and elevation. The global trend matches the strong performance we <a href=\"https://research.google/blog/insights-into-population-dynamics-a-foundation-model-for-geospatial-inference/\">originally demonstrated</a> in the US only.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/EarthAI-4-PopDynEmbeddings.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"1oe99\">Similarity per dimension of the Population Dynamics Foundations embeddings, visualized by US zip code. The patterns across dimensions capture the diverse characteristics of the US population.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Environment</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"445ko\">Our previously-published research demonstrates state-of-the-art forecasts for <a href=\"https://deepmind.google/science/weathernext/\" target=\"_blank\" rel=\"noopener noreferrer\">medium-range weather</a>, <a href=\"https://blog.google/technology/research/indian-farmers-monsoon-prediction/\" target=\"_blank\" rel=\"noopener noreferrer\">monsoon onsets</a>, <a href=\"https://blog.google/products/maps/google-maps-apis-environment-sustainability/\" target=\"_blank\" rel=\"noopener noreferrer\">air quality</a> and <a href=\"https://blog.google/technology/ai/expanding-flood-forecasting-coverage-helping-partners/\" target=\"_blank\" rel=\"noopener noreferrer\">riverine floods</a>. We've recently expanded these Environment models to make <a href=\"https://arxiv.org/abs/2510.13050\" target=\"_blank\" rel=\"noopener noreferrer\">precipitation nowcasts for the entire planet</a>, and we’re now covering 2 billion people with forecasts for the most significant riverine floods.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Increased predictive power by combining models</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"445ko\">While each foundation model provides powerful insights, our findings confirm that combining models yields even more predictive power. This synergistic approach produces a more comprehensive and accurate understanding of real-world phenomena and dramatically improves predictions across critical applications.</p><p data-block-key=\"ca795\">For example, <a href=\"https://www.fema.gov/flood-maps/products-tools/national-risk-index\" target=\"_blank\" rel=\"noopener noreferrer\">FEMA’s National Risk Index</a> shows which communities are most at risk to natural hazards like floods and storms, based on a variety of factors including economic and social vulnerability as well as physical and environmental risk. By fusing embeddings that capture socio-economic features from our Population Dynamics Foundations and landscape features from <a href=\"https://deepmind.google/discover/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/\" target=\"_blank\" rel=\"noopener noreferrer\">AlphaEarth Foundations</a>, we improved prediction of FEMA’s National Risk Index by an average of 11% in <a href=\"https://en.wikipedia.org/wiki/Coefficient_of_determination\" target=\"_blank\" rel=\"noopener noreferrer\">R²</a> across 20 different hazards, versus using either data source alone, with the most significant gains in predicting risk from tornadoes (+25% R²) and riverine flooding (+17% R²).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Complex problem-solving via Geospatial Reasoning</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"445ko\">The example above illustrates that tackling real-world problems requires insights from multiple models with diverse capabilities. Orchestrating these Earth AI insights is simplified by our new Gemini-powered Geospatial Reasoning agent. The agent deconstructs complex, natural language queries and plans a dynamic, multi-step path to an answer. To execute each step, the agent can call on “expert” sub-agents that are equipped with Earth AI models described above, as well as the vast, real-world data found in <a href=\"https://datacommons.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Data Commons</a>, <a href=\"https://earthengine.google.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Earth Engine</a>, and geospatial-specific tools. This modular network of agents allows for extensibility and customization.</p><p data-block-key=\"1vriv\">To see how it works, consider a user who wishes to identify specific populations that are vulnerable to the risk of an oncoming storm. The agent executes a transparent series of reasoning steps:</p><ol><li data-block-key=\"dtr4d\">Invoke the Environment model to identify the specific geographic areas that are forecast to be at risk of hurricane force winds.</li><li data-block-key=\"30lur\">Query <a href=\"https://datacommons.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Data Commons</a> for demographic statistics to identify higher-population counties in the area of predicted landfall.</li><li data-block-key=\"iok3\">Retrieve official boundaries for the counties of interest from <a href=\"https://cloud.google.com/bigquery/public-data\" target=\"_blank\" rel=\"noopener noreferrer\">BigQuery’s public datasets</a>.</li><li data-block-key=\"5o23i\">Perform a spatial intersection between the wind zones and official county boundaries.</li><li data-block-key=\"4vp8\">Identify the most vulnerable postal codes by training a model on the fly using our Population Dynamics Foundations and county level statistics.</li><li data-block-key=\"2te22\">Use Remote Sensing Foundations object detection model to identify critical infrastructure in satellite imagery taken over one of the most vulnerable postal codes.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/EarthAI-5a-GeospatialDemo.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"prcis\">To assist a user in understanding vulnerability to an oncoming storm, our Gemini-powered Geospatial Reasoning agent uses our Environment model to identify the likely path of hurricane force winds, intersects this with country boundaries and population density from Big Query and Data Commons, and reasons across all of this data to pick the most critical locations. It also trains a model on the fly to generate higher resolution vulnerability data using Population Dynamics Foundations. And identifies critical infrastructure in satellite imagery using Remote Sensing Foundations.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"445ko\">To evaluate the agent, we developed two new methods for evaluation: a <i>Q&amp;A benchmark</i> for fact-finding and analysis with verifiable ground truth answers based on publicly available data and <i>Crisis Response</i> case studies for complex, predictive scenarios (e.g., solving the entire challenge above).</p><p data-block-key=\"1ffva\">On the Q&amp;A benchmark, our Geospatial Reasoning Agent achieved an overall accuracy of 0.82, significantly outperforming the baseline Gemini 2.5 Pro (0.50) and Gemini 2.5 Flash (0.39) agents (scores derived from <a href=\"https://en.wikipedia.org/wiki/ROUGE_(metric)\" target=\"_blank\" rel=\"noopener noreferrer\">ROUGE-L</a> F1 and percentage error, higher is better). This highlights the importance of giving agents access to specialized geospatial models and tools for these types of queries.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-7-QAPerformance.width-1250.png\" alt=\"Earth AI performance on QA\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-7-QAPerformance.width-1250.png\" alt=\"Earth AI performance on QA\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"1oe99\">Visualizing the performance of agents on the Q&amp;A benchmark. The Geospatial Reasoning agent outperformed the baseline Gemini 2.5 Pro agent by 37% in the Descriptive and Retrieval category, and 124% in the more complex Analytical and Relational category, for an overall 64% higher score (scores derived from ROUGE-L F1 and percentage error).</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"445ko\">In the more complex <i>Crisis Response</i> scenarios, our <a href=\"http://goo.gle/earthai-techreport\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a> demonstrates the benefit of orchestrating a diverse set of Environment, Remote Sensing and Population Dynamics insights via case studies. Leveraging specialized sub-agents for geospatial and demographic analysis, we’re able to solve real-world analysis tasks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Unlocking our planet's potential, together</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"445ko\">Earth AI represents a fundamental leap in planetary understanding. Our findings show that a multimodal, reasoning-based approach, built upon a foundation of state-of-the-art geospatial AI models, can unlock insights that are intractable with siloed analysis alone.</p><p data-block-key=\"2jlht\">We are just beginning to explore the full potential of Earth AI and are committed to expanding access in order to help the global community address the planet’s most pressing challenges. For example:</p><ul><li data-block-key=\"1a7l8\"><a href=\"https://x.company/projects/bellwether/\" target=\"_blank\" rel=\"noopener noreferrer\">Bellwether</a>, a Google X moonshot, is using our weather forecasts, Population Dynamics Foundations embeddings, satellite image analysis and property databases to predict building damage before a storm strikes, helping their insurance clients pay claims faster so homeowners can start rebuilding sooner — saving them time, money and stress.</li><li data-block-key=\"25e3m\">United Nations Global Pulse uses Earth AI Imagery models to <a href=\"https://www.unglobalpulse.org/ai-from-google-research-and-un-boosts-humanitarian-disaster-response-wider-coverage-faster-damage-assessments/\" target=\"_blank\" rel=\"noopener noreferrer\">assess damage after natural disasters</a>, enabling governments and international organizations to rapidly respond to crises.</li><li data-block-key=\"bvpp3\"><a href=\"https://www.givedirectly.org/\" target=\"_blank\" rel=\"noopener noreferrer\">GiveDirectly</a> is using Geospatial Reasoning with our flood forecasts to identify at-risk communities and <a href=\"https://www.givedirectly.org/flood-forecast-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">send cash aid</a> to help households prepare for and mitigate disaster.</li></ul><p data-block-key=\"36bsf\">In addition to supporting UN Global Pulse, GiveDirectly, and other organizations using Earth AI, <a href=\"http://google.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Google.org</a> is providing funds to partners like <a href=\"https://www.khushibaby.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Khushi Baby</a>, <a href=\"https://coopersmith.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Cooper/Smith</a>, <a href=\"https://www.directrelief.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Direct Relief</a> and <a href=\"http://froncort.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Froncort.ai</a> who are utilizing Population Dynamics Foundations to model infectious diseases and improve public health action globally. New enterprise users of Earth AI include <a href=\"https://www.publicstorage.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Public Storage</a>, <a href=\"https://carto.com/\" target=\"_blank\" rel=\"noopener noreferrer\">CARTO</a> and <a href=\"https://visionaespacial.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Visiona Space Technology</a> (part of <a href=\"https://www.embraer.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Embraer</a>).</p><p data-block-key=\"e60f0\">We want to hear how Earth AI might be helpful to you. We encourage organizations to <a href=\"https://forms.gle/VmdqBHrMah6g9z948\" target=\"_blank\" rel=\"noopener noreferrer\">express interest</a> in getting early access to Remote Sensing Foundations (available as <a href=\"https://console.cloud.google.com/vertex-ai/model-garden/google/earth-ai\" target=\"_blank\" rel=\"noopener noreferrer\">Imagery models in Vertex AI</a>), Population Dynamics Foundations, and Geospatial Reasoning.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "可验证的量子优势 (原标题: A verifiable quantum advantage)",
      "link": "https://research.google/blog/a-verifiable-quantum-advantage/",
      "pubDate": "Tue, 21 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-21T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 可验证的量子优势：量子回声算法与OTOC\n\n### 引言：混沌系统与量子计算的挑战\n\n自然界中充满了混沌现象，其特点是系统对微小扰动具有高度敏感性。无论是宏观世界（如天气模式、人口动态）还是量子系统（如原子核磁化、高温超导体中的电子流），混沌都普遍存在。模拟量子混沌系统对经典计算而言极具挑战性，因为计算成本呈指数级增长，这使得量子计算机成为实现量子优势的理想平台。虽然2019年曾通过对高度混沌量子态的比特串采样展示了超越经典的量子计算，但由于比特串在大型量子系统中不重复出现，其实用性有限。\n\n### 量子回声算法与时序关联器（OTOC）\n\n在《自然》杂志封面文章“在量子遍历边缘观察到建设性干涉”中，我们介绍并实验演示了一种名为“量子回声”的量子算法。该算法的核心是测量一个量子可观测量的期望值，即“时序关联器”（Out-of-Time-Order Correlator, OTOC）。OTOC及其高阶推广是一类描述量子动力学如何变得混沌的新型可观测量。与比特串不同，电流、速度、磁化强度和密度等量子期望值是可验证的计算结果，在不同的量子计算机上运行时保持不变。期望值的广泛相关性及其可验证性，为使用OTOC解决经典计算机无法解决的实际问题指明了道路。值得注意的是，在Willow量子芯片上运行量子回声算法，对于一组基准量子电路而言，已经达到了超越经典的范畴。\n\n### OTOC的实验实现与多体干涉\n\n在实践中，OTOC代表了一系列量子操作结束后单个量子比特的状态。在Willow上运行量子回声的实验中，共有103个量子比特经历了随机量子电路形式的“正向”（U）和“反向”（U†）演化。对所有量子比特相互独立的状态施加正向演化，会使系统进入一个高度混沌且所有量子比特之间都存在量子关联的状态。在两次时间演化之间，对一个量子比特施加一个扰动（单量子比特操作B）。接着是另一个探测（单量子比特操作M）。重复此过程一到两次，即可得到一阶或二阶OTOC。在没有B的情况下，正向和反向演化会将系统恢复到初始的独立量子比特状态。而包含扰动B则会引发“蝴蝶效应”：经过这种受扰动的正向和反向演化后，整个系统最终会进入一个与初始状态截然不同的、所有量子比特之间都存在量子关联的混沌状态。\n\n我们从实验中获得的一个关键见解是，高阶OTOCs表现出复杂的量子干涉效应，类似于传统干涉仪。这被称为多体干涉，意味着许多粒子的量子态相互干涉，就像水波可能相互干涉一样，导致复杂的整体效应。在这里，扰动B和M充当了修改系统轨迹的不完美镜子。由于“往返”演化次数的增加，高阶OTOCs对扰动变得更加敏感，轨迹在B和M之间反弹。当满足共振条件（即演化U†是U的精确逆运算）时，干涉是建设性的，它会放大混沌态中存在的量子关联子集。更具体地说，这种干涉测量揭示了演化U如何在施加操作B和M的两个量子比特之间产生关联。它可以用作表征U演化的敏感工具。\n\n![OTOC-1](https://storage.googleapis.com/gweb-research2023-media/images/OTOC-1.width-1250.png)\n\n左图：测量不同阶数k的OTOC的量子电路。量子比特从基态开始，其中一个量子比特处于|𝜓M〉状态。量子处理器实现复杂的许多体演化（U），包括应用于二维网格上相邻量子比特的单量子比特和双量子比特操作。在用门B扰动一个量子比特后，演化被反转（U†），然后对最初准备的量子比特|𝜓M〉进行探测操作M。这重复k次，然后测量另一个量子比特M。右图：不同阶数OTOC作为干涉仪的概念表示。\n\n### 量子优势的关键：信号放大与计算鸿沟\n\n多体干涉的干涉性质带来了两个对实现量子优势至关重要的结果。\n\n1.  **信号放大与效率提升**\n\n    首先，正向和反向演化部分逆转了混沌效应，并放大了最终测量的量子信号。我们观察到了这种放大在OTOC信号中的特征。更具体地说，OTOC信号幅度（由随机电路集合中OTOC值分布的宽度表征）随时间呈负幂律衰减，而没有反向演化的量子信号则呈指数衰减。OTOC的缓慢幂律衰减表明，在量子计算机上测量这些量比经典模拟效率显著更高，因为经典模拟的成本随时间呈指数增长。\n\n    ![OTOC-2](https://storage.googleapis.com/gweb-research2023-media/images/OTOC-2.width-1250.png)\n\n    左图：没有时间反演（灰色）和有时间反演（品红色、蓝色、绿色）测量的信号随时间变化的值。纵轴显示了相关函数C(1)以及一阶/二阶OTOCs C(2)和C(4)在随机电路上的标准差。右图：在Willow设备上测量的一组二阶OTOC值，估计在Frontier超级计算机上模拟每个数据点需要3.2年。横轴标记了随机电路的实例。本次实验使用了105个可用量子比特中的65个。\n\n2.  **经典计算的复杂性障碍**\n\n    多体干涉的第二个结果是经典复杂性。量子计算的核心任务是识别量子计算机和经典计算机在特定计算任务上的计算成本差距。我们通过两种方式解决了这个问题：(1) 通过理论分析和实验相结合，揭示了已知经典算法在实现与我们在Willow上进行的OTOC计算相同结果方面的根本障碍；(2) 通过直接实现和成本估算，测试了九种相关经典模拟算法的性能。\n\n    在第一种方法中，我们发现量子干涉是经典计算的障碍。量子力学的一个显著特征是，预测实验结果需要分析概率幅，而不是像经典力学那样分析概率。一个著名的例子是光的纠缠，它表现为光子（光的基元粒子）之间持续很长距离的量子关联，或超导电路中的宏观量子隧穿现象。我们二阶OTOC数据中的干涉（即经过两次反向和正向电路循环的OTOC）揭示了概率和概率幅之间的类似区别。至关重要的是，概率是非负数，而概率幅可以是任意符号，并由复数描述。总而言之，这些特征意味着它们包含的信息集合要复杂得多。我们的实验不是一对光子或一个超导结，而是由65个量子比特的指数级大空间中的概率幅来描述的。对这样一个量子力学系统进行精确描述需要存储和处理内存中2^65个复数，这超出了超级计算机的能力。此外，我们电路中的量子混沌确保了每个概率幅都同等重要，因此使用压缩描述系统的算法所需的内存和处理时间也超出了超级计算机的能力。\n\n    我们进一步的理论和实验分析表明，要通过数值计算预测我们的实验数据，必须仔细考虑概率幅的符号。这给一类高效的经典算法（量子蒙特卡洛）带来了重大障碍，这些算法在描述大型量子力学空间中的量子现象（例如液氦-4的超流性）方面取得了成功。这些算法依赖于概率描述，但我们的分析表明，这种方法会导致计算输出中出现无法控制的误差。我们对依赖于压缩表示和高效量子蒙特卡洛的算法的直接实现证实了预测二阶OTOC数据的不可行性。我们在Willow上的实验大约耗时2小时，而经典超级计算机估计需要13,000倍的时间来完成这项任务。这一结论是在估计投入10个人年进行经典红队测试我们的量子结果，并实现了总共九种经典模拟算法之后得出的。\n\n### OTOC电路的实际应用：哈密顿量学习\n\n在确立了OTOC超越经典的复杂性之后，我们开始探索如何将其应用于解决具有实际意义的现实世界问题。为此，我们提出了哈密顿量学习方案，即量子计算机模拟自然界中物理系统（如分子）的OTOC信号，这些系统的参数尚未完全已知。然后，我们将量子计算机的OTOC信号与有关物理系统的真实世界数据进行比较，并观察它们何时最吻合。通过寻找这种吻合，我们旨在获得比其他技术更精确的系统参数估计。为了使该方案实用化，我们必须找到自然界中可以执行我们的量子回声算法的系统，并在我们的量子硬件上模拟这些系统。作为实现这一目标的一步，在“通过多体核自旋回声进行分子几何的量子计算”中，我们展示了使用核磁共振（NMR）光谱学验证了这一概念。在NMR中，人们利用核自旋在强磁场中的进动来了解分子和材料的结构，例如人体中的蛋白质或手机中的电池组件。核自旋遵循量子力学定律，在某些条件下（即在固体或类固体材料中）它们表现出上述相同的量子混沌行为。这使得它们成为OTOC协议的完美候选者。\n\n在这份将提交同行评审的预印本中，我们在加州大学伯克利分校派恩斯磁共振中心测量了溶解在液晶中的两种有机分子的OTOCs。然后，该实验在我们的Willow芯片上进行了模拟，从而改进了分子结构的模型。由于模拟真实世界系统的固有复杂性以及我们当前芯片的性能限制，这一初步演示尚未超越经典。然而，我们的结果展示了对分子细节的敏感性，我们相信这条道路将带来量子计算的首批有用应用。\n\n![OTOC-3](https://storage.googleapis.com/gweb-research2023-media/images/OTOC-3.width-1250.png)\n\n通过量子计算机改进物理量子系统知识的示意图，称为哈密顿量学习。\n\n### 结论\n\n我们进行了首次量子计算实验，测量了一个既可以通过另一台量子计算机或自然量子系统进行验证，又超越了已知经典算法模拟能力的量子可观测量。这项实验得益于我们最近的硬件进步，为量子计算机在探测分子等物理系统的微观结构方面的首次实际应用铺平了道路。\n\n### 致谢\n\n这项工作涉及量子AI团队的许多成员，以及Google DeepMind和加州大学伯克利分校、达特茅斯学院、QSimulate和NVIDIA的外部合作者。",
      "shortSummary": "研究人员提出并实验验证了“量子回声”算法，通过测量可验证的“时序关联器”（OTOC）期望值，实现了超越经典计算的量子优势。OTOC能描述量子混沌，其结果在Willow量子芯片上比超级计算机快13,000倍。该算法利用多体干涉放大量子信号，并因量子概率幅的复杂性而使经典模拟面临根本障碍。这项突破为量子计算机在哈密顿量学习等实际应用中探测分子微观结构铺平了道路，有望改进分子结构模型。",
      "translated_title": "可验证的量子优势",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/OTOC-1.width-1250.png",
          "alt": "OTOC-1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/OTOC-2.width-1250.png",
          "alt": "OTOC-2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/OTOC-3.width-1250.png",
          "alt": "OTOC-3",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"9rtfc\">Nature is brimming with chaos, a phenomenon characterized by the high sensitivity of a system toward small perturbations. In the macroscopic world, notable examples of chaotic systems include weather patterns, wherein a small change in initial conditions leads to vastly different outcomes over time (often dubbed “the <a href=\"https://en.wikipedia.org/wiki/Butterfly_effect\" target=\"_blank\" rel=\"noopener noreferrer\">butterfly effect</a>”), and population dynamics, where small shifts in local populations may eventually affect the entire ecosystem. Chaos is similarly abundant in quantum systems, with examples including the <a href=\"https://en.wikipedia.org/wiki/Residual_dipolar_coupling\" target=\"_blank\" rel=\"noopener noreferrer\">dynamics of magnetization of atomic nuclei</a> when subjected to a time-varying magnetic field, and the <a href=\"https://en.wikipedia.org/wiki/Fermi_liquid_theory#Non-Fermi_liquids\" target=\"_blank\" rel=\"noopener noreferrer\">flow of electrons in high-temperature superconductors</a>.</p><p data-block-key=\"50aui\">Simulating quantum-chaotic systems is challenging for classical computation due to exponentially scaling computational cost, making quantum computers ideal for achieving quantum advantage. In 2019, we demonstrated the first <a href=\"https://research.google/blog/quantum-supremacy-using-a-programmable-superconducting-processor/\">beyond-classical quantum computation</a> by sampling bitstrings from a highly chaotic quantum state of <a href=\"https://en.wikipedia.org/wiki/Qubit\" target=\"_blank\" rel=\"noopener noreferrer\">qubits</a>. However, this <a href=\"https://research.google/blog/validating-random-circuit-sampling-as-a-benchmark-for-measuring-quantum-progress/\">random circuit sampling</a> approach has limited practical utility since the same bitstring never appears twice in a large quantum system, restricting its ability to reveal useful information.</p><p data-block-key=\"e7hrb\">In “<a href=\"https://doi.org/10.1038/s41586-025-09526-6\" target=\"_blank\" rel=\"noopener noreferrer\">Observation of constructive interference at the edge of quantum ergodicity</a>”, featured on the cover of <a href=\"https://www.nature.com/articles/s41586-025-09526-6\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature</i></a>, we introduce and experimentally demonstrate a quantum algorithm which we call Quantum Echoes. The heart of the algorithm is measuring the expectation value of a quantum <a href=\"https://en.wikipedia.org/wiki/Observable#:~:text=In%20physics%2C%20an%20observable%20is,of%20the%20space%20in%20question.\" target=\"_blank\" rel=\"noopener noreferrer\">observable</a>, called the <a href=\"http://www.scholarpedia.org/article/Out-of-time-order_correlations_and_quantum_chaos\" target=\"_blank\" rel=\"noopener noreferrer\">out-of-time-order correlator</a> (OTOC). OTOC and its higher order generalizations are a new family of observables that describe how quantum dynamics become chaotic. Unlike bitstrings, quantum expectation values, e.g., current, velocity, magnetization and density, are verifiable computational outcomes that remain the same when run on different quantum computers. The wide relevance of expectation values combined with their verifiability indicates a direct path toward using OTOCs to solve real-world problems using quantum computers, which are not possible to solve on classical computers. Remarkably, we show that running the Quantum Echoes algorithm on the <a href=\"https://blog.google/technology/research/google-willow-quantum-chip/\" target=\"_blank\" rel=\"noopener noreferrer\">Willow</a> quantum chip is already in the beyond-classical regime for a set of benchmarking quantum circuits.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Out-of-time-order correlator</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9rtfc\">In practice, OTOC represents the state of a single qubit at the end of a series of quantum operations. In our experiments running Quantum Echoes on <a href=\"https://blog.google/technology/research/google-willow-quantum-chip/\" target=\"_blank\" rel=\"noopener noreferrer\">Willow</a>, a total of 103 qubits underwent both “forward” (<i>U</i>) and “backward” (<i>U</i><sup class=\"superscript\">†</sup>) evolutions in the form of random quantum circuits. A forward evolution applied to a state where all qubits are independent from each other brings the system to a highly chaotic state with quantum correlations across all qubits. A perturbation, a one-qubit operation <i>B</i>, is applied to a qubit in between the two time evolutions. This circuit is followed by another probe, a one-qubit operation <i>M</i>. Repeating this process once or twice leads to an OTOC of first or second order. In absence of <i>B</i> the forward and backward evolution returns the system to the initial state, where all qubits are independent. Inclusion of the perturbation <i>B</i> sets off a butterfly effect: after such perturbed forward and backward evolution, the whole system ends in a chaotic state with quantum correlations across all qubits that is very different from the initial state.</p><p data-block-key=\"dh297\">A crucial insight we obtained from our experiments is that higher-order OTOCs exhibit complex quantum interference effects analogous to a traditional <a href=\"https://en.wikipedia.org/wiki/Interferometry\" target=\"_blank\" rel=\"noopener noreferrer\">interferometer</a>. This is known as many-body interference, meaning the quantum states of many particles interfere with each other, much like waves of water might interfere, leading to complex overall effects. Here the perturbations, <i>B</i> and <i>M</i>, act as imperfect mirrors that modify the system’s trajectories. Higher order OTOCs become more sensitive to the perturbation due to increasing number of “round trip” evolutions, where the trajectories bounce off of <i>B</i> and <i>M</i>. When a <a href=\"https://en.wikipedia.org/wiki/Resonance\" target=\"_blank\" rel=\"noopener noreferrer\">resonance condition</a> is satisfied, which corresponds to evolution <i>U</i><sup class=\"superscript\">†</sup> being the exact inverse of <i>U</i>, the interference is constructive and it amplifies the subset of quantum correlations from the totality of those present in the chaotic state. More specifically, this interferometry reveals how the evolution <i>U</i> generates correlations between the two qubits where operations <i>B</i> and <i>M</i> were applied. It can be used as a sensitive instrument to characterize the evolution of <i>U</i>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/OTOC-1.width-1250.png\" alt=\"OTOC-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/OTOC-1.width-1250.png\" alt=\"OTOC-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"pqe83\"><b><i>Left</i></b><i>: Quantum circuit measuring OTOCs of different orders, k. Qubits are initiated in the ground state, with one qubit in the state denoted by |𝜓M〉. A complex many-body evolution (U) is implemented by the quantum processor, consisting of one- &amp; two-qubit operations applied to neighboring qubits on a two-dimensional grid. The evolution is reversed (U</i><i><sup class=\"superscript\">†</sup></i><i>) after perturbing one qubit with gate B, followed by a probe operation M on the initially prepared qubit |𝜓M〉. This repeats k times before measuring another qubit M.</i> <b><i>Right</i></b><i>: Conceptual representation of OTOCs of different order as interferometers.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"9rtfc\">The interference nature of the OTOC leads to two consequences crucial for attaining quantum advantage. First, the forward and backward evolutions partially reverse the effects of chaos and amplify the quantum signal measured at the end. We observed the signature of this amplification in OTOC signals. More specifically, OTOC signal magnitude, characterized by the width of the distribution of OTOC values over the ensemble of random circuits, scales as a negative <a href=\"https://en.wikipedia.org/wiki/Power_law\" target=\"_blank\" rel=\"noopener noreferrer\">power</a> of time, whereas quantum signals measured without back evolutions decay exponentially. The slow power law decay of OTOCs suggests that measuring these quantities on a quantum computer is significantly more efficient than classical simulations, where costs increase exponentially over time.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/OTOC-2.width-1250.png\" alt=\"OTOC-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/OTOC-2.width-1250.png\" alt=\"OTOC-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"pqe83\"><b><i>Left</i></b><i>: Time-dependent values of signals measured without time inversion (</i><b><i>gray</i></b><i>) and with time inversion (</i><b><i>magenta</i></b><i>,</i> <b><i>blue</i></b><i>,</i> <b><i>green</i></b><i>). The vertical axis shows standard deviation over random circuits for correlation function, C</i><i><sup class=\"superscript\">(1)</sup></i><i>, and the first/second order OTOCs, C</i><i><sup class=\"superscript\">(2)</sup></i> <i>and C</i><i><sup class=\"superscript\">(4)</sup></i><i>.</i> <b><i>Right</i></b><i>: A set of 2nd order OTOC values measured on a Willow device that are estimated to require 3.2 years to simulate each data point on the</i> <a href=\"https://en.wikipedia.org/wiki/Frontier_(supercomputer)\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Frontier supercomputer</i></a><i>. The horizontal axis labels instances of random circuits. A total of 65 qubits out of the 105 available qubits are used in this experiment.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The computational gap between quantum and classical processors</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9rtfc\">The second consequence of many-body interference is classical complexity. A central task for quantum computing is to identify the computational cost gap between quantum and classical computers on specific computational tasks. We approached this in two ways: (1) through a combination of theoretical analysis and experiments, we revealed the fundamental obstacles to known classical algorithms in achieving the same outcome as our OTOC calculations on Willow, and (2) we tested the performance of nine relevant classical simulation algorithms by direct implementation and cost estimation.</p><p data-block-key=\"bngck\">In the first approach we identified that quantum interference is an obstacle for classical computation. A distinct characteristic of quantum mechanics is that predicting an outcome of an experiment requires analyzing <a href=\"https://en.wikipedia.org/wiki/Probability_amplitude\" target=\"_blank\" rel=\"noopener noreferrer\">probability amplitudes</a> rather than probabilities as in classical mechanics. A well known example is the entanglement of light that manifests in quantum correlations between photons, elementary particles of light, that persist over long distances (<a href=\"https://www.nobelprize.org/prizes/physics/2022/summary/\" target=\"_blank\" rel=\"noopener noreferrer\">2022 Physics Nobel Laureates</a>) or macroscopic quantum tunneling phenomena in superconducting circuits (<a href=\"https://blog.google/inside-google/company-announcements/googler-michel-devoret-awarded-the-nobel-prize-in-physics/\" target=\"_blank\" rel=\"noopener noreferrer\">2025 Physics Nobel Laureates</a>).</p><p data-block-key=\"86a3m\">The interference in our second order OTOC data (i.e., an OTOC that runs through the backward and forward circuit loop twice) reveals a similar distinction between probabilities and probability amplitudes. Crucially, probabilities are non-negative numbers, whereas probability amplitudes can be of an arbitrary sign and are described by complex numbers. Taken together, these features mean they contain a much more complex collection of information. Instead of a pair of photons or a single superconducting junction, our experiment is described by probability amplitudes across an exponentially large space of 65 qubits. An exact description of such a quantum mechanical system requires storing and processing 2<sup class=\"superscript\">65</sup> complex numbers in memory, which is beyond the capacity of supercomputers. Moreover, quantum chaos in our circuits ensures that every amplitude is equally important, and therefore algorithms using a compressed description of the system require memory and processing time beyond the capacity of supercomputers.</p><p data-block-key=\"4e84g\">Our further theoretical and experimental analysis revealed that carefully accounting for the signs of the probability amplitudes is necessary to predict our experimental data by a numerical calculation. This presents a significant barrier for a class of efficient classical algorithms, <a href=\"https://en.wikipedia.org/wiki/Quantum_Monte_Carlo\" target=\"_blank\" rel=\"noopener noreferrer\">quantum Monte Carlo</a>, that have been successful at describing quantum phenomena in a large quantum mechanical space (e.g., <a href=\"https://en.wikipedia.org/wiki/Superfluid_helium-4\" target=\"_blank\" rel=\"noopener noreferrer\">superfluidity of liquid Helium-4</a>). These algorithms rely on description in terms of probabilities, yet our analysis demonstrates that such approaches would result in an uncontrollable error in the computation output.</p><p data-block-key=\"br31a\">Our direct implementation of algorithms relying on both compressed representation and efficient quantum Monte Carlo confirmed the impossibility of predicting second-order OTOC data. Our experiments on Willow took approximately 2 hours, a task estimated to require 13,000 times longer on a classical supercomputer. This conclusion was reached after an estimated 10 person years spent in classical red teaming of our quantum result, implementing a total of nine classical simulation algorithms as a result.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Practical application of OTOC circuits</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9rtfc\">Having established the beyond-classical complexity of OTOCs, we began exploring how they could be applied to solving real-world problems of practical interest. To this end, we proposed <a href=\"https://en.wikipedia.org/wiki/Inverse_problem\" target=\"_blank\" rel=\"noopener noreferrer\">Hamiltonian learning</a>, a scheme where the quantum computer simulates OTOC signals from a physical system in nature, such as molecules, whose system parameters are not fully known. Then, we compare the quantum computer OTOC signals against real-world data about the physical system and observe when they best agree. By looking for this agreement, we aim to obtain a more precise estimate of system parameters than what is possible through other techniques.</p><p data-block-key=\"3dgr4\">To make this scheme practical, we have to find systems in nature that can perform our Quantum Echoes algorithm, and simulate these systems on our quantum hardware. As a step toward this goal, in \"<a href=\"https://quantumai.google/static/site-assets/downloads/quantum-computation-molecular-geometry-via-nuclear-spin-echoes.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Quantum computation of molecular geometry via many-body nuclear spin echoes</a>”, we show that we tested this concept using <a href=\"https://en.wikipedia.org/wiki/Nuclear_magnetic_resonance\" target=\"_blank\" rel=\"noopener noreferrer\">nuclear magnetic resonance</a> (NMR) spectroscopy. In NMR, one uses the precession of nuclear spins in a large magnetic field to learn the structure of molecules and materials, like the proteins in your body or the battery components in your phone. Nuclear spins obey the laws of quantum mechanics, and under certain conditions (namely in solids or solid-like materials) they demonstrate the same quantum-chaotic behavior described above. This makes them a perfect candidate for the OTOC protocol.<br></p><p data-block-key=\"dfme5\">In this pre-print, which will be submitted for peer review, we measured OTOCs on two organic molecules dissolved in liquid crystal at the <a href=\"https://pmrc.berkeley.edu/\" target=\"_blank\" rel=\"noopener noreferrer\">Pines Magnetic Resonance Center</a> at UC Berkeley. This experiment was then simulated on our Willow chip, resulting in improved models of the molecular structure. Due to the inherent complexity of simulating real-world systems and performance limits of our current chip, this initial demonstration is not yet beyond classical. However, our results demonstrate sensitivity to molecular details and we're confident that this path will lead to some of the first useful applications of quantum computation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/OTOC-3.width-1250.png\" alt=\"OTOC-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/OTOC-3.width-1250.png\" alt=\"OTOC-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"pqe83\"><i>A schematic for refining knowledge of a physical quantum system through the quantum computer, known as Hamiltonian learning.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9rtfc\">We have performed the first quantum computing experiment measuring a quantum observable that is both verifiable through another quantum computer or a natural quantum system, and beyond the simulation capacity of known classical algorithms. This experiment was made possible by our recent <a href=\"https://blog.google/technology/research/quantum-hardware-verifiable-advantage/\" target=\"_blank\" rel=\"noopener noreferrer\">hardware advancement</a>, and paves the way toward the first real-world application of quantum computers in probing the microscopic structures of physical systems such as molecules.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9rtfc\"><i>This work involved many members of the Quantum AI team, along with Google DeepMind and external collaborators at</i> <a href=\"https://www.berkeley.edu/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>UC Berkeley</i></a><i>,</i> <a href=\"https://home.dartmouth.edu/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Dartmouth College</i></a><i>,</i> <a href=\"https://qsimulate.com/home\" target=\"_blank\" rel=\"noopener noreferrer\"><i>QSimulate</i></a><i>, and</i> <a href=\"https://www.nvidia.com/en-us/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>NVIDIA</i></a><i>.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "一张图片胜过千言（隐私）：连贯合成相册的分层生成 (原标题: A picture's worth a thousand (private) words: Hierarchical generation of coherent synthetic photo albums)",
      "link": "https://research.google/blog/a-pictures-worth-a-thousand-private-words-hierarchical-generation-of-coherent-synthetic-photo-albums/",
      "pubDate": "Sun, 19 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-19T16:00:00.000Z",
      "creator": "Google",
      "summary": "本文介绍了一种创新方法，用于私密地生成连贯的合成相册，以满足现代AI对既保护隐私又具有丰富结构和上下文的多模态数据的需求。\n\n### 差分隐私（DP）与生成式AI的挑战\n\n*   **差分隐私（DP）**：提供严格的数学保证，确保数据集中敏感的个人信息受到保护，即使数据用于分析。\n*   **传统DP的复杂性**：近二十年来，研究人员为各种数据分析和机器学习方法开发了DP版本，但这要求组织对每种分析技术进行私有化，过程复杂、繁琐且易出错。\n*   **生成式AI的解决方案**：像Gemini这样的生成式AI模型提供了一个更简单、高效的方案。它们不需单独修改每种分析方法，而是创建一个原始数据集的单一私有合成版本。\n    *   **合成数据特性**：这种合成数据融合了常见的数据模式，不包含任何个体用户的独特细节。\n    *   **隐私与代表性**：通过使用差分隐私训练算法（如DP-SGD）对生成模型进行微调，确保合成数据集既私密又高度代表真实数据。\n    *   **简化工作流程**：任何标准的、非私有分析技术或建模都可以在这个安全（且高度代表性）的替代数据集上进行，从而简化工作流程。\n*   **现有工作的局限性**：大多数关于私有合成数据生成的研究都集中在简单的输出，如短文本或单个图像。然而，使用多模态数据（图像、视频等）的现代应用依赖于对复杂真实世界系统和行为的建模，这是简单、非结构化文本数据无法充分捕捉的。\n\n### 新方法：连贯合成相册的分层生成\n\n为了解决对丰富、结构化图像数据集的合成版本需求，研究人员引入了一种私密生成合成相册的新方法。这项任务带来了超越生成单个图像的独特挑战，特别是需要在一个顺序相册中保持多个照片的主题连贯性和角色一致性。\n\n该方法基于将复杂的图像数据转换为文本，然后再转换回来，并采用分层生成策略。\n\n#### 工作原理\n\n1.  **生成结构化文本表示**：\n    *   用AI模型为相册中的每张照片生成详细的文本描述（caption）。\n    *   使用AI模型为每个相册生成文本摘要（summary）。\n2.  **私有微调大型语言模型（LLMs）**：\n    *   私有微调一对大型语言模型以生成类似的结构化表示。\n    *   第一个模型训练用于生成相册摘要。\n    *   第二个模型训练用于基于相册摘要生成单个照片描述。\n3.  **分层生成结构化表示**：\n    *   首先生成相册的摘要。\n    *   然后，以该摘要为上下文，生成相册中每张照片的详细文本描述。\n4.  **文本到图像转换**：\n    *   将生成的结构化表示（文本）使用文本到图像的AI模型转换为图像集。\n\n![Illustration of our method for generating synthetic photo albums.](https://storage.googleapis.com/gweb-research2023-media/images/PGSD-1-Cartoon.width-1250.png)\n*图示：合成相册生成方法概述。*\n\n#### 中间文本表示的优势\n\n*   **LLM的优势**：文本生成是大型语言模型的主要优势。\n*   **隐私增强**：文本摘要本质上增强了隐私，因为用文本描述图像是一个有损操作，即使没有启用差分隐私，合成照片也不太可能是原始照片的精确副本。\n*   **资源效率**：生成图像的成本远高于生成文本。通过首先生成文本，可以在花费资源生成图像之前根据内容筛选相册。\n\n#### 分层生成策略的优势\n\n*   **内部一致性**：确保相册中的照片内部一致，因为相册中的每张照片描述都是在相同的相册摘要上下文下生成的。\n*   **计算资源节省**：分两步（先相册摘要，后照片描述）生成结构化表示，相对于一步生成所有表示，显著节省了计算资源。由于训练成本随上下文长度呈二次方增长，训练两个上下文较短的模型远比训练一个上下文较长的模型成本低。\n\n#### 文本作为中间体的效用演示（非DP）\n\n通过一个简单的演示（未启用差分隐私，以便进行并排比较），展示了这种方法的强大之处。研究人员提示Gemini用数百个词描述一张图像，然后将描述文本反馈给Gemini，提示它生成一张与描述匹配的图像。这说明了文本作为合成图像生成中间体的实用性。\n\n![Privately generated synthetic photo example](https://storage.googleapis.com/gweb-research2023-media/images/PGSD-2-Example.width-1250.png)\n*图示：左侧为原始图像，右侧为根据Gemini对原始图像的文本描述生成的合成图像。*\n\n### 评估与结果\n\n该方法在YFCC100M数据集上进行了测试，该数据集包含近1亿张在知识共享许可下发布的图像。通过将同一用户在同一小时内拍摄的照片分组，形成了“相册”。为大型语言模型构建了训练集，并确保每个用户对任何训练集的贡献不超过一个示例，以保证差分隐私的有效性。\n\n生成合成相册后，评估了它们与原始相册的相似程度：\n\n1.  **MAUVE分数**：计算了原始和合成结构化表示（相册摘要和照片描述）之间的MAUVE分数，这是一种基于神经嵌入的语义相似性度量。\n    ![MAUVE scores between real and synthetic album summaries & captions](https://storage.googleapis.com/gweb-research2023-media/images/PGSD-3-Mauve.width-1250.png)\n    *图示：真实与合成相册摘要及照片描述之间的MAUVE分数。MAUVE分数越高表示相似度越大。隐私参数ε值越高意味着隐私约束越弱。*\n2.  **常见主题分析**：计算了相册摘要中最常见的主题，发现真实数据和合成数据之间非常相似。\n    ![Real album summaries vs synthetic album summaries](https://storage.com/gweb-research2023-media/images/PGSD-4-Topics.width-1250.png)\n    *图示：真实相册摘要与合成相册摘要中最常见的主题对比。*\n3.  **直接视觉检查**：合成相册的直接视觉检查显示，每个相册通常都围绕一个共同主题，就像真实的相册一样。\n    ![Privately generated synthetic photo albums](https://storage.googleapis.com/gweb-research2023-media/images/PGSD-5-Albums.width-1250.png)\n    *图示：两个私密生成的合成相册示例。每个相册都保持一个特定主题（上：采苹果之旅；下：情侣参观草地）。*\n\n### 结论\n\n现代AI的挑战需要私密、结构化且上下文丰富的数据，而简单的非结构化数据无法满足这一需求。通过将分层、以文本为中间体的方法应用于生成连贯合成相册的艰巨任务，研究人员成功展示了将合成数据的好处扩展到简单文本或孤立图像之外的途径。\n\n这种方法为隐私保护AI创新开辟了激动人心的新途径，有助于解决对大量高质量数据需求与保护用户隐私之间长期存在的矛盾，为关键行业更安全、更普遍的AI发展铺平了道路。",
      "shortSummary": "该研究提出一种分层生成连贯合成相册的新方法，以解决现代AI对私密、结构化多模态数据的需求。传统差分隐私（DP）复杂，而生成式AI通过创建私有合成数据集简化了流程。新方法将图像数据转换为文本描述和相册摘要，利用私有微调的大型语言模型生成新的文本表示，再将其转换为图像。这种方法确保了相册内照片的主题一致性和隐私保护，同时提高了计算效率，为隐私保护AI创新开辟了新途径。",
      "translated_title": "一张图片胜过千言（隐私）：连贯合成相册的分层生成",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PGSD-1-Cartoon.width-1250.png",
          "alt": "Illustration of our method for generating synthetic photo albums.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PGSD-2-Example.width-1250.png",
          "alt": "Privately generated synthetic photo example",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PGSD-3-Mauve.width-1250.png",
          "alt": "MAUVE scores between real and synthetic album summaries & captions",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PGSD-4-Topics.width-1250.png",
          "alt": "Real album summaries vs synthetic album summaries",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PGSD-5-Albums.width-1250.png",
          "alt": "Privately generated synthetic photo albums",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"e3q59\"><a href=\"https://en.wikipedia.org/wiki/Differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">Differential privacy</a> (DP) provides a powerful, mathematically rigorous assurance that sensitive individual information in a dataset remains protected, even when a dataset is used for analysis. Since DP’s <a href=\"https://link.springer.com/chapter/10.1007/11681878_14\" target=\"_blank\" rel=\"noopener noreferrer\">inception nearly two decades ago</a>, researchers have developed differentially private versions of myriad data analysis and machine learning methods, ranging from calculating simple statistics to <a href=\"https://research.google/blog/protecting-users-with-differentially-private-synthetic-training-data/\">fine-tuning complex AI models</a>. However, the requirement for organizations to privatize every analytical technique can be complex, burdensome, and error-prone.</p><p data-block-key=\"dbfdq\">Generative AI models like <a href=\"https://gemini.google.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a> offer a simpler, more efficient solution. Instead of separately modifying every analysis method, they create a single private synthetic version of the original dataset. This synthetic data is an amalgamation of common data patterns, containing no unique details from any individual user. By using a differentially private training algorithm, such as <a href=\"https://arxiv.org/abs/1607.00133\" target=\"_blank\" rel=\"noopener noreferrer\">DP-SGD</a>, to fine-tune the generative model on the original dataset, we ensure the synthetic dataset is both private and highly representative of the real data. Any standard, non-private analytical technique or modeling can then be performed on this safe (and highly representative) substitute dataset, simplifying workflows. DP fine-tuning is a versatile tool that is particularly valuable for generating high-volume, controlled datasets in situations where access to high-quality, representative data is unavailable.</p><p data-block-key=\"fiueb\">Most published work on private synthetic data generation has focused on <a href=\"https://arxiv.org/pdf/2210.14348\" target=\"_blank\" rel=\"noopener noreferrer\">simple outputs</a> like short text passages or individual images, but modern applications using multi-modal data (images, video, etc.) rely on modeling complex, real-world systems and behaviors, which simple, unstructured text data cannot adequately capture.</p><p data-block-key=\"794po\">We introduce a new method for privately generating synthetic photo albums as a way to address this need for synthetic versions of rich, structured image-based datasets. This task presents unique challenges beyond generating individual images, specifically the need to maintain thematic coherence and character consistency across multiple photos within a sequential album. Our method is based on translating complex image data to text and back. Our results show that this process, with rigorous DP guarantees enabled, successfully preserves the high-level semantic information and thematic coherence in datasets necessary for effective analysis and modeling applications.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How (and why) our method works</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"e3q59\">Our method differs from most other approaches to generating private synthetic image data in two major respects: (1) we use an intermediate text representation and (2) we generate the data hierarchically.</p><p data-block-key=\"mmvg\">Here’s how it works:</p><ol><li data-block-key=\"13lt2\">We generate a structured text representation of each original album, replacing each photo in the album with an AI-generated detailed text caption, and also using an AI model to produce a text summary of each album.</li><li data-block-key=\"ddg4l\">We then privately fine-tune a pair of large language models to produce similar structured representations. The first model is trained to generate album summaries, and the second model is trained to generate individual photo captions based on an album summary.</li><li data-block-key=\"5o70l\">We use the models to generate structured representations of photo albums in a hierarchical manner. For each photo album, we first generate a summary of the album, and then using that summary as context, we generate a detailed text caption of each photo in the album.</li><li data-block-key=\"466tp\">The generated structured representations are then converted into sets of images using a text-to-image AI model.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-1-Cartoon.width-1250.png\" alt=\"Illustration of our method for generating synthetic photo albums.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-1-Cartoon.width-1250.png\" alt=\"Illustration of our method for generating synthetic photo albums.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cnnu1\"><i>Illustration of our method for generating synthetic photo albums.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"e3q59\">Generating text as an intermediate step towards generating images has a number of advantages. First, text generation is the main strength of a large language model. Second, text summarization is inherently privacy enhancing, since describing an image by text is a lossy operation, so synthetic photos are unlikely to be exact copies of the originals, even when differential privacy is not enabled. Finally, generating images is far more costly than generating text, so by first generating text, we can filter albums based on their content before expending resources to produce the images in which we are most interested.</p><p data-block-key=\"1lmil\">Our hierarchical generation strategy ensures that the photos in each album are internally consistent, since each photo caption in an album is generated with the same album summary as context. Also, generating the structured representations in two steps (first the album summaries, and then the photo captions) preserves significant computational resources relative to generating each representation in one shot. Since training cost scales quadratically with context length (due to <a href=\"https://arxiv.org/pdf/2209.04881\" target=\"_blank\" rel=\"noopener noreferrer\">self-attention</a>), training two models with shorter contexts is far less costly than training a single model with a long context.</p><p data-block-key=\"1si9j\">It may seem that describing images with words is too lossy an operation to preserve any interesting characteristics of the original images, but a simple demonstration (without differential privacy, to allow for side-by-side comparison) illustrates the power of this approach. In the figure below, we prompted Gemini to describe an image using several hundred words, and then fed the response text back to Gemini, prompting it to generate an image matching the description. While this circular series of transformations does not satisfy differential privacy, it does illustrate the utility of text as an intermediary for synthetic image generation. As the saying goes, a picture is worth a thousand words — and it seems that it is not worth much more than that!</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-2-Example.width-1250.png\" alt=\"Privately generated synthetic photo example\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-2-Example.width-1250.png\" alt=\"Privately generated synthetic photo example\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cnnu1\"><b><i>Left:</i></b><i> Original image.</i> <b><i>Right:</i></b><i> Synthetic image.</i></p><p data-block-key=\"c1mff\"><i>We asked Gemini to describe the original image in text, and then prompted Gemini to generate the synthetic image based on the text description.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"e3q59\">Concurrent work by <a href=\"https://arxiv.org/abs/2506.07555\" target=\"_blank\" rel=\"noopener noreferrer\">Wang <i>et al</i>.</a> showed how one can leverage text-based intermediaries to generate differentially private single images using <a href=\"https://arxiv.org/abs/2403.01749\" target=\"_blank\" rel=\"noopener noreferrer\">Private Evolution</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Evaluation and results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"e3q59\">We tested our method on the <a href=\"https://arxiv.org/abs/1503.01817\" target=\"_blank\" rel=\"noopener noreferrer\">YFCC100M</a> dataset, a repository containing nearly 100 million images that have been released under the Creative Commons license. We formed “albums” from these images by grouping together photos taken by the same user within the same hour. We constructed training sets for the large language models described above, taking care that no user contributes more than one example to any training set (contribution bounding is necessary to ensure the validity of the differential privacy guarantee).</p><p data-block-key=\"biu12\">After applying our method to generate synthetic photo albums, we evaluated how well they resemble the original albums. First, we computed the <a href=\"https://arxiv.org/abs/2102.01454\" target=\"_blank\" rel=\"noopener noreferrer\">MAUVE score</a>, a neural embedding–based measure of semantic similarity, between the original and synthetic structured representations.</p><p data-block-key=\"eddr7\">The figure below shows the MAUVE scores between real and synthetic album summaries, as well as real and synthetic photo captions, both before and after fine-tuning.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-3-Mauve.width-1250.png\" alt=\"MAUVE scores between real and synthetic album summaries &amp; captions\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-3-Mauve.width-1250.png\" alt=\"MAUVE scores between real and synthetic album summaries &amp; captions\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cnnu1\"><b><i>Left:</i></b><i> MAUVE scores between real and synthetic album summaries.</i> <b><i>Right:</i></b><i> MAUVE scores between real and synthetic photo captions. Higher MAUVE scores indicate greater similarity. Higher values of the privacy parameter ε imply weaker privacy constraints.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"e3q59\">Next, we calculated the most common topics in the album summaries, shown in the table below, and found that they were very similar between real and synthetic data.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-4-Topics.width-1250.png\" alt=\"Real album summaries vs synthetic album summaries\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-4-Topics.width-1250.png\" alt=\"Real album summaries vs synthetic album summaries\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cnnu1\"><b><i>Left:</i></b><i> Most common topics in real album summaries.</i> <b><i>Right:</i></b><i> Most common topics in synthetic album summaries.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"e3q59\">Finally, direct visual examination of the synthetic photos albums shows that each album is typically centered on a common theme, just like real photo albums, as demonstrated by the examples in the figure below.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-5-Albums.width-1250.png\" alt=\"Privately generated synthetic photo albums\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-5-Albums.width-1250.png\" alt=\"Privately generated synthetic photo albums\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cnnu1\"><i>Two synthetically-generated photo albums. Each album maintains a specific theme (</i><b><i>top:</i></b><i> apple picking trip;</i> <b><i>bottom:</i></b><i> couple visits a meadow).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"e3q59\">The challenges of modern AI require data that is not only private, but also structurally and contextually rich, a need that simple, unstructured data can’t meet. By applying our hierarchical, text-as-intermediate method to the demanding task of generating coherent synthetic photo albums, we’ve successfully shown a pathway for extending the benefits of synthetic data beyond simple text or isolated images.</p><p data-block-key=\"85ng9\">This methodology opens exciting new avenues for privacy-preserving AI innovation. It helps resolve the persistent tension between the need for large, high-quality data and the imperative to protect user privacy, paving the way for safer and more generalized AI development across critical industries.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"e3q59\"><i>This work is the result of a collaboration between many people at Google Research, including (in alphabetical order by last name): Kareem Amin, Alex Bie, Rudrajit Das, Alessandro Epasto, Weiwei Kong, Alex Kurakin, Natalia Ponomareva, Monica Ribero, Jane Shapiro, Umar Syed, and Sergei Vassilvitskii.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "仅用少量示例教Gemini识别爆发恒星 (原标题: Teaching Gemini to spot exploding stars with just a few examples)",
      "link": "https://research.google/blog/teaching-gemini-to-spot-exploding-stars-with-just-a-few-examples/",
      "pubDate": "Sun, 19 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-19T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 仅用少量示例教Gemini识别爆发恒星\n\n### 引言：天文数据挑战与传统模型局限\n现代天文学是一场宇宙规模的寻宝游戏。全球望远镜每晚扫描天空，寻找像爆发恒星（超新星）这样短暂的事件，这些事件能为我们提供关于宇宙运作的关键见解。这些巡天会生成数百万条潜在发现的警报，但其中绝大多数并非真正的宇宙事件，而是卫星轨迹、宇宙射线撞击或其他仪器伪影造成的“虚假”信号。\n\n多年来，天文学家一直使用专门的机器学习模型，如卷积神经网络（CNN），来筛选这些数据。尽管这些模型有效，但它们通常充当“黑箱”，只提供简单的“真实”或“虚假”标签，而没有解释。这迫使科学家要么盲目信任输出，要么花费无数时间手动验证候选事件——随着下一代望远镜（如Vera C. Rubin天文台）预计每晚生成1000万条警报，这一瓶颈将很快变得难以逾越。\n\n### 创新方法：Gemini的多模态少样本学习\n这一挑战促使我们提出了一个基本问题：一个旨在同时理解文本和图像的通用多模态模型，能否不仅与这些专业模型的准确性相匹配，还能解释它所看到的内容？在《自然天文学》上发表的论文“大型语言模型对瞬态图像分类的文本解释”中，我们证明答案是肯定的。我们展示了Google的Gemini模型如何能够转变为一个专家级的天文学助手，以高精度分类宇宙事件，并且最重要的是，用通俗易懂的语言解释其推理过程。我们通过对Gemini采用少样本学习来实现这一目标，每个巡天仅提供15个带注释的示例和简洁的指令，以准确分类和解释宇宙事件。\n\n**从少量示例中学习的新方法**\n我们没有在数百万张标记图像上训练一个专门模型，而是对一个通用模型使用了少样本学习技术。我们为Gemini提供了三个主要天文巡天（Pan-STARRS、MeerLICHT和ATLAS）的每个巡天仅15个带注释的示例。每个示例包括三张小图像：一张瞬态警报的新图像、一张来自先前观测的同一片天空区域的参考图像，以及一张突出两者之间变化的差分图像。除了这些图像，我们还提供了一组简洁的指令、一份专家编写的简短分类说明，以及一个兴趣评分（例如，对于可能的超新星为“高兴趣”，对于变星为“低兴趣”，对于虚假信号为“无兴趣”），并附带该评分的解释。\n\n该模型必须学会从各种望远镜中分类瞬态事件，每个望远镜都具有不同的分辨率、像素尺度和相机特性。如下图所示，同一天体在这些巡天中可能看起来大相径庭，但Gemini能够从提供的少量示例中进行泛化。\n\n**图片 1: Gemini在不同巡天数据上的泛化能力**\n![ExplodingStars1_Surveys](https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars1_Surveys.width-1250.png)\n*描述：Gemini模型能够在具有不同像素尺度和分辨率的巡天数据中运行。同一瞬态事件在Pan-STARRS（顶部）、MeerLICHT（中部）和ATLAS（底部）三个不同巡天中观测到。每行从左到右依次是新图像、参考图像和差分图像。图像块的像素大小均为100×100，但由于巡天特定的像素尺度（Pan-STARRS：0.25″/像素，MeerLICHT：0.56″/像素，ATLAS：1.8″/像素），其角天空覆盖范围有所不同。*\n\n### 卓越成果：高精度分类与可解释性\n仅凭这些最少的输入指导，我们要求Gemini分类数千个新的警报。该模型在三个数据集中平均达到了93%的准确率，这与需要大量精心策划训练数据集的专业CNN模型相当。\n\n但与传统分类器不同的是，我们提示Gemini不仅输出一个标签，还为每个候选事件生成：\n*   描述其观察到的特征和决策逻辑的文本解释。\n*   帮助天文学家优先安排后续观测的兴趣评分。\n\n这使得模型从一个黑箱转变为一个透明、交互式的合作伙伴。科学家可以阅读解释以理解模型的推理过程，从而建立信任并实现更细致的决策。\n\n**图片 2: Gemini提供可读的瞬态分类和后续优先级**\n![ExplodingStars2_Example](https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars2_Example.width-1250.png)\n*描述：Gemini提供人类可读的瞬态分类和后续优先级。每个示例包括候选瞬态的新图像、参考图像和差分图像，随后是Gemini的分类、文本描述和后续兴趣评分。图中所示示例来自MeerLICHT数据集。*\n\n### 可靠性评估与不确定性量化\n构建一个可靠系统的一个关键步骤是确保其输出的质量。我们召集了一个由12位专业天文学家组成的小组，他们审查了200个Gemini的分类和解释。他们使用一个单一的、锚定的0-5连贯性评估标准（0 = 幻觉，5 = 完全连贯），该标准与文本如何与新/参考/差分图像匹配相关联，并进行了一个简单的“是/可能/否”检查，以确认后续兴趣评分与解释一致。他们将模型的描述评为高度连贯和有用，证实了与专家推理的一致性。\n\n但也许我们最重要的发现是Gemini能够有效评估自身的不确定性。我们提示模型为其自身的解释分配一个“连贯性评分”。我们发现低连贯性评分是分类不正确的有力指标。换句话说，模型擅长告诉我们它何时可能出错。\n\n**图片 3: Gemini的评估结果**\n![ExplodingStars3_Results](https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars3_Results.width-1250.png)\n*描述：左图：12位天文学家对200个MeerLICHT瞬态事件的平均连贯性评分，按平均分排序（蓝色）。大多数示例获得高分（4-5），表明与用户期望高度一致。插图：模型分配的兴趣评分与其自身解释之间的一致性，几乎所有案例都被标记为自洽（即“是”）。右图：按Gemini分类的正确性（TPs和TNs为绿色，FPs和FNs为红色）划分的平均用户连贯性评分。正确分类的示例往往比不正确分类的示例具有更高的连贯性评分。*\n\n### “人机协作”工作流的未来\n这种能力对于构建可靠的“人机协作”工作流来说是一个游戏规则的改变者。通过自动标记其最不确定的案例，系统可以将天文学家的注意力集中在最需要的地方。这创造了一个强大的反馈循环。通过审查标记的案例并将其中一些具有挑战性的示例重新添加到提示中，我们可以迅速提高模型的性能。通过这种迭代过程，我们将模型在MeerLICHT数据集上的准确率从约93.4%提高到约96.7%，展示了系统如何与人类专家合作学习和改进。\n\n### 科学发现的未来展望\n我们相信这种方法标志着科学发现新时代的到来——一个由能够对复杂科学数据集进行推理并用自然语言解释其输出的模型所加速的时代。由于这种方法只需要少量示例和通俗易懂的指令，它有可能在许多不同领域快速适应新的科学仪器、巡天和研究目标。我们设想这项技术将成为科学领域“智能代理助手”的基础。这样的系统可以整合多个数据源，检查自身的置信度，请求后续观测，并仅将最有前景的发现上报给人类科学家。这项工作展示了一条通向与我们共同学习、解释其推理并赋能任何领域研究人员专注于最重要事情——提出下一个伟大问题——的系统的道路。",
      "shortSummary": "一项新研究展示了Google的Gemini模型如何利用少样本学习，仅通过少量示例就能高精度识别爆发恒星等天文瞬态事件。该模型不仅达到93%的平均准确率，与传统专业模型相当，还能提供详细的文本解释和兴趣评分，将“黑箱”分类器转变为透明的交互式伙伴。Gemini还能评估自身不确定性，有效指导天文学家关注关键案例，并通过人机协作循环快速提升性能，预示着科学发现的新时代。",
      "translated_title": "仅用少量示例教Gemini识别爆发恒星",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars1_Surveys.width-1250.png",
          "alt": "ExplodingStars1_Surveys",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars2_Example.width-1250.png",
          "alt": "ExplodingStars2_Example",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars3_Results.width-1250.png",
          "alt": "ExplodingStars3_Results",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"m40sy\">Modern astronomy is a treasure hunt on a cosmic scale. Every night, telescopes around the globe scan the skies, searching for fleeting events like exploding stars (<a href=\"https://en.wikipedia.org/wiki/Supernova\" target=\"_blank\" rel=\"noopener noreferrer\">supernovae</a>) that give us crucial insights into the workings of the universe. These surveys generate millions of alerts about potential discoveries, but there’s a catch: the vast majority are not real cosmic events but \"bogus\" signals from satellite trails, cosmic ray hits, or other instrumental artefacts.</p><p data-block-key=\"4lso\">For years, astronomers have used specialized machine learning models, like <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\" target=\"_blank\" rel=\"noopener noreferrer\">convolutional neural networks</a> (CNNs), to sift through this data. While effective, these models often act as “black boxes,” providing a simple \"real\" or \"bogus\" label with no explanation. This forces scientists to either blindly trust the output or spend countless hours manually verifying candidates — a bottleneck that will soon become insurmountable with next-generation telescopes like the <a href=\"https://rubinobservatory.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Vera C. Rubin Observatory</a>, expected to generate <a href=\"https://www.scientificamerican.com/article/rubin-observatory-data-flood-will-let-the-universe-alert-astronomers-10/\" target=\"_blank\" rel=\"noopener noreferrer\">10 million alerts per night</a>.</p><p data-block-key=\"a4o3a\">This challenge led us to ask a fundamental question: could a general-purpose, multimodal model, designed to understand text and images together, not only match the accuracy of these specialized models but also <i>explain</i> what it sees? In our paper, “<a href=\"https://www.nature.com/articles/s41550-025-02670-z\" target=\"_blank\" rel=\"noopener noreferrer\">Textual interpretation of transient image classifications from large language models</a>”, published in <a href=\"https://www.nature.com/natastron/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature Astronomy</i></a>, we demonstrate that the answer is a resounding yes. We show how Google’s Gemini model can be transformed into an expert astronomy assistant that can classify cosmic events with high accuracy and, crucially, explain its reasoning in plain language. We accomplished this by employing few-shot learning with Gemini, providing it with just 15 annotated examples per survey and concise instructions to accurately classify and explain cosmic events.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"m40sy\">A new approach: Learning from a few examples</h2><p data-block-key=\"blu8p\">Instead of training a specialized model on millions of labeled images, we used a technique called <a href=\"https://www.promptingguide.ai/techniques/fewshot\" target=\"_blank\" rel=\"noopener noreferrer\">few-shot learning</a> on a general-purpose model. We gave Gemini just 15 annotated examples for each of three major astronomical surveys: <a href=\"https://en.wikipedia.org/wiki/Pan-STARRS\" target=\"_blank\" rel=\"noopener noreferrer\">Pan-STARRS</a>, <a href=\"https://science.uct.ac.za/meerlicht/meerlicht\" target=\"_blank\" rel=\"noopener noreferrer\">MeerLICHT</a>, and <a href=\"https://atlas.fallingstar.com/\" target=\"_blank\" rel=\"noopener noreferrer\">ATLAS</a>. Each example consisted of three small images: a <i>new</i> image of the transient alert, a <i>reference</i> image of the same patch of sky from a previous observation, and a <i>difference</i> image that highlights the change between the two. Alongside these images, we provided a concise set of instructions, a short expert-written note explaining the classification, and an interest score (e.g., “high interest” for a likely supernova, “low interest” for a variable star, or “no interest” for a bogus signal) along with an explanation of that score.</p><p data-block-key=\"c449\">The model had to learn to classify transients from a diverse set of telescopes, each with different resolutions, pixel scales, and camera characteristics. As shown below, the same celestial object can appear quite different across these surveys, but Gemini was able to generalize from the few examples provided.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars1_Surveys.width-1250.png\" alt=\"ExplodingStars1_Surveys\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars1_Surveys.width-1250.png\" alt=\"ExplodingStars1_Surveys\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"5qfdw\"><i>Gemini operates across surveys with diverse pixel scales and resolutions. The same transient is observed in three different surveys, with rows corresponding to Pan-STARRS (</i><b><i>top</i></b><i>), MeerLICHT (</i><b><i>middle</i></b><i>) and ATLAS (</i><b><i>bottom</i></b><i>). Each row includes, from</i> <b><i>left</i></b><i> to</i> <b><i>right</i></b><i>, a new image, a reference image and a difference image. The image stamps are all the same size in pixels (100 × 100) but differ in angular sky coverage due to survey-specific pixel scales: Pan-STARRS (0.25\" per pixel), MeerLICHT (0.56\" per pixel) and ATLAS (1.8\" per pixel).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"m40sy\">Guided only by this minimal input, we asked Gemini to classify thousands of new alerts. The model achieved an average accuracy of 93% across the three datasets, which is on par with specialized CNNs that require massive, curated training datasets.</p><p data-block-key=\"f7r54\">But unlike a traditional classifier, we prompted Gemini not just to output a label but also to generate for every candidate:</p><ol><li data-block-key=\"e6h3c\">A <i>textual explanation</i> describing the features it observed and the logic behind its decision.</li><li data-block-key=\"fa8cn\">An <i>interest score</i> to help astronomers prioritize follow-up observations.</li></ol><p data-block-key=\"ao7h5\">This turns the model from a black box into a transparent, interactive partner. Scientists can read the explanation to understand the model’s reasoning, building trust and allowing for more nuanced decision-making.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars2_Example.width-1250.png\" alt=\"ExplodingStars2_Example\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars2_Example.width-1250.png\" alt=\"ExplodingStars2_Example\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"5qfdw\"><i>Gemini provides human-readable transient classifications and follow-up priorities. Each example consists of a new, reference and difference image for a candidate transient, followed by the Gemini classification, textual description and follow-up interest score. The examples shown in the figure are from the MeerLICHT dataset.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"m40sy\">Knowing when to ask for help</h2><p data-block-key=\"ftr31\">A critical step in building a reliable system is ensuring the quality of its output. We assembled a panel of 12 professional astronomers who reviewed 200 of Gemini’s classifications and explanations. Using a single, anchored 0–5 coherence rubric (0 = hallucination, 5 = perfectly coherent) tied to how well the text matched the new/reference/difference images, plus a simple Yes/Maybe/No check that the follow-up interest score agreed with the explanation, they rated the model’s descriptions as highly coherent and useful, confirming alignment with expert reasoning.</p><p data-block-key=\"e74hk\">But perhaps our most important finding was that Gemini can effectively assess its own uncertainty. We prompted the model to assign a “coherence score” to its own explanations. We discovered that low-coherence scores were a powerful indicator of an incorrect classification. In other words, the model is good at telling us when it’s likely to be wrong. The details:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars3_Results.width-1250.png\" alt=\"ExplodingStars3_Results\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars3_Results.width-1250.png\" alt=\"ExplodingStars3_Results\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"5qfdw\"><b><i>Left:</i></b> <i>Average coherence scores from 12 astronomers for 200 MeerLICHT transients, sorted by mean score (</i><b><i>blue</i></b><i>). Most examples received high values (4–5), indicating close alignment with user expectations.</i> <b><i>Inset:</i></b><i> The consistency between the interest score assigned by the model &amp; its own explanation, with nearly all cases marked as self-consistent (i.e., “Yes”).</i> <b><i>Right</i></b><i>: Average user coherence scores, split by the correctness of the classification made by Gemini. Correctly classified examples (TPs &amp; TNs,</i> <b><i>green</i></b><i>) tend to have higher coherence scores than incorrect ones (FPs &amp; FNs,</i> <b><i>red</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"m40sy\">This capability is a game-changer for building reliable \"human-in-the-loop\" workflows. By automatically flagging its most uncertain cases, the system can focus astronomers' attention where it is most needed. This creates a powerful feedback loop. By reviewing the flagged cases and adding a few of these challenging examples back into the prompt, we can rapidly improve the model’s performance. Using this iterative process, we improved the model's accuracy on the MeerLICHT dataset from ~93.4% to ~96.7%, demonstrating how the system can learn and improve in partnership with human experts.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"1ht7a\">The future of scientific discovery</h2><p data-block-key=\"1vu00\">We believe this approach marks a step toward a new era of scientific discovery — one accelerated by models that can both reason over complex scientific datasets and explain their outputs in natural language., but by models that can reason, explain their output, and collaborate with researchers.</p><p data-block-key=\"1n1n2\">Because this method requires only a small set of examples and plain-language instructions, it can potentially be rapidly adapted for new scientific instruments, surveys, and research goals across many different fields. We envision this technology as a foundation for \"agentic assistants\" in science. Such systems could integrate multiple data sources, check their own confidence, request follow-up observations, and escalate only the most promising discoveries to human scientists.</p><p data-block-key=\"fklo3\">This work shows a path toward systems that learn with us, explain their reasoning, and empower researchers in any field to focus on what matters most: asking the next great question.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"1ht7a\">Acknowledgements</h2><p data-block-key=\"djkdj\"><i>This research was a collaborative effort. We extend our sincere thanks to our co-authors Steven Bloemen, Stephen J. Smartt, Paul J. Groot, Paul Vreeswijk, and Ken W. Smith.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "解决虚拟机难题：AI 如何优化云计算 (原标题: Solving virtual machine puzzles: How AI is optimizing cloud computing)",
      "link": "https://research.google/blog/solving-virtual-machine-puzzles-how-ai-is-optimizing-cloud-computing/",
      "pubDate": "Thu, 16 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-16T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 解决虚拟机难题：AI 如何优化云计算\n\n云计算数据中心面临着一个持续的挑战：如何高效地分配处理任务（即虚拟机，VM）。这类似于一个俄罗斯方块游戏，但“方块”的生命周期未知且变化迅速。低效的VM分配会导致“资源搁浅”（浪费容量）和“空闲主机”不足（系统更新和大型VM部署所需）。传统的AI方法通过预测VM生命周期来解决这一经典的装箱问题，但单次预测的错误可能导致效率下降。\n\n### LAVA系统：基于学习分布和适应误预测的生命周期感知VM分配\n\n为了克服这些挑战，研究人员引入了LAVA系统，该系统包含三个核心算法：\n\n*   **非侵入式生命周期感知调度（NILAS）**\n*   **生命周期感知VM分配（LAVA）**\n*   **生命周期感知重新调度（LARS）**\n\n该系统的核心是“持续再预测”机制，它不依赖于VM创建时的一次性生命周期预测，而是持续自动更新VM的预期剩余生命周期预测。\n\n### VM的秘密生命：再预测与概率分布\n\n研究发现，VM的生命周期通常是不可预测的，并遵循长尾分布。例如，绝大多数VM（88%）的生命周期不到一小时，但它们仅消耗总资源的极小部分（2%）。相反，少数长生命周期的VM对整体资源效率影响巨大。\n\n![VM数量和计算份额的相对贡献图](https://storage.googleapis.com/gweb-research2023-media/images/LAVA-2-Contributions.width-1250.png)\n\n*VM生命周期分布（左）与资源消耗（右）。最短作业（0-10分钟，深蓝色）占数量的53%，但资源消耗可忽略不计。相比之下，运行时间最长的作业（>30天，橙色）占资源消耗的18%，但数量占比可忽略不计。*\n\n为了应对这种不确定性，LAVA系统设计了一个机器学习模型，该模型预测VM生命周期的概率分布，而非单一平均值。这种方法受生存分析启发，能够捕捉VM行为的内在不确定性。更重要的是，系统利用此分布持续更新预测。例如，一个VM运行了五天后，系统会重新评估其预期剩余生命周期，随着VM运行时间的增加，预测准确性也随之提高。\n\n![VM生命周期分布图](https://storage.googleapis.com/gweb-research2023-media/images/LAVA-1-Lifetimes.width-1250.png)\n\n*VM生命周期分布。VM调度时，预期（平均）生命周期为0.2天。运行1天后，预期剩余生命周期为4天。运行7天后，预期剩余生命周期为10天。*\n\n### 新型调度算法\n\n基于这种更强大的预测模型，研究人员开发了三种新颖的算法来改进VM分配：\n\n1.  **非侵入式生命周期感知调度（NILAS）**：\n    *   将生命周期预测整合到现有评分函数中，通过优先选择所有VM预计在相似时间退出的主机，旨在创建更多空闲机器。\n    *   由于使用再预测，对初始预测准确性不那么敏感，能够纠正错误。\n    *   已部署在Google的大规模集群管理器Borg上，显著改善了VM分配。\n\n2.  **生命周期感知VM分配（LAVA）**：\n    *   将短生命周期的VM放置在带有一个或多个长生命周期VM的主机上，以填补资源空白。\n    *   目标是让短生命周期的VM迅速退出，而不延长主机的整体生命周期。\n    *   通过在VM超出预期截止日期时增加主机的预期生命周期来主动适应误预测。\n    *   模拟结果表明，该策略能最大限度地减少碎片化并确保主机最终被释放。\n\n3.  **生命周期感知重新调度（LARS）**：\n    *   在碎片整理和维护期间，根据预测的剩余生命周期对主机上的VM进行排序，并优先迁移生命周期最长的VM。\n    *   短生命周期的VM在迁移前自然退出。\n    *   模拟结果显示，LARS有望将所需的总迁移次数减少约4.5%。\n\n### 解决大规模部署的挑战\n\n开发强大的模型和算法只是解决方案的一部分。在大规模环境中可靠地运行它们需要重新思考模型部署方法：\n\n*   **避免循环依赖**：传统的将机器学习模型部署在专用推理服务器上的做法会创建循环依赖，因为这些服务器本身运行在集群调度系统上。解决方案是将模型直接编译到Borg调度器二进制文件中，消除了循环依赖，并确保了模型以与调度器其他代码更改相同的严格流程进行测试和部署。这还带来了额外的好处：模型的中位延迟仅为9微秒（µs），比使用独立模型服务器的方法快780倍，这对于频繁的再预测和性能敏感任务至关重要。\n*   **解决预测瓶颈**：对于最大的区域，所需的预测数量可能成为瓶颈。通过引入主机生命周期评分缓存来解决，该缓存仅在VM添加到或从主机移除时，或当主机的预期生命周期到期时才更新预测。这种缓存机制确保了高性能，并允许系统在整个集群中部署。\n\n### 成果\n\nNILAS算法自2024年初以来已在Google的生产数据中心运行，取得了显著成果：\n\n*   **空闲主机增加**：生产试点和全集群推广显示，空闲主机增加了2.3-9.2个百分点（pp）。这直接与效率相关，因为1个百分点的改进通常相当于节省集群容量的1%。\n*   **资源搁浅减少**：在一些试点实验中，NILAS将CPU搁浅减少了约3%，内存搁浅减少了2%。这意味着主机更多的资源可供新VM使用。\n*   **LAVA模拟**：表明它将在NILAS的基础上进一步提高约0.4个百分点。\n*   **LARS模拟**：表明它有可能将维护所需的VM实时迁移次数减少4.5%。\n\n### 结论\n\n这项工作是迈向机器学习系统日益优化数据中心管理未来的基础性一步。所开发的技术，特别是再预测的使用以及模型和系统的协同设计，具有普适性。研究表明，在不牺牲可靠性或延迟的情况下，将先进的机器学习技术集成到系统基础设施堆栈的最底层是可行的，同时还能实现显著的效率提升。",
      "shortSummary": "Google开发了LAVA系统，利用AI优化云计算中的虚拟机（VM）分配。该系统通过“持续再预测”技术，动态更新VM生命周期预测，解决了传统方法中单次预测不准确的问题。LAVA包含NILAS、LAVA和LARS三种算法，旨在提高资源利用率、减少资源搁浅并优化维护。NILAS已在Google生产环境部署，显著增加了空闲主机并减少了资源搁浅。通过将模型直接编译到调度器中，解决了大规模部署的挑战，确保了低延迟和高可靠性。这项工作为ML驱动的数据中心管理奠定了基础。",
      "translated_title": "解决虚拟机难题：AI 如何优化云计算",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/LAVA-2-Contributions.width-1250.png",
          "alt": "Graph of relative contributions by number and compute fraction.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/LAVA-1-Lifetimes.width-1250.png",
          "alt": "Plot showing distribution of VM lifetimes",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"pt8nw\">Imagine a puzzle game similar to Tetris with pieces rapidly falling onto a stack. Some fit perfectly. Others don’t. The goal is to pack the blocks as tightly and efficiently as possible. This game is a loose analogy to the challenge faced by cloud data centers several times every second as they try to allocate processing jobs (called virtual machines or VMs) as efficiently as possible. But in this case, the “pieces” (or VMs) appear and disappear, some with a lifespan of only minutes, and others, days. In spite of the initially unknown VM lifespans, we still want to fill as much of the physical servers as possible with these VMs for the sake of efficiency. If only we knew the approximate lifespan of a job, we could clearly allocate much better.</p><p data-block-key=\"483c4\">At the scale of large data centers, efficient resource use is especially critical for both economic and environmental reasons. Poor VM allocation can lead to \"resource stranding\", where a server's remaining resources are too small or unbalanced to host new VMs, effectively wasting capacity. Poor VM allocation also reduces the number of \"empty hosts\", which are essential for tasks like system updates and provisioning large, resource-intensive VMs.</p><p data-block-key=\"3s50u\">This classic <a href=\"https://en.wikipedia.org/wiki/Bin_packing_problem\" target=\"_blank\" rel=\"noopener noreferrer\">bin packing problem</a> is made more complex by this incomplete information about VM behavior. AI can help with this problem by using learned models to predict VM lifetimes. However, this often relies on a single prediction at the VM's creation. The challenge with this approach is that a single misprediction can tie up an entire host for an extended period, degrading efficiency.</p><p data-block-key=\"8v1nd\">In “<a href=\"https://arxiv.org/abs/2412.09840v1\" target=\"_blank\" rel=\"noopener noreferrer\">LAVA: Lifetime-Aware VM Allocation with Learned Distributions and Adaptation to Mispredictions</a>”, we introduce a trio of algorithms — non-invasive lifetime aware scoring (NILAS), lifetime-aware VM allocation (LAVA), and lifetime-aware rescheduling (LARS) — which are designed to solve the bin packing problem of efficiently fitting VMs onto physical servers. This system uses a process we call “continuous reprediction”, which means it doesn’t rely on the initial, one-time guess of a VM’s lifespan made at its creation. Instead, the model constantly and automatically updates its prediction for a VM's expected remaining lifetime as the VM continues to run.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The secret life of VMs: Repredictions and probability distributions</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">One of the key insights driving this research is the recognition that VM lifetimes are often unpredictable and follow a <a href=\"https://en.wikipedia.org/wiki/Long_tail\" target=\"_blank\" rel=\"noopener noreferrer\">long-tailed distribution</a>. For example, while the vast majority of VMs (88%) live for less than an hour, these short-lived VMs consume only a tiny fraction (2%) of the total resources. This means that the placement of a small number of long-lived VMs has a disproportionately large impact on overall resource efficiency.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/LAVA-2-Contributions.width-1250.png\" alt=\"Graph of relative contributions by number and compute fraction.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/LAVA-2-Contributions.width-1250.png\" alt=\"Graph of relative contributions by number and compute fraction.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"b4xcm\"><i>Distribution of VM lifetimes of scheduled VMs (</i><b><i>left</i></b><i>) vs. their resource consumption (</i><b><i>right</i></b><i>). Interestingly, the shortest jobs (0–10 min, dark blue), which account for 53% by number, take a negligible fraction of resources. In contrast, the longest running jobs (&gt;30 days, orange), which take considerable resources (18%), amount to a negligible fraction by number.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"pt8nw\">Instead of trying to predict a single, average lifetime, which can be misleading for VMs with bi-modal or highly varied lifespans, we designed an ML model that predicts a probability distribution for a VM's lifetime. This approach, inspired by <a href=\"https://en.wikipedia.org/wiki/Survival_analysis\" target=\"_blank\" rel=\"noopener noreferrer\">survival analysis</a>, allows the model to capture the inherent uncertainty of a VM's behavior.</p><p data-block-key=\"91kfi\">More importantly, our system uses this distribution to continuously update its predictions. We ask, “Given a VM has been running for five days, what is its expected remaining lifetime?” As a VM continues to run, the system gains more information, and its lifetime prediction becomes more accurate. Our algorithms are specifically co-designed to leverage these repredictions, actively responding to mispredictions and improving the accuracy over time.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/LAVA-1-Lifetimes.width-1250.png\" alt=\"Plot showing distribution of VM lifetimes\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/LAVA-1-Lifetimes.width-1250.png\" alt=\"Plot showing distribution of VM lifetimes\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"b4xcm\"><i>Lifetime distribution of VM lifetimes. When the VM is scheduled, the expected (average) lifetime is 0.2 days. After it has run for 1 day, the expected remaining lifetime is 4 days. After 7 days, the expected remaining lifetime is 10 days.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">A new class of scheduling algorithms</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">With this new, more robust prediction model, we developed three novel algorithms to improve VM allocation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">1. Non-Invasive Lifetime Aware Scheduling (NILAS)</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">NILAS is a non-invasive algorithm that incorporates lifetime predictions into an existing scoring function. It ranks potential hosts for a new VM by considering the repredicted exit times of all existing VMs on that host. By prioritizing hosts where all VMs are expected to exit at a similar time, NILAS aims to create more empty machines. Our use of repredictions is less sensitive to prediction accuracy and allows NILAS to correct for errors. The NILAS algorithm has been deployed on our <a href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43438.pdf\">large-scale cluster manager</a>, Borg, where it significantly improves VM allocation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">2. Lifetime-Aware VM Allocation (LAVA)</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">LAVA is a more fundamental departure from existing scheduling mechanisms. While NILAS aims to pack VMs with similar lifetimes, LAVA does the opposite: it puts shorter-lived VMs on hosts with one or more long-lived VMs. The goal is to fill in resource gaps with short-lived VMs that are at least an order of magnitude shorter than the host’s anticipated lifespan, so that they exit quickly without extending the host’s overall lifespan. LAVA also actively adapts to mispredictions by increasing a host’s anticipated lifespan if a VM outlives its expected deadline. Simulations show that this strategy minimizes fragmentation and ensures that hosts are eventually freed up.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">3. Lifetime-Aware Rescheduling (LARS)</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">LARS uses our lifetime predictions to minimize VM disruptions during defragmentation and maintenance. When a host needs to be defragmented, LARS sorts the VMs on that host by their predicted remaining lifetime and migrates the longest-lived VMs first. Shorter-lived VMs exit naturally before migration. Simulations with LARS indicate it has the potential to reduce the total number of migrations required by around 4.5%.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Addressing the challenge of deployment at scale</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">Developing powerful models and algorithms is only one part of the solution. Getting them to work reliably at large scale required us to rethink our approach to model deployment.</p><p data-block-key=\"cj0j9\">A common practice is to serve machine learning models on dedicated inference servers. However, this would have created a <a href=\"https://en.wikipedia.org/wiki/Circular_dependency\" target=\"_blank\" rel=\"noopener noreferrer\">circular dependency</a>, as these servers would themselves run on our cluster scheduling system. A failure in the model serving layer could then cause a cascading failure in the scheduler itself, which is unacceptable for a mission-critical system.</p><p data-block-key=\"34gvd\">Our solution was to compile the model directly into the <a href=\"https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/\">Borg scheduler binary</a>. This approach eliminated the circular dependency and ensured that the model was tested and rolled out with the same rigorous process as any other code change to the scheduler. This also yielded an additional benefit: the model's median latency is just 9 microseconds (µs), which is 780 times faster than a comparable approach that uses separate model servers. This low latency is crucial for running repredictions frequently and for using the model in performance-sensitive tasks, like maintenance and defragmentation.</p><p data-block-key=\"dpkp\">We also found that for our largest zones, the number of required predictions could become a bottleneck. We addressed this by introducing a host lifetime score cache, which only updates predictions when a VM is added or removed from a host, or when a host's expected lifetime expires. This caching mechanism ensures high performance and allows us to deploy our system fleet-wide.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">Our NILAS algorithm has been running in Google's production data centers since early 2024. The results are clear and significant.</p><ul><li data-block-key=\"27kt6\"><i>Increased empty hosts:</i> Our production pilots and fleet-wide rollouts have shown an increase in empty hosts by 2.3–9.2 percentage points (pp). This metric directly correlates with efficiency, as a 1 pp improvement is typically equivalent to saving 1% of a cluster's capacity.</li><li data-block-key=\"arb0l\"><i>Reduced resource stranding:</i> In some pilot experiments, NILAS reduced CPU stranding by approximately 3% and memory stranding by 2%. This means more of a host's resources are available to be used by new VMs.</li></ul><p data-block-key=\"7qme9\">Simulations running LAVA suggest it will provide a further ~0.4 pp improvement over NILAS. Similarly, simulations with LARS indicate that it has the potential to reduce the number of VM live migrations needed for maintenance by 4.5%.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">We believe this work is a foundational step towards a future where data center management is increasingly optimized by machine learning systems. The techniques we developed, particularly the use of repredictions and the co-design of models and systems, are generalizable to other tasks. We have demonstrated that it is possible to integrate advanced machine learning techniques into the lowest layers of a system’s infrastructure stack without sacrificing reliability or latency, while still delivering significant efficiency gains.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\"><i>LAVA is a large collaborative project that spanned multiple teams across Google, including Google Cloud, Google DeepMind, Google Research, and SystemsResearch@Google. Key contributors include Jianheng Ling, Pratik Worah, Yawen Wang, Yunchuan Kong, Anshul Kapoor, Chunlei Wang, Clifford Stein, Diwakar Gupta, Jason Behmer, Logan A. Bush, Prakash Ramanan, Rajesh Kumar, Thomas Chestna, Yajing Liu, Ying Liu, Ye Zhao, Kathryn S. McKinley, Meeyoung Park, and Martin Maas.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "使用AI通过DeepSomatic识别肿瘤中的基因变异 (原标题: Using AI to identify genetic variants in tumors with DeepSomatic)",
      "link": "https://research.google/blog/using-ai-to-identify-genetic-variants-in-tumors-with-deepsomatic/",
      "pubDate": "Wed, 15 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-15T16:00:00.000Z",
      "creator": "Google",
      "summary": "DeepSomatic是Google Research与加州大学圣克鲁斯基因组学研究所等合作开发的一款人工智能工具，旨在更准确地识别肿瘤细胞中的基因变异。这项工作已发表在《自然生物技术》杂志上，是Google利用AI方法理解和治疗癌症的广泛努力的一部分，目标是加速癌症研究并推进精准医疗。\n\n### 癌症与基因变异的挑战\n*   **癌症的本质**：癌症是一种基因疾病，细胞分裂的遗传控制出现异常。识别肿瘤细胞中的基因突变是研究癌症和制定治疗方案的关键步骤。\n*   **体细胞变异的复杂性**：与DeepVariant（Google早期用于识别遗传性生殖系变异的工具）不同，DeepSomatic专注于识别癌症中更复杂的体细胞变异。这些变异是在出生后获得的，可能由环境暴露或DNA复制错误引起。\n*   **识别难度**：识别体细胞变异比识别遗传变异更困难，因为肿瘤细胞可能包含不同频率的多种变异，且测序错误率可能高于样本中体细胞变异的实际频率。\n\n### DeepSomatic的工作原理与训练\n*   **核心技术**：DeepSomatic是一个灵活的模型，利用卷积神经网络（CNN）来识别肿瘤变异。它适用于所有主要的测序平台，支持不同类型的样本处理，并能将其学习能力扩展到未包含在训练中的癌症类型。\n*   **数据处理流程**：\n    1.  将基因测序数据（来自肿瘤细胞和非癌细胞）转换为一组图像，这些图像代表测序数据、染色体比对、输出质量及其他变量。\n    2.  DeepSomatic的卷积神经网络处理这些图像。\n    3.  区分参考基因组、个体中的非癌生殖系变异以及肿瘤中由癌症引起的体细胞变异，同时排除测序过程中产生的微小错误。\n    4.  最终输出一份癌症相关变异（突变）列表。\n\n    ![DeepSomatic概览](https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-1-Overview.width-1250.png)\n    *DeepSomatic检测基因组数据中的癌症变异。首先，将肿瘤细胞和非癌细胞的测序数据转换为图像。DeepSomatic通过其卷积神经网络处理这些图像，以区分参考基因组、个体中的非癌生殖系变异以及肿瘤中由癌症引起的体细胞变异，同时排除微小的测序错误。结果是癌症引起的变异或突变列表。*\n\n*   **训练数据（CASTLE）**：\n    *   为训练准确模型，研究团队与UC Santa Cruz和美国国家癌症研究所合作，创建了一个新的高质量训练和评估数据集。\n    *   对来自四个乳腺癌样本和两个肺癌样本（研究细胞系）的肿瘤细胞和伴随的正常细胞进行了测序。\n    *   使用Illumina短读长测序、PacBio长读长测序和Oxford Nanopore Technology长读长测序三种领先平台进行全基因组测序。\n    *   结合所有三个平台的输出，以消除平台特异性错误，创建了一个名为“癌症标准长读长评估数据集”（CASTLE）的单一、准确的参考数据集。\n\n    ![突变率图](https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-2-Variants.width-1250.png)\n    *用于训练DeepSomatic的基准数据集。每个条形图显示了在四个乳腺癌样本和两个肺癌样本中发现的突变数量，颜色代表不同类型的突变。肺癌显示出由环境毒素引起的一种显著突变类型，包括图中绿色的SBS4。但即使是同一种癌症，其突变特征也显示出巨大差异。这些个体差异可以预测其对治疗的反应效果。*\n\n*   **“仅肿瘤”模式**：DeepSomatic还能够在没有非肿瘤序列可用的“仅肿瘤”模式下识别体细胞变异，例如在白血病等血癌中，很难从血液样本中获取纯净的正常细胞。\n\n### DeepSomatic的性能表现\n*   **高准确性**：DeepSomatic模型在所有三个主要测序平台上均优于其他现有方法，以更高的准确性识别出更多的肿瘤变异。\n*   **擅长识别插入和缺失（Indels）**：\n    *   在Illumina测序数据上，DeepSomatic在识别Indels方面的F1-score达到90%，而次优方法为80%。\n    *   在Pacific Biosciences测序数据上，DeepSomatic的F1-score超过80%，而次优方法不到50%。\n\n    ![乳腺癌准确性图](https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-3-BreastCancer.width-1250.png)\n    *DeepSomatic（紫色）在研究中广泛使用的乳腺癌样本上的结果，与其他工具进行比较。有几种软件工具可以识别Illumina数据中的癌症变异，而对于PacBio和Oxford Nanopore Technologies生成的长读长测序数据，只有一种替代方案（粉色）。F1-score衡量发现的变异数量和准确性。DeepSomatic在识别单字母基因代码变异（单核苷酸变异）方面表现略好，并在涉及Indels的变异方面显示出显著改进。*\n\n*   **处理复杂样本的能力**：\n    *   在福尔马林固定石蜡包埋（FFPE）样本（一种常见的组织保存方法，会引入DNA损伤）上，DeepSomatic表现出色。\n    *   在全外显子组测序（WES）数据（一种更经济的方法，仅关注基因组中编码蛋白质的约1%）上，DeepSomatic也优于其他工具。\n    *   这表明DeepSomatic可用于分析质量较低或历史肿瘤样本，并适用于仅进行外显子组测序的临床数据。\n\n    ![FFPE和WES准确性图](https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-4-FFPE-WES.width-1250.png)\n    *DeepSomatic在经过更复杂预处理步骤的样本上具有显著更高的准确性，包括：福尔马林固定石蜡包埋（FFPE），一种用于保存组织样本的方法（左），以及全外显子组测序（WES），一种仅对基因组中编码蛋白质的部分进行测序的方法（右）。中间部分显示了一个经过FFPE保存并使用全外显子组测序的样本。*\n\n### 推广到其他癌症类型\n*   **胶质母细胞瘤**：DeepSomatic成功识别了这种侵袭性脑癌的变异，展示了其学习泛化能力。\n*   **儿童白血病**：与堪萨斯城儿童慈善医院合作，DeepSomatic分析了八个已测序的儿童白血病样本，不仅识别了已知的变异，还发现了10个新变异，证明了其在“仅肿瘤”样本上的有效性。\n\n### 未来展望\n*   研究实验室和临床医生有望开始使用DeepSomatic工具。\n*   检测已知癌症变异有助于选择现有治疗方案（如化疗、免疫疗法）。\n*   识别新癌症变异可能促成全新的疗法。\n*   最终目标是更深入地了解每个肿瘤，发现其驱动因素，并为患者提供最有效的治疗。",
      "shortSummary": "DeepSomatic是Google Research推出的一款AI工具，利用卷积神经网络更准确地识别肿瘤中的体细胞基因变异。它能处理多种测序平台数据，并支持肿瘤-正常样本配对及仅肿瘤样本模式。DeepSomatic在识别插入和缺失（Indels）方面表现优异，并能有效分析FFPE和WES等复杂样本。该工具已开源，旨在加速癌症研究，推动精准医疗，帮助临床医生选择现有疗法并发现潜在新疗法。",
      "translated_title": "使用AI通过DeepSomatic识别肿瘤中的基因变异",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-1-Overview.width-1250.png",
          "alt": "Overview of DeepSomatic",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-2-Variants.width-1250.png",
          "alt": "Plot of mutation rates",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-3-BreastCancer.width-1250.png",
          "alt": "Plot of accuracy on breast cancer",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-4-FFPE-WES.width-1250.png",
          "alt": "Plot of accuracy on FFPE & WES",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ry233\">Cancer is fundamentally a genetic disease in which the genetic controls on cell division go awry. Many types of cancer exist, and each poses unique challenges as it can have distinct genetic underpinnings. A powerful way to study cancer, and a critical step toward creating a treatment plan, is to identify the genetic mutations in tumor cells. Indeed, clinicians will now often sequence the genomes of biopsied tumor cells to inform treatment plans that specifically disrupt how that cancer grows.</p><p data-block-key=\"ftn7v\">With partners at the University of California, Santa Cruz <a href=\"https://genomics.ucsc.edu/\" target=\"_blank\" rel=\"noopener noreferrer\">Genomics Institute</a> and other federal and academic researchers, our new paper, “<a href=\"https://www.nature.com/articles/s41587-025-02839-x\" target=\"_blank\" rel=\"noopener noreferrer\">DeepSomatic: Accurate somatic small variant discovery for multiple sequencing technologies</a>” in <a href=\"https://www.nature.com/nbt/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature Biotechnology</i></a> presents a tool that leverages machine learning to identify genetic variants in tumor cells more accurately than current methods. DeepSomatic is a flexible model that uses convolutional neural networks to identify tumor variants. It works on data from all major sequencing platforms, for different types of sample processing, and can extend its learning to cancer types not included in training.</p><p data-block-key=\"8b6u7\">We have made both <a href=\"https://github.com/google/deepsomatic\" target=\"_blank\" rel=\"noopener noreferrer\">the tool</a> and the <a href=\"https://github.com/CASTLE-Panel/castle\" target=\"_blank\" rel=\"noopener noreferrer\">high-quality training dataset</a> we created openly available to the research community. This work is part of broader Google efforts to develop AI methods to understand cancer and help scientists treat cancer, including analyzing mammogram images for <a href=\"https://health.google/mammography/\" target=\"_blank\" rel=\"noopener noreferrer\">breast cancer screening</a>, CT scans for <a href=\"https://research.google/blog/computer-aided-diagnosis-for-lung-cancer-screening/\">lung cancer screening</a>, as well as a partnership aimed at using AI to <a href=\"https://blog.google/technology/health/google-ai-institute-womens-cancers/\" target=\"_blank\" rel=\"noopener noreferrer\">advance research on gynecological cancers</a>. Our hope is to speed cancer research and further the goal of precision medicine.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Genetic variation acquired after birth</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\">Genome sequencing is used in research and medical clinics to identify genetic variations between an individual and the <a href=\"https://www.genome.gov/genetics-glossary/Human-Genome-Reference-Sequence\" target=\"_blank\" rel=\"noopener noreferrer\">human reference genome</a>. Distinguishing between real variants and simple errors made during the sequencing process is challenging. That’s why almost a decade ago Google Research introduced <a href=\"https://research.google/blog/deepvariant-highly-accurate-genomes-with-deep-neural-networks/\">DeepVariant</a> to identify inherited variants, also called <a href=\"https://www.genomicseducation.hee.nhs.uk/genotes/knowledge-hub/constitutional-germline-vs-somatic-tumour-variants/\" target=\"_blank\" rel=\"noopener noreferrer\">germline variants</a>, that came from parents and are found in all of the body’s cells.</p><p data-block-key=\"c63af\">The genetics of cancer is more complex. Cancer is often driven by variants acquired after birth. Environmental exposure that damages DNA, such as UV light or chemical carcinogens, as well as random errors that occur during DNA replication, can cause cells in the body, known as somatic cells, to acquire new variants. Sometimes, these acquired variants change the normal behavior of cells, and can cause them to replicate when they shouldn’t. This process drives the initial development of cancer, as well as its later progression to more fast-growing and invasive stages.</p><p data-block-key=\"1tabm\">Identifying variants specific to some of a person’s somatic cells is much harder than identifying inherited variants. Tumor cells can contain a diverse set of acquired variants at different frequencies, and the error rate of sequencing can be higher than the rate a somatic variant is present in a sample.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Training DeepSomatic to spot genetic variation in tumor cells</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\">We developed <a href=\"https://github.com/google/deepsomatic\" target=\"_blank\" rel=\"noopener noreferrer\">DeepSomatic</a> to address these challenges and accurately identify somatic variants. In most clinical and research settings, cancer is studied by sequencing the tumor cells acquired through biopsy, as well as normal cells that are unaffected by the tumor growth and contain more typical inherited genetic variations. DeepSomatic is trained to identify variations observed in tumor cells that are not inherited variants. These types of variations can provide critical insights about which variations are driving the tumor growth. DeepSomatic is also able to identify somatic variation in tumor-only mode where a non-tumor sequence is not available, for example in a blood cancer like leukemia where it is hard to get only normal cells from a blood draw. The ability to extend to different types of use-cases that follow common ways clinicians and researchers study cancer makes DeepSomatic applicable to many research and clinical settings.</p><p data-block-key=\"ae6vl\">Like our earlier tool, <a href=\"https://research.google/blog/deepvariant-highly-accurate-genomes-with-deep-neural-networks/\">DeepVariant</a>, the DeepSomatic model works by first turning genetic sequencing data into a <a href=\"https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/\" target=\"_blank\" rel=\"noopener noreferrer\">set of images</a>. The images represent the sequencing data, alignment along the chromosome, the quality of the output, and other variables. DeepSomatic then uses its <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\" target=\"_blank\" rel=\"noopener noreferrer\">convolutional neural network</a> on data from tumor cells and non-cancerous cells to differentiate between the reference genome, the non-cancer germline variants in that individual, and the cancer-caused somatic variants in the tumor, while discarding variations caused by small errors acquired during the sequencing process. The result is a list of cancer-related variants, or mutations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-1-Overview.width-1250.png\" alt=\"Overview of DeepSomatic\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-1-Overview.width-1250.png\" alt=\"Overview of DeepSomatic\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"48u3i\">DeepSomatic detects cancer variants in genomic data. First, sequencing data from the tumor cells and non-cancerous cells are turned into an image. DeepSomatic passes these images through its convolutional neural network to differentiate between the reference genome, the non-cancer germline variants in that individual, and the cancer-caused somatic variants in the tumor, while discarding variations caused by small sequencing errors. The result is a list of cancer-caused variants, or mutations.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ry233\">Training accurate models that can identify genetic variation for different cancer types requires comprehensive, high-quality data and truth sets. For this work we created a new training and evaluation dataset for detecting variants in tumor cells. With our partners at UC Santa Cruz and the <a href=\"https://www.cancer.gov/\" target=\"_blank\" rel=\"noopener noreferrer\">National Cancer Institute</a>, we sequenced tumor cells and accompanying normal cells from four breast cancer samples and two lung cancer samples from research cell lines.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-2-Variants.width-1250.png\" alt=\"Plot of mutation rates\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-2-Variants.width-1250.png\" alt=\"Plot of mutation rates\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"48u3i\">Benchmark dataset used to train DeepSomatic. Each bar shows the number of mutations found in four breast cancer samples and two lung cancer samples, with color representing different types of mutations. Lung cancer displays a notable type of mutation caused by environmental toxins, including <a href=\"https://signal.mutationalsignatures.com/explore/referenceCancerSignature/63/prevalence\" target=\"_blank\" rel=\"noopener noreferrer\">SBS4</a> shown in green. But even the same type of cancer shows big differences in its mutational signature. These individual differences can predict how well it will respond to a treatment.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ry233\">To create an accurate training dataset, we did whole-genome sequencing of these six samples using three leading platforms: <a href=\"https://www.illumina.com/systems/sequencing-platforms.html\" target=\"_blank\" rel=\"noopener noreferrer\">Illumina’s short-read sequencing</a>, <a href=\"https://www.pacb.com/technology/hifi-sequencing/\" target=\"_blank\" rel=\"noopener noreferrer\">PacBio’s long-read sequencing</a>, and <a href=\"https://nanoporetech.com/platform\" target=\"_blank\" rel=\"noopener noreferrer\">Oxford Nanopore Technology’s long-read sequencing</a>. Output from all three platforms was combined to remove platform-specific errors and create a single, accurate reference dataset we call the <a href=\"https://github.com/CASTLE-Panel/castle\" target=\"_blank\" rel=\"noopener noreferrer\">Cancer Standards Long-read Evaluation dataset</a> (CASTLE) for genetic diversity in tumor and normal cells.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Testing DeepSomatic’s ability to spot cancer-related variants</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\">We trained DeepSomatic on three of the breast cancer genomes and the two lung cancer genomes in the CASTLE reference dataset. We then tested DeepSomatic’s performance in several ways, including on the single breast cancer genome that was not included in its training data, and on chromosome 1 from each sample, which we also excluded from the training.</p><p data-block-key=\"1u39v\">Results show that DeepSomatic models developed for each of the three major sequencing platforms performed better than other methods, identifying more tumor variants with higher accuracy. The tools used for comparison on short-read sequencing data were <a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC4242521/\" target=\"_blank\" rel=\"noopener noreferrer\">SomaticSniper</a>, <a href=\"https://gatk.broadinstitute.org/hc/en-us/articles/360037593851-Mutect2\" target=\"_blank\" rel=\"noopener noreferrer\">MuTect2</a> and <a href=\"https://www.nature.com/articles/s41592-018-0051-x\" target=\"_blank\" rel=\"noopener noreferrer\">Strelka2</a> (with SomaticSniper specifically for single nucleotide variants, or SNVs). For long-read sequencing data we compared against <a href=\"https://www.biorxiv.org/content/10.1101/2023.08.17.553778v1\" target=\"_blank\" rel=\"noopener noreferrer\">ClairS</a>, a deep learning model trained on synthetic data.</p><p data-block-key=\"9ghlt\">In our tests DeepSomatic identified 329,011 somatic variants across the six reference cell lines and a seventh preserved sample. DeepSomatic does particularly well at identifying cancer variations that involve insertions and deletions (“Indels”) of genetic code. For these types of variants, DeepSomatic substantially increased the <a href=\"https://en.wikipedia.org/wiki/F-score\" target=\"_blank\" rel=\"noopener noreferrer\">F1-score</a>, a balanced measure of how well the model finds true variants in a sample (recall) while not making false positives (precision). On Illumina sequencing data the next-best method scored 80% at identifying Indels, while DeepSomatic scored 90%. On Pacific Biosciences sequencing data, the next-best method scored less than 50% at identifying Indels, and DeepSomatic scored more than 80%.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-3-BreastCancer.width-1250.png\" alt=\"Plot of accuracy on breast cancer\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-3-BreastCancer.width-1250.png\" alt=\"Plot of accuracy on breast cancer\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"48u3i\">DeepSomatic results (<b>purple</b>) for a breast cancer sample widely used in research, compared to other tools. Several software tools identify cancer variants in Illumina’s data, while only a single alternative (<b>pink</b>) exists for the long-read sequencing data generated by PacBio and Oxford Nanopore Technologies. The F1-score measures how many variants are discovered and with what accuracy. DeepSomatic performs slightly better for single-letter variations in genetic code, known as single nucleotide variations, and shows major improvements for variations that involve Indels.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ry233\">The seventh sample was one of the previously used research cell lines of a breast cancer tumor that was preserved using <a href=\"https://en.wikipedia.org/wiki/Fixation_(histology)\" target=\"_blank\" rel=\"noopener noreferrer\">formalin-fixed-paraffin-embedded</a> (FFPE). This common preservation method introduces additional patterns of DNA damage that can complicate genetic analysis. This sample was also sequenced using whole <a href=\"https://en.wikipedia.org/wiki/Exome_sequencing\" target=\"_blank\" rel=\"noopener noreferrer\">exome sequencing</a> (WES), a more affordable method that focuses only on the roughly 1% of the genome that codes for proteins. When DeepSomatic was trained on these types of sample data and then tested on chromosome 1, which was reserved from training, it again outperformed other tools, suggesting it can be used to identify variants in lower-quality or historic tumor samples, potentially rescuing samples that have been harder to sequence, and working on clinical data where only the exome was sequenced.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-4-FFPE-WES.width-1250.png\" alt=\"Plot of accuracy on FFPE &amp; WES\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-4-FFPE-WES.width-1250.png\" alt=\"Plot of accuracy on FFPE &amp; WES\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"48u3i\">DeepSomatic has notably higher accuracy on samples prepared with more complicated pre-processing steps involving: <a href=\"https://en.wikipedia.org/wiki/Fixation_(histology)\" target=\"_blank\" rel=\"noopener noreferrer\">fixed formalin paraffin embedded</a> (FFPE), a method used to preserve tissue samples (<b>left</b>), and whole <a href=\"https://en.wikipedia.org/wiki/Exome_sequencing\" target=\"_blank\" rel=\"noopener noreferrer\">exome sequencing</a> (WES), a method to sequence only the parts of the genome that code for proteins (<b>right</b>). The middle section shows a sample that was preserved with FFPE and also sequenced using whole exome sequencing.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Applying DeepSomatic to other cancers</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\">To test DeepSomatic’s performance on other types of cancers, we analyzed a single sample of <a href=\"https://www.mayoclinic.org/diseases-conditions/glioblastoma/symptoms-causes/syc-20569077\" target=\"_blank\" rel=\"noopener noreferrer\">glioblastoma</a>, an aggressive form of brain cancer that arises from a small number of variants. DeepSomatic was able to pinpoint those variants, showing that it can generalize its learning to apply it to a different cancer type.</p><p data-block-key=\"2ga4q\">We also worked with partners at <a href=\"https://www.childrensmercy.org/childrens-mercy-research-institute/about/\" target=\"_blank\" rel=\"noopener noreferrer\">Children’s Mercy</a> in Kansas City to analyze eight previously sequenced samples of <a href=\"https://www.childrensmercy.org/departments-and-clinics/division-of-pediatric-hematology-oncology-and-blood-and-marrow-transplantation/cancer-center/leukemia-and-lymphoma-program/understanding-leukemia/\" target=\"_blank\" rel=\"noopener noreferrer\">pediatric leukemia</a>, a cancer of the white blood cells that is the most common childhood cancer. Leukemia exists in the bloodstream, so a “normal” non-cancer blood sample is not possible. Despite that challenge, DeepSomatic identified the previously known variants as well as 10 new ones, showing that it can work with a tumor-only sample.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">What’s next</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\">Our hope is that research labs and clinicians can begin to use this tool. Detecting known cancer variants could help choose between existing treatments, such as chemotherapy, immunotherapy or other methods. Identifying new cancer variants could potentially lead to brand-new therapies. We hope people can take these tools and learn more about each cancer tumor, find what’s driving it, and ultimately deliver the most effective treatments to patients.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\"><i>We thank all research participants whose participation in research programs and donation of cell lines made this work and other biomedical research possible. We thank our collaborators at UC Santa Cruz Genomics Institute, the National Cancer Institute, the Frederick National Laboratory for Cancer Research, Children’s Mercy Hospital, and NYU. We thank Hannah Hickey for writing contributions. We thank Avinatan Hassidim, Katherine Chou, Lizzie Dorfman, and Yossi Matias for research leadership support. We thank Resham Parikh and Isha Mishra for communications support.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Coral NPU：面向边缘AI的全栈平台 (原标题: Coral NPU: A full-stack platform for Edge AI)",
      "link": "https://research.google/blog/coral-npu-a-full-stack-platform-for-edge-ai/",
      "pubDate": "Tue, 14 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-14T16:00:00.000Z",
      "creator": "Google",
      "summary": "# Coral NPU：面向边缘AI的全栈平台\n\n生成式AI极大地改变了我们对技术的期望，但未来的技术飞跃在于将AI智能直接嵌入到我们的个人环境中，使其能在我们穿戴和携带的设备上运行，实现真正的辅助性、私密且全天候的体验。然而，这带来了三大核心挑战：\n\n*   **性能差距**：复杂的机器学习模型需要大量计算，远超边缘设备的功耗、散热和内存限制。\n*   **碎片化成本**：为多样化的专有处理器编译和优化ML模型既困难又昂贵，阻碍了设备间性能的一致性。\n*   **用户信任赤字**：个人AI必须优先考虑个人数据的隐私和安全。\n\n为解决这些问题，我们推出了 **Coral NPU**，这是一个全栈平台，旨在为硬件设计者和ML开发者提供构建下一代私密、高效边缘AI设备所需的工具。Coral NPU与Google Research和Google DeepMind合作设计，是一种AI优先的硬件架构，旨在实现超低功耗、始终在线的边缘AI。它提供统一的开发体验，简化了环境感知等应用的部署，并专为在可穿戴设备上实现全天候AI而设计，同时最大限度地减少电池使用，并可配置以支持更高性能的用例。\n\n## Coral NPU：AI优先的架构\n\n为低功耗边缘设备进行开发的开发者面临一个基本权衡：选择通用CPU还是专用加速器。通用CPU提供灵活性和广泛的软件支持，但缺乏针对ML工作负载的领域特定架构，导致性能低下且功耗效率不高。相反，专用加速器提供高ML效率，但缺乏灵活性，难以编程，不适合通用任务。\n\nCoral NPU架构通过颠覆传统的芯片设计来直接解决这个问题。它优先考虑ML矩阵引擎而非标量计算，从芯片层面优化AI架构，创建了一个专为更高效的设备端推理而构建的平台。\n\n作为一套完整的参考神经网络处理单元（NPU）架构，Coral NPU为下一代节能、ML优化系统级芯片（SoC）提供了构建模块。该架构基于一套符合RISC-V ISA的架构IP块，旨在实现最小功耗，非常适合始终在线的环境感知。基础设计提供512 GOPS（每秒十亿次操作）的性能，同时仅消耗几毫瓦，从而为边缘设备、听戴设备、AR眼镜和智能手表提供强大的设备端AI能力。\n\n![Coral NPU 生态系统](https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-1.width-1250.png)\n*Coral NPU生态系统的统一视图，展示了面向SoC设计者和ML开发者的端到端堆栈。基于RISC-V的开放和可扩展架构使SoC设计者能够灵活修改基础设计，或将其用作预配置的NPU。*\n\nCoral NPU架构包括以下组件：\n\n*   **标量核心**：一个轻量级、C语言可编程的RISC-V前端，管理数据流到后端核心，采用简单的“运行至完成”模型，实现超低功耗和传统CPU功能。\n*   **向量执行单元**：一个强大的单指令多数据（SIMD）协处理器，符合RISC-V向量指令集（RVV）v1.0，能够同时对大型数据集进行操作。\n*   **矩阵执行单元**：一个高效的量化外积乘累加（MAC）引擎，专为加速基本神经网络操作而构建。（注：该矩阵执行单元仍在开发中，将于今年晚些时候在GitHub上发布。）\n\n![Coral NPU 架构转变](https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-2.width-1250.png)\n*可视化从传统设计到Coral NPU的架构转变。*\n\n## 统一的开发者体验\n\nCoral NPU架构是一个简单、C语言可编程的目标，可以与IREE和TFLM等现代编译器无缝集成。这使得它能够轻松支持TensorFlow、JAX和PyTorch等ML框架。\n\nCoral NPU包含一个全面的软件工具链，包括针对TensorFlow的TFLM编译器、通用MLIR编译器、C编译器、自定义内核和模拟器等专业解决方案。这为开发者提供了灵活的路径。例如，来自JAX等框架的模型首先使用StableHLO方言导入到MLIR格式。然后，这个中间文件被输入到IREE编译器，该编译器应用一个硬件特定的插件来识别Coral NPU的架构。之后，编译器执行渐进式降低——这是一个关键的优化步骤，代码通过一系列方言系统地转换，使其更接近机器的本地语言。优化后，工具链生成一个最终的、紧凑的二进制文件，准备在边缘设备上高效执行。这套行业标准的开发工具简化了ML模型的编程，并可以在各种硬件目标上提供一致的体验。\n\n![Coral NPU 编译器工具链](https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-3.width-1250.png)\n*Coral NPU编译器工具链，展示了从ML模型创建到优化、编译再到设备端部署的完整流程。*\n\nCoral NPU的协同设计过程侧重于两个关键领域：首先，该架构高效加速了当今设备端视觉和音频应用中领先的基于编码器的架构。其次，我们正与Gemma团队紧密合作，优化Coral NPU以支持小型Transformer模型，确保加速器架构支持下一代边缘生成式AI。这种双重关注意味着Coral NPU有望成为第一个开放、基于标准、低功耗的NPU，旨在将LLM带到可穿戴设备上。\n\n## 目标应用\n\nCoral NPU旨在实现超低功耗、始终在线的边缘AI应用，特别关注环境感知系统。其主要目标是在可穿戴设备、手机和物联网（IoT）设备上实现全天候AI体验，同时最大限度地减少电池使用。\n\n潜在用例包括：\n\n*   **情境感知**：检测用户活动（例如，步行、跑步）、接近度或环境（例如，室内/室外、移动中），以启用“请勿打扰”模式或其他情境感知功能。\n*   **音频处理**：语音和语音检测、关键词识别、实时翻译、转录以及基于音频的辅助功能。\n*   **图像处理**：人物和物体检测、面部识别、手势识别和低功耗视觉搜索。\n*   **用户交互**：通过手势、音频提示或其他传感器驱动输入实现控制。\n\n## 硬件强制隐私\n\nCoral NPU的一个核心原则是通过硬件强制安全来建立用户信任。我们的架构正在设计中，以支持CHERI等新兴技术，该技术提供细粒度的内存级安全和可扩展的软件隔离。通过这种方法，我们希望能够将敏感的AI模型和个人数据隔离在硬件强制的沙箱中，从而减轻基于内存的攻击。\n\n## 构建生态系统\n\n开放硬件项目的成功依赖于强大的合作伙伴关系。为此，我们正与Synaptics合作，Synaptics是我们在嵌入式计算、无线连接和物联网多模态传感领域的首个战略硅合作伙伴和领导者。Synaptics在其技术日上宣布了其新的Astra™ SL2610系列AI原生物联网处理器。该产品线采用了其Torq™ NPU子系统，这是业界首个Coral NPU架构的生产实现。该NPU的设计支持Transformer模型和动态操作符，使开发者能够为消费和工业物联网构建面向未来的边缘AI系统。\n\n此次合作支持了我们对统一开发者体验的承诺。Synaptics Torq™边缘AI平台基于IREE和MLIR的开源编译器和运行时构建。这次合作是朝着为智能、情境感知设备构建共享、开放标准迈出的重要一步。\n\n## 解决边缘核心危机\n\n通过Coral NPU，我们正在为个人AI的未来构建一个基础层。我们的目标是提供一个通用、开源和安全的平台，供行业在此基础上进行构建，从而培育一个充满活力的生态系统。这使得开发者和硅供应商能够超越当今碎片化的格局，在边缘计算的共享标准上进行协作，从而加速创新。",
      "shortSummary": "Coral NPU是一个面向边缘AI的全栈平台，旨在解决将生成式AI引入个人设备所面临的性能、碎片化和隐私挑战。它是一种AI优先、超低功耗的NPU架构，基于RISC-V，专为可穿戴设备和物联网的始终在线边缘AI设计。Coral NPU提供统一的开发体验，支持主流ML框架和编译器，可实现环境感知、情境识别和实时处理等应用。它通过硬件强制安全保障用户隐私，并与Synaptics合作，已实现首次生产部署，旨在构建一个开放、安全的边缘AI生态系统。",
      "translated_title": "Coral NPU：面向边缘AI的全栈平台",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-1.width-1250.png",
          "alt": "Coral NPU-1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-2.width-1250.png",
          "alt": "Coral NPU-2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-3.width-1250.png",
          "alt": "Coral NPU-3",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"tf5oi\">Generative AI has fundamentally reshaped our expectations of technology. We've seen the power of large-scale cloud-based models to create, reason and assist in incredible ways. However, the next great technological leap isn't just about making cloud models bigger; it's about embedding their intelligence directly into our immediate, personal environment. For AI to be truly assistive — proactively helping us navigate our day, translating conversations in real-time, or understanding our physical context — it must run on the devices we wear and carry. This presents a core challenge: embedding ambient AI onto battery-constrained edge devices, freeing them from the cloud to enable truly private, all-day assistive experiences.</p><p data-block-key=\"1a1a\">To move from the cloud to personal devices, we must solve three critical problems:</p><ul><li data-block-key=\"eu56\"><i>The performance gap:</i> Complex, state-of-the-art machine learning (ML) models demand more compute, far exceeding the limited power, thermal, and memory budgets of an edge device.</li><li data-block-key=\"7buqm\"><i>The fragmentation tax:</i> Compiling and optimizing ML models for a diverse landscape of proprietary processors is difficult and costly, hindering consistent performance across devices.</li><li data-block-key=\"a3ap1\"><i>The user trust deficit:</i> To be truly helpful, personal AI must prioritize the privacy and security of personal data and context.</li></ul><p data-block-key=\"ejp6o\">Today we introduce <a href=\"https://developers.google.com/coral\" target=\"_blank\" rel=\"noopener noreferrer\">Coral NPU</a>, a full-stack platform that builds on our original work from <a href=\"https://gweb-coral-full.uc.r.appspot.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Coral</a> to provide hardware designers and ML developers with the tools needed to build the next generation of private, efficient edge AI devices. Co-designed in partnership with Google Research and Google DeepMind, Coral NPU is an AI-first hardware architecture built to enable the next generation of ultra-low-power, always-on edge AI. It offers a unified developer experience, making it easier to deploy applications like ambient sensing. It's specifically designed to enable all-day AI on wearable devices while minimizing battery usage and being configurable for higher performance use cases. We’ve released our <a href=\"https://developers.google.com/coral\" target=\"_blank\" rel=\"noopener noreferrer\">documentation and tools</a> so that developers and designers can start building today.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Coral NPU: An AI-first architecture</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tf5oi\">Developers building for low-power edge devices face a fundamental trade-off, choosing between general purpose CPUs and specialized accelerators. General-purpose CPUs offer crucial flexibility and broad software support but lack the domain-specific architecture for demanding ML workloads, making them less performant and power-inefficient. Conversely, specialized accelerators provide high ML efficiency but are inflexible, difficult to program, and ill-suited for general tasks.</p><p data-block-key=\"1fta9\">This hardware problem is magnified by a highly fragmented software ecosystem. With starkly different programming models for CPUs and ML blocks, developers are often forced to use proprietary compilers and complex command buffers. This creates a steep learning curve and makes it difficult to combine the unique strengths of different compute units. Consequently, the industry lacks a mature, low-power architecture that can easily and effectively support multiple ML development frameworks.</p><p data-block-key=\"a5efi\">The Coral NPU architecture directly addresses this by reversing traditional chip design. It prioritizes the ML matrix engine over scalar compute, optimizing architecture for AI from silicon up and creating a platform purpose-built for more efficient, on-device inference.</p><p data-block-key=\"43d11\">As a complete, reference <a href=\"https://en.wikipedia.org/wiki/Neural_processing_unit\" target=\"_blank\" rel=\"noopener noreferrer\">neural processing unit</a> (NPU) architecture, Coral NPU provides the building blocks for the next generation of energy-efficient, ML-optimized <a href=\"https://en.wikipedia.org/wiki/System_on_a_chip\" target=\"_blank\" rel=\"noopener noreferrer\">systems on chip</a> (SoCs). The architecture is based on a set of <a href=\"https://riscv.org/specifications/ratified/\" target=\"_blank\" rel=\"noopener noreferrer\">RISC-V ISA</a> compliant architectural IP blocks and is designed for minimal power consumption, making it ideal for always-on ambient sensing. The base design delivers performance in the 512 <a href=\"https://en.wikipedia.org/wiki/Floating_point_operations_per_second\" target=\"_blank\" rel=\"noopener noreferrer\">giga operations per second</a> (GOPS) range while consuming just a few milliwatts, thus enabling powerful on-device AI for edge devices, hearables, AR glasses, and smartwatches.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-1.width-1250.png\" alt=\"Coral NPU-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-1.width-1250.png\" alt=\"Coral NPU-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vajmb\"><i>A unified view of the Coral NPU ecosystem, showcasing end-to-end stack for SoC designers and ML developers.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-vertical-padding --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"tf5oi\">The open and extensible <a href=\"https://developers.google.com/coral/guides/architecture\" target=\"_blank\" rel=\"noopener noreferrer\">architecture</a> based on RISC-V gives SoC designers flexibility to modify the base design, or use it as a pre-configured NPU. The Coral NPU architecture includes the following components:</p><ul><li data-block-key=\"bclpo\"><i>A scalar core:</i> A lightweight, C-programmable RISC-V frontend that manages data flow to the back-end cores, using a simple \"run-to-completion\" model for ultra-low power consumption and traditional CPU functions.</li><li data-block-key=\"1u961\"><i>A vector execution unit:</i> A robust single instruction multiple data (<a href=\"https://www.sciencedirect.com/topics/computer-science/single-instruction-multiple-data\" target=\"_blank\" rel=\"noopener noreferrer\">SIMD</a>) co-processor compliant with the RISC-V Vector instruction set (RVV) v1.0, enabling simultaneous operations on large data sets.</li><li data-block-key=\"2ju85\"><i>A matrix execution unit:</i> A highly efficient quantized outer product <a href=\"https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation\" target=\"_blank\" rel=\"noopener noreferrer\">multiply-accumulate</a> (MAC) engine purpose-built to accelerate fundamental neural network operations. Note that the matrix execution unit is still under development and will be released on <a href=\"https://github.com/google-coral/coralnpu\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a> later this year.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-2.width-1250.png\" alt=\"Coral NPU-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-2.width-1250.png\" alt=\"Coral NPU-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vajmb\"><i>Visualizing the architectural shift from traditional design to the Coral NPU.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Unified developer experience</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tf5oi\">The Coral NPU architecture is a simple, C-programmable target that can seamlessly integrate with modern compilers like <a href=\"https://iree.dev/\" target=\"_blank\" rel=\"noopener noreferrer\">IREE</a> and <a href=\"https://github.com/tensorflow/tflite-micro\" target=\"_blank\" rel=\"noopener noreferrer\">TFLM</a>. This enables easy support for ML frameworks like <a href=\"https://www.tensorflow.org/\" target=\"_blank\" rel=\"noopener noreferrer\">TensorFlow</a>, <a href=\"https://docs.jax.dev/en/latest/\" target=\"_blank\" rel=\"noopener noreferrer\">JAX</a>, and <a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">PyTorch</a>.</p><p data-block-key=\"46n8n\">Coral NPU incorporates a comprehensive software toolchain, including specialized solutions like the TFLM compiler for TensorFlow, alongside a general-purpose <a href=\"https://mlir.llvm.org/\" target=\"_blank\" rel=\"noopener noreferrer\">MLIR</a> compiler, C compiler, custom kernels, and a simulator. This provides developers with flexible pathways. For example, a model from a framework like JAX is first imported into the MLIR format using the <a href=\"https://openxla.org/stablehlo\" target=\"_blank\" rel=\"noopener noreferrer\">StableHLO</a> dialect. This intermediate file is then fed into the IREE compiler, which applies a hardware-specific plug-in to recognize the Coral NPU's architecture. From there, the compiler performs progressive lowering — a critical optimization step where the code is systematically translated through a series of dialects, moving closer to the machine's native language. After optimization, the toolchain generates a final, compact binary file ready for efficient execution on the edge device. This suite of industry-standard developer tools helps simplify the programming of ML models and can allow for a consistent experience across various hardware targets.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-3.width-1250.png\" alt=\"Coral NPU-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Coral_NPU-3.width-1250.png\" alt=\"Coral NPU-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"vajmb\"><i>The Coral NPU compiler toolchain, illustrating the complete flow from ML model creation through optimization and compilation to on-device deployment.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"tf5oi\">Coral NPU’s co-design process focuses on two key areas. First, the architecture efficiently accelerates the leading encoder-based architectures used in today's on-device vision and audio applications. Second, we are collaborating closely with the <a href=\"https://deepmind.google/models/gemma/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma</a> team to optimize Coral NPU for small transformer models, helping to ensure the accelerator architecture supports the next generation of generative AI at the edge.</p><p data-block-key=\"brs14\">This dual focus means Coral NPU is on track to be the first open, standards-based, low-power NPU designed to bring LLMs to wearables. For developers, this provides a single, validated path to deploy both current and future models with maximum performance at minimal power.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Target applications</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tf5oi\">Coral NPU is designed to enable ultra-low-power, always-on edge AI applications, particularly focused on ambient sensing systems. Its primary goal is to enable all day AI-experiences on wearables, mobile phones and <a href=\"https://en.wikipedia.org/wiki/Internet_of_things\" target=\"_blank\" rel=\"noopener noreferrer\">Internet of Things</a> (IoT) devices minimizing battery usage.</p><p data-block-key=\"54nt5\">Potential use cases include:</p><ul><li data-block-key=\"7himc\"><i>Contextual awareness:</i> Detecting user activity (e.g., walking, running), proximity, or environment (e.g., indoors/outdoors, on-the-go) to enable \"do-not-disturb\" modes or other context-aware features.</li><li data-block-key=\"ff339\"><i>Audio processing:</i> Voice and speech detection, keyword spotting, live translation, transcription, and audio-based accessibility features.</li><li data-block-key=\"ai106\"><i>Image processing:</i> Person and object detection, facial recognition, gesture recognition, and low-power visual search.</li><li data-block-key=\"9m2mn\"><i>User interaction:</i> Enabling control via hand gestures, audio cues, or other sensor-driven inputs.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Hardware-enforced privacy</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tf5oi\">A core principle of Coral NPU is building user trust through hardware-enforced security. Our architecture is being designed to support emerging technologies like <a href=\"https://www.cl.cam.ac.uk/research/security/ctsrd/cheri/\" target=\"_blank\" rel=\"noopener noreferrer\">CHERI</a>, which provides fine-grained memory-level safety and scalable software compartmentalization. With this approach, we hope to enable sensitive AI models and personal data to be isolated in a hardware-enforced sandbox, mitigating memory-based attacks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Building an ecosystem</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tf5oi\">Open hardware projects rely on strong partnerships to succeed. To that end, we’re collaborating with <a href=\"https://www.synaptics.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Synaptics</a>, our first strategic silicon partner and a leader in embedded compute, wireless connectivity, and multimodal sensing for the IoT. Today, at their Tech Day, Synaptics announced their new <a href=\"https://www.synaptics.com/sl2610-press-release\" target=\"_blank\" rel=\"noopener noreferrer\">Astra™ SL2610</a> line of <a href=\"https://www.synaptics.com/sl2610-product-line\" target=\"_blank\" rel=\"noopener noreferrer\">AI-Native IoT Processors</a>. This product line features their <a href=\"https://www.synaptics.com/torq-github\" target=\"_blank\" rel=\"noopener noreferrer\">Torq™ NPU</a> subsystem, the industry’s first production implementation of the Coral NPU architecture. The NPU’s design is transformer-capable and supports dynamic operators, enabling developers to build future-ready Edge AI systems for consumer and industrial IoT.</p><p data-block-key=\"4r0e3\">This partnership supports our commitment to a unified developer experience. The Synaptics Torq™ Edge AI platform is built on an open-source compiler and runtime based on IREE and MLIR. This collaboration is a significant step toward building a shared, open standard for intelligent, context-aware devices.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Solving core crises of the Edge</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"tf5oi\">With Coral NPU, we are building a foundational layer for the future of personal AI. Our goal is to foster a vibrant ecosystem by providing a common, open-source, and secure platform for the industry to build upon. This empowers developers and silicon vendors to move beyond today's fragmented landscape and collaborate on a shared standard for edge computing, enabling faster innovation. Learn more about <a href=\"https://developers.google.com/coral\" target=\"_blank\" rel=\"noopener noreferrer\">Coral NPU</a> and start building today.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9lzmb\"><i>We would like to thank the core contributors and leadership team for this work, particularly Billy Rutledge, Ben Laurie, Derek Chow, Michael Hoang, Naveen Dodda, Murali Vijayaraghavan, Gregory Kielian, Matthew Wilson, Bill Luan, Divya Pandya, Preeti Singh, Akib Uddin, Stefan Hall, Alex Van Damme, David Gao, Lun Dong, Julian Mullings-Black, Roman Lewkow, Shaked Flur, Yenkai Wang, Reid Tatge, Tim Harvey, Tor Jeremiassen, Isha Mishra, Kai Yick, Cindy Liu, Bangfei Pan, Ian Field, Srikanth Muroor, Jay Yagnik, Avinatan Hassidim, and Yossi Matias.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "XR Blocks：加速AI + XR创新 (原标题: XR Blocks: Accelerating AI + XR innovation)",
      "link": "https://research.google/blog/xr-blocks-accelerating-ai-xr-innovation/",
      "pubDate": "Wed, 08 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-08T16:00:00.000Z",
      "creator": "Google",
      "summary": "## XR Blocks：加速AI + XR创新\n\n### 引言：弥合AI与XR之间的鸿沟\n\n当前，人工智能（AI）与扩展现实（XR）的结合有望开启沉浸式智能计算的新范式。然而，这两个领域之间存在显著的生态系统差距。AI的研发得益于JAX、PyTorch、TensorFlow等成熟框架以及ImageNet、LMArena等基准测试的加速，而AI驱动的XR交互原型开发仍是一个高摩擦过程，通常需要开发者手动集成感知、渲染和交互等低级系统。\n\n为了弥合这一鸿沟，我们推出了 **XR Blocks**（在ACM UIST 2025上发布），这是一个跨平台框架，旨在加速以人为中心的AI + XR创新。这是我们之前针对非XR用例、通过可视化编程简化机器学习管道原型的Visual Blocks for ML研究的重大进展。\n\n### XR Blocks框架概述\n\nXR Blocks提供了一个模块化架构，包含即插即用组件，用于AI + XR中的核心抽象：用户、世界、界面、AI和代理。其核心使命是加速感知型AI + XR应用的快速原型开发。该工具包基于WebXR、threejs、LiteRT和Gemini等易于访问的技术构建，降低了XR创作者的入门门槛。我们通过一系列开源模板、实时演示和GitHub上的源代码展示了其效用，目标是赋能社区快速将概念转化为交互式原型。更多能力概述可在我们的方向性论文和预告视频中找到。\n\n### 设计原则\n\n我们的架构和API设计选择遵循以下三个原则：\n\n*   **拥抱简洁性和可读性：** 受Python禅意的启发，我们优先考虑清晰、人类可读的抽象。开发者的脚本应像对所需体验的高级描述。简单的任务应易于实现，复杂的逻辑应保持明确和可理解。\n*   **优先考虑创作者体验：** 我们的主要目标是使智能和感知型XR应用的创作尽可能无缝。我们认为创作者应专注于用户体验，而不是传感器融合、AI模型集成或跨平台交互逻辑等低级“管道”工作。\n*   **实用主义而非完整性：** 鉴于AI和XR领域发展迅速，我们遵循实用主义的设计理念。一个试图做到完美、全面复杂的框架在发布时可能已经过时。我们倾向于一个简单、模块化、适应性强的架构，它可以在桌面模拟器和Android XR设备上运行，适用于广泛的应用。\n\n### 框架工作原理\n\nXR Blocks框架从Visual Blocks for ML和InstructPipe中汲取灵感，提供了一个高层级、以人为中心的抽象层，将交互的“意图”（Script，下文详述）与低级实现的“方式”分离。\n\n![XR Blocks框架](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks1_Framework.width-1250.png)\n\nXR Blocks加速了桌面模拟器和Android XR设备上实时AI + XR应用的原型开发。例如：\n*   **(a) XR真实感：** 在模拟中原型化深度感知、基于物理的交互，并将相同的代码部署到真实世界的XR设备。\n*   **(b) XR交互：** 将自定义手势模型无缝集成到桌面模拟器和设备上的XR部署。\n*   **(c) AI + XR集成：** 构建智能、上下文感知的助手，例如提供主动建议和不显眼交互的Sensible Agent原型。\n\n### 抽象层：现实模型 (Reality Model)\n\n我们提出了一个由高级抽象组成的新“现实模型”，以指导XR Blocks框架的实现。与为端到端无监督训练设计的“世界模型”不同，我们的现实模型由可替换的XR交互模块组成。我们设计的核心是 **Script**，它是应用程序的叙事和逻辑中心。Script操作六个核心原语：\n\n*   **用户与物理世界：** 我们的模型以用户为中心，包括手部、凝视和虚拟形象。物理世界允许Script查询感知的现实，例如深度、估计的光照条件和对象。\n*   **虚拟界面与上下文：** 该模型通过虚拟UI元素增强混合现实，从2D面板到完全3D资产。感知管道分析环境、活动和交互历史的上下文。一个示例应用可在Sensible Agent中找到。\n*   **智能与社交实体：** 我们将AI驱动的代理和远程人类伙伴视为模型中的主要实体。这使得在DialogLab中实现混合人机对话中的动态群组对话。\n\n![XR Blocks框架的现实模型概念图](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks2_RealityModel.width-1250.png)\n\n### 实现：核心引擎 (Core Engine)\n\n这个现实模型由XR Blocks的模块化核心引擎实现，该引擎提供高级API，使开发者能够利用以下子系统，而无需掌握其底层实现：\n\n*   **感知与输入管道：** 摄像头、深度和声音模块持续为现实模型提供并更新物理现实的表示。输入模块标准化来自各种设备的用户操作，为XR Blocks提供解释的原始数据。\n*   **AI作为核心工具：** `ai`模块充当中央神经系统，提供简单而强大的函数（`.query`、`.runModel`），使大型模型成为可访问的工具。\n*   **体验与可视化工具包：** 为了实现快速创建，该工具包提供了一个常用功能库。`ux`模块提供可重用的交互行为，如`.selectable`和`.draggable`，而`ui`和`effect`模块处理界面渲染和复杂视觉效果，如遮挡。\n\n![XR Blocks核心引擎的模块化架构](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks3_Architecture.width-1250.png)\n\n通过将抽象的现实模型与具体的核心引擎分离，XR Blocks实现了强大的新创意工作流。目标是让创作者更快地从高层级、以人为中心的想法转向交互式原型。我们设想未来，任何声明性提示，例如“当用户捏住一个物体时，代理应该为它生成一首诗”，都可以直接转化为XR Blocks中的高级指令：\n\n![XR Blocks中的高层级指令](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks4_Instructions.width-1250.png)\n\n因此，创作者的提示不再是伪代码，而是实现逻辑的直接总结。我们设想这个框架能更无缝地将用户意图转化为系统级执行流，组合来自输入、声音、AI、世界、UI和代理模块的功能，以通过用户交互生成新兴的智能行为。\n\n![XR Blocks的交互语法](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks5_Interaction.width-1250.png)\n\n### 应用场景\n\n我们提供了一套交互式应用程序，以展示XR Blocks框架的表达能力和灵活性。这些示例展示了我们的框架如何实现以前过于复杂和昂贵而无法构建的复杂体验的快速原型开发，从而促进创建逼真、交互式和智能的混合现实世界：\n\n![XR Blocks的应用](https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks_Applications.width-1250.png)\n\nXR Blocks的应用包括：\n1.  **XR真实感：** 深度感知和基于物理的球池和泼水游戏；几何感知阴影、带遮挡的3D高斯泼溅和光照估计。\n2.  **XR交互：** 由自定义ML模型、动态滑动识别、与物理世界的触摸和抓取赋能的沉浸式表情符号和剪刀石头布游戏。\n3.  **AI + XR：** 与对话式AI、XR对象、XR中的眼镜模拟以及用真实世界摄像头生成诗歌的集成。\n\n当这个现实模型与生成式AI深度集成以创建动态、个性化环境时，框架的真正力量得以实现。我们通过构建诸如“增强对象智能”（XR-Objects）之类的系统来证明这一点，该系统赋予日常物理对象交互式数字功能，例如动态虚拟按钮。XR Blocks也作为Sensible Agent（在ACM UIST 2025上发布）的基础，这是一个用于主动和不显眼AR辅助的系统。我们的架构提供了代理的核心感知和交互逻辑，提供了我们主要目标的一个例子：通过提供强大、高层级的工具，XR Blocks赋能人机交互研究人员绕过低级实现，直接专注于人机协作的认知原理等更高阶的挑战。SDK演示视频展示了XR Blocks与对话式AI的结合、Android XR上的物理碰撞与深度感知，以及在设备上运行LiteRT与自定义手势模型以触发XR动画。\n\n### 结论与未来展望\n\n创建智能XR体验目前过于碎片化，在创作者的愿景和实现之间设置了主要障碍。我们介绍了XR Blocks，一个通过提供高层级抽象层来消除这种复杂性的架构和工具包，它将“意图”（what）与“方式”（how，低级实现）分离，极大地加速了上下文感知应用的快速原型开发。这是迈向未来编程、设计和对话界限消失的奠基性一步，使我们能够像编写故事一样流畅地编写现实。XR Blocks远非完美，这项工作是最初的愿景文档，旨在邀请更多创作者加入我们的旅程，基于我们相信拥有正确工具集，每个人都可以通过AI释放内在创造力。\n\n### 致谢\n\n这项工作是Google多个团队的联合协作成果，由David Li和Ruofei Du（同等主要贡献）、Nels Numan、Xun Qian、Yanhe Chen和Zhongyi Zhou（同等次要贡献，按字母顺序排列）以及Evgenii Alekseev、Geonsun Lee、Alex Cooper、Min Xia、Scott Chung、Jeremy Nelson、Xiuxiu Yuan、Jolica Dias、Tim Bettridge、Benjamin Hersh、Michelle Huynh、Konrad Piascik、Ricardo Cabello和David Kim等研究人员和工程师共同贡献。我们感谢Mahdi Tayarani、Max Dzitsiuk、Patrick Hackett、Seeyam Qiu、Brian Collins、Steve Toh、Eric Gonzalez、Nicolás Peña Moreno、Yi-Fei Li、Ziyi Liu、Jing Jin对我们早期提案和WebXR实验的反馈和讨论。我们感谢Max Spear、Adarsh Kowdle和Guru Somadder的方向性贡献和周到评审。",
      "shortSummary": "XR Blocks是一个跨平台框架，旨在弥合人工智能（AI）和扩展现实（XR）生态系统之间的鸿沟。它通过提供模块化架构和高层级抽象，加速感知型AI + XR应用的快速原型开发。该框架基于WebXR等技术，简化了AI模型集成、传感器融合和跨平台交互等复杂任务，使创作者能够专注于用户体验。XR Blocks通过现实模型和核心引擎，支持从高层级意图到交互式原型的快速转化，并已通过多种应用场景展示其强大功能，旨在赋能社区释放AI创造力。",
      "translated_title": "XR Blocks：加速AI + XR创新",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks1_Framework.width-1250.png",
          "alt": "XRBlocks1_Framework",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks2_RealityModel.width-1250.png",
          "alt": "XRBlocks2_RealityModel",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks3_Architecture.width-1250.png",
          "alt": "XRBlocks3_Architecture",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks4_Instructions.width-1250.png",
          "alt": "XRBlocks4_Instructions",
          "title": "",
          "position": 5
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks5_Interaction.width-1250.png",
          "alt": "XRBlocks5_Interaction",
          "title": "",
          "position": 6
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"x6cvv\">The combination of artificial intelligence (AI) and extended reality (XR) has the potential to unlock a new paradigm of immersive intelligent computing. However, a significant gap exists between the ecosystems of these two fields today. AI research and development is accelerated by mature frameworks like <a href=\"https://jax.readthedocs.io/en/latest/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">JAX</a>, <a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">PyTorch</a>, <a href=\"https://www.tensorflow.org/\" target=\"_blank\" rel=\"noopener noreferrer\">TensorFlow</a>, and benchmarks like <a href=\"https://www.image-net.org/\" target=\"_blank\" rel=\"noopener noreferrer\">ImageNet</a> and <a href=\"https://lmarena.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">LMArena</a>. Meanwhile, prototyping novel AI-driven XR interactions remains a high-friction process, often requiring practitioners to manually integrate disparate, low-level systems for perception, rendering, and interaction.</p><p data-block-key=\"5pqcb\">To bridge this gap, we introduce <a href=\"http://xrblocks.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">XR Blocks</a> (presented at <a href=\"https://uist.acm.org/2025/\" target=\"_blank\" rel=\"noopener noreferrer\">ACM UIST 2025</a>), a cross-platform framework designed to accelerate human-centered AI + XR innovation. This is a significant step from our prior research in <a href=\"https://research.google/blog/visual-blocks-for-ml-accelerating-machine-learning-prototyping-with-interactive-tools/\">Visual Blocks for ML</a>, which targets non-XR use cases and streamlines prototyping machine learning pipelines with visual programming. XR Blocks provides a modular architecture with plug-and-play components for core abstraction in AI + XR: <i>user</i>, <i>world</i>, <i>interface</i>, <i>AI</i>, and <i>agents</i>. Crucially, it is designed with the mission of accelerating rapid prototyping of perceptive AI + XR apps. Built upon accessible technologies (<a href=\"https://immersiveweb.dev/\" target=\"_blank\" rel=\"noopener noreferrer\">WebXR</a>, <a href=\"https://threejs.org/\" target=\"_blank\" rel=\"noopener noreferrer\">threejs</a>, <a href=\"https://ai.google.dev/edge/litert\" target=\"_blank\" rel=\"noopener noreferrer\">LiteRT</a>, <a href=\"https://gemini.google.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a>), our toolkit lowers the barrier to entry for XR creators. We demonstrate its utility through a set of open-source <a href=\"https://xrblocks.github.io/docs/templates/Basic/\" target=\"_blank\" rel=\"noopener noreferrer\">templates</a>, <a href=\"https://xrblocks.github.io/docs/samples/ModelViewer/\" target=\"_blank\" rel=\"noopener noreferrer\">live demos</a>, and <a href=\"https://github.com/google/xrblocks\" target=\"_blank\" rel=\"noopener noreferrer\">source code on GitHub</a>, with the goal of empowering the community to quickly move from concept to interactive prototype. You can find an overview of these capabilities in our <a href=\"https://arxiv.org/abs/2509.25504\" target=\"_blank\" rel=\"noopener noreferrer\">directional paper</a> and <a href=\"https://www.youtube.com/watch?v=75QJHTsAoB8\" target=\"_blank\" rel=\"noopener noreferrer\">teaser video</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"75QJHTsAoB8\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=75QJHTsAoB8\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kk7t9\"><i>Introductory video of XR Blocks.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xfvek\">Design principles</h2><p data-block-key=\"1ctfe\">Our architectural and API design choices are guided by three principles:</p><ul><li data-block-key=\"f851k\"><i>Embrace simplicity and readability:</i> Inspired by <a href=\"https://en.wikipedia.org/wiki/Zen_of_Python\" target=\"_blank\" rel=\"noopener noreferrer\">Python's Zen</a>, we prioritize clean, human-readable abstractions. A developer's script should read like a high-level description of the desired experience. Simple tasks should be simple to implement, and complex logic should remain explicit and understandable.</li><li data-block-key=\"43oej\"><i>Prioritize the creator experience</i>: Our primary goal is to make authoring intelligent and perceptive XR applications as seamless as possible. We believe that creators should focus on the user experience, not on the low-level “plumbing” of sensor fusion, AI model integration, or cross-platform interaction logic.</li><li data-block-key=\"5dabh\"><i>Pragmatism over completeness</i>: We follow a design philosophy of pragmatism, since the fields of AI and XR are evolving quickly. A comprehensive, complex framework that attempts to be perfect will be obsolete upon release. We favor a simple, modular, and adaptable architecture that runs on both desktop and <a href=\"https://www.android.com/xr/\" target=\"_blank\" rel=\"noopener noreferrer\">Android XR</a> devices for a wide range of applications.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"xfvek\">XR Blocks framework</h2><p data-block-key=\"6k5u2\">Drawing inspiration from <a href=\"https://visualblocks.withgoogle.com/#/\" target=\"_blank\" rel=\"noopener noreferrer\">Visual Blocks for ML</a> and <a href=\"https://research.google/blog/instructpipe-generating-visual-blocks-pipelines-with-human-instructions-and-llms/\">InstructPipe</a>, we designed the XR Blocks framework to provide a high-level, human-centered abstraction layer that separates the <i>what</i> of an interaction (denoted as <i>Script</i>, described more below) from the <i>how</i> of its low-level implementation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks1_Framework.width-1250.png\" alt=\"XRBlocks1_Framework\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks1_Framework.width-1250.png\" alt=\"XRBlocks1_Framework\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kk7t9\"><i>XR Blocks accelerates the prototyping of real-time AI + XR applications across desktop simulators and</i> <a href=\"http://android.com/xr\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Android XR</i></a><i> devices. Examples: (a) XR Realism: Prototype depth-aware, physics-based interactions in simulation and deploy the same code to real-world XR devices. (b) XR Interactions: Seamlessly integrate custom gesture models to desktop simulator and on-device XR deployment. (c) AI + XR Integration: Build intelligent, context-aware assistants, like the</i> <a href=\"https://research.google/blog/sensible-agent-a-framework-for-unobtrusive-interaction-with-proactive-ar-agents/\"><i>Sensible Agent</i></a><i> prototype that provides proactive suggestions with unobtrusive interactions.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"xfvek\">Abstractions</h3><p data-block-key=\"5j9q1\">We propose a new <i>Reality Model</i> composed of high-level abstractions to guide the implementation of the XR Blocks framework. Unlike the <a href=\"https://arxiv.org/abs/1803.10122\" target=\"_blank\" rel=\"noopener noreferrer\">World Model</a> designed for end-to-end unsupervised training, our <i>Reality Model</i> consists of replaceable modules for XR interaction. At the heart of our design is <i>Script</i>, the narrative and logical center of an application. <i>Script</i> operates on six first-class primitives (described and visualized below):</p><ul><li data-block-key=\"7hci8\"><i>User &amp; the physical world:</i> Our model is centered around the <i>User</i>, consisting of hands, gaze, and avatar. The physical <i>world</i> allows <i>Script</i> to query the perceived reality such as depth (<a href=\"https://xrblocks.github.io/docs/samples/DepthMap\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>), estimated lighting condition (<a href=\"https://xrblocks.github.io/docs/samples/Lighting\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>), and objects (<a href=\"https://xrblocks.github.io/docs/samples/Gemini-XRObject\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>).</li><li data-block-key=\"b8t27\"><i>Virtual interfaces &amp; context:</i> The model augments the blended reality with virtual UI elements, from 2D panels (<a href=\"https://xrblocks.github.io/docs/samples/UI\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>) to fully 3D assets (<a href=\"https://xrblocks.github.io/docs/samples/ModelViewer/\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>). The perception pipeline analyzes the context of environment, activities, and histories of interaction. An example application can be found in <a href=\"https://research.google/blog/sensible-agent-a-framework-for-unobtrusive-interaction-with-proactive-ar-agents/\">Sensible Agent</a> (discussed more below).</li><li data-block-key=\"e7sot\"><i>Intelligent &amp; Social Entities</i>: We treat AI-driven <i>agents</i> and remote human <i>peers</i> as primary entities within the model. This enables dynamic group conversations in hybrid human-AI conversations in <a href=\"https://dl.acm.org/doi/10.1145/3746059.3747696\" target=\"_blank\" rel=\"noopener noreferrer\">DialogLab</a>.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks2_RealityModel.width-1250.png\" alt=\"XRBlocks2_RealityModel\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks2_RealityModel.width-1250.png\" alt=\"XRBlocks2_RealityModel\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kk7t9\"><i>The conceptual Reality Model of the XR Blocks framework. At the center, Script contains the application’s logic and operates on a unified model of first-class primitives including the user, the physical world, AI agents, and the application context.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"xfvek\">Implementation</h3><p data-block-key=\"c09j3\">This Reality Model is realized by XR Blocks’s modular Core engine, which provides high-level APIs that enable developers to harness the following subsystems without needing to master the implementation:</p><ul><li data-block-key=\"e15ud\"><i>Perception &amp; input pipeline:</i> The <code>camera</code>, <code>depth</code>, and <code>sound</code> modules continuously feed and update the Reality Model’s representation of physical reality. The <code>input</code> module normalizes user actions from various devices, providing the raw data for XR Blocks to interpret.</li><li data-block-key=\"2r6v8\"><i>AI as a core utility:</i> The <code>ai</code> module acts as a central nervous system, providing simple yet powerful functions (<code>.query</code>, <code>.runModel</code>) that make large models an accessible utility.</li><li data-block-key=\"fahdq\"><i>Experience &amp; visualization toolkit:</i> To enable rapid creation, the toolkit provides a library of common affordances. The <code>ux</code> module offers reusable interaction behaviors like <code>.selectable</code> and <code>.draggable</code> (<a href=\"https://xrblocks.github.io/docs/samples/ModelViewer/\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>), while the <code>ui</code> and <code>effect</code> modules handle the rendering of interfaces and complex visual effects like occlusion (<a href=\"https://xrblocks.github.io/docs/samples/Occlusion\" target=\"_blank\" rel=\"noopener noreferrer\">demo</a>).</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks3_Architecture.width-1250.png\" alt=\"XRBlocks3_Architecture\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks3_Architecture.width-1250.png\" alt=\"XRBlocks3_Architecture\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kk7t9\"><i>The modular architecture of the XR Blocks’s core engine, which consists of essential subsystems to realize the framework’s high-level abstractions, spanning perception (</i><code><i>depth</i></code><i>,</i> <code><i>input</i></code><i>), AI integration (</i><code><i>ai</i></code><i>,</i> <code><i>agent</i></code><i>), and user experience (</i><code><i>ui</i></code><i>,</i> <code><i>ux</i></code><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xfvek\">By separating the abstract Reality Model from the concrete Core engine, XR Blocks enables a powerful new creative workflow. The goal is to allow creators to move from high level, human-centric ideas to interactive prototypes much more quickly. We envision a future where any declarative prompt, <i>“When the user pinches at an object, an agent should generate a poem of it”</i>, could be directly translated to high-level instructions in XR Blocks:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks4_Instructions.width-1250.png\" alt=\"XRBlocks4_Instructions\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks4_Instructions.width-1250.png\" alt=\"XRBlocks4_Instructions\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"xfvek\">Hence, the creator’s prompt is no longer pseudocode but a direct summary of the implementation logic. We envision this framework to more seamlessly translate such user intent into a system-level execution flow, composing capabilities from the <code>input</code>, <code>sound</code>, <code>ai</code>, <code>world</code>, <code>ui</code>, and <code>agent</code> modules to generate an emergent, intelligent behavior with user interaction.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks5_Interaction.width-1250.png\" alt=\"XRBlocks5_Interaction\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks5_Interaction.width-1250.png\" alt=\"XRBlocks5_Interaction\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kk7t9\"><i>The Interaction Grammar of XR Blocks, which abstracts user input by distinguishing between two types of interaction. Explicit events are direct, low-level inputs (e.g., a touch or click), while implicit intents are higher-level interpretations (e.g., a gesture or voice command), allowing creators to build interaction against user intent.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"fbybx\">Application scenarios</h2><p data-block-key=\"5nimk\">We provide a suite of interactive applications to demonstrate the expressive power and flexibility of the XR Blocks framework. These examples showcase how our framework enables the rapid prototyping of sophisticated experiences that were previously too complex and costly to build, facilitating the creation of realistic, interactive, and intelligent mixed-reality worlds:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks_Applications.width-1250.png\" alt=\"XRBlocks_Applications\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/XRBlocks_Applications.width-1250.png\" alt=\"XRBlocks_Applications\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8ze35\"><i>Applications of XR Blocks. (1) XR Realism: Depth-aware and physics-based ball pit (</i><a href=\"https://xrblocks.github.io/docs/samples/Ballpit\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>) and splash games (</i><a href=\"https://xrblocks.github.io/docs/samples/Splash\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>); geometry-aware shadows (</i><a href=\"https://xrblocks.github.io/docs/samples/Occlusion\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>), 3D Gaussian splatting with occlusion, and lighting estimation. (2) XR Interaction: Immersive emoji (</i><a href=\"https://xrblocks.github.io/docs/samples/XR-Emoji\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>) and rock paper scissors game (</i><a href=\"https://xrblocks.github.io/docs/samples/RockPaperScissors\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>) empowered by custom ML models, dynamic swipe recognition, touch and grab with the physical world. (3) AI + XR: Integration with conversational AI (</i><a href=\"https://xrblocks.github.io/docs/samples/Gemini-Icebreakers\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>), XR objects (</i><a href=\"https://xrblocks.github.io/docs/samples/Gemini-XRObject\" target=\"_blank\" rel=\"noopener noreferrer\"><i>demo</i></a><i>), glasses simulation in XR, and poem generation with a real-world camera.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fbybx\">The true power of the framework is realized when this Reality Model is deeply integrated with generative AI to create dynamic, personalized environments. We demonstrate this by building systems like Augmented Object Intelligence (<a href=\"https://research.google/blog/augmented-object-intelligence-with-xr-objects/\">XR-Objects</a>), which imbues everyday physical objects with interactive digital affordances, such as dynamic virtual buttons. XR Blocks also serves as the foundation for <a href=\"https://research.google/blog/sensible-agent-a-framework-for-unobtrusive-interaction-with-proactive-ar-agents/\">Sensible Agent</a> (published on ACM UIST 2025), a system for proactive and unobtrusive AR assistance. Our architecture provides the agent's core perception and interaction logic, providing an example of our primary goal: by providing robust, high-level tools, XR Blocks empowers Human-Computer Interaction researchers to bypass low-level implementation and focus directly on higher-order challenges like the cognitive principles of human-agent collaboration.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/XRBlocks6_Demos.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8ze35\"><i>Demonstrations of XR Blocks SDK. (1) Using XR Blocks with conversational AI to automatically generate and test user prompts. (2) Running physical collision with depth sensing on Android XR. (3) Running LiteRT on a device with a custom gesture model to trigger XR animation.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"fbybx\">Conclusion and future directions</h2><p data-block-key=\"c4i24\">Creating intelligent XR experiences is currently too fragmented, placing a major barrier between a creator's vision and its realization. We presented <a href=\"https://xrblocks.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">XR Blocks</a>, an architecture and toolkit that dissolves this complexity by providing a high-level abstraction layer that separates <i>what</i> (the intent) from the <i>how</i> (the low-level implementation), dramatically accelerating the prototyping of context-aware applications. This is a foundational step toward a future where the boundaries between programming, design, and conversation disappear, enabling us to script realities as fluidly as we script stories. XR Blocks is far from perfect, and this work serves as <a href=\"https://arxiv.org/abs/2509.25504\" target=\"_blank\" rel=\"noopener noreferrer\">an initial visionary document</a> to invite more creators to join our journey, based on our belief that <i>with the right set of tools, everyone can unleash their inner creativity with AI</i>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"fbybx\">Acknowledgements</h2><p data-block-key=\"ccr67\"><i>This work is a joint collaboration across multiple teams at Google. The following researchers and engineers contributed to this work: David Li and Ruofei Du (equal primary contributions), Nels Numan, Xun Qian, Yanhe Chen, and Zhongyi Zhou, (equal secondary contributions, sorted alphabetically), as well as Evgenii Alekseev, Geonsun Lee, Alex Cooper, Min Xia, Scott Chung, Jeremy Nelson, Xiuxiu Yuan, Jolica Dias, Tim Bettridge, Benjamin Hersh, Michelle Huynh, Konrad Piascik, Ricardo Cabello, and David Kim. We would like to thank Mahdi Tayarani, Max Dzitsiuk, Patrick Hackett, Seeyam Qiu, Brian Collins, Steve Toh, Eric Gonzalez, Nicolás Peña Moreno, Yi-Fei Li, Ziyi Liu, Jing Jin for their feedback and discussion on our early-stage proposal and WebXR experiments. We thank Max Spear, Adarsh Kowdle, and Guru Somadder for the directional contribution and thoughtful reviews.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    }
  ],
  "lastUpdated": "2025-10-30T10:32:03.995Z"
}