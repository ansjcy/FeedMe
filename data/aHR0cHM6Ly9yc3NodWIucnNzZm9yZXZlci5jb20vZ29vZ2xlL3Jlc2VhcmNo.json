{
  "sourceUrl": "https://rsshub.rssforever.com/google/research",
  "title": "Google Research Blog",
  "description": "Google Research Blog - Powered by RSSHub",
  "link": "https://research.google/blog",
  "items": [
    {
      "title": "加速研究突破与实际应用的神奇循环 (原标题: Accelerating the magic cycle of research breakthroughs and real-world applications)",
      "link": "https://research.google/blog/accelerating-the-magic-cycle-of-research-breakthroughs-and-real-world-applications/",
      "pubDate": "Thu, 30 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-30T16:00:00.000Z",
      "creator": "Google",
      "summary": "在最近的Research@活动中，Google Research分享了其在地球理解、基因组学和量子计算等领域的最新进展，并强调了研究突破与实际应用之间“神奇循环”的加速。这个循环通过更强大的模型、加速科学发现的智能工具以及开放平台得以推动。\n\n### 最新研究突破\n\n文章重点介绍了三项关键突破：\n\n*   **Google Earth AI：前所未有的地球理解**\n    *   Earth AI是一系列强大的地理空间AI模型和推理系统，旨在解决全球性挑战，提供对地球事件前所未有的理解。\n    *   多年来，Google开发了洪水、野火、气旋、空气质量、花粉、天气预报、农业、人口动态等先进的地理空间AI模型，已帮助全球数百万人。\n    *   已扩展对新的遥感基础模型和全球人口动态基础模型的访问。\n    *   河流洪水预报模型已覆盖150个国家的20亿人口。\n    *   Earth AI整合并合成大量的真实世界图像、人口和环境数据，利用大型语言模型（LLMs）及其推理能力，使非专业用户也能通过自然语言提问并获取洞察。\n    *   该技术将赋能Google Earth的Gemini功能，允许用户在卫星图像中搜索物体，并已向Google Cloud的受信任测试者开放。\n\n*   **DeepSomatic与Cell2Sentence：迈向癌症精准医疗**\n    *   DeepSomatic是Google在《自然生物技术》上发表的最新AI工具，旨在帮助科学界和医疗从业者。\n    *   该工具建立在Google十年基因组学研究的基础上，通过将基因测序数据转化为图像，并使用卷积神经网络区分参考基因组、非癌种系变异和肿瘤中的癌变体细胞变异。\n    *   识别癌症变异有望带来全新的疗法，并帮助临床医生选择治疗方案（如化疗或免疫疗法）。\n    *   Children's Mercy医院正利用DeepSomatic为患者提供个性化治疗方案。\n    *   Google还与Google DeepMind合作发布了用于单细胞分析的270亿参数基础模型C2S-Scale，为癌症细胞行为提供了新的假设。\n\n*   **量子回声：迈向实际应用的一大步**\n    *   Google对量子计算进行了战略性长期投资，其硬件里程碑是预计2024年末推出的Willow芯片。\n    *   量子AI硬件首席科学家Michel Devoret等人在1980年代的研究为今天的超导量子比特奠定了基础，并因此成为2025年诺贝尔物理学奖得主。\n    *   Google在《自然》杂志封面发表了一项新的可验证量子优势成果——“量子回声”算法，在Willow芯片上运行速度比世界上最快的超级计算机上的最佳经典算法快13,000倍。\n    *   该算法提供了一种新方法来解释核磁共振光谱中观察到的真实世界分子中原子间的相互作用，是世界上第一个展示可验证量子优势的算法，预示着量子计算在药物设计和实现聚变能源方面的实际应用潜力。\n    *   预计五年内将看到量子计算的实际应用。\n\n### 从加速科学发现到算法创新\n\n文章还分享了Google在多个领域的工作，推动突破性研究和加速实际解决方案：\n\n*   **健康与科学**：AI联合科学家（多智能体AI系统）加速科学和生物医学发现；Gemini支持的编码智能体帮助科学家编写专家级经验软件；AMIE（对话式医疗AI智能体）在临床推理和沟通方面与初级保健医生相当，正与Beth Israel Deaconess Medical Center合作进行真实患者测试；MedGemma（Google最强大的多模态医疗理解开放模型）已获得超过100万次下载和4万名独立用户。\n*   **事实性与效率**：持续推进LLMs的事实性和基础研究，包括LLMs如何表达不确定性、编码事实知识的能力等；扩展到多模态内容，如时间对齐字幕和对比序列视频扩散方法，以提高图像和视频模型的质量；通过推测解码和推测级联等技术，提高LLMs的效率和能源创新。\n*   **算法创新**：新的广告模型、大规模优化、Google地图路线增强、印度语音搜索改进；隐私研究包括机密联邦分析、差分隐私合成数据和可证明的AI使用隐私洞察；TimesFM每月处理数亿次查询，并引入了上下文微调的新方法；探索通过LearnLM和Learn Your Way改进学习和教育；利用扩散模型实现实时游戏引擎，模拟沉浸式世界环境。\n\n### AI作为人类智慧的放大器\n\n研究的“神奇循环”正迅速加速，由更强大的模型、如AI联合科学家和基于AI的专家级经验软件等智能工具，以及MedGemma、HAI-DEF和DeepSomatic等开放平台推动。AI不再仅仅是一个工具，而是不可或缺的伙伴和合作者，赋能研究人员、工程师、医护人员和教育工作者。人类智慧与AI强大能力的融合将进一步推动创新，加速其在全球范围内的影响，为全人类开创科学发现的新时代。",
      "shortSummary": "Google Research在Research@活动中展示了其在地球AI、癌症精准医疗（DeepSomatic）和量子计算（量子回声）方面的最新突破。这些进展加速了研究突破与实际应用之间的“神奇循环”，通过强大的模型、智能工具和开放平台推动创新。Google Earth AI提供前所未有的地球理解；DeepSomatic助力癌症基因变异识别；量子回声算法展示可验证量子优势，预示五年内实际应用。AI正成为人类智慧的放大器，赋能各领域，加速全球科学发现和创新。",
      "translated_title": "加速研究突破与实际应用的神奇循环",
      "images": [],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"k9jkp\">Last week at our flagship Research@ event in Mountain View, we shared some of Google Research’s latest announcements, from understanding earth to advancements in genomics to advancements in quantum computing. Working collaboratively with colleagues across the company, our teams drive breakthrough research and accelerate real-world solutions for products, businesses, science and society. As research comes to reality, we uncover new research opportunities, driving innovation further and faster. I call this powerful, cyclical relationship between research and real-world impact <a href=\"https://blog.google/technology/research/what-is-google-research/\" target=\"_blank\" rel=\"noopener noreferrer\">the magic cycle of research</a>.</p><p data-block-key=\"dg3fp\">This cycle is accelerating significantly these days, propelled by more powerful models, new agentic tools that help accelerate scientific discovery, and open platforms and tools. We see this momentum <a href=\"https://research.google/blog/google-research-2024-breakthroughs-for-impact-at-every-scale/\">across domains</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"3ZRpjMniPus\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=3ZRpjMniPus\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"jb2tj\">Our latest research breakthroughs</h2><p data-block-key=\"b64n9\">At Research@MTV last week, we <a href=\"https://blog.google/technology/research/google-research-team-tackles-big-challenges-with-science/\" target=\"_blank\" rel=\"noopener noreferrer\">highlighted</a> three of our latest breakthroughs: Google Earth AI, DeepSomatic, and Quantum Echoes.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"jb2tj\">Google Earth AI: Unprecedented planetary understanding</h3><p data-block-key=\"aci11\"><a href=\"https://ai.google/earth-ai\" target=\"_blank\" rel=\"noopener noreferrer\">Earth AI</a> is a powerful collection of geospatial AI models and reasoning designed to address critical global challenges; it gives users an unprecedented level of understanding about what is happening across the planet.</p><p data-block-key=\"bf8e2\">For years we’ve been developing state-of-the-art geo-spatial AI models including <a href=\"https://blog.google/technology/ai/google-ai-global-flood-forecasting/\" target=\"_blank\" rel=\"noopener noreferrer\">floods</a>, <a href=\"https://blog.google/outreach-initiatives/sustainability/google-wildfire-boundary-maps-europe-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">wildfires</a>, <a href=\"https://blog.google/technology/google-deepmind/weather-lab-ai-cyclone-prediction-tracking/\" target=\"_blank\" rel=\"noopener noreferrer\">cyclones</a>, <a href=\"https://blog.google/products/maps/google-maps-apis-environment-sustainability/\" target=\"_blank\" rel=\"noopener noreferrer\">air quality</a>, <a href=\"https://blog.google/products/maps/google-maps-apis-environment-sustainability/\" target=\"_blank\" rel=\"noopener noreferrer\">pollen</a>, weather <a href=\"https://blog.google/intl/en-africa/products/explore-get-answers/nowcasting-on-search-is-bringing-ai-powered-weather-forecasts-to-users-across-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">nowcasting</a> and <a href=\"https://deepmind.google/science/weathernext/\" target=\"_blank\" rel=\"noopener noreferrer\">long range forecasting</a>, <a href=\"https://blog.google/intl/en-in/company-news/new-milestones-in-our-journey-to-build-inclusive-and-helpful-ai-for-india/\" target=\"_blank\" rel=\"noopener noreferrer\">agriculture</a>, <a href=\"https://research.google/blog/insights-into-population-dynamics-a-foundation-model-for-geospatial-inference/\">population dynamics</a>, <a href=\"https://deepmind.google/discover/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/\" target=\"_blank\" rel=\"noopener noreferrer\">AlphaEarth Foundations</a> and <a href=\"https://research.google/blog/introducing-mobility-ai-advancing-urban-transportation/\">mobility</a>. These models, developed by teams across Google, are already helping millions of people worldwide and we keep making progress. We have just expanded access to our new Remote Sensing Foundations and new global Population Dynamics Foundations. And we can now share that our riverine flood models — expanded over the years to <a href=\"https://blog.google/technology/ai/expanding-flood-forecasting-coverage-helping-partners/\" target=\"_blank\" rel=\"noopener noreferrer\">cover 700 million</a> people in 100 countries — now provide forecasts covering over 2B people in 150 countries for significant riverine flood events.</p><p data-block-key=\"5637f\"><a href=\"https://blog.google/technology/research/new-updates-and-more-access-to-google-earth-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Earth AI</a> is a Google-wide program building on our long-standing efforts. Our latest research updates to Earth AI integrate and synthesize these vast amounts of real-world imagery, population and environmental data. Using LLMs and their reasoning capabilities, the Earth AI geospatial reasoning agent can understand nuanced concepts and discover correlations across multiple datasets and models. This agent allows users to ask complex questions and receive answers in plain language, making Earth AI capabilities accessible even to non-experts. Users can quickly generate insights from business logic use cases and supply chain management to crisis resilience and international policy.</p><p data-block-key=\"5gu9e\">In our evaluations, Geospatial Reasoning Agent improved responses over baseline models that did not have access to Earth AI models and tools. We share the results in our <a href=\"https://research.google/blog/google-earth-ai-unlocking-geospatial-insights-with-foundation-models-and-cross-modal-reasoning/\">research blog</a> and our <a href=\"https://arxiv.org/abs/2510.18318\" target=\"_blank\" rel=\"noopener noreferrer\">technical report</a>.</p><p data-block-key=\"93hn3\">Google Earth with <a href=\"https://medium.com/google-earth/a-whole-new-way-to-work-onearth-introducing-google-earths-new-professional-plans-f80c20b944c9\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini capabilities</a> will soon be powered by our Earth AI imagery models, enabling users to search for objects in satellite imagery. Plus, our powerful models are now available to <a href=\"https://forms.gle/VmdqBHrMah6g9z948\" target=\"_blank\" rel=\"noopener noreferrer\">trusted testers</a> on Google Cloud. And we continue to hear from our partners about diverse important use cases, including testimonials from <a href=\"https://www.youtube.com/watch?v=8-macH8ozr4&amp;list=PL95lT3XlM14ROFtYnlBDYZbKipH3JLaC7&amp;index=2\" target=\"_blank\" rel=\"noopener noreferrer\">Give Directly</a>, <a href=\"https://www.youtube.com/watch?v=8-macH8ozr4\" target=\"_blank\" rel=\"noopener noreferrer\">McGill and Partners</a>, <a href=\"https://medium.com/cooper-smith/unlocking-next-generation-health-predictions-with-googles-geospatial-foundational-models-a-0139832cf23f\" target=\"_blank\" rel=\"noopener noreferrer\">Cooper/Smith</a>, <a href=\"https://www.youtube.com/watch?v=ZxmB8Z5i1Ls&amp;list=PL95lT3XlM14ROFtYnlBDYZbKipH3JLaC7&amp;index=3\" target=\"_blank\" rel=\"noopener noreferrer\">WPP</a>, <a href=\"https://www.youtube.com/watch?v=_4GVYutXE_I\" target=\"_blank\" rel=\"noopener noreferrer\">WHO AFRO</a>, <a href=\"https://www.youtube.com/watch?v=FviGaVEByS4&amp;list=PL95lT3XlM14ROFtYnlBDYZbKipH3JLaC7&amp;index=4\" target=\"_blank\" rel=\"noopener noreferrer\">Planet Labs and Airbus</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"n625h\">DeepSomatic &amp; Cell2Sentence: Toward precision medicine to fight cancer</h3><p data-block-key=\"3af5e\"><a href=\"https://research.google/blog/using-ai-to-identify-genetic-variants-in-tumors-with-deepsomatic/\">DeepSomatic</a>, published in <a href=\"https://www.nature.com/articles/s41587-025-02839-x\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature Biotechnology</i></a><i>,</i> is our newest of many AI tools designed to help the scientific community and health practitioners.</p><p data-block-key=\"3lkui\">DeepSomatic builds on <a href=\"https://blog.google/technology/research/ten-years-google-genomics/\" target=\"_blank\" rel=\"noopener noreferrer\">10 years of genomics</a> research at Google. Since 2015, we’ve been building models like <a href=\"https://research.google/pubs/deepconsensus-improves-the-accuracy-of-sequences-with-a-gap-aware-sequence-transformer/\">DeepConsensus</a> and <a href=\"https://research.google/blog/learning-deepvariants-hidden-powers/\">DeepVariant</a> to help us better understand the genome. With these models, we’ve helped map <a href=\"https://www.nature.com/articles/s41587-019-0217-9\" target=\"_blank\" rel=\"noopener noreferrer\">human</a> and <a href=\"https://www.irri.org/news-and-events/news/google-blog-analyzing-3k-rice-genomes-characterized-deepvariant\" target=\"_blank\" rel=\"noopener noreferrer\">non-human</a> genomes and used this information to inform our understanding of disease.</p><p data-block-key=\"c4hi1\">Some cancers have complex genetic signatures that may make them targets for tailored treatments based on their specific mutations. So, we asked ourselves if we could sequence the genomes of these cancerous cells more precisely. The result, DeepSomatic, is our new <a href=\"https://github.com/google/deepsomatic\" target=\"_blank\" rel=\"noopener noreferrer\">open-source</a> AI-powered tool to help scientists and doctors make sense of genetic variants in cancer cells.</p><p data-block-key=\"d4v3a\">The model works by first turning genetic sequencing data into a <a href=\"https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/\" target=\"_blank\" rel=\"noopener noreferrer\">set of images</a> and then using a <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\" target=\"_blank\" rel=\"noopener noreferrer\">convolutional neural network</a> to differentiate between the reference genome, the non-cancer germline variants in that individual, and the cancer-caused somatic variants in the tumor.</p><p data-block-key=\"bc4f7\">Identifying cancer variants could potentially lead to brand-new therapies, and it could help clinicians decide between treatments such as chemotherapy and immunotherapy. Our partners at <a href=\"https://www.childrensmercy.org/childrens-mercy-research-institute/about/\" target=\"_blank\" rel=\"noopener noreferrer\">Children’s Mercy</a> are using it to pinpoint <a href=\"https://www.medrxiv.org/content/10.1101/2024.11.05.24316078v1\" target=\"_blank\" rel=\"noopener noreferrer\">how and why a particular form of cancer</a> affects a patient in order to create personalized cures.</p><p data-block-key=\"2m7e2\">DeepSomatic follows other breakthroughs which share the same goal of using AI to help fight cancer. We also just released a 27 billion parameter foundation model for single-cell analysis, <a href=\"https://www.biorxiv.org/content/10.1101/2025.04.14.648850v3\" target=\"_blank\" rel=\"noopener noreferrer\">C2S-Scale</a>, in collaboration with Google DeepMind. This builds upon <a href=\"https://research.google/blog/teaching-machines-the-language-of-biology-scaling-large-language-models-for-next-generation-single-cell-analysis/\">our work from earlier this year</a>, in collaboration with Yale, and recently generated a <a href=\"https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/\" target=\"_blank\" rel=\"noopener noreferrer\">novel hypothesis</a> about cancer cellular behavior. With more clinical tests, this may reveal a promising new pathway for developing therapies to fight cancer.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"n625h\">Quantum Echoes: A big step toward real-world applications</h3><p data-block-key=\"50jv4\">To accelerate the next exponential wave of scientific discovery, we’re looking to our strategic, long-term investment in quantum computing.</p><p data-block-key=\"7d8h5\">Our foundation rests on decades of research, leading to our hardware milestone on the <a href=\"https://research.google/blog/making-quantum-error-correction-work/\">Willow chip</a> in late 2024. This work is supported by Michel Devoret, our Chief Scientist of Quantum Hardware, who together with with former Quantum AI hardware lead John Martinis, and John Clarke of the University of California, Berkeley, became <a href=\"https://blog.google/inside-google/company-announcements/googler-michel-devoret-awarded-the-nobel-prize-in-physics/\" target=\"_blank\" rel=\"noopener noreferrer\">2025 Physics Nobel Laureates</a> for their research in the 1980s that laid the groundwork for today's superconducting qubits.</p><p data-block-key=\"88g91\">Now we’ve announced a new <a href=\"https://research.google/blog/a-verifiable-quantum-advantage/\">verifiable quantum advantage</a>, published in the cover of <a href=\"https://www.nature.com/articles/s41586-025-09526-6\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature</i></a>. Our “<a href=\"https://blog.google/technology/research/quantum-echoes-willow-verifiable-quantum-advantage/\" target=\"_blank\" rel=\"noopener noreferrer\">Quantum Echoes</a>” algorithm runs on our Willow chip 13,000 times faster than the best classical algorithm on one of the world’s fastest supercomputers. It offers a new way to explain interactions between atoms in a real world molecule observed <a href=\"https://arxiv.org/abs/2510.19550\" target=\"_blank\" rel=\"noopener noreferrer\">using nuclear magnetic resonance spectroscopy</a>. This is the world’s first algorithm to demonstrate verifiable quantum advantage and points towards practical applications of quantum computing that are beyond the capabilities of classical computers.</p><p data-block-key=\"9jol8\">Quantum computing has the potential to meaningfully advance drug design and help make fusion energy a reality. And given our latest breakthrough, we’re optimistic that we’ll start to see real-world applications within five years.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"ltTMP_ACkIA\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=ltTMP_ACkIA\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"bs40u\">Fireside Chat about Quantum AI with James Manyika and Hartmut Neven.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"9mtq0\">From accelerating scientific discovery to algorithmic innovation</h2><p data-block-key=\"dpmhs\">We also shared some of the work across various domains where teams are driving breakthrough research and accelerating real-world solutions. The breadth and depth of the opportunities is ever increasing. Here are a few recent examples.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"9mtq0\">Health &amp; Science</h3><p data-block-key=\"dok25\"><a href=\"https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/\">AI co-scientist</a> is a multi-agent AI system built as a virtual scientific collaborator to help scientists generate novel hypotheses and research proposals, and to accelerate scientific and biomedical discoveries. Our new <a href=\"https://research.google/blog/accelerating-scientific-discovery-with-ai-powered-empirical-software/\">AI-powered empirical software</a> system, a Gemini-backed coding agent, helps scientists write expert-level empirical software. It accelerates the historically slow task of creating custom software to evaluate and iteratively improve scientific hypotheses. This opens the door to a future where scientists can easily, rapidly, and systematically investigate hundreds or thousands of potential solutions to the problems that motivate their research.</p><p data-block-key=\"e77td\"><a href=\"https://research.google/blog/amie-a-research-ai-system-for-diagnostic-medical-reasoning-and-conversations/\">AMIE</a>, a conversational medical AI agent, demonstrates clinical reasoning and communication on par with primary care physicians in both <a href=\"https://research.google/blog/amie-gains-vision-a-research-ai-agent-for-multi-modal-diagnostic-dialogue/\">multimodal</a> and <a href=\"https://research.google/blog/from-diagnosis-to-treatment-advancing-amie-for-longitudinal-disease-management/\">multi-visit settings</a>. As we explore how AMIE may translate to real-world environments, we are testing it under <a href=\"https://research.google/blog/enabling-physician-centered-oversight-for-amie/\">physician oversight</a>, including in a <a href=\"https://research.google/blog/advancing-amie-towards-specialist-care-and-real-world-validation/\">partnership</a> with Beth Israel Deaconess Medical Center to evaluate AMIE with real-world patients.</p><p data-block-key=\"6vr16\"><a href=\"https://research.google/blog/medgemma-our-most-capable-open-models-for-health-ai-development/\">MedGemma</a>, part of our Health AI Developer Foundations (<a href=\"https://developers.google.com/health-ai-developer-foundations\" target=\"_blank\" rel=\"noopener noreferrer\">HAI-DEF</a>) collection, is Google's most capable open model for multimodal medical comprehension. Since launch MedGemma and HAI-DEF have &gt;1M downloads and &gt;40K unique users.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"9mtq0\">Factuality &amp; Efficiency</h3><p data-block-key=\"110h0\">We continue advancing our research on factuality and grounding for LLMs, including studying how LLMs <a href=\"https://arxiv.org/abs/2505.24858\" target=\"_blank\" rel=\"noopener noreferrer\">convey uncertainty</a>, assessing whether LLMs <a href=\"https://arxiv.org/abs/2503.15299\" target=\"_blank\" rel=\"noopener noreferrer\">encode more factual knowledge</a> in their parameters than they express in their outputs, and more. We expand to multimodal content - for example, <a href=\"https://arxiv.org/abs/2405.04682\" target=\"_blank\" rel=\"noopener noreferrer\">Time-Aligned Captions</a> and our <a href=\"https://arxiv.org/abs/2407.11814\" target=\"_blank\" rel=\"noopener noreferrer\">contrastive sequential video diffusion</a> method focus on making scenes in videos visually consistent, helping improve the quality of our image and video models.</p><p data-block-key=\"ac56q\">Improving the efficiency of LLMs remains a high priority goal across the industry. Building on our <a href=\"https://research.google/blog/looking-back-at-speculative-decoding/\">speculative decoding</a> work which enabled substantial efficiency gains without any compromise on quality, we keep seeing many new <a href=\"https://scholar.google.co.il/scholar?oi=bibs&amp;hl=en&amp;cites=13896317670253353040&amp;as_sdt=5\" target=\"_blank\" rel=\"noopener noreferrer\">approaches</a>, such as our recent <a href=\"https://research.google/blog/speculative-cascades-a-hybrid-approach-for-smarter-faster-llm-inference/\">speculative cascades</a>. We keep advancing other techniques for efficiency and for <a href=\"https://arxiv.org/abs/2508.15734\" target=\"_blank\" rel=\"noopener noreferrer\">energy innovation</a> techniques.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"9mtq0\">Algorithmic innovation</h3><p data-block-key=\"hu69\">Algorithmic research contributes to new Ads model connecting advertisers to customers, continued research on our large-scale optimisations, enhancements to Google Maps <a href=\"https://research.google/blog/how-we-created-hov-specific-etas-in-google-maps/\">routing</a> and <a href=\"https://research.google/blog/speech-to-retrieval-s2r-a-new-approach-to-voice-search/\">improved voice search</a> in India. Privacy research includes recent advances such as <a href=\"https://research.google/blog/discovering-new-words-with-confidential-federated-analytics/\">confidential federated analytics</a>, <a href=\"https://research.google/blog/generating-synthetic-data-with-differentially-private-llm-inference/\">differentially private synthetic data</a> and <a href=\"https://research.google/blog/toward-provably-private-insights-into-ai-use/\">provably private insights into AI use</a>. We are making progress on <a href=\"https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/\">TimesFM</a> which has hundreds of millions of queries per month in BigQuery alone, and recently introduced a novel approach using <a href=\"https://research.google/blog/time-series-foundation-models-can-be-few-shot-learners/\">in-context fine-tuning</a>.</p><p data-block-key=\"c0npb\">We keep exploring new ways to improve learning and education, building on our earlier work on <a href=\"https://cloud.google.com/solutions/learnlm?hl=en&amp;e=13802955\" target=\"_blank\" rel=\"noopener noreferrer\">LearnLM</a>, such as <a href=\"https://research.google/blog/learn-your-way-reimagining-textbooks-with-generative-ai/\">Learn Your Way</a> to improve learning efficacy. And we keep exploring AI innovations such as the use of diffusion models for <a href=\"https://arxiv.org/abs/2408.14837\" target=\"_blank\" rel=\"noopener noreferrer\">real-time game engines</a>, which inspire new horizons for simulating immersive world environments.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"rnq9kQCclvo\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=rnq9kQCclvo\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"gxown\"><i>At Research@ Mountain View, Yossi Matias joins</i> <i>Alex Kantrowitz on the Big Technology Podcast to discuss our research efforts in areas like cancer treatment and Quantum.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"9mtq0\">AI as an amplifier of human ingenuity</h2><p data-block-key=\"90fa7\">The magic cycle of research is quickly gaining momentum. This is propelled by more powerful models, by agentic tools like the <a href=\"https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/\">AI co-scientist</a> and AI-based <a href=\"https://research.google/blog/accelerating-scientific-discovery-with-ai-powered-empirical-software/\">expert-level empirical software</a> that help accelerate scientific discovery, and open platforms and tools like <a href=\"https://research.google/blog/medgemma-our-most-capable-open-models-for-health-ai-development/\">MedGemma</a>, <a href=\"https://developers.google.com/health-ai-developer-foundations\" target=\"_blank\" rel=\"noopener noreferrer\">HAI-DEF</a> and <a href=\"https://research.google/blog/using-ai-to-identify-genetic-variants-in-tumors-with-deepsomatic/\">DeepSomatic</a>. Innovation today is happening at unprecedented speed.</p><p data-block-key=\"1mnrg\">The latest advancements point to a world where AI is not just a tool, but an essential partner and collaborator. This partnership is already taking shape in tangible ways, empowering researchers, engineers, healthcare workers, and educators. With humans at the steering wheel, we can leverage AI to bring new ideas to life and take on the challenges that matter most.</p><p data-block-key=\"34jir\">This fusion of human ingenuity with the powerful capabilities of AI will fuel further innovation and accelerate its impact for people at a global scale, defining a new era of scientific discovery for the benefit of everyone, everywhere.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "迈向可验证的AI使用隐私洞察 (原标题: Toward provably private insights into AI use)",
      "link": "https://research.google/blog/toward-provably-private-insights-into-ai-use/",
      "pubDate": "Wed, 29 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-29T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 迈向可验证的AI使用隐私洞察\n\n## 引言：生成式AI与隐私挑战\n生成式AI（GenAI）赋能个性化体验，并生成包括摘要、转录等在内的非结构化数据。开发者需要了解AI的实际使用情况，以通过识别常见应用和故障模式来增强工具。尤其当这些工具应用于设备端数据时，在洞察生成过程中提供日益强大的隐私保障至关重要。\n\n## 可验证的隐私洞察 (PPI) 简介\n本文介绍了“可验证的隐私洞察（Provably Private Insights, PPI）”，这是一个新的目标，旨在生成关于人们如何使用大型语言模型（LLMs）和GenAI工具的动态洞察，同时保证个人数据不可检查且聚合洞察是匿名的。\n\nGoogle宣布推出首个此类PPI系统，该系统结合了LLM、差分隐私（Differential Privacy, DP）和可信执行环境（Trusted Execution Environments, TEEs）来分析非结构化GenAI数据。该系统证明服务器端处理仅限于隐私保护计算，并且可以完全进行外部检查。\n\n## PPI系统的工作原理\n1.  **“数据专家”LLM**：GenAI工具开发者可以使用一个“数据专家”LLM来分析用户交互，回答诸如“正在讨论什么主题？”或“用户是否感到沮丧？”等问题。\n2.  **差分隐私 (DP)**：LLM的答案通过DP进行聚合，从而提供用户群体中GenAI功能使用情况的全面视图，而不会暴露未聚合的原始数据。\n3.  **可信执行环境 (TEE)**： “数据专家”LLM本身驻留在TEE中，确保其安全运行。\n\n### 赋能技术：机密联邦分析 (CFA)\nPPI由机密联邦分析（Confidential Federated Analytics, CFA）技术支持，该技术最初部署在Gboard中。CFA允许开源分析软件在TEE中运行，为服务器端数据处理的机制和隐私属性提供完全透明性。Google的CFA利用机密计算在处理过程中保护未聚合的用户数据，并且只发布具有正式（用户级）DP保证的输出。CFA提供强大的数据隔离和匿名化保证，无论分析师运行何种查询。\n\n### 数据处理流程与隐私保障\n*   用户设备首先决定哪些数据应上传进行分析。设备加密并上传这些数据，同时上传服务器被授权用于解密的处理步骤。\n*   上传的数据由TEE托管的密钥管理服务管理的加密密钥保护，该服务仅向设备批准的处理步骤发布解密密钥。\n*   设备可以验证密钥管理服务是预期的开源代码（包含在公共、防篡改的透明度日志Rekor中），并且代码正在正确配置的、Google无法访问的TEE中运行。\n*   密钥管理服务反过来验证批准的公共处理步骤正在TEE中运行。数据上不能执行其他分析，任何人都无法访问来自单个设备的数据。\n\n![PPI系统概览](https://storage.googleapis.com/gweb-research2023-media/images/ProvablyPrivateInsights_Overview.width-1250.png)\n\n### 洞察生成步骤\n1.  **结构化摘要**：通过一系列明确定义的处理步骤获取私有洞察。首先，非结构化原始数据由LLM分析，旨在提取特定问题的答案，例如输入的类别或主题。\n2.  **差分隐私聚合**：处理过程首先使用开源的Gemma 3模型将转录内容分类到感兴趣的类别中。然后对这些类别进行求和，以计算带有差分隐私噪声的直方图，确保输出直方图不会受到任何一个用户的强烈影响。\n3.  **LLM提示的灵活性**：LLM的提示可以频繁更改，因为DP保证适用于聚合算法，无论LLM提示如何。即使开发者提出的问题旨在识别单个用户，差分隐私统计数据也不会泄露。\n\n## 端到端可验证性\n系统所有与隐私相关的部分（从私有聚合算法到TEE堆栈，以及LLM本身）都是开源且可复现构建的。用于分析数据的工作流签名也是公开的。结合TEE证明运行软件系统状态的能力，数据处理管道的每个部分都可以与已发布的代码进行可验证的关联。这使得外部方能够验证Google的隐私声明，确保数据仅按声称的代码处理。\n\n## PPI在Pixel Recorder应用中的应用\nGoogle Pixel上的Recorder应用提供强大的AI功能，如转录、摘要和说话人识别。开发者面临的关键挑战是理解用户如何与这些功能互动。例如，用户是在创建“备忘录”、“提醒”还是录制“商务会议”？\n\n在Recorder应用中，一部分转录内容（来自在设置中启用“为所有人改进”的用户）使用公共密钥进行加密，该密钥由中央TEE托管的密钥库管理，并通过Google的Project Oak证明堆栈在AMD安全加密虚拟化-安全嵌套分页（SEV-SNP）CPU上运行。密钥库确保上传的数据只能由预先批准的处理步骤解密，这些步骤本身也经过证明在TEE中运行预期的处理步骤。一个在AMD SEV-SNP TEE中运行的Gemma 3 4B模型将转录内容分类为主题，然后通过差分隐私进行聚合。外部方可以验证原始转录内容从未离开TEE的安全环境，并且只有摘要输出类别的私有总和被发布给Google。\n\n![Recorder转录内容主题的差分隐私分布示例](https://storage.googleapis.com/gweb-research2023-media/images/ProvablyPrivateInsights2_Example.width-1250.png)\n*   图片：Gemma分类的Recorder转录内容在各种主题上的差分隐私分布示例。内部矩形大小与相对主题频率成比例。\n\n### 性能评估\nPPI还可以帮助评估设备端GenAI功能的性能，例如Recorder生成的摘要的准确性。CFA可以在结构化摘要组件中运行LLM自动评估器，该评估器也驻留在TEE中，可以评估设备端模型的结果，从而确保更准确和隐私保护的评估。这允许开发者根据真实用户交互微调设备端模型，而不会损害个人隐私。\n\n## 未来展望\n这项工作表明，可验证的隐私洞察是可行的：利用LLM分析真实世界的GenAI工具使用情况，然后聚合为差分隐私统计数据，所有这些都伴随着服务器端处理步骤的完全透明性。洞察生成过程的每一步都旨在提供最先进的数据隔离和匿名化，外部验证者可以检查方法的源代码以及Google运行这些方法的证明。\n\n未来将推出更多应用，包括差分隐私聚类和合成数据生成，所有这些都将具有相同的可验证性和机密性。通过未来支持更高吞吐量加速器（如Google TPU）的机密使用，将实现更丰富的分析，包括详细的转录分析和自动评估。现在，在不将敏感用户数据暴露在机密计算边界之外的情况下，可以生成具有强大用户级DP保证的洞察。",
      "shortSummary": "Google推出“可验证的隐私洞察（PPI）”系统，旨在安全分析生成式AI（GenAI）工具的实际使用情况。该系统结合大型语言模型（LLM）进行数据分析、差分隐私（DP）进行匿名聚合，以及可信执行环境（TEE）保障处理过程的机密性。PPI确保个人数据不可检查，聚合洞察匿名化，且服务器端处理可外部验证。例如，Pixel Recorder应用已采用PPI，利用开源Gemma模型在TEE中对用户转录内容进行分类和隐私聚合，从而在保护用户隐私的前提下，帮助开发者改进AI功能。所有隐私相关部分均开源且可验证。",
      "translated_title": "迈向可验证的AI使用隐私洞察",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ProvablyPrivateInsights_Overview.width-1250.png",
          "alt": "ProvablyPrivateInsights_Overview",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ProvablyPrivateInsights2_Example.width-1250.png",
          "alt": "ProvablyPrivateInsights2_Example",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"343fi\">Generative AI (GenAI) enables personalized experiences and powers the creation of unstructured data, including summaries, transcriptions, and more. Insights into real-world AI use [<a href=\"https://arxiv.org/abs/2412.13678\" target=\"_blank\" rel=\"noopener noreferrer\">1</a>, <a href=\"https://www.nber.org/system/files/working_papers/w34255/w34255.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">2</a>] can help GenAI developers enhance their tools by understanding common applications and identifying failure modes. And especially when those tools are applied to on-device data, our goal is to offer increasingly robust privacy guarantees during the insight generation process. This post introduces provably private insights (PPI), a new north star for generating dynamic insights into how people use LLMs and GenAI tools while guaranteeing that individual data is not inspectable and that aggregate insights are anonymous.</p><p data-block-key=\"3aguf\">Today we announce a first-of-its kind PPI system that leverages the power of large language models (LLMs), <a href=\"https://en.wikipedia.org/wiki/Differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">differential privacy</a> (DP), and <a href=\"https://en.wikipedia.org/wiki/Trusted_execution_environment\" target=\"_blank\" rel=\"noopener noreferrer\">trusted execution environments</a> (TEEs) to analyze unstructured GenAI data. This system proves that server-side processing is limited to privacy-preserving computations and can be fully externally inspected. With our system, GenAI tool developers can analyze interactions using a “data expert” LLM, tasked with answering questions like “what topic is being discussed?” or “is the user frustrated?” The LLM’s answers are aggregated with DP to provide a comprehensive view of GenAI feature usage across the user population without exposing unaggregated data. The “data expert” LLM itself resides within the TEE. PPI is enabled by <a href=\"https://research.google/blog/discovering-new-words-with-confidential-federated-analytics/\">confidential federated analytics</a> (CFA), a technique first deployed in <a href=\"https://play.google.com/store/apps/details?id=com.google.android.inputmethod.latin&amp;hl=en_US\" target=\"_blank\" rel=\"noopener noreferrer\">Gboard</a>, where open source analysis software runs in TEEs, offering complete transparency into the mechanisms and privacy properties of server-side data processing. Our deployment of PPI in the <a href=\"https://play.google.com/store/apps/details?id=com.google.android.apps.recorder&amp;hl=en_US\" target=\"_blank\" rel=\"noopener noreferrer\">Recorder</a> application for Pixel leverages Google’s latest open-source <a href=\"https://blog.google/technology/developers/gemma-3/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma models</a> as the “data expert” to offer insights into Recorder usage.</p><p data-block-key=\"9b1vb\">To encourage the external community to verify our claims, we’ve <a href=\"https://github.com/google-parfait/confidential-federated-compute\" target=\"_blank\" rel=\"noopener noreferrer\">open-sourced LLM-powered privacy preserving insights</a> as part of confidential federated analytics in <a href=\"https://github.com/google-parfait\" target=\"_blank\" rel=\"noopener noreferrer\">Google Parfait</a>, along with the rest of our TEE-hosted confidential federated analytics stack.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"343fi\">How provably private insights are possible</h2><p data-block-key=\"93q2\">Google’s CFA leverages <a href=\"https://en.wikipedia.org/wiki/Confidential_computing\" target=\"_blank\" rel=\"noopener noreferrer\">confidential computing</a> to protect unaggregated user data during processing, and only releases outputs with a formal (user-level) DP guarantee. CFA provides strong data isolation and anonymization guarantees regardless of what query an analyst runs.</p><p data-block-key=\"c0n9v\">In this technique, user devices first decide what data should be uploaded for analysis. Devices encrypt and upload this data, along with the processing steps that the server is authorized to use for decryption. Uploaded data is protected with encryption keys managed by a TEE-hosted key management service, which releases decryption keys only to device-approved processing steps. Devices can verify that the key management service is the expected open source code (included in a public, tamper-resistant transparency log, <a href=\"https://docs.sigstore.dev/logging/overview/\" target=\"_blank\" rel=\"noopener noreferrer\">Rekor</a>), and that the code is running in a properly configured TEE that is inaccessible to Google. The key management service in turn verifies that the approved, public processing steps are running in TEEs. No other analyses can be performed on the data and no human can access data from individual devices.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ProvablyPrivateInsights_Overview.width-1250.png\" alt=\"ProvablyPrivateInsights_Overview\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ProvablyPrivateInsights_Overview.width-1250.png\" alt=\"ProvablyPrivateInsights_Overview\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"343fi\">Private insights are derived by passing the data through a well-defined set of processing steps. First, unstructured raw data is analyzed by an LLM tasked with extracting the answer to a specific question, such as the category or topic of the input (“structured summarization”). Processing begins by using an <a href=\"https://www.kaggle.com/models/google/gemma-3\" target=\"_blank\" rel=\"noopener noreferrer\">open-source Gemma 3 model</a> to classify transcripts into categories of interest. These classes are then summed to compute a histogram of topics with <a href=\"https://en.wikipedia.org/wiki/Differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">differentially private</a> noise guaranteeing that the output histogram cannot be strongly influenced by any one user. The LLM’s prompt can be changed frequently, because the DP guarantee applies to the aggregation algorithm regardless of the LLM prompt. Even if the developer asked a question designed to single out one user, the differential private statistics would not reveal it.</p><p data-block-key=\"fnsv4\">All privacy-relevant parts of our system are open source and <a href=\"https://en.wikipedia.org/wiki/Reproducible_builds\" target=\"_blank\" rel=\"noopener noreferrer\">reproducibly buildable</a> — from the <a href=\"https://github.com/google-parfait/tensorflow-federated/tree/main/tensorflow_federated/cc/core/impl/aggregation/core\" target=\"_blank\" rel=\"noopener noreferrer\">private aggregation algorithm</a> to the <a href=\"https://github.com/google-parfait/confidential-federated-compute\" target=\"_blank\" rel=\"noopener noreferrer\">TEE stack</a> — and the LLM itself is also open source. The <a href=\"https://github.com/google-parfait/confidential-federated-compute/tree/main/reference_values/access_policy\" target=\"_blank\" rel=\"noopener noreferrer\">signatures</a> of the workflows used to analyze the data are also public. When combined with TEEs' ability to attest to the state of the system running the software, every part of the data processing pipeline can be verifiably linked to published code. This provides external parties the ability to verify our privacy claims. This commitment to end-to-end verifiability is how the system makes progress toward being provable — we anchor on this capability, allowing third parties to inspect the open-source code and confirm that it is exactly the code we claim to run, thereby proving to clients that this is the only code their data will be processed with, subject to <a href=\"https://www.arxiv.org/abs/2506.15924\" target=\"_blank\" rel=\"noopener noreferrer\">known weaknesses</a> in current-generation TEEs.</p><p data-block-key=\"7gel5\">In short, provably private insights can be generated by an LLM-powered structured summarization workflow in confidential federated analytics. The combination of structured summarization with differentially private histogram generation enables deeper understanding into how the GenAI tools are used in the real world, all while guaranteeing privacy. Technical details of the system can be found in the <a href=\"https://arxiv.org/abs/2510.21684\" target=\"_blank\" rel=\"noopener noreferrer\">whitepaper</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"343fi\">How provably private insights are used in Recorder</h2><p data-block-key=\"3ssdg\"><a href=\"https://play.google.com/store/apps/details?id=com.google.android.apps.recorder&amp;hl=en_US\" target=\"_blank\" rel=\"noopener noreferrer\">Google’s Recorder app</a> on Pixel offers powerful AI features, such as transcription, summarization, and speaker labeling. A key challenge for the application developers is to understand how users interact with these features. For instance, are users creating \"Notes to self,\" \"Reminders,\" or recording \"Business meetings\"? Traditional count-based analytics are insufficient to analyze such data without the help of structured summarization or another form of classification. In a traditional setting, a system would log these transcripts to a central server for classification, and then run (differentially) private count queries on the results. PPI operates in a similar way but without the risk of data being used for any other purpose.</p><p data-block-key=\"bl5mh\">In the Recorder application, a subset of transcripts (from users who have enabled “Improve for everyone” in settings) are encrypted with a <a href=\"https://github.com/google-parfait/confidential-federated-compute/blob/main/reference_values/access_policy/recorder.txtpb\" target=\"_blank\" rel=\"noopener noreferrer\">public key</a> managed by a central TEE-hosted keystore protected via <a href=\"https://github.com/project-oak/oak\" target=\"_blank\" rel=\"noopener noreferrer\">Google’s Project Oak</a> attestation stack running on <a href=\"https://www.amd.com/en/developer/sev.html\" target=\"_blank\" rel=\"noopener noreferrer\">AMD Secure Encrypted Virtualization-Secure Nested Paging</a> (SEV-SNP) CPUs. The keystore ensures that the uploaded data can be decrypted only by pre-approved processing steps, themselves attested to running the expected processing steps in TEEs. A <a href=\"https://www.kaggle.com/models/google/gemma-3\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma 3 4B model</a> running within the AMD SEV-SNP TEE classifies the transcripts into topics, which are then aggregated with differential privacy. External parties can verify that raw transcripts never leave the secure environment of the TEE, and only private sums of the summarized output categories are released to Google.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-vertical-padding --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ProvablyPrivateInsights2_Example.width-1250.png\" alt=\"ProvablyPrivateInsights2_Example\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ProvablyPrivateInsights2_Example.width-1250.png\" alt=\"ProvablyPrivateInsights2_Example\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"skkud\"><i>An example differentially private distribution of Recorder transcripts across various topics, as categorized by Gemma. Inner rectangle size is proportional to relative topic frequency.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"343fi\">PPI can also help evaluate the performance of on-device GenAI features, such as the accuracy of summaries generated by Recorder. Instead of relying solely on synthetic data, which may not accurately represent real-world use, CFA can run an LLM auto-rater as a part of the structured summarization component. This auto-rater LLM also resides within the TEE and can assess the results of the on-device model, ensuring a more accurate and privacy-preserving evaluation. This allows developers to fine-tune the on-device model based on real user interactions without compromising individual privacy.</p><p data-block-key=\"fkaue\">The configuration we’re running in Recorder is available in <a href=\"https://github.com/google-parfait/confidential-federated-compute/tree/main/reference_values/access_policy\" target=\"_blank\" rel=\"noopener noreferrer\">our GitHub repository</a> which can be connected to the specific code paths and privacy guarantees by following <a href=\"https://github.com/google-parfait/confidential-federated-compute/tree/main/docs\" target=\"_blank\" rel=\"noopener noreferrer\">these instructions</a>. The Recorder configuration guarantees that whatever LLM query is run, it is passed through the auto-tuned DP histogram aggregator with <a href=\"https://dl.acm.org/doi/10.1561/0400000042\" target=\"_blank\" rel=\"noopener noreferrer\">strict privacy guarantees</a> (user-level ε = 1 used in the figure above).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"343fi\">What’s next?</h2><p data-block-key=\"ac91u\">This work demonstrates that provably private insights are possible: real-world GenAI tool use is analyzed with LLMs and then aggregated into differentially private statistics, all with full transparency into the server-side processing steps. Every step of the insight generation process has been designed to offer state-of-the-art data isolation and anonymization, and external verifiers can check the source code of the methods and the proof that we run them.</p><p data-block-key=\"3ms7\">Moreover, we’ve shared LLM-powered structured summarization as a first application. We expect others, including differentially private <a href=\"https://arxiv.org/abs/2506.04681\" target=\"_blank\" rel=\"noopener noreferrer\">clustering</a> and <a href=\"https://arxiv.org/pdf/2510.18232\" target=\"_blank\" rel=\"noopener noreferrer\">synthetic data generation</a> to follow, all with the same level of verifiability and confidentiality. And with future work to enable confidential use of higher-throughput accelerators such as <a href=\"https://cloud.google.com/tpu\" target=\"_blank\" rel=\"noopener noreferrer\">Google TPUs</a>, richer analyses will become possible, including detailed transcript analysis and auto-rating. Insight generation is now possible without exposing sensitive user data outside of the confidential computation boundary, and with strong user-level DP guarantees for generated insights. We are excited that the technology for provably private insights is maturing just as GenAI tools are beginning to apply to on-device and sensitive-data experiences.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"343fi\">Acknowledgements</h2><p data-block-key=\"cjf7c\"><i>We thank the teams at Google that helped with algorithm design, infrastructure implementation, and production maintenance of this system, in particular teams led by Marco Gruteser, Peter Kairouz, and Timon Van Overveldt, with product manager Prem Eruvbetine, including: Albert Cheu, Brett McLarnon, Chunxiang (Jake) Zheng, Edo Roth, Emily Glanz, Grace Ni, James Bell-Clark, Kassem Fawaz, Katharine Daly, Krzysztof Ostrowski, Maya Spivak, Mira Holford, Nova Fallen, Rakshita Tandon, Ren Yi, Stanislav Chiknavaryan, Stefan Dierauf, Steve He, and Zoe Gong. We also thank close partners who supported this system through technologies and the Recorder integration, including: Allen Su, Austin Hsu, Console Chen, Daniel Minare Ho, Dennis Cheng, Jan Wassenberg, Kristi Bradford, Ling Li, Mina Askari, Miranda Huang, Tam Le, Yao-Nan Chen, and Zhimin Yao. This work was supported by Corinna Cortes, Jay Yagnik, Ramanan Rajeswaran, Seang Chau, and Yossi Matias. We additionally thank Peter Kairouz, Marco Gruteser, Mark Simborg, and Kimberly Schwede for feedback and contributions to the writing of this post.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "StreetReaderAI：通过情境感知多模态AI使街景无障碍化 (原标题: StreetReaderAI: Towards making street view accessible via context-aware multimodal AI)",
      "link": "https://research.google/blog/streetreaderai-towards-making-street-view-accessible-via-context-aware-multimodal-ai/",
      "pubDate": "Tue, 28 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-28T16:00:00.000Z",
      "creator": "Google",
      "summary": "# StreetReaderAI：通过情境感知多模态AI使街景无障碍化\n\n## 1. 背景与问题\n*   当前主流地图服务的街景工具虽然革新了虚拟导航和探索方式，但对盲人及低视力用户而言，屏幕阅读器无法解读街景图像，也缺乏替代文本，导致其无法使用。\n*   多模态AI和图像理解技术为解决这一问题提供了机会，有望使拥有超过2200亿张图像、覆盖110多个国家和地区的Google街景对视障社区更具包容性，提供沉浸式视觉体验并开辟新的探索可能性。\n\n## 2. StreetReaderAI 介绍\n*   在UIST’25上发布的论文“StreetReaderAI：通过情境感知多模态AI使街景无障碍化”中，研究人员推出了StreetReaderAI。\n*   这是一个概念验证（proof-of-concept）的无障碍街景原型，利用情境感知、实时AI和无障碍导航控制。\n*   该项目由盲人和视力正常的可访问性研究人员团队迭代设计，借鉴了无障碍第一人称游戏和导航工具（如Shades of Doom、BlindSquare、SoundScape）的经验。\n\n## 3. 核心功能\n*   **实时AI生成描述：** 提供附近道路、交叉口和地点的实时AI生成描述。\n*   **动态对话：** 与多模态AI代理就场景和当地地理进行动态对话。\n*   **无障碍平移和移动：** 通过语音命令或键盘快捷键在全景图像之间进行无障碍平移和移动。\n\n## 4. 技术实现\n*   StreetReaderAI通过将地理信息源和用户当前视野输入到Gemini中，提供情境感知的街景场景描述。\n*   利用Gemini Live实现关于场景和当地地理特征的实时互动对话。\n\n## 5. 导航体验\n*   StreetReaderAI提供沉浸式的第一人称探索体验，类似于以音频为主要界面的视频游戏。\n*   支持键盘和语音无缝交互：\n    *   **视角调整：** 用户可以使用左右箭头键调整视角。AI会提供方向（如“当前朝向：北”或“东北”）的音频反馈，并指示是否可以向前移动以及是否正对着附近的地标或地点。\n    *   **虚拟移动：** 用户可以使用上/下箭头键进行“虚拟步行”。AI会描述移动距离和附近的地理信息。\n    *   **快速移动：** 用户还可以使用“跳跃”或“传送”功能快速移动到新位置。\n\n## 6. AI子系统：虚拟向导\nStreetReaderAI的核心是两个由Gemini支持的AI子系统：AI Describer（AI描述器）和AI Chat（AI聊天）。两者都接收静态提示、可选的用户配置文件以及关于用户当前位置的动态信息（如附近地点、道路信息和当前视野图像）。\n\n### 6.1 AI Describer (AI描述器)\n*   作为情境感知场景描述工具，它结合动态地理信息和当前街景图像分析，生成实时音频描述。\n*   **两种模式：**\n    *   “默认”模式：强调盲人行人的导航和安全。\n    *   “导游”模式：提供额外的旅游信息（如历史和建筑背景）。\n*   利用Gemini预测盲人或低视力旅行者可能感兴趣的、与当前场景和当地地理相关的后续问题。\n*   **AI描述器如何结合多模态数据支持情境感知场景描述的图示：**\n    ![AI Describer Diagram](https://storage.googleapis.com/gweb-research2023-media/images/StreetReaderAI-3.width-1250.png)\n\n### 6.2 AI Chat (AI聊天)\n*   在AI Describer的基础上，允许用户询问关于当前视图、过去视图和附近地理的问题。\n*   使用Google的多模态Live API，支持实时交互、功能调用，并临时保留单个会话中的所有交互记忆。\n*   系统会跟踪并发送每次平移或移动交互以及用户的当前视图和地理上下文。\n*   **强大的“记忆”功能：** 会话上下文窗口最大可达1,048,576个输入令牌（约相当于4000多张输入图像），使其能够记住用户的位置和上下文。例如，用户可以走过一个公交车站，转弯后问“等等，那个公交车站在哪里？”，AI代理能回忆起之前的上下文并回答。\n\n## 7. 用户测试与结果\n*   研究人员与11位盲人屏幕阅读器用户进行了面对面实验室研究，参与者学习并使用StreetReaderAI探索多个地点和评估潜在步行路线。\n*   **用户反馈：**\n    *   总体评价积极，有用性评分中位数为7（1-7级量表，平均6.4，标准差0.9）。\n    *   参与者强调了虚拟导航与AI的结合、AI Chat界面的无缝性以及所提供信息的价值。\n    *   被认为是对现有街景工具无障碍性不足的重大改进。\n    *   互动式AI聊天功能被描述为使关于街道和地点的对话既引人入胜又实用。\n*   **使用数据：**\n    *   参与者访问了350多个全景图，发出了1000多次AI请求。\n    *   AI Chat的使用频率是AI Describer的六倍，表明用户更倾向于个性化、对话式查询。\n*   **改进空间：** 参与者有时难以正确辨别方向、区分AI回答的真实性以及确定AI知识的局限性。\n\n## 8. AI Chat交互问题类型分析\n研究分析了917次AI Chat交互，并标记了23类问题。最常见的四种问题类型包括：\n1.  **空间定位 (27.0%)：** 关注物体的位置和距离，例如“公交车站离我有多远？”和“垃圾桶在长凳的哪一边？”\n2.  **物体存在 (26.5%)：** 查询关键特征（如人行道、障碍物、门）是否存在，例如“这里有斑马线吗？”\n3.  **一般描述 (18.4%)：** 请求当前视图的摘要，例如“我前面有什么？”\n4.  **物体/地点位置 (14.9%)：** 询问事物在哪里，例如“最近的交叉口在哪里？”或“你能帮我找到门吗？”\n\n## 9. StreetReaderAI 准确性\n在参与者向AI Chat提出的816个问题中：\n*   **正确回答：** 703个 (86.3%)\n*   **不正确：** 32个 (3.9%)\n*   **部分正确：** 26个 (3.2%)\n*   **AI拒绝回答：** 54个 (6.6%)\n在32个不正确回答中：\n*   20个 (62.5%) 是假阴性（例如，说自行车架不存在但实际存在）。\n*   12个 (37.5%) 是误识别（例如，将黄色减速带误认为斑马线）或由于AI Chat尚未在街景中看到目标而导致的错误。\n*   研究指出，需要在实验室环境之外探索StreetReaderAI的性能。\n\n## 10. 未来展望\nStreetReaderAI是使街景工具对所有人无障碍化的一个有前景的开端。未来的扩展机会包括：\n*   **迈向地理视觉代理：** 设想更自主的AI Chat代理，可以自行探索（例如，用户询问“这条路上的下一个公交车站在哪里？”，代理自动导航并报告）。\n*   **支持路线规划：** 支持完整的起点到终点路线规划（例如，代理可以“预先步行”路线，生成对盲人友好的摘要，指出潜在障碍）。\n*   **更丰富的音频界面：** 探索更丰富的非语言反馈，包括空间化音频和从图像本身合成的沉浸式3D音频景观。",
      "shortSummary": "StreetReaderAI是一个概念验证原型，旨在通过情境感知多模态AI使街景对盲人及低视力用户无障碍化。它提供实时AI描述、与AI代理的动态对话以及无障碍导航控制。该系统由AI Describer和AI Chat组成，后者具有强大的记忆功能。用户测试显示其有用性高，尤其偏爱AI Chat。尽管存在方向辨别和AI准确性挑战，StreetReaderAI在无障碍导航方面取得了显著进展，并为未来的自主地理视觉代理和路线规划奠定了基础。",
      "translated_title": "StreetReaderAI：通过情境感知多模态AI使街景无障碍化",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/StreetReaderAI-3.width-1250.png",
          "alt": "StreetReaderAI-3",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"mnsbf\">Interactive streetscape tools, available today in every major mapping service, have revolutionized how people virtually navigate and explore the world — from previewing routes and inspecting destinations to remotely visiting world-class tourist locations. But to date, screen readers have not been able to interpret street view imagery, and alt text is unavailable. We now have an opportunity to redefine this immersive streetscape experience to be inclusive for all with multimodal AI and image understanding. This could eventually allow a service like Google Street View, which has over 220 billion images spanning 110+ countries and territories, to be more accessible to people in the blind and low-vision community, offering an immersive visual experience and opening up new possibilities for exploration.</p><p data-block-key=\"a7psv\">In “<a href=\"https://arxiv.org/pdf/2508.08524\" target=\"_blank\" rel=\"noopener noreferrer\">StreetReaderAI: Making Street View Accessible Using Context-Aware Multimodal AI</a>”, presented at <a href=\"https://uist.acm.org/2025/\" target=\"_blank\" rel=\"noopener noreferrer\">UIST’25</a>, we introduce StreetReaderAI, a proof-of-concept accessible street view prototype that uses context-aware, real-time AI and accessible navigation controls. StreetReaderAI was designed iteratively by a team of blind and sighted accessibility researchers, drawing on previous work in accessible first-person gaming and navigation tools, such as <a href=\"https://www.gmagames.com/sod.html\" target=\"_blank\" rel=\"noopener noreferrer\">Shades of Doom</a>, <a href=\"https://www.blindsquare.com/\" target=\"_blank\" rel=\"noopener noreferrer\">BlindSquare</a>, and <a href=\"https://www.microsoft.com/en-us/research/product/soundscape/\" target=\"_blank\" rel=\"noopener noreferrer\">SoundScape</a>. Key capabilities include:</p><ul><li data-block-key=\"9pu1u\">Real-time AI-generated descriptions of nearby roads, intersections, and places.</li><li data-block-key=\"5qf3g\">Dynamic conversation with a multimodal AI agent about scenes and local geography.</li><li data-block-key=\"9oaqi\">Accessible panning and movement between panoramic images using voice commands or keyboard shortcuts.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/StreetReaderAI-AIDescriber.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"xrzw7\"><i>StreetReaderAI provides a context-aware description of the street view scene by inputting geographic information sources and the user’s current field-of-view into Gemini. For the full audio-video experience, including sound, please refer to this</i> <a href=\"https://youtu.be/cm9gd-502TI\" target=\"_blank\" rel=\"noopener noreferrer\"><i>YouTube video</i></a><i>.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/AIChat_BigBen-PrivacyRedacted.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"xrzw7\"><i>StreetReaderAI uses</i> <a href=\"https://ai.google.dev/gemini-api/docs/live\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Gemini Live</i></a><i> to provide a real-time, interactive conversation about the scene and local geographic features. For the full audio-video experience, including sound, please refer to this</i> <a href=\"https://youtu.be/unsNaq-ea3s\" target=\"_blank\" rel=\"noopener noreferrer\"><i>YouTube video</i></a><i>.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Navigating in StreetReaderAI</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">StreetReaderAI offers an immersive, first-person exploration experience, much like a video game where audio is the primary interface.</p><p data-block-key=\"f9d2t\">StreetReaderAI provides seamless navigation through both keyboard and voice interaction. Users can explore their surroundings using the left and right arrow keys to shift their view. As the user pans, StreetReaderAI shares audio feedback, voicing the current heading as a cardinal or intercardinal direction (<i>e.g.,</i> “<i>Now facing: North</i>” or “<i>Northeast</i>”). It also expresses whether the user can move forward and if they are currently facing a nearby landmark or place.</p><p data-block-key=\"9ccll\">To move, the user can take “virtual steps” using the up arrow or move backward with the down arrow. As a user moves through the virtual streetscape, StreetReaderAI describes how far the user traveled and key geographic information, such as nearby places. Users can also use “jump” or “teleport” features to quickly move to new locations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How StreetReaderAI serves as a virtual guide</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">The core of StreetReaderAI is its two underlying AI subsystems backed by Gemini: AI Describer and AI Chat. Both subsystems take in a static prompt and optional user profile as well as dynamic information about the user’s current location, such as nearby places, road information, and the current field-of-view image (i.e., what’s being shown in Street View).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">AI Describer</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">AI Describer functions as a context-aware scene description tool that combines dynamic geographic information about the user’s virtual location along with an analysis of the current Street View image to generate a real-time audio description.</p><p data-block-key=\"6gha8\">It has two modes: a “default<i>”</i> prompt emphasizing navigation and safety for blind pedestrians, and a “tour guide<i>”</i> prompt that provides additional tourism information (e.g., historic and architectural context). We also use Gemini to predict likely follow-up questions specific to the current scene and local geography that may be of interest to blind or low-vision travelers.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/StreetReaderAI-3.width-1250.png\" alt=\"StreetReaderAI-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/StreetReaderAI-3.width-1250.png\" alt=\"StreetReaderAI-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"xrzw7\"><i>A diagram of how AI Describer combines multimodal data to support context-aware scene descriptions.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">AI Chat</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">AI Chat builds on AI Describer but allows users to ask questions about their current view, past views, and nearby geography. The chat agent uses <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/live-api\" target=\"_blank\" rel=\"noopener noreferrer\">Google's Multimodal Live API</a>, which supports real-time interaction, function calling, and temporarily retains memory of all interactions within a single session. We track and send each pan or movement interaction along with the user's current view and geographic context (e.g., nearby places, current heading).</p><p data-block-key=\"6v4fs\">What makes AI Chat so powerful is its ability to hold a temporary “memory” of the user's session — the context window is set to a maximum of 1,048,576 input tokens, which is roughly equivalent to over 4k input images. Because AI Chat receives the user's view and location with every virtual step, it collects information about the user’s location and context. A user can virtually walk past a bus stop, turn a corner, and then ask, “<i>Wait, where was that bus stop?</i>” The agent can recall its previous context, analyze the current geographic input, and answer, “<i>The bus stop is behind you, approximately 12 meters away.</i>”</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Testing StreetReaderAI with blind users</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">To evaluate StreetReaderAI, we conducted an in-person lab study with eleven blind screen reader users. During the sessions, participants learned about StreetReaderAI and used it to explore multiple locations and evaluate potential walking routes to destinations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/StreetReaderAI-BusStopTask.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"jvl9c\"><i>A blind participant using StreetReaderAI to explore potential travel to a bus stop and inquire about bus stop features, such as the existence of benches and a shelter. For the full audio-video experience, including sound, please refer to this</i> <a href=\"https://youtu.be/rt7Dlqv0eoA\" target=\"_blank\" rel=\"noopener noreferrer\"><i>YouTube video</i></a><i>.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"mnsbf\">Overall, participants reacted positively to StreetReaderAI, rating the overall usefulness 6.4 (median=7; SD=0.9) on a <a href=\"https://en.wikipedia.org/wiki/Likert_scale\" target=\"_blank\" rel=\"noopener noreferrer\">Likert scale</a> from 1–7 (where 1 was ‘not at all useful’ and 7 was ‘very useful’), emphasizing the interplay between virtual navigation and AI, the seamlessness of the interactive AI Chat interface, and the value of information provided. Qualitative feedback from participants consistently highlighted StreetReaderAI's significant accessibility advancement for navigation, noting that existing street view tools lack this level of accessibility. The interactive AI chat feature was also described as making conversations about streets and places both engaging and helpful.</p><p data-block-key=\"evnc8\">During the study, participants visited over 350 panoramas and made over 1,000 AI requests. Interestingly, AI Chat was used six times more often than AI Describer, indicating a clear preference for personalized, conversational inquiries. While participants found value in StreetReaderAI and adeptly combined virtual world navigation with AI interactions, there is room for improvement: participants sometimes struggled with properly orienting themselves, distinguishing the veracity of AI responses, and determining the limits of AI knowledge.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/StreetReaderAI-PlaygroundTask.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"jvl9c\"><i>In one study task, participants were given the instruction, “Find out about an unfamiliar playground to plan a trip with your two young nieces.” This video clip illustrates the diversity of questions asked and the responsiveness of StreetReaderAI. For the full audio-video experience, including sound, please refer to this</i> <a href=\"https://youtu.be/Uxj5fSCp1Dg\" target=\"_blank\" rel=\"noopener noreferrer\"><i>YouTube video</i></a><i>.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Results</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">As the first study of an accessible street view system, our research also provides the first-ever analysis of the types of questions blind people ask about streetscape imagery. We analyzed all 917 AI Chat interactions and annotated each with up to three tags drawn from an emergent list of 23 question type categories. The four most common question types included:</p><ul><li data-block-key=\"c3cvo\"><b>Spatial orientation</b>: 27.0% of participants were most interested in the location and distance of objects, e.g., “<i>How far is the bus stop from where I'm standing?</i>” and <i>“Which side are the garbage cans next to the bench?”</i></li><li data-block-key=\"akthh\"><b>Object existence</b>: 26.5% of participants queried for the presence of key features like sidewalks, obstacles, and doors; <i>“Is there a crosswalk here?”</i></li><li data-block-key=\"e9jjp\"><b>General description</b>: 18.4% of participants started AI Chat by requesting a summary of the current view, often asking, “<i>What's in front of me?</i>”</li><li data-block-key=\"84eds\"><b>Object/place location</b>: 14.9% of participants asked <i>where</i> things were, such as, “<i>Where is the nearest intersection?</i>” or “<i>Can you help me find the door?</i>”</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">StreetReaderAI accuracy</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">Because StreetReaderAI relies so significantly on AI, a critical challenge is response accuracy. Of the 816 questions that participants asked AI Chat:</p><ul><li data-block-key=\"4i79q\">703 (86.3%) were correctly answered.</li><li data-block-key=\"e5j91\">32 (3.9%) were incorrect (3.9%).</li><li data-block-key=\"7tdgb\">The remaining were either: partially correct (26; 3.2%) or the AI refused to answer (54; 6.6%).</li></ul><p data-block-key=\"2rj55\">Of the 32 incorrect responses:</p><ul><li data-block-key=\"20m48\">20 (62.5%) were false negatives, <i>e.g.,</i> stating that a bike rack did not exist when it did.</li><li data-block-key=\"98cf5\">12 (37.5%) were misidentifications (<i>e.g.,</i> a yellow speed bump interpreted as a crosswalk) or misc errors due to AI Chat not yet seeing the target in street view.</li></ul><p data-block-key=\"61i5r\">More work is necessary to explore how StreetReaderAI performs in other contexts and beyond lab settings.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">What’s next?</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\">StreetReaderAI is a promising first step toward making streetscape tools accessible to all. Our <a href=\"https://research.google/pubs/streetviewai-making-street-view-accessible-using-context-aware-multimodal-ai/\">study</a> highlights <i>what</i> information blind users desire from and ask about streetscape imagery and the potential for multimodal AI to answer their questions.</p><p data-block-key=\"bp879\">There are several other opportunities to expand on this work:</p><ul><li data-block-key=\"mbdd\"><b>Towards Geo-visual Agents:</b> We envision a more autonomous AI Chat agent that can explore on its own. For example, a user could ask, “<i>What’s the next bus stop down this road?</i>” and the agent could automatically navigate the Street View network, find the stop, analyze its features (benches, shelters), and report back.</li><li data-block-key=\"946n0\"><b>Supporting Route Planning:</b> Similarly, StreetReaderAI does not yet support full origin-to-destination routing. Imagine asking, “<i>What’s the walk like from the nearest subway station to the library?</i>” A future AI agent could “pre-walk” the route, analyzing every Street View image to generate a blind-friendly summary, noting potential obstacles, and identifying the exact location of the library’s door.</li><li data-block-key=\"987rq\"><b>Richer Audio Interface:</b> The primary output of StreetReaderAI is speech. We are also exploring richer, non-verbal feedback, including spatialized audio and fully immersive 3D audio soundscapes synthesized from the images themselves.</li></ul><p data-block-key=\"fokff\">Though a “proof-of-concept” research prototype, StreetReaderAI helps demonstrate the potential of making immersive streetscape environments accessible.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mnsbf\"><i>This research was conducted by Jon E. Froehlich, Alexander J. Fiannaca, Nimer Jaber, Victor Tsaran, Shaun K. Kane, and Philip Nelson. We thank Project Astra and the Google Geo teams for their feedback as well as our participants. Diagram icons are from Noun Project, including: “</i><a href=\"https://thenounproject.com/icon/ai-prompt-7906362/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>prompt icon</i></a><i>” by Firdaus Faiz, “</i><a href=\"https://thenounproject.com/icon/html-7643378/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>command functions</i></a><i>” by Kawalan Icon, “</i><a href=\"https://thenounproject.com/icon/place-7892484/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>dynamic geo-context</i></a><i>” by Didik Darmanto, and “</i><a href=\"https://thenounproject.com/icon/ai-7865331/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>MLLM icon</i></a><i>” by Funtasticon.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "我们如何构建个人健康教练 (原标题: How we are building the personal health coach)",
      "link": "https://research.google/blog/how-we-are-building-the-personal-health-coach/",
      "pubDate": "Sun, 26 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-26T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 我们如何构建个人健康教练\n\n## 解决现有健康旅程的痛点\n\n文章指出，当前的健康和健身旅程存在碎片化、通用性和难以获取的问题。例如，初级保健医生可能会建议专科就诊或减肥以更好地管理糖尿病，但通常不会提供营养师或健身教练的联系方式，这使得用户需要自行连接这些服务。\n\n## 我们的愿景：AI 驱动的个人健康教练\n\n为了解决上述问题，我们的愿景是提供一个主动、个性化和适应性强的 AI 驱动的个人健康教练。该教练将无缝实现以下功能：\n\n*   **主动洞察**：提供关于睡眠、健身和健康的预见性洞察。\n*   **个性化指导**：基于行为科学、既定的健康和福祉原则，以及个人健康指标（如活动和其他生理数据）提供个性化指导。\n*   **个性化辅导**：通过可操作、适应性强的计划，帮助用户设定目标并养成可持续的习惯。\n\n## 推出与可用性\n\n从明天开始，我们将在接下来的一周内，为符合条件的美国 Fitbit Premium Android 用户推出健康教练的可选公开预览版，并很快扩展到 iOS 用户。选择参与的用户将被要求同意提供其 Fitbit 数据访问权限，以接收个性化洞察。\n\n## 技术基础与创新\n\n这项创新得益于 Gemini 模型的进步，以及 Fitbit 应用中全新的 AI 优先个人健康教练体验，以及 Fitbit、Google Research 和 Google DeepMind 在尖端研究方面的持续进展。构建一个健康和福祉产品需要时间和严谨性，我们以深思熟虑和迭代的方式进行，以科学为基础，并结合科学界和用户的反馈。\n\n### 引导 Gemini 进行健康辅导\n\n回答“我锻炼后睡眠会更好吗？”这类看似简单的问题，需要教练具备主动、个性化和适应性，这涉及多项技术创新：\n\n1.  **理解和数值推理生理时间序列数据**：教练需要理解并对睡眠和活动等生理时间序列数据进行数值推理，其能力类似于 PH-LLM 所展示的。对于此类问题，教练会验证近期数据的可用性，选择正确的指标，对比相关日期，根据个人基线和人群统计数据进行情境化分析，整合与教练的过往互动，并最终利用分析结果提供量身定制的答案和洞察。\n2.  **多智能体框架**：我们利用一个多智能体框架来协调专家子智能体，以提供清晰、一致和全面的支持，包括：\n    *   **对话智能体**：负责多轮对话、意图理解、智能体编排、上下文收集和响应生成。\n    *   **数据科学智能体**：迭代使用工具来获取、分析和总结相关数据（例如，睡眠和锻炼数据），并根据需要利用代码生成能力。\n    *   **领域专家智能体**：例如健身专家，分析用户数据以生成个性化健身计划，并根据进展和上下文变化进行调整。\n\n    ![个人健康教练的工作原理示意图](https://storage.googleapis.com/gweb-research2023-media/images/PH-coach-1.width-1250.png)\n    *个人健康教练的工作原理示意图。用户可以进行对话式提问。在后台，对话智能体处理对话、收集上下文并协调其他智能体。其他智能体包括专注于检索和分析相关数据的数据科学智能体，以及理解特定领域（如健身）的领域专家智能体。*\n\n3.  **对基础模型的精心引导**：尽管基础模型能力强大，但在健康和福祉领域，需要仔细引导才能使其发挥作用。我们开发了基于消费者健康和福祉需求的评估方法，以指导系统指令并改进 Gemini 协助用户核心能力。\n\n这些创新共同带来了更个性化和有益的指导。\n\n## 专家验证和用户迭代设计的重要性\n\n我们深知仅有技术卓越是不够的，可靠性和安全性至关重要。\n\n### 科学基础与专家反馈\n\n*   我们将教练建立在科学和成熟的辅导及健身框架之上。\n*   我们采用以人为本的设计，整合了专家和用户反馈，包括：\n    *   召集由顶尖专家组成的消费者健康咨询小组，为教练的开发提供反馈和指导。\n    *   通过专业健身教练的意见扩展运动科学基础，以纳入特定情境的方法。\n    *   开发新颖的方法与专家合作，在个人健康和福祉的细微领域促进共识。\n    *   在大型同意研究中（例如，来自 Fitbit Insights Explorer、睡眠和症状检查器实验室）积极征求了数千名用户的反馈。\n\n### 持续评估与改进\n\n我们使用 SHARP 评估框架（安全性、有用性、准确性、相关性和个性化）持续验证个人健康教练。这种多层次评估涉及超过 100 万次人工标注和超过 10 万小时的由通才和运动、睡眠、家庭医学、心脏病学、内分泌学、运动和行为科学等各个领域的专家进行的人工评估。这一过程通过自动评估器进一步扩展和规模化，以确保健康建议的科学准确性。该框架还整合了教练的实际表现，从而在最关键的领域实现持续改进。\n\n## 帮助我们构建您的教练\n\n加入公开预览版，并在应用内或通过我们的社区论坛分享您的反馈。您将帮助塑造教练，使其能为您做更多事情，并与您共同成长。",
      "shortSummary": "我们正在构建一个由AI驱动的个人健康教练，旨在解决当前健康旅程的碎片化问题。该教练基于Gemini模型和多智能体框架，提供主动、个性化和适应性的睡眠、健身及健康指导，帮助用户设定目标并养成习惯。它通过理解生理数据、专家系统和持续的用户反馈与科学验证（SHARP框架）来确保可靠性和准确性。目前已向符合条件的美国Fitbit Premium Android用户推出公开预览版。",
      "translated_title": "我们如何构建个人健康教练",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PH-coach-1.width-1250.png",
          "alt": "PH-coach-1",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fl9us\">Historically, health and fitness journeys have been fragmented, generic and inaccessible, whether within existing apps or through general health and fitness journeys outside of apps. For instance, a primary care provider might suggest seeing a specialist or losing weight for better diabetes management, but often without providing connections to a nutritionist or fitness coach. This leaves users with the burden of connecting these dots themselves.</p><p data-block-key=\"2u7lq\">Our vision is to address this by offering a proactive, personalized and adaptive AI-powered personal health coach. This coach will seamlessly enable:</p><ul><li data-block-key=\"b8to4\">Proactive insights on sleep, fitness and health</li><li data-block-key=\"9pvk4\">Personalized guidance grounded in behavioral science, established health and wellness principles and individual health metrics, such as activity and other physiological data</li><li data-block-key=\"1756a\">Personalized coaching to set goals and build sustainable habits through actionable, adaptive plans</li></ul><p data-block-key=\"1n2cb\">Starting tomorrow and over the next week, we are rolling out an optional <a href=\"https://blog.google/products/fitbit/personal-health-coach-public-preview/\" target=\"_blank\" rel=\"noopener noreferrer\">public preview of the health coach</a> for eligible US-based <a href=\"https://store.google.com/product/fitbit_premium?hl=en-US&amp;pli=1\" target=\"_blank\" rel=\"noopener noreferrer\">Fitbit Premium</a> Android users and expanding to iOS users soon. Users who opt in to participate will be presented the consent to provide access to their Fitbit data to receive personalized insights.</p><p data-block-key=\"a9d5t\">This innovation is powered by advances in <a href=\"https://ai.google.dev/gemini-api/docs/models\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini models</a> plus our new AI-first personal health coach experience on the Fitbit app, and our continuous progress in cutting-edge research across Fitbit, Google Research and Google DeepMind. Building from the ground up takes time and rigor, a commitment especially important for health and wellness. We are approaching this thoughtfully and iteratively, grounded in science, incorporating feedback from the scientific community and users. We will continue to be open about our work through publications and updates, and here we provide a quick peek into what went into building the personal health coach.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Steering Gemini for health coaching</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fl9us\">“Do I get better sleep after exercising?” sounds like a simple question, but answering it like a proactive, personalized and adaptive coach required several technical innovations.</p><p data-block-key=\"4rl0\">First, we need the coach to understand and do numerical reasoning on physiological time series data such as sleep and activity, using capabilities similar to those showcased by <a href=\"https://research.google/blog/advancing-personal-health-and-wellness-insights-with-ai/\">PH-LLM</a>. For questions like this, the coach verifies recent data availability, chooses the right metrics, contrasts relevant days, contextualizes results against personal baselines and population-level statistics, incorporates prior interactions with the coach, and finally uses the analysis to provide tailored answers and insights.</p><p data-block-key=\"f4inb\">Second, we utilize a <a href=\"https://research.google/blog/the-anatomy-of-a-personal-health-agent/\">multi-agent framework</a> that coordinates expert sub-agents to provide clear, consistent and holistic support, such as (1) a <a href=\"https://arxiv.org/pdf/2503.19328\" target=\"_blank\" rel=\"noopener noreferrer\">conversational agent</a> for multi-turn conversations, intent understanding, agent orchestration, context gathering and response generation; (2) a data science agent that iteratively uses tools to fetch, analyze, and summarize relevant data (e.g., sleep and workout data), <a href=\"https://arxiv.org/pdf/2406.06464\" target=\"_blank\" rel=\"noopener noreferrer\">leveraging code-generation capabilities as needed</a>; and (3) a domain expert, such as a fitness expert that analyzes user data to generate personalized fitness plans and adapt them as progress and context change.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PH-coach-1.width-1250.png\" alt=\"PH-coach-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PH-coach-1.width-1250.png\" alt=\"PH-coach-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"jj9h8\"><i>Schematic of how the personal health coach works. The user can ask questions conversationally. Behind the scenes, a conversation agent handles the conversation, gathers context, and orchestrates other agents. The other agents include a data science agent focusing on retrieving and analyzing relevant data, and a domain expert agent that understands specific fields such as fitness.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fl9us\">Third, while foundational models are incredibly capable, careful steer is required for it to be useful in the health and wellness context. We developed evaluations based on consumer health and wellness needs to inform the system instructions and improve upon Gemini’s core capabilities in assisting our users.</p><p data-block-key=\"einqj\"><a href=\"https://research.google/blog/the-anatomy-of-a-personal-health-agent/\">Working together</a>, these innovations result in more personalized and beneficial guidance.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The critical role of expert validation and iterative design with users</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fl9us\">We know that technical excellence alone isn’t enough, and reliability and safety are paramount.</p><p data-block-key=\"81mub\">First, we grounded the coach in scientific and well established <a href=\"https://services.google.com/fh/files/blogs/evolving_cardio_load_2025.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">coaching and fitness frameworks</a>.</p><p data-block-key=\"7sbql\">We also used human-centered design to integrate expert and user feedback including:</p><ul><li data-block-key=\"a61ei\">Convening a <a href=\"https://store.google.com/intl/en/ideas/articles/google-consumer-health-advisory-panel/\" target=\"_blank\" rel=\"noopener noreferrer\">Consumer Health Advisory Panel</a> of leading experts to provide feedback and guidance for the development of the coach.</li><li data-block-key=\"86hoc\">Extending the sport science foundation with input with professional fitness coaches to incorporate context-specific approaches.</li><li data-block-key=\"3mvn\">Developing <a href=\"https://arxiv.org/pdf/2508.09349\" target=\"_blank\" rel=\"noopener noreferrer\">novel methods</a> to collaborate with experts, fostering consensus in nuanced areas of personal health and wellness.</li><li data-block-key=\"ih7j\">Actively soliciting feedback from thousands of users in large-scale consented research studies (e.g., from Fitbit Insights Explorer, Sleep and Symptom Checker <a href=\"https://support.google.com/fitbit/answer/14566053?hl=en#zippy=%2Cwhat-personalized-sleep-schedule-lab-is\" target=\"_blank\" rel=\"noopener noreferrer\">Labs</a>).</li></ul><p data-block-key=\"cq03p\">Finally, we continuously validate the personal health coach using dimensions of safety, helpfulness, accuracy, relevance, and personalization, collectively known as the <a href=\"https://services.google.com/fh/files/blogs/winslow_2025_sharp_framework.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">SHARP evaluation framework</a>. This multi-level assessment involves over 1 million human annotations and more than 100k hours of human evaluation by generalists and experts across various fields such as sports, sleep, family medicine, cardiology, endocrinology, exercise and behavioral science. This process is further extended and scaled with autoraters to ensure that wellness recommendations are scientifically accurate. The framework also incorporates the coach’s real-world performance, enabling ongoing improvements in the most critical areas.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Help us build your coach</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fl9us\">Join the <a href=\"https://blog.google/products/fitbit/personal-health-coach-public-preview/\" target=\"_blank\" rel=\"noopener noreferrer\">public preview</a> and share your feedback in the app or through our <a href=\"https://community.fitbit.com/t5/Personal-health-coach-public-preview/bd-p/PHAPP\" target=\"_blank\" rel=\"noopener noreferrer\">community forum</a>. You will help shape the coach, so it can do more for and with you.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Google Earth AI：利用基础模型和跨模态推理解锁地理空间洞察 (原标题: Google Earth AI: Unlocking geospatial insights with foundation models and cross-modal reasoning)",
      "link": "https://research.google/blog/google-earth-ai-unlocking-geospatial-insights-with-foundation-models-and-cross-modal-reasoning/",
      "pubDate": "Wed, 22 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-22T16:00:00.000Z",
      "creator": "Google",
      "summary": "Google Earth AI 旨在通过结合强大的基础模型和基于 Gemini 的地理空间推理代理，解决跨领域洞察的复杂挑战，实现行星尺度的复杂推理。该系统将复杂问题分解为多步骤计划，利用基础模型、海量数据存储和地理空间工具执行计划，并整合结果以提供全面的答案。\n\n### 最新创新\n\nGoogle Earth AI 引入了以下新功能：\n\n*   **新的图像和人口基础模型**：提供了详细的技术信息和评估，展示了最先进的性能。\n*   **地理空间推理代理演示**：展示了如何利用这些模型解决复杂的多步骤地理空间查询。\n\n### Earth AI 的构建模块\n\nEarth AI 的核心在于其先进的基础模型：\n\n*   **图像模型（遥感基础模型）**\n    *   **能力**：包括视觉-语言模型、开放词汇目标检测和适应性视觉骨干。\n    *   **自然语言查询**：用户可以使用自然语言提问，例如在风暴后的图像中“查找所有被洪水淹没的道路”，并获得快速准确的答案。\n    *   **性能**：模型在大量高分辨率航拍图像和文本描述语料库上进行训练，在多个公共地球观测基准上取得了最先进的结果。例如，在基于文本的图像搜索任务中平均改进超过16%，零样本新目标检测模型的基线准确率翻倍。\n    *   **图片**：\n        *   Earth AI 流程图，展示了其如何结合最先进的模型与地理空间推理代理来解决关键的全球挑战。\n        ![Flowchart of Earth AI](https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-1a-Overview.width-1250.png)\n        *   模型评估显示，遥感优化型 RS-OWL-ViT-v2 模型（“我们的”）在零样本设置下，相对于 OWL-ViT-v2 开放词汇检测模型，平均精度（AP50）有显著提高，并说明了 FLAME + RS-OWL-ViT-v2 组合方法（“我们的”）在新型类别少样本检测中相对于 SIoU 的优势。\n        ![EarthAI-2-RemoteSensingEvalFinal](https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-2-RemoteSensingEvalFinal.width-1250.png)\n\n*   **人口模型（人口动态基础模型）**\n    *   **目标**：理解人与地点之间复杂的相互作用。\n    *   **创新**：引入了两个关键创新：跨17个国家的全球一致嵌入，以及每月更新的嵌入，以捕捉人类活动不断变化的动态，这对于时间敏感的预测至关重要。\n    *   **效果**：在独立研究中表现出显著效果，例如，牛津大学研究人员发现，将这些嵌入整合到巴西登革热预测模型中，将12个月预测的长期 R²（衡量模型解释实际疾病率的指标）从0.456提高到0.656。\n    *   **图片**：\n        *   人口动态基础模型在17个国家/地区的评估；R² 分数（范围0-1，越高越好）按国家/地区预测人口密度、树木覆盖、夜间灯光和海拔。全球趋势与我们最初在美国展示的强大性能相符。\n        ![Earth AI Population Dynamics plot](https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-3-PopDynEval.width-1250.png)\n\n*   **环境模型**\n    *   **预测能力**：提供中程天气、季风、空气质量和河流洪水的最新预测。\n    *   **扩展**：最近已扩展到为全球提供降水临近预报，并为20亿人提供最严重的河流洪水预报。\n\n### 模型组合的预测能力增强\n\n研究发现，结合不同模型能够产生更强大的预测能力。这种协同方法可以更全面、准确地理解现实世界现象，并显著改善关键应用的预测。例如，通过融合人口动态基础模型中的社会经济特征嵌入和 AlphaEarth 基础模型中的景观特征，FEMA 国家风险指数对20种不同灾害的预测平均 R² 提高了11%，其中龙卷风风险预测提高了25%，河流洪水风险预测提高了17%。\n\n### 通过地理空间推理解决复杂问题\n\n由 Gemini 驱动的地理空间推理代理简化了 Earth AI 洞察的协调。该代理能够将复杂的自然语言查询分解为动态的多步骤计划，并调用“专家”子代理，利用 Earth AI 模型、Data Commons、Earth Engine 和地理空间特定工具来执行每个步骤。这种模块化的代理网络具有可扩展性和可定制性。\n\n**案例演示：识别易受风暴影响的人口**\n\n1.  调用环境模型识别可能受到飓风风力影响的特定地理区域。\n2.  查询 Data Commons 获取人口统计数据，以识别预测登陆区域内人口较多的县。\n3.  从 BigQuery 的公共数据集中检索相关县的官方边界。\n4.  对风区和官方县边界进行空间交叉。\n5.  利用人口动态基础模型和县级统计数据，动态训练模型以识别最脆弱的邮政编码。\n6.  使用遥感基础模型的目标检测功能，识别最脆弱邮政编码区域内的关键基础设施。\n\n**代理评估**\n\n*   **Q&A 基准测试**：地理空间推理代理在 Q&A 基准测试中取得了0.82的总体准确率，显著优于基线 Gemini 2.5 Pro (0.50) 和 Gemini 2.5 Flash (0.39) 代理。\n*   **图片**：\n    *   代理在 Q&A 基准测试中的表现可视化。地理空间推理代理在描述性和检索类别中比基线 Gemini 2.5 Pro 代理高出37%，在更复杂的分析和关系类别中高出124%，总分高出64%。\n    ![Earth AI performance on QA](https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-7-QAPerformance.width-1250.png)\n*   **危机响应案例研究**：论文通过案例研究展示了协调环境、遥感和人口动态等多样化洞察的益处。\n\n### 共同释放地球潜力\n\nEarth AI 代表着行星理解的根本性飞跃。多模态、基于推理的方法，建立在最先进的地理空间 AI 模型基础上，能够解锁单一分析无法实现的洞察。Google 致力于扩大访问权限，以帮助全球社区应对地球最紧迫的挑战。\n\n**实际应用案例**：\n\n*   **Bellwether (Google X)**：利用 Earth AI 预测风暴前的建筑物损坏，帮助保险客户更快支付索赔。\n*   **联合国全球脉动**：使用 Earth AI 图像模型评估自然灾害后的损失，实现快速危机响应。\n*   **GiveDirectly**：利用地理空间推理和洪水预报识别高风险社区并提供现金援助。\n*   **Google.org 资助合作伙伴**：如 Khushi Baby、Cooper/Smith 等，利用人口动态基础模型建模传染病并改善全球公共卫生行动。\n*   **新企业用户**：包括 Public Storage、CARTO 和 Visiona Space Technology。\n\nGoogle 鼓励组织表达对早期访问遥感基础模型（Vertex AI 中的图像模型）、人口动态基础模型和地理空间推理的兴趣。",
      "shortSummary": "Google Earth AI 结合强大的基础模型和基于 Gemini 的地理空间推理代理，旨在实现行星尺度的复杂地理空间洞察。它引入了新的图像和人口基础模型，并展示了其推理代理如何处理多步骤查询。通过整合遥感、人口动态和环境模型，Earth AI 显著提升了预测能力，例如在灾害风险评估和传染病建模方面。该平台已应用于预测风暴损害、评估灾后损失和提供灾害援助等领域，致力于帮助全球应对地球挑战，并邀请开发者和企业表达合作兴趣。",
      "translated_title": "Google Earth AI：利用基础模型和跨模态推理解锁地理空间洞察",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-1a-Overview.width-1250.png",
          "alt": "Flowchart of Earth AI",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-2-RemoteSensingEvalFinal.width-1250.png",
          "alt": "EarthAI-2-RemoteSensingEvalFinal",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-3-PopDynEval.width-1250.png",
          "alt": "Earth AI Population Dynamics plot",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-7-QAPerformance.width-1250.png",
          "alt": "Earth AI performance on QA",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"445ko\">For years, Google has developed AI models that enhance our understanding of the planet. These models help keep Google products fresh, for example, ensuring Maps is accurate by <a href=\"https://blog.google/technology/research/open-buildings-ai-powered-maps-for-a-changing-world/\" target=\"_blank\" rel=\"noopener noreferrer\">analyzing satellite images</a> and giving Search users the most up-to-date alerts about <a href=\"https://blog.google/intl/en-africa/products/explore-get-answers/nowcasting-on-search-is-bringing-ai-powered-weather-forecasts-to-users-across-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">weather</a> and <a href=\"https://blog.google/technology/ai/expanding-flood-forecasting-coverage-helping-partners/\" target=\"_blank\" rel=\"noopener noreferrer\">natural</a> <a href=\"https://blog.google/outreach-initiatives/sustainability/google-wildfire-boundary-maps-europe-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">disasters</a>.</p><p data-block-key=\"f8odl\">As individual models grow more powerful, we’ve learned that many real-world questions require the combination of insights <i>across</i> domains. Answering complex queries like, <i>\"Where is a hurricane likely to make landfall? Which communities are most vulnerable and how should they prepare?\"</i> requires reasoning about imagery, population and the environment.</p><p data-block-key=\"a9dl3\">Earlier this year, we introduced <a href=\"https://blog.google/technology/ai/google-earth-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Earth AI</a> to solve this core challenge. By pairing our family of powerful foundation models with a <a href=\"https://research.google/blog/geospatial-reasoning-unlocking-insights-with-generative-ai-and-multiple-foundation-models/\">geospatial reasoning</a> agent, which uses our latest Gemini models, it’s becoming possible to perform complex, real-world reasoning at planetary scale. The models provide detailed understanding of our planet, grounded in real-world data. The agent, in turn, acts as an intelligent orchestrator. It deconstructs a complex question into a multi-step plan; executes the plan by calling on these foundation models, querying vast datastores, and using geospatial tools; and finally fuses the results at each step into a holistic answer.</p><p data-block-key=\"pma0\">Today, we're <a href=\"http://goo.gle/earthai-techreport\" target=\"_blank\" rel=\"noopener noreferrer\">introducing new Earth AI innovations</a>:</p><ol><li data-block-key=\"5pdsv\">New Imagery and Population foundation models, along with technical details and evaluations showing state-of-the-art performance.</li><li data-block-key=\"22iue\">Demonstrations of our geospatial reasoning agent using these models to solve complex, multi-step geospatial queries.</li></ol><p data-block-key=\"22evo\">To learn more, we invite you to read our full technical paper, \"<a href=\"http://goo.gle/earthai-techreport\" target=\"_blank\" rel=\"noopener noreferrer\">Google Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning</a>\". You can also get involved by <a href=\"https://forms.gle/1DPfcuys2AU63HgZ8\" target=\"_blank\" rel=\"noopener noreferrer\">expressing interest</a> as we expand access to these new capabilities for developers and enterprises.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-1a-Overview.width-1250.png\" alt=\"Flowchart of Earth AI\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-1a-Overview.width-1250.png\" alt=\"Flowchart of Earth AI\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"hz039\">Earth AI unites state-of-the-art models with geospatial reasoning agents to address critical global challenges.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Building blocks of Earth AI: State-of-the-art foundation models</h2>\n            \n        \n        \n    </div>\n\n\n\n    \n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Imagery</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"445ko\">Our new Remote Sensing Foundations models simplify and accelerate satellite imagery analysis using three core capabilities: vision-language models, open-vocabulary object detection, and adaptable vision backbones. Users can ask natural language queries, like <i>\"find all flooded roads\"</i> in an image captured after a storm, and get rapid, accurate answers. Our models are trained on a large corpus of high-resolution overhead imagery, paired with text descriptions. They achieve state-of-the-art results on multiple public Earth observation benchmarks. For instance, we achieve &gt;16% average improvement on text-based image search tasks, while our zero-shot model for novel object detection more than doubles the baseline accuracy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-2-RemoteSensingEvalFinal.width-1250.png\" alt=\"EarthAI-2-RemoteSensingEvalFinal\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-2-RemoteSensingEvalFinal.width-1250.png\" alt=\"EarthAI-2-RemoteSensingEvalFinal\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"1oe99\">Model evaluation shows significant Average Precision (AP50) improvement of our Remote-Sensing optimized RS-OWL-ViT-v2 model (“Ours”) over the OWL-ViT-v2 open vocabulary detection model in a zero-shot setting and illustrates the advantage of the combined FLAME + RS-OWL-ViT-v2 approach (\"Ours\") over SIoU for few-shot detection on novel classes.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Population</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"445ko\">This area of research, which includes <a href=\"https://research.google/blog/introducing-mobility-ai-advancing-urban-transportation/\">Mobility AI</a> and <a href=\"https://research.google/blog/insights-into-population-dynamics-a-foundation-model-for-geospatial-inference/\">Population Dynamics Foundations</a>, aims to understand the complex interplay between people and places. Our latest research in Population Dynamics Foundations introduces two key innovations: globally-consistent embeddings across 17 countries and monthly updated embeddings that capture the changing dynamics of human activity, which are critical for time-sensitive predictions. Population Dynamics Foundations has shown remarkable effectiveness in independent studies; for example, researchers at the <a href=\"https://www.ox.ac.uk/\" target=\"_blank\" rel=\"noopener noreferrer\">University of Oxford</a> found that incorporating these embeddings into a forecasting model for <a href=\"https://en.wikipedia.org/wiki/Dengue_fever\" target=\"_blank\" rel=\"noopener noreferrer\">Dengue fever</a> in Brazil improved long-range <a href=\"https://en.wikipedia.org/wiki/Coefficient_of_determination\" target=\"_blank\" rel=\"noopener noreferrer\">R²</a> (a metric that measures how well a model explains the actual disease rates) from 0.456 to 0.656 for 12-month predictions.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-3-PopDynEval.width-1250.png\" alt=\"Earth AI Population Dynamics plot\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-3-PopDynEval.width-1250.png\" alt=\"Earth AI Population Dynamics plot\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"1oe99\">Evaluation of our Population Dynamics Foundations across 17 countries; R2 score (range is 0–1, higher is better) by country for predicting population density, tree cover, night time lights, and elevation. The global trend matches the strong performance we <a href=\"https://research.google/blog/insights-into-population-dynamics-a-foundation-model-for-geospatial-inference/\">originally demonstrated</a> in the US only.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/EarthAI-4-PopDynEmbeddings.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"1oe99\">Similarity per dimension of the Population Dynamics Foundations embeddings, visualized by US zip code. The patterns across dimensions capture the diverse characteristics of the US population.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Environment</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"445ko\">Our previously-published research demonstrates state-of-the-art forecasts for <a href=\"https://deepmind.google/science/weathernext/\" target=\"_blank\" rel=\"noopener noreferrer\">medium-range weather</a>, <a href=\"https://blog.google/technology/research/indian-farmers-monsoon-prediction/\" target=\"_blank\" rel=\"noopener noreferrer\">monsoon onsets</a>, <a href=\"https://blog.google/products/maps/google-maps-apis-environment-sustainability/\" target=\"_blank\" rel=\"noopener noreferrer\">air quality</a> and <a href=\"https://blog.google/technology/ai/expanding-flood-forecasting-coverage-helping-partners/\" target=\"_blank\" rel=\"noopener noreferrer\">riverine floods</a>. We've recently expanded these Environment models to make <a href=\"https://arxiv.org/abs/2510.13050\" target=\"_blank\" rel=\"noopener noreferrer\">precipitation nowcasts for the entire planet</a>, and we’re now covering 2 billion people with forecasts for the most significant riverine floods.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Increased predictive power by combining models</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"445ko\">While each foundation model provides powerful insights, our findings confirm that combining models yields even more predictive power. This synergistic approach produces a more comprehensive and accurate understanding of real-world phenomena and dramatically improves predictions across critical applications.</p><p data-block-key=\"ca795\">For example, <a href=\"https://www.fema.gov/flood-maps/products-tools/national-risk-index\" target=\"_blank\" rel=\"noopener noreferrer\">FEMA’s National Risk Index</a> shows which communities are most at risk to natural hazards like floods and storms, based on a variety of factors including economic and social vulnerability as well as physical and environmental risk. By fusing embeddings that capture socio-economic features from our Population Dynamics Foundations and landscape features from <a href=\"https://deepmind.google/discover/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/\" target=\"_blank\" rel=\"noopener noreferrer\">AlphaEarth Foundations</a>, we improved prediction of FEMA’s National Risk Index by an average of 11% in <a href=\"https://en.wikipedia.org/wiki/Coefficient_of_determination\" target=\"_blank\" rel=\"noopener noreferrer\">R²</a> across 20 different hazards, versus using either data source alone, with the most significant gains in predicting risk from tornadoes (+25% R²) and riverine flooding (+17% R²).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Complex problem-solving via Geospatial Reasoning</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"445ko\">The example above illustrates that tackling real-world problems requires insights from multiple models with diverse capabilities. Orchestrating these Earth AI insights is simplified by our new Gemini-powered Geospatial Reasoning agent. The agent deconstructs complex, natural language queries and plans a dynamic, multi-step path to an answer. To execute each step, the agent can call on “expert” sub-agents that are equipped with Earth AI models described above, as well as the vast, real-world data found in <a href=\"https://datacommons.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Data Commons</a>, <a href=\"https://earthengine.google.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Earth Engine</a>, and geospatial-specific tools. This modular network of agents allows for extensibility and customization.</p><p data-block-key=\"1vriv\">To see how it works, consider a user who wishes to identify specific populations that are vulnerable to the risk of an oncoming storm. The agent executes a transparent series of reasoning steps:</p><ol><li data-block-key=\"dtr4d\">Invoke the Environment model to identify the specific geographic areas that are forecast to be at risk of hurricane force winds.</li><li data-block-key=\"30lur\">Query <a href=\"https://datacommons.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Data Commons</a> for demographic statistics to identify higher-population counties in the area of predicted landfall.</li><li data-block-key=\"iok3\">Retrieve official boundaries for the counties of interest from <a href=\"https://cloud.google.com/bigquery/public-data\" target=\"_blank\" rel=\"noopener noreferrer\">BigQuery’s public datasets</a>.</li><li data-block-key=\"5o23i\">Perform a spatial intersection between the wind zones and official county boundaries.</li><li data-block-key=\"4vp8\">Identify the most vulnerable postal codes by training a model on the fly using our Population Dynamics Foundations and county level statistics.</li><li data-block-key=\"2te22\">Use Remote Sensing Foundations object detection model to identify critical infrastructure in satellite imagery taken over one of the most vulnerable postal codes.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/EarthAI-5a-GeospatialDemo.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"prcis\">To assist a user in understanding vulnerability to an oncoming storm, our Gemini-powered Geospatial Reasoning agent uses our Environment model to identify the likely path of hurricane force winds, intersects this with country boundaries and population density from Big Query and Data Commons, and reasons across all of this data to pick the most critical locations. It also trains a model on the fly to generate higher resolution vulnerability data using Population Dynamics Foundations. And identifies critical infrastructure in satellite imagery using Remote Sensing Foundations.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"445ko\">To evaluate the agent, we developed two new methods for evaluation: a <i>Q&amp;A benchmark</i> for fact-finding and analysis with verifiable ground truth answers based on publicly available data and <i>Crisis Response</i> case studies for complex, predictive scenarios (e.g., solving the entire challenge above).</p><p data-block-key=\"1ffva\">On the Q&amp;A benchmark, our Geospatial Reasoning Agent achieved an overall accuracy of 0.82, significantly outperforming the baseline Gemini 2.5 Pro (0.50) and Gemini 2.5 Flash (0.39) agents (scores derived from <a href=\"https://en.wikipedia.org/wiki/ROUGE_(metric)\" target=\"_blank\" rel=\"noopener noreferrer\">ROUGE-L</a> F1 and percentage error, higher is better). This highlights the importance of giving agents access to specialized geospatial models and tools for these types of queries.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-7-QAPerformance.width-1250.png\" alt=\"Earth AI performance on QA\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EarthAI-7-QAPerformance.width-1250.png\" alt=\"Earth AI performance on QA\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"1oe99\">Visualizing the performance of agents on the Q&amp;A benchmark. The Geospatial Reasoning agent outperformed the baseline Gemini 2.5 Pro agent by 37% in the Descriptive and Retrieval category, and 124% in the more complex Analytical and Relational category, for an overall 64% higher score (scores derived from ROUGE-L F1 and percentage error).</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"445ko\">In the more complex <i>Crisis Response</i> scenarios, our <a href=\"http://goo.gle/earthai-techreport\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a> demonstrates the benefit of orchestrating a diverse set of Environment, Remote Sensing and Population Dynamics insights via case studies. Leveraging specialized sub-agents for geospatial and demographic analysis, we’re able to solve real-world analysis tasks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Unlocking our planet's potential, together</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"445ko\">Earth AI represents a fundamental leap in planetary understanding. Our findings show that a multimodal, reasoning-based approach, built upon a foundation of state-of-the-art geospatial AI models, can unlock insights that are intractable with siloed analysis alone.</p><p data-block-key=\"2jlht\">We are just beginning to explore the full potential of Earth AI and are committed to expanding access in order to help the global community address the planet’s most pressing challenges. For example:</p><ul><li data-block-key=\"1a7l8\"><a href=\"https://x.company/projects/bellwether/\" target=\"_blank\" rel=\"noopener noreferrer\">Bellwether</a>, a Google X moonshot, is using our weather forecasts, Population Dynamics Foundations embeddings, satellite image analysis and property databases to predict building damage before a storm strikes, helping their insurance clients pay claims faster so homeowners can start rebuilding sooner — saving them time, money and stress.</li><li data-block-key=\"25e3m\">United Nations Global Pulse uses Earth AI Imagery models to <a href=\"https://www.unglobalpulse.org/ai-from-google-research-and-un-boosts-humanitarian-disaster-response-wider-coverage-faster-damage-assessments/\" target=\"_blank\" rel=\"noopener noreferrer\">assess damage after natural disasters</a>, enabling governments and international organizations to rapidly respond to crises.</li><li data-block-key=\"bvpp3\"><a href=\"https://www.givedirectly.org/\" target=\"_blank\" rel=\"noopener noreferrer\">GiveDirectly</a> is using Geospatial Reasoning with our flood forecasts to identify at-risk communities and <a href=\"https://www.givedirectly.org/flood-forecast-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">send cash aid</a> to help households prepare for and mitigate disaster.</li></ul><p data-block-key=\"36bsf\">In addition to supporting UN Global Pulse, GiveDirectly, and other organizations using Earth AI, <a href=\"http://google.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Google.org</a> is providing funds to partners like <a href=\"https://www.khushibaby.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Khushi Baby</a>, <a href=\"https://coopersmith.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Cooper/Smith</a>, <a href=\"https://www.directrelief.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Direct Relief</a> and <a href=\"http://froncort.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Froncort.ai</a> who are utilizing Population Dynamics Foundations to model infectious diseases and improve public health action globally. New enterprise users of Earth AI include <a href=\"https://www.publicstorage.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Public Storage</a>, <a href=\"https://carto.com/\" target=\"_blank\" rel=\"noopener noreferrer\">CARTO</a> and <a href=\"https://visionaespacial.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Visiona Space Technology</a> (part of <a href=\"https://www.embraer.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Embraer</a>).</p><p data-block-key=\"e60f0\">We want to hear how Earth AI might be helpful to you. We encourage organizations to <a href=\"https://forms.gle/VmdqBHrMah6g9z948\" target=\"_blank\" rel=\"noopener noreferrer\">express interest</a> in getting early access to Remote Sensing Foundations (available as <a href=\"https://console.cloud.google.com/vertex-ai/model-garden/google/earth-ai\" target=\"_blank\" rel=\"noopener noreferrer\">Imagery models in Vertex AI</a>), Population Dynamics Foundations, and Geospatial Reasoning.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "可验证的量子优势 (原标题: A verifiable quantum advantage)",
      "link": "https://research.google/blog/a-verifiable-quantum-advantage/",
      "pubDate": "Tue, 21 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-21T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 可验证的量子优势：量子回声算法与OTOC\n\n### 引言：混沌系统与量子计算的挑战\n\n自然界中充满了混沌现象，其特点是系统对微小扰动具有高度敏感性。无论是宏观世界（如天气模式、人口动态）还是量子系统（如原子核磁化、高温超导体中的电子流），混沌都普遍存在。模拟量子混沌系统对经典计算而言极具挑战性，因为计算成本呈指数级增长，这使得量子计算机成为实现量子优势的理想平台。虽然2019年曾通过对高度混沌量子态的比特串采样展示了超越经典的量子计算，但由于比特串在大型量子系统中不重复出现，其实用性有限。\n\n### 量子回声算法与时序关联器（OTOC）\n\n在《自然》杂志封面文章“在量子遍历边缘观察到建设性干涉”中，我们介绍并实验演示了一种名为“量子回声”的量子算法。该算法的核心是测量一个量子可观测量的期望值，即“时序关联器”（Out-of-Time-Order Correlator, OTOC）。OTOC及其高阶推广是一类描述量子动力学如何变得混沌的新型可观测量。与比特串不同，电流、速度、磁化强度和密度等量子期望值是可验证的计算结果，在不同的量子计算机上运行时保持不变。期望值的广泛相关性及其可验证性，为使用OTOC解决经典计算机无法解决的实际问题指明了道路。值得注意的是，在Willow量子芯片上运行量子回声算法，对于一组基准量子电路而言，已经达到了超越经典的范畴。\n\n### OTOC的实验实现与多体干涉\n\n在实践中，OTOC代表了一系列量子操作结束后单个量子比特的状态。在Willow上运行量子回声的实验中，共有103个量子比特经历了随机量子电路形式的“正向”（U）和“反向”（U†）演化。对所有量子比特相互独立的状态施加正向演化，会使系统进入一个高度混沌且所有量子比特之间都存在量子关联的状态。在两次时间演化之间，对一个量子比特施加一个扰动（单量子比特操作B）。接着是另一个探测（单量子比特操作M）。重复此过程一到两次，即可得到一阶或二阶OTOC。在没有B的情况下，正向和反向演化会将系统恢复到初始的独立量子比特状态。而包含扰动B则会引发“蝴蝶效应”：经过这种受扰动的正向和反向演化后，整个系统最终会进入一个与初始状态截然不同的、所有量子比特之间都存在量子关联的混沌状态。\n\n我们从实验中获得的一个关键见解是，高阶OTOCs表现出复杂的量子干涉效应，类似于传统干涉仪。这被称为多体干涉，意味着许多粒子的量子态相互干涉，就像水波可能相互干涉一样，导致复杂的整体效应。在这里，扰动B和M充当了修改系统轨迹的不完美镜子。由于“往返”演化次数的增加，高阶OTOCs对扰动变得更加敏感，轨迹在B和M之间反弹。当满足共振条件（即演化U†是U的精确逆运算）时，干涉是建设性的，它会放大混沌态中存在的量子关联子集。更具体地说，这种干涉测量揭示了演化U如何在施加操作B和M的两个量子比特之间产生关联。它可以用作表征U演化的敏感工具。\n\n![OTOC-1](https://storage.googleapis.com/gweb-research2023-media/images/OTOC-1.width-1250.png)\n\n左图：测量不同阶数k的OTOC的量子电路。量子比特从基态开始，其中一个量子比特处于|𝜓M〉状态。量子处理器实现复杂的许多体演化（U），包括应用于二维网格上相邻量子比特的单量子比特和双量子比特操作。在用门B扰动一个量子比特后，演化被反转（U†），然后对最初准备的量子比特|𝜓M〉进行探测操作M。这重复k次，然后测量另一个量子比特M。右图：不同阶数OTOC作为干涉仪的概念表示。\n\n### 量子优势的关键：信号放大与计算鸿沟\n\n多体干涉的干涉性质带来了两个对实现量子优势至关重要的结果。\n\n1.  **信号放大与效率提升**\n\n    首先，正向和反向演化部分逆转了混沌效应，并放大了最终测量的量子信号。我们观察到了这种放大在OTOC信号中的特征。更具体地说，OTOC信号幅度（由随机电路集合中OTOC值分布的宽度表征）随时间呈负幂律衰减，而没有反向演化的量子信号则呈指数衰减。OTOC的缓慢幂律衰减表明，在量子计算机上测量这些量比经典模拟效率显著更高，因为经典模拟的成本随时间呈指数增长。\n\n    ![OTOC-2](https://storage.googleapis.com/gweb-research2023-media/images/OTOC-2.width-1250.png)\n\n    左图：没有时间反演（灰色）和有时间反演（品红色、蓝色、绿色）测量的信号随时间变化的值。纵轴显示了相关函数C(1)以及一阶/二阶OTOCs C(2)和C(4)在随机电路上的标准差。右图：在Willow设备上测量的一组二阶OTOC值，估计在Frontier超级计算机上模拟每个数据点需要3.2年。横轴标记了随机电路的实例。本次实验使用了105个可用量子比特中的65个。\n\n2.  **经典计算的复杂性障碍**\n\n    多体干涉的第二个结果是经典复杂性。量子计算的核心任务是识别量子计算机和经典计算机在特定计算任务上的计算成本差距。我们通过两种方式解决了这个问题：(1) 通过理论分析和实验相结合，揭示了已知经典算法在实现与我们在Willow上进行的OTOC计算相同结果方面的根本障碍；(2) 通过直接实现和成本估算，测试了九种相关经典模拟算法的性能。\n\n    在第一种方法中，我们发现量子干涉是经典计算的障碍。量子力学的一个显著特征是，预测实验结果需要分析概率幅，而不是像经典力学那样分析概率。一个著名的例子是光的纠缠，它表现为光子（光的基元粒子）之间持续很长距离的量子关联，或超导电路中的宏观量子隧穿现象。我们二阶OTOC数据中的干涉（即经过两次反向和正向电路循环的OTOC）揭示了概率和概率幅之间的类似区别。至关重要的是，概率是非负数，而概率幅可以是任意符号，并由复数描述。总而言之，这些特征意味着它们包含的信息集合要复杂得多。我们的实验不是一对光子或一个超导结，而是由65个量子比特的指数级大空间中的概率幅来描述的。对这样一个量子力学系统进行精确描述需要存储和处理内存中2^65个复数，这超出了超级计算机的能力。此外，我们电路中的量子混沌确保了每个概率幅都同等重要，因此使用压缩描述系统的算法所需的内存和处理时间也超出了超级计算机的能力。\n\n    我们进一步的理论和实验分析表明，要通过数值计算预测我们的实验数据，必须仔细考虑概率幅的符号。这给一类高效的经典算法（量子蒙特卡洛）带来了重大障碍，这些算法在描述大型量子力学空间中的量子现象（例如液氦-4的超流性）方面取得了成功。这些算法依赖于概率描述，但我们的分析表明，这种方法会导致计算输出中出现无法控制的误差。我们对依赖于压缩表示和高效量子蒙特卡洛的算法的直接实现证实了预测二阶OTOC数据的不可行性。我们在Willow上的实验大约耗时2小时，而经典超级计算机估计需要13,000倍的时间来完成这项任务。这一结论是在估计投入10个人年进行经典红队测试我们的量子结果，并实现了总共九种经典模拟算法之后得出的。\n\n### OTOC电路的实际应用：哈密顿量学习\n\n在确立了OTOC超越经典的复杂性之后，我们开始探索如何将其应用于解决具有实际意义的现实世界问题。为此，我们提出了哈密顿量学习方案，即量子计算机模拟自然界中物理系统（如分子）的OTOC信号，这些系统的参数尚未完全已知。然后，我们将量子计算机的OTOC信号与有关物理系统的真实世界数据进行比较，并观察它们何时最吻合。通过寻找这种吻合，我们旨在获得比其他技术更精确的系统参数估计。为了使该方案实用化，我们必须找到自然界中可以执行我们的量子回声算法的系统，并在我们的量子硬件上模拟这些系统。作为实现这一目标的一步，在“通过多体核自旋回声进行分子几何的量子计算”中，我们展示了使用核磁共振（NMR）光谱学验证了这一概念。在NMR中，人们利用核自旋在强磁场中的进动来了解分子和材料的结构，例如人体中的蛋白质或手机中的电池组件。核自旋遵循量子力学定律，在某些条件下（即在固体或类固体材料中）它们表现出上述相同的量子混沌行为。这使得它们成为OTOC协议的完美候选者。\n\n在这份将提交同行评审的预印本中，我们在加州大学伯克利分校派恩斯磁共振中心测量了溶解在液晶中的两种有机分子的OTOCs。然后，该实验在我们的Willow芯片上进行了模拟，从而改进了分子结构的模型。由于模拟真实世界系统的固有复杂性以及我们当前芯片的性能限制，这一初步演示尚未超越经典。然而，我们的结果展示了对分子细节的敏感性，我们相信这条道路将带来量子计算的首批有用应用。\n\n![OTOC-3](https://storage.googleapis.com/gweb-research2023-media/images/OTOC-3.width-1250.png)\n\n通过量子计算机改进物理量子系统知识的示意图，称为哈密顿量学习。\n\n### 结论\n\n我们进行了首次量子计算实验，测量了一个既可以通过另一台量子计算机或自然量子系统进行验证，又超越了已知经典算法模拟能力的量子可观测量。这项实验得益于我们最近的硬件进步，为量子计算机在探测分子等物理系统的微观结构方面的首次实际应用铺平了道路。\n\n### 致谢\n\n这项工作涉及量子AI团队的许多成员，以及Google DeepMind和加州大学伯克利分校、达特茅斯学院、QSimulate和NVIDIA的外部合作者。",
      "shortSummary": "研究人员提出并实验验证了“量子回声”算法，通过测量可验证的“时序关联器”（OTOC）期望值，实现了超越经典计算的量子优势。OTOC能描述量子混沌，其结果在Willow量子芯片上比超级计算机快13,000倍。该算法利用多体干涉放大量子信号，并因量子概率幅的复杂性而使经典模拟面临根本障碍。这项突破为量子计算机在哈密顿量学习等实际应用中探测分子微观结构铺平了道路，有望改进分子结构模型。",
      "translated_title": "可验证的量子优势",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/OTOC-1.width-1250.png",
          "alt": "OTOC-1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/OTOC-2.width-1250.png",
          "alt": "OTOC-2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/OTOC-3.width-1250.png",
          "alt": "OTOC-3",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"9rtfc\">Nature is brimming with chaos, a phenomenon characterized by the high sensitivity of a system toward small perturbations. In the macroscopic world, notable examples of chaotic systems include weather patterns, wherein a small change in initial conditions leads to vastly different outcomes over time (often dubbed “the <a href=\"https://en.wikipedia.org/wiki/Butterfly_effect\" target=\"_blank\" rel=\"noopener noreferrer\">butterfly effect</a>”), and population dynamics, where small shifts in local populations may eventually affect the entire ecosystem. Chaos is similarly abundant in quantum systems, with examples including the <a href=\"https://en.wikipedia.org/wiki/Residual_dipolar_coupling\" target=\"_blank\" rel=\"noopener noreferrer\">dynamics of magnetization of atomic nuclei</a> when subjected to a time-varying magnetic field, and the <a href=\"https://en.wikipedia.org/wiki/Fermi_liquid_theory#Non-Fermi_liquids\" target=\"_blank\" rel=\"noopener noreferrer\">flow of electrons in high-temperature superconductors</a>.</p><p data-block-key=\"50aui\">Simulating quantum-chaotic systems is challenging for classical computation due to exponentially scaling computational cost, making quantum computers ideal for achieving quantum advantage. In 2019, we demonstrated the first <a href=\"https://research.google/blog/quantum-supremacy-using-a-programmable-superconducting-processor/\">beyond-classical quantum computation</a> by sampling bitstrings from a highly chaotic quantum state of <a href=\"https://en.wikipedia.org/wiki/Qubit\" target=\"_blank\" rel=\"noopener noreferrer\">qubits</a>. However, this <a href=\"https://research.google/blog/validating-random-circuit-sampling-as-a-benchmark-for-measuring-quantum-progress/\">random circuit sampling</a> approach has limited practical utility since the same bitstring never appears twice in a large quantum system, restricting its ability to reveal useful information.</p><p data-block-key=\"e7hrb\">In “<a href=\"https://doi.org/10.1038/s41586-025-09526-6\" target=\"_blank\" rel=\"noopener noreferrer\">Observation of constructive interference at the edge of quantum ergodicity</a>”, featured on the cover of <a href=\"https://www.nature.com/articles/s41586-025-09526-6\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature</i></a>, we introduce and experimentally demonstrate a quantum algorithm which we call Quantum Echoes. The heart of the algorithm is measuring the expectation value of a quantum <a href=\"https://en.wikipedia.org/wiki/Observable#:~:text=In%20physics%2C%20an%20observable%20is,of%20the%20space%20in%20question.\" target=\"_blank\" rel=\"noopener noreferrer\">observable</a>, called the <a href=\"http://www.scholarpedia.org/article/Out-of-time-order_correlations_and_quantum_chaos\" target=\"_blank\" rel=\"noopener noreferrer\">out-of-time-order correlator</a> (OTOC). OTOC and its higher order generalizations are a new family of observables that describe how quantum dynamics become chaotic. Unlike bitstrings, quantum expectation values, e.g., current, velocity, magnetization and density, are verifiable computational outcomes that remain the same when run on different quantum computers. The wide relevance of expectation values combined with their verifiability indicates a direct path toward using OTOCs to solve real-world problems using quantum computers, which are not possible to solve on classical computers. Remarkably, we show that running the Quantum Echoes algorithm on the <a href=\"https://blog.google/technology/research/google-willow-quantum-chip/\" target=\"_blank\" rel=\"noopener noreferrer\">Willow</a> quantum chip is already in the beyond-classical regime for a set of benchmarking quantum circuits.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Out-of-time-order correlator</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9rtfc\">In practice, OTOC represents the state of a single qubit at the end of a series of quantum operations. In our experiments running Quantum Echoes on <a href=\"https://blog.google/technology/research/google-willow-quantum-chip/\" target=\"_blank\" rel=\"noopener noreferrer\">Willow</a>, a total of 103 qubits underwent both “forward” (<i>U</i>) and “backward” (<i>U</i><sup class=\"superscript\">†</sup>) evolutions in the form of random quantum circuits. A forward evolution applied to a state where all qubits are independent from each other brings the system to a highly chaotic state with quantum correlations across all qubits. A perturbation, a one-qubit operation <i>B</i>, is applied to a qubit in between the two time evolutions. This circuit is followed by another probe, a one-qubit operation <i>M</i>. Repeating this process once or twice leads to an OTOC of first or second order. In absence of <i>B</i> the forward and backward evolution returns the system to the initial state, where all qubits are independent. Inclusion of the perturbation <i>B</i> sets off a butterfly effect: after such perturbed forward and backward evolution, the whole system ends in a chaotic state with quantum correlations across all qubits that is very different from the initial state.</p><p data-block-key=\"dh297\">A crucial insight we obtained from our experiments is that higher-order OTOCs exhibit complex quantum interference effects analogous to a traditional <a href=\"https://en.wikipedia.org/wiki/Interferometry\" target=\"_blank\" rel=\"noopener noreferrer\">interferometer</a>. This is known as many-body interference, meaning the quantum states of many particles interfere with each other, much like waves of water might interfere, leading to complex overall effects. Here the perturbations, <i>B</i> and <i>M</i>, act as imperfect mirrors that modify the system’s trajectories. Higher order OTOCs become more sensitive to the perturbation due to increasing number of “round trip” evolutions, where the trajectories bounce off of <i>B</i> and <i>M</i>. When a <a href=\"https://en.wikipedia.org/wiki/Resonance\" target=\"_blank\" rel=\"noopener noreferrer\">resonance condition</a> is satisfied, which corresponds to evolution <i>U</i><sup class=\"superscript\">†</sup> being the exact inverse of <i>U</i>, the interference is constructive and it amplifies the subset of quantum correlations from the totality of those present in the chaotic state. More specifically, this interferometry reveals how the evolution <i>U</i> generates correlations between the two qubits where operations <i>B</i> and <i>M</i> were applied. It can be used as a sensitive instrument to characterize the evolution of <i>U</i>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/OTOC-1.width-1250.png\" alt=\"OTOC-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/OTOC-1.width-1250.png\" alt=\"OTOC-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"pqe83\"><b><i>Left</i></b><i>: Quantum circuit measuring OTOCs of different orders, k. Qubits are initiated in the ground state, with one qubit in the state denoted by |𝜓M〉. A complex many-body evolution (U) is implemented by the quantum processor, consisting of one- &amp; two-qubit operations applied to neighboring qubits on a two-dimensional grid. The evolution is reversed (U</i><i><sup class=\"superscript\">†</sup></i><i>) after perturbing one qubit with gate B, followed by a probe operation M on the initially prepared qubit |𝜓M〉. This repeats k times before measuring another qubit M.</i> <b><i>Right</i></b><i>: Conceptual representation of OTOCs of different order as interferometers.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"9rtfc\">The interference nature of the OTOC leads to two consequences crucial for attaining quantum advantage. First, the forward and backward evolutions partially reverse the effects of chaos and amplify the quantum signal measured at the end. We observed the signature of this amplification in OTOC signals. More specifically, OTOC signal magnitude, characterized by the width of the distribution of OTOC values over the ensemble of random circuits, scales as a negative <a href=\"https://en.wikipedia.org/wiki/Power_law\" target=\"_blank\" rel=\"noopener noreferrer\">power</a> of time, whereas quantum signals measured without back evolutions decay exponentially. The slow power law decay of OTOCs suggests that measuring these quantities on a quantum computer is significantly more efficient than classical simulations, where costs increase exponentially over time.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/OTOC-2.width-1250.png\" alt=\"OTOC-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/OTOC-2.width-1250.png\" alt=\"OTOC-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"pqe83\"><b><i>Left</i></b><i>: Time-dependent values of signals measured without time inversion (</i><b><i>gray</i></b><i>) and with time inversion (</i><b><i>magenta</i></b><i>,</i> <b><i>blue</i></b><i>,</i> <b><i>green</i></b><i>). The vertical axis shows standard deviation over random circuits for correlation function, C</i><i><sup class=\"superscript\">(1)</sup></i><i>, and the first/second order OTOCs, C</i><i><sup class=\"superscript\">(2)</sup></i> <i>and C</i><i><sup class=\"superscript\">(4)</sup></i><i>.</i> <b><i>Right</i></b><i>: A set of 2nd order OTOC values measured on a Willow device that are estimated to require 3.2 years to simulate each data point on the</i> <a href=\"https://en.wikipedia.org/wiki/Frontier_(supercomputer)\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Frontier supercomputer</i></a><i>. The horizontal axis labels instances of random circuits. A total of 65 qubits out of the 105 available qubits are used in this experiment.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The computational gap between quantum and classical processors</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9rtfc\">The second consequence of many-body interference is classical complexity. A central task for quantum computing is to identify the computational cost gap between quantum and classical computers on specific computational tasks. We approached this in two ways: (1) through a combination of theoretical analysis and experiments, we revealed the fundamental obstacles to known classical algorithms in achieving the same outcome as our OTOC calculations on Willow, and (2) we tested the performance of nine relevant classical simulation algorithms by direct implementation and cost estimation.</p><p data-block-key=\"bngck\">In the first approach we identified that quantum interference is an obstacle for classical computation. A distinct characteristic of quantum mechanics is that predicting an outcome of an experiment requires analyzing <a href=\"https://en.wikipedia.org/wiki/Probability_amplitude\" target=\"_blank\" rel=\"noopener noreferrer\">probability amplitudes</a> rather than probabilities as in classical mechanics. A well known example is the entanglement of light that manifests in quantum correlations between photons, elementary particles of light, that persist over long distances (<a href=\"https://www.nobelprize.org/prizes/physics/2022/summary/\" target=\"_blank\" rel=\"noopener noreferrer\">2022 Physics Nobel Laureates</a>) or macroscopic quantum tunneling phenomena in superconducting circuits (<a href=\"https://blog.google/inside-google/company-announcements/googler-michel-devoret-awarded-the-nobel-prize-in-physics/\" target=\"_blank\" rel=\"noopener noreferrer\">2025 Physics Nobel Laureates</a>).</p><p data-block-key=\"86a3m\">The interference in our second order OTOC data (i.e., an OTOC that runs through the backward and forward circuit loop twice) reveals a similar distinction between probabilities and probability amplitudes. Crucially, probabilities are non-negative numbers, whereas probability amplitudes can be of an arbitrary sign and are described by complex numbers. Taken together, these features mean they contain a much more complex collection of information. Instead of a pair of photons or a single superconducting junction, our experiment is described by probability amplitudes across an exponentially large space of 65 qubits. An exact description of such a quantum mechanical system requires storing and processing 2<sup class=\"superscript\">65</sup> complex numbers in memory, which is beyond the capacity of supercomputers. Moreover, quantum chaos in our circuits ensures that every amplitude is equally important, and therefore algorithms using a compressed description of the system require memory and processing time beyond the capacity of supercomputers.</p><p data-block-key=\"4e84g\">Our further theoretical and experimental analysis revealed that carefully accounting for the signs of the probability amplitudes is necessary to predict our experimental data by a numerical calculation. This presents a significant barrier for a class of efficient classical algorithms, <a href=\"https://en.wikipedia.org/wiki/Quantum_Monte_Carlo\" target=\"_blank\" rel=\"noopener noreferrer\">quantum Monte Carlo</a>, that have been successful at describing quantum phenomena in a large quantum mechanical space (e.g., <a href=\"https://en.wikipedia.org/wiki/Superfluid_helium-4\" target=\"_blank\" rel=\"noopener noreferrer\">superfluidity of liquid Helium-4</a>). These algorithms rely on description in terms of probabilities, yet our analysis demonstrates that such approaches would result in an uncontrollable error in the computation output.</p><p data-block-key=\"br31a\">Our direct implementation of algorithms relying on both compressed representation and efficient quantum Monte Carlo confirmed the impossibility of predicting second-order OTOC data. Our experiments on Willow took approximately 2 hours, a task estimated to require 13,000 times longer on a classical supercomputer. This conclusion was reached after an estimated 10 person years spent in classical red teaming of our quantum result, implementing a total of nine classical simulation algorithms as a result.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Practical application of OTOC circuits</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9rtfc\">Having established the beyond-classical complexity of OTOCs, we began exploring how they could be applied to solving real-world problems of practical interest. To this end, we proposed <a href=\"https://en.wikipedia.org/wiki/Inverse_problem\" target=\"_blank\" rel=\"noopener noreferrer\">Hamiltonian learning</a>, a scheme where the quantum computer simulates OTOC signals from a physical system in nature, such as molecules, whose system parameters are not fully known. Then, we compare the quantum computer OTOC signals against real-world data about the physical system and observe when they best agree. By looking for this agreement, we aim to obtain a more precise estimate of system parameters than what is possible through other techniques.</p><p data-block-key=\"3dgr4\">To make this scheme practical, we have to find systems in nature that can perform our Quantum Echoes algorithm, and simulate these systems on our quantum hardware. As a step toward this goal, in \"<a href=\"https://quantumai.google/static/site-assets/downloads/quantum-computation-molecular-geometry-via-nuclear-spin-echoes.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Quantum computation of molecular geometry via many-body nuclear spin echoes</a>”, we show that we tested this concept using <a href=\"https://en.wikipedia.org/wiki/Nuclear_magnetic_resonance\" target=\"_blank\" rel=\"noopener noreferrer\">nuclear magnetic resonance</a> (NMR) spectroscopy. In NMR, one uses the precession of nuclear spins in a large magnetic field to learn the structure of molecules and materials, like the proteins in your body or the battery components in your phone. Nuclear spins obey the laws of quantum mechanics, and under certain conditions (namely in solids or solid-like materials) they demonstrate the same quantum-chaotic behavior described above. This makes them a perfect candidate for the OTOC protocol.<br></p><p data-block-key=\"dfme5\">In this pre-print, which will be submitted for peer review, we measured OTOCs on two organic molecules dissolved in liquid crystal at the <a href=\"https://pmrc.berkeley.edu/\" target=\"_blank\" rel=\"noopener noreferrer\">Pines Magnetic Resonance Center</a> at UC Berkeley. This experiment was then simulated on our Willow chip, resulting in improved models of the molecular structure. Due to the inherent complexity of simulating real-world systems and performance limits of our current chip, this initial demonstration is not yet beyond classical. However, our results demonstrate sensitivity to molecular details and we're confident that this path will lead to some of the first useful applications of quantum computation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/OTOC-3.width-1250.png\" alt=\"OTOC-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/OTOC-3.width-1250.png\" alt=\"OTOC-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"pqe83\"><i>A schematic for refining knowledge of a physical quantum system through the quantum computer, known as Hamiltonian learning.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9rtfc\">We have performed the first quantum computing experiment measuring a quantum observable that is both verifiable through another quantum computer or a natural quantum system, and beyond the simulation capacity of known classical algorithms. This experiment was made possible by our recent <a href=\"https://blog.google/technology/research/quantum-hardware-verifiable-advantage/\" target=\"_blank\" rel=\"noopener noreferrer\">hardware advancement</a>, and paves the way toward the first real-world application of quantum computers in probing the microscopic structures of physical systems such as molecules.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"9rtfc\"><i>This work involved many members of the Quantum AI team, along with Google DeepMind and external collaborators at</i> <a href=\"https://www.berkeley.edu/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>UC Berkeley</i></a><i>,</i> <a href=\"https://home.dartmouth.edu/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Dartmouth College</i></a><i>,</i> <a href=\"https://qsimulate.com/home\" target=\"_blank\" rel=\"noopener noreferrer\"><i>QSimulate</i></a><i>, and</i> <a href=\"https://www.nvidia.com/en-us/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>NVIDIA</i></a><i>.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "一张图片胜过千言（隐私）：连贯合成相册的分层生成 (原标题: A picture's worth a thousand (private) words: Hierarchical generation of coherent synthetic photo albums)",
      "link": "https://research.google/blog/a-pictures-worth-a-thousand-private-words-hierarchical-generation-of-coherent-synthetic-photo-albums/",
      "pubDate": "Sun, 19 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-19T16:00:00.000Z",
      "creator": "Google",
      "summary": "本文介绍了一种创新方法，用于私密地生成连贯的合成相册，以满足现代AI对既保护隐私又具有丰富结构和上下文的多模态数据的需求。\n\n### 差分隐私（DP）与生成式AI的挑战\n\n*   **差分隐私（DP）**：提供严格的数学保证，确保数据集中敏感的个人信息受到保护，即使数据用于分析。\n*   **传统DP的复杂性**：近二十年来，研究人员为各种数据分析和机器学习方法开发了DP版本，但这要求组织对每种分析技术进行私有化，过程复杂、繁琐且易出错。\n*   **生成式AI的解决方案**：像Gemini这样的生成式AI模型提供了一个更简单、高效的方案。它们不需单独修改每种分析方法，而是创建一个原始数据集的单一私有合成版本。\n    *   **合成数据特性**：这种合成数据融合了常见的数据模式，不包含任何个体用户的独特细节。\n    *   **隐私与代表性**：通过使用差分隐私训练算法（如DP-SGD）对生成模型进行微调，确保合成数据集既私密又高度代表真实数据。\n    *   **简化工作流程**：任何标准的、非私有分析技术或建模都可以在这个安全（且高度代表性）的替代数据集上进行，从而简化工作流程。\n*   **现有工作的局限性**：大多数关于私有合成数据生成的研究都集中在简单的输出，如短文本或单个图像。然而，使用多模态数据（图像、视频等）的现代应用依赖于对复杂真实世界系统和行为的建模，这是简单、非结构化文本数据无法充分捕捉的。\n\n### 新方法：连贯合成相册的分层生成\n\n为了解决对丰富、结构化图像数据集的合成版本需求，研究人员引入了一种私密生成合成相册的新方法。这项任务带来了超越生成单个图像的独特挑战，特别是需要在一个顺序相册中保持多个照片的主题连贯性和角色一致性。\n\n该方法基于将复杂的图像数据转换为文本，然后再转换回来，并采用分层生成策略。\n\n#### 工作原理\n\n1.  **生成结构化文本表示**：\n    *   用AI模型为相册中的每张照片生成详细的文本描述（caption）。\n    *   使用AI模型为每个相册生成文本摘要（summary）。\n2.  **私有微调大型语言模型（LLMs）**：\n    *   私有微调一对大型语言模型以生成类似的结构化表示。\n    *   第一个模型训练用于生成相册摘要。\n    *   第二个模型训练用于基于相册摘要生成单个照片描述。\n3.  **分层生成结构化表示**：\n    *   首先生成相册的摘要。\n    *   然后，以该摘要为上下文，生成相册中每张照片的详细文本描述。\n4.  **文本到图像转换**：\n    *   将生成的结构化表示（文本）使用文本到图像的AI模型转换为图像集。\n\n![Illustration of our method for generating synthetic photo albums.](https://storage.googleapis.com/gweb-research2023-media/images/PGSD-1-Cartoon.width-1250.png)\n*图示：合成相册生成方法概述。*\n\n#### 中间文本表示的优势\n\n*   **LLM的优势**：文本生成是大型语言模型的主要优势。\n*   **隐私增强**：文本摘要本质上增强了隐私，因为用文本描述图像是一个有损操作，即使没有启用差分隐私，合成照片也不太可能是原始照片的精确副本。\n*   **资源效率**：生成图像的成本远高于生成文本。通过首先生成文本，可以在花费资源生成图像之前根据内容筛选相册。\n\n#### 分层生成策略的优势\n\n*   **内部一致性**：确保相册中的照片内部一致，因为相册中的每张照片描述都是在相同的相册摘要上下文下生成的。\n*   **计算资源节省**：分两步（先相册摘要，后照片描述）生成结构化表示，相对于一步生成所有表示，显著节省了计算资源。由于训练成本随上下文长度呈二次方增长，训练两个上下文较短的模型远比训练一个上下文较长的模型成本低。\n\n#### 文本作为中间体的效用演示（非DP）\n\n通过一个简单的演示（未启用差分隐私，以便进行并排比较），展示了这种方法的强大之处。研究人员提示Gemini用数百个词描述一张图像，然后将描述文本反馈给Gemini，提示它生成一张与描述匹配的图像。这说明了文本作为合成图像生成中间体的实用性。\n\n![Privately generated synthetic photo example](https://storage.googleapis.com/gweb-research2023-media/images/PGSD-2-Example.width-1250.png)\n*图示：左侧为原始图像，右侧为根据Gemini对原始图像的文本描述生成的合成图像。*\n\n### 评估与结果\n\n该方法在YFCC100M数据集上进行了测试，该数据集包含近1亿张在知识共享许可下发布的图像。通过将同一用户在同一小时内拍摄的照片分组，形成了“相册”。为大型语言模型构建了训练集，并确保每个用户对任何训练集的贡献不超过一个示例，以保证差分隐私的有效性。\n\n生成合成相册后，评估了它们与原始相册的相似程度：\n\n1.  **MAUVE分数**：计算了原始和合成结构化表示（相册摘要和照片描述）之间的MAUVE分数，这是一种基于神经嵌入的语义相似性度量。\n    ![MAUVE scores between real and synthetic album summaries & captions](https://storage.googleapis.com/gweb-research2023-media/images/PGSD-3-Mauve.width-1250.png)\n    *图示：真实与合成相册摘要及照片描述之间的MAUVE分数。MAUVE分数越高表示相似度越大。隐私参数ε值越高意味着隐私约束越弱。*\n2.  **常见主题分析**：计算了相册摘要中最常见的主题，发现真实数据和合成数据之间非常相似。\n    ![Real album summaries vs synthetic album summaries](https://storage.com/gweb-research2023-media/images/PGSD-4-Topics.width-1250.png)\n    *图示：真实相册摘要与合成相册摘要中最常见的主题对比。*\n3.  **直接视觉检查**：合成相册的直接视觉检查显示，每个相册通常都围绕一个共同主题，就像真实的相册一样。\n    ![Privately generated synthetic photo albums](https://storage.googleapis.com/gweb-research2023-media/images/PGSD-5-Albums.width-1250.png)\n    *图示：两个私密生成的合成相册示例。每个相册都保持一个特定主题（上：采苹果之旅；下：情侣参观草地）。*\n\n### 结论\n\n现代AI的挑战需要私密、结构化且上下文丰富的数据，而简单的非结构化数据无法满足这一需求。通过将分层、以文本为中间体的方法应用于生成连贯合成相册的艰巨任务，研究人员成功展示了将合成数据的好处扩展到简单文本或孤立图像之外的途径。\n\n这种方法为隐私保护AI创新开辟了激动人心的新途径，有助于解决对大量高质量数据需求与保护用户隐私之间长期存在的矛盾，为关键行业更安全、更普遍的AI发展铺平了道路。",
      "shortSummary": "该研究提出一种分层生成连贯合成相册的新方法，以解决现代AI对私密、结构化多模态数据的需求。传统差分隐私（DP）复杂，而生成式AI通过创建私有合成数据集简化了流程。新方法将图像数据转换为文本描述和相册摘要，利用私有微调的大型语言模型生成新的文本表示，再将其转换为图像。这种方法确保了相册内照片的主题一致性和隐私保护，同时提高了计算效率，为隐私保护AI创新开辟了新途径。",
      "translated_title": "一张图片胜过千言（隐私）：连贯合成相册的分层生成",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PGSD-1-Cartoon.width-1250.png",
          "alt": "Illustration of our method for generating synthetic photo albums.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PGSD-2-Example.width-1250.png",
          "alt": "Privately generated synthetic photo example",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PGSD-3-Mauve.width-1250.png",
          "alt": "MAUVE scores between real and synthetic album summaries & captions",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PGSD-4-Topics.width-1250.png",
          "alt": "Real album summaries vs synthetic album summaries",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/PGSD-5-Albums.width-1250.png",
          "alt": "Privately generated synthetic photo albums",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"e3q59\"><a href=\"https://en.wikipedia.org/wiki/Differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">Differential privacy</a> (DP) provides a powerful, mathematically rigorous assurance that sensitive individual information in a dataset remains protected, even when a dataset is used for analysis. Since DP’s <a href=\"https://link.springer.com/chapter/10.1007/11681878_14\" target=\"_blank\" rel=\"noopener noreferrer\">inception nearly two decades ago</a>, researchers have developed differentially private versions of myriad data analysis and machine learning methods, ranging from calculating simple statistics to <a href=\"https://research.google/blog/protecting-users-with-differentially-private-synthetic-training-data/\">fine-tuning complex AI models</a>. However, the requirement for organizations to privatize every analytical technique can be complex, burdensome, and error-prone.</p><p data-block-key=\"dbfdq\">Generative AI models like <a href=\"https://gemini.google.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a> offer a simpler, more efficient solution. Instead of separately modifying every analysis method, they create a single private synthetic version of the original dataset. This synthetic data is an amalgamation of common data patterns, containing no unique details from any individual user. By using a differentially private training algorithm, such as <a href=\"https://arxiv.org/abs/1607.00133\" target=\"_blank\" rel=\"noopener noreferrer\">DP-SGD</a>, to fine-tune the generative model on the original dataset, we ensure the synthetic dataset is both private and highly representative of the real data. Any standard, non-private analytical technique or modeling can then be performed on this safe (and highly representative) substitute dataset, simplifying workflows. DP fine-tuning is a versatile tool that is particularly valuable for generating high-volume, controlled datasets in situations where access to high-quality, representative data is unavailable.</p><p data-block-key=\"fiueb\">Most published work on private synthetic data generation has focused on <a href=\"https://arxiv.org/pdf/2210.14348\" target=\"_blank\" rel=\"noopener noreferrer\">simple outputs</a> like short text passages or individual images, but modern applications using multi-modal data (images, video, etc.) rely on modeling complex, real-world systems and behaviors, which simple, unstructured text data cannot adequately capture.</p><p data-block-key=\"794po\">We introduce a new method for privately generating synthetic photo albums as a way to address this need for synthetic versions of rich, structured image-based datasets. This task presents unique challenges beyond generating individual images, specifically the need to maintain thematic coherence and character consistency across multiple photos within a sequential album. Our method is based on translating complex image data to text and back. Our results show that this process, with rigorous DP guarantees enabled, successfully preserves the high-level semantic information and thematic coherence in datasets necessary for effective analysis and modeling applications.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How (and why) our method works</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"e3q59\">Our method differs from most other approaches to generating private synthetic image data in two major respects: (1) we use an intermediate text representation and (2) we generate the data hierarchically.</p><p data-block-key=\"mmvg\">Here’s how it works:</p><ol><li data-block-key=\"13lt2\">We generate a structured text representation of each original album, replacing each photo in the album with an AI-generated detailed text caption, and also using an AI model to produce a text summary of each album.</li><li data-block-key=\"ddg4l\">We then privately fine-tune a pair of large language models to produce similar structured representations. The first model is trained to generate album summaries, and the second model is trained to generate individual photo captions based on an album summary.</li><li data-block-key=\"5o70l\">We use the models to generate structured representations of photo albums in a hierarchical manner. For each photo album, we first generate a summary of the album, and then using that summary as context, we generate a detailed text caption of each photo in the album.</li><li data-block-key=\"466tp\">The generated structured representations are then converted into sets of images using a text-to-image AI model.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-1-Cartoon.width-1250.png\" alt=\"Illustration of our method for generating synthetic photo albums.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-1-Cartoon.width-1250.png\" alt=\"Illustration of our method for generating synthetic photo albums.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cnnu1\"><i>Illustration of our method for generating synthetic photo albums.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"e3q59\">Generating text as an intermediate step towards generating images has a number of advantages. First, text generation is the main strength of a large language model. Second, text summarization is inherently privacy enhancing, since describing an image by text is a lossy operation, so synthetic photos are unlikely to be exact copies of the originals, even when differential privacy is not enabled. Finally, generating images is far more costly than generating text, so by first generating text, we can filter albums based on their content before expending resources to produce the images in which we are most interested.</p><p data-block-key=\"1lmil\">Our hierarchical generation strategy ensures that the photos in each album are internally consistent, since each photo caption in an album is generated with the same album summary as context. Also, generating the structured representations in two steps (first the album summaries, and then the photo captions) preserves significant computational resources relative to generating each representation in one shot. Since training cost scales quadratically with context length (due to <a href=\"https://arxiv.org/pdf/2209.04881\" target=\"_blank\" rel=\"noopener noreferrer\">self-attention</a>), training two models with shorter contexts is far less costly than training a single model with a long context.</p><p data-block-key=\"1si9j\">It may seem that describing images with words is too lossy an operation to preserve any interesting characteristics of the original images, but a simple demonstration (without differential privacy, to allow for side-by-side comparison) illustrates the power of this approach. In the figure below, we prompted Gemini to describe an image using several hundred words, and then fed the response text back to Gemini, prompting it to generate an image matching the description. While this circular series of transformations does not satisfy differential privacy, it does illustrate the utility of text as an intermediary for synthetic image generation. As the saying goes, a picture is worth a thousand words — and it seems that it is not worth much more than that!</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-2-Example.width-1250.png\" alt=\"Privately generated synthetic photo example\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-2-Example.width-1250.png\" alt=\"Privately generated synthetic photo example\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cnnu1\"><b><i>Left:</i></b><i> Original image.</i> <b><i>Right:</i></b><i> Synthetic image.</i></p><p data-block-key=\"c1mff\"><i>We asked Gemini to describe the original image in text, and then prompted Gemini to generate the synthetic image based on the text description.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"e3q59\">Concurrent work by <a href=\"https://arxiv.org/abs/2506.07555\" target=\"_blank\" rel=\"noopener noreferrer\">Wang <i>et al</i>.</a> showed how one can leverage text-based intermediaries to generate differentially private single images using <a href=\"https://arxiv.org/abs/2403.01749\" target=\"_blank\" rel=\"noopener noreferrer\">Private Evolution</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Evaluation and results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"e3q59\">We tested our method on the <a href=\"https://arxiv.org/abs/1503.01817\" target=\"_blank\" rel=\"noopener noreferrer\">YFCC100M</a> dataset, a repository containing nearly 100 million images that have been released under the Creative Commons license. We formed “albums” from these images by grouping together photos taken by the same user within the same hour. We constructed training sets for the large language models described above, taking care that no user contributes more than one example to any training set (contribution bounding is necessary to ensure the validity of the differential privacy guarantee).</p><p data-block-key=\"biu12\">After applying our method to generate synthetic photo albums, we evaluated how well they resemble the original albums. First, we computed the <a href=\"https://arxiv.org/abs/2102.01454\" target=\"_blank\" rel=\"noopener noreferrer\">MAUVE score</a>, a neural embedding–based measure of semantic similarity, between the original and synthetic structured representations.</p><p data-block-key=\"eddr7\">The figure below shows the MAUVE scores between real and synthetic album summaries, as well as real and synthetic photo captions, both before and after fine-tuning.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-3-Mauve.width-1250.png\" alt=\"MAUVE scores between real and synthetic album summaries &amp; captions\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-3-Mauve.width-1250.png\" alt=\"MAUVE scores between real and synthetic album summaries &amp; captions\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cnnu1\"><b><i>Left:</i></b><i> MAUVE scores between real and synthetic album summaries.</i> <b><i>Right:</i></b><i> MAUVE scores between real and synthetic photo captions. Higher MAUVE scores indicate greater similarity. Higher values of the privacy parameter ε imply weaker privacy constraints.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"e3q59\">Next, we calculated the most common topics in the album summaries, shown in the table below, and found that they were very similar between real and synthetic data.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-4-Topics.width-1250.png\" alt=\"Real album summaries vs synthetic album summaries\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-4-Topics.width-1250.png\" alt=\"Real album summaries vs synthetic album summaries\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cnnu1\"><b><i>Left:</i></b><i> Most common topics in real album summaries.</i> <b><i>Right:</i></b><i> Most common topics in synthetic album summaries.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"e3q59\">Finally, direct visual examination of the synthetic photos albums shows that each album is typically centered on a common theme, just like real photo albums, as demonstrated by the examples in the figure below.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-5-Albums.width-1250.png\" alt=\"Privately generated synthetic photo albums\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/PGSD-5-Albums.width-1250.png\" alt=\"Privately generated synthetic photo albums\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cnnu1\"><i>Two synthetically-generated photo albums. Each album maintains a specific theme (</i><b><i>top:</i></b><i> apple picking trip;</i> <b><i>bottom:</i></b><i> couple visits a meadow).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"e3q59\">The challenges of modern AI require data that is not only private, but also structurally and contextually rich, a need that simple, unstructured data can’t meet. By applying our hierarchical, text-as-intermediate method to the demanding task of generating coherent synthetic photo albums, we’ve successfully shown a pathway for extending the benefits of synthetic data beyond simple text or isolated images.</p><p data-block-key=\"85ng9\">This methodology opens exciting new avenues for privacy-preserving AI innovation. It helps resolve the persistent tension between the need for large, high-quality data and the imperative to protect user privacy, paving the way for safer and more generalized AI development across critical industries.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"e3q59\"><i>This work is the result of a collaboration between many people at Google Research, including (in alphabetical order by last name): Kareem Amin, Alex Bie, Rudrajit Das, Alessandro Epasto, Weiwei Kong, Alex Kurakin, Natalia Ponomareva, Monica Ribero, Jane Shapiro, Umar Syed, and Sergei Vassilvitskii.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "仅用少量示例教Gemini识别爆发恒星 (原标题: Teaching Gemini to spot exploding stars with just a few examples)",
      "link": "https://research.google/blog/teaching-gemini-to-spot-exploding-stars-with-just-a-few-examples/",
      "pubDate": "Sun, 19 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-19T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 仅用少量示例教Gemini识别爆发恒星\n\n### 引言：天文数据挑战与传统模型局限\n现代天文学是一场宇宙规模的寻宝游戏。全球望远镜每晚扫描天空，寻找像爆发恒星（超新星）这样短暂的事件，这些事件能为我们提供关于宇宙运作的关键见解。这些巡天会生成数百万条潜在发现的警报，但其中绝大多数并非真正的宇宙事件，而是卫星轨迹、宇宙射线撞击或其他仪器伪影造成的“虚假”信号。\n\n多年来，天文学家一直使用专门的机器学习模型，如卷积神经网络（CNN），来筛选这些数据。尽管这些模型有效，但它们通常充当“黑箱”，只提供简单的“真实”或“虚假”标签，而没有解释。这迫使科学家要么盲目信任输出，要么花费无数时间手动验证候选事件——随着下一代望远镜（如Vera C. Rubin天文台）预计每晚生成1000万条警报，这一瓶颈将很快变得难以逾越。\n\n### 创新方法：Gemini的多模态少样本学习\n这一挑战促使我们提出了一个基本问题：一个旨在同时理解文本和图像的通用多模态模型，能否不仅与这些专业模型的准确性相匹配，还能解释它所看到的内容？在《自然天文学》上发表的论文“大型语言模型对瞬态图像分类的文本解释”中，我们证明答案是肯定的。我们展示了Google的Gemini模型如何能够转变为一个专家级的天文学助手，以高精度分类宇宙事件，并且最重要的是，用通俗易懂的语言解释其推理过程。我们通过对Gemini采用少样本学习来实现这一目标，每个巡天仅提供15个带注释的示例和简洁的指令，以准确分类和解释宇宙事件。\n\n**从少量示例中学习的新方法**\n我们没有在数百万张标记图像上训练一个专门模型，而是对一个通用模型使用了少样本学习技术。我们为Gemini提供了三个主要天文巡天（Pan-STARRS、MeerLICHT和ATLAS）的每个巡天仅15个带注释的示例。每个示例包括三张小图像：一张瞬态警报的新图像、一张来自先前观测的同一片天空区域的参考图像，以及一张突出两者之间变化的差分图像。除了这些图像，我们还提供了一组简洁的指令、一份专家编写的简短分类说明，以及一个兴趣评分（例如，对于可能的超新星为“高兴趣”，对于变星为“低兴趣”，对于虚假信号为“无兴趣”），并附带该评分的解释。\n\n该模型必须学会从各种望远镜中分类瞬态事件，每个望远镜都具有不同的分辨率、像素尺度和相机特性。如下图所示，同一天体在这些巡天中可能看起来大相径庭，但Gemini能够从提供的少量示例中进行泛化。\n\n**图片 1: Gemini在不同巡天数据上的泛化能力**\n![ExplodingStars1_Surveys](https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars1_Surveys.width-1250.png)\n*描述：Gemini模型能够在具有不同像素尺度和分辨率的巡天数据中运行。同一瞬态事件在Pan-STARRS（顶部）、MeerLICHT（中部）和ATLAS（底部）三个不同巡天中观测到。每行从左到右依次是新图像、参考图像和差分图像。图像块的像素大小均为100×100，但由于巡天特定的像素尺度（Pan-STARRS：0.25″/像素，MeerLICHT：0.56″/像素，ATLAS：1.8″/像素），其角天空覆盖范围有所不同。*\n\n### 卓越成果：高精度分类与可解释性\n仅凭这些最少的输入指导，我们要求Gemini分类数千个新的警报。该模型在三个数据集中平均达到了93%的准确率，这与需要大量精心策划训练数据集的专业CNN模型相当。\n\n但与传统分类器不同的是，我们提示Gemini不仅输出一个标签，还为每个候选事件生成：\n*   描述其观察到的特征和决策逻辑的文本解释。\n*   帮助天文学家优先安排后续观测的兴趣评分。\n\n这使得模型从一个黑箱转变为一个透明、交互式的合作伙伴。科学家可以阅读解释以理解模型的推理过程，从而建立信任并实现更细致的决策。\n\n**图片 2: Gemini提供可读的瞬态分类和后续优先级**\n![ExplodingStars2_Example](https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars2_Example.width-1250.png)\n*描述：Gemini提供人类可读的瞬态分类和后续优先级。每个示例包括候选瞬态的新图像、参考图像和差分图像，随后是Gemini的分类、文本描述和后续兴趣评分。图中所示示例来自MeerLICHT数据集。*\n\n### 可靠性评估与不确定性量化\n构建一个可靠系统的一个关键步骤是确保其输出的质量。我们召集了一个由12位专业天文学家组成的小组，他们审查了200个Gemini的分类和解释。他们使用一个单一的、锚定的0-5连贯性评估标准（0 = 幻觉，5 = 完全连贯），该标准与文本如何与新/参考/差分图像匹配相关联，并进行了一个简单的“是/可能/否”检查，以确认后续兴趣评分与解释一致。他们将模型的描述评为高度连贯和有用，证实了与专家推理的一致性。\n\n但也许我们最重要的发现是Gemini能够有效评估自身的不确定性。我们提示模型为其自身的解释分配一个“连贯性评分”。我们发现低连贯性评分是分类不正确的有力指标。换句话说，模型擅长告诉我们它何时可能出错。\n\n**图片 3: Gemini的评估结果**\n![ExplodingStars3_Results](https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars3_Results.width-1250.png)\n*描述：左图：12位天文学家对200个MeerLICHT瞬态事件的平均连贯性评分，按平均分排序（蓝色）。大多数示例获得高分（4-5），表明与用户期望高度一致。插图：模型分配的兴趣评分与其自身解释之间的一致性，几乎所有案例都被标记为自洽（即“是”）。右图：按Gemini分类的正确性（TPs和TNs为绿色，FPs和FNs为红色）划分的平均用户连贯性评分。正确分类的示例往往比不正确分类的示例具有更高的连贯性评分。*\n\n### “人机协作”工作流的未来\n这种能力对于构建可靠的“人机协作”工作流来说是一个游戏规则的改变者。通过自动标记其最不确定的案例，系统可以将天文学家的注意力集中在最需要的地方。这创造了一个强大的反馈循环。通过审查标记的案例并将其中一些具有挑战性的示例重新添加到提示中，我们可以迅速提高模型的性能。通过这种迭代过程，我们将模型在MeerLICHT数据集上的准确率从约93.4%提高到约96.7%，展示了系统如何与人类专家合作学习和改进。\n\n### 科学发现的未来展望\n我们相信这种方法标志着科学发现新时代的到来——一个由能够对复杂科学数据集进行推理并用自然语言解释其输出的模型所加速的时代。由于这种方法只需要少量示例和通俗易懂的指令，它有可能在许多不同领域快速适应新的科学仪器、巡天和研究目标。我们设想这项技术将成为科学领域“智能代理助手”的基础。这样的系统可以整合多个数据源，检查自身的置信度，请求后续观测，并仅将最有前景的发现上报给人类科学家。这项工作展示了一条通向与我们共同学习、解释其推理并赋能任何领域研究人员专注于最重要事情——提出下一个伟大问题——的系统的道路。",
      "shortSummary": "一项新研究展示了Google的Gemini模型如何利用少样本学习，仅通过少量示例就能高精度识别爆发恒星等天文瞬态事件。该模型不仅达到93%的平均准确率，与传统专业模型相当，还能提供详细的文本解释和兴趣评分，将“黑箱”分类器转变为透明的交互式伙伴。Gemini还能评估自身不确定性，有效指导天文学家关注关键案例，并通过人机协作循环快速提升性能，预示着科学发现的新时代。",
      "translated_title": "仅用少量示例教Gemini识别爆发恒星",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars1_Surveys.width-1250.png",
          "alt": "ExplodingStars1_Surveys",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars2_Example.width-1250.png",
          "alt": "ExplodingStars2_Example",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars3_Results.width-1250.png",
          "alt": "ExplodingStars3_Results",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"m40sy\">Modern astronomy is a treasure hunt on a cosmic scale. Every night, telescopes around the globe scan the skies, searching for fleeting events like exploding stars (<a href=\"https://en.wikipedia.org/wiki/Supernova\" target=\"_blank\" rel=\"noopener noreferrer\">supernovae</a>) that give us crucial insights into the workings of the universe. These surveys generate millions of alerts about potential discoveries, but there’s a catch: the vast majority are not real cosmic events but \"bogus\" signals from satellite trails, cosmic ray hits, or other instrumental artefacts.</p><p data-block-key=\"4lso\">For years, astronomers have used specialized machine learning models, like <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\" target=\"_blank\" rel=\"noopener noreferrer\">convolutional neural networks</a> (CNNs), to sift through this data. While effective, these models often act as “black boxes,” providing a simple \"real\" or \"bogus\" label with no explanation. This forces scientists to either blindly trust the output or spend countless hours manually verifying candidates — a bottleneck that will soon become insurmountable with next-generation telescopes like the <a href=\"https://rubinobservatory.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Vera C. Rubin Observatory</a>, expected to generate <a href=\"https://www.scientificamerican.com/article/rubin-observatory-data-flood-will-let-the-universe-alert-astronomers-10/\" target=\"_blank\" rel=\"noopener noreferrer\">10 million alerts per night</a>.</p><p data-block-key=\"a4o3a\">This challenge led us to ask a fundamental question: could a general-purpose, multimodal model, designed to understand text and images together, not only match the accuracy of these specialized models but also <i>explain</i> what it sees? In our paper, “<a href=\"https://www.nature.com/articles/s41550-025-02670-z\" target=\"_blank\" rel=\"noopener noreferrer\">Textual interpretation of transient image classifications from large language models</a>”, published in <a href=\"https://www.nature.com/natastron/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature Astronomy</i></a>, we demonstrate that the answer is a resounding yes. We show how Google’s Gemini model can be transformed into an expert astronomy assistant that can classify cosmic events with high accuracy and, crucially, explain its reasoning in plain language. We accomplished this by employing few-shot learning with Gemini, providing it with just 15 annotated examples per survey and concise instructions to accurately classify and explain cosmic events.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"m40sy\">A new approach: Learning from a few examples</h2><p data-block-key=\"blu8p\">Instead of training a specialized model on millions of labeled images, we used a technique called <a href=\"https://www.promptingguide.ai/techniques/fewshot\" target=\"_blank\" rel=\"noopener noreferrer\">few-shot learning</a> on a general-purpose model. We gave Gemini just 15 annotated examples for each of three major astronomical surveys: <a href=\"https://en.wikipedia.org/wiki/Pan-STARRS\" target=\"_blank\" rel=\"noopener noreferrer\">Pan-STARRS</a>, <a href=\"https://science.uct.ac.za/meerlicht/meerlicht\" target=\"_blank\" rel=\"noopener noreferrer\">MeerLICHT</a>, and <a href=\"https://atlas.fallingstar.com/\" target=\"_blank\" rel=\"noopener noreferrer\">ATLAS</a>. Each example consisted of three small images: a <i>new</i> image of the transient alert, a <i>reference</i> image of the same patch of sky from a previous observation, and a <i>difference</i> image that highlights the change between the two. Alongside these images, we provided a concise set of instructions, a short expert-written note explaining the classification, and an interest score (e.g., “high interest” for a likely supernova, “low interest” for a variable star, or “no interest” for a bogus signal) along with an explanation of that score.</p><p data-block-key=\"c449\">The model had to learn to classify transients from a diverse set of telescopes, each with different resolutions, pixel scales, and camera characteristics. As shown below, the same celestial object can appear quite different across these surveys, but Gemini was able to generalize from the few examples provided.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars1_Surveys.width-1250.png\" alt=\"ExplodingStars1_Surveys\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars1_Surveys.width-1250.png\" alt=\"ExplodingStars1_Surveys\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"5qfdw\"><i>Gemini operates across surveys with diverse pixel scales and resolutions. The same transient is observed in three different surveys, with rows corresponding to Pan-STARRS (</i><b><i>top</i></b><i>), MeerLICHT (</i><b><i>middle</i></b><i>) and ATLAS (</i><b><i>bottom</i></b><i>). Each row includes, from</i> <b><i>left</i></b><i> to</i> <b><i>right</i></b><i>, a new image, a reference image and a difference image. The image stamps are all the same size in pixels (100 × 100) but differ in angular sky coverage due to survey-specific pixel scales: Pan-STARRS (0.25\" per pixel), MeerLICHT (0.56\" per pixel) and ATLAS (1.8\" per pixel).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"m40sy\">Guided only by this minimal input, we asked Gemini to classify thousands of new alerts. The model achieved an average accuracy of 93% across the three datasets, which is on par with specialized CNNs that require massive, curated training datasets.</p><p data-block-key=\"f7r54\">But unlike a traditional classifier, we prompted Gemini not just to output a label but also to generate for every candidate:</p><ol><li data-block-key=\"e6h3c\">A <i>textual explanation</i> describing the features it observed and the logic behind its decision.</li><li data-block-key=\"fa8cn\">An <i>interest score</i> to help astronomers prioritize follow-up observations.</li></ol><p data-block-key=\"ao7h5\">This turns the model from a black box into a transparent, interactive partner. Scientists can read the explanation to understand the model’s reasoning, building trust and allowing for more nuanced decision-making.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars2_Example.width-1250.png\" alt=\"ExplodingStars2_Example\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars2_Example.width-1250.png\" alt=\"ExplodingStars2_Example\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"5qfdw\"><i>Gemini provides human-readable transient classifications and follow-up priorities. Each example consists of a new, reference and difference image for a candidate transient, followed by the Gemini classification, textual description and follow-up interest score. The examples shown in the figure are from the MeerLICHT dataset.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"m40sy\">Knowing when to ask for help</h2><p data-block-key=\"ftr31\">A critical step in building a reliable system is ensuring the quality of its output. We assembled a panel of 12 professional astronomers who reviewed 200 of Gemini’s classifications and explanations. Using a single, anchored 0–5 coherence rubric (0 = hallucination, 5 = perfectly coherent) tied to how well the text matched the new/reference/difference images, plus a simple Yes/Maybe/No check that the follow-up interest score agreed with the explanation, they rated the model’s descriptions as highly coherent and useful, confirming alignment with expert reasoning.</p><p data-block-key=\"e74hk\">But perhaps our most important finding was that Gemini can effectively assess its own uncertainty. We prompted the model to assign a “coherence score” to its own explanations. We discovered that low-coherence scores were a powerful indicator of an incorrect classification. In other words, the model is good at telling us when it’s likely to be wrong. The details:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars3_Results.width-1250.png\" alt=\"ExplodingStars3_Results\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ExplodingStars3_Results.width-1250.png\" alt=\"ExplodingStars3_Results\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"5qfdw\"><b><i>Left:</i></b> <i>Average coherence scores from 12 astronomers for 200 MeerLICHT transients, sorted by mean score (</i><b><i>blue</i></b><i>). Most examples received high values (4–5), indicating close alignment with user expectations.</i> <b><i>Inset:</i></b><i> The consistency between the interest score assigned by the model &amp; its own explanation, with nearly all cases marked as self-consistent (i.e., “Yes”).</i> <b><i>Right</i></b><i>: Average user coherence scores, split by the correctness of the classification made by Gemini. Correctly classified examples (TPs &amp; TNs,</i> <b><i>green</i></b><i>) tend to have higher coherence scores than incorrect ones (FPs &amp; FNs,</i> <b><i>red</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"m40sy\">This capability is a game-changer for building reliable \"human-in-the-loop\" workflows. By automatically flagging its most uncertain cases, the system can focus astronomers' attention where it is most needed. This creates a powerful feedback loop. By reviewing the flagged cases and adding a few of these challenging examples back into the prompt, we can rapidly improve the model’s performance. Using this iterative process, we improved the model's accuracy on the MeerLICHT dataset from ~93.4% to ~96.7%, demonstrating how the system can learn and improve in partnership with human experts.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"1ht7a\">The future of scientific discovery</h2><p data-block-key=\"1vu00\">We believe this approach marks a step toward a new era of scientific discovery — one accelerated by models that can both reason over complex scientific datasets and explain their outputs in natural language., but by models that can reason, explain their output, and collaborate with researchers.</p><p data-block-key=\"1n1n2\">Because this method requires only a small set of examples and plain-language instructions, it can potentially be rapidly adapted for new scientific instruments, surveys, and research goals across many different fields. We envision this technology as a foundation for \"agentic assistants\" in science. Such systems could integrate multiple data sources, check their own confidence, request follow-up observations, and escalate only the most promising discoveries to human scientists.</p><p data-block-key=\"fklo3\">This work shows a path toward systems that learn with us, explain their reasoning, and empower researchers in any field to focus on what matters most: asking the next great question.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"1ht7a\">Acknowledgements</h2><p data-block-key=\"djkdj\"><i>This research was a collaborative effort. We extend our sincere thanks to our co-authors Steven Bloemen, Stephen J. Smartt, Paul J. Groot, Paul Vreeswijk, and Ken W. Smith.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "解决虚拟机难题：AI 如何优化云计算 (原标题: Solving virtual machine puzzles: How AI is optimizing cloud computing)",
      "link": "https://research.google/blog/solving-virtual-machine-puzzles-how-ai-is-optimizing-cloud-computing/",
      "pubDate": "Thu, 16 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-16T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 解决虚拟机难题：AI 如何优化云计算\n\n云计算数据中心面临着一个持续的挑战：如何高效地分配处理任务（即虚拟机，VM）。这类似于一个俄罗斯方块游戏，但“方块”的生命周期未知且变化迅速。低效的VM分配会导致“资源搁浅”（浪费容量）和“空闲主机”不足（系统更新和大型VM部署所需）。传统的AI方法通过预测VM生命周期来解决这一经典的装箱问题，但单次预测的错误可能导致效率下降。\n\n### LAVA系统：基于学习分布和适应误预测的生命周期感知VM分配\n\n为了克服这些挑战，研究人员引入了LAVA系统，该系统包含三个核心算法：\n\n*   **非侵入式生命周期感知调度（NILAS）**\n*   **生命周期感知VM分配（LAVA）**\n*   **生命周期感知重新调度（LARS）**\n\n该系统的核心是“持续再预测”机制，它不依赖于VM创建时的一次性生命周期预测，而是持续自动更新VM的预期剩余生命周期预测。\n\n### VM的秘密生命：再预测与概率分布\n\n研究发现，VM的生命周期通常是不可预测的，并遵循长尾分布。例如，绝大多数VM（88%）的生命周期不到一小时，但它们仅消耗总资源的极小部分（2%）。相反，少数长生命周期的VM对整体资源效率影响巨大。\n\n![VM数量和计算份额的相对贡献图](https://storage.googleapis.com/gweb-research2023-media/images/LAVA-2-Contributions.width-1250.png)\n\n*VM生命周期分布（左）与资源消耗（右）。最短作业（0-10分钟，深蓝色）占数量的53%，但资源消耗可忽略不计。相比之下，运行时间最长的作业（>30天，橙色）占资源消耗的18%，但数量占比可忽略不计。*\n\n为了应对这种不确定性，LAVA系统设计了一个机器学习模型，该模型预测VM生命周期的概率分布，而非单一平均值。这种方法受生存分析启发，能够捕捉VM行为的内在不确定性。更重要的是，系统利用此分布持续更新预测。例如，一个VM运行了五天后，系统会重新评估其预期剩余生命周期，随着VM运行时间的增加，预测准确性也随之提高。\n\n![VM生命周期分布图](https://storage.googleapis.com/gweb-research2023-media/images/LAVA-1-Lifetimes.width-1250.png)\n\n*VM生命周期分布。VM调度时，预期（平均）生命周期为0.2天。运行1天后，预期剩余生命周期为4天。运行7天后，预期剩余生命周期为10天。*\n\n### 新型调度算法\n\n基于这种更强大的预测模型，研究人员开发了三种新颖的算法来改进VM分配：\n\n1.  **非侵入式生命周期感知调度（NILAS）**：\n    *   将生命周期预测整合到现有评分函数中，通过优先选择所有VM预计在相似时间退出的主机，旨在创建更多空闲机器。\n    *   由于使用再预测，对初始预测准确性不那么敏感，能够纠正错误。\n    *   已部署在Google的大规模集群管理器Borg上，显著改善了VM分配。\n\n2.  **生命周期感知VM分配（LAVA）**：\n    *   将短生命周期的VM放置在带有一个或多个长生命周期VM的主机上，以填补资源空白。\n    *   目标是让短生命周期的VM迅速退出，而不延长主机的整体生命周期。\n    *   通过在VM超出预期截止日期时增加主机的预期生命周期来主动适应误预测。\n    *   模拟结果表明，该策略能最大限度地减少碎片化并确保主机最终被释放。\n\n3.  **生命周期感知重新调度（LARS）**：\n    *   在碎片整理和维护期间，根据预测的剩余生命周期对主机上的VM进行排序，并优先迁移生命周期最长的VM。\n    *   短生命周期的VM在迁移前自然退出。\n    *   模拟结果显示，LARS有望将所需的总迁移次数减少约4.5%。\n\n### 解决大规模部署的挑战\n\n开发强大的模型和算法只是解决方案的一部分。在大规模环境中可靠地运行它们需要重新思考模型部署方法：\n\n*   **避免循环依赖**：传统的将机器学习模型部署在专用推理服务器上的做法会创建循环依赖，因为这些服务器本身运行在集群调度系统上。解决方案是将模型直接编译到Borg调度器二进制文件中，消除了循环依赖，并确保了模型以与调度器其他代码更改相同的严格流程进行测试和部署。这还带来了额外的好处：模型的中位延迟仅为9微秒（µs），比使用独立模型服务器的方法快780倍，这对于频繁的再预测和性能敏感任务至关重要。\n*   **解决预测瓶颈**：对于最大的区域，所需的预测数量可能成为瓶颈。通过引入主机生命周期评分缓存来解决，该缓存仅在VM添加到或从主机移除时，或当主机的预期生命周期到期时才更新预测。这种缓存机制确保了高性能，并允许系统在整个集群中部署。\n\n### 成果\n\nNILAS算法自2024年初以来已在Google的生产数据中心运行，取得了显著成果：\n\n*   **空闲主机增加**：生产试点和全集群推广显示，空闲主机增加了2.3-9.2个百分点（pp）。这直接与效率相关，因为1个百分点的改进通常相当于节省集群容量的1%。\n*   **资源搁浅减少**：在一些试点实验中，NILAS将CPU搁浅减少了约3%，内存搁浅减少了2%。这意味着主机更多的资源可供新VM使用。\n*   **LAVA模拟**：表明它将在NILAS的基础上进一步提高约0.4个百分点。\n*   **LARS模拟**：表明它有可能将维护所需的VM实时迁移次数减少4.5%。\n\n### 结论\n\n这项工作是迈向机器学习系统日益优化数据中心管理未来的基础性一步。所开发的技术，特别是再预测的使用以及模型和系统的协同设计，具有普适性。研究表明，在不牺牲可靠性或延迟的情况下，将先进的机器学习技术集成到系统基础设施堆栈的最底层是可行的，同时还能实现显著的效率提升。",
      "shortSummary": "Google开发了LAVA系统，利用AI优化云计算中的虚拟机（VM）分配。该系统通过“持续再预测”技术，动态更新VM生命周期预测，解决了传统方法中单次预测不准确的问题。LAVA包含NILAS、LAVA和LARS三种算法，旨在提高资源利用率、减少资源搁浅并优化维护。NILAS已在Google生产环境部署，显著增加了空闲主机并减少了资源搁浅。通过将模型直接编译到调度器中，解决了大规模部署的挑战，确保了低延迟和高可靠性。这项工作为ML驱动的数据中心管理奠定了基础。",
      "translated_title": "解决虚拟机难题：AI 如何优化云计算",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/LAVA-2-Contributions.width-1250.png",
          "alt": "Graph of relative contributions by number and compute fraction.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/LAVA-1-Lifetimes.width-1250.png",
          "alt": "Plot showing distribution of VM lifetimes",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"pt8nw\">Imagine a puzzle game similar to Tetris with pieces rapidly falling onto a stack. Some fit perfectly. Others don’t. The goal is to pack the blocks as tightly and efficiently as possible. This game is a loose analogy to the challenge faced by cloud data centers several times every second as they try to allocate processing jobs (called virtual machines or VMs) as efficiently as possible. But in this case, the “pieces” (or VMs) appear and disappear, some with a lifespan of only minutes, and others, days. In spite of the initially unknown VM lifespans, we still want to fill as much of the physical servers as possible with these VMs for the sake of efficiency. If only we knew the approximate lifespan of a job, we could clearly allocate much better.</p><p data-block-key=\"483c4\">At the scale of large data centers, efficient resource use is especially critical for both economic and environmental reasons. Poor VM allocation can lead to \"resource stranding\", where a server's remaining resources are too small or unbalanced to host new VMs, effectively wasting capacity. Poor VM allocation also reduces the number of \"empty hosts\", which are essential for tasks like system updates and provisioning large, resource-intensive VMs.</p><p data-block-key=\"3s50u\">This classic <a href=\"https://en.wikipedia.org/wiki/Bin_packing_problem\" target=\"_blank\" rel=\"noopener noreferrer\">bin packing problem</a> is made more complex by this incomplete information about VM behavior. AI can help with this problem by using learned models to predict VM lifetimes. However, this often relies on a single prediction at the VM's creation. The challenge with this approach is that a single misprediction can tie up an entire host for an extended period, degrading efficiency.</p><p data-block-key=\"8v1nd\">In “<a href=\"https://arxiv.org/abs/2412.09840v1\" target=\"_blank\" rel=\"noopener noreferrer\">LAVA: Lifetime-Aware VM Allocation with Learned Distributions and Adaptation to Mispredictions</a>”, we introduce a trio of algorithms — non-invasive lifetime aware scoring (NILAS), lifetime-aware VM allocation (LAVA), and lifetime-aware rescheduling (LARS) — which are designed to solve the bin packing problem of efficiently fitting VMs onto physical servers. This system uses a process we call “continuous reprediction”, which means it doesn’t rely on the initial, one-time guess of a VM’s lifespan made at its creation. Instead, the model constantly and automatically updates its prediction for a VM's expected remaining lifetime as the VM continues to run.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The secret life of VMs: Repredictions and probability distributions</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">One of the key insights driving this research is the recognition that VM lifetimes are often unpredictable and follow a <a href=\"https://en.wikipedia.org/wiki/Long_tail\" target=\"_blank\" rel=\"noopener noreferrer\">long-tailed distribution</a>. For example, while the vast majority of VMs (88%) live for less than an hour, these short-lived VMs consume only a tiny fraction (2%) of the total resources. This means that the placement of a small number of long-lived VMs has a disproportionately large impact on overall resource efficiency.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/LAVA-2-Contributions.width-1250.png\" alt=\"Graph of relative contributions by number and compute fraction.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/LAVA-2-Contributions.width-1250.png\" alt=\"Graph of relative contributions by number and compute fraction.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"b4xcm\"><i>Distribution of VM lifetimes of scheduled VMs (</i><b><i>left</i></b><i>) vs. their resource consumption (</i><b><i>right</i></b><i>). Interestingly, the shortest jobs (0–10 min, dark blue), which account for 53% by number, take a negligible fraction of resources. In contrast, the longest running jobs (&gt;30 days, orange), which take considerable resources (18%), amount to a negligible fraction by number.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"pt8nw\">Instead of trying to predict a single, average lifetime, which can be misleading for VMs with bi-modal or highly varied lifespans, we designed an ML model that predicts a probability distribution for a VM's lifetime. This approach, inspired by <a href=\"https://en.wikipedia.org/wiki/Survival_analysis\" target=\"_blank\" rel=\"noopener noreferrer\">survival analysis</a>, allows the model to capture the inherent uncertainty of a VM's behavior.</p><p data-block-key=\"91kfi\">More importantly, our system uses this distribution to continuously update its predictions. We ask, “Given a VM has been running for five days, what is its expected remaining lifetime?” As a VM continues to run, the system gains more information, and its lifetime prediction becomes more accurate. Our algorithms are specifically co-designed to leverage these repredictions, actively responding to mispredictions and improving the accuracy over time.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/LAVA-1-Lifetimes.width-1250.png\" alt=\"Plot showing distribution of VM lifetimes\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/LAVA-1-Lifetimes.width-1250.png\" alt=\"Plot showing distribution of VM lifetimes\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"b4xcm\"><i>Lifetime distribution of VM lifetimes. When the VM is scheduled, the expected (average) lifetime is 0.2 days. After it has run for 1 day, the expected remaining lifetime is 4 days. After 7 days, the expected remaining lifetime is 10 days.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">A new class of scheduling algorithms</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">With this new, more robust prediction model, we developed three novel algorithms to improve VM allocation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">1. Non-Invasive Lifetime Aware Scheduling (NILAS)</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">NILAS is a non-invasive algorithm that incorporates lifetime predictions into an existing scoring function. It ranks potential hosts for a new VM by considering the repredicted exit times of all existing VMs on that host. By prioritizing hosts where all VMs are expected to exit at a similar time, NILAS aims to create more empty machines. Our use of repredictions is less sensitive to prediction accuracy and allows NILAS to correct for errors. The NILAS algorithm has been deployed on our <a href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43438.pdf\">large-scale cluster manager</a>, Borg, where it significantly improves VM allocation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">2. Lifetime-Aware VM Allocation (LAVA)</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">LAVA is a more fundamental departure from existing scheduling mechanisms. While NILAS aims to pack VMs with similar lifetimes, LAVA does the opposite: it puts shorter-lived VMs on hosts with one or more long-lived VMs. The goal is to fill in resource gaps with short-lived VMs that are at least an order of magnitude shorter than the host’s anticipated lifespan, so that they exit quickly without extending the host’s overall lifespan. LAVA also actively adapts to mispredictions by increasing a host’s anticipated lifespan if a VM outlives its expected deadline. Simulations show that this strategy minimizes fragmentation and ensures that hosts are eventually freed up.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">3. Lifetime-Aware Rescheduling (LARS)</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">LARS uses our lifetime predictions to minimize VM disruptions during defragmentation and maintenance. When a host needs to be defragmented, LARS sorts the VMs on that host by their predicted remaining lifetime and migrates the longest-lived VMs first. Shorter-lived VMs exit naturally before migration. Simulations with LARS indicate it has the potential to reduce the total number of migrations required by around 4.5%.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Addressing the challenge of deployment at scale</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">Developing powerful models and algorithms is only one part of the solution. Getting them to work reliably at large scale required us to rethink our approach to model deployment.</p><p data-block-key=\"cj0j9\">A common practice is to serve machine learning models on dedicated inference servers. However, this would have created a <a href=\"https://en.wikipedia.org/wiki/Circular_dependency\" target=\"_blank\" rel=\"noopener noreferrer\">circular dependency</a>, as these servers would themselves run on our cluster scheduling system. A failure in the model serving layer could then cause a cascading failure in the scheduler itself, which is unacceptable for a mission-critical system.</p><p data-block-key=\"34gvd\">Our solution was to compile the model directly into the <a href=\"https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/\">Borg scheduler binary</a>. This approach eliminated the circular dependency and ensured that the model was tested and rolled out with the same rigorous process as any other code change to the scheduler. This also yielded an additional benefit: the model's median latency is just 9 microseconds (µs), which is 780 times faster than a comparable approach that uses separate model servers. This low latency is crucial for running repredictions frequently and for using the model in performance-sensitive tasks, like maintenance and defragmentation.</p><p data-block-key=\"dpkp\">We also found that for our largest zones, the number of required predictions could become a bottleneck. We addressed this by introducing a host lifetime score cache, which only updates predictions when a VM is added or removed from a host, or when a host's expected lifetime expires. This caching mechanism ensures high performance and allows us to deploy our system fleet-wide.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">Our NILAS algorithm has been running in Google's production data centers since early 2024. The results are clear and significant.</p><ul><li data-block-key=\"27kt6\"><i>Increased empty hosts:</i> Our production pilots and fleet-wide rollouts have shown an increase in empty hosts by 2.3–9.2 percentage points (pp). This metric directly correlates with efficiency, as a 1 pp improvement is typically equivalent to saving 1% of a cluster's capacity.</li><li data-block-key=\"arb0l\"><i>Reduced resource stranding:</i> In some pilot experiments, NILAS reduced CPU stranding by approximately 3% and memory stranding by 2%. This means more of a host's resources are available to be used by new VMs.</li></ul><p data-block-key=\"7qme9\">Simulations running LAVA suggest it will provide a further ~0.4 pp improvement over NILAS. Similarly, simulations with LARS indicate that it has the potential to reduce the number of VM live migrations needed for maintenance by 4.5%.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\">We believe this work is a foundational step towards a future where data center management is increasingly optimized by machine learning systems. The techniques we developed, particularly the use of repredictions and the co-design of models and systems, are generalizable to other tasks. We have demonstrated that it is possible to integrate advanced machine learning techniques into the lowest layers of a system’s infrastructure stack without sacrificing reliability or latency, while still delivering significant efficiency gains.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"pt8nw\"><i>LAVA is a large collaborative project that spanned multiple teams across Google, including Google Cloud, Google DeepMind, Google Research, and SystemsResearch@Google. Key contributors include Jianheng Ling, Pratik Worah, Yawen Wang, Yunchuan Kong, Anshul Kapoor, Chunlei Wang, Clifford Stein, Diwakar Gupta, Jason Behmer, Logan A. Bush, Prakash Ramanan, Rajesh Kumar, Thomas Chestna, Yajing Liu, Ying Liu, Ye Zhao, Kathryn S. McKinley, Meeyoung Park, and Martin Maas.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "使用AI通过DeepSomatic识别肿瘤中的基因变异 (原标题: Using AI to identify genetic variants in tumors with DeepSomatic)",
      "link": "https://research.google/blog/using-ai-to-identify-genetic-variants-in-tumors-with-deepsomatic/",
      "pubDate": "Wed, 15 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-15T16:00:00.000Z",
      "creator": "Google",
      "summary": "DeepSomatic是Google Research与加州大学圣克鲁斯基因组学研究所等合作开发的一款人工智能工具，旨在更准确地识别肿瘤细胞中的基因变异。这项工作已发表在《自然生物技术》杂志上，是Google利用AI方法理解和治疗癌症的广泛努力的一部分，目标是加速癌症研究并推进精准医疗。\n\n### 癌症与基因变异的挑战\n*   **癌症的本质**：癌症是一种基因疾病，细胞分裂的遗传控制出现异常。识别肿瘤细胞中的基因突变是研究癌症和制定治疗方案的关键步骤。\n*   **体细胞变异的复杂性**：与DeepVariant（Google早期用于识别遗传性生殖系变异的工具）不同，DeepSomatic专注于识别癌症中更复杂的体细胞变异。这些变异是在出生后获得的，可能由环境暴露或DNA复制错误引起。\n*   **识别难度**：识别体细胞变异比识别遗传变异更困难，因为肿瘤细胞可能包含不同频率的多种变异，且测序错误率可能高于样本中体细胞变异的实际频率。\n\n### DeepSomatic的工作原理与训练\n*   **核心技术**：DeepSomatic是一个灵活的模型，利用卷积神经网络（CNN）来识别肿瘤变异。它适用于所有主要的测序平台，支持不同类型的样本处理，并能将其学习能力扩展到未包含在训练中的癌症类型。\n*   **数据处理流程**：\n    1.  将基因测序数据（来自肿瘤细胞和非癌细胞）转换为一组图像，这些图像代表测序数据、染色体比对、输出质量及其他变量。\n    2.  DeepSomatic的卷积神经网络处理这些图像。\n    3.  区分参考基因组、个体中的非癌生殖系变异以及肿瘤中由癌症引起的体细胞变异，同时排除测序过程中产生的微小错误。\n    4.  最终输出一份癌症相关变异（突变）列表。\n\n    ![DeepSomatic概览](https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-1-Overview.width-1250.png)\n    *DeepSomatic检测基因组数据中的癌症变异。首先，将肿瘤细胞和非癌细胞的测序数据转换为图像。DeepSomatic通过其卷积神经网络处理这些图像，以区分参考基因组、个体中的非癌生殖系变异以及肿瘤中由癌症引起的体细胞变异，同时排除微小的测序错误。结果是癌症引起的变异或突变列表。*\n\n*   **训练数据（CASTLE）**：\n    *   为训练准确模型，研究团队与UC Santa Cruz和美国国家癌症研究所合作，创建了一个新的高质量训练和评估数据集。\n    *   对来自四个乳腺癌样本和两个肺癌样本（研究细胞系）的肿瘤细胞和伴随的正常细胞进行了测序。\n    *   使用Illumina短读长测序、PacBio长读长测序和Oxford Nanopore Technology长读长测序三种领先平台进行全基因组测序。\n    *   结合所有三个平台的输出，以消除平台特异性错误，创建了一个名为“癌症标准长读长评估数据集”（CASTLE）的单一、准确的参考数据集。\n\n    ![突变率图](https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-2-Variants.width-1250.png)\n    *用于训练DeepSomatic的基准数据集。每个条形图显示了在四个乳腺癌样本和两个肺癌样本中发现的突变数量，颜色代表不同类型的突变。肺癌显示出由环境毒素引起的一种显著突变类型，包括图中绿色的SBS4。但即使是同一种癌症，其突变特征也显示出巨大差异。这些个体差异可以预测其对治疗的反应效果。*\n\n*   **“仅肿瘤”模式**：DeepSomatic还能够在没有非肿瘤序列可用的“仅肿瘤”模式下识别体细胞变异，例如在白血病等血癌中，很难从血液样本中获取纯净的正常细胞。\n\n### DeepSomatic的性能表现\n*   **高准确性**：DeepSomatic模型在所有三个主要测序平台上均优于其他现有方法，以更高的准确性识别出更多的肿瘤变异。\n*   **擅长识别插入和缺失（Indels）**：\n    *   在Illumina测序数据上，DeepSomatic在识别Indels方面的F1-score达到90%，而次优方法为80%。\n    *   在Pacific Biosciences测序数据上，DeepSomatic的F1-score超过80%，而次优方法不到50%。\n\n    ![乳腺癌准确性图](https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-3-BreastCancer.width-1250.png)\n    *DeepSomatic（紫色）在研究中广泛使用的乳腺癌样本上的结果，与其他工具进行比较。有几种软件工具可以识别Illumina数据中的癌症变异，而对于PacBio和Oxford Nanopore Technologies生成的长读长测序数据，只有一种替代方案（粉色）。F1-score衡量发现的变异数量和准确性。DeepSomatic在识别单字母基因代码变异（单核苷酸变异）方面表现略好，并在涉及Indels的变异方面显示出显著改进。*\n\n*   **处理复杂样本的能力**：\n    *   在福尔马林固定石蜡包埋（FFPE）样本（一种常见的组织保存方法，会引入DNA损伤）上，DeepSomatic表现出色。\n    *   在全外显子组测序（WES）数据（一种更经济的方法，仅关注基因组中编码蛋白质的约1%）上，DeepSomatic也优于其他工具。\n    *   这表明DeepSomatic可用于分析质量较低或历史肿瘤样本，并适用于仅进行外显子组测序的临床数据。\n\n    ![FFPE和WES准确性图](https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-4-FFPE-WES.width-1250.png)\n    *DeepSomatic在经过更复杂预处理步骤的样本上具有显著更高的准确性，包括：福尔马林固定石蜡包埋（FFPE），一种用于保存组织样本的方法（左），以及全外显子组测序（WES），一种仅对基因组中编码蛋白质的部分进行测序的方法（右）。中间部分显示了一个经过FFPE保存并使用全外显子组测序的样本。*\n\n### 推广到其他癌症类型\n*   **胶质母细胞瘤**：DeepSomatic成功识别了这种侵袭性脑癌的变异，展示了其学习泛化能力。\n*   **儿童白血病**：与堪萨斯城儿童慈善医院合作，DeepSomatic分析了八个已测序的儿童白血病样本，不仅识别了已知的变异，还发现了10个新变异，证明了其在“仅肿瘤”样本上的有效性。\n\n### 未来展望\n*   研究实验室和临床医生有望开始使用DeepSomatic工具。\n*   检测已知癌症变异有助于选择现有治疗方案（如化疗、免疫疗法）。\n*   识别新癌症变异可能促成全新的疗法。\n*   最终目标是更深入地了解每个肿瘤，发现其驱动因素，并为患者提供最有效的治疗。",
      "shortSummary": "DeepSomatic是Google Research推出的一款AI工具，利用卷积神经网络更准确地识别肿瘤中的体细胞基因变异。它能处理多种测序平台数据，并支持肿瘤-正常样本配对及仅肿瘤样本模式。DeepSomatic在识别插入和缺失（Indels）方面表现优异，并能有效分析FFPE和WES等复杂样本。该工具已开源，旨在加速癌症研究，推动精准医疗，帮助临床医生选择现有疗法并发现潜在新疗法。",
      "translated_title": "使用AI通过DeepSomatic识别肿瘤中的基因变异",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-1-Overview.width-1250.png",
          "alt": "Overview of DeepSomatic",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-2-Variants.width-1250.png",
          "alt": "Plot of mutation rates",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-3-BreastCancer.width-1250.png",
          "alt": "Plot of accuracy on breast cancer",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-4-FFPE-WES.width-1250.png",
          "alt": "Plot of accuracy on FFPE & WES",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ry233\">Cancer is fundamentally a genetic disease in which the genetic controls on cell division go awry. Many types of cancer exist, and each poses unique challenges as it can have distinct genetic underpinnings. A powerful way to study cancer, and a critical step toward creating a treatment plan, is to identify the genetic mutations in tumor cells. Indeed, clinicians will now often sequence the genomes of biopsied tumor cells to inform treatment plans that specifically disrupt how that cancer grows.</p><p data-block-key=\"ftn7v\">With partners at the University of California, Santa Cruz <a href=\"https://genomics.ucsc.edu/\" target=\"_blank\" rel=\"noopener noreferrer\">Genomics Institute</a> and other federal and academic researchers, our new paper, “<a href=\"https://www.nature.com/articles/s41587-025-02839-x\" target=\"_blank\" rel=\"noopener noreferrer\">DeepSomatic: Accurate somatic small variant discovery for multiple sequencing technologies</a>” in <a href=\"https://www.nature.com/nbt/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature Biotechnology</i></a> presents a tool that leverages machine learning to identify genetic variants in tumor cells more accurately than current methods. DeepSomatic is a flexible model that uses convolutional neural networks to identify tumor variants. It works on data from all major sequencing platforms, for different types of sample processing, and can extend its learning to cancer types not included in training.</p><p data-block-key=\"8b6u7\">We have made both <a href=\"https://github.com/google/deepsomatic\" target=\"_blank\" rel=\"noopener noreferrer\">the tool</a> and the <a href=\"https://github.com/CASTLE-Panel/castle\" target=\"_blank\" rel=\"noopener noreferrer\">high-quality training dataset</a> we created openly available to the research community. This work is part of broader Google efforts to develop AI methods to understand cancer and help scientists treat cancer, including analyzing mammogram images for <a href=\"https://health.google/mammography/\" target=\"_blank\" rel=\"noopener noreferrer\">breast cancer screening</a>, CT scans for <a href=\"https://research.google/blog/computer-aided-diagnosis-for-lung-cancer-screening/\">lung cancer screening</a>, as well as a partnership aimed at using AI to <a href=\"https://blog.google/technology/health/google-ai-institute-womens-cancers/\" target=\"_blank\" rel=\"noopener noreferrer\">advance research on gynecological cancers</a>. Our hope is to speed cancer research and further the goal of precision medicine.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Genetic variation acquired after birth</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\">Genome sequencing is used in research and medical clinics to identify genetic variations between an individual and the <a href=\"https://www.genome.gov/genetics-glossary/Human-Genome-Reference-Sequence\" target=\"_blank\" rel=\"noopener noreferrer\">human reference genome</a>. Distinguishing between real variants and simple errors made during the sequencing process is challenging. That’s why almost a decade ago Google Research introduced <a href=\"https://research.google/blog/deepvariant-highly-accurate-genomes-with-deep-neural-networks/\">DeepVariant</a> to identify inherited variants, also called <a href=\"https://www.genomicseducation.hee.nhs.uk/genotes/knowledge-hub/constitutional-germline-vs-somatic-tumour-variants/\" target=\"_blank\" rel=\"noopener noreferrer\">germline variants</a>, that came from parents and are found in all of the body’s cells.</p><p data-block-key=\"c63af\">The genetics of cancer is more complex. Cancer is often driven by variants acquired after birth. Environmental exposure that damages DNA, such as UV light or chemical carcinogens, as well as random errors that occur during DNA replication, can cause cells in the body, known as somatic cells, to acquire new variants. Sometimes, these acquired variants change the normal behavior of cells, and can cause them to replicate when they shouldn’t. This process drives the initial development of cancer, as well as its later progression to more fast-growing and invasive stages.</p><p data-block-key=\"1tabm\">Identifying variants specific to some of a person’s somatic cells is much harder than identifying inherited variants. Tumor cells can contain a diverse set of acquired variants at different frequencies, and the error rate of sequencing can be higher than the rate a somatic variant is present in a sample.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Training DeepSomatic to spot genetic variation in tumor cells</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\">We developed <a href=\"https://github.com/google/deepsomatic\" target=\"_blank\" rel=\"noopener noreferrer\">DeepSomatic</a> to address these challenges and accurately identify somatic variants. In most clinical and research settings, cancer is studied by sequencing the tumor cells acquired through biopsy, as well as normal cells that are unaffected by the tumor growth and contain more typical inherited genetic variations. DeepSomatic is trained to identify variations observed in tumor cells that are not inherited variants. These types of variations can provide critical insights about which variations are driving the tumor growth. DeepSomatic is also able to identify somatic variation in tumor-only mode where a non-tumor sequence is not available, for example in a blood cancer like leukemia where it is hard to get only normal cells from a blood draw. The ability to extend to different types of use-cases that follow common ways clinicians and researchers study cancer makes DeepSomatic applicable to many research and clinical settings.</p><p data-block-key=\"ae6vl\">Like our earlier tool, <a href=\"https://research.google/blog/deepvariant-highly-accurate-genomes-with-deep-neural-networks/\">DeepVariant</a>, the DeepSomatic model works by first turning genetic sequencing data into a <a href=\"https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/\" target=\"_blank\" rel=\"noopener noreferrer\">set of images</a>. The images represent the sequencing data, alignment along the chromosome, the quality of the output, and other variables. DeepSomatic then uses its <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\" target=\"_blank\" rel=\"noopener noreferrer\">convolutional neural network</a> on data from tumor cells and non-cancerous cells to differentiate between the reference genome, the non-cancer germline variants in that individual, and the cancer-caused somatic variants in the tumor, while discarding variations caused by small errors acquired during the sequencing process. The result is a list of cancer-related variants, or mutations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-1-Overview.width-1250.png\" alt=\"Overview of DeepSomatic\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-1-Overview.width-1250.png\" alt=\"Overview of DeepSomatic\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"48u3i\">DeepSomatic detects cancer variants in genomic data. First, sequencing data from the tumor cells and non-cancerous cells are turned into an image. DeepSomatic passes these images through its convolutional neural network to differentiate between the reference genome, the non-cancer germline variants in that individual, and the cancer-caused somatic variants in the tumor, while discarding variations caused by small sequencing errors. The result is a list of cancer-caused variants, or mutations.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ry233\">Training accurate models that can identify genetic variation for different cancer types requires comprehensive, high-quality data and truth sets. For this work we created a new training and evaluation dataset for detecting variants in tumor cells. With our partners at UC Santa Cruz and the <a href=\"https://www.cancer.gov/\" target=\"_blank\" rel=\"noopener noreferrer\">National Cancer Institute</a>, we sequenced tumor cells and accompanying normal cells from four breast cancer samples and two lung cancer samples from research cell lines.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-2-Variants.width-1250.png\" alt=\"Plot of mutation rates\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-2-Variants.width-1250.png\" alt=\"Plot of mutation rates\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"48u3i\">Benchmark dataset used to train DeepSomatic. Each bar shows the number of mutations found in four breast cancer samples and two lung cancer samples, with color representing different types of mutations. Lung cancer displays a notable type of mutation caused by environmental toxins, including <a href=\"https://signal.mutationalsignatures.com/explore/referenceCancerSignature/63/prevalence\" target=\"_blank\" rel=\"noopener noreferrer\">SBS4</a> shown in green. But even the same type of cancer shows big differences in its mutational signature. These individual differences can predict how well it will respond to a treatment.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ry233\">To create an accurate training dataset, we did whole-genome sequencing of these six samples using three leading platforms: <a href=\"https://www.illumina.com/systems/sequencing-platforms.html\" target=\"_blank\" rel=\"noopener noreferrer\">Illumina’s short-read sequencing</a>, <a href=\"https://www.pacb.com/technology/hifi-sequencing/\" target=\"_blank\" rel=\"noopener noreferrer\">PacBio’s long-read sequencing</a>, and <a href=\"https://nanoporetech.com/platform\" target=\"_blank\" rel=\"noopener noreferrer\">Oxford Nanopore Technology’s long-read sequencing</a>. Output from all three platforms was combined to remove platform-specific errors and create a single, accurate reference dataset we call the <a href=\"https://github.com/CASTLE-Panel/castle\" target=\"_blank\" rel=\"noopener noreferrer\">Cancer Standards Long-read Evaluation dataset</a> (CASTLE) for genetic diversity in tumor and normal cells.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Testing DeepSomatic’s ability to spot cancer-related variants</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\">We trained DeepSomatic on three of the breast cancer genomes and the two lung cancer genomes in the CASTLE reference dataset. We then tested DeepSomatic’s performance in several ways, including on the single breast cancer genome that was not included in its training data, and on chromosome 1 from each sample, which we also excluded from the training.</p><p data-block-key=\"1u39v\">Results show that DeepSomatic models developed for each of the three major sequencing platforms performed better than other methods, identifying more tumor variants with higher accuracy. The tools used for comparison on short-read sequencing data were <a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC4242521/\" target=\"_blank\" rel=\"noopener noreferrer\">SomaticSniper</a>, <a href=\"https://gatk.broadinstitute.org/hc/en-us/articles/360037593851-Mutect2\" target=\"_blank\" rel=\"noopener noreferrer\">MuTect2</a> and <a href=\"https://www.nature.com/articles/s41592-018-0051-x\" target=\"_blank\" rel=\"noopener noreferrer\">Strelka2</a> (with SomaticSniper specifically for single nucleotide variants, or SNVs). For long-read sequencing data we compared against <a href=\"https://www.biorxiv.org/content/10.1101/2023.08.17.553778v1\" target=\"_blank\" rel=\"noopener noreferrer\">ClairS</a>, a deep learning model trained on synthetic data.</p><p data-block-key=\"9ghlt\">In our tests DeepSomatic identified 329,011 somatic variants across the six reference cell lines and a seventh preserved sample. DeepSomatic does particularly well at identifying cancer variations that involve insertions and deletions (“Indels”) of genetic code. For these types of variants, DeepSomatic substantially increased the <a href=\"https://en.wikipedia.org/wiki/F-score\" target=\"_blank\" rel=\"noopener noreferrer\">F1-score</a>, a balanced measure of how well the model finds true variants in a sample (recall) while not making false positives (precision). On Illumina sequencing data the next-best method scored 80% at identifying Indels, while DeepSomatic scored 90%. On Pacific Biosciences sequencing data, the next-best method scored less than 50% at identifying Indels, and DeepSomatic scored more than 80%.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-3-BreastCancer.width-1250.png\" alt=\"Plot of accuracy on breast cancer\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-3-BreastCancer.width-1250.png\" alt=\"Plot of accuracy on breast cancer\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"48u3i\">DeepSomatic results (<b>purple</b>) for a breast cancer sample widely used in research, compared to other tools. Several software tools identify cancer variants in Illumina’s data, while only a single alternative (<b>pink</b>) exists for the long-read sequencing data generated by PacBio and Oxford Nanopore Technologies. The F1-score measures how many variants are discovered and with what accuracy. DeepSomatic performs slightly better for single-letter variations in genetic code, known as single nucleotide variations, and shows major improvements for variations that involve Indels.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ry233\">The seventh sample was one of the previously used research cell lines of a breast cancer tumor that was preserved using <a href=\"https://en.wikipedia.org/wiki/Fixation_(histology)\" target=\"_blank\" rel=\"noopener noreferrer\">formalin-fixed-paraffin-embedded</a> (FFPE). This common preservation method introduces additional patterns of DNA damage that can complicate genetic analysis. This sample was also sequenced using whole <a href=\"https://en.wikipedia.org/wiki/Exome_sequencing\" target=\"_blank\" rel=\"noopener noreferrer\">exome sequencing</a> (WES), a more affordable method that focuses only on the roughly 1% of the genome that codes for proteins. When DeepSomatic was trained on these types of sample data and then tested on chromosome 1, which was reserved from training, it again outperformed other tools, suggesting it can be used to identify variants in lower-quality or historic tumor samples, potentially rescuing samples that have been harder to sequence, and working on clinical data where only the exome was sequenced.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-4-FFPE-WES.width-1250.png\" alt=\"Plot of accuracy on FFPE &amp; WES\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepSomatic-4-FFPE-WES.width-1250.png\" alt=\"Plot of accuracy on FFPE &amp; WES\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"48u3i\">DeepSomatic has notably higher accuracy on samples prepared with more complicated pre-processing steps involving: <a href=\"https://en.wikipedia.org/wiki/Fixation_(histology)\" target=\"_blank\" rel=\"noopener noreferrer\">fixed formalin paraffin embedded</a> (FFPE), a method used to preserve tissue samples (<b>left</b>), and whole <a href=\"https://en.wikipedia.org/wiki/Exome_sequencing\" target=\"_blank\" rel=\"noopener noreferrer\">exome sequencing</a> (WES), a method to sequence only the parts of the genome that code for proteins (<b>right</b>). The middle section shows a sample that was preserved with FFPE and also sequenced using whole exome sequencing.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Applying DeepSomatic to other cancers</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\">To test DeepSomatic’s performance on other types of cancers, we analyzed a single sample of <a href=\"https://www.mayoclinic.org/diseases-conditions/glioblastoma/symptoms-causes/syc-20569077\" target=\"_blank\" rel=\"noopener noreferrer\">glioblastoma</a>, an aggressive form of brain cancer that arises from a small number of variants. DeepSomatic was able to pinpoint those variants, showing that it can generalize its learning to apply it to a different cancer type.</p><p data-block-key=\"2ga4q\">We also worked with partners at <a href=\"https://www.childrensmercy.org/childrens-mercy-research-institute/about/\" target=\"_blank\" rel=\"noopener noreferrer\">Children’s Mercy</a> in Kansas City to analyze eight previously sequenced samples of <a href=\"https://www.childrensmercy.org/departments-and-clinics/division-of-pediatric-hematology-oncology-and-blood-and-marrow-transplantation/cancer-center/leukemia-and-lymphoma-program/understanding-leukemia/\" target=\"_blank\" rel=\"noopener noreferrer\">pediatric leukemia</a>, a cancer of the white blood cells that is the most common childhood cancer. Leukemia exists in the bloodstream, so a “normal” non-cancer blood sample is not possible. Despite that challenge, DeepSomatic identified the previously known variants as well as 10 new ones, showing that it can work with a tumor-only sample.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">What’s next</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\">Our hope is that research labs and clinicians can begin to use this tool. Detecting known cancer variants could help choose between existing treatments, such as chemotherapy, immunotherapy or other methods. Identifying new cancer variants could potentially lead to brand-new therapies. We hope people can take these tools and learn more about each cancer tumor, find what’s driving it, and ultimately deliver the most effective treatments to patients.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ry233\"><i>We thank all research participants whose participation in research programs and donation of cell lines made this work and other biomedical research possible. We thank our collaborators at UC Santa Cruz Genomics Institute, the National Cancer Institute, the Frederick National Laboratory for Cancer Research, Children’s Mercy Hospital, and NYU. We thank Hannah Hickey for writing contributions. We thank Avinatan Hassidim, Katherine Chou, Lizzie Dorfman, and Yossi Matias for research leadership support. We thank Resham Parikh and Isha Mishra for communications support.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    }
  ],
  "lastUpdated": "2025-11-02T10:27:41.236Z"
}