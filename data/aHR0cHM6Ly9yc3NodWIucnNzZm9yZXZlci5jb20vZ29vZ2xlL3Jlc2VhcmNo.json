{
  "sourceUrl": "https://rsshub.rssforever.com/google/research",
  "title": "Google Research Blog",
  "description": "Google Research Blog - Powered by RSSHub",
  "link": "https://research.google/blog",
  "items": [
    {
      "title": "学习你的方式：用生成式AI重塑教科书 (原标题: Learn Your Way: Reimagining textbooks with generative AI)",
      "link": "https://research.google/blog/learn-your-way-reimagining-textbooks-with-generative-ai/",
      "pubDate": "Mon, 15 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-15T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 引言：教科书的局限与生成式AI的潜力\n\n传统教科书作为教育的基石，存在根本性局限：它们是“一刀切”的媒介。教科书的手动创建需要大量人力，导致其缺乏替代视角、多种格式和定制化变体，而这些本可以使学习更有效和更具吸引力。谷歌正在探索如何利用生成式AI（GenAI）自动生成替代表示或个性化示例，同时保留原始材料的完整性。目标是重塑教科书，使其像每个学习者一样独特，赋予学生塑造自己学习旅程的能力。\n\n## “学习你的方式”（Learn Your Way）介绍\n\n谷歌实验室推出了“学习你的方式”，这是一项研究实验，旨在探索GenAI如何改变教育材料，为每位学生创造更有效、更具吸引力、以学习者为中心的体验。早期研究表明，使用Learn Your Way的学生在记忆测试中的得分比使用标准数字阅读器的学生高出11个百分点。\n\n## 核心方法论：以学习为基础，以学生为中心\n\n谷歌的方法建立在两大支柱之上，共同增强学习体验：\n\n1.  **生成内容的多种多模态表示。**\n2.  **迈向个性化的基础步骤。**\n\n该方法受到双重编码理论的启发，该理论指出在不同表示形式之间建立心理联系可以强化大脑中潜在的概念图式。随后的研究也表明，当学生以各种格式积极参与信息时，他们会建立更强大、更完整的材料心理模型。此外，个性化正日益成为K-12教育环境中的理想标准，谷歌的研究也反映了这一点。目标是通过根据学生属性调整内容来增强教育内容的关联性和有效性。同时，还融入了测验功能，可以根据学习者的实时反应进一步定制体验，从而增强学习动机和深度。\n\n## 技术实现：LearnLM与分层方法\n\n实现这一愿景涉及使用LearnLM（谷歌一流的、融入教学法的模型家族，现已直接集成到Gemini 2.5 Pro中）的分层技术方法。起始点是教科书PDF，但该方法也可用于其他形式的源材料。\n\n### 个性化管道\n\nLearn Your Way界面要求学习者选择他们的年级和兴趣（例如，体育、音乐、食物）。原始源材料首先根据学习者报告的年级重新调整难度，同时保持其内容范围。随后，用个性化示例替换通用示例，这些示例根据学习者报告的兴趣进行定制。由此产生的文本作为生成所有其他表示的基础，有效地传播了个性化效果，并为进一步的个性化建立了管道。\n\n![个性化牛顿定律文本示例](https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-1-final-hero.width-1250.png)\n*描述：针对两个学习者档案（顶部）个性化描述牛顿定律的通用文本，为后续内容表示（底部）提供了基础。*\n\n### 多模态内容表示\n\n在源材料个性化之后，系统会生成内容的多种表示形式：\n\n*   **思维导图和时间线：** 直接利用Gemini的广泛能力。\n*   **带旁白的幻灯片：** 需要更复杂的管道，将多个专业AI代理和工具编织在一起，以实现有效的教学效果。\n*   **教育插图：** 即使是最先进的通用图像模型也难以有效生成，因此谷歌专门微调了一个专用模型来生成教育插图。\n\n强大基础模型、多步骤代理工作流和微调组件的结合，使得系统能够生成各种高质量的多模态学习表示。\n\n## “学习你的方式”体验\n\nLearn Your Way界面整合了多种个性化内容表示，包括：\n\n1.  **沉浸式文本：** 将内容分解为易于理解的部分，辅以生成的图像和嵌入式问题，将被动阅读转化为遵循学习科学原理的主动多模态体验。\n2.  **章节测验：** 通过允许用户交互式评估学习情况并发现现有知识空白来促进主动学习。\n3.  **幻灯片与旁白：** 提供涵盖整个源材料的演示文稿，包括填空等互动活动，以及模拟录制课程的旁白版本。\n4.  **音频课程：** 提供AI教师与学生之间的模拟对话，辅以视觉辅助，模拟真实学习者如何与材料互动，包括表达误解，并由教师进行澄清。\n5.  **思维导图：** 分层组织知识，允许学习者在宏观和细节之间缩放。\n\n上述表示形式为学习者提供了选择，并且都根据他们选择的年级和个人兴趣进行调整。在整个体验过程中，互动测验提供动态反馈，指导学生重新访问他们遇到困难的特定内容区域。这标志着谷歌迈向真正个性化的第一步。\n\n![Learn Your Way 界面](https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-2.width-1250.png)\n*描述：Learn Your Way 界面提供了对多种表示形式和练习机会的便捷访问。*\n\n## 教学评估\n\n为了评估Learn Your Way的教学性能，谷歌将OpenStax（免费教育教科书提供商）的十种不同源材料转换为三种不同的个性化设置。源材料涵盖了从历史到物理的各种学科。三位教学主题专家随后使用教学标准（如准确性、覆盖范围和LearnLM学习科学原则）对转换后的材料进行了评估。\n\n![谷歌学习能力和体验开发与评估的顶级教学原则](https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-3.width-1250.png)\n*描述：指导谷歌新学习能力和体验开发与评估的顶级教学原则。*\n\n![专家对四项关键标准的评分](https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-4.width-1250.png)\n*描述：专家对不同转换的四项关键标准的评分。*\n\n结果高度积极，所有教学标准的平均专家评分均达到0.85或更高。\n\n## 效用研究\n\n谷歌最近对芝加哥地区60名15-18岁、阅读水平相似的学生进行了一项随机对照研究。参与者有长达40分钟的时间学习教科书中关于青少年大脑发育的内容，并被随机分配使用Learn Your Way或传统的数字PDF阅读器进行学习。\n\n**结果亮点：**\n\n*   **积极的学习成果：** Learn Your Way组在学习会话后的即时评估中平均得分高出9%。\n*   **更好的长期记忆：** 3-5天后的记忆测试中，Learn Your Way组得分高出11%（78% vs 67%）。\n*   **积极的用户情绪：** 100%使用Learn Your Way的学生表示该工具让他们在参加评估时更自信，而数字阅读器对照组中只有70%。93%的学生表示未来会使用Learn Your Way进行学习，而数字阅读器组中只有67%。\n*   **有价值的体验：** 定性访谈的见解表明，学生们认为Learn Your Way具有巨大价值。\n\n![Learn Your Way 组在即时评估中得分更高](https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-5.width-1250.png)\n*描述：使用Learn Your Way的小组在即时评估中的平均得分比使用数字阅读器的小组高出9%。*\n\n## 未来展望\n\n研究结果表明，生成式AI可以用于构建不仅更有效，而且更赋能的学习体验。通过将静态教科书演变为互动神器，并赋予学生更大的学习自主权，学习记忆力得到了提高。这项工作仅仅是探索的开始。谷歌设想了更多定制内容的方式，朝着持续适应每个学习者独特需求和进度的系统迈进。在迈向个性化教育的下一步时，谷歌将继续以教学原则为基础进行研究，衡量AI对学习效用的影响，以便未来每位学生都能获得为其量身定制的高质量、引人入胜的学习体验。",
      "shortSummary": "谷歌推出“学习你的方式”（Learn Your Way），一项利用生成式AI重塑教科书的研究实验。该平台通过个性化内容和提供沉浸式文本、测验、幻灯片、音频课程和思维导图等多种学习形式，解决传统教科书“一刀切”的局限。研究显示，使用Learn Your Way的学生在即时评估中得分高出9%，在长期记忆测试中得分高出11%，且用户满意度极高。该项目旨在为每位学生提供更有效、更具吸引力的个性化学习体验。",
      "translated_title": "学习你的方式：用生成式AI重塑教科书",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-1-final-hero.width-1250.png",
          "alt": "Learn-Your-Way-1-final-hero",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-2.width-1250.png",
          "alt": "Learn-Your-Way-2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-3.width-1250.png",
          "alt": "Learn-Your-Way-3",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-4.width-1250.png",
          "alt": "Learn-Your-Way-4",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-5.width-1250.png",
          "alt": "Learn-Your-Way-5",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ucxot\">Textbooks are a cornerstone of education, but they have a fundamental limitation: they are a one-size-fits-all medium. The manual creation of textbooks demands significant human effort, and as a result they lack alternative perspectives, multiple formats and tailored variations that can make learning more effective and engaging. At Google, we’re exploring how we can use generative AI (GenAI) to automatically generate alternative representations or personalized examples, while preserving the integrity of the source material. What if students had the power to shape their own learning journey, exploring materials using various formats that fit their evolving needs? What if we could reimagine the textbook to be as unique as every learner?</p><p data-block-key=\"e8fqs\">Recent advances in GenAI are bringing this vision closer to reality. Today we are excited to introduce <a href=\"https://learnyourway.withgoogle.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Learn Your Way</a>, now on <a href=\"https://labs.google/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Labs</a>, a research experiment that explores how GenAI can transform educational materials to create a more effective, engaging, learner-driven experience for every student. Here we outline the research and pedagogy underpinning Learn Your Way, with more details in the accompanying <a href=\"https://services.google.com/fh/files/misc/ai_augmented_textbook.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">tech report</a>. We also report early indicators of its impact: in our efficacy study, students using Learn Your Way scored 11 percentage points higher on retention tests than students using a standard digital reader.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Learn_Your_Way.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Grounded in learning, built for the student</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">Our approach is built on two key pillars that work together to augment the learning experience: (1) generating various multimodal representations of the content, and (2) taking foundational steps toward personalization.</p><p data-block-key=\"7jca2\">The seminal <a href=\"https://www.researchgate.net/publication/225249172_Dual_Coding_Theory_and_Education\" target=\"_blank\" rel=\"noopener noreferrer\">dual coding theory</a> states that forging mental connections between different representations strengthens the underlying conceptual schema in our brain. Subsequent <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0360131599000299\" target=\"_blank\" rel=\"noopener noreferrer\">research</a> indeed showed that when students actively engage with information in various formats, they build a more robust and complete mental model of the material. Inspired by this, our approach empowers students with the agency to choose and intermix multiple formats and modalities to best help them understand the material. In addition, personalization is increasingly becoming an <a href=\"https://par.nsf.gov/servlets/purl/10274018\" target=\"_blank\" rel=\"noopener noreferrer\">aspirational standard</a> in K-12 educational settings, and so our research reflects this. We aim to enhance the relatability and effectiveness of educational content by adapting it to student attributes. Moreover, we incorporate quizzing capabilities that enable us to further tailor the experience according to the learners’ real-time responses. Such personalization can be a powerful method for <a href=\"https://www.researchgate.net/publication/320564894_The_Role_of_Situational_Interest_in_Personalized_Learning\" target=\"_blank\" rel=\"noopener noreferrer\">enhancing motivation</a> and <a href=\"https://journals.sagepub.com/doi/full/10.3102/00346543221148478\" target=\"_blank\" rel=\"noopener noreferrer\">deepening learning</a>.</p><p data-block-key=\"35lic\">Bringing this to life involves a layered technical approach using <a href=\"https://blog.google/outreach-initiatives/education/google-learnlm-gemini-generative-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">LearnLM</a>, our best-in-class pedagogy-infused family of models, now integrated directly into <a href=\"https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini 2.5 Pro</a>. The first layer is a unique personalization pipeline that serves as the basis for the second layer of multiple content representations. Our starting point is a textbook PDF, although our approach could be used with other forms of source material.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">The personalization pipeline</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">The Learn Your Way interface asks the learner to select their grade and interests (e.g., sports, music, food). The original source material is first re-leveled to the learner’s reported grade level, while maintaining the scope of its content. This is followed by the strategic replacement of generic examples with ones that are personalized to the learner’s reported interests. The resulting text serves as the basis for the generation of all the other representations, effectively propagating the personalization effect and setting up a pipeline for further personalization.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-1-final-hero.width-1250.png\" alt=\"Learn-Your-Way-1-final-hero\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-1-final-hero.width-1250.png\" alt=\"Learn-Your-Way-1-final-hero\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ycqxy\"><i>Personalization of a generic text describing Newton’s law for two learner profiles (top) provides the basis for following representations of the content (bottom).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Multiple representations of content</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">Following the source personalization, we generate multiple representations of the content. For some content representations, such as mind maps and timelines, Gemini’s broad capabilities are used directly. Other features such as narrated slides, require more elaborate pipelines that weave together multiple specialized AI agents and tools to achieve an effective pedagogical result. Finally, specialized tasks, such as generating effective educational visuals, proved too challenging even for state-of-the-art general-purpose image models. To overcome this, we fine-tuned a dedicated model specifically for generating educational illustrations. The combination of a powerful base model, multi-step agentic workflows, and fine-tuned components allows us to generate a wide range of high-quality multimodal representations for learning.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The Learn Your Way experience</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">Our research comes to life in Learn Your Way. The interface brings together multiple, personalized representations of content including: (1) immersive text, (2) section-level quizzes, (3) slides &amp; narration, (4) audio lessons, and (5) mind maps.</p><ul><li data-block-key=\"digp2\"><b>Immersive text:</b> Breaks the content up into digestible sections that are augmented with generated images and embedded questions. Put together, these transform passive reading into an active multimodal experience that follows learning science principles.</li><li data-block-key=\"8kkuc\"><b>Section-level quizzes</b>: Promote active learning by allowing a user to interactively assess their learning, and uncover existing knowledge gaps.</li><li data-block-key=\"aq4r1\"><b>Slides &amp; narration:</b> Offers presentations that span the entire source material and include engaging activities like fill-in-the-blanks, as well as a narrated version, mimicking a recorded lesson.</li><li data-block-key=\"act6h\"><b>Audio lesson:</b> Provides simulated conversations, coupled with visual aids, between an AI-powered teacher and a student that models how a real learner might engage with the material, including the expression of misconceptions, which are clarified by the teacher.</li><li data-block-key=\"eqbtv\"><b>Mind map:</b> Organizes the knowledge hierarchically and allows learners to zoom in and out from the big picture to the details.</li></ul><p data-block-key=\"eef4f\">The above representations give learners choice and are all adapted to their selected grade level and personal interests. Throughout the experience, the interactive quizzes provide dynamic feedback, guiding students to revisit specific content areas where they struggled. This marks our first steps towards true personalization.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-2.width-1250.png\" alt=\"Learn-Your-Way-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-2.width-1250.png\" alt=\"Learn-Your-Way-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ycqxy\"><i>The Learn You Way interface provides easy access to multiple representations and practice opportunities.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Pedagogical evaluation</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">To evaluate Learn You Way's pedagogical performance, we transformed ten varied source materials from <a href=\"https://openstax.org/\" target=\"_blank\" rel=\"noopener noreferrer\">OpenStax</a> (a provider of free educational textbooks) to three different personalization settings. The source materials covered various subjects from history to physics. Three pedagogical subject matter experts then evaluated the transformed materials using pedagogical criteria, such as accuracy, coverage, and the <a href=\"https://blog.google/outreach-initiatives/education/google-learnlm-gemini-generative-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">LearnLM</a> learning science principles.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-3.width-1250.png\" alt=\"Learn-Your-Way-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-3.width-1250.png\" alt=\"Learn-Your-Way-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ycqxy\"><i>Top pedagogical principles that guide the development and evaluation of new learning capabilities and experiences at Google</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ucxot\">The results were highly positive, with an average expert rating of 0.85 or higher across all pedagogical criteria. See the <a href=\"https://services.google.com/fh/files/misc/ai_augmented_textbook.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">tech report</a> for more evaluation details.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-4.width-1250.png\" alt=\"Learn-Your-Way-4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-4.width-1250.png\" alt=\"Learn-Your-Way-4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ycqxy\"><i>Expert ratings for the different transformations for four key criteria.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Efficacy study</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">An AI-powered learning tool is only valuable if it both effectively improves learning outcomes and students want to use it. Learn Your Way now serves as a research platform for us to conduct studies with partners around the world to explore how AI-powered transformations and personalization affects outcomes, and to ensure that what we build is effective and <a href=\"https://blog.google/intl/en-africa/company-news/outreach-and-initiatives/5-ways-were-bringing-ai-innovations-to-people-across-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">locally relevant</a>.</p><p data-block-key=\"rstv\">Recently, we conducted a randomized controlled study with 60 students from the Chicago area, ages 15–18 and with similar reading levels. Participants were given up to 40 minutes to learn about adolescent brain development from a textbook, and randomly assigned to learn using Learn Your Way or a traditional digital PDF reader.</p><p data-block-key=\"e1qod\">We assessed students with a quiz immediately after the study session, and with a retention test 3–5 days later, using assessments designed by pedagogical experts to be a good measure of content comprehension. We also surveyed them about the learning experience, and to gain deeper insights beyond these quantitative metrics, each student participated in a 30-minute qualitative interview where they could share more nuanced feedback about their experience.</p><p data-block-key=\"2tkk3\">The results were compelling and statistically significant. Here are the highlights. See the <a href=\"https://services.google.com/fh/files/misc/ai_augmented_textbook.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">tech report</a> for more details.</p><ul><li data-block-key=\"1603p\"><b>Positive learning outcomes:</b> The Learn Your Way group scored, on average, 9% higher on the immediate assessment following the study session.</li><li data-block-key=\"2plg8\"><b>Better long-term retention:</b> Similarly, the Learn Your Way group scored 11% higher on the retention assessment 3-5 days later (78% vs. 67%).</li><li data-block-key=\"c6kod\"><b>Positive user sentiment:</b> 100% of students who used Learn Your Way reported that they felt the tool made them more comfortable taking the assessment, compared to 70% in the digital reader control group. 93% said they would want to use Learn Your Way for future learning, compared to just 67% for the digital reader.</li><li data-block-key=\"4mg4i\"><b>Valuable experience</b>: Insights from the qualitative interviews revealed that students found great value in Learn Your Way.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-5.width-1250.png\" alt=\"Learn-Your-Way-5\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Learn-Your-Way-5.width-1250.png\" alt=\"Learn-Your-Way-5\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"ycqxy\"><i>The group using Learn Your Way scored 9% higher on average on an immediate assessment than the group using a digital reader</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Experience Learn Your Way</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">To give a concrete feel for the Learn Your Way interactive experience, today we are releasing <a href=\"http://learnyourway.withgoogle.com/\" target=\"_blank\" rel=\"noopener noreferrer\">example experiences on Google Labs</a>, including:</p><ul><li data-block-key=\"16uqm\"><a href=\"https://learnyourway.withgoogle.com/scopes/e6ivLL1E/immersive-text/0\" target=\"_blank\" rel=\"noopener noreferrer\">A lesson on immune system challenges</a></li><li data-block-key=\"6vmtm\"><a href=\"https://learnyourway.withgoogle.com/scopes/RMUcoWft\" target=\"_blank\" rel=\"noopener noreferrer\">Learn about how to organize economies</a></li><li data-block-key=\"70e8i\"><a href=\"https://learnyourway.withgoogle.com/scopes/ggPsy1Wb\" target=\"_blank\" rel=\"noopener noreferrer\">Discover what sociology is?</a></li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The path forward</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\">Our findings suggest that generative AI can be used to build learning experiences that are not only more effective but also more empowering. By evolving the static textbook into an interactive artifact and giving students greater agency over <i>how</i> they learn, we saw learning retention improve.</p><p data-block-key=\"88po1\">This work is just the beginning of our exploration. We envision many more ways to tailor content, moving towards systems that continuously adapt to each learner's unique needs and progress. As we take our next steps towards personalized education, we will continue to ground our research in pedagogical principles, measuring the impact of AI on learning efficacy, so that in the future every student might have access to a high-quality, engaging learning experience that is custom built for them.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ucxot\"><i>Shout out to our Google Research LearnLM team who have contributed to this work: Alicia Martín, Amir Globerson, Amy Wang, Anirudh Shekhawat, Anisha Choudhury, Anna Iurchenko, Avinatan Hassidim, Ayça Çakmakli, Ayelet Shasha Evron, Charlie Yang, Courtney Heldreth, Dana Oria, Diana Akrong, Hairong Mu, Ian Li, Ido Cohen, Komal Singh, Lev Borovoi, Lidan Hackmon, Lior Belinsky, Michael Fink, Preeti Singh, Rena Levitt, Shashank Agarwal, Shay Sharon, Sophie Allweis, Tracey Lee-Joe, Xiaohong Hao, Yael Gold-Zamir, Yishay Mor, and Yoav Bar Sinai. Special thanks to our executive champions: Niv Efron, Avinatan Hassidim, Yossi Matias and Ben Gomes.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "VaultGemma：全球最强大的差分隐私大型语言模型 (原标题: VaultGemma: The world's most capable differentially private LLM)",
      "link": "https://research.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/",
      "pubDate": "Thu, 11 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-11T16:00:00.000Z",
      "creator": "Google",
      "summary": "# VaultGemma：全球最强大的差分隐私大型语言模型\n\n## 引言：差分隐私与LLM的挑战\n随着人工智能日益融入生活，将隐私作为核心构建AI变得至关重要。差分隐私（DP）通过添加校准噪声以防止模型记忆化，提供了一种数学上稳健的解决方案。然而，将DP应用于大型语言模型（LLM）会引入权衡，包括降低训练稳定性、显著增加批次大小和计算成本。理解这些权衡对于该领域的发展至关重要。\n\n## VaultGemma：基于差分隐私缩放定律的创新\n为了解决这些挑战，我们与Google DeepMind合作开展了题为“差分隐私语言模型的缩放定律”的新研究。这项研究建立了能够准确模拟这些复杂性的定律，提供了计算、隐私和效用之间权衡的完整图景。\n\n受此研究指导，我们推出了VaultGemma，这是迄今为止最大的（10亿参数）开放模型，从零开始使用差分隐私进行训练。我们正在Hugging Face和Kaggle上发布其权重，并附带一份技术报告，以推动下一代私密AI的发展。\n\n## 理解差分隐私缩放定律\n通过精心设计的实验方法，我们旨在量化在DP训练中增加模型大小、批次大小和迭代次数的益处。我们的工作假设模型学习效果主要取决于“噪声-批次比”，即为隐私添加的随机噪声量与用于训练的数据组（批次）大小之间的比较。\n\n为了建立DP缩放定律，我们进行了一系列全面的实验，评估了不同模型大小和噪声-批次比下的性能。由此产生的经验数据，结合其他变量之间已知的确定性关系，使我们能够回答各种有趣的缩放定律式查询，例如：“在给定计算预算、隐私预算和数据预算的情况下，实现最低训练损失的最佳训练配置是什么？”\n\n![VaultGemma1_ScalingLaws](https://storage.googleapis.com/gweb-research2023-media/images/VaultGemma1_ScalingLaws.width-1250.png)\n*图1：差分隐私缩放定律的结构。我们确定预测损失可以主要通过模型大小、迭代次数和噪声-批次比来准确建模，从而简化了计算、隐私和数据预算之间复杂的相互作用。*\n\n## 关键发现：计算、隐私与数据的协同效应\n在深入探讨完整的缩放定律之前，理解计算预算、隐私预算和数据预算之间从隐私核算角度的动态和协同作用非常有用。例如，单独增加隐私预算会导致收益递减，除非同时增加计算预算（FLOPs）或数据预算（tokens）。\n\n![vg-gif](https://storage.googleapis.com/gweb-research2023-media/images/vg-gif.width-800.gif)\n*图2：增加隐私预算（epsilon）和计算预算（批次大小）对噪声-批次比的边际效益。*\n\n进一步探索这种协同作用，研究表明最佳训练配置会根据不同的约束条件而变化。随着隐私和计算预算的变化，建议会在投资更大的模型与使用更大的批次大小或更多迭代进行训练之间进行调整。一个关键发现是，在DP训练中，应该使用比非DP训练更小的模型和更大的批次大小。虽然这一普遍见解适用于许多设置，但最佳训练配置会随隐私和数据预算而变化。\n\n## 应用缩放定律构建VaultGemma\nGemma模型以责任和安全为核心设计，使其成为开发生产级DP训练模型（如VaultGemma）的天然基础。\n\n### 算法进步：大规模训练\n我们推导出的缩放定律是训练有用Gemma模型与DP的重要第一步。我们利用这些缩放定律来确定训练一个计算最优的10亿参数Gemma 2模型所需的计算量，以及如何在批次大小、迭代次数和序列长度之间分配计算以实现最佳效用。\n\n在研究缩放定律和实际训练VaultGemma之间的一个显著差异是我们对泊松采样的处理。我们最初使用直接的统一批次加载数据方法，但后来切换到泊松采样，以在最少噪声的情况下获得最佳隐私保证。我们通过使用我们最近在可扩展DP-SGD方面的工作解决了由此带来的挑战，该工作允许我们以固定大小的批次处理数据（通过添加额外填充或修剪），同时保持强大的隐私保护。\n\n## VaultGemma的成果与性能\n凭借我们新的缩放定律和先进的训练算法，我们构建了VaultGemma，这是迄今为止最大的（10亿参数）开放模型，采用差分隐私完全预训练，能够产生高实用性模型。\n\n从训练VaultGemma中，我们发现我们的缩放定律高度准确。VaultGemma的最终训练损失与我们的方程预测值非常接近，验证了我们的研究，并为社区提供了未来私密模型开发的可靠路线图。\n\n![VaultGemma4_Performance](https://storage.googleapis.com/gweb-research2023-media/images/VaultGemma4_Performance.width-1250.png)\n*图3：VaultGemma 1B（差分隐私）与其非隐私对应模型（Gemma3 1B）以及旧基线模型（GPT-2 1.5B）的性能比较。结果量化了隐私所需的当前资源投入，并表明现代DP训练产生的效用与大约五年前的非隐私模型相当。*\n\n我们还在一系列标准学术基准（如HellaSwag、BoolQ、PIQA、SocialIQA、TriviaQA、ARC-C、ARC-E）上比较了我们模型与非隐私对应模型的下游性能。为了量化隐私所需的当前资源投入，我们还与一个旧的、大小相似的GPT-2模型进行了比较，该模型在这些基准上表现相似。这种比较表明，当今的私密训练方法产生的模型效用与大约五年前的非隐私模型相当，突显了我们的工作将帮助社区系统性弥补的重要差距。\n\n## 形式化隐私保证与经验记忆化\n\n### 形式化隐私保证\nVaultGemma以序列级DP保证（ε ≤ 2.0，δ ≤ 1.1e-10）进行训练，其中一个序列包含从异构数据源中提取的1024个连续token。这意味着，如果任何（潜在私密）事实或推断的信息出现在单个序列中，VaultGemma基本上不会知道该事实；对任何查询的响应在统计上将与从未在该序列上训练过的模型的结果相似。然而，如果许多训练序列包含与特定事实相关的信息，那么VaultGemma通常能够提供该信息。\n\n### 经验记忆化\n序列级DP可证明地限制了任何单个训练序列（示例）对最终模型的影响。我们用训练文档中的50个token前缀提示模型，以查看它是否会生成相应的50个token后缀。VaultGemma 1B未显示出可检测到的训练数据记忆化，成功证明了DP训练的有效性。\n\n## 结论\nVaultGemma代表着在构建强大且设计上私密的AI方面迈出了重要一步。通过开发和应用对DP缩放定律的全新、稳健理解，我们成功训练并发布了迄今为止最大的开放DP训练语言模型。\n\n尽管DP训练模型与非DP训练模型之间仍存在效用差距，但我们相信通过对DP训练机制设计的更多研究，可以系统性地缩小这一差距。我们希望VaultGemma及随附的研究能赋能社区，为所有人构建下一代安全、负责任、私密的AI。",
      "shortSummary": "VaultGemma是首个从零开始训练的10亿参数开放差分隐私（DP）大型语言模型。它基于与Google DeepMind合作开发的DP缩放定律，旨在解决DP应用于LLM时的计算-隐私-效用权衡。研究发现，DP训练需要使用更小的模型和更大的批次。VaultGemma在序列级DP保证下，未检测到训练数据记忆化，其性能与约五年前的非隐私模型相当。该模型的发布旨在推动私密AI的发展，并系统性地缩小DP与非DP模型之间的效用差距。",
      "translated_title": "VaultGemma：全球最强大的差分隐私大型语言模型",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/VaultGemma1_ScalingLaws.width-1250.png",
          "alt": "VaultGemma1_ScalingLaws",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/vg-gif.width-800.gif",
          "alt": "vg-gif",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/VaultGemma4_Performance.width-1250.png",
          "alt": "VaultGemma4_Performance",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"w2rqp\">As AI becomes more integrated into our lives, building it with privacy at its core is a critical frontier for the field. <a href=\"https://en.wikipedia.org/wiki/Differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">Differential privacy</a> (DP) offers a mathematically sound solution by adding calibrated noise to prevent memorization. However, applying DP to LLMs introduces trade-offs. Understanding these trade-offs is crucial. Applying DP noise alters traditional <a href=\"https://arxiv.org/abs/2203.15556\" target=\"_blank\" rel=\"noopener noreferrer\">scaling laws</a> — rules describing performance dynamics — by reducing training stability (the model's ability to learn consistently without experiencing catastrophic events like loss spikes or divergence) and significantly increasing batch size (a collection of training examples sent to the model simultaneously for processing) and computation costs.</p><p data-block-key=\"kvsp\">Our new research, “<a href=\"https://arxiv.org/abs/2501.18914\" target=\"_blank\" rel=\"noopener noreferrer\">Scaling Laws for Differentially Private Language Models</a>”, conducted in partnership with Google DeepMind, establishes laws that accurately model these intricacies, providing a complete picture of the compute-privacy-utility trade-offs. Guided by this research, we’re excited to introduce VaultGemma, the largest (1B-parameters), open model trained from scratch with differential privacy. We are releasing the weights on <a href=\"https://huggingface.co/google/vaultgemma-1b\" target=\"_blank\" rel=\"noopener noreferrer\">Hugging Face</a> and <a href=\"https://www.kaggle.com/models/google/vaultgemma\" target=\"_blank\" rel=\"noopener noreferrer\">Kaggle</a>, alongside a <a href=\"https://services.google.com/fh/files/blogs/vaultgemma_tech_report.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">technical report</a>, to advance the development of the next generation of private AI.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"w2rqp\">Understanding the scaling laws</h2><p data-block-key=\"emls0\">With a carefully thought-out experimental methodology, we aimed to quantify the benefit of increasing model sizes, batch sizes, and iterations in the context of DP training. Our work required making some simplifying assumptions to overcome the exponential number of combinations one might consider trying. We assumed that how well the model learns depends mostly on the \"noise-batch ratio” which compares the amount of random noise we add for privacy to the size of the data groups (batches) we use for training. This assumption works because the privacy noise we add is much greater than any natural randomness that comes from sampling the data.</p><p data-block-key=\"fbiiv\">To establish a DP scaling law, we conducted a comprehensive set of experiments to evaluate performance across a variety of model sizes and noise-batch ratios. The resulting empirical data, together with known deterministic relationships between other variables, allows us to answer a variety of interesting scaling-laws–style queries, such as, “For a given compute budget, privacy budget, and data budget, what is the optimal training configuration to achieve the lowest possible training loss?”</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/VaultGemma1_ScalingLaws.width-1250.png\" alt=\"VaultGemma1_ScalingLaws\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/VaultGemma1_ScalingLaws.width-1250.png\" alt=\"VaultGemma1_ScalingLaws\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"i2gro\"><i>The structure of our DP scaling laws. We establish that predicted loss can be accurately modeled using primarily the model size, iterations and the noise-batch ratio, simplifying the complex interactions between the compute, privacy, and data budgets.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"sv3qw\">Key findings: A powerful synergy</h2><p data-block-key=\"cr1ma\">Before diving into the full scaling laws, it’s useful to understand the dynamics and synergies between the compute budget, privacy budget, and data budget from a privacy accounting perspective — i.e., understand how these factors influence the noise-batch ratio for a fixed model size and number of iterations. This analysis is significantly cheaper to do as it does not require any model training, yet it yields a number of useful insights. For instance, increasing the privacy budget in isolation leads to diminishing returns, unless coupled with a corresponding increase in either the compute budget (<a href=\"https://en.wikipedia.org/wiki/Floating_point_operations_per_second\" target=\"_blank\" rel=\"noopener noreferrer\">FLOPs</a>) or data budget (tokens).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/vg-gif.width-800.gif\" alt=\"vg-gif\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/vg-gif.width-800.gif\" alt=\"vg-gif\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"yu3nj\"><i>Marginal benefit of increasing the privacy budget (epsilon) and the compute budget (batch size) in terms of their effect on the noise-batch ratio.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"sv3qw\">To explore this synergy further, the visualization below shows how the optimal training configuration changes based on different constraints. As the privacy and compute budgets change, notice how the recommendation shifts between investing in a larger model versus training with larger batch sizes or more iterations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/VaultGemma3_TrainingLossFin.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"yu3nj\"><i>Predicted training loss for different settings of data/privacy/compute budget, and a further detailed breakdown by the number of iterations, batch size, and model size. The plots show both the minimum achievable loss for different budget settings, along with the optimal hyper-parameter configurations.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"sv3qw\">This data provides a wealth of useful insights for practitioners. While all the insights are reported in the paper, a key finding is that one should train a much smaller model with a much larger batch size than would be used without DP. This general insight should be unsurprising to a DP expert given the importance of large batch sizes. While this general insight holds across many settings, the optimal training configurations do change with the privacy and data budgets. Understanding the exact trade-off is crucial to ensure that both the compute and privacy budgets are used judiciously in real training scenarios. The above visualizations also reveal that there is often wiggle room in the training configurations —&nbsp;i.e., a range of model sizes might provide very similar utility if paired with the correct number of iterations and/or batch size.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"sv3qw\">Applying the scaling laws to build VaultGemma</h2><p data-block-key=\"69vq3\">The <a href=\"https://deepmind.google/models/gemma/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma</a> models are designed with responsibility and safety at their core. This makes them a natural foundation for developing a production-quality, DP-trained model like VaultGemma.</p><h3 data-block-key=\"d6ih5\">Algorithmic advancements: Training at scale</h3><p data-block-key=\"ac2lq\">The scaling laws we derived above represent an important first step towards training a useful Gemma model with DP. We used the scaling laws to determine both how much compute we needed to train a compute-optimal 1B parameter Gemma 2-based model with DP, and how to allocate that compute among batch size, iterations, and sequence length to achieve the best utility.</p><p data-block-key=\"2tili\">One prominent gap between the research underlying the scaling laws and the actual training of VaultGemma was our handling of <a href=\"https://en.wikipedia.org/wiki/Poisson_sampling\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Poisson sampling</i></a>, which is a central component of <a href=\"https://arxiv.org/abs/1607.00133\" target=\"_blank\" rel=\"noopener noreferrer\">DP-SGD</a>. We initially used a straightforward method of loading data in uniform batches but then switched to Poisson sampling to get the best privacy guarantees with the least amount of noise. This method posed two main challenges: it created batches of different sizes, and it required a specific, randomized order for processing the data. We solved this by using our recent work on <a href=\"https://arxiv.org/abs/2411.04205\" target=\"_blank\" rel=\"noopener noreferrer\">Scalable DP-SGD</a>, which allows us to process data in fixed-size batches — either by adding extra padding or trimming them — while still maintaining strong privacy protections.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"sv3qw\">Results</h2><p data-block-key=\"efh07\">Armed with our new scaling laws and advanced training algorithms, we built VaultGemma, to date the largest (1B-parameters) open model fully pre-trained with differential privacy with an approach that can yield high-utility models.</p><p data-block-key=\"25ku7\">From training VaultGemma, we found our scaling laws to be highly accurate. The final training loss of VaultGemma was remarkably close to what our equations predicted, validating our research and providing the community with a reliable roadmap for future private model development.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/VaultGemma4_Performance.width-1250.png\" alt=\"VaultGemma4_Performance\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/VaultGemma4_Performance.width-1250.png\" alt=\"VaultGemma4_Performance\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"yu3nj\"><i>Performance comparison of VaultGemma 1B (differentially private) against its non-private counterpart (Gemma3 1B) and an older baseline (GPT-2 1.5B). The results quantify the current resource investment required for privacy and demonstrate that modern DP training yields utility comparable to non-private models from roughly five years ago.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"sv3qw\">We also compare downstream performance of our model against its non-private counterpart across a range of standard academic benchmarks (i.e., <a href=\"https://arxiv.org/abs/1905.07830\" target=\"_blank\" rel=\"noopener noreferrer\">HellaSwag</a>, <a href=\"https://arxiv.org/abs/1905.10044\" target=\"_blank\" rel=\"noopener noreferrer\">BoolQ</a>, <a href=\"https://arxiv.org/abs/1911.11641\" target=\"_blank\" rel=\"noopener noreferrer\">PIQA</a>, <a href=\"https://arxiv.org/abs/1904.09728\" target=\"_blank\" rel=\"noopener noreferrer\">SocialIQA</a>, <a href=\"https://arxiv.org/abs/1705.03551\" target=\"_blank\" rel=\"noopener noreferrer\">TriviaQA</a>, <a href=\"https://arxiv.org/abs/1911.01547\" target=\"_blank\" rel=\"noopener noreferrer\">ARC-</a>C, <a href=\"https://arxiv.org/abs/1911.01547\" target=\"_blank\" rel=\"noopener noreferrer\">ARC-</a>E ). To put this performance in perspective and quantify the current resource investment required for privacy, we also include a comparison to an older similar-sized GPT-2 model, which performs similarly on these benchmarks. This comparison illustrates that today’s private training methods produce models with utility comparable to that of non-private models from roughly 5 years ago, highlighting the important gap our work will help the community systematically close.</p><p data-block-key=\"36cfk\">Finally, the model comes with strong theoretical and empirical privacy protections.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"sv3qw\">Formal privacy guarantee</h3><p data-block-key=\"2ivmr\">In general, both the privacy parameters (ε, δ) and the privacy <i>unit</i> are important considerations when doing DP training, as these together determine what the trained model can learn. VaultGemma was trained with a <i>sequence</i>-level DP guarantee of (ε ≤ 2.0, δ ≤ 1.1e-10), where a sequence consists of 1024 consecutive tokens extracted from heterogeneous data sources. Specifically, we used the same training mixture that was used to train the <a href=\"https://arxiv.org/abs/2408.00118\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma 2</a> model, consisting of a number of documents of varying lengths. During pre-processing, long documents are split up and tokenized into multiple sequences, and shorter documents are packed together into a single sequence. While the sequence-level privacy unit was a natural choice for our training mixture, in situations where there is a clear mapping between data and users, <a href=\"https://research.google/blog/fine-tuning-llms-with-user-level-differential-privacy/\">user-level differential privacy</a> would be a better choice.</p><p data-block-key=\"b4na6\">What does this mean in practice? Informally speaking, because we provide protection at the sequence level, if information relating to any (potentially private) fact or inference occurs in a single sequence, then VaultGemma essentially does not know that fact: the response to any query will be statistically similar to the result from a model that never trained on the sequence in question. However, if many training sequences contain information relevant to a particular fact, then in general VaultGemma will be able to provide that information.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"sv3qw\">Empirical memorization</h3><p data-block-key=\"1td9o\">To complement our sequence-level DP guarantee, we conduct additional tests of the empirical privacy properties of the trained model. To do so, we prompted the model with a 50-token prefix from a training document to see if it would generate the corresponding 50-token suffix. VaultGemma 1B shows no detectable memorization of its training data and successfully demonstrates the efficacy of DP training.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"sv3qw\">Conclusion</h2><p data-block-key=\"ej03m\">VaultGemma represents a significant step forward in the journey toward building AI that is both powerful and private by design. By developing and applying a new, robust understanding of the scaling laws for DP, we have successfully trained and released the largest open, DP-trained language model to date.</p><p data-block-key=\"bv1kj\">While a utility gap still exists between DP-trained and non-DP–trained models, we believe this gap can be systematically narrowed with more research on mechanism design for DP training. We hope that VaultGemma and our accompanying research will empower the community to build the next generation of safe, responsible, and private AI for everyone.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"sv3qw\">Acknowledgements</h2><p data-block-key=\"doolt\"><i>We'd like to thank the entire Gemma and Google Privacy teams for their contributions and support throughout this project, in particular, Peter Kairouz, Brendan McMahan and Dan Ramage for feedback on the blog post, Mark Simborg and Kimberly Schwede for help with visualizations, and the teams at Google that helped with algorithm design, infrastructure implementation, and production maintenance. The following people directly contributed to the work presented here (ordered alphabetically): Borja Balle, Zachary Charles, Christopher A. Choquette-Choo, Lynn Chua, Prem Eruvbetine, Badih Ghazi, Steve He, Yangsibo Huang, Armand Joulin, George Kaissis, Pritish Kamath, Ravi Kumar, Daogao Liu, Ruibo Liu, Pasin Manurangsi, Thomas Mesnard, Andreas Terzis, Tris Warkentin, Da Yu, and Chiyuan Zhang.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "推测性级联——一种更智能、更快速的LLM推理混合方法 (原标题: Speculative cascades — A hybrid approach for smarter, faster LLM inference)",
      "link": "https://research.google/blog/speculative-cascades-a-hybrid-approach-for-smarter-faster-llm-inference/",
      "pubDate": "Wed, 10 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-10T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 推测性级联：更智能、更快速的LLM推理混合方法\n\n大型语言模型（LLM）已彻底改变我们与技术的互动方式，但其推理过程（生成响应）通常缓慢且计算成本高昂。在不牺牲质量的前提下，提高LLM的速度并降低成本是一个关键挑战。\n\n## 现有优化方法\n\n文章介绍了两种主要的LLM推理优化方法，并以“Buzz Aldrin是谁？”的简单问题为例进行了说明：\n\n### 1. 级联（Cascades）\n*   **原理**：通过策略性地优先使用小型、快速模型，在必要时才调用大型、昂贵的LLM，以优化效率。\n*   **工作方式**：小型模型首先处理查询，并根据其置信度决定是自行响应还是将任务转交给更强大但成本更高的大型模型。\n*   **目标**：降低计算成本，高效分配资源，允许质量存在一定变动。\n*   **局限性**：如果小型模型不自信，需要等待其完成判断后才能启动大型模型，存在顺序执行的瓶颈。\n\n### 2. 推测解码（Speculative Decoding）\n*   **原理**：在不改变最终输出结果的前提下，优化LLM的延迟和吞吐量。\n*   **工作方式**：使用一个小型、快速的“草稿”模型预测一系列未来词元，然后由大型“目标”模型并行快速验证这些推测词元。如果草稿被接受，大型模型相当于一步生成了多个词元，从而大大加速了过程，并保证输出与大型模型独立生成的结果完全相同。\n*   **目标**：优先降低速度和延迟。\n*   **局限性**：可能增加内存使用，计算节省较少，因为大型模型仍需执行大量工作。在“Buzz Aldrin”示例中，即使小型模型给出了正确答案，但由于与大型模型首个词元不匹配（“Buzz”≠“Edwin”），整个草稿被拒绝，导致速度优势丧失，且最终输出不一定更优。\n\n## 两种方法的对比与权衡\n\n下图总结了级联和推测解码在目标和权衡上的根本差异：\n\n![SpecCascades-0.5-Table](https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-0.5-Table.width-1250.png)\n\n下图直观展示了标准级联和推测解码所提供的权衡：\n\n![SpecCascades-1-TradeOffs](https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-1-TradeOffs.width-1250.png)\n*   左图：标准级联通过改变置信度阈值提供不同的成本-质量权衡（绿星为小型模型，红星为大型模型，点代表不同权衡）。\n*   右图：推测解码的权衡（蓝星）。\n\n## 推测性级联：两全其美的混合方法\n\n文章引入了“推测性级联”（speculative cascades），它结合了标准级联的分层处理思想和推测解码的加速机制。\n\n*   **核心创新**：用灵活的“延迟规则”取代了推测解码中严格的验证机制。\n*   **工作方式**：\n    1.  小型模型生成“草稿”输出。\n    2.  大型模型同时并行验证该草稿并提供自己的评分。\n    3.  关键的“灵活延迟规则”会根据两个模型的输出动态地逐词元决定是接受小型模型的草稿还是转交给大型模型。\n*   **优势**：\n    *   避免了标准级联的顺序瓶颈。\n    *   即使小型模型的答案与大型模型的首选输出不完全匹配，系统也能接受其良好的答案。\n    *   在相同的质量水平下，比推测解码更快，即每次调用大型模型能生成更多词元。\n    *   实现了比单独使用任一技术更好的LLM输出质量和更低的计算成本。\n\n### 灵活的延迟规则\n\n推测性级联的强大之处在于其灵活性，延迟规则可以根据不同需求进行定制，例如：\n\n*   **简单置信度检查**：仅当小型模型对其预测不自信时才延迟。\n*   **比较检查**：如果大型模型比小型模型明显更自信，则延迟。\n*   **成本效益分析**：仅当大型模型的置信度提升超过拒绝小型模型草稿的“成本”时才延迟。\n*   **词元特定检查**：如果小型模型草稿的词元不在大型模型“批准列表”（其排名靠前的词元）中，则延迟。\n\n下图展示了推测性级联的框图：\n\n![SpecCascades-2-BlockDiagram](https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-2-BlockDiagram.width-1250.png)\n*   与标准推测解码类似，草稿过程涉及小型草稿模型的自回归采样。\n*   但验证过程不同：它通过延迟规则考虑小型和大型模型的组合输出分布，而不仅仅依赖于大型模型的输出。\n\n下图通过一个GSM8K数据集中的数学问题示例，可视化了推测性级联与推测解码的行为对比：\n\n![SpecCascades-3-Comparison](https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-3-Comparison.width-800.gif)\n*   推测性级联能够更快地达到正确答案。\n\n## 实验结果\n\n文章在摘要、推理和编码等一系列基准测试中对推测性级联进行了测试。结果表明：\n\n*   推测性级联相比推测解码具有明显优势。\n*   在标准的质量-效率图上，推测性级联始终提供更好的权衡。\n*   在相同的质量水平下，推测性级联方法更快，即每次调用大型模型能生成更多词元。\n\n下图展示了推测性级联在数学推理和摘要任务上的性能：\n\n![SpecCascades-4-Performance](https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-4-Performance.width-1250.png)\n*   推测性级联变体（蓝色和橙色）相比标准推测解码（绿色星号）实现了更好的质量-延迟权衡。\n\n## 结论\n\n随着LLM日益融入日常应用，优化其性能已成为实际需求。推测性级联通过重新思考级联和推测解码的结合方式，为开发者提供了一个更强大、更灵活的工具。这种混合方法允许对成本-质量平衡进行精细控制，为开发更智能、更快速的应用程序铺平了道路。",
      "shortSummary": "大型语言模型（LLM）推理缓慢且成本高昂。为解决此问题，文章提出了“推测性级联”方法，它结合了传统级联和推测解码的优点。该方法利用小型模型生成草稿，大型模型并行验证，并通过灵活的逐词元延迟规则，动态决定接受小型模型输出或转交给大型模型。这避免了传统方法的瓶颈，显著提高了LLM推理速度，同时在保证输出质量的前提下降低了计算成本，实现了更优的成本-质量权衡，使LLM应用更智能、更快速。",
      "translated_title": "推测性级联——一种更智能、更快速的LLM推理混合方法",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-0.5-Table.width-1250.png",
          "alt": "SpecCascades-0.5-Table",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-1-TradeOffs.width-1250.png",
          "alt": "SpecCascades-1-TradeOffs",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-2-BlockDiagram.width-1250.png",
          "alt": "SpecCascades-2-BlockDiagram",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-3-Comparison.width-800.gif",
          "alt": "SpecCascades-3-Comparison",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-4-Performance.width-1250.png",
          "alt": "SpecCascades-4-Performance",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"t9039\">LLMs have transformed how we interact with technology, powering everything from advanced search capabilities to creative coding assistants. But this power comes at a cost: inference (the process of generating a response) can be slow and computationally expensive. As we deploy these models to more users, making them faster and less expensive without sacrificing quality is a critical challenge.</p><p data-block-key=\"1pvn0\">One way to accomplish this would be to use <a href=\"https://openreview.net/pdf?id=XUZ2S0JVJP\" target=\"_blank\" rel=\"noopener noreferrer\">cascades</a>, which aim to optimize LLM efficiency by strategically using smaller, faster models before engaging a larger, more expensive LLM. This approach involves a <i>deferral rule</i> where the smaller model decides if it can handle a query or if it needs to pass the task to a more capable, but costlier, large model. The goal is to process as much as possible cheaply and quickly, only incurring the high cost of the large LLM for complex tasks that truly require its advanced capabilities, potentially yielding favorable cost-quality trade-offs. Cascades prioritize computational cost reduction and efficient resource allocation, while allowing for some variability in quality.</p><p data-block-key=\"70hdi\">Another approach, <a href=\"https://research.google/blog/looking-back-at-speculative-decoding/\">speculative decoding</a>, optimizes an LLM’s latency and throughput <i>without altering the final result</i>. It achieves this by employing a smaller, faster \"drafter\" model to predict a sequence of future tokens. These speculated tokens are then quickly verified in parallel by the larger “target” model. If the draft is accepted, the large model effectively generates multiple tokens in a single step, greatly accelerating the process while guaranteeing that the final output is identical to what the large model would have produced on its own. This approach prioritizes speed and latency reduction, potentially at the cost of increased memory usage and less computational savings, since the larger model still performs substantial work.</p><p data-block-key=\"3ie9m\">In “<a href=\"https://arxiv.org/abs/2405.19261\" target=\"_blank\" rel=\"noopener noreferrer\">Faster Cascades via Speculative Decoding</a>”, we introduce “speculative cascades”, a new approach that combines the best of both cascades and speculative decoding. It delivers better LLM output quality at a lower computational cost than either technique alone by sometimes deferring to the smaller LLM for the sake of efficiency. We tested new speculative cascading techniques against standard cascading and speculative decoding baselines using <a href=\"https://ai.google.dev/gemma/docs\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma</a> and <a href=\"https://research.google/blog/exploring-transfer-learning-with-t5-the-text-to-text-transfer-transformer/\">T5</a> models on various language tasks, including summarization, translation, reasoning, coding, and question answering. The results show that the proposed speculative cascades achieve better cost-quality trade-offs, often yielding higher speed-ups and better quality metrics compared to the baselines.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">A deeper look</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"t9039\">To fully understand and appreciate the speculative cascades approach, we first compare cascades and speculative decoding with a simple example. Imagine you ask an LLM a straightforward question:</p><p data-block-key=\"bt6cf\"><b>Prompt:</b> \"<span class=\"rte-font-courier\">Who is Buzz Aldrin?</span>\"</p><p data-block-key=\"4c9f2\">Let's say we have two models available to answer this: a small, fast \"drafter\" model and a large, powerful \"expert\" model.</p><p data-block-key=\"30s2s\">Here's how they might respond:</p><ul><li data-block-key=\"7afbh\"><b>Small Model:</b> <span class=\"rte-font-courier\">Buzz Aldrin is an American former astronaut, engineer, and fighter pilot, best known as the second person to walk on the Moon.<br><br></span></li><li data-block-key=\"60evu\"><b>Large Model:</b> <span class=\"rte-font-courier\">Edwin \"Buzz\" Aldrin, a pivotal figure in the history of space exploration, is an American former astronaut, engineer, and fighter pilot who is best known for being the second human to walk on the Moon.</span></li></ul><p data-block-key=\"cmc5l\">Both models provide excellent, factually correct answers, but they interpret the user's intent slightly differently. The small model delivers a quick, factual summary, while the large model provides a more formal, encyclopedic-style entry. Depending on the user's need — be it a fast fact or a detailed overview — either response could be considered ideal. The key is that they represent two distinct, equally valid styles.</p><p data-block-key=\"11i2p\">Now, let's see how the two main speed-up techniques handle this scenario.</p><p data-block-key=\"do2eo\">With cascades, the small \"drafter\" model gets the prompt first. If it's confident in its answer, it replies. If not, it defers the entire task to the large \"expert\" model.</p><p data-block-key=\"ajqif\"><b>In our example:</b></p><ol><li data-block-key=\"1c0nq\">The small model generates its concise and correct answer.</li><li data-block-key=\"d3e78\">It checks its confidence and, finding it high, sends the response to the user.</li></ol><p data-block-key=\"45cg5\">This works! We get a great answer quickly. But the process is sequential. If the small model <i>hadn't</i> been confident, we would have wasted time waiting for it to finish, only to then start the large model from scratch. This sequential \"wait-and-see\" approach is a fundamental bottleneck.</p><p data-block-key=\"9b4k2\">With speculative decoding, the small model quickly drafts the first few tokens of the answer, and the large model verifies it in parallel, correcting the first mistake it finds.</p><p data-block-key=\"b7302\"><b>In our example:</b></p><ol><li data-block-key=\"1n312\">The small model drafts the beginning of its answer: [<span class=\"rte-font-courier\">Buzz</span>, <span class=\"rte-font-courier\">Aldrin</span>, <span class=\"rte-font-courier\">is</span>, <span class=\"rte-font-courier\">an</span>, ...]</li><li data-block-key=\"b56s2\">The large model verifies this draft. Its own preferred first token is <span class=\"rte-font-courier\">Edwin</span>.</li><li data-block-key=\"5j6np\">Since <span class=\"rte-font-courier\">Buzz</span> ≠ <span class=\"rte-font-courier\">Edwin</span>, the very first token is a mismatch.</li><li data-block-key=\"ci65q\">The entire draft is <i>rejected</i> and the first token is replaced with <span class=\"rte-font-courier\">Edwin</span>. The process then repeats from this corrected point to generate the rest of the answer, but the initial speed advantage has been lost.</li></ol><p data-block-key=\"9btog\">Even though the small model produced a good answer, the requirement to match the large model token-by-token forces a rejection. We lose the speed benefit and end up with an answer that is not necessarily superior. While the above example uses a simple token matching rejection rule, in the full paper, we also include the potential for a \"probabilistic match\" that provides greater flexibility in the token-by-token comparison.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Different goals, different trade-offs</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"t9039\">The \"<span class=\"rte-font-courier\">Buzz Aldrin</span>\" example reveals a fundamental difference between these two techniques, as summarized below:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-0.5-Table.width-1250.png\" alt=\"SpecCascades-0.5-Table\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-0.5-Table.width-1250.png\" alt=\"SpecCascades-0.5-Table\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-1-TradeOffs.width-1250.png\" alt=\"SpecCascades-1-TradeOffs\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-1-TradeOffs.width-1250.png\" alt=\"SpecCascades-1-TradeOffs\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"f3ur2\"><i>A visual representation of the trade-offs offered by standard cascades (</i><b><i>left</i></b><i>) and speculative decoding (</i><b><i>right</i></b><i>). In both graphs, the green star is the small, fast model (low cost, lower quality) and the red star is the large, slow model (high cost, higher quality). The dots in the left graph represent different trade-offs offered by cascades by varying its confidence threshold; the blue star in the right graph represents the trade-off offered by speculative decoding.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Speculative cascades: Best of both worlds</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"t9039\">Speculative cascades combine the idea of tiered processing from standard cascades with the speedup mechanism of speculative decoding. It involves a smaller model generating a \"draft\" output that a larger model then quickly verifies in parallel. The key innovation is replacing the strict verification of speculative decoding with a flexible “deferral rule”<b>.</b> This rule dynamically decides, on a token-by-token basis, whether to accept the small model's draft or defer to the large model. This avoids the sequential bottleneck of standard cascades while allowing the system to accept a good answer from the small model even if it doesn't exactly match the large model's preferred output.</p><p data-block-key=\"7ecph\"><b>In our example:</b></p><ol><li data-block-key=\"7cjdd\">The small model drafts the beginning of its answer: [<span class=\"rte-font-courier\">Buzz</span>, <span class=\"rte-font-courier\">Aldrin</span>, <span class=\"rte-font-courier\">is</span>, <span class=\"rte-font-courier\">an</span>, ...]</li><li data-block-key=\"26jv4\">Simultaneously, the large model evaluates the draft, providing its own scores.</li><li data-block-key=\"e9v8v\"><i>The crucial step:</i> A flexible deferral rule looks at both outputs and decides whether a deferral is warranted.</li><li data-block-key=\"aidge\">If the system decides <i>not to defer</i>, it accepts the small model's draft tokens. The process then efficiently repeats from this new point, drafting and verifying the next chunk of text until the answer is complete.</li></ol><p data-block-key=\"1nk0\">The power of this method lies in its flexibility, as the deferral rule can be tailored to different needs.</p><p data-block-key=\"6s5o7\">For example, we could tell the system to defer based on:</p><ul><li data-block-key=\"1j410\"><i>A simple confidence check</i>: Defer only if the small model isn't very confident in its own prediction.</li><li data-block-key=\"67efs\"><i>A comparative check</i>: Defer if the large model is significantly more confident than the small model.</li><li data-block-key=\"7ees\"><i>A cost-benefit analysis</i>: Defer only if the large model's confidence boost outweighs the \"cost\" of rejecting the small model's draft.</li><li data-block-key=\"6u3i0\"><i>A token-specific check</i>: Given an \"approved list\" of the best next words according to the large model (its top-ranked tokens), we defer if the small model's drafted token is <i>not</i> on this list.</li></ul><p data-block-key=\"61ar4\">This ability to plug in different decision-making logic is what gives speculative cascades their unique blend of speed, quality, and adaptability.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-2-BlockDiagram.width-1250.png\" alt=\"SpecCascades-2-BlockDiagram\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-2-BlockDiagram.width-1250.png\" alt=\"SpecCascades-2-BlockDiagram\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"f3ur2\"><i>Block diagram illustrating a speculative cascade between a small and large model. As with standard speculative decoding, the drafting process involves auto-regressive sampling from the small drafter model. However, the verification process is different: it considers the combined output distribution of both the small and large models via a deferral rule, rather than solely relying on the large model's output.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"t9039\">Below, we visualize the behaviour of speculative cascading versus speculative decoding on a prompt from the <a href=\"https://github.com/openai/grade-school-math\" target=\"_blank\" rel=\"noopener noreferrer\">GSM8K dataset</a>. The prompt asks, “Mary has 30 sheep. She gets 1 kg of milk from half of them and 2 kg of milk from the other half every day. How much milk does she collect every day?“ By carefully leveraging the small model's output on certain tokens, speculative cascading can reach a correct solution faster than regular speculative decoding.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-3-Comparison.width-800.gif\" alt=\"SpecCascades-3-Comparison\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-3-Comparison.width-800.gif\" alt=\"SpecCascades-3-Comparison\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"f3ur2\"><i>Comparison of speculative cascades and speculative decoding on a grade school math question from the GSM8K dataset</i>.<i> The draft tokens are shown in yellow and the verified tokens in red. The speculative cascades approach generates the correct answer, and does so faster than speculative decoding.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Experiments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"t9039\">We tested speculative cascades on a range of benchmarks, including summarization, reasoning, and coding. The results show a clear advantage over speculative decoding. On a standard quality-versus-efficiency graph, speculative cascades consistently provide better trade-offs. This means for the same quality level as speculative decoding, our method is faster, i.e., generates more tokens per call to the larger model.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-4-Performance.width-1250.png\" alt=\"SpecCascades-4-Performance\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SpecCascades-4-Performance.width-1250.png\" alt=\"SpecCascades-4-Performance\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"f3ur2\"><i>Speculative cascades variants (blue and orange) achieve better quality-latency trade-offs compared to standard speculative decoding (green star) on math reasoning and summarization tasks. See</i> <a href=\"https://arxiv.org/abs/2405.19261\" target=\"_blank\" rel=\"noopener noreferrer\"><i>paper</i></a><i> for details.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Towards faster and smarter AI with speculative cascades</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"t9039\">As LLMs become more integrated into daily applications, optimizing their performance isn’t just a technical goal, it’s a practical necessity. By rethinking how cascades and speculative decoding can work together, speculative cascades provide a more powerful and flexible tool for developers. This hybrid approach allows for fine-grained control over the cost-quality balance, paving the way for applications that are both smarter and faster.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"t9039\"><i>This work is a collaborative effort with Wittawat Jitkrittum, Ankit Singh Rawat, Seungyeon Kim, Neha Gupta and Sanjiv Kumar. We are grateful to Ananda Theertha Suresh and Ziteng Sun for their insightful discussions, and Yale Cong, Mark Simborg, and Kimberly Schwede for their help in crafting this blog.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "使用NucleoBench和AdaBeam进行更智能的核酸设计 (原标题: Smarter nucleic acid design with NucleoBench and AdaBeam)",
      "link": "https://research.google/blog/smarter-nucleic-acid-design-with-nucleobench-and-adabeam/",
      "pubDate": "Wed, 10 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-10T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 使用NucleoBench和AdaBeam实现更智能的核酸设计\n\n### 引言：核酸设计的挑战\n\n在现代医学中，设计具有特定治疗特性的新型DNA和RNA序列是一项关键挑战。这些分子是下一代疗法（如更精确的CRISPR基因疗法和更稳定有效的mRNA疫苗）的基石。然而，寻找正确的序列极其困难，例如，一个小的RNA功能区（5' UTR）可能有超过2x10^120种序列组合，使得穷举搜索优化其功能变得不可能。尽管AI模型在预测核酸序列特性方面取得了巨大进展，但利用这些模型生成最佳序列的算法仍有创新空间。缺乏标准化的评估阻碍了将强大的预测模型转化为最佳治疗分子的进程。\n\n### 解决方案：NucleoBench和AdaBeam\n\n为了解决这一差距，Google Research和Move37 Labs合作推出了**NucleoBench**，这是首个用于比较核酸设计算法的大规模标准化基准。通过在16个不同的生物学挑战中运行超过400,000次实验，研究团队创建了一个严格评估和理解不同算法性能的框架。基于这些洞察，他们开发了**AdaBeam**，一种混合设计算法，在16项任务中的11项上优于现有方法，并能更有效地扩展到定义生物学AI未来的大型复杂模型。AdaBeam及其所有算法实现均已免费提供，以促进进一步创新。\n\n### 计算核酸设计的核心挑战与工作流程\n\n计算机辅助设计新核酸序列通常遵循四个步骤：\n1.  **生成数据**：收集具有所需特性（例如，与癌症相关蛋白结合）的高质量序列数据集。\n2.  **训练预测模型**：使用这些数据训练一个模型（通常是神经网络），该模型可以根据DNA或RNA序列预测其特性。\n3.  **生成候选序列**：这是关键的设计步骤。使用优化算法生成模型预测具有最高所需特性的新序列。\n4.  **验证候选序列**：在湿实验室中合成并测试最有前景的序列，以验证其是否如预测般工作。\n5.  **重新训练（可选）**：根据验证数据重新训练模型。\n\n![计算核酸设计的典型工作流程](https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-1.width-1250.png)\n\n*计算核酸设计的典型工作流程。本研究重点关注步骤3的设计算法。*\n\n目前，不同的研究团队使用不同的算法并在不同的任务上进行测试，这使得无法确定哪些方法是真正最好的。大多数现有基准依赖于模拟退火或香草遗传算法等方法，这些算法在现代深度学习出现前几十年就已经开发出来，无法利用神经网络模型中的关键信息（如梯度）。\n\n### NucleoBench基准的详细介绍\n\n为了创建一个全面且公平的基准，研究团队选择了多种无梯度和基于梯度的设计算法。无梯度算法包括定向进化和模拟退火等成熟方法，它们分别受到进化和物理过程的启发。这些算法将预测性AI模型视为“黑箱”，在无需理解模型内部工作原理的情况下测试新序列。它们的优势在于简单性和广泛适用性，但也可能因此错过模型提供的宝贵线索。基于梯度的设计算法利用神经网络的内部工作原理，包括FastSeqProp和Ledidi等更现代的算法。它们使用模型的梯度（即最陡峭改进的方向）智能地指导搜索更好的序列，但计算时间比仅使用神经网络输出更长。\n\nNucleoBench是迄今为止最全面的核酸设计算法基准，允许对算法进行公平的“同类比较”。研究团队在相同的16项任务上，使用相同的起始序列评估了9种不同的算法，从而获得了前所未有的统计能力来得出有意义的结论。这些任务涵盖了广泛的生物学挑战，包括：\n*   控制特定细胞类型（如肝细胞或神经元细胞）中的基因表达。\n*   最大化转录因子（调节基因的蛋白质）的结合。\n*   改善染色质的物理可及性以进行生物分子相互作用。\n*   使用Enformer等大规模模型预测超长DNA序列的基因表达。\n\n| 任务类别                 | 描述                                                                                              | 任务数量 | 序列长度 (bp) | 速度 (ms / 示例) |\n| :----------------------- | :------------------------------------------------------------------------------------------------ | :------- | :------------ | :--------------- |\n| 细胞类型特异性顺式调控活性 | DNA序列如何控制来自同一DNA分子的基因表达。细胞类型包括：前体血细胞、肝细胞、神经元细胞。 | 3        | 200           | 2                |\n| 转录因子结合             | 特定转录因子与特定DNA片段结合的可能性。                                                           | 11       | 3000          | 55               |\n| 染色质可及性             | DNA与其他分子相互作用的物理可及性。                                                               | 1        | 3000          | 260              |\n| 选择性基因表达           | 基因表达预测。                                                                                    | 1        | 196,608 / 256*| 15,000           |\n\n*NucleoBench设计任务总结。* *模型输入长度为200K碱基对（bp），但只编辑256 bp。\n\n研究团队引入了计算机科学中的有序和无序束搜索算法，以测试序列编辑顺序的固定方法与更灵活的随机顺序方法之间的比较。他们还创建了**Gradient Evo**，这是一种新颖的混合算法，通过使用模型梯度指导其突变来增强定向进化算法，以独立评估梯度对于编辑位置选择与选择特定编辑的重要性。\n\n### AdaBeam：一种创新的混合自适应束搜索算法\n\n研究团队还开发了**AdaBeam**，一种混合自适应束搜索算法，它结合了无序束搜索和AdaLead（一种表现最佳的非梯度设计算法）中最有效的元素。自适应搜索算法通常不会随机探索；相反，它们的行为会随着搜索结果而改变，以将其精力集中在序列空间中最有前景的区域。AdaBeam的混合方法维护一个“束”（即迄今为止找到的最佳候选序列集合），并贪婪地扩展特别有前景的候选序列，直到它们被充分探索。\n\n在实践中，AdaBeam从一组候选序列及其分数开始。在每一轮中，它首先选择一小部分得分最高的序列作为“父代”。对于每个父代，AdaBeam通过进行随机数量的随机但有指导的突变来生成一组新的“子代”序列。然后，它遵循一条简短的贪婪探索路径，使算法能够快速在适应度景观中“上坡”。经过充分探索后，所有新生成的子代都被汇集在一起，算法选择绝对最佳的序列形成下一轮的起始种群，重复该循环。这种自适应选择和有针对性突变的过程使AdaBeam能够高效地专注于高性能序列。\n\n计算机辅助设计任务由于其极其庞大的搜索空间而带来了困难的工程问题。随着我们尝试设计更长的序列（如mRNA序列）并使用现代大型神经网络来指导设计，这些困难变得更加突出。AdaBeam通过使用固定计算的概率采样（而不是随序列长度扩展的计算）在长序列上特别高效。为了使AdaBeam能够与大型模型配合使用，研究团队通过引入一种称为“梯度拼接”的技巧来减少设计过程中的峰值内存消耗。然而，现有不具备这些功能的设计算法难以扩展到长序列和大型模型，特别是基于梯度的算法受影响更大。为了促进公平比较，研究团队限制了设计序列的长度，即使AdaBeam可以处理更长更大的序列。例如，尽管DNA表达预测模型Enformer运行在约200K核苷酸序列上，但设计仅限于256个核苷酸。\n\n![NucleoBench中的设计算法总结](https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-2.width-1250.png)\n\n*NucleoBench中的设计算法总结。实线下方是本研究中设计出的算法。*\n\n### 评估结果\n\n研究团队根据每个算法生成的序列的最终适应度分数来评估每个设计算法。适应度分数定义为序列根据预测模型在生物学任务上的表现。为确保公平性，他们进行了超过400,000次实验，其中每个设计算法在每个任务上都获得了固定的时间量和完全相同的100个起始序列。他们还测量了收敛速度，跟踪每个算法找到其最佳解决方案的速度，因为更快的算法可以节省宝贵的时间和计算资源。\n\n研究团队通过测量算法的最终分数受随机机会和起始序列影响的程度来表征性能变异性。他们通过使用五个不同的随机种子重新运行实验来量化算法随机性的影响。为了评估起始点的影响，他们分析了给予每个设计算法的100个相同起始序列的最终分数方差。他们使用Friedman检验来调查是否存在“本质上困难的起始序列”，即所有算法都难以优化的序列。\n\n为了评估性能排名的分布，研究团队比较了NucleoBench基准中每个实验中九种算法在每个任务和起始序列独特组合下的最终性能。然后分配一个基于排名的“顺序分数”（0到8），其中0分分配给表现最佳的算法，1分分配给第二名，依此类推。每个小提琴形状是通过聚合单个算法在超过400,000次实验中获得的所有排名分数构建的，小提琴在任何一点的宽度表示该算法获得特定排名的频率。\n\n![每种算法最终分数的分布](https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-3.width-1250.png)\n\n*每种算法最终分数的分布。X轴是设计算法，Y轴是聚合顺序分数。顺序分数通过根据每个（任务，起始序列）对的所有最终序列的性能，为每个（任务，起始序列，设计算法）元组分配一个整数[0, 9]来确定。0是表现最好的。聚合分数通过对所有此类分数进行平均计算。*\n\n在现有方法中，基于梯度的方法是主导者。然而，研究团队发现AdaBeam超越了它们，这表明依赖梯度并非实现顶级性能和可扩展性的唯一途径。\n\nAdaBeam在几个关键方面改进了以前的方法：\n*   **效率**：它用更快的计算取代了AdaLead的采样步骤，使长序列的速度提高了一倍。\n*   **智能探索**：它使用一种明显更有效的“无序”方法来决定在哪里编辑序列。\n*   **先进工程**：它使用梯度拼接显著减少内存使用，从而能够应用于Enformer等大型模型。\n\n在NucleoBench的16项任务中，AdaBeam在11次中表现最佳。它还被证明是收敛到高质量解决方案最快的算法之一，展示了卓越的扩展特性，这对于应对生物学中下一代AI挑战至关重要。\n\n### 未来方向与负责任的创新\n\nNucleoBench基准揭示了严格标准化评估的重要性，并发现了令人惊讶的结果，例如初始序列的关键影响以及一些既定算法特征的无效性。然而，重大挑战依然存在。最好的基于梯度的方法仍然难以扩展到最大的模型和最长的序列，通过更好的软件工程可以实现显著的可扩展性提升。虽然AdaBeam算法树立了新的技术标杆，但未来的工作必须专注于符合生物学约束并提高可扩展性的算法。\n\n这项工作的核心原则是对生物安全和负责任创新的承诺。AdaBeam代表了生物序列设计向前迈出的一步，但它仅根据预先存在的预测模型改进优化。换句话说，它是一个优化器，而不是一个原创者；该算法只能设计序列以最大化用户提供的预测模型定义的目标。通过将AdaBeam作为开源工具发布，研究团队赋能研究人员，同时确保“人在回路中”仍然是生物分子设计的核心。像AdaBeam这样的算法可以帮助科学家设计更有效的mRNA疫苗，创建更安全的CRISPR基因疗法，并开发针对各种疾病的新型疗法，使AI驱动的药物发现的承诺更接近现实。\n\n### 致谢\n\n这项工作是Joel Shor（Move37 Labs）、Erik Strand（Move37 Labs, MIT）和Cory Y. McLean（Google Research）之间的合作成果。感谢Sager Gosai、Daniel Friedman、Anna Lewis、Vikram Agarwal和Michael Brenner在整个项目中的指导、讨论和支持。",
      "shortSummary": "核酸序列设计对现代医学至关重要，但面临巨大的序列空间和缺乏标准化评估的挑战。Google Research和Move37 Labs为此推出了**NucleoBench**，一个大规模标准化基准，并基于此开发了**AdaBeam**。AdaBeam是一种混合设计算法，在16项生物学任务中的11项上超越现有方法，并能高效扩展到大型模型。它通过智能探索、高效采样和内存优化，加速了AI驱动的药物发现。AdaBeam已开源，旨在促进生物序列设计的创新，并强调负责任的“人在回路中”的设计原则。",
      "translated_title": "使用NucleoBench和AdaBeam进行更智能的核酸设计",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-1.width-1250.png",
          "alt": "NucleoBench-1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-2.width-1250.png",
          "alt": "NucleoBench-2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-3.width-1250.png",
          "alt": "The distribution of final scores for each algorithm",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"n21gz\">Designing new DNA and RNA sequences with specific therapeutic properties is a critical challenge in modern medicine. These molecules are the building blocks for next-generation treatments, from more precise <a href=\"https://en.wikipedia.org/wiki/CRISPR_gene_editing\" target=\"_blank\" rel=\"noopener noreferrer\">CRISPR</a> gene therapies to more stable and effective <a href=\"https://pubmed.ncbi.nlm.nih.gov/39608721/\" target=\"_blank\" rel=\"noopener noreferrer\">mRNA vaccines</a>. However, finding the right sequence is like searching for a single grain of sand on a vast beach. For instance, a small functional region of an RNA molecule called the <a href=\"https://en.wikipedia.org/wiki/Five_prime_untranslated_region\" target=\"_blank\" rel=\"noopener noreferrer\">5' UTR</a> can be one of over 2x10<sup class=\"superscript\">120</sup> possible sequences, making a brute-force search to optimize its function impossible.</p><p data-block-key=\"b5st1\">What if we could use AI to navigate this vast search space, drastically cutting down the time and cost of drug discovery? While various efforts have made <a href=\"https://www.biorxiv.org/content/10.1101/2024.12.25.630221v2\" target=\"_blank\" rel=\"noopener noreferrer\">great strides</a> in developing AI models that <a href=\"https://www.nature.com/articles/s41592-021-01252-x\" target=\"_blank\" rel=\"noopener noreferrer\"><i>predict</i> the properties</a> of a given nucleic acid sequence, there remains opportunity to innovate on the algorithms that use these models to <i>generate</i> optimal sequences. A lack of standardized evaluation hinders progress and prevents us from translating powerful predictive models into the best possible therapeutic molecules.</p><p data-block-key=\"ai4e3\">To address this gap, in a research collaboration between Google Research and <a href=\"https://move37labs.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Move37 Labs</a>, we <a href=\"https://www.biorxiv.org/content/10.1101/2025.06.20.660785v3\" target=\"_blank\" rel=\"noopener noreferrer\">introduce NucleoBench</a>, the first large-scale, standardized benchmark for comparing nucleic acid design algorithms. By running over 400,000 experiments across 16 distinct biological challenges, we've created a framework to rigorously evaluate and understand how different algorithms perform. The insights from this work enabled us to develop <a href=\"https://pypi.org/project/nucleobench/\" target=\"_blank\" rel=\"noopener noreferrer\">AdaBeam</a>, a hybrid design algorithm that outperforms existing methods on 11 of the 16 tasks and scales more effectively to the large and complex models that are defining the future of AI in biology. We have made AdaBeam and all of our algorithm implementations <a href=\"https://github.com/move37-labs/nucleobench\" target=\"_blank\" rel=\"noopener noreferrer\">freely available</a> to spur further innovation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The core challenge</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"n21gz\">The process of designing a new nucleic acid sequence using computers generally follows four steps:</p><ol><li data-block-key=\"d6smh\"><b>Generate data</b>: Collect a high-quality dataset of sequences with the desired property (e.g., binding to a cancer-related protein).</li><li data-block-key=\"9iv8s\"><b>Train a predictive model</b>: Use this data to train a model (often a neural network) that can predict the property from a DNA or RNA sequence.</li><li data-block-key=\"bej4p\"><b>Generate candidate sequences</b>: This is the crucial design step. Use an optimization algorithm to generate new sequences that the model predicts will have the highest possible score for the desired property.</li><li data-block-key=\"6g6pm\"><b>Validate candidates</b>: Synthesize and test the most promising sequences in a wet lab to see if they work as predicted.</li><li data-block-key=\"dsv4a\"><b>Retrain</b> [Optional]: Retrain the model on validation data.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-1.width-1250.png\" alt=\"NucleoBench-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-1.width-1250.png\" alt=\"NucleoBench-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"3cj2n\"><i>The typical workflow for computational nucleic acid design.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"n21gz\">In this work we focus on the design algorithms of step 3. At present, different research groups use different algorithms and test them on different tasks, making it impossible to know which methods are truly the best. Most existing benchmarks rely on algorithms like <a href=\"https://en.wikipedia.org/wiki/Simulated_annealing\" target=\"_blank\" rel=\"noopener noreferrer\">simulated annealing</a> or vanilla <a href=\"https://en.wikipedia.org/wiki/Genetic_algorithm\" target=\"_blank\" rel=\"noopener noreferrer\">genetic algorithms</a>, which were developed decades before modern deep learning and cannot take advantage of crucial information, like gradients, from the neural network models.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">NucleoBench</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"n21gz\">To create a comprehensive and fair benchmark, we selected a diverse slate of gradient-free and gradient-based design algorithms. Gradient-free algorithms include well-established methods like <a href=\"https://www.nature.com/articles/nrm2805\" target=\"_blank\" rel=\"noopener noreferrer\">directed evolution</a> and simulated annealing, which are inspired by processes in evolution and physics, respectively. These algorithms treat the predictive AI model as a \"black box\", and test new sequences without needing to understand <i>how</i> the model works internally. Their strength lies in their simplicity and broad applicability, but this means they potentially miss out on valuable clues from the model.</p><p data-block-key=\"cl0jv\">Gradient-based design algorithms leverage the internal workings of neural networks and include more modern algorithms like <a href=\"https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04437-5\" target=\"_blank\" rel=\"noopener noreferrer\">FastSeqProp</a> and <a href=\"https://www.biorxiv.org/content/10.1101/2020.05.21.109686v1\" target=\"_blank\" rel=\"noopener noreferrer\">Ledidi</a>. They use the model's gradients (i.e., the direction of steepest improvement) to intelligently guide the search for better sequences, but take longer to compute than just using the output of the neural network.</p><p data-block-key=\"9rg8h\">To our knowledge, NucleoBench is the most comprehensive benchmark for nucleic acid design algorithms thus far and allows for a fair, apples-to-apples comparison between algorithms. We evaluated 9 different algorithms on the same 16 tasks with the same starting sequences, giving us unprecedented statistical power to draw meaningful conclusions. These tasks span a wide range of biological challenges, including:</p><ul><li data-block-key=\"1m5be\"><b>Controlling gene expression</b> in specific cell types (e.g., liver or neuronal cells)</li><li data-block-key=\"dggfc\"><b>Maximizing the binding of transcription factors</b> (proteins that regulate genes)</li><li data-block-key=\"1l06b\"><b>Improving the physical accessibility of chromatin</b> for biomolecular interactions</li><li data-block-key=\"27hrt\"><b>Predicting gene expression from very long DNA sequences</b> using large-scale models like <a href=\"https://www.nature.com/articles/s41592-021-01252-x\" target=\"_blank\" rel=\"noopener noreferrer\">Enformer</a></li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    <div class=\"rich-text --theme- --mode-\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <table>\n<tbody>\n<tr>\n<td><strong>Task Category</strong></td>\n<td><strong>Description</strong></td>\n<td><strong>Num Tasks</strong></td>\n<td><strong>Seq Len (bp)</strong></td>\n<td><strong>Speed (ms / example)</strong></td>\n</tr>\n<tr>\n<td><a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC10441439/\" target=\"_blank\" rel=\"noopener noreferrer\">Cell-type specific cis-regulatory activity</a></td>\n<td>How DNA sequences control gene expression from the same DNA molecule. Cell types include: precursor blood cells, liver cells, neuronal cells</td>\n<td>3</td>\n<td>200</td>\n<td>2</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/jmschrei/bpnet-lite\" target=\"_blank\" rel=\"noopener noreferrer\">Transcription factor binding</a></td>\n<td>How likely a specific transcription factor will bind to a particular stretch of DNA</td>\n<td>11</td>\n<td>3000</td>\n<td>55</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/jmschrei/bpnet-lite\" target=\"_blank\" rel=\"noopener noreferrer\">Chromatin accessibility</a></td>\n<td>How physically accessible DNA is for interactions with other molecules</td>\n<td>1</td>\n<td>3000</td>\n<td>260</td>\n</tr>\n<tr>\n<td><a href=\"https://www.nature.com/articles/s41592-021-01252-x\" target=\"_blank\" rel=\"noopener noreferrer\">Selective gene expression</a></td>\n<td>Prediction of gene expression</td>\n<td>1</td>\n<td>196,608 / 256*</td>\n<td>15,000</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<p style=\"text-align: center;\"><em><small>Summary of design tasks in NucleoBench. *Model input length is 200K base pairs (bp), but only 256 bp are edited.</small></em></p>\n</div>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"n21gz\">We introduced ordered and unordered <a href=\"https://en.wikipedia.org/wiki/Beam_search\" target=\"_blank\" rel=\"noopener noreferrer\">beam search</a> algorithms, staples from computer science, to test how fixing the order of sequence edits compares to a more flexible, random-order approach. We also created Gradient Evo, a novel hybrid that enhances the directed evolution algorithm by using model gradients to guide its mutations to independently evaluate how important gradients were for edit location selection versus selecting a specific edit.</p><p data-block-key=\"4nku6\">We also developed AdaBeam, a hybrid adaptive beam search algorithm that combines the most effective elements of unordered beam search with <a href=\"https://arxiv.org/abs/2010.02141\" target=\"_blank\" rel=\"noopener noreferrer\">AdaLead</a>, a top-performing, non-gradient design algorithm. Adaptive search algorithms don't typically explore randomly; instead, their behavior changes as a result of the search to focus their efforts on the most promising areas of the sequence space. AdaBeam’s hybrid approach maintains a \"beam\", or a collection of the best candidate sequences found so far, and greedily expands on particularly promising candidates until they’ve been sufficiently explored.</p><p data-block-key=\"eggb8\">In practice, AdaBeam begins with a population of candidate sequences and their scores. In each round, it first selects a small group of the highest-scoring sequences to act as \"parents\". For each parent, AdaBeam generates a new set of \"child\" sequences by making a random number of random-but-guided mutations. It then follows a short, greedy exploration path, allowing the algorithm to quickly \"walk uphill\" in the fitness landscape. After sufficient exploration, all the newly generated children are pooled together, and the algorithm selects the absolute best ones to form the starting population for the next round, repeating the cycle. This process of adaptive selection and targeted mutation allows AdaBeam to efficiently focus on high-performing sequences.</p><p data-block-key=\"burv\">Computer-assisted design tasks pose difficult engineering problems, owing to the incredibly large search space. These difficulties become more acute as we attempt to design longer sequences, such as mRNA sequences, and use modern, large neural networks to guide the design. AdaBeam is particularly efficient on long sequences by using fixed-compute probabilistic sampling instead of computations that scale with sequence length. To enable AdaBeam to work with large models, we reduce peak memory consumption during design by introducing a trick we call “gradient concatenation.” However, existing design algorithms that don’t have these features have difficulty scaling to long sequences and large models. Gradient-based algorithms are particularly affected. To facilitate a fair comparison, we limit the length of the designed sequences, even though AdaBeam can scale longer and larger. For example, even though the DNA expression prediction model <a href=\"https://www.nature.com/articles/s41592-021-01252-x\" target=\"_blank\" rel=\"noopener noreferrer\">Enformer</a> runs on ~200K nucleotide sequences, we limit design to just 256 nucleotides.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-2.width-1250.png\" alt=\"NucleoBench-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-2.width-1250.png\" alt=\"NucleoBench-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"3cj2n\">Summary of design algorithms in NucleoBench. Below the solid line are design algorithms devised in this work.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Evaluation</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"n21gz\">We evaluate each design algorithm based on the final fitness score of the sequence each produced. The fitness score is defined as how well the sequence performed on the biological task according to the predictive model. To ensure fairness, we ran over 400,000 experiments where each design algorithm was given a fixed amount of time and the exact same 100 starting sequences for each task. We also measured the convergence speed, tracking how quickly each algorithm found its best solution, as faster algorithms save valuable time and computational resources.</p><p data-block-key=\"ei839\">We characterized performance variability by measuring how much an algorithm's final score was influenced by random chance versus its starting sequence. We quantified the effect of algorithmic randomness by re-running experiments with five different random seeds. To assess the impact of the starting point, we analyzed the variance in final scores across the 100 identical start sequences given to each design algorithm. We used a <a href=\"https://en.wikipedia.org/wiki/Friedman_test\" target=\"_blank\" rel=\"noopener noreferrer\">Friedman test</a> to investigate whether \"intrinsically difficult start sequences\", or sequences that are hard for all algorithms to optimize, exist.</p><p data-block-key=\"a3ndb\">To assess the distribution of performance ranks, we compared the final performance for each of the nine algorithms across every experiment in the NucleoBench benchmark for each unique combination of a task and a starting sequence. A rank-based \"order score\" from 0 to 8 was then assigned, with 0 going to the best-performing algorithm, 1 to the second-best, and so on. Each violin shape is constructed by aggregating all the rank scores a single algorithm received across the 400,000+ experiments, with the width of the violin at any point showing how frequently that algorithm achieved a particular rank.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-3.width-1250.png\" alt=\"The distribution of final scores for each algorithm\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/NucleoBench-3.width-1250.png\" alt=\"The distribution of final scores for each algorithm\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"3cj2n\"><i>The distribution of final scores for each algorithm. X-axis is the design algorithm, y-axis is the aggregate order score. Order scores are determined by assigning an integer [0, 9] for each (task, start sequence, design algorithm) tuple according to the performances of all the final sequences for that (task, start sequence) pair. 0 is the top performer. Aggregate scores are computed by averaging over all such scores.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"n21gz\">Gradient-based methods were the reigning champions amongst existing methods. However, we found that AdaBeam outperformed them, demonstrating that relying on gradients is not the only path to top-tier performance and scalability.</p><p data-block-key=\"2pdug\">AdaBeam improves upon previous methods in several key ways:</p><ul><li data-block-key=\"96hff\"><b>Efficiency</b>: It replaces AdaLead’s sampling step with a faster calculation, doubling its speed on long sequences.</li><li data-block-key=\"75nk7\"><b>Smart Exploration</b>: It uses a significantly more effective \"unordered\" approach to deciding where to edit a sequence.</li><li data-block-key=\"bgefo\"><b>Advanced Engineering</b>: It uses gradient concatenation to substantially reduce memory usage, enabling application to massive models like Enformer.</li></ul><p data-block-key=\"7iutv\">Across the 16 tasks in NucleoBench, AdaBeam was the best-performing algorithm 11 times. It also proved to be one of the fastest to converge on a high-quality solution, demonstrating superior scaling properties that are essential for tackling the next generation of AI challenges in biology.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Future directions</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"n21gz\">Our <a href=\"https://github.com/move37-labs/nucleobench\" target=\"_blank\" rel=\"noopener noreferrer\">NucleoBench</a> benchmark reveals the importance of rigorous, standardized evaluation and uncovers surprising findings, such as the critical impact of the initial sequence and the ineffectiveness of some established algorithm features. However, significant challenges remain. The best gradient-based methods still struggle to scale to the largest models and longest sequences, and substantial scalability gains can be realized through better software engineering. While our new algorithm, AdaBeam, sets a new state-of-the-art, future work must focus on algorithms that adhere to biological constraints and improve scalability.</p><p data-block-key=\"3kltt\">A core principle of our work is a commitment to biosafety and responsible innovation. While AdaBeam represents a step forward for biological sequence design, it only improves the optimization according to a pre-existing predictive model. In other words, it is an optimizer, not an originator; the algorithm can only design sequences to maximize a goal defined by a user-provided predictive model. By releasing AdaBeam as an open-source tool, we empower researchers while ensuring the “human-in-the-loop” remains central to the design of biological molecules. Algorithms like AdaBeam can help scientists design more effective mRNA vaccines, create safer CRISPR gene therapies, and develop novel treatments for a wide range of diseases, bringing the promise of AI-driven drug discovery closer to reality.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"n21gz\"><i>This work represents a collaboration between Joel Shor (Move37 Labs), Erik Strand (Move37 Labs, MIT), and Cory Y. McLean (Google Research). We thank Sager Gosai, Daniel Friedman, Anna Lewis, Vikram Agarwal, and Michael Brenner for their guidance, discussions, and support throughout this project.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "利用AI驱动的经验软件加速科学发现 (原标题: Accelerating scientific discovery with AI-powered empirical software)",
      "link": "https://research.google/blog/accelerating-scientific-discovery-with-ai-powered-empirical-software/",
      "pubDate": "Mon, 08 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-08T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 利用AI驱动的经验软件加速科学发现\n\n## 引言：科学研究的瓶颈与AI的潜力\n\n科学研究中，彻底评估假设对于获得更可靠、更全面的答案至关重要，但所需工作量巨大，形成了阻碍发现进程的瓶颈。特别是，现代科学研究严重依赖计算实验来建模、模拟和分析复杂现象。在这种情况下，假设评估通常需要创建定制软件，这是一项缓慢且具有挑战性的任务。鉴于大型语言模型（LLMs）在执行传统编码任务方面的能力日益增强，研究人员探索了它们是否也能生成高质量的定制软件，以评估和迭代改进科学假设。\n\n## AI系统概述：Gemini驱动的经验软件生成引擎\n\n研究团队发布了一篇论文，描述了一个“旨在帮助科学家编写专家级经验软件的AI系统”，该系统基于Gemini构建。该系统将明确定义的问题和评估方法作为输入，充当一个系统化的代码优化研究引擎：\n\n*   它能提出新颖的方法论和架构概念。\n*   将其实现为可执行代码。\n*   实证验证其性能。\n*   利用树搜索（受AlphaZero启发）优化性能，迭代数千种代码变体。\n\n该系统在基因组学、公共卫生、地理空间分析、神经科学、时间序列预测和数值分析等六个多学科基准上进行了测试，并在所有这些基准上均达到了专家级性能。\n\n## 经验软件与可评分任务\n\n科学研究本质上是迭代的，通常需要研究人员测试数十或数百个模型或参数才能取得突破。即使对于经验丰富的程序员科学家来说，编码、调试和优化软件也极其耗时。手动编码每个新想法既缓慢又低效，使得系统性探索潜在解决方案几乎不可能。\n\n该系统的核心是**经验软件**的创始概念。与通常仅凭功能正确性判断的传统软件不同，经验软件的设计主要目标是最大化预定义的质量分数。可以通过应用经验软件有效解决的问题或挑战被称为**可评分任务**。这些可评分任务在科学、应用数学和工程领域普遍存在。\n\n## 系统工作原理\n\n该系统的输入是一个可评分任务，其中包括问题描述、评分指标以及适用于训练、验证和评估的数据。用户还可以提供上下文，例如来自外部文献的想法或优先考虑的方法论指令。\n\n系统随后生成研究思想，包括已知方法的程序化复现、优化和重组，从而形成新颖且高性能的方法。思想被实现为可执行代码，系统使用带有上置信界（UCB）的树搜索策略来创建软件候选树，并决定哪些候选值得进一步探索。然后，它利用LLM重写代码以尝试提高其质量分数，并能以前所未有的规模详尽且不知疲倦地进行解决方案搜索，快速识别高质量解决方案，将探索时间从数月缩短至数小时或数天。其输出作为编码解决方案，是可验证、可解释和可复现的。\n\n![AI驱动的经验软件概览](https://storage.googleapis.com/gweb-research2023-media/images/AI-powered-empirical_software-overview.width-1250.png)\n\n*算法示意图：将可评分任务和研究思想输入LLM，LLM在沙盒中生成评估代码。此代码随后用于树搜索，其中新节点被创建并使用LLM迭代改进。*\n\n## 效果展示：六大基准测试\n\n代码生成AI系统的评估历来侧重于来自编程竞赛或软件工程的任务，这些任务虽然有价值，但未能捕捉科学发现固有的全部挑战。本研究展示了该系统不仅能编写语法正确的代码，还能为六个多样化且具挑战性的基准问题生成新颖解决方案，这些问题推动了当前计算方法和人类专业知识的边界。这些基准的多样性使研究人员能够集体评估系统在零样本泛化、高维信号处理、不确定性量化、复杂数据语义解释和系统级建模等领域的能力。所有这些基准问题的最高分解决方案均已公开，包括一个交互式网站，供有兴趣复现结果的人探索完整的候选解决方案树。\n\n### 1. 基因组学：单细胞RNA测序数据批次整合\n\n单细胞RNA测序（scRNA-seq）是一种强大的技术，可提供个体细胞水平的基因表达高分辨率视图。联合分析许多不同数据集所需的一个主要挑战是去除样本中存在的复杂批次效应，同时保留真实的生物信号。现有近300种工具可执行scRNA-seq数据批次整合，并已开发出多个基准来评估批次效应去除和生物变异性保留的指标。使用OpenProblems V2.0.0批次整合基准（将13个指标组合成一个总分），该系统发现了40种新方法，其性能优于顶尖专家开发的方法。最高分解决方案（通过成功结合两种现有方法ComBat和BBKNN）比最佳已发表方法（ComBat）总体提高了14%。\n\n![OpenProblems基准测试v2.0.0非对照方法总排行榜](https://storage.googleapis.com/gweb-research2023-media/images/AI-powered-empirical_software-barplot-final.width-1250.png)\n\n*OpenProblems基准测试v2.0.0非对照方法总排行榜。蓝色条表示本系统在有无思想重组以及Gemini深度研究下的结果。点击放大图片。*\n\n### 2. 公共卫生：美国COVID-19住院预测\n\n美国COVID-19预测的主要基准是COVID-19预测中心（CovidHub），这是一项由疾病控制与预防中心（CDC）协调的大型合作项目。CovidHub吸引了数十个专家团队的竞争性且方法多样的提交。他们的任务是预测美国所有州和地区未来一个月的COVID-19新增住院人数。这些预测使用平均加权区间分数（WIS）进行评估，该分数通过总结模型在每个每周预测中所有位置的性能来评估概率预测的质量。然后将单个提交聚合到CovidHub Ensemble模型中，该模型被认为是美国预测COVID-19住院的黄金标准。该系统生成了14个模型，其性能优于官方CovidHub Ensemble。\n\n![时间序列排行榜](https://storage.googleapis.com/gweb-research2023-media/images/AI-powered-empirical_software-leaderboard.width-1250.png)\n\n*时间序列排行榜显示了参与COVID-19预测中心的团队每周预测性能，按绝对平均WIS排序（每个单元格内的数字）。分数汇总了52个司法管辖区和四个预测范围。单元格的背景颜色可视化了相对于CovidHub-ensemble的性能，蓝色表示WIS较低（更好），红色表示WIS较高（更差）。本方法（Google Retrospective），即表格的第一行，优于CovidHub-ensemble。点击放大图片。*\n\n### 3. 地理空间分析：遥感图像分割\n\n高分辨率遥感图像的语义分割是地理空间分析中的一个常见问题，对于监测土地利用、评估人类活动对环境的影响以及管理自然灾害等多种应用至关重要。这项任务涉及准确地为图像中的单个像素分配类别标签，要求模型对场景形成空间和上下文理解，不仅识别存在哪些对象，还要精确确定它们的边界。使用密集标注遥感数据集（DLRSD）基准（使用平均交并比mIoU评估方法），该系统生成的前三个解决方案略优于当前最先进水平，mIoU大于0.80。所有三个解决方案都建立在现有模型、库和策略之上。其中两个利用标准UNet++和U-Net模型，但与ImageNet上预训练的强大编码器配对。第三个使用SegFormer，一种最先进的基于Transformer的架构。所有三个都采用了广泛的测试时增强（TTA）。\n\n![AI驱动的经验软件图像分割](https://storage.googleapis.com/gweb-research2023-media/images/AI-powered-empirical_software-segmentation.width-1250.png)\n\n*遥感分割模型的输入是图像（顶行），输出是新图像，通常称为分割掩码，其中每个像素都被分配一个特定的类别标签。中行是DLRSD基准提供的真实掩码。底行是使用该系统最高分解决方案生成的分割掩码。高分分割模型将与真实掩码具有高度视觉相似性。*\n\n### 4. 神经科学：全脑神经活动预测\n\n研究人员将该方法应用于斑马鱼活动预测基准（ZAPBench），这是一个用于预测斑马鱼整个脊椎动物大脑中超过70,000个神经元活动的最新基准。该系统发现了一种新颖的时间序列预测模型，达到了最先进的性能，超越了所有现有基线。这包括一个计算密集型、基于视频的模型，该模型预测3D体积，并且是之前表现最佳的解决方案。作为概念验证，研究人员还证明了该系统可以设计结合生物物理神经元模拟器（Jaxley）的混合模型，为更可解释的预测模型铺平了道路。\n\n### 5. 数值分析：困难积分的数值评估\n\n该系统在数学领域的困难积分数值评估任务中进行了评估。在此任务中，该系统生成了一个解决方案，正确评估了19个保留积分中的17个，而标准数值方法失败了。\n\n### 6. 通用时间序列预测：GIFT-Eval\n\n最后，研究人员使用通用时间序列预测模型评估（GIFT-Eval）评估了该系统在时间序列预测通用问题上的表现。GIFT-Eval是一个包含28个数据集的基准，跨越七个不同领域，具有从秒到年等10种不同频率。该系统通过在整个GIFT-Eval数据集的平均平均绝对比例误差上进行单代码爬坡，成功地从零开始创建了一个统一的通用预测库。\n\n## 结论：加速科学发现的未来\n\nLLMs的最新进展已经为全球研究人员提供了轻松获取知识和想法的新途径，并且LLMs正日益被视为自动化科学研究中重复繁琐方面的手段。本研究探索了LLMs是否能用于普遍存在、必不可少且极其耗时的任务——生产定制软件以评估和迭代改进科学假设。其动机是设想一个未来，科学家可以轻松、快速、系统地调查数百或数千个潜在解决方案，以解决激发他们研究的问题。该系统能快速生成专家级解决方案，将探索一组想法所需的时间从数月缩短至数小时或数天。这有望为从学生到教授的科学家节省大量时间，使他们能够专注于真正的创造性和批判性挑战，并继续定义和优先处理科学研究可以帮助解决的基础研究问题和社会挑战。",
      "shortSummary": "一项新研究发布了一个基于Gemini的AI系统，旨在通过自动化定制软件生成来加速科学发现。该系统能为科学假设评估提供专家级经验软件，将探索时间从数月缩短至数小时或数天。它在基因组学、公共卫生、地理空间分析等六个多学科基准测试中表现出色，甚至超越了专家水平。通过优化代码和迭代搜索，该系统使科学家能更专注于创新和关键挑战，从而显著提升研究效率。",
      "translated_title": "利用AI驱动的经验软件加速科学发现",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AI-powered-empirical_software-overview.width-1250.png",
          "alt": "AI-powered-empirical software-overview",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AI-powered-empirical_software-barplot-final.width-1250.png",
          "alt": "AI-powered-empirical software-barplot",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AI-powered-empirical_software-leaderboard.width-1250.png",
          "alt": "leaderboard bar plot",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AI-powered-empirical_software-segmentation.width-1250.png",
          "alt": "AI-powered-empirical software-segmentation",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"i5thc\">In scientific research, thoroughly evaluating hypotheses is essential to developing more robust and comprehensive answers, but the required work forms a bottleneck, hindering the pace of discovery. In particular, much of modern scientific research depends on computational experiments to model, simulate, and analyze complex phenomena. Here, hypothesis evaluation often requires creating custom software, a slow and challenging task. Given the increasing capability of large language models (LLMs) to <a href=\"https://cloud.google.com/use-cases/ai-code-generation\" target=\"_blank\" rel=\"noopener noreferrer\">perform traditional coding tasks</a>, we wondered if they could similarly generate high-quality custom software for evaluating and iteratively improving scientific hypotheses.</p><p data-block-key=\"8seu2\">Today we are releasing a paper describing an \"<a href=\"https://arxiv.org/abs/2509.06503\" target=\"_blank\" rel=\"noopener noreferrer\">AI system designed to help scientists write expert-level empirical software</a>\", built using <a href=\"https://ai.google.dev/gemini-api/docs/models\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a>. Taking as input a well-defined problem and a means of evaluation, our system acts as a systematic code-optimizing research engine: it can propose novel methodological and architectural concepts, implement them as executable code and empirically validate their performance. It then searches and iterates through thousands of code variants, using <a href=\"https://en.wikipedia.org/wiki/Search_tree\" target=\"_blank\" rel=\"noopener noreferrer\">tree search</a> to optimize performance. We tested our system using six benchmarks representing distinct multidisciplinary challenges, spanning the fields of genomics, public health, geospatial analysis, neuroscience, time-series forecasting, and numerical analysis. Our system achieves expert-level performance across all of these benchmarks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Empirical software and scorable tasks</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"i5thc\">Scientific research is inherently iterative, often requiring researchers to test dozens or hundreds of models or parameters to achieve a breakthrough. Even for scientists who are experienced programmers, coding, debugging, and optimizing software is incredibly time-consuming. Manually coding each new idea is slow and inefficient, making systematic exploration of potential solutions practically impossible.</p><p data-block-key=\"e64j9\">At the heart of our system lies the foundational concept of empirical software. Unlike conventional software, which is often judged by functional correctness alone, empirical software is designed with a primary objective: to maximize a predefined quality score. A problem or challenge that can be effectively addressed and solved through the application of empirical software is termed a scorable task. These scorable tasks are prevalent across science, applied mathematics, and engineering.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How it works</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"i5thc\">The input to our system is a scorable task, which includes a problem description, a scoring metric, and data suitable for training, validation, and evaluation. A user can also provide context, such as ideas from external literature, or directives for methodologies to prioritize.</p><p data-block-key=\"38jet\">The system then generates research ideas, including programmatic reproduction, optimization, and recombination of known methods, leading to novel and highly performant approaches. Ideas are implemented as executable code and the system uses a tree search strategy with an upper confidence bound (inspired by <a href=\"https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/\" target=\"_blank\" rel=\"noopener noreferrer\">AlphaZero</a>) to create a tree of software candidates and decide which candidates warrant further exploration. It then uses an LLM to rewrite the code to attempt to improve its quality score, and can exhaustively and tirelessly carry out solution searches at an unprecedented scale, identifying high-quality solutions quickly, reducing exploration time from months to hours or days. Its outputs, as coded solutions, are verifiable, interpretable and reproducible.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AI-powered-empirical_software-overview.width-1250.png\" alt=\"AI-powered-empirical software-overview\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AI-powered-empirical_software-overview.width-1250.png\" alt=\"AI-powered-empirical software-overview\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"osvwz\"><i>Schematic of the algorithm that feeds a scorable task and research ideas to an LLM, which generates evaluation code in a sandbox. This code is then used in a tree search, where new nodes are created and iteratively improved using the LLM.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Demonstrated effectiveness</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"i5thc\">The evaluation of code generating AI systems has historically focused on tasks derived from competitive programming or software engineering, which, while valuable, fail to capture the full spectrum of challenges inherent in scientific discovery. We demonstrate proficiency not merely in writing syntactically correct code, but in generating novel solutions to six diverse and challenging benchmark problems that push the boundaries of current computational methods and human expertise. The diversity of these benchmarks allows us to collectively assess proficiency in areas such as <a href=\"https://en.wikipedia.org/wiki/Zero-shot_learning\" target=\"_blank\" rel=\"noopener noreferrer\">zero-shot generalization</a>, <a href=\"https://en.wikipedia.org/wiki/Multidimensional_signal_processing\" target=\"_blank\" rel=\"noopener noreferrer\">high-dimensional signal processing</a>, <a href=\"https://en.wikipedia.org/wiki/Uncertainty_quantification\" target=\"_blank\" rel=\"noopener noreferrer\">uncertainty quantification</a>, <a href=\"https://www.sciencedirect.com/topics/social-sciences/semantic-interpretation\" target=\"_blank\" rel=\"noopener noreferrer\">semantic interpretation</a> of complex data, and <a href=\"https://en.wikipedia.org/wiki/Modelling_biological_systems\" target=\"_blank\" rel=\"noopener noreferrer\">systems-level modeling</a>. The top scoring solutions to each of these benchmark problems are openly available for anyone interested in reproducing our results, including as an <a href=\"https://google-research.github.io/score\" target=\"_blank\" rel=\"noopener noreferrer\">interactive website</a> to explore the full candidate solution trees.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Genomics: Batch integration of single cell RNA sequencing data</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"i5thc\"><a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC8964935/\" target=\"_blank\" rel=\"noopener noreferrer\">Single-cell RNA sequencing</a> (scRNA-seq) is a powerful technology that provides a high-resolution view of gene expression at the individual cell level. A major challenge required to jointly analyze many disparate datasets is to remove complex <a href=\"https://www.nature.com/articles/nrg2825\" target=\"_blank\" rel=\"noopener noreferrer\">batch effects</a> present across samples while preserving true biological signals. <a href=\"https://www.scrna-tools.org/tools?sort=name&amp;cats=Integration\" target=\"_blank\" rel=\"noopener noreferrer\">Nearly 300 tools</a> exist to perform batch integration of scRNA-seq data, and multiple benchmarks have been developed for assessing metrics of batch effect removal and conservation of biological variability. Using the <a href=\"https://openproblems.bio/\" target=\"_blank\" rel=\"noopener noreferrer\">OpenProblems</a> <a href=\"https://openproblems.bio/benchmarks/batch_integration?version=v2.0.0\" target=\"_blank\" rel=\"noopener noreferrer\">V2.0.0 batch integration benchmark</a>, which combines 13 metrics into one overall score, our system discovered 40 novel methods that outperformed top expert-developed methods. The highest-scoring solution achieved a 14% overall improvement over the best published method (<a href=\"https://academic.oup.com/nargab/article/2/3/lqaa078/5909519\" target=\"_blank\" rel=\"noopener noreferrer\">ComBat</a>) by successfully combining two existing methods (ComBat and <a href=\"https://academic.oup.com/bioinformatics/article/36/3/964/5545955\" target=\"_blank\" rel=\"noopener noreferrer\">BBKNN</a>).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    \n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AI-powered-empirical_software-barplot-final.width-1250.png\" alt=\"AI-powered-empirical software-barplot\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AI-powered-empirical_software-barplot-final.width-1250.png\" alt=\"AI-powered-empirical software-barplot\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"osvwz\"><i>Overall leaderboard for OpenProblems benchmark v2.0.0 non-control methods. In blue are results from our system with and without recombination of ideas, and</i> <a href=\"https://gemini.google/overview/deep-research/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Gemini Deep Research</i></a><i>.</i> <a href=\"https://storage.googleapis.com/gweb-research2023-media/images/AI-powered-empirical_software-barplot-final.original.png\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Click</i></a><i> to enlarge image.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Public health: Prediction of U.S. COVID-19 hospitalizations</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"i5thc\">The primary U.S. benchmark for COVID-19 forecasting is the <a href=\"https://covid19forecasthub.org/\" target=\"_blank\" rel=\"noopener noreferrer\">COVID-19 Forecast Hub</a> (CovidHub), a large collaborative effort coordinated by the <a href=\"https://www.cdc.gov/\" target=\"_blank\" rel=\"noopener noreferrer\">Centers for Disease Control and Prevention</a> (CDC). CovidHub attracts competitive and methodologically diverse submissions from dozens of expert-led teams. Their task is to forecast new COVID-19 hospitalizations across all the U.S. states and its territories for up to a month ahead. These forecasts are evaluated using average <a href=\"https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008618\" target=\"_blank\" rel=\"noopener noreferrer\">weighted interval score</a> (WIS), which assesses the quality of probabilistic forecasts by summarizing a model's performance across all locations for every weekly prediction over the season. Individual submissions are then aggregated into the <a href=\"https://www.cdc.gov/cfa-modeling-and-forecasting/covid19-data-vis/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">CovidHub Ensemble model</a>, which is considered the gold standard in the U.S. for forecasting COVID-19 hospitalizations. Our system generated 14 models that outperform the official CovidHub Ensemble.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AI-powered-empirical_software-leaderboard.width-1250.png\" alt=\"leaderboard bar plot\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AI-powered-empirical_software-leaderboard.width-1250.png\" alt=\"leaderboard bar plot\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"osvwz\"><i>Time-series leaderboard showing weekly forecasting performance for teams participating in the COVID-19 Forecast Hub, ordered by absolute average WIS (number within each cell). Scores are aggregated across 52 jurisdictions and four forecast horizons. The cell’s background color visualizes the performance relative to the CovidHub-ensemble, with blue indicating a lower (better) WIS and red indicating a higher (worse) WIS. Our method, the top row of the table (Google Retrospective) outperforms CovidHub-ensemble.</i> <a href=\"https://storage.googleapis.com/gweb-research2023-media/images/AI-powered-empirical_software-leaderboard.original.png\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Click</i></a><i> to enlarge image.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Geospatial analysis: Segmentation of remote sensing images</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"i5thc\">Semantic segmentation of high-resolution <a href=\"https://www.usgs.gov/centers/california-water-science-center/science/science-topics/remote-sensing\" target=\"_blank\" rel=\"noopener noreferrer\">remote sensing</a> images is a common problem in geospatial analysis, and is essential for diverse applications, ranging from <a href=\"https://www.sciencedirect.com/science/article/pii/S0034425722003728\" target=\"_blank\" rel=\"noopener noreferrer\">monitoring land use</a>, <a href=\"https://www.mdpi.com/2072-4292/11/23/2783\" target=\"_blank\" rel=\"noopener noreferrer\">assessing the environmental impacts of human activity</a>, and <a href=\"https://www.researchgate.net/publication/285929471_Remote_Sensing_and_GIS_for_Natural_Hazards_Assessment_and_Disaster_Risk_Management\" target=\"_blank\" rel=\"noopener noreferrer\">managing natural disasters</a>. This task, which involves accurately assigning class labels to individual pixels in an image, requires a model to develop a spatial and contextual understanding of the scene, identifying not just what objects are present, but precisely where their boundaries lie.</p><p data-block-key=\"7jetu\">Using the <a href=\"https://www.mdpi.com/2072-4292/10/6/964\" target=\"_blank\" rel=\"noopener noreferrer\">dense labeling remote sensing dataset</a> (DLRSD) benchmark, which evaluates methods using a mean <a href=\"https://giou.stanford.edu/\" target=\"_blank\" rel=\"noopener noreferrer\">intersection over union</a> (mIoU), the top three solutions generated by our system are slightly better than current state of the art, with mIoU greater than 0.80. All three solutions build upon existing models, libraries and strategies. Two leverage standard <a href=\"https://arxiv.org/abs/1807.10165\" target=\"_blank\" rel=\"noopener noreferrer\">UNet++ and U-Net</a> models but paired with powerful encoders pre-trained on <a href=\"https://www.image-net.org/\" target=\"_blank\" rel=\"noopener noreferrer\">ImageNet</a>. The third uses <a href=\"https://arxiv.org/abs/2105.15203\" target=\"_blank\" rel=\"noopener noreferrer\">SegFormer</a>, a state of the art <a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" rel=\"noopener noreferrer\">Transformer</a>-based architecture. All three employ extensive <a href=\"https://dl.acm.org/doi/10.1145/3065386\" target=\"_blank\" rel=\"noopener noreferrer\">test-time augmentation</a> (TTA).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AI-powered-empirical_software-segmentation.width-1250.png\" alt=\"AI-powered-empirical software-segmentation\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AI-powered-empirical_software-segmentation.width-1250.png\" alt=\"AI-powered-empirical software-segmentation\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"osvwz\"><i>The input to remote sensing segmentation models is an image (</i><b><i>top row</i></b><i>), and the output is a new image, often called a segmentation mask, where each pixel is assigned a specific class label. The</i> <b><i>middle row</i></b><i> is the true mask as provided by the DLRSD benchmark. The</i> <b><i>bottom row</i></b><i> is segmentation masks generated using our system's top scoring solution. High-scoring segmentation models will have close visual similarity to the ground truth mask.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Neuroscience: Whole-brain neural activity prediction</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"i5thc\">We applied our method to the <a href=\"https://research.google/blog/improving-brain-models-with-zapbench/\">Zebrafish Activity Prediction Benchmark</a> (ZAPBench), a recent benchmark for forecasting the activity of over 70,000 neurons across an entire vertebrate brain. Our system discovered a novel <a href=\"https://cloud.google.com/learn/what-is-time-series\" target=\"_blank\" rel=\"noopener noreferrer\">time-series forecasting</a> model that achieved state-of-the-art performance, surpassing all existing baselines. This includes a computationally intensive, <a href=\"https://arxiv.org/abs/2503.00073\" target=\"_blank\" rel=\"noopener noreferrer\">video-based model</a> that forecasts 3D volumes and was the previous top performing solution. As a proof of concept, we also demonstrated that our system can design hybrid models that incorporate a biophysical neuron simulator (<a href=\"https://www.biorxiv.org/content/10.1101/2024.08.21.608979v1\" target=\"_blank\" rel=\"noopener noreferrer\">Jaxley</a>), paving the way for more interpretable predictive models.</p><p data-block-key=\"hc5r\">While each of these examples is compelling in its own right, our system to generate empirical software is striking in its generalizability. We additionally evaluated our system in the context of mathematics on the task of numerical evaluation of difficult integrals. In this task, our system generated a solution that correctly evaluated 17 out of 19 held-out <a href=\"https://en.wikipedia.org/wiki/Integral\" target=\"_blank\" rel=\"noopener noreferrer\">integrals</a>, where the <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.quad.html\" target=\"_blank\" rel=\"noopener noreferrer\">standard</a> numerical method failed. Lastly, we evaluated our system on the general problem of time series forecasting, using the <a href=\"https://www.salesforce.com/blog/gift-eval-time-series-benchmark/\" target=\"_blank\" rel=\"noopener noreferrer\">General Time Series Forecasting Model Evaluation</a> (GIFT-Eval), a benchmark derived from 28 datasets spanning seven diverse domains, with 10 different frequencies, from seconds to years. Our system successfully created a unified, general purpose forecasting library from scratch, by hill climbing with a single code on the average <a href=\"https://en.wikipedia.org/wiki/Mean_absolute_scaled_error\" target=\"_blank\" rel=\"noopener noreferrer\">mean absolute scaled error</a> on the entire GIFT-Eval dataset. See the <a href=\"https://arxiv.org/abs/2509.06503\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a> for more details.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"i5thc\">Recent advances in LLMs have already given researchers worldwide new ways to easily <a href=\"https://notebooklm.google/\" target=\"_blank\" rel=\"noopener noreferrer\">engage with knowledge and ideas</a>, and LLMs are increasingly being pursued as a means of automating the rote and toilsome aspects of scientific research. We explored whether LLMs could be useful for the ubiquitous, essential, and highly time-consuming task of producing custom software for evaluating and iteratively improving scientific hypotheses, motivated by the possibility of a future where scientists can easily, rapidly, and systematically investigate hundreds or thousands of potential solutions to the questions and problems that motivate their research. Our system quickly generates expert-level solutions reducing the time required for exploration of a set of ideas from months to hours or days. This promises to save significant time for scientists, from students to professors, to focus on truly creative and critical challenges, and to continue to define and prioritize the fundamental research questions and societal challenges that scientific research can help address.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"i5thc\"><i>We thank and acknowledge the contributions from all of the co-authors of the manuscript. Thanks to Shibl Mourad, John Platt, Erica Brand, Katherine Chou, Ronit Levavi Morad, Yossi Matias, and James Manyika for their support and leadership.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "谷歌AI如何助力变革健康专业教育 (原标题: How Google’s AI can help transform health professions education)",
      "link": "https://research.google/blog/how-googles-ai-can-help-transform-health-professions-education/",
      "pubDate": "Tue, 26 Aug 2025 16:00:00 GMT",
      "isoDate": "2025-08-26T16:00:00.000Z",
      "creator": "Google",
      "summary": "全球健康劳动力正面临严重短缺，预计到2030年将有超过1100万医疗工作者的缺口。谷歌正在研究如何利用AI来变革健康专业教育，以弥补这一差距。\n\n**研究背景与目标**\n\n谷歌的研究旨在探索其AI模型如何作为有效的个性化学习工具应用于医学学习环境。目前已发布两项相关研究：\n\n1.  **《生成式AI在医学教育中的应用：一项针对医学生和AI临床推理导师的案例研究》** (CHI 2025发表)：采用定性方法，通过跨学科协同设计工作坊、快速原型开发和用户研究，理解并为医学学习者设计AI工具。\n2.  **《LearnLM：改进用于学习的Gemini》** (最新更新)：定量评估了LearnLM（基于Gemini并针对学习进行微调的模型家族）在医学教育场景中的表现，通过医学生和医生教育者的偏好评分进行评估。\n\n两项研究均表明，学习者对能够适应其需求并具备导师行为（如提供建设性反馈和促进批判性思维）的AI工具表现出浓厚兴趣。医生教育者认为LearnLM在教学法上表现更佳，行为“更像一位非常优秀的人类导师”，这些新功能现已随Gemini 2.5 Pro提供。\n\n**理解医学学习者：定性研究**\n\n谷歌采用以学习者为中心的方法，指导开发负责任的AI工具，以实现个性化学习路径和增强基于能力的教学方法。核心步骤包括：\n\n*   **形成性用户体验（UX）研究**：通过协同设计工作坊，召集医学生、临床医生、医学教育者、UX设计师和AI研究人员，共同定义AI在该领域的应用机会。\n*   **AI导师原型开发**：根据工作坊的见解，开发了一个AI导师原型，旨在通过合成临床案例指导学习者进行临床推理。\n*   **定性用户研究**：对8名参与者（4名医学生和4名住院医生）进行了为期一小时的定性用户研究，评估AI导师原型的帮助程度。研究通过半结构化访谈和原型互动进行，旨在了解学习需求、挑战以及对AI辅助教育的态度。\n*   **主要发现**：医学学习者对能够适应个体学习风格和知识差距的AI工具表现出浓厚兴趣。参与者还强调了导师行为的重要性，如管理认知负荷、提供建设性反馈以及鼓励提问和反思。\n\n![GenAI for Medical Education-final](https://storage.googleapis.com/gweb-research2023-media/images/GenAI_for_Medical_Education-final.width-1250.png)\n*图：旨在通过跨学科协同设计工作坊、快速研究原型开发和定性用户研究来理解和构建医学学习者的参与式研究过程概述。*\n\n**满足医学学习者的需求：定量研究**\n\n基于定性研究的见解，谷歌进行了一项盲法可行性研究，定量评估LearnLM在医学教育环境中的教学质量，并与Gemini 1.5 Pro作为基础模型进行比较。\n\n*   **评估场景设计**：与专家合作，设计了50个合成评估场景，涵盖从临床前（如血小板活化）到临床（如新生儿黄疸）的医学教育主题，反映了医学教育的核心能力和标准。\n*   **参与者与互动**：招募了处于临床前和临床阶段的医学生，以随机和盲法方式与LearnLM和基础模型进行互动对话。学生扮演不同类型的学习者，生成了290个对话用于分析。每个场景都提供了学习目标、基础材料、学习者角色、对话计划和初始查询等上下文信息。\n\n![GenAI for Medical Education-2](https://storage.googleapis.com/gweb-research2023-media/images/GenAI_for_Medical_Education-2.width-1250.png)\n*图：用于评估LearnLM在医学教育环境中能力的示例场景。*\n\n*   **评分标准与结果**：\n    *   **医学生评分**：学生通过并排比较两种模型的互动，从四个标准（整体体验、满足学习需求、愉悦度、可理解性）进行评分。学生对LearnLM的互动愉悦度表现出最强的积极偏好（平均+9.9%）。\n    *   **医生教育者评分**：教育者通过审查对话记录和场景规范，并排比较两种模型的对话，从五个标准（展示教学法、行为像一位非常优秀的人类导师、指令遵循、适应学习者、支持学习目标）提供偏好评分。教育者在所有五个比较标准上始终偏好LearnLM，尤其是在展示教学法（平均+6.1%）和行为“更像一位非常优秀的人类导师”（+6.8%）方面。\n\n![GenAI for Medical Education-3](https://storage.googleapis.com/gweb-research2023-media/images/GenAI_for_Medical_Education-3.width-1250.png)\n*图：医生教育者和医学生表达的偏好，显示了在医学教育场景中偏好每个模型的评分比例。*\n\n这项研究表明LearnLM有潜力改变教育和学习范式，并扩大合格的健康劳动力队伍。所有用于模型开发或评估的数据均不包含真实的患者数据。\n\n**重塑健康专业教育**\n\n谷歌在诺贝尔论坛的MedEd on the Edge会议上分享了这项研究，并与国际医学教育界举办了实践研讨会。谷歌认识到教育者在这一快速发展的知识领域中扮演着教学专家和探索者的双重角色。实现负责任的未来需要关注挑战，如确保准确性、减轻偏见以及维护人际互动和监督的关键作用。这强调了重新评估能力和可信赖的专业活动，以及培养适应性专业知识的课程的重要性，不仅关注AI在教育中的应用，还要教授AI本身的基础理解。生成式AI可以在此融合点上，作为促进更深层次理解和批判性思维的催化剂。\n\n**结论**\n\n这项研究为有效设计和实施个性化学习体验奠定了基础，通过重塑健康专业教育，提供了加速临床能力并最终改善健康结果的机会。谷歌致力于与健康专业教育界合作，深思熟虑并负责任地培养未来的医疗专业人员，使其在AI增强的医疗环境中蓬勃发展。",
      "shortSummary": "谷歌正利用AI解决全球医疗劳动力短缺问题。两项研究表明，其AI模型（如LearnLM，基于Gemini）能作为有效的个性化学习工具。定性研究发现医学生对适应性AI和导师式行为有强烈需求；定量研究显示，医生教育者认为LearnLM在教学法上更优，更像“优秀人类导师”，医学生也觉得其互动更愉悦。这些AI工具（现已集成于Gemini 2.5 Pro）有望变革健康专业教育，加速临床能力培养，并为AI增强的医疗未来做好准备，同时强调负责任的开发和人类监督。",
      "translated_title": "谷歌AI如何助力变革健康专业教育",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GenAI_for_Medical_Education-final.width-1250.png",
          "alt": "GenAI for Medical Education-final",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GenAI_for_Medical_Education-2.width-1250.png",
          "alt": "GenAI for Medical Education-2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GenAI_for_Medical_Education-3.width-1250.png",
          "alt": "GenAI for Medical Education-3",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"2zlcn\">The global health workforce is facing a critical shortage, with projections indicating a deficit exceeding <a href=\"https://www.who.int/health-topics/health-workforce#tab=tab_1\" target=\"_blank\" rel=\"noopener noreferrer\">11 million healthcare workers by 2030</a>. At Google, we are researching how AI can transform education for health professions to help close this gap with studies exploring how Google’s AI models can serve as effective personalized learning tools in medical learning environments.</p><p data-block-key=\"dqd7r\">Today we present two such studies. First, in “<a href=\"https://dl.acm.org/doi/10.1145/3706599.3721208\" target=\"_blank\" rel=\"noopener noreferrer\">Generative AI for medical education: Insights from a case study with medical students and an AI tutor for clinical reasoning</a>”, published at <a href=\"https://chi2025.acm.org/\" target=\"_blank\" rel=\"noopener noreferrer\">CHI 2025</a>, we took a qualitative approach to understanding and designing for medical learners through interdisciplinary co-design workshops, rapid prototyping, and user studies. Next, in our latest update of “<a href=\"https://arxiv.org/pdf/2412.16429#page=31\" target=\"_blank\" rel=\"noopener noreferrer\">LearnLM: Improving Gemini for Learning</a>”, we quantitatively assessed <a href=\"https://cloud.google.com/solutions/learnlm\" target=\"_blank\" rel=\"noopener noreferrer\">LearnLM</a> — our <a href=\"https://ai.google.dev/gemini-api/docs/models\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a>-based family of models fine-tuned for learning — on medical education scenarios through preference ratings from both medical students and physician educators. Both studies revealed a strong interest in AI tools that can adapt to learners and incorporate preceptor-like behaviors, such as providing constructive feedback and promoting critical thinking. Physician educators rated LearnLM as demonstrating better pedagogy and behaving “more like a very good human tutor” compared to base models. These novel capabilities are now available with <a href=\"https://blog.google/outreach-initiatives/education/google-gemini-learnlm-update/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini 2.5 Pro</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Understanding the medical learner</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"2zlcn\">Employing a learner-centered approach has been critical in guiding our development of <a href=\"https://ai.google/principles/\" target=\"_blank\" rel=\"noopener noreferrer\">responsible AI</a> tools that scale individualized learner pathways and augment competency-based approaches. Central to this approach, we first conducted formative user experience (UX) <a href=\"https://dl.acm.org/doi/10.1145/3706599.3721208\" target=\"_blank\" rel=\"noopener noreferrer\">research</a> to understand medical learner needs. Through a participatory design process, we began with a co-design workshop that convened an interdisciplinary panel of medical students, clinicians, medical educators, UX designers, and AI researchers to define opportunities for incorporating AI in this space. Insights from this session guided the development of an AI tutor prototype, explicitly designed to guide learners through clinical reasoning anchored on a synthetic clinical vignette.</p><p data-block-key=\"c6vif\">We then evaluated the AI tutor prototype’s helpfulness in a qualitative user study with eight participants (4 medical students and 4 residents). The study aimed to elicit participant learning needs and challenges as well as their attitudes toward AI assistance in education. Each participant engaged in a 1-hour session with a UX researcher involving semi-structured interviews and interactive sessions with the prototype. All sessions were remote and conducted through video conferencing software. Participants accessed the prototype through a web link and shared their screen while interacting with the prototype.</p><p data-block-key=\"b8u73\">Our thematic analysis of medical learner interviews revealed various challenges to acquiring clinical reasoning skills and the potential for generative AI in addressing these challenges. For example, medical learners expressed a significant interest in AI tools capable of adapting to unique individual learning styles and knowledge gaps. Participants also highlighted the importance of preceptor-like behaviors, such as managing cognitive load, providing constructive feedback, and encouraging questions and reflection.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GenAI_for_Medical_Education-final.width-1250.png\" alt=\"GenAI for Medical Education-final\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GenAI_for_Medical_Education-final.width-1250.png\" alt=\"GenAI for Medical Education-final\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kvkcq\"><i>Overview of the participatory research process aimed at understanding and building for medical learners through an interdisciplinary co-design workshop, rapid research prototyping, and qualitative user studies.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Meeting medical learners where they are</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"2zlcn\">Building on these insights, we conducted a blinded <a href=\"https://arxiv.org/pdf/2412.16429#page=31\" target=\"_blank\" rel=\"noopener noreferrer\">feasibility study</a> with medical students and physician educators to quantitatively assess <a href=\"https://cloud.google.com/solutions/learnlm\" target=\"_blank\" rel=\"noopener noreferrer\">LearnLM's</a> pedagogical qualities in medical education settings compared with <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/1-5-pro\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini 1.5 Pro</a> as the base model. In collaboration with experts, we designed a set of 50 synthetic evaluation scenarios across a range of medical education subjects, from pre-clinical topics, such as <a href=\"https://en.wikipedia.org/wiki/Coagulation#Platelet_activation_and_platelet_plug_formation\" target=\"_blank\" rel=\"noopener noreferrer\">platelet activation</a>, to clinical topics, like <a href=\"https://en.wikipedia.org/wiki/Neonatal_jaundice\" target=\"_blank\" rel=\"noopener noreferrer\">neonatal jaundice</a>, reflecting the <a href=\"https://www.aamc.org/about-us/mission-areas/medical-education/cbme\" target=\"_blank\" rel=\"noopener noreferrer\">core competencies</a> and <a href=\"https://wfme.org/standards/\" target=\"_blank\" rel=\"noopener noreferrer\">standards</a> in medical education.</p><p data-block-key=\"4jd0u\">We recruited medical students from both preclinical and clinical phases of training to engage in interactive conversations with both LearnLM and the base model, in a randomized and blinded manner. Students used the evaluation scenarios to role-play as different types of learners across a range of learning goals and personas, generating 290 conversations for analysis. Each scenario provided learners with context to standardize the interaction as much as possible between both models, including a learning goal, grounding materials, a learner persona, a conversation plan, and the initial query used by the learner to start the conversation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GenAI_for_Medical_Education-2.width-1250.png\" alt=\"GenAI for Medical Education-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GenAI_for_Medical_Education-2.width-1250.png\" alt=\"GenAI for Medical Education-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kvkcq\"><i>Example scenario used to evaluate LearnLM capabilities in the context of medical education settings.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"2zlcn\">Students then rated model behavior by comparing the two interactions for each scenario side-by-side across four criteria: (1) overall experience, (2) meeting learning needs, (3) enjoyability, and (4) understandability. Physician educators rated model behavior by reviewing conversation transcripts and scenario specifications. For each scenario, educators reviewed the transcripts from both learner-model conversations side-by-side, and provided preference ratings across five criteria: (1) demonstrating pedagogy, (2) behaving like a very good human tutor, (3) instruction following, (4) adapting to the learner, and (5) supporting the learning goal. We collected a median of three independent educator reviews per conversation pair. All preference ratings were done in a randomized and blinded manner using 7-point scales, which reflected a spectrum of preference strengths including the option to express no preference between the two models.</p><p data-block-key=\"9ff49\">Physician educators consistently preferred LearnLM across all five of the comparison criteria. They judged LearnLM particularly positively in terms of demonstrating better pedagogy (on average, +6.1% on our rating scale) and for behaving “more like a very good human tutor” (+6.8%). When we simply look at whether educators expressed <i>any</i> preference one way or the other — regardless of its magnitude — LearnLM emerged as their choice in a clear majority of assessments across every criterion. Medical students indicated the strongest positive preference in terms of LearnLM being more enjoyable to interact with (on average, +9.9% on our rating scale). Student preferences were less pronounced for the other three comparison criteria, while directionally also favoring LearnLM.</p><p data-block-key=\"a34m7\">This study points to LearnLM’s potential to transform education and learning paradigms and scale a competent health workforce. None of the data used for model development or evaluation in this study included real patient data. See the <a href=\"https://arxiv.org/pdf/2412.16429\" target=\"_blank\" rel=\"noopener noreferrer\">tech report</a> for modeling details.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GenAI_for_Medical_Education-3.width-1250.png\" alt=\"GenAI for Medical Education-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GenAI_for_Medical_Education-3.width-1250.png\" alt=\"GenAI for Medical Education-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kvkcq\"><i>Preferences expressed by physician educators and medical students, showing the proportion of ratings that favored each model across medical education scenarios.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Reimagining health professions education</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"2zlcn\">We recently shared this research at the <a href=\"https://mededontheedge.com/\" target=\"_blank\" rel=\"noopener noreferrer\">MedEd on the Edge</a> conference at the <a href=\"https://sv.wikipedia.org/wiki/Nobel_Forum\" target=\"_blank\" rel=\"noopener noreferrer\">Nobel Forum</a> and facilitated a hands-on workshop with the international medical education community to explore these possibilities. We recognize the dual role of educators as both pedagogical experts and explorers in this rapidly evolving knowledge domain. Realizing a responsible future requires careful attention to challenges such as ensuring accuracy, mitigating bias, and maintaining the crucial role of human interaction and oversight. It underscores the need to re-evaluate competencies and entrustable professional activities, and for curricula that cultivate adaptive expertise, focusing not only on AI applications in education, but also on teaching foundational understanding of AI itself. At this convergence, generative AI can serve as a catalyst for the desired productive struggle to foster deeper understanding and critical thinking. As the journey has only just begun, below are a few examples of how Google’s AI can potentially transform health professions education.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"jjHdbGuhq48\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=jjHdbGuhq48\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"kvkcq\"><i>Examples of how educators and learners can use Google’s AI to reimagine education for health professions. LearnLM capabilities are now integrated and available with Gemini 2.5 Pro.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"2zlcn\">This research continues to lay the groundwork toward the effective design and implementation of personalized learning experiences, offering an opportunity to accelerate clinical competency and ultimately improve health outcomes by reimagining health professions education. We are committed to partnering with the health professions education community to thoughtfully and responsibly prepare future healthcare professionals to thrive in an AI-augmented healthcare landscape.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"2zlcn\"><i>The research described here is a joint effort across Google Research, Google for Health, Google DeepMind, and partnering teams. The following researchers contributed to this work: Kevin McKee, Dan Gillick, Irina Jurenka, Markus Kunesch, Kaiz Alarakyia, Miriam Schneider, Jenn Sturgeon, Maggie Shiels, Amy Wang, Roma Ruparel, Anna Iurchenko, Mahvish Nagda, Julie Anne Séguin, Divya Pandya, Patricia Strachan, Renee Wong, Renee Schneider, Viknesh Sounderajah, Pete Clardy, Garth Graham, Megan Jones Bell, Michael Howell, Jonathan Krause, Christopher Semturs, Dale Webster, Avinatan Hassidim, Joëlle Barral, Ronit Levavi Morad and Yossi Matias. Special thanks to participants who contributed to these studies.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "一个可扩展的健康语言模型评估框架 (原标题: A scalable framework for evaluating health language models)",
      "link": "https://research.google/blog/a-scalable-framework-for-evaluating-health-language-models/",
      "pubDate": "Mon, 25 Aug 2025 16:00:00 GMT",
      "isoDate": "2025-08-25T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 一个可扩展的健康语言模型评估框架\n\n## 引言与问题\n大型语言模型（LLMs）在分析复杂数据和生成个性化健康响应方面展现出巨大潜力。然而，当前对LLMs的评估方法严重依赖人工专家，导致成本高昂、劳动密集、难以扩展，且易受偏见和评估者间一致性低的影响。\n\n## 提出的解决方案\n本文介绍了一个旨在简化开放式问题人工和自动化评估的框架，名为“可扩展的健康语言模型评估框架”。该方法通过使用一套最小化的、有针对性的评分标准问题，将复杂的、多方面的评估问题分解为可采用简单布尔（是/否）响应的细粒度评估目标，从而识别模型响应中的关键缺陷。具体而言，文章引入了**自适应精确布尔评分标准（Adaptive Precise Boolean rubrics）**作为可扩展健康评估的范式。\n\n## 自适应精确布尔评分标准的设计\n1.  **精确布尔评分标准（Precise Boolean rubrics）**：\n    *   通过迭代过程，将高复杂度的响应选项（如开放式文本或多点李克特量表）转换为更细粒度的二元（是/否）响应选项。\n    *   主要目标是提高标注任务中的评估者间一致性，并生成更稳健、可操作的评估信号，从而促进程序化解释和响应优化。\n    *   细粒度的“是/否”格式减少了主观解释，即使问题数量增加也能促进更一致的评估。\n2.  **自适应精确布尔评分标准（Adaptive Precise Boolean rubrics）**：\n    *   由于精确布尔评分标准的细粒度特性，评估标准数量显著增加，人工标注资源消耗巨大。\n    *   为减轻负担，该方法动态过滤大量评分标准问题，仅保留与被评估数据最相关的标准。\n    *   这种数据驱动的适应性减少了每个LLM响应所需的评估数量，因为用户查询和LLM输出通常具有集中的主题性。\n3.  **自动化适应过程**：\n    *   利用Gemini作为零样本评分标准问题分类器，根据用户查询、LLM响应和特定评分标准来判断该标准是否相关。\n    *   通过三位医学专家提供的评分标准问题分类标注建立真值数据集，并采用多数投票确定共识标注，以验证这种自适应方法。\n\n![EvalHealth2_Example](https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth2_Example.width-1250.png)\n图1：查询和响应示例，突出显示了响应中特定相关部分，以及对评估评分标准问题（李克特量表、精确布尔和自适应精确布尔）的响应示例。\n\n## 关键结果\n\n### 1. 提高评估者间一致性并缩短评估时间\n*   与传统的李克特量表相比，数据驱动的精确布尔评分标准在评估者间一致性（通过组内相关系数ICC衡量）方面显著更高。\n*   自适应精确布尔评分标准在保持高评估者间一致性的同时，将评估时间缩短了50%以上，甚至比李克特量表评估更快，从而提高了LLM评估的可扩展性。\n*   这种更简单的评分方式提供了更高质量的信号。\n\n![EvalHealth3_ICC](https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth3_ICC.width-1250.png)\n图2：左图：不同子组（人类评估者——专家和非专家，以及自动化评估）之间的评估者间相关性，通过组内相关系数（ICC）衡量。右图：自适应精确布尔评分标准所需时间约为李克特量表问题的一半。\n\n### 2. 提高对响应质量的敏感性\n*   李克特量表的平均评分对输入上下文的改进（例如，提供更丰富的健康数据）敏感性有限，尤其是在自动化评估中，表明其在捕捉响应质量细微变化方面的粒度不足。\n*   相比之下，布尔评分标准的平均分数与提供的用户数据量呈清晰的正相关，表明其在衡量响应质量增量改进方面具有卓越能力。\n\n![EvalHealth4_SensitivityFinal](https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth4_SensitivityFinal.width-1250.png)\n图3：对平均评分的影响：使用布尔评分标准进行的自动化评估评分与人类评分更一致/相关。此外，用自适应集合替换所有问题对信号影响很小。\n\n### 3. 自动化自适应精确布尔评分标准\n*   通过使用Gemini作为零样本分类器来自动化过滤过程，预测单个评分标准问题的相关性。该分类器在识别相关问题方面的平均准确率为0.77，F1分数为0.83。\n*   结果显示，使用自动化过滤器的自动自适应布尔评分标准在ICC方面保持了同等的改进，并显示出与人类自适应布尔评分标准相似的评分趋势。\n*   这表明一个不完美但有效的自动化分类器足以捕捉到基本的评估信号，这对于构建完全自动化和可扩展的评估流程至关重要。\n\n![EvalHealth5_AdaptationFinFinal](https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth5_AdaptationFinFinal.width-1250.png)\n图4：(A) 使用Gemini 1.5 Pro作为零样本评分标准问题分类器对精确布尔评分标准进行自适应，与人类驱动的自适应相比，并未降低ICC。(B) 自动自适应评分标准显示出与人类自适应评分标准相似的平均评分趋势，表明自动自适应评估标准足以捕捉基于人类自适应的评估信号。\n\n### 4. 卓越地识别响应质量差距\n*   为验证框架的鲁棒性，研究评估了其检测由真实研究参与者数据生成的LLM响应缺陷的能力。\n*   使用了来自“可穿戴设备用于代谢健康（WEAR-ME）”研究的去识别化数据。\n*   对每个参与者，LLM在两种条件下回答健康查询：“未更改”（包含完整的真实健康数据）和“已更改”（故意省略关键生物标志物并指示LLM不使用个人健康数据）。\n\n![EvalHealth6_Application](https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth6_Application.width-1250.png)\n图5：在真实健康研究（WEAR-ME）中应用所提出的方法。\n\n![EvalHealth7_Ablation](https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth7_Ablation.width-1250.png)\n图6：提示消融方案示意图。\n\n*   使用自动化评估系统，通过李克特和精确布尔评分标准对两种响应进行评分。较高的正差异分数（未更改响应分数减去已更改响应分数）表明评估框架成功检测到质量下降。\n*   精确布尔框架始终产生较大的正差异分数，表明它可靠地检测到已更改响应的质量较低。\n*   相比之下，李克特量表的差异分数不一致且幅度较小，未能可靠地标记低质量响应。\n*   这些结果表明，精确布尔框架对个人数据的包含显著更敏感，使其成为自动化评估流程中更强大的工具。\n\n![EvalHealth8_Likert](https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth8_Likert.width-1250.png)\n图7：使用李克特评分标准测量自动评估器对提示更改的敏感性。\n\n![EvalHealth9_Boolean](https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth9_Boolean.width-1250.png)\n图8：使用所提出的精确布尔评分标准测量自动评估器对提示更改的敏感性。\n\n## 结论与未来方向\n研究结果表明，使用自适应精确布尔评分标准：\n*   与李克特量表相比，显著降低了评估者间变异性。\n*   将专家和非专家评估者的评估时间缩短了一半。\n*   实现了自动化评估与专家人工判断的对等。\n*   与真实世界的可穿戴设备、生物标志物和上下文数据结合时，能更敏感地检测质量差异。\n\n这种方法在扩展和简化专业领域LLM评估方面取得了显著进展。该框架是领域无关的，可应用于健康和个性化评估之外的领域。未来的工作可以扩展到更广泛的用户画像和健康领域，并进一步自动化初始布尔问题的创建过程。",
      "shortSummary": "本文提出一个名为“自适应精确布尔评分标准”的可扩展框架，用于高效评估健康语言模型（LLMs）。针对现有评估方法成本高、效率低、一致性差的问题，该框架将复杂评估分解为细粒度的布尔（是/否）问题，并动态筛选相关问题。实验证明，该方法显著提高了评估者间一致性，将评估时间缩短一半，提升了对响应质量变化的敏感性，并能更有效地识别质量缺陷。通过自动化适应过程，实现了与人工专家判断相当的自动化评估，为LLM的规模化评估提供了强大工具。",
      "translated_title": "一个可扩展的健康语言模型评估框架",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth2_Example.width-1250.png",
          "alt": "EvalHealth2_Example",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth3_ICC.width-1250.png",
          "alt": "EvalHealth3_ICC",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth4_SensitivityFinal.width-1250.png",
          "alt": "EvalHealth4_SensitivityFinal",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth5_AdaptationFinFinal.width-1250.png",
          "alt": "EvalHealth5_AdaptationFinFinal",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth6_Application.width-1250.png",
          "alt": "EvalHealth6_Application",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"mci23\">Large language models can be used to analyze and interpret complex data. Our previous work has shown how they can be used to <a href=\"https://www.nature.com/articles/s41591-025-03888-0\" target=\"_blank\" rel=\"noopener noreferrer\">generate useful, personalized responses when provided with user-specific health information</a> that encompasses lifestyle, biomarkers, and context. Rigorous and efficient evaluation methodologies are crucial to ensure the accuracy, precision, relevance, and safety of responses. However, current evaluation practices heavily rely on human experts, meaning they are cost-prohibitive, labor-intensive, and not scalable. Furthermore, tasks involving human judgement often require careful design to avoid biases and low inter-rater consistency.</p><p data-block-key=\"8ain8\">With the above in mind, in “<a href=\"https://arxiv.org/abs/2503.23339v2\" target=\"_blank\" rel=\"noopener noreferrer\">A Scalable Framework for Evaluating Health Language Models</a>”, we introduce an evaluation framework that aims to streamline human and automated evaluation of open questions. Our method helps identify critical gaps in model responses using a minimal set of targeted rubric questions that break complex, multi-faceted evaluation questions into granular evaluation targets that can be answered via simple boolean responses. Specifically, we introduce <i>Adaptive Precise Boolean rubrics</i> as a paradigm for scalable health evaluations. We hypothesized that a small set of granular, boolean (Yes/No) criteria would enhance consistency and efficiency in complex query evaluation. Existing work has demonstrated that \"granularizing\" complex evaluation criteria into a larger set of focused, boolean rubrics improves rater reliability for general-domain tasks like summarization and dialogue. Our work extends these frameworks by applying them to the health domain, accounting for user personalization with health data in both the LLM responses and the evaluations. We validate this approach in metabolic health, a domain encompassing diabetes, cardiovascular disease, and obesity.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/EvalHealth1_OverviewFinal.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"0h3fr\"><i>A set of representative health queries and wearable data are used to construct inputs to the language model, these are then evaluated using our proposed evaluation rubric framework.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"3eijn\">Designing Adaptive Precise Boolean rubrics</h2><p data-block-key=\"54g92\">We first used an iterative process to transform rubric criteria characterized by high-complexity response options (e.g., open-ended text or multi-point <a href=\"https://en.wikipedia.org/wiki/Likert_scale\" target=\"_blank\" rel=\"noopener noreferrer\">Likert</a> scales) into a more granular set of rubric criteria employing binary response options (i.e., boolean “Yes” or “No”) — an approach we call <i>Precise Boolean</i> rubrics. The primary objective in developing the Precise Boolean rubrics was to enhance inter-rater reliability in annotation tasks and to generate a more robust and actionable evaluation signal, thereby facilitating programmatic interpretation and response refinement. The increased granularity afforded by the simple Yes/No format mitigates subjective interpretation and fosters more consistent evaluations, even with a larger number of total questions.</p><p data-block-key=\"bcv2e\">Due to the granular nature of our rubric design, the resulting Precise Boolean rubrics consisted of a substantially larger number of evaluation criteria compared to the starting Likert-scale rubrics. While auto-eval techniques are well equipped to handle the increased volume of evaluation criteria, the completion of the proposed Precise Boolean rubrics by human annotators was prohibitively resource intensive. To mitigate such burden, we refined the Precise Boolean approach to dynamically filter the extensive set of rubric questions, retaining only the most pertinent criteria, conditioned on the specific data being evaluated. This data-driven adaptation, referred to as the <i>Adaptive Precise Boolean</i> rubric, enabled a reduction in the number of evaluations required for each LLM response. This is because user queries and corresponding LLM outputs often exhibit a focused topicality, thus requiring evaluation against only the subset of rubric criteria relevant to those themes.</p><p data-block-key=\"8kt1c\">To convert the Precise Boolean rubrics to Adaptive Precise Boolean ones, we leveraged <a href=\"https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a> as a zero-shot rubric question classifier. Input to the LLM includes the user query, the corresponding LLM response under evaluation, and a specific rubric criterion. The LLM then outputs whether the criterion is relevant or not. To validate this adaptive approach, we established a ground-truth dataset through rubric question classification annotations provided by three medical experts, with majority voting employed to determine the consensus annotation. Rubrics obtained based on using this ground-truth dataset in order to do adaptation are referred to as <i>Human-Adaptive Precise Boolean rubrics</i>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth2_Example.width-1250.png\" alt=\"EvalHealth2_Example\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth2_Example.width-1250.png\" alt=\"EvalHealth2_Example\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"0h3fr\"><i>An example of a query and response highlighting references to specific relevant parts of the response, alongside examples of responses to evaluation rubric questions (Likert, Precise Boolean, and Adaptive Precise Boolean).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"3eijn\">Key results</h2><h3 data-block-key=\"el59b\">Enhanced inter-rater agreement and reduced evaluation time</h3><p data-block-key=\"eqc0\">Current evaluation of LLMs in health often uses Likert scales. We compared this baseline to our data-driven Precise Boolean rubrics. Our results showed significantly higher inter-rater reliability using Precise Boolean rubrics, measured by <a href=\"https://en.wikipedia.org/wiki/Intraclass_correlation\" target=\"_blank\" rel=\"noopener noreferrer\">intra-class correlation coefficients</a> (ICC), compared to traditional Likert rubrics.</p><p data-block-key=\"47niu\">A key advantage of our approach is its efficiency. The Adaptive Precise Boolean rubrics resulted in high inter-rater agreement of the full Precise Boolean rubric while reducing evaluation time by over 50%. This efficiency gain makes our method faster than even Likert scale evaluations, enhancing the scalability of LLM assessment. The fact that this also provides higher inter-rater reliability supports the argument that this simpler scoring also provides a higher quality signal.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth3_ICC.width-1250.png\" alt=\"EvalHealth3_ICC\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth3_ICC.width-1250.png\" alt=\"EvalHealth3_ICC\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"0h3fr\"><b><i>Left:</i></b><i> Inter-rater correlation, as measured by intra-class correlation coefficient (ICC), between different subgroups — human evaluators (expert and non-expert) and automated evaluation.</i> <b><i>Right:</i></b><i> Adaptive Precise Boolean rubrics take about half the time compared to likert scale questions.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"3eijn\">Improved sensitivity to response quality</h3><p data-block-key=\"c44v3\">To test the efficacy of our rubrics, we investigated their sensitivity to variations in response quality. We systematically augmented user queries with increasing levels of contextual health data, hypothesizing that richer queries would elicit higher-quality LLM responses, the results to support this will be discussed in detail below.</p><p data-block-key=\"27g8c\">Average ratings from Likert scales showed limited sensitivity to these improvements in input context, particularly in automated evaluations. This suggests a lack of granularity in Likert scales for capturing subtle variations in response quality. In contrast, the average scores from our boolean rubrics showed a clear, positive correlation with the amount of user data provided, indicating a superior ability to measure incremental improvements in response quality.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth4_SensitivityFinal.width-1250.png\" alt=\"EvalHealth4_SensitivityFinal\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth4_SensitivityFinal.width-1250.png\" alt=\"EvalHealth4_SensitivityFinal\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"0h3fr\"><i>Implications on Average Ratings: Ratings obtained from auto-evals using the boolean rubrics are more consistent/correlated with human ratings. In addition, replacing all questions with an adaptive set has little impact on the signal.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"3eijn\">Auto-Adaptive Precise Boolean rubrics</h3><p data-block-key=\"7upnu\">The Precise Boolean rubric framework is comprehensive, but for any given query, only a subset of its questions are relevant. We automated this filtering process by using <a href=\"https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#sundar-note\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a> as a zero-shot classifier to predict the relevance of individual rubric questions based on the input query and the LLM response. The classifier achieved an average accuracy of 0.77 and an <a href=\"https://en.wikipedia.org/wiki/F-score\" target=\"_blank\" rel=\"noopener noreferrer\">F1 score</a> of 0.83 in identifying relevant questions. We found that the Auto-Adaptive Boolean rubrics, using this automated filter, maintained an equivalent improvement in ICC and showed similar scoring trends as the Human-Adaptive Boolean rubrics. This suggests that an imperfect but effective automated classifier is sufficient to capture the essential evaluation signal. This finding is critical for building fully automated and scalable evaluation pipelines.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth5_AdaptationFinFinal.width-1250.png\" alt=\"EvalHealth5_AdaptationFinFinal\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth5_AdaptationFinFinal.width-1250.png\" alt=\"EvalHealth5_AdaptationFinFinal\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"0h3fr\"><i>(</i><b><i>A</i></b><i>) Adaptation of Precise Boolean rubrics using Gemini 1.5 Pro as a zero-shot rubric question classifier does not degrade ICC compared to using human driven adaptation. (</i><b><i>B</i></b><i>) Auto-Adaptive rubrics shows a similar average rating trend to Human-Adaptive rubrics, indicating that the Auto-Adaptive evaluation criteria are sufficient to capture the evaluation signals present based on human adaptation.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"3eijn\">Superior identification of response quality gaps</h3><p data-block-key=\"ff4bl\">To demonstrate robustness, we evaluated our framework's ability to detect flaws in LLM responses generated from real research participants’ data. We used de-identified data from the <a href=\"https://arxiv.org/abs/2505.03784\" target=\"_blank\" rel=\"noopener noreferrer\">Wearables for Metabolic Health (WEAR-ME) study</a>, a large-scale (n≈1500) research project that collected wearable, biomarker, and questionnaire data conducted with approval from an <a href=\"https://www.fda.gov/about-fda/cder-offices-and-divisions/institutional-review-boards-irbs-and-protection-human-subjects-clinical-trials\" target=\"_blank\" rel=\"noopener noreferrer\">Institutional Review Board</a> (IRB). All participants provided electronic informed consent and a specific HIPAA Authorization via the Google Health Studies app before enrollment, acknowledging that their de-identified data would be used for research purposes.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth6_Application.width-1250.png\" alt=\"EvalHealth6_Application\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth6_Application.width-1250.png\" alt=\"EvalHealth6_Application\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"qwshg\"><i>Application of proposed approach on a real health study (WEAR-ME).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3eijn\">For this specific analysis, we selected 141 participants with confirmed metabolic conditions (e.g., Class III obesity, diabetes, hypercholesterolemia) to test the frameworks’ sensitivity. For each participant, we prompted an LLM to answer health queries under two conditions:</p><ol><li data-block-key=\"4fmnk\"><i>Unaltered:</i> The prompt included the participant's complete, real health data.</li><li data-block-key=\"fs9uv\"><i>Altered:</i> The prompt deliberately omitted key biomarkers relevant to the participant's condition and instructed the LLM not to use personal health data.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth7_Ablation.width-1250.png\" alt=\"EvalHealth7_Ablation\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth7_Ablation.width-1250.png\" alt=\"EvalHealth7_Ablation\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"0h3fr\"><i>Illustration of our prompt ablation scheme.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"3eijn\">We then used an automated evaluation system to score both the responses using both Likert and Precise Boolean rubrics. A higher positive <i>discrepancy score</i> (score of unaltered response minus score of altered response) indicates that the evaluation framework successfully detected the drop in quality.</p><p data-block-key=\"49giu\">As shown below, the Precise Boolean framework consistently produced a large, positive discrepancy score, indicating it reliably detected that the altered responses were of lower quality. In contrast, the Likert scale's discrepancy score was inconsistent and smaller in magnitude, failing to reliably flag the lower-quality responses. These results demonstrate that the Precise Boolean framework is significantly more sensitive to the inclusion of personal data, making it a more robust tool for automated evaluation pipelines.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-vertical-padding --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth8_Likert.width-1250.png\" alt=\"EvalHealth8_Likert\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth8_Likert.width-1250.png\" alt=\"EvalHealth8_Likert\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth9_Boolean.width-1250.png\" alt=\"EvalHealth9_Boolean\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EvalHealth9_Boolean.width-1250.png\" alt=\"EvalHealth9_Boolean\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"d5wzl\"><i>Measuring the sensitivity of an auto-rater to prompt alterations using Likert rubrics and the proposed Precise Boolean rubrics.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"krdjg\">Conclusion and future directions</h2><p data-block-key=\"8647h\">Our findings show that using Adaptive Precise Boolean rubrics<b>:</b></p><ol><li data-block-key=\"85uf5\">Substantially reduces inter-rater variability compared to Likert scales.</li><li data-block-key=\"b4pr7\">Halves evaluation time for both expert and non-expert evaluators.</li><li data-block-key=\"btj55\">Achieves automated evaluation parity with expert human judgment.</li><li data-block-key=\"5pi4s\">More sensitively detects quality discrepancies when integrated with real-world wearable, biomarker, and contextual data.</li></ol><p data-block-key=\"8p2um\">This approach offers a significant advancement in scaling and streamlining LLM evaluation in specialized domains. While LLMs hold promise for health applications, this paper focuses on the critical need for robust evaluation methodologies and does not present the models as approved medical devices.</p><p data-block-key=\"bja1\">Our framework is domain-agnostic and could be applied beyond health and personalized evaluation. The use of a health and wellness context for validation is for illustrative and research purposes only. This research is not tied to any specific product or service. The LLMs discussed are used in a controlled research setting and any real-world health application would be subject to its own validation and potential regulatory review. There are some limitations to this approach, in some situations the nuanced rating provided by Likert scale can be useful. Future work can expand on our results by incorporating a wider variety of user personas and health domains. Additionally, the process of creating the initial boolean questions from Likert criteria could be further automated by incorporating LLMs, enhancing the framework's scalability from its inception.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"krdjg\">Acknowledgements</h2><p data-block-key=\"c0aqd\"><i>The following researchers contributed to this work: Neil Mallinar, A. Ali Heydari, Xin Liu, Anthony Z. Faranesh, Brent Winslow, Nova Hammerquist, Benjamin Graef, Cathy Speed*, Mark Malhotra, Shwetak Patel, Javier L. Prieto*, Daniel McDuff, and Ahmed A. Metwally.</i></p><p data-block-key=\"ejl9f\"></p><p data-block-key=\"biqun\"></p><p data-block-key=\"c9kv2\"><i>* Work done while at Google.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "从大型模型到移动魔术：YouTube实时生成式AI特效背后的技术 (原标题: From massive models to mobile magic: The tech behind YouTube real-time generative AI effects)",
      "link": "https://research.google/blog/from-massive-models-to-mobile-magic-the-tech-behind-youtube-real-time-generative-ai-effects/",
      "pubDate": "Wed, 20 Aug 2025 16:00:00 GMT",
      "isoDate": "2025-08-20T16:00:00.000Z",
      "creator": "Google",
      "summary": "# YouTube实时生成式AI特效技术揭秘：从大型模型到移动魔术\n\nYouTube Shorts上的特效是其乐趣的重要组成部分，但要实现“魔术般”的效果，它们必须在创作者录制时在相机中实时运行。这带来了一个挑战：如何将大型生成式AI模型的最新功能（如卡通风格转换）应用于创作者的手机上？\n\n## 解决方案核心：知识蒸馏与模型小型化\n\nYouTube的解决方案是一个管道，它将大型模型的能力提炼成一个专注于单一任务的小型模型。这种范围的缩小创建了一个紧凑、高效的模型，可以直接在手机上逐帧处理视频。通过这种方法，YouTube已为Shorts上的创作者推出了20多种实时特效。\n\n### 1. 数据准备：高质量与多样性\n\n*   **基础数据：** 工作的基础是高质量数据。团队使用获得许可的图像构建了一个人脸数据集。\n*   **数据过滤：** 数据集经过精心过滤，确保在不同性别、年龄和肤色（通过Monk肤色量表测量）之间具有多样性和均匀分布，以确保特效对所有人有效。\n\n### 2. 教师-学生模型：知识蒸馏\n\n该方法围绕“知识蒸馏”概念，采用“教师-学生”模型训练方法：\n\n*   **教师模型：** 一个大型、强大、预训练的生成模型，擅长创建所需的视觉效果，但速度太慢，无法实时使用。\n    *   最初使用定制训练的StyleGAN2模型，可与StyleCLIP等工具结合，根据文本描述操纵面部特征。\n    *   项目后期过渡到更复杂的生成模型，如Google DeepMind的Imagen，显著提升了保真度、图像多样性和艺术控制力。\n*   **学生模型：** 最终在用户设备上运行的模型，需要小巧、快速、高效。\n    *   设计采用基于UNet的架构，非常适合图像到图像的任务。\n    *   编码器使用MobileNet骨干网络，解码器使用MobileNet块，以优化移动设备性能。\n\n### 3. 蒸馏过程：迭代式教学\n\n为了实现生产级特效，YouTube开发了一种强大的训练方法，解决了合成数据蒸馏的局限性（常导致伪影和高频细节减少）。该方法利用真实世界数据生成“图像对”，并训练学生模型以实现更高效的超参数搜索。\n\n*   **数据生成：** 通过教师模型处理大量图像数据集，创建数千个“前后”图像对。\n    *   生成过程中融入了增强功能，如添加AR眼镜、太阳镜以及合成手遮挡。\n    *   使用“枢轴调优反演（Pivotal Tuning Inversion, PTI）”来保留用户身份。\n*   **学生训练：** 学生模型在这些配对图像上进行训练。\n    *   结合使用L1、LPIPS、Adaptive和对抗性损失函数，确保学生模型的输出不仅在数值上准确，而且在视觉上真实和美观。\n    *   采用神经架构搜索来优化模型架构参数（如“深度乘数”和“宽度乘数”），以适应各种用例和特效类型。\n\n![实时生成式AI特效示例](https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-0-Examples.width-800.gif)\n实时生成式AI特效对视频流的实时转换。从左到右：原始、设备端美妆“粉嫩水润”、“卡通”和“卡通化”效果。\n\n![蒸馏管道示意图](https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-1-DistillationPipeline.width-1250.png)\n“永不眨眼”特效的蒸馏管道高级示意图。\n\n### 4. 关键挑战：保留用户身份\n\n*   **问题：** 图像编辑发生在“潜在空间”中，生成式模型在重新生成整个帧时，容易扭曲关键特征，改变肤色、眼镜或衣物，导致输出不再像用户本人。这被称为“反演问题”。\n*   **解决方案：枢轴调优反演（PTI）**\n    1.  原始图像通过编码器转换为嵌入（枢轴代码），并使用生成器生成初始反演。\n    2.  使用PTI迭代过程微调生成器，以保留人脸身份和细节。\n    3.  通过编辑嵌入（通常使用StyleCLIP等技术创建的向量）来应用所需效果。\n    4.  使用微调后的生成器和编辑后的嵌入生成最终输出图像。\n\n![生成管道示意图](https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-2-GenPipeline.width-1250.png)\n该管道将生成器微调到用户的独特面部，允许在潜在空间中应用编辑，而不会在最终图像中失去其相似性。请注意，初始反演可能缺少一些精细细节，导致外观略有不同。\n\n### 5. 设备端运行：MediaPipe\n\n学生模型训练完成后，需要集成到能在手机上高效运行的管道中。YouTube使用Google AI Edge的开源跨平台多模态ML管道框架MediaPipe构建了设备端解决方案。\n\n*   **推理管道工作流程：**\n    1.  MediaPipe Face Mesh模块检测视频流中的一个或多个人脸。\n    2.  由于学生模型对人脸对齐敏感，管道会计算一个稳定、旋转的人脸裁剪，以确保一致性。\n    3.  裁剪后的图像转换为张量，并输入到精简的学生模型中。\n    4.  学生模型应用特效（例如，微笑或卡通风格），然后将结果图像实时扭曲并无缝合成到原始视频帧上。\n\n![设备端推理管道示意图](https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-3-InferencePipeline.width-1250.png)\n设备端推理管道：MediaPipe Face Mesh检测、裁剪和对齐人脸以供学生模型使用。\n\n*   **性能要求：** 为确保用户体验流畅，这些体验需要以至少30帧/秒的速度运行（即管道每帧执行时间需快于33毫秒）。\n    *   模型推理延迟：Pixel 8 Pro（Google Tensor G3）约为6毫秒，iPhone 13 GPU约为10.6毫秒。\n    *   通过GPU加速，对各种移动设备进行了大量优化，以确保所有用户都能获得流畅体验。\n\n## 成果与未来展望\n\n这项技术自2023年以来一直是YouTube Shorts的关键要素，成功推出了众多热门功能，包括基于表情的特效（如“永不眨眼”）、万圣节主题面具（如“复活僵尸”）和沉浸式全帧特效（如“卡通2”），极大地扩展了YouTube视频创作者的创意可能性。\n\n![实时生成式AI特效演示](https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-4a-Smile.width-800.gif)\n![实时生成式AI特效演示](https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-4b-NvrBlink.width-800.gif)\n![实时生成式AI特效演示](https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-4c-Zombie.width-800.gif)\nYouTube Shorts上实时生成式AI特效的实际应用，包括基于表情的特效，如“总是微笑”（左）和“永不眨眼”（中），以及万圣节主题面具，如“复活僵尸”（右）。\n\n通过弥合大型生成模型与移动硬件限制之间的鸿沟，YouTube正在重新定义实时设备端生成特效的技术可能性。这仅仅是一个开始，团队正积极致力于集成最新的模型（如Veo 3），并显著降低入门级设备的延迟，进一步普及YouTube Shorts中尖端生成式AI的访问。",
      "shortSummary": "YouTube开发了一套技术，将大型生成式AI模型的能力带到移动设备上，为YouTube Shorts提供实时特效。核心方法是“知识蒸馏”，将强大的“教师”模型提炼成小巧高效的“学生”模型，并在高质量数据集上训练。为解决身份失真问题，采用了“枢轴调优反演”技术。设备端运行通过MediaPipe实现，确保流畅性能。该技术已推出20多款特效，显著增强了移动创作体验，未来还将继续优化和普及。",
      "translated_title": "从大型模型到移动魔术：YouTube实时生成式AI特效背后的技术",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-0-Examples.width-800.gif",
          "alt": "YTEffects-0-Examples",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-1-DistillationPipeline.width-1250.png",
          "alt": "YTEffects-1-DistillationPipeline",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-2-GenPipeline.width-1250.png",
          "alt": "YTEffects-2-GenPipeline",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-3-InferencePipeline.width-1250.png",
          "alt": "YTEffects-3-InferencePipeline",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-4a-Smile.width-800.gif",
          "alt": "YTEffects-4a-Smile",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"iiwr9\">Effects are a huge part of the fun on <a href=\"https://support.google.com/youtube/answer/10059070?hl=en\" target=\"_blank\" rel=\"noopener noreferrer\">YouTube Shorts</a>, but for them to feel magical, they need to work in real-time in the camera as the creator is recording. This presents a challenge: how do we apply the latest capabilities of large generative AI models, such as cartoon style transfer, on creators' phones?</p><p data-block-key=\"2ovib\">Our solution is a pipeline that distills the capability of a large model into a much smaller one focused on a single task. This narrowing of scope creates a compact, efficient model that can run directly on a phone, processing video frame-by-frame. Using this method, we've launched over 20 real-time effects for YouTube creators on Shorts. In this post, we'll detail how we accomplish this: including data curation, training, and the on-device setup.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-0-Examples.width-800.gif\" alt=\"YTEffects-0-Examples\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-0-Examples.width-800.gif\" alt=\"YTEffects-0-Examples\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cw6lr\"><i>Real-time transformation of video streams using a selection of real-time generative AI effects. From left to right: original, on-device makeup “</i><a href=\"https://www.youtube.com/effect/67d2024e-0000-223f-960a-d4f547f91714\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Pink dewy</i></a><i>”, ”</i><a href=\"https://www.linkedin.com/posts/googleresearch_dli2025-activity-7364197761409314816-5SWx?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAEFRf8B42XtbmIRgkdfsPAb_bPjQzAHui0\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Cartoon</i></a><i>” and a “</i><a href=\"https://www.youtube.com/effect/678bfb3e-0000-2bbc-8ebe-2405887ff630\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Toon</i></a><i>” effect.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">It all starts with data</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"iiwr9\">The foundation of our work is high-quality data. We began by building a face dataset using properly licensed images. We meticulously filtered our datasets to ensure they were diverse and uniformly distributed across different genders, ages, and skin tones (as measured by the <a href=\"https://skintone.google/get-started\" target=\"_blank\" rel=\"noopener noreferrer\">Monk Skin Tone Scale</a>) to build effects that work well for everyone.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The teacher and the student</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"iiwr9\">Our approach revolves around a concept called <a href=\"https://en.wikipedia.org/wiki/Knowledge_distillation\" target=\"_blank\" rel=\"noopener noreferrer\">knowledge distillation</a>, which uses a \"teacher–student\" model training method. We start with a \"teacher\" — a large, powerful, pre-trained generative model that is an expert at creating the desired visual effect but is far too slow for real-time use. The type of teacher model varies depending on the goal. Initially, we used a custom-trained <a href=\"https://arxiv.org/abs/1912.04958\" target=\"_blank\" rel=\"noopener noreferrer\">StyleGAN2</a> model, which was trained on our curated dataset for real-time facial effects. This model could be paired with tools like<a href=\"https://arxiv.org/abs/2103.17249\" target=\"_blank\" rel=\"noopener noreferrer\"> StyleCLIP</a>, which allowed it to manipulate facial features based on text descriptions. This provided a strong foundation. As our project advanced, we transitioned to more sophisticated generative models like Google DeepMind’s <a href=\"https://deepmind.google/models/imagen/\" target=\"_blank\" rel=\"noopener noreferrer\">Imagen</a>. This strategic shift significantly enhanced our capabilities, enabling higher-fidelity and more diverse imagery, greater artistic control, and a broader range of styles for our on-device generative AI effects.</p><p data-block-key=\"abdv\">The \"student\" is the model that ultimately runs on the user’s device. It needs to be small, fast, and efficient. We designed a student model with a <a href=\"https://link.springer.com/content/pdf/10.1007/978-3-319-24574-4_28.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">UNet</a>-based architecture, which is excellent for image-to-image tasks. It uses a <a href=\"https://research.google/blog/mobilenets-open-source-models-for-efficient-on-device-vision/\">MobileNet</a> backbone as its encoder, a design known for its performance on mobile devices, paired with a decoder that utilizes MobileNet blocks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Distillation: Iteratively teaching the student</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"iiwr9\">To achieve production-ready effects, we developed a robust training methodology that addresses the limitations of synthetic data distillation, which often leads to artifacts and reduced high-frequency details. Our approach leverages real-world data to generate \"image pairs\" and train student models to enable a more efficient hyperparameter search.</p><p data-block-key=\"1100m\">The distillation process for training the smaller student model involves two key steps:</p><ol><li data-block-key=\"b0j7r\"><b>Data Generation:</b> We process a large dataset of images through the teacher model to create thousands of \"before and after\" image pairs. During generation, we incorporate augmentations, such as adding AR glasses and sunglasses, and occlusion with synthetic <a href=\"https://research.google/blog/on-device-real-time-hand-tracking-with-mediapipe/\">hands</a>. We also use <a href=\"https://arxiv.org/abs/2106.05744\" target=\"_blank\" rel=\"noopener noreferrer\">Pivotal Tuning Inversion</a> to preserve user identity.<br></li><li data-block-key=\"b6epd\"><b>Student Training:</b> The student model is then trained on these paired images. We utilize a combination of <a href=\"https://en.wikipedia.org/wiki/Mean_absolute_error\" target=\"_blank\" rel=\"noopener noreferrer\">L1</a>, <a href=\"https://arxiv.org/abs/1801.03924\" target=\"_blank\" rel=\"noopener noreferrer\">LPIPS</a>, <a href=\"https://arxiv.org/abs/1701.03077v10\" target=\"_blank\" rel=\"noopener noreferrer\">Adaptive</a>, and <a href=\"https://developers.google.com/machine-learning/gan/loss\" target=\"_blank\" rel=\"noopener noreferrer\">Adversarial</a> loss functions to ensure the student's output is not only numerically accurate but also visually realistic and aesthetically pleasing. Furthermore, we employ a <a href=\"https://en.wikipedia.org/wiki/Neural_architecture_search\" target=\"_blank\" rel=\"noopener noreferrer\">neural architecture search</a> to optimize model architecture parameters (like \"depth multiplier\" and \"width multiplier\") allowing us to identify efficient architectures tailored to various use cases and effect types.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-1-DistillationPipeline.width-1250.png\" alt=\"YTEffects-1-DistillationPipeline\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-1-DistillationPipeline.width-1250.png\" alt=\"YTEffects-1-DistillationPipeline\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cw6lr\"><i>High-level schema of distillation pipeline the “</i><a href=\"https://youtube.com/effect/63fec95a-0000-2d83-b984-14223bb7b9e2\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Never Blink</i></a><i>” effect.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">A critical challenge: Preserving user identity</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"iiwr9\">The \"editing\" of the image happens in \"latent\" space, which is a compressed numerical representation of the image where meaningful features are encoded. The process of converting raw pixels to latent representation is called “inversion”. A major challenge in image-to-image generative models for facial effects is preserving a person's identity because the effect regenerates the entire frame. A naïve approach can easily distort key features, changing a person's skin tone, glasses, or clothing, resulting in an output that no longer looks like them. This issue, often called the \"inversion problem\", happens when a model struggles to accurately represent a real person's face in its latent space.</p><p data-block-key=\"flpgs\">To solve this, we employ a technique called <a href=\"https://arxiv.org/abs/2106.05744\" target=\"_blank\" rel=\"noopener noreferrer\">pivotal tuning inversion</a> (PTI). Here is a simplified version of how it works:</p><ol><li data-block-key=\"bvjoq\">The original image is transformed into an embedding, referred to as a <i>pivotal code</i>, using an encoder and generating an initial inversion with a generator (see below). This is typically a representation similar to the original image, but not identical (e.g., skin tone and facial details may not be accurate).<br></li><li data-block-key=\"64gqu\">We fine-tune a generator using the PTI iterative process to preserve face identity and details. This results in a new generator that performs better for a specific face and its embedding neighborhood.<br></li><li data-block-key=\"6qonf\">We apply the desired effect by editing the embedding, typically using a prepared vector created with techniques such as StyleCLIP.<br></li><li data-block-key=\"cl9jt\">We generate the final output image with an edited face using a fine-tuned generator and an edited embedding.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-2-GenPipeline.width-1250.png\" alt=\"YTEffects-2-GenPipeline\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-2-GenPipeline.width-1250.png\" alt=\"YTEffects-2-GenPipeline\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cw6lr\"><i>The pipeline fine-tunes a generator to the user's unique face, allowing us to apply edits in the latent space without losing their likeness in the final image. Note that the initial inversion may lack some fine details, resulting in a slightly different appearance.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Running on device with MediaPipe from Google AI Edge</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"iiwr9\">Once the student model is trained, it needs to be integrated into a pipeline that can run efficiently on a phone. We built our on-device solution using <a href=\"https://ai.google.dev/edge/mediapipe/solutions/guide\" target=\"_blank\" rel=\"noopener noreferrer\">MediaPipe</a>, our open-source framework for building cross-platform multimodal ML pipelines, from <a href=\"http://ai.google.dev/edge\" target=\"_blank\" rel=\"noopener noreferrer\">Google AI Edge</a>. The final inference pipeline works as follows:</p><ol><li data-block-key=\"2ee3s\">First, the <a href=\"https://github.com/google-ai-edge/mediapipe/wiki/MediaPipe-Face-Mesh\" target=\"_blank\" rel=\"noopener noreferrer\">MediaPipe Face Mesh</a> module detects one or more faces in the video stream.<br></li><li data-block-key=\"19hqr\">Because student models are sensitive to face alignment, the pipeline computes a stable, rotated crop of the face to ensure consistency.<br></li><li data-block-key=\"f3tvv\">This cropped image is converted into a tensor and fed into our lean student model.<br></li><li data-block-key=\"c43t8\">The student model applies the effect (e.g., a smile or a cartoon style), and the resulting image is warped back and seamlessly composited onto the original video frame in real-time.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-3-InferencePipeline.width-1250.png\" alt=\"YTEffects-3-InferencePipeline\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-3-InferencePipeline.width-1250.png\" alt=\"YTEffects-3-InferencePipeline\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cw6lr\"><i>On-device inference pipeline: MediaPipe Face Mesh detects, crops, and aligns faces for the student model.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"iiwr9\">These experiences need to run at a minimum of 30 frames per second to feel responsive to the user, so the pipeline must execute faster than 33 milliseconds per frame. The model inference latencies are ~6 ms for Pixel 8 Pro on Google Tensor G3 and 10.6 ms for iPhone 13 GPU. We invested heavily in optimizing these pipelines for a wide range of mobile devices, leveraging GPU acceleration to ensure a smooth experience for everyone.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The result: Enhanced mobile creativity</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"iiwr9\">This technology has been a crucial element of YouTube Shorts since 2023, enabling the successful launch of numerous popular features, including expression-based effects (e.g., <a href=\"https://youtube.com/effect/63fec95a-0000-2d83-b984-14223bb7b9e2\" target=\"_blank\" rel=\"noopener noreferrer\">Never blink</a>), Halloween-themed masks (e.g., <a href=\"https://youtube.com/effect/670d68b0-0000-286d-b0a7-089e08292e4c\" target=\"_blank\" rel=\"noopener noreferrer\">Risen zombie</a>), and immersive full-frame effects (e.g., <a href=\"https://youtube.com/effect/678bfb3e-0000-2bbc-8ebe-2405887ff630\" target=\"_blank\" rel=\"noopener noreferrer\">Toon 2</a>). These significantly expanded creative possibilities for YouTube video creators.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n              --vertical-center\n            \n            \n                glue-grid__col--span-4-md\n            \n          \">\n            <div class=\"--mobile-spacer\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-4a-Smile.width-800.gif\" alt=\"YTEffects-4a-Smile\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-4a-Smile.width-800.gif\" alt=\"YTEffects-4a-Smile\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n              --vertical-center\n            \n            \n                glue-grid__col--span-4-md\n            \n          \">\n            <div class=\"--mobile-spacer\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-4b-NvrBlink.width-800.gif\" alt=\"YTEffects-4b-NvrBlink\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-4b-NvrBlink.width-800.gif\" alt=\"YTEffects-4b-NvrBlink\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n              --vertical-center\n            \n            \n                glue-grid__col--span-4-md\n            \n          \">\n            <div class=\"--mobile-spacer\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-4c-Zombie.width-800.gif\" alt=\"YTEffects-4c-Zombie\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-4c-Zombie.width-800.gif\" alt=\"YTEffects-4c-Zombie\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cw6lr\"><i>Real-time generative AI effects in action on YouTube Shorts, including expression-based effects like “</i><a href=\"https://www.youtube.com/effect/647912dc-0000-2d0c-aef9-582429bbd350\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Always smile</i></a><i>” (</i><b><i>left</i></b><i>) and \"</i><a href=\"https://youtube.com/effect/63fec95a-0000-2d83-b984-14223bb7b9e2\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Never blink</i></a><i>\" (</i><b><i>middle</i></b><i>) and Halloween-themed masks like \"</i><a href=\"https://youtube.com/effect/670d68b0-0000-286d-b0a7-089e08292e4c\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Risen zombie</i></a><i>\" (</i><b><i>right</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"iiwr9\">By bridging the gap between massive generative models and the constraints of mobile hardware, we are defining what is technically possible for real-time, on-device generative effects. This is just the beginning; we are actively working on integrating our newest models, like <a href=\"https://deepmind.google/models/veo/\" target=\"_blank\" rel=\"noopener noreferrer\">Veo 3</a>, and significantly reducing latency for entry-level devices, further democratizing access to cutting-edge generative AI in YouTube Shorts.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"iiwr9\"><i>We would like to thank our co-authors and collaborators: Sarah Xu, Maciej Pęśko, Paweł Andruszkiewicz, Jacob Rockwell, Ronny Votel, Robert (Guohui) Wang, Tingbo Hou, Karthik Raveendran, Jianing Wei, Matthias Grundmann, Omer Tov, Ariel Ephrat, Shiran Zada, and Inbar Mosseri.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "使用差分隐私分区选择大规模保护私有数据 (原标题: Securing private data at scale with differentially private partition selection)",
      "link": "https://research.google/blog/securing-private-data-at-scale-with-differentially-private-partition-selection/",
      "pubDate": "Tue, 19 Aug 2025 16:00:00 GMT",
      "isoDate": "2025-08-19T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 使用差分隐私分区选择大规模保护私有数据\n\n## 引言\n\n大型用户数据集对推动人工智能和机器学习模型至关重要，能带来改进服务、更准确预测和个性化体验等益处。共享这些数据集可以加速研究和创新，但同时也伴随着潜在的数据隐私风险。\n\n**差分隐私 (DP) 分区选择**是一种识别独特项目子集的方法，该子集可以从大量数据中安全共享，其依据是这些项目在许多个人贡献中出现的频率或显著性。通过在分区选择中应用差分隐私保护，可以确保任何人都无法得知任何单个用户的数据是否促成了最终列表中的特定项目。这通过添加受控噪声并仅选择在添加噪声后仍足够常见的项目来实现，从而保障个人隐私。\n\nDP 是许多重要数据科学和机器学习任务的第一步，包括：\n*   从大型私有语料库中提取词汇（或 n-gram）。\n*   以隐私保护方式分析数据流。\n*   获取用户数据的直方图。\n*   提高私有模型微调的效率。\n\n对于用户查询等海量数据集，并行算法至关重要。它能将问题分解为多个可同时计算的小部分，从而处理数百亿条数据，在不牺牲数据效用的前提下提供强大的隐私保障。\n\n## 新算法：可伸缩的自适应加权私有分区选择\n\n我们最近在 ICML2025 上发表的论文《可伸缩的自适应加权私有分区选择》(Scalable Private Partition Selection via Adaptive Weighting) 介绍了一种高效的并行算法，可将 DP 分区选择应用于各种数据发布。该算法在并行算法中表现最佳，可扩展到数百亿条数据，比之前的顺序算法处理的数据量大三个数量级。为鼓励研究社区的协作和创新，我们已在 GitHub 上开源了 DP 分区选择。\n\n### 算法工作原理\n\nDP 分区选择的目标是在严格保持用户级 DP 的同时，最大化从数据集合中选出的独特项目数量。这意味着属于许多用户的热门项目通常可以安全地保留用于下游计算任务，而仅属于单个用户的项目则不会被包含。算法设计者必须在选择数据集项目时，在隐私-效用权衡之间寻求最佳平衡，同时遵守差分隐私要求。\n\n### 标准范式：加权、加噪和过滤\n\n差分隐私分区选择的传统方法包括三个核心步骤：\n\n1.  **加权 (Weight)**：为每个项目计算一个“权重”，通常代表项目的频率或跨用户的聚合。这形成了一个权重直方图。一个关键的隐私要求是“低敏感度”，即任何单个用户贡献的总权重是有限的。在标准的非自适应设置中，用户独立于其他用户的贡献来为其项目分配权重。\n2.  **加噪 (Noise)**：为确保 DP，向每个项目的计算权重添加随机噪声（例如，具有特定标准差的高斯噪声）。这模糊了精确计数，防止攻击者推断个人的存在或数据。\n3.  **过滤 (Filter)**：最后，应用由 DP 参数确定的阈值。只有噪声权重超过此阈值的项目才会被包含在最终输出中。\n\n![加权、加噪和过滤范式](https://storage.googleapis.com/gweb-research2023-media/images/DPPS-1-Weightin.width-1250.png)\n*加权、加噪和过滤范式。在所有图中，x 轴表示项目（A-F），y 轴表示分配给项目的权重。算法首先计算项目的权重直方图（左），添加噪声（中），然后返回噪声权重高于阈值的项目（右）。*\n\n### 通过自适应加权提高效用\n\n标准非自适应方法的局限性在于潜在的“浪费”。高度热门的项目可能会获得远超通过隐私阈值所需的权重，从而导致“过度分配”。这些多余的权重本可以更有效地用于提升那些刚好低于阈值的项目，从而增加发布的项目总数并提高输出的效用。\n\n我们引入了**自适应加权**来解决这个问题。与非自适应方法中每个用户的贡献是独立的做法不同，自适应设计允许用户对项目的权重贡献考虑其他用户的贡献。这需要在不损害隐私或计算效率的前提下实现微妙的平衡。\n\n我们的新算法 **MaxAdaptiveDegree (MAD)** 策略性地重新分配权重。它识别具有显著“过剩权重”（远高于阈值）的项目，并将部分权重重新分配给“分配不足”（刚好低于阈值）的项目。这种自适应重新分配确保了更多不那么频繁的项目能够跨越隐私阈值并被包含在输出中。此外，MAD 保持了与基线相同的低敏感度界限和效率，这意味着它在并行处理框架（如 MapReduce-like 系统）中提供相同的强大隐私保障和可伸缩性，但具有严格优越的效用。\n\n此外，我们将这一概念扩展到多轮 DP 分区选择框架。我们展示了如何安全地在轮次之间发布中间噪声权重向量。这些额外信息允许更大的自适应性，因为我们可以减少未来对先前获得过多权重（可能再次被过度分配）或过少权重（可能永远无法跨越阈值）的项目的分配。这进一步优化了权重分布，在不牺牲隐私的情况下最大化了效用，并进一步增加了输出中的项目。\n\n## 实验结果\n\n我们对 MAD 算法（单次或多次迭代）与可伸缩的私有分区选择基线进行了广泛实验比较。\n\n![算法返回项目数量的比较](https://storage.googleapis.com/gweb-research2023-media/images/DPPS-2-Table.width-1250.png)\n*我们的算法：两轮 (MAD2R) 和其他基线（“Basic”代表均匀加权算法，以及 DP-SIPS）在九个公开数据集上返回的项目数量比较。通过利用新的自适应技术，我们的两轮算法在许多数据集上取得了最先进的结果。*\n\n如上表所示，仅进行两次迭代的 MAD (MAD2R) 在许多数据集上取得了最先进的结果——通常比其他方法（即使是使用更多轮次的方法）输出显著更多的项目，同时保持相同的隐私保障。我们的论文中提出的理论结果表明，我们的单轮算法 (MAD) 应该始终优于上述单轮高斯加权基线，实验结果证实了这一理论假设。我们新方法的卓越性能在各种隐私参数和超参数选择下都得到了验证。我们算法的 Python 内存实现示例可在开源仓库中获取。\n\n在大型公开 Common Crawl 数据集（包含近 8000 亿条目）上，我们将条目视为“用户”，将这些条目中的单词视为项目，从而获得了记录级 DP。在该数据集上，我们的两轮 MAD 算法输出了一组项目，覆盖了 99.9% 的条目（每个条目在输出中至少有一个项目）和 97% 的数据库条目（对应于输出中的一个项目），同时满足了 DP 保障。\n\n![两轮 MAD 算法返回的项目数量](https://storage.googleapis.com/gweb-research2023-media/images/DPPS-3-Counts.width-1250.png)\n*我们的两轮 MAD 算法在 Common Crawl 数据集上按频率计数（拥有该项目的条目数量）返回的项目数量。虽然 MAD 无法返回会违反隐私的低频项目，但它输出了绝大多数属于许多条目的项目。*\n\n## 结论\n\n我们引入了新方法来改进 DP 分区选择算法中的效用-隐私权衡。我们的算法在接近万亿条目的数据集上取得了最先进的结果。我们希望该算法能帮助从业者在严格遵守用户隐私的同时，在他们的工作流程中实现更高的效用。\n\n## 致谢\n\n我们感谢 Google Research 算法与优化团队的 Alessandro Epasto 和 Vincent Cohen-Addad 对这项工作的贡献。",
      "shortSummary": "本文介绍了一种名为“差分隐私 (DP) 分区选择”的技术，旨在从大规模数据集中安全地提取有价值信息，同时严格保护用户隐私。针对传统方法的局限性，我们提出了“可伸缩的自适应加权私有分区选择”算法（MAD），通过自适应加权策略，将多余权重重新分配给不那么频繁的项目。实验表明，MAD 算法在保持强大隐私保障和可伸缩性的前提下，显著提高了数据效用，在处理数百亿条数据时取得了最先进的结果，并已开源。",
      "translated_title": "使用差分隐私分区选择大规模保护私有数据",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DPPS-1-Weightin.width-1250.png",
          "alt": "DPPS-1-Weightin",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DPPS-2-Table.width-1250.png",
          "alt": "DPPS-2-Table",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DPPS-3-Counts.width-1250.png",
          "alt": "DPPS-3-Counts",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"jn8lu\">Large, user-based datasets are invaluable for advancing AI and machine learning models. They drive innovation that directly benefits users through improved services, more accurate predictions, and personalized experiences. Collaborating on and sharing such datasets can accelerate research, foster new applications, and contribute to the broader scientific community. However, leveraging these powerful datasets also comes with potential data privacy risks.</p><p data-block-key=\"b8u1a\">The process of identifying a specific, meaningful subset of unique items that can be shared safely from a vast collection based on how frequently or prominently they appear across many individual contributions (like finding all the common words used across a huge set of documents) is called “differentially private (DP) partition selection”. By applying differential privacy protections in partition selection, it’s possible to perform that selection in a way that prevents anyone from knowing whether any single individual's data contributed a specific item to the final list. This is done by adding controlled noise and only selecting items that are sufficiently common even after that noise is included, ensuring individual privacy. DP is the first step in many important data science and machine learning tasks, including extracting vocabulary (or <i>n</i>-grams) from a large private corpus (a necessary step of many textual analysis and language modeling applications), analyzing <a href=\"https://arxiv.org/pdf/2303.18086\" target=\"_blank\" rel=\"noopener noreferrer\">data streams</a> in a privacy preserving way, obtaining <a href=\"https://arxiv.org/pdf/2103.16787\" target=\"_blank\" rel=\"noopener noreferrer\">histograms</a> over user data, and <a href=\"https://arxiv.org/abs/2311.08357\" target=\"_blank\" rel=\"noopener noreferrer\">increasing efficiency</a> in private model fine-tuning.</p><p data-block-key=\"dgrb2\">In the context of massive datasets like user queries, a <i>parallel</i> algorithm is crucial. Instead of processing data one piece at a time (like a <i>sequential</i> algorithm would), a parallel algorithm breaks the problem down into many smaller parts that can be computed simultaneously across multiple processors or machines. This practice isn't just for optimization; it's a fundamental necessity when dealing with the scale of modern data. Parallelization allows the processing of vast amounts of information all at once, enabling researchers to handle datasets with hundreds of billions of items. With this, it’s possible to achieve robust privacy guarantees without sacrificing the utility derived from large datasets.</p><p data-block-key=\"8lb69\">In our recent publication, “<a href=\"https://arxiv.org/abs/2502.08878\" target=\"_blank\" rel=\"noopener noreferrer\">Scalable Private Partition Selection via Adaptive Weighting</a>”, which appeared at <a href=\"https://icml.cc/virtual/2025/poster/45510\" target=\"_blank\" rel=\"noopener noreferrer\">ICML2025</a>, we introduce an efficient parallel algorithm that makes it possible to apply <a href=\"https://arxiv.org/pdf/2006.03684\" target=\"_blank\" rel=\"noopener noreferrer\">DP partition selection</a> to various data releases. Our algorithm provides the best results across the board among parallel algorithms and scales to datasets with hundreds of billions of items, up to three orders of magnitude larger than those analyzed by prior sequential algorithms. To encourage collaboration and innovation by the research community, we are open-sourcing <a href=\"https://github.com/jusyc/dp_partition_selection\" target=\"_blank\" rel=\"noopener noreferrer\">DP partition selection on GitHub</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How the algorithm works</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"jn8lu\">The goal of DP partition selection is to maximize the number of unique items selected from a union of sets of data, while strictly preserving user-level DP. This means that very popular items, belonging to many users, can often be safely preserved for downstream computational tasks, whereas items belonging to only a single user would not be included. The algorithm designer must aim for an optimal privacy-utility trade-off in selecting items from the dataset while respecting the differential privacy requirement.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">The standard paradigm: Weight, noise, and filter</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"jn8lu\">The conventional approach to differentially private partition selection involves three core steps:</p><ol><li data-block-key=\"8a10c\"><i>Weight:</i> For each item, a \"weight\" is computed, typically representing the item’s frequency or some aggregation across users. This forms a weight histogram. A crucial privacy requirement is \"low sensitivity\", meaning that the total weight contributed by any single user is bounded. In a standard non-adaptive setting, users assign weights to their items independently of what other users contribute (e.g., the <a href=\"https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Gaussian weighting</a> baseline).<br></li><li data-block-key=\"6hp00\"><i>Noise:</i> To ensure DP, random noise (e.g., <a href=\"https://en.wikipedia.org/wiki/Gaussian_noise\" target=\"_blank\" rel=\"noopener noreferrer\">Gaussian noise</a> with a specific standard deviation) is added to each item's computed weight. This obfuscates the exact counts, preventing an attacker from inferring an individual's presence or data.<br></li><li data-block-key=\"23fi4\"><i>Filter:</i> Finally, a threshold determined by the DP parameters is applied. Only items whose noisy weights exceed this threshold are included in the final output.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DPPS-1-Weightin.width-1250.png\" alt=\"DPPS-1-Weightin\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DPPS-1-Weightin.width-1250.png\" alt=\"DPPS-1-Weightin\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"9j0dt\"><i>The weight, noise, and filter paradigm. In all plots, the x-axis represents items (A–F) and the y-axis represents the weight assigned to the items. The algorithm first computes a weight histogram over items (</i><b><i>left</i></b><i>), adds noise (</i><b><i>center</i></b><i>), and returns items with noisy weight above a threshold (</i><b><i>right</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Improving utility with adaptive weighting</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"jn8lu\">A limitation of the standard, non-adaptive approach is potential \"wastage\". Highly popular items might receive significantly more weight than necessary to cross the privacy threshold, effectively \"over-allocating\" weight. This excess weight could have been more effectively used to boost items that are just below the threshold, thereby increasing the overall number of items released and improving the utility of the output.</p><p data-block-key=\"91gmd\">We introduce adaptivity into the weight assignment process to address this. Unlike non-adaptive methods where each user's contribution is independent, an adaptive design allows the weight contributed by a user to an item to consider contributions from other users. This is a delicate balance, as it must be achieved without compromising privacy or computational efficiency.</p><p data-block-key=\"7uof8\">Our novel algorithm, MaxAdaptiveDegree (MAD), strategically reallocates weight. It identifies items with significant \"excess weight\" (far above the threshold) and reroutes some of that weight to \"under-allocated\" items (those just below the threshold). This adaptive reallocation ensures that more less-frequent items can cross the privacy threshold and be included in the output. Moreover, MAD maintains both the same low-sensitivity bounds and efficiency as the baseline, meaning it offers the same strong privacy guarantees and scalability in parallel processing frameworks (like <a href=\"https://en.wikipedia.org/wiki/MapReduce\" target=\"_blank\" rel=\"noopener noreferrer\">MapReduce</a>-like systems), but with strictly superior utility.</p><p data-block-key=\"1044t\">Furthermore, we extend this concept to multi-round DP partition selection frameworks. We demonstrate how to safely release intermediate noisy weight vectors between rounds. This additional information allows for even greater adaptivity, as we can reduce future weight allocations to items that previously received too much weight (and were likely to be over-allocated again) or too little weight (and were unlikely to ever cross the threshold). This further refines the weight distribution, maximizing utility without sacrificing privacy, and further increases the items in output.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Experiments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"jn8lu\">We conducted extensive experiments comparing our MAD algorithm with one or multiple iterations against scalable baselines for private partition selection.</p><p data-block-key=\"770h0\">As shown in the table below, MAD with just two iterations (column MAD2R) achieves state-of-the-art results across many datasets — often outputting significantly more items than other methods (even those using more rounds) while retaining the same privacy guarantees.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DPPS-2-Table.width-1250.png\" alt=\"DPPS-2-Table\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DPPS-2-Table.width-1250.png\" alt=\"DPPS-2-Table\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"9j0dt\"><i>Comparison of the number of items returned by our algorithms: Two-round (MAD2R) and other baselines (“Basic”, which represents the uniformly weighted algorithm, and</i> <a href=\"https://arxiv.org/abs/2301.01998\" target=\"_blank\" rel=\"noopener noreferrer\"><i>DP-SIPS</i></a><i>) on nine publicly-available datasets. By leveraging new techniques for adaptivity, our two-round algorithm achieves state-of-art results across many datasets.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"jn8lu\">In our <a href=\"https://arxiv.org/abs/2502.08878\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>, we present theoretical results that suggest our single-round algorithm (MAD) should always outperform the single-round Gaussian weighting baseline mentioned above. Our results demonstrate that this theoretical hypothesis appears to be correct. The excellent performance of our new methods holds across a wide selection of privacy parameters and hyperparameter choices. An example in-memory implementation of our algorithm in Python is available in the <a href=\"https://github.com/jusyc/dp_partition_selection\" target=\"_blank\" rel=\"noopener noreferrer\">open-source repo</a>.</p><p data-block-key=\"5j03u\">On the large-scale publicly-available <a href=\"https://commoncrawl.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Common Crawl</a> dataset (comprising close to 800 billion entries), we obtained record-level DP by treating entries as “users” and the words in these entries as items. On this dataset, our two-iteration MAD algorithm output a set of items that covered 99.9% of the entries (each of which has at least one item in the output) and 97% of the database entries (corresponding to an item that is in the output) while satisfying the DP guarantee.</p><p data-block-key=\"3h49e\">With just two iterations, our algorithm achieved state-of-the-art results in a wide range of parameter settings. As expected from our theoretical results, our algorithm always outperformed the baseline.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DPPS-3-Counts.width-1250.png\" alt=\"DPPS-3-Counts\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DPPS-3-Counts.width-1250.png\" alt=\"DPPS-3-Counts\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"9j0dt\"><i>The number of items our two-iteration MAD algorithm returns by frequency count (the number of entries that have that item) on the Common Crawl dataset. While MAD cannot return low frequency items that would violate privacy, it outputs the vast majority of items that belong to many entries.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"jn8lu\">We introduce new methods to improve the utility-privacy trade-off in DP partition selection algorithms. Our algorithm achieves state-of-the-art results on datasets approaching a trillion entries. We hope this algorithm will help practitioners achieve higher utility across their workflows while strictly respecting user privacy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"jn8lu\"><i>We thank Alessandro Epasto, Vincent Cohen-Addad, part of the</i> <a href=\"https://research.google/teams/algorithms-optimization/\"><i>Algorithm and Optimization team</i></a><i> in</i> <a href=\"https://research.google/\"><i>Google Research</i></a><i>, who contributed to this work.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "超越十亿参数的负担：使用条件生成器解锁数据合成 (原标题: Beyond billion-parameter burdens: Unlocking data synthesis with a conditional generator)",
      "link": "https://research.google/blog/beyond-billion-parameter-burdens-unlocking-data-synthesis-with-a-conditional-generator/",
      "pubDate": "Wed, 13 Aug 2025 16:00:00 GMT",
      "isoDate": "2025-08-13T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 超越十亿参数的负担：使用条件生成器解锁数据合成\n\n### 引言\n\n生成大规模差分隐私（DP）合成数据面临巨大挑战，主要源于隐私-计算-效用之间的根本权衡。强大的隐私保证往往会损害合成数据质量，或需要大量的计算资源。当前流行的解决方案是在“私有数据”上私有微调十亿参数级大型语言模型（LLM），然后从微调后的模型中采样生成合成数据。然而，这种方法计算成本高昂，对于资源受限的应用来说难以实现。近期提出的 Aug-PE 和 Pre-Text 算法虽然探索了仅需 LLM API 访问的合成数据生成方法，但它们通常严重依赖手动提示来生成初始数据集，并且在迭代数据选择过程中未能有效利用私有信息。\n\n### CTCL 框架：一种新颖的解决方案\n\n在 ICML 2025 上发表的论文“Synthesizing Privacy-Preserving Text Data via Fine-Tuning Without Fine-Tuning Billion-Scale LLMs”中，我们提出了 CTCL（Data Synthesis with ConTrollability and CLustering），一个用于生成隐私保护合成数据的新颖框架，它无需微调十亿参数级 LLM，也无需领域特定的提示工程。CTCL 使用一个轻量级的 1.4 亿参数模型，使其适用于资源受限的应用。通过对主题信息进行条件化，生成的合成数据可以匹配私有领域的主题分布。此外，与 Aug-PE 算法不同，CTCL 允许生成无限量的合成数据样本，而无需支付额外的隐私成本。\n\n### CTCL 框架的构建\n\nCTCL 框架旨在从私有数据集中生成高质量的合成数据，同时保护隐私。它将整个过程分解为三个主要步骤，并依赖于两个核心组件：CTCL-Topic 和 CTCL-Generator。\n\n*   **CTCL-Topic**：一个通用主题模型，用于捕获数据集的高级主题。\n*   **CTCL-Generator**：一个强大的语言模型，可以根据特定关键词创建文档。\n\n这两个组件均使用大型公共语料库开发，是学习不同私有领域并从中生成合成数据的基础。\n\n#### 步骤 1：开发 CTCL-Topic 和 CTCL-Generator\n\n这两个组件只需使用大规模公共语料库开发一次，之后可用于学习不同的私有领域。\n\n*   **CTCL-Topic**：从维基百科（包含约 600 万文档的多元语料库）中提取的主题模型。它遵循 BERTopic 方法，将每个文档嵌入，聚类成约 1000 个主题，并用 10 个关键词表示每个主题。\n*   **CTCL-Generator**：一个轻量级（1.4 亿参数）条件语言模型，接受自由形式的文档描述（如文档类型、关键词等）作为输入，并生成满足输入条件的文档。为了构建预训练数据，我们对 SlimPajama 中的每个文档使用 Gemma-2-2B 进行提示，生成“多方面描述文档”的结果，形成包含 4.3 亿个描述-文档对的数据集。然后，我们使用此数据集在 BART-base（一个 1.4 亿参数的语言模型）之上进行持续预训练，从而得到 CTCL-Generator。\n\n![BBPB-1-CTCLTopic](https://storage.googleapis.com/gweb-research2023-media/images/BBPB-1-CTCLTopic.width-1250.png)\n\n*图 1：步骤 1：使用大规模公共语料库开发通用主题模型 CTCL-Topic 和具有强大可控性的轻量级 1.4 亿参数 CTCL-Generator。*\n\n#### 步骤 2：学习私有领域\n\n我们使用 CTCL-Topic 捕获整个私有语料库的高级分布信息。这通过收集代表私有数据主题分布的私有直方图（即每个主题在私有数据中的百分比）来实现。此主题直方图将在步骤 3 中用于采样。\n\n在收集主题直方图的同时，私有数据集中的每个文档都已关联到一个主题。然后，我们将私有数据集转换为关键词和文档对的数据集，其中每个文档的 10 个关键词来自其在 CTCL-Topic 中对应的关键词。随后，我们使用 DP 对 CTCL-Generator 在此数据集上进行微调。\n\n![BBPB-2a-TCTLGen](https://storage.googleapis.com/gweb-research2023-media/images/BBPB-2a-TCTLGen.width-1250.png)\n\n*图 2：步骤 2：为了学习私有领域，我们从私有数据中收集 DP 主题直方图，并使用 DP 对 CTCL-Generator 在私有数据上进行微调。*\n\n#### 3. 生成合成数据\n\n根据 DP 主题直方图，按比例对经过 DP 微调的 CTCL-Generator 进行采样。具体来说，给定所需的合成数据集大小（例如 N）和 DP 主题直方图（例如主题 1 占 x%，主题 2 占 y% 等），我们就可以知道每个主题的目标样本数量（即主题 1 占 N*x%，主题 2 占 N*y% 等）。对于每个主题，我们使用相应的 10 个关键词作为输入，通过经过 DP 微调的 CTCL-Generator 生成数据。根据 DP 的后处理特性，CTCL-Generator 可以生成任意数量的合成数据，而无需支付额外的隐私成本。\n\n![BBPB-3a-SynthDataGen](https://storage.googleapis.com/gweb-research2023-media/images/BBPB-3a-SynthDataGen.width-1250.png)\n\n*图 3：步骤 3：基于 DP 主题直方图和经过 DP 微调的 CTCL-Generator 生成隐私保护合成数据。*\n\n### 实验与结果\n\n我们在四个数据集上进行了实验，其中三个数据集对应下游生成任务，一个数据集对应分类任务。生成任务通常比分类任务更具挑战性，因为它要求合成数据保留私有数据中细粒度的文本信息，而分类任务仅需保持标签和单词之间的共现模式。\n\n*   **生成任务**：PubMed（医学论文摘要）、Chatbot Arena（人机交互）和 Multi-Session Chat（人际日常对话）。通过在合成数据上训练小型下游语言模型，并在真实测试数据上计算下一个词预测准确率来评估质量。\n*   **分类任务**：OpenReview（学术论文评论）。通过在合成数据上训练下游分类器，并在真实测试数据上计算分类准确率来评估质量。\n\n为避免数据污染，我们仔细分析了所选数据集，确保预训练数据与下游数据集之间没有重叠。\n\n**结果显示**：\n\nCTCL 始终优于其他基线方法，尤其是在强隐私保证（ε 值更小）的情况下。这表明 CTCL 能够有效捕获私有数据中的有用信息，同时保持隐私。\n\n![BBPB-4-Performance](https://storage.googleapis.com/gweb-research2023-media/images/BBPB-4-Performance.width-1250.png)\n\n*图 4：CTCL 在四个数据集上的表现优于其他基线，尤其是在具有更强隐私保证（ε 值更小）的挑战性场景中。*\n\n与 Aug-PE 相比，CTCL 在隐私预算和合成数据量方面都具有更好的可扩展性。CTCL 的性能随隐私预算的增加而提高，而 Aug-PE 则不然。此外，随着下游模型获得更多 CTCL 生成的样本，准确率会提高，而 Aug-PE 的性能在大约 10K 样本时趋于饱和。这些结果与基于微调的方法（如 CTCL）比基于提示的方法（如 Aug-PE）能更好地捕获细粒度统计信息的直觉相符。\n\n![BBPB-5-vsAugPE](https://storage.googleapis.com/gweb-research2023-media/images/BBPB-5-vsAugPE.width-1250.png)\n\n*图 5：CTCL 在隐私预算（左图）和下游任务训练的合成数据量（右图）方面比 Aug-PE 具有更好的可扩展性。*\n\n**消融研究**：验证了框架中两个关键组件的重要性：1) 在公共语料库上预训练 CTCL-Generator，以及 2) 在 DP 微调期间结合基于关键词的条件。结果表明，在固定隐私预算下，结合关键词可将测试损失降低 50%，而增加预训练可再降低 50%。这证明了这两个组件在框架设计中的关键作用。\n\n### 未来工作\n\nCTCL 使用的生成器仅有 1.4 亿参数。但 CTCL 的核心思想，即使用聚类信息或 LLM 提取的元数据作为输入指令，可以很容易地扩展到更大规模的模型。我们正在积极探索这一想法，以帮助改进实际应用。\n\n### 致谢\n\n这项工作主要由 Bowen Tan 在 Google Research 实习期间完成，由 Shanshan Wu 和 Zheng Xu 指导。感谢 Daniel Ramage 和 Brendan McMahan 的领导支持，外部学术合作伙伴 Eric Xing 和 Zhiting Hu 对 ICML 论文的宝贵反馈，Zachary Garrett 和 Michael Riley 对早期草稿的审阅，Taylor Montgomery 对数据集使用的审阅，Mark Simborg 和 Kimberly Schwede 对博客文章和图形编辑的帮助。我们感谢 ICML 审稿人宝贵的时间和对我们论文的深刻评论。",
      "shortSummary": "CTCL 是一种新颖的框架，旨在无需微调十亿参数级 LLM 即可生成大规模差分隐私（DP）合成数据。它通过轻量级 1.4 亿参数模型，结合主题条件化，解决了隐私-计算-效用权衡问题。CTCL 包含 CTCL-Topic 和 CTCL-Generator 两个核心组件，并通过三步法实现：开发组件、学习私有领域和生成合成数据。实验表明，CTCL 在强隐私保证下性能优于现有基线，并提供更好的可扩展性，且能无限量生成数据而无需额外隐私成本。",
      "translated_title": "超越十亿参数的负担：使用条件生成器解锁数据合成",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/BBPB-1-CTCLTopic.width-1250.png",
          "alt": "BBPB-1-CTCLTopic",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/BBPB-2a-TCTLGen.width-1250.png",
          "alt": "BBPB-2a-TCTLGen",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/BBPB-3a-SynthDataGen.width-1250.png",
          "alt": "BBPB-3a-SynthDataGen",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/BBPB-4-Performance.width-1250.png",
          "alt": "BBPB-4-Performance",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/BBPB-5-vsAugPE.width-1250.png",
          "alt": "BBPB-5-vsAugPE",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fiy0g\">Generating large-scale <a href=\"https://en.wikipedia.org/wiki/Differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">differentially private</a> (DP) synthetic data is challenging due to the fundamental privacy–computation–utility trade-off, where strong privacy guarantees can either hurt the quality of the synthetic data, or require large amounts of computation. A popular solution is to <a href=\"https://research.google/blog/protecting-users-with-differentially-private-synthetic-training-data/\">privately fine-tune</a> a billion-size large language model (LLM) on the “private data” (a standard term referring to the dataset on which one plans to offer privacy guarantees) and then sample from the fine-tuned model to generate synthetic data. This approach is computationally expensive and hence unattainable for <a href=\"https://research.google/blog/synthetic-and-federated-privacy-preserving-domain-adaptation-with-llms-for-mobile-applications/\">resource-constrained applications</a>. So, recently proposed <a href=\"https://arxiv.org/abs/2403.01749\" target=\"_blank\" rel=\"noopener noreferrer\">Aug-PE</a> and <a href=\"https://arxiv.org/abs/2406.02958\" target=\"_blank\" rel=\"noopener noreferrer\">Pre-Text</a> algorithms have explored generating synthetic data that only requires LLM API access. However, they usually depend heavily on manual prompts to generate the initial dataset and are ineffective in using private information in their iterative data selection process.</p><p data-block-key=\"999gh\">In “<a href=\"https://arxiv.org/abs/2503.12347\" target=\"_blank\" rel=\"noopener noreferrer\">Synthesizing Privacy-Preserving Text Data via Fine-Tuning <i>Without</i> Fine-Tuning Billion-Scale LLMs</a>”, presented at <a href=\"https://icml.cc//virtual/2025/poster/45901\" target=\"_blank\" rel=\"noopener noreferrer\">ICML 2025</a>, we propose CTCL (Data Synthesis with ConTrollability and CLustering), a novel framework for generating privacy-preserving synthetic data without fine-tuning billion-scale LLMs or domain-specific prompt engineering. CTCL uses a lightweight 140 million parameter model, making it practical for <a href=\"https://research.google/blog/synthetic-and-federated-privacy-preserving-domain-adaptation-with-llms-for-mobile-applications/\">resource-constrained applications</a>. By conditioning on the topic information, the generated synthetic data can match the distribution of topics from the private domain. Finally, unlike the <a href=\"https://arxiv.org/abs/2403.01749\" target=\"_blank\" rel=\"noopener noreferrer\">Aug-PE</a> algorithm, CTCL allows generating unlimited synthetic data samples without paying additional privacy costs. We evaluated CTCL across diverse datasets, demonstrating that it consistently outperforms baselines, particularly under strong privacy guarantees. Ablation studies confirmed the crucial impact of its pre-training and keyword-based conditioning, while experiments also showed CTCL's improved scalability compared to the <a href=\"https://arxiv.org/abs/2403.01749\" target=\"_blank\" rel=\"noopener noreferrer\">Aug-PE</a> algorithm.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Creating the data synthesis framework</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fiy0g\">The CTCL Framework is designed to generate high-quality synthetic data from private datasets while preserving privacy. It achieves this by breaking down the process into three main steps. Before we dive into the details, it's essential to understand the two core components that make this framework work: CTCL-Topic and CTCL-Generator. CTCL-Topic is a universal topic model that captures the high-level themes of a dataset, while CTCL-Generator is a powerful language model that can create documents based on specific keywords. These two components, developed using large public corpora, are the foundation for learning different private domains and generating synthetic data from them.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Step 1: Developing CTCL-Topic and CTCL-Generator</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fiy0g\">Both components are developed only <i>once</i> using large-scale public corpora and can then be used later for learning different private domains. CTCL-Topic is a topic model extracted from <a href=\"https://dumps.wikimedia.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Wikipedia</a>, a diverse corpus containing around 6 million documents. We follow <a href=\"https://maartengr.github.io/BERTopic/\" target=\"_blank\" rel=\"noopener noreferrer\">BERTopic</a> to embed each document, cluster them into around 1K clusters (i.e., 1K topics), and represent each cluster by 10 keywords.</p><p data-block-key=\"8ohfr\">CTCL-Generator is a lightweight (140M-parameter) conditional language model that accepts free-form document descriptions as inputs (e.g., document type, keywords, etc.) and generates documents satisfying the input conditions. To construct the pre-training data, for each document in <a href=\"https://huggingface.co/datasets/cerebras/SlimPajama-627B\" target=\"_blank\" rel=\"noopener noreferrer\">SlimPajama</a>, we prompt <a href=\"https://arxiv.org/abs/2408.00118\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma-2-2B</a> to “Describe the document in multiple aspects.” The result is a dataset comprising 430M description–document pairs. We then use this dataset to perform continual pre-training on top of <a href=\"https://arxiv.org/abs/1910.13461\" target=\"_blank\" rel=\"noopener noreferrer\">BART-base</a> (a 140M-parameter language model), yielding the CTCL-Generator.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/BBPB-1-CTCLTopic.width-1250.png\" alt=\"BBPB-1-CTCLTopic\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/BBPB-1-CTCLTopic.width-1250.png\" alt=\"BBPB-1-CTCLTopic\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"plyed\"><i>Step 1: A universal topic model CTCL-Topic and a lightweight 140M-parameter CTCL-Generator with strong controllability are developed using large-scale public corpora.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Step 2: Learning the private domain</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fiy0g\">We then use CTCL-Topic to capture the high-level distributional information from the entire private corpus. This is done by collecting a private histogram representing the topic-wise distribution of the private data, i.e., the percentage of each topic in the private data. This topic histogram will be used later in Step 3 for sampling.</p><p data-block-key=\"altlo\">While collecting the topic histogram, each document in the private dataset has been associated with a topic. We then transform the private dataset into a dataset of keywords and document pairs, the 10 keywords for each document are obtained from their corresponding topic in CTCL-Topic. We then fine-tune the CTCL-Generator with DP on this dataset.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/BBPB-2a-TCTLGen.width-1250.png\" alt=\"BBPB-2a-TCTLGen\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/BBPB-2a-TCTLGen.width-1250.png\" alt=\"BBPB-2a-TCTLGen\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"plyed\"><i>Step 2: To learn the private domain, we collect a DP topic histogram from the private data, and fine-tune the CTCL-Generator with DP on the private data.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Step 3: Generating synthetic data</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fiy0g\">The DP fine-tuned CTCL-Generator is sampled proportionally for each topic according to the DP topic histogram. Specifically, given the desired size of the synthetic dataset (say, <i>N</i>) and the DP topic histogram (say, <i>x</i>% for Topic 1, <i>y</i>% for Topic 2, etc.), we know the number of target samples for each topic (i.e., <i>x%*N</i> for Topic 1, <i>y</i>%*<i>N</i> for Topic 2, etc.). For each topic, we use the corresponding 10 keywords as input to the DP fine-tuned CTCL-Generator to generate data. An arbitrary amount of synthetic data can be generated by CTCL-Generator without paying additional privacy costs, following the <a href=\"https://www.cis.upenn.edu/~aaroth/privacybook.html\" target=\"_blank\" rel=\"noopener noreferrer\">post-processing property of DP</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/BBPB-3a-SynthDataGen.width-1250.png\" alt=\"BBPB-3a-SynthDataGen\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/BBPB-3a-SynthDataGen.width-1250.png\" alt=\"BBPB-3a-SynthDataGen\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"plyed\"><i>Step 3: Privacy-preserving synthetic data is generated based on the DP topic histogram and the DP fine-tuned CTCL-Generator.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Experiments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fiy0g\">We conducted experiments on four datasets, where three datasets correspond with downstream generative tasks and one dataset with a classification task. Generative tasks are typically more challenging than classification tasks. This is because the generative tasks are evaluated by the next-token prediction accuracy, which requires the synthetic data to preserve fine-grained textual information from the private data. In contrast, the classification tasks only require maintaining the co-occurrence patterns between labels and words in the private data.</p><p data-block-key=\"afnhr\">The three generative tasks are chosen to cover a diverse set of practical scenarios: <a href=\"https://openreview.net/forum?id=FKwtKzglFb\" target=\"_blank\" rel=\"noopener noreferrer\">PubMed</a> (medical paper abstracts), <a href=\"https://openreview.net/forum?id=uccHPGDlao\" target=\"_blank\" rel=\"noopener noreferrer\">Chatbot Arena</a> (human-to-machine interactions), and <a href=\"https://parl.ai/projects/msc/\" target=\"_blank\" rel=\"noopener noreferrer\">Multi-Session Chat</a> (human-to-human daily dialogues). To evaluate the quality of the generated synthetic data, we followed the setup of <a href=\"https://arxiv.org/abs/2403.01749\" target=\"_blank\" rel=\"noopener noreferrer\">Aug-PE</a> to train a small downstream language model on the synthetic data and then compute the next-token prediction accuracy on the real test data.</p><p data-block-key=\"etu8e\">The classification task is performed on the <a href=\"https://github.com/AI-secure/aug-pe/tree/main/data/openreview\" target=\"_blank\" rel=\"noopener noreferrer\">OpenReview</a> (academic paper reviews) dataset. To evaluate the quality of the generated synthetic data, we train a downstream classifier on the synthetic data, and compute the classification accuracy on the real test data.</p><p data-block-key=\"1rsp8\">To mitigate concerns regarding data contamination, we carefully analyzed our selected datasets. Our analysis showed no overlap between our pre-training data and the downstream datasets.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fiy0g\">CTCL consistently outperforms the other baselines, especially in the strong privacy guarantee regime. The plot below compares CTCL and the following baseline algorithms: Downstream DPFT (i.e., directly DP fine-tuning downstream model on the private data without using synthetic data), <a href=\"https://arxiv.org/abs/2403.01749\" target=\"_blank\" rel=\"noopener noreferrer\">Aug-PE</a> (an augmented version of the Private Evolution algorithm), DP fine-tuning an LLM of similar size to CTCL to generate <a href=\"https://aclanthology.org/2023.acl-long.74/\" target=\"_blank\" rel=\"noopener noreferrer\">synthetic</a> <a href=\"https://arxiv.org/abs/2306.01684\" target=\"_blank\" rel=\"noopener noreferrer\">data</a>, with <a href=\"https://arxiv.org/abs/2402.13659\" target=\"_blank\" rel=\"noopener noreferrer\">post-generation resampling</a>. The plot below illustrates CTCL's improved performance, particularly for the more challenging setting that satisfies a stronger privacy guarantee (i.e., smaller <i>ε</i> value). This demonstrates CTCL’s strong ability to effectively capture useful information from the private data while maintaining privacy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/BBPB-4-Performance.width-1250.png\" alt=\"BBPB-4-Performance\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/BBPB-4-Performance.width-1250.png\" alt=\"BBPB-4-Performance\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"plyed\"><i>CTCL demonstrates improved performance over other baselines across the four datasets, especially in the challenging regime with stronger privacy guarantees (as shown by smaller ε).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fiy0g\">Also, compared to <a href=\"https://arxiv.org/abs/2403.01749\" target=\"_blank\" rel=\"noopener noreferrer\">Aug-PE</a>, CTCL has better scalability in terms of both the privacy budget and synthetic data size. As shown by the left plot below, CTCL improves with an increased privacy budget while Aug-PE does not. This limitation may stem from Aug-PE’s constrained capacity (i.e., only via the nearest neighbors) to effectively capture information in the private data. The right plot shows that accuracy increases as the downstream model is given access to more CTCL-generated samples, while the performance of Aug-PE saturates around 10K examples. These results align with the intuition that fine-tuning–based methods (e.g., CTCL) can better capture fine-grained statistics than prompting-based methods (e.g., <a href=\"https://arxiv.org/abs/2403.01749\" target=\"_blank\" rel=\"noopener noreferrer\">Aug-PE</a>).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/BBPB-5-vsAugPE.width-1250.png\" alt=\"BBPB-5-vsAugPE\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/BBPB-5-vsAugPE.width-1250.png\" alt=\"BBPB-5-vsAugPE\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"plyed\"><i>CTCL has better scalability than Aug-PE in terms of privacy budget (</i><b><i>left</i></b><i>) and continues to improve as the downstream tasks are trained on more synthetic data (</i><b><i>right</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fiy0g\">And finally, ablation studies validate the importance of two key components in our framework: 1) pre-training the CTCL-Generator on public corpus, and 2) incorporating keyword-based conditions during DP fine-tuning. Specifically, starting from the standard DP fine-tuning, we sequentially introduce these components and measure the downstream model’s test loss. For a fixed privacy budget, our results show that incorporating keywords during DP fine-tuning reduces the test loss by 50%, and adding pre-training gives another 50% reduction. This demonstrates that both components are crucial in our framework design.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Future Work</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fiy0g\">Our experiments synthesizing data with <a href=\"https://arxiv.org/abs/2503.12347\" target=\"_blank\" rel=\"noopener noreferrer\">ConTrollability and CLustering (CTCL)</a> uses a generator of only 140M parameters. But the key idea of CTCL, i.e., using clustering information or LLM extracted metadata as input instructions, can be easily extended to larger size models. We are actively working on exploring this idea to help improve real-world applications.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fiy0g\"><i>This work was primarily done by Bowen Tan during his internship at Google Research, under the guidance of Shanshan Wu and Zheng Xu. We thank Daniel Ramage and Brendan McMahan for leadership support, external academic partners Eric Xing and Zhiting Hu for helpful feedback on the ICML paper, Zachary Garrett and Michael Riley for reviewing an early draft, Taylor Montgomery for reviewing the dataset usage, Mark Simborg and Kimberly Schwede for help editing the blogpost and graphics. We are grateful to the ICML reviewers for their valuable time and insightful comments on our paper.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    }
  ],
  "lastUpdated": "2025-09-17T10:28:06.324Z"
}