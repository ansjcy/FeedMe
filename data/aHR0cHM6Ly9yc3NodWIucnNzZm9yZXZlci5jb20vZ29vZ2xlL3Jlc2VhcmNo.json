{
  "sourceUrl": "https://rsshub.rssforever.com/google/research",
  "title": "Google Research Blog",
  "description": "Google Research Blog - Powered by RSSHub",
  "link": "https://research.google/blog",
  "items": [
    {
      "title": "Google Research 2025: Bolder breakthroughs, bigger impact",
      "link": "https://research.google/blog/google-research-2025-bolder-breakthroughs-bigger-impact/",
      "pubDate": "Wed, 17 Dec 2025 16:00:00 GMT",
      "isoDate": "2025-12-17T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Google Research 2025: Bolder breakthroughs, bigger impact",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EOY-0d-Hero.width-1250.png",
          "alt": "A timeline of Google Research’s 2025 moments",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EOY-3-FACTS.width-1250.png",
          "alt": "A data table ranks 15 AI models by their \"FACTS Score,\" showing Gemini 3 Pro in first place with a score of 68.8.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EOY-8-NLMGenomics.width-1250.png",
          "alt": "Two Google Research banners featuring microscopic cells and a DNA strand ask questions about linking genetics to health and genome sequencing.",
          "title": "",
          "position": 5
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EOY-10-FireSat.width-1250.png",
          "alt": "A satellite map of the Oregon-California border featuring a thermal infrared inset pinpointing a small fire near Medford.",
          "title": "",
          "position": 7
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EOY-12-MobilityAI.width-1250.png",
          "alt": "A \"Traffic Simulation API\" dashboard displays a map of Seattle with purple-shaded road segments analyzing the impact of lane closures.",
          "title": "",
          "position": 8
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qt6m0\">Google Research teams have invested over the years in advancing research and technology in a diverse range of strategic areas. We are working across time horizons, from bold moonshots and curiosity-driven transformative research where we explore the art of the possible, to innovation and applied research with accelerated impact. The <a href=\"https://research.google/blog/accelerating-the-magic-cycle-of-research-breakthroughs-and-real-world-applications/\">Magic Cycle of research</a> is accelerating — we’re driving research breakthroughs and translating them into real-world solutions, with impact on products, science and society, in close collaboration with many teams across Google and global partners.</p><p data-block-key=\"fc6rj\">This was quite a year! Our foundational AI breakthroughs helped make generative models more efficient, factual, multilingual, and multi-cultural, and we introduced generative UI. We advanced new architectures and algorithmic research and pioneered AI tools and agentic models that help accelerate scientific discovery. We achieved quantum breakthroughs that bring us closer to real-world applications of quantum computing; advanced research on Earth sciences to enable a level of planetary understanding never before possible; drove forward scientific domains including genomics, biology and neuroscience; and made headway on societal priorities like climate resilience, health and education.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-0d-Hero.width-1250.png\" alt=\"A timeline of Google Research’s 2025 moments\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-0d-Hero.width-1250.png\" alt=\"A timeline of Google Research’s 2025 moments\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"un5jf\"><i>A look back at some of Google Research's 2025 moments realized in collaboration with many teams across Google. This image was created with Nano Banana.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Factual, multilingual, multi-cultural GenAI\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Advancing generative models to be more efficient, factual, multilingual and multi-cultural</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">To help fuel this era of rapid innovation, we’re investing in efficiency, making Google products more cost and energy efficient, and setting the bar for the industry. We continue to develop new approaches based on <a href=\"https://research.google/blog/looking-back-at-speculative-decoding/\">speculative decoding</a>, such as <a href=\"https://arxiv.org/abs/2403.10444\" target=\"_blank\" rel=\"noopener noreferrer\">block verification</a>, to further accelerate efficiency gains. At the other end of the infrastructure stack, <a href=\"https://research.google/blog/solving-virtual-machine-puzzles-how-ai-is-optimizing-cloud-computing/\">LAVA</a> is a new scheduling algorithm that continuously re-predicts the lifespans of tasks on virtual machines. It is designed to optimize resource efficiency in large cloud data centers, without sacrificing reliability.</p><p data-block-key=\"birtg\">Equally critical, our pioneering research on LLM factuality, dating back to 2021, helps make <a href=\"https://blog.google/products/gemini/gemini-3/#gemini-3\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini 3</a> our most capable and factual LLM yet. It achieves state-of-the-art performance on public factuality benchmarks like <a href=\"https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified\" target=\"_blank\" rel=\"noopener noreferrer\">SimpleQA Verified</a> and the new <a href=\"https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/\" target=\"_blank\" rel=\"noopener noreferrer\">FACTS benchmark suite</a> that we released with Google DeepMind and Kaggle. Users can be confident that products such as the Gemini app, <a href=\"https://blog.google/products/search/generative-ai-google-search-may-2024/\" target=\"_blank\" rel=\"noopener noreferrer\">AI Overviews</a> and <a href=\"https://blog.google/products/search/google-search-ai-mode-update/#ai-mode-search\" target=\"_blank\" rel=\"noopener noreferrer\">AI Mode</a> in Search, and Vertex AI all provide outputs grounded in world knowledge. This year we studied how LLMs <a href=\"https://arxiv.org/abs/2505.24858\" target=\"_blank\" rel=\"noopener noreferrer\">convey uncertainty</a>; presented a framework for assessing whether LLMs <a href=\"https://arxiv.org/abs/2503.15299\" target=\"_blank\" rel=\"noopener noreferrer\">encode more factual knowledge</a> in their parameters than they express in their outputs; presented a multilingual dataset that evaluates cross-lingual knowledge, called <a href=\"https://research.google/blog/eclektic-a-novel-benchmark-for-evaluating-cross-lingual-knowledge-transfer-in-llms/\">ECLeKTic</a>; and more.</p><p data-block-key=\"tc29\">We also explored the role of <a href=\"https://research.google/blog/deeper-insights-into-retrieval-augmented-generation-the-role-of-sufficient-context/\">sufficient context</a> in <a href=\"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\" target=\"_blank\" rel=\"noopener noreferrer\">retrieval augmented generation</a> systems, which enhance LLMs by providing them with relevant external context. We <a href=\"https://arxiv.org/abs/2411.06037\" target=\"_blank\" rel=\"noopener noreferrer\">demonstrated</a> that it is possible to know when an LLM has enough information to provide a correct answer to a question. This work supported the launch of the <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/retrieval-and-ranking#llm_reranker\" target=\"_blank\" rel=\"noopener noreferrer\">LLM Re-Ranker</a> in the <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/rag-overview\" target=\"_blank\" rel=\"noopener noreferrer\">Vertex AI RAG Engine</a>, leading to better retrieval metrics and system accuracy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"FACTS\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-3-FACTS.width-1250.png\" alt=\"A data table ranks 15 AI models by their &quot;FACTS Score,&quot; showing Gemini 3 Pro in first place with a score of 68.8.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-3-FACTS.width-1250.png\" alt=\"A data table ranks 15 AI models by their &quot;FACTS Score,&quot; showing Gemini 3 Pro in first place with a score of 68.8.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"mr25z\"><i>We evaluated leading LLMs on the</i> <a href=\"https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>FACTS Benchmark Suite</i></a><i>, which includes four different factuality benchmarks. The table above lists 15 leading models and their overall FACTS score. Gemini 3 Pro leads in overall performance.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qt6m0\">With the rise of multimodal content, we’ve expanded our work on factuality to images, audio, video, 3D environments and LLM-generated applications. This work helps to improve the quality of Google’s video and image model families, including <a href=\"https://deepmind.google/models/veo/\" target=\"_blank\" rel=\"noopener noreferrer\">Veo</a>, <a href=\"https://blog.google/products/gemini/gemini-app-updates-io-2025/#imagen-4-veo-3\" target=\"_blank\" rel=\"noopener noreferrer\">Imagen</a> and <a href=\"https://blog.google/products/gemini/updated-image-editing-model/\" target=\"_blank\" rel=\"noopener noreferrer\">Nano Banana</a>. It is a great example of the cycle of research and how we’re continuously adapting to real user needs. Our latest research includes making <a href=\"https://arxiv.org/abs/2504.17502\" target=\"_blank\" rel=\"noopener noreferrer\">text-to-image generation</a> and <a href=\"https://arxiv.org/pdf/2506.07631\" target=\"_blank\" rel=\"noopener noreferrer\">image captions</a> more accurate, and creating <a href=\"https://arxiv.org/abs/2505.22657\" target=\"_blank\" rel=\"noopener noreferrer\">3DMem-Bench</a> for evaluating an agent’s ability to reason over long-term memory in 3D.</p><p data-block-key=\"f4fhj\">Our long-running multilinguality research helped <a href=\"https://deepmind.google/models/gemma/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma</a> expand to over 140 languages, making it today’s best multilingual open model. We’re also augmenting our models with socio-cultural intelligence, attuning them to diverse user needs and global contexts. We introduced <a href=\"https://arxiv.org/abs/2510.06124\" target=\"_blank\" rel=\"noopener noreferrer\">TUNA</a>, a comprehensive taxonomy of user needs and actions, launched a community-based data <a href=\"https://research.google/blog/amplify-initiative-localized-data-for-globalized-ai/\">collection platform</a> to target under-represented languages and geographies, and developed new methods to ground models in <a href=\"https://arxiv.org/pdf/2502.13497\" target=\"_blank\" rel=\"noopener noreferrer\">diverse cultural knowledge</a> and datasets. This research helps to ensure that Google models can connect with users globally in responsible and culturally-aware ways.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Interactive interfaces with generative UI\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Introducing interactive interfaces with generative UI</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">In a world where users expect more engaging and visual experiences, we introduced a novel implementation of <a href=\"https://research.google/blog/generative-ui-a-rich-custom-visual-interactive-user-experience-for-any-prompt/\">generative UI</a> in Gemini 3. This powerful capability enables AI models to dynamically create immersive visual experiences and interactive interfaces, such as web pages, games, tools and apps, in response to a prompt. Our research comes to life in AI Mode on <a href=\"http://blog.google/products/search/gemini-3-search-ai-mode\" target=\"_blank\" rel=\"noopener noreferrer\">Google Search</a>, and in experiments such as dynamic view, in the <a href=\"https://blog.google/products/gemini/gemini-3-gemini-app\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini app</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"GenUI Van Gogh demo\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"JALZmOVlR7s\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=JALZmOVlR7s\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"mr25z\"><i>Example of generative UI capabilities in dynamic view in the Gemini app. This is based on the prompt, “</i>Create a Van Gogh gallery with life context for each piece.<i>” More examples can be found</i> <a href=\"https://generativeui.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>here</i></a><i>.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"Generative UI RNA questions in Search\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/EOY-5-GenUI.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"mr25z\"><i>Example of generative UI capabilities in AI Mode in Google Search. This is based on the prompt, “</i>Show me how rna polymerase works. what are the stages of transcription and how is it different in prokaryotic and eukaryotic cells.<i>”</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Quantum computing\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Quantum computing: The next frontier</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">Our strategic investment in quantum computing is poised to accelerate the next frontier of computing and scientific discovery. In the 1980s, Clarke, Devoret, and Martinis laid the foundations for superconducting qubits, which led to their recognition as <a href=\"https://blog.google/inside-google/company-announcements/googler-michel-devoret-awarded-the-nobel-prize-in-physics/\" target=\"_blank\" rel=\"noopener noreferrer\">2025 Physics Nobel Laureates</a>. The 40-year <a href=\"https://quantumai.google/roadmap\" target=\"_blank\" rel=\"noopener noreferrer\">journey</a> since has yielded the nascent quantum computing industry and led to breakthroughs like our recently announced <a href=\"https://research.google/blog/a-verifiable-quantum-advantage/\">verifiable quantum advantage</a>, published on the cover of <a href=\"https://www.nature.com/articles/s41586-025-09526-6\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature</i></a>. This work describes our “<a href=\"https://blog.google/technology/research/quantum-echoes-willow-verifiable-quantum-advantage/\" target=\"_blank\" rel=\"noopener noreferrer\">Quantum Echoes</a>” algorithm, which runs on our <a href=\"https://research.google/blog/making-quantum-error-correction-work/\">Willow chip</a> 13,000 times faster than the best classical algorithm on one of the world’s fastest supercomputers. It offers a new way to explain interactions between atoms in a molecule observed <a href=\"https://arxiv.org/abs/2510.19550\" target=\"_blank\" rel=\"noopener noreferrer\">using nuclear magnetic resonance spectroscopy</a>. It brings us closer to <a href=\"https://blog.google/technology/research/useful-quantum-computing-applications/\" target=\"_blank\" rel=\"noopener noreferrer\">real-world applications</a> of quantum computing, such as advancing drug design and helping to make fusion energy a reality.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"Circuits to Chandeliers: A Quantum History\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"lpzR_rPbrac\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=lpzR_rPbrac\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Accelerating scientific discovery\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Accelerating scientific discovery</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">AI-powered models and platforms are fundamentally changing <i>how</i> science is conducted. We released <a href=\"https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/\">AI co-scientist</a>, a collaboration across Google Research, Cloud AI and Google DeepMind. This multi-agent <a href=\"https://arxiv.org/abs/2502.18864\" target=\"_blank\" rel=\"noopener noreferrer\">AI system</a> helps scientists generate novel hypotheses. We also shared our <a href=\"https://research.google/blog/accelerating-scientific-discovery-with-ai-powered-empirical-software/\">AI-powered empirical software</a> system, a Gemini-backed coding agent to help scientists write expert-level empirical software to evaluate and iterate on hypotheses. These tools accelerate the very process of making scientific discoveries. They open the door to a future where every scientist in a lab has a team of AI assistants simultaneously investigating thousands of potential solutions to the scientific challenges that motivate their research. Already at Stanford, our AI co-scientist has helped <a href=\"https://advanced.onlinelibrary.wiley.com/doi/pdf/10.1002/advs.202508751\" target=\"_blank\" rel=\"noopener noreferrer\">identify drugs</a> that could be repurposed to treat liver fibrosis. At Imperial College London, researchers working on antimicrobial resistance <a href=\"https://www.imperial.ac.uk/news/261293/googles-ai-co-scientist-could-enhance-research/\" target=\"_blank\" rel=\"noopener noreferrer\">found</a> that it produced the same hypothesis in days that their team took years to develop.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"An overview of the AI co-scientist\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/EOY-7-AICoScientist.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"mr25z\"><i>An overview of the AI co-scientist. It uses a coalition of specialized agents who iteratively generate, evaluate, and refine hypotheses.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Biology, genomics, and neuroscience\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Advancing science — from biology to genomics to neuroscience</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"xa7ab\">We continue to advance core scientific research. <a href=\"https://research.google/blog/using-ai-to-identify-genetic-variants-in-tumors-with-deepsomatic/\">DeepSomatic</a> and <a href=\"https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/\" target=\"_blank\" rel=\"noopener noreferrer\">C2S-Scale</a> join the AI-powered fight against cancer and are paving the way for brand-new therapies. Published in <a href=\"https://www.nature.com/articles/s41587-025-02839-x\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature Biotechnology</i></a><i>,</i> DeepSomatic is an <a href=\"https://github.com/google/deepsomatic\" target=\"_blank\" rel=\"noopener noreferrer\">open-source</a> tool that builds on <a href=\"https://blog.google/technology/research/ten-years-google-genomics/\" target=\"_blank\" rel=\"noopener noreferrer\">10 years of genomics</a> research at Google and helps scientists and doctors identify genetic variants in cancer cells. Our partners at <a href=\"https://www.childrensmercy.org/childrens-mercy-research-institute/about/\" target=\"_blank\" rel=\"noopener noreferrer\">Children’s Mercy</a> are using it to understand <a href=\"https://www.medrxiv.org/content/10.1101/2024.11.05.24316078v1\" target=\"_blank\" rel=\"noopener noreferrer\">how and why a particular form of cancer</a> affects a patient in order to develop personalized cures. C2S-Scale, which we released in collaboration with Google DeepMind and Yale, is a 27 billion parameter foundation model for single-cell analysis that made headlines for generating a <a href=\"https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/\" target=\"_blank\" rel=\"noopener noreferrer\">novel hypothesis</a> about cancer cellular behavior.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"NotebookLM summaries of our genomics research\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-8-NLMGenomics.width-1250.png\" alt=\"Two Google Research banners featuring microscopic cells and a DNA strand ask questions about linking genetics to health and genome sequencing.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-8-NLMGenomics.width-1250.png\" alt=\"Two Google Research banners featuring microscopic cells and a DNA strand ask questions about linking genetics to health and genome sequencing.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"mr25z\"><i>Our public NotebookLMs make our decade of genomics research more accessible and allow people to dive deeper into topics that interest them. Explore \"</i><a href=\"https://notebooklm.google.com/notebook/31c20c44-8c94-4f81-a2b8-a020a761d122\" target=\"_blank\" rel=\"noopener noreferrer\"><i>How do scientists link genetics to health?</i></a><i>\" and “</i><a href=\"https://notebooklm.google.com/notebook/a09e40ad-d41f-43af-a3ca-5fc82bd459e5\" target=\"_blank\" rel=\"noopener noreferrer\"><i>How can scientists know what's in your genome?</i></a><i>”</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qt6m0\">Turning to neuroscience, we <a href=\"https://www.nature.com/articles/s41586-025-08985-1\" target=\"_blank\" rel=\"noopener noreferrer\">published in <i>Nature</i></a> the first-ever method for using commonly available light microscopes to comprehensively map all the neurons and their connections in a block of brain tissue. Working with the <a href=\"https://ista.ac.at/en/home/\" target=\"_blank\" rel=\"noopener noreferrer\">Institute of Science and Technology Austria</a>, we applied our suite of image analysis and ML tools for <a href=\"https://sites.research.google/neural-mapping/\">connectomics</a>, leveraging over a decade of contributions we’ve made to this scientific field to understand the workings of the brain. We hope the method, called <a href=\"https://research.google/blog/a-new-light-on-neural-connections/\">LICONN</a>, will enable more labs around the world to pursue connectomics studies.</p><p data-block-key=\"bp5mb\">We also <a href=\"https://zapbench-release.storage.googleapis.com/landing.html\" target=\"_blank\" rel=\"noopener noreferrer\">open-sourced</a> the <a href=\"https://research.google/blog/improving-brain-models-with-zapbench/\">Zebrafish Activity Prediction Benchmark</a> (ZAPBench) in collaboration with <a href=\"https://www.janelia.org/\" target=\"_blank\" rel=\"noopener noreferrer\">HHMI Janelia</a> and <a href=\"https://lichtmanlab.fas.harvard.edu/\" target=\"_blank\" rel=\"noopener noreferrer\">Harvard</a>. With recordings of more than 70,000 neurons from the larval zebrafish brain, it will enable scientists to <a href=\"https://arxiv.org/abs/2503.02618\" target=\"_blank\" rel=\"noopener noreferrer\">investigate the relationship</a> between the structural wiring and dynamic neural activity across an entire vertebrate brain for the first time.</p><p data-block-key=\"8r98j\">Plus, we demonstrated how LLMs can help us understand the human brain. In a <a href=\"https://www.nature.com/articles/s41593-022-01026-4\" target=\"_blank\" rel=\"noopener noreferrer\">series</a> of <a href=\"https://www.nature.com/articles/s41467-024-46631-y\" target=\"_blank\" rel=\"noopener noreferrer\">studies</a> conducted over five years with <a href=\"https://hassonlab.princeton.edu/\" target=\"_blank\" rel=\"noopener noreferrer\">Princeton University</a>, <a href=\"https://nyulangone.org/locations/comprehensive-epilepsy-center\" target=\"_blank\" rel=\"noopener noreferrer\">NYU</a>, and <a href=\"https://www.deepcognitionlab.com/\" target=\"_blank\" rel=\"noopener noreferrer\">HUJI</a>, we explored connections in the ways the human brain and deep language models <a href=\"https://research.google/blog/deciphering-language-processing-in-the-human-brain-through-llm-representations/\">process natural language</a>. We discovered <a href=\"https://www.nature.com/articles/s41562-025-02105-9\" target=\"_blank\" rel=\"noopener noreferrer\">remarkable alignment</a> between the neural activity in the speech and language areas of the human brain and the speech and language embeddings of a Transformer-based speech-to-text model, and showed how the <a href=\"https://www.nature.com/articles/s41467-025-65518-0.epdf?sharing_token=XqLw7uzRcb-8uuak6fH8BdRgN0jAjWel9jnR3ZoTv0PSBTWDWtB7_YH7achJWXIc1XVbsxtqM_vvMoZPLZuQIoT45xjjIkxPMOnKorePnnoS9QL1SRZ58ZI65TPosI2w5PF8h5ZazOVlXhxgrnGSxw1GguylV5BBtKxLUyCAfLM%3D\" target=\"_blank\" rel=\"noopener noreferrer\">temporal structure</a> of language processing in the brain corresponds to the layered hierarchy of deep language models. Our research indicates that language representation in deep learning models could offer a novel framework for understanding the brain’s neural code; it also paves the way for innovative approaches to creating artificial neural networks with better information processing capabilities.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Planetary intelligence and crisis resilience\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Enabling planetary intelligence and crisis resilience</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\"><a href=\"https://research.google/blog/google-earth-ai-unlocking-geospatial-insights-with-foundation-models-and-cross-modal-reasoning/\">Earth AI</a> is Google’s family of geospatial AI models and reasoning agents that provides users with actionable insights, grounded in real-world understanding. Developed in collaboration with teams across Google, it builds on our years of modeling the world, paired with Gemini’s advanced reasoning, to offer an unprecedented level of understanding about our planet. It <a href=\"https://research.google/blog/google-earth-ai-unlocking-geospatial-insights-with-foundation-models-and-cross-modal-reasoning/\">brings together</a> many of Google’s geospatial models and technologies such as <a href=\"https://research.google/blog/google-earth-ai-unlocking-geospatial-insights-with-foundation-models-and-cross-modal-reasoning/\">remote sensing imagery</a>, <a href=\"https://research.google/blog/fast-accurate-climate-modeling-with-neuralgcm/\">weather</a>, <a href=\"https://blog.google/products/maps/google-maps-apis-environment-sustainability/\" target=\"_blank\" rel=\"noopener noreferrer\">air quality</a>, <a href=\"https://sites.research.google/gr/floodforecasting/\">floods</a>, <a href=\"https://research.google/blog/insights-into-population-dynamics-a-foundation-model-for-geospatial-inference/\">population dynamics</a>, <a href=\"https://deepmind.google/discover/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/\" target=\"_blank\" rel=\"noopener noreferrer\">AlphaEarth Foundations</a>, <a href=\"https://research.google/blog/introducing-mobility-ai-advancing-urban-transportation/\">mobility</a>, <a href=\"https://blog.google/technology/research/open-buildings-ai-powered-maps-for-a-changing-world/\" target=\"_blank\" rel=\"noopener noreferrer\">maps</a> and more. Thanks to Gemini’s reasoning power, Earth AI can synthesize vast datasets about the planet to generate insights in minutes that would previously take years of research. Earth AI offerings are available in <a href=\"https://mapsplatform.google.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Maps Platform</a>, <a href=\"https://blog.google/technology/research/new-updates-and-more-access-to-google-earth-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Earth and to Trusted Testers via Google Cloud</a>, and are already being used by partners, helping cities, enterprises and nonprofits with critical tasks from <a href=\"https://www.youtube.com/watch?v=ZxmB8Z5i1Ls&amp;list=PL95lT3XlM14ROFtYnlBDYZbKipH3JLaC7&amp;index=3\" target=\"_blank\" rel=\"noopener noreferrer\">urban planning</a> to <a href=\"https://www.youtube.com/watch?v=8-macH8ozr4&amp;list=PL95lT3XlM14ROFtYnlBDYZbKipH3JLaC7&amp;index=5\" target=\"_blank\" rel=\"noopener noreferrer\">disaster response</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"Google Earth AI\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"UZ4RaLGDXI4\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=UZ4RaLGDXI4\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qt6m0\">We’ve also made significant strides with the climate models that feed our AI capabilities for understanding the Earth, helping communities to prepare for and respond to severe weather and natural disasters. This year, in collaboration with the <a href=\"https://www.earthfirealliance.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Earth Fire Alliance</a>, the <a href=\"https://www.moore.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Moore Foundation</a> and <a href=\"https://www.muonspace.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Muon Space</a>, we launched the first satellite in the <a href=\"https://sites.research.google/gr/wildfires/firesat/\">FireSat</a> constellation. Named as one of <a href=\"https://time.com/collections/best-inventions-2025/7318230/firesat/\" target=\"_blank\" rel=\"noopener noreferrer\">TIME magazine’s best inventions</a> of 2025, FireSat uses AI to provide critical near–real-time insights for first responders. It has already <a href=\"https://blog.google/technology/research/first-firesat-images/\" target=\"_blank\" rel=\"noopener noreferrer\">detected</a> small wildfires not caught by other space-based systems, and when fully operational with over 50 satellites, it will be able to detect a classroom-sized wildfire anywhere on Earth.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"FireSat\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-10-FireSat.width-1250.png\" alt=\"A satellite map of the Oregon-California border featuring a thermal infrared inset pinpointing a small fire near Medford.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-10-FireSat.width-1250.png\" alt=\"A satellite map of the Oregon-California border featuring a thermal infrared inset pinpointing a small fire near Medford.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"mr25z\"><i>FireSat is equipped with a custom mid-wave infrared (MWIR) sensor that detected a small, relatively cool roadside fire near Medford, Oregon that was not detected by other space-based systems. It is seen here overlaid on a Google Earth basemap. Credit: Muon Space and Earth Fire Alliance.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qt6m0\">We also <a href=\"https://blog.google/technology/research/new-updates-and-more-access-to-google-earth-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">expanded</a> our flood forecasting models to cover over 2 billion people in 150 countries for the most significant riverine flood events, helping communities stay safe and informed. We partnered with our colleagues at Google DeepMind to debut an experimental model for <a href=\"https://deepmind.google/blog/how-were-supporting-better-tropical-cyclone-prediction-with-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">cyclone predictions</a> using stochastic neural networks that's helping weather agencies predict a cyclone’s path up to 15 days in advance. Moreover, we collaborated with Google DeepMind to launch <a href=\"https://deepmind.google/science/weathernext/\" target=\"_blank\" rel=\"noopener noreferrer\">WeatherNext 2</a>, which delivers our most accurate, mid-range AI weather forecasts to date. It’s now available to users of Search, Gemini and Pixel Weather as well as to developers on Google Maps and Google Cloud.</p><p data-block-key=\"d5150\">At the start of the year, we expanded <a href=\"https://blog.google/intl/en-africa/products/explore-get-answers/nowcasting-on-search-is-bringing-ai-powered-weather-forecasts-to-users-across-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">Nowcasting on Search to Africa</a>, bringing highly precise, short-term precipitation forecasts to users across the continent for the first time. We have since made this available for users worldwide. Powered by our <a href=\"https://arxiv.org/abs/2510.13050\" target=\"_blank\" rel=\"noopener noreferrer\">MetNet</a> model, it represents the first AI weather model on Search to operate at a global scale. In India, the University of Chicago and the Indian Ministry of Agriculture and Farmers’ Welfare used Google’s <a href=\"https://blog.google/technology/research/indian-farmers-monsoon-prediction/\" target=\"_blank\" rel=\"noopener noreferrer\">NeuralGCM model</a> to send longer-range monsoon forecasts to 38 million farmers, helping them make critical decisions about what to plant and when.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Health AI\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Advancing Health AI</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">As we make scientific breakthroughs with the potential to significantly reform healthcare, we’re working with partners and healthcare professionals to bring new capabilities responsibly to people around the world. <a href=\"https://research.google/blog/amie-a-research-ai-system-for-diagnostic-medical-reasoning-and-conversations/\">AMIE</a> is our conversational medical agent developed together with Google DeepMind and published in <a href=\"https://www.nature.com/articles/s41586-025-08866-7\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature</i></a>. It can now reason through <a href=\"https://research.google/blog/amie-gains-vision-a-research-ai-agent-for-multi-modal-diagnostic-dialogue/\">multimodal</a> evidence and support longitudinal <a href=\"https://research.google/blog/from-diagnosis-to-treatment-advancing-amie-for-longitudinal-disease-management/\">disease management</a> as well as or better than primary care physicians under simulated settings with professional patient actors. We’re exploring how this research could enable a physician-centered model with <a href=\"https://research.google/blog/enabling-physician-centered-oversight-for-amie/\">asynchronous oversight</a> of AMIE. We also launched <a href=\"https://community.fitbit.com/t5/The-Pulse-Fitbit-Community-Blog/Introducing-the-new-Plan-for-Care-Fitbit-Lab/ba-p/5796289\" target=\"_blank\" rel=\"noopener noreferrer\">Plan for Care Lab</a>, Fitbit’s latest experimental capability, to a select number of opt-in users. It’s designed to help users access personalized support when assessing symptoms at home and preparing for an upcoming doctor’s visit. In addition, <a href=\"https://research.google/blog/medgemma-our-most-capable-open-models-for-health-ai-development/\">MedGemma</a>, Google's most capable open model for multimodal medical comprehension, is available as part of our <a href=\"https://developers.google.com/health-ai-developer-foundations\" target=\"_blank\" rel=\"noopener noreferrer\">Health AI Developer Foundations</a> (HAI-DEF). It can support tasks such as classification, report generation, or interpreting complex electronic health records, making it useful for medical research and product development. Since launch, MedGemma and HAI-DEF have &gt;2M downloads. Plus, our <a href=\"https://developers.google.com/open-health-stack\" target=\"_blank\" rel=\"noopener noreferrer\">Open Health Stack</a> was recognized at the <a href=\"https://www.weforum.org/stories/2025/01/social-innovation-has-moved-from-the-margins-to-the-mainstream/\" target=\"_blank\" rel=\"noopener noreferrer\">World Economic Forum</a> for helping to address inequities in health access. It provides the building blocks for developers to create next-gen, data-driven healthcare apps for use in low-resource settings.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Learning and education\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Advancing learning and education</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">Gemini is now infused with <a href=\"https://blog.google/outreach-initiatives/education/google-learnlm-gemini-generative-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">LearnLM</a>, Google’s family of models fine-tuned for learning, <a href=\"https://storage.googleapis.com/deepmind-media/LearnLM/LearnLM_paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">announced</a> last year. We launched <a href=\"https://research.google/blog/learn-your-way-reimagining-textbooks-with-generative-ai/\">Learn Your Way</a> on <a href=\"https://learnyourway.withgoogle.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Labs</a>, powered by LearnLM’s foundational capabilities. It explores the future of textbooks by generating multiple engaging representations of the source material. It transforms static textbooks into active learning experiences that are tailored for every student, with interactive quizzes that enable real-time assessment, feedback, and content personalization. In our<a href=\"https://arxiv.org/abs/2509.13348\" target=\"_blank\" rel=\"noopener noreferrer\"> efficacy study</a>, students using it scored 11 percentage points higher on retention tests. We also piloted our LearnLM model for <a href=\"https://blog.google/intl/en-africa/company-news/outreach-and-initiatives/5-ways-were-bringing-ai-innovations-to-people-across-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">answer assessment</a> with thousands of high school students in Ghana. Plus, we explored the intersection of education and health through a learner-centric approach quantifying the benefits of LearnLM in <a href=\"https://research.google/blog/how-googles-ai-can-help-transform-health-professions-education/\">medical education</a> settings.</p><p data-block-key=\"bo45o\">This research brings us closer to realizing a future where AI makes learning more effective for everyone. In collaboration with teams across Google, we published “<a href=\"https://services.google.com/fh/files/misc/future_of_learning.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">AI and the Future of Learning</a>”, sharing our approach, grounded in learning science, to responsibly enable AI for learning. We’re creating personalized teaching experiences, empowering educators, and working to address challenges such as critical thinking and equal access.</p><p data-block-key=\"chemf\">In parallel, our AI Literacy efforts aim to inspire the next generation of innovators. <a href=\"https://research.google/ai-quests/intl/en_gb\">AI Quests</a>, <a href=\"https://blog.google/outreach-initiatives/education/ai-quests/\" target=\"_blank\" rel=\"noopener noreferrer\">launched</a> with the <a href=\"https://acceleratelearning.stanford.edu/\" target=\"_blank\" rel=\"noopener noreferrer\">Stanford Accelerator for Learning</a>, allows students to step into the shoes of Google researchers and use AI to solve challenges like flood forecasting and detecting eye disease. During <a href=\"https://blog.google/outreach-initiatives/education/next-gen-computer-science-innovators/\" target=\"_blank\" rel=\"noopener noreferrer\">Computer Science Education Week</a>, hundreds of Googler volunteers brought these quests to classrooms around the world.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/EOY-11-LearnYourWay.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"mr25z\"><i>Learn Your Way explores how GenAI can transform educational materials into more effective, engaging, learner-driven experiences. It generates multiple representations of the source material, tailored for each student.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"ML foundations and algorithmic research\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Advancing ML foundations and algorithmic research</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">Our broad foundational ML and algorithmic research is the bedrock for groundbreaking advances across domains. This work provides the essential frameworks that power products and services, and underpins the development of next-generation models and intelligent systems. We improved voice search, for example, with our new <a href=\"https://research.google/blog/speech-to-retrieval-s2r-a-new-approach-to-voice-search/\">Speech-to-Retrieval</a> engine, which directly interprets and retrieves information from a spoken query without having to convert it first to text. And our state-of-the-art <a href=\"https://research.google/blog/rich-human-feedback-for-text-to-image-generation/\">predictive modeling</a> of rich human feedback improved text-to-image generation quality in products, including <a href=\"https://developers.googleblog.com/en/imagen-3-arrives-in-the-gemini-api/\" target=\"_blank\" rel=\"noopener noreferrer\">Imagen3</a>, <a href=\"https://blog.google/products/ads-commerce/new-creative-updates-advertisers-generate-lifestyle/\" target=\"_blank\" rel=\"noopener noreferrer\">creative generation</a> and <a href=\"https://blog.google/products/ads-commerce/google-ai-ads-creative/\" target=\"_blank\" rel=\"noopener noreferrer\">editing</a> in Google Ads, and <a href=\"https://blog.google/products/shopping/google-shopping-ai-mode-virtual-try-on-update/\" target=\"_blank\" rel=\"noopener noreferrer\">virtual try on</a> for shopping. We also extended this research to improve video generation quality in the <a href=\"https://blog.google/products/google-cloud/sphere-wizard-of-oz/\" target=\"_blank\" rel=\"noopener noreferrer\">Wizard of Oz film launch</a> at <a href=\"https://www.thesphere.com/shows/wizard-of-oz-experience\" target=\"_blank\" rel=\"noopener noreferrer\">Sphere</a> in Las Vegas.</p><p data-block-key=\"f7the\">The impact of our algorithmic research extends well beyond Google products. Our <a href=\"https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/\">TimesFM</a> model, which <a href=\"https://cloud.google.com/blog/products/data-analytics/timesfm-models-in-bigquery-and-alloydb\" target=\"_blank\" rel=\"noopener noreferrer\">helps businesses</a> with time-series forecasting, now has hundreds of millions of queries per month in <a href=\"https://cloud.google.com/bigquery\" target=\"_blank\" rel=\"noopener noreferrer\">BigQuery</a> and <a href=\"https://cloud.google.com/products/alloydb\" target=\"_blank\" rel=\"noopener noreferrer\">AlloyDB</a>. We introduced a novel approach using <a href=\"https://research.google/blog/time-series-foundation-models-can-be-few-shot-learners/\">in-context fine-tuning</a>, which teaches the model how to learn from multiple examples at inference time to further enhance its performance. Our <a href=\"https://research.google/blog/introducing-mobility-ai-advancing-urban-transportation/\">Mobility AI</a> model leverages our two decades of innovation in maps and transportation to provide transportation agencies with powerful tools for data-driven policymaking and traffic management. It can understand traffic and parking patterns, simulate systems to allow engineers to test different scenarios, and identify effective solutions for transportation networks. This complements our consumer-facing breakthroughs in Google Maps and Search, such as specialized models for <a href=\"https://research.google/blog/how-we-created-hov-specific-etas-in-google-maps/\">calculating ETAs</a> and <a href=\"https://research.google/blog/optimizing-llm-based-trip-planning/\">optimizing trip planning</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-12-MobilityAI.width-1250.png\" alt=\"A &quot;Traffic Simulation API&quot; dashboard displays a map of Seattle with purple-shaded road segments analyzing the impact of lane closures.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-12-MobilityAI.width-1250.png\" alt=\"A &quot;Traffic Simulation API&quot; dashboard displays a map of Seattle with purple-shaded road segments analyzing the impact of lane closures.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"mr25z\"><i>Our Mobility AI Traffic Simulation API is built for modeling complex, city-scale traffic scenarios. The tool provides high-fidelity simulations of road closures, helping to de-risk major infrastructure investments and validate emergency response plans. It was launched in Seattle, Denver, Boston, Philadelphia and Orlando. The above image shows an example simulation in Seattle. Watch the full demo</i> <a href=\"https://www.youtube.com/watch?v=Pv91I43VpOQ\" target=\"_blank\" rel=\"noopener noreferrer\"><i>here</i></a><i>.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qt6m0\">Additionally, we’ve explored a range of topics in economics and computation from pricing dynamics in <a href=\"https://arxiv.org/abs/2502.20346\" target=\"_blank\" rel=\"noopener noreferrer\">modular marketplaces</a> and in <a href=\"https://arxiv.org/abs/2503.10910\" target=\"_blank\" rel=\"noopener noreferrer\">procurement auctions</a>, to <a href=\"https://dl.acm.org/doi/10.1145/3736252.3742578\" target=\"_blank\" rel=\"noopener noreferrer\">data-driven mechanism design</a> and <a href=\"https://dl.acm.org/doi/10.1145/3696410.3714881\" target=\"_blank\" rel=\"noopener noreferrer\">various</a> <a href=\"https://dl.acm.org/doi/10.1145/3736252.3742545\" target=\"_blank\" rel=\"noopener noreferrer\">approaches</a> to optimize ad auctions. We also studied swap regret and <a href=\"https://arxiv.org/abs/2502.20229\" target=\"_blank\" rel=\"noopener noreferrer\">correlated equilibria in games</a>.</p><p data-block-key=\"6ninf\">As AI becomes increasingly integrated into our daily lives, building it with privacy at its core is critical for users and industries. To this end, we’ve developed and published <a href=\"https://research.google/blog/fine-tuning-llms-with-user-level-differential-privacy/\">novel</a> <a href=\"https://research.google/blog/beyond-billion-parameter-burdens-unlocking-data-synthesis-with-a-conditional-generator/\">algorithms</a> for <a href=\"https://research.google/blog/synthetic-and-federated-privacy-preserving-domain-adaptation-with-llms-for-mobile-applications/\">private</a> learning and <a href=\"https://research.google/blog/securing-private-data-at-scale-with-differentially-private-partition-selection/\">private</a> <a href=\"https://research.google/blog/toward-provably-private-insights-into-ai-use/\">analytics</a>, and open sourced robust software tools to enable <a href=\"https://research.google/blog/discovering-new-words-with-confidential-federated-analytics/\">external verifiability</a>. For example, we introduced <a href=\"https://research.google/blog/parfait-enabling-private-ai-with-research-tools/\">Parfait</a>, a new GitHub organization for businesses and open-source projects. It has supported Google deployments of federated learning and analytics from <a href=\"https://support.google.com/gboard/answer/12373137?hl=en#zippy=%2Cfederated-learning\" target=\"_blank\" rel=\"noopener noreferrer\">Gboard</a> to <a href=\"https://arxiv.org/abs/2412.07962\" target=\"_blank\" rel=\"noopener noreferrer\">Google Maps</a>. We also announced <a href=\"https://research.google/blog/differentially-private-machine-learning-at-scale-with-jax-privacy/\">Jax Privacy 1.0</a>, a library for ML with differential privacy, which we used to train <a href=\"https://research.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/\">VaultGemma</a>, the largest and most capable open model trained from scratch with differential privacy, with weights available on <a href=\"https://huggingface.co/google/vaultgemma-1b\" target=\"_blank\" rel=\"noopener noreferrer\">Hugging Face</a> and <a href=\"https://www.kaggle.com/models/google/vaultgemma\" target=\"_blank\" rel=\"noopener noreferrer\">Kaggle</a>. By leveling up our privacy capabilities, we offer much stronger protections to businesses and users</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Novel architectures\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Introducing novel architectures</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">Our foundational ML research introduces advanced approaches to enable new opportunities. <a href=\"https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/\">Nested Learning</a> is a new ML paradigm that represents a leap forward in our understanding of deep learning. It treats model architecture and optimization as a single system that contains several, smaller, nested optimization problems. By unifying these elements, it solves the problem of catastrophic forgetting, when LLMs become forgetful and less capable at old tasks after learning new tasks. This research could help us build the next generation of more capable, self-improving AI. Meanwhile, our <a href=\"https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/\">Titans architecture and the MIRAS framework</a> mark a significant advancement in sequence modelling. They allow AI models to work much faster and handle massive contexts by employing deep neural networks that learn to memorize as data comes in, improving AI’s long-term memory.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"MIRAS Framework\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-13-MIRAS.width-800.gif\" alt=\"A diagram titled &quot;MIRAS Framework&quot; illustrates how input tokens are processed through associative memory, featuring a sliding focus on individual data blocks.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EOY-13-MIRAS.width-800.gif\" alt=\"A diagram titled &quot;MIRAS Framework&quot; illustrates how input tokens are processed through associative memory, featuring a sliding focus on individual data blocks.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"mr25z\"><i>In the MIRAS framework, we aim to learn an associative memory, mapping between keys and values. For each token, the memory module internally optimizes its inner attentional bias while using its retention gate to make sure that it does not deviate from its past state.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qt6m0\">We also introduced <a href=\"https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/\">MUVERA</a>, a novel retrieval algorithm that reduces complex multi-vector retrieval back to single-vector maximum inner product search, achieving state-of-the-art performance with significantly improved efficiency. It creates new possibilities for information retrieval for use in applications such as recommendation systems and natural language processing. And our progress on <a href=\"https://research.google/blog/graph-foundation-models-for-relational-data/\">graph foundational models</a> pushes the frontiers of graph learning. While most graph neural networks are fixed to a specific graph on which the model has been trained, we developed graph foundational models capable of generalizing to arbitrary tables, features and tasks. This opens up new avenues for model reuse.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Collaboration and open research\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Collaborating with the research ecosystem</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">We partner with the academic community, industry leaders, governments and scientific institutes around the world. We also continue to engage the ecosystem through our Research@ events from <a href=\"https://research.google/blog/accelerating-the-magic-cycle-of-research-breakthroughs-and-real-world-applications/\">Mountain View</a> to <a href=\"https://www.youtube.com/watch?v=sikTOH-0J_c\" target=\"_blank\" rel=\"noopener noreferrer\">Tokyo</a>, <a href=\"https://blog.google/intl/en-au/company-news/technology/research-sydney-charting-new-ai-frontiers-alongside-the-research-ecosystem-in-australia/\" target=\"_blank\" rel=\"noopener noreferrer\">Sydney</a> and <a href=\"https://blog.google/technology/research/ai-collaboration-poland-2025/\" target=\"_blank\" rel=\"noopener noreferrer\">Poland</a>, and we support hundreds of PhD students in Google’s <a href=\"https://blog.google/outreach-initiatives/google-org/phd-fellowship-program-2025/\" target=\"_blank\" rel=\"noopener noreferrer\">Fellowship Program</a>.</p><p data-block-key=\"abe8b\">As a global team, we continue to expand our footprint beyond our major hubs. Having solidified our research <a href=\"https://blog.google/intl/en-africa/company-news/outreach-and-initiatives/supporting-the-future-of-ai-research-in-africa-and-globally/\" target=\"_blank\" rel=\"noopener noreferrer\">investment</a> and <a href=\"https://blog.google/intl/en-africa/company-news/outreach-and-initiatives/5-ways-were-bringing-ai-innovations-to-people-across-africa/#:~:text=Jul%2024%2C%202025,Mail\" target=\"_blank\" rel=\"noopener noreferrer\">innovation</a> in Africa (Accra and Nairobi) and our presence in Australia, we are now preparing to inaugurate a new Google Research hub in Singapore in 2026.</p><p data-block-key=\"j9dl\">We share our work through publications, conferences, academic talks, benchmarks, datasets and open-source releases. We’ve <a href=\"https://research.google/conferences-and-events/?&amp;year=2025\">sponsored and hosted workshops at conferences</a>, most recently at <a href=\"https://research.google/conferences-and-events/google-at-neurips-2025/\">NeurIPS</a>. We recently introduced an <a href=\"https://research.google/blog/gemini-provides-automated-feedback-for-theoretical-computer-scientists-at-stoc-2026/\">experimental program</a> that provided automated feedback to scientists before they submit their conference papers for peer review, helping them to rigorously verify their work and accelerate research workflows. Plus, we launched <a href=\"https://notebooklm.google.com/notebook/24d50377-8c14-4851-bcc2-b2d67b039041\" target=\"_blank\" rel=\"noopener noreferrer\">Google Research Featured Notebooks</a> in collaboration with NotebookLM, to make research more accessible to a broader community.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"AI as an amplifier of human ingenuity\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">AI as an amplifier of human ingenuity</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"qt6m0\">This is a golden age for research. Never before have technical breakthroughs and scientific progress so quickly materialized into impactful, real-world solutions, which, in turn, bring to the fore new data and questions that inspire new avenues of foundational research. This <a href=\"https://research.google/blog/accelerating-the-magic-cycle-of-research-breakthroughs-and-real-world-applications/\">magic cycle</a> is accelerating significantly, propelled by more powerful models, new agentic tools that support scientific discovery, and open platforms and tools.</p><p data-block-key=\"77o7f\">Together with our Google colleagues and partners, we’re advancing research and technologies that aim to be helpful in diverse areas. Our research, grounded in a rigorous dedication to safety and trust, serves to unlock human potential — whether that’s to help a scientist accelerate their research, or a student learn more effectively and master new concepts, or to empower a doctor, developer or teacher.</p><p data-block-key=\"atf0n\">It is truly an exciting time to be in research. We’re able to leverage the full stack of Google AI infrastructure, models, platforms, and world-class talent, and contribute to products used by billions. We will keep building on our legacy, asking the biggest questions of today, and aiming to enable the solutions of tomorrow. We’ll keep advancing AI in a bold and responsible way, for the benefit of society, to help enhance human capacity and make AI an amplifier of human ingenuity.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"Accelerating the Speed and Scope of Discovery\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"VqSrYdDM0Zw\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=VqSrYdDM0Zw\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Acknowledgements\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n            <div class=\" component-intro__description --has-heading\">\n                <p data-block-key=\"2nd4e\"><i>With thanks to everyone in Google Research, and many collaborators, who have contributed to this blog and the work represented here.</i></p>\n            </div>\n        \n    </div>\n\n\n\n    \n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Gemini provides automated feedback for theoretical computer scientists at STOC 2026",
      "link": "https://research.google/blog/gemini-provides-automated-feedback-for-theoretical-computer-scientists-at-stoc-2026/",
      "pubDate": "Sun, 14 Dec 2025 16:00:00 GMT",
      "isoDate": "2025-12-14T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Gemini provides automated feedback for theoretical computer scientists at STOC 2026",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/STOC-2-Quotes2.width-1250.png",
          "alt": "Three testimonials from university professors praising the tool for identifying significant errors and bugs in their research papers.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/STOC-1-Quotes2.width-1250.png",
          "alt": "A testimonial graphic from Professor Shuchi Chawla praising the tool for providing valuable feedback that exceeded expectations.",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"o2vbn\">The pursuit of truth in theoretical computer science and mathematics relies on the highest standards of proof, rigor, and clarity. While peer review is the crucial final check, the process of drafting and refining complex theoretical work often takes months, with simple errors, inconsistent variables, or subtle logical gaps frequently slowing down the entire research pipeline. But could a highly specialized AI tool act as a fast, rigorous collaborator, helping authors pre-vet their work before it ever reaches human reviewers?</p><p data-block-key=\"bjaqs\">To test this potential, we created an <a href=\"https://acm-stoc.org/stoc2026/stoc2026-LLM_feedback.html\" target=\"_blank\" rel=\"noopener noreferrer\">experimental program</a> for the <a href=\"https://acm-stoc.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Annual ACM Symposium on Theory of Computing</a> (STOC 2026) — one of the most prestigious venues in theoretical computer science. This program offered authors automated, pre-submission feedback generated by a specialized Gemini AI tool. Our objective was to provide constructive suggestions and identify potential technical issues within 24 hours of submission, helping authors polish their final drafts before the submission deadline.</p><p data-block-key=\"3naog\">The responses were very positive: the tool successfully identified a variety of issues, including calculation and logic errors. Here we report how we developed the tool and the results of its use.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Optimized for mathematical rigor\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Optimized for mathematical rigor</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o2vbn\">The feedback tool leveraged inference scaling methods in an advanced version of <a href=\"https://blog.google/products/gemini/gemini-2-5-deep-think/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini 2.5 Deep Think</a>. This setup enables the method to simultaneously explore and combine multiple possible solutions before giving a final answer, rather than pursuing a single, linear chain of thought. By combining different reasoning and evaluation traces, the method reduces inherent hallucinations and focuses on the most salient issues.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Feedback format\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Feedback format</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o2vbn\">Authors received structured feedback divided into key sections: a summary of the paper's contributions, a list of potential mistakes and improvements (often analyzing specific lemmas or theorems), and a list of minor corrections and typos. See some <a href=\"https://www.cs.cmu.edu/~dwoodruf/stoc/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">feedback examples</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Impact and technical depth\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Impact and technical depth</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o2vbn\">The tool successfully identified a wide range of issues, from inconsistent variable names to complex problems like calculation errors, incorrect application of inequalities, and logical gaps in proofs. As one author noted, the tool found \"a critical bug... that made our proof entirely incorrect,\" further adding that it was an \"embarrassingly simple bug that evaded us for months.\"</p><p data-block-key=\"eo65d\">Over 120 participants responded to our post-experiment survey and gave us consent, and the responses were very positive, with individuals citing the model’s success at finding critical errors and its ability to return insightful commentary. In summary:</p><ul><li data-block-key=\"qpe5\">&gt;80% of submitted papers at the time our experiment ended had opted-in for our AI review</li><li data-block-key=\"ev3be\">97% found the feedback helpful</li><li data-block-key=\"dkstm\">97% would use this tool again for future submissions</li><li data-block-key=\"a51g7\">81% found the model improved clarity or readability of the paper</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/STOC-2-Quotes2.width-1250.png\" alt=\"Three testimonials from university professors praising the tool for identifying significant errors and bugs in their research papers.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/STOC-2-Quotes2.width-1250.png\" alt=\"Three testimonials from university professors praising the tool for identifying significant errors and bugs in their research papers.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"The user experience\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The user experience</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o2vbn\">Beyond technical accuracy, authors valued the speed and neutrality of the AI review. Participants noted receiving feedback in just two days. Others praised the \"neutral tone and rigor\" of the output, finding it a useful complement to human readers.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/STOC-1-Quotes2.width-1250.png\" alt=\"A testimonial graphic from Professor Shuchi Chawla praising the tool for providing valuable feedback that exceeded expectations.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/STOC-1-Quotes2.width-1250.png\" alt=\"A testimonial graphic from Professor Shuchi Chawla praising the tool for providing valuable feedback that exceeded expectations.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Interpreting the output\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Interpreting the output</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o2vbn\">Because participants are experts in their respective fields, they were able to readily distinguish helpful insights from occasional \"hallucinations\". While the model sometimes struggled — particularly with parsing complex notation or interpreting figures — authors weren't dismissive of the LLM's output. Rather, they carefully filtered out the noise and extracted the important and correct parts of the output, and then used the feedback as a starting point for verification. This outcome clearly demonstrates the potential for AI to serve as a collaborative partner, augmenting the research workflow by helping human experts to make informed decisions based on the model's rigorous outputs.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Educational impact and future outlook\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Educational impact and future outlook</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o2vbn\">The research community surveyed in this experiment saw significant potential for this tool in training the next generation. 75% of surveyed authors believed the tool has educational value for students by offering immediate feedback on mathematical rigor and presentation clarity.</p><p data-block-key=\"629om\">This pilot demonstrated the potential for specialized AI tools to serve as collaborative partners in fundamental areas, establishing a target for potential future research initiatives. Our overall goal is not to replace the critical peer review process, but rather to augment and enhance it. Reflecting this, 88% of participants expressed strong interest in having continuous access to such a tool throughout their entire research process.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Acknowledgements\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o2vbn\"><i>Vincent Cohen-Addad, Rajesh Jayaram, Jon Schneider, and David Woodruff co-led this project</i><footnote id=\"8746db74-de5b-4496-adf4-df5aa8c2d3ab\">[8746db]</footnote><i>, with key contributions by Lalit Jain, Jieming Mao, and Vahab Mirrokni. We also thank the STOC 2026 PC chair Artur Czumaj and the many other authors who participated in this experiment and provided their valuable feedback, helpful suggestions, and discussions, including Mohammad Taghi Hajiaghayi, Ravi Kumar, Yossi Matias, and Sergei Vassilvitskii. Finally, this work builds on the efforts of the Deep Think team: Garrett Bingham, Irene Cai, Heng-Tze Cheng, Yong Cheng, Kristen Chiafullo, Vincent Cohen-Addad, Paul Covington, Golnaz Ghiasi, Chenjie Gu, Huan Gui, Ana Hosseini, Dawsen Hwang, Lalit Jain, Vihan Jain, Ragha Kotikalapudi, Chenkai Kuang, Chenkai Kuang, Maciej Kula, Nate Kushman, Jane Labanowski, Quoc Le, Jonathan Lee, Zhaoqi Leng, Steve Li, YaGuang Li, Hanzhao (Maggie) Lin, Evan Liu, Yuan Liu, Thang Luong, Jieming Mao, Vahab Mirrokni, Pol Moreno, Nigamaa Nayakanti, Aroonalok Pyne, Shubha Raghvendra, Sashank Reddi, Nikunj Saunshi, Siamak Shakeri, Archit Sharma, Xinying Song, Qijun Tan, Yi Tay, Trieu Trinh, Theophane Weber, Winnie Xu, Zicheng Xu, Shunyu Yao, Lijun Yu, Hao Zhou, Honglei Zhuang, and Song Zuo.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Spotlight on innovation: Google-sponsored Data Science for Health Ideathon across Africa",
      "link": "https://research.google/blog/spotlight-on-innovation-google-sponsored-data-science-for-health-ideathon-across-africa/",
      "pubDate": "Thu, 11 Dec 2025 16:00:00 GMT",
      "isoDate": "2025-12-11T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Spotlight on innovation: Google-sponsored Data Science for Health Ideathon across Africa",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Ideathon1_Group.width-1250.png",
          "alt": "Ideathon1_Group",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"uoa5v\">Generative AI is rapidly changing healthcare, opening up opportunities to better address real-world health challenges. Across the African continent, there is broad interest in tackling such challenges, ranging from cervical cancer screening to maternal health support.</p><p data-block-key=\"93mda\">With the above in mind, in collaboration with three pan-African data science and machine learning communities — <a href=\"https://www.linkedin.com/company/sisonkebiotik/posts/?feedView=all\" target=\"_blank\" rel=\"noopener noreferrer\">SisonkeBiotik</a>, <a href=\"https://ro-ya-cv4africa.github.io/homepage/\" target=\"_blank\" rel=\"noopener noreferrer\">Ro’ya</a>, and <a href=\"https://dsi-africa.org/#carouselWithCaptions\" target=\"_blank\" rel=\"noopener noreferrer\">DS-I Africa</a> — we hosted an Africa-wide Data Science for Health Ideathon, focused on leveraging <a href=\"https://research.google/blog/medgemma-our-most-capable-open-models-for-health-ai-development/\">Google’s open Health AI models</a> to tackle real-world health challenges.</p><p data-block-key=\"ai2pi\">From over 30 ambitious and technically rich submissions, six finalist teams were selected for their bold ideas, strong foundations, and potential for meaningful impact across African health systems. These teams received mentorship from global experts and technical resources provided by Google Research and Google DeepMind.</p><p data-block-key=\"8oqa5\">This reflects broad interest across the African continent on understanding how AI can be used to create local solutions to local priorities around health, agriculture and climate. This is part of Google’s broader <a href=\"https://blog.google/intl/en-africa/company-news/outreach-and-initiatives/supporting-the-future-of-ai-research-in-africa-and-globally/\" target=\"_blank\" rel=\"noopener noreferrer\">initiatives</a> on AI for Africa including in areas of <a href=\"https://research.google/blog/afrimed-qa-benchmarking-large-language-models-for-global-health/\">health</a>, <a href=\"https://blog.google/intl/en-africa/products/explore-get-answers/bringing-googles-best-ai-tools-to-university-students-across-africa-at-no-cost/\" target=\"_blank\" rel=\"noopener noreferrer\">education</a>, <a href=\"https://blog.google/outreach-initiatives/google-org/ai-collaboratives-wildfires-food-security/\" target=\"_blank\" rel=\"noopener noreferrer\">food security</a>, <a href=\"https://blog.google/intl/en-africa/company-news/outreach-and-initiatives/google-helps-africa-build-an-ai-data-future-with-225m-in-support/\" target=\"_blank\" rel=\"noopener noreferrer\">infrastructure</a> and <a href=\"https://www.youtube.com/watch?v=JzXhGUhL99c&amp;t=2s\" target=\"_blank\" rel=\"noopener noreferrer\">languages</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"uoa5v\">A new model for collaborative health innovation</h2><p data-block-key=\"4shq1\">An <i>ideathon</i>, much like a hackathon, is a platform for interdisciplinary teams to design solutions to crucial challenges, valuing the idea creativity as much as its execution.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Ideathon1_Group.width-1250.png\" alt=\"Ideathon1_Group\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Ideathon1_Group.width-1250.png\" alt=\"Ideathon1_Group\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"j7h2b\"><i>The launch of the Ideathon took place at the 2025 Deep Learning Indaba conference in Kigali, Rwanda.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"uoa5v\">The Ideathon was launched during the <a href=\"https://ds4healthafrica.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\">2025 Data Science for Health in Africa Workshop</a> at the <a href=\"https://deeplearningindaba.com/2025/workshops/\" target=\"_blank\" rel=\"noopener noreferrer\">Deep Learning Indaba</a>, the gathering of Africa’s machine learning (ML) and AI community, dedicated to strengthening and celebrating African AI, which took place in Kigali, Rwanda. The Ideathon built on the shared vision of building capacity across African AI and health communities. Participants also had the opportunity at the workshop to attend a hands-on <a href=\"http://goo.gle/medgemma\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma</a> tutorial led by Dr. Sekou Remy and Dr. Mercy Asiedu, research scientists at Google Research.</p><p data-block-key=\"focpb\">Teams were encouraged to explore Google’s open health AI models, namely: <a href=\"https://deepmind.google/models/gemma/medgemma/\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma</a>, <a href=\"https://deepmind.google/models/gemma/txgemma/\" target=\"_blank\" rel=\"noopener noreferrer\">TxGemma</a>, and <a href=\"https://developers.google.com/health-ai-developer-foundations/medsiglip\" target=\"_blank\" rel=\"noopener noreferrer\">MedSigLIP</a>, and apply these models to healthcare challenges ranging from diagnostics to policy frameworks. Submissions were judged on innovation, feasibility, contextual relevance, and the creative use of Google’s AI tools in African settings.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"uoa5v\">From idea to impact: A two-phase journey</h2><p data-block-key=\"b5jnq\">The Ideathon unfolded in two phases:</p><ol><li data-block-key=\"44q2r\"><i>Idea development</i>: Teams introduced their members, defined healthcare problems, and outlined AI-driven approaches using Google’s models. Selected teams received mentorship, Google Cloud Vertex AI compute credits, and technical documentation to refine their ideas.</li><li data-block-key=\"17qch\"><i>Prototype &amp; pitch</i>: Teams submitted demo videos showcasing their solutions, which were then reviewed by a panel of expert judges who chose six finalists for the final, live pitch.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"uoa5v\">The final pitch: Creativity meets purpose</h2><p data-block-key=\"c1icv\">Six finalist teams presented their solutions during the <a href=\"https://cassyni.com/events/Dun95er1wKLsGCL3jB9Zxh\" target=\"_blank\" rel=\"noopener noreferrer\">DS4H Ideathon Live Pitch</a>, broadcast globally, followed by a team Q&amp;A session with the judges. The event emphasized not only technical excellence but also vision, collaboration, and contextual innovation in African healthcare.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"uoa5v\">Winning teams and awards</h2><p data-block-key=\"5jose\">Here are the winning projects that showcase how Google's open models can serve as a foundation on which developers may build and help solve pressing healthcare challenges in Africa:</p><h3 data-block-key=\"gr1h\">First place and Audience Choice Award: Dawa Health – <i>AI-powered multilingual cervical-cancer education &amp; screening tool</i></h3><p data-block-key=\"257vk\">Built on <a href=\"https://developers.google.com/health-ai-developer-foundations/medsiglip\" target=\"_blank\" rel=\"noopener noreferrer\">MedSigLIP</a> and enhanced with <a href=\"https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/rag-overview\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini RAG</a>. Midwives first upload colposcopy images via WhatsApp. The MedSigLIP-based classifier then identifies abnormalities indicating precancer/cancer in real time, while Gemini provides contextual clinical guidance using <a href=\"https://www.who.int/\" target=\"_blank\" rel=\"noopener noreferrer\">WHO</a> and Zambian protocols [<a href=\"https://drive.google.com/file/d/1hSPelTCuLHUIBgwak0ywFeFGAj0XKeDv/view\" target=\"_blank\" rel=\"noopener noreferrer\">1</a>, <a href=\"https://dspace.unza.zm/server/api/core/bitstreams/8a2bfed0-ff6d-46b6-af75-8f09af0e7f7d/content\" target=\"_blank\" rel=\"noopener noreferrer\">2</a>].</p><p data-block-key=\"cnqpr\"><i>Team: Tafadzwa Munzwa, Khanyisile Magagula, Kudzal Mwedzi, Tariro Munzwa.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Ideathon2_DawaHealth.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"j7h2b\"><i>The winning prototype: Dawa Health’s Innovative solution for cervical cancer screening building on MedSigLIP and Gemini RAG.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"uoa5v\">Second place: Solver – <i>CerviScreen AI</i></h3><p data-block-key=\"ct0gj\">A <a href=\"https://fastapi.tiangolo.com/\" target=\"_blank\" rel=\"noopener noreferrer\">FastAPI</a> web app for automated cervical-cytology screening based on MedGemma-27B-IT, fine-tuned with <a href=\"https://arxiv.org/abs/2106.09685\" target=\"_blank\" rel=\"noopener noreferrer\">LoRA</a> on the <a href=\"https://bids.berkeley.edu/cric-cervix-collection\" target=\"_blank\" rel=\"noopener noreferrer\">CRIC dataset</a>. Outputs annotated images and concise clinical recommendations to assist cytopathologists.</p><p data-block-key=\"73lgh\"><i>Team: Bonaventure F.P. Dossou, Ariane Houetohossou, Aurel Tchokponhoue, Ghilith Gbaguidi.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Ideathon3_Screening.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"j7h2b\"><i>Solver Team building on MedGemma-27B-IT for Cervical-Cytology Screening.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"uoa5v\">Third place: Mkunga – <i>Maternal AI call center</i></h3><p data-block-key=\"11b9u\">Adapting MedGemma and Gemini to provide maternal health advice in Swahili using <a href=\"https://docs.cloud.google.com/text-to-speech/docs/gemini-tts\" target=\"_blank\" rel=\"noopener noreferrer\">TTS/STT</a>, deployed on <a href=\"https://cloud.google.com/vertex-ai?gclsrc=aw.ds&amp;gad_source=1&amp;gad_campaignid=23058943583&amp;gclid=Cj0KCQiAi9rJBhCYARIsALyPDtvxBdsKqiFBW0_MWAc326EpZyKVMgk5GAQZ5pG2adiVGHJyDhqvrfYaArJ1EALw_wcB\" target=\"_blank\" rel=\"noopener noreferrer\">Vertex AI</a>. Aims to scale as a low-cost telehealth assistant.</p><p data-block-key=\"cd7nu\"><i>Team: Malik Lanlokun, Beryl Apondi, Janet Njiru, Joseph Wacira.</i></p><h3 data-block-key=\"ablpm\">Best proof-of-concept (offline / low-connectivity): HexAI – <i>DermaDetect</i></h3><p data-block-key=\"b2mj3\">Offline-first mobile app that enables community health workers to triage skin conditions using adapted versions of on-device MedSigLIP and cloud-based MedGemma for advanced analysis, creating a “data flywheel” for continuous improvement.</p><p data-block-key=\"14eku\"><i>Team: Ebrima S Jallow, Omar Keita.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Ideathon4_HexAI.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"j7h2b\"><i>HexAI building on MedSigLIP and MedGemma for dermatological condition triage.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"uoa5v\">Most fun solution: MamaLens Lab</h3><p data-block-key=\"877en\">Multilingual offline Android assistant for community health workers, adapting MedGemma, MedSigLIP, and <a href=\"https://developers.googleblog.com/en/introducing-txgemma-open-models-improving-therapeutics-development/\" target=\"_blank\" rel=\"noopener noreferrer\">TxGemma</a> to assess pregnancy risk in English and Yoruba.</p><p data-block-key=\"7mhpo\"><i>Team: Ilerioluwakiiye Abolade, Faith Nchifor, Toyibat Adele, Simeon Krah.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"uoa5v\">What comes next</h2><p data-block-key=\"f3k3k\">The end of the Ideathon marks the beginning of sustained development and scaling. Several finalist teams are exploring ways to scale and deploy their solutions, supported by networks built during the program. The winning team, Dawa Health, is currently piloting an early access program for cervical cancer screening using a MedSigLIP-based classifier with plans to scale to 50,000 patients next year, effectively demonstrating scalable, real world impact.</p><p data-block-key=\"9rise\">To learn more about Google's open models and start exploring them for your own use cases, visit our <a href=\"http://goo.gle/hai-def\" target=\"_blank\" rel=\"noopener noreferrer\">HAI-DEF site</a>. We welcome your <a href=\"https://services.google.com/fb/forms/hai-def-feedback\" target=\"_blank\" rel=\"noopener noreferrer\">feedback</a> on the models and your use cases.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"uoa5v\">Acknowledgements</h2><p data-block-key=\"7bked\"><i>This initiative was made possible through the vision, collaboration, and funding support of the organizing team and partners: Comfort Adesina (Ideathon Chair, SisonkeBiotik), Abraham Owodunni (Technical Coordinator, SisonkeBiotik), Taliya Weinstein (Ideathon Host and Advisor, SisonkeBiotik), Ashery Mbilinyi (Ideathon Judge), Lukman Ismaila (Ideathon Judge), and Google Research and DeepMind: Mercy Asiedu, Sekou L. Remy, Sunny Jansen, Fereshteh Mahvar, Tiffany Chen,</i> <i>Yun Liu, Katherine Heller, Joelle Barrel, Richa Tiwari, Sunny Virmani.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "A differentially private framework for gaining insights into AI chatbot use",
      "link": "https://research.google/blog/a-differentially-private-framework-for-gaining-insights-into-ai-chatbot-use/",
      "pubDate": "Tue, 09 Dec 2025 16:00:00 GMT",
      "isoDate": "2025-12-09T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "A differentially private framework for gaining insights into AI chatbot use",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_hero1.width-1250.jpg",
          "alt": "Gaining_insights_into_AI_chatbot_use_hero1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_hero.width-1250.png",
          "alt": "Gaining_insights_into_AI_chatbot_use_hero",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_2.width-1250.png",
          "alt": "Gaining_insights_into_AI_chatbot_use_2",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_3.width-1250.png",
          "alt": "Gaining_insights_into_AI_chatbot_use_3",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"k5pub\">Large language model (LLM) chatbots are used by hundreds of millions of people daily for tasks ranging from drafting emails and writing code to planning vacations and creating menus for cafes. Understanding these high-level use cases is incredibly valuable for platform providers looking to improve services or enforce safety policies. It also offers the public insights into how AI is shaping our world.</p><p data-block-key=\"ergf9\">But this raises a critical question: How can we gain valuable insights when the conversations themselves might contain private or sensitive information?</p><p data-block-key=\"d3h83\">Existing approaches, like the <a href=\"https://www.anthropic.com/research/clio\" target=\"_blank\" rel=\"noopener noreferrer\">CLIO</a> framework, attempt to solve this by using an LLM to summarize conversations while prompting it to strip out <a href=\"https://en.wikipedia.org/wiki/Personal_data\" target=\"_blank\" rel=\"noopener noreferrer\">personally identifiable information</a> (PII). While a good first step, this method relies on heuristic privacy protections. The resulting privacy guarantee is difficult to formalize and may not hold up as models evolve, making these systems difficult to maintain and audit. This limitation led us to ask if it is possible to achieve similar utility with formal, end-to-end privacy guarantees.</p><p data-block-key=\"7hpa6\">In our paper, \"<a href=\"https://arxiv.org/abs/2506.04681\" target=\"_blank\" rel=\"noopener noreferrer\">Urania: Differentially Private Insights into AI Use</a>,\" presented at <a href=\"https://colmweb.org/\" target=\"_blank\" rel=\"noopener noreferrer\">COLM 2025</a>, we introduce a new framework that generates insights from LLM chatbot interactions with rigorous <a href=\"https://en.wikipedia.org/wiki/Differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">differential privacy</a> (DP) guarantees. This framework uses a <a href=\"https://research.google/blog/practical-differentially-private-clustering/\">DP clustering algorithm</a> and keyword extraction method to ensure that no single conversation overly influences the result (i.e., the output summaries do not reveal information about any single individual's conversation). Here we explain the algorithm and demonstrate that this framework is indeed providing better privacy guarantees than prior solutions.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_hero1.width-1250.jpg\" alt=\"Gaining_insights_into_AI_chatbot_use_hero1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_hero1.width-1250.jpg\" alt=\"Gaining_insights_into_AI_chatbot_use_hero1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"qlcu8\"><i>Gemini-generated image showing schematically how the algorithm works for one cluster of conversations.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Privacy-preserving framework for insights mining</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"k5pub\">DP uses a privacy budget parameter, ε, to measure the maximum allowed influence of any single user's contributions to the final output of a model. Our framework is designed to rely on two key properties of DP:</p><ol><li data-block-key=\"410n7\"><i>Post-processing:</i> If <i>B</i> is an <i>ε</i>-DP algorithm and <i>A</i> is any non-DP algorithm, then running <i>A</i> on the output of <i>B</i> keeps things private at a <i>ε</i>-DP level.</li><li data-block-key=\"auflb\"><i>Composition:</i> If <i>A</i> and <i>B</i> are two separate <i>ε</i>-DP algorithms, running <i>A</i> on the <i>dataset</i> and the output of <i>B</i> still keeps the entire process private at a 2<i>ε</i>-DP level.</li></ol><p data-block-key=\"9pfmg\">This differentially private pipeline is designed to ensure end-to-end user data protection through the following stages:</p><ol><li data-block-key=\"feaob\"><b>DP clustering</b>: Conversations are first converted into numerical representations (embeddings). The framework then groups representations that are close to each other using a <a href=\"https://research.google/blog/practical-differentially-private-clustering/\">DP clustering algorithm</a>. This ensures that no single conversation overly influences the cluster centers.</li><li data-block-key=\"9393b\"><b>DP keyword extraction</b>: Keywords are extracted from each conversation. For every cluster, our approach computes a histogram of keywords, i.e., it counts the number of times each keyword appears in the cluster, using a DP histogram mechanism (e.g., [<a href=\"https://arxiv.org/abs/2006.03684\" target=\"_blank\" rel=\"noopener noreferrer\">1</a>, <a href=\"https://arxiv.org/abs/2301.01998\" target=\"_blank\" rel=\"noopener noreferrer\">2</a>]). We add noise to the histogram in order to mask the influence of individual conversations, ensuring that only keywords common to multiple users are selected, preventing unique or sensitive terms from being exposed. We explore three methods for creation of keywords for each conversation:<ol><li data-block-key=\"9ve1l\">LLM guided selection: We provide an LLM the conversation and ask it to create the top five most relevant keywords;</li><li data-block-key=\"fuvdn\">DP version of <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" target=\"_blank\" rel=\"noopener noreferrer\">TF-IDF</a>: We get all the words in the conversation and weight them proportionally to the number of times they appear in the text and inversely proportional to the number of times they appear in the corpus; and</li><li data-block-key=\"9r76v\">LLM guided approach where there is an initial list of keywords obtained from public data: Instead of asking the LLM to generate keywords for each conversation independently, we create a list of potential keywords and ask LLM to choose the top 5 most relevant keywords from the list.</li></ol></li><li data-block-key=\"71r3d\"><b>LLM summarization from keywords</b>: Finally, an LLM generates a high-level summary for each cluster using only the privately selected keywords. The LLM never sees the original conversations in the cluster, only the anonymized keywords. This post-processing property ensures the end-to-end privacy of the entire framework.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_hero.width-1250.png\" alt=\"Gaining_insights_into_AI_chatbot_use_hero\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_hero.width-1250.png\" alt=\"Gaining_insights_into_AI_chatbot_use_hero\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"hhudz\"><i>The framework’s data flow. Yellow nodes denote non-DP data, green nodes represent operations that are either DP or per conversation, light blue nodes denote private data, and dark blue nodes represent non-private operations.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"k5pub\">By integrating DP at its core, this framework’s privacy guarantees are mathematical, not heuristic. They don't depend on an LLM's ability to perfectly redact private data: in other words, even if keywords contain PII or some other sensitive data, the generated summaries are not going to contain this data. In more practical terms, this guarantee makes it impossible for the LLM to reveal sensitive data (e.g., due to prompt injection attacks).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Putting this framework to the test</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"k5pub\">To evaluate our framework’s utility (summary quality) and privacy (protection strength), we compared its performance against Simple-CLIO, a non-private baseline we created inspired by <a href=\"https://www.anthropic.com/research/clio\" target=\"_blank\" rel=\"noopener noreferrer\">CLIO</a>. The baseline follows a two-step process:</p><ol><li data-block-key=\"30t2i\">Conversations are converted into embeddings and clustered non-privately.</li><li data-block-key=\"25ftm\">For each cluster, a sample of conversations is fed to an LLM to generate a summary of those samples.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">The privacy-utility trade-off</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"k5pub\">As expected, we observed a trade-off: stronger privacy settings (lower values of the privacy parameter <i>ϵ</i>) led to a decrease in the granularity of the summaries. For instance, topic coverage dropped as the privacy budget tightened, because the DP clustering algorithm produced fewer and less precise clusters.</p><p data-block-key=\"cg6p6\">However, the results also held a surprise. In head-to-head comparisons, LLM evaluators often preferred the private summaries generated by our framework. In one evaluation, the DP-generated summaries were favored up to 70% of the time. This suggests that the constraints imposed by this DP pipeline — forcing summaries to be based on general, frequent keywords — can lead to outputs that are more concise and focused than those from an unconstrained, non-private approach.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Empirical privacy evaluation</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"k5pub\">To test the framework’s robustness, we ran a <a href=\"https://arxiv.org/abs/1610.05820\" target=\"_blank\" rel=\"noopener noreferrer\">membership inference-style attack</a> designed to identify whether a specific sensitive conversation was included in the dataset. The results were clear: the attack on the DP pipeline performed about as well as random guessing, achieving an <a href=\"https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\" target=\"_blank\" rel=\"noopener noreferrer\">area under the curve</a> (AUC) score of 0.53 (i.e., the integral of the <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\" rel=\"noopener noreferrer\"><i>ROC</i></a> curve). In contrast, the attack was more successful against the non-private pipeline, which had a higher AUC of 0.58, indicating greater information leakage. This experiment provides empirical evidence that our privacy framework offers significantly stronger protection against privacy leakage.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_2.width-1250.png\" alt=\"Gaining_insights_into_AI_chatbot_use_2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_2.width-1250.png\" alt=\"Gaining_insights_into_AI_chatbot_use_2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"hhudz\"><i>The</i> <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\" rel=\"noopener noreferrer\"><i>ROC</i></a><i> curve for DP pipeline shows performance close to random guessing (AUC = 0.53), demonstrating its robustness.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_3.width-1250.png\" alt=\"Gaining_insights_into_AI_chatbot_use_3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Gaining_insights_into_AI_chatbot_use_3.width-1250.png\" alt=\"Gaining_insights_into_AI_chatbot_use_3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"hhudz\"><i>The</i> <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\" target=\"_blank\" rel=\"noopener noreferrer\"><i>ROC</i></a><i> curve for the non-private pipeline is more vulnerable (AUC = 0.58).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Looking ahead</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"k5pub\">Our work is a first step toward building systems that can analyze large-scale text corpora with formal privacy guarantees. We've shown that it's possible to balance the need for meaningful insights with stringent user privacy.</p><p data-block-key=\"6t4ua\">Looking forward, we see several exciting avenues for future research. These include adapting the framework for online settings where new conversations are constantly added, exploring alternative DP mechanisms to further improve the utility-privacy trade-off, and adding support for multi-modal conversations (i.e., conversations involving images, videos, and audio).</p><p data-block-key=\"9t7eu\">As AI becomes more integrated into our daily lives, developing privacy-preserving methods for understanding its use is not just a technical challenge — it's a fundamental requirement for building trustworthy and responsible AI.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"k5pub\">Thanks to all project contributors, whose essential efforts were pivotal to its success. Special thanks to our colleagues: Yaniv Carmel, Edith Cohen, Rudrajit Das, Chris Dibak, Vadym Doroshenko, Alessandro Epasto, Prem Eruvbetine, Dem Gerolemou, Badih Ghazi, Miguel Guevara, Steve He, Peter Kairouz, Pritish Kamath, Nir Kerem, Ravi Kumar, Ethan Leeman, Pasin Manurangsi, Shlomi Pasternak, Mikhail Pravilov, Adam Sealfon, Yurii Sushko, Da Yu, Chiyuan Zhang.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Titans + MIRAS: Helping AI have long-term memory",
      "link": "https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/",
      "pubDate": "Wed, 03 Dec 2025 16:00:00 GMT",
      "isoDate": "2025-12-03T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Titans + MIRAS: Helping AI have long-term memory",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Titans-1-Overview.width-1250.png",
          "alt": "A diagram illustrating a neural architecture with three layers: Contextual Memory (learning), Core (in-context learning), and Persistent Memory (fixed weights).",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Titans-3-Performance1.width-1250.png",
          "alt": "Two line charts showing that LMM and MM models maintain lower perplexity than Mamba as sequence length increases across 360M and 760M parameter scales.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Titans-4-Performance2.width-1250.png",
          "alt": "Line graph showing Titans (MAC)-FT maintains improved accuracy over increasing sequence lengths compared to GPT-4, Mamba-FT, and other models.",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"il1w2\">The <a href=\"https://en.wikipedia.org/wiki/Transformer_(deep_learning)\" target=\"_blank\" rel=\"noopener noreferrer\">Transformer architecture</a> revolutionized <a href=\"https://medium.com/machine-learning-basics/sequence-modelling-b2cdf244c233\" target=\"_blank\" rel=\"noopener noreferrer\">sequence modeling</a> with its introduction of <a href=\"https://en.wikipedia.org/wiki/Attention_%28machine_learning%29\" target=\"_blank\" rel=\"noopener noreferrer\">attention</a>, a mechanism by which models look back at earlier inputs to prioritize relevant input data. However, computational cost increases drastically with sequence length, which limits the ability to scale Transformer-based models to extremely long contexts, such as those required for full-document understanding or genomic analysis.</p><p data-block-key=\"36kb5\">The research community explored various approaches for solutions, such as efficient linear <a href=\"https://www.d2l.ai/chapter_recurrent-modern/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">recurrent neural networks</a> (RNNs) and <a href=\"https://huggingface.co/blog/lbourdois/get-on-the-ssm-train\" target=\"_blank\" rel=\"noopener noreferrer\">state space models</a> (SSMs) like <a href=\"https://arxiv.org/pdf/2405.21060\" target=\"_blank\" rel=\"noopener noreferrer\">Mamba-2</a>. These models offer fast, linear scaling by compressing context into a fixed-size. However, this fixed-size compression cannot adequately capture the rich information in very long sequences.</p><p data-block-key=\"40m00\">In two new papers, <a href=\"https://arxiv.org/abs/2501.00663\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Titans</i></a> and <a href=\"https://arxiv.org/pdf/2504.13173\" target=\"_blank\" rel=\"noopener noreferrer\"><i>MIRAS</i></a>, we introduce an architecture and theoretical blueprint that combine the speed of RNNs with the accuracy of transformers. Titans is the specific architecture (the tool), and MIRAS is the theoretical framework (the blueprint) for generalizing these approaches. Together, they advance the concept of test-time memorization, the ability of an AI model to maintain long-term memory by incorporating more powerful “surprise” metrics (i.e., unexpected pieces of information) while the model is running and without dedicated offline retraining.</p><p data-block-key=\"eic3n\">The MIRAS framework, as demonstrated by Titans, introduces a meaningful shift toward real-time adaptation. Instead of compressing information into a static state, this architecture actively learns and updates its own parameters as data streams in. This crucial mechanism enables the model to incorporate new, specific details into its core knowledge instantly.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Titans: Learning new context on the fly\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Titans: Learning new context on the fly</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">An effective learning system requires distinct yet interconnected memory modules, mirroring the <a href=\"https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/\">human brain's separation of short-term and long-term memory</a>.</p><p data-block-key=\"dpe7v\">While attention mechanisms excel for precise, short-term memory, Titans introduces a novel neural <a href=\"https://arxiv.org/abs/2306.07174#:~:text=LongMem%20can:%20*%20Memorize%20long%20past%20context,Yan%20*%20Jianfeng%20Gao%20*%20Furu%20Wei\" target=\"_blank\" rel=\"noopener noreferrer\">long-term memory module</a>, that, unlike the fixed-size vector or matrix memory in traditional RNNs, acts as a deep neural network (specifically, a <a href=\"https://en.wikipedia.org/wiki/Multilayer_perceptron\" target=\"_blank\" rel=\"noopener noreferrer\">multi-layer perceptron</a>). This memory module provides significantly higher expressive power, allowing the model to summarize large volumes of information without losing important context. The model isn't simply taking notes; it's understanding and synthesizing the entire story.</p><p data-block-key=\"e95op\">Crucially, Titans doesn’t just passively store data. It actively learns <i>how</i> to recognize and retain important relationships and conceptual themes that connect tokens across the entire input. A key aspect of this ability is what we call the “surprise metric”. In human psychology, we know we quickly and easily forget routine, expected events but remember things that break the pattern — unexpected, surprising, or highly emotional events.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Titans-1-Overview.width-1250.png\" alt=\"A diagram illustrating a neural architecture with three layers: Contextual Memory (learning), Core (in-context learning), and Persistent Memory (fixed weights).\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Titans-1-Overview.width-1250.png\" alt=\"A diagram illustrating a neural architecture with three layers: Contextual Memory (learning), Core (in-context learning), and Persistent Memory (fixed weights).\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"759np\">Overview of the Titans (MAC) architecture. It uses a long-term memory to compress the past data and then incorporate the summary into the context and pass it to attention. Attention can then decide if it needs to attend to the summary of the past or not.</p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"il1w2\">In the context of Titans, the \"surprise metric\" is the model detecting a large difference between what it currently remembers and what the new input is telling it.</p><ul><li data-block-key=\"a9lns\"><i>Low surprise</i>: If the new word is \"cat\" and the model's memory state already expects an animal word, the gradient (surprise) is low. It can safely skip memorizing the word \"cat\" in its permanent long-term state.</li><li data-block-key=\"2t2sa\"><i>High surprise</i>: If the model's memory state is summarizing a serious financial report, and the new input is a picture of a banana peel (the unexpected event), the gradient (surprise) will be very high. This signals that the new input is important or anomalous, and it must be prioritized for permanent storage in the long-term memory module.</li></ul><p data-block-key=\"djj22\">The model uses this internal error signal (the gradient) as a mathematical equivalent of saying, \"This is unexpected and important!\" This allows the Titans architecture to selectively update its long-term memory only with the most novel and context-breaking information, keeping the overall process fast and efficient.</p><p data-block-key=\"dm2am\">Titans refines this mechanism by incorporating two critical elements:</p><ol><li data-block-key=\"bb101\"><i>Momentum</i>: The model considers both \"momentary surprise\" (the current input) and \"past surprise\" (the recent context flow). This ensures relevant subsequent information is also captured, even if those tokens are not individually surprising.</li><li data-block-key=\"b269a\"><i>Forgetting (weight decay)</i>: To manage the finite capacity of the memory when dealing with extremely long sequences, Titans employ an adaptive weight decay mechanism. This acts as a forgetting gate, allowing the model to discard information that is no longer needed.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"MIRAS: A unified view of sequence modeling\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">MIRAS: A unified view of sequence modeling</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">Every major breakthrough in sequence modeling — from modern transformers to the new, lightning-fast linear RNNs — is essentially the same thing under the hood: a highly complex <a href=\"https://www.geeksforgeeks.org/computer-organization-architecture/associative-memory/\" target=\"_blank\" rel=\"noopener noreferrer\">associative memory</a> module.</p><p data-block-key=\"d91su\">Accordingly, what makes MIRAS both unique and practical is the way it views AI modeling. Instead of seeing diverse architectures, it sees different methods of solving the same problem: efficiently combining new information with old memories without letting the essential concepts be forgotten<b>.</b></p><p data-block-key=\"7u78e\">MIRAS defines a sequence model through four key design choices:</p><ul><li data-block-key=\"abcem\"><i>Memory architecture</i>: The structure that stores information (e.g., a vector, matrix, or a deep multi-layer perceptron, like in Titans).</li><li data-block-key=\"5s0u1\"><i>Attentional bias</i>: The internal learning objective the model optimizes that determines what it prioritizes.</li><li data-block-key=\"4qd03\"><i>Retention gate</i>: The memory regularizer. MIRAS reinterprets \"forgetting mechanisms\" as specific forms of <a href=\"https://dev.to/nareshnishad/day-27-regularization-techniques-for-large-language-models-llms-4af3\" target=\"_blank\" rel=\"noopener noreferrer\">regularization</a> that balance new learning against retaining past knowledge.</li><li data-block-key=\"9savd\"><i>Memory algorithm</i>: The optimization algorithm used to update the memory.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/MIRAS_Framework_Animation.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"759np\">The MIRAS framework overview. In the MIRAS framework, we aim to learn an associative memory, mapping between keys and values. For each token, the memory module internally optimizes its inner attentional bias while using its retention gate to make sure that it does not deviate from its past state. The optimization process is done through gradient-based optimizer.</p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Transcending the mean squared error paradigm\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Transcending the mean squared error paradigm</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">Virtually all successful existing sequence models rely on <a href=\"https://en.wikipedia.org/wiki/Mean_squared_error\" target=\"_blank\" rel=\"noopener noreferrer\">mean squared error</a> (MSE) or <a href=\"https://medium.com/advanced-deep-learning/understanding-vector-similarity-b9c10f7506de\" target=\"_blank\" rel=\"noopener noreferrer\">dot-product similarity</a> for both their bias and retention. This reliance can make models sensitive to outliers and limit their expressive power.</p><p data-block-key=\"1cust\">MIRAS transcends this limitation by providing a generative framework to explore a more rich design space informed by the literature in optimization and statistics. This allows for the creation of novel architectures with <a href=\"https://en.wikipedia.org/wiki/Non-Euclidean_geometry\" target=\"_blank\" rel=\"noopener noreferrer\">non-Euclidean objectives</a> and regularization.</p><p data-block-key=\"40qp3\">Using MIRAS, we created three specific attention-free models:</p><ul><li data-block-key=\"fpeib\"><i>YAAD</i>: We designed this MIRAS variant to be less sensitive to major errors or \"outliers\" (like a single typo in a large document). It uses a gentler math penalty (<a href=\"https://en.wikipedia.org/wiki/Huber_loss\" target=\"_blank\" rel=\"noopener noreferrer\">Huber loss</a>) for mistakes, so it doesn't overreact to one-off issues. This makes the model more robust when the input data is messy or inconsistent.</li><li data-block-key=\"28vrl\"><i>MONETA</i>: This model explores the use of more complex and strict mathematical penalties (called <a href=\"https://en.wikipedia.org/wiki/Norm_(mathematics)\" target=\"_blank\" rel=\"noopener noreferrer\">generalized norms</a>). It investigates whether using these more disciplined rules for both what the model attends to and what it forgets can lead to a more powerful and stable long-term memory system overall.</li><li data-block-key=\"d4e49\"><i>MEMORA</i>: This model focuses on achieving the best possible memory stability by forcing its memory to act like a strict probability map. By using this constraint, it ensures that every time the memory state is updated, the changes are controlled and balanced. This guarantees a clean, stable process for integrating new information.Virtually all successful existing sequence models rely on <a href=\"https://en.wikipedia.org/wiki/Mean_squared_error\" target=\"_blank\" rel=\"noopener noreferrer\">mean squared error</a> (MSE) or <a href=\"https://medium.com/advanced-deep-learning/understanding-vector-similarity-b9c10f7506de\" target=\"_blank\" rel=\"noopener noreferrer\">dot-product similarity</a> for both their bias and retention. This reliance can make models sensitive to outliers and limit their expressive power.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Experiments and results\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Experiments and results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">We rigorously compared Titans along with MIRAS variants (YAAD, MONETA, MEMORA) against leading architectures, including <a href=\"https://arxiv.org/abs/2003.04974\" target=\"_blank\" rel=\"noopener noreferrer\">Transformer++</a>, <a href=\"https://arxiv.org/pdf/2405.21060\" target=\"_blank\" rel=\"noopener noreferrer\">Mamba-2</a>, and <a href=\"https://arxiv.org/pdf/2412.06464\" target=\"_blank\" rel=\"noopener noreferrer\">Gated DeltaNet</a>. We further validated versatility by testing Titans on genomic modeling (DNA) and time-series forecasting, proving the architecture generalizes effectively beyond text.</p><p data-block-key=\"a9v6c\">Across both standard language modeling datasets (<a href=\"https://c4model.com/\" target=\"_blank\" rel=\"noopener noreferrer\">C4</a>, <a href=\"https://huggingface.co/datasets/Salesforce/wikitext\" target=\"_blank\" rel=\"noopener noreferrer\">WikiTex</a>t) and <a href=\"https://medium.com/@hetzer2807/zero-shot-reasoning-unleashed-the-magic-of-large-language-models-4e877dfe470e\" target=\"_blank\" rel=\"noopener noreferrer\">zero-shot reasoning tasks</a> (<a href=\"https://arxiv.org/abs/1905.07830\" target=\"_blank\" rel=\"noopener noreferrer\">HellaSwag</a>, PIQA), our models consistently demonstrated higher accuracy and <a href=\"https://en.wikipedia.org/wiki/Perplexity\" target=\"_blank\" rel=\"noopener noreferrer\">perplexity</a> (a measure of how surprised an LLM is when looking at a piece of text).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"The power of deep memory\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">The power of deep memory</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">Ablation studies clearly show that the depth of the memory architecture is crucial. When comparing long-term memory modules of the same size but different depths, modules with deeper memories consistently achieve lower perplexity in language modeling. Furthermore, they exhibit better scaling properties, maintaining performance as the sequence length increases significantly.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Titans-3-Performance1.width-1250.png\" alt=\"Two line charts showing that LMM and MM models maintain lower perplexity than Mamba as sequence length increases across 360M and 760M parameter scales.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Titans-3-Performance1.width-1250.png\" alt=\"Two line charts showing that LMM and MM models maintain lower perplexity than Mamba as sequence length increases across 360M and 760M parameter scales.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"759np\"><i>The effect of memory depth on the perplexity across 360M and 760M parameter scales.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Language modeling and efficiency\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Language modeling and efficiency</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">In language modeling and commonsense reasoning tasks, Titans architectures outperform state-of-the-art linear recurrent models (such as Mamba-2 and Gated DeltaNet) and Transformer++ baselines of comparable sizes. The novel MIRAS variants (MONETA, YAAD, MEMORA) also achieve improved performance compared to these baselines, validating the benefit of exploring robust, non-MSE optimization mechanisms. Importantly, these models maintain efficient, parallelizable training and fast linear inference speeds.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Extreme long-context recall\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Extreme long-context recall</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">The most significant advantage of these new architectures is their ability to handle extremely long contexts. This is highlighted in the <a href=\"https://github.com/booydar/babilong\" target=\"_blank\" rel=\"noopener noreferrer\">BABILong benchmark</a>, a task requiring reasoning across facts distributed in extremely long documents. In this challenging setting, Titans outperforms all baselines, including extremely large models like GPT-4, despite having many fewer parameters. Titans further demonstrates the capability to scale effectively to context window sizes larger than 2 million tokens.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Titans-4-Performance2.width-1250.png\" alt=\"Line graph showing Titans (MAC)-FT maintains improved accuracy over increasing sequence lengths compared to GPT-4, Mamba-FT, and other models.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Titans-4-Performance2.width-1250.png\" alt=\"Line graph showing Titans (MAC)-FT maintains improved accuracy over increasing sequence lengths compared to GPT-4, Mamba-FT, and other models.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"759np\"><i>Performance of Titans on extreme long-context reasoning.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"Conclusion\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"il1w2\">The introduction of Titans and the MIRAS framework marks a significant advancement in sequence modeling. By employing deep neural networks as memory modules that learn to memorize as data is coming in, these approaches overcome the limitations of fixed-size recurrent states. Furthermore, MIRAS provides a powerful theoretical unification, revealing the connection between online optimization, associative memory, and architectural design. By moving beyond the standard Euclidean paradigm, this research opens the door to a new generation of sequence models that combine the efficiency of RNNs with the expressive power needed for the era of long-context AI.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "From Waveforms to Wisdom: The New Benchmark for Auditory Intelligence",
      "link": "https://research.google/blog/from-waveforms-to-wisdom-the-new-benchmark-for-auditory-intelligence/",
      "pubDate": "Tue, 02 Dec 2025 16:00:00 GMT",
      "isoDate": "2025-12-02T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "From Waveforms to Wisdom: The New Benchmark for Auditory Intelligence",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/image2.width-1250.png",
          "alt": "Infographic titled Massive Sound Embedding Benchmark (MSEB) displaying icons for eight audio tasks, such as Retrieval, Classification, and Transcription.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/image1.width-1250.png",
          "alt": "Bar chart comparing performance metrics for Text and Sound inputs across \"SuperTasks\" like Retrieval, Reasoning, and Classification.",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"04uji\">Sound is a critical part of <a href=\"https://courses.lumenlearning.com/waymaker-psychology/chapter/multi-modal-perception/#:~:text=For%20example%2C%20if%20the%20middle,Right%20there\" target=\"_blank\" rel=\"noopener noreferrer\">multimodal perception</a>. For a system — be it a voice assistant, a next-generation security monitor, or an autonomous agent — to behave naturally, it must demonstrate a full spectrum of auditory capabilities. These capabilities include transcription, classification, retrieval, reasoning, segmentation, clustering, reranking, and reconstruction.</p><p data-block-key=\"erg92\">These diverse functions rely on transforming raw sound into an intermediate representation, or <a href=\"https://huggingface.co/spaces/hesamation/primer-llm-embedding\" target=\"_blank\" rel=\"noopener noreferrer\">embedding</a>. But research into improving the auditory capabilities of multimodal perception models has been fragmented, and there remain important unanswered questions: How do we compare performance across domains like human speech and bioacoustics? What is the <i>true</i> performance potential we are leaving on the table? And could a single, general-purpose sound embedding serve as the foundation for all these capabilities?</p><p data-block-key=\"bn8cg\">To investigate these queries and accelerate progress toward robust machine sound intelligence, we created the <a href=\"https://github.com/google-research/mseb/tree/main\" target=\"_blank\" rel=\"noopener noreferrer\">Massive Sound Embedding Benchmark</a> (MSEB), presented at <a href=\"https://neurips.cc/virtual/2025/loc/san-diego/poster/121597\" target=\"_blank\" rel=\"noopener noreferrer\">NeurIPS 2025</a>.</p><p data-block-key=\"932nn\">MSEB provides the necessary structure to answer these questions by:</p><ul><li data-block-key=\"brt1v\">Standardizing evaluation for a comprehensive suite of eight real-world capabilities that we believe every human-like intelligent system must possess.</li><li data-block-key=\"h3sm\">Providing an open and extensible framework that allows researchers to seamlessly integrate and evaluate any model type — from conventional downstream uni-modal models to cascade models to end-to-end multimodal embedding models.</li><li data-block-key=\"8e1pi\">Establishing clear performance goals to objectively highlight research opportunities beyond current state-of-the-art approaches.</li></ul><p data-block-key=\"fl1pu\">Our initial experiments confirm that current sound representations are far from universal, revealing substantial performance \"headroom” (i.e., maximum improvement possible) across all eight tasks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The three pillars of MSEB: A unified framework</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">MSEB is built on three foundational pillars designed to provide the community with the tools needed to build the next generation of sound understanding models.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">1. Diverse datasets for real-world scenarios</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">A benchmark is only as strong as its data. MSEB includes a curated collection of accessible datasets that better reflect our diverse global user community. The cornerstone of our benchmark is the <a href=\"https://huggingface.co/datasets/google/svq\" target=\"_blank\" rel=\"noopener noreferrer\">Simple Voice Questions</a> (SVQ) dataset, a new resource featuring 177,352 short, spoken queries across 26 locales and 17 languages. These recordings were captured in four distinct acoustic environments (clean, background speech, traffic noise, and media noise), and include rich metadata on speaker attributes and time-aligned salient terms. We collected and open-sourced this resource, available on <a href=\"https://huggingface.co/datasets/google/svq/viewer\" target=\"_blank\" rel=\"noopener noreferrer\">Hugging Face</a>.</p><p data-block-key=\"4t0bt\">MSEB also integrates high-quality, public datasets covering a variety of sound domains:</p><ul><li data-block-key=\"92lni\"><a href=\"https://huggingface.co/datasets/FBK-MT/Speech-MASSIVE\" target=\"_blank\" rel=\"noopener noreferrer\">Speech-MASSIVE</a>: For multilingual spoken language understanding and intent classification.</li><li data-block-key=\"c5qpu\"><a href=\"https://huggingface.co/datasets/Fhrozen/FSD50k\" target=\"_blank\" rel=\"noopener noreferrer\">FSD50K</a>: A large dataset for multi-label environmental sound event recognition (200 classes from the <a href=\"https://research.google.com/audioset/ontology/index.html\">AudioSet Ontology</a>).</li><li data-block-key=\"40c5u\"><a href=\"https://huggingface.co/datasets/DBD-research-group/BirdSet\" target=\"_blank\" rel=\"noopener noreferrer\">BirdSet</a>: A massive-scale benchmark for avian bioacoustics, including complex soundscape recordings.</li></ul><p data-block-key=\"4tacb\">We’re actively working on creating and adding more relevant and large-scale datasets to MSEB. We invite the community to share their suggestions and express interest in collaboration through our <a href=\"https://github.com/google-research/mseb/tree/main/mseb\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub repo</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">2. A comprehensive suite of eight core capabilities</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">The design of MSEB is built on the premise that the future of AI-based sound interaction is multimodal. Every task uses sound as the critical input, but also incorporates information from other modalities (like text context or knowledge bases) to simulate realistic scenarios.</p><p data-block-key=\"diln9\">MSEB is structured around eight core “super-tasks”, i.e., tasks that represent a capability vital for an intelligent system:</p><ul><li data-block-key=\"53jlg\"><i>Retrieval (voice search)</i>: Simulates voice search by finding relevant documents or passages in a knowledge base from a spoken query.</li><li data-block-key=\"csvqq\"><i>Reasoning (intelligent assistants)</i>: Tests the ability to find a precise answer within a given document or passage based on a spoken question.</li><li data-block-key=\"9hbci\"><i>Classification (monitoring/security)</i>: Categorizes sounds based on speaker attributes, user intent, recording environment, or specific sound events.</li><li data-block-key=\"4ksci\"><i>Transcription</i>: Converts the audio signal into a verbatim text representation (like automatic speech recognition, or ASR, for spoken languages).</li><li data-block-key=\"3a90f\"><i>Segmentation (indexing)</i>: Identifies the most important terms within a sound clip and localizes them with precise start and end times.</li><li data-block-key=\"3c6p0\"><i>Clustering (organization)</i>: Groups a collection of sound samples based on shared attributes (like speaker identity or environment) without relying on predefined labels.</li><li data-block-key=\"a3rmt\"><i>Reranking (hypothesis refinement)</i>: Reorders a list of ambiguous text hypotheses (e.g., ASR output) to better match the original spoken query.</li><li data-block-key=\"5c70t\"><i>Reconstruction (generative AI)</i>: Tests the quality of the embedding by measuring the fidelity with which the original audio waveform can be regenerated from it.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/image2.width-1250.png\" alt=\"Infographic titled Massive Sound Embedding Benchmark (MSEB) displaying icons for eight audio tasks, such as Retrieval, Classification, and Transcription.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/image2.width-1250.png\" alt=\"Infographic titled Massive Sound Embedding Benchmark (MSEB) displaying icons for eight audio tasks, such as Retrieval, Classification, and Transcription.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"i7px7\"><i>MSEB tasks range from information access (retrieval, reranking, reasoning), to fundamental core perception (classification, transcription, segmentation), to higher-level organization generation (clustering, reconstruction).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"04uji\">Future development is focused on practical, multimodal tasks in new domains, like music or combinations with images.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">3. A robust evaluation framework and headroom baselines</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">The primary goal of MSEB is to establish strong baselines and reveal the headroom in current AI models by evaluating them across two main task categories:</p><ul><li data-block-key=\"fe0k3\"><i>Semantic (e.g., voice search, reasoning)</i>: Do the models correctly understand the meaning and intent of the spoken words, even when the audio is noisy?</li><li data-block-key=\"e2afs\"><i>Acoustic (e.g., classification, clustering)</i>: Do the models accurately identify who is speaking or what the environmental sound is, regardless of meaning?</li></ul><p data-block-key=\"4ovqc\">The model-agnostic design of the MSEB library is built to evaluate a range of models — from cascade systems to novel end-to-end audio encoders — all within a standardized, comparative framework.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Comparison methodology</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">We used the MSEB framework to test the performance of current sound embedding models to see how close the models are to being truly intelligent and universal.</p><p data-block-key=\"7kl64\">For semantic tasks, the models were compared against the ground-truth text input. For non-semantic tasks, the models are compared against the best current dedicated solution to set a solid performance baseline that any new, general-purpose model must surpass.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Core limitations of existing sound representations</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">The results demonstrate that existing AI models have measurable flaws across all key sound-understanding capabilities, which demonstrates the need for an evaluation framework like MSEB.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/image1.width-1250.png\" alt=\"Bar chart comparing performance metrics for Text and Sound inputs across &quot;SuperTasks&quot; like Retrieval, Reasoning, and Classification.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/image1.width-1250.png\" alt=\"Bar chart comparing performance metrics for Text and Sound inputs across &quot;SuperTasks&quot; like Retrieval, Reasoning, and Classification.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"i7px7\"><i>MSEB’s evaluation of AI models across key tasks shows important deficiencies and room for improvement. The metrics used for comparison include the</i> <a href=\"https://en.wikipedia.org/wiki/Mean_reciprocal_rank\" target=\"_blank\" rel=\"noopener noreferrer\"><i>MRR</i></a><i>,</i> <a href=\"https://en.wikipedia.org/wiki/F-score\" target=\"_blank\" rel=\"noopener noreferrer\"><i>F1</i></a><i>,</i> <a href=\"https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)\" target=\"_blank\" rel=\"noopener noreferrer\"><i>mAP</i></a><i>,</i> <a href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision\" target=\"_blank\" rel=\"noopener noreferrer\"><i>ACC</i></a><i>,</i> <a href=\"https://en.wikipedia.org/wiki/Word_error_rate\" target=\"_blank\" rel=\"noopener noreferrer\"><i>WER</i></a><i>,</i> <a href=\"https://en.wikipedia.org/wiki/Discounted_cumulative_gain\" target=\"_blank\" rel=\"noopener noreferrer\"><i>NDCG</i></a><i>,</i> <a href=\"https://www.cs.columbia.edu/~amaxwell/pubs/clustering_metric_paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\"><i>VMeasure</i></a><i>, and</i> <a href=\"https://arxiv.org/abs/1812.08466\" target=\"_blank\" rel=\"noopener noreferrer\"><i>FAD</i></a><i>.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"04uji\">This evaluation reveals five major problems that currently limit the capability of sound-processing AI:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">1. Semantic bottlenecks</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">For tasks relying on language content (retrieval, reasoning, reranking), the ASR stage consistently and universally bottlenecks performance, resulting in loss of <a href=\"https://medium.com/@therealitydrift/semantic-fidelity-when-ai-gets-the-facts-right-but-the-meaning-wrong-8dd270b4017f\" target=\"_blank\" rel=\"noopener noreferrer\">semantic fidelity</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">2. Misaligned objectives</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">The standard practice in speech technology involves a cascade model: transcribing speech to text, and then relying on that text for all downstream tasks. This is fundamentally wrong because it forces optimization onto the wrong metric. The ASR component is solely trained to minimize word error rate, a goal that is severely misaligned with the needs of real-world applications — which often requires maximizing the relevance, accuracy, or reasoning capability of the output, independent of perfect transcription.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">3. Non-universality</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">Models exhibit a severe lack of reliability, meaning performance varies drastically by language. The systems only work well for major, common languages. When tested on less common languages, the transcription quality collapses, causing critical task failures in search, ranking, and segmentation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">4. Lack of robustness</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">The quality of sound reconstruction degrades sharply under noise. When background noise is introduced, the model’s ability to accurately interpret the original sound and environment struggles significantly. This establishes the most challenging benchmarks for the system, highlighting its difficulty in handling complex, general environmental sounds found in real-world settings (like a busy office or a noisy street).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">5. Over-complexity</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">For simple tasks that don't involve understanding meaning (like identifying who is speaking), complicated, pre-trained AI models are surprisingly no better than just using the raw representation of the sound waves. This often leads developers to waste their efforts on overly complex models when basic data works just as well.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\">The results demonstrate a substantial performance gap in existing general sound-based approaches across all eight super-tasks. This widespread underperformance, relative to the maximum potential defined by these ceilings, underscores the critical need for more research into unified and robust sound representations that can close the gap in machine auditory intelligence.</p><p data-block-key=\"fcbrv\">We envision MSEB as a dynamic and growing platform for the entire sound processing community. We invite you to contribute to this effort by using MSEB to evaluate your own sound representation techniques, contributing new tasks and datasets to the benchmark to help it grow, and joining the collaborative effort to push the boundaries of what's possible in machine sound intelligence.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"04uji\"><i>This project was led by Ehsan Variani, Georg Heigold, Tom Bagby, and Cyril Allauzen. The authors sincerely thank all who contributed to this project, whose critical input made it possible. We are especially grateful to our colleagues Hawi Abraham, Shankar Kumar, Ji Ma, Michael Riley, Sunil Vemuri, and Travis Trekel. We also wish to acknowledge those who helped prepare this post: Mark Simborg for his extensive editing, Kimberly Schwede for the wonderful illustrations, and Mickey Wurts for his valuable assistance.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Reducing EV range anxiety: How a simple AI model predicts port availability",
      "link": "https://research.google/blog/reducing-ev-range-anxiety-how-a-simple-ai-model-predicts-port-availability/",
      "pubDate": "Thu, 20 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-20T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Reducing EV range anxiety: How a simple AI model predicts port availability",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-1-30minWeights.width-1250.png",
          "alt": "Plot of feature weights for each hour for the 30 minute horizon.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-2-60minWeights.width-1250.png",
          "alt": "Plot of feature weights for each hour for the 60 minute horizon.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-3-ErrorRates.width-1250.png",
          "alt": "Table comparing error rates on the availability of at least one free port (30 to 60-Minute Horizon).",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"mhje4\">The transition to electric vehicles (EVs) is accelerating globally, bringing with it the critical need for a reliable and robust charging infrastructure. While building out more physical charging stations is an important step, an equally important task is maximizing the efficiency of this infrastructure and minimizing \"range anxiety”, a term used to describe an EV driver’s fear of running out of battery before reaching their destination or the nearest available charging station. These concerns led us to design an <a href=\"https://research.google/blog/addressing-range-anxiety-with-smart-electric-vehicle-routing/\">approach for EV routing</a> that reduces range anxiety by integrating charging stations into the navigational route based on the battery level and destination.</p><p data-block-key=\"d0uuh\">This week <a href=\"https://blog.google/products/maps/holiday-gemini-tips-new-explore-tab/\" target=\"_blank\" rel=\"noopener noreferrer\">we announced</a> a new lightweight, highly efficient prediction model that can answer the core question, “<i>What is the probability that an EV charging port will be available at a specific station a certain number of minutes from now?</i>” We found that the most sophisticated model isn't always the best solution. By co-designing the model and the deployment infrastructure, we were able to create a highly effective prediction system based on a simple <a href=\"https://en.wikipedia.org/wiki/Linear_regression\" target=\"_blank\" rel=\"noopener noreferrer\">linear regression</a> approach. This model’s simplicity is its strength, allowing it to rely on easily accessible features while still achieving performance improvements over a strong baseline. Our work demonstrates that combining intuitive real-world logic with machine learning can deliver significant operational and user experience benefits.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Creating the model</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\">Our goal was to maximize predictive power while minimizing the feature set (i.e., the specific, measurable data points the model uses to make a prediction) to ensure speed and low-latency deployment. After testing various architectures, including a decision tree and a simple neural network, a straightforward linear regression model proved to be the most performant and robust for this specific task.</p><p data-block-key=\"73a8o\">We trained the model using real-time availability data from charging networks to calculate the true number of available charging ports within a certain number of minutes from the current observation time using criteria for model features and weights. We uniformly sampled ports from two distinct regions (CA and Germany). Larger stations were more likely to be included in the training set because they see more traffic than isolated ports and more closely reflect real-world usage.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Features</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\">The model uses the hour of the day as a key piece of information (a \"feature\"). It treats each hour (or hour range) separately. For example, \"9 AM\" is one feature, and \"5 PM\" is another.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Weights</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\">The \"weights\" are the specific numerical values that the linear regression algorithm learns during training. These numbers dictate how much each specific hour of the day affects the final prediction.</p><ul><li data-block-key=\"ed5n\">A positive weight means that during that hour (e.g., 7:00 AM), ports tend to get occupied (the occupancy is increasing).</li><li data-block-key=\"cleil\">A negative weight means that during that hour (e.g., 5:00 PM), ports tend to get freed up (the occupancy is decreasing).</li><li data-block-key=\"7cqut\">A zero or near-zero weight means that during that hour (e.g., 3:00 AM), there is little change in port status.</li></ul><p data-block-key=\"91vn8\">These “hour feature weights\" are the model's learned coefficients that quantify the predictable rate of EV port occupancy change for every hour of the day. Essentially, the model learns to express the difference between the current number of available ports and the future number of available ports as a function of the hour feature weights.</p><p data-block-key=\"eebfc\">The feature weights learned for each hour of the day are particularly insightful because they directly represent the rate at which port occupancy changes. As illustrated by the chart below, there are clear, predictable trends tied to driver schedules:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-1-30minWeights.width-1250.png\" alt=\"Plot of feature weights for each hour for the 30 minute horizon.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-1-30minWeights.width-1250.png\" alt=\"Plot of feature weights for each hour for the 30 minute horizon.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"ifwbm\"><i>Feature weights for each hour for the 30 minute horizon. They correspond to the rate at which port occupancy changes at each 30 minute bucket.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-2-60minWeights.width-1250.png\" alt=\"Plot of feature weights for each hour for the 60 minute horizon.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-2-60minWeights.width-1250.png\" alt=\"Plot of feature weights for each hour for the 60 minute horizon.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"ifwbm\"><i>Feature weights for each hour for the 60 minute horizon. They correspond to the rate at which port occupancy changes at each 60 minute bucket.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"mhje4\">Note that the model only differentiates from the current state when the change rate is significant (e.g., rush hour) or the station is large (more ports amplify the predicted change), which are intuitively the correct times to issue an updated prediction.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Experiments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\">Our evaluation was designed to be rigorous and representative of real-world usage. For both the 30-minute and 60-minute time horizons, we evaluated predictions on 100 randomly selected stations, sampling their occupancy status 48 times daily (every 30 minutes) for a full week.</p><p data-block-key=\"6khd7\">The model was benchmarked against a remarkably strong baseline: the \"Keep Current State\" approach. This baseline simply assumes that the number of available ports a certain number of minutes (<i>H</i>) in the future will be exactly the same as the current number.</p><p data-block-key=\"2lqm6\">While simple, this baseline is very hard to beat, especially over short horizons. For example, our data showed that on the US East Coast, never more than 10% of ports change their availability state within a 30-minute block. Since most of the time the state doesn't change, the simplest prediction — no change — is correct most of the time, making the task of adding predictive value extremely difficult.</p><p data-block-key=\"6taqm\">We focused on two key metrics to measure the model’s accuracy for predicting the exact number of free ports: mean squared error (MSE) and mean absolute error (MAE). A ratio of MSE/MAE ≥ 1 free port measures the accuracy of the most critical binary task for the user: “Will I find at least one free port (Yes/No)?”</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\">The evaluation confirmed that the linear regression model provides crucial gains over the strong \"Keep Current State\" baseline, primarily by correctly identifying the infrequent, yet vital, moments of high occupancy turnover.</p><p data-block-key=\"em9tt\">We sampled test instances from among stations with at least 6 ports with horizons of 30 to 60 minutes, a realistic set of cases for charging in urban environments. We evaluated the model for the task of predicting the availability of at least one port in a station. This evaluation focused on the station profile and time of day when the model would differentiate from the baseline, namely large stations at times of significant rates of change.</p><p data-block-key=\"2selt\">The table below presents the fraction of time in which we provide a wrong prediction (which is equivalent to the MAE for this problem) for the times of highest change (8am and 8pm).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-3-ErrorRates.width-1250.png\" alt=\"Table comparing error rates on the availability of at least one free port (30 to 60-Minute Horizon).\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/EVCharging-3-ErrorRates.width-1250.png\" alt=\"Table comparing error rates on the availability of at least one free port (30 to 60-Minute Horizon).\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"ifwbm\"><i>Comparison of error rates on the availability of at least one free port (30 to 60-Minute Horizon).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"mhje4\">In summary, deploying the regression model allows us to reduce the number of bad predictions by approximately 20% in morning peak times and by approximately 40% in evening peak times.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Regional differences</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\">Further examinations revealed that while the shape of the change rate curve (when ports fill vs. when they empty) is similar across regions, the magnitude of the change is distinct enough to warrant separate models. For instance, training the model separately for regions like California and Germany yielded better performance than pooling all data together, suggesting that it’s necessary to account for unique regional EV usage patterns.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\">We have successfully developed and deployed a lightweight, linear regression model that effectively predicts EV charging port availability. By focusing on simplicity, speed, and co-designing the model with the existing infrastructure, we bypassed the complexity and latency associated with more detailed, but often unscalable, approaches.</p><p data-block-key=\"bg92t\">The resulting model provides a crucial predictive advantage over a strong \"Keep Current State\" baseline, particularly during high-traffic periods. This capability translates directly into an improved user experience: reduced anxiety, smarter routing decisions, and a better overall experience that supports the continued growth of electric mobility. Future work will focus on extending the prediction horizons to provide even greater value for long-distance travel planning.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"mhje4\"><i>We thank our collaborators Achir Ramadhan, Sreenivas Gollapudi, Shubham Gupta, Ilya Eyzerman, and Ivan Kuznetsov.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Real-time speech-to-speech translation",
      "link": "https://research.google/blog/real-time-speech-to-speech-translation/",
      "pubDate": "Tue, 18 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-18T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Real-time speech-to-speech translation",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST1.width-1250.png",
          "alt": "RTS2ST1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST2.width-1250.png",
          "alt": "RTS2ST2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST3.width-1250.png",
          "alt": "RTS2ST3",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST4.width-1250.png",
          "alt": "RTS2ST4",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST5.width-1250.png",
          "alt": "RTS2ST5",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"gi1wx\">Real-time communication is an integral part of both our professional and personal lives. When speaking to people remotely across language barriers, it can be difficult to truly connect by just relying on state-of-the-art translated captions, as they lack personality and real-time responsiveness essential for fluid conversation. The arrival of <a href=\"https://research.google/blog/introducing-translatotron-an-end-to-end-speech-to-speech-translation-model\">speech-to-speech translation</a> (S2ST) bridges this gap by directly generating translated audio, leading to more natural communication. Existing speech-to-speech translation systems often incur significant delays (4–5s), tend to accumulate errors, and typically lack personalization.</p><p data-block-key=\"325ls\">Today we describe an innovative end-to-end S2ST model that overcomes these limitations, enabling live translation in the original speaker's voice with only 2 second delay. The novel architecture leverages a streaming framework and, with training on time-synchronized data, significantly reduces the delay between the original input and the translated speech. To support a breadth of languages, we introduce a scalable time-synced data acquisition pipeline that allows us to gradually expand the system to include more languages. This technology has demonstrated its effectiveness through successful deployment in real-time sensitive use cases.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qrznl\">Cascaded S2ST</h2><p data-block-key=\"6p2r8\">Prior real-time speech-to-speech technologies employed a cascaded pipeline of individual processing blocks:</p><ol><li data-block-key=\"6s8ba\">Firstly, the source audio is transcribed to text using <a href=\"https://en.wikipedia.org/wiki/Speech_recognition\" target=\"_blank\" rel=\"noopener noreferrer\">automatic speech recognition</a> (ASR) AI models.</li><li data-block-key=\"fuunf\">Next, the transcribed text is translated word-for-word to the target language using <a href=\"https://en.wikipedia.org/wiki/Machine_translation\" target=\"_blank\" rel=\"noopener noreferrer\">automatic speech translation</a> (AST).</li><li data-block-key=\"9mjqi\">Finally, the translated text is converted back to audio using text-to-speech pipelines (TTS).</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST1_Cascade.width-1250.png\" alt=\"RTS2ST1_Cascade\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST1_Cascade.width-1250.png\" alt=\"RTS2ST1_Cascade\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"b1hy5\"><i>Schematic representation of classic, cascade-style speech-to-speech translation system.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qrznl\">Despite the high quality of the individual cascade components, achieving a seamless, real-time S2ST experience has been challenging due to three primary factors:</p><ol><li data-block-key=\"grol\">Significant delays of 4–5 seconds, forcing turn-based conversations.</li><li data-block-key=\"bbctr\">Accumulated errors at each stage of the translation process.</li><li data-block-key=\"ba4or\">A notable lack of personalization due to the general-purpose TTS technology.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qrznl\">A novel end-to-end, personalized S2ST</h2><p data-block-key=\"3e2vl\">To significantly advance S2ST, we created a scalable data acquisition pipeline and developed an end-to-end model that provides direct, real-time language translation with just a two-second delay:</p><ol><li data-block-key=\"fl20f\"><i>Scalable data acquisition pipeline</i>: We created a data processing pipeline to convert raw audio into a time-synchronized input/target dataset. This was achieved by integrating existing ASR and TTS technologies with precise alignment steps, ensuring translated audio best matches the input. Rigorous filtering and validation were employed to remove difficult-to-align examples.</li><li data-block-key=\"32p9v\"><i>Real-time speech-to-speech translation architecture</i>: We introduced an audio-specific streaming machine learning architecture to support training on this time-synchronized data. Building on the <a href=\"https://research.google/blog/audiolm-a-language-modeling-approach-to-audio-generation/\">AudioLM framework</a> and fundamental transformer blocks, this architecture is designed to handle continuous audio streams, allowing the model to decide when to output translations. It is also structured to manage hierarchical audio representations using the <a href=\"https://arxiv.org/abs/2508.05207\" target=\"_blank\" rel=\"noopener noreferrer\">SpectroStream</a> codec technology.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    <div class=\"rich-text --theme- --mode-\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p style=\"text-align: center;\"><video muted=\"\" controls=\"controls\" width=\"67%\" height=\"67%\"> <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/spanish_english_s2st_viz.mp4\" type=\"video/mp4\"> </video></p>\n<table class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\" cellspacing=\"0\" cellpadding=\"0\" align=\"center\">\n<tbody>\n<tr>\n<td class=\"tr-caption\" style=\"text-align: center;\">An example of our personalized S2ST applied to a Spanish original translated to English.</td>\n</tr>\n</tbody>\n</table>\n</div>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qrznl\">Scalable data acquisition pipeline</h2><p data-block-key=\"eo8vt\">For a given language pair, initial work starts with raw audio acquisition. We utilize a diverse set of audio sources, including data generated by TTS models. This audio undergoes a cleaning and filtering process to ensure it contains a single speaker of the source language and has an appropriate noise level. After initial data collection, an ASR step transcribes the source text. With both source audio and text available, <a href=\"https://www.danielpovey.com/files/htkbook.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">the forced alignment algorithm</a> generates alignment timesteps (audio-to-text mapping). Any audio segments where alignment fails are discarded.</p><p data-block-key=\"6hb40\">The remaining clips are machine translated from the source into the target language. Subsequently, a series of automated filters validate the translated output, ensuring accuracy and correspondence to the input text. Next, the original transcribed and translated texts are also aligned to generate corresponding timestamp annotations (text-to-translated text mapping).</p><p data-block-key=\"baqfg\">Using <a href=\"https://research.google/blog/audiolm-a-language-modeling-approach-to-audio-generation/\">a custom text-to-speech generation engine</a>, the translated text is converted into translated audio, preserving the voice characteristics from the original audio while producing natural-sounding output. The pipeline concludes with one more forced alignment step of the translated text and the generated speech (speech-to-text mapping).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST2_Pipeline.width-1250.png\" alt=\"RTS2ST2_Pipeline\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST2_Pipeline.width-1250.png\" alt=\"RTS2ST2_Pipeline\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"b1hy5\"><i>Streaming audio-to-audio translation dataset generation.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qrznl\">Utilizing the three generated alignments from the previous steps, the overlap between them is calculated, yielding alignment masks between the source and target audio. These alignment masks are then used to guide the loss computation during training.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-vertical-padding --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST3.width-1250.png\" alt=\"RTS2ST3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST3.width-1250.png\" alt=\"RTS2ST3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n\n  \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n    \n\n\n\n\n\n<div class=\"sound-player --theme-light --mode-standalone\" data-gt-id=\"sound_player\" data-gt-component-name=\"component name\">\n    <div class=\"sound-player__wrapper\">\n        \n            \n\n\n<div class=\"sound-player__column\">\n    \n\n\n\n\n    \n\n<audio controls=\"\">\n    \n    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/RealTimeS2ST_es_alignment.wav\" type=\"audio/wav\">\n    Your browser does not support the audio element.\n</audio>\n\n    \n\n    \n</div>\n\n        \n    </div>\n</div>\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-vertical-padding --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST4.width-1250.png\" alt=\"RTS2ST4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST4.width-1250.png\" alt=\"RTS2ST4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n\n  \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n    \n\n\n\n\n\n<div class=\"sound-player --theme-light --mode-standalone\" data-gt-id=\"sound_player\" data-gt-component-name=\"component name\">\n    <div class=\"sound-player__wrapper\">\n        \n            \n\n\n<div class=\"sound-player__column\">\n    \n\n\n\n\n    \n\n<audio controls=\"\">\n    \n    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/RealTimeS2ST_en_alignment.wav\" type=\"audio/wav\">\n    Your browser does not support the audio element.\n</audio>\n\n    \n        <div class=\"caption --center\">\n            <p data-block-key=\"80ok3\"><i>Text alignment of input and translated audio with corresponding overlaps.</i></p>\n        </div>\n    \n\n    \n</div>\n\n        \n    </div>\n</div>\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"gi1wx\">Invalid overlaps or translations that fail to meet delay requirements are filtered from the training dataset. The remaining aligned data is used to train the streaming S2ST model in chunks of up to 60 seconds. Various audio augmentation techniques are also applied during training, including sample rate reduction, reverberation, saturation, and denoising.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qrznl\">Real-time speech-to-speech translation architecture</h2><p data-block-key=\"38c83\">The end-to-end S2ST model leverages fundamental transformer blocks and consists of two main components:</p><ul><li data-block-key=\"5ra6j\"><i>Streaming encoder:</i> Summarizes source audio data based on the preceding 10 seconds of input.</li><li data-block-key=\"moc\"><i>Streaming decoder:</i> Predicts translated audio autoregressively, using the compressed encoder state and predictions from previous iterations.</li></ul><p data-block-key=\"2g26o\">A feature of these models is their representation of audio as a 2D set of tokens, known as <a href=\"https://arxiv.org/abs/2107.03312\" target=\"_blank\" rel=\"noopener noreferrer\">RVQ audio tokens</a>. As shown below, the X-axis represents time, while the Y-axis represents a set of tokens that describe the current audio segment. When summed, all tokens in a specific set can be readily converted into an audio stream using an ML codec. The number of tokens controls the audio quality for every segment, with more tokens yielding higher fidelity. The model predicts tokens sequentially, prioritizing those at the beginning. Typically, 16 tokens are sufficient for high-quality audio representation of a 100 ms chunk.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST5.width-1250.png\" alt=\"RTS2ST5\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RTS2ST5.width-1250.png\" alt=\"RTS2ST5\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"b1hy5\"><i>Schematic representation of audio-to-audio streaming inference for S2ST.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qrznl\">The model outputs a single text token in addition to the audio tokens. This text token acts as an extra prior for audio generation and enables direct metric calculation (<a href=\"https://cloud.google.com/translate/docs/advanced/automl-evaluate#bleu\" target=\"_blank\" rel=\"noopener noreferrer\">BLEU</a>) without relying on proxy ASR systems.</p><p data-block-key=\"dh8dl\">During training, a per-token loss is applied to the model to ensure accurate translation. The model's prediction delay, or lookahead, can be adjusted by shifting ground truth tokens to the right, allowing for flexibility based on the target language's complexity. For real-time conversations, a standard 2-second delay is typically used, which is suitable for most languages. While a longer lookahead improves translation quality by providing more context, it negatively impacts the real-time communication experience.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RealTimeS2ST6_Ablation.width-1250.png\" alt=\"RealTimeS2ST6_Ablation\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RealTimeS2ST6_Ablation.width-1250.png\" alt=\"RealTimeS2ST6_Ablation\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"b1hy5\"><i>Ablation of lookahead and corresponding quality of translation for Spanish / English language pair (BLEU, higher is better).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qrznl\">In addition to the internal 2 second delay, the model's inference time contributes to the overall system latency. To minimize this and achieve real-time performance, we implemented several optimization techniques, including hybrid low-bit (<a href=\"https://blog.tensorflow.org/2024/04/faster-dynamically-quantized-inference-with-xnnpack.html\" target=\"_blank\" rel=\"noopener noreferrer\">int8 and int4</a>) quantization and optimized <a href=\"https://arxiv.org/abs/2207.12598\" target=\"_blank\" rel=\"noopener noreferrer\">CFG</a> precomputation.</p><p data-block-key=\"bv7i0\">The examples of translation for different language pairs with developed models with corresponding ground truth (taken from publicly available <a href=\"https://arxiv.org/abs/2201.03713\" target=\"_blank\" rel=\"noopener noreferrer\">CVSS dataset</a>) follow:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    <div class=\"rich-text --theme- --mode-\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <table style=\"border-collapse: collapse; width: 100%;\" border=\"0\">\n<tbody>\n<tr>\n<td style=\"width: 15%;\">\n<p><strong>Direction</strong></p>\n</td>\n<td style=\"width: 24.973%; text-align: center;\">\n<p><strong>Input audio</strong></p>\n</td>\n<td style=\"width: 24.973%; text-align: center;\">\n<p><strong>Translated audio</strong></p>\n</td>\n<td style=\"width: 34%; text-align: center;\">\n<p><strong>Ground truth</strong></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">Spanish to English</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/es_en_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/es_en_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">In coastal areas there is a larger accumulation of water molecules in the air.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">English to Spanish</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_es_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_es_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">Su portaaviones insignia, el Portaviones Formidable fue tocado por un kamikaze sin gravísimas consecuencias.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">German to English</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/de_en_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/de_en_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">The electrician used a piece of aluminum foil to splice the fuse.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\"> English to German</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_de_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_de_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">Margaret versucht mit allen Mitteln, die bevorstehende Katastrophe zu verhindern.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">Italian to English</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/it_en_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/it_en_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">By clicking on the right, on the bell icon, you can activate Pash notifications so that they are updated in real time.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">English to Italian</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_it_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_it_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">La voce popolare parla dei proprietari terrieri, dei mafiosi e dei rappresentanti del Partito conservatore e dei loro nomi, che sono noti a tutti.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">Portuguese to English</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/pt_en_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/pt_en_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">This text is made available under the respective license.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">English to Portuguese</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_pt_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_pt_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">Um homem de camisa azul observa algo projetado na frente dele.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">French to English</span></p>\n</td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/fr_en_source.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 24.973%;\"><audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/fr_en_translation.wav\" controls=\"controls\"></audio></td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">The volunteer firefighters are a full part of our civil security arsenal.</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"width: 15%;\">\n<p><span style=\"font-weight: 400;\">English to French</span></p>\n</td>\n<td style=\"width: 24.973%;\">\n<p><span style=\"font-weight: 400;\">&nbsp;<audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_fr_source.wav\" controls=\"controls\"></audio></span></p>\n</td>\n<td style=\"width: 24.973%;\">\n<p><span style=\"font-weight: 400;\">&nbsp;<audio src=\"https://storage.googleapis.com/gweb-research2023-media/media/en_fr_translation.wav\" controls=\"controls\"></audio></span></p>\n</td>\n<td style=\"width: 34%;\">\n<p><span style=\"font-weight: 400;\">Ce gouvernement et cette majorité portent donc seuls la responsabilité de cette situation.</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n<table class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\" cellspacing=\"0\" cellpadding=\"0\" align=\"center\">\n<tbody>\n<tr>\n<td class=\"tr-caption\" style=\"text-align: center;\">Exemplar bidirectional translation generated by the trained models for English, Spanish, German, Italian, Portuguese, and French languages with corresponding ground truth.</td>\n</tr>\n</tbody>\n</table>\n</div>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qrznl\">Real-world applications</h2><p data-block-key=\"e3d53\">The new end-to-end S2ST technology has been launched in two key areas, highlighting the importance of real-time cross-language communication. It is now available in <a href=\"https://blog.google/products/workspace/google-meet-langauge-translation-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Meet</a> on servers, and as a built-in <a href=\"https://store.google.com/intl/en/ideas/articles/pixel-live-translate/\" target=\"_blank\" rel=\"noopener noreferrer\">on-device feature</a> for the new Pixel 10 devices. Although the products utilize different strategies for running the S2ST pipeline, they share training data and model architecture. The Pixel Voice Translate on-device feature also employs a cascade approach to maximize language coverage. To mitigate potential feature misuse, prior to each translation session, we inform the end-user that the translation is synthetically generated.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"hyXqcsWOONo\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=hyXqcsWOONo\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"5y2ft\"><i>The new end-to-end S2ST technology enables Google Meet speech translation feature.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"k3ba8\">The current end-to-end model delivers robust performance for five Latin-based language pairs (English to and from Spanish, German, French, Italian, Portuguese), enabling our initial product launches. We are also observing promising capabilities in other languages, such as Hindi, that we plan to develop further. Future enhancements will focus on improving the dynamism of the model's lookahead. This will enable the S2ST technology to seamlessly adjust to languages with word orders significantly different from English, facilitating more contextual rather than literal word-for-word translation.</p><p data-block-key=\"e0dg1\">We believe that this breakthrough in S2ST technology will revolutionize real-time, cross-language communication, turning a long-envisioned concept into reality.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"k3ba8\">Acknowledgements</h2><p data-block-key=\"f31cm\"><i>We are sincerely grateful to everyone who contributed to this project; their critical contributions were instrumental in making it a reality. We are particularly thankful to our colleagues Kevin Kilgour, Pen Li, Félix de Chaumont Quitry, Michael Dooley, Jeremy Thorpe, Mihajlo Velimirović, Alex Tudor, Christian Frank, Daniel Johansson, Hanna Silén, Christian Schuldt, Henrik Lundin, Esbjörn Dominique, Marcus Wirebrand, Daniel Kallander, Pablo Barrera González, Huib Kleinhout, Niklas Blum, Fredric Lindstrom, Esha Uboweja, Karthik Raveendran, Frédéric Rechtenstein, Xing Li, Queenie Zhang, Cheng Yang, Jason Fan, Matsvei Zhdanovich, Jianing Wei, and Matthias Grundmann.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Generative UI: A rich, custom, visual interactive user experience for any prompt",
      "link": "https://research.google/blog/generative-ui-a-rich-custom-visual-interactive-user-experience-for-any-prompt/",
      "pubDate": "Mon, 17 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-17T16:00:00.000Z",
      "creator": "Google",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Generative UI: A rich, custom, visual interactive user experience for any prompt",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GenUI-0b-Hero.width-1250.png",
          "alt": "A collage showing three different AI-generated user interfaces. They include a page for \"Tailored Fashion Advice\", a science education screen featuring 'Fractals', and a children's learning game interface with math practice options.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GenUI-4-Overview.width-1250.png",
          "alt": "Flowchart illustrating the process for generating a web page. The LLM takes input from the user prompt taking into account underlying system instructions, and utilizes Tools where needed. It outputs HTML/CSS/JS to the user's browser.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GenUI-0-Hero.width-1250.png",
          "alt": "Three teal-themed web design concepts displayed side-by-side: \"Clash of the Titans,\" \"Your Perfect Pizza Night,\" and \"Fabulous Flamingo Flair.\"",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"j0qlf\">Generative UI is a powerful capability in which an AI model generates not only content but an entire user experience. Today we introduce a novel implementation of generative UI, which dynamically creates immersive visual experiences and interactive interfaces — such as web pages, games, tools, and applications — that are automatically designed and fully customized in response to any question, instruction, or prompt. These prompts can be as simple as a single word, or as long as needed for detailed instructions. These new types of interfaces are markedly different from the static, predefined interfaces in which AI models typically render content.</p><p data-block-key=\"f5456\">In our new paper, “<a href=\"https://generativeui.github.io/static/pdfs/paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Generative UI: LLMs are Effective UI Generators</a>”, we describe the core principles that enabled our implementation of generative UI and demonstrate the effective viability of this new paradigm. Our evaluations indicate that, when ignoring generation speed, the interfaces from our generative UI implementations are strongly preferred by human raters compared to standard LLM outputs. This work represents a first step toward fully AI-generated user experiences, where users automatically get dynamic interfaces tailored to their needs, rather than having to select from an existing catalog of applications.</p><p data-block-key=\"9kn4n\">Our research on generative UI, also referred to as generative interfaces, comes to life today in the <a href=\"https://blog.google/products/gemini/gemini-3-gemini-app\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini app</a> through an experiment called dynamic view and in AI Mode in <a href=\"http://blog.google/products/search/gemini-3-search-ai-mode\" target=\"_blank\" rel=\"noopener noreferrer\">Google Search</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GenUI-0b-Hero.width-1250.png\" alt=\"A collage showing three different AI-generated user interfaces. They include a page for &quot;Tailored Fashion Advice&quot;, a science education screen featuring 'Fractals', and a children's learning game interface with math practice options.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GenUI-0b-Hero.width-1250.png\" alt=\"A collage showing three different AI-generated user interfaces. They include a page for &quot;Tailored Fashion Advice&quot;, a science education screen featuring 'Fractals', and a children's learning game interface with math practice options.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"do5os\"><i>Generative UI is useful for a range of applications. For any user question, need, or prompt, as simple as a single word or as complex as elaborate instructions, the model creates a fully custom interface.</i> <b><i>Left:</i></b> <a href=\"https://generativeui.github.io/static/demos/carousel.html?result=fashion-advisor\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Getting tailored fashion advice</i></a><i>.</i> <b><i>Middle:</i></b> <a href=\"https://generativeui.github.io/static/demos/carousel.html?result=fractal-explorer\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Learning about fractals</i></a><i>.</i> <b><i>Right:</i></b> <a href=\"https://generativeui.github.io/static/demos/carousel.html?result=basketball-math\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Teaching mathematics</i></a><i>.</i></p><p data-block-key=\"4ukk0\"><i>For more examples see the</i> <a href=\"https://generativeui.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>project page</i></a><i>.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Bringing generative UI to Google products</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"j0qlf\">Generative UI capabilities will be rolled out as two experiments in the <a href=\"https://blog.google/products/gemini/gemini-3-gemini-app\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini app</a>: dynamic view and visual layout. When using dynamic view, an experience built upon our generative UI implementation, Gemini designs and codes a fully customized interactive response for each prompt, using Gemini’s agentic coding capabilities. It customizes the experience with an understanding that explaining the microbiome to a 5 year old requires different content and a different set of features than explaining it to an adult, just as creating a gallery of social media posts for a business requires a completely different interface to generating a plan for an upcoming trip.</p><p data-block-key=\"cvpbe\">Dynamic view can be used for a wide range of scenarios, from learning about <a href=\"https://generativeui.github.io/static/demos/carousel.html?result=rolling-an-8\" target=\"_blank\" rel=\"noopener noreferrer\">probability</a> to helping in practical tasks like <a href=\"https://generativeui.github.io/static/demos/carousel.html?result=thanksgiving\" target=\"_blank\" rel=\"noopener noreferrer\">event planning</a> and getting <a href=\"https://generativeui.github.io/static/demos/carousel.html?result=fashion-advisor\" target=\"_blank\" rel=\"noopener noreferrer\">fashion advice</a>. The interfaces allow users to learn, play or explore interactively. Dynamic view, along with visual layout, are rolling out today. To help us learn about these experiments, users may initially see only one of them.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Dynamic_View_Van_Gogh_1920x1080.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"vu0ul\">Example of generative UI in dynamic view based on the prompt, “Create a Van Gogh gallery with life context for each piece”.</p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"j0qlf\">Generative UI experiences are also integrated into <a href=\"http://blog.google/products/search/gemini-3-search-ai-mode\" target=\"_blank\" rel=\"noopener noreferrer\">Google Search</a> starting with AI Mode, unlocking dynamic visual experiences with interactive tools and simulations that are generated specifically for a user’s question. Now, thanks to Gemini 3’s unparalleled multimodal understanding and powerful agentic coding capabilities, Gemini 3 in AI Mode can interpret the intent behind any prompt to instantly build bespoke generative user interfaces. By generating interactive tools and simulations on the fly, it creates a dynamic environment optimized for deep comprehension and task completion. Generative UI capabilities in AI Mode are available for Google AI Pro and Ultra subscribers in the U.S. starting today. Select \"Thinking\" from the model drop-down menu in AI Mode to try it out.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/AIM-CAPYBARA-RNA-1920x1080-Under20MB.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"3jqbs\">Example of AI Mode in Google Search with the prompt, “show me how rna polymerase works. what are the stages of transcription and how is it different in prokaryotic and eukaryotic cells”.</p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How the generative UI implementation works</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"j0qlf\">Our generative UI implementation, described in the <a href=\"https://generativeui.github.io/static/pdfs/paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>, uses Google’s Gemini 3 Pro model with three important additions:</p><ol><li data-block-key=\"appno\"><i>Tool access</i>: A server provides access to several key tools, like image generation and web search. This allows the results to be made accessible to the model to increase quality or sent directly to the user’s browser to improve efficiency.</li><li data-block-key=\"br80m\"><i>Carefully crafted system instructions</i>: The system is guided by detailed instructions that include the goal, planning, examples and technical specifications, including formatting, tool manuals, and tips for avoiding common errors.</li><li data-block-key=\"cqq2h\"><i>Post-processing</i>: The model’s outputs are passed through a set of post-processors to address potential common issues.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GenUI-4-Overview.width-1250.png\" alt=\"Flowchart illustrating the process for generating a web page. The LLM takes input from the user prompt taking into account underlying system instructions, and utilizes Tools where needed. It outputs HTML/CSS/JS to the user's browser.\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GenUI-4-Overview.width-1250.png\" alt=\"Flowchart illustrating the process for generating a web page. The LLM takes input from the user prompt taking into account underlying system instructions, and utilizes Tools where needed. It outputs HTML/CSS/JS to the user's browser.\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"do5os\"><i>A high-level system overview of the generative UI implementation.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"j0qlf\">For some products, it might be preferable to consistently see results in specific styles. Our implementation could be configured for these products so that all results, including generated assets, are created in a consistent style for all users. Without specific styling instructions, the generative UI will select a style automatically, or the user can influence styling in their prompt, as in the case of dynamic view in the Gemini app.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GenUI-0-Hero.width-1250.png\" alt=\"Three teal-themed web design concepts displayed side-by-side: &quot;Clash of the Titans,&quot; &quot;Your Perfect Pizza Night,&quot; and &quot;Fabulous Flamingo Flair.&quot;\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GenUI-0-Hero.width-1250.png\" alt=\"Three teal-themed web design concepts displayed side-by-side: &quot;Clash of the Titans,&quot; &quot;Your Perfect Pizza Night,&quot; and &quot;Fabulous Flamingo Flair.&quot;\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"do5os\"><i>Screenshots of generative UI results with consistent “Wizard Green” styling.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Generative UI outputs are strongly preferred over standard formats</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"j0qlf\">To facilitate consistent evaluations and comparisons of generative UI implementations, we created PAGEN, a dataset of human expert–made websites and will soon be releasing it to the research community.</p><p data-block-key=\"7bgiq\">To evaluate user preferences, we compared our new generative UI experience against various different formats: a website designed for a specific prompt by human-experts, the top Google Search result for the query, and baseline LLM outputs in raw text or the standard markdown formats.</p><p data-block-key=\"64aut\">The sites designed by human experts had the highest preference rates. These were followed closely by the results from our generative UI implementation, with a substantial gap from all other output methods. This evaluation did not take into account generation speed. We also show that the performance of generative UI strongly depends on the performance of the underlying model, and that our newest models perform substantially better. See more details in the <a href=\"https://generativeui.github.io/static/pdfs/paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Opportunities ahead</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"j0qlf\">We are still in the early days of generative UI, and important opportunities for improvement remain. For example, our current implementation can sometimes take a minute or more to generate results, and there are occasional inaccuracies in the outputs; these are areas of ongoing research. Generative UI is an example of the <a href=\"https://blog.google/technology/research/google-research-team-tackles-big-challenges-with-science/\" target=\"_blank\" rel=\"noopener noreferrer\">magic cycle of research</a>, where research breakthroughs lead to product innovation that opens up new opportunities for addressing user needs and in turn fuel further research. We see potential in extending generative UI to access a wider set of services, adapt to additional context and human feedback, and deliver increasingly more helpful visual and interactive interfaces. We are excited about the further opportunities ahead for generative UI.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "利用人工智能区分天然林与其他林木覆盖，助力无砍伐供应链 (原标题: Separating natural forests from other tree cover with AI for deforestation-free supply chains)",
      "link": "https://research.google/blog/separating-natural-forests-from-other-tree-cover-with-ai-for-deforestation-free-supply-chains/",
      "pubDate": "Wed, 12 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-12T16:00:00.000Z",
      "creator": "Google",
      "summary": "森林对地球至关重要，它们调节降雨、缓解洪水、储存碳并维持大部分陆地物种。然而，森林砍伐仍在以惊人的速度进行。在保护工作中，一个关键挑战是利用卫星数据区分拥有数百年历史的天然生态系统与新种植的森林或林木作物种植园。大多数现有地图仅显示“林木覆盖”，这是一种对任何木本植被的基本测量，导致“混淆视听”，将短期种植园的采伐与不可替代、生物多样性丰富的天然林的永久性损失混为一谈。\n\n### 区分天然林的重要性\n\n对天然林进行区分比以往任何时候都更加重要，原因如下：\n\n*   **新全球法规**：例如欧盟无砍伐产品法规（EUDR），该法规要求在欧盟销售的咖啡、可可、橡胶、木材和棕榈油等产品不得来自2020年12月31日之后被砍伐或退化的土地。其目标是保护原始森林和自然再生林等天然林。\n*   **政策需求**：这项政策催生了对2020年天然林可靠、高分辨率和全球一致地图的需求。\n*   **气候共识**：保护这些森林也是COP30的核心支柱，会议认识到它们在气候稳定和人类福祉中的关键作用。\n\n### “2020年世界天然林”地图\n\n为了满足这一需求，Google DeepMind与Google Research合作，并与世界资源研究所（WRI）和国际应用系统分析研究所（IIASA）协作，发布了《2020年世界天然林》这一新的地图和数据集，并发表在《自然科学数据》杂志上。该项目为森林砍伐和退化监测提供了关键基线。\n\n*   **核心特点**：这是首个全球一致的10米分辨率地图，能够将天然林与其他人造林木覆盖区分开来。\n*   **准确性**：经全球独立数据集验证，该地图达到了92.2%的同类最佳准确率。\n*   **用途**：我们希望这一公开可用的基线能帮助企业进行尽职调查，支持政府监测森林砍伐，并赋能保护组织将其努力集中于保护最重要的区域。\n\n![天然林与人工林](https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-1.width-1250.png)\n\n*   Gemini生成的图像显示天然林（左）与人工林（右）接壤。全球卫星模型难以区分它们，使保护生物多样性更丰富的天然林的工作复杂化。\n\n![2020年全球天然林分布](https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-2.width-1250.png)\n\n*   2020年全球天然林的范围（原始分辨率为10米）。\n\n### AI如何区分森林类型\n\n利用单一卫星图像区分天然林与复杂的农林复合系统或拥有50年历史的人工林非常困难。为了克服这一挑战，我们开发了一个AI模型，它像林业专家一样，观察一块土地一年内的变化，分割1280 x 1280米的区域，并估计其中每个10 x 10米像素是天然林的可能性。这使得模型能够根据周围环境而不是单一快照进行评估。\n\n*   **模型技术**：这种新颖的多模态时空视觉Transformer（MTSViT）模型分析季节性Sentinel-2卫星图像和地形数据（例如海拔和坡度），以及样本的地理坐标。\n*   **识别机制**：通过观察卫星图像随时间的变化，模型识别出独特的谱、时序和纹理特征（即用于识别不同森林类型的数据模式），从而区分复杂的天然林与均匀、快速生长的商业种植园以及其他土地利用和土地覆盖类型。\n\n### 地图生成流程\n\n为了构建《2020年世界天然林》地图，我们采样了全球超过120万个1280 x 1280米、10米分辨率的区域，创建了一个大规模、多源的训练数据集。我们利用这些数据训练MTSViT模型识别天然林和其他土地类型的复杂模式。然后，我们将训练好的MTSViT模型应用于地球上所有陆地，生成了一张无缝、全球一致的10米概率地图。为了严格验证地图，我们通过重新利用一个专注于2015年全球森林管理的独立数据集，并将其标签更新至2020年，创建了一个评估数据集。\n\n![天然林地图生成工作流程](https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-3.width-1250.png)\n\n*   天然林地图生成的端到端工作流程（标注数据生成、处理、模型训练、地图生成和验证步骤）。\n\n### 未来展望：森林理解的新愿景\n\n我们希望《2020年世界天然林》基线能成为政策制定者、审计师和寻求遵守EUDR等新无砍伐法规的公司宝贵资源。但森林并非静态。为了真正支持全球保护和可持续发展，我们需要区分更多类别的森林，并关键地了解它们如何随时间变化。这包括区分和定位关键森林类型：天然林（碳密集和生物多样性丰富的森林）、人工林、种植园和商业林木作物（如生态友好的咖啡和可可农林复合系统）。\n\n*   **新一代地图**：为了推进这项工作，我们正在开发一系列新的多年度全球森林类型地图，由下一代AI模型驱动。这些地图将把世界陆地分为六种不同类型：原始森林、自然再生林、人工林、种植园林、林木作物和其他土地覆盖。我们预计在2026年发布这些综合地图。\n*   **开放数据集**：为了鼓励更广泛的研究社区为此努力做出贡献，我们还发布了两个大规模基准数据集：\n    *   **Planted数据集**：一个全球、多传感器、长时间序列的集合，包含超过230万个时间序列分类示例。它专门设计用于帮助AI模型识别全球64种不同（物种或属）类型的人工林和林木作物。\n    *   **Forest Typology (ForTy) 基准**：提供了一个真正全球规模的数据集，包含20万个多源、多时序图像块，带有像素级标签，用于核心任务（天然林、人工林和林木作物）的语义分割。\n\n### 助力保护地球\n\n将气候和自然雄心转化为行动需要透明、可信和高分辨率的数据。我们致力于使这些工具尽可能易于获取。我们希望这些新的数据集和工具能帮助政府、企业和社区共同努力，实现其无砍伐目标，并保护我们赖以生存的关键生态系统。\n\n**致谢**：这项研究由Google Deepmind和Google Research与WRI和IIASA共同开发。",
      "shortSummary": "Google DeepMind等机构发布了“2020年世界天然林”地图，利用AI技术以10米分辨率在全球范围内区分天然林与其他人造林木覆盖。该地图准确率达92.2%，旨在为企业遵守欧盟无砍伐产品法规（EUDR）等新规提供关键基线数据。通过分析卫星图像和地形数据，AI模型能识别天然林的复杂模式。未来将推出更详细的森林类型地图和开放数据集，以促进研究，共同推动全球森林保护和可持续发展。",
      "translated_title": "利用人工智能区分天然林与其他林木覆盖，助力无砍伐供应链",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-1.width-1250.png",
          "alt": "Natural Forests-1",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-2.width-1250.png",
          "alt": "Natural Forests-2",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-3.width-1250.png",
          "alt": "Natural Forests-3",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"1h9b3\">Forests are vital for our planet as they regulate rainfall, mitigate floods, store and sequester carbon, and help sustain the majority of the <a href=\"https://zenodo.org/records/6417333\" target=\"_blank\" rel=\"noopener noreferrer\">planet’s land-based species</a>. Despite their importance, deforestation continues at an alarming rate. A key challenge in conservation efforts is differentiating centuries-old natural ecosystems from newly planted forests or tree crop plantations with satellite data. Most existing maps simply show \"tree cover,\" a basic measure of any woody vegetation, leading to an \"apples-to-oranges\" comparison. This conflates the harvesting of a short-term plantation with the permanent loss of an irreplaceable, biodiversity-rich natural forest.</p><p data-block-key=\"dg5d6\">The need for this distinction is more important than ever due to new global regulations, like the <a href=\"https://environment.ec.europa.eu/topics/forests/deforestation/regulation-deforestation-free-products_en\" target=\"_blank\" rel=\"noopener noreferrer\">European Union Regulation on Deforestation-free Products</a> (EUDR). This regulation mandates that products like coffee, cocoa, rubber, timber, and palm oil sold in the EU cannot come from land that was deforested or degraded after December 31, 2020, with the goal of protecting natural forests, like primary and naturally regenerating forests. This policy creates a need for a reliable, high-resolution, and globally-consistent map of natural forests as they existed in 2020. The protection of these forests is also a central pillar for <a href=\"https://unfccc.int/cop30\" target=\"_blank\" rel=\"noopener noreferrer\">COP30</a>, which recognizes their crucial role in climate stability and human well-being.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-1.width-1250.png\" alt=\"Natural Forests-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-1.width-1250.png\" alt=\"Natural Forests-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"2ra3h\"><i>Gemini generated image showing natural forest (</i><b><i>left</i></b><i>) bordering a planted forest (</i><b><i>right</i></b><i>). Global satellite-based models struggle to distinguish between them, complicating efforts to protect the more biodiversity-rich natural forest.</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"1h9b3\">In an effort to help meet this need, together with <a href=\"https://deepmind.google/\" target=\"_blank\" rel=\"noopener noreferrer\">Google DeepMind</a>, we’re releasing <a href=\"https://plus.figshare.com/articles/dataset/Natural_forests_of_the_world_2020_-_probability_maps/28881731\" target=\"_blank\" rel=\"noopener noreferrer\">Natural Forests of the World 2020</a>, a new map and dataset, published in <a href=\"https://www.doi.org/10.1038/s41597-025-06097-z\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature Scientific Data</i></a>. This project stems from a collaboration with the <a href=\"https://www.wri.org/\" target=\"_blank\" rel=\"noopener noreferrer\">World Resources Institute</a> and the <a href=\"https://iiasa.ac.at/\" target=\"_blank\" rel=\"noopener noreferrer\">International Institute for Applied Systems Analysis</a>, and provides a critical baseline for deforestation and degradation monitoring. We provide the first globally consistent, <a href=\"https://developers.google.com/earth-engine/datasets/catalog/projects_nature-trace_assets_forest_typology_natural_forest_2020_v1_0_collection\" target=\"_blank\" rel=\"noopener noreferrer\">10-meter resolution map</a> that differentiates natural forests from other tree cover and achieves a best-in-class accuracy of 92.2% when validated against a <a href=\"https://figshare.com/articles/dataset/Natural_forests_of_the_world_2020_-_validation_dataset/30051517?file=57664081\" target=\"_blank\" rel=\"noopener noreferrer\">global independent dataset</a>. We hope that this publicly available baseline can help companies conduct due diligence, support governments in monitoring deforestation, and empower conservation groups to target their efforts to protect what matters most.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-2.width-1250.png\" alt=\"Natural Forests-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-2.width-1250.png\" alt=\"Natural Forests-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"2ra3h\"><i>The global extent of natural forests in 2020 (originally at 10-meter resolution).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How AI can separate the forest from the trees</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1h9b3\">Distinguishing a natural forest from a complex agroforestry system or a 50-year-old planted forest is difficult using a single satellite image. To overcome this, we developed an AI model that acts like a forester, observing a patch of land over the course of a year, segmenting a 1280 x 1280 meter patch and estimating the likelihood that each 10 x 10 meter pixel within it is a natural forest. This allows the model to make assessments based on the surrounding context, rather than a single snapshot. This novel <i>multi-modal temporal-spatial</i> <a href=\"https://en.wikipedia.org/wiki/Vision_transformer\" target=\"_blank\" rel=\"noopener noreferrer\"><i>vision transformer</i></a> (MTSViT) model analyzes seasonal <a href=\"https://sentinels.copernicus.eu/web/sentinel/copernicus/sentinel-2\" target=\"_blank\" rel=\"noopener noreferrer\">Sentinel-2</a> satellite imagery and topographical data (e.g., elevation and slope), along with the sample’s geographical coordinate. By observing satellite imagery over time, the model identifies distinct spectral, temporal, and texture signatures (i.e., data patterns used to recognize different forest types) that differentiate complex, natural forests from uniform, fast-growing commercial plantations and other land use and land cover.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video  \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/natural_forests_video_im_560p_lq.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n\n        \n\n<div class=\"glue-ambient-video__button-2 glue-ambient-video__button--mute-unmute\" aria-label=\"Video Mute/Unmute\">\n    <div class=\"glue-ambient-video__tooltip glue-ambient-video__tooltip--mute\">\n        <span class=\"glue-ambient-video__tooltip-unmute glue-label\">unmute video</span>\n        <span class=\"glue-ambient-video__tooltip-mute glue-label\">mute video</span>\n    </div>\n    <div class=\"glue-ambient-video__icon glue-ambient-video__icon-sound\">\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" fill=\"#FFF\" class=\"glue-ambient-video__icon-mute\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M16.5 12c0-1.77-1.02-3.29-2.5-4.03v2.21l2.45 2.45c.03-.2.05-.41.05-.63zm2.5 0c0 .94-.2 1.82-.54 2.64l1.51 1.51C20.63 14.91 21 13.5 21 12c0-4.28-2.99-7.86-7-8.77v2.06c2.89.86 5 3.54 5 6.71zM4.27 3L3 4.27 7.73 9H3v6h4l5 5v-6.73l4.25 4.25c-.67.52-1.42.93-2.25 1.18v2.06c1.38-.31 2.63-.95 3.69-1.81L19.73 21 21 19.73l-9-9L4.27 3zM12 4L9.91 6.09 12 8.18V4z\"></path>\n        </svg>\n        <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"18px\" viewBox=\"0 0 24 24\" width=\"18px\" class=\"glue-ambient-video__icon-unmute\" fill=\"#FFF\">\n            <path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n            <path d=\"M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z\"></path>\n        </svg>\n    </div>\n</div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"z7b76\">To build the Natural Forests of the World 2020 map, we sampled over 1.2 million global 1280 x 1280 meter patch locations at 10-meter resolution to create a massive, multi-source training dataset. We used this data to train the MTSViT model to recognize complex patterns of natural forests and other land types. We then applied the trained MTSViT model across all land on Earth, generating a seamless, globally consistent 10-meter probability map. To rigorously validate the map, we created an <a href=\"https://figshare.com/articles/dataset/Natural_forests_of_the_world_2020_-_validation_dataset/30051517?file=57664081\" target=\"_blank\" rel=\"noopener noreferrer\">evaluation dataset</a> by repurposing an <a href=\"https://www.nature.com/articles/s41597-022-01332-3\" target=\"_blank\" rel=\"noopener noreferrer\">independent dataset</a> focused on global forest management for 2015 and updating its labels to focus on natural forests for 2020. See more details in the <a href=\"https://doi.org/10.1038/s41597-025-06097-z\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-3.width-1250.png\" alt=\"Natural Forests-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Natural_Forests-3.width-1250.png\" alt=\"Natural Forests-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n        \n          <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n          \n          <p data-block-key=\"2ra3h\"><i>End-to-end workflow of the Natural Forests map generation (annotating data generation, processing, model training, map generation, and validation steps).</i></p>\n        </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">What's next: A new vision for forest understanding</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1h9b3\">We hope that the Natural Forests of the World 2020 baseline proves to be a valuable resource for policymakers, auditors, and companies seeking to comply with new deforestation-free regulations such as the EUDR. But forests are not static. To truly support global conservation and sustainability, we need to distinguish between more classes of forest and, crucially, understand how they change over time. This involves differentiating between and locating key forest types: natural forests (carbon-dense and biodiversity-rich forests), planted forests, plantations, and commercial tree crops (such as ecosystem-friendly coffee and cocoa agroforestry systems).</p><p data-block-key=\"e8q12\">To advance this effort, we’re developing a new multi-year series of global forest type maps, powered by next-generation AI models. These maps will categorize the world's land into six distinct types: Primary Forest, Naturally Regenerating Forest, Planted Forest, Plantation Forest, Tree Crops, and Other Land Cover. We expect to release these comprehensive maps in 2026.</p><p data-block-key=\"2b9sn\">To encourage the broader research community to contribute to this effort, we have also released two large-scale benchmark datasets. These datasets are important for developing and rigorously testing the next generation of AI models designed to analyze the world’s forests. The <a href=\"https://arxiv.org/abs/2406.18554\" target=\"_blank\" rel=\"noopener noreferrer\">Planted dataset</a> is a global, multi-sensor long-temporal collection featuring over 2.3 million time-series classification examples. It is specifically designed to help AI models recognize 64 different (species or genera) types of planted forests and tree crops worldwide. The <a href=\"https://arxiv.org/abs/2505.01805\" target=\"_blank\" rel=\"noopener noreferrer\">Forest Typology</a> (ForTy) benchmark provides a truly global-scale dataset with 200,000 multi-source and multi-temporal image patches with per-pixel labels for semantic segmentation models. This resource is tailored for the core task of mapping the key classes: natural forest, planted forest, and tree crops.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Helping to protect our planet</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1h9b3\">Turning climate and nature ambitions into action requires transparent, trusted, and high-resolution data. We are committed to making these tools as accessible as possible. We hope these new datasets and tools will help governments, companies, and communities work together to meet their deforestation-free goals and protect the critical ecosystems on which we all depend.</p><p data-block-key=\"159ar\">Learn more about our AI and sustainability efforts by checking out <a href=\"https://blog.google/technology/research/new-updates-and-more-access-to-google-earth-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Earth AI</a>, <a href=\"https://cloud.google.com/blog/topics/sustainability/look-back-at-a-year-of-earth-engine-advancements\" target=\"_blank\" rel=\"noopener noreferrer\">Google Earth Engine</a>, and <a href=\"https://deepmind.google/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/\" target=\"_blank\" rel=\"noopener noreferrer\">AlphaEarth Foundations</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"1h9b3\"><i>This research was co-developed by Google Deepmind and Google Research in collaboration with WRI and IIASA.</i></p><p data-block-key=\"1fca\"><i>We thank our collaborators at Google, World Resources Institute (WRI) / Global Forest Watch (GFW), and International Institute for Applied Systems Analysis (IIASA): Anton Raichuk, Charlotte Stanton, Dan Morris, Drew Purves, Elizabeth Goldman, Katelyn Tarrio, Keith Anderson, Maxim Neumann, Mélanie Rey, Michelle J. Sims, Myroslava Lesiv, Nicholas Clinton, Petra Poklukar, Radost Stanimirova, Sarah Carter, Steffen Fritz, Yuchang Jiang.</i></p><p data-block-key=\"ftoen\"><i>Special thanks to early map reviewers: Andrew Lister (United States Forest Service), Astrid Verheggen (Joint Research Centre), Clement Bourgoin (Joint Research Centre), Erin Glen (WRI), Frederic Achard (Joint Research Centre), Jonas Fridman (Swedish University of Agricultural Sciences), Jukka Meiteninen (VTT), Karen Saunders (World Wildlife Fund Canada), Louis Reymondin (Alliance Bioversity International - CIAT), Martin Herold (GFZ Helmholtz Centre for Geosciences), Olga Nepomshina (GFZ Helmholtz Centre for Geosciences), Peter Potapov (University of Maryland/WRI), Rene Colditz (Joint Research Centre), Thibaud Vantalon (Alliance Bioversity International - CIAT), and Viviana Zalles (WRI).</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    }
  ],
  "lastUpdated": "2026-01-11T10:31:35.039Z"
}