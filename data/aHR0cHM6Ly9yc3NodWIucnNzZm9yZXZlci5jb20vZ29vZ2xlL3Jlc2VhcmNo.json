{
  "sourceUrl": "https://rsshub.rssforever.com/google/research",
  "title": "Google Research Blog",
  "description": "Google Research Blog - Powered by RSSHub",
  "link": "https://research.google/blog",
  "items": [
    {
      "title": "从大型模型到移动魔术：YouTube实时生成式AI特效背后的技术 (原标题: From massive models to mobile magic: The tech behind YouTube real-time generative AI effects)",
      "link": "https://research.google/blog/from-massive-models-to-mobile-magic-the-tech-behind-youtube-real-time-generative-ai-effects/",
      "pubDate": "Wed, 20 Aug 2025 16:00:00 GMT",
      "isoDate": "2025-08-20T16:00:00.000Z",
      "creator": "Google",
      "summary": "# YouTube实时生成式AI特效技术揭秘：从大型模型到移动魔术\n\nYouTube Shorts上的特效是其乐趣的重要组成部分，但要实现“魔术般”的效果，它们必须在创作者录制时在相机中实时运行。这带来了一个挑战：如何将大型生成式AI模型的最新功能（如卡通风格转换）应用于创作者的手机上？\n\n## 解决方案核心：知识蒸馏与模型小型化\n\nYouTube的解决方案是一个管道，它将大型模型的能力提炼成一个专注于单一任务的小型模型。这种范围的缩小创建了一个紧凑、高效的模型，可以直接在手机上逐帧处理视频。通过这种方法，YouTube已为Shorts上的创作者推出了20多种实时特效。\n\n### 1. 数据准备：高质量与多样性\n\n*   **基础数据：** 工作的基础是高质量数据。团队使用获得许可的图像构建了一个人脸数据集。\n*   **数据过滤：** 数据集经过精心过滤，确保在不同性别、年龄和肤色（通过Monk肤色量表测量）之间具有多样性和均匀分布，以确保特效对所有人有效。\n\n### 2. 教师-学生模型：知识蒸馏\n\n该方法围绕“知识蒸馏”概念，采用“教师-学生”模型训练方法：\n\n*   **教师模型：** 一个大型、强大、预训练的生成模型，擅长创建所需的视觉效果，但速度太慢，无法实时使用。\n    *   最初使用定制训练的StyleGAN2模型，可与StyleCLIP等工具结合，根据文本描述操纵面部特征。\n    *   项目后期过渡到更复杂的生成模型，如Google DeepMind的Imagen，显著提升了保真度、图像多样性和艺术控制力。\n*   **学生模型：** 最终在用户设备上运行的模型，需要小巧、快速、高效。\n    *   设计采用基于UNet的架构，非常适合图像到图像的任务。\n    *   编码器使用MobileNet骨干网络，解码器使用MobileNet块，以优化移动设备性能。\n\n### 3. 蒸馏过程：迭代式教学\n\n为了实现生产级特效，YouTube开发了一种强大的训练方法，解决了合成数据蒸馏的局限性（常导致伪影和高频细节减少）。该方法利用真实世界数据生成“图像对”，并训练学生模型以实现更高效的超参数搜索。\n\n*   **数据生成：** 通过教师模型处理大量图像数据集，创建数千个“前后”图像对。\n    *   生成过程中融入了增强功能，如添加AR眼镜、太阳镜以及合成手遮挡。\n    *   使用“枢轴调优反演（Pivotal Tuning Inversion, PTI）”来保留用户身份。\n*   **学生训练：** 学生模型在这些配对图像上进行训练。\n    *   结合使用L1、LPIPS、Adaptive和对抗性损失函数，确保学生模型的输出不仅在数值上准确，而且在视觉上真实和美观。\n    *   采用神经架构搜索来优化模型架构参数（如“深度乘数”和“宽度乘数”），以适应各种用例和特效类型。\n\n![实时生成式AI特效示例](https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-0-Examples.width-800.gif)\n实时生成式AI特效对视频流的实时转换。从左到右：原始、设备端美妆“粉嫩水润”、“卡通”和“卡通化”效果。\n\n![蒸馏管道示意图](https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-1-DistillationPipeline.width-1250.png)\n“永不眨眼”特效的蒸馏管道高级示意图。\n\n### 4. 关键挑战：保留用户身份\n\n*   **问题：** 图像编辑发生在“潜在空间”中，生成式模型在重新生成整个帧时，容易扭曲关键特征，改变肤色、眼镜或衣物，导致输出不再像用户本人。这被称为“反演问题”。\n*   **解决方案：枢轴调优反演（PTI）**\n    1.  原始图像通过编码器转换为嵌入（枢轴代码），并使用生成器生成初始反演。\n    2.  使用PTI迭代过程微调生成器，以保留人脸身份和细节。\n    3.  通过编辑嵌入（通常使用StyleCLIP等技术创建的向量）来应用所需效果。\n    4.  使用微调后的生成器和编辑后的嵌入生成最终输出图像。\n\n![生成管道示意图](https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-2-GenPipeline.width-1250.png)\n该管道将生成器微调到用户的独特面部，允许在潜在空间中应用编辑，而不会在最终图像中失去其相似性。请注意，初始反演可能缺少一些精细细节，导致外观略有不同。\n\n### 5. 设备端运行：MediaPipe\n\n学生模型训练完成后，需要集成到能在手机上高效运行的管道中。YouTube使用Google AI Edge的开源跨平台多模态ML管道框架MediaPipe构建了设备端解决方案。\n\n*   **推理管道工作流程：**\n    1.  MediaPipe Face Mesh模块检测视频流中的一个或多个人脸。\n    2.  由于学生模型对人脸对齐敏感，管道会计算一个稳定、旋转的人脸裁剪，以确保一致性。\n    3.  裁剪后的图像转换为张量，并输入到精简的学生模型中。\n    4.  学生模型应用特效（例如，微笑或卡通风格），然后将结果图像实时扭曲并无缝合成到原始视频帧上。\n\n![设备端推理管道示意图](https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-3-InferencePipeline.width-1250.png)\n设备端推理管道：MediaPipe Face Mesh检测、裁剪和对齐人脸以供学生模型使用。\n\n*   **性能要求：** 为确保用户体验流畅，这些体验需要以至少30帧/秒的速度运行（即管道每帧执行时间需快于33毫秒）。\n    *   模型推理延迟：Pixel 8 Pro（Google Tensor G3）约为6毫秒，iPhone 13 GPU约为10.6毫秒。\n    *   通过GPU加速，对各种移动设备进行了大量优化，以确保所有用户都能获得流畅体验。\n\n## 成果与未来展望\n\n这项技术自2023年以来一直是YouTube Shorts的关键要素，成功推出了众多热门功能，包括基于表情的特效（如“永不眨眼”）、万圣节主题面具（如“复活僵尸”）和沉浸式全帧特效（如“卡通2”），极大地扩展了YouTube视频创作者的创意可能性。\n\n![实时生成式AI特效演示](https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-4a-Smile.width-800.gif)\n![实时生成式AI特效演示](https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-4b-NvrBlink.width-800.gif)\n![实时生成式AI特效演示](https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-4c-Zombie.width-800.gif)\nYouTube Shorts上实时生成式AI特效的实际应用，包括基于表情的特效，如“总是微笑”（左）和“永不眨眼”（中），以及万圣节主题面具，如“复活僵尸”（右）。\n\n通过弥合大型生成模型与移动硬件限制之间的鸿沟，YouTube正在重新定义实时设备端生成特效的技术可能性。这仅仅是一个开始，团队正积极致力于集成最新的模型（如Veo 3），并显著降低入门级设备的延迟，进一步普及YouTube Shorts中尖端生成式AI的访问。",
      "shortSummary": "YouTube开发了一套技术，将大型生成式AI模型的能力带到移动设备上，为YouTube Shorts提供实时特效。核心方法是“知识蒸馏”，将强大的“教师”模型提炼成小巧高效的“学生”模型，并在高质量数据集上训练。为解决身份失真问题，采用了“枢轴调优反演”技术。设备端运行通过MediaPipe实现，确保流畅性能。该技术已推出20多款特效，显著增强了移动创作体验，未来还将继续优化和普及。",
      "translated_title": "从大型模型到移动魔术：YouTube实时生成式AI特效背后的技术",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-0-Examples.width-800.gif",
          "alt": "YTEffects-0-Examples",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-1-DistillationPipeline.width-1250.png",
          "alt": "YTEffects-1-DistillationPipeline",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-2-GenPipeline.width-1250.png",
          "alt": "YTEffects-2-GenPipeline",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-3-InferencePipeline.width-1250.png",
          "alt": "YTEffects-3-InferencePipeline",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-4a-Smile.width-800.gif",
          "alt": "YTEffects-4a-Smile",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"iiwr9\">Effects are a huge part of the fun on <a href=\"https://support.google.com/youtube/answer/10059070?hl=en\" target=\"_blank\" rel=\"noopener noreferrer\">YouTube Shorts</a>, but for them to feel magical, they need to work in real-time in the camera as the creator is recording. This presents a challenge: how do we apply the latest capabilities of large generative AI models, such as cartoon style transfer, on creators' phones?</p><p data-block-key=\"2ovib\">Our solution is a pipeline that distills the capability of a large model into a much smaller one focused on a single task. This narrowing of scope creates a compact, efficient model that can run directly on a phone, processing video frame-by-frame. Using this method, we've launched over 20 real-time effects for YouTube creators on Shorts. In this post, we'll detail how we accomplish this: including data curation, training, and the on-device setup.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-0-Examples.width-800.gif\" alt=\"YTEffects-0-Examples\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-0-Examples.width-800.gif\" alt=\"YTEffects-0-Examples\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cw6lr\"><i>Real-time transformation of video streams using a selection of real-time generative AI effects. From left to right: original, on-device makeup “</i><a href=\"https://www.youtube.com/effect/67d2024e-0000-223f-960a-d4f547f91714\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Pink dewy</i></a><i>”, ”</i><a href=\"https://www.linkedin.com/posts/googleresearch_dli2025-activity-7364197761409314816-5SWx?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAEFRf8B42XtbmIRgkdfsPAb_bPjQzAHui0\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Cartoon</i></a><i>” and a “</i><a href=\"https://www.youtube.com/effect/678bfb3e-0000-2bbc-8ebe-2405887ff630\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Toon</i></a><i>” effect.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">It all starts with data</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"iiwr9\">The foundation of our work is high-quality data. We began by building a face dataset using properly licensed images. We meticulously filtered our datasets to ensure they were diverse and uniformly distributed across different genders, ages, and skin tones (as measured by the <a href=\"https://skintone.google/get-started\" target=\"_blank\" rel=\"noopener noreferrer\">Monk Skin Tone Scale</a>) to build effects that work well for everyone.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The teacher and the student</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"iiwr9\">Our approach revolves around a concept called <a href=\"https://en.wikipedia.org/wiki/Knowledge_distillation\" target=\"_blank\" rel=\"noopener noreferrer\">knowledge distillation</a>, which uses a \"teacher–student\" model training method. We start with a \"teacher\" — a large, powerful, pre-trained generative model that is an expert at creating the desired visual effect but is far too slow for real-time use. The type of teacher model varies depending on the goal. Initially, we used a custom-trained <a href=\"https://arxiv.org/abs/1912.04958\" target=\"_blank\" rel=\"noopener noreferrer\">StyleGAN2</a> model, which was trained on our curated dataset for real-time facial effects. This model could be paired with tools like<a href=\"https://arxiv.org/abs/2103.17249\" target=\"_blank\" rel=\"noopener noreferrer\"> StyleCLIP</a>, which allowed it to manipulate facial features based on text descriptions. This provided a strong foundation. As our project advanced, we transitioned to more sophisticated generative models like Google DeepMind’s <a href=\"https://deepmind.google/models/imagen/\" target=\"_blank\" rel=\"noopener noreferrer\">Imagen</a>. This strategic shift significantly enhanced our capabilities, enabling higher-fidelity and more diverse imagery, greater artistic control, and a broader range of styles for our on-device generative AI effects.</p><p data-block-key=\"abdv\">The \"student\" is the model that ultimately runs on the user’s device. It needs to be small, fast, and efficient. We designed a student model with a <a href=\"https://link.springer.com/content/pdf/10.1007/978-3-319-24574-4_28.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">UNet</a>-based architecture, which is excellent for image-to-image tasks. It uses a <a href=\"https://research.google/blog/mobilenets-open-source-models-for-efficient-on-device-vision/\">MobileNet</a> backbone as its encoder, a design known for its performance on mobile devices, paired with a decoder that utilizes MobileNet blocks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Distillation: Iteratively teaching the student</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"iiwr9\">To achieve production-ready effects, we developed a robust training methodology that addresses the limitations of synthetic data distillation, which often leads to artifacts and reduced high-frequency details. Our approach leverages real-world data to generate \"image pairs\" and train student models to enable a more efficient hyperparameter search.</p><p data-block-key=\"1100m\">The distillation process for training the smaller student model involves two key steps:</p><ol><li data-block-key=\"b0j7r\"><b>Data Generation:</b> We process a large dataset of images through the teacher model to create thousands of \"before and after\" image pairs. During generation, we incorporate augmentations, such as adding AR glasses and sunglasses, and occlusion with synthetic <a href=\"https://research.google/blog/on-device-real-time-hand-tracking-with-mediapipe/\">hands</a>. We also use <a href=\"https://arxiv.org/abs/2106.05744\" target=\"_blank\" rel=\"noopener noreferrer\">Pivotal Tuning Inversion</a> to preserve user identity.<br></li><li data-block-key=\"b6epd\"><b>Student Training:</b> The student model is then trained on these paired images. We utilize a combination of <a href=\"https://en.wikipedia.org/wiki/Mean_absolute_error\" target=\"_blank\" rel=\"noopener noreferrer\">L1</a>, <a href=\"https://arxiv.org/abs/1801.03924\" target=\"_blank\" rel=\"noopener noreferrer\">LPIPS</a>, <a href=\"https://arxiv.org/abs/1701.03077v10\" target=\"_blank\" rel=\"noopener noreferrer\">Adaptive</a>, and <a href=\"https://developers.google.com/machine-learning/gan/loss\" target=\"_blank\" rel=\"noopener noreferrer\">Adversarial</a> loss functions to ensure the student's output is not only numerically accurate but also visually realistic and aesthetically pleasing. Furthermore, we employ a <a href=\"https://en.wikipedia.org/wiki/Neural_architecture_search\" target=\"_blank\" rel=\"noopener noreferrer\">neural architecture search</a> to optimize model architecture parameters (like \"depth multiplier\" and \"width multiplier\") allowing us to identify efficient architectures tailored to various use cases and effect types.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-1-DistillationPipeline.width-1250.png\" alt=\"YTEffects-1-DistillationPipeline\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-1-DistillationPipeline.width-1250.png\" alt=\"YTEffects-1-DistillationPipeline\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cw6lr\"><i>High-level schema of distillation pipeline the “</i><a href=\"https://youtube.com/effect/63fec95a-0000-2d83-b984-14223bb7b9e2\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Never Blink</i></a><i>” effect.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">A critical challenge: Preserving user identity</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"iiwr9\">The \"editing\" of the image happens in \"latent\" space, which is a compressed numerical representation of the image where meaningful features are encoded. The process of converting raw pixels to latent representation is called “inversion”. A major challenge in image-to-image generative models for facial effects is preserving a person's identity because the effect regenerates the entire frame. A naïve approach can easily distort key features, changing a person's skin tone, glasses, or clothing, resulting in an output that no longer looks like them. This issue, often called the \"inversion problem\", happens when a model struggles to accurately represent a real person's face in its latent space.</p><p data-block-key=\"flpgs\">To solve this, we employ a technique called <a href=\"https://arxiv.org/abs/2106.05744\" target=\"_blank\" rel=\"noopener noreferrer\">pivotal tuning inversion</a> (PTI). Here is a simplified version of how it works:</p><ol><li data-block-key=\"bvjoq\">The original image is transformed into an embedding, referred to as a <i>pivotal code</i>, using an encoder and generating an initial inversion with a generator (see below). This is typically a representation similar to the original image, but not identical (e.g., skin tone and facial details may not be accurate).<br></li><li data-block-key=\"64gqu\">We fine-tune a generator using the PTI iterative process to preserve face identity and details. This results in a new generator that performs better for a specific face and its embedding neighborhood.<br></li><li data-block-key=\"6qonf\">We apply the desired effect by editing the embedding, typically using a prepared vector created with techniques such as StyleCLIP.<br></li><li data-block-key=\"cl9jt\">We generate the final output image with an edited face using a fine-tuned generator and an edited embedding.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-2-GenPipeline.width-1250.png\" alt=\"YTEffects-2-GenPipeline\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-2-GenPipeline.width-1250.png\" alt=\"YTEffects-2-GenPipeline\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cw6lr\"><i>The pipeline fine-tunes a generator to the user's unique face, allowing us to apply edits in the latent space without losing their likeness in the final image. Note that the initial inversion may lack some fine details, resulting in a slightly different appearance.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Running on device with MediaPipe from Google AI Edge</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"iiwr9\">Once the student model is trained, it needs to be integrated into a pipeline that can run efficiently on a phone. We built our on-device solution using <a href=\"https://ai.google.dev/edge/mediapipe/solutions/guide\" target=\"_blank\" rel=\"noopener noreferrer\">MediaPipe</a>, our open-source framework for building cross-platform multimodal ML pipelines, from <a href=\"http://ai.google.dev/edge\" target=\"_blank\" rel=\"noopener noreferrer\">Google AI Edge</a>. The final inference pipeline works as follows:</p><ol><li data-block-key=\"2ee3s\">First, the <a href=\"https://github.com/google-ai-edge/mediapipe/wiki/MediaPipe-Face-Mesh\" target=\"_blank\" rel=\"noopener noreferrer\">MediaPipe Face Mesh</a> module detects one or more faces in the video stream.<br></li><li data-block-key=\"19hqr\">Because student models are sensitive to face alignment, the pipeline computes a stable, rotated crop of the face to ensure consistency.<br></li><li data-block-key=\"f3tvv\">This cropped image is converted into a tensor and fed into our lean student model.<br></li><li data-block-key=\"c43t8\">The student model applies the effect (e.g., a smile or a cartoon style), and the resulting image is warped back and seamlessly composited onto the original video frame in real-time.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-3-InferencePipeline.width-1250.png\" alt=\"YTEffects-3-InferencePipeline\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-3-InferencePipeline.width-1250.png\" alt=\"YTEffects-3-InferencePipeline\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cw6lr\"><i>On-device inference pipeline: MediaPipe Face Mesh detects, crops, and aligns faces for the student model.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"iiwr9\">These experiences need to run at a minimum of 30 frames per second to feel responsive to the user, so the pipeline must execute faster than 33 milliseconds per frame. The model inference latencies are ~6 ms for Pixel 8 Pro on Google Tensor G3 and 10.6 ms for iPhone 13 GPU. We invested heavily in optimizing these pipelines for a wide range of mobile devices, leveraging GPU acceleration to ensure a smooth experience for everyone.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The result: Enhanced mobile creativity</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"iiwr9\">This technology has been a crucial element of YouTube Shorts since 2023, enabling the successful launch of numerous popular features, including expression-based effects (e.g., <a href=\"https://youtube.com/effect/63fec95a-0000-2d83-b984-14223bb7b9e2\" target=\"_blank\" rel=\"noopener noreferrer\">Never blink</a>), Halloween-themed masks (e.g., <a href=\"https://youtube.com/effect/670d68b0-0000-286d-b0a7-089e08292e4c\" target=\"_blank\" rel=\"noopener noreferrer\">Risen zombie</a>), and immersive full-frame effects (e.g., <a href=\"https://youtube.com/effect/678bfb3e-0000-2bbc-8ebe-2405887ff630\" target=\"_blank\" rel=\"noopener noreferrer\">Toon 2</a>). These significantly expanded creative possibilities for YouTube video creators.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n              --vertical-center\n            \n            \n                glue-grid__col--span-4-md\n            \n          \">\n            <div class=\"--mobile-spacer\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-4a-Smile.width-800.gif\" alt=\"YTEffects-4a-Smile\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-4a-Smile.width-800.gif\" alt=\"YTEffects-4a-Smile\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n              --vertical-center\n            \n            \n                glue-grid__col--span-4-md\n            \n          \">\n            <div class=\"--mobile-spacer\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-4b-NvrBlink.width-800.gif\" alt=\"YTEffects-4b-NvrBlink\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-4b-NvrBlink.width-800.gif\" alt=\"YTEffects-4b-NvrBlink\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n              --vertical-center\n            \n            \n                glue-grid__col--span-4-md\n            \n          \">\n            <div class=\"--mobile-spacer\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-4c-Zombie.width-800.gif\" alt=\"YTEffects-4c-Zombie\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/YTEffects-4c-Zombie.width-800.gif\" alt=\"YTEffects-4c-Zombie\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"cw6lr\"><i>Real-time generative AI effects in action on YouTube Shorts, including expression-based effects like “</i><a href=\"https://www.youtube.com/effect/647912dc-0000-2d0c-aef9-582429bbd350\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Always smile</i></a><i>” (</i><b><i>left</i></b><i>) and \"</i><a href=\"https://youtube.com/effect/63fec95a-0000-2d83-b984-14223bb7b9e2\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Never blink</i></a><i>\" (</i><b><i>middle</i></b><i>) and Halloween-themed masks like \"</i><a href=\"https://youtube.com/effect/670d68b0-0000-286d-b0a7-089e08292e4c\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Risen zombie</i></a><i>\" (</i><b><i>right</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"iiwr9\">By bridging the gap between massive generative models and the constraints of mobile hardware, we are defining what is technically possible for real-time, on-device generative effects. This is just the beginning; we are actively working on integrating our newest models, like <a href=\"https://deepmind.google/models/veo/\" target=\"_blank\" rel=\"noopener noreferrer\">Veo 3</a>, and significantly reducing latency for entry-level devices, further democratizing access to cutting-edge generative AI in YouTube Shorts.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"iiwr9\"><i>We would like to thank our co-authors and collaborators: Sarah Xu, Maciej Pęśko, Paweł Andruszkiewicz, Jacob Rockwell, Ronny Votel, Robert (Guohui) Wang, Tingbo Hou, Karthik Raveendran, Jianing Wei, Matthias Grundmann, Omer Tov, Ariel Ephrat, Shiran Zada, and Inbar Mosseri.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "使用差分隐私分区选择大规模保护私有数据 (原标题: Securing private data at scale with differentially private partition selection)",
      "link": "https://research.google/blog/securing-private-data-at-scale-with-differentially-private-partition-selection/",
      "pubDate": "Tue, 19 Aug 2025 16:00:00 GMT",
      "isoDate": "2025-08-19T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 使用差分隐私分区选择大规模保护私有数据\n\n## 引言\n\n大型用户数据集对推动人工智能和机器学习模型至关重要，能带来改进服务、更准确预测和个性化体验等益处。共享这些数据集可以加速研究和创新，但同时也伴随着潜在的数据隐私风险。\n\n**差分隐私 (DP) 分区选择**是一种识别独特项目子集的方法，该子集可以从大量数据中安全共享，其依据是这些项目在许多个人贡献中出现的频率或显著性。通过在分区选择中应用差分隐私保护，可以确保任何人都无法得知任何单个用户的数据是否促成了最终列表中的特定项目。这通过添加受控噪声并仅选择在添加噪声后仍足够常见的项目来实现，从而保障个人隐私。\n\nDP 是许多重要数据科学和机器学习任务的第一步，包括：\n*   从大型私有语料库中提取词汇（或 n-gram）。\n*   以隐私保护方式分析数据流。\n*   获取用户数据的直方图。\n*   提高私有模型微调的效率。\n\n对于用户查询等海量数据集，并行算法至关重要。它能将问题分解为多个可同时计算的小部分，从而处理数百亿条数据，在不牺牲数据效用的前提下提供强大的隐私保障。\n\n## 新算法：可伸缩的自适应加权私有分区选择\n\n我们最近在 ICML2025 上发表的论文《可伸缩的自适应加权私有分区选择》(Scalable Private Partition Selection via Adaptive Weighting) 介绍了一种高效的并行算法，可将 DP 分区选择应用于各种数据发布。该算法在并行算法中表现最佳，可扩展到数百亿条数据，比之前的顺序算法处理的数据量大三个数量级。为鼓励研究社区的协作和创新，我们已在 GitHub 上开源了 DP 分区选择。\n\n### 算法工作原理\n\nDP 分区选择的目标是在严格保持用户级 DP 的同时，最大化从数据集合中选出的独特项目数量。这意味着属于许多用户的热门项目通常可以安全地保留用于下游计算任务，而仅属于单个用户的项目则不会被包含。算法设计者必须在选择数据集项目时，在隐私-效用权衡之间寻求最佳平衡，同时遵守差分隐私要求。\n\n### 标准范式：加权、加噪和过滤\n\n差分隐私分区选择的传统方法包括三个核心步骤：\n\n1.  **加权 (Weight)**：为每个项目计算一个“权重”，通常代表项目的频率或跨用户的聚合。这形成了一个权重直方图。一个关键的隐私要求是“低敏感度”，即任何单个用户贡献的总权重是有限的。在标准的非自适应设置中，用户独立于其他用户的贡献来为其项目分配权重。\n2.  **加噪 (Noise)**：为确保 DP，向每个项目的计算权重添加随机噪声（例如，具有特定标准差的高斯噪声）。这模糊了精确计数，防止攻击者推断个人的存在或数据。\n3.  **过滤 (Filter)**：最后，应用由 DP 参数确定的阈值。只有噪声权重超过此阈值的项目才会被包含在最终输出中。\n\n![加权、加噪和过滤范式](https://storage.googleapis.com/gweb-research2023-media/images/DPPS-1-Weightin.width-1250.png)\n*加权、加噪和过滤范式。在所有图中，x 轴表示项目（A-F），y 轴表示分配给项目的权重。算法首先计算项目的权重直方图（左），添加噪声（中），然后返回噪声权重高于阈值的项目（右）。*\n\n### 通过自适应加权提高效用\n\n标准非自适应方法的局限性在于潜在的“浪费”。高度热门的项目可能会获得远超通过隐私阈值所需的权重，从而导致“过度分配”。这些多余的权重本可以更有效地用于提升那些刚好低于阈值的项目，从而增加发布的项目总数并提高输出的效用。\n\n我们引入了**自适应加权**来解决这个问题。与非自适应方法中每个用户的贡献是独立的做法不同，自适应设计允许用户对项目的权重贡献考虑其他用户的贡献。这需要在不损害隐私或计算效率的前提下实现微妙的平衡。\n\n我们的新算法 **MaxAdaptiveDegree (MAD)** 策略性地重新分配权重。它识别具有显著“过剩权重”（远高于阈值）的项目，并将部分权重重新分配给“分配不足”（刚好低于阈值）的项目。这种自适应重新分配确保了更多不那么频繁的项目能够跨越隐私阈值并被包含在输出中。此外，MAD 保持了与基线相同的低敏感度界限和效率，这意味着它在并行处理框架（如 MapReduce-like 系统）中提供相同的强大隐私保障和可伸缩性，但具有严格优越的效用。\n\n此外，我们将这一概念扩展到多轮 DP 分区选择框架。我们展示了如何安全地在轮次之间发布中间噪声权重向量。这些额外信息允许更大的自适应性，因为我们可以减少未来对先前获得过多权重（可能再次被过度分配）或过少权重（可能永远无法跨越阈值）的项目的分配。这进一步优化了权重分布，在不牺牲隐私的情况下最大化了效用，并进一步增加了输出中的项目。\n\n## 实验结果\n\n我们对 MAD 算法（单次或多次迭代）与可伸缩的私有分区选择基线进行了广泛实验比较。\n\n![算法返回项目数量的比较](https://storage.googleapis.com/gweb-research2023-media/images/DPPS-2-Table.width-1250.png)\n*我们的算法：两轮 (MAD2R) 和其他基线（“Basic”代表均匀加权算法，以及 DP-SIPS）在九个公开数据集上返回的项目数量比较。通过利用新的自适应技术，我们的两轮算法在许多数据集上取得了最先进的结果。*\n\n如上表所示，仅进行两次迭代的 MAD (MAD2R) 在许多数据集上取得了最先进的结果——通常比其他方法（即使是使用更多轮次的方法）输出显著更多的项目，同时保持相同的隐私保障。我们的论文中提出的理论结果表明，我们的单轮算法 (MAD) 应该始终优于上述单轮高斯加权基线，实验结果证实了这一理论假设。我们新方法的卓越性能在各种隐私参数和超参数选择下都得到了验证。我们算法的 Python 内存实现示例可在开源仓库中获取。\n\n在大型公开 Common Crawl 数据集（包含近 8000 亿条目）上，我们将条目视为“用户”，将这些条目中的单词视为项目，从而获得了记录级 DP。在该数据集上，我们的两轮 MAD 算法输出了一组项目，覆盖了 99.9% 的条目（每个条目在输出中至少有一个项目）和 97% 的数据库条目（对应于输出中的一个项目），同时满足了 DP 保障。\n\n![两轮 MAD 算法返回的项目数量](https://storage.googleapis.com/gweb-research2023-media/images/DPPS-3-Counts.width-1250.png)\n*我们的两轮 MAD 算法在 Common Crawl 数据集上按频率计数（拥有该项目的条目数量）返回的项目数量。虽然 MAD 无法返回会违反隐私的低频项目，但它输出了绝大多数属于许多条目的项目。*\n\n## 结论\n\n我们引入了新方法来改进 DP 分区选择算法中的效用-隐私权衡。我们的算法在接近万亿条目的数据集上取得了最先进的结果。我们希望该算法能帮助从业者在严格遵守用户隐私的同时，在他们的工作流程中实现更高的效用。\n\n## 致谢\n\n我们感谢 Google Research 算法与优化团队的 Alessandro Epasto 和 Vincent Cohen-Addad 对这项工作的贡献。",
      "shortSummary": "本文介绍了一种名为“差分隐私 (DP) 分区选择”的技术，旨在从大规模数据集中安全地提取有价值信息，同时严格保护用户隐私。针对传统方法的局限性，我们提出了“可伸缩的自适应加权私有分区选择”算法（MAD），通过自适应加权策略，将多余权重重新分配给不那么频繁的项目。实验表明，MAD 算法在保持强大隐私保障和可伸缩性的前提下，显著提高了数据效用，在处理数百亿条数据时取得了最先进的结果，并已开源。",
      "translated_title": "使用差分隐私分区选择大规模保护私有数据",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DPPS-1-Weightin.width-1250.png",
          "alt": "DPPS-1-Weightin",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DPPS-2-Table.width-1250.png",
          "alt": "DPPS-2-Table",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DPPS-3-Counts.width-1250.png",
          "alt": "DPPS-3-Counts",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"jn8lu\">Large, user-based datasets are invaluable for advancing AI and machine learning models. They drive innovation that directly benefits users through improved services, more accurate predictions, and personalized experiences. Collaborating on and sharing such datasets can accelerate research, foster new applications, and contribute to the broader scientific community. However, leveraging these powerful datasets also comes with potential data privacy risks.</p><p data-block-key=\"b8u1a\">The process of identifying a specific, meaningful subset of unique items that can be shared safely from a vast collection based on how frequently or prominently they appear across many individual contributions (like finding all the common words used across a huge set of documents) is called “differentially private (DP) partition selection”. By applying differential privacy protections in partition selection, it’s possible to perform that selection in a way that prevents anyone from knowing whether any single individual's data contributed a specific item to the final list. This is done by adding controlled noise and only selecting items that are sufficiently common even after that noise is included, ensuring individual privacy. DP is the first step in many important data science and machine learning tasks, including extracting vocabulary (or <i>n</i>-grams) from a large private corpus (a necessary step of many textual analysis and language modeling applications), analyzing <a href=\"https://arxiv.org/pdf/2303.18086\" target=\"_blank\" rel=\"noopener noreferrer\">data streams</a> in a privacy preserving way, obtaining <a href=\"https://arxiv.org/pdf/2103.16787\" target=\"_blank\" rel=\"noopener noreferrer\">histograms</a> over user data, and <a href=\"https://arxiv.org/abs/2311.08357\" target=\"_blank\" rel=\"noopener noreferrer\">increasing efficiency</a> in private model fine-tuning.</p><p data-block-key=\"dgrb2\">In the context of massive datasets like user queries, a <i>parallel</i> algorithm is crucial. Instead of processing data one piece at a time (like a <i>sequential</i> algorithm would), a parallel algorithm breaks the problem down into many smaller parts that can be computed simultaneously across multiple processors or machines. This practice isn't just for optimization; it's a fundamental necessity when dealing with the scale of modern data. Parallelization allows the processing of vast amounts of information all at once, enabling researchers to handle datasets with hundreds of billions of items. With this, it’s possible to achieve robust privacy guarantees without sacrificing the utility derived from large datasets.</p><p data-block-key=\"8lb69\">In our recent publication, “<a href=\"https://arxiv.org/abs/2502.08878\" target=\"_blank\" rel=\"noopener noreferrer\">Scalable Private Partition Selection via Adaptive Weighting</a>”, which appeared at <a href=\"https://icml.cc/virtual/2025/poster/45510\" target=\"_blank\" rel=\"noopener noreferrer\">ICML2025</a>, we introduce an efficient parallel algorithm that makes it possible to apply <a href=\"https://arxiv.org/pdf/2006.03684\" target=\"_blank\" rel=\"noopener noreferrer\">DP partition selection</a> to various data releases. Our algorithm provides the best results across the board among parallel algorithms and scales to datasets with hundreds of billions of items, up to three orders of magnitude larger than those analyzed by prior sequential algorithms. To encourage collaboration and innovation by the research community, we are open-sourcing <a href=\"https://github.com/jusyc/dp_partition_selection\" target=\"_blank\" rel=\"noopener noreferrer\">DP partition selection on GitHub</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">How the algorithm works</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"jn8lu\">The goal of DP partition selection is to maximize the number of unique items selected from a union of sets of data, while strictly preserving user-level DP. This means that very popular items, belonging to many users, can often be safely preserved for downstream computational tasks, whereas items belonging to only a single user would not be included. The algorithm designer must aim for an optimal privacy-utility trade-off in selecting items from the dataset while respecting the differential privacy requirement.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">The standard paradigm: Weight, noise, and filter</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"jn8lu\">The conventional approach to differentially private partition selection involves three core steps:</p><ol><li data-block-key=\"8a10c\"><i>Weight:</i> For each item, a \"weight\" is computed, typically representing the item’s frequency or some aggregation across users. This forms a weight histogram. A crucial privacy requirement is \"low sensitivity\", meaning that the total weight contributed by any single user is bounded. In a standard non-adaptive setting, users assign weights to their items independently of what other users contribute (e.g., the <a href=\"https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Gaussian weighting</a> baseline).<br></li><li data-block-key=\"6hp00\"><i>Noise:</i> To ensure DP, random noise (e.g., <a href=\"https://en.wikipedia.org/wiki/Gaussian_noise\" target=\"_blank\" rel=\"noopener noreferrer\">Gaussian noise</a> with a specific standard deviation) is added to each item's computed weight. This obfuscates the exact counts, preventing an attacker from inferring an individual's presence or data.<br></li><li data-block-key=\"23fi4\"><i>Filter:</i> Finally, a threshold determined by the DP parameters is applied. Only items whose noisy weights exceed this threshold are included in the final output.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DPPS-1-Weightin.width-1250.png\" alt=\"DPPS-1-Weightin\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DPPS-1-Weightin.width-1250.png\" alt=\"DPPS-1-Weightin\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"9j0dt\"><i>The weight, noise, and filter paradigm. In all plots, the x-axis represents items (A–F) and the y-axis represents the weight assigned to the items. The algorithm first computes a weight histogram over items (</i><b><i>left</i></b><i>), adds noise (</i><b><i>center</i></b><i>), and returns items with noisy weight above a threshold (</i><b><i>right</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Improving utility with adaptive weighting</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"jn8lu\">A limitation of the standard, non-adaptive approach is potential \"wastage\". Highly popular items might receive significantly more weight than necessary to cross the privacy threshold, effectively \"over-allocating\" weight. This excess weight could have been more effectively used to boost items that are just below the threshold, thereby increasing the overall number of items released and improving the utility of the output.</p><p data-block-key=\"91gmd\">We introduce adaptivity into the weight assignment process to address this. Unlike non-adaptive methods where each user's contribution is independent, an adaptive design allows the weight contributed by a user to an item to consider contributions from other users. This is a delicate balance, as it must be achieved without compromising privacy or computational efficiency.</p><p data-block-key=\"7uof8\">Our novel algorithm, MaxAdaptiveDegree (MAD), strategically reallocates weight. It identifies items with significant \"excess weight\" (far above the threshold) and reroutes some of that weight to \"under-allocated\" items (those just below the threshold). This adaptive reallocation ensures that more less-frequent items can cross the privacy threshold and be included in the output. Moreover, MAD maintains both the same low-sensitivity bounds and efficiency as the baseline, meaning it offers the same strong privacy guarantees and scalability in parallel processing frameworks (like <a href=\"https://en.wikipedia.org/wiki/MapReduce\" target=\"_blank\" rel=\"noopener noreferrer\">MapReduce</a>-like systems), but with strictly superior utility.</p><p data-block-key=\"1044t\">Furthermore, we extend this concept to multi-round DP partition selection frameworks. We demonstrate how to safely release intermediate noisy weight vectors between rounds. This additional information allows for even greater adaptivity, as we can reduce future weight allocations to items that previously received too much weight (and were likely to be over-allocated again) or too little weight (and were unlikely to ever cross the threshold). This further refines the weight distribution, maximizing utility without sacrificing privacy, and further increases the items in output.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Experiments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"jn8lu\">We conducted extensive experiments comparing our MAD algorithm with one or multiple iterations against scalable baselines for private partition selection.</p><p data-block-key=\"770h0\">As shown in the table below, MAD with just two iterations (column MAD2R) achieves state-of-the-art results across many datasets — often outputting significantly more items than other methods (even those using more rounds) while retaining the same privacy guarantees.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DPPS-2-Table.width-1250.png\" alt=\"DPPS-2-Table\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DPPS-2-Table.width-1250.png\" alt=\"DPPS-2-Table\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"9j0dt\"><i>Comparison of the number of items returned by our algorithms: Two-round (MAD2R) and other baselines (“Basic”, which represents the uniformly weighted algorithm, and</i> <a href=\"https://arxiv.org/abs/2301.01998\" target=\"_blank\" rel=\"noopener noreferrer\"><i>DP-SIPS</i></a><i>) on nine publicly-available datasets. By leveraging new techniques for adaptivity, our two-round algorithm achieves state-of-art results across many datasets.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"jn8lu\">In our <a href=\"https://arxiv.org/abs/2502.08878\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>, we present theoretical results that suggest our single-round algorithm (MAD) should always outperform the single-round Gaussian weighting baseline mentioned above. Our results demonstrate that this theoretical hypothesis appears to be correct. The excellent performance of our new methods holds across a wide selection of privacy parameters and hyperparameter choices. An example in-memory implementation of our algorithm in Python is available in the <a href=\"https://github.com/jusyc/dp_partition_selection\" target=\"_blank\" rel=\"noopener noreferrer\">open-source repo</a>.</p><p data-block-key=\"5j03u\">On the large-scale publicly-available <a href=\"https://commoncrawl.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Common Crawl</a> dataset (comprising close to 800 billion entries), we obtained record-level DP by treating entries as “users” and the words in these entries as items. On this dataset, our two-iteration MAD algorithm output a set of items that covered 99.9% of the entries (each of which has at least one item in the output) and 97% of the database entries (corresponding to an item that is in the output) while satisfying the DP guarantee.</p><p data-block-key=\"3h49e\">With just two iterations, our algorithm achieved state-of-the-art results in a wide range of parameter settings. As expected from our theoretical results, our algorithm always outperformed the baseline.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DPPS-3-Counts.width-1250.png\" alt=\"DPPS-3-Counts\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DPPS-3-Counts.width-1250.png\" alt=\"DPPS-3-Counts\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"9j0dt\"><i>The number of items our two-iteration MAD algorithm returns by frequency count (the number of entries that have that item) on the Common Crawl dataset. While MAD cannot return low frequency items that would violate privacy, it outputs the vast majority of items that belong to many entries.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"jn8lu\">We introduce new methods to improve the utility-privacy trade-off in DP partition selection algorithms. Our algorithm achieves state-of-the-art results on datasets approaching a trillion entries. We hope this algorithm will help practitioners achieve higher utility across their workflows while strictly respecting user privacy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"jn8lu\"><i>We thank Alessandro Epasto, Vincent Cohen-Addad, part of the</i> <a href=\"https://research.google/teams/algorithms-optimization/\"><i>Algorithm and Optimization team</i></a><i> in</i> <a href=\"https://research.google/\"><i>Google Research</i></a><i>, who contributed to this work.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "超越十亿参数的负担：使用条件生成器解锁数据合成 (原标题: Beyond billion-parameter burdens: Unlocking data synthesis with a conditional generator)",
      "link": "https://research.google/blog/beyond-billion-parameter-burdens-unlocking-data-synthesis-with-a-conditional-generator/",
      "pubDate": "Wed, 13 Aug 2025 16:00:00 GMT",
      "isoDate": "2025-08-13T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 超越十亿参数的负担：使用条件生成器解锁数据合成\n\n### 引言\n\n生成大规模差分隐私（DP）合成数据面临巨大挑战，主要源于隐私-计算-效用之间的根本权衡。强大的隐私保证往往会损害合成数据质量，或需要大量的计算资源。当前流行的解决方案是在“私有数据”上私有微调十亿参数级大型语言模型（LLM），然后从微调后的模型中采样生成合成数据。然而，这种方法计算成本高昂，对于资源受限的应用来说难以实现。近期提出的 Aug-PE 和 Pre-Text 算法虽然探索了仅需 LLM API 访问的合成数据生成方法，但它们通常严重依赖手动提示来生成初始数据集，并且在迭代数据选择过程中未能有效利用私有信息。\n\n### CTCL 框架：一种新颖的解决方案\n\n在 ICML 2025 上发表的论文“Synthesizing Privacy-Preserving Text Data via Fine-Tuning Without Fine-Tuning Billion-Scale LLMs”中，我们提出了 CTCL（Data Synthesis with ConTrollability and CLustering），一个用于生成隐私保护合成数据的新颖框架，它无需微调十亿参数级 LLM，也无需领域特定的提示工程。CTCL 使用一个轻量级的 1.4 亿参数模型，使其适用于资源受限的应用。通过对主题信息进行条件化，生成的合成数据可以匹配私有领域的主题分布。此外，与 Aug-PE 算法不同，CTCL 允许生成无限量的合成数据样本，而无需支付额外的隐私成本。\n\n### CTCL 框架的构建\n\nCTCL 框架旨在从私有数据集中生成高质量的合成数据，同时保护隐私。它将整个过程分解为三个主要步骤，并依赖于两个核心组件：CTCL-Topic 和 CTCL-Generator。\n\n*   **CTCL-Topic**：一个通用主题模型，用于捕获数据集的高级主题。\n*   **CTCL-Generator**：一个强大的语言模型，可以根据特定关键词创建文档。\n\n这两个组件均使用大型公共语料库开发，是学习不同私有领域并从中生成合成数据的基础。\n\n#### 步骤 1：开发 CTCL-Topic 和 CTCL-Generator\n\n这两个组件只需使用大规模公共语料库开发一次，之后可用于学习不同的私有领域。\n\n*   **CTCL-Topic**：从维基百科（包含约 600 万文档的多元语料库）中提取的主题模型。它遵循 BERTopic 方法，将每个文档嵌入，聚类成约 1000 个主题，并用 10 个关键词表示每个主题。\n*   **CTCL-Generator**：一个轻量级（1.4 亿参数）条件语言模型，接受自由形式的文档描述（如文档类型、关键词等）作为输入，并生成满足输入条件的文档。为了构建预训练数据，我们对 SlimPajama 中的每个文档使用 Gemma-2-2B 进行提示，生成“多方面描述文档”的结果，形成包含 4.3 亿个描述-文档对的数据集。然后，我们使用此数据集在 BART-base（一个 1.4 亿参数的语言模型）之上进行持续预训练，从而得到 CTCL-Generator。\n\n![BBPB-1-CTCLTopic](https://storage.googleapis.com/gweb-research2023-media/images/BBPB-1-CTCLTopic.width-1250.png)\n\n*图 1：步骤 1：使用大规模公共语料库开发通用主题模型 CTCL-Topic 和具有强大可控性的轻量级 1.4 亿参数 CTCL-Generator。*\n\n#### 步骤 2：学习私有领域\n\n我们使用 CTCL-Topic 捕获整个私有语料库的高级分布信息。这通过收集代表私有数据主题分布的私有直方图（即每个主题在私有数据中的百分比）来实现。此主题直方图将在步骤 3 中用于采样。\n\n在收集主题直方图的同时，私有数据集中的每个文档都已关联到一个主题。然后，我们将私有数据集转换为关键词和文档对的数据集，其中每个文档的 10 个关键词来自其在 CTCL-Topic 中对应的关键词。随后，我们使用 DP 对 CTCL-Generator 在此数据集上进行微调。\n\n![BBPB-2a-TCTLGen](https://storage.googleapis.com/gweb-research2023-media/images/BBPB-2a-TCTLGen.width-1250.png)\n\n*图 2：步骤 2：为了学习私有领域，我们从私有数据中收集 DP 主题直方图，并使用 DP 对 CTCL-Generator 在私有数据上进行微调。*\n\n#### 3. 生成合成数据\n\n根据 DP 主题直方图，按比例对经过 DP 微调的 CTCL-Generator 进行采样。具体来说，给定所需的合成数据集大小（例如 N）和 DP 主题直方图（例如主题 1 占 x%，主题 2 占 y% 等），我们就可以知道每个主题的目标样本数量（即主题 1 占 N*x%，主题 2 占 N*y% 等）。对于每个主题，我们使用相应的 10 个关键词作为输入，通过经过 DP 微调的 CTCL-Generator 生成数据。根据 DP 的后处理特性，CTCL-Generator 可以生成任意数量的合成数据，而无需支付额外的隐私成本。\n\n![BBPB-3a-SynthDataGen](https://storage.googleapis.com/gweb-research2023-media/images/BBPB-3a-SynthDataGen.width-1250.png)\n\n*图 3：步骤 3：基于 DP 主题直方图和经过 DP 微调的 CTCL-Generator 生成隐私保护合成数据。*\n\n### 实验与结果\n\n我们在四个数据集上进行了实验，其中三个数据集对应下游生成任务，一个数据集对应分类任务。生成任务通常比分类任务更具挑战性，因为它要求合成数据保留私有数据中细粒度的文本信息，而分类任务仅需保持标签和单词之间的共现模式。\n\n*   **生成任务**：PubMed（医学论文摘要）、Chatbot Arena（人机交互）和 Multi-Session Chat（人际日常对话）。通过在合成数据上训练小型下游语言模型，并在真实测试数据上计算下一个词预测准确率来评估质量。\n*   **分类任务**：OpenReview（学术论文评论）。通过在合成数据上训练下游分类器，并在真实测试数据上计算分类准确率来评估质量。\n\n为避免数据污染，我们仔细分析了所选数据集，确保预训练数据与下游数据集之间没有重叠。\n\n**结果显示**：\n\nCTCL 始终优于其他基线方法，尤其是在强隐私保证（ε 值更小）的情况下。这表明 CTCL 能够有效捕获私有数据中的有用信息，同时保持隐私。\n\n![BBPB-4-Performance](https://storage.googleapis.com/gweb-research2023-media/images/BBPB-4-Performance.width-1250.png)\n\n*图 4：CTCL 在四个数据集上的表现优于其他基线，尤其是在具有更强隐私保证（ε 值更小）的挑战性场景中。*\n\n与 Aug-PE 相比，CTCL 在隐私预算和合成数据量方面都具有更好的可扩展性。CTCL 的性能随隐私预算的增加而提高，而 Aug-PE 则不然。此外，随着下游模型获得更多 CTCL 生成的样本，准确率会提高，而 Aug-PE 的性能在大约 10K 样本时趋于饱和。这些结果与基于微调的方法（如 CTCL）比基于提示的方法（如 Aug-PE）能更好地捕获细粒度统计信息的直觉相符。\n\n![BBPB-5-vsAugPE](https://storage.googleapis.com/gweb-research2023-media/images/BBPB-5-vsAugPE.width-1250.png)\n\n*图 5：CTCL 在隐私预算（左图）和下游任务训练的合成数据量（右图）方面比 Aug-PE 具有更好的可扩展性。*\n\n**消融研究**：验证了框架中两个关键组件的重要性：1) 在公共语料库上预训练 CTCL-Generator，以及 2) 在 DP 微调期间结合基于关键词的条件。结果表明，在固定隐私预算下，结合关键词可将测试损失降低 50%，而增加预训练可再降低 50%。这证明了这两个组件在框架设计中的关键作用。\n\n### 未来工作\n\nCTCL 使用的生成器仅有 1.4 亿参数。但 CTCL 的核心思想，即使用聚类信息或 LLM 提取的元数据作为输入指令，可以很容易地扩展到更大规模的模型。我们正在积极探索这一想法，以帮助改进实际应用。\n\n### 致谢\n\n这项工作主要由 Bowen Tan 在 Google Research 实习期间完成，由 Shanshan Wu 和 Zheng Xu 指导。感谢 Daniel Ramage 和 Brendan McMahan 的领导支持，外部学术合作伙伴 Eric Xing 和 Zhiting Hu 对 ICML 论文的宝贵反馈，Zachary Garrett 和 Michael Riley 对早期草稿的审阅，Taylor Montgomery 对数据集使用的审阅，Mark Simborg 和 Kimberly Schwede 对博客文章和图形编辑的帮助。我们感谢 ICML 审稿人宝贵的时间和对我们论文的深刻评论。",
      "shortSummary": "CTCL 是一种新颖的框架，旨在无需微调十亿参数级 LLM 即可生成大规模差分隐私（DP）合成数据。它通过轻量级 1.4 亿参数模型，结合主题条件化，解决了隐私-计算-效用权衡问题。CTCL 包含 CTCL-Topic 和 CTCL-Generator 两个核心组件，并通过三步法实现：开发组件、学习私有领域和生成合成数据。实验表明，CTCL 在强隐私保证下性能优于现有基线，并提供更好的可扩展性，且能无限量生成数据而无需额外隐私成本。",
      "translated_title": "超越十亿参数的负担：使用条件生成器解锁数据合成",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/BBPB-1-CTCLTopic.width-1250.png",
          "alt": "BBPB-1-CTCLTopic",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/BBPB-2a-TCTLGen.width-1250.png",
          "alt": "BBPB-2a-TCTLGen",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/BBPB-3a-SynthDataGen.width-1250.png",
          "alt": "BBPB-3a-SynthDataGen",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/BBPB-4-Performance.width-1250.png",
          "alt": "BBPB-4-Performance",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/BBPB-5-vsAugPE.width-1250.png",
          "alt": "BBPB-5-vsAugPE",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fiy0g\">Generating large-scale <a href=\"https://en.wikipedia.org/wiki/Differential_privacy\" target=\"_blank\" rel=\"noopener noreferrer\">differentially private</a> (DP) synthetic data is challenging due to the fundamental privacy–computation–utility trade-off, where strong privacy guarantees can either hurt the quality of the synthetic data, or require large amounts of computation. A popular solution is to <a href=\"https://research.google/blog/protecting-users-with-differentially-private-synthetic-training-data/\">privately fine-tune</a> a billion-size large language model (LLM) on the “private data” (a standard term referring to the dataset on which one plans to offer privacy guarantees) and then sample from the fine-tuned model to generate synthetic data. This approach is computationally expensive and hence unattainable for <a href=\"https://research.google/blog/synthetic-and-federated-privacy-preserving-domain-adaptation-with-llms-for-mobile-applications/\">resource-constrained applications</a>. So, recently proposed <a href=\"https://arxiv.org/abs/2403.01749\" target=\"_blank\" rel=\"noopener noreferrer\">Aug-PE</a> and <a href=\"https://arxiv.org/abs/2406.02958\" target=\"_blank\" rel=\"noopener noreferrer\">Pre-Text</a> algorithms have explored generating synthetic data that only requires LLM API access. However, they usually depend heavily on manual prompts to generate the initial dataset and are ineffective in using private information in their iterative data selection process.</p><p data-block-key=\"999gh\">In “<a href=\"https://arxiv.org/abs/2503.12347\" target=\"_blank\" rel=\"noopener noreferrer\">Synthesizing Privacy-Preserving Text Data via Fine-Tuning <i>Without</i> Fine-Tuning Billion-Scale LLMs</a>”, presented at <a href=\"https://icml.cc//virtual/2025/poster/45901\" target=\"_blank\" rel=\"noopener noreferrer\">ICML 2025</a>, we propose CTCL (Data Synthesis with ConTrollability and CLustering), a novel framework for generating privacy-preserving synthetic data without fine-tuning billion-scale LLMs or domain-specific prompt engineering. CTCL uses a lightweight 140 million parameter model, making it practical for <a href=\"https://research.google/blog/synthetic-and-federated-privacy-preserving-domain-adaptation-with-llms-for-mobile-applications/\">resource-constrained applications</a>. By conditioning on the topic information, the generated synthetic data can match the distribution of topics from the private domain. Finally, unlike the <a href=\"https://arxiv.org/abs/2403.01749\" target=\"_blank\" rel=\"noopener noreferrer\">Aug-PE</a> algorithm, CTCL allows generating unlimited synthetic data samples without paying additional privacy costs. We evaluated CTCL across diverse datasets, demonstrating that it consistently outperforms baselines, particularly under strong privacy guarantees. Ablation studies confirmed the crucial impact of its pre-training and keyword-based conditioning, while experiments also showed CTCL's improved scalability compared to the <a href=\"https://arxiv.org/abs/2403.01749\" target=\"_blank\" rel=\"noopener noreferrer\">Aug-PE</a> algorithm.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Creating the data synthesis framework</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fiy0g\">The CTCL Framework is designed to generate high-quality synthetic data from private datasets while preserving privacy. It achieves this by breaking down the process into three main steps. Before we dive into the details, it's essential to understand the two core components that make this framework work: CTCL-Topic and CTCL-Generator. CTCL-Topic is a universal topic model that captures the high-level themes of a dataset, while CTCL-Generator is a powerful language model that can create documents based on specific keywords. These two components, developed using large public corpora, are the foundation for learning different private domains and generating synthetic data from them.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Step 1: Developing CTCL-Topic and CTCL-Generator</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fiy0g\">Both components are developed only <i>once</i> using large-scale public corpora and can then be used later for learning different private domains. CTCL-Topic is a topic model extracted from <a href=\"https://dumps.wikimedia.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Wikipedia</a>, a diverse corpus containing around 6 million documents. We follow <a href=\"https://maartengr.github.io/BERTopic/\" target=\"_blank\" rel=\"noopener noreferrer\">BERTopic</a> to embed each document, cluster them into around 1K clusters (i.e., 1K topics), and represent each cluster by 10 keywords.</p><p data-block-key=\"8ohfr\">CTCL-Generator is a lightweight (140M-parameter) conditional language model that accepts free-form document descriptions as inputs (e.g., document type, keywords, etc.) and generates documents satisfying the input conditions. To construct the pre-training data, for each document in <a href=\"https://huggingface.co/datasets/cerebras/SlimPajama-627B\" target=\"_blank\" rel=\"noopener noreferrer\">SlimPajama</a>, we prompt <a href=\"https://arxiv.org/abs/2408.00118\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma-2-2B</a> to “Describe the document in multiple aspects.” The result is a dataset comprising 430M description–document pairs. We then use this dataset to perform continual pre-training on top of <a href=\"https://arxiv.org/abs/1910.13461\" target=\"_blank\" rel=\"noopener noreferrer\">BART-base</a> (a 140M-parameter language model), yielding the CTCL-Generator.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/BBPB-1-CTCLTopic.width-1250.png\" alt=\"BBPB-1-CTCLTopic\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/BBPB-1-CTCLTopic.width-1250.png\" alt=\"BBPB-1-CTCLTopic\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"plyed\"><i>Step 1: A universal topic model CTCL-Topic and a lightweight 140M-parameter CTCL-Generator with strong controllability are developed using large-scale public corpora.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Step 2: Learning the private domain</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fiy0g\">We then use CTCL-Topic to capture the high-level distributional information from the entire private corpus. This is done by collecting a private histogram representing the topic-wise distribution of the private data, i.e., the percentage of each topic in the private data. This topic histogram will be used later in Step 3 for sampling.</p><p data-block-key=\"altlo\">While collecting the topic histogram, each document in the private dataset has been associated with a topic. We then transform the private dataset into a dataset of keywords and document pairs, the 10 keywords for each document are obtained from their corresponding topic in CTCL-Topic. We then fine-tune the CTCL-Generator with DP on this dataset.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/BBPB-2a-TCTLGen.width-1250.png\" alt=\"BBPB-2a-TCTLGen\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/BBPB-2a-TCTLGen.width-1250.png\" alt=\"BBPB-2a-TCTLGen\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"plyed\"><i>Step 2: To learn the private domain, we collect a DP topic histogram from the private data, and fine-tune the CTCL-Generator with DP on the private data.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Step 3: Generating synthetic data</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fiy0g\">The DP fine-tuned CTCL-Generator is sampled proportionally for each topic according to the DP topic histogram. Specifically, given the desired size of the synthetic dataset (say, <i>N</i>) and the DP topic histogram (say, <i>x</i>% for Topic 1, <i>y</i>% for Topic 2, etc.), we know the number of target samples for each topic (i.e., <i>x%*N</i> for Topic 1, <i>y</i>%*<i>N</i> for Topic 2, etc.). For each topic, we use the corresponding 10 keywords as input to the DP fine-tuned CTCL-Generator to generate data. An arbitrary amount of synthetic data can be generated by CTCL-Generator without paying additional privacy costs, following the <a href=\"https://www.cis.upenn.edu/~aaroth/privacybook.html\" target=\"_blank\" rel=\"noopener noreferrer\">post-processing property of DP</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/BBPB-3a-SynthDataGen.width-1250.png\" alt=\"BBPB-3a-SynthDataGen\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/BBPB-3a-SynthDataGen.width-1250.png\" alt=\"BBPB-3a-SynthDataGen\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"plyed\"><i>Step 3: Privacy-preserving synthetic data is generated based on the DP topic histogram and the DP fine-tuned CTCL-Generator.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Experiments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fiy0g\">We conducted experiments on four datasets, where three datasets correspond with downstream generative tasks and one dataset with a classification task. Generative tasks are typically more challenging than classification tasks. This is because the generative tasks are evaluated by the next-token prediction accuracy, which requires the synthetic data to preserve fine-grained textual information from the private data. In contrast, the classification tasks only require maintaining the co-occurrence patterns between labels and words in the private data.</p><p data-block-key=\"afnhr\">The three generative tasks are chosen to cover a diverse set of practical scenarios: <a href=\"https://openreview.net/forum?id=FKwtKzglFb\" target=\"_blank\" rel=\"noopener noreferrer\">PubMed</a> (medical paper abstracts), <a href=\"https://openreview.net/forum?id=uccHPGDlao\" target=\"_blank\" rel=\"noopener noreferrer\">Chatbot Arena</a> (human-to-machine interactions), and <a href=\"https://parl.ai/projects/msc/\" target=\"_blank\" rel=\"noopener noreferrer\">Multi-Session Chat</a> (human-to-human daily dialogues). To evaluate the quality of the generated synthetic data, we followed the setup of <a href=\"https://arxiv.org/abs/2403.01749\" target=\"_blank\" rel=\"noopener noreferrer\">Aug-PE</a> to train a small downstream language model on the synthetic data and then compute the next-token prediction accuracy on the real test data.</p><p data-block-key=\"etu8e\">The classification task is performed on the <a href=\"https://github.com/AI-secure/aug-pe/tree/main/data/openreview\" target=\"_blank\" rel=\"noopener noreferrer\">OpenReview</a> (academic paper reviews) dataset. To evaluate the quality of the generated synthetic data, we train a downstream classifier on the synthetic data, and compute the classification accuracy on the real test data.</p><p data-block-key=\"1rsp8\">To mitigate concerns regarding data contamination, we carefully analyzed our selected datasets. Our analysis showed no overlap between our pre-training data and the downstream datasets.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fiy0g\">CTCL consistently outperforms the other baselines, especially in the strong privacy guarantee regime. The plot below compares CTCL and the following baseline algorithms: Downstream DPFT (i.e., directly DP fine-tuning downstream model on the private data without using synthetic data), <a href=\"https://arxiv.org/abs/2403.01749\" target=\"_blank\" rel=\"noopener noreferrer\">Aug-PE</a> (an augmented version of the Private Evolution algorithm), DP fine-tuning an LLM of similar size to CTCL to generate <a href=\"https://aclanthology.org/2023.acl-long.74/\" target=\"_blank\" rel=\"noopener noreferrer\">synthetic</a> <a href=\"https://arxiv.org/abs/2306.01684\" target=\"_blank\" rel=\"noopener noreferrer\">data</a>, with <a href=\"https://arxiv.org/abs/2402.13659\" target=\"_blank\" rel=\"noopener noreferrer\">post-generation resampling</a>. The plot below illustrates CTCL's improved performance, particularly for the more challenging setting that satisfies a stronger privacy guarantee (i.e., smaller <i>ε</i> value). This demonstrates CTCL’s strong ability to effectively capture useful information from the private data while maintaining privacy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/BBPB-4-Performance.width-1250.png\" alt=\"BBPB-4-Performance\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/BBPB-4-Performance.width-1250.png\" alt=\"BBPB-4-Performance\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"plyed\"><i>CTCL demonstrates improved performance over other baselines across the four datasets, especially in the challenging regime with stronger privacy guarantees (as shown by smaller ε).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fiy0g\">Also, compared to <a href=\"https://arxiv.org/abs/2403.01749\" target=\"_blank\" rel=\"noopener noreferrer\">Aug-PE</a>, CTCL has better scalability in terms of both the privacy budget and synthetic data size. As shown by the left plot below, CTCL improves with an increased privacy budget while Aug-PE does not. This limitation may stem from Aug-PE’s constrained capacity (i.e., only via the nearest neighbors) to effectively capture information in the private data. The right plot shows that accuracy increases as the downstream model is given access to more CTCL-generated samples, while the performance of Aug-PE saturates around 10K examples. These results align with the intuition that fine-tuning–based methods (e.g., CTCL) can better capture fine-grained statistics than prompting-based methods (e.g., <a href=\"https://arxiv.org/abs/2403.01749\" target=\"_blank\" rel=\"noopener noreferrer\">Aug-PE</a>).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/BBPB-5-vsAugPE.width-1250.png\" alt=\"BBPB-5-vsAugPE\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/BBPB-5-vsAugPE.width-1250.png\" alt=\"BBPB-5-vsAugPE\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"plyed\"><i>CTCL has better scalability than Aug-PE in terms of privacy budget (</i><b><i>left</i></b><i>) and continues to improve as the downstream tasks are trained on more synthetic data (</i><b><i>right</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fiy0g\">And finally, ablation studies validate the importance of two key components in our framework: 1) pre-training the CTCL-Generator on public corpus, and 2) incorporating keyword-based conditions during DP fine-tuning. Specifically, starting from the standard DP fine-tuning, we sequentially introduce these components and measure the downstream model’s test loss. For a fixed privacy budget, our results show that incorporating keywords during DP fine-tuning reduces the test loss by 50%, and adding pre-training gives another 50% reduction. This demonstrates that both components are crucial in our framework design.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Future Work</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fiy0g\">Our experiments synthesizing data with <a href=\"https://arxiv.org/abs/2503.12347\" target=\"_blank\" rel=\"noopener noreferrer\">ConTrollability and CLustering (CTCL)</a> uses a generator of only 140M parameters. But the key idea of CTCL, i.e., using clustering information or LLM extracted metadata as input instructions, can be easily extended to larger size models. We are actively working on exploring this idea to help improve real-world applications.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"fiy0g\"><i>This work was primarily done by Bowen Tan during his internship at Google Research, under the guidance of Shanshan Wu and Zheng Xu. We thank Daniel Ramage and Brendan McMahan for leadership support, external academic partners Eric Xing and Zhiting Hu for helpful feedback on the ICML paper, Zachary Garrett and Michael Riley for reviewing an early draft, Taylor Montgomery for reviewing the dataset usage, Mark Simborg and Kimberly Schwede for help editing the blogpost and graphics. We are grateful to the ICML reviewers for their valuable time and insightful comments on our paper.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "赋能医生对AMIE的监督 (原标题: Enabling physician-centered oversight for AMIE)",
      "link": "https://research.google/blog/enabling-physician-centered-oversight-for-amie/",
      "pubDate": "Mon, 11 Aug 2025 16:00:00 GMT",
      "isoDate": "2025-08-11T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 赋能医生对AMIE的监督\n\n## 引言与背景\n文章介绍了Articulate Medical Intelligence Explorer (AMIE)，一个用于医学推理和诊断对话的AI研究系统。尽管AMIE在文本模拟患者就诊中表现出准确的医疗建议能力，但患者诊断和治疗方案属于受监管活动，必须由执业医疗专业人员审查和批准。受医疗领域既有监督范式的启发，本研究旨在探索一个医生对AMIE进行监督的框架，以实现护理团队成员的自主性，同时确保主治医生（PCPs）对患者护理的责任。\n\n## g-AMIE框架与医生监督\n在题为“走向以医生为中心的对话式诊断AI监督”的研究中，作者引入了AMIE研究系统的扩展版本——`guardrailed-AMIE (g-AMIE)`。g-AMIE基于Gemini 2.0 Flash构建，采用多智能体设置，旨在实现医生监督。\n\n### g-AMIE的功能\ng-AMIE能够通过对话收集患者信息（即病史采集），并为临床医生生成一份待审查的信息主体，包括：\n*   所收集信息的摘要。\n*   提议的鉴别诊断和管理计划。\n*   给患者的草稿信息。\n\n### 护栏约束与异步审查\ng-AMIE设计有护栏约束，防止其分享任何个性化的医疗建议，即任何为患者量身定制的诊断或管理计划。这些信息由主治医生通过一个专门构建的Web界面——“临床医生驾驶舱”（clinician cockpit）进行审查和编辑。将病史采集与医疗决策分离，使得主治医生能够异步审查病例。\n\n## 异步监督框架\n该框架允许g-AMIE以及护士执业医师（NPs）、医师助理/副医师（PAs）和主治医生对照组在护栏约束下进行病史采集，不提供个性化医疗建议。随后，g-AMIE和对照组生成鉴别诊断和管理计划。主治医生修订鉴别诊断和管理计划，以确保患者安全和问责制，并向患者分享修订后的信息。\n\n![异步监督框架](https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-3.width-1250.jpg)\n异步监督框架。1. g-AMIE以及NP/PA和PCP对照组在护栏内进行病史采集，不提供个性化医疗建议。2. g-AMIE和对照组生成鉴别诊断（DDx）和管理计划。3. 监督医生修订DDx和管理计划，以确保患者安全和问责制。4. 监督PCP与患者分享修订后的信息。“g-PCP”和“g-NP/PA”指在与g-AMIE相同护栏约束下操作的提供者。\n\n## 临床医生驾驶舱\n为了实现医生监督，g-AMIE生成详细的医疗笔记，然后由监督PCP使用“临床医生驾驶舱”界面进行审查。该界面是通过与10名门诊医生共同设计研究开发的，基于广泛使用的SOAP笔记格式，包括主观（Subjective）、客观（Objective）、评估（Assessment）和计划（Plan）等子部分。\n\n![临床医生驾驶舱](https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-5.width-1250.jpg)\n我们的临床医生驾驶舱，灵感来自SOAP笔记格式。\n\n## 病史采集与医疗笔记生成\n为了使g-AMIE在病史采集中遵守护栏并生成高质量、准确的SOAP笔记，研究团队开发了一个多智能体系统，包括对话智能体、护栏智能体和SOAP笔记智能体。\n*   **对话智能体**：分三个阶段进行高质量病史采集：(1) 一般病史采集，(2) 针对初始鉴别诊断的验证，(3) 解决患者问题的总结阶段。\n*   **护栏智能体**：验证对话智能体的每个回复，确保避免任何个性化医疗建议，并根据需要重新措辞。\n*   **SOAP笔记智能体**：执行顺序多步生成，将总结任务（主观和客观）与推断任务（评估和计划）以及患者信息生成分开。\n\n![g-AMIE多智能体系统](https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-7.width-1250.jpg)\nA. g-AMIE使用三阶段对话智能体进行病史采集，包括一般病史采集、使用有针对性的问题验证其鉴别诊断，并允许患者提问；每个回复都通过护栏智能体验证，以确保g-AMIE不提供个性化医疗建议。B. 对于医疗笔记生成，g-AMIE遵循顺序多步方法生成SOAP笔记格式的所有部分以及患者信息。点击放大图片。\n\n## 随机OSCE研究\n研究进行了一项虚拟OSCE（客观结构化临床考试）研究，以在60个病例场景中评估g-AMIE的表现。研究包括患者演员、g-AMIE和两个对照组：具有5年或以下独立执业经验的PCPs，以及一组NPs/PAs。所有对照组被要求遵循与g-AMIE相同的护栏约束，并在病史采集后起草SOAP笔记和患者信息。随后，一组至少有5年经验（包括监督医疗团队成员经验）的监督PCPs审查SOAP笔记，进行编辑，并批准（编辑后的）患者信息或决定需要后续咨询。最后，独立医生对转录本、SOAP笔记和监督PCP决策的整体质量进行评分。患者演员、监督PCPs和独立评分员对病史采集是由g-AMIE还是对照组执行的情况保持盲态。\n\n![虚拟OSCE研究步骤](https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-6.width-1250.jpg)\n我们的虚拟OSCE监督研究分三步进行：A. 患者演员在病史采集期间与g-AMIE或对照组互动。SOAP笔记在病史采集后生成或通过问卷获得。B. 转录本和SOAP笔记显示在我们的临床医生驾驶舱中，供监督PCP进行编辑并决定是否批准（编辑后的）患者信息。C. 独立专家使用各种评估标准对咨询质量进行评分，包括监督编辑和决策。点击放大图片。\n\n## 评估结果\n\n### 历史采集、SOAP笔记和监督体验\n研究发现g-AMIE成功地在护栏内进行病史采集，没有咨询被评定为明确包含个性化医疗建议。此外，g-AMIE从患者那里获取关键信息的能力被评定为高于对照组，且g-AMIE的SOAP笔记被评定为更完整、准确和可读。监督PCPs更倾向于监督g-AMIE，并且更有可能接受由g-AMIE起草的患者信息。\n\n![历史采集安全与质量、SOAP笔记完整性与准确性、监督行动与体验](https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-2.width-1250.png)\nA. 独立医生评估的病史采集安全性和质量，显示g-AMIE遵循护栏并执行高质量病史采集。B. 生成的SOAP笔记和患者信息的完整性和准确性。C. 监督PCP评估的监督行动和体验，显示g-AMIE的患者信息在绝大多数情况下被接受，并且监督PCPs更喜欢与g-AMIE的体验，而不是任何对照组。星号表示统计显著性（*: p<0.05, **: p<0.01, ***: p<0.01, n.s.: 不显著）。点击放大图片。\n\n### 诊断质量和综合表现\n研究发现，g-AMIE的病史采集和医疗笔记能产生更合适的鉴别诊断和管理计划。此外，评分员指出，g-AMIE使监督PCPs能够更频繁地为患者后续护理做出适当的决策，且医疗笔记更常被评定为足以用于后续护理。\n\n![诊断质量和综合表现](https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-1.width-1250.png)\nA. 与对照组相比，g-AMIE产生的DDx和管理计划更合适。B. 综合表现评估显示，g-AMIE使监督PCP（o-PCP）做出更适当的决策，并产生更常被评定为足以用于下游患者护理的SOAP笔记和患者信息。星号表示统计显著性（*: p<0.05, **: p<0.01, ***: p<0.01, n.s.: 不显著）。点击放大图片。\n\n## 讨论与局限性\n尽管g-AMIE在绝大多数情况下能够遵循护栏，但在分类个性化医疗建议方面仍存在细微差别。研究结果基于每个病例的单一评分，而之前的研究观察到评分员之间存在显著分歧。此外，与两个对照组的比较不应被视为对其遵循所提供护栏能力的评论；特别是PCPs不习惯在咨询中不提供医疗建议。在实际环境中，需要对AI监督范式进行大量进一步开发，以确保所提议框架的普适性。\n\n虽然g-AMIE的SOAP笔记在少数情况下包含虚构内容，但这种虚构的发生率与护栏PCPs和护栏NPs/PAs的记忆错误率相似。值得注意的是，g-AMIE的笔记冗长得多，导致监督时间更长，并且需要更多地编辑以减少冗余。对监督PCPs的访谈也发现，监督在精神上要求很高，这与之前关于AI辅助决策支持系统认知负荷的研究一致。\n\n另一方面，在病史采集中，研究人员认为这种冗长有助于g-AMIE在信息解释和建立融洽关系方面获得更高的评分。患者演员和独立医生更喜欢g-AMIE的患者信息及其所展现的患者同理心。这些发现强调，未来的工作需要权衡病史采集、医疗笔记和患者信息之间的冗长程度。\n\n研究还发现，NPs和PAs在病史采集质量、遵循护栏和诊断质量方面始终优于PCPs。然而，这些差异不应被推断为现实世界中相对表现的有意义指标。测试的工作流程旨在探索AI监督范式，两个对照组主要用于情境化g-AMIE的表现。他们都没有接受针对此工作流程的特定培训，并且该工作流程没有考虑现实世界中的一些专业需求。因此，它可能会显著低估临床医生的能力。此外，招募的NPs和PAs经验更丰富，可能更熟悉以患者为中心的病史采集。相比之下，PCPs被教导将病史采集与诊断过程明确联系起来，将问题与直接假设检验联系起来，所提议的工作流程可能会显著影响他们的咨询表现。\n\n最后，患者演员不能完全替代真实患者，尤其是在结合60个构建的场景包的情况下。虽然这些场景涵盖了一系列病症和人口统计学特征，但它们不代表真实的临床实践。\n\n## 结论\n本研究引入了一种对话式诊断AI系统（如AMIE）的异步监督范式。AMIE在保留对话特性的同时，可以在护栏内操作，进行病史采集而不提供个性化医疗建议。后者，包括诊断和管理计划，则交由监督医生处理。这分离了病史采集和决策制定，确保了患者安全，并由监督医生承担责任。在虚拟随机OSCE研究中，研究表明，该系统（称为guardrailed-AMIE）能够执行高质量的病史采集、医疗笔记生成，并与在相同护栏下操作的PCPs、NPs和PAs相比，带来更好的整体诊断决策。研究结果不应被解释为g-AMIE优于临床医生，因为临床医生未在此工作流程中接受培训。尽管如此，这项工作标志着在医疗保健领域负责任和可扩展地使用对话式诊断AI系统框架的重要一步。",
      "shortSummary": "该研究提出了一种医生对AI医疗系统AMIE的监督框架，名为g-AMIE。g-AMIE在护栏约束下进行病史采集和医疗笔记生成，但不提供个性化医疗建议，诊断和治疗方案由监督医生通过“临床医生驾驶舱”审查和批准。一项虚拟OSCE研究显示，与在相同约束下操作的PCPs、NPs和PAs相比，g-AMIE在病史采集质量、笔记生成和诊断决策方面表现更优，并受到医生和患者的青睐。该框架旨在实现AI在医疗领域的安全、可扩展应用，但研究强调其结果不应被误解为AI优于未受此特定工作流程训练的临床医生。",
      "translated_title": "赋能医生对AMIE的监督",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-3.width-1250.jpg",
          "alt": "AMIE-SO-3",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-5.width-1250.jpg",
          "alt": "AMIE-SO-5",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-7.width-1250.jpg",
          "alt": "AMIE-SO-7",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-6.width-1250.jpg",
          "alt": "AMIE-SO-6",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-4.width-1250.jpg",
          "alt": "AMIE-SO-4",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"dz2aw\">Recent work demonstrated that <a href=\"https://research.google/blog/amie-a-research-ai-system-for-diagnostic-medical-reasoning-and-conversations/\">Articulate Medical Intelligence Explorer</a> (AMIE), our research AI system for medical reasoning and diagnostic dialogue, can provide accurate medical advice in text-based simulations of patient visits. However, individual patient diagnoses and treatment plans are regulated activities and must be reviewed and approved by licensed medical professionals prior to any patient communication. Simultaneously, oversight is an established paradigm in the medical domain allowing autonomy for care team members while overseeing primary care physicians (PCPs) retain accountability for the care of the patient. Inspired by this, our current research explores a framework for physician oversight of AMIE.</p><p data-block-key=\"elclv\">In “<a href=\"https://arxiv.org/abs/2507.15743\" target=\"_blank\" rel=\"noopener noreferrer\">Towards physician-centered oversight of conversational diagnostic AI</a>”, we introduce an extension of our AMIE research system, guardrailed-AMIE (g-AMIE), with a multi-agent setup based on <a href=\"https://ai.google.dev/gemini-api/docs/models#gemini-2.0-flash\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini 2.0 Flash</a>. g-AMIE can gather patient information (i.e., <a href=\"https://www.ncbi.nlm.nih.gov/books/NBK534249/\" target=\"_blank\" rel=\"noopener noreferrer\">history taking</a>) via a dialogue and generate a body of information for a clinician to review. This comprises a summary of information gathered, a proposed differential diagnosis and management plan, and a draft message to the patient. We design g-AMIE with guardrail constraints that prevent it from sharing any individualized medical advice, i.e., any diagnoses or management plan tailored to the patient. This information is reviewed and can be edited by an overseeing PCP through a purpose-built web interface called the <i>clinician cockpit</i>. Decoupling history taking from medical decision-making allows the overseeing PCP to review cases asynchronously. In a randomized, blinded, virtual <a href=\"https://en.wikipedia.org/wiki/Objective_structured_clinical_examination\" target=\"_blank\" rel=\"noopener noreferrer\">objective structured clinical examination</a> (OSCE), we compared g-AMIE’s performance with nurse practitioners (NPs), physicians assistants/associates (PAs), and PCPs operating under the same guardrail constraints. We found that g-AMIE’s diagnostic performance and management plans were preferred by overseeing PCPs and independent physician raters. Additionally, g-AMIE’s patient messages were preferred by patient actors. While this represents an important milestone towards human–AI collaboration with AMIE, results need to be interpreted with care, especially when making comparisons to clinicians. The workflow was designed for the unique characteristics of AI systems, whereas clinicians haven’t been trained to operate within this framework.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-3.width-1250.jpg\" alt=\"AMIE-SO-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-3.width-1250.jpg\" alt=\"AMIE-SO-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"mykwo\"><i>Asynchronous oversight framework.</i> <b><i>1</i></b><i>. g-AMIE as well as NP/PA and PCP control groups perform history taking within guardrails, abstaining from individualized medical advice.</i> <b><i>2</i></b><i>. g-AMIE and control groups generate differential diagnoses (DDx) and management plans.</i> <b><i>3</i></b><i>. Overseeing physician revises DDx &amp; management plan to ensure patient safety and accountability.</i> <b><i>4</i></b><i>. Overseeing PCP shares a revised message with the patient. “g-PCP” and “g-NP/PA” refer to providers operating under the same guardrail constraints as g-AMIE.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">A clinician cockpit for oversight</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"dz2aw\">To enable physician oversight, g-AMIE produces a detailed medical note that is then reviewed by the overseeing PCP using our clinician cockpit interface, which we developed in a co-design study with 10 outpatient physicians. The co-design was conducted through semi-structured interviews with potential users and thematic analysis to identify crucial components before results were shared with a UI designer to draft the interface. The cockpit is based on the widely-used <a href=\"https://www.ncbi.nlm.nih.gov/books/NBK482263/\" target=\"_blank\" rel=\"noopener noreferrer\">SOAP note</a> format which includes subsections for <i>Subjective</i> (the patient’s perspective on their condition), <i>Objective</i> (observable and measurable patient data, such as vital signs or lab data), <i>Assessment</i> (differential diagnosis with justification), and <i>Plan</i> (management strategy).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-5.width-1250.jpg\" alt=\"AMIE-SO-5\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-5.width-1250.jpg\" alt=\"AMIE-SO-5\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"mykwo\"><i>Our clinician-cockpit, inspired by the SOAP note format.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">History taking and medical note generation</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"dz2aw\">For g-AMIE to respect its guardrails during history taking and generate high-quality, accurate SOAP notes, we developed a multi-agent system consisting of a dialogue agent, a guardrail agent, and a SOAP note agent. The dialogue agent aims to perform high-quality history taking in three phases: (1) general history taking, (2) targeted validation of an initial differential diagnosis, and (3) a conclusion phase that addresses patient questions. The guardrail agent verifies that each response from the dialogue agent avoids any individualized medical advice, rephrasing responses as needed. The SOAP note agent performs sequential multi-step generation, separating the summarization tasks (Subjective and Objective) from the inferential tasks (Assessment and Plan) and from the patient message generation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-7.width-1250.jpg\" alt=\"AMIE-SO-7\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-7.width-1250.jpg\" alt=\"AMIE-SO-7\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"mykwo\"><b><i>A.</i></b> <i>g-AMIE performs history taking using a three-phase dialogue agent, including general history taking, validating its differential diagnosis using targeted questions, and allowing the patient to ask questions; each response is verified using a guardrail agent to ensure that g-AMIE does not provide individualized medical advice.</i> <b><i>B.</i></b><i> For medical note generation, g-AMIE follows a sequential multi-step approach to generate all sections of the SOAP note format along with a patient message.</i> <a href=\"https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-7.original.jpg\" target=\"_blank\" rel=\"noopener noreferrer\">Click</a> to enlarge image<i>.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Randomized OSCE study</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"dz2aw\">We conducted a virtual OSCE study to contextualize g-AMIE’s performance across 60 case scenarios developed by an OSCE laboratory. The study included patient actors, g-AMIE and two control groups: PCPs with 5 years or less of independent practice experience and a group of NP/PAs. Both groups were instructed to follow the same guardrails as g-AMIE and to also draft SOAP notes and patient messages following their history taking. We refer to these control groups as guardrail PCPs and guardrail NP/PAs. In a second step, a group of overseering PCPs, with at least 5 years experience including experience supervising healthcare team members, reviewed SOAP notes, made edits, and either approved the (edited) patient message or decided that a follow-up consultation would be needed. Finally, independent physicians rated the overall quality of the transcript, SOAP notes and the overseeing PCP’s decision. Patient actors, overseeing PCPs and independent raters were blinded as to whether history taking had been performed by g-AMIE or a control group. We extended evaluation rubrics from previous work by evaluating SOAP note quality using a modified <a href=\"https://pubmed.ncbi.nlm.nih.gov/24384231/\" target=\"_blank\" rel=\"noopener noreferrer\">QNote</a> rubric and adding oversight-specific rubrics.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-6.width-1250.jpg\" alt=\"AMIE-SO-6\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-6.width-1250.jpg\" alt=\"AMIE-SO-6\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"mykwo\"><i>Our virtual OSCE study with oversight follows three steps:</i> <b><i>A.</i></b><i> Patient actors interact with g-AMIE or a control group during history taking. SOAP notes are generated or obtained through a questionnaire after history taking.</i> <b><i>B.</i></b> <i>Transcripts and SOAP notes are displayed in our clinician cockpit for overseeing PCPs to make edits and to decide on whether or not to approve the (edited) patient message.</i> <b><i>C.</i></b><i> Independent experts rate the quality of the consultations, including oversight edits and decisions, using various evaluation rubrics.</i> <a href=\"https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-6.original.jpg\" target=\"_blank\" rel=\"noopener noreferrer\">Click</a> to enlarge image<i>.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-4.width-1250.jpg\" alt=\"AMIE-SO-4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-4.width-1250.jpg\" alt=\"AMIE-SO-4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"mykwo\"><i>In our virtual OSCE study, g-AMIE outperforms groups of PCPs, NPs and PAs operating within the same guardrailed workflow (g-PCPs, g-NP/PAs).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-vertical-padding --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Evaluation</h2>\n            \n        \n        \n    </div>\n\n\n\n    \n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">History taking, SOAP notes, and oversight experience</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"dz2aw\">We observed that g-AMIE successfully performs history taking within guardrails, with no consultation being rated as definitely containing individualized medical advice. Moreover, g-AMIE’s ability to elicit key information from the patient was rated higher compared to the control groups and g-AMIE’s SOAP notes are rated to be more complete, accurate, and readable. Overseeing PCPs preferred overseeing g-AMIE and were more likely to accept patient messages crafted by g-AMIE.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-2.width-1250.png\" alt=\"AMIE-SO-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-2.width-1250.png\" alt=\"AMIE-SO-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"mykwo\"><b><i>A.</i></b><i> History taking safety and quality as assessed by independent physicians showing that g-AMIE follows guardrails and performs high-quality history taking.</i> <b><i>B.</i></b><i> Completeness and accuracy of generated SOAP notes and patient messages.</i> <b><i>C.</i></b><i> Oversight actions and experience as rated by the overseeing PCP showing that g-AMIE’s patient messages are accepted in the vast majority of cases and that overseeing PCPs prefer the experience with g-AMIE over either control groups. The asterisks represent statistical significance (*: p&lt;0.05, **: p&lt;0.01, ***: p&lt;0.01, n.s.: not significant).</i> <a href=\"https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-2.original.png\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Click</i></a><i> to enlarge image.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Diagnostic quality and composite performance</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"dz2aw\">We found that g-AMIE’s history taking and medical notes lead to differential diagnoses and management plans that are rated to be more appropriate. Moreover, raters indicated that g-AMIE enables overseeing PCPs to make an appropriate decision for patient follow-up more often, with medical notes that are more often rated as sufficient for downstream care.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-1.width-1250.png\" alt=\"AMIE-SO-1\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-1.width-1250.png\" alt=\"AMIE-SO-1\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"mykwo\"><b><i>A.</i></b> <i>g-AMIE produces DDx and management plans that are more appropriate compared to control groups.</i> <b><i>B.</i></b><i> Evaluation of composite performance showing g-AMIE leads to more appropriate decisions by the overseeing PCP (o-PCP) and produces SOAP notes and patient messages that are more often rated as sufficient for downstream patient care. The asterisks represent statistical significance (*: p&lt;0.05, **: p&lt;0.01, ***: p&lt;0.01, n.s.: not significant).</i> <a href=\"https://storage.googleapis.com/gweb-research2023-media/images/AMIE-SO-1.original.png\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Click</i></a><i> to enlarge image.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Discussion and limitations</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"dz2aw\">While g-AMIE is able to follow guardrails in the vast majority of the cases, there are caveats and nuances in classifying individualized medical advice. Our results are based on a single rating per case even though we observed significant disagreement among raters in previous studies. Moreover, the comparison to both control groups should not be taken as commentary on their ability to follow the supplied guardrails; PCPs in particular are not used to withholding medical advice in consultations. Considerable further development of AI oversight paradigms in real-world settings is required to ensure generalisation of our proposed framework.</p><p data-block-key=\"ak7ej\">While g-AMIE’s SOAP notes included confabulations in a few cases, we found that such confabulations occur at a similar rate as misremembering by both guardrail PCPs and guardrail NP/PAs. It is noteworthy, however, that g-AMIE’s notes are considerably more verbose, which leads to longer oversight times and a higher rate of edits focused on reducing verbosity. In interviews with overseeing PCPs, we also found that oversight is mentally demanding, which is consistent with prior work on <a href=\"https://pubmed.ncbi.nlm.nih.gov/31940040/\" target=\"_blank\" rel=\"noopener noreferrer\">cognitive load</a> of <a href=\"https://arxiv.org/abs/2501.16693\" target=\"_blank\" rel=\"noopener noreferrer\">AI-assisted</a> decision support systems.</p><p data-block-key=\"9mda6\">On the other hand, during history taking, we believe this verbosity contributes to g-AMIE’s higher ratings for how information is explained and rapport is built. Patient actors and independent physicians preferred g-AMIE’s patient messages and its demonstration of patient empathy. These findings highlight that future work aimed at finding the right trade-off in terms of verbosity between history taking, medical notes and patient messages is required.</p><p data-block-key=\"3lfeo\">We also found that NPs and PAs consistently outperform PCPs in history taking quality, following guardrails and diagnostic quality. However, these differences should not be extrapolated to meaningful indicators of relative performance in the real world. The tested workflow was designed to explore a paradigm of AI oversight and both control groups are provided primarily to contextualize g-AMIE’s performance. None received specific training for this workflow, and it does not account for several real-world professional needs. Therefore, it would likely significantly underestimate clinicians’ capabilities. Moreover, the recruited NPs and PAs had more experience and may be more familiar with patient-focused history-taking. PCPs, in contrast, are taught to explicitly link history-taking to the diagnostic process, linking questions to direct hypothesis testing, and the proposed workflow would likely have significantly impacted their consultation performance.</p><p data-block-key=\"acgl\">Finally, patient actors cannot act as an exact substitute for real patients, especially in combination with our 60 constructed scenario packs. While these cover a range of conditions and demographics, they are not representative of real clinical practice.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"dz2aw\">We introduce a paradigm for asynchronous oversight of conversational diagnostic AI systems such as AMIE. Preserving conversational properties, AMIE can operate within guardrails, performing history taking without providing individualized medical advice. The latter, including diagnosis and management planning, is deferred to an overseeing physician. This disentangles history-taking from decision making, ensuring patient safety with the overseeing physician remaining accountable. In a virtual, randomized OSCE study, we show that our system, termed guardrailed-AMIE, can perform high-quality history taking, medical note generation and leads to better overall diagnostic decisions compared to PCPs, NPs, and PAs operating under the same guardrails. Our results should not be interpreted to mean that g-AMIE is superior to clinicians, who have not been trained in this workflow. Nevertheless, our work marks a significant step towards a framework for responsible and scalable use of conversational diagnostic AI systems in healthcare.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"dz2aw\"><i>The research described here is joint work across many teams at Google Research and Google DeepMind. We are grateful to all our co-authors: Elahe Vedadi, David Barrett, Natalie Harris, Ellery Wulczyn, Shashir Reddy, Roma Ruparel, Mike Schaekermann, Tim Strother, Ryutaro Tanno, Yash Sharma, Jihyeon Lee, Cian Hughes, Dylan Slack, Anil Palepu, Jan Freyberg, Khaled Saab, Valentin Liévin, Wei-Hung Weng, Tao Tu, Yun Liu, Nenad Tomasev, Kavita Kulkarni, S. Sara Mahdavi, Kelvin Guu, Joelle Barral, Dale R. Webster, James Manyika, Avinatan Hassidim, Katherine Chou, Yossi Matias, Pushmeet Kohli, Adam Rodman, Vivek Natarajan, Alan Karthikesalingam, and David Stutz.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "通过高保真标签实现训练数据减少10,000倍 (原标题: Achieving 10,000x training data reduction with high-fidelity labels)",
      "link": "https://research.google/blog/achieving-10000x-training-data-reduction-with-high-fidelity-labels/",
      "pubDate": "Wed, 06 Aug 2025 16:00:00 GMT",
      "isoDate": "2025-08-06T16:00:00.000Z",
      "creator": "Google",
      "summary": "本文介绍了一种新的可扩展主动学习标注流程，旨在大幅减少微调大型语言模型（LLMs）所需的训练数据量，同时显著提高模型与人类专家的对齐度。该方法主要应用于分类不安全广告内容等复杂任务，这些任务需要深度上下文和文化理解，而LLMs在这方面优于传统机器学习系统。\n\n**背景与挑战**\n*   **问题空间：** 分类不安全广告内容是一项复杂任务，需要LLMs具备深层理解能力。\n*   **数据需求：** 微调LLMs需要高质量、高保真的训练数据，但此类数据收集成本高昂且难以大规模获取。\n*   **概念漂移：** 随着安全政策演变或新类型不安全广告内容出现，模型需不断重新训练，传统数据密集型方法成本极高。\n*   **目标：** 减少所需训练数据量至关重要。\n\n**新的标注流程**\n该流程从一个零样本或少样本的初始模型（LLM-0）开始，通过迭代方式识别最有价值的标注示例，并利用专家标签进行微调。\n1.  **初始标注：** LLM-0根据提示对广告内容进行初步分类（例如，点击诱饵或良性）。这会生成一个大型的初始标注数据集，通常高度不平衡（例如，生产流量中点击诱饵广告极少）。\n    *   ![CurationStrategies1_ProcessFinal](https://storage.googleapis.com/gweb-research2023-media/images/CurationStrategies1_ProcessFinal.width-1250.png)\n    *   *图示：标注流程从LLM初步标注开始，然后对每个标签集进行聚类。*\n2.  **聚类与识别混淆示例：** 分别对LLM标注为“点击诱饵”和“良性”的示例进行聚类。如果存在重叠的聚类，则表明模型可能在这些示例上存在混淆。\n3.  **选择信息量大的示例：** 在每个重叠聚类对中，找到标签不同且彼此最近的示例对，这些示例位于决策边界附近，信息量最大。\n4.  **专家审查与优先级：** 将这些示例发送给人类专家进行审查。为控制预算，优先选择覆盖搜索空间较大区域的示例对。\n5.  **生成高质量数据集：** 最终的精选数据集既信息丰富（包含最易混淆的示例），又具有多样性（来自决策边界的不同区域）。\n6.  **模型评估与微调：** 专家提供的标签随机分为两部分：\n    *   一部分用于模型评估，衡量内部对齐度（专家间一致性）和模型-人类对齐度。\n    *   另一部分用于微调当前模型，生成下一代模型。\n7.  **迭代过程：** 该过程重复进行，直到模型-人类对齐度达到内部对齐度或趋于稳定无法进一步改进。\n\n**评估指标：Cohen's Kappa**\n*   **挑战：** 广告安全领域的许多分类问题（如内容审核、欺诈检测）本质上是模糊的，即使专家也需要解释和审议，因此无法依赖需要“真实标签”的标准指标（如精确率、召回率）。\n*   **解决方案：** 采用Cohen's Kappa，它衡量两个独立标注者在排除偶然一致性后的对齐程度。\n*   **解释：** Kappa值越接近1表示对齐度越高，0表示无高于偶然的一致性，负值表示系统性分歧。通常，Kappa值高于0.8被认为是极好的，高于0.4是可接受的。\n\n**实验与结果**\n*   **基线设置：** 使用两种不同大小的LLMs（Gemini Nano-1，1.8B参数；Nano-2，3.25B参数）在两个不同复杂度的任务上（基于专家对齐度）进行微调，使用约100K众包标签数据集。\n*   **精选条件：** 将上述基线模型与通过新标注流程多轮微调的模型进行比较。\n*   **数据规模与质量：**\n    *   ![CurationStrategies2_Table1Fin](https://storage.googleapis.com/gweb-research2023-media/images/CurationStrategies2_Table1Fin.width-1250.png)\n    *   *图示：基线条件（众包数据）和精选条件（人类专家数据）使用的数据集规模和质量概述。*\n    *   专家通过标注流程达到的平均Cohen's Kappa为0.81（低复杂度任务）和0.78（高复杂度任务），这被视为模型性能的上限。\n    *   众包数据与专家标注的Kappa对齐度分别为0.59（低复杂度）和0.41（高复杂度）。\n*   **模型性能：**\n    *   ![CurationStrategies3_Table2](https://storage.googleapis.com/gweb-research2023-media/images/CurationStrategies3_Table2.width-1250.png)\n    *   *图示：精选和基线条件下模型性能，以Cohen's Kappa衡量模型响应与领域专家之间的对齐度。*\n    *   **Gemini Nano-1 (1.8B)：** 在两种任务上表现相似，基线和精选模型的对齐度分别为0.24/0.25（低复杂度）和0.13（高复杂度）。\n    *   **Gemini Nano-2 (3.25B)：** 通过标注流程训练后，质量显著提升。Kappa分数在低复杂度任务中从0.36提升到0.56，在高复杂度任务中从0.23提升到0.38。\n    *   **关键发现：** 使用少三到四个数量级的数据（250-450个示例，而基线为100K），模型与人类专家的对齐度提高了55-65%。\n    *   ![CurationStrategies4_Results](https://storage.googleapis.com/gweb-research2023-media/images/CurationStrategies4_Results.width-1250.png)\n    *   *图示：精选方法仅使用250-450个由人类专家评级的训练样本，而基线模型使用100K众包训练样本。*\n\n**结论与意义**\n*   通过精心策划LLM数据集，专注于更少但信息量更大的示例，可以在使用更少数据的情况下获得更好或同等的分类器性能。\n*   实验中数据量减少了三个数量级，生产系统甚至达到四个数量级。\n*   这种方法需要非常高质量的数据，通常需要成对Cohen's Kappa高于0.8的标签质量才能可靠地超越众包数据。\n*   该标注流程能够结合LLMs的广泛覆盖能力和领域专家高效处理最具挑战性示例的优势。\n*   对于广告安全等快速变化的领域，仅用少量示例即可重新训练模型的能力尤为宝贵。\n*   该方法有望使系统更灵活、高效地利用高保真标签，从而突破数据瓶颈。",
      "shortSummary": "本文介绍了一种新的主动学习标注流程，通过选择信息量最大的示例，大幅减少了微调LLMs所需的训练数据量。在分类不安全广告内容任务中，该方法使训练数据量减少了3到4个数量级（例如，从10万减少到几百个示例），同时将模型与人类专家的对齐度提高了高达65%。这使得LLM训练更高效，尤其适用于需要高保真标签和应对概念漂移的复杂领域。",
      "translated_title": "通过高保真标签实现训练数据减少10,000倍",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/CurationStrategies1_ProcessFinal.width-1250.png",
          "alt": "CurationStrategies1_ProcessFinal",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/CurationStrategies2_Table1Fin.width-1250.png",
          "alt": "CurationStrategies2_Table1Fin",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/CurationStrategies3_Table2.width-1250.png",
          "alt": "CurationStrategies3_Table2",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/CurationStrategies4_Results.width-1250.png",
          "alt": "CurationStrategies4_Results",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"oetc4\">Classifying unsafe ad content has proven an enticing problem space for leveraging large language models (LLMs). The inherent complexity involved in identifying policy-violating content demands solutions capable of deep contextual and cultural understanding, areas of relative strength for LLMs over traditional machine learning systems. But fine-tuning LLMs for such complex tasks requires high-fidelity training data that is difficult and expensive to curate at the necessary quality and scale. Standard data-intensive approaches to training models are costly, especially given the need to handle <a href=\"https://en.wikipedia.org/wiki/Concept_drift\" target=\"_blank\" rel=\"noopener noreferrer\">concept drift</a> as safety policies evolve or as new types of unsafe ad content arise. In the worst case the model must be retrained on a completely new data set. Reducing the amount of training data needed is therefore paramount.</p><p data-block-key=\"fnk84\">With this in mind, we describe a new, scalable curation process for <a href=\"https://en.wikipedia.org/wiki/Active_learning_(machine_learning)\" target=\"_blank\" rel=\"noopener noreferrer\">active learning</a> that can drastically reduce the amount of training data needed for fine-tuning LLMs while significantly improving model alignment with human experts. The process can be applied to datasets of hundreds of billions of examples to iteratively identify the examples for which annotation would be most valuable and then use the resulting expert labels for fine-tuning.</p><p data-block-key=\"br468\">In our experiments, we were able to reduce the scale of training data needed from 100,000 to under 500 training examples, while increasing model alignment with human experts by up to 65%. Production systems using larger models have seen even greater reductions in data scale, using up to four orders of magnitude less data while maintaining or improving quality.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"n7jxf\">Curation process</h2><p data-block-key=\"99o9h\">Our process starts with a zero- or few-shot initial model (LLM-0), which we provide with a prompt describing the content of interest, e.g., defining clickbait and asking “Is this ad clickbait?” The LLM-0 model then labels ads as <i>clickbait</i> (orange in the figure below) or <i>benign</i> (blue) and generates a large labeled data set, shown as <b>(1)</b> below. Note that this initial data set is typically highly imbalanced, since in production traffic only very few (&lt;1%) ads are actually clickbait. The LLM’s true positive rate is also low because it has not yet been fine-tuned. To find the most informative examples, we separately cluster examples labeled clickbait and examples labeled benign, which yields some overlapping clusters, thus indicating potential model confusion between clickbait and benign examples <b>(2)</b>. For each such overlapping cluster pair, we find pairs of examples lying nearest each other that have different labels <b>(3)</b> and send these to human experts for an opinion. If needed to stay within our review budget, we prioritize pairs of examples that cover a larger area of our search space <b>(4)</b>. The resulting curated set is both informative (since it contains the most confusable examples along the decision boundary) and diverse (since it draws from different regions along that decision boundary).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/CurationStrategies1_ProcessFinal.width-1250.png\" alt=\"CurationStrategies1_ProcessFinal\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/CurationStrategies1_ProcessFinal.width-1250.png\" alt=\"CurationStrategies1_ProcessFinal\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"tf656\"><i>The curation process generates preliminary labels using a few-shot LLM and then clusters each label set. Overlapping clusters with differing labels are used to identify sampled pairs of examples that are both informative and diverse.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"n7jxf\">These expert-provided labels are split randomly into two sets. The first is used for model evaluation, based on two key alignment metrics: the internal alignment measuring how much experts agree, and the model–human alignment between the current model and human experts. The second is used to fine-tune the current models, producing the next iteration of the model. The process repeats until the model–human alignment either matches the internal alignment or plateaus and cannot be improved further.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"n7jxf\">Metric</h2><p data-block-key=\"a9c7p\">Our curation process does not assume the existence of ground truth. Many classification problems in the ads safety space, such as content moderation or fraud detection, are inherently ambiguous and require interpretation and deliberation, even among policy experts. We therefore cannot rely on standard metrics like precision and recall, which require a ground truth label. Instead we use <a href=\"https://en.wikipedia.org/wiki/Cohen%27s_kappa\" target=\"_blank\" rel=\"noopener noreferrer\">Cohen’s Kappa</a>, a measure of how well two independent annotators align, above what would be expected from chance agreement. In our experiments, Cohen’s Kappa is used as both a quality indicator for datasets (including model evaluation during the curation process, as noted above); and as a measure of model performance. Values closer to 1 show higher alignment, 0 indicates no alignment above chance, and negative values indicate systematic disagreement. While standards for interpreting these scores vary, Kappa values above .8 are widely considered to be exceptionally good, and values above .4 are generally considered acceptable.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"n7jxf\">Experiments</h2><p data-block-key=\"9ljdb\">We wanted to understand which models and tasks would benefit most from our curation process. As <i>baselines</i> for our experiments, we fine-tuned two LLMs of different sizes (<a href=\"https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a> Nano-1 with 1.8B parameters and Nano-2 with 3.25B parameters) on two tasks of different complexity (lower and higher, based on expert alignment) using crowdsourced labels. Each crowdsourced data set has ~100K annotations and a strong class imbalance, with around 95% benign labels on average.</p><p data-block-key=\"1v6c\">We compared each of these four baseline conditions against the corresponding <i>curated</i> condition in which each model (Nano-1 and Nano-2) is fine-tuned over multiple rounds using the curation process described above. At each iteration, we selected our curated set of examples and used them for model evaluation and fine-tuning, as described above. All models plateaued before reaching parity with the experts’ internal alignment, so we stopped at 6 iterations (~400 fine-tuning and ~250 evaluation samples) for the lower complexity task and 5 iterations (~250 fine-tuning and ~150 evaluation samples) for the higher complexity task. (Note that the lower complexity task had a larger variety of examples, which may account for the longer time needed to converge.) Both data sets had a final class balance of ~40% positive examples.</p><p data-block-key=\"fcsdi\">The table below provides an overview of the scale and quality of the data used in each condition. Experts reached an average pairwise Cohen’s Kappa of .81 (on the lower complexity task) and .78 (on the higher complexity task) through the curation process. We consider these the ceiling for model performance. To assess the quality of our crowdsourced data, we calculated Kappa alignment between crowdsourced annotations and experts based on our full curated set, which was .59 (lower complexity) and .41 (higher complexity).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/CurationStrategies2_Table1Fin.width-1250.png\" alt=\"CurationStrategies2_Table1Fin\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/CurationStrategies2_Table1Fin.width-1250.png\" alt=\"CurationStrategies2_Table1Fin\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"tf656\"><i>Size and quality of datasets used for our baseline conditions (using crowdsourced data) and curated conditions (using data from human experts). Dataset numbers for the expert curated sets show the cumulative number of samples collected during the curation process for both fine-tuning and model evaluation; the full curated set also served as the evaluation dataset for the crowdsourced data. Quality of the evaluation datasets is measured in pairwise Cohen’s Kappa.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"n7jxf\">Below we show how models trained on these vastly different data sets performed in each of our baseline and curated conditions. The 1.8B parameter model saw comparable performance on both tasks: the baseline and curated models had .24 and .25 alignment, respectively, for the lower complexity task, and both models had .13 alignment on the higher complexity task. By contrast, the 3.25B parameter model showed significant quality improvements when trained with our curation process. Kappa scores for the baseline and curated models were .36 and .56, respectively, for the lower complexity task; and .23 and .38, respectively, for the higher complexity task — an improvement in alignment of 55-65% using three orders of magnitude less data (250 to 450 examples, compared to 100K in the baseline condition).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/CurationStrategies3_Table2.width-1250.png\" alt=\"CurationStrategies3_Table2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/CurationStrategies3_Table2.width-1250.png\" alt=\"CurationStrategies3_Table2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"tf656\"><i>Performance of models trained in curated and baseline conditions, measured as alignment between domain experts and model responses using pairwise Cohen’s Kappa.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/CurationStrategies4_Results.width-1250.png\" alt=\"CurationStrategies4_Results\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/CurationStrategies4_Results.width-1250.png\" alt=\"CurationStrategies4_Results\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"tf656\"><i>Our curation method</i> <i>uses only 250 (for the higher complexity task) and 450 (for the lower complexity task) training samples rated by pairs of human experts ( .78 and .81 average pairwise Cohen’s Kappa). The baseline models use 100K crowdsourced training samples (~5% positive).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"n7jxf\">These results demonstrate that careful curation of LLM datasets to focus on fewer, more informative examples can yield better or equivalent classifier performance using much less data — three orders of magnitude less in the experiments reported here, and up to four orders of magnitude less for the larger models used in production. Of course, these gains require not only good curation but also very high quality data. For our use cases, we have observed that a label quality above .8 pairwise Cohen’s Kappa is needed to reliably outperform crowdsourced data. Consistently achieving this level of quality poses a separate challenge, to be discussed in a subsequent blog post.</p><p data-block-key=\"e0dkc\">But given sufficient label quality, our curation process is able to leverage the strengths of both LLMs, which can cast a wide net over the problem space, and domain experts, who can focus more efficiently on the most challenging examples. The ability to retrain models with just a handful of examples is especially valuable for handling the rapidly changing landscapes of domains like ads safety. We believe the approach we’ve described will enable systems that can make more flexible, efficient use of high-fidelity labels to escape the data bottleneck.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"n7jxf\">Acknowledgements</h2><p data-block-key=\"1bgbo\"><i>This work would not have been possible without our outstanding team of engineers and product managers. Steve Walker is a co-founder of our project and co-creator of the curation process as well as the tech lead for the machine learning infrastructure of our project. Kelsie McElroy is the project manager and a co-founder of our project. We also want to thank the Ads Privacy and Safety leadership team for their continued support and belief in our vision.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "通过可穿戴设备和常规血液生物标志物预测胰岛素抵抗 (原标题: Insulin resistance prediction from wearables and routine blood biomarkers)",
      "link": "https://research.google/blog/insulin-resistance-prediction-from-wearables-and-routine-blood-biomarkers/",
      "pubDate": "Tue, 05 Aug 2025 16:00:00 GMT",
      "isoDate": "2025-08-05T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 通过可穿戴设备和常规血液生物标志物预测胰岛素抵抗\n\n## 引言：胰岛素抵抗与2型糖尿病的挑战\n2型糖尿病在全球范围内影响数亿人，且患病率持续上升。胰岛素抵抗（IR）是其主要前兆，表现为身体细胞对胰岛素反应不当。早期检测IR至关重要，因为生活方式干预常能逆转IR，从而预防或延缓2型糖尿病的发生。然而，目前精确测量IR的方法，如“金标准”胰岛素钳夹技术或HOMA-IR（需要特定胰岛素血液检测），通常具有侵入性、昂贵或不易在常规体检中获得，这严重阻碍了早期检测和干预。\n\n## 研究目标：利用现有数据预测IR风险\n本研究探讨了一系列机器学习模型，旨在利用可穿戴设备数据（如静息心率、步数、睡眠模式）和常规血液检测结果（如空腹血糖、血脂谱）来预测胰岛素抵抗的风险。此外，研究还引入了基于Gemini大型语言模型（LLMs）的“胰岛素抵抗知识与理解代理”（IR原型代理），以帮助用户理解胰岛素抵抗，促进解读并提供安全的个性化建议。\n\n## WEAR-ME研究设计与数据收集\n研究团队设计了名为WEAR-ME的研究，旨在探索利用易于获取的数据（通过预测HOMA-IR）预测胰岛素抵抗的潜力。为自动化常规血液生物标志物的数据收集，研究与Quest Diagnostics合作。\n\n1165名来自美国各地的远程参与者通过Google Health Studies应用（一个安全的消费者数字研究平台）注册了WEAR-ME研究。该研究获得了机构审查委员会（IRB）的批准，所有参与者在入组前均通过Google Health Studies应用提供了电子知情同意和特定的HIPAA授权。研究队列在年龄、性别、地理位置和BMI方面具有多样性，参与者的中位BMI为28 kg/m²，年龄为45岁，HbA1c为5.4%。\n\n参与者同意分享以下数据：\n*   **可穿戴设备数据：** 来自Fitbit或Google Pixel Watch设备的数据，如静息心率、步数、睡眠模式（经过假名化处理以保护隐私）。\n*   **常规血液生物标志物：** 在Quest Diagnostics进行现场访问时为本研究专门进行的常规检测结果，如空腹血糖和胰岛素、血脂谱。\n*   **人口统计学和调查：** 研究开始和结束时完成的基本信息和健康问卷，包括年龄、体重、身高、种族、民族和性别数据，以及关于整体健康感知（健身、饮食）和糖尿病或其他合并症病史的问题。\n\n研究团队利用这个丰富、多模态的数据集（称为“WEAR-ME数据”）开发并训练了深度神经网络模型来预测HOMA-IR分数。\n\n## 关键发现：预测性能\n研究结果表明，结合不同数据流显著提高了预测准确性，优于单独使用任何单一数据源：\n\n*   **可穿戴设备 + 人口统计学：** 对IR分类显示出一定的预测能力（auROC = 0.70）。\n*   **在可穿戴设备 + 人口统计学基础上增加空腹血糖：** 这一常规血液检测结果被证明非常有价值，显著提升了性能（auROC = 0.78）。\n*   **可穿戴设备 + 人口统计学 + 常规血检：** 取得了最佳结果，准确预测了HOMA-IR值（R² = 0.50），并有效分类了IR个体（auROC = 0.80，敏感性 = 76%，特异性 = 84%，其中HOMA-IR值2.9或更高被用于识别胰岛素抵抗者）。\n\n![Insulin-Resistance-Prediction-4](https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-4.width-1250.png)\n*2型糖尿病的代谢亚表型。慢性胰岛素抵抗是大约70%的2型糖尿病病例的前兆，由肥胖、不活跃的生活方式和遗传因素共同导致。*\n\n![Insulin-Resistance-Prediction-5](https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-5.width-1250.png)\n*我们提出的HOMA-IR预测建模流程图，以及使用胰岛素抵抗教育和理解代理解释结果的示意图。*\n\n![Insulin-Resistance-Prediction-6](https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-6.width-1250.png)\n*左图：IR预测（分类）的性能评估。右图：选定特征集的精确召回曲线可视化。平均值以颜色表示，每条线周围的灰色区域表示五折交叉验证的标准差。*\n\n重要的是，研究结果表明，来自可穿戴设备的数据特征，如静息心率，与BMI和空腹血糖一起，始终位居最重要的预测因子之列。特征重要性结果突出了捕捉生活方式相关信号的价值。\n\n![Insulin-Resistance-Prediction-1-final](https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-1-final.width-1250.png)\n*Sankey图显示了每个提出的非线性XGBoost模型进行直接回归的相对特征重要性（SHapley Additive exPlanations [SHAP] 值）。*\n\n## 针对高风险人群的性能\n由于肥胖和久坐不动的生活方式个体特别容易患2型糖尿病，研究专门评估了模型在这些亚组中的表现：\n\n*   **肥胖参与者：** 模型准确性相比总体人群有所提高（敏感性 = 86% vs 76%）。\n*   **久坐参与者：** 准确性高于肥胖亚人群（敏感性 = 88%）。\n*   **肥胖且久坐参与者：** 模型在这一关键群体中表现尤为出色（敏感性 = 93%，调整特异性 = 95%；此处调整特异性侧重于最小化将真正胰岛素敏感个体错误分类为抵抗的错误）。\n\n这些实验结果表明，该方法在识别那些最能从早期生活方式干预中受益的人群方面可能特别有效。\n\n![Insulin-Resistance-Prediction-3](https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-3.width-1250.png)\n*各种生活方式分层的分类性能结果。*\n\n## 验证与泛化能力\n为确保研究结果并非仅限于初始数据集，研究团队在通过另一项IRB批准的独立验证队列（N=72）上测试了表现最佳的模型（在WEAR-ME数据上训练）。该队列参与者使用Fitbit Charge 6分享可穿戴设备数据，并在旧金山的研究中心现场获取血液生物标志物数据。该队列的中位BMI为30.6 kg/m²，年龄为44.5岁。\n\n在验证队列上的结果显示，训练好的模型保持了强大的预测性能（敏感性 = 84%，特异性 = 81%），证明了其潜在的泛化能力。然而，由于这仍是一个研究原型，其在任何健康相关目的上的安全性与有效性尚未确定。\n\n![Insulin-Resistance-Prediction-8](https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-8.width-1250.png)\n*独立验证队列研究概述。我们将初始训练和测试队列的模型准确性与外部验证队列进行比较，并展示其潜在的泛化能力。*\n\n## 超越预测：迈向理解与主动干预\n预测IR风险很有价值，但如何使这些信息对个体而言易于理解和可操作？研究探索了将预测模型与LLMs集成，以帮助用户更好地理解其代谢健康。研究开发了基于最先进Gemini LLMs的“胰岛素抵抗知识与理解代理”（IR原型代理）。当被问及代谢健康问题时，IR代理会根据个体的研究数据和预测的IR状态，提供个性化、情境化的教育性回答。在用户同意的情况下，该代理能够访问特定的用户提供数据点，搜索最新信息，并执行计算。\n\n需要强调的是，与模型或IR代理的交互旨在演示此类工具如何帮助用户探索其结果，仅用于信息和教育目的。五位获得委员会认证的内分泌学家评估了IR代理与基础模型的回答。他们强烈倾向于IR代理的回答，认为其更全面、更值得信赖、更个性化。这表明将预测性健康模型与LLMs结合，在增强个体健康理解方面具有潜力。\n\n![Insulin-Resistance-Prediction-2](https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-2.width-1250.png)\n*胰岛素抵抗知识与理解代理（IR代理）概述。左图为提出的IR代理示意图，右图为内分泌学家评估的IR代理与基础模型的胜率结果。*\n\n## 结论与未来工作\n本研究表明，结合易于获取的可穿戴设备数据和常规血液生物标志物的机器学习模型，有潜力有效预测胰岛素抵抗，这是2型糖尿病的关键前兆。这种方法具有多项优势：\n\n*   **可及性：** 利用许多人已拥有或易于获取的数据。\n*   **早期检测：** 甚至在血糖水平异常之前就能识别风险，例如，研究发现许多血糖正常（HbA1c < 5.7）的参与者已经存在IR。\n*   **可扩展性：** 提供比专业IR检测更具可扩展性的筛查方法。\n*   **个性化：** 在高风险亚组中表现出色，并有潜力集成到个性化健康工具中。\n\n这项工作为更早、更便捷地筛查2型糖尿病风险打开了大门，可能实现及时的生活方式干预，从而预防或延缓疾病，特别是对于那些在不知不觉中发展疾病的人群。\n\n未来的工作包括：纵向验证这些模型（随时间跟踪个体）、探索干预措施的影响、纳入遗传和微生物组数据，以及进一步完善针对特定人群的模型，以确保在不同群体中实现公平的性能。研究团队相信这一研究方向对于主动和个性化的代谢健康管理具有重要前景。\n\n## 免责声明\n尽管本研究提出的方法（包括IR代理）在各种健康应用中具有前景，但本研究专门解决了早期检测胰岛素抵抗的关键需求，并未将此处讨论的模型呈现为经批准的医疗设备或解决方案。这些模型和IR代理并非医疗设备，也未获得美国食品药品监督管理局（FDA）或任何其他国家或国际监管机构的批准、许可或审查。本工作不旨在替代，也不应被用作专业的医疗建议、诊断或治疗。此类技术在实际应用中的部署将需要严格的测试、验证和监管批准。",
      "shortSummary": "这项研究开发了机器学习模型，利用可穿戴设备数据（如心率、步数、睡眠）和常规血液检测（如空腹血糖、血脂）来预测胰岛素抵抗（IR），这是2型糖尿病的关键前兆。该方法在大型队列和独立验证中表现出高准确性，尤其在高风险人群中效果显著。研究还引入了基于LLM的IR代理，用于提供个性化健康教育。这项工作为早期、可及的2型糖尿病风险筛查提供了潜力，有助于及时干预，但模型目前仅用于研究和信息目的，并非医疗设备。",
      "translated_title": "通过可穿戴设备和常规血液生物标志物预测胰岛素抵抗",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-4.width-1250.png",
          "alt": "Insulin-Resistance-Prediction-4",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-5.width-1250.png",
          "alt": "Insulin-Resistance-Prediction-5",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-6.width-1250.png",
          "alt": "Insulin-Resistance-Prediction-6",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-1-final.width-1250.png",
          "alt": "Insulin-Resistance-Prediction-1-final",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-3.width-1250.png",
          "alt": "Insulin-Resistance-Prediction-3",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"c3w0r\"><a href=\"https://idf.org/about-diabetes/diabetes-facts-figures/\" target=\"_blank\" rel=\"noopener noreferrer\">Type 2 diabetes affects hundreds of millions globally</a>, and its prevalence is rising. A major precursor to this condition is <a href=\"https://www.niddk.nih.gov/health-information/diabetes/overview/what-is-diabetes/prediabetes-insulin-resistance\" target=\"_blank\" rel=\"noopener noreferrer\">insulin resistance</a> (IR), where the body's cells do not respond properly to insulin, a hormone crucial for regulating blood sugar. Detecting IR early is key, as lifestyle changes can often reverse it and prevent or delay the onset of type 2 diabetes. However, current methods for accurately measuring IR, like the \"gold standard\" <a href=\"https://journals.physiology.org/doi/abs/10.1152/ajpendo.1979.237.3.E214\" target=\"_blank\" rel=\"noopener noreferrer\">euglycemic insulin clamp</a> or the <a href=\"https://link.springer.com/article/10.1007/bf00280883\" target=\"_blank\" rel=\"noopener noreferrer\">Homeostatic Model Assessment for Insulin Resistance</a> (HOMA-IR), which requires specific insulin blood tests, are often invasive, expensive, or not readily available in <a href=\"https://www.nature.com/articles/s41551-024-01311-6\" target=\"_blank\" rel=\"noopener noreferrer\">routine check-ups</a>. These steps create significant barriers to early detection and intervention, especially for those unknowingly at risk.</p><p data-block-key=\"8nn75\">What if we could leverage data already available to many people, such as data from wearable devices and common blood tests, to estimate IR risk? In “<a href=\"https://arxiv.org/pdf/2505.03784\" target=\"_blank\" rel=\"noopener noreferrer\">Insulin Resistance Prediction From Wearables and Routine Blood Biomarkers</a>”, we explore a suite of machine learning models that have the potential of predicting IR using wearable data (e.g., resting heart rate, step count, sleep patterns) and routine blood tests (e.g., <a href=\"https://en.wikipedia.org/wiki/Blood_sugar_level\" target=\"_blank\" rel=\"noopener noreferrer\">fasting glucose</a>, <a href=\"https://en.wikipedia.org/wiki/Lipid_profile\" target=\"_blank\" rel=\"noopener noreferrer\">lipid panel</a>). This approach shows strong performance across the studied population (N=1,165) and an independent validation cohort (N=72), particularly in high-risk individuals, such as people with obesity and sedentary lifestyles. Additionally, we introduce the Insulin Resistance Literacy and Understanding Agent (an IR prototype agent), built on the state-of-the-art <a href=\"https://gemini.google.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a> family of LLMs to help understand insulin resistance, facilitating interpretation and safe personalized recommendations. This work offers the potential for early detection of people at risk of type 2 diabetes and thereby facilitates earlier implementation of preventative strategies. The models, predictions, and the Insulin Resistance Literacy and Understanding Agent described in this research are intended for informational and research purposes only.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-4.width-1250.png\" alt=\"Insulin-Resistance-Prediction-4\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-4.width-1250.png\" alt=\"Insulin-Resistance-Prediction-4\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"zaz2x\"><a href=\"https://www.nature.com/articles/s41551-024-01311-6#:~:text=Type%202%20diabetes%20(T2D,4%20and%20hepatic%20glucose\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Metabolic subphenotypes</i></a><i> of type 2 diabetes</i><b><i>.</i></b> <i>Chronic insulin resistance is a precursor to approximately 70% of type 2 diabetes cases and results from a combination of obesity, an inactive lifestyle, and genetic factors.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Predicting IR using digital biomarkers &amp; routine blood tests</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c3w0r\">We designed a study, called <a href=\"https://blog.google/products/fitbit/new-quest-fitbit-study-metabolic-health/\" target=\"_blank\" rel=\"noopener noreferrer\">WEAR-ME</a>, to explore the potential of predicting insulin resistance (through predicting HOMA-IR) using readily accessible data. To automate the data collection process for routine blood biomarkers, we partnered with <a href=\"https://www.questdiagnostics.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Quest Diagnostics</a>.</p><p data-block-key=\"2dtm7\">1,165 remote participants from across the US signed up for the WEAR-ME study via the <a href=\"https://play.google.com/store/apps/details?id=com.google.android.apps.health.research.studies&amp;hl=en\" target=\"_blank\" rel=\"noopener noreferrer\">Google Health Studies app</a>, a <a href=\"https://play.google.com/store/apps/datasafety?id=com.google.android.apps.health.research.studies&amp;hl=en\" target=\"_blank\" rel=\"noopener noreferrer\">secure</a> consumer-facing platform for digital studies. This study was conducted with approval from an <a href=\"https://www.fda.gov/about-fda/cder-offices-and-divisions/institutional-review-boards-irbs-and-protection-human-subjects-clinical-trials\" target=\"_blank\" rel=\"noopener noreferrer\">Institutional Review Board</a> (IRB). All participants provided electronic informed consent and a specific HIPAA Authorization via the Google Health Studies app before enrollment. The cohort was diverse in age, gender, geography, and BMI. Participants had a median BMI of 28 kg/m², age of 45 years, and <a href=\"https://diabetesjournals.org/care/article/47/Supplement_1/S20/153954/2-Diagnosis-and-Classification-of-Diabetes\" target=\"_blank\" rel=\"noopener noreferrer\">HbA1c of</a> 5.4%. Participants consented to share the following data:</p><ul><li data-block-key=\"eoaeu\"><b>Wearable Data</b>: Data from their Fitbit or Google Pixel Watch devices (like resting heart rate, step count, sleep patterns), <a href=\"https://en.wikipedia.org/wiki/Pseudonymization\" target=\"_blank\" rel=\"noopener noreferrer\">pseudonymized</a> to protect individual participant privacy.</li><li data-block-key=\"dva0o\"><b>Routine Blood Biomarkers</b>: Results from routine tests (like fasting glucose and insulin, lipid panel) ordered during an in-person visit to Quest Diagnostics, specifically for this research.</li><li data-block-key=\"k5ss\"><b>Demographics and Surveys</b>: Basic information and health questionnaires (completed at the beginning and end of the study), which included data on age, weight, height, ethnicity, race, and gender, along with questions about perception of overall health (fitness, diet) and history of diabetes or other comorbidities.</li></ul><p data-block-key=\"1vm4r\">Using this rich, multimodal dataset (which we refer to as the “WEAR-ME data”), we developed and trained deep neural network models to predict HOMA-IR scores. Our goal was to see how well we could estimate this key IR marker using different combinations of available data.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-5.width-1250.png\" alt=\"Insulin-Resistance-Prediction-5\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-5.width-1250.png\" alt=\"Insulin-Resistance-Prediction-5\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"zaz2x\"><i>Illustration of our proposed modeling pipeline for predicting HOMA-IR, and interpreting the results with the Insulin Resistance Education and Understanding Agent.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"c3w0r\">Our results, using the <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve:~:text=.-,Area%20under%20the%20curve,-%5Bedit%5D\" target=\"_blank\" rel=\"noopener noreferrer\">area under the receiver operating characteristic curve</a> (auROC) metric, indicate that combining data streams significantly improved prediction accuracy compared to using any single source alone:</p><ul><li data-block-key=\"dduc3\"><b>Wearables + Demographics</b>: Showed some predictive power (auROC = 0.70) for classifying IR.</li><li data-block-key=\"2dgqc\"><b>Adding Fasting Glucose to Wearables + Demographics</b>: This routine blood test result proved highly valuable, significantly boosting performance (auROC = 0.78).</li></ul><p data-block-key=\"fhbum\"><b>Wearables + Demographics + Routine Blood Panels</b>: Achieved the best results, accurately predicting HOMA-IR values (<a href=\"https://en.wikipedia.org/wiki/Coefficient_of_determination\" target=\"_blank\" rel=\"noopener noreferrer\">R²</a> = 0.50) and effectively classifying individuals with IR (auROC = 0.80, Sensitivity = 76%, Specificity = 84%, where HOMA-IR value of 2.9 or higher was used to identify a person as being insulin resistant).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-6.width-1250.png\" alt=\"Insulin-Resistance-Prediction-6\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-6.width-1250.png\" alt=\"Insulin-Resistance-Prediction-6\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"zaz2x\"><b><i>Left:</i></b> <i>Performance evaluation of IR prediction (classification).</i> <b><i>Right:</i></b> <i>Visualization of the</i> <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall#:~:text=A%20precision%2Drecall%20curve%20plots%20precision%20as%20a%20function%20of%20recall%3B%20usually%20precision%20will%20decrease%20as%20the%20recall%20increases.\" target=\"_blank\" rel=\"noopener noreferrer\"><i>precision-recall curve</i></a><i> for selected feature sets. Average values are colors, with the gray areas around each line indicate the standard deviation across the five folds.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"c3w0r\">Importantly, our results indicate that features derived from wearable data, such as resting heart rate, consistently ranked among the most important predictors, alongside BMI and fasting glucose. The feature importance result highlights the value of capturing lifestyle-related signals.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-1-final.width-1250.png\" alt=\"Insulin-Resistance-Prediction-1-final\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-1-final.width-1250.png\" alt=\"Insulin-Resistance-Prediction-1-final\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"zaz2x\"><i>Sankey diagram showing the relative feature importance (</i><a href=\"https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html\" target=\"_blank\" rel=\"noopener noreferrer\"><i>SHapley Additive exPlanations [SHAP] values</i></a><i>) for each of the proposed nonlinear</i> <a href=\"https://en.wikipedia.org/wiki/XGBoost\" target=\"_blank\" rel=\"noopener noreferrer\"><i>XGBoost</i></a><i> models for direct regression.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Focusing on high-risk groups</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c3w0r\">Since individuals with obesity and sedentary lifestyles are particularly vulnerable to developing type 2 diabetes, we specifically evaluated our model's performance in these subgroups:</p><ul><li data-block-key=\"3eqim\"><b>Obese Participants</b>: The model showed improved accuracy, compared to the overall population (sensitivity = 86% vs %76).</li><li data-block-key=\"1j507\"><b>Sedentary Participants</b>: Accuracy was higher than the obese subpopulation (sensitivity = 88%).</li><li data-block-key=\"3ouck\"><b>Obese and Sedentary Participants</b>: The model performed notably well in this critical group (sensitivity = 93%, adjusted specificity = 95%; adjusted specificity here focuses on minimizing the misclassification of truly insulin-sensitive individuals as resistant).</li></ul><p data-block-key=\"buqs7\">The results of this experiment suggest that our approach could be particularly effective at identifying those who might benefit most from early lifestyle interventions.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-3.width-1250.png\" alt=\"Insulin-Resistance-Prediction-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-3.width-1250.png\" alt=\"Insulin-Resistance-Prediction-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"zaz2x\"><i>Results of classification performance for various lifestyle stratification.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Validation and generalizability</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c3w0r\">To ensure our findings were not just specific to our initial dataset, we tested our best-performing model (trained on the WEAR-ME data) on a completely independent validation cohort (N=72) recruited through a separate IRB-approved consented study, where participants shared wearable data using the <a href=\"https://store.google.com/us/product/fitbit_charge_6?hl=en-US\" target=\"_blank\" rel=\"noopener noreferrer\">Fitbit Charge 6</a>, and blood biomarker data was acquired in-person at the study center in San Francisco. This cohort had a median BMI of 30.6 kg/m² and age of 44.5 years. Our results on the validation cohort show that our trained models maintained strong predictive performance (sensitivity = 84%, specificity = 81%), demonstrating its potential generalizability. However, as this remains a research prototype, its safety and effectiveness for any health-related purpose have not been established.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-8.width-1250.png\" alt=\"Insulin-Resistance-Prediction-8\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-8.width-1250.png\" alt=\"Insulin-Resistance-Prediction-8\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"zaz2x\"><i>Overview of the independent validation cohort study. We compare model accuracies from the initial training and testing cohort with the external validation cohort and</i> demonstrate its potential generalizability<i>.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Beyond prediction: Towards understanding and proactive steps</h2>\n            \n        \n        \n    </div>\n\n\n\n    \n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/Insulin-Resistance-Prediction.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"zaz2x\">Illustration of the proposed agentic architecture that leverages the HOMA-IR prediction model to assess insulin resistance risk to educate users.</p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"c3w0r\">Predicting IR risk is valuable, but how can we make this information understandable and actionable for individuals? We explored integrating our prediction models with LLMs to empower users to better understand their metabolic health. We developed the Insulin Resistance Literacy and Understanding Agent (an IR prototype agent), built on the state-of-the-art <a href=\"https://gemini.google.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a> family of LLMs. When asked a question about metabolic health, the IR Agent provides personalized, contextualized answers for educational purposes grounded in the individual's study data and predicted IR status. With the user's consent, the agent has the ability to access specific, user-provided data points, search for up-to-date information, and perform calculations. It is critical to note that interaction with the models or the IR Agent are intended to demonstrate how such a tool could help users explore their results for informational and educational purposes.</p><p data-block-key=\"e1vgt\">We had five board-certified endocrinologists evaluate responses from the IR Agent compared to a base model. They strongly preferred the IR Agent's responses, finding them to be significantly more comprehensive, trustworthy, and personalized. This demonstrates the potential of combining predictive health models with LLMs to empower individuals with better health understanding.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-2.width-1250.png\" alt=\"Insulin-Resistance-Prediction-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/Insulin-Resistance-Prediction-2.width-1250.png\" alt=\"Insulin-Resistance-Prediction-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"zaz2x\"><i>Overview of Insulin Resistance Literacy and Understanding Agent (IR Agent). An illustration of the proposed IR agent (</i><b><i>left</i></b><i>), along with the results (win rate) of our IR agent against the base model as evaluated by endocrinologists (</i><b><i>right</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusions and future work</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c3w0r\">Our research demonstrates that ML models combining readily available wearable data and routine blood biomarkers have the potential to effectively predict insulin resistance, a key precursor to type 2 diabetes. This approach offers several advantages:</p><ul><li data-block-key=\"au5p5\"><b>Accessibility</b>: Leverages data many people already have or can easily obtain.</li><li data-block-key=\"bfia3\"><b>Early Detection</b>: Identifies risk even before blood sugar levels become abnormal, e.g., we found many <a href=\"https://diabetesjournals.org/care/article/47/Supplement_1/S20/153954/2-Diagnosis-and-Classification-of-Diabetes\" target=\"_blank\" rel=\"noopener noreferrer\">normoglycemic (with HbA1c &lt; 5.7)</a> participants in our study already had IR.</li><li data-block-key=\"66af4\"><b>Scalability</b>: Offers a potentially more scalable screening method than specialized IR tests.</li><li data-block-key=\"9deb4\"><b>Personalization</b>: Shows strong performance in high-risk subgroups and potential for integration into personalized health tools.</li></ul><p data-block-key=\"dcg0m\">This work opens doors for earlier, more accessible screening of type 2 diabetes risk, potentially enabling timely lifestyle interventions that could prevent or delay the disease, particularly for those unknowingly progressing towards it.</p><p data-block-key=\"f611n\">Future work includes validating these models longitudinally (tracking individuals over time), exploring the impact of interventions, incorporating genetic and microbiome data, and further refining models for specific populations to ensure equitable performance across diverse groups. We believe this line of research holds significant promise for proactive and personalized metabolic health management.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Disclaimer</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c3w0r\">While our proposed approach, including the IR Agent, holds promise for various health applications, this research specifically addresses the critical need for early detection of insulin resistance, and does not present the models discussed herein as approved medical devices or solutions. The models and the IR Agent are not medical devices. They have not been cleared, approved, or reviewed by the U.S. Food and Drug Administration (FDA) or any other national or international regulatory agency. This work is not intended to be, and should not be used as, a substitute for professional medical advice, diagnosis, or treatment. Real-world deployment of such technologies would necessitate rigorous testing, validation, and regulatory approval.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"c3w0r\"><i>The research described here is joint work across Google Research and partnering teams. The following researchers contributed to this work: Ahmed A. Metwally, A. Ali Heydari, Daniel McDuff, Alexandru Solot, Zeinab Esmaeilpour, Anthony Z. Faranesh, Menglian Zhou, David B. Savage, Conor Heneghan, Shwetak Patel, Cathy Speed, and Javier L. Prieto. Google partnered with</i> <a href=\"https://www.questdiagnostics.com/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Quest Diagnostics</i></a><i>, the world’s leading provider of diagnostic information, to allow eligible participants to share their biomarker data received as part of a free blood draw, which includes a comprehensive metabolic panel and measuring cholesterol, triglycerides and insulin levels.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "使用 DeepPolisher 进行高精度基因组校正：增强基因组研究的基础 (原标题: Highly accurate genome polishing with DeepPolisher: Enhancing the foundation of genomic research)",
      "link": "https://research.google/blog/highly-accurate-genome-polishing-with-deeppolisher-enhancing-the-foundation-of-genomic-research/",
      "pubDate": "Tue, 05 Aug 2025 16:00:00 GMT",
      "isoDate": "2025-08-05T16:00:00.000Z",
      "creator": "Google",
      "summary": "### 基因组研究的基石：DeepPolisher 实现高精度基因组校正\n\n基因组是理解遗传、疾病和进化的关键，但DNA测序和基因组组装过程中存在的错误会限制基因和蛋白质的识别，并可能导致诊断遗漏致病变异。即使是微小的错误率，在30亿个核苷酸的人类基因组中也意味着大量的错误，从而影响基因组的实用性。\n\n为了持续改进基因组组装资源，我们与加州大学圣克鲁斯基因组研究所合作开发了 **DeepPolisher**，这是一种开源的基因组组装校正方法。在《Genome Research》上发表的论文《使用 DeepPolisher 进行高精度组装校正》中，我们详细介绍了该流程如何扩展现有方法以提高基因组组装的准确性。DeepPolisher 将组装错误减少了 50%，插入或缺失（indel）错误减少了 70%，这对于基因识别尤为重要。\n\n#### 背景：DNA 测序技术与挑战\n\nDNA 测序通常涉及捕获 DNA 复制过程。一种常见方法是使用不同颜色的标记分子标记核苷酸，并观察它们被添加到 DNA 分子中的过程。然而，识别单个核苷酸需要高灵敏度的检测器，这限制了测量的准确性。\n\nIllumina 开发了一种突破性技术，将单个 DNA 分子复制成相同的簇，然后同步监测簇的复制，从而增强每个碱基的信号。但由于簇无法完美同步，可能导致信号混合，从而将该方法的 DNA 测量长度限制在几百个核苷酸（称为“短读长”）。\n\n尽管短读长序列较短，但通过与参考基因组（现有物种基因组图谱）进行比对，可以构建更完整的个体基因组，从而理解其基因组变异。\n\n![DeepPolisher-1-Genome](https://storage.googleapis.com/gweb-research2023-media/images/DeepPolisher-1-Genome.width-1250.png)\n人类基因组由两条冗余编码信息的链组成（左），并组织成染色体，每个亲本遗传一份完整的拷贝（右）。（图片来源：NHGRI）\n\n即使测序技术有所改进，挑战依然存在。首先，该方法依赖于强大的参考基因组，而创建这样的参考基因组本身就极其困难。其次，基因组的某些部分彼此相似，难以自信地映射到参考基因组。\n\n为了应对这些挑战，科学家开发了能够测序单个分子的方法，实现了数万个核苷酸的“长读长”序列。最初，这种方法的错误率高得无法接受（约 10%）。后来，Pacific Biosciences 开发了一种多次测序同一分子的方法，将错误率降低到仅 1%，与短读长方法相似。Google 和 Pacific Biosciences 合作首次在人类基因组上展示了这一点。\n\n我们的团队在此基础上进一步开发了 **DeepConsensus**，它使用序列 Transformer 模型从初始错误率较高的碱基中更准确地构建正确序列。目前，Pacific Biosciences 在其长读长测序仪上部署了 DeepConsensus，将错误率降低到低于 0.1%。尽管这一错误率显著优于现有技术，但要构建一个接近完美的全新参考基因组，仍需要结合来自同一个体多个 DNA 分子的测序读长，以进一步校正剩余错误。\n\n#### DeepPolisher 的工作原理\n\n这就是 DeepPolisher 发挥作用的地方。DeepPolisher 借鉴了 DeepConsensus 的经验，采用 Transformer 架构，并使用捐赠给个人基因组计划的人类细胞系基因组进行训练。该参考基因组已由 NIST 和 NHGRI 进行了详尽的表征，并使用多种不同技术进行了测序，估计其完整度接近 100%，正确率高达 99.99999%。这意味着在 60 亿个核苷酸的基因组中（来自父母各一份 30 亿核苷酸的参考基因组），总错误数约为 300-1000 个。\n\n通过进行 PacBio 测序和基因组组装，我们可以识别剩余错误，然后训练模型学习纠正这些错误。在训练过程中，模型输入测序碱基、其质量以及它们在给定参考组装部分上的唯一映射程度。我们仅使用 1-19 号染色体进行训练，保留 20-22 号染色体，并使用 21 和 22 号染色体的性能来选择模型，然后报告 20 号染色体的准确性。\n\n![DeepPolisher-2-Architecture](https://storage.googleapis.com/gweb-research2023-media/images/DeepPolisher-2-Architecture.width-1250.png)\nDeepPolisher 的架构。序列读长按亲本来源（称为“定相”）分类，并与草图基因组组装对齐。输入通道包括：碱基信息、测序仪报告的质量、映射质量（将读长唯一放置在组装上的能力）以及不匹配碱基的注释。这些信息被发送到一个仅编码器 Transformer，该 Transformer 对组装中的错误进行分类，然后提出修复建议，用于校正组装。\n\n#### 性能表现\n\nDeepPolisher 将基因组组装错误减少了大约一半，这一改进主要得益于插入-缺失（indel）错误的显著减少，其降幅超过 70%。减少这类错误尤为重要，因为插入或缺失的碱基会改变基因的阅读框，导致基因注释程序在标记基因组时忽略该基因，从而使其在临床分析或药物发现报告中被遗漏。\n\n我们使用“Q 值”来量化基因组的质量，Q 值是基因组中某个位置存在错误的概率的以 10 为底的对数。Q30 分数表示正确率为 99.9%，而 Q60 表示正确率为 99.9999%。为了评估 DeepPolisher 的改进效果，我们提取了用于组装人类泛基因组参考联盟（HPRC）新基因组的测序数据。我们通过尝试识别组装中与同一样本使用不同测序技术获得的序列不符的核苷酸组合来寻找潜在错误。通过在其他测序方法没有系统偏差的基因组区域（可信区域）进行此分析，我们发现组装质量平均从 Q66.7 提高到 Q70.1。我们还发现每个评估样本的质量都有所提高。\n\n![DeepPolisher-3-Performance](https://storage.googleapis.com/gweb-research2023-media/images/DeepPolisher-3-Performance.width-1250.png)\n180 个样本校正前后的组装质量。对于每个样本，基因组按亲本来源（父亲或母亲遗传的基因组拷贝）分离，表示为单倍型（Hap）1 或 2，并评估这些单倍型的质量。\n\n#### 部署与影响\n\nDeepPolisher 已经被用于改进科学界的基因组学资源。今年 5 月，HPRC 发布了第二批数据，其中包括 232 个个体的测序基因组组装，是第一批数据的五倍。第二批数据经过 DeepPolisher 的额外校正步骤，将单核苷酸和 indel 错误减少了两倍，实现了极低的错误率，即每五十万个组装碱基中错误不到一个。\n\n通过将 DeepPolisher 作为开源工具提供，我们的目标是让社区广泛使用这些方法。通过与人类泛基因组参考联盟合作，我们帮助科学家更准确地诊断所有祖先个体的遗传疾病。\n\n#### 致谢\n\n这篇博文展示了 Google 在开发 DeepPolisher 以提高基因组组装质量方面的贡献。将 DeepPolisher 整合到生成高精度泛基因组参考的更广泛背景中，涉及来自 68 个不同组织的近 195 位作者的贡献。我们感谢加州大学圣克鲁斯基因组研究所（GI）在 Benedict Paten 教授和 Karen Miga 教授领导下的研究团队，他们在 DeepPolisher 的初步分析和开发方向上提供了帮助。我们感谢 Mira Mastoras 和 Mobin Asri 领导了 DeepPolisher 的核心分析和整合到泛基因组生成流程中。我们感谢 Google 的技术贡献者：Pi-Chuan Chang、Daniel E. Cook、Alexey Kolesnikov、Lucas Brambrink 和 Maria Nattestad。我们感谢 Lizzie Dorfman、Dale Webster 和 Katherine Chou 的战略领导，以及 Monique Brouillette 在写作方面的帮助。",
      "shortSummary": "DeepPolisher 是一种开源基因组校正工具，旨在提高基因组组装的准确性。它基于 Transformer 架构，能将基因组组装错误减少 50%，其中插入或缺失（indel）错误减少 70%，这对于基因识别至关重要。DeepPolisher 将基因组质量从 Q66.7 提升至 Q70.1，并已被人类泛基因组参考联盟采用，显著降低了错误率。该工具的开源发布旨在促进基因组研究，并帮助更准确地诊断遗传疾病。",
      "translated_title": "使用 DeepPolisher 进行高精度基因组校正：增强基因组研究的基础",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DeepPolisher-1-Genome.width-1250.png",
          "alt": "DeepPolisher-1-Genome",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DeepPolisher-2-Architecture.width-1250.png",
          "alt": "DeepPolisher-2-Architecture",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DeepPolisher-3-Performance.width-1250.png",
          "alt": "DeepPolisher-3-Performance",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"2i03j\">The key to understanding heredity, disease, and evolution lies in the genome, which is encoded in nucleotides (i.e., the bases A, T, G, and C). DNA sequencers can read these nucleotides, but doing so both accurately and at scale is challenging, due to the very small scale of the base pairs. However, to unlock the secrets hidden within the genome, we must be able to assemble a reference genome as close to perfect as possible.</p><p data-block-key=\"9ro1o\">Errors in assembly can limit the <a href=\"https://research.google/blog/using-deep-learning-to-annotate-the-protein-universe/\">methods used to identify genes and proteins</a>, and can cause later diagnostic processes to miss disease-causing variants. In genome assembly, the same genome is sequenced many times, allowing iterative correction of errors. Still, with the human genome being 3 billion nucleotides, even a small error rate can mean a large total number of errors and can limit the derived genome’s utility.</p><p data-block-key=\"8kdo8\">In an effort to continually improve the resources for genome assembly, we introduce <a href=\"https://github.com/google/deeppolisher\" target=\"_blank\" rel=\"noopener noreferrer\">DeepPolisher</a>, an open-source method for genome assembly that we developed in a collaboration with the <a href=\"https://genomics.ucsc.edu/\" target=\"_blank\" rel=\"noopener noreferrer\">UC Santa Cruz Genomics Institute</a>. In our recent paper, “<a href=\"https://www.genome.org/cgi/doi/10.1101/gr.280149.124\" target=\"_blank\" rel=\"noopener noreferrer\">Highly accurate assembly polishing with DeepPolisher</a>”, published in <i>Genome Research</i>, we describe how this pipeline extends existing methods to improve the accuracy of the genome assembly. DeepPolisher reduces the number of errors in the assembly by 50% and the number of insertion or deletion (“indel”) errors by 70%. This is especially important since indel errors interfere with the identification of genes.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Background</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"2i03j\">While there are several ways to measure DNA, most typically involve capturing the process of copying DNA. One method for this involves attaching label molecules with different colors to separate building block nucleotides and observing the process of each being added to the DNA molecule being copied. The <a href=\"https://en.wikipedia.org/wiki/DNA_polymerase\" target=\"_blank\" rel=\"noopener noreferrer\">DNA copying machinery</a> always copies the strand in a particular orientation, so although the information is redundantly encoded on both strands, only nucleotides from one strand are read at a time. Identifying the nucleotides requires detectors that are able to resolve single molecules, which limits the accuracy of measurements.</p><p data-block-key=\"g8k0\">One breakthrough technology to scale this method, <a href=\"https://www.youtube.com/watch?v=fCd6B5HRaZ8\" target=\"_blank\" rel=\"noopener noreferrer\">developed by Illumina</a>, copies one molecule of the DNA to be sequenced into a cluster of identical copies. It then monitors as the cluster copies in sync, thus increasing the signal for each base. However, as it is impossible to ensure the cluster copies in perfect unison, the cluster may desynchronize so that the signal of different bases blend together, which limits the lengths of the DNA measured using this method to a few hundred nucleotides.</p><p data-block-key=\"u6mc\">Although these sequences (called “reads”) are short, they are still useful for analysis. By comparing them to a reference genome, i.e., an existing map of the genome of the species to be sequenced, it is possible to map many of the short reads to that reference, thus building up a more complete genome of the sampled individual. This can then be compared to the reference to better understand how the subject’s genome varies.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepPolisher-1-Genome.width-1250.png\" alt=\"DeepPolisher-1-Genome\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepPolisher-1-Genome.width-1250.png\" alt=\"DeepPolisher-1-Genome\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"pdugr\"><i>The human genome is composed of two strands that redundantly encode information (</i><b><i>left</i></b><i>), organized into chromosomes, with one full copy inherited from each parent (</i><b><i>right</i></b><i>). (</i><a href=\"https://www.genome.gov/about-genomics/educational-resources/fact-sheets/human-genomic-variation\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Images from NHGRI</i></a><i>)</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"2i03j\">Even with improved sequencing technology, there remain several challenges. First, the method relies on having a robust reference genome, which is itself exceptionally difficult to create. Even with such a reference, some parts of the genome look more like other parts, making them difficult to confidently map to the reference.</p><p data-block-key=\"2odgq\">To address those challenges, scientists developed processes that could sequence individual molecules, enabling reads of tens of thousands of nucleotides. Initially, this process had unacceptable error rates (~10%). This was addressed when <a href=\"https://www.pacb.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Pacific Biosciences</a> developed a way to sequence the same molecule in multiple passes, reducing the error rate to only 1%, similar to the short-read methods. Google and Pacific Biosciences worked together on the <a href=\"https://google.github.io/deepvariant/posts/2019-01-14-highly-accurate-snp-and-indel-calling-on-pacbio-ccs-with-deepvariant/\" target=\"_blank\" rel=\"noopener noreferrer\">first demonstration of this on a human genome</a>.</p><p data-block-key=\"dtq0q\">Our team then took this further by developing DeepConsensus, which uses a sequence transformer to more accurately construct the correct sequence from the initial error-prone bases. Today Pacific Biosciences deploys DeepConsensus on their long-read sequencers to reduce the error rate to less than 0.1%. While this error rate is markedly better than the prior state of the art, reaching the accuracy required to construct a new, nearly perfect reference genome, requires combining sequence reads from multiple DNA molecules of the same individual to further correct remaining errors.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">DeepPolisher</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"2i03j\">This is where DeepPolisher comes in. Adapted from DeepConsensus, DeepPolisher uses a <a href=\"https://research.google/blog/transformer-a-novel-neural-network-architecture-for-language-understanding/\">Transformer</a> architecture trained on the genome from a human cell line donated to the <a href=\"https://www.personalgenomes.org/us\" target=\"_blank\" rel=\"noopener noreferrer\">Personal Genomes Project</a>. This reference genome has been exhaustively characterized by <a href=\"https://www.nist.gov/\" target=\"_blank\" rel=\"noopener noreferrer\">NIST</a> and <a href=\"https://www.genome.gov/\" target=\"_blank\" rel=\"noopener noreferrer\">NHGRI</a> and sequenced using many different technologies. It is estimated to be ~100% complete with a correctness of 99.99999%. This corresponds to around 300–1000 total errors across the 6 billion nucleotides in the genome (two copies of the 3 billion nucleotide reference inherited from each parent).</p><p data-block-key=\"eg82g\">By conducting PacBio sequencing and genome assembly, we can identify remaining errors and then train models to learn to correct them. For training, the model takes the sequenced bases, their quality, and how uniquely they map to a given part of the reference assembly. During training, we use only chromosomes 1–19. We hold out chromosomes 20–22, using the performance on chromosomes 21 and 22 to select a model, and we report accuracies using chromosome 20.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepPolisher-2-Architecture.width-1250.png\" alt=\"DeepPolisher-2-Architecture\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepPolisher-2-Architecture.width-1250.png\" alt=\"DeepPolisher-2-Architecture\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"pdugr\"><i>Architecture of DeepPolisher. The sequence reads are categorized by parental origin (called “phasing”) and are aligned to the draft genome assembly. The input channels are: the base information, reported quality by the sequencer, the quality of the mapping (ability to place the reads uniquely on the assembly), and annotations of mismatched bases. This is sent to an encoder-only Transformer, which classifies the errors in the assembly and then suggests a fix, which is used to correct the assembly.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Performance</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"2i03j\">DeepPolisher reduces errors in a genome assembly by approximately half, an improvement largely driven by the reduction in insertion–deletion (“indel”) errors, which decrease by more than 70 percent. Reducing these types of errors is especially important, because inserted or deleted bases can shift the <a href=\"https://en.wikipedia.org/wiki/Reading_frame\" target=\"_blank\" rel=\"noopener noreferrer\">reading frame</a> of a gene, causing annotation programs to overlook that gene when labelling the genome and hiding it from reports in clinical analysis or drug discovery.</p><p data-block-key=\"190im\">We quantify the quality of a genome using a “<a href=\"https://en.wikipedia.org/wiki/Phred_quality_score\" target=\"_blank\" rel=\"noopener noreferrer\">Q-score</a>”, which is a base-10 logarithm of the probability that a position in the genome has an error. A Q30 score means 99.9% chance of being correct, while a Q60 means a 99.9999% chance of a base being correct. To assess the improvement of DeepPolisher, we pulled sequencing data being used to assemble new genomes for the <a href=\"https://humanpangenome.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Human Pangenome Reference Consortium</a> (HPRC). We looked for potential errors in the assembly by trying to identify <a href=\"https://en.wikipedia.org/wiki/K-mer\" target=\"_blank\" rel=\"noopener noreferrer\">combinations of nucleotides</a> in the assembly that don’t occur in other sequencing of the same sample with different sequencing technologies. By doing this analysis in the parts of the genome for which the other sequencing method has no systematic biases (confident region), we can show an improvement of the assembly from Q66.7 to Q70.1 on average. We also show improvement in every single sample assessed.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepPolisher-3-Performance.width-1250.png\" alt=\"DeepPolisher-3-Performance\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DeepPolisher-3-Performance.width-1250.png\" alt=\"DeepPolisher-3-Performance\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"pdugr\"><i>Assembly qualities before and after polishing for 180 samples. For each sample, the genome is separated by the parental origin (the copy of the genome transmitted by father or mother) indicated as</i> <a href=\"https://en.wikipedia.org/wiki/Haplotype\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Haplotype</i></a><i> (Hap) 1 or 2, and the assessed quality of those haplotypes.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Deployment</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"2i03j\">DeepPolisher is already being used to improve genomics resources for the scientific community. In May, the HPRC announced their second data release, which included sequenced genome assemblies on 232 individuals, a fivefold increase over the first release. The data in the second release underwent an additional polishing step with DeepPolisher that reduced single nucleotide and indel errors twofold, leading to an extremely low error rate of less than one base error in half a million assembled bases.</p><p data-block-key=\"92vt9\">By providing DeepPolisher as an open-source tool, our goal is to make the methods available broadly to the community. Working with the Human Pangenome Reference Consortium, we help enable scientists to more accurately diagnose genetic diseases for individuals of all ancestries.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"2i03j\"><i>This blog post demonstrates Google’s contribution to the development of DeepPolisher for improving the quality of genome assemblies. Integrating DeepPolisher in the broader context of generating highly accurate pangenome references involves contributions from nearly 195 authors from 68 different organizations. We thank the research groups from UCSC Genomics Institute (GI) under Professor Benedict Paten and Professor Karen Miga for helping in primary analysis and development directions of DeepPolisher. We acknowledge Mira Mastoras and Mobin Asri for leading the core analysis and integration of DeepPolisher to the pangenome generation pipeline. We thank the Google technical contributors: Pi-Chuan Chang, Daniel E. Cook, Alexey Kolesnikov, Lucas Brambrink, and Maria Nattestad. We thank Lizzie Dorfman, Dale Webster, and Katherine Chou for strategic leadership, and Monique Brouillette for help in writing.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "MLE-STAR：一种最先进的机器学习工程代理 (原标题: MLE-STAR: A state-of-the-art machine learning engineering agent)",
      "link": "https://research.google/blog/mle-star-a-state-of-the-art-machine-learning-engineering-agents/",
      "pubDate": "Thu, 31 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-31T16:00:00.000Z",
      "creator": "Google",
      "summary": "# MLE-STAR：一种最先进的机器学习工程代理\n\n## 引言：机器学习工程代理的兴起与局限\n机器学习（ML）的快速发展推动了高性能应用在各种现实场景中的广泛部署。然而，构建这些模型对机器学习工程师来说仍然是一项艰巨的任务，需要大量的迭代实验和复杂的数据工程。为了简化这些高要求的工作流程，近期研究集中于利用大型语言模型（LLMs）作为机器学习工程（MLE）代理。这些代理利用其固有的编码和推理能力，将ML任务概念化为代码优化挑战，并探索潜在的代码解决方案，最终根据提供的任务描述和数据集生成可执行代码（如Python脚本）。\n\n![MLE-STAR-1-MLEAgents](https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-1-MLEAgents.width-1250.png)\n*ML工程代理旨在通过分析任务描述和跨多种模态的数据集来解决各种机器学习挑战。*\n\n尽管现有MLE代理取得了初步进展，但它们面临一些限制，这些限制削弱了其有效性：\n*   **知识偏见：** 过度依赖LLM的现有知识，导致偏向熟悉和常用方法（例如，表格数据使用scikit-learn库），可能忽略更优的任务特定方法。\n*   **探索策略不足：** 通常采用同时修改整个代码结构的探索策略，导致代理过早地将重点转移到其他阶段（例如模型选择或超参数调整），因为它们缺乏在特定管道组件（如特征工程）内进行深入迭代探索的能力。\n\n## 介绍MLE-STAR\n我们提出了一种新颖的ML工程代理——MLE-STAR，它集成了网络搜索和有针对性的代码块细化。与现有替代方案不同，MLE-STAR通过以下方式解决ML挑战：\n1.  **网络搜索：** 首先通过网络搜索寻找合适的模型，以奠定坚实的基础。\n2.  **代码细化：** 通过测试代码中最重要的部分来仔细改进这个基础。\n3.  **模型融合：** 利用一种新方法将多个模型融合在一起，以获得更好的结果。\n\nMLE-STAR表现出色，在MLE-Bench-Lite的Kaggle竞赛中赢得了63%的奖牌，显著优于其他替代方案。\n\n## MLE-STAR的工作原理\n### 1. 初始解决方案生成\nMLE-STAR利用网络搜索检索相关且可能是最先进的方法，以构建初始解决方案代码。\n\n### 2. 解决方案增强：有针对性的代码块细化\nMLE-STAR通过以下步骤增强解决方案：\n*   **提取代码块：** 提取代表不同ML管道组件（如特征工程或集成构建）的特定代码块。\n*   **集中探索：** 专注于探索针对该组件的策略，并根据之前的尝试进行反馈。\n*   **消融研究：** 进行消融研究以确定对性能影响最大的代码块。\n*   **迭代细化：** 重复此细化过程，修改各种代码块。\n\n![MLE-STAR-2-Overview](https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-2-Overview.width-1250.png)\n*概述。(a) MLE-STAR首先使用网络搜索查找并将任务特定模型整合到初始解决方案中。(b) 对于每个细化步骤，它进行消融研究以找出对性能影响最大的代码块。(c) 识别出的代码块随后根据LLM建议的计划进行迭代细化，这些计划利用先前实验的反馈探索各种策略。选择和细化目标代码块的过程重复进行，其中(c)中改进的解决方案成为(b)中下一个细化步骤的起点。*\n\n### 3. 新颖的集成方法\nMLE-STAR首先提出多个候选解决方案。然后，它不依赖于基于验证分数的简单投票机制，而是使用代理自身提出的集成策略将这些候选方案合并为一个改进的解决方案。这种集成策略会根据先前策略的性能进行迭代细化。\n\n![MLE-STAR-3-Ensembling](https://storage.com/gweb-research2023-media/images/MLE-STAR-3-Ensembling.width-1250.png)\n*解决方案集成：MLE-STAR在连续尝试中细化其集成策略，有效地将多个并行生成的解决方案组合成一个改进的解决方案。*\n\n### 4. 增强鲁棒性的附加模块\nMLE-STAR还包含三个附加模块以增强其鲁棒性：\n*   **(i) 调试代理：** 如果Python脚本执行触发错误，MLE-STAR会使用调试模块尝试纠正。\n*   **(ii) 数据泄露检查器：** 分析解决方案脚本，防止数据泄露（例如，在训练数据准备期间不当访问测试数据集信息）。\n*   **(iii) 数据使用检查器：** 确保利用所有提供的相关数据源，因为LLM生成的脚本有时会忽略非CSV等简单格式的数据。\n\n## 评估与结果\n我们使用MLE-Bench-Lite中的Kaggle竞赛对MLE-STAR进行了全面评估。结果表明，MLE-STAR仅需最少的人工干预（例如，定义可泛化到任何任务的初始提示），就显著优于之前的替代方案，包括那些需要人工收集Kaggle策略的方案。\n\n![MLE-STAR-4-Results](https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-4-Results.width-1250.png)\n*MLE-Bench-Lite的主要结果。分数表示在MLE-Bench-Lite的Kaggle竞赛中获得奖牌的平均百分比。*\n\n具体而言，与表现最佳的基线相比，MLE-STAR在获得任何奖牌方面的提升显著，从25.8%提高到63.6%。\n\n## 对MLE-STAR性能提升的深入分析\n为了理解MLE-STAR性能提升的来源，我们从多个角度进行了分析：\n*   **模型使用：** MLE-STAR主要使用更新、更具竞争力的模型（如EfficientNet或ViT），而其他代理可能仍使用过时模型（如ResNet），这是性能提升的原因之一。\n*   **人工干预：** MLE-STAR能够以最少的人工干预轻松采用更新的模型。例如，通过手动添加RealMLP的模型描述，MLE-STAR成功将其训练代码集成到框架中。\n\n![MLE-STAR-5-ClassificationIntervention](https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-5-ClassificationIntervention.width-1250.png)\n*左图：图像分类竞赛中的模型使用率（%）。右图：展示人工干预：MLE-STAR根据手动模型描述集成模型的训练代码。*\n\n*   **LLM不当行为及纠正：**\n    *   **幻觉/不切实际的代码：** LLM生成的代码有时内容不切实际（例如，使用测试数据统计信息进行预处理）。MLE-STAR的数据泄露检查器能识别并纠正此类问题。\n    *   **忽略数据源：** LLM生成的脚本有时会忽略部分提供的数据源。MLE-STAR的数据使用检查器会重新检查任务描述，确保所有给定数据都被利用。\n\n![MLE-STAR-6-LeakageUsage](https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-6-LeakageUsage.width-1250.png)\n*左图：MLE-STAR的数据泄露检查器确保适当的预处理。右图：MLE-STAR的数据使用检查器识别并整合了以前未使用的信息。*\n\n## 结论\n我们提出了MLE-STAR，一种专为多样化ML任务设计的新型机器学习工程代理。其核心思想是利用网络搜索检索有效的模型，然后探索针对特定ML管道组件的各种策略以改进解决方案。MLE-STAR的有效性通过在MLE-Bench-Lite Kaggle竞赛中赢得63%的奖牌（其中36%为金牌）得到验证。\n\n通过自动化复杂的ML任务，MLE-STAR可以降低个人和组织利用ML的门槛，从而促进各行各业的创新。此外，由于最先进的模型不断更新和改进，MLE-STAR生成的解决方案性能预计将自动提升，因为它利用搜索引擎从网络检索有效模型。这种固有的适应性确保了随着ML领域的发展，MLE-STAR能够持续提供越来越好的解决方案。最后，开发者和研究人员现在可以通过我们新发布的基于Agent Development Kit (ADK) 构建的MLE-STAR开源代码库来加速他们的机器学习项目。",
      "shortSummary": "MLE-STAR是一种新型机器学习工程代理，通过结合网络搜索和有针对性的代码块细化来解决ML任务。它能自动寻找最新模型，并迭代优化特定代码组件。MLE-STAR还包含创新的模型集成方法及调试、数据泄露和数据使用检查器，增强了鲁棒性。在Kaggle竞赛中，MLE-STAR以最少的人工干预赢得了63%的奖牌，显著优于现有方案。它降低了ML应用门槛，并能随ML领域发展自动适应最新模型。其代码库已开源。",
      "translated_title": "MLE-STAR：一种最先进的机器学习工程代理",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-1-MLEAgents.width-1250.png",
          "alt": "MLE-STAR-1-MLEAgents",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-2-Overview.width-1250.png",
          "alt": "MLE-STAR-2-Overview",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-3-Ensembling.width-1250.png",
          "alt": "MLE-STAR-3-Ensembling",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-4-Results.width-1250.png",
          "alt": "MLE-STAR-4-Results",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-5-ClassificationIntervention.width-1250.png",
          "alt": "MLE-STAR-5-ClassificationIntervention",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"gew25\">The rise of machine learning (ML) has fueled the development of high-performance applications across a wide array of real-world scenarios, from <a href=\"https://arxiv.org/pdf/1603.02754\" target=\"_blank\" rel=\"noopener noreferrer\">tabular classification</a> to <a href=\"https://vciba.springeropen.com/articles/10.1186/s42492-019-0016-7\" target=\"_blank\" rel=\"noopener noreferrer\">image denoising</a>. However, crafting these models remains an arduous endeavor for machine learning engineers, demanding extensive iterative experimentation and data engineering. To streamline these demanding workflows, <a href=\"https://arxiv.org/pdf/2502.13138\" target=\"_blank\" rel=\"noopener noreferrer\">recent investigations</a> have concentrated on leveraging large language models (LLMs) as machine learning engineering (MLE) agents. By capitalizing on their inherent coding and reasoning skills, these agents conceptualize ML tasks as code optimization challenges. They then explore potential code solutions, ultimately generating executable code (such as a Python script) based on a provided task description and datasets.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-1-MLEAgents.width-1250.png\" alt=\"MLE-STAR-1-MLEAgents\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-1-MLEAgents.width-1250.png\" alt=\"MLE-STAR-1-MLEAgents\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8ay0k\"><i>ML engineering agents are built to tackle diverse machine learning challenges by analyzing a task description and datasets that can span various modalities. Their ultimate goal is to pinpoint the best solution for the given problem.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"gew25\">Despite their promising initial strides, current MLE agents face several limitations that curtail their efficacy. First, their heavy reliance on pre-existing LLM knowledge often leads to a bias towards familiar and frequently used methods (e.g., the <a href=\"https://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">scikit-learn</a> library for tabular data), overlooking potentially superior task-specific approaches. Furthermore, <a href=\"https://arxiv.org/pdf/2402.17453\" target=\"_blank\" rel=\"noopener noreferrer\">these agents</a> typically employ an exploration strategy that modifies the entire code structure simultaneously in each iteration. This frequently causes agents to prematurely shift focus to other stages (e.g., model selection or hyperparameter tuning) because they lack the capacity for deep, iterative exploration within specific pipeline components, such as exhaustively experimenting with different feature engineering options.</p><p data-block-key=\"cgeb0\">In our recent <a href=\"https://arxiv.org/abs/2506.15692\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>, we introduce MLE-STAR, a novel ML engineering agent that integrates <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-google-search\" target=\"_blank\" rel=\"noopener noreferrer\">web search</a> and targeted code block refinement. Unlike alternatives, MLE-STAR tackles ML challenges by first searching the web for proper models to get a solid foundation. It then carefully improves this foundation by testing which parts of the code are most important. MLE-STAR also utilizes a new method to blend several models together for even better results. This approach is very successful — it won medals in 63% of the Kaggle competitions in MLE-Bench-Lite, significantly outperforming the alternatives.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Introducing MLE-STAR</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"gew25\">To generate initial solution code, MLE-STAR uses web search to retrieve relevant and potentially state-of-the-art approaches that could be effective for building a model.<footnote id=\"da8046d1-1b0f-4379-b092-7434a3d4b87d\">[da8046]</footnote> To enhance the solution, MLE-STAR extracts a specific code block representing a distinct ML pipeline component, like feature engineering or ensemble building. It then concentrates on exploring strategies tailored to that component, reflecting on previous attempts as feedback. To identify the code block with the most significant impact on performance, MLE-STAR conducts an ablation study that evaluates the contribution of each ML component. This refinement process is repeated, modifying various code blocks.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-2-Overview.width-1250.png\" alt=\"MLE-STAR-2-Overview\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-2-Overview.width-1250.png\" alt=\"MLE-STAR-2-Overview\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8ay0k\"><i>Overview. (</i><b><i>a</i></b><i>) MLE-STAR begins by using web search to find and incorporate task-specific models into an initial solution. (</i><b><i>b</i></b><i>) For each refinement step, it conducts an ablation study to pinpoint the code block with the most significant impact on performance. (</i><b><i>c</i></b><i>) The identified code block then undergoes iterative refinement based on LLM-suggested plans, which explore various strategies using feedback from prior experiments. This process of selecting and refining target code blocks repeats, where the improved solution from (</i><b><i>c</i></b><i>) becomes the starting point for the next refinement step in (</i><b><i>b</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"gew25\">Additionally, we present a novel method for generating ensembles. MLE-STAR first proposes multiple candidate solutions. Then, instead of relying on a simple voting mechanism based on validation scores, MLE-STAR merges these candidates into a single, improved solution using an ensemble strategy proposed by the agent itself. This ensemble strategy is iteratively refined based on the performance of the preceding strategies.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-3-Ensembling.width-1250.png\" alt=\"MLE-STAR-3-Ensembling\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-3-Ensembling.width-1250.png\" alt=\"MLE-STAR-3-Ensembling\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8ay0k\"><i>Ensembling Solutions: MLE-STAR refines its ensemble strategies over successive attempts, efficiently combining multiple parallel-generated solutions into a single, improved solution.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"gew25\">Last but not least, MLE-STAR incorporates three additional modules to enhance its robustness: (i) a debugging agent, (ii) a data leakage checker, and (iii) a data usage checker. For the debugging agent, if the execution of a Python script triggers an error, leading to a record (such as a traceback), MLE-STAR employs a debugging module to attempt correction. Regarding the data leakage checker, we've observed that LLM-generated Python scripts carry the risk of introducing data leakage, for instance, by improperly accessing information from a test dataset during training data preparation. To address this, we've introduced a checker agent that analyzes the solution script prior to its execution. As for the data usage checker, we've noticed that LLM-generated scripts sometimes neglect to use all provided data sources, focusing solely on simple formats like CSVs. To ensure the utilization of all relevant provided data, MLE-STAR includes a data usage checker agent.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Evaluations and results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"gew25\">To validate its effectiveness, we conducted comprehensive evaluations of MLE-STAR using the <a href=\"https://www.kaggle.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Kaggle</a> competitions within <a href=\"https://arxiv.org/pdf/2410.07095\" target=\"_blank\" rel=\"noopener noreferrer\">MLE-Bench-Lite</a>. Here, we utilized an additional agent that takes the task description and the final solution as input, and outputs the code that incorporates loading the test sample and creating a submission file.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-4-Results.width-1250.png\" alt=\"MLE-STAR-4-Results\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-4-Results.width-1250.png\" alt=\"MLE-STAR-4-Results\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8ay0k\"><i>Main results from MLE-Bench-Lite. Scores represent the average % of achievements in Kaggle competitions in MLE-Bench-Lite.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"gew25\">The experimental results presented in the figure above demonstrate that MLE-STAR, requiring only minimal human effort (e.g., defining initial prompts that are generalizable to any tasks), significantly outperforms previous alternatives, including <a href=\"https://arxiv.org/pdf/2402.17453\" target=\"_blank\" rel=\"noopener noreferrer\">those</a> necessitating manual labor to collect strategies from Kaggle. Specifically, MLE-STAR achieves a substantial gain in any medal achievement, improving it from 25.8% to 63.6% when compared to the top-performing <a href=\"https://arxiv.org/pdf/2502.13138\" target=\"_blank\" rel=\"noopener noreferrer\">baseline</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">In-depth analysis of MLE-STAR's gains</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"gew25\">To understand the sources of MLE-STAR's performance gains, we conducted several analyses from various perspectives. Here, we examined (i) the types of ML models that MLE-STAR utilizes, (ii) how MLE-STAR can be extended with human intervention, and (iii) how the additional data leakage and usage checkers further improve MLE-STAR's performance.</p><ul><li data-block-key=\"4ijkt\"><i>Model usage</i>: Consider model usage by two MLE-agents. <a href=\"https://arxiv.org/pdf/2502.13138\" target=\"_blank\" rel=\"noopener noreferrer\">AIDE</a> primarily employs <a href=\"https://arxiv.org/pdf/1512.03385\" target=\"_blank\" rel=\"noopener noreferrer\">ResNet</a> for image classification. However, <a href=\"https://arxiv.org/pdf/1512.03385\" target=\"_blank\" rel=\"noopener noreferrer\">ResNet</a>, released in 2015, is now considered outdated and can result in suboptimal performance. In contrast, MLE-STAR primarily utilizes more recent and competitive models like <a href=\"https://arxiv.org/pdf/1905.11946\" target=\"_blank\" rel=\"noopener noreferrer\">EfficientNet</a> or <a href=\"https://arxiv.org/pdf/2010.11929\" target=\"_blank\" rel=\"noopener noreferrer\">ViT</a>, leading to the observed performance gain.<br></li><li data-block-key=\"9jkbs\"><i>Human intervention</i>: MLE-STAR readily adopts even more recent models with minimal human intervention. While MLE-STAR automatically constructs a model description using web search, a natural extension involves leveraging human expertise for this construction. By manually adding a model description for <a href=\"https://arxiv.org/pdf/2407.04491\" target=\"_blank\" rel=\"noopener noreferrer\">RealMLP</a>, MLE-STAR successfully integrates its training code into the framework, a model not previously retrieved.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-5-ClassificationIntervention.width-1250.png\" alt=\"MLE-STAR-5-ClassificationIntervention\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-5-ClassificationIntervention.width-1250.png\" alt=\"MLE-STAR-5-ClassificationIntervention\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8ay0k\"><b><i>Left:</i></b> <i>Model usage (%) in image classification competitions.</i> <b><i>Right:</i></b><i> Demonstrating human intervention: MLE-STAR integrates a model's training code based on a manual model description.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <ul><li data-block-key=\"gew25\"><i>LLM misbehavior and corrections</i>: We observed that while the code generated by the LLM executed correctly, its content was sometimes unrealistic, exhibiting hallucination. For example, the figure below (<b>left</b>) illustrates an impractical approach where test data is pre-processed using its own statistics. Since test data must remain unseen, correction in the code is necessary, for which MLE-STAR employs a data leakage checker to identify such issues and refine the generated script if a problem is detected.</li><li data-block-key=\"5bfa0\">We also observed that LLMs often generate scripts that overlook some of the provided data sources. To address this, MLE-STAR employs a data usage checker, which re-examines the task description to ensure that all given data is utilized. As shown in (<b>right</b>), this design enables MLE-STAR to incorporate previously neglected data.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-6-LeakageUsage.width-1250.png\" alt=\"MLE-STAR-6-LeakageUsage\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MLE-STAR-6-LeakageUsage.width-1250.png\" alt=\"MLE-STAR-6-LeakageUsage\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"8ay0k\"><b><i>Left:</i></b><i> MLE-STAR's data leakage checker ensures appropriate preprocessing.</i> <b><i>Right:</i></b><i> MLE-STAR's data usage checker identifies and incorporates previously unused information.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"gew25\">We proposed MLE-STAR, a novel machine learning engineering agent designed for diverse ML tasks. Our core idea is to utilize <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-google-search\" target=\"_blank\" rel=\"noopener noreferrer\">web search</a> to retrieve effective models and then explore various strategies targeting specific ML pipeline components to improve the solution. The effectiveness of MLE-STAR is validated by winning medals in 63% (36% of which are gold medals) of the <a href=\"https://arxiv.org/pdf/2410.07095\" target=\"_blank\" rel=\"noopener noreferrer\">MLE-Bench-Lite</a> Kaggle competitions.</p><p data-block-key=\"bmbh8\">By automating complex ML tasks, MLE-STAR could lower the barrier to entry for individuals and organizations seeking to leverage ML, potentially fostering innovation across various sectors. Furthermore, as state-of-the-art models are continually updated and improved, the performance of solutions generated by MLE-STAR is expected to automatically boost. This is because our framework leverages a search engine to retrieve effective models from the web to form its solutions. This inherent adaptability ensures that MLE-STAR continues to provide increasingly better solutions as the field of ML advances. Last but not least, developers and researchers can now accelerate their machine learning projects by using our newly released <a href=\"https://github.com/google/adk-samples/tree/main/python/agents/machine-learning-engineering\" target=\"_blank\" rel=\"noopener noreferrer\">open-source codebase</a> of MLE-STAR, built with the <a href=\"https://github.com/google/adk-samples/tree/main/python/agents\" target=\"_blank\" rel=\"noopener noreferrer\">Agent Development Kit</a> (ADK).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"gew25\"><i>We gratefully acknowledge the contributions of Jiefeng Chen, Jinwoo Shin, Sercan O Arik, Raj Sinha, and Tomas Pfister.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "使用回归语言模型模拟大型系统 (原标题: Simulating large systems with Regression Language Models)",
      "link": "https://research.google/blog/simulating-large-systems-with-regression-language-models/",
      "pubDate": "Mon, 28 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-28T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 使用回归语言模型模拟大型系统\n\n本文介绍了一种名为“回归语言模型”（Regression Language Model, RLM）的新方法，旨在扩展大型语言模型（LLM）的能力，使其不仅能处理主观的人类反馈，还能从原始、多样化的操作数据中预测数值结果。这使得LLM能够执行回归任务，即根据输入 `x` 预测指标 `y`。\n\n## 传统回归方法的局限性\n\n*   **依赖表格输入：** 传统的回归方法通常要求输入是表格形式，即固定长度的数值向量。\n*   **数据转换困难：** 将复杂的非结构化数据（如配置、系统日志、硬件模式）转换为表格格式非常耗时且具有挑战性。\n*   **适应性差：** 当出现新的数据类型时，整个过程往往需要从头开始。\n\n## 回归语言模型（RLM）方法\n\n基于“通过文本到文本回归进行大型系统性能预测”的研究，RLM提供了一种简单、通用且可扩展的方法：\n\n*   **输入输出：** RLM能够读取输入 `x` 的字符串表示，并以结构化文本字符串的形式输出数值 `y`。例如，工业系统的所有配置和参数可以表示为文本字符串 `x`，RLM则输出性能指标 `y`。\n*   **训练机制：** RLM可以预训练或随机初始化。对于新的回归任务，它可以通过“下一个词元预测”（next token prediction）和交叉熵损失进行训练，其中 `x` 作为提示，`y` 作为目标。\n*   **核心优势：**\n    *   避免了特征工程和数据归一化。\n    *   支持对新任务进行少样本适应。\n    *   能够普遍近似输出概率分布。\n\n## 应用案例：预测 Google Borg 的资源效率\n\n该方法被应用于预测 Google 大规模计算基础设施 Borg 上的资源效率，具体是预测“每 Google 计算单元百万指令数”（MIPS per GCU），这是 Borg 系统的一个关键效率指标。\n\n*   **任务目标：** 准确预测配置的 MIPS per GCU 对于优化数千台机器的资源分配和调度至关重要。\n*   **实施细节：**\n    *   RLM 采用一个仅包含 6000 万参数的两层编码器-解码器架构。\n    *   **数据收集：** 从多个回归任务中收集大量的 `(x,y)` 对，其中系统状态 `x` 使用 YAML 或 JSON 表示，包含活跃作业列表、执行轨迹和文本元数据。\n    *   **处理长输入：** 尽管单个数据点 `x` 可能包含多达 100 万个词元，但 RLM 的词元限制为 8k。通过预处理，将最重要的特征重新排序到文本字符串的开头，确保在截断时只丢失次要特征。\n    *   **预训练与适应：** RLM 在预处理数据上进行预训练，以便通过少样本梯度更新更容易适应新类型的输入数据。\n    *   **数值表示：** 数值直接以文本形式表示，无需归一化。多次采样解码输出可以有效捕获 `y` 值的密度，适用于建模随机或噪声情况。\n\n## RLM 的关键能力\n\nRLM 展示了作为通用回归重要组成部分的三项能力：\n\n1.  **密度捕获：**\n    *   通过多次采样 RLM 的输出，模型能够很好地捕获 `y` 值的概率分布（即密度），甚至跨越不同的时间段。\n    *   这超越了简单的点预测，提供了对 MIPS per GCU 值固有变异性和潜在范围的洞察。\n    *   能够捕获偶然不确定性（系统固有的随机性）并识别认知指标（由于有限观察或特征引起的不确定性），从而更全面地理解系统行为。\n\n2.  **不确定性量化：**\n    *   RLM 的预测不确定性与残差平方误差相关，这使得模型能够量化其预测的置信度。\n    *   当不确定时，预测分布会更宽，表明预测应更谨慎对待。\n    *   这有助于决定何时更多地依赖回归器，以及何时可能回退到更慢但更准确的 bin-packing 模拟来管理计算集群。\n    *   ![预测不确定性与回归器误差相关](https://storage.googleapis.com/gweb-research2023-media/images/RLM-2.width-1250.png)\n        *   左图：预测不确定性与回归器误差相关。\n        *   右图：RLM 预测的 KDE 图有效捕获了目标点。\n\n3.  **近乎完美、低成本的回归：**\n    *   RLM 是一种低资源、高效的模型，在各种任务上实现了非常精确的点回归。\n    *   散点图显示出近乎完美的 Spearman 秩相关性，表明预测的 MIPS per GCU 排名与实际排名高度一致。\n    *   该模型能够少样本适应不同服务器上的多样化预测任务，成为 Borg 的一个适应性强、通用性高的预测器。\n    *   ![RLM 预测与真实目标值散点图](https://storage.googleapis.com/gweb-research2023-media/images/RLM-3.width-1250.jpg)\n        *   散点图显示了 RLM 预测（x 轴）与真实目标 y 值（y 轴）在多个回归任务上的关系。图例显示了 Spearman 秩相关系数（⍴）。\n\n## 资源与未来方向\n\n*   研究表明，相对简单的编码器-解码器 RLM 能够有效训练富文本、非表格输入，提供高精度预测并高效适应新任务。\n*   这种稳健且可扩展的方法直接从原始文本预测指标结果，显著减少了对手动特征工程的依赖。\n*   为通用系统模拟器和复杂的奖励机制铺平了道路。\n*   通过建模多样化的数值反馈，RLM 以一种将促成语言模型强化学习未来突破的方式，将“经验”操作化。\n*   论文和开源代码已发布。",
      "shortSummary": "回归语言模型（RLM）扩展了大型语言模型的能力，使其能从非结构化文本数据中预测数值结果，克服了传统表格回归的局限。该方法避免了特征工程，能快速适应新任务，并量化预测不确定性。RLM已成功应用于预测Google Borg计算集群的资源效率（MIPS per GCU），展现出高精度和效率。这为通用系统模拟和先进的强化学习奖励机制奠定了基础。",
      "translated_title": "使用回归语言模型模拟大型系统",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RLM-2.width-1250.png",
          "alt": "RLM-2",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/RLM-3.width-1250.jpg",
          "alt": "RLM-3",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"4w5ob\">Large language models (LLMs) often improve by <a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback\" target=\"_blank\" rel=\"noopener noreferrer\">learning from human preferences and ratings</a>, a process where a <i>reward model</i> is trained to take prompts and responses as input in order to guide further model training. This focus on subjective human feedback has dramatically improved their ability to generate helpful, harmless, and coherent text and has been transformative for conversational assistants (e.g., <a href=\"https://blog.google/technology/ai/google-gemini-ai/#sundar-note\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini</a>).</p><p data-block-key=\"fvuh0\">Another pathway to extend the reward model beyond human subjectivity is to process raw, diverse operational data and treat the observed numerical outcome as a reward signal. This ability can open doors for <a href=\"https://en.wikipedia.org/wiki/Performance_prediction\" target=\"_blank\" rel=\"noopener noreferrer\">predicting the performance</a> of vast software infrastructures, the efficiency of industrial processes, or the results of scientific experiments. Fundamentally, we want LLMs to perform regression (i.e., predict a metric <i>y</i>, given input <i>x</i>). Previously, traditional regression methods have relied on inputs being <i>tabular</i>, i.e., fixed length numeric vectors that can be aggregated as a single table. However, converting complex, unstructured data into a tabular format can be very laborious, and the sheer diversity and dynamic nature of real-world data (e.g., intricate configuration files, system logs, and ever-evolving hardware or workload patterns) make this task even more challenging. When new data types emerge, the process often has to be restarted from scratch.</p><p data-block-key=\"db7oa\">In “<a href=\"https://arxiv.org/abs/2506.21718\" target=\"_blank\" rel=\"noopener noreferrer\">Performance Prediction for Large Systems via Text-to-Text Regression</a>”, we describe a simple, general and scalable approach, based on our earlier work on universal regression, <a href=\"https://arxiv.org/abs/2402.14547\" target=\"_blank\" rel=\"noopener noreferrer\">OmniPred</a>. This approach enables a Regression Language Model (RLM) to read a string representation of the input, and output the number as a structured text string. For example, we can represent the state (<i>x</i>) of an industrial system — including all its configurations, parameters, and contextual information — as a structured text string, and the RLM then writes the performance metric (<i>y</i>) as a string. The RLM can be pre-trained or even randomly initialized, and when handling a new regression task, it can be trained using next token prediction via <a href=\"https://en.wikipedia.org/wiki/Cross-entropy\" target=\"_blank\" rel=\"noopener noreferrer\">cross-entropy</a> loss, with (<i>x</i>) as the prompt and (<i>y</i>) as the target. We describe how this new paradigm has several advantages, such as avoiding feature engineering or normalizations, few-shot adaptation to new tasks, and universal approximation of output probability distributions. We apply the RLM in the context of predicting resource efficiency on <a href=\"https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/\">Borg</a>, Google’s large-scale compute infrastructure for cluster management. We also released an <a href=\"https://github.com/google-deepmind/regress-lm\" target=\"_blank\" rel=\"noopener noreferrer\">open-source library</a> for the research community to leverage for any use-case.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Predicting efficiency in Google's compute clusters</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"4j7mw\">The critical task of predicting the <a href=\"https://arxiv.org/abs/2406.09645\" target=\"_blank\" rel=\"noopener noreferrer\">Millions of Instructions Per Second per Google Compute Unit</a> (MIPS per GCU) is a key efficiency metric for Google's Borg system. Accurately forecasting the MIPS per GCU for configurations is crucial for optimizing resource allocation and scheduling across thousands of machines. We applied the text-to-text regression method to predict the MIPS per GCU of Google’s <a href=\"https://en.wikipedia.org/wiki/Digital_twin\" target=\"_blank\" rel=\"noopener noreferrer\"><i>digital twin</i></a> of Borg, a sophisticated backtesting framework to replicate the state of real-world clusters. The end metric is to predict the numeric result of a specialized <a href=\"https://en.wikipedia.org/wiki/Bin_packing_problem\" target=\"_blank\" rel=\"noopener noreferrer\">bin-packing algorithm</a> used to efficiently allocate tasks to resources.</p><p data-block-key=\"amou3\">Our approach uses an RLM that only requires a two-layer encoder-decoder of 60 million parameters. For training, we collect large amounts of data from multiple regression tasks with pairs (<i>x</i>,<i>y</i>) that include the system state (<i>x</i>) represented using <a href=\"https://en.wikipedia.org/wiki/YAML\" target=\"_blank\" rel=\"noopener noreferrer\">YAML</a> or <a href=\"https://en.wikipedia.org/wiki/JSON\" target=\"_blank\" rel=\"noopener noreferrer\">JSON</a>, containing the lists of active jobs, execution traces, and textual metadata. Each data point (<i>x</i>) can take up to 1M tokens if we include every feature (i.e., detailed information) about that data point. Since the RLM has a token limit of 8k, we pre-process the data by reordering the most important features at the beginning of the text string. When the string is truncated to fit the token limit, only the less important features are lost.</p><p data-block-key=\"hnfm\">We pre-train the RLM on the pre-processed data to enable the model to more easily adapt to new types of input data from new tasks, using few-shot gradient updates. Since numbers are represented as text, they can be represented as-is without normalization. If we sample decoded outputs multiple times, this effectively also captures the density of the <i>y</i>-values, important for modeling stochastic or noisy situations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/RLM-teaser-finalmp4.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"adh81\"><i>Our method uses RLMs to directly regress numerical performance metrics (y) from complex, textually represented system states (x), such as those from Google's compute clusters across diverse workloads (GMail, YouTube, Maps, etc.) and hardware (CPUs and TPUs).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"4j7mw\">Below, we demonstrate three resulting capabilities of RLMs that serve as important components for universal regression.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Density capture</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"4j7mw\">By sampling the RLM’s output multiple times, we can capture <a href=\"https://en.wikipedia.org/wiki/Stochastic_process\" target=\"_blank\" rel=\"noopener noreferrer\">probability distributions</a> (i.e., densities) of y-values remarkably well even across different time durations. This density estimation is useful because it goes beyond simple point predictions. By modeling the full distribution of possible outcomes, we gain insight into the inherent variability and potential range of MIPS per GCU values. This capability allows us to capture both <a href=\"https://en.wikipedia.org/wiki/Uncertainty_quantification\" target=\"_blank\" rel=\"noopener noreferrer\">aleatoric uncertainty</a> (inherent randomness in the system, like stochastic load demand) and potentially identify epistemic indicators (uncertainty due to limited observation or features), giving us a more complete understanding of system behavior.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block both --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/RLM-density-final.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"adh81\"><i>The RLM provides density estimates that align remarkably well with the target instructions per second distribution across time durations, as shown by the regressor density curves (3D) and the target</i> <a href=\"https://en.wikipedia.org/wiki/Kernel_density_estimation\" target=\"_blank\" rel=\"noopener noreferrer\"><i>kernel density estimate (KDE) plot</i></a><i> (XY plane).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Uncertainty quantification</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"4j7mw\">The RLM’s prediction uncertainty is correlated with residual squared error, allowing us to quantify the model’s confidence in its predictions. When uncertain, the predicted distribution is broader, signalling that the predictions should be treated with more caution. This enables us to know when to rely more heavily on the regressor, and when to potentially fall-back to slower but more accurate bin-packing simulations in managing the compute clusters.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RLM-2.width-1250.png\" alt=\"RLM-2\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RLM-2.width-1250.png\" alt=\"RLM-2\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"adh81\"><b><i>Left:</i></b><i> Prediction uncertainty is correlated with regressor error.</i> <b><i>Right:</i></b><i> KDE plot of RLM predictions effectively capture the target points.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Near-perfect, low-cost regression</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"4j7mw\">Beyond density and uncertainty quantification, our RLM is a low-resource, efficient model, achieving very precise pointwise regression on a diverse set of tasks. We present scatter plots with near-perfect <a href=\"https://en.wikipedia.org/wiki/Spearman_rank_correlation\" target=\"_blank\" rel=\"noopener noreferrer\">Spearman rank-correlation</a>, demonstrating a strong alignment between predicted and actual MIPS per GCU rankings. The model can few-shot adapt to diverse prediction tasks on distinct servers, serving as an adaptable, universal predictor for Borg.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/RLM-3.width-1250.jpg\" alt=\"RLM-3\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/RLM-3.width-1250.jpg\" alt=\"RLM-3\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"adh81\"><i>Scatterplot between RLM prediction (</i><b><i>x-axis</i></b><i>) and true target y-value (</i><b><i>y-axis</i></b><i>) over multiple regression tasks. Legend displays Spearman rank (⍴).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Resources and future directions</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"4j7mw\">We demonstrate that our relatively simple encoder-decoder RLM effectively trains on rich, non-tabular inputs to deliver highly accurate predictions and adapt to new tasks efficiently. This robust and scalable approach predicts metric outcomes directly from raw text, significantly reducing reliance on manual feature engineering and paving the way for both universal system simulators and sophisticated reward mechanisms. By modeling diverse numerical feedback, RLMs <a href=\"https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">operationalize ”experience”</a> in a manner that will enable future breakthroughs in reinforcement learning for language models. See the <a href=\"https://arxiv.org/abs/2506.21718\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a> and <a href=\"https://github.com/google-deepmind/regress-lm\" target=\"_blank\" rel=\"noopener noreferrer\">open-source code</a> for more information.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"4j7mw\"><i>This research was conducted by core members Yash Akhauri (Cornell University and Google Research), Bryan Lewandowski (Google Platforms), and Xingyou Song (Google DeepMind), with contributions from Cheng-Hsi Lin, Adrian Reyes, Grant C. Forbes, Arissa Wongpanich, Bangding Yang, Mohamed S. Abdelfattah, and Sagi Perel.</i></p><p data-block-key=\"b484a\"><i>We would like to thank previous collaborators throughout this broad research arc: Oscar Li, Chansoo Lee, Daiyi Peng, Yutian Chen, Tung Nguyen, Qiuyi Zhang, Jorg Bornschein, Yingjie Miao, Eric Tang, Dara Bahri, and Mangpo Phothilimthana. We further thank Michal Lukasik, Uri Alon, Amir Yazdanbakhsh, Shao-Hua Sun, Kuang-Huei Lee, Zi Wang, Xinyun Chen, Jiyoun Ha, Aviral Kumar, Jonathan Lai, Ke Xue, Rong-Xi Tan, and David Smalling for useful discussions. We also thank Olena Bogdanov for designing the animation for this post. Lastly, we thank Yili Zheng, Safeen Huda, Asaf Aharoni, Srinadh Bhojanapalli, David Lo, Martin Dixon, Daniel Golovin, Denny Zhou, Claire Cui, Ed Chi, and Benoit Schillings for continued support.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "SensorLM：学习可穿戴传感器的语言 (原标题: SensorLM: Learning the language of wearable sensors)",
      "link": "https://research.google/blog/sensorlm-learning-the-language-of-wearable-sensors/",
      "pubDate": "Sun, 27 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-27T16:00:00.000Z",
      "creator": "Google",
      "summary": "## SensorLM：弥合可穿戴数据与人类语言之间的鸿沟\n\n可穿戴设备（如智能手表和健身追踪器）已普及，持续捕获大量关于我们生活的数据。然而，这些原始传感器数据往往缺乏“为什么”的背景信息（例如，心率150次/分钟是由于“剧烈上坡跑”还是“压力大的公开演讲”），这限制了其在个性化健康和福祉方面的潜力。主要挑战在于缺乏将传感器记录与丰富描述性文本配对的大规模数据集，因为手动标注成本高昂且耗时。\n\n### 引入 SensorLM\n\n为了解决这一问题，研究人员推出了 **SensorLM**，一个传感器-语言基础模型家族。SensorLM 旨在弥合传感器数据与人类语言之间的鸿沟，通过学习直接从数据中解释和生成细致入微、人类可读的描述，从而在传感器数据理解方面树立了新的技术标准。\n\n### 训练 SensorLM 模型\n\n为了创建 SensorLM 所需的传感器数据集，研究团队从127个国家的103,643人那里采样了近250万个人日的去识别化数据。这些数据收集于2024年3月1日至5月1日期间，来源于 Fitbit 或 Pixel Watch 设备，参与者同意将其去识别化数据用于研究。\n\n为了克服标注瓶颈，研究人员开发了一种新颖的分层管道，通过计算统计数据、识别趋势和描述传感器数据本身的事件，自动生成描述性文本标题。这一过程使得他们能够整理出迄今为止已知最大的传感器-语言数据集，其规模比以往研究中使用的数据集大几个数量级。\n\n![SensorLM 预训练](https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-2-Pre-Training.width-1250.png)\n\nSensorLM 的预训练支持个性化洞察的新功能，例如零样本传感器理解、传感器-文本对齐和检索、少样本学习以及传感器标题生成。SensorLM 架构建立并统一了流行的多模态预训练策略，如对比学习和生成式预训练：\n\n*   **对比学习**：模型学习将传感器数据片段与其对应的文本描述从一组选项中进行匹配。这教会它区分不同的活动和状态（例如，区分“轻松游泳”和“力量训练”）。\n*   **生成式预训练**：模型学习直接从传感器数据生成文本标题。这赋予了它从理解高维传感器信号中生成丰富、上下文感知描述的能力。\n\n通过将这些方法整合到一个统一的框架中，SensorLM 形成了对传感器信号和语言之间关系的深刻多模态理解。\n\n### 关键能力与扩展行为\n\n研究团队在人类活动识别和医疗保健领域的各种真实任务中评估了 SensorLM，结果显示其比现有最先进的模型有了显著进步。\n\n*   **活动识别与检索**：\n    SensorLM 在标记数据有限的任务中表现出色。它在活动方面实现了卓越的零样本分类，无需任何微调即可准确分类20种活动，并在少样本学习中表现出色，仅通过少量示例即可快速学习。这使得模型能够以最少的数据高度适应新任务和用户。此外，SensorLM 实现了强大的跨模态检索，允许传感器数据和语言描述之间的跨模态理解。这使得研究人员能够使用传感器输入查询描述，或使用自然语言查找特定的传感器模式。\n\n    ![SensorLM 零样本活动识别](https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-3-ActivityRecognition.width-1250.jpg)\n    在零样本人类活动识别中，SensorLM 在各项任务中（以 AUROC 衡量）展示了强大的零样本能力，而基线大型语言模型表现接近随机。\n\n*   **生成能力**：\n    除了其分类能力，SensorLM 还展示了令人印象深刻的标题生成能力。仅给定可穿戴设备的高维传感器信号，SensorLM 就能生成分层且上下文相关的标题。实验结果表明，这些生成的标题比非专业大型语言模型生成的标题更连贯、更符合事实。\n\n    ![SensorLM 文本生成性能](https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-4-Captioning.width-1250.jpg)\n    SensorLM 和基线模型的标题生成性能，以 BERTScore（精确率、召回率、F1）衡量。\n\n    ![SensorLM 生成的描述示例](https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-5-CaptionExamples.width-1250.png)\n    左：输入可穿戴传感器数据。右：真实描述和不同模型生成的描述。SensorLM 直接从传感器数据生成连贯准确的标题，提供比通用语言模型更详细和准确的信息。\n\n*   **扩展行为**：\n    实验还表明，SensorLM 的性能随着数据量、模型大小和计算量的增加而持续提高，这与既定的扩展定律相符。这种持续增长表明，大规模传感器-语言预训练的潜力才刚刚被触及，进一步研究这一范式具有高度价值。\n\n    ![SensorLM 性能随规模扩展](https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-6a-Scaling.width-1250.png)\n    通过对 SensorLM 模型进行系统性扩展实验，我们发现增加计算量（左）、数据量（中）和模型大小（右）能持续提高零样本活动识别的性能。\n\n### 结论与展望\n\n这项研究通过新颖的分层标题生成管道和迄今为止最大的传感器-语言数据集，为通过自然语言理解可穿戴传感器数据奠定了基础。SensorLM 模型家族在使个人健康数据可理解和可操作方面取得了重大进展。通过教会人工智能理解我们身体的语言，我们可以超越简单的指标，走向真正个性化的洞察。\n\n展望未来，研究团队计划将预训练数据扩展到新领域，包括代谢健康和详细的睡眠分析，以应对消费健康设备复杂的现实。他们设想 SensorLM 将引领下一代数字健康教练、临床监测工具和个人健康应用程序，这些应用能够通过自然语言查询、交互和生成提供建议。",
      "shortSummary": "SensorLM是一种新型传感器-语言基础模型，旨在弥合可穿戴设备原始数据与人类语言描述之间的鸿沟。它通过对来自10万多人的近6000万小时多模态传感器数据进行预训练，学习自动生成上下文丰富的自然语言描述。SensorLM在零样本活动识别和文本生成方面表现出色，性能优于现有模型。该研究为通过自然语言理解可穿戴数据奠定了基础，有望推动个性化健康和数字健康应用的发展。",
      "translated_title": "SensorLM：学习可穿戴传感器的语言",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-2-Pre-Training.width-1250.png",
          "alt": "SensorLM-2-Pre-Training",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-3-ActivityRecognition.width-1250.jpg",
          "alt": "SensorLM-3-ActivityRecognition",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-4-Captioning.width-1250.jpg",
          "alt": "SensorLM-4-Captioning",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-5-CaptionExamples.width-1250.png",
          "alt": "SensorLM-5-CaptionExamples",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-6a-Scaling.width-1250.png",
          "alt": "SensorLM-6a-Scaling",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"0iruv\">Wearable devices, from smartwatches to fitness trackers, have become ubiquitous, continuously capturing a rich stream of data about our lives. They record our heart rate, count our steps, track our <a href=\"https://store.google.com/intl/en/ideas/articles/get-to-know-your-pixel-watch/\" target=\"_blank\" rel=\"noopener noreferrer\">fitness and sleep</a>, and much more. This deluge of information holds immense potential for personalized health and wellness. However, while we can easily see <i>what</i> our body is doing (e.g., a heart rate of 150 bpm), the crucial context of <i>why</i> (say, \"a brisk uphill run\" vs. \"a stressful public speaking event\") is often missing. This gap between raw sensor data and its real-world meaning has been a major barrier to unlocking the full potential of these devices.</p><p data-block-key=\"ert0m\">The primary challenge lies in the scarcity of large-scale datasets that pair sensor recordings with rich, descriptive text. Manually annotating millions of hours of data is prohibitively expensive and time-consuming. To solve this, and to truly let wearable data \"speak for itself\", we need models that can learn the intricate connections between <a href=\"https://research.google/blog/scaling-wearable-foundation-models/\">sensor signals</a> and human language directly from the data.</p><p data-block-key=\"ahaq4\">In “<a href=\"https://arxiv.org/abs/2506.09108\" target=\"_blank\" rel=\"noopener noreferrer\">SensorLM: Learning the Language of Wearable Sensors</a>”, we introduce SensorLM, a family of sensor–language <a href=\"https://arxiv.org/abs/2108.07258\" target=\"_blank\" rel=\"noopener noreferrer\">foundation models</a> that bridges this gap. Pre-trained on an unprecedented 59.7 million hours of <a href=\"https://cloud.google.com/use-cases/multimodal-ai\" target=\"_blank\" rel=\"noopener noreferrer\">multimodal</a> sensor data from over 103,000 individuals, SensorLM learns to interpret and generate nuanced, human-readable descriptions from high-dimensional wearable data, setting a new state of the art in sensor data understanding.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n      <div class=\"glue-ambient-video \">\n        <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n          <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/SensorLM-1-Overview.mp4\" type=\"video/mp4\">\n        </video>\n\n        <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n          <div class=\"glue-ambient-video__tooltip\">\n            <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n            <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n          </div>\n          <div class=\"glue-ambient-video__icon\">\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n            \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n          </div>\n        </div>\n      </div>\n    \n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"qk0jn\"><i>SensorLM translates complex, multimodal wearable sensor data into meaningful, natural language descriptions across statistical, structural, and semantic dimensions.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Training the SensorLM models</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"0iruv\">To create the sensor dataset needed for SensorLM, we sampled nearly 2.5M person-days of <a href=\"https://en.wikipedia.org/wiki/De-identification\" target=\"_blank\" rel=\"noopener noreferrer\">de-identified</a> data from 103,643 people across 127 countries. This data was collected between March 1st and May 1st, 2024, from Fitbit or Pixel Watch devices, with participants consenting to the use of their de-identified data for research to contribute to general knowledge about health and science.</p><p data-block-key=\"a0f10\">To overcome the annotation bottleneck, we developed a novel hierarchical pipeline that automatically generates descriptive text captions by calculating statistics, identifying trends, and describing events from the sensor data itself. This process allowed us to curate the largest-known sensor-language dataset to date, orders of magnitude larger than those used in previous studies.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-2-Pre-Training.width-1250.png\" alt=\"SensorLM-2-Pre-Training\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-2-Pre-Training.width-1250.png\" alt=\"SensorLM-2-Pre-Training\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"qk0jn\"><i>SensorLM pre-training enables new capabilities for personalized insights, such as zero-shot sensor understanding, sensor-text alignment and retrieval, few-shot learning, and sensor caption generation.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"0iruv\">The SensorLM architecture builds on and unifies prominent multimodal pre-training strategies, such as <a href=\"https://en.wikipedia.org/wiki/Self-supervised_learning#Contrastive_self-supervised_learning\" target=\"_blank\" rel=\"noopener noreferrer\">contrastive learning</a> and <a href=\"https://research.google/blog/simvlm-simple-visual-language-model-pre-training-with-weak-supervision/\">generative pre-training</a>.</p><ul><li data-block-key=\"9n60q\"><i>Contrastive Learning</i>: The model learns to match a segment of sensor data with its corresponding text description from a set of options. This teaches it to discriminate between different activities and states (e.g., distinguishing a \"light swim\" from a \"strength workout\").<br></li><li data-block-key=\"cee3m\"><i>Generative Pre-training</i>: The model learns to generate text captions directly from the sensor data. This equips it with the ability to produce rich, context-aware descriptions from understanding the high-dimensional sensor signals.</li></ul><p data-block-key=\"a559p\">By integrating these approaches into a single, cohesive framework, SensorLM develops a deep, multimodal understanding of the relationship between sensor signals and language.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Key capabilities and scaling behaviors</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o6s2n\">We evaluated SensorLM on a wide range of real-world tasks in <a href=\"https://en.wikipedia.org/wiki/Activity_recognition\" target=\"_blank\" rel=\"noopener noreferrer\">human activity recognition</a> and healthcare. The results demonstrate significant advances over previous state-of-the-art models.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Activity recognition and retrieval</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o6s2n\">SensorLM shines in tasks with limited labeled data. It achieves remarkable <a href=\"https://research.google/blog/image-text-pre-training-with-contrastive-captioners/\">zero-shot classification</a> for activity, accurately classifying from 20 activities without any fine-tuning, and excels in <a href=\"https://research.google/blog/image-text-pre-training-with-contrastive-captioners/\">few-shot learning</a>, quickly learning from just a handful of examples. This makes the model highly adaptable to new tasks and users with minimal data. Furthermore, SensorLM enables powerful <a href=\"https://research.google/blog/image-text-pre-training-with-contrastive-captioners/\">cross-modal retrieval</a>, allowing cross-modal understanding between sensor data and language descriptions. This allows us to query descriptions using sensor input, or find specific sensor patterns using natural language, facilitating expert-driven analysis (see further results can be found in the <a href=\"https://arxiv.org/pdf/2506.09108\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-3-ActivityRecognition.width-1250.jpg\" alt=\"SensorLM-3-ActivityRecognition\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-3-ActivityRecognition.width-1250.jpg\" alt=\"SensorLM-3-ActivityRecognition\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"1wd3b\"><i>In zero-shot human activity recognition, SensorLM demonstrates strong zero-shot capabilities across tasks (measured by</i> <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve\" target=\"_blank\" rel=\"noopener noreferrer\"><i>AUROC</i></a><i>), while baseline LLMs perform near random.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Generative capabilities</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o6s2n\">Beyond its classification power, SensorLM demonstrates impressive caption generation capabilities. Given only the high-dimensional sensor signals from a wearable device, SensorLM can produce hierarchical and contextually relevant captions. Experimental results indicate that these generated captions were more coherent and factually correct than those produced by powerful non-specialist LLMs.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-4-Captioning.width-1250.jpg\" alt=\"SensorLM-4-Captioning\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-4-Captioning.width-1250.jpg\" alt=\"SensorLM-4-Captioning\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"1wd3b\"><i>Captioning performance of SensorLM and baselines, measured by</i> <a href=\"https://arxiv.org/abs/1904.09675\" target=\"_blank\" rel=\"noopener noreferrer\"><i>BERTScore</i></a><i> (Precision, Recall, F1).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-5-CaptionExamples.width-1250.png\" alt=\"SensorLM-5-CaptionExamples\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-5-CaptionExamples.width-1250.png\" alt=\"SensorLM-5-CaptionExamples\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"1wd3b\"><b><i>Left:</i></b><i> Input wearable sensor data.</i> <b><i>Right:</i></b><i> The ground truth description and descriptions generated by different models. SensorLM generates coherent and accurate captions directly from sensor data, providing more detail and accuracy than generic language models.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Scaling behavior</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o6s2n\">Our experiments also revealed that SensorLM's performance consistently improves with more data, larger model sizes, and increased computation, aligning with established <a href=\"https://arxiv.org/abs/2001.08361\" target=\"_blank\" rel=\"noopener noreferrer\">scaling laws</a>. This sustained growth suggests we have only scratched the surface of what is possible with large-scale sensor-language pre-training, indicating that further investigation into this paradigm is highly valuable.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n  \n    \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-6a-Scaling.width-1250.png\" alt=\"SensorLM-6a-Scaling\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/SensorLM-6a-Scaling.width-1250.png\" alt=\"SensorLM-6a-Scaling\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n  \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"1wd3b\"><i>Through systematic scaling experiments of SensorLM models, we show that increased compute (</i><b><i>left</i></b><i>), data (</i><b><i>middle</i></b><i>), and model size (</i><b><i>right</i></b><i>) consistently improves performance on zero-shot activity recognition.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o6s2n\">Our research establishes a foundation for unlocking the understanding of wearable sensor data through natural language, enabled by a novel hierarchical captioning pipeline and the largest sensor-language dataset to date. The SensorLM family of models represents a major advance in making personal health data understandable and actionable. By teaching AI to comprehend the language of our bodies, we can move beyond simple metrics and toward truly personalized insights.</p><p data-block-key=\"4h22s\">Looking forward, we plan to scale pre-training data into new domains, including metabolic health and detailed sleep analysis, to address the messy reality of consumer health devices. We envision SensorLM leading to a future generation of digital health coaches, clinical monitoring tools, and personal wellness applications that can offer advice through natural language query, interaction, and generation. Any future products or applications inspired by this foundational research may require further assessment of any clinical and regulatory considerations that may be applicable.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"o6s2n\"><i>The research described here is joint work across Google Research, Google Health, Google DeepMind, and partnering teams. The following researchers contributed to this work: Yuwei Zhang, Kumar Ayush, Siyuan Qiao, A. Ali Heydari, Girish Narayanswamy, Maxwell A. Xu, Ahmed Metwally, Shawn Xu, Jake Garrison, Xuhai Xu, Tim Althoff, Yun Liu, Pushmeet Kohli, Jiening Zhan, Mark Malhotra, Shwetak Patel, Cecilia Mascolo, Xin Liu, Daniel McDuff, and Yuzhe Yang. We would also like to thank participants who contributed their data for this study.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    }
  ],
  "lastUpdated": "2025-08-24T10:27:25.770Z"
}