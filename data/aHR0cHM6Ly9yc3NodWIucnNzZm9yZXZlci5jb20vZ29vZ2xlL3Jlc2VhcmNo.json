{
  "sourceUrl": "https://rsshub.rssforever.com/google/research",
  "title": "Google Research Blog",
  "description": "Google Research Blog - Powered by RSSHub",
  "link": "https://research.google/blog",
  "items": [
    {
      "title": "REGEN：用自然语言赋能个性化推荐 (原标题: REGEN: Empowering personalized recommendations with natural language)",
      "link": "https://research.google/blog/regen-empowering-personalized-recommendations-with-natural-language/",
      "pubDate": "Thu, 26 Jun 2025 16:00:00 GMT",
      "isoDate": "2025-06-26T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 引言：推荐系统的演变与挑战\n\n大型语言模型（LLMs）正在重塑推荐系统与用户的交互方式。传统的推荐流程侧重于根据用户过去的互动预测下一个可能喜欢的物品（如书籍、鞋子、办公用品）。然而，真正的目标远不止于此：我们希望系统能够与用户互动，理解他们的需求，通过自然语言反馈进行调整，并解释推荐的理由。目前，尚无现有数据集能够探索这些新能力。\n\n## REGEN数据集：赋能对话式推荐\n\n为了弥补这一空白，我们开发了**REGEN（Reviews Enhanced with GEnerative Narratives）**，这是一个新的基准数据集，它整合了物品推荐、由合成用户评论组成的自然语言特征以及包含购买原因和产品推荐的个性化叙述。REGEN并非从零开始，而是借助Gemini 1.5 Flash，通过合成缺失的对话元素，增强了广泛使用的亚马逊产品评论数据集。该数据集使我们能够探索和评估新的推荐架构，这些架构既能整合用户反馈（如FLARE），也能输出与推荐一致的自然语言（如LUMEN）。我们的结果表明，在REGEN数据集上训练的LLMs能够有效地生成推荐和上下文叙述，其性能可与最先进的推荐系统和语言模型相媲美。\n\n## 构建REGEN：核心组件\n\n现有用于训练对话式推荐系统的数据集往往未能捕捉真实世界对话的细微差别。它们可能侧重于序列物品预测、简短对话片段，或者缺乏明确的用户反馈。我们选择亚马逊产品评论数据集，是因为它特别适用于大型词汇表，这些词汇表可能对LLM来说是陌生的。\n\nREGEN通过两个关键组件丰富了亚马逊评论数据集：\n\n*   **用户评论（Critiques）**\n    *   评论是对话式推荐的一个关键方面，允许用户表达他们的偏好并引导系统。在REGEN中，评论的生成旨在引导推荐系统从当前物品转向一个相似的、期望的物品。例如，用户可能会评论一支“红色圆珠笔”，说“我更喜欢一支黑色的”。\n    *   为确保评论的相关性，我们仅为足够相似的相邻物品对生成评论，使用亚马逊评论提供的分层物品类别作为相似性的代理。Gemini 1.5 Flash模型为每对物品生成了几个评论选项，我们从中随机选择一个包含在数据集中。\n\n*   **叙述（Narratives）**\n    *   叙述提供了关于推荐物品的丰富上下文信息，增强了用户体验。REGEN包含多种叙述，例如：\n        *   **购买原因**：解释为什么某个物品可能适合用户。\n        *   **产品推荐**：描述突出物品的优点和特点。\n        *   **用户摘要**：用户偏好和购买历史的简洁概况。\n    *   这些叙述在上下文和长度上各不相同，为训练对话式推荐系统提供了丰富的数据集。\n\n## 实验设计：联合生成式对话推荐任务\n\n为了有效评估REGEN，我们不仅想测试模型是否能推荐正确的物品，还想看看它们是否能传达其推理过程，适应反馈，并生成符合用户需求的语言。因此，我们构建了一种新型任务：**联合生成式对话推荐**。这个想法简单而强大——给定购买历史，以及可选的自然语言评论（例如，“我需要更多存储空间的东西”），模型必须推荐下一个物品并生成关于它的上下文叙述。\n\n这项任务反映了用户在有机会用自己的语言表达偏好时，与推荐系统自然互动的方式。它也摆脱了分离建模，即推荐和语言生成分别处理。相反，我们将两者视为统一的、端到端目标的一部分。\n\n为了探索不同的建模方法，我们开发并实现了两种基线架构：\n\n*   **混合系统（Hybrid System）**\n    *   其中一个序列推荐器（FLARE）根据协同过滤和内容信号预测下一个物品。然后，该输出被输入到一个轻量级LLM（Gemma 2B），由其负责生成叙述。这种设置反映了生产系统中常见的架构，其中不同的组件专注于管道的不同阶段。\n\n*   **LUMEN（LLM-based Unified Multi-task Model with Critiques, Recommendations, and Narratives）**\n    *   LUMEN在一个单一的LLM内部完成所有任务。它经过端到端训练，以连贯的方式处理评论、生成推荐和产生叙述。在解码过程中，模型决定何时发出物品ID以及何时继续生成自然语言。我们修改了词汇表和嵌入层以支持两种类型的输出——物品标记和文本标记——这使得模型能够将物品推荐视为生成过程的另一个部分。\n\n这种双重方法——混合式与完全生成式——使我们能够衡量模块化和集成之间的权衡，并为衡量模型处理这种更全面的对话任务的能力提供了坚实的基础。\n\n## 实验结果与分析\n\n我们的实验表明，REGEN能够有意义地挑战和区分模型在推荐和生成任务上的表现。在亚马逊产品评论数据集的办公用品领域，我们观察到将用户评论纳入输入始终能提高两种架构的推荐指标。例如，FLARE混合模型在衡量所需物品出现在前10个预测结果中的频率（即Recall@10）的指标上，其已达到最先进的性能（0.124），当在办公用品数据集中包含评论时，该性能提高到0.1402，这是一个显著的提升，强调了语言引导细化的价值。\n\nLUMEN的性能具有竞争力，尽管在传统推荐指标上略低。考虑到在一次通过中联合生成物品和叙述的难度增加，这并不令人意外。然而，其真正的优势在于它能够保持物品和所生成文本之间的连贯性。与模块化管道不同，模块化管道中组件之间的脱节可能导致尴尬或通用的解释，LUMEN的叙述往往更自然地与用户的历史和评论上下文保持一致。\n\n在生成方面，我们使用BLEU、ROUGE和语义相似度评估了输出。混合模型在BLEU和ROUGE上通常得分更高，特别是对于产品推荐和购买原因，这可能是因为LLM被提供了正确的物品作为提示。相比之下，LUMEN的n-gram重叠略低，但保持了强大的语义对齐，特别是对于更多依赖长期用户行为而非特定物品的用户摘要（详见论文）。\n\n![FLARE-3a-Benchmarks](https://storage.googleapis.com/gweb-research2023-media/images/FLARE-3a-Benchmarks.width-1250.png)\n\n这些结果突出了一些有趣的动态。主要依赖于用户的叙述（如用户偏好摘要）对两种模型来说都更容易持续生成。但是当叙述与物品上下文紧密耦合时（如产品推荐），性能更多地取决于推荐的准确性。如果模型推荐了错误的物品，它可能会破坏整个叙述。这种影响在LUMEN中更为明显，因为物品和叙述是共同生成的，这使得它成为端到端对齐的更严格测试。\n\n我们还在更大的物品空间（服装领域，拥有超过370,000个独特物品，比其他任何产品类别大5-60倍）上评估了性能。据我们所知，没有其他人在如此大的服装数据集上进行评估，这是FLARE和REGEN的一个关键区别。即使在这种更复杂的设置中，混合系统也表现良好，并且在包含评论时，Recall@10再次出现明显提升（从0.1264到0.1355），验证了REGEN作为奖励细致、用户引导推理的基准设计。\n\n![FLARE-Recommendation](https://storage.googleapis.com/gweb-research2023-media/images/FLARE-Recommendation.width-1250.png)\n\n## 结论与展望\n\nREGEN提供了一个包含一致用户偏好、推荐和生成叙述的数据集，从而能够研究LLM在对话式推荐中的能力。我们使用LUMEN（一个基于LLM的联合推荐和叙述生成模型）以及序列推荐模型评估了REGEN，展示了其效用。我们相信REGEN是研究对话式推荐模型能力的基础资源，是迈向个性化多轮系统的关键一步。\n\nREGEN通过将语言作为基本元素整合，推进了对话式推荐，增强了推荐器解释和响应用户偏好的方式。这种方法促进了对多轮交互的研究，其中系统可以进行扩展对话，根据不断变化的用户反馈来完善推荐。\n\n该数据集还鼓励开发更复杂的模型和训练方法。它支持探索模型容量的扩展、利用先进的训练技术以及将方法适应于亚马逊评论之外的不同领域，例如旅行、教育和音乐。\n\n最终，REGEN为推荐系统设定了新的方向，强调理解和交互，为更直观、支持性和人性化的推荐体验铺平了道路。",
      "shortSummary": "REGEN是一个新的基准数据集，旨在推动基于大型语言模型（LLMs）的个性化对话式推荐系统。它通过合成用户评论和个性化叙述来增强现有亚马逊产品评论数据集，使模型能够理解自然语言反馈并生成解释性内容。实验表明，在REGEN上训练的LLMs（如LUMEN和混合模型）能有效生成推荐和上下文叙述，性能与现有最先进系统相当，尤其在整合用户反馈和保持推荐与解释连贯性方面表现出色。REGEN为开发更智能、更具交互性的推荐系统提供了关键资源。",
      "translated_title": "REGEN：用自然语言赋能个性化推荐",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/FLARE-3a-Benchmarks.width-1250.png",
          "alt": "FLARE-3a-Benchmarks",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/FLARE-Recommendation.width-1250.png",
          "alt": "FLARE-Recommendation",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7fyz5\">Large language models (LLMs) are reshaping how recommender systems interact with users. Traditional recommendation pipelines focus on predicting the next item a user might like — books, shoes, office supplies, etc. — based on past interactions. But the real goal goes further: we want systems that interact with users, understand their needs, adapt through natural language feedback, and explain why a recommendation makes sense. However, no datasets currently exist to explore these new capabilities.</p><p data-block-key=\"5h38j\">To address this gap we developed <a href=\"https://arxiv.org/pdf/2503.11924\" target=\"_blank\" rel=\"noopener noreferrer\">Reviews Enhanced with GEnerative Narratives</a> (REGEN), a new <a href=\"https://www.kaggle.com/datasets/googleai/regen-reviews-enhanced-with-generative-narratives\" target=\"_blank\" rel=\"noopener noreferrer\">benchmark dataset</a> that incorporates item recommendations, natural language features composed of synthetic user critiques, and personalized narratives comprising purchase reasons and product endorsements. Rather than start from scratch, we augmented the widely-used <a href=\"https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Product Reviews dataset</a> by synthesizing missing conversational elements with the help of <a href=\"https://ai.google.dev/gemini-api/docs/models#gemini-1.5-flash\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini 1.5 Flash</a>. This dataset allows us to explore and benchmark new recommender architectures that incorporate both user feedback (e.g., <a href=\"https://arxiv.org/pdf/2409.11699\" target=\"_blank\" rel=\"noopener noreferrer\">FLARE</a>) as well those that output natural language consistent with the recommendations (e.g., <a href=\"https://arxiv.org/pdf/2503.11924\" target=\"_blank\" rel=\"noopener noreferrer\">LUMEN</a>). Our results show that LLMs trained on our dataset effectively generate both recommendations and contextual narratives, achieving performance comparable to state-of-the-art recommenders and language models.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Building the REGEN dataset</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7fyz5\">Existing datasets for training conversational recommenders often fall short in capturing the nuances of real-world conversations. They may focus on sequential item prediction, short dialog snippets, or lack explicit user feedback. We chose the Amazon Product Reviews dataset because of its specific utility for large vocabularies, potentially unfamiliar to an LLM.</p><p data-block-key=\"bdcfu\">REGEN enriches the Amazon Reviews dataset with two key components:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Critiques</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7fyz5\">Critiques are a crucial aspect of conversational recommendation, allowing users to express their preferences and guide the system. In REGEN, critiques are generated to steer the recommender from a current item to a similar, desired item. For example, a user might critique a \"red ball-point pen\" by saying, \"I'd prefer a black one\".</p><p data-block-key=\"9cejc\">To ensure the relevance of critiques, we generate them only for adjacent item pairs that are sufficiently similar, using the Amazon Reviews–provided hierarchical item categories as a proxy for similarity. The Gemini 1.5 Flash model generates several critique options for each pair, from which we select one at random to include in the dataset.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h3 class=\"\">Narratives</h3>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7fyz5\">Narratives provide rich contextual information about recommended items, enhancing the user experience. REGEN includes diverse narratives, such as:</p><ul><li data-block-key=\"53ip4\"><i>Purchase reasons</i>: Explanations for why an item might be suitable for a user.</li><li data-block-key=\"80ko6\"><i>Product endorsements</i>: Descriptions highlighting the benefits and features of an item.</li><li data-block-key=\"50do9\"><i>User summaries</i>: Concise profiles of user preferences and purchase history.</li></ul><p data-block-key=\"5gbcr\">These narratives vary in contextualization and length, providing a rich dataset for training conversational recommenders.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Experiments</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7fyz5\">To evaluate REGEN effectively, we didn’t just want to test if models could recommend the right item, we wanted to see if they could communicate their reasoning, adapt to feedback, and generate language that feels tailored to the user. So we framed a new kind of task: conversational recommendation that’s jointly generative. The idea is simple but powerful — given a provided purchase history, and optionally a natural language critique (e.g., “I need something with more storage”), a model must recommend the next item <i>and</i> generate a contextual narrative about it.</p><p data-block-key=\"53f82\">This task reflects how users naturally interact with recommendation systems when given the opportunity to express preferences in their own words. It also moves away from disjointed modeling, where recommendation and language generation are handled separately. Instead, we treat both as part of a unified, end-to-end objective.</p><p data-block-key=\"a1bi9\">To explore different modeling approaches, we developed and implemented two baseline architectures. The first is a hybrid system, where a sequential recommender (FLARE) predicts the next item based on collaborative filtering and content signals. That output is then fed into a lightweight LLM (<a href=\"https://huggingface.co/google/gemma-2b\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma 2B</a>), which is responsible for generating the narrative. This setup reflects a common architecture in production systems, where different components specialize in different stages of the pipeline.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n            <div class=\"glue-ambient-video \">\n                <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/FLARE-1-FLARE.mp4\" type=\"video/mp4\">\n                </video>\n\n                <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n                    <div class=\"glue-ambient-video__tooltip\">\n                    <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                    <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                    </div>\n                    <div class=\"glue-ambient-video__icon\">\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                    </div>\n                </div>\n            </div>\n        \n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7fyz5\">The second architecture is LUMEN (LLM-based Unified Multi-task Model with Critiques, Recommendations, and Narratives). LUMEN does everything inside a single LLM. It’s trained end-to-end to handle critiques, generate recommendations, and produce narratives in a coherent way. During decoding, the model decides when to emit an item ID and when to continue generating natural language. We modified the vocabulary and embedding layers to support both types of outputs — item tokens and text tokens — which allowed the model to treat item recommendation as just another part of the generative process.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n            <div class=\"glue-ambient-video \">\n                <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/FLARE-2b-LUMEN.mp4\" type=\"video/mp4\">\n                </video>\n\n                <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n                    <div class=\"glue-ambient-video__tooltip\">\n                    <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                    <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                    </div>\n                    <div class=\"glue-ambient-video__icon\">\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                    </div>\n                </div>\n            </div>\n        \n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7fyz5\">This dual approach — hybrid versus fully generative — lets us benchmark the trade-offs between modularity and integration, and provides a solid foundation for measuring how well models can tackle this more holistic conversational task.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7fyz5\">Our experiments show that REGEN can meaningfully challenge and differentiate models across both recommendation and generation tasks. In the Amazon Product Reviews dataset's Office domain, we observed that incorporating user critiques into the input consistently improved recommendation metrics across both architectures. For example, the FLARE hybrid model’s already state-of-the-art performance (0.124) on a metric which measures how often a desired item appears in the top 10 predicted results (known as Recall@10) increased to 0.1402 when critiques were included in the Office dataset, a notable bump that underscores the value of language-guided refinement.</p><p data-block-key=\"6nrlm\">LUMEN’s performance was competitive, albeit slightly lower on traditional recommendation metrics. That’s not surprising, given the increased difficulty of generating the item and narrative jointly in a single pass. However, its real strength lies in its ability to maintain coherence between the item and the text it produces. Unlike modular pipelines, where disconnects between components can lead to awkward or generic explanations, LUMEN’s narratives tend to align more naturally with the user’s history and critique context.</p><p data-block-key=\"4qjcj\">On the generation side, we evaluated outputs using <a href=\"https://en.wikipedia.org/wiki/BLEU\" target=\"_blank\" rel=\"noopener noreferrer\">BLEU</a>, <a href=\"https://en.wikipedia.org/wiki/ROUGE_(metric)\" target=\"_blank\" rel=\"noopener noreferrer\">ROUGE</a>, and <a href=\"https://en.wikipedia.org/wiki/Semantic_similarity\" target=\"_blank\" rel=\"noopener noreferrer\">semantic similarity</a>. The hybrid model generally scored higher on BLEU and ROUGE, especially for product endorsements and purchase reasons, likely because the LLM was given the correct item as a prompt. LUMEN, by contrast, had slightly lower <a href=\"https://en.wikipedia.org/wiki/N-gram\" target=\"_blank\" rel=\"noopener noreferrer\"><i>n</i>-gram</a> overlap but maintained strong semantic alignment, particularly for user summaries that relied more on long-term user behavior than on the specific item (see <a href=\"https://arxiv.org/pdf/2503.11924\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a> for details).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/FLARE-3a-Benchmarks.width-1250.png\" alt=\"FLARE-3a-Benchmarks\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/FLARE-3a-Benchmarks.width-1250.png\" alt=\"FLARE-3a-Benchmarks\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7fyz5\">These results highlight a few interesting dynamics. Narratives that depend primarily on the user, like summaries of their preferences, are easier for both models to generate consistently. But when the narrative is tightly coupled with the item context, like a product endorsement, performance hinges more on recommendation accuracy. If the model recommends the wrong item, it can throw off the entire narrative. This effect is more pronounced in LUMEN, where both the item and the narrative are co-generated, making it a stricter test of end-to-end alignment.</p><p data-block-key=\"dse3\">We also evaluated performance on a much larger item space using the Clothing domain, which has over 370,000 unique items (5x–60x larger than any other product category). No one else we’re aware of performs evaluations on this much larger Clothing dataset, a key distinction of FLARE and REGEN. Even in this more complex setting, the hybrid system held up well, and again we saw clear gains in Recall@10 from 0.1264 to 0.1355 when critiques were included, validating the design of REGEN as a benchmark that rewards nuanced, user-guided reasoning.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/FLARE-Recommendation.width-1250.png\" alt=\"FLARE-Recommendation\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/FLARE-Recommendation.width-1250.png\" alt=\"FLARE-Recommendation\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7fyz5\">REGEN provides a dataset with consistent user preferences, recommendations, and generated narratives, enabling the study of LLM capabilities in conversational recommendation. We evaluated REGEN using LUMEN, an LLM-based model for joint recommendation and narrative generation, demonstrating its utility, along with sequential recommender models. We believe REGEN serves as a fundamental resource for studying the capabilities of conversational recommender models, a crucial step towards personalized multi-turn systems.</p><p data-block-key=\"5n641\">REGEN advances conversational recommendation by integrating language as a fundamental element, enhancing how recommenders interpret and respond to user preferences. This approach fosters research into multi-turn interactions, where systems can engage in extended dialogues to refine recommendations based on evolving user feedback.</p><p data-block-key=\"b1ehv\">The dataset also encourages the development of more sophisticated models and training methodologies. It supports exploration into scaling model capacity, utilizing advanced training techniques, and adapting the methodology across different domains beyond Amazon reviews, such as travel, education, and music.</p><p data-block-key=\"ffahk\">Ultimately, REGEN sets a new direction for recommender systems, emphasizing comprehension and interaction, which paves the way for more intuitive, supportive, and human-like recommendation experiences.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7fyz5\"><i>We would like to thank our co-authors of the FLARE and REGEN papers without whom this work would not be possible: Liam Hebert from University of Waterloo and Kun Su, James Pine, Marialena Kyriakidi, Yuri Vasilevski, Raghavendra Vasudeva, Ambarish Jash, Sukhdeep Sodhi, Anushya Subbiah, from Google Research. Additionally, we are grateful for the support and guidance of our leadership Vikram Aggarwal, John Anderson, Dima Kuzmin, Emil Praun and Sarvjeet Singh. We also are grateful to Kimberly Schwede, Mark Simborg and the Google Research Blog editorial staff for helping us present our work to a larger audience. Finally we appreciate the authors of “</i><a href=\"https://cseweb.ucsd.edu/~jmcauley/pdfs/emnlp19a.pdf\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Justifying Recommendations Using Distantly-Labeled Reviews and Fine-Grained Aspects</i></a><i>” for releasing the Amazon Product Reviews dataset used in our work.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "MUVERA：使多向量检索像单向量搜索一样快 (原标题: MUVERA: Making multi-vector retrieval as fast as single-vector search)",
      "link": "https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/",
      "pubDate": "Tue, 24 Jun 2025 16:00:00 GMT",
      "isoDate": "2025-06-24T16:00:00.000Z",
      "creator": "Google",
      "summary": "## MUVERA：实现多向量检索的单向量速度\n\n### 引言：信息检索与嵌入模型\n\n现代信息检索（IR）的核心是神经嵌入模型。这些模型将每个数据点转换为一个“单向量嵌入”，使得语义相似的数据点被转换为数学上相似的向量，并通过最大内积搜索（MIPS）算法实现高效检索。然而，多向量模型（如ColBERT）的出现显著提升了IR任务的性能。与单向量不同，多向量模型使用一组嵌入来表示每个数据点，并利用更复杂的相似性函数（如Chamfer相似性）来捕捉数据点之间更丰富的关系。尽管这种方法提高了准确性，但由于嵌入数量的增加和相似性评分的复杂性，它带来了巨大的计算挑战，使得检索成本显著提高。\n\n### MUVERA：弥合效率鸿沟的解决方案\n\n在论文《MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encodings》中，我们提出了一种新颖的多向量检索算法MUVERA，旨在弥合单向量和多向量检索之间的效率差距。MUVERA通过构建查询和文档的**固定维度编码（FDEs）**，将复杂的多向量检索问题简化为单向量MIPS。FDEs是单向量，其内积可以近似多向量相似性。这种方法允许我们利用高度优化的MIPS算法来检索初始候选集，然后使用精确的多向量相似性进行重新排序，从而在不牺牲准确性的前提下实现高效的多向量检索。我们已在GitHub上提供了FDE构建算法的开源实现。\n\n### 多向量检索的挑战\n\n多向量模型通常为每个查询或文档生成多个嵌入（通常是每个词元一个）。查询和文档之间的相似性通常使用Chamfer匹配来计算，它衡量每个查询嵌入与最接近的文档嵌入之间的最大相似性，然后将这些相似性累加起来。Chamfer相似性提供了一种“整体”衡量，反映了查询的每个部分如何与文档的某个部分相关联。\n\n尽管多向量表示提供了改进的可解释性和泛化能力，但它们带来了显著的检索挑战：\n\n*   **嵌入量增加**：为每个词元生成嵌入会大大增加需要处理的嵌入数量。\n*   **复杂且计算密集型相似性评分**：Chamfer匹配是一种非线性操作，需要进行矩阵乘法，比单向量点积更昂贵。\n*   **缺乏高效的次线性搜索方法**：单向量检索受益于高度优化的算法（例如基于空间划分的算法），这些算法可以同时实现高准确性和次线性搜索时间。多向量相似性的复杂性阻碍了这些快速几何技术的直接应用，从而阻碍了大规模高效检索。\n\n传统的单向量MIPS算法无法直接应用于多向量检索，因为一个文档可能与单个查询词元具有高相似性，但整体上文档可能并不相关。这使得需要更复杂和计算密集型的检索方法。\n\n### MUVERA：固定维度编码的解决方案\n\nMUVERA通过将多向量相似性搜索简化为单向量MIPS，从而显著加快了复杂多向量数据的检索速度。MUVERA的核心思想是将整个多向量组压缩成一个更易于处理的单向量，即固定维度编码（FDE）。关键在于，比较这些简化的FDEs所得到的结果，与比较原始更复杂的多向量集所得到的结果非常接近。这使得我们可以使用为单向量设计的更快搜索方法。\n\nMUVERA的工作原理简化如下：\n\n1.  **FDE生成**：MUVERA采用映射将查询和文档的多向量集转换为FDEs。这些映射旨在以固定长度的向量捕获基本的相似性信息。\n2.  **基于MIPS的检索**：文档的FDEs使用标准的MIPS求解器进行索引。给定一个查询，计算其FDE，然后MIPS求解器高效地检索最相似的文档FDEs。\n3.  **重新排序**：通过MIPS检索到的初始候选集使用原始的Chamfer相似性进行重新排序，以提高准确性。\n\nMUVERA的一个关键优势是FDE转换是**数据无关**的。这意味着它不依赖于特定的数据集，使其对数据分布的变化具有鲁棒性，并适用于流式应用。此外，与模型生成的单向量不同，FDEs保证在指定误差范围内近似真实的Chamfer相似性。因此，在重新排序阶段之后，MUVERA保证能找到最相似的多向量表示。\n\n### 理论基础\n\nMUVERA的方法灵感来源于概率树嵌入技术，但我们将其应用于内积和Chamfer相似性。FDE生成的核心思想是将嵌入空间划分为多个区域。如果查询和文档中的相似向量落入同一区域，我们就可以有效地近似它们的相似性。由于我们事先不知道查询和文档向量之间的最佳匹配，我们使用随机分区方案。MUVERA还提供了理论保证，证明FDEs能强近似Chamfer相似性，这为使用单向量代理进行多向量检索提供了一种有原则的方法，并具有可证明的准确性。\n\n### 实验结果\n\n我们在BEIR基准测试中的多个信息检索数据集上评估了MUVERA。实验表明，与现有最先进的方法PLAID相比，MUVERA在显著降低延迟的同时，始终保持高检索准确性。\n\n主要发现包括：\n\n*   **召回率提高**：MUVERA优于单向量启发式方法（PLAID也采用的常见方法），在检索显著更少候选文档的情况下实现了更好的召回率。例如，FDEs只需检索5-20倍的候选文档即可达到固定的召回率。\n\n    ![MUVERA3_Recall](https://storage.googleapis.com/gweb-research2023-media/images/MUVERA3_Recall.width-1250.png)\n    不同维度的固定维度编码（FDE）与单向量启发式（SV）的召回率对比。请注意，10240维的FDEs与原始MV表示（用于SV启发式）具有几乎相同的表示大小，但在搜索中所需的比较次数显著减少（即使对于20k维的FDEs也是如此）。\n\n*   **延迟降低**：与高度优化的多向量检索系统PLAID相比，MUVERA在BEIR数据集上平均实现了10%更高的召回率，同时延迟显著降低了90%。\n\n    ![MUVERA5_ResultsFinal](https://storage.googleapis.com/gweb-research2023-media/images/MUVERA5_ResultsFinal.width-1250.png)\n    MUVERA与PLAID在BEIR基准测试上的表现。\n\n此外，我们发现MUVERA的FDEs可以使用乘积量化进行有效压缩，将内存占用减少32倍，而对检索质量的影响最小。这些结果突显了MUVERA显著加速多向量检索的潜力，使其在实际应用中更具可行性。\n\n### 结论\n\n我们提出了MUVERA，一种新颖高效的多向量检索算法，具有可证明的近似质量保证和良好的实际性能。通过将多向量搜索简化为单向量MIPS，MUVERA利用现有优化搜索技术，以显著提高的效率实现了最先进的性能。感兴趣的读者可以在GitHub上找到我们FDE构建算法的开源实现。\n\n我们的工作为高效多向量检索开辟了新途径，这对于搜索引擎、推荐系统和自然语言处理等各种应用至关重要。我们相信，对MUVERA的进一步研究和优化将带来更大的性能提升和多向量检索技术的更广泛采用。",
      "shortSummary": "MUVERA是一种创新的多向量检索算法，旨在解决多向量模型（如ColBERT）在提高准确性同时带来的计算开销。它通过将多向量数据转换为固定维度编码（FDEs），将复杂的多向量检索问题简化为高效的单向量MIPS。MUVERA利用MIPS进行快速初步检索，再通过精确的多向量相似性进行重新排序，从而在不牺牲准确性的前提下显著降低检索延迟（比PLAID降低90%）并提高召回率。该方法数据无关且具有理论保证，为实际应用中的高效多向量检索提供了可行方案。",
      "translated_title": "MUVERA：使多向量检索像单向量搜索一样快",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MUVERA3_Recall.width-1250.png",
          "alt": "MUVERA3_Recall",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MUVERA5_ResultsFinal.width-1250.png",
          "alt": "MUVERA5_ResultsFinal",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"0xkr2\">Neural <a href=\"https://developers.google.com/machine-learning/crash-course/embeddings\" target=\"_blank\" rel=\"noopener noreferrer\">embedding models</a> have become a cornerstone of modern <a href=\"https://en.wikipedia.org/wiki/Information_retrieval\" target=\"_blank\" rel=\"noopener noreferrer\">information retrieval</a> (IR). Given a query from a user (e.g., “How tall is Mt Everest?”), the goal of IR is to find information relevant to the query from a very large collection of data (e.g., the billions of documents, images, or videos on the Web). Embedding models transform each datapoint into a single-vector “embedding”, such that <i>semantically</i> similar datapoints are transformed into <i>mathematically</i> similar vectors. The embeddings are generally compared via the <a href=\"https://en.wikipedia.org/wiki/Inner_product_space#Euclidean_vector_space\" target=\"_blank\" rel=\"noopener noreferrer\">inner-product similarity</a>, enabling efficient retrieval through optimized <a href=\"https://en.wikipedia.org/wiki/Maximum_inner-product_search\" target=\"_blank\" rel=\"noopener noreferrer\">maximum inner product search</a> (MIPS) algorithms. However, recent advances, particularly the introduction of multi-vector models like <a href=\"https://www.answer.ai/posts/colbert-pooling.html\" target=\"_blank\" rel=\"noopener noreferrer\">ColBERT</a>, have demonstrated significantly improved performance in IR tasks.</p><p data-block-key=\"ef8b9\">Unlike single-vector embeddings, multi-vector models represent each data point with a <i>set</i> of embeddings, and leverage more sophisticated similarity functions that can capture richer relationships between datapoints. For example, the popular <a href=\"https://www.sciencedirect.com/topics/engineering/chamfer-matching\" target=\"_blank\" rel=\"noopener noreferrer\">Chamfer similarity measure</a> used in state-of-the-art multi-vector models captures when the information in one multi-vector embedding is contained within another multi-vector embedding. While this multi-vector approach boosts accuracy and enables retrieving more relevant documents, it introduces substantial computational challenges. In particular, the increased number of embeddings and the complexity of multi-vector similarity scoring make retrieval significantly more expensive.</p><p data-block-key=\"bkmgl\">In “<a href=\"https://arxiv.org/abs/2405.19504\" target=\"_blank\" rel=\"noopener noreferrer\">MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encodings</a>”, we introduce a novel multi-vector retrieval algorithm designed to bridge the efficiency gap between single- and multi-vector retrieval. We transform multi-vector retrieval into a simpler problem by constructing fixed dimensional encodings (FDEs) of queries and documents, which are single vectors whose inner product approximates multi-vector similarity, thus reducing complex multi-vector retrieval back to single-vector maximum inner product search (MIPS). This new approach allows us to leverage the highly-optimized MIPS algorithms to retrieve an initial set of candidates that can then be re-ranked with the exact multi-vector similarity, thereby enabling efficient multi-vector retrieval without sacrificing accuracy. We have provided an open-source implementation of our FDE construction algorithm on <a href=\"https://github.com/google/graph-mining/tree/main/sketching/point_cloud\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"0xkr2\">The challenge of multi-vector retrieval</h2><p data-block-key=\"6ktb8\">Multi-vector models generate multiple embeddings per query or document, often one embedding per token. One typically calculates the similarity between a query and a document using Chamfer matching, which measures the maximum similarity between each query embedding and the closest <a href=\"https://github.com/Yeema/ecommerce-review-score-classification/blob/master/resources/docs/TUTORIAL_5_DOCUMENT_EMBEDDINGS.md\" target=\"_blank\" rel=\"noopener noreferrer\">document embedding</a>, and then adds these similarities up across all query vectors (the standard method of computing multi-vector similarity). The Chamfer similarity, therefore, provides a \"holistic\" measure of how each part of the query relates to some part of the document.</p><p data-block-key=\"5s9if\">While multi-vector representations offer advantages like improved interpretability and generalization, they pose significant retrieval challenges:</p><ul><li data-block-key=\"1kcio\"><i>Increased embedding volume</i>: Generating embeddings per token drastically increases the number of embeddings to be processed.</li><li data-block-key=\"50lte\"><i>Complex and compute-intensive similarity scoring</i>: Chamfer matching is a non-linear operation requiring a matrix product, which is more expensive than a single vector <a href=\"https://en.wikipedia.org/wiki/Dot_product\" target=\"_blank\" rel=\"noopener noreferrer\">dot-product</a>.</li><li data-block-key=\"5mtsc\"><i>Lack of efficient sublinear search methods</i>: Single-vector retrieval benefits from highly optimized algorithms (e.g., based on <a href=\"https://en.wikipedia.org/wiki/Space_partitioning\" target=\"_blank\" rel=\"noopener noreferrer\">space partitioning</a>) that simultaneously achieve high accuracy and sublinear search times, avoiding exhaustive comparisons. The complex nature of multi-vector similarity prevents the direct application of these fast geometric techniques, hindering efficient retrieval at scale.</li></ul><p data-block-key=\"fsa1h\">Unfortunately, traditional single-vector MIPS algorithms cannot be directly applied to multi-vector retrieval — for example, a document might have a token with high similarity to a single query token, but overall, the document might not be very relevant. This problem necessitates more complex and computationally intensive retrieval methods.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"0xkr2\">MUVERA: A solution with fixed dimensional encodings</h2><p data-block-key=\"67rm1\">MUVERA offers an elegant solution by reducing multi-vector similarity search to single-vector MIPS to make retrieval over complex multi-vector data much faster. Imagine you have a large dataset of \"multi-vector sets\" (i.e., sets of vectors) where each set describes some datapoint, but searching through each of these sets is slow. MUVERA's trick is to take that whole group of multi-vectors and squeeze them into a single, easier-to-handle vector that we call a <i>fixed dimensional encoding</i> (FDE). A key part is that if you compare these simplified FDEs, their comparison closely matches what you'd get if you compared the original, more complex multi-vector sets. This lets us use much quicker search methods designed for single vectors.</p><p data-block-key=\"919ud\">Here's a simplified breakdown of how MUVERA works:</p><ol><li data-block-key=\"2be5p\"><i>FDE generation</i>: MUVERA employs mappings to convert query and document multi-vector sets into FDEs. These mappings are designed to capture the essential similarity information in a fixed-length vector.</li><li data-block-key=\"2t4u9\"><i>MIPS-based retrieval</i>: The FDEs of documents are indexed using a standard MIPS solver. Given a query, its FDE is computed, and the MIPS solver efficiently retrieves the most similar document FDEs.</li><li data-block-key=\"6taf6\"><i>Re-ranking</i>: The initial candidates retrieved by MIPS are re-ranked using the original Chamfer similarity for improved accuracy.</li></ol><p data-block-key=\"ed9lp\">A key advantage of MUVERA is that the FDE transformation is data-oblivious. This means it doesn't depend on the specific dataset, making it both robust to changes in data distribution and suitable for streaming applications. Additionally, unlike single-vectors produced by a model, FDE’s are guaranteed to approximate the true Chamfer similarity to within a specified error. Thus, after the re-ranking stage, MUVERA is guaranteed to find the most similar multi-vector representations.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n            <div class=\"glue-ambient-video \">\n                <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/MUVERA1_Query.mp4\" type=\"video/mp4\">\n                </video>\n\n                <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n                    <div class=\"glue-ambient-video__tooltip\">\n                    <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                    <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                    </div>\n                    <div class=\"glue-ambient-video__icon\">\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                    </div>\n                </div>\n            </div>\n        \n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"6ie81\"><i>Illustration of the construction of query FDE's. Each token (shown as a word in this example) is mapped to a high-dimensional vector (2-D in the example for simplicity). The high-dimensional space is randomly partitioned by hyperplane cuts. Each piece of space is assigned a block of coordinates in the output FDE, which is set to the sum of the coordinates of the query vectors that land in that piece.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n            <div class=\"glue-ambient-video \">\n                <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/MUVERA2_Document.mp4\" type=\"video/mp4\">\n                </video>\n\n                <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n                    <div class=\"glue-ambient-video__tooltip\">\n                    <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                    <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                    </div>\n                    <div class=\"glue-ambient-video__icon\">\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                    </div>\n                </div>\n            </div>\n        \n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"6ie81\"><i>Illustration of the construction of document FDE's. The construction is the same as the query construction, except that the vectors falling in a given piece of the partitioned space are averaged together instead of summed, which accurately captures the asymmetric nature of the Chamfer similarity.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"0xkr2\">Theoretical foundations</h2><p data-block-key=\"d79eo\">Our approach is inspired by techniques used in <a href=\"https://arxiv.org/abs/2111.03528\" target=\"_blank\" rel=\"noopener noreferrer\">probabilistic tree embeddings</a>, a powerful tool in the theory of geometric algorithms. However, we adapt these techniques to work with inner products and Chamfer similarity.</p><p data-block-key=\"ebt62\">The core idea behind FDE generation is to partition the embedding space into sections (illustrated in the figure above). If similar vectors from a query and a document fall into the same section, we can approximate their similarity efficiently. However, since we don't know the optimal matching between query and document vectors beforehand, we use a randomized partitioning scheme.</p><p data-block-key=\"4jl3a\">We also provide theoretical guarantees for MUVERA, proving that FDEs offer a strong approximation of Chamfer similarity (you can read more in <a href=\"https://arxiv.org/abs/2405.19504\" target=\"_blank\" rel=\"noopener noreferrer\">the paper</a>). This is a significant result, as it provides a principled way to perform multi-vector retrieval using single-vector proxies with provable accuracy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"0xkr2\">Experimental results</h2><p data-block-key=\"9aa8i\">We evaluated MUVERA on several information retrieval datasets from the <a href=\"https://arxiv.org/abs/2104.08663\" target=\"_blank\" rel=\"noopener noreferrer\">BEIR</a> benchmarks. Our experiments demonstrate that MUVERA consistently achieves high retrieval accuracy with significantly reduced latency compared to the previous state-of-the-art method known as <a href=\"https://arxiv.org/pdf/2205.09707\" target=\"_blank\" rel=\"noopener noreferrer\">PLAID</a>.</p><p data-block-key=\"4h9g9\">Our key findings include:</p><p data-block-key=\"ji43\"><i>Improved recall:</i> MUVERA outperforms the single-vector heuristic, a common approach used in multi-vector retrieval (which PLAID also employs), achieving better recall while retrieving significantly fewer candidate documents (shown in the figure below). For instance, FDE’s retrieve 5–20x fewer candidates to achieve a fixed recall.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MUVERA3_Recall.width-1250.png\" alt=\"MUVERA3_Recall\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MUVERA3_Recall.width-1250.png\" alt=\"MUVERA3_Recall\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"6ie81\"><i>Recall of fixed dimensional encodings (FDE) of varying dimensions vs. a single-vector heuristic (SV). Note 10240-dimensional FDE’s have nearly the same representation size as the original MV representation (used in SV heuristic), while requiring significantly fewer comparisons in the search (true even for 20k-dimensional FDE’s).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"0xkr2\"><i>Reduced latency:</i> Compared to PLAID, a highly optimized multi-vector retrieval system based on the single-vector heuristic, MUVERA achieves an average of 10% higher recall with a remarkable 90% reduction in latency across the BEIR datasets (shown in the figure below).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MUVERA5_ResultsFinal.width-1250.png\" alt=\"MUVERA5_ResultsFinal\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MUVERA5_ResultsFinal.width-1250.png\" alt=\"MUVERA5_ResultsFinal\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"6ie81\"><i>MUVERA vs. PLAID over BEIR benchmarks.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"0xkr2\">Moreover, we found that MUVERA's FDEs can be <i>effectively compressed</i> using product quantization, reducing memory footprint by 32x with minimal impact on retrieval quality.</p><p data-block-key=\"avedq\">These results highlight MUVERA's potential to significantly accelerate multi-vector retrieval, making it more practical for real-world applications.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"0xkr2\">Conclusion</h2><p data-block-key=\"f5c08\">We have presented MUVERA, a novel and efficient multi-vector retrieval algorithm with provable guarantees on its approximation quality and good practical performance. By reducing multi-vector search to single-vector MIPS, MUVERA leverages existing optimized search techniques and achieves state-of-the-art performance with significantly improved efficiency. Interested readers can find an open-source implementation of our FDE construction algorithm on <a href=\"https://github.com/google/graph-mining/tree/main/sketching/point_cloud\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a>.</p><p data-block-key=\"dnb18\">Our work opens up new avenues for efficient multi-vector retrieval, which is crucial for various applications, including search engines, recommendation systems, and natural language processing. We believe that further research and optimization of MUVERA will lead to even greater performance gains and broader adoption of multi-vector retrieval techniques.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"0xkr2\">Acknowledgements</h2><p data-block-key=\"6n887\"><i>The work summarized in this blog post was done in collaboration with Majid Hadian, Jason Lee, and Vahab Mirrokni. Lastly, we thank Kimberly Schwede for their valuable help with making the animation in this blog post.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "从研究到气候韧性 (原标题: From research to climate resilience)",
      "link": "https://research.google/blog/how-ai-is-helping-us-build-a-more-resilient-planet/",
      "pubDate": "Mon, 23 Jun 2025 16:00:00 GMT",
      "isoDate": "2025-06-23T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 从研究到气候韧性：谷歌AI助力应对气候危机\n\n谷歌研究部门致力于探索“可能性艺术”，并将突破性研究转化为现实世界的影响。目前，谷歌正利用先进的AI技术和创新，增强气候韧性，应对野火、洪水、极端天气和气旋等紧迫的气候危机，旨在提供及时、可靠的威胁预测，帮助人们保持安全并建设社区韧性。\n\n![AIforthePlanet-0b-Hero](https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-0b-Hero.width-1250.png)\n\n## AI驱动的全球洪水预报\n\n*   **突破性进展**：几年前，准确可靠的洪水预测被认为是“不可能的挑战”。如今，谷歌研究部门开创性的全球水文AI模型（已发表在《自然》杂志）能够提前长达七天准确预测全球河流洪水。\n*   **覆盖范围**：预测结果可在谷歌的Flood Hub平台获取，覆盖全球100多个国家超过7亿人口，赋能政府和当地社区保护生命和生计。\n*   **数据扩展**：通过API和Flood Hub上的专家数据层，为研究人员和专家提供更广泛的覆盖。AI被用于分析历史数据并创建“虚拟水位计”，以弥补数据稀缺地区（覆盖150个国家）的不足。\n*   **全球合作**：与世界气象组织（WMO）以及捷克共和国、尼日利亚、乌拉圭和越南等国的气象部门合作，共同推动洪水预报的全球规模化。\n\n## 提高气旋预报的提前期和准确性\n\n*   **挑战与影响**：气旋（台风或飓风）在过去50年造成了1.4万亿美元的经济损失，提前预警至关重要，但预测其路径、强度和影响非常困难。\n*   **AI应用**：谷歌DeepMind和谷歌研究团队正在探索AI潜力，以改进气旋预测和准备工作。目前可提前15天生成多达50种可能情景，预测气旋的存在、路径、强度、大小和结构。\n*   **Weather Lab**：谷歌推出了交互式网站Weather Lab，分享正在进行的研究，并向专家和公众提供最新、最准确的天气模型。\n*   **合作伙伴**：与美国国家飓风中心（NHC）合作，NHC将在大西洋飓风季节利用这些实验模型，以期改进预报并提供更早、更准确的预警。\n\n![AIforthePlanet-2-Hurricanes](https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-2-Hurricanes.width-1250.png)\n\n## 短期预报（Nowcasting）：提升实时天气信息的可及性\n\n*   **解决痛点**：AI被应用于改进日常天气预报，尤其解决了非洲等传统基础设施有限地区可靠预报稀缺的挑战。\n*   **技术细节**：生成超本地、短期天气预测（即“短期预报”），提供全球降水预测，分辨率为5公里，每15分钟更新一次，可提前12小时。\n*   **核心模型**：这项变革性能力源于谷歌最先进的AI神经天气模型MetNet-3。\n*   **全球推广**：谷歌研究团队利用全球可用的卫星观测数据，通过谷歌搜索直接向非洲用户提供AI驱动的天气预报，弥补了地面雷达等传统基础设施的不足。\n*   **经济效益**：短期预报和其他天气模型可帮助农民应对不断变化的天气条件，潜在提高产量、减少浪费、降低运营成本并增强经济韧性。\n*   **合作研究**：与利兹大学和英国气象局的专家合作，探索模型在各种应用中的潜力，推动气象科学发展。\n\n![AIforthePlanet-3a-Nowcasting](https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-3a-Nowcasting.width-1250.png)\n\n## 利用AI理解和减轻日益增长的野火威胁\n\n*   **实时监测**：多年来，谷歌利用卫星图像和AI近乎实时地检测野火边界，并将关键信息分享给急救人员和受影响社区（已在27个国家/地区的谷歌搜索和地图上提供）。\n*   **FireSat卫星星座**：为根本性改变野火检测方式，谷歌推出了FireSat专用卫星星座。\n    *   首颗FireSat卫星已于3月发射，能检测小至5x5米教室大小的火灾（远超现有技术只能发现足球场大小火灾的能力）。\n    *   由50颗卫星组成的星座将每20分钟提供一次全球高分辨率图像，为消防部门提供快速行动的时间。\n    *   FireSat的全球视角也将帮助科学家更好地研究火灾蔓延。\n*   **多方合作**：该项目是谷歌研究、Google.org与地球火灾联盟、Muon Space、摩尔基金会以及野火管理机构等多方合作的成果。\n\n## 地理空间推理：开创性的地球洞察\n\n*   **新前沿**：除了预测和预报，谷歌正通过“地理空间推理”开创新的前沿，利用AI从地理空间数据中获取地球洞察。\n*   **框架功能**：地理空间推理框架将地球模型与生成式AI结合，加速地理空间问题解决。它允许用户以自然语言提问（例如，在自然灾害前应优先疏散哪些脆弱社区），并获得基于可靠地理空间数据的全面答案和可视化。\n*   **赋能专家**：通过谷歌云平台使复杂的行星分析更易于访问，赋能专家和开发者将谷歌先进的地理空间地球模型与其数据整合，解锁强大洞察。\n*   **行业应用**：自4月推出以来，已引起公共卫生、气候韧性、商业应用（如保险）等广泛领域的兴趣。\n*   **合作伙伴**：与Maxar、Sust-Global、Airbus、Planet Labs和Carto等合作伙伴合作，将潜力转化为实际解决方案。\n\n## 利用AI减少交通排放和改善空气质量\n\n*   **航空业**：AI正在使航空运输更可持续。\n    *   **凝结尾迹**：凝结尾迹占航空气候影响的三分之一。与美国航空等合作伙伴的早期实际演示表明，AI驱动的预测帮助飞行员将凝结尾迹减少了54%。\n    *   **Contrails API**：谷歌正通过Contrails API向航空业提供这些AI预测，并与EUROCONTROL等合作伙伴合作将其整合到飞行计划系统中。\n    *   **非营利组织**：Google.org正提供300万美元，与Breakthrough Energy共同资助一个新的非营利组织contrails.org，专注于将研究转化为航空业的实际工具。\n\n![AIforthePlanet-6-Contrails](https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-6-Contrails.width-1250.png)\n\n*   **地面交通**：与城市合作改善交通流量。\n    *   **Project Green Light**：利用AI和谷歌地图驾驶趋势数据，调整交通信号灯配时，以减少车辆燃油排放并改善空气质量。\n    *   **成效**：该项目已证明可将交叉路口停车次数减少高达30%，交叉路口排放量平均减少10%以上。\n\n## 结论\n\n谷歌研究部门在过去几年取得的突破性研究已对自然灾害防备产生了深远影响。这仅仅是可能性的开始。谷歌坚信，推进AI和科学研究将在提供及时、可靠的全球预测和建设更好的气候韧性方面发挥关键作用。",
      "shortSummary": "谷歌研究正利用AI和技术应对气候危机，构建全球气候韧性。其工作涵盖：通过AI模型实现提前7天的全球洪水预报；利用AI改进气旋路径和强度预测；通过MetNet-3模型提供超本地实时天气信息（短期预报）；利用卫星AI检测和FireSat星座提升野火预警能力；以及通过地理空间推理框架提供地球洞察。此外，AI还应用于减少航空凝结尾迹和优化城市交通信号，以降低排放、改善空气质量。这些努力旨在提供及时可靠的预测，帮助全球社区应对自然灾害。",
      "translated_title": "从研究到气候韧性",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-0b-Hero.width-1250.png",
          "alt": "AIforthePlanet-0b-Hero",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-2-Hurricanes.width-1250.png",
          "alt": "AIforthePlanet-2-Hurricanes",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-3a-Nowcasting.width-1250.png",
          "alt": "AIforthePlanet-3a-Nowcasting",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-6-Contrails.width-1250.png",
          "alt": "AIforthePlanet-6-Contrails",
          "title": "",
          "position": 7
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7b5wd\">At Google Research, we're driven by exploring the art of the possible. Our research impacts products, businesses, scientific discovery, and society. Today, the opportunity to help solve seemingly impossible problems with breakthrough research is greater than ever, as is the opportunity to translate this research into real-world impact.</p><p data-block-key=\"a9ohf\">I'm excited to share how we're advancing research and harnessing tech innovation to help build more resilience, tackling the urgent challenges of climate crises, such as wildfires, floods, extreme weather, and cyclones. Can we deliver timely, reliable predictions of these threats, helping people stay safe and communities build resilience?</p><p data-block-key=\"6ak23\">Our work with AI over the past years, advancing climate science and tackling these difficult problems, is already making a tangible difference. It clearly demonstrates AI’s potential to build towards better climate resilience.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-0b-Hero.width-1250.png\" alt=\"AIforthePlanet-0b-Hero\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-0b-Hero.width-1250.png\" alt=\"AIforthePlanet-0b-Hero\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">From impossible to global scale: AI-powered flood forecasting</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7b5wd\">A few years ago, providing accurate, reliable flood predictions was widely considered an impossible challenge. Today, Google Research’s <a href=\"https://blog.google/technology/ai/google-ai-global-flood-forecasting/\" target=\"_blank\" rel=\"noopener noreferrer\">groundbreaking</a> global hydrological AI model, <a href=\"https://www.nature.com/articles/s41586-024-07145-1\" target=\"_blank\" rel=\"noopener noreferrer\">published in <i>Nature</i></a>, enables us to accurately <a href=\"https://sites.research.google/gr/floodforecasting/\">forecast riverine floods</a> around the world up to seven days in advance. Our forecasts are available on Google’s <a href=\"https://sites.research.google/floods/l/0/0/3\">Flood Hub</a> platform and cover over 700 million people across more than 100 countries worldwide, empowering governments and local communities to protect lives and livelihoods.</p><p data-block-key=\"ftgqe\">We’re also providing researchers and experts with <a href=\"https://blog.google/technology/ai/expanding-flood-forecasting-coverage-helping-partners/\" target=\"_blank\" rel=\"noopener noreferrer\">expanded coverage</a> via an API as well as an expert data layer on Flood Hub. We’ve employed AI to analyze historical data and create “<a href=\"https://research.google/blog/a-flood-forecasting-ai-model-trained-and-evaluated-globally/\">virtual gauges</a>” for locations where data is scarce and physical gauges are not available. This significantly expands our coverage to data-scarce regions across 150 countries, addressing the need, raised by our partners, for greater information in these regions, which are particularly vulnerable. We’re also working with the <a href=\"https://wmo.int/media/news/wmo-embraces-private-sector-and-academia-ai\" target=\"_blank\" rel=\"noopener noreferrer\">WMO</a> and meteorological services in other countries, including the Czech Republic, Nigeria, Uruguay, and Vietnam, on efforts to scale flood forecasting globally, further enhancing community resilience worldwide.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"iZmltPAFhY0\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=iZmltPAFhY0\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Improving the lead time and accuracy of cyclone forecasts</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7b5wd\">Cyclones — also known as typhoons or hurricanes — devastate communities and endanger lives. In the last 50 years, they have caused <a href=\"https://wmo.int/topics/tropical-cyclone\" target=\"_blank\" rel=\"noopener noreferrer\">1.4 trillion dollars in economic losses</a>. Advanced warning could make a tremendous difference, but it is difficult to predict whether oceanic storms will become dangerous cyclones, where they might make landfall, and what the impact will be. For decades, scientists have relied on supercomputers to simulate the laws of physics to estimate the future path, intensity and size of these storms. Now Google DeepMind and Google Research teams are exploring AI’s potential to improve how we <a href=\"https://deepmind.google/discover/blog/weather-lab-cyclone-predictions-with-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">predict</a> and prepare for cyclones. We can currently predict existence, track, intensity, size, and structure, generating up to 50 possible scenarios as far as 15 days in advance. It’s still early days and we're working with experts around the world, including partners in academia, government agencies, and non-profit organizations, to refine and expand the impact of our work.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-2-Hurricanes.width-1250.png\" alt=\"AIforthePlanet-2-Hurricanes\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-2-Hurricanes.width-1250.png\" alt=\"AIforthePlanet-2-Hurricanes\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7b5wd\">Earlier this month we announced <a href=\"https://deepmind.google.com/science/weatherlab\" target=\"_blank\" rel=\"noopener noreferrer\">Weather Lab</a>, an interactive website where we are sharing ongoing research, making our newest and most accurate weather models available to experts and the public. We also announced a <a href=\"https://wmo.int/media/news/wmo-embraces-private-sector-and-academia-ai\" target=\"_blank\" rel=\"noopener noreferrer\">partnership</a> with the US <a href=\"https://www.nhc.noaa.gov/\" target=\"_blank\" rel=\"noopener noreferrer\">National Hurricane Center</a>, who will be leveraging these experimental models this summer throughout the Atlantic Hurricane Season. We hope this data can help improve NHC forecasts and provide earlier and more accurate warnings for hazards linked to tropical cyclones.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Nowcasting: Improving access to reliable, real-time weather information</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7b5wd\">We’re applying AI to improve forecasts for everyday weather, too. This addresses an acute challenge in regions with limited traditional infrastructure, including across Africa, where reliable forecasting is often scarce. We’re generating hyper-local, short-term weather predictions — known as <a href=\"https://blog.google/intl/en-africa/products/explore-get-answers/nowcasting-on-search-is-bringing-ai-powered-weather-forecasts-to-users-across-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">nowcasting</a> — and can now make available these global precipitation predictions with a 5km resolution, updated every 15 minutes, up to 12 hours ahead.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-3a-Nowcasting.width-1250.png\" alt=\"AIforthePlanet-3a-Nowcasting\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-3a-Nowcasting.width-1250.png\" alt=\"AIforthePlanet-3a-Nowcasting\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7vhvz\">This transformative capability stems from <a href=\"https://research.google/blog/metnet-3-a-state-of-the-art-neural-weather-model-available-in-google-products/\">MetNet-3</a>, our state-of-the-art AI neural weather model. To scale nowcasting globally, Google Research teams are leveraging globally available satellite observations in our models, allowing us to bring the power of AI-driven weather forecasts directly to people across Africa via Google Search. This innovative approach overcomes the critical gap where traditional weather forecasting infrastructure, like ground-based radar, is scarce. For users across Africa, this translates to more reliable, real-time weather information that improves their daily lives. The agricultural community tells us that nowcasting and other weather models can help farmers react to changing conditions, potentially improving yields, reducing waste, lowering operating costs, and enhancing economic resilience.</p><p data-block-key=\"t7d0\">We are collaborating with leading experts, including meteorologists at the University of Leeds and the <a href=\"https://www.metoffice.gov.uk/\" target=\"_blank\" rel=\"noopener noreferrer\">UK Met office</a>, exploring how they can use our models for various applications. Such partnerships with local scientific communities are crucial not only to continue to refine the accuracy of our forecasts, but also to advance the science of meteorology and improve weather predictions for everyone, everywhere.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Leveraging AI to understand and mitigate the growing threat of wildfires</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7b5wd\"><a href=\"https://sites.research.google/gr/wildfires/\">Wildfires</a> are increasing in frequency and intensity, putting lives, homes, and ecosystems at risk. For years we have been leveraging satellite imagery and AI to detect the boundaries of wildfires in near real time. We share this vital information with first responders and affected communities, enabling quicker and more effective responses. <a href=\"https://blog.google/outreach-initiatives/sustainability/google-wildfire-boundary-maps-europe-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">Wildfire boundary</a> information is available on Google Search and Maps in 27 countries.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"pmQlXLaHT_Y\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=pmQlXLaHT_Y\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7b5wd\">During this ongoing work, we recognized the need for more accurate information on earlier, smaller fires that can rapidly escalate. We set out to fundamentally change how we detect wildfires with a dedicated satellite constellation, <a href=\"https://sites.research.google/gr/wildfires/firesat/\">FireSat</a>. In March, the <a href=\"https://blog.google/feed/firesat-first-satellite-launch/\" target=\"_blank\" rel=\"noopener noreferrer\">first FireSat satellite</a> was launched, representing a major leap forward in our ability to mitigate the threat of wildfires worldwide. FireSat consists of satellites that can detect and track wildfires as small as a 5x5 meter classroom, a significant improvement over current technology that can only spot fires the size of a football field. A constellation of 50 satellites will provide high-resolution imagery updated globally every 20 minutes, giving fire authorities time for quick action. Additionally, FireSat’s unprecedented global view of fires will allow scientists to better study fire propagation. This leap is the result of a partnership between Google Research and Google.org with the <a href=\"https://www.earthfirealliance.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Earth Fire Alliance</a>, <a href=\"https://www.muonspace.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Muon Space</a>, the <a href=\"https://www.moore.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Moore Foundation</a>, wildfire authorities, and many others.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Geospatial Reasoning: Pioneering planetary insights</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7b5wd\">Beyond prediction and forecasting, we’re pioneering new frontiers by enabling insights about our planet with <a href=\"https://research.google/blog/geospatial-reasoning-unlocking-insights-with-generative-ai-and-multiple-foundation-models/\">Geospatial Reasoning</a>. Our long-standing investment in applying AI to geospatial data is evident in our extensive portfolio of work — from AI models that power weather forecasts and help predict floods and wildfires to remote sensing and analyzing population dynamics. The Geospatial Reasoning framework brings Earth models together with generative AI to accelerate geospatial problem solving. It enables people to ask questions in natural language about, for example, which vulnerable communities should be evacuated first before a natural disaster and to receive comprehensive answers and visualizations that are grounded in robust geospatial data. This can help provide critical insights to governments, local agencies and businesses, creating new opportunities for building community resilience.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"g9F-_tCakL8\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=g9F-_tCakL8\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7b5wd\">We’re removing cost and expertise barriers by making sophisticated planetary analysis on <a href=\"https://cloud.google.com/gcp\" target=\"_blank\" rel=\"noopener noreferrer\">Google Cloud Platform</a> more accessible, and empowering experts and developers to integrate Google’s advanced geospatial Earth models with their data to unlock powerful insights. Since the launch of Geospatial Reasoning in April, we've seen interest from a wide range of sectors eager to harness its potential, from public health to climate resilience to commercial applications, such as insurance. We are now collaborating with partners like <a href=\"https://www.maxar.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Maxar</a>, <a href=\"https://www.sustglobal.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Sust-Global</a>, <a href=\"https://www.airbus.com/en\" target=\"_blank\" rel=\"noopener noreferrer\">Airbus</a>, <a href=\"https://www.planet.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Planet Labs</a> and <a href=\"https://carto.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Carto</a> to translate the potential into tangible solutions.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Using AI to reduce transport-related emissions and improve air quality</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7b5wd\">AI is paving the way for making air and ground transportation more sustainable, with reduced carbon emission and improved air quality in our cities. One third of aviation’s climate impact are the result of <a href=\"https://sites.research.google/contrails/\">contrails</a>, the condensation trails sometimes seen behind airplanes. In early real-world demonstrations with partners like American Airlines, our AI-powered forecasts helped pilots reduce contrails by 54%. We’re now making these AI-powered forecasts available across the aviation industry through our <a href=\"https://developers.google.com/contrails\" target=\"_blank\" rel=\"noopener noreferrer\">Contrails API</a> and working with partners like <a href=\"https://www.eurocontrol.int/\" target=\"_blank\" rel=\"noopener noreferrer\">EUROCONTROL</a> to integrate the forecasts into their flight planning systems. <a href=\"http://google.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Google.org</a> is providing 3 million dollars to co-catalyze a new nonprofit with <a href=\"https://www.breakthroughenergy.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Breakthrough Energy</a>, called <a href=\"http://contrails.org/\" target=\"_blank\" rel=\"noopener noreferrer\">contrails.org</a>, which will focus on translating research into tangible tools for aviation.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-6-Contrails.width-1250.png\" alt=\"AIforthePlanet-6-Contrails\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AIforthePlanet-6-Contrails.width-1250.png\" alt=\"AIforthePlanet-6-Contrails\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7b5wd\">For ground transportation, we’re teaming up with cities to improve traffic flow. <a href=\"https://sites.research.google/gr/greenlight/\">Project Green Light</a> uses AI and <a href=\"https://blog.google/outreach-initiatives/sustainability/google-ai-project-greenlight/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Maps driving trends</a> to propose adjustment to traffic light timing in order to reduce vehicle gas emissions, also resulting in improved air quality. Green Light has <a href=\"https://blog.google/outreach-initiatives/sustainability/project-green-light-boston-expansion/\" target=\"_blank\" rel=\"noopener noreferrer\">demonstrated</a> potential to reduce stops at intersections by up to 30% and reduce emissions at intersections by an average of over 10%.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7b5wd\">Our breakthrough research over the past years is already having a profound impact on natural disaster preparedness. Yet, this is just the beginning of what’s possible. We are confident that advancing AI and scientific research can play a key role in addressing the difficult problems of timely, reliable global predictions, working towards better climate resilience. I am incredibly optimistic that collectively we’ll ultimately get closer to a point where no one is surprised by a natural disaster coming their way.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "通过多模态AI与M-REGLE解锁丰富的遗传学见解 (原标题: Unlocking rich genetic insights through multimodal AI with M-REGLE)",
      "link": "https://research.google/blog/unlocking-rich-genetic-insights-through-multimodal-ai-with-m-regle/",
      "pubDate": "Sun, 22 Jun 2025 16:00:00 GMT",
      "isoDate": "2025-06-22T16:00:00.000Z",
      "creator": "Google",
      "summary": "# M-REGLE：通过多模态AI解锁丰富的遗传学见解\n\n## 引言：多模态健康数据的重要性\n\n当前，从尖端医疗技术到智能手表，各种设备正以前所未有的规模生成数据。电子健康记录、医学影像、诊断测试、基因组数据乃至智能手表的实时测量数据汇聚成海量信息，供研究人员和临床医生分析。这些多样化的数据流，即使在同一器官系统内，也常携带着独特且重叠的信号。例如，在心血管系统中，心电图（ECG）测量心脏的电活动，而光电容积描记图（PPG，智能手表常见）则追踪血容量变化。对这些模态进行协同分析，可以同时评估心脏的电系统及其泵血效率，从而提供更全面的心脏健康图景。将这些生理特征与来自大型国家级生物样本库的遗传信息相结合，有助于识别疾病的遗传基础。\n\n## 背景：从单模态到多模态的需求\n\n我们早期的工作REGLE在利用健康数据进行遗传发现方面取得了成功，但它被设计用于单一数据类型（即单模态设置）。另一种方法是分别分析每种模态，然后尝试将结果拼凑起来（我们称之为U-REGLE或单模态REGLE），但这可能不是最有效的方式。U-REGLE可能会错过不同模态之间微妙的共享信息。因此，我们假设联合建模这些互补数据流将增强重要的生物信号，减少噪声，并带来更强大的遗传发现。\n\n## M-REGLE的提出与优势\n\n我们近期在《美国人类遗传学杂志》上发表了论文“利用多模态AI改进心血管性状的遗传分析”，其中介绍了我们开发的多模态版REGLE，名为M-REGLE。M-REGLE允许同时分析多种类型的临床数据。与前身U-REGLE相比，M-REGLE产生更低的重建误差，识别出更多的遗传关联，并在预测心脏疾病方面优于风险评分。\n\n## M-REGLE工作原理\n\n### 核心理念\n\nM-REGLE的核心前提是，不同的临床模态，特别是那些与单一器官系统（如循环系统）相关的模态，编码着互补和重叠的信息。例如，在12导联心电图中，不同的导联放置在身体的不同位置。为了确定心脏病发作的位置或诊断心律失常，医生会分析特定导联的信息。M-REGLE的方法是在表征学习过程之前结合多种模态（如12导联ECG或一个导联加上PPG数据），提供了一种更准确的工具，在发现遗传关联、分析复杂生理数据和预测疾病方面表现更优。\n\n### 多步骤方法：联合学习\n\n为了有效地实现这一点，M-REGLE采用了一种稳健的多步骤联合学习方法：\n\n1.  **数据融合**：M-REGLE首先将多种模态（如12个不同的ECG导联或ECG与PPG波形）结合起来，而不是单独查看它们。\n2.  **特征学习**：它随后使用卷积变分自编码器（CVAE）从这些多数据流中学习一个压缩的、组合的“签名”（潜在因子）。CVAE旨在以较低维度、大部分不相关的表示形式捕获最基本的信息。它由编码器和解码器网络组成，编码器将ECG和PPG波形压缩为潜在因子，解码器网络则从创建的潜在因子重建波形。\n3.  **独立性确保**：为了确保学习到的因子真正独立，对这些CVAE生成的签名应用主成分分析（PCA）。\n4.  **遗传关联**：最后，通过全基因组关联研究（GWAS）找到计算出的独立因子与遗传数据之间的关联（显著相关性）。\n5.  **结果整合**：对这些个体GWAS的结果进行统计学组合，以精确定位与潜在生理系统相关的基因变异。\n\n![MREGLE-1-Overview](https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-1-Overview.width-1250.png)\n\n## M-REGLE的性能提升\n\n### 更好的学习表示\n\nM-REGLE在U-REGLE的基础上实现了进步，能够持续生成更好的数据“学习表示”。医学数据（如ECG）包含数百个独立数据点。在分析多种医学模态时，M-REGLE不是单独处理每种模态，而是捕获最重要的特征并将其浓缩为“潜在因子”。这种方法显著降低了重建误差，并且比单独从每种模态学习更好地捕获了原始波形的基本信息。对于12导联ECG，M-REGLE将重建误差降低了72.5%。\n\n### 可解释性洞察\n\n生成式AI的优势之一是其可解释性能力。在我们的研究中，我们使用M-REGLE嵌入来展示这些嵌入与ECG和PPG波形之间的联系，特别是改变单个嵌入坐标如何改变M-REGLE解码器重建的ECG和PPG波形。\n\n我们专注于识别能够最好地区分房颤（AFib）样本和非房颤样本的坐标。M-REGLE在位置4、6和10的嵌入被发现最具区分性。当我们改变第4个M-REGLE嵌入的值（从[-2, 2]）同时保持其余M-REGLE嵌入固定时，我们观察到重建的ECG导联I和PPG发生相应变化：ECG导联I的T波段幅度发生变化，PPG信号的双波切迹出现微小改变。双波切迹提供有关心血管功能和健康的宝贵信息。例如，不那么突出或缺失的双波切迹通常与动脉僵硬度增加相关。\n\n![MREGLE-2-EmbeddingEffect](https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-2-EmbeddingEffect.width-800.gif)\n\n### 增强的遗传学发现\n\nM-REGLE在识别心血管疾病的遗传关联方面也比U-REGLE有所改进。对于12导联ECG，M-REGLE比单模态方法多识别了19.3%的关联遗传位点（基因组区域）。对于ECG导联I + PPG，M-REGLE多发现了13.0%的位点。重要的是，这些发现的绝大多数（12导联ECG的35个中有24个，ECG导联I + PPG的12个中有11个）复制了GWAS目录中报告的ECG或PPG性状的已知遗传关联。M-REGLE还发现了几个以前未与这些性状关联的新位点，其中一些在其他数据库中显示出与心血管性状的联系。\n\n![MREGLE-3-Discoveries](https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-3-Discoveries.width-1250.png)\n\n### 改进的多基因风险评分\n\n多基因风险评分（PRS）量化个体患某种疾病的遗传风险。我们发现，使用M-REGLE（来自12导联ECG数据）识别的遗传变异开发的PRS在预测心脏疾病（最显著的是房颤AFib）方面显著优于U-REGLE。M-REGLE的PRS在识别高风险个体方面表现显著更好。这些AFib的PRS改进不仅在英国生物样本库中观察到，还在其他大型数据集（如印第安纳生物样本库、EPIC-Norfolk和英国女性心脏健康研究）中得到了独立验证。\n\n![MREGLE-4-Comparison](https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-4-Comparison.width-1250.png)\n\n## M-REGLE为何有效？\n\nM-REGLE的强大之处在于其处理信息的方式。通过从一开始就考虑多种模态，M-REGLE获得了三大优势：\n\n1.  **高效捕获共享信息**：它只需学习一次共享信息，而不是在每种模态中重复学习。\n2.  **增强独特和互补信号**：它能提升每种模态提供的独特和互补信号。\n3.  **减少噪声**：一种模态的信息可能有助于澄清或过滤掉另一种模态中的噪声。\n\n所有这些都导致了更清晰、更稳健的信号，从而实现强大的下游遗传分析。\n\n## 未来展望\n\n这项研究是利用日益丰富的多模态健康数据向前迈出的一步。M-REGLE提供了一种发现复杂疾病新遗传联系、提高疾病风险预测能力以及潜在识别新治疗靶点的方法。此外，随着智能可穿戴设备持续收集ECG和PPG等生理数据，M-REGLE等方法对于将健康数据转化为洞察力并最终改善健康结果至关重要。",
      "shortSummary": "M-REGLE是一种新型多模态AI模型，用于遗传分析。它通过联合分析心电图（ECG）和光电容积描记图（PPG）等多种临床数据，克服了传统单模态方法的局限性。M-REGLE能更有效地捕获共享信息、增强独特信号并减少噪声，从而生成更好的数据表示、识别更多与心血管疾病相关的遗传位点，并显著提高房颤等疾病的多基因风险评分预测准确性。这项研究为利用丰富的健康数据解锁遗传学见解、改善疾病预测和开发新疗法铺平了道路。",
      "translated_title": "通过多模态AI与M-REGLE解锁丰富的遗传学见解",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-1-Overview.width-1250.png",
          "alt": "MREGLE-1-Overview",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-2-EmbeddingEffect.width-800.gif",
          "alt": "MREGLE-2-EmbeddingEffect",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-3-Discoveries.width-1250.png",
          "alt": "MREGLE-3-Discoveries",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-4-Comparison.width-1250.png",
          "alt": "MREGLE-4-Comparison",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"8t3jr\">Everything from medical specialists with cutting-edge technology to simple smartwatches are generating data on an unprecedented scale. The aggregation of electronic health records, medical imaging, diagnostic tests, genomic data, and even real-time measurements from smartwatches creates a wealth of data for researchers and clinicians to analyze. These diverse data streams often carry unique and overlapping signals, even within the same organ system.</p><p data-block-key=\"13r16\">In the cardiovascular system, for example, an <a href=\"https://en.wikipedia.org/wiki/Electrocardiography\" target=\"_blank\" rel=\"noopener noreferrer\">electrocardiogram</a> (ECG) measures the heart's electrical activity, while a <a href=\"https://en.wikipedia.org/wiki/Photoplethysmogram\" target=\"_blank\" rel=\"noopener noreferrer\">photoplethysmogram</a> (PPG) — common in <a href=\"https://blog.google/products/fitbit/irregular-heart-rhythm-notifications/\" target=\"_blank\" rel=\"noopener noreferrer\">smartwatches</a> — tracks blood volume changes. The co-analysis of these modalities can simultaneously assess both the heart’s electrical system and its pumping efficiency, thus providing a more complete picture of heart health. Integrating these physiological signatures with genetic information from large nation-level biobanks could enable the identification of the genetic underpinnings of disease.</p><p data-block-key=\"5e6f4\">Our earlier work, <a href=\"https://research.google/blog/harnessing-hidden-genetic-information-in-clinical-data-with-regle/\">REGLE</a>, was successful for genetic discovery using health data, but it was designed for a single data type (i.e., the unimodal setting). Alternatively, analyzing each modality separately and then trying to piece together the findings later (what we refer to as <a href=\"https://www.nature.com/articles/s41588-024-01831-6\" target=\"_blank\" rel=\"noopener noreferrer\">U-REGLE</a> or Unimodal REGLE) also might not be the most efficient way. U-REGLE could miss subtle shared information between different modalities. Instead, we hypothesized that <i>jointly</i> modeling these complementary data streams would boost the important biological signals, reduce noise, and lead to more powerful genetic discoveries.</p><p data-block-key=\"1ggvs\">Here we present our recent paper, “<a href=\"https://www.cell.com/ajhg/fulltext/S0002-9297(25)00231-9\" target=\"_blank\" rel=\"noopener noreferrer\">Utilizing multimodal AI to improve genetic analyses of cardiovascular traits</a>”, which we published in the <a href=\"https://www.cell.com/ajhg/home\" target=\"_blank\" rel=\"noopener noreferrer\"><i>American Journal of Human Genetics</i></a>. We developed a multimodal version of REGLE, called M-REGLE, that allows the analysis of multiple types of clinical data together at once. M-REGLE produces lower reconstruction error, identifies more genetic associations, and outperforms risk scores in predicting cardiac disease compared to its predecessor, U-REGLE.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-1-Overview.width-1250.png\" alt=\"MREGLE-1-Overview\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-1-Overview.width-1250.png\" alt=\"MREGLE-1-Overview\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"nconb\"><i>M-REGLE overview steps compared to running our previous model REGLE on each modality separately (U-REGLE, or Unimodal REGLE).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The challenge: Seeing the whole picture</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"8t3jr\">The central premise of M-REGLE is that different clinical modalities, especially those pertaining to a single organ system (like the circulatory system), encode both complementary and overlapping information. In a 12-lead ECG, for example, the different leads are placed in distinct locations on the body. To determine the location of a heart attack, or diagnose arrhythmias, physicians analyze information from specific leads. M-REGLE’s approach, which combines multiple modalities (like the 12 leads of the ECG or one lead plus PPG data) before the representation learning process, offers a more accurate tool that is superior at finding genetic associations, analyzing complex physiological data, and predicting disease.</p><p data-block-key=\"bcdfm\">To effectively do this, M-REGLE employs a robust, multi-step approach that uses joint learning. Instead of looking at 12 different ECG leads or an ECG and a PPG waveform separately, M-REGLE first combines them. It then uses a <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\" target=\"_blank\" rel=\"noopener noreferrer\">convolutional</a> <a href=\"https://en.wikipedia.org/wiki/Variational_autoencoder\" target=\"_blank\" rel=\"noopener noreferrer\">variational autoencoder</a> (CVAE) to learn a compressed, combined \"signature\" (latent factors) from these multiple data streams. The CVAE is designed to capture the most essential information in a lower-dimensional, largely uncorrelated representation. It consists of encoder and decoder networks where the encoder compresses the ECG and PPG waveforms to latent factors and the decoder network reconstructs the waveforms from the created latent factors. To ensure the learned factors are truly independent, <a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\" target=\"_blank\" rel=\"noopener noreferrer\">principal component analysis</a> (PCA) is applied to these CVAE-generated signatures. Finally, we find associations (significant correlation) between computed independent factors and genetic data via <a href=\"https://en.wikipedia.org/wiki/Genome-wide_association_study\" target=\"_blank\" rel=\"noopener noreferrer\">genome-wide association studies</a> (GWAS). The results from these individual GWAS are statistically combined to pinpoint genetic variations associated with the underlying physiological system.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Better learned representations</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"8t3jr\">M-REGLE advances U-REGLE to consistently produce better \"learned representations\" of the data. Medical data, like an ECG, consists of hundreds of individual data points. When analyzing multiple medical modalities, instead of processing the modalities individually, M-REGLE captures the most important characteristics and condenses it into “latent factors.” This approach resulted in significantly lower reconstruction errors and did a better job of capturing the essential information from the original waveforms compared to learning from each modality separately. For 12-lead ECGs, M-REGLE reduced reconstruction error by 72.5%.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Interpretability sheds some light on embeddings</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"8t3jr\">One of the advantages of generative AI is its interpretability capability. In our study, we used M-REGLE embeddings to show the connection between these embeddings and ECG and PPG waveforms, specifically how altering individual embedding coordinates changes the reconstructed ECG and PPG waveforms from the M-REGLE decoder.</p><p data-block-key=\"aaj2k\">We focused on identifying coordinates that would best distinguish between samples with and without atrial fibrillation (AFib). M-REGLE embeddings at position 4, 6, and 10 were found to be the most distinctive. When we changed values in the 4th M-REGLE embedding from [-2, 2] while keeping the rest of M-REGLE embeddings fixed, we observed corresponding changes in the reconstructed ECG lead I and PPG: the <a href=\"https://en.wikipedia.org/wiki/T_wave\" target=\"_blank\" rel=\"noopener noreferrer\">T-wave</a> segment of ECG lead I changed in magnitude, and the dicrotic notch of the PPG signal showed a small alteration. The dicrotic notch provides valuable information about cardiovascular function and health. For example, a less prominent or absent dicrotic notch is often associated with increased arterial stiffness.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-2-EmbeddingEffect.width-800.gif\" alt=\"MREGLE-2-EmbeddingEffect\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-2-EmbeddingEffect.width-800.gif\" alt=\"MREGLE-2-EmbeddingEffect\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"nconb\"><i>The effect of varying the 4th M-REGLE embedding on the reconstructed ECG Lead I and PPG, which leads to a reduction in the magnitude of the T-wave segment of ECG lead I (</i><b><i>left</i></b><i>) and a change in prominence of the dichroic notch in the PPG (</i><b><i>right</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Enhanced genetic discovery</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"8t3jr\">M-REGLE also made improvements over U-REGLE in the identification of genetic associations with cardiovascular disease. For 12-lead ECGs, M-REGLE identified 19.3% more associated genetic loci (regions in the genome) than the unimodal approach. For ECG lead I + PPG, M-REGLE found 13.0% more loci. Importantly, a vast majority of these findings (24/35 for 12 lead ECG and 11/12 for ECG lead I + PPG) replicated known genetic associations for ECG or PPG traits as reported in the <a href=\"https://en.wikipedia.org/wiki/GWAS_catalog\" target=\"_blank\" rel=\"noopener noreferrer\">GWAS catalog</a>. M-REGLE also uncovered several new loci not previously associated with these traits, some of which showed links to cardiovascular traits in other databases.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-3-Discoveries.width-1250.png\" alt=\"MREGLE-3-Discoveries\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-3-Discoveries.width-1250.png\" alt=\"MREGLE-3-Discoveries\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"nconb\"><i>A 3-way Venn diagram of the GWAS catalog loci, loci discovered by M-REGLE (12-lead ECG) and loci discovered by U-REGLE. GWAS catalog indicates previously discovered loci.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Improved polygenic risk scores</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"8t3jr\">A <a href=\"https://en.wikipedia.org/wiki/Polygenic_score\" target=\"_blank\" rel=\"noopener noreferrer\">polygenic risk score</a> (PRS) quantifies an individual's genetic risk for a disease. We found that PRS developed using genetic variants identified by M-REGLE (from the 12-lead ECG data) significantly outperformed those from U-REGLE in predicting cardiac disease, most notably in atrial fibrillation (AFib). M-REGLE's PRS was significantly better at identifying individuals at risk. These PRS improvements for AFib were not just seen in the <a href=\"https://www.ukbiobank.ac.uk/\" target=\"_blank\" rel=\"noopener noreferrer\">UK Biobank</a>, but also independently validated in other large datasets, like the <a href=\"https://indianabiobank.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Indiana Biobank</a>, <a href=\"https://www.epic-norfolk.org.uk/\" target=\"_blank\" rel=\"noopener noreferrer\">EPIC-Norfolk</a>, and the <a href=\"https://www.ucl.ac.uk/british-womens-heart-health-study/\" target=\"_blank\" rel=\"noopener noreferrer\">British Women's Heart and Health Study</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-4-Comparison.width-1250.png\" alt=\"MREGLE-4-Comparison\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/MREGLE-4-Comparison.width-1250.png\" alt=\"MREGLE-4-Comparison\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"nconb\"><i>Comparison of M-REGLE PRSs for atrial fibrillation (AFib). Prevalence,</i> <a href=\"https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\" target=\"_blank\" rel=\"noopener noreferrer\"><i>AUC-ROC</i></a><i>, and</i> <a href=\"https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\" target=\"_blank\" rel=\"noopener noreferrer\"><i>AUC-PR</i></a><i> are computed (* indicates a statistically significant difference).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Why does M-REGLE work?</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"8t3jr\">The power of M-REGLE lies in how it handles information. By considering multiple modalities at the outset, M-REGLE gains three main advantages. First, it efficiently captures shared information, learning it once instead of multiple times across each modality. Second, it boosts the unique and complementary signals that each modality provides. Third, M-REGLE reduces noise, as information from one modality might help clarify or filter out noise in another. This all leads to a clearer, more robust signal for powerful downstream genetic analysis.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">The future is multimodal</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"8t3jr\">This research is a step forward in leveraging the rich, multimodal health data becoming increasingly available. M-REGLE offers a way to uncover new genetic links to complex disease, improve our ability to predict disease risk, and potentially identify new targets for therapies. In addition, with the rise of smart wearables continuously collecting physiological data like ECG and PPG, methods like M-REGLE will be crucial for translating health data into insights and ultimately, better health outcomes.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"8t3jr\"><i>This work represents a collaborative achievement by many contributors and institutions. We sincerely thank our collaborators for their essential input: Yuchen Zhou, Justin Cosentino, Howard Yang, Andrew Carroll, Cory Y. McLean, Babak Behsaz (Google); Zachary R. McCaw (University of North Carolina); Tae-Hwi Schwantes-An, Dongbing Lai (Indiana University); Mahantesh I. Biradar, Robert Luben, Jorgen Engmann, Rui Providencia, Anthony P. Khawaja (University College London); Patricia B Munroe (Queen Mary University of London). Our thanks also go to Anastasiya Belyaeva for reviewing the manuscript, Greg Corrado, Shravya Shetty, and Michael Brenner for their support, and Monique Brouillette for her help in writing this blog post.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "量子计算的多彩未来 (原标题: A colorful quantum future)",
      "link": "https://research.google/blog/a-colorful-quantum-future/",
      "pubDate": "Sun, 22 Jun 2025 16:00:00 GMT",
      "isoDate": "2025-06-22T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 量子计算中的错误纠正：从物理比特到逻辑比特\n\n量子计算的未来发展离不开错误纠正技术。在大规模量子计算机中，错误纠正通过将多个易受噪声影响的“物理比特”（超导电路）组合成一个整体，形成一个对噪声更鲁棒的“逻辑比特”，从而存储量子信息。此前，研究人员已成功实现了表面码量子错误纠正实验，其性能已达到系统扩展所需的阈值，原则上意味着通过增加物理比特数量即可构建近乎完美的逻辑比特。\n\n## 错误纠正的必要性与挑战\n\n在量子计算的进一步发展中，面临两大核心问题：\n1.  如何最小化每个逻辑比特所需的物理比特数量？\n2.  如何最大化逻辑操作和量子算法的速度？\n\n解决这些问题的一种方法是提高物理比特的质量（即降低物理错误率），这能减少所需的编码距离，从而减少每个逻辑比特所需的物理比特，并加速逻辑操作。另一种方法是提高错误纠正代码本身的效率。\n\n## 颜色码：一种高效的替代方案\n\n今天，研究人员兴奋地宣布实验演示了一种“颜色码”系统，它为广受研究的表面码提供了一个有前景的替代方案。在最新发表于《自然》杂志的论文《超导量子处理器上颜色码的扩展与逻辑》中，研究团队实现了基于颜色码的容错量子计算机所需的基本构建模块。\n\n与表面码类似，颜色码也是一种利用多个物理比特编码单个逻辑比特，从而实现错误检测和纠正的方法。然而，颜色码采用了一种不同的奇偶校验测量几何模式（即六边形瓦片的三角形补丁）。\n\n### 几何结构与物理比特效率\n\n在评估错误纠正码时，“距离”是一个重要指标，它表示导致一个逻辑错误的物理错误数量。表面码的逻辑比特通常以正方形布局，其边长等于代码距离。而颜色码的几何结构不同，它需要将六边形平铺成一个三角形，其边长也等于代码距离。这意味着在相同代码距离下，颜色码所需的面积远小于表面码，从而暗示需要更少的物理比特。\n\n![ColorCode1_Comparison](https://storage.googleapis.com/gweb-research2023-media/images/ColorCode1_Comparison.width-1250.png)\n比较三角形颜色码与正方形表面码。颜色码的几何结构具有优势，因为在相同的代码距离下，它使用的物理比特更少。\n\n### 性能权衡与实验进展\n\n尽管颜色码在物理比特效率上具有优势，但也存在权衡。实现错误纠正电路和解码输出在颜色码中更为困难（例如，因为匹配算法在该代码上的准确性不如表面码）。实际上，这意味着颜色码更难跨越错误纠正阈值（即错误纠正工作所需的性能水平）。\n\n然而，最新的研究表明，结合Willow芯片和最近的解码进展，颜色码已能实现低于阈值的性能。具体而言，研究结果比较了距离为3和5的颜色码，显示在较高距离下，逻辑错误率降低了1.56倍。虽然这一初始结果小于之前使用表面码实现的2.31倍，但研究人员预计，随着规模的扩大和设备改进，颜色码的几何优势可能会开始显现。\n\n![ColorCode2_Graph](https://storage.googleapis.com/gweb-research2023-media/images/ColorCode2_Graph.width-1250.png)\n显示颜色码中错误累积速率的数据。在此图中，我们看到距离为5的颜色码表现优于距离为3的颜色码（错误累积速度更慢）。这表明更高距离的颜色码将表现更好，原则上意味着通过简单地增加更多量子比特，我们可以获得更高的逻辑性能。\n\n## 颜色码的优势\n\n### 更快的单比特逻辑操作\n\n颜色码的真正优势在于逻辑操作。与表面码相比，颜色码中的许多单比特操作变得更容易实现。在颜色码中，许多逻辑操作可以在一个步骤中完成，而表面码需要多个错误纠正周期才能完成相同的操作。例如，逻辑哈达玛门在颜色码中预计只需约20纳秒，而在同一设备上的表面码中可能需要长达1000倍的时间。这使得逻辑门运行更快，最终加速量子算法的执行。此外，由于算法所需的错误纠正周期大大减少，每个周期可以容忍更多的错误，从而减少所需的物理比特。\n\n研究人员利用这一特性，演示了执行多个单比特逻辑操作，并通过“逻辑随机基准测试”验证了结果。\n\n### 高效的“魔术态”生成\n\n量子计算的一个关键组成部分是生成一种称为“魔术态”（或T态）的特殊状态。魔术态至关重要，因为它们是生成关键的任意量子比特旋转所必需的。没有这些任意量子比特旋转，量子算法将无法超越经典方法。长期以来，这种状态的生成被认为是实际量子算法中最昂贵的部分。然而，最近的理论进展表明，利用颜色码，可以构建一个高效的电路，称为“培养”（cultivation），来生成这些魔术态。这对量子算法的未来发展来说是个好消息！\n\n这进一步强调了在设备上实现颜色码的重要性。在实验中，研究人员演示了“培养”协议的第一步，即以99%的保真度将一个“不完美”的魔术态注入到颜色码逻辑比特中。\n\n### 灵活的双比特操作\n\n运行量子算法所需的另一个关键操作是双比特门。这些操作通常通过将两个独立的逻辑比特合并成一个补丁，然后将它们分开来完成。在实验中，研究人员成功地纠缠了两个逻辑比特，并以86%到91%的保真度将信息从一个逻辑比特传输到另一个逻辑比特，从而演示了这种技术。值得注意的是，颜色码在这方面也略显灵活，因为它允许合并操作在3个不同的基（X、Y和Z）上进行，而表面码只有两个（X和Z）。\n\n![ColorCode3_Diagram](https://storage.googleapis.com/gweb-research2023-media/images/ColorCode3_Diagram.width-1250.png)\n图示两个逻辑比特的合并与分离，以将逻辑状态（由粗黑线表示）从一个补丁传输到另一个补丁。\n\n## 未来展望\n\n经过多年对表面码的专注研究，这些结果表明，存在可行的替代方案，可以在超导量子处理器上高效地实现低于其错误纠正阈值的性能。尽管表面码仍然是主要的工作配置，但现在看来，颜色码也可能成为大规模量子计算机的一部分。首先，它是“培养”协议的关键组成部分，该协议应能更有效地注入“魔术态”。其次，随着量子硬件的发展，无论从空间（物理比特）还是时间（错误纠正周期数）角度来看，颜色码甚至可能比表面码更高效。",
      "shortSummary": "量子计算的未来依赖于错误纠正。文章介绍了“颜色码”，一种比传统“表面码”更高效的错误纠正方案。颜色码采用独特的几何结构，能用更少物理比特实现相同代码距离，并显著加速逻辑操作（如哈达玛门）。它还能高效生成关键的“魔术态”，并提供更灵活的双比特操作。尽管实现挑战较大，但实验已证明其可行性。颜色码有望成为未来大规模量子计算机的重要组成部分，提升整体性能。",
      "translated_title": "量子计算的多彩未来",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ColorCode1_Comparison.width-1250.png",
          "alt": "ColorCode1_Comparison",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ColorCode2_Graph.width-1250.png",
          "alt": "ColorCode2_Graph",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/ColorCode3_Diagram.width-1250.png",
          "alt": "ColorCode3_Diagram",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"vdgen\">Error correction is a key component of tomorrow’s large scale quantum computers. Using error correction, we can combine many “physical qubits” (in our devices, these are small superconducting circuits that can store quantum information, but are sensitive to noise) into an ensemble that works together to store a single “logical qubit” that is more robust to noise. Just a few months ago, we <a href=\"https://research.google/blog/making-quantum-error-correction-work/\">announced</a> that we had implemented a <a href=\"https://research.google/blog/suppressing-quantum-errors-by-scaling-a-surface-code-logical-qubit/\">surface code</a> quantum error correction experiment that exceeded the performance threshold required to get benefits from scaling up the system. This means that, in principle, we can now make a near-perfect logical qubit by “simply” adding more and more physical qubits.</p><p data-block-key=\"43c0e\">Thinking ahead to the next steps in this journey raises the following questions:</p><ol><li data-block-key=\"f1gh4\">How can we minimize the number of physical qubits per logical qubit?</li><li data-block-key=\"3flkf\">How can we maximize the speed of logical operations and quantum algorithms?</li></ol><p data-block-key=\"2h781\">One way to make progress on both these questions, exemplified by the release of our <a href=\"https://quantum-y1-update-dot-googwebreview.appspot.com/en/site-assets/downloads/willow-spec-sheet.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Willow</a> chip, is to improve the quality of the physical qubits (i.e., lower physical error rates). This reduces the required code distance (i.e. the minimum number of simultaneous physical errors required to produce a logical error) leading to fewer physical qubits per logical qubit and faster logical operations, which generally scale with the code distance. However, another way is to make the error correction code more <i>efficient</i>.</p><p data-block-key=\"1dboa\">Today, we are excited to report the experimental demonstration of a “color code” system that provides an advantageous alternative to the well studied <a href=\"https://research.google/blog/suppressing-quantum-errors-by-scaling-a-surface-code-logical-qubit/\">surface code</a>. In our latest <a href=\"https://www.nature.com/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Nature</i></a> publication, “<a href=\"https://www.nature.com/articles/s41586-025-09061-4\" target=\"_blank\" rel=\"noopener noreferrer\">Scaling and logic in the color code on a superconducting quantum processor</a>”, we implement the building blocks required for a resource efficient, fault-tolerant quantum computer based on the color code. Just like the surface code, the color code is a way to encode each logical qubit using many physical qubits in such a way that errors can be detected and corrected as they occur. However, the color code uses a different geometrical pattern of parity measurement (i.e., a triangular patch of hexagonal tiles) which requires fewer physical qubits and boasts more efficient logical gates than the surface code, but this comes at the expense of requiring deeper physical circuits and a different decoding algorithm.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"vdgen\">Triangles are smaller than squares, but at what cost?</h2><p data-block-key=\"engl\">When evaluating an error correction code, a useful metric is the “distance” of the code, which tells us the number of physical errors that result in one logical error. In the surface code, we need to lay out our logical qubit in a square where edge length is equal to the distance of the code. The color code geometry is different, however, since it requires a hexagonal tiling into a triangular shape with edge length equal to the distance of the code. This means that the area of the color code is much less than that of a surface code of the same distance, which suggests that we need fewer physical qubits.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ColorCode1_Comparison.width-1250.png\" alt=\"ColorCode1_Comparison\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ColorCode1_Comparison.width-1250.png\" alt=\"ColorCode1_Comparison\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"2hm1s\"><i>Comparing the triangular color code to the square surface code. The geometry of the color code is advantageous because it uses fewer physical qubits for the same code distance.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"vdgen\">But there is a tradeoff! It turns out that implementing the error correction circuit and decoding the output is more difficult in the color code (e.g., because matching algorithms are not as accurate on this code). In practice, this means that it is harder to cross the error correction threshold (i.e., the level of performance required for error correction to work). Nevertheless, our recent publication shows that our Willow chip, in combination with recent <a href=\"https://blog.google/technology/google-deepmind/alphaqubit-quantum-error-correction/\" target=\"_blank\" rel=\"noopener noreferrer\">decoding</a> <a href=\"https://arxiv.org/abs/2503.10988\" target=\"_blank\" rel=\"noopener noreferrer\">advances</a>, is now capable of below-threshold performance using the color code.</p><p data-block-key=\"ap3jq\">Specifically, our result compares a color code with a distance of 3 and 5, and shows that the logical error rate is suppressed by a factor of 1.56× at the higher distance. While this initial result is smaller than the factor of 2.31× we’ve previously <a href=\"https://research.google/blog/making-quantum-error-correction-work/\">achieved</a> using surface codes, we expect that the geometrical advantage may start to win out at larger scale and with additional device improvements.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ColorCode2_Graph.width-1250.png\" alt=\"ColorCode2_Graph\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ColorCode2_Graph.width-1250.png\" alt=\"ColorCode2_Graph\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"2hm1s\"><i>Data showing the rate of error accumulation in the color code. In this plot, we see that the distance-5 color code outperforms the distance-3 color code (by accumulating errors more slowly). Showing that the higher distance color code would perform better, means that we can, in principle, obtain much higher logical performance by simply adding more qubits.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"vdgen\">Faster single qubit logical operations</h2><p data-block-key=\"3c1qe\">Where the color code really starts to shine is when we take a look at logical operations. Here some single qubit operations become easier than in the surface code. In the color code, many logical operations can be implemented in a single step, while the surface code requires many cycles of error correction to do the same thing. For example, the logical <a href=\"https://en.wikipedia.org/wiki/Quantum_logic_gate\" target=\"_blank\" rel=\"noopener noreferrer\">Hadamard</a> gate is expected to take ~20ns in the color code, whereas it can take 1000× longer to execute on a surface code in the same device. This makes it much faster to run logical gates and ultimately faster to run quantum algorithms. Moreover, since an algorithm would take many fewer error correction cycles, we can afford more error per cycle and hence fewer physical qubits.</p><p data-block-key=\"1sjd\">Using this feature, we demonstrated that we could perform many single qubit logical operations and verified the results using “logical randomized benchmarking” (more in <a href=\"https://www.nature.com/articles/s41586-025-09061-4\" target=\"_blank\" rel=\"noopener noreferrer\">the paper</a>).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"vdgen\">Injecting arbitrary states</h2><p data-block-key=\"816du\">A key ingredient of quantum computing is the ability to generate a special state called the “magic state” (or <a href=\"https://arxiv.org/abs/2409.17595\" target=\"_blank\" rel=\"noopener noreferrer\">T-state</a>). “Magic states” are important because they are required to generate the critically important arbitrary qubit rotation. Without these arbitrary qubit rotations quantum algorithms would not outperform classical methods. The generation of this state was long presumed to be the most expensive part of practical quantum algorithms. However, <a href=\"https://arxiv.org/pdf/2409.17595\" target=\"_blank\" rel=\"noopener noreferrer\">recent theoretical advances</a> have shown that, using the color code, one can construct an efficient circuit, called “cultivation”, to generate these magic states. This is great news for the future of quantum algorithms!</p><p data-block-key=\"21e5q\">This reinforces the importance of implementing color codes on our devices. In our experiment, we demonstrated the first step of the “cultivation” protocol, which is to inject an “imperfect” magic state into a color code logical qubit with 99% fidelity.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"vdgen\">Stitching patches to implement 2-qubit operations</h2><p data-block-key=\"3ga3t\">Another critical operation needed to run quantum algorithms are 2-qubit gates. These operations are generally done by merging two separate logical qubits into one single patch and then pulling them apart. In our experiments, we were able to demonstrate this technique by successfully entangling two logical qubits and transferring information from one logical qubit to the other with a fidelity of 86% to 91%. It is interesting to note that the color code is also slightly more flexible in that respect since it allows the merge operation to act on 3 different bases (X, Y, and Z), rather than the two available in the surface code (X and Z).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/ColorCode3_Diagram.width-1250.png\" alt=\"ColorCode3_Diagram\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/ColorCode3_Diagram.width-1250.png\" alt=\"ColorCode3_Diagram\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"2hm1s\"><i>Diagram showing the merging and separation of two logical qubits to transfer the logical state (represented by the bolded black line) from one patch to the other.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"vdgen\">What’s next</h2><p data-block-key=\"16gbm\">After years of focus on the surface code, these results show that there are viable alternatives that can efficiently be implemented below their error correction threshold on our superconducting quantum processors. While the surface code is still our main workhorse configuration, it now seems likely that the color code will also be part of our large scale quantum computer. First, it is a critical part of the “cultivation” protocol which should allow us to inject “magic states” more efficiently. Second, as our quantum hardware evolves, the color code may even become more efficient than the surface code, both from a space (physical qubits) and time (number of error correction cycles) perspective.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "优化基于LLM的旅行规划 (原标题: Optimizing LLM-based trip planning)",
      "link": "https://research.google/blog/optimizing-llm-based-trip-planning/",
      "pubDate": "Thu, 05 Jun 2025 16:00:00 GMT",
      "isoDate": "2025-06-05T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 优化基于LLM的旅行规划\n\n## 引言\n\n现实世界的规划任务通常涉及“硬性”的量化约束（如预算、日程安排、开放时间）和“软性”的定性目标（如自然语言表达的用户偏好）。大型语言模型（LLM）因其庞大的训练数据集而擅长处理旅行规划中非量化部分，例如判断餐厅是否适合儿童或观赏风景的最佳时间。然而，LLM在处理需要详细和最新现实世界信息（如公交票价、火车时刻表）或复杂交互要求的量化物流约束方面可靠性较低，这可能导致生成不切实际的计划，例如建议参观一个已关闭的博物馆。\n\n## 解决方案：搜索中的AI旅行建议\n\n为了克服LLM在处理量化约束方面的不足，我们最近在“搜索”中推出了“AI旅行建议”功能，旨在提供实用且可行的逐日行程。我们的解决方案采用了一种混合系统：它首先利用LLM建议一个初始计划，然后结合一个优化算法，该算法在确保与LLM计划相似性的同时，也考虑旅行时间、开放时间等现实世界因素。这种方法有效地将LLM处理软性需求的能力与满足硬性物流约束所需的算法精度相结合。\n\n## 工作原理\n\n### 1. LLM初始计划生成\n\n系统首先将用户查询传递给LLM（我们使用的是最新Gemini模型的一个版本）。LLM会根据用户兴趣生成一个初始旅行计划，其中包含活动列表、建议持续时间及重要性等详细信息。尽管此初始计划能很好地满足用户兴趣，但可能存在可行性问题，例如建议访问一个已关闭的场所。\n\n![混合LLM和优化系统示意图](https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs2_Diagram.width-1250.png)\n\n*图：我们的混合LLM和优化系统示意图。LLM建议初始计划，然后我们进行优化，结合现实世界约束，生成最终行程。*\n\n### 2. 可行性增强与优化\n\n为了解决初始计划的可行性问题，我们在LLM之上添加了几个组件：\n\n*   **数据校准：** 我们首先根据最新的开放时间和旅行时间对初始行程进行校准。\n*   **替代活动检索：** 同时，我们还使用搜索后端检索额外的相关活动，作为LLM建议计划可能需要修改时的潜在替代方案。\n*   **优化输入：** 初始计划、替代活动和校准数据随后被输入到优化算法中，以找到一个与初始计划相似且确保可行性的旅行计划。\n\n## 优化算法\n\n该算法主要分为两个阶段：\n\n### 阶段1：单日优化\n\n*   对于旅行中单日内的每个活动子集（最多合理的最大尺寸），算法会确定这些活动在一天内的最佳调度。\n*   然后，根据与原始计划的相似性以及受开放时间和旅行时间约束的调度可行性来分配一个质量得分（完全不可行的调度将获得零分）。\n*   由于一天内的活动数量较少，通过优化的动态规划实现，可以通过穷举搜索计算得分。\n\n### 阶段2：整体行程优化（多日）\n\n*   在第二阶段，算法寻找一个整体行程（即每天的活动集合），在满足两天活动不重叠的约束下，最大化每天的总得分。\n*   这是一个加权集合打包问题，虽然是NP完全问题，但鉴于优化目标旨在保持与初始行程的接近，局部搜索启发式方法被证明是有效的。\n*   从初始行程开始，算法通过在两天之间交换活动来进行局部调整，只要这能增加总得分。此过程重复进行，直到收敛，从而产生最终行程。\n\n![行程优化示意图](https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs3_GIF.width-800.gif)\n\n*图：通过局部调整优化行程的示意图。初始行程包括在科学博物馆关闭后参观。算法将其重新安排到第一天，并移至更早的时段。这使得有时间将第三天的上午徒步旅行重新安排到第二天，第二天有地理位置更近的活动，从而形成最终行程。*\n\n## 案例演示\n\n### 案例1：处理定性偏好（纽约博物馆）\n\n以下查询说明了LLM提供的详细理解如何比传统检索系统更好地满足用户需求：\n\n**查询：** “为我规划一个纽约周末之旅，参观许多鲜为人知的博物馆，并避开大量人群。”\n\n*   **混合系统：** 我们的系统生成了一个符合用户请求的行程，建议了电影与电视博物馆和纽约交通博物馆等鲜为人知的博物馆。\n*   **仅依赖搜索：** 如果移除LLM建议，仅依赖搜索检索到的活动，生成的行程会包含一些鲜为人知的博物馆，但也包括大都会艺术博物馆和古根海姆博物馆，这些著名博物馆直接与用户的请求相矛盾。\n\n![混合系统与仅搜索系统生成的行程对比](https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs4_ItineraryFinal.width-1250.png)\n\n*图：混合系统（左）生成的行程完全包含鲜为人知的博物馆，符合用户请求。仅依赖搜索检索到的活动，一些著名博物馆会进入旅行计划（右）。*\n\n### 案例2：纠正量化约束问题（旧金山旅行）\n\n以下查询演示了混合系统纠正原始LLM建议行程中物流问题的情况：\n\n**查询：** “为我规划一次旧金山之旅。我想参观艺术博物馆，并去一些可以俯瞰城市全景的地方。”\n\n*   **LLM初始计划：** 在测试中，LLM建议了不错的景点（如笛洋美术馆和科伊特塔），但其中一天的活动安排不自然，要求用户跨城市长途旅行。\n*   **优化纠正：** 优化步骤成功纠正了此问题，生成了一个物流上可行的计划，同时保留了原始意图。\n\n![LLM建议与优化后行程的地图对比](https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs5_Maps.width-1250.png)\n\n*图：LLM建议的行程（左）在其中一天包含跨城市的长时间旅行。应用优化算法后，我们获得了更自然的活动分组（右）。*\n\n## 未来展望\n\n除了旅行规划，LLM还可以应用于组织活动或安排差事等其他日常任务。对于这些应用，开发能够让LLM可靠地应对现实世界约束的系统至关重要。我们的旅行规划工作是解决这一挑战的更大持续努力的一部分。\n\n## 致谢\n\n本博客文章基于与Wei Chen、Lucas Guzman、Shirley Loayza Sanchez和Weiye Yao合作完成的工作。我们还要感谢Sreenivas Gollapudi、Kostas Kollias和Gokul Varadhan提供的有益指导。",
      "shortSummary": "文章介绍了优化基于LLM的旅行规划的混合系统。该系统结合了LLM处理用户偏好等定性需求的能力，以及优化算法处理开放时间、旅行时间等量化约束的精度。LLM生成初始计划，随后通过算法进行校准和优化，以确保行程的实用性和可行性。案例表明，该方法能更好地满足用户特定偏好并纠正物流上的不合理安排，为现实世界规划任务提供了更可靠的解决方案。",
      "translated_title": "优化基于LLM的旅行规划",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs2_Diagram.width-1250.png",
          "alt": "AlgoLLMs2_Diagram",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs3_GIF.width-800.gif",
          "alt": "AlgoLLMs3_GIF",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs4_ItineraryFinal.width-1250.png",
          "alt": "AlgoLLMs4_ItineraryFinal",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs5_Maps.width-1250.png",
          "alt": "AlgoLLMs5_Maps",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fhg4u\">Many real-world planning tasks involve both harder “quantitative” constraints (e.g., budgets or scheduling requirements) and softer “qualitative” objectives (e.g., user preferences expressed in natural language). Consider someone planning a week-long vacation. Typically, this planning would be subject to various clearly quantifiable constraints, such as budget, travel logistics, and visiting attractions only when they are open, in addition to a number of constraints based on personal interests and preferences that aren’t easily quantifiable.</p><p data-block-key=\"18c09\">Large language models (LLMs) are trained on massive datasets and have internalized an impressive amount of world knowledge, often including an understanding of typical human preferences. As such, they are generally good at taking into account the not-so-quantifiable parts of trip planning, such as the ideal time to visit a scenic view or whether a restaurant is kid-friendly. However, they are less reliable at handling quantitative logistical constraints, which may require detailed and up-to-date real-world information (e.g., bus fares, train schedules, etc.) or complex interacting requirements (e.g., minimizing travel across multiple days). As a result, LLM-generated plans can at times include impractical elements, such as visiting a museum that would be closed by the time you can travel there.</p><p data-block-key=\"f91cn\">We recently introduced <a href=\"https://blog.google/products/search/summer-travel-tips-ai-overviews-hotel-price-tracking/\" target=\"_blank\" rel=\"noopener noreferrer\">AI trip ideas in Search</a>, a feature that suggests day-by-day itineraries in response to trip-planning queries. In this blog, we describe some of the work that went into overcoming one of the key challenges in launching this feature: ensuring the produced itineraries are practical and feasible. Our solution employs a hybrid system that uses an LLM to suggest an initial plan combined with an algorithm that jointly optimizes for similarity to the LLM plan and real-world factors, such as travel time and opening hours. This approach integrates the LLM’s ability to handle soft requirements with the algorithmic precision needed to meet hard logistical constraints.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"fhg4u\">How it works</h2><p data-block-key=\"1d4pi\">Given an incoming user query, we first pass the query to an LLM, which for our system is a version of our latest <a href=\"https://deepmind.google/models/gemini/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini models</a>. The LLM suggests an initial trip plan consisting of a list of activities along with relevant details, such as the suggested duration and level of importance to the user query. The initial plan is well-tailored to the user’s interests but may have feasibility issues, like suggesting an establishment that has recently closed.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs2_Diagram.width-1250.png\" alt=\"AlgoLLMs2_Diagram\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs2_Diagram.width-1250.png\" alt=\"AlgoLLMs2_Diagram\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"gqk8g\"><i>Diagram of our hybrid LLM and optimization system. The LLM suggests the initial plan, and we then perform optimization incorporating real-world constraints to produce the final itinerary.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fhg4u\">To address the feasibility issues, we add a few components on top of the LLM. We start by grounding the initial itinerary with up-to-date opening hours and travel times. In parallel, we also use search backends to retrieve additional relevant activities that serve as potential substitutes in case the LLM-suggested plan needs to be modified. The initial plan, substitute activities, and grounding data are then fed into an optimization algorithm to find a trip plan similar to the initial plan that also ensures feasibility.</p><p data-block-key=\"3b4fm\">There are two main stages to the algorithm. The first stage operates on the level of a single day within the trip. For each subset of activities (up to a reasonable maximum size), we determine an optimal scheduling of those activities in a day. We then assign it a quality score determined primarily based on similarity to the original plan and feasibility of the scheduling subject to opening hour and travel time constraints (e.g., a completely infeasible schedule would receive a score of zero). Since the number of activities within a day is small, we found that the scores can be computed by exhaustive search with a sufficiently optimized dynamic programming-based implementation.</p><p data-block-key=\"8tafc\">In the second stage, we search for an overall itinerary (i.e., sets of activities for each day) that maximizes the total score of the days, subject to the constraint that no two days' activities can overlap. This is a weighted variant of the <a href=\"https://en.wikipedia.org/wiki/Set_packing\" target=\"_blank\" rel=\"noopener noreferrer\">set packing problem</a>, which is well-known to be <a href=\"https://en.wikipedia.org/wiki/NP-completeness\" target=\"_blank\" rel=\"noopener noreferrer\">NP-complete</a> and thus computationally intractable. However, given that our optimization objective tries to stay close to the initial itinerary by design, we found that local search heuristics were effective. Starting from the initial itinerary, we make local adjustments by exchanging activities between pairs of days so long as this would increase the total score. This procedure is repeated until convergence, resulting in the final itinerary.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs3_GIF.width-800.gif\" alt=\"AlgoLLMs3_GIF\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs3_GIF.width-800.gif\" alt=\"AlgoLLMs3_GIF\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"gqk8g\"><i>Illustration of optimizing an itinerary with local adjustments. The initial itinerary includes a visit to the science museum after it closes. The algorithm reschedules it to day 1, where it gets moved to an earlier slot. This leaves time to reschedule the morning hike from day 3 to day 2, which has geographically closer activities, resulting in the final itinerary.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"fhg4u\">Examples</h2><p data-block-key=\"fnll7\">The following query illustrates how the detailed understanding LLMs provide can better serve the user’s needs than a traditional retrieval system.</p><p data-block-key=\"c83v5\"><i>Query: “Plan me a weekend trip to NYC visiting lots of lesser known museums and avoiding large crowds.”</i></p><p data-block-key=\"9pab9\">Our system produces an itinerary matching the user’s request, suggesting museums such as the Museum of the Moving Image and the New York Transit Museum. If we remove the LLM suggestion and rely only on the search-retrieved activities, the resulting itinerary includes some lesser known museums but also the Metropolitan Museum of Art and the Guggenheim Museum, famous museums directly contradicting the user’s request.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs4_ItineraryFinal.width-1250.png\" alt=\"AlgoLLMs4_ItineraryFinal\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs4_ItineraryFinal.width-1250.png\" alt=\"AlgoLLMs4_ItineraryFinal\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"gqk8g\"><i>The itinerary produced by the hybrid system (</i><b><i>left</i></b><i>) includes exclusively lesser known museums, per the user’s request. Relying solely on Search-retrieved activities, some of the famous museums make it into the trip plan (</i><b><i>right</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"fhg4u\">On the other hand, the next query demonstrates a case where the hybrid system corrects an issue with the original LLM-suggested itinerary:</p><p data-block-key=\"50f3l\"><i>Query: “Plan me a trip to San Francisco. I want to visit art museums and go somewhere with panoramic views of the city.”</i></p><p data-block-key=\"7i4ja\">During our testing, the LLM suggested a number of good attractions, such as the de Young museum (which also includes an observation tower) and the iconic Coit Tower. However, the itinerary scheduled the activities for one of the days in an unnatural way, requiring the user to travel across the city. We were able to correct this in the optimization step, resulting in a logistically feasible plan that preserves the original intent.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs5_Maps.width-1250.png\" alt=\"AlgoLLMs5_Maps\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/AlgoLLMs5_Maps.width-1250.png\" alt=\"AlgoLLMs5_Maps\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"gqk8g\"><i>The LLM-suggested itinerary (</i><b><i>left</i></b><i>) includes a long travel across the city on one of the days. After applying an optimization algorithm, we obtain a more natural grouping of activities (</i><b><i>right</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"fhg4u\">Future work</h2><p data-block-key=\"14u23\">Beyond trip planning, LLMs can be applied to many other everyday tasks, such as organizing an event or scheduling errands. For these types of applications, it is crucial to develop systems that allow LLMs to reliably navigate the constraints of the real world. Our trip planning work is part of a larger ongoing effort to address this challenge — stay tuned for further updates in this space.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"fhg4u\">Acknowledgements</h2><p data-block-key=\"2ug7e\"><i>This blog post is based on work done in collaboration with Wei Chen, Lucas Guzman, Shirley Loayza Sanchez, and Weiye Yao. We also thank Sreenivas Gollapudi, Kostas Kollias, and Gokul Varadhan for helpful guidance.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "放大：利用生成式人工智能进行高效区域环境风险评估 (原标题: Zooming in: Efficient regional environmental risk assessment with generative AI)",
      "link": "https://research.google/blog/zooming-in-efficient-regional-environmental-risk-assessment-with-generative-ai/",
      "pubDate": "Wed, 04 Jun 2025 16:00:00 GMT",
      "isoDate": "2025-06-04T16:00:00.000Z",
      "creator": "Google",
      "summary": "地球系统模型是预测和应对地球未来环境变化的最佳工具，但其高分辨率运行的巨大计算成本限制了其在精细尺度（如城市级别，约10公里）进行区域预测的能力。传统的地球系统模型通常只能达到夏威夷岛屿大小（约100公里）的分辨率。\n\n为了弥补这一分辨率差距，研究人员提出了一种新颖的生成式人工智能方法，称为**动态-生成式降尺度（dynamical-generative downscaling）**，并已发表在《美国国家科学院院刊》上。该方法利用概率扩散模型（一种强大的生成式AI）将全球气候预测转化为局部（约10公里）的当前和未来环境风险评估，且成本远低于现有技术。\n\n### 传统降尺度方法的局限性\n\n*   **动态降尺度：** 通过区域气候模型（RCMs）对全球模型数据进行更精细的模拟。它能提供最物理真实的局部预测，但计算成本极高，不适合处理大量气候预测数据。\n*   **统计降尺度：** 速度更快，但难以准确捕捉复杂的局部天气模式（尤其是极端事件），且难以可靠地推广到未训练的未来条件。\n\n### 动态-生成式降尺度方法\n\n该方法结合了动态降尺度的物理真实性和人工智能的速度与模式识别能力，分两步进行：\n\n1.  **基于物理的第一步：** 区域气候模型将全球地球系统数据降尺度到中间分辨率（例如50公里）。这一步计算成本较低，但关键在于它将不同全球模型的输出转换为统一的网格和物理表示，为AI系统的高效学习奠定基础。\n2.  **AI添加精细细节：** 新开发的生成式AI系统，即**区域残差扩散降尺度模型（R2D2）**，接管任务。R2D2通过学习高分辨率天气数据的示例，学会向中间分辨率输出添加逼真的精细尺度细节（如复杂地形的影响），从而高效地将其提升到目标高分辨率（通常小于10公里）。该模型专注于学习中间分辨率和高分辨率场之间的“残差”差异，这使得学习任务更容易，并提高了对未见环境条件的泛化能力。\n\n![DynGenDown-1-Cartoon](https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-1-Cartoon.width-1250.png)\n\n*降尺度方法提高了地球有限区域气候预测的分辨率。这使我们能够理解地形和区域过程对极端天气的影响。* \n\n![DynGenDown-2-Example](https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-2-Example.width-1250.png)\n\n*动态-生成式降尺度通过两步过程从地球系统模型中提取区域气候信息。区域气候模型（RCM）提供基于物理的第一步，生成式AI添加更精细的细节。*\n\n### 评估与结果\n\n研究团队使用“美国西部动态降尺度数据集（WUS-D3）”对模型进行了训练和评估。结果显示：\n\n*   **更高的准确性：** 相比统计方法，动态-生成式降尺度将温度、降水、相对湿度和风速等各种天气变量的精细尺度误差降低了40%以上。它还能有效纠正粗分辨率气候模拟中的系统偏差。\n*   **逼真的天气模式：** AI生成的结果捕捉了逼真的空间模式、不同天气变量之间的相关性（如风速和风向，或热量和湿度），以及复合极端事件（如同时发生的热浪和干旱）的可能性。\n*   **更好的不确定性估计：** 通过降尺度大量气候预测集合，该框架提供了比统计方法或仅动态降尺度少量地球系统模型更全面的未来环境条件范围图景。\n*   **捕捉区域极端事件：** 该方法在捕捉由区域现象引起的复杂环境风险方面表现出卓越的技能，例如南加州圣安娜风引起的野火风险。\n\n![DynGenDown-3a-Performance](https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-3a-Performance.width-1250.jpg)\n\n*我们的模型（R2D2）与统计降尺度基线（BCSD、STAR-ESDM）在风速（左）、相对湿度（中）和温度（右）集合预测中的精细尺度预测误差（a-c）和能量谱（d-f）比较。新方法显示出更低的误差（CRPS）和更逼真的天气模式，通过R2D2和目标能量谱的对齐来衡量。底部面板比较了夏季炎热干燥极端事件（g-i）和秋季风极端事件（j-l）的协变。*\n\n![DynGenDown-4a-Wildfire](https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-4a-Wildfire.width-1250.jpg)\n\n*在21世纪末气候预测的环境条件下，南加州强圣安娜风事件期间的野火风险模型预测。野火风险通过圣安娜野火威胁指数的天气分量来衡量。动态-生成式方法（a）与更昂贵的动态降尺度方法（b）相比，能很好地捕捉野火风险。统计降尺度方法如STAR-ESDM（c）和BCSD（d）通常难以捕捉此类复合风险的幅度和空间结构。*\n\n### 突破的意义\n\n动态-生成式降尺度是获得低于10公里可操作尺度的全面未来区域气候预测的重大一步。它使得对大型地球系统模型集合进行降尺度在计算上变得可行——研究估计，对于测试的8模型集合，计算成本节省了85%，对于更大的集合，这一数字还会增加。这种快速高效的AI推理步骤类似于谷歌的SEEDS和GenCast天气预报模型的运行方式，能够对区域环境风险进行全面评估。\n\n通过以一小部分计算成本提供更准确、更具概率完整性的区域气候预测，动态-生成式降尺度可以显著改善环境风险评估。这有助于在农业、水资源管理、能源基础设施和自然灾害防备等关键领域做出更明智的适应和弹性政策决策。",
      "shortSummary": "一项新颖的生成式AI方法——“动态-生成式降尺度”——解决了地球系统模型在精细尺度进行区域环境风险预测的计算成本高昂问题。该方法结合了物理模型和AI（R2D2），首先通过区域气候模型进行粗略降尺度，再由AI添加精细细节。与传统方法相比，它能以85%的成本节省实现更高的预测准确性、更逼真的天气模式、更好的不确定性估计，并有效捕捉区域极端事件（如野火风险）。这项突破将显著改善环境风险评估，支持农业、水资源管理和灾害防备等领域的决策。",
      "translated_title": "放大：利用生成式人工智能进行高效区域环境风险评估",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-1-Cartoon.width-1250.png",
          "alt": "DynGenDown-1-Cartoon",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-2-Example.width-1250.png",
          "alt": "DynGenDown-2-Example",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-3a-Performance.width-1250.jpg",
          "alt": "DynGenDown-3a-Performance",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-4a-Wildfire.width-1250.jpg",
          "alt": "DynGenDown-4a-Wildfire",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7espt\"><a href=\"https://www.nobelprize.org/prizes/physics/2021/popular-information/\" target=\"_blank\" rel=\"noopener noreferrer\">Earth system models</a> represent our best tool to predict and prepare for future changes to Earth’s environment. However, the immense computational cost of running these models at high resolution limits their ability to make regional projections at fine scales. Indeed, a typical limiting scale for these models is comparable in size to the island of Hawai’i (~100 km). Obtaining more granular projections, for instance at the city level (~10 km), is critical for planning everything from farming strategies and water management to protecting communities from floods, heatwaves, and wildfires.</p><p data-block-key=\"b8dk0\">To address this need, we are excited to announce a novel generative AI method that bridges the resolution gap between Earth system models and downstream users’ needs. <a href=\"https://doi.org/10.1073/pnas.2420288122\" target=\"_blank\" rel=\"noopener noreferrer\">Published</a> in the <a href=\"https://www.pnas.org/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Proceedings of the National Academy of Sciences</i></a>, we present a dynamical-generative downscaling method that applies probabilistic diffusion models — a powerful class of generative AI capable of learning complex data distributions — to the output of well-established physics-based models to translate global climate projections into local (~10 km) assessments of present and future environmental risk. Dynamical-generative downscaling produces detailed local environmental risk assessments at a small fraction of the cost of existing state-of-the-art techniques, which are too computationally expensive to apply to the wealth of climate projection data that is <a href=\"https://wcrp-cmip.org/\" target=\"_blank\" rel=\"noopener noreferrer\">now available</a>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">From global projections to regional environmental risk</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7espt\">To capture local changes in environmental conditions at resolutions of 10 km or higher, scientists typically use a technique called \"dynamical downscaling\". This involves taking coarse information from global Earth system models and running much finer-grained simulations with regional climate models (RCMs) over a specific area. Think of it like using a magnifying glass on a global map.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-1-Cartoon.width-1250.png\" alt=\"DynGenDown-1-Cartoon\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-1-Cartoon.width-1250.png\" alt=\"DynGenDown-1-Cartoon\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"n3zhp\"><i>Downscaling methods increase the resolution of climate projections over limited-area regions of the Earth. This enables us to understand the effect of topography and regional processes on weather extremes. Rendering generated by</i> <a href=\"https://deepmind.google/technologies/imagen-3/\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Imagen 3</i></a><i>.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7espt\">While dynamical downscaling provides the most physically realistic local projections, it has a major drawback: it is computationally expensive. Running these detailed simulations takes substantial computing power, making it impractical to downscale the many different climate projections needed to fully capture the <a href=\"https://www.ipcc.ch/report/ar6/wg1/chapter/chapter-4/\" target=\"_blank\" rel=\"noopener noreferrer\">range</a> of possible future environmental conditions. Faster statistical downscaling methods exist, but they often struggle to accurately capture complex local weather patterns (especially extreme events) or to generalize reliably to future conditions for which they were not trained.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">A more efficient approach: Physics meets generative AI</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7espt\">Instead, we propose to combine the physical realism of dynamical downscaling with the speed and pattern-recognition power of artificial intelligence. The dynamical-generative downscaling process works in two steps:</p><ol><li data-block-key=\"d3jm0\"><i>Physics-based first pass:</i> First, a regional climate model downscales global Earth system data, but only to an intermediate, and still coarse, resolution (e.g., 50 km). This step is much cheaper computationally than going straight to very high resolution, but crucially, it translates the varied outputs of different global models into a common grid and physical representation of the Earth. This process sets the stage for efficient learning by AI systems.<br><br></li><li data-block-key=\"587l3\"><i>AI adds the fine details:</i> Next, a newly developed generative AI system, the Regional Residual Diffusion-based Downscaling model (“R2D2”), takes over. Trained on examples of high-resolution weather data, R2D2 learns to add realistic, fine-scale details (like the effects of complex terrain) to the intermediate-resolution output, efficiently bringing it up to the target high resolution (typically less than 10 km). The model focuses on learning the difference, or \"residual\", between the intermediate and high-resolution fields, which makes the learning task easier and improves generalization to unseen environmental conditions.</li></ol>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-2-Example.width-1250.png\" alt=\"DynGenDown-2-Example\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-2-Example.width-1250.png\" alt=\"DynGenDown-2-Example\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"n3zhp\"><i>Dynamical-generative downscaling extracts regional climate information from Earth system models in a two-step process. A regional climate model (RCM) provides a physics-based first pass, and generative AI adds the finer details.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"7espt\">This hybrid approach leverages the strengths of both methods: the RCM provides a physically grounded base and handles the diversity of global models, while the AI excels at efficiently generating the high-resolution details and capturing the full range of regional environmental conditions. Importantly, the R2D2 model only needs training data from one dynamically downscaled Earth system model to learn how to effectively downscale outputs originating from different Earth system models. This enables our model to amortize the training cost when applied to large ensembles of climate projections.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Efficient and reliable regional climate projections</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7espt\">We trained and evaluated our model using the Western United States Dynamically Downscaled Dataset (<a href=\"https://doi.org/10.5194/gmd-17-2265-2024\" target=\"_blank\" rel=\"noopener noreferrer\">WUS-D3</a>). WUS-D3 contains an ensemble of regional climate projections over the Western United States, downscaled to 9 km resolution using the \"gold standard\", but expensive dynamical downscaling <a href=\"https://www.mmm.ucar.edu/models/wrf\" target=\"_blank\" rel=\"noopener noreferrer\">WRF</a> model. We trained our model on a single WUS-D3 climate projection, and evaluated its skill by downscaling 7 additional climate projections from the WUS-D3 ensemble. We compared the results against the computationally expensive dynamical downscaling, our target, and two popular statistical downscaling methods: <a href=\"https://doi.org/10.1029/2001JD000659\" target=\"_blank\" rel=\"noopener noreferrer\">BCSD</a> and <a href=\"https://doi.org/10.1029/2023EF004107\" target=\"_blank\" rel=\"noopener noreferrer\">STAR-ESDM</a>. The results were compelling:</p><ul><li data-block-key=\"6jkrq\"><i>Higher accuracy:</i> Dynamical-generative downscaling reduces fine-scale errors by over 40% compared to statistical methods across various weather variables like temperature, precipitation, relative humidity, and wind speed, as measured by the <a href=\"https://doi.org/10.1175/1520-0434(2000)015%3C0559:DOTCRP%3E2.0.CO;2\" target=\"_blank\" rel=\"noopener noreferrer\">continuous ranked probability score</a> (CRPS). It also effectively corrects systematic biases in the coarse-resolution climate simulations, critical for accurate environmental risk assessment of coastal and mountainous regions.<br><br></li><li data-block-key=\"2j4p0\"><i>Realistic weather patterns:</i> The AI-generated results capture realistic spatial patterns, correlations between different weather variables (like wind speed and direction, or heat and humidity), and importantly, the likelihood of compound extreme events (like concurrent heat and drought). Identifying spatial patterns and correlations is important for downstream applications in hydrology, energy forecasting, or natural hazard risk assessment.<br><br></li><li data-block-key=\"6f3sm\"><i>Better uncertainty estimates:</i> By downscaling large ensembles of climate projections, our framework provides a more comprehensive picture of the range of potential future environmental conditions than could be achieved by either the statistical methods or by dynamically downscaling only a smaller subset of Earth system models. This greater reliability is crucial for robust risk assessment. For example, dynamical-generative downscaling reduces the error in projections of extreme summer heat and winter precipitation by over 20% and 10% compared to those baselines, respectively, as measured by the mean absolute error in the 99th climatological percentile.<br><br></li><li data-block-key=\"84tof\"><i>Capturing regional extremes:</i> The method shows remarkable skill in capturing complex environmental risks due to regional phenomena, e.g., wildfire risk due to Santa Ana winds in Southern California. Projecting wildfire risk requires the accurate detection of fine-scale correlations between temperature, humidity, and wind extremes. Statistical downscaling methods, such as BCSD and STAR-ESDM, struggle at capturing granular correlations between meteorological fields, underestimating the risk of concurrent hazards.</li></ul>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-3a-Performance.width-1250.jpg\" alt=\"DynGenDown-3a-Performance\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-3a-Performance.width-1250.jpg\" alt=\"DynGenDown-3a-Performance\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"n3zhp\"><i>A comparison of fine-scale prediction errors (</i><b><i>a</i></b><i>–</i><b><i>c</i></b><i>) and</i> <a href=\"https://en.wikipedia.org/wiki/Energy_cascade\" target=\"_blank\" rel=\"noopener noreferrer\"><i>energy spectra</i></a><i> (</i><b><i>d–f</i></b><i>) between our model (R2D2) and the statistical downscaling baselines (BCSD, STAR-ESDM) for ensemble projections of wind speed (</i><b><i>left</i></b><i>), relative humidity (</i><b><i>center</i></b><i>), and temperature (</i><b><i>right</i></b><i>). The new method shows lower error (CRPS) and more realistic weather patterns, as measured by the alignment of the R2D2 and target energy spectra. The bottom panels compare the covariation of hot and dry summer extremes (</i><b><i>g</i></b><i>–</i><b><i>i</i></b><i>) and wind extremes in the fall (</i><b><i>j–l</i></b><i>).</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-4a-Wildfire.width-1250.jpg\" alt=\"DynGenDown-4a-Wildfire\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/DynGenDown-4a-Wildfire.width-1250.jpg\" alt=\"DynGenDown-4a-Wildfire\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"n3zhp\"><i>Model projections of wildfire risk during a strong Santa Ana wind event in Southern California under environmental conditions taken from an end-of-century climate projection. Wildfire risk is measured by the weather component of the</i> <a href=\"https://www.fs.usda.gov/science-technology/fire/forecasting/santa-ana-wildfire-threat-index\" target=\"_blank\" rel=\"noopener noreferrer\"><i>Santa Ana Wildfire Threat Index</i></a><i>. The dynamical-generative approach (</i><b><i>a</i></b><i>) captures wildfire risk well compared to the more expensive dynamical downscaling method (</i><b><i>b</i></b><i>). Statistical downscaling methods such as STAR-ESDM (</i><b><i>c</i></b><i>) and BCSD (</i><b><i>d</i></b><i>) often struggle to capture the magnitude and spatial structure of such compound risks.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Why this breakthrough matters</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7espt\">Dynamical-generative downscaling represents a significant step towards obtaining comprehensive future regional climate projections at actionable scales below 10 km. It makes downscaling large ensembles of Earth system models computationally feasible — our study estimates computational cost savings of 85% for the 8-model ensemble tested, a figure that would increase for larger ensembles. The fast and efficient AI inference step is similar to how Google’s <a href=\"https://research.google/blog/generative-ai-to-quantify-uncertainty-in-weather-forecasting/\">SEEDS</a> and <a href=\"https://deepmind.google/discover/blog/gencast-predicts-weather-and-the-risks-of-extreme-conditions-with-sota-accuracy/\" target=\"_blank\" rel=\"noopener noreferrer\">GenCast</a> weather forecasting models operate, enabling a thorough assessment of regional environmental risk.</p><p data-block-key=\"chlkh\">By providing more accurate and probabilistically complete regional climate projections at a fraction of the computational cost, dynamical-generative downscaling can dramatically improve environmental risk assessments. This enables better-informed decisions for adaptation and resilience policies across vital sectors like agriculture, water resource management, energy infrastructure, and natural hazard preparedness.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"7espt\"><i>We would like to thank our co-authors Zhong Yi Wan, Leonardo Zepeda-Núñez, Tapio Schneider, John Anderson, and Fei Sha. We would also like to acknowledge Stephan Hoyer, Lizao Li, Alex Hall, and Stefan Rahimi for insightful comments on our work.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "学习澄清：基于行动的对比自训练多轮对话 (原标题: Learning to clarify: Multi-turn conversations with Action-Based Contrastive Self-Training)",
      "link": "https://research.google/blog/learning-to-clarify-multi-turn-conversations-with-action-based-contrastive-self-training/",
      "pubDate": "Mon, 02 Jun 2025 16:00:00 GMT",
      "isoDate": "2025-06-02T16:00:00.000Z",
      "creator": "Google",
      "summary": "大型语言模型（LLMs）在开发智能对话代理方面取得了显著进展，但它们在多轮对话中仍缺乏关键技能，例如在面对歧义时进行澄清。LLMs往往会过度猜测或隐含地推断用户意图，而非主动提问以寻求澄清。高质量对话样本的稀缺性限制了LLMs学习最佳对话行动的能力。\n\n在ICLR 2025上发表的论文《学习澄清：基于行动的对比自训练多轮对话》中，研究人员提出了**基于行动的对比自训练（ACT）**算法。ACT是一种基于直接偏好优化（DPO）的准在线偏好优化算法，旨在实现数据高效的多轮对话策略学习。该方法在表格问答和机器阅读理解等多个真实世界对话任务中，展示了其在数据高效微调场景下的有效性。此外，论文还引入了**AmbigSQL**，一项用于澄清复杂结构化查询语言（SQL）代码生成的信息查询请求的新任务，以促进数据分析代理的开发。研究还提出通过评估LLMs隐式识别和推理对话中歧义的能力来衡量其作为对话代理的功能。\n\nACT在对话建模方面比监督微调（SFT）和DPO等标准微调方法有显著改进。\n\n**对话代理的澄清能力**\n\n一个能够进行消歧的对话代理，可以在存在歧义时识别并提出澄清问题，从而获得更准确的最终答案。\n\n![对话代理澄清能力概述](https://storage.googleapis.com/gweb-research2023-media/images/L2C1-OverviewHero.width-1250.png)\n\n**对话推理的新视角**\n\n传统的神经对话代理通常包含两个核心组件：对话理解和规划模块（例如，判断是否需要提出澄清问题的二元预测任务），以及执行这些对话行动的生成模块（即形成澄清问题或尝试回答）。然而，在现代交互范式中，LLMs通常被端到端地应用于对话应用程序，而没有中间规划阶段。ACT提出将对话行动规划直接优化为响应生成的一个隐式子任务，称之为**隐式行动规划**。\n\nLLM的下游使用训练通常包括三个阶段：预训练、用于指令遵循的监督微调（SFT），以及与人类偏好对齐的微调。DPO是常用的最终对齐算法，但它通常与多轮对话的本质不符。ACT算法旨在解决这些问题。\n\n**ACT算法阶段**\n\n1.  **阶段1：基于行动的对比数据生成**\n    *   目标：构建一个偏好数据集，包含一对对话响应，其中一个代表“获胜”行动，另一个代表“失败”行动。\n    *   过程：从初始对话数据集开始，对于数据集中的每个回合，使用对话历史和必要的任务特定上下文作为输入提示。将该回合视为“获胜响应”（表达一个行动，例如“澄清”）。然后使用条件生成模型合成一个“被拒绝的响应”（代表相反的行动，例如“回答”）。此阶段的结果是一个配对数据集，其中每个被拒绝的响应都是合成生成的。\n\n    ![ACT数据生成阶段概述](https://storage.googleapis.com/gweb-research2023-media/images/L2C2-DataGeneration.width-1250.png)\n\n2.  **阶段2：对比自训练**\n    *   目标：使用DPO目标微调策略模型，但采用“在线策略学习”（on-policy learning）。\n    *   核心理念：DPO类算法通过优化分配给获胜和失败响应的对数概率来工作。在线策略响应采样产生高概率的token序列。对话改进需要多轮优化，这难以仅用单轮对比配对来表达。\n    *   过程：不直接使用固定对比对进行离线梯度更新，而是执行在线策略采样。首先确定响应是否表达了正确的行动（例如，一个澄清问题），如果是，则模拟轨迹结果并根据原始对话中给出的信息查询意图评估结果。根据结果是否正确，用模拟的多轮轨迹替换阶段1中对比对的获胜或失败响应。策略模型使用DPO目标进行更新。\n\n    ![ACT微调阶段概述](https://storage.googleapis.com/gweb-research2023-media/images/L2C4-Final.width-1250.png)\n\n**推动最先进的多轮建模能力**\n\n研究人员使用开放权重LLMs在多样化的多轮对话数据集上对ACT进行了实验，包括PACIFIC（表格与文本推理）、Abg-CoQA（密集段落推理）和AmbigSQL（文本到SQL生成）。实验结果显示：\n\n*   **PACIFIC数据集表现：** 在PACIFIC的所有三种数据高效设置中，ACT在所有指标上都取得了最强的性能，优于SFT和提示Gemini（后者具有额外的测试时间计算优势）。\n    *   在仅使用50个对话作为微调数据时，ACT在衡量模型隐式识别歧义的能力方面，比SFT实现了高达19.1%的相对改进（Macro F1从69.0提高到82.2）。\n    *   与基于适配器的SFT与Gemini Pro相比，ACT在多轮任务性能方面的数据效率大大提高，轨迹级DROP F1的相对改进高达35.7%（从45.6提高到61.9）。\n    *   在这些有限数据设置下，使用ACT进行微调使模型能够匹配或超越使用上下文学习的前沿LLMs，尽管在推理过程中没有上下文示例。\n    *   **关键发现：** 在线策略学习和多轮轨迹模拟对于改进多轮目标完成至关重要。\n\n    ![ACT在数据高效对话建模中显著优于标准微调方法](https://storage.googleapis.com/gweb-research2023-media/images/L2C4-Results.width-1250.png)\n\n**ACT性能增益归因（消融研究）**\n\n研究人员进行了多项实验以理解ACT各组件的益处：\n\n*   **基于行动的偏好是否必要？** “ACT w/ Random Actions”的表现不如正常ACT，表明行动选择的重要性。\n*   **是否需要在线策略采样？** “ACT w/o on-policy sampling”（在阶段1构建的数据集上评估正常离线策略DPO）显示出一些改进，但远不如完整的ACT。这可能是因为离线策略的负面响应不一定位于策略模型的语言流形中，分布偏移可能难以通过离线学习克服。\n*   **轨迹模拟是否必要？** “ACT w/ sampling w/o simulation”的结果表明，轨迹级模拟对于提高多轮性能至关重要，特别是策略模型推理其自身澄清问题的能力。\n*   **ACT是否模型无关？** 实验表明，ACT可以提高性能，无论基础模型是否预先与人类反馈对齐，尽管对齐可以作为改进的模型初始化。总体而言，ACT在提高基础模型性能方面是模型无关的。\n\n![ACT各组件的消融研究（使用PACIFIC数据集）](https://storage.googleapis.com/gweb-research2023-media/images/L2C5-Table.width-1250.png)\n\n**多轮对话建模的未来**\n\nACT是一种模型无关的准在线对比微调方法，适用于样本高效的对话任务适应，并提供了一套对话代理评估的工作流程。研究结果表明，ACT在有限数据条件下对任务适应非常有效。未来的工作可能考虑将ACT与现有复杂任务（如文本到SQL生成）的复杂微调方法相结合，以及推广到大规模数据和多任务环境。",
      "shortSummary": "大型语言模型（LLMs）在多轮对话中常缺乏澄清能力。为解决此问题，研究提出“基于行动的对比自训练”（ACT）算法。ACT是一种数据高效的准在线偏好优化方法，通过隐式行动规划和多轮轨迹模拟，显著提升了LLMs在歧义识别和澄清方面的表现。实验证明，ACT在数据受限场景下，相比传统微调和DPO等方法，能大幅提高对话模型性能，甚至超越使用上下文学习的前沿LLMs，为构建更智能的对话代理提供了新途径。",
      "translated_title": "学习澄清：基于行动的对比自训练多轮对话",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/L2C1-OverviewHero.width-1250.png",
          "alt": "L2C1-OverviewHero",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/L2C2-DataGeneration.width-1250.png",
          "alt": "L2C2-DataGeneration",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/L2C4-Final.width-1250.png",
          "alt": "L2C4-Final",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/L2C4-Results.width-1250.png",
          "alt": "L2C4-Results",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/L2C5-Table.width-1250.png",
          "alt": "L2C5-Table",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"4n8wb\">Large language models (LLMs) that have been optimized through human feedback have rapidly emerged as a <a href=\"https://arxiv.org/abs/2312.11805\" target=\"_blank\" rel=\"noopener noreferrer\">leading paradigm</a> for developing intelligent conversational agents. However, despite their strong performance across many benchmarks, LLM-based agents can still lack multi-turn conversational skills such as disambiguation — when they are faced with ambiguity, they often overhedge or implicitly guess users' true intents rather than asking clarifying questions. Yet high-quality conversation samples are often limited, constituting a bottleneck for LLMs' ability to learn optimal dialogue actions.</p><p data-block-key=\"3at5l\">In “<a href=\"https://openreview.net/pdf?id=SIE6VFps9x\" target=\"_blank\" rel=\"noopener noreferrer\">Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training</a>” (presented at <a href=\"https://iclr.cc/Conferences/2025\" target=\"_blank\" rel=\"noopener noreferrer\">ICLR 2025</a>), we propose Action-Based Contrastive Self-Training (ACT), a quasi-online preference optimization algorithm based on <a href=\"https://neurips.cc/virtual/2023/oral/73865\" target=\"_blank\" rel=\"noopener noreferrer\">Direct Preference Optimization</a> (DPO), which enables data-efficient dialogue policy learning in multi-turn conversation modeling. We demonstrate ACT's efficacy under data-efficient tuning scenarios using multiple real-world conversational tasks such as <a href=\"https://aclanthology.org/2022.emnlp-main.469/\" target=\"_blank\" rel=\"noopener noreferrer\">tabular-grounded question-answering</a> and <a href=\"https://www.akbc.ws/2021/assets/pdfs/SlDZ1o8FsJU.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">machine reading comprehension</a>. We also introduce AmbigSQL, a novel task for disambiguating information-seeking requests for complex <a href=\"https://cloud.google.com/sql\" target=\"_blank\" rel=\"noopener noreferrer\">Structured Query Language</a> (SQL) code generation to facilitate the development of data analysis agents. Additionally, we propose evaluating the ability of LLMs to function as conversational agents by examining whether they can implicitly recognize and reason about ambiguity in conversation. ACT demonstrates substantial conversation modeling improvements over standard tuning approaches like supervised fine-tuning and DPO.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/L2C1-OverviewHero.width-1250.png\" alt=\"L2C1-OverviewHero\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/L2C1-OverviewHero.width-1250.png\" alt=\"L2C1-OverviewHero\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"v70yh\"><i>A conversational agent capable of disambiguation could recognize when there is ambiguity and ask a clarifying question towards a more accurate final answer.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qbpr9\">A novel perspective on conversational reasoning</h2><p data-block-key=\"7gplf\">Traditional neural approaches to building conversational agents typically consist of two core components: a module for dialogue understanding and planning (e.g., a binary prediction task to determine whether it is appropriate to ask a clarifying question), and a generation module which can execute such conversational actions (i.e., forming a clarifying question or answer attempt). However, in the modern interaction paradigm, LLMs are typically adapted for end-to-end usage in conversational applications without an intermediate planning stage. We propose directly optimizing conversational action planning as an implicit subtask of response generation. We refer to this paradigm as implicit action planning.</p><p data-block-key=\"8s2r5\">Training an LLM for downstream use consists of three phases: pre-training, <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/samples/generativeaionvertexai-tuning-basic\" target=\"_blank\" rel=\"noopener noreferrer\">supervised fine-tuning</a> (SFT) for instruction-following, and tuning for <a href=\"https://cloud.google.com/blog/products/ai-machine-learning/rlhf-on-google-cloud\" target=\"_blank\" rel=\"noopener noreferrer\">alignment with human preferences</a>. A common algorithm used for this final alignment phase is <a href=\"https://openreview.net/pdf?id=HPuSIXJaa9\" target=\"_blank\" rel=\"noopener noreferrer\">DPO, an off-policy contrastive learning algorithm</a> designed to optimize the probabilities of winning and losing sequences such as conversation responses. However, such algorithms are typically still misaligned with the multi-turn nature of conversations. The proposed ACT algorithm seeks to address these issues.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"qbpr9\">Phase 1: Action-based contrastive data generation</h3><p data-block-key=\"3h973\">The first phase of building ACT involves constructing a preference dataset, consisting of pairs of conversational responses where one resembles a winning action and one resembles a losing action.</p><p data-block-key=\"55dje\">We start from an initial conversational dataset. For each turn in the dataset, we use the conversation history as part of the input prompt (“Show me information…” in the figure below) in addition to any necessary task-specific context (such as a SQL database schema) and treat that turn as the winning response (“What specific …”, below). This winning response expresses an action (here, “Clarify”) and thus we synthesize a rejected response representing some converse action (here, “Answer”) using some <a href=\"https://aclanthology.org/2023.acl-short.82/\" target=\"_blank\" rel=\"noopener noreferrer\">conditional generation model</a>. The result of this stage is a pairwise dataset where each of the rejected responses is synthetically generated.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/L2C2-DataGeneration.width-1250.png\" alt=\"L2C2-DataGeneration\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/L2C2-DataGeneration.width-1250.png\" alt=\"L2C2-DataGeneration\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"v70yh\"><i>Overview of the data generation phase of ACT.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h3 data-block-key=\"qbpr9\">Phase 2: Contrastive self-training</h3><p data-block-key=\"8pqj9\">The second phase involves tuning the policy model using the DPO objective. We can use the prompts from Phase 1, but rather than directly running DPO using the previously constructed contrastive pairs, we perform on-policy learning according to a few intuitions:</p><ul><li data-block-key=\"6st4r\">DPO-like algorithms work by optimizing the log probabilities assigned to the winning and losing responses.</li><li data-block-key=\"1q7d\">By construction, on-policy response sampling yields high-probability token sequences.</li><li data-block-key=\"9mji7\">Conversational improvements require multi-turn optimization, which are difficult to express using only single-turn contrast pairings.</li></ul><p data-block-key=\"2cl9h\">The figure below demonstrates how ACT works according to these intuitions. Rather than directly running an offline gradient update using the fixed contrastive pairs, we perform on-policy sampling. We first determine whether the response expresses the correct action (e.g., a clarifying question), and if so, then we <i>simulate the result of the trajectory</i> and evaluate the outcome against the information-seeking intent given in the original conversation. Depending on if the outcome is correct, we replace either the winning or losing response from the contrastive pair in Phase 1 with the simulated <i>multi-turn trajectory</i>.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/L2C4-Final.width-1250.png\" alt=\"L2C4-Final\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/L2C4-Final.width-1250.png\" alt=\"L2C4-Final\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"v70yh\"><i>Overview of the tuning phase of ACT. For each initial contrastive pairing from the Phase 1 preference dataset, we sample an on-policy response from the model being tuned. We evaluate the trajectory resulting from the sampled response then update the contrastive pairing by either replacing the existing winning or losing response. The policy is updated using the DPO objective.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qbpr9\">Pushing state-of-the-art multi-turn modeling capabilities</h2><p data-block-key=\"70a\">We experimented with ACT using open-weight LLMs on a diverse set of multi-turn conversational datasets: <a href=\"https://aclanthology.org/2022.emnlp-main.469/\" target=\"_blank\" rel=\"noopener noreferrer\">PACIFIC</a> (reasoning over tables mixed with text), <a href=\"https://openreview.net/forum?id=SlDZ1o8FsJU\" target=\"_blank\" rel=\"noopener noreferrer\">Abg-CoQA</a> (reasoning over dense passages), and AmbigSQL (text-to-SQL generation). We compared against various competitive baselines, including:</p><ul><li data-block-key=\"1kgvc\">Supervised fine-tuning with cross-entropy loss (SFT)</li><li data-block-key=\"cd5qp\">Iterative Reasoning Preference Optimization (IRPO)</li><li data-block-key=\"3736h\">Prompting Gemini 1.5 with in-context learning (ICL) examples</li><li data-block-key=\"nit9\">Prompting Claude 3.5 with ICL examples</li></ul><h3 data-block-key=\"2g6u0\">Conversational question answering with tabular grounding on PACIFIC</h3><p data-block-key=\"4d49g\">In the figure below, we see that across all three data-efficient settings considered for PACIFIC, ACT achieves the strongest performance across all metrics compared to both SFT and prompting Gemini, which has the advantage of additional test-time computation. In particular, ACT achieves up to a 19.1% relative improvement over SFT when measuring the tuned model's ability to implicitly recognize ambiguity (from 69.0 to 82.2 <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\" target=\"_blank\" rel=\"noopener noreferrer\">Macro F1</a>) given only 50 conversations as tuning data. We also observe that ACT has greatly improved data efficiency compared to adapter-based SFT with Gemini Pro, with a relative improvement of as high as 35.7% in multi-turn task performance (from 45.6 to 61.9 in terms of trajectory-level <a href=\"https://aclanthology.org/N19-1246/\" target=\"_blank\" rel=\"noopener noreferrer\">DROP F1</a>). Additionally, tuning with ACT in these limited data settings grants the model the ability to match or outperform frontier LLMs used with in-context learning despite having zero in-context examples during inference. Overall, we find that on-policy learning and multi-turn trajectory simulation are crucial for improved multi-turn goal completion.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/L2C4-Results.width-1250.png\" alt=\"L2C4-Results\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/L2C4-Results.width-1250.png\" alt=\"L2C4-Results\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"v70yh\"><i>ACT greatly outperforms standard tuning approaches in data-efficient settings for conversational modeling.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qbpr9\">In our <a href=\"https://openreview.net/pdf?id=SIE6VFps9x\" target=\"_blank\" rel=\"noopener noreferrer\">paper</a>, we present extended results on the PACIFIC corpus, where we demonstrate that ACT outperforms IRPO. There, we additionally present our findings on Abg-CoQA and AmbigSQL.</p><p data-block-key=\"q5ps\"></p><h3 data-block-key=\"8au6j\">Attributing the performance gains from ACT</h3><p data-block-key=\"brt9m\">We conducted several experiments to understand the benefits of each component of ACT, the results of which are below:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/L2C5-Table.width-1250.png\" alt=\"L2C5-Table\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/L2C5-Table.width-1250.png\" alt=\"L2C5-Table\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"v70yh\"><i>Ablation study of various components of ACT using PACIFIC.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"qbpr9\"><b><i>Are action-based preferences necessary?</i></b> One of the key factors of ACT is that the contrastive pairs highlight differences between conversational actions. In “ACT w/ Random Actions”, we additionally examine the importance of action selection by randomly sampling both the winning and losing action when constructing the preference pair, and observe this underperforms normal ACT.</p><p data-block-key=\"83hrr\"><b><i>Do we need on-policy sampling?</i></b> In “ACT w/o on-policy sampling”, we examine the importance of on-policy sampling by evaluating normal off-policy DPO on the dataset as constructed in Phase 1. While we do observe some improvements over SFT (e.g., from 69.0 to 74.8 Macro F1), the overall improvements are much larger when using on-policy sampling as with full ACT. This may be due to the fact that the off-policy negative responses are not guaranteed to lie in the language manifold of the policy model, and distribution shift may be too difficult to overcome with off-policy learning.</p><p data-block-key=\"dn7kt\"><b><i>Is trajectory simulation necessary?</i></b> ACT is better-aligned with multi-turn conversations due to its trajectory simulation. Without multi-turn simulation, our approach can be viewed similarly to on-policy DPO variants like IRPO, but with a conversation-specific reward signal which accounts for conversation actions and task heuristics. In “ACT w/ sampling w/o simulation”, we find that this trajectory-level simulation is critical to improving multi-turn performance, especially the policy model’s ability to reason about its own clarification questions.</p><p data-block-key=\"f50m1\"><b><i>Is ACT model agnostic?</i></b> The base model in our main experiments, <a href=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\" target=\"_blank\" rel=\"noopener noreferrer\">Zephyr</a>, is obtained by aligning <a href=\"https://mistral.ai/news/announcing-mistral-7b\" target=\"_blank\" rel=\"noopener noreferrer\">Mistral</a>. In “ACT with unaligned foundation models” we observe a performance gap of 6.5 Action F1 and 4.3 Trajectory F1 after ACT tuning for the two models. However, our results demonstrate ACT can improve performance regardless of pre-existing alignment with human feedback, although it can help as an improved model initialization. Overall, we find that improving base model performance with ACT is model agnostic.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qbpr9\">The future of multi-turn conversation modeling</h2><p data-block-key=\"6ihu3\">We propose ACT, a model agnostic quasi-online contrastive tuning approach for sample-efficient conversational task adaptation, along with a workflow for evaluation of conversational agents. We demonstrate encouraging evidence that ACT is highly effective for task adaptation in the limited data regime. Future work may also consider combining ACT with existing sophisticated tuning approaches for complex tasks like text-to-SQL generation, as well as generalization to large-scale data and multi-task environments.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <h2 data-block-key=\"qbpr9\">Acknowledgements</h2><p data-block-key=\"1srhj\"><i>We are deeply grateful for helpful feedback on our manuscript from Hanjun Dai, and advice from Ta-Chung Chi and Kun Qian. We would also like to recognize Chris Baron and Vipin Nair, whose efforts have been crucial to the success of this work. This work was completed in Google Cloud AI Research where Maximillian Chen was a Student Researcher and Ruoxi Sun was a Research Scientist.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "使用用户级差分隐私微调大型语言模型 (原标题: Fine-tuning LLMs with user-level differential privacy)",
      "link": "https://research.google/blog/fine-tuning-llms-with-user-level-differential-privacy/",
      "pubDate": "Thu, 22 May 2025 16:00:00 GMT",
      "isoDate": "2025-05-22T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 使用用户级差分隐私微调大型语言模型\n\n## 引言与背景\n*   **模型微调与隐私挑战**：现代机器学习（ML）模型常需针对特定领域数据进行微调以优化性能，然而这些数据往往涉及隐私敏感信息。差分隐私（DP）通过在训练过程中注入噪声，严格保证训练模型尊重训练数据的隐私。\n*   **个体示例级DP的局限性**：大多数DP研究关注个体示例的隐私（示例级DP）。但如果一个用户贡献了大量示例，攻击者即使无法了解单个示例，也可能推断出该用户的某些信息。\n*   **用户级DP的重要性**：用户级DP是一种更强的隐私形式，它保证攻击者无法通过模型了解用户是否包含在训练数据集中。这更符合当今社会的数据所有权模式，并常用于联邦学习中，因为分布式设备（如手机）通常包含由单个用户拥有的多个示例，需要更严格的隐私保障。\n    *   ![用户级DP概述](https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-1-Overview.width-1250.png)\n    *   用户级DP确保无法判断某个用户的所有数据是否被包含在训练中，而不仅仅是其数据的一部分。\n*   **用户级DP的挑战**：与示例级DP相比，用户级DP的学习难度更高，需要注入更多的噪声，且模型越大，问题越严重。\n\n## 在数据中心扩展用户级DP到大型语言模型（LLMs）\n*   **研究目标**：论文《在固定计算预算下使用用户级差分隐私进行学习》旨在将用户级DP扩展到在数据中心训练的大型语言模型（LLMs）。\n*   **数据中心训练的优势**：与联邦学习相比，数据中心训练更灵活，可以查询单个示例或整个用户，并且可以在每个训练轮次选择要查询的用户。\n*   **研究核心问题**：如何利用数据中心训练的这种灵活性来获得更好的训练结果？\n*   **聚焦LLM微调**：由于DP训练计算成本高昂，不适用于整个LLM的训练，且微调更可能涉及私有领域特定数据，因此研究重点放在LLM的DP微调上。目标是确定最佳算法并利用数据中心灵活性进一步优化它们，因为即使是微小的噪声减少也能带来显著的质量提升。\n\n## 使用用户级隐私训练模型\n*   **DP-SGD原理**：随机梯度下降（SGD）通过将训练数据分成小批次，计算每个示例的“梯度”并应用于模型。DP-SGD通过向梯度添加随机噪声来实现DP，从而使模型获得不完美但保护隐私的信息。\n    *   ![DP-SGD步骤可视化](https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-2a-Visualization.width-800.gif)\n    *   DP-SGD非常适合实现示例级DP，因为它直接限制了每个示例对模型的影响。\n*   **实现用户级DP的两种方法**：\n    1.  **预处理**：首先限制每个用户对训练数据集贡献的示例数量（“贡献上限”）。\n    2.  **两种采样方法**：\n        *   **示例级采样（ELS）**：直接应用DP-SGD，但通过添加更多噪声将示例级DP保证转换为用户级DP。它随机采样示例来形成批次。\n        *   **用户级采样（ULS）**：随机采样用户，然后将所有来自这些采样用户的示例组成批次。ULS在数据中心环境中类似于联邦学习（但在实际联邦学习中没有随机采样）。\n    *   ![用户贡献上限示例](https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-3a-Example.width-1250.png)\n    *   ![ELS与ULS批次形成对比](https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-4a-ELSvULS.width-800.gif)\n    *   **关键参数**：两种算法都有一个关键参数需要优化——预处理中使用的“贡献上限”。\n\n## 针对LLMs的算法优化\n*   **ELS的噪声优化**：发现以往工作添加的噪声比实际需要的多几个数量级。通过新的证明，可以在保持相同隐私保证的前提下显著减少噪声，从而提高模型质量。\n*   **ELS和ULS的贡献上限优化**：\n    *   “默认”选择是使用每个用户都满足的贡献上限（即不进行预处理），但这会导致为保护大量数据贡献者而添加大量噪声。\n    *   设置较小的贡献上限可以减少所需噪声，但代价是丢弃大量数据。\n    *   由于LLM训练成本高昂，无法尝试训练多个模型来选择最佳贡献上限，因此需要一种在训练前有效选择贡献上限的策略。\n    *   **ELS策略**：经过大规模实验，发现将贡献上限设置为每个用户持有的示例数量的中位数是一种有效策略。\n    *   **ULS策略**：提供一个总噪声随贡献上限变化的预测函数，选择使该预测最小化的贡献上限是一种有效策略。\n\n## 实验结果\n*   **ELS噪声减少**：与以往工作相比，新分析证明所需的噪声水平呈指数级降低（隐私保证ε仅呈近似线性衰减，而非指数衰减）。\n    *   ![贡献上限与隐私保证](https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-5-ContributionBound.width-1250.png)\n*   **ELS与ULS对比**：在语言模型微调任务中，使用相同总示例数进行训练，对3.5亿参数的Transformer模型在StackOverflow和CC-News数据集上进行微调。\n    *   ![CCNews和StackOverflow上的ELS、ULS和无微调对比](https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-6a-ELSvULS.width-1250.png)\n    *   ![CCNews和StackOverflow上的ELS、ULS和无微调对比](https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-6b-ELSvULS.width-1250.png)\n    *   **结果**：在大多数情况下，ULS是更好的算法。例外情况（至少对于CC-News）是需要更高隐私或计算量较少的情况。值得注意的是，由于这些优化，尽管有严格的隐私要求，两种方法都比预训练模型表现更好。\n\n## 结论\n*   **主要贡献**：优化了ELS和ULS两种DP-SGD变体在数据中心实现用户级DP的性能。\n*   **具体优化**：为ELS提供了新的隐私保证，并为ELS和ULS开发了无需多次训练即可设置贡献上限的启发式方法。这些优化为重要参数提供了有充分依据的选择。\n*   **可行性与优势**：尽管用户级DP是严格的隐私定义，且使用DP训练大型模型面临挑战，但实验证明，通过这些优化，使用用户级DP微调LLMs是可行且优于仅使用预训练模型的。\n*   **实际意义**：这项工作使模型训练者能够对敏感数据集进行模型微调，同时为用户提供强大的隐私保护。",
      "shortSummary": "该研究专注于使用用户级差分隐私（DP）微调大型语言模型（LLMs），以解决敏感数据隐私问题。文章优化了两种DP-SGD变体：示例级采样（ELS）和用户级采样（ULS），使其适用于数据中心训练。通过为ELS提供新的隐私保证和为两种方法开发贡献上限的启发式选择策略，显著减少了所需噪声并提高了模型质量。实验表明，ULS通常表现更优，且经过优化的DP微调方法在严格隐私要求下，仍能超越预训练模型，实现对敏感数据的有效模型训练。",
      "translated_title": "使用用户级差分隐私微调大型语言模型",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-1-Overview.width-1250.png",
          "alt": "UserLvlDP-1-Overview",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-2a-Visualization.width-800.gif",
          "alt": "UserLvlDP-2a-Visualization",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-3a-Example.width-1250.png",
          "alt": "UserLvlDP-3a-Example",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-4a-ELSvULS.width-800.gif",
          "alt": "UserLvlDP-4a-ELSvULS",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-5-ContributionBound.width-1250.png",
          "alt": "UserLvlDP-5-ContributionBound",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ifmk7\">The machine learning community has consistently found that while modern machine learning (ML) models are powerful, they <a href=\"https://arxiv.org/pdf/2103.08493\" target=\"_blank\" rel=\"noopener noreferrer\">often need to be fine-tuned on domain-specific data</a> to maximize performance. This can be problematic or even impossible, as informative data is often privacy-sensitive. Differential privacy (DP) allows us to train ML models while rigorously guaranteeing that the learned model respects the privacy of its training data, by injecting noise into the training process.</p><p data-block-key=\"eauhh\">Most work on DP focuses on the privacy of individual examples (i.e., <a href=\"https://desfontain.es/blog/differential-privacy-in-more-detail.html\" target=\"_blank\" rel=\"noopener noreferrer\">example-level DP</a>). This has drawbacks. If a particular user has many examples training the model, attackers may be able to learn something about that user even if they can’t learn about their individual examples.</p><p data-block-key=\"8jgo8\"><a href=\"https://arxiv.org/abs/1710.06963\" target=\"_blank\" rel=\"noopener noreferrer\">User-level DP</a> is a stronger form of privacy that guarantees that an attacker who uses a model can’t learn things about the user, like whether or not the user’s data is included in the training dataset. User-level DP better reflects how data is actually owned in today’s society. It’s used frequently in <a href=\"https://research.google/blog/federated-learning-collaborative-machine-learning-without-centralized-training-data/\">federated learning</a> to train models across distributed devices like cell phones. Those devices often have many examples, all owned by a single user, and require more stringent privacy guarantees.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-1-Overview.width-1250.png\" alt=\"UserLvlDP-1-Overview\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-1-Overview.width-1250.png\" alt=\"UserLvlDP-1-Overview\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"37bs3\"><i>User-level DP makes it so you can’t tell if</i> <b><i>all</i></b><i> of someone’s data was included in training or not, rather than just one piece of their data.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ifmk7\">Learning with user-level DP is strictly harder than example-level DP, and requires adding significantly more noise. This is a problem that gets worse as the model gets larger!</p><p data-block-key=\"2tkl7\">In “<a href=\"https://arxiv.org/abs/2407.07737\" target=\"_blank\" rel=\"noopener noreferrer\">Learning with User-Level Differential Privacy Under Fixed Compute Budgets</a>”, we set out to scale user-level DP to large language models (LLMs) trained in the datacenter. Datacenter training is much more flexible than federated learning. In federated learning, one can only perform queries on users and not on individual examples, and it is not possible to choose which users are available to query. In datacenter training, one can query both individual examples and whole users, and it is possible to choose which ones to query every round. Our central question is, how can we use this increased flexibility to achieve better training results?</p><p data-block-key=\"630i4\">Rather than training the full LLM with DP, we focus on LLM DP fine-tuning as DP requires more computation, which might be unaffordable for full LLM training, and fine-tuning is more likely to require private domain-specific data. We determine which algorithms worked best and how to use the flexibility of datacenter training to further optimize them. This optimization is important for LLMs as even small reductions in noise can result in significant quality gains. We also show that even with the added flexibility in the datacenter, our proposed training strategy looks more like an algorithm for federated learning.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Training models with user-level privacy</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ifmk7\"><a href=\"https://en.wikipedia.org/wiki/Stochastic_gradient_descent\" target=\"_blank\" rel=\"noopener noreferrer\">Stochastic gradient descent</a> (SGD) is a common model training algorithm that randomly divides training data into small batches, computes model updates, called “gradients”, for each example in the batch, and applies them to the model. To train with DP, we modify this slightly by adding random noise to the gradients, essentially combining DP with SGD in a process referred to as DP-SGD. The noise makes it so the model gets imperfect information about the examples during training, which is good for privacy! But since we are giving the model imperfect information, its ability to learn from the examples is necessarily weaker than if we gave it perfect information. However, DP is a prerequisite for using private data, and imperfect information about the private data is better than no information at all.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-2a-Visualization.width-800.gif\" alt=\"UserLvlDP-2a-Visualization\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-2a-Visualization.width-800.gif\" alt=\"UserLvlDP-2a-Visualization\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"37bs3\"><i>Visualizing the steps in (DP-)SGD.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ifmk7\">DP-SGD is great for achieving example-level DP because it directly limits how much each example affects the model. It’s been <a href=\"https://research.google/blog/bridging-the-gap-in-differentially-private-model-training/\">extensively</a> <a href=\"https://research.google/blog/differential-privacy-accounting-by-connecting-the-dots/\">studied</a> at Google, including use cases such as <a href=\"https://research.google/blog/private-ads-prediction-with-dp-sgd/\">ads modelling</a> and <a href=\"https://research.google/blog/protecting-users-with-differentially-private-synthetic-training-data/\">synthetic data generation</a>.</p><p data-block-key=\"did4l\">How do we change DP-SGD if we want user-level DP? We need to make sure to limit the effect each user has on the model.</p><p data-block-key=\"7c1mm\">There are two ways to achieve this. For both, we first pre-process the dataset so that each user only contributes a bounded number of examples to the training dataset. Then we either:</p><ol><li data-block-key=\"dp1dq\">Apply DP-SGD as-is. By adding more noise than before, we can turn our example-level DP guarantee into user-level DP (this is the hard part!).</li><li data-block-key=\"d00is\">Instead of sampling random examples in DP-SGD to form batches, sample random users, and then take all examples from the sampled users to form the batch.</li></ol><p data-block-key=\"eskpp\">The big difference between these methods is in the data we sample. The first samples random examples, so we call it “Example-Level Sampling” (ELS). The second samples random users, so we call it “User-Level Sampling” (ULS). ULS looks a lot like federated learning in the datacenter (and is actually used in real federated learning settings, but without random sampling). Note that because of the random sampling step, both algorithms are not feasible in federated learning because devices are not necessarily available for every round of training!</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-3a-Example.width-1250.png\" alt=\"UserLvlDP-3a-Example\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-3a-Example.width-1250.png\" alt=\"UserLvlDP-3a-Example\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"37bs3\"><i>An example dataset before and after we bound the contribution of each user to at most 3.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- For GIFs, use a default width -->\n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-4a-ELSvULS.width-800.gif\" alt=\"UserLvlDP-4a-ELSvULS\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-4a-ELSvULS.width-800.gif\" alt=\"UserLvlDP-4a-ELSvULS\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"37bs3\"><i>A visual comparison of how ELS and ULS form batches.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ifmk7\">Both ELS and ULS have a key parameter to optimize: the bound on the number of examples each user can contribute to the dataset, which we call the “contribution bound”, that we use in pre-processing. As we will discuss later, this parameter needs to be carefully chosen to optimize performance.</p><p data-block-key=\"f4umd\">Which of these algorithms works better, especially at scale? It’s not obvious, and it isn’t something that we found an answer to in the literature. That’s what we set out to find.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Making these algorithms work for LLMs</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ifmk7\">If we run these algorithms “out-of-the-box” for LLMs, things go badly. So, we came up with optimizations to the algorithms that fix the key issues with running them “out-of-the-box”.</p><p data-block-key=\"5g4e3\">For ELS, we had to go from example-level DP guarantees to user-level DP guarantees. We found that <a href=\"https://privacytools.seas.harvard.edu/sites/g/files/omnuum6656/files/privacytools/files/manuscript_2016.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">previous work</a> was adding orders of magnitude more noise than was actually necessary. We were able to prove that we can add significantly less noise, making the model much better while retaining the same privacy guarantees.</p><p data-block-key=\"7hcld\">For both ELS and ULS, we had to figure out how to optimize the contribution bound. A “default” choice is to choose a contribution bound that every user already satisfies; that is, we don’t do any pre-processing. However, some users may contribute a large amount of data, and we will need to add large amounts of noise to provide privacy to these users. Setting a smaller contribution bound reduces the amount of noise we need to add, but the cost is having to discard a lot of data. Because LLM training runs are expensive, we can’t afford to try training a bunch of models with different contribution bounds and pick the best one — we need an effective strategy to pick the contribution bound <i>before</i> we start training.</p><p data-block-key=\"a8hkp\">After lengthy experimentation at scale, for ELS we found that setting the contribution bound to be the median number of examples held by each user was an effective strategy. For ULS, we give a prediction for the total noise added as a function of the contribution bound, and found that choosing the contribution bound minimizing this prediction was an effective strategy.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Results</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ifmk7\">We first compared the amount of noise we proved was necessary for ELS to the noise <a href=\"https://salil.seas.harvard.edu/sites/g/files/omnuum4266/files/salil/files/the_complexity_of_differential_privacy.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">past work</a> suggested was necessary. The noise levels we determined by our analyses reflected an exponential reduction in the noise needed:</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-5-ContributionBound.width-1250.png\" alt=\"UserLvlDP-5-ContributionBound\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-5-ContributionBound.width-1250.png\" alt=\"UserLvlDP-5-ContributionBound\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"37bs3\"><i>Black-box calculations suggest that privacy guarantees (ε) decay exponentially in the contribution bound. Our new bound shows the privacy guarantee only decays near-linearly in the contribution bound!</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ifmk7\">We next compared ELS and ULS, both using our optimizations, on language model fine-tuning tasks, where each algorithm saw the same total number of examples per round of training. We fine-tuned a 350 million parameter transformer model on the <a href=\"https://arxiv.org/abs/2307.09619\" target=\"_blank\" rel=\"noopener noreferrer\">StackOverflow and CC-News datasets</a>, two standard research datasets for studying user-level DP.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-6a-ELSvULS.width-1250.png\" alt=\"UserLvlDP-6a-ELSvULS\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-6a-ELSvULS.width-1250.png\" alt=\"UserLvlDP-6a-ELSvULS\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-6b-ELSvULS.width-1250.png\" alt=\"UserLvlDP-6b-ELSvULS\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/UserLvlDP-6b-ELSvULS.width-1250.png\" alt=\"UserLvlDP-6b-ELSvULS\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"37bs3\"><i>Comparing ELS, ULS, and no fine-tuning on CCNews and StackOverflow.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"ifmk7\">We found that in most cases, ULS is the better algorithm. The exception (at least for CC-News) was in cases where we wanted more privacy or don’t use much compute. Notably, in part thanks to our optimizations, both methods performed better than the pre-trained model, despite the strict privacy requirement.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"ifmk7\">In this work, we optimized the performance of ELS and ULS, two variants of DP-SGD that achieve user-level DP and are enabled by training in the datacenter. We optimized ELS by giving new privacy guarantees for it and by developing an heuristic for setting the contribution bound without needing to do multiple training runs. We similarly optimized ULS by developing an heuristic for setting the contribution bound, again without needing to do training runs. Our optimizations provide well-justified choices for important parameters that previously were chosen in an <i>ad hoc</i> manner.</p><p data-block-key=\"cgs4i\">Despite user-level DP being a strict privacy definition and the challenges of training large models with DP, our experiments demonstrated that fine-tuning LLMs with user-level DP is both feasible and advantageous over sticking to pre-trained models thanks to our optimizations. Our work thus enables model trainers to fine-tune their models to sensitive datasets while still providing strong protections to their users.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    },
    {
      "title": "Google 研究在 Google I/O 2025 (原标题: Google Research at Google I/O 2025)",
      "link": "https://research.google/blog/google-research-at-google-io-2025/",
      "pubDate": "Wed, 21 May 2025 16:00:00 GMT",
      "isoDate": "2025-05-21T16:00:00.000Z",
      "creator": "Google",
      "summary": "# Google 研究在 Google I/O 2025\n\n在每年的 Google I/O 大会上，Google 都会分享其最先进的技术，展示它们如何提供帮助、带来新体验，以及开发者和社区如何利用它们进行创新。许多这些新技术源于 Google Research 多年的工作，通常与其他团队合作，基于人工智能及其他计算机科学领域的多次突破。今年的 I/O 强调了将研究变为现实的影响。正如 Sundar Pichai 所说：“所有这些进展意味着我们正处于 AI 平台转变的新阶段。数十年的研究现在正在全球范围内成为人们、企业和社区的现实。”\n\n除了 Google Research 对 Gemini 和 I/O 舞台上重点介绍的生成式 AI 产品的贡献外，以下是今年的一些亮点，它们都利用了 Google Research 多年来的努力，以实现研究的“魔力循环”。\n\n## 医疗健康领域的 AI 进展：MedGemma 和 AMIE\n\n自 2022 年首次推出 Med-PaLM 以来，Google 研究团队持续推进 AI，以使医疗健康更易于获取和有效。\n\n*   **MedGemma**：\n    *   Google 在 I/O 大会上宣布了 MedGemma，这是其在多模态医学文本和图像理解方面最强大的开放模型。\n    *   它基于 Gemma 3，旨在作为开发者构建健康应用（如分析放射影像或总结临床数据）的起点。\n    *   其小尺寸使其能高效地针对特定需求进行微调。\n    *   在 MedQA 基准测试中，其在临床知识和推理任务上的基线性能与大得多的模型相似。\n    *   MedGemma 是开放的，可在开发者首选环境（包括 Google Cloud Platform 或本地）中运行。\n    *   MedGemma 4B 和 27B（仅文本）模型现已作为健康 AI 开发者基础（HAI-DEF）的一部分，在 HuggingFace 和 Vertex Model Garden 上提供。\n    *   ![MedGemma 的基线性能](https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-2-CostPerformance.width-1250.png)\n        *   *MedGemma 在临床知识和推理任务上的基线性能与大得多的模型相似。*\n*   **AMIE**：\n    *   与 Google DeepMind 合作开发的 AMIE 也在 I/O 上被重点提及。\n    *   AMIE 是一种用于医疗诊断对话的研究型 AI 代理。\n    *   新的多模态版本能够智能地解释和推理视觉医学信息，帮助临床医生实现更准确的诊断。\n\n## 学习领域的 AI 进展：LearnLM\n\n近两年来，Google Research 和 Google 内部团队一直与教育专家合作开发 LearnLM，这是一个针对学习进行微调的模型家族。\n\n*   **LearnLM 在 Gemini 2.5 中可用**：\n    *   在 I/O 上宣布，LearnLM 将直接在 Gemini 2.5 中提供，使其成为全球领先的学习模型。\n    *   最新技术报告显示，Gemini 2.5 Pro 在学习科学原理方面优于其他模型，是教育工作者的首选。\n    *   它具有先进的 STEM 推理、多模态理解、测验和评估能力等。\n    *   ![LearnLM 的教学法](https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-3-Pedagogy.width-1250.png)\n*   **新的 Gemini 测验体验**：\n    *   Google Research 团队帮助设计和优化了 Gemini 中的新测验体验。\n    *   学生（18 岁以上）可以要求 Gemini 根据他们的课堂笔记或课程文档创建自定义测验，并提供关于正确和错误答案的反馈和解释。\n*   **LearnLM 提示指南与合作**：\n    *   探索 LearnLM 提示指南，以最大化 Gemini 的教学价值，例如，要求它扮演生物老师或调整文本难度。\n    *   Google 正在与合作伙伴合作，将 LearnLM 模型的强大功能引入教育环境，例如与 Kayma 合作在加纳高中试点自动评估。\n\n## Gemma 的多语言和效率：让模型普惠可用\n\n作为 Google 使世界信息普遍可访问使命的一部分，Google 正在推进多语言研究，以确保 LLM 在不同语言中产生可靠输出。\n\n*   **Gemma3 的多语言能力**：\n    *   两个月前，Google 推出了 Gemma3，其研究帮助 Gemma 扩展到 140 多种语言，使其成为当今最佳的多语言开放模型。\n*   **Gemma3n 的效率**：\n    *   在 I/O 上宣布，这些功能现已通过 Gemmaverse 的最新成员 Gemma3n 提供，该模型只需 2GB 内存即可运行，专为设备上应用而构建。\n    *   效率方面的努力使 Gemma3n 模型能够降低延迟并更节能。\n*   **ECLeKTic 基准**：\n    *   Google Research 最近推出了 ECLeKTic，这是一个用于评估 LLM 中跨语言知识迁移的新型基准。\n\n## 高效且有根据的模型：助力搜索中的 AI 模式\n\n随着 LLM 规模的扩大和需求的增加，提高模型效率同时保持甚至提升其质量，是实现这些高性能模型普惠化的关键。\n\n*   **效率突破**：\n    *   Google Research 在效率方面取得了突破，例如在推测解码和级联方面的工作，这些已成为行业标准。\n*   **事实一致性与准确性**：\n    *   Google 发布了关于事实一致性技术和评估的研究，并通过双重检查和 FACTS Grounding 排行榜（与 Google DeepMind 和 Kaggle 合作发布）设定了事实性和依据性的标准。\n*   **AI 模式**：\n    *   Google 将其研究贡献给了搜索中的 AI 模式，以显著改善用户体验。\n    *   在 I/O 上宣布的 AI 模式是 Google 最强大的 AI 搜索，具有高级推理能力。\n    *   它正在向美国所有用户推出，允许人们通过后续问题和相关网站链接进行更深入的研究。\n    *   效率工作使模型运行更可靠，输出更快；事实性研究改进了 AI 模式的网页搜索方式，确保提供的答案高度准确并基于多个来源。\n\n## 多模态事实性：助力 Imagen4、Gemini 2.5 和 Vids 中的 AI 头像\n\n随着多模态内容的普及，Google 的事实性团队正在推进多模态事实性研究，以确保 Google 产品的高准确性标准。\n\n*   **Imagen4**：改进了 Gemini 应用中 Imagen4 的质量，该模型可提供逼真的视觉效果。\n*   **Vids 中的 AI 头像**：帮助评估模型和图像字幕的质量，使用户能够在几秒钟内创建带有 AI 头像的视频内容。\n*   **Gemini 2.5**：显著增强了 Gemini 2.5 模型的视频理解能力，特别是高运动理解，使其更能评估健康和健身领域的人体运动。\n\n## Sparkify：将任何问题转化为动画视频\n\nGoogle 团队支持推出了新的 Labs 实验 Sparkify。\n\n*   它结合了 Gemini、MusicLM、AudioLM 和 Veo 的强大功能。\n*   允许用户将任何问题或想法转化为选择设计风格的短小引人入胜的动画视频。\n*   该项目建立在底层模型及其事实性之上。\n\n## FireSat：实现更早地检测小型野火\n\n作为 Google 长期以来帮助减少野火破坏性影响的努力的一部分，Google Research 与 Earth Fire Alliance、Moore Foundation 和 Muon Space 合作开发了 FireSat。\n\n*   **FireSat 卫星星座**：\n    *   FireSat 是一个卫星星座，旨在实现更早、更准确的全球野火检测。\n    *   它使用高分辨率多光谱卫星图像和 AI 为急救人员提供近乎实时的洞察，并允许科学家和机器学习专家研究火灾蔓延。\n    *   3 月份，星座中的 50 多颗卫星中的第一颗已发射。\n*   **扩展现有工作**：\n    *   这项工作扩展了 Google 的野火边界跟踪（在搜索和地图中提供关键信息）和合成 Firebench 数据集（在 Google Cloud Platform 上发布以推进该领域的科学研究）。\n\n## 量子 AI：真实世界应用的切实潜力\n\n在 Dialogues 舞台上，WAYE 创始人 Sinead Bovell 和 Google 量子硬件团队高级总监 Julian Kelly 讨论了量子计算的前景以及仍需克服的工程和科学挑战。\n\n*   **量子 AI 进展**：\n    *   Julian 强调了 Google Research 量子 AI 团队的最新进展，包括 Willow 芯片和量子纠错等领域的进展。\n    *   经典计算机无法完成的计算可以在量子芯片上在几分钟内完成，为未来各种真实世界应用铺平道路。\n    *   彻底改变药物发现和能源效率等领域的潜力正变得越来越切实。\n*   **互动体验**：\n    *   Google 还为 I/O 现场的参与者创建了互动量子 AI 游戏体验：量子迷宫跑者。\n\n## AI 联合科学家：加速科学发现\n\nGoogle 的 AI 联合科学家（在 I/O 上提及，与 Google DeepMind 合作开发）是一个基于 Gemini 的多智能体系统。\n\n*   **功能与应用**：\n    *   它能够合成信息并执行复杂的推理任务。\n    *   它被设计为科学家的协作工具，帮助他们创建新颖的假设和研究提案，并加速生物医学发现。\n    *   它在急性髓系白血病的药物再利用和肝纤维化新治疗靶点的假设提出等领域展现了潜力。\n*   **其他科学研究加速努力**：\n    *   **地理空间推理**：旨在推进公共卫生、城市规划、综合业务规划、气候科学等。\n    *   **神经科学**：\n        *   LICONN：首个使用常用光学显微镜全面绘制脑组织中神经元及其连接的方法。\n        *   斑马鱼活动预测基准（ZAPBench）：首次允许研究人员调查整个脊椎动物大脑中结构连接和动态神经活动之间的关系。\n    *   **基因组学**：\n        *   REGLE：一种无监督深度学习模型，帮助研究人员发现与遗传变异的关联。\n        *   开源新的 DeepVariant 模型：作为个性化泛基因组参考合作的一部分，可将分析不同祖先基因组时的错误减少 30%。\n\n## 结论\n\n本文重点介绍的研究代表了 Google Research 团队正在进行的一些工作，这些团队正在推动各个领域的突破并将其变为现实。在这个研究的黄金时代，研究与现实世界应用之间的“魔力循环”越来越快，范围也越来越广，I/O 是展示这如何对人类、企业、科学和社会产生更大影响的绝佳机会。",
      "shortSummary": "Google I/O 2025 展示了 Google Research 的最新 AI 突破，将研究变为现实。亮点包括：医疗健康领域的 MedGemma 和 AMIE；教育领域的 LearnLM，使 Gemini 成为领先学习模型；Gemma 模型的跨语言和高效率；助力搜索中 AI 模式的改进；多模态事实性提升 Imagen4 和 Gemini 2.5；创意工具 Sparkify；用于早期野火检测的 FireSat 卫星；量子 AI 的实际应用潜力；以及通过 AI 联合科学家加速科学发现，涵盖地理空间、神经科学和基因组学等领域。",
      "translated_title": "Google 研究在 Google I/O 2025",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-2-CostPerformance.width-1250.png",
          "alt": "GRatIO25-2-CostPerformance",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-3-Pedagogy.width-1250.png",
          "alt": "GRatIO25-3-Pedagogy",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wrbp3\">Each year at Google I/O, we share some of Google’s most advanced technologies. We show how they can be helpful and provide new experiences, and how developers and other communities can use them to innovate. Many of these new technologies emerged from years of work within Google Research, many in collaboration with other teams, building on multiple successive breakthroughs in AI and other areas of computer science. This year’s I/O highlights the impact of bringing <a href=\"https://blog.google/technology/ai/io-2025-keynote/\" target=\"_blank\" rel=\"noopener noreferrer\">research to reality</a>. As Sundar put it: “What all this progress means is that we’re in a new phase of the AI platform shift. Where decades of research are now becoming reality for people, businesses and communities all over the world.”</p><p data-block-key=\"dbi8g\">In addition to Google Research’s contributions to Gemini and the generative AI products highlighted on the I/O stage, here are some of our favorites this year, tapping into years-long efforts from Google Research to realize the <a href=\"https://blog.google/technology/research/what-is-google-research/\" target=\"_blank\" rel=\"noopener noreferrer\">magic cycle</a> of research.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">MedGemma and AMIE: Advancing healthcare with AI</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">Since we first introduced <a href=\"https://www.nature.com/articles/s41586-023-06291-2\" target=\"_blank\" rel=\"noopener noreferrer\">Med-PaLM</a> in 2022, followed by <a href=\"https://www.nature.com/articles/s41591-024-03423-7\" target=\"_blank\" rel=\"noopener noreferrer\">Med-PaLM2</a> and <a href=\"https://research.google/blog/advancing-medical-ai-with-med-gemini/\">Med-Gemini</a>, our research teams have been continuously advancing AI to make healthcare more accessible and effective. At I/O, we announced <a href=\"https://blog.google/technology/developers/google-ai-developer-updates-io-2025/\" target=\"_blank\" rel=\"noopener noreferrer\">MedGemma</a>, Google’s most capable open model for multimodal medical text and image comprehension. It has the potential to speed up the development of new healthcare products.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wrbp3\">MedGemma is based on <a href=\"https://blog.google/technology/developers/gemma-3/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma 3</a> and designed to be a starting point for developers building health applications, such as analyzing radiology images or summarizing clinical data. Its small size makes it efficient for fine-tuning for specific needs, and when evaluated on the <a href=\"https://arxiv.org/pdf/2009.13081\" target=\"_blank\" rel=\"noopener noreferrer\">MedQA benchmark</a>, its baseline performance on clinical knowledge and reasoning tasks is similar to that of much larger models. Since MedGemma is open, it can be run in the developer’s preferred environment, including on the Google Cloud Platform or locally. Both MedGemma 4B and the 27B text-only models are <a href=\"https://developers.google.com/health-ai-developer-foundations/medgemma\" target=\"_blank\" rel=\"noopener noreferrer\">now available</a> on HuggingFace and Vertex Model Garden as part of our <a href=\"http://goo.gle/hai-def\" target=\"_blank\" rel=\"noopener noreferrer\">Health AI Developer Foundations</a> (HAI-DEF).</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --small\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-2-CostPerformance.width-1250.png\" alt=\"GRatIO25-2-CostPerformance\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-2-CostPerformance.width-1250.png\" alt=\"GRatIO25-2-CostPerformance\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"xpbds\"><i>MedGemma’s baseline performance on clinical knowledge and reasoning tasks is similar to that of much larger models.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wrbp3\">MedGemma follows our recent announcement about AMIE, developed in collaboration with Google DeepMind, that was also highlighted at I/O. <a href=\"https://research.google/blog/amie-a-research-ai-system-for-diagnostic-medical-reasoning-and-conversations/\">AMIE</a> is a research <a href=\"https://www.nature.com/articles/s41586-025-08869-4\" target=\"_blank\" rel=\"noopener noreferrer\">AI agent</a> for medical <a href=\"https://www.nature.com/articles/s41586-025-08866-7\" target=\"_blank\" rel=\"noopener noreferrer\">diagnostic conversations</a>. The new <a href=\"https://research.google/blog/amie-gains-vision-a-research-ai-agent-for-multi-modal-diagnostic-dialogue/\">multimodal version</a> can intelligently interpret and reason about visual medical information, helping clinicians towards more accurate diagnoses.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">LearnLM: Making Gemini the world’s leading model for learning</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">For nearly two years, our teams at Google Research and across Google have been collaborating with educational experts on <a href=\"https://storage.googleapis.com/deepmind-media/LearnLM/LearnLM_paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">LearnLM</a>, a family of fine-tuned models for learning. At I/O we announced that LearnLM will now be <a href=\"https://cloud.google.com/solutions/learnlm?e=48754805&amp;hl=en\" target=\"_blank\" rel=\"noopener noreferrer\">available</a> directly in Gemini 2.5, making it the world’s <a href=\"https://blog.google/outreach-initiatives/education/google-gemini-learnlm-update/\" target=\"_blank\" rel=\"noopener noreferrer\">leading model for learning</a>. Our latest <a href=\"https://storage.googleapis.com/deepmind-media/LearnLM/learnLM_may25.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">tech report</a> demonstrates that Gemini 2.5 Pro outperforms alternative models on learning science principles and is the preferred choice for educators. It has advanced STEM reasoning, multimodal understanding, quizzing and assessment capabilities, and much more.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n\n\n\n\n\n    <!-- Determine the appropriate width based on image_width -->\n    \n        \n    \n\n\n<!-- For mobile images, use a default width -->\n\n\n<picture class=\"media__image media__image\">\n    \n    \n        <source media=\"(min-width: 768px)\" srcset=\"https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-3-Pedagogy.width-1250.png\" alt=\"GRatIO25-3-Pedagogy\">\n    \n    <img src=\"https://storage.googleapis.com/gweb-research2023-media/images/GRatIO25-3-Pedagogy.width-1250.png\" alt=\"GRatIO25-3-Pedagogy\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n    \n</picture>\n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wrbp3\">We also launched a new <a href=\"https://blog.google/products/gemini/gemini-app-updates-io-2025/#quizzes\" target=\"_blank\" rel=\"noopener noreferrer\">quiz experience in Gemini</a>, which our Research team helped to design and optimize for learning. Students (ages 18+) can ask Gemini to create custom quizzes to help them study any topic, based on their class notes or course documents, and it will offer feedback and explanations about right and wrong answers.</p><p data-block-key=\"enldb\">Explore our <a href=\"https://services.google.com/fh/files/misc/learnlm_prompt_guide.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">LearnLM prompt guide</a> to maximize the pedagogical value of Gemini, for example, by asking it to act as a biology teacher or to adjust the difficulty level of text for a particular school grade.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --medium\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n            <div class=\"glue-ambient-video \">\n                <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/GRatIO25-4-Releveling.mp4\" type=\"video/mp4\">\n                </video>\n\n                <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n                    <div class=\"glue-ambient-video__tooltip\">\n                    <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                    <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                    </div>\n                    <div class=\"glue-ambient-video__icon\">\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                    </div>\n                </div>\n            </div>\n        \n    \n\n\n            </div>\n          </div>\n      \n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n\n\n    <p data-block-key=\"wrbp3\">As well as infusing pedagogy into Google products, we’re working with partners to bring the powerful capabilities of our LearnLM models to educational settings. Along with <a href=\"https://kayma.com/kayma-english.html\" target=\"_blank\" rel=\"noopener noreferrer\">Kayma</a>, we piloted the <a href=\"http://milgo.io/en\" target=\"_blank\" rel=\"noopener noreferrer\">automatic assessment</a> of both short and long-form content with thousands of students and educators in high schools in Ghana, and we’re working to scale to more students and countries.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Multilinguality and Efficiency in Gemma: Making our models accessible and useful for everyone</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">As part of Google’s mission to make the world’s information universally accessible, we are advancing research into multilinguality to ensure that LLMs produce reliable outputs in different languages and are truly useful for everyone around the world. Two months ago, Google introduced <a href=\"https://blog.google/technology/developers/gemma-3/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemma3</a>, and our research helped Gemma expand to over 140 languages, making it today’s best multilingual open model. At I/O, we announced that these capabilities are now available with the latest addition to the Gemmaverse, Gemma3n, a model that can run on as little as two gigabytes of RAM and is built for on-device applications. Our efforts around efficiency enable the Gemma3n model to reduce latency and be more energy consumption friendly.</p><p data-block-key=\"48b9m\">To help developers build and improve multilingual models, Google Research recently introduced <a href=\"https://research.google/blog/eclektic-a-novel-benchmark-for-evaluating-cross-lingual-knowledge-transfer-in-llms/\">ECLeKTic</a>, a novel benchmark for evaluating cross-lingual knowledge transfer in LLMs.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Efficient and grounded models: Contributing to AI Mode in Search</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">As LLMs grow larger and demand increases, our ability to improve model efficiency while maintaining and even elevating their quality determines our success in democratizing access to these high-performing models. Google Research has made breakthroughs in efficiency that have become industry standards, for example, our work on <a href=\"https://research.google/blog/looking-back-at-speculative-decoding/\">speculative decoding</a> and <a href=\"https://arxiv.org/pdf/2307.02764\" target=\"_blank\" rel=\"noopener noreferrer\">cascades</a>.</p><p data-block-key=\"d9btd\">We have published research on factual consistency <a href=\"https://arxiv.org/abs/2306.00186\" target=\"_blank\" rel=\"noopener noreferrer\">techniques</a> and <a href=\"https://arxiv.org/abs/2204.04991\" target=\"_blank\" rel=\"noopener noreferrer\">evaluations</a>, and set the bar on factuality and grounding with features like <a href=\"https://blog.google/products/gemini/google-gemini-new-features-july-2024/\" target=\"_blank\" rel=\"noopener noreferrer\">double-check</a> and the <a href=\"https://storage.googleapis.com/deepmind-media/FACTS/FACTS_grounding_paper.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">FACTS Grounding leaderboard</a>, released in collaboration with Google DeepMind and Kaggle. Now, we have contributed our research to <a href=\"https://blog.google/products/search/google-search-ai-mode-update/#ai-mode-search\" target=\"_blank\" rel=\"noopener noreferrer\">AI Mode</a>, to meaningfully improve the experience for users.</p><p data-block-key=\"3kd8n\">Announced at I/O, AI Mode is Google’s most powerful AI search yet with advanced reasoning capabilities. It is rolling out to all users in the U.S., allowing people to conduct deeper research with follow-up questions and links to relevant sites. Our work on efficiency enables the models to run more reliably and serve quicker outputs, and our factuality research has improved the way AI Mode searches the web, helping to ensure that the answers provided are highly accurate and grounded in multiple sources with relevant links.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Multimodal factuality: Contributing to Imagen4, Gemini 2.5, and AI Avatars in Vids</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">As multimodal content becomes ubiquitous, our factuality team is advancing research around multimodal factuality to ensure high accuracy standards across Google products. We improved the quality of <a href=\"https://blog.google/products/gemini/gemini-app-updates-io-2025/#imagen-4-veo-3\" target=\"_blank\" rel=\"noopener noreferrer\">Imagen4</a> on the Gemini app, the latest image model announced at I/O, which can deliver visuals with lifelike detail. For <a href=\"https://www.youtube.com/watch?v=X2G3utAQuAU\" target=\"_blank\" rel=\"noopener noreferrer\">AI avatars</a> in Vids, a new feature that enables users to create video content with their chosen AI avatars in a matter of seconds, we helped to evaluate the quality of the model and image captions. We also delivered significant enhancements to the video understanding capabilities of Gemini 2.5 models, specifically targeting high motion understanding so that Gemini is more capable of assessing human motion across health and fitness domains.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Sparkify: Turning any question into an animated video</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">Our teams helped support the <a href=\"https://blog.google/outreach-initiatives/education/google-gemini-learnlm-update/\" target=\"_blank\" rel=\"noopener noreferrer\">launch</a> of the new Labs experiment, <a href=\"https://sparkify.withgoogle.com/explore\" target=\"_blank\" rel=\"noopener noreferrer\">Sparkify</a>. Bringing together the power of Gemini, <a href=\"https://musiclm.com/\" target=\"_blank\" rel=\"noopener noreferrer\">MusicLM</a>, <a href=\"https://research.google/blog/audiolm-a-language-modeling-approach-to-audio-generation/\">AudioLM</a>, and <a href=\"https://deepmind.google/models/veo/\" target=\"_blank\" rel=\"noopener noreferrer\">Veo</a>, Sparkify allows users to turn any question or idea into a short and engaging animated video in the design style of their choice. The project builds on the underlying models and their factuality. <a href=\"http://sparkify.withgoogle.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Sign up for the waitlist</a> for a chance to try it out.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme- --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n    <div data-gt-id=\"media\" data-gt-component-name=\"\">\n        \n\n        \n\n\n\n\n        \n            \n                \n\n\n<div class=\"glue-video\">\n    <div class=\"glue-video__preview-container glue-video__preview-container--inline\">\n    <picture class=\"glue-video__preview\">\n      \n        <img class=\"glue-video__preview-image\" alt=\"Video preview image\" src=\"\" referrerpolicy=\"no-referrer\">\n      \n    </picture>\n    <div class=\"glue-video__info\">\n      <svg class=\"glue-icon glue-video__play-button glue-video__play-button--white\" role=\"presentation\">\n        <use href=\"/gr/static/assets/icons/glue-icons.svg#video-youtube\"></use>\n      </svg>\n      <div class=\"glue-video__label-container\">\n        <h2 class=\"glue-headline headline-5 glue-video__label\">Watch the film</h2>\n      </div>\n    </div>\n  </div>\n  <div class=\"glue-video__container glue-video__container--inline\" data-glue-yt-video-vid=\"lstWBbY-Iqk\">\n  </div>\n  <div class=\"glue-video__nojs\">\n    <p><a href=\"https://www.youtube.com/watch?v=lstWBbY-Iqk\">Link to Youtube Video</a></p>\n  </div>\n  \n</div>\n                \n            \n        \n    </div>\n\n\n\n    </div>\n</section>\n\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">FireSat: Enabling the detection of smaller wildfires earlier</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">As part of our long-established <a href=\"https://sites.research.google/gr/wildfires/\">efforts</a> to help reduce the devastating impacts of wildfires, Google Research has partnered with the <a href=\"https://www.earthfirealliance.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Earth Fire Alliance</a>, the <a href=\"https://www.moore.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Moore Foundation</a> and <a href=\"https://www.muonspace.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Muon Space</a> to develop <a href=\"https://sites.research.google/gr/wildfires/firesat/\">FireSat</a>. FireSat is a constellation of satellites built for earlier and more accurate global wildfire detection. It uses high-res multispectral <a href=\"https://blog.google/outreach-initiatives/sustainability/google-ai-wildfire-detection/\" target=\"_blank\" rel=\"noopener noreferrer\">satellite imagery and AI</a> to provide near real-time insights for first responders, and to allow scientists and ML experts to study fire propagation. In March, we launched the first of over 50 satellites in the constellation. This work expands upon our <a href=\"https://blog.google/outreach-initiatives/sustainability/google-wildfire-boundary-maps-europe-africa/\" target=\"_blank\" rel=\"noopener noreferrer\">wildfire boundary tracking</a>, which makes critical information available in Search and Maps, and the synthetic <a href=\"https://arxiv.org/abs/2406.08589\" target=\"_blank\" rel=\"noopener noreferrer\">Firebench dataset</a>, which we released on the <a href=\"https://github.com/google-research/firebench\" target=\"_blank\" rel=\"noopener noreferrer\">Google Cloud Platform</a> to advance scientific research in the field.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  \n\n\n\n\n\n  <div class=\"\n      dynamic_media\n      glue-grid__col\n      glue-grid__col--span-4-sm\n      glue-grid__col--span-12-md\n      glue-grid__col--span-12-lg\n      --\n      \n        --flex\n        --center\n      \n    \" data-gt-id=\"dynamic_media\" data-gt-component-name=\"\">\n  \n\n\n\n    <div class=\"glue-grid\n      \n        --remove-gap\n        --full\n      \">\n      \n          <div class=\"\n            dynamic_media__item\n            glue-grid__col\n            glue-grid__col--span-4-sm\n            \n                glue-grid__col--span-12-md\n            \n            \n                glue-grid__col--span-12-lg\n            \n          \">\n            <div class=\"\">\n              \n\n\n\n    \n        \n            <div class=\"glue-ambient-video \">\n                <video class=\"glue-ambient-video__container\" playsinline=\"\" muted=\"true\" loop=\"true\">\n                    <source src=\"https://storage.googleapis.com/gweb-research2023-media/media/GRatIO25-6-FireSat.mp4\" type=\"video/mp4\">\n                </video>\n\n                <div class=\"glue-ambient-video__button\" aria-label=\"Video Play/pause\">\n                    <div class=\"glue-ambient-video__tooltip\">\n                    <span class=\"glue-ambient-video__tooltip-play glue-label\">play silent looping video</span>\n                    <span class=\"glue-ambient-video__tooltip-pause glue-label\">pause silent looping video</span>\n                    </div>\n                    <div class=\"glue-ambient-video__icon\">\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-play\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#play-button\"></use>\n</svg>\n\n                    \n\n\n\n  \n\n<svg role=\"presentation\" aria-hidden=\"true\" class=\"glue-icon glue-icon--18px glue-ambient-video__icon-pause\">\n  <use href=\"/gr/static/assets/icons/glue-icons.svg#pause-button\"></use>\n</svg>\n\n                    </div>\n                </div>\n            </div>\n        \n    \n\n\n            </div>\n          </div>\n      \n      \n      <div class=\"glue-grid__col glue-grid__col--span-4-sm  glue-grid__col--span-12-md caption --center\">\n        <p data-block-key=\"xpbds\"><i>FireSat is the first satellite constellation for the early detection of wildfires in high resolution imagery.</i></p>\n      </div>\n      \n    </div>\n  </div>\n\n\n\n\n\n\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Quantum AI: Tangible potential for real-world applications</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">On the Dialogues Stage, Sinead Bovell, founder of WAYE, and Julian Kelly, Senior Director, from our Quantum Hardware team, discussed the promise of quantum computing and the engineering and scientific challenges that still need to be overcome. Julian highlighted the recent advances from Google Research's <a href=\"https://quantumai.google/\" target=\"_blank\" rel=\"noopener noreferrer\">Quantum AI</a> team, including our <a href=\"https://blog.google/technology/research/google-willow-quantum-chip/\" target=\"_blank\" rel=\"noopener noreferrer\">Willow chip</a> and progress in areas like <a href=\"https://research.google/blog/making-quantum-error-correction-work/\">quantum error correction</a>. Computations that are beyond reach for classical computers can be completed on a quantum chip in minutes, paving the way for various <a href=\"https://blog.google/technology/research/google-quantum-computer-real-world-applications/\" target=\"_blank\" rel=\"noopener noreferrer\">real-world applications</a> in the future. The potential to revolutionize fields like drug discovery and energy efficiency is becoming increasingly tangible.</p><p data-block-key=\"b55a5\">We also created an interactive Quantum AI game experience for attendees on the ground at I/O: the Quantum Maze Runner. Players had to race against the clock to complete the maze, and then see how the quantum computer would solve it.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">AI Co-Scientist: Accelerating scientific discovery</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">Our <a href=\"https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/\">AI co-scientist,</a> mentioned at I/O and developed in collaboration with Google DeepMind, is a multi-agent system based on Gemini that can synthesize information and perform complex reasoning tasks. It is designed as a collaborative tool for scientists, to aid them in creating novel hypotheses and research proposals, and to help accelerate biomedical discoveries. It has <a href=\"https://arxiv.org/abs/2502.18864\" target=\"_blank\" rel=\"noopener noreferrer\">demonstrated</a> potential in areas such as drug repurposing for acute myeloid leukemia and proposing hypotheses for novel treatment targets for liver fibrosis.</p><p data-block-key=\"biv0h\">It is one of our many efforts to <a href=\"https://blog.google/technology/research/google-research-scientific-discovery/\" target=\"_blank\" rel=\"noopener noreferrer\">accelerate scientific research</a> across the wider ecosystem. Our new <a href=\"https://research.google/blog/geospatial-reasoning-unlocking-insights-with-generative-ai-and-multiple-foundation-models/\">Geospatial Reasoning</a> initiative aims to advance public health, urban planning, integrated business planning, climate science and more. We’re also advancing neuroscience, with our recent publication on <a href=\"https://www.nature.com/articles/s41586-025-08985-1\" target=\"_blank\" rel=\"noopener noreferrer\">LICONN,</a> the first-ever method for using commonly available light microscopes to comprehensively map neurons and their connections in brain tissue, and with the release of the <a href=\"https://research.google/blog/improving-brain-models-with-zapbench/\">Zebrafish Activity Prediction Benchmark</a> (ZAPBench), which allows researchers to <a href=\"https://arxiv.org/abs/2503.02618\" target=\"_blank\" rel=\"noopener noreferrer\">investigate the relationship</a> between the structural wiring and dynamic neural activity across an entire vertebrate brain for the first time. We’re also advancing research into genomics to help diagnose rare diseases; <a href=\"https://www.nature.com/articles/s41588-024-01831-6\" target=\"_blank\" rel=\"noopener noreferrer\">REGLE</a> is an unsupervised deep learning model that helps researchers discover associations with genetic variants. And we open sourced new <a href=\"https://github.com/google/deepvariant/blob/r1.8/docs/pangenome-aware-wgs-vg-case-study.md\" target=\"_blank\" rel=\"noopener noreferrer\">DeepVariant models</a> as part of a collaboration on <a href=\"https://www.nature.com/articles/s41592-024-02407-2\" target=\"_blank\" rel=\"noopener noreferrer\">Personalized Pangenome References</a>, which can reduce errors by 30% when analyzing genomes of diverse ancestries.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Conclusion</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\">The research highlighted here represents some of the ongoing work done by the Google Research teams who are driving breakthroughs across a variety of fields and bringing them to reality. In this <a href=\"https://blog.google/technology/research/what-is-google-research/\" target=\"_blank\" rel=\"noopener noreferrer\">golden age of research</a>, the “magic cycle” between research and real-world application is increasingly faster and broader in scope, and I/O was a great opportunity to showcase how this leads to greater impact on people, businesses, science and society.</p>\n</div>\n\n    </div>\n</section>\n\n                    \n                    \n    \n\n\n<section class=\"component-as-block --no-padding-top --theme-light --dbl-padding\">\n    <div class=\"glue-page\">\n        \n  <div class=\"rich-text --theme-light --mode-standalone\" data-gt-id=\"rich_text\" data-gt-component-name=\"\">\n    \n\n\n    <div class=\"component-intro \">\n        \n            \n                <h2 class=\"\">Acknowledgements</h2>\n            \n        \n        \n    </div>\n\n\n\n    <p data-block-key=\"wrbp3\"><i>With thanks to the many teams and collaborators who have contributed to this blog and the work represented here.</i></p>\n</div>\n\n    </div>\n</section>\n\n                    \n                "
    }
  ],
  "lastUpdated": "2025-06-30T10:32:44.021Z"
}