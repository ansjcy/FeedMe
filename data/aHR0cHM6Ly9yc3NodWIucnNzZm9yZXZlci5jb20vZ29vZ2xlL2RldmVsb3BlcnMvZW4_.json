{
  "sourceUrl": "https://rsshub.rssforever.com/google/developers/en",
  "title": "Google Developers Blog",
  "description": "Google Developers Blog - Powered by RSSHub",
  "link": "https://developers.googleblog.com",
  "items": [
    {
      "title": "代理式体验：MCP 是您 AI 未来的正确工具吗？ (原标题: The agentic experience: Is MCP the right tool for your AI future?)",
      "link": "https://developers.googleblog.com/en/the-agentic-experience-is-mcp-the-right-tool-for-your-ai-future/",
      "pubDate": "Wed, 23 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-23T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 企业AI集成与Apigee的角色\n\n随着企业竞相将人工智能（AI）投入运营，挑战不仅在于构建和部署大型语言模型（LLM），还在于如何将其无缝集成到现有API生态系统中，同时保持企业级的安全性、治理和合规性。Apigee致力于在此旅程中引领企业，通过增强生成式AI代理的安全性、可扩展性和治理能力，从而简化其与应用程序的集成。\n\n### 模型上下文协议（MCP）的兴起与局限\n\n模型上下文协议（MCP）已成为集成离散API作为工具的事实标准方法。然而，MCP正在迅速演变，并且尚未完全满足企业对身份验证（AuthN）、授权（AuthZ）和可观测性（Observability）的需求。将API转化为代理式工具的旅程，远不止一个单一协议所能涵盖。\n\n### Apigee如何赋能企业级AI\n\nApigee作为Google Cloud的原生API管理平台，能够将您现有的企业API带入AI领域。Apigee承诺将继续引导企业在不断变化的AI格局中进行代理式转型，并确保为所有AI工作负载提供一流的企业级功能。\n\n#### 使MCP具备企业级能力\n\n在网络中利用MCP服务需要特定的安全约束。例如，您可能希望为MCP服务器本身添加身份验证，并根据消费应用程序授权对特定工具的访问。此外，您可能需要提供一流的可观测性信息，以跟踪哪些工具正在被谁使用。最后，您需要确保MCP服务器所提供的下游API也具备最低限度的安全保障。\n\nApigee提供了一个MCP服务器的开源示例，它精确地提供了这种类型的API安全性，并且所有这些功能现在都可用于您的MCP服务并得到支持。\n\n#### 通过Apigee API产品实现安全与治理\n\n此示例展示了如何利用Apigee的API产品对工具进行身份验证和授权控制。此外，最终位于MCP服务器背后的API（在此案例中部署到Cloud Run）本身也托管在Apigee上，因此它们获得了与Apigee上托管的其他任何API相同的安全性、分发和可观测性功能。这弥合了托管API与探索性AI交互之间的鸿沟，利用Apigee丰富的功能集来保护、扩展和治理您的AI旅程。此演示展示了如何立即启动并运行MCP服务器，同时提供所需的必要企业级控制。即使MCP标准发生变化，此设置也易于适应，因为它最终只是像其他任何后端一样通过Apigee提供服务。\n\n如下图所示，Apigee将API产品引入这些代理和MCP工具，并将其转化为AI产品。这些AI产品拥有自己的消费者和开发者，就像其他任何API一样。\n\n![API产品到AI产品](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image5_2ySlzKi.original.png)\n\n#### 参考架构与资源\n\nApigee的GitHub仓库提供了快速入门指南、示例工件和文档，将帮助您构建和部署Apigee中的参考MCP服务架构，并理解通过利用API产品将API暴露为AI代理工具的步骤。\n\n### 未来展望\n\nAI集成之旅将随着时间推移而不断调整和变化：MCP正在演变，例如从最初无身份验证到使用OAuth进行授权和资源服务的转变。Google Apigee致力于与这种演变同步发展。\n\n了解更多关于如何使用Apigee操作生成式AI应用程序，并查阅Apigee的AI策略文档。",
      "shortSummary": "Apigee帮助企业将AI代理（如LLMs）安全、可扩展地集成到现有API生态系统中。尽管模型上下文协议（MCP）是工具集成方法，但其缺乏企业级安全功能。Apigee提供了一个开源MCP服务器示例，利用其API管理平台为MCP服务提供认证、授权和可观测性，确保企业级安全与治理，并支持API产品化，从而简化AI集成并适应未来变化。",
      "translated_title": "代理式体验：MCP 是您 AI 未来的正确工具吗？",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image5_2ySlzKi.original.png",
          "alt": "API product to AI product",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "Apigee helps enterprises integrate large language models (LLMs) into existing API ecosystems securely and scalably, addressing challenges like authentication and authorization not fully covered by the evolving Model Context Protocol (MCP), and offering an open-source MCP server example that demonstrates how to implement enterprise-ready API security for AI agents."
    },
    {
      "title": "介绍 Opal：描述、创建和分享你的 AI 迷你应用 (原标题: Introducing Opal: describe, create, and share your AI mini-apps)",
      "link": "https://developers.googleblog.com/en/introducing-opal/",
      "pubDate": "Wed, 23 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-23T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 介绍 Opal：描述、创建和分享你的 AI 迷你应用\n\n**发布日期：** 2025年7月24日\n\nGoogle Labs 宣布推出 **Opal**，这是一款全新的实验性工具，旨在简化 AI 模型、提示和工具的整合，从而轻松构建和分享功能强大的 AI 应用。\n\n### Opal 是什么？\n\nOpal 是一个来自 Google Labs 的创新实验性工具，它允许用户通过简单的自然语言和可视化编辑，构建并分享强大的 AI 迷你应用。这些迷你应用能够将提示、AI 模型调用和各种工具串联起来，形成完整的工作流程。\n\n### 主要用途\n\nOpal 旨在加速 AI 想法和工作流程的实现，其主要用途包括：\n\n*   加速 AI 概念和工作流程的原型开发。\n*   通过功能性应用展示概念验证（PoC）。\n*   构建自定义 AI 应用以显著提高工作效率。\n\n### 可用性\n\nOpal 目前已在美国地区以**公开测试版**形式推出。Google Labs 强调，通过与社区的合作，能够更好地构建和完善这款新产品。\n\n### 核心功能（无需代码）\n\n尽管 Opal 仍处于实验阶段，但它已具备强大的功能，旨在将用户的想法变为现实，且无需编写代码：\n\n1.  **创建工作流程：**\n    *   Opal 简化并可视化了软件开发中“工作流程”的概念，即用户完成特定目标所采取的步骤序列。\n    *   它通过将提示、AI 模型调用和其他工具串联起来，帮助用户构建强大的多步骤应用。\n    *   用户只需描述逻辑，Opal 即可自动构建相应的可视化工作流程。\n\n2.  **进行编辑：**\n    *   Opal 将用户的指令转化为可视化工作流程，提供精细的控制，而无需用户接触任何代码。\n    *   用户可以通过对话式自然语言命令、可视化编辑器或两者结合的方式，构建和修改他们的 AI 迷你应用。\n    *   无论是调整某个步骤的提示、添加新功能还是调用工具，都可以通过可视化编辑器或直接描述变更来实现。\n\n3.  **分享应用：**\n    *   一旦应用构建完成并准备就绪，用户可以将其作为独立应用分享给他人。\n    *   其他用户可以使用自己的 Google 账户立即使用这些分享的应用。\n\n### 开始使用 Opal\n\n为了帮助用户快速上手，Opal 提供了一个**演示画廊**，其中包含各种入门模板。用户可以直接使用这些预构建的 AI 应用，也可以根据自身具体需求进行修改和定制。\n\n![Opal 模板画廊](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Just_Gallery.original.png)\n\n### 愿景\n\nOpal 引入了一种全新的 AI 创作方式。它旨在赋能创作者、创新者和实践者，通过将简单的提示转化为强大的迷你应用，从而构建他们所设想的工具。",
      "shortSummary": "Opal 是 Google Labs 推出的一款实验性工具，旨在帮助用户通过自然语言和可视化编辑，无需代码即可描述、创建和分享 AI 迷你应用。它能将提示、AI 模型和工具串联成工作流程，加速 AI 想法原型开发，并构建自定义应用以提升生产力。Opal 目前在美国提供公开测试版，并提供模板画廊以方便用户快速上手。",
      "translated_title": "介绍 Opal：描述、创建和分享你的 AI 迷你应用",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Just_Gallery.original.png",
          "alt": "Gallery of Opal templates",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "Opal is a new experimental tool from Google Labs that helps you compose prompts into dynamic, multi-step mini-apps using natural language, removing the need for code, allowing users to build and deploy shareable AI apps with powerful features and seamless integration with existing Google tools."
    },
    {
      "title": "AI人物播客第五季上线：认识塑造未来的人工智能建设者 (原标题: People of AI podcast Season 5 is here: Meet the builders shaping the future)",
      "link": "https://developers.googleblog.com/en/people-of-ai-podcast-season-5/",
      "pubDate": "Wed, 23 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-23T16:00:00.000Z",
      "creator": "Google",
      "summary": "## AI人物播客第五季：聚焦AI建设者\n\nGoogle的“AI人物播客”（People of AI podcast）第五季已于2025年7月24日回归，本季将深入探讨不断变化的AI世界，并带来一个全新的视角：聚焦AI的“建设者”（Builders）。这些建设者包括开发者、初创公司、创始人以及所有积极塑造AI未来的人。\n\n### 聚焦“建设者”的原因\n\nAI领域正在迅速发展，对“可能”的定义、开发者角色以及日常工作方式都在快速演变。几年前还在研究论文中的概念，如今已迅速成为数百万用户日常使用的产品和应用的核心部分。生成式AI已触手可及，并嵌入到用户喜爱的工具中，以科幻般的方式激发着人们的想象力。因此，本季播客将重点转向这些正在构建AI的“建设者”。\n\n### 首集亮点：探索AI前沿\n\n首集节目邀请到Google DeepMind研究副总裁Clement Farabet。节目深入探讨了AI的演变，从卷积神经网络（CNNs）到Transformer模型，再到大型语言模型（LLMs）的强大能力，以及这些进步如何为智能体系统（agentic systems）铺平道路。Clement Farabet以工程师、企业家和自称“疯狂科学家”的独特视角，分享了构建能够真正学习、行动、在现实世界中提供帮助并负责任的AI智能体的激动人心的可能性和关键挑战。\n\n### 第五季新变化\n\n除了新的焦点，本季播客主持人Ashley Oldacre将与资深播客主持人Christina Warren搭档，为对话带来深刻的技术见解和全新视角。他们将带领听众深入了解那些正在改变一切的初创公司、研究人员和行业领袖的幕后故事，揭示建设者们正在努力解决的问题以及AI如何融入解决方案。\n\n### 本季部分嘉宾预告\n\n本季将邀请多位行业专家和创新者，包括：\n\n*   **AI开发与教育：** 听取资深开发者Muhammad Farooq和Jerome Hardaway关于未来开发和AI教育的见解。\n*   **AI电影创作：** 了解导演Eliza McNitt和Google DeepMind的Veo团队（Ben Wiley, Kory Mathewson）如何创作出首部AI制作电影《ANCESTRA》的幕后创意过程。\n*   **AI助手未来：** 与Bibo Xu（Project Astra）和Jérôme Monceaux（Enchanted Tools）讨论AI助手的未来发展。\n*   **AI跨领域应用：** 探索创新者Max Child和James Wilsterman（Volley）、Andrew Carr（Cartwheel）以及Dr. Dhruv Joshi（Cloudphysician）如何将AI融入游戏、动画和医疗保健领域。\n\n### 播客目标\n\n“AI人物播客”第五季是了解推动AI革命的真实世界创新者的绝佳机会。他们的故事将激励听众进行创造性思考，克服挑战，并为负责任且有影响力的AI未来做出贡献。\n\n### 订阅与关注\n\n请在goo.gle/PAI订阅“AI人物播客”，并在LinkedIn和X上关注Google AI Developers，获取最新动态和幕后内容。",
      "shortSummary": "Google的“AI人物播客”第五季已上线，本季将聚焦AI的“建设者”，即塑造AI未来的开发者、初创公司和创始人。节目将深入探讨AI的快速演变，包括生成式AI和智能体系统，并邀请Google DeepMind的Clement Farabet等行业领袖分享见解。新主持人Christina Warren加入，共同揭示AI创新者如何解决现实问题，并探讨AI在各领域的应用，旨在激励听众为负责任的AI未来贡献力量。",
      "translated_title": "AI人物播客第五季上线：认识塑造未来的人工智能建设者",
      "images": [],
      "contentSource": "完整文章",
      "content": "Co-hosted by Ashley Oldacre and Christina Warren, People of AI podcast's Season 5 will focus on the builders in the space of AI, highlighting the unique journeys, challenges, and triumphs of these innovators."
    },
    {
      "title": "在 Firebase Studio 中为流行框架释放新的 AI 能力 (原标题: Unleashing new AI capabilities for popular frameworks in Firebase Studio)",
      "link": "https://developers.googleblog.com/en/new-ai-capabilities-for-popular-frameworks-in-firebase-studio/",
      "pubDate": "Tue, 22 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-22T16:00:00.000Z",
      "creator": "Google",
      "summary": "## Firebase Studio 推出新 AI 功能，赋能流行框架\n\nFirebase Studio 是一个基于云的 AI 工作空间，旨在帮助开发者构建和发布下一代 AI 驱动的应用程序。继之前推出多功能代理模式、模型上下文协议（MCP）基础支持以及与 Gemini CLI 集成之后，Google 在 I/O Connect India 大会上公布了多项新更新，旨在结合 Gemini 的强大功能与这些新特性，帮助开发者利用其喜爱的框架和语言，将想法快速转化为应用程序。\n\n这些新更新包括：\n\n*   **AI 优化模板加速代理开发**\n*   **与 Firebase 后端服务无缝集成**\n*   **通过分叉工作区更好地控制不断演进的代码库**\n\n选择在印度发布这些公告具有特殊意义，因为印度拥有全球最大的应用程序开发者社区之一。例如，Google Android 开发者专家 Pankaj Rai 利用 Firebase Studio 构建了学生指导平台 Mentor Sarthi，并将其个人造型应用 GlamMEAi 推广到网络平台，而无需学习网络开发。\n\n### 详细功能介绍：\n\n1.  **引入 AI 优化模板，助力极速构建**\n    *   Firebase Studio 更新了 Flutter、Angular、React、Next.js 和通用 Web 的工作区模板，新增了利用 Gemini 的 AI 功能，帮助开发者构建和部署美观、强大的应用程序。\n    *   使用 AI 优化模板创建项目时，工作区将默认进入自主代理模式，并采用重新设计过的布局，帮助开发者快速进入工作流。\n    *   开发者可以获得 Gemini 的协助，独立规划和执行任务，无需等待分步批准。同时，仍可根据任务需求切换到“询问”和“代理”模式，无论是生成整个应用程序、优化功能、运行测试、生成文档还是添加新功能。\n    *   每个模板中都添加了 `airules.md` 文件，为 Gemini 提供专门的指令，以指导代码生成并为每种特定类型的应用程序创建文档。这使得 Gemini 能更好地了解相关的编码标准、错误处理方法、依赖项和开发最佳实践。\n    *   未来几周，Firebase Studio 将更新 Go、Node.js、.NET 等更多流行框架和语言的模板。\n\n2.  **与 Firebase 服务无缝集成**\n    *   从即日起，开发者只需向 Gemini 发出提示，即可将 Firebase 后端服务集成到应用程序中。\n    *   如果使用 App Prototyping Agent 进行“氛围编码”或使用 AI 优化模板，只需告诉 Gemini 应用程序需要实现什么功能，它就会推荐相应的 Firebase 服务。\n    *   Gemini 将自动导入所需的库或包，更新代码以集成所需服务，并指导完成任何额外的设置或配置步骤，从而节省开发者的时间和点击次数。\n    *   所有这些功能均可免费开始使用。\n\n3.  **分叉工作区，赋能实验与协作**\n    *   现在，开发者可以分叉工作区，创建一份完全相同的副本，包括保留原始工作区中的所有代理聊天历史记录。\n    *   这提供了一个可靠的备份，可以随时恢复，让开发者能够自信地构建新功能和尝试新事物，而无需担心犯下不可逆转的错误。\n    *   分叉工作区还可用于调试和故障排除，而不会影响主开发工作区，并通过与他人共享“进行中”的工作区来使协作更加顺畅。\n\n4.  **增强提示功能并增加项目上传大小**\n    *   Firebase Studio 持续改进 AI 辅助开发体验，提供更大的控制和灵活性。\n    *   新增的“增强提示”功能允许开发者在 Firebase Studio 内与 Gemini 协作并完善其提示和应用程序想法。\n    *   项目上传大小已显著增加到 100MB，使得开发者可以将更大、更复杂的现有项目导入 Firebase Studio。\n\nFirebase Studio 致力于使 AI 辅助开发更加直观、快速和愉悦，无论开发者拥有数十年的编码经验还是完全没有经验，都能将他们最雄心勃勃的应用程序想法变为现实。",
      "shortSummary": "Firebase Studio 推出重大更新，旨在通过 AI 赋能应用开发。新功能包括：AI 优化模板，加速 Flutter、React 等框架的开发；与 Firebase 后端服务无缝集成，Gemini 可自动推荐并导入服务；支持分叉工作区，便于实验、备份和协作；以及增强提示功能和将项目上传大小提升至 100MB。这些更新旨在让 AI 辅助开发更直观、高效，帮助全球开发者将创意变为现实。",
      "translated_title": "在 Firebase Studio 中为流行框架释放新的 AI 能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "New AI capabilities for popular frameworks in Firebase Studio include AI-optimized templates, streamlined integration with Firebase backend services, and the ability to fork workspaces for experimentation and collaboration, making AI-assisted app development more intuitive and faster for developers worldwide."
    },
    {
      "title": "Gemini 2.5 Flash-Lite 现已稳定版发布并普遍可用 (原标题: Gemini 2.5 Flash-Lite is now stable and generally available)",
      "link": "https://developers.googleblog.com/en/gemini-25-flash-lite-is-now-stable-and-generally-available/",
      "pubDate": "Mon, 21 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-21T16:00:00.000Z",
      "creator": "Google",
      "summary": "## Gemini 2.5 Flash-Lite 稳定版发布\n\n2025年7月22日，谷歌正式发布了 Gemini 2.5 Flash-Lite 的稳定版本。作为 Gemini 2.5 模型家族中速度最快、成本最低（输入每百万代币0.10美元，输出每百万代币0.40美元）的模型，它的推出旨在推动“每美元智能”的边界，并提供可选的原生推理能力，以应对更严苛的用例。该模型与此前的 2.5 Pro 和 2.5 Flash 一起，完善了可用于大规模生产的 Gemini 2.5 模型系列。\n\n### 核心优势\n\nGemini 2.5 Flash-Lite 在性能和成本之间取得了平衡，同时不牺牲质量，尤其适用于对延迟敏感的任务，如翻译和分类。其突出特点包括：\n\n*   **一流的速度**：在广泛的提示样本上，Gemini 2.5 Flash-Lite 的延迟均低于 2.0 Flash-Lite 和 2.0 Flash。\n*   **成本效益**：它是目前成本最低的 2.5 模型，定价为每百万输入代币0.10美元，每百万输出代币0.40美元，使得处理大量请求变得经济实惠。此外，音频输入价格相比预览版降低了40%。\n*   **智能且小巧**：在编码、数学、科学、推理和多模态理解等广泛基准测试中，其整体质量均高于 2.0 Flash-Lite。\n*   **功能全面**：使用 2.5 Flash-Lite 进行开发，可获得100万代币的上下文窗口、可控的思考预算，并支持原生工具，如 Google 搜索接地（Grounding）、代码执行和 URL 上下文。\n\n### 模型能力对比\n\n![Gemini 2.5 Flash-Lite、2.5 Flash 和 2.5 Pro 的能力对比表](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/gemini_2-5_ga_family_1-1__dark.original.png)\n\n### Gemini 2.5 Flash-Lite 的实际应用\n\n自 2.5 Flash-Lite 发布以来，已有一些成功的部署案例：\n\n*   **Satlyt**：正在构建一个去中心化的空间计算平台，用于实时总结在轨遥测数据、自主任务管理和卫星间通信解析。2.5 Flash-Lite 的速度使其关键的在轨诊断延迟降低了45%，功耗比基线模型降低了30%。\n*   **HeyGen**：利用 AI 创建视频内容头像。他们使用 Gemini 2.5 Flash-Lite 自动化视频规划、分析和优化内容，并将视频翻译成180多种语言，为用户提供全球化的个性化体验。\n*   **DocsHound**：将产品演示转化为文档。通过 Gemini 2.5 Flash-Lite 处理长视频并以低延迟提取数千张截图，将素材快速转化为全面的文档和 AI 代理训练数据，远快于传统方法。\n*   **Evertune**：帮助品牌了解其在 AI 模型中的表现。Gemini 2.5 Flash-Lite 大幅加快了他们的分析和报告生成速度，使其能够快速扫描和合成大量模型输出，为客户提供动态、及时的洞察。\n\n### 如何开始使用\n\n您可以通过在代码中指定“gemini-2.5-flash-lite”来开始使用该模型。如果您正在使用预览版本，可以直接切换到“gemini-2.5-flash-lite”，它们是相同的底层模型。预览别名计划于8月25日移除。现在即可在 Google AI Studio 和 Vertex AI 中试用 Gemini 2.5 Flash-Lite 的稳定版本。",
      "shortSummary": "Gemini 2.5 Flash-Lite 稳定版已于2025年7月22日发布，是 Gemini 2.5 模型家族中速度最快、成本最低的模型（输入每百万代币0.10美元，输出0.40美元）。它在性能、成本和质量之间取得平衡，尤其适用于延迟敏感任务。该模型提供100万代币上下文窗口和原生工具支持，并在编码、推理等多方面表现出色。Satlyt、HeyGen、DocsHound 和 Evertune 等公司已成功将其应用于各自领域，显著提升了效率。用户可在 Google AI Studio 和 Vertex AI 中使用。",
      "translated_title": "Gemini 2.5 Flash-Lite 现已稳定版发布并普遍可用",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/gemini_2-5_ga_family_1-1__dark.original.png",
          "alt": "Comparative table showing capabilities of Gemini 2.5 Flash-Lite, 2.5 Flash, and 2.5 Pro",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "Gemini 2.5 Flash-Lite, previously in preview, is now stable and generally available. This cost-efficient model is ~1.5x faster than 2.0 Flash-Lite and 2.0 Flash, offers high quality, and includes 2.5 family features like a 1 million-token context window and multimodality."
    },
    {
      "title": "Gemini 2.5 的对话式图像分割 (原标题: Conversational image segmentation with Gemini 2.5)",
      "link": "https://developers.googleblog.com/en/conversational-image-segmentation-gemini-2-5/",
      "pubDate": "Sun, 20 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-20T16:00:00.000Z",
      "creator": "Google",
      "summary": "## Gemini 2.5 的对话式图像分割：深度理解视觉世界\n\n### AI视觉理解的演进\n\n人工智能对图像的视觉理解能力已取得巨大进步。最初，AI通过边界框识别物体“在哪里”；随后，分割模型能精确勾勒物体形状；近期，开放词汇模型允许使用“蓝色滑雪靴”或“木琴”等非预定义标签进行分割。然而，真正的挑战——对话式图像分割（与文献中的指称表达分割密切相关）——要求更深层次的理解，即解析复杂的描述性短语，例如识别“最远的那辆车”，而不仅仅是“一辆车”。\n\n如今，Gemini 2.5 凭借其先进的视觉理解能力，将对话式图像分割提升到新水平，能够“理解”用户要求其“看到”的内容。\n\n### 利用对话式图像分割查询\n\n这项功能的强大之处在于其支持的查询类型。通过超越简单的单词标签，用户可以以更直观、更强大的方式与视觉数据交互。以下是五类查询示例：\n\n1.  **物体关系**\n    Gemini 可以根据物体之间复杂的相互关系来识别它们，例如：\n    *   关系理解：“拿着雨伞的人”\n    *   排序：“从左数第三本书”\n    *   比较属性：“花束中最枯萎的花”\n\n2.  **条件逻辑**\n    用户可以使用条件逻辑进行查询，例如“素食食物”或带有否定词的查询，如“没有坐着的人”。\n    ![在办公室会议中，自然语言查询“没有坐着的人”被用来在两名站立的个体上叠加分割掩码。](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/people_seating.original.png)\n\n3.  **抽象概念**\n    Gemini 的世界知识在此类查询中发挥作用。它可以分割那些没有简单、固定视觉定义的抽象概念，如“损坏”、“一团糟”或“机会”。\n    ![在厨房台面上，自然语言分割叠加层根据抽象查询“需要清理的区域”突出显示了溢出物。](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/spill_seg.original.png)\n\n4.  **图像内文本**\n    当仅凭外观不足以区分物体的精确类别时，用户可以通过图像中存在的文字标签来指代它。这需要模型具备 OCR 能力，而这正是 Gemini 2.5 的优势之一。\n    ![在面包店环境中，模型使用自然语言分割在“开心果果仁蜜饼”上叠加掩码，根据图像内文本将其与附近的其他糕点区分开来。](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/baklava_seg.original.png)\n\n5.  **多语言标签**\n    Gemini 不受单一语言限制，可以处理多种不同语言的标签。\n    ![一盘食物上带有自然语言分割叠加层，识别出各种组成部分，模型根据提示“tous les objects en français”提供了相应的法语标签。](https://storage.com/gweb-developer-goog-blog-assets/images/breakfast_francais_2.original.png)\n\n### 对话式图像分割的实际应用\n\n这些查询类型能够开启新的使用场景：\n\n1.  **释放创造力：交互式媒体编辑**\n    这项能力改变了创意工作流程。设计师现在可以用语言指导软件，而不是使用复杂的选择工具，从而实现更流畅、直观的过程，例如要求选择“建筑物投下的阴影”。\n    ![公园的鸟瞰图展示了自然语言分割叠加层，识别出“建筑物的阴影”。](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/shadow_seg.original.png)\n\n2.  **构建更安全的世界：智能安全与合规监控**\n    在工作场所安全方面，需要识别的是情况而不仅仅是物体。通过“突出显示工厂车间未戴安全帽的员工”这样的提示，Gemini 将整个条件指令理解为一个单一查询，生成一个精确的、仅针对不合规个体的最终掩码。\n    ![在建筑工地，应用自然语言分割叠加层以识别“未戴安全帽的人”。](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/construction_seg.original.png)\n\n3.  **理赔的未来：细致的保险损害评估**\n    “损害”是一个具有多种视觉形式的抽象概念。保险理赔员现在可以使用“分割受天气损坏的房屋”等提示，Gemini 将利用其世界知识识别与该类型损害相关的特定凹痕和纹理，并将其与简单的反光或锈迹区分开来。\n    ![在住宅区的航拍照片中，自然语言分割被用来在每栋“受损房屋”上叠加掩码。](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/tornado3_seg.original.png)\n\n### 对开发者的意义\n\n1.  **灵活的语言**：超越僵化、预定义的类别。自然语言方法提供了灵活性，可以为特定行业和用户的“长尾”视觉查询构建解决方案。\n2.  **简化的开发者体验**：通过单一 API 即可在几分钟内开始构建。无需寻找、训练和托管单独的专业分割模型，降低了构建复杂视觉应用的门槛。\n\n### 立即开始构建\n\nGoogle 相信，将语言与视觉直接进行像素级连接将开启新一代智能应用。开发者可以通过 Google AI Studio 的交互式空间理解演示或 Python Colab 环境开始使用。更多信息可查阅 Gemini API 开发者指南。\n\n### 推荐最佳实践\n\n为获得最佳结果，建议遵循以下实践：\n*   使用 `gemini-2.5-flash` 模型。\n*   禁用思考集 (`thinkingBudget=0`)。\n*   遵循推荐的提示，并请求 JSON 作为输出格式，提供包含 2D 边界框、分割掩码和文本标签的 JSON 列表。\n\n### 致谢\n\n文章感谢 Weicheng Kuo、Rich Munoz、Huizhong Chen 等人在 Gemini 分割方面的工作，以及 Junyan Xu、Guillaume Vernade 和整个 Gemini 图像理解团队的贡献。",
      "shortSummary": "Gemini 2.5 推出了革命性的对话式图像分割功能，使AI能深度理解复杂的自然语言查询。它超越了传统分割，支持基于物体关系、条件逻辑、抽象概念、图像内文本和多语言标签的精确分割。这项技术极大地提升了交互式媒体编辑、智能安全监控和保险损害评估等领域的应用潜力。对于开发者而言，它提供了灵活的语言处理能力和简化的开发体验，通过单一API即可快速构建复杂的视觉应用。",
      "translated_title": "Gemini 2.5 的对话式图像分割",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/people_seating.original.png",
          "alt": "Within an office meeting, the natural language query \"the people who are not sitting\" is used to overlay segmentation masks on the two individuals who are standing.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/spill_seg.original.png",
          "alt": "On a kitchen counter, a natural language segmentation overlay highlights a spill in response to the abstract query, \"area that should be cleaned up\".",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/baklava_seg.original.png",
          "alt": "In a bakery setting, the model uses natural language segmentation to overlay masks on \"the pistachio baklava\" , distinguishing it from other nearby pastries based on in-image text.",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/breakfast_francais_2.original.png",
          "alt": "A plate of food has natural language segmentation overlays identifying various components, with the model providing corresponding labels in French as requested by the prompt \"tous les objects en français\".",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/shadow_seg.original.png",
          "alt": "An aerial view of a park demonstrates a natural language segmentation overlay identifying \"the shadow of the building\".",
          "title": "",
          "position": 5
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/construction_seg.original.png",
          "alt": "At a construction site, a natural language segmentation overlay is applied to identify \"the people not wearing a hard hat\".",
          "title": "",
          "position": 6
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/tornado3_seg.original.png",
          "alt": "In an aerial photo of a subdivision, natural language segmentation is used to overlay masks on each \"damaged house\".",
          "title": "",
          "position": 7
        }
      ],
      "contentSource": "完整文章",
      "content": "Gemini's advanced capability for conversational image segmentation allows intuitive interaction with visual data by understanding complex phrases, conditional logic, and abstract concepts, streamlining developer experience and opening doors for new applications in media editing, safety monitoring, and damage assessment."
    },
    {
      "title": "使用 Veo 3 进行构建，现已在 Gemini API 中推出 (原标题: Build with Veo 3, now available in the Gemini API)",
      "link": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
      "pubDate": "Wed, 16 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-16T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 使用 Veo 3 进行构建，现已在 Gemini API 中推出\n\nGoogle 于2025年7月17日宣布，通过 Gemini API 和 Vertex AI 向开发者提供 Veo 3 的付费预览。Veo 3 是 Google 在2025年 Google I/O 大会上首次亮相的视频模型，全球用户已使用它生成了数千万高质量视频。\n\n### Veo 3 的核心能力\nVeo 3 是 Google 首个集成高保真视频输出和原生音频的视频模型，目前支持文本到视频生成，并将很快支持图像到视频。\n\n### 开发者应用案例\n开发者已开始利用 Veo 3 进行实验，探索其在内容构思、快速迭代和提高效率方面的潜力：\n\n*   **Cartwheel:** 开发了一个系统，能将人类的2D视频转换为生产就绪的3D动画。Cartwheel 利用 Veo 3 生成逼真流畅的人体动作，然后将其转化为客户所需的3D动画。\n*   **Volley:** 使用 Veo 3 制作游戏内的过场动画以推进故事。Veo 3 帮助 Volley 的设计师快速迭代游戏内容，为即将推出的 RPG 游戏《Wit's End》提供最佳输出。\n\n### Veo 3 的主要功能\nVeo 3 旨在处理各种视频生成任务，从电影叙事到动态角色动画，不仅能生成令人惊叹的视觉效果，还能生成对话和音效等音频，从而创造更沉浸式的体验：\n\n*   **同步音频 (Synchronized Sound):** 原生生成丰富的音频（对话、效果和音乐），并与视频同步。\n*   **电影级质量 (Cinematic Quality):** 制作高质量、高清晰度的视频，捕捉提示中的创意细节，包括复杂的纹理和微妙的光影效果。\n*   **真实物理效果 (Realistic Physics):** 模拟真实世界物理，实现真实的运动，如自然的角色动作、准确的水流和阴影投射。\n\n### 获取与定价\n\n*   **可用性:** Veo 3 可通过 Gemini API（付费预览）和 Vertex AI 获得。它也在 Google AI Studio 中作为 SDK 模板和交互式 Starter App 提供，方便付费层级用户快速原型开发。此外，Google AI 订阅者可在 Gemini 应用和 Flow 中使用 Veo 3，企业客户则可通过 Vertex AI 访问。\n*   **定价:** 视频和音频输出的定价为每秒0.75美元。未来将推出 Veo 3 Fast，提供更快速、更经济的视频创作选项。\n*   **负责任的构建:** 所有由 Veo 3 模型生成的视频都将包含数字 SynthID 水印。\n*   **入门资源:** 开发者可以通过查阅文档、教程（cookbook）和 Veo 3 入门应用（付费层级）来开始使用。",
      "shortSummary": "Google已通过Gemini API和Vertex AI向开发者推出Veo 3的付费预览。Veo 3是首个集成高保真视频和原生音频的视频模型，支持文本到视频，并即将支持图像到视频。它提供同步音频、电影级质量和真实物理效果，帮助开发者快速迭代和高效创作。定价为每秒0.75美元，未来将推出更快的Veo 3 Fast。所有生成视频均含SynthID水印，可通过Google AI Studio等平台访问。",
      "translated_title": "使用 Veo 3 进行构建，现已在 Gemini API 中推出",
      "images": [],
      "contentSource": "完整文章",
      "content": "Veo 3, Google’s latest AI video generation model, is now available in paid preview via the Gemini API and Google AI Studio. Unveiled at Google I/O 2025, Veo 3 can generate both video and synchronized audio, including dialogue, background sounds, and even animal noises. This model delivers realistic visuals, natural lighting, and physics, with accurate lip syncing and sound that matches on-screen action."
    },
    {
      "title": "解锁 Gemini 的推理：Vertex AI 上 Logprobs 的分步指南 (原标题: Unlock Gemini’s reasoning: A step-by-step guide to logprobs on Vertex AI)",
      "link": "https://developers.googleblog.com/en/unlock-gemini-reasoning-with-logprobs-on-vertex-ai/",
      "pubDate": "Tue, 15 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-15T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 解锁 Gemini 的推理：Vertex AI 上 Logprobs 的分步指南\n\n本文详细介绍了如何在 Vertex AI 上的 Gemini API 中使用 Logprobs 功能，以深入理解模型的决策过程，并将其应用于构建更智能、更可靠的应用程序。\n\n## 什么是 Logprobs？\n\nLogprobs（对数概率）是模型分配给某个词元（token）的概率分数的自然对数。理解其关键概念有助于解释模型置信度：\n\n*   **概率**：介于 0 到 1 之间。\n*   **自然对数**：0 到 1 之间任何数的自然对数都是负数。\n*   **对数概率为 0**：表示 100% 的确定性（即概率为 1）。\n*   **解释**：Logprob 分数越接近 0，表示模型对其选择的置信度越高。\n\n## 环境设置\n\n在开始使用 Logprobs 之前，需要进行以下环境配置：\n\n1.  **安装 Google GenAI SDK**：使用 `pip install -U -q google-genai` 命令安装或升级 Python SDK。\n2.  **配置 Google Cloud 项目**：设置您的 `PROJECT_ID` 和 `location`（例如 `global`）以进行身份验证。\n3.  **初始化客户端和模型**：创建 `genai.Client` 实例，并指定要使用的 Gemini 模型（例如 `gemini-2.5-flash`）。\n\n## 启用和处理 Logprobs\n\n通过在请求的 `generation_config` 中设置以下两个参数来启用 Logprobs：\n\n*   `response_logprobs=True`：指示模型返回其输出中选择的词元的对数概率（默认为 `False`）。\n*   `logprobs=[integer]`：要求模型同时返回指定数量（1 到 20）的每个步骤中排名靠前的替代词元的对数概率。\n\n文章提供了一个示例，展示了如何使用 Logprobs 对句子进行分类，并提供了一个 `print_logprobs` 辅助函数来以可读格式打印结果，详细展示了模型对每个词元的预测及其替代词元。\n\n**输出解读示例**：\n\n如果模型返回 `Token: 'Neutral' (-0.0214)`，以及替代词元如 `'Positive': (-4.8219)` 和 `'Negative': (-5.6293)`，这表明模型对“Neutral”的置信度极高（-0.0214 非常接近 0），而对其他选项的置信度则低得多。\n\n## Logprobs 的应用场景\n\nLogprobs 不仅仅是一个调试工具，它能帮助开发者构建更智能、更可靠、更具上下文感知能力的应用程序。\n\n### 1. 更智能的分类\n\nLogprobs 将分类从简单的答案转变为透明的决策过程，从而构建更强大的系统：\n\n*   **检测歧义以进行人工审查**：当模型对分类结果的置信度不高时（例如，前两个选项的 Logprobs 差异很小），可以将其标记为需要人工审查。\n*   **基于置信度的阈值设置**：将所选词元的 Logprob 转换回原始概率分数（使用 `math.exp()`），然后根据预设的置信度阈值（例如 90%）决定是否接受分类结果。\n\n### 2. 动态自动补全\n\n通过反复查询模型并检查其顶级候选词元的 Logprobs，可以构建一个动态的自动补全功能，实时提供最相关的下一个词建议。\n\n文章通过模拟用户输入“The best thing about living in Toronto is the”来展示此功能。随着上下文的增加，模型对下一个词的预测变得越来越具体和准确。\n\n![自动补全分析](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image1_Gl9YFyT.original.png)\n\n**分析**：\n\n*   对于“The”这样的通用词，建议是通用的起始词。\n*   当上下文完整为“The best thing about living in Toronto is the”时，模型能自信地预测出多伦多的具体属性，如“diversity”（多样性）和“food”（美食），这展示了其深层的上下文理解能力。\n\n### 3. 定量 RAG 评估\n\nLogprobs 可以用于评估检索增强生成（RAG）系统，衡量其答案与检索到的上下文的一致性。\n\n当大型语言模型（LLM）拥有相关上下文时，其生成事实一致答案的置信度会提高，这反映在更高的 Logprobs 上。通过计算生成答案的平均 Logprob，可以获得一个“基础性”或“置信度”分数。\n\n文章设置了虚构的知识库，并测试了三种场景：良好检索、不良检索和无检索。\n\n![Logprobs 图表](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/logprobs_chart.original.jpg)\n\n**分析**：\n\n结果显示出清晰的相关性。“良好检索”场景的得分最高（最接近零），因为模型对其答案得到文本支持的置信度很高。这使得 Logprobs 成为自动化评估和改进 RAG 系统的强大指标。\n\n## 展望\n\nLogprobs 功能为开发者提供了深入了解模型决策过程的能力，并可应用于分析分类结果、构建动态自动补全功能和评估 RAG 系统。这只是该功能众多应用场景中的一小部分，鼓励开发者进一步探索。更多信息可查阅入门笔记本和 Gemini API 官方文档。",
      "shortSummary": "本文介绍了 Vertex AI 上 Gemini API 中新推出的 Logprobs 功能。该功能允许开发者查看模型选择的词元及其替代词元的概率分数，从而深入理解模型的推理过程。文章详细讲解了 Logprobs 的概念、启用方法，并展示了其在智能分类（检测歧义、置信度阈值）、动态自动补全和定量 RAG 评估等关键应用场景中的实际价值，帮助开发者构建更智能、更可靠的 AI 应用。",
      "translated_title": "解锁 Gemini 的推理：Vertex AI 上 Logprobs 的分步指南",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image1_Gl9YFyT.original.png",
          "alt": "auto-complete analysis",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/logprobs_chart.original.jpg",
          "alt": "logprobs_chart",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "The `logprobs` feature has been officially introduced in the Gemini API on Vertex AI, provides insight into the model's decision-making by showing probability scores for chosen and alternative tokens. This step-by-step guide will walk you through how to enable and interpret this feature and apply it to powerful use cases such as confident classification, dynamic autocomplete, and quantitative RAG evaluation."
    },
    {
      "title": "斯坦福大学的Marin基础模型：首个使用JAX开发的完全开放模型 (原标题: Stanford’s Marin foundation model: The first fully open model developed using JAX)",
      "link": "https://developers.googleblog.com/en/stanfords-marin-foundation-model-first-fully-open-model-developed-using-jax/",
      "pubDate": "Tue, 15 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-15T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 斯坦福大学Marin基础模型：首个使用JAX开发的完全开放模型\n\n## 引言：扩展AI模型“开放”的定义\n\n当前AI时代，强大的基础模型被开放共享，加速了创新。斯坦福大学CRFM（基础模型研究中心）的Marin项目旨在扩展“开放”的定义，使其涵盖模型背后的整个科学过程。该项目被设计为一个“开放实验室”，目标不仅是共享模型本身，还要公开完整的研发历程，包括代码、数据集、数据方法、实验、超参数和训练日志。这种高水平的透明度通过提供一个独特、完全可复现的资源，补充了现有生态系统，使研究人员能够深入审查、构建和信任所开发的模型。Marin项目致力于为基础模型研究营造一个更透明、更易于访问的未来。\n\n![AI模型开放性谱系](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/ai-model-openness-spectrum-stanford-marin-open-.original_4oMY0az.png)\n\n## 核心发布与技术栈\n\n该“开放实验室”的首批发布是Marin-8B-Base和Marin-8B-Instruct模型。秉承项目原则，模型、数据、代码和分词器均在宽松的Apache 2.0许可下发布。实现完全可复现性是一个艰巨的工程问题，需要控制大规模分布式系统中的每一个变异源。项目的成功取决于一个能够在大规模下提供可复现性保证，并最大限度提高效率以训练具有领先性价比的基础模型的技术栈。团队选择了JAX作为基础，并构建了一个新的框架——Levanter，以充分利用JAX的强大功能。\n\n## 构建开放基础模型的核心挑战与解决方案\n\n为了成功创建真正开放、可扩展和可复现的基础模型，CRFM团队解决了以下几个工程挑战：\n\n### 1. 在单个加速器上实现最大速度\n\n*   **问题**：核心训练循环执行数十亿次，Python等解释型语言的开销会造成巨大的性能瓶颈。操作逐步调度还会导致过多的内存流量和开销，尤其是在TPU等硬件上，吞吐量取决于高效执行融合操作。\n*   **解决方案**：\n    *   Levanter将整个多阶段训练步骤（前向传播、损失、反向传播和更新）封装成一个单一函数，并使用`@jax.jit`装饰器。JAX的XLA编译器将整个过程转换为一个高度优化的机器码内核，融合操作以最大限度地利用硬件。\n    *   使用`jax.value_and_grad`在一次传递中计算损失及其梯度，避免冗余计算。\n    *   JAX还支持梯度检查点等高级技术，节省内存并允许使用更大的批次大小。\n    *   Levanter还使用了JAX强大的Pallas-based Splash Attention内核，这是几乎所有大型语言模型核心的关键操作——点积注意力的高度优化实现。\n\n### 2. 管理大规模并行计算的复杂性\n\n*   **问题**：训练最先进的模型需要扩展到数千个加速器芯片。手动管理模型和数据的分区以及设备间的通信极其复杂，代码很快变得难以阅读、调试和适应。\n*   **解决方案**：\n    *   JAX的`@jax.jit`装饰器无缝支持单程序多数据（SPMD）并行化，自动化底层数据分片和通信。XLA编译器自动调度加速器之间的通信，最大限度地减少网络等待时间，最大化计算时间。\n    *   为了使`jit`的功能更易于安全使用，Levanter开发了Haliax库，用于命名张量。通过使用人类可读的名称（如“embed”或“batch”）而不是位置索引来引用张量轴，代码变得自文档化且健壮。\n    *   这种抽象允许通过更改配置文件中的几行代码，轻松定义和修改复杂的分片策略，如完全分片数据并行（FSDP）和张量并行，而无需触及模型代码。\n\n### 3. 构建和管理弹性、高成本效益的计算集群\n\n*   **问题**：大规模训练需要灵活访问大型计算集群。项目严重依赖抢占式TPU实例来管理成本，这意味着需要一种方法来轻松地将许多较小、分散的TPU切片组合成一个逻辑集群，并对频繁中断具有弹性。\n*   **解决方案**：\n    *   利用Google Cloud TPU Multislice技术，允许训练作业使用多个TPU切片，如同一个大型系统。\n    *   Levanter使用Ray来编排此过程，在训练作业期间无缝扩展或缩减TPU切片数量，并确保在任何单个切片被抢占时作业仍保持弹性。\n    *   得益于JAX和XLA，Levanter和Marin在GPU上也获得了类似的高性能结果。\n\n### 4. 通过完美可复现性培养科学信任\n\n*   **问题**：Marin项目的核心目标是实现可验证的科学。这要求即使在训练暂停、重启或在不同机器配置之间移动时也能获得可复现的结果，这是一个重大的技术障碍。\n*   **解决方案**：\n    *   这是驱动Levanter设计的根本要求。项目选择JAX正是因为其强大的可复现性保证，例如其默认使用确定性伪随机数生成器（PRNGs）。\n    *   Marin-8B的训练验证了这一选择，该训练涉及在不同TPU切片和硬件类型之间迁移，同时在抢占发生时成功保持了逐比特的可复现性。\n    *   Levanter还包括一个基于Google Tensorstore库的健壮数据加载系统。Levanter的数据存储提供对任何批次训练数据的确定性、随机访问，无论作业重启或数据源更改如何，这对于支持中期训练等高级训练策略至关重要。JAX的确定性和Levanter的数据存储也使可解释性研究人员能够轻松理解特定数据在训练期间如何影响模型。\n\n### 5. 创建一个内聚的框架\n\n*   **问题**：虽然JAX提供了强大的引擎，但没有现有高级框架能满足项目对可读性、大规模可扩展性和逐比特确定性的严格综合要求。需要一个完整、有主见的系统来编排整个训练过程。\n*   **解决方案**：\n    *   从头开始构建了Levanter，一个JAX原生的框架，旨在成为所需系统：逐比特确定性、可扩展并具有高级分布式策略，以及弹性。\n    *   JAX不仅仅是一个库，它是一个用于构建新工具的“元框架”。Levanter建立在其成熟、高性能的TPU支持以及高级抽象（jit）与低级控制（Pallas）的无缝集成之上。\n    *   这种方法在JAX社区中很常见，该社区已经产生了Flax、Equinox、Orbax和Optax等充满活力的库生态系统，它们协同工作，使像Marin这样的团队能够构建强大的解决方案。\n\n## Marin-8B的训练历程：“Tootsie”过程\n\n上述原则、工具和库在Marin-8B的训练运行中得到了实施和应用。模型架构是Llama风格的Transformer。\n\n![Marin 8B-Base模型架构一览](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Marin-8B-Base-model-architecture_2.original.png)\n\nMarin-8B的训练是一个自适应的旅程，而非静态、单一的运行，内部称之为“Tootsie”过程。这一真实研究工作流程的诚实描绘已公开。该过程涵盖了超过12万亿个token，涉及多个阶段，适应了新数据、技术，甚至不同的硬件配置——在训练过程中在大型、多切片TPU配置（2x v5e-256到1x v4-2048 pods）之间迁移。团队持续优化数据混合，纳入更高质量的来源，并调整学习率和批次大小等超参数以优化性能。这种“混乱”的现实是一个强大的教育工具，而JAX和Levanter堆栈在处理这些重大转变同时保持逐比特可复现性的能力，有力地证明了其鲁棒性。\n\n## 加入Marin社区\n\nMarin项目是参与基础模型开发和为JAX生态系统做出贡献的公开邀请。Marin的旅程回答了“开放的下一步是什么？”这个问题。这项创建“开放实验室”的努力得益于JAX生态系统的技术能力。其性能、可移植性和为可复现性而设计的底层架构是使研究“完整旅程”可访问的关键要素。\n\n通过共享从数据方法到训练日志的一切，项目旨在提供一个完全可复现的资源——一个赋能研究人员深入审查、构建和信任AI工作的资源。项目相信这是迈向AI更透明未来的协作一步。邀请您加入这个“开放实验室”——使用Marin，为研究做出贡献，并帮助构建下一波创新和值得信赖的基础模型。\n\n项目的核心资源是官方网站marin.community。在那里，您可以找到Hugging Face上发布的模型，在GitHub上探索“开放实验室”，阅读Marin文档，并深入了解Levanter训练框架。您还可以在Colab中通过一个简单的推理示例试用Marin。活跃的讨论正在Discord频道中进行，您可以在那里直接与其他开发者互动。对于JAX生态系统的新手，官方JAX文档提供了优秀的资源，包括快速入门指南。",
      "shortSummary": "斯坦福大学的Marin项目发布了首个使用JAX开发的完全开放基础模型，旨在将AI模型的“开放”定义扩展至整个科学研究过程。该项目发布了Marin-8B模型和Levanter框架，强调代码、数据和训练日志的完全可复现性与透明度。JAX的强大功能解决了速度、并行、集群管理和确定性复现等核心挑战，实现了“开放实验室”愿景。此举旨在促进AI研究的信任、协作与透明化，并邀请社区参与贡献。",
      "translated_title": "斯坦福大学的Marin基础模型：首个使用JAX开发的完全开放模型",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/ai-model-openness-spectrum-stanford-marin-open-.original_4oMY0az.png",
          "alt": "The Spectrum of AI Model Openness",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Marin-8B-Base-model-architecture_2.original.png",
          "alt": "Marin 8B-Base model architecture at a glance",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "The Marin project aims to expand the definition of 'open' in AI to include the entire scientific process, not just the model itself, by making the complete development journey accessible and reproducible. This effort, powered by the JAX framework and its Levanter tool, allows for deep scrutiny, trust in, and building upon foundation models, fostering a more transparent future for AI research."
    },
    {
      "title": "使用 ADK 和 Gemini CLI 简化你的 Agent ‘氛围构建’流程 (原标题: Simplify your Agent \"vibe building\" flow with ADK and Gemini CLI)",
      "link": "https://developers.googleblog.com/en/simplify-agent-building-adk-gemini-cli/",
      "pubDate": "Tue, 15 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-15T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 使用 ADK 和 Gemini CLI 简化 Agent 开发流程\n\n本文介绍了 Agent Development Kit (ADK) 的最新更新，以及其与 Gemini CLI 结合使用如何显著简化和加速 AI Agent 的开发过程，旨在消除开发中的摩擦，提升开发者的“直觉编程”（vibe coding）体验。\n\n### 消除开发摩擦\n\n对于开发者而言，进入“心流”状态至关重要，而文档阅读、框架选择和上下文切换等因素常常会打断这种状态。ADK 与 Gemini CLI 的结合旨在解决这些痛点，通过在开发者与 Gemini 之间建立无缝对话，将高层级想法迅速转化为功能性 Agent，从而让开发者能将更多时间用于解决实际问题，而非从零开始编写代码。\n\n### 核心机制：优化的 llms-full.txt 文件\n\n此次升级的核心是 ADK 仓库中经过改进的 `llms-full.txt` 文件。该文件可视为 ADK 框架的精简指南，详细列出了所有组件、功能和最佳实践。其主要改进包括：\n\n*   **文件大小缩减**：长度缩短了 50% 以上，减少了 token 消耗。\n*   **LLM 友好性**：更易于大型语言模型（LLMs）理解。\n\n这些改进使得 Gemini CLI 能够**全面理解 ADK 框架**，而不会占用过多的上下文窗口或出现“上下文腐烂”问题。这显著降低了成本，并提高了代码生成的准确性。Gemini CLI 获得了对框架的深度原生理解，能够将高层级计划直接转化为准确、符合习惯的多 Agent 代码，从而**大幅加速原型开发速度**，使开发者能够快速实验和迭代。\n\n### 实战演示：构建 AI GitHub 问题标签 Agent\n\n文章通过一个实际案例展示了 ADK 和 Gemini CLI 的强大功能：构建一个 AI Agent 来自动标记 GitHub 问题，以便于优先级排序和分配。整个过程仅需几个简单步骤：\n\n1.  **步骤 0：下载 llms-full.txt**\n    *   将 `llms-full.txt` 文件从 `adk-python` 仓库下载到工作目录。\n\n2.  **步骤 1：与 Gemini CLI 构思计划**\n    *   开发者向 Gemini CLI 描述构建 AI Agent 的高层级目标（例如，使用 ADK 框架和 Gemini 为 GitHub 仓库问题打标签）。\n    *   Gemini CLI 随即生成一个详细的四阶段计划：\n        *   **阶段 1：项目设置与配置**：初始化项目、安装依赖（`google-adk`、`PyGithub`）、配置 GitHub 认证（PAT）。\n        *   **阶段 2：开发 GitHub 自定义工具**：创建 `github_tools.py` 文件，包含 `get_issue`、`get_available_labels` 和 `apply_label` 等函数，供 Agent 调用。\n        *   **阶段 3：构建标签 Agent**：在 `main.py` 中定义 Agent，配置 Gemini 模型（如 `gemini-1.5-flash`），集成自定义工具，并编写核心指令（prompt）指导 Agent 的工作流程（读取问题 -> 获取标签 -> 分析并选择 -> 应用标签）。\n        *   **阶段 4：创建应用程序入口点**：在 `main.py` 中创建简单的 CLI 接口，接受仓库名和问题编号作为输入，触发 Agent 执行标签任务。\n    *   在此阶段，开发者无需编写任何代码，即可定义完整的逻辑。\n\n3.  **步骤 2：将计划转化为 ADK Agent 代码**\n    *   开发者指示 Gemini CLI 使用 `llms-full.txt` 生成基于上述计划的 Python ADK 代码。\n    *   Gemini CLI 立即生成了完整的、功能性的 Agent 应用程序代码，包括 `labeling_agent` 的定义（使用 `google.adk.agents`、`google.genai` 等）和 `github_tools.py` 中的具体函数实现。文章提到还生成了 `requirements.txt` 和 `label_config.py`。\n    *   由于 `llms-full.txt` 提供了必要的上下文，CLI 自动处理了所有样板代码。\n\n4.  **步骤 3 & 4：在心流中测试和改进**\n    *   代码生成后，开发者可以立即进行本地测试。如果发现 bug 或有改进想法（例如，添加新的标签或在 Agent 完成后生成摘要），可以直接向 Gemini CLI 提出修改请求。\n    *   Gemini CLI 会在保持上下文的情况下重构代码，无需手动编辑，从而保持开发者的心流状态。\n\n5.  **步骤 5：循环迭代**\n    *   新的工作流程形成了一个高效的迭代循环：\n        1.  构思 Agent 逻辑。\n        2.  使用 Gemini CLI 生成 ADK 代码。\n        3.  即时测试。\n        4.  通过简单的对话请求进行改进。\n    *   这种循环允许开发者持续优化 Agent，而不会受到传统开发中常见的摩擦干扰。\n\n### 总结\n\nADK 与 Gemini CLI 的结合提供了一种前所未有的快速、流畅的 Agent 开发体验。通过利用优化的 `llms-full.txt` 文件，Gemini CLI 能够深度理解 ADK 框架，将高层级想法迅速转化为可运行的代码，并支持在开发过程中进行无缝的迭代和改进，从而让开发者能够更专注于解决实际问题。",
      "shortSummary": "ADK (Agent Development Kit) 与 Gemini CLI 的结合显著简化了 AI Agent 开发。通过优化的 `llms-full.txt` 文件，Gemini CLI 能深度理解 ADK 框架，将高层级想法迅速转化为准确的 Agent 代码。这加速了原型开发，并支持在开发过程中通过对话进行无缝迭代。开发者能将更多精力放在解决实际问题上，而非繁琐的编码工作，从而实现更流畅、高效的“直觉编程”体验。",
      "translated_title": "使用 ADK 和 Gemini CLI 简化你的 Agent ‘氛围构建’流程",
      "images": [],
      "contentSource": "完整文章",
      "content": "The updated Agent Development Kit (ADK) simplifies and accelerates the process of building AI agents by providing the CLI with a deep, cost-effective understanding of the ADK framework, allowing developers to quickly ideate, generate, test, and improve functional agents through conversational prompts, eliminating friction and keeping them in a productive \"flow\" state."
    }
  ],
  "lastUpdated": "2025-07-27T10:30:43.932Z"
}