{
  "sourceUrl": "https://rsshub.rssforever.com/google/developers/en",
  "title": "Google Developers Blog",
  "description": "Google Developers Blog - Powered by RSSHub",
  "link": "https://developers.googleblog.com",
  "items": [
    {
      "title": "Gemini for Home：拓展平台，开启智能家居AI新纪元 (原标题: Gemini for Home: Expanding the Platform for a New Era of Smart Home AI)",
      "link": "https://developers.googleblog.com/en/gemini-for-home-expanding-the-platform-for-a-new-era-of-smart-home-ai/",
      "pubDate": "Tue, 30 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-30T16:00:00.000Z",
      "creator": "Google",
      "summary": "# Gemini for Home：拓展平台，开启智能家居AI新纪元\n\nGoogle Home平台产品管理总监Ravi Akella于2025年10月1日宣布推出“Gemini for Home”，标志着智能家居领域的一个关键时刻。这项变革性举措将Gemini引入智能家居设备，取代了扬声器和显示器上的Google Assistant，为摄像头带来了更高水平的智能，并全面改进了Google Home应用。对于生态系统中的数万名开发者而言，这预示着一个充满新机遇的时代。\n\n## 平台扩展的两大核心方向\n\nGoogle在2025年Google I/O大会上曾分享了将Gemini引入Home API的计划。现在，Google Home平台通过以下两种关键方式进一步扩展了这一愿景：\n\n### 1. 为现有集成设备赋能Gemini新功能\n\n*   **广泛兼容性**：通过“Works with Google Home”计划，已有超过8亿台设备通过Google Home云到云API和Matter连接到Google生态系统。Gemini的强大功能将惠及所有这些设备。\n*   **核心对话优势**：此次平台升级的核心对话功能无需开发者进行额外工作。用户将能够使用更自然、更具对话性的语言来控制设备并创建复杂的自动化。\n*   **开发者行动呼吁**：随着Gemini for Home本月开始向用户推出，开发者必须立即对现有的“Works with Google Home”集成进行全面测试，以确保设备在新对话功能下的可靠性。Google将在今年晚些时候提供改进的数据和工具，帮助开发者监控设备性能和可靠性。\n\n### 2. 推出下一代AI摄像头构建计划\n\n*   **扩大选择范围**：为将Gemini的力量带给所有人，Google将“Works with Google Home”的承诺提升到新水平，通过新增一类合作伙伴设备来扩展平台，为客户提供更多选择。\n*   **摄像头制造商专属计划**：Google为摄像头制造商推出了一个专用计划，作为Google Home平台的一部分。这不仅是一套指导方针，更是一个完整的工具包，旨在帮助开发者构建下一代AI摄像头。\n*   **全面的开发工具**：\n    *   **硬件参考设计**：提供新的硬件参考设计，包括推荐的系统级芯片（SoCs）、图像传感器以及来自批准供应商列表（AVL）的其他组件。\n    *   **嵌入式SDK**：推出新的设备端Google Home摄像头嵌入式SDK。该SDK在Nest和合作伙伴摄像头设备中通用，提供一流的设备端机器学习（ML）框架，以统一的方式处理实时视图和事件历史，并与Google Home的摄像头和智能云协同工作。这使得这些产品比之前仅支持实时视图的“Works With Google Home”（WWGH）摄像头更加先进。\n*   **智能架构**：Google的家庭智能架构采用分体式计算模型，其中设备端ML向摄像头云提供关键帧和元数据，摄像头云随后将这些信息与事件视频一起保存和优先处理，以支持Gemini AI描述等功能。\n*   **无缝集成**：参与此计划的开发者可以将其硬件与Gemini for Home摄像头功能以及整个Google Home生态系统无缝集成。\n*   **首批合作伙伴**：Google宣布Walmart成为首批合作伙伴，将推出新的onn有线室内摄像头和onn有线视频门铃。这些设备以实惠的价格提供强大的性能，包括高达1080p的实时视图分辨率和智能警报。由于它们由Google Home提供支持，用户可以在扬声器、智能显示器、手机、平板电脑、手表、电视、网页以及使用Google Home API构建的自有应用中，访问Gemini for Home中新的摄像头理解功能，并深度集成到整个Google Home生态系统中。\n\nGoogle Home致力于成为开发者构建家庭AI体验的最佳平台。Google邀请开发者共同构建未来真正有用、由AI驱动的智能家居。",
      "shortSummary": "Google于2025年10月1日推出“Gemini for Home”，旨在开启智能家居AI新纪元。Gemini将取代扬声器和显示器上的Google Assistant，为摄像头带来更高智能，并全面改进Google Home应用。该平台通过两大方式扩展：一是为现有8亿多设备赋能Gemini对话功能，要求开发者立即测试集成；二是推出针对AI摄像头制造商的全新计划，提供硬件参考设计和嵌入式SDK。Walmart作为首批合作伙伴，将推出Gemini驱动的智能摄像头。Google邀请开发者共同构建AI驱动的未来智能家居。",
      "translated_title": "Gemini for Home：拓展平台，开启智能家居AI新纪元",
      "images": [],
      "contentSource": "完整文章",
      "content": "Google Home is enabling new Gemini-powered features for our partners’ devices and launching a new program to help them build the next generation of AI cameras."
    },
    {
      "title": "使用 Gemini 解锁多光谱数据 (原标题: Unlocking Multi-Spectral Data with Gemini)",
      "link": "https://developers.googleblog.com/en/unlocking-multi-spectral-data-with-gemini/",
      "pubDate": "Tue, 30 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-30T16:00:00.000Z",
      "creator": "Google",
      "summary": "### 使用 Gemini 解锁多光谱数据\n\n本文介绍了如何利用 Google 的 Gemini 模型处理多光谱数据，为应用程序赋予“超人视觉”，从而以全新的方式理解世界。\n\n#### 什么是多光谱图像？\n*   **定义：** 与标准数字照片仅包含红、绿、蓝（RGB）三个波段不同，多光谱传感器能捕捉电磁波谱中多个不同波段的数据，包括人眼不可见的近红外（NIR）和短波红外（SWIR）等。\n*   **重要性：** 传统上，开发者主要处理 RGB 图像，但多光谱图像能提供更丰富、更深入的世界信息。\n\n#### 多光谱图像为何具有颠覆性？\n多光谱数据在多个领域具有显著优势：\n*   **植被健康：** 健康植物反射大量近红外光。通过分析近红外波段，可以比仅凭绿色照片更准确地评估作物健康状况或监测森林砍伐。\n*   **水体检测：** 水吸收红外光，这使得水体与陆地易于区分，可用于绘制洪泛区地图或分析水质。\n*   **烧伤痕迹：** 短波红外（SWIR）波段擅长穿透烟雾，识别野火后最近烧毁的区域。\n*   **材料识别：** 不同的矿物质和人造材料具有独特的光谱“指纹”，可用于从太空识别它们。\n\n#### Gemini 如何改变游戏规则？\n*   **历史挑战：** 过去，使用多光谱数据需要专门的工具、复杂的处理流程和定制的机器学习模型。\n*   **Gemini 的解决方案：** 借助 Gemini 模型原生的多模态能力，开发者可以利用其强大的推理引擎，通过一种简单技术来分析这些丰富的数据，无需定制训练的专业模型。\n\n#### 将不可见光映射到可见颜色：核心技术\nGemini 等大型多模态模型经过大量图像和文本数据的预训练，理解“红色汽车”或“绿色森林”等概念。关键在于将我们关心的不可见波段映射到 Gemini 已理解的 RGB 通道中。\n*   **方法：** 创建“伪彩色合成”图像。目的不是使其看起来自然，而是将科学数据编码成模型可以处理的格式。\n*   **三步流程：**\n    1.  **选择波段：** 根据具体问题，选择三个重要的光谱波段。\n    2.  **标准化和映射：** 将每个波段的数据缩放到标准的 0-255 整数范围，并将其分配给新图像的红、绿、蓝通道。\n    3.  **提供上下文提示：** 将新创建的图像传递给 Gemini，并**关键地**在提示中告知模型这些颜色代表什么。\n        *   这一步至关重要，它能实时教导模型如何解释自定义的新图像。\n\n#### 实际应用示例\nGemini 2.5 在遥感领域表现出色，能够成功理解并分类多光谱图像。\n\n*   **土地覆盖分类（EuroSat 数据集）：**\n    *   Gemini 2.5 成功将以下图像准确分类为“永久作物”、“河流”和“工业区”。\n        *   ![永久作物](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image2_pTsJOzo.original.png)\n        *   ![河流](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image8_wUdRThz.original.png)\n        *   ![工业区](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image15_oDEJX2h.original.png)\n\n*   **挑战性场景：河流识别**\n    *   **初始问题：** 一张河流图像，模型最初错误地将其分类为“森林”。\n        *   ![初始河流图像（误分类）](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image5_EpSEWlF.original.png)\n    *   **解决方案：** 引入并构建多光谱伪图像，并提供详细提示后，Gemini 2.5 成功识别为“河流”。模型的推理轨迹显示，它利用了多光谱输入，特别是 NDWI 图像，来推断这是水体。\n        *   ![多光谱输入后的河流图像（正确分类）](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image5_tbaVc1d.original.png)\n\n*   **挑战性场景：森林识别**\n    *   **初始问题：** 一张森林图像，模型最初根据蓝/绿区域将其分类为“海湖”。\n        *   ![初始森林图像（误分类）](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image6_vNtbRRw.original.png)\n    *   **解决方案：** 包含多光谱输入后，模型轻松将其分类为“森林”，推理轨迹显示它显著利用了额外的输入。\n        *   ![多光谱输入后的森林图像（正确分类）](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image2_WIk4nAJ.original.png)\n\n这些示例表明，额外的多光谱输入对于做出更好的决策至关重要。由于模型无需更改，因此可以以相同方式添加其他类型的输入。\n\n#### Gemini 的强大之处\n*   **降低门槛：** 这种方法极大地降低了开发者分析复杂卫星数据的门槛。\n*   **快速原型开发：** 可以在数小时而非数周内快速开发新应用，无需深入的遥感专业知识。\n*   **上下文学习：** 借助 Gemini 强大的上下文学习能力，开发者只需提供清晰的提示和自定义图像，即可动态指导模型解释不同的光谱数据，以完成从农业监测到城市规划的各种任务。\n*   **新时代：** AI 驱动的环境监测、精准农业和灾害响应时代已经到来。开发者可以利用 NASA Earthdata、Copernicus Open Access Hub 或 Google Earth Engine 等公共卫星数据源，开始训练应用程序以全新的视角看待世界。",
      "shortSummary": "Google 的 Gemini 模型使开发者能够轻松利用多光谱数据，超越传统 RGB 视觉，实现“超人视觉”。通过将不可见光谱波段映射到 RGB 通道并结合上下文提示，Gemini 2.5 能够分析复杂的卫星数据，用于植被健康、水体检测和材料识别等任务。这显著降低了遥感数据分析的门槛，加速了AI驱动的环境监测和精准农业等应用的开发。",
      "translated_title": "使用 Gemini 解锁多光谱数据",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image2_pTsJOzo.original.png",
          "alt": "image2",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image8_wUdRThz.original.png",
          "alt": "image8",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image15_oDEJX2h.original.png",
          "alt": "image15",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image5_EpSEWlF.original.png",
          "alt": "image5",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image5_tbaVc1d.original.png",
          "alt": "image5",
          "title": "",
          "position": 5
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image6_vNtbRRw.original.png",
          "alt": "image6",
          "title": "",
          "position": 6
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image2_WIk4nAJ.original.png",
          "alt": "image2",
          "title": "",
          "position": 7
        }
      ],
      "contentSource": "完整文章",
      "content": "Multi-spectral imagery, which captures wavelengths beyond human vision, offers a \"superhuman\" way to understand the world, and Google's Gemini models make this accessible without specialized training. By mapping invisible bands to RGB channels and providing context in the prompt, developers can leverage Gemini's power for tasks like environmental monitoring and agriculture."
    },
    {
      "title": "推出 Tunix：一个用于大型语言模型后训练的 JAX 原生库 (原标题: Introducing Tunix: A JAX-Native Library for LLM Post-Training)",
      "link": "https://developers.googleblog.com/en/introducing-tunix-a-jax-native-library-for-llm-post-training/",
      "pubDate": "Mon, 29 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-29T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 推出 Tunix：一个用于大型语言模型后训练的 JAX 原生库\n\nGoogle ML Frameworks 于 2025 年 9 月 30 日宣布推出 Tunix，这是一个全新的开源 JAX 原生库，专门用于大型语言模型（LLM）的后训练。Tunix 旨在弥补模型对齐方面的关键空白，为开发者和研究人员提供一个全面且易于使用的工具包，以实现大规模模型对齐。该库针对 TPU 性能进行了优化，尤其是在与 MaxText 结合使用时。\n\n![Tunix logo](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Tunix_logo.original.png)\n\n## Tunix 的核心优势\n\nTunix 提供了以下关键特性，以优化 LLM 的后训练体验：\n\n*   **完整的算法套件：** 在一个统一的库中，提供生产就绪的训练器，支持监督微调（SFT）、偏好调优、知识蒸馏以及 PPO、GRPO 和 GSPO 等高级强化学习方法。\n*   **“白盒”设计：** 赋予开发者对训练循环和其他后训练代码的完全控制权，无需通过多层抽象即可轻松定制。\n*   **无缝 JAX 集成：** 作为 JAX 原生库，Tunix 提供强大且易于使用的解决方案，用于对齐您已在使用的开源模型。\n\n## 初始版本提供的功能\n\nTunix 的初始版本提供了模块化且易于使用的 API，可无缝集成到 JAX 生态系统中，支持最常见的后训练工作流：\n\n1.  **监督微调 (SFT)：**\n    *   `PeftTrainer` 是模型无关的，支持全权重微调以及 LoRA 和 QLoRA 等流行的参数高效调优方法（通过与 `qwix` 库集成）。\n2.  **偏好调优：**\n    *   `DPOTrainer` 通过实现直接偏好优化（DPO）来简化对齐。这种强大的技术使用简单的偏好和拒绝响应数据集，无需训练和管理单独的奖励模型。\n3.  **强化学习 (RL)：** Tunix 提供了一套 RL 训练器，用于将模型行为与人类偏好和指令对齐：\n    *   `PPOLearner`：通过实现近端策略优化（PPO），提供 RLHF 的黄金标准 Actor-Critic 方法，对于复杂、序列任务（尤其是涉及工具使用的智能体工作流）至关重要。\n    *   `GRPOLearner`：提供高效的无批评者 RL 算法，实现组相对策略优化（GRPO），通过规范化一组生成响应的奖励来指导模型，而无需单独的批评者模型的复杂性和成本。\n    *   `GSPO-token`：GRPO 算法的变体，为调整令牌级别优势计算提供更好的灵活性，并能提高多轮 RL 训练的稳定性。\n4.  **知识蒸馏：**\n    *   `DistillationTrainer` 通过训练一个更小、更高效的“学生”模型来复制更大“教师”模型的输出，从而实现模型压缩。这对于在具有严格延迟或成本限制的生产环境中部署高性能模型至关重要。\n    *   Tunix 开箱即用支持以下蒸馏算法：\n        *   **基于 Logit 的蒸馏：** 利用教师模型的最终输出概率作为“软目标”来指导学生模型。\n        *   **注意力迁移：** 利用教师模型的注意力特征来指导学生模型。\n\n此外，Tunix 还提供 PyPI 包 (`pip install google-tunix`)、示例代码，并支持训练使用 LLM 进行推理并与外部环境交互的智能体。\n\n## 量化结果\n\nTunix 提供了多个 Python Notebooks 帮助用户上手。以下结果展示了 Tunix 的 GRPO 实现的有效性：\n\n*   在 GSM8K 数学推理基准测试中，使用 Tunix 对 Gemma 2 2B-IT 模型进行微调，pass@1 答案准确率相对提升约 12%。\n*   评估性能时使用了 pass@1（贪婪搜索）和 pass@5（多样性采样）两种方法，以衡量一次或五次尝试的正确性。\n*   评估重点关注三个关键指标：\n    *   **答案准确率：** 最终数值答案正确的预测百分比。\n    *   **答案（部分）准确率：** 模型答案在正确答案的 10% 范围内的更灵活指标。\n    *   **格式准确率：** 模型正确使用所需推理和答案令牌的样本百分比。\n\n![Tunix-table](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Tunix-table.original.png)\n\n基线 pass@1 准确率约为 52%，与 Eleuther 的 LM Eval Harness 报告的基础模型约 51% 的结果一致，证实了设置的有效性。尽管绝对准确率对提示格式敏感，但后训练带来的显著性能提升在不同设置下保持一致。\n\n## 研究人员和创新者的认可\n\nTunix 已获得领先学术实验室和 AI 初创公司的信任和使用：\n\n*   **Brandeis 大学计算机科学助理教授 Hongfu Liu：** 赞赏 Tunix 的“白盒”设计，使其团队能够完全控制训练循环，轻松修改和适应代码以满足特定的研究需求，加速迭代数据分析。\n*   **加州大学圣地亚哥分校助理教授 Hao Zhang（vLLM、Chatbot Arena 联合创始人）：** 强调 Tunix 允许直接在 JAX 上构建，利用 TPU 和轻松并行化。他认为 Tunix 是一个轻量级库，代码库清晰易管理，提供高水平的模型和超参数定制，且学习曲线平缓。\n*   **Precur AI 联合创始人兼 CTO Hanjun Dai：** 肯定 Tunix 设计的广度（涵盖 SFT、RL 和蒸馏），使其能够统一整个智能体开发堆栈。其与 JAX 和 TPU 生态系统的原生集成，以及与 Flax 和 Qwix 的轻松定制，使其成为一个强大且易于融入工作流的框架。\n\n## 社区与协作\n\nTunix 采取开放式开发，邀请社区参与、试用和贡献。您可以通过提供的表单表达参与意愿，例如开发新的智能体功能或环境、增强算法或建立研究伙伴关系。源代码、问题跟踪器、深入文档和讨论可在 GitHub 仓库和 tunix.readthedocs.io 上找到。此外，还准备了一系列 Python Notebooks 作为上手示例。文章还提及 MaxText 是一个高性能、高可扩展性的开源 LLM 库，用纯 Python/JAX 编写，面向 Google Cloud TPUs 和 GPUs 进行训练。",
      "shortSummary": "Google 推出 Tunix，这是一个 JAX 原生开源库，专为大型语言模型（LLM）的后训练和对齐而设计。它提供全面的算法套件，包括监督微调、偏好调优、强化学习（PPO, GRPO, GSPO）和知识蒸馏。Tunix 采用“白盒”设计，允许开发者完全控制和定制训练流程，并针对 TPU 性能优化，与 JAX 生态系统无缝集成。量化结果显示，它能显著提升模型性能，例如在 GSM8K 基准测试中将 Gemma 2 2B-IT 模型的准确率提升约 12%。该库已获得研究人员和行业合作伙伴的认可，并鼓励社区参与贡献。",
      "translated_title": "推出 Tunix：一个用于大型语言模型后训练的 JAX 原生库",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Tunix_logo.original.png",
          "alt": "Tunix logo",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/download.original.png",
          "alt": "download",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Tunix-table.original.png",
          "alt": "Tunix-table",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "完整文章",
      "content": "For developers and researchers in the JAX ecosystem, the path from a pre-trained model to a fully al..."
    },
    {
      "title": "Gemma 详解：EmbeddingGemma 架构与开发方法 (原标题: Gemma explained: EmbeddingGemma Architecture and Recipe)",
      "link": "https://developers.googleblog.com/en/gemma-explained-embeddinggemma-architecture-and-recipe/",
      "pubDate": "Sun, 28 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-28T16:00:00.000Z",
      "creator": "Google",
      "summary": "# Gemma 详解：EmbeddingGemma 架构与开发方法\n\n本文深入探讨了 EmbeddingGemma 模型的架构及其开发方法。EmbeddingGemma 是 Gemma 系列模型中的一员，旨在将文本转化为数值表示（即“嵌入”），从而实现搜索、检索增强生成（RAG）和文本理解等任务。\n\n## EmbeddingGemma 的起源与演变\n\nEmbeddingGemma 并非从零开始构建，而是基于预训练的 300M 参数 Gemma 3 模型。它通过 T5Gemma 的适配方法进行转换，将原始的仅解码器 Gemma 模型转变为编码器-解码器架构。EmbeddingGemma 随后从这个新模型的编码器部分进行初始化，从而继承了大量“世界知识”而无需额外的预训练。\n\n![从 Gemma 3 解码器到强大的文本嵌入器](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/training.original.png)\n*图示：从 Gemma 3 解码器到强大的文本嵌入器的演变过程*\n\n## 嵌入的形成过程\n\nEmbeddingGemma 利用 Sentence Transformers 等框架生成嵌入。给定一段输入文本序列，EmbeddingGemma 通过一系列精心设计的步骤将其处理成一个简洁的向量表示。这些步骤包括：\n\n1.  **Transformer (编码器)**：输入序列首先通过一个仅编码器的 Transformer 模型。该 Transformer 利用双向注意力机制理解输入上下文中每个 token 的含义，为每个 token 生成一个 768 维的向量序列。\n2.  **池化 (Pooling)**：Transformer 的输出是一系列 token 嵌入。池化层的作用是将这个可变长度的序列转换为整个输入的单个固定大小的嵌入。EmbeddingGemma 采用“均值池化”（Mean Pooling）策略，即计算所有 token 嵌入的平均值。\n3.  **全连接层 (Dense)**：接下来，应用一个线性投影层，将嵌入维度从 768 扩展到 3072。\n4.  **全连接层 (Dense)**：然后，再应用另一个线性投影层，将学习到的 3072 维嵌入缩放到最终目标维度 768。\n5.  **归一化 (Normalize)**：最后，应用欧几里得归一化，以实现高效的相似性比较。这是一种比 Gemma 其他模型中使用的 RMSNorm 更简单、成本更低的操作。\n\n![EmbeddingGemma 嵌入形成过程](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/embeddinggemma.original.png)\n*图示：EmbeddingGemma 如何从输入文本形成最终嵌入的视觉化表示*\n\n## EmbeddingGemma 的学习机制\n\nEmbeddingGemma 通过在训练过程中优化三种不同的加权损失函数组合来学习创建强大的嵌入：\n\n1.  **噪声对比估计 (NCE) 损失**：\n    *   教授模型相似性和对比的基本概念。\n    *   拉近“正样本对”（查询与正确答案）在嵌入空间中的距离。\n    *   推远“负样本对”（查询与不正确答案）的距离。\n    *   关键在于包含“难负样本”（语义上与查询相似但不正确或不完整的答案），这迫使模型学习区分正确与近似正确的细微差别。\n2.  **全局正交正则化器 (GOR)**：\n    *   鼓励 EmbeddingGemma 生成在嵌入空间中分布更广的嵌入。\n    *   这使得嵌入对量化具有鲁棒性，并能通过近似最近邻 (ANN) 算法在向量数据库中实现高效搜索。\n3.  **几何嵌入蒸馏 (Geometric Embedding Distillation)**：\n    *   这是一种知识蒸馏形式，EmbeddingGemma 从一个更大、更强大的 Gemini Embedding 模型（作为教师模型）中学习。\n    *   通过最小化两个嵌入模型在查询和段落嵌入之间的 L2 距离，EmbeddingGemma 有效地继承了教师模型的知识和能力。\n\n![EmbeddingGemma 训练中使用的三种损失函数](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/loss.original.png)\n*图示：EmbeddingGemma 训练中使用的三种不同损失函数（NCE 损失、GOR 和蒸馏损失）的视觉化表示*\n\n通过结合这些损失函数，EmbeddingGemma 学习生成结构良好、富有表现力且鲁棒的表示，从而在实际搜索和检索任务中实现强大的性能。\n\n## 俄罗斯套娃表示学习 (Matryoshka Representation Learning, MRL)\n\nMRL 是一种允许在较大表示中嵌套较小、高质量表示的技术。例如，尽管 EmbeddingGemma 的嵌入具有 768 维，但您可以截断嵌入以获得 512、256 甚至 128 维的较小嵌入，同时保持高质量。\n\n在训练期间，损失函数不仅应用于最终的 768 维嵌入，还应用于该嵌入的重叠子集（前 512、256 和 128 维）。这确保了即使是完整嵌入的截断版本也是强大而完整的表示。\n\n![俄罗斯套娃嵌入](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/mrl.original.png)\n*图示：俄罗斯套娃嵌入，展示了不同维度如何影响质量与效率之间的权衡*\n\n这意味着用户可以根据其应用程序的需求，在性能和效率之间选择合适的权衡，而无需训练或管理多个模型。\n\n## 开发方法\n\nEmbeddingGemma 的开发过程包括几个阶段：\n\n*   **编码器-解码器训练**：如前所述，适配 Gemma 3，并使用 UL2 在 Gemma 3 数据上进行进一步预训练。\n*   **预微调 (Pre Fine-tuning)**：在大量、多样化的任务（问答、句子相似性、代码检索、网络搜索）和语言上进行训练，使用更大的批次大小和（查询，目标）对，不包含难负样本。\n*   **微调 (Fine-tuning)**：在更小、更高质量的特定任务数据集混合上精炼模型，使用难负样本和更小的批次大小。任务混合率通过贝叶斯优化进行优化。\n*   **模型融合 (Model Souping)**：通过平均微调运行中模型的参数来组合模型，以提高质量和鲁棒性。最终模型是来自多个不同微调混合的检查点的未加权平均。\n*   **量化感知训练 (Quantization-Aware Training, QAT)**：在微调期间应用，以提供量化版本（例如，int4 每块和混合精度每通道），这些版本具有更小的内存占用，同时质量下降最小。\n\n通过精心适配强大的基础模型并采用多方面的训练方法，EmbeddingGemma 的架构旨在提供高效且多功能的文本表示，适用于广泛的应用。\n\n## 总结与展望\n\n本文详细介绍了 EmbeddingGemma 的架构、其起源、嵌入生成过程以及开发方法。EmbeddingGemma 等模型正在推动语义技术向更高效、更强大的方向发展，预计将在检索增强生成 (RAG)、设备端 AI 和超个性化等关键领域带来显著进步。模型的权重已在 Hugging Face、Kaggle 和 Vertex AI 上发布。",
      "shortSummary": "EmbeddingGemma 是一个基于 Gemma 3 模型的文本嵌入模型，旨在将文本转化为数值嵌入，应用于搜索、检索增强生成（RAG）等任务。其核心架构包含 Transformer 编码器、均值池化和多层全连接网络，并采用欧几里得归一化。模型通过噪声对比估计、全局正交正则化和几何嵌入蒸馏三种损失函数进行训练。它还支持俄罗斯套娃表示学习，允许在不同维度下保持高质量。多阶段开发方法确保了其高效和多功能性，推动了语义技术的发展。",
      "translated_title": "Gemma 详解：EmbeddingGemma 架构与开发方法",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/training.original.png",
          "alt": "training",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/embeddinggemma.original.png",
          "alt": "embeddinggemma",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/loss.original.png",
          "alt": "loss",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/mrl.original.png",
          "alt": "mrl",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "完整文章",
      "content": "The previous posts in the “Gemma explained” series provided a detailed overview of the Gemma model f..."
    },
    {
      "title": "利用AG-UI将ADK智能体与精美前端结合，提升用户体验 (原标题: Delight users by combining ADK Agents with Fancy Frontends using AG-UI)",
      "link": "https://developers.googleblog.com/en/delight-users-by-combining-adk-agents-with-fancy-frontends-using-ag-ui/",
      "pubDate": "Thu, 25 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-25T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 通过将ADK智能体与AG-UI的精美前端结合来取悦用户\n\n在生成式AI应用开发领域，过去一年取得了飞速进展。后端智能体系统在推理、规划和执行复杂任务方面展现出惊人的能力。然而，一个强大的智能体只是成功的一半，如何将其智能无缝连接到应用程序前端，并创建真正协作的用户体验，一直是开发者面临的挑战。\n\n### 解决方案：ADK与AG-UI的集成\n本文重点介绍了Agent Development Kit (ADK) 与 AG-UI 的新集成。AG-UI是一个用于构建丰富、交互式AI用户体验的开放协议。这种结合使开发者能够轻松构建复杂的后端AI智能体，并为其提供生产就绪、协作式的前端。\n\n### 您的智能体后端：ADK\nADK（智能体开发工具包）是一个开箱即用的开源工具包，旨在构建能够“做事”而非仅仅“聊天”的AI智能体。它抽象了智能体工程中最困难的部分，让开发者能够专注于应用程序的独特逻辑。\n\nADK为您的智能体提供以下核心能力：\n*   **多步规划：** 能够推理问题，将其分解为多个步骤并按顺序执行。\n*   **工具使用：** 与外部API、服务和数据源无缝集成，赋予智能体真实世界的能力。\n*   **状态管理：** 强大的进度和记忆跟踪，无需从头构建复杂的链式逻辑。\n\n通过ADK，您可以在数小时内将想法转化为可工作的智能体原型，同时完全控制智能体的角色及其可访问的工具。尽管ADK提供了本地开发和调试UI（`adk web`），但它并非面向用户的界面。此前，任何UI与智能体的集成都是定制化的，缺乏标准化的通信方式。\n\n### 连接前端：AG-UI\nCopilotKit团队发布了AG-UI，这是一个开放协议和UI层，旨在标准化智能体与用户通过丰富UI直接对话的方式。AG-UI专注于直接面向用户的智能体（而非后台智能体），并利用中间件和客户端集成来泛化支持任何前端和后端。它提供了一种标准化的方式，让后端智能体与前端应用程序通信，从而实现AI与人类用户之间的实时、有状态的协作。\n\nAG-UI的创建者还提供了CopilotKit，这是一个开源库，包含可直接使用的React组件和Hooks，它们与AG-UI协同工作。这意味着您可以在几分钟内让应用程序运行起一个精致的聊天界面、侧边栏及其他UI元素。\n\n![ADK + AG UI Blog Sep 25 2025](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/ADK__AG_UI_Blog_Sep_25_2025.original.png)\n\n### 强强联合：ADK + AG-UI 集成解锁的功能\n通过将强大的后端（ADK）与灵活的前端协议（AG-UI）结合，您现在可以构建真正交互式的AI应用程序。ADK运行智能体的“大脑”和工具编排，而AG-UI则提供与UI组件的通信通道，以实现无缝的用户体验。\n\n此集成开箱即用，解锁了一系列新功能：\n*   **生成式UI：** 超越文本。您的智能体可以直接在聊天中生成和渲染UI组件，提供丰富的上下文信息和操作。\n*   **共享状态：** 前端和后端智能体共享对应用程序状态的共同理解，允许智能体响应UI中的用户操作，反之亦然。\n*   **人机协作（Human-in-the-Loop）：** 用户可以在智能体执行操作之前对其进行监督、批准或纠正，有助于确保安全性和控制。\n*   **前端工具：** 赋能智能体直接与前端交互，例如代表用户填写表单、导航页面或批注文档。\n\n### 开始构建：快速入门指南\n入门非常简单，只需运行一个命令即可克隆一个全栈启动仓库，其中包含预配置的ADK后端和使用CopilotKit的Next.js前端：\n\n`npx copilotkit@latest create -f adk`\n\n获得启动项目后，核心逻辑也很简单：\n\n1.  **在后端定义您的智能体（`/backend/agent.ts`）：**\n    在ADK后端中，您定义智能体的指令并为其提供工具。例如，一个获取股票价格的工具。\n    ```javascript\n    // backend/agent.ts\n    import { adk } from \"@copilotkit/adk\";\n    adk.setSystemMessage(\n      \"You are a helpful assistant that can fetch stock prices.\"\n    );\n    adk.addTool(\"getStockPrice\", {\n      description: \"Get the current stock price for a given ticker symbol.\",\n      parameters: {\n        type: \"object\",\n        properties: {\n          ticker: { type: \"string\", description: \"The stock ticker symbol (e.g., GOOGL).\" },\n        },\n        required: [\"ticker\"],\n      },\n      handler: async ({ ticker }) => {\n        console.log(`Fetching stock price for ${ticker}...`);\n        const price = (Math.random() * 1000).toFixed(2);\n        return `The current price of ${ticker} is $${price}.`;\n      },\n    });\n    export default adk;\n    ```\n\n2.  **连接前端（`/frontend/src/app/page.tsx`）：**\n    在React/Next.js前端中，您使用`CopilotKit`提供程序包装您的应用程序，并使用UI组件。\n    ```javascript\n    // frontend/src/app/page.tsx\n    \"use client\";\n    import { CopilotKit } from \"@copilotkit/react-core\";\n    import { CopilotChat } from \"@copilotkit/react-ui\";\n    import \"@copilotkit/react-ui/styles.css\";\n    export default function Home() {\n      return (\n        <CopilotKit url=\"http://localhost:5001/api/adk\"> {/* URL to your ADK backend */}\n          <main>\n            <h1>Welcome to Your ADK + AG-UI App!</h1>\n            <p>Ask me to get a stock price.</p>\n          </main>\n          <CopilotChat />\n        </CopilotKit>\n      );\n    }\n    ```\n\n这样，您就拥有了一个功能齐全的AI智能体，带有一个精致的前端，随时可以进行定制和扩展。\n\n### 立即开始构建\n停止纠结于将智能体逻辑连接到用户界面的复杂性。通过ADK和AG-UI的集成，您可以专注于最重要的事情：交付强大、智能且协作的AI应用程序。\n\n### 资源：\n*   快速入门仓库：`npx copilotkit@latest create -f adk`\n*   官方文档：`https://docs.copilotkit.ai/adk`\n*   GitHub上的启动项目：`https://github.com/copilotkit/with-adk`\n*   AG-UI Dojo教程：`https://dojo.ag-ui.com/adk-middleware/feature/shared_state`",
      "shortSummary": "ADK（Agent Development Kit）与AG-UI（开放协议）的集成，旨在简化生成式AI应用的开发。ADK提供多步规划、工具使用和状态管理等后端智能体核心能力。AG-UI则标准化了智能体与用户UI的直接通信，支持生成式UI、共享状态、人机协作及前端工具等高级功能。此集成使开发者能快速构建具有强大智能体和精美前端的交互式、协作性AI应用，显著提升用户体验和开发效率。",
      "translated_title": "利用AG-UI将ADK智能体与精美前端结合，提升用户体验",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/ADK__AG_UI_Blog_Sep_25_2025.original.png",
          "alt": "ADK + AG UI Blog Sep 25 2025",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "Delight users by combining ADK Agents with Fancy Frontends using AG-UIFor developers building genera..."
    },
    {
      "title": "Apigee Operator for Kubernetes 与 GKE Inference Gateway 集成，用于认证和 AI/LLM 策略 (原标题: Apigee Operator for Kubernetes and GKE Inference Gateway integration for Auth and AI/LLM policies)",
      "link": "https://developers.googleblog.com/en/apigee-operator-for-kubernetes-and-gke-inference-gateway-integration-for-auth-and-aillm-policies/",
      "pubDate": "Thu, 25 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-25T16:00:00.000Z",
      "creator": "Google",
      "summary": "# Apigee Operator for Kubernetes 与 GKE Inference Gateway 集成，用于认证和 AI/LLM 策略\n\n## 引言：API 在 AI/Agent 中的关键作用\n文章强调了 API 在生成式 AI 和自动化代理工作流中的核心地位，指出没有 API，AI 功能将难以被广泛访问和利用。API 通过使模型对自动化代理和人类用户可用，从而释放了生成式 AI 的力量。\n\n## GKE Inference Gateway 概述\nGKE Inference Gateway 是 Google Kubernetes Engine (GKE) Gateway 的一个扩展，专为服务生成式人工智能 (AI) 工作负载提供优化的路由和负载均衡。它简化了 AI 推理工作负载的部署、管理和可观测性。其主要功能包括：\n*   **推理优化负载均衡**：根据模型服务器的指标智能分配请求，以优化 AI 模型服务。\n*   **动态 LoRA 微调模型服务**：支持在通用加速器上服务动态 LoRA (Low-Rank Adaptation) 微调模型，通过多路复用减少 GPU 和 TPU 需求。\n*   **推理优化自动扩缩**：利用模型服务器指标实现 GKE Horizontal Pod Autoscaler (HPA) 的自动扩缩。\n*   **模型感知路由**：根据 GKE 集群内 OpenAI API 规范中定义的模型名称路由推理请求。\n*   **模型特定服务关键性**：允许指定 AI 模型的服务关键性，优先处理对延迟敏感的请求，而非对延迟容忍的批量推理作业。\n*   **集成 AI 安全**：与 Google Cloud Model Armor 集成，对模型提示和响应进行 AI 安全检查。\n*   **推理可观测性**：提供请求速率、延迟、错误和饱和度等推理请求的可观测性指标。\n\n## 面临的挑战\n大多数使用 GKE Inference Gateway 的企业客户希望保护和优化其代理/AI 工作负载。他们希望发布和商业化其代理 API，同时利用 Apigee 提供的高质量 API 治理功能，作为其代理 API 商业化策略的一部分。\n\n## 解决方案：GKE Inference Gateway 与 Apigee Operator for Kubernetes 的集成\nGKE Inference Gateway 通过引入 `GCPTrafficExtension` 资源解决了这一挑战。该资源使得 GKE Gateway 能够通过服务扩展（或 ext-proc）机制，向策略决策点 (PDP) 发起“侧向”调用。Apigee Operator for Kubernetes 利用这一服务扩展机制，对流经 GKE Inference Gateway 的 API 流量强制执行 Apigee 策略，从而为 GKE Inference Gateway 用户带来了 Apigee API 治理的优势。\n\n### 集成工作流程\nGKE Inference Gateway 和 Apigee Operator for Kubernetes 的协同工作步骤如下：\n1.  **配置 Apigee 实例**：GKE Inference Gateway 管理员在 Google Cloud 上配置一个 Apigee 实例。\n2.  **安装 Apigee Operator for Kubernetes**：管理员在其 GKE 集群中安装 Apigee Operator for Kubernetes，并将其连接到新配置的 Apigee 实例。\n3.  **创建 ApigeeBackendService**：创建一个 `ApigeeBackendService` 资源，该资源充当 Apigee 数据平面的代理。\n4.  **应用流量扩展**：将 `ApigeeBackendService` 作为 `GCPTrafficExtension` 中的 `backendRef` 进行引用。\n5.  **强制执行策略**：将 `GCPTrafficExtension` 应用到 GKE Inference Gateway，使 Apigee 能够对流经网关的 API 流量强制执行策略。\n\n![Apigee + GKE IG Diagram (2)](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Apigee__GKE_IG_Diagram_2.original.png)\n\n## Apigee Operator for Kubernetes：LLM 的 API 管理\nApigee 提供了一个全面的 API 管理层，适用于 Google Cloud、其他公共云和本地基础设施上的传统事务性 API 和大型语言模型 (LLM)。该平台提供强大的策略引擎、完整的 API 生命周期管理以及先进的 AI/ML 驱动分析功能。Apigee 在 Gartner Magic Quadrant 中被公认为 API 管理领域的领导者，服务于具有复杂 API 需求的大型企业。\n\n通过与 GKE Inference Gateway 的新集成，GKE 用户可以利用 Apigee 的全套功能来管理、治理和商业化其通过 API 提供的 AI 工作负载。这包括 API 生产者能够将 API 打包成 API 产品，并通过自助服务开发者门户提供给开发者。用户还可以获得 Apigee 的增值服务，如 API 安全和详细的 API 分析。\n\n### 集成后可用的 Apigee 策略\nGKE 用户可以通过集成访问 Apigee 管理的以下策略：\n*   API 密钥\n*   配额\n*   速率限制\n*   Google 访问令牌\n*   键值存储\n*   OpenAPI 规范验证\n*   流量峰值管理\n*   自定义 JavaScript\n*   响应缓存\n*   外部服务调用\n\nApigee Operator for Kubernetes 还支持管理员模板规则，允许组织管理员在其整个组织中强制执行策略规则。\n\n### 未来计划：Apigee AI 策略\n未来计划包括支持 Apigee AI 策略，涵盖：\n*   Model Armor 安全\n*   语义缓存\n*   令牌计数和强制\n*   基于提示的模型路由\n\n## 总结\n通过 GKE Inference Gateway 利用 Apigee 一流的 API 管理和安全能力，企业现在可以统一其 AI 服务和 API 治理层。借助 Apigee 全功能的 API 管理平台，企业可以专注于其核心任务：在 GKE 上运行推理引擎，利用公共云中一流的 AI 基础设施。",
      "shortSummary": "Apigee Operator for Kubernetes 与 GKE Inference Gateway 实现了集成，为 AI/LLM 工作负载提供了全面的 API 管理和治理。GKE Inference Gateway 优化了 AI 推理服务，而 Apigee Operator 则通过 `GCPTrafficExtension` 机制，将 Apigee 的认证、配额、速率限制等策略应用于流经网关的 API 流量。此集成使企业能够安全地发布、管理和商业化其代理/AI API，统一了 AI 服务与 API 治理层，并计划支持更多 AI 相关的策略。",
      "translated_title": "Apigee Operator for Kubernetes 与 GKE Inference Gateway 集成，用于认证和 AI/LLM 策略",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Apigee__GKE_IG_Diagram_2.original.png",
          "alt": "Apigee + GKE IG Diagram (2)",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "No AI/Agents without APIs!Many users interact with generative AI daily without realizing the crucial..."
    },
    {
      "title": "你的AI现在是本地专家：通过Google地图进行基础信息关联现已全面推出 (原标题: Your AI is now a local expert: Grounding with Google Maps is now GA)",
      "link": "https://developers.googleblog.com/en/your-ai-is-now-a-local-expert-grounding-with-google-maps-is-now-ga/",
      "pubDate": "Thu, 25 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-25T16:00:00.000Z",
      "creator": "Google",
      "summary": "## Vertex AI中Google地图基础信息关联功能全面推出\n\nGoogle于2025年9月26日宣布，Vertex AI中的“通过Google地图进行基础信息关联”（Grounding with Google Maps）功能现已全面推出（GA）。此功能旨在帮助开发者构建与Google地图实时、最新信息连接的生成式AI应用。\n\n### 重要性与定义\n\n*   **重要性：** 开发者在构建生成式AI应用时面临的一个关键挑战是确保模型输出的事实性和可靠性，尤其是在人们依赖这些信息做出实际决策时。此功能旨在解决这一问题。\n*   **“基础信息关联”（Grounding）定义：** 这是一个使大型语言模型（LLM）能够利用来自Google或客户自身数据的可验证信息源的过程。\n\n### 功能亮点与优势\n\n通过Google地图进行基础信息关联，开发者现在可以构建基于全球超过2.5亿商家和地点地理空间上下文的AI应用，这些信息来自Google地图及其活跃贡献者社区分享的本地洞察。\n\n该功能以多种方式帮助开发者构建事实可靠的生成式AI输出，提供更优质、更个性化的结果：\n\n*   **提供及时细节：** AI应用可访问每日更新的Google地图信息，从而向用户提供餐厅当前营业时间或商店是否暂时关闭等及时细节。\n*   **处理主观问题：** 结合Google地图用户评论的上下文，使用户能够轻松询问关于地点的主观问题，例如“这家咖啡馆的氛围如何？”或“这里适合安静的晚餐吗？”（*此功能目前仅在美国和印度可用。*）\n*   **加速商业发现：** 将Google地图数据与您的业务数据结合。例如，房地产公司可以为不同客户档案创建物业摘要，为有小孩的家庭突出附近的公园和学校，而为年轻专业人士则可能侧重于当地夜生活和餐厅。\n*   **提供全面查询结果：** 结合Google搜索和Google地图的基础信息关联功能。当用户询问“今晚公园里的音乐会我们可以带孩子去吗？”时，Gemini可以动态选择使用Google搜索查找表演者和场地政策，并使用Google地图提供儿童友好度信息以及附近建议。\n\n### 跨行业应用\n\nGoogle地图基础信息关联功能可在多个行业中发挥作用，早期采用的领域包括：\n\n*   **旅游和观光：** 帮助旅行者获得酒店、餐厅和景点推荐，涵盖从决定住宿地点、行程规划到当地探索的整个旅行体验。\n    *   万豪国际执行副总裁兼首席客户官Peggy Roe表示，这有助于提供真实的优质服务，深化与客人的互动。\n*   **房地产：** 为潜在买家提供关于社区的详细信息，包括定制房源摘要和类似社区的建议。\n    *   Compass工程副总裁Yotam Lemberger指出，这将通过生成式AI改善购房和销售体验，提供更个性化的位置洞察。\n*   **设备：** 个人助理可帮助用户发现附近景点并回答关于社区的问题。\n*   **社交媒体：** 帮助朋友共同研究活动和餐饮，包括集体探索区域、提供附近餐厅建议或帮助用户了解当地夜生活。\n\n### 如何开始\n\nVertex AI中的Google地图基础信息关联功能现已与Gemini一同提供，并包含一个便于实验的层级（Gemini Pro 10K提示）。要了解更多信息并开始使用，请访问相关文档、查看交互式演示，并访问Google Cloud Console中的Vertex AI Studio。开发者只需将Google Maps工具添加到Gemini API即可实现Google地图基础信息关联。",
      "shortSummary": "Google宣布Vertex AI中的“通过Google地图进行基础信息关联”功能现已全面推出。此功能使开发者能够构建与Google地图实时、最新信息连接的生成式AI应用，确保模型输出的事实性和可靠性。它允许AI利用全球超过2.5亿商家和地点的地理空间上下文，提供个性化、及时的信息，并支持旅游、房地产、设备和社交媒体等多个行业。开发者可通过Gemini API轻松集成此功能，并享有免费试用层级。",
      "translated_title": "你的AI现在是本地专家：通过Google地图进行基础信息关联现已全面推出",
      "images": [],
      "contentSource": "完整文章",
      "content": "We are excited to announce Grounding with Google Maps in Vertex AI is now Generally Available (GA). ..."
    },
    {
      "title": "持续推出我们的最新模型，发布改进版 Gemini 2.5 Flash 和 Flash-Lite (原标题: Continuing to bring you our latest models, with an improved Gemini 2.5 Flash and Flash-Lite release)",
      "link": "https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/",
      "pubDate": "Wed, 24 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-24T16:00:00.000Z",
      "creator": "Google",
      "summary": "# Gemini 2.5 Flash 和 Flash-Lite 更新发布\n\n*   **发布日期与目标**\n    *   **发布日期**：2025年9月25日\n    *   **发布内容**：Gemini 2.5 Flash 和 2.5 Flash-Lite 的更新版本。\n    *   **可用平台**：Google AI Studio 和 Vertex AI。\n    *   **核心目标**：持续提升模型质量和效率。\n\n*   **关键改进概览**\n    *   **质量与速度提升**：与当前稳定模型相比，Gemini 2.5 Flash 和 2.5 Flash-Lite 预览模型的质量和速度均有提升。\n        *   ![Intelligence vs End-to-End response time](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/rev_21_benchmarks_2.original.png)\n    *   **输出令牌效率**：\n        *   Gemini 2.5 Flash-Lite 的输出令牌（及成本）减少50%。\n        *   Gemini 2.5 Flash 的输出令牌（及成本）减少24%。\n        *   ![Output token efficiency](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/rev_21_benchmarks_1.original.png)\n\n*   **更新版 Gemini 2.5 Flash-Lite 详情**\n    *   基于三个关键主题进行训练和构建：\n        1.  **更好的指令遵循能力**：显著提升了遵循复杂指令和系统提示的能力。\n        2.  **减少冗余**：生成更简洁的答案，有效降低高吞吐量应用的令牌成本和延迟。\n        3.  **更强的多模态与翻译能力**：提供更准确的音频转录、更好的图像理解和改进的翻译质量。\n    *   **测试模型字符串**：`gemini-2.5-flash-lite-preview-09-2025`\n\n*   **更新版 Gemini 2.5 Flash 详情**\n    *   主要在两个收到持续反馈的领域进行了改进：\n        1.  **更好的智能体工具使用**：改进了模型使用工具的方式，在更复杂、智能体化和多步骤应用中表现更佳。在关键智能体基准测试中表现显著提升，SWE-Bench Verified 提升5%（从48.9%到54%）。\n        2.  **更高效率**：在“思考开启”模式下，模型成本效率显著提高，以更少的令牌实现更高质量的输出，降低延迟和成本。\n    *   **早期测试者反馈**：Manus 联合创始人兼首席科学家 Yichao ‘Peak’ Ji 表示，新模型在长周期智能体任务中性能提升15%，卓越的成本效率使其能够以前所未有的规模扩展。\n    *   **测试模型字符串**：`gemini-2.5-flash-preview-09-2025`\n\n*   **使用 Gemini 进行构建**\n    *   发布预览版本旨在让开发者测试最新改进、提供反馈并构建生产级体验。\n    *   本次发布并非稳定版本，而是为未来的稳定版本奠定基础。\n\n*   **引入 `-latest` 别名**\n    *   为每个模型系列引入 `-latest` 别名（如 `gemini-flash-latest` 和 `gemini-flash-lite-latest`），始终指向最新模型版本。\n    *   方便开发者试验新功能，无需为每次发布更新代码。\n    *   **重要提示**：在通过 `-latest` 更新或弃用特定版本前，将提前两周通过电子邮件通知。\n    *   **注意**：使用 `-latest` 别名时，速率限制、成本和可用功能可能在不同版本之间波动。\n    *   **稳定性应用**：对于需要更高稳定性的应用，建议继续使用 `gemini-2.5-flash` 和 `gemini-2.5-flash-lite`。\n\n*   **未来展望**\n    *   Google 将继续推动 Gemini 的前沿发展，未来将分享更多信息。",
      "shortSummary": "Google于2025年9月25日发布了Gemini 2.5 Flash和Flash-Lite的更新版本，旨在提升模型质量和效率。新版Flash-Lite在指令遵循、简洁性及多模态/翻译能力上有所增强。Flash模型则改进了智能体工具使用和整体效率，并在SWE-Bench Verified等基准测试中取得显著进展。为简化最新模型访问，Google还引入了`-latest`别名。这些预览版将为未来的稳定版本提供基础，并允许开发者提前测试和反馈。",
      "translated_title": "持续推出我们的最新模型，发布改进版 Gemini 2.5 Flash 和 Flash-Lite",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/rev_21_benchmarks_2.original.png",
          "alt": "Intelligence vs End-to-End response time",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/rev_21_benchmarks_1.original.png",
          "alt": "Output token efficiency",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "Google is releasing updated Gemini 2.5 Flash and Flash-Lite preview models with improved quality, speed, and efficiency. These releases introduce a \"-latest\" alias for easy access to the newest versions, allowing developers to test and provide feedback to shape future stable releases."
    },
    {
      "title": "使用Gemini Robotics-ER 1.5构建下一代物理智能体 (原标题: Building the Next Generation of Physical Agents with Gemini Robotics-ER 1.5)",
      "link": "https://developers.googleblog.com/en/building-the-next-generation-of-physical-agents-with-gemini-robotics-er-15/",
      "pubDate": "Wed, 24 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-24T16:00:00.000Z",
      "creator": "Google",
      "summary": "# Gemini Robotics-ER 1.5：下一代机器人具身推理模型\n\nGoogle AI于2025年9月25日发布了其最先进的机器人具身推理模型Gemini Robotics-ER 1.5，并向所有开发者开放预览。这是首个广泛可用的Gemini Robotics模型，旨在作为机器人的高级推理“大脑”。\n\n## 核心能力与优势\n\nGemini Robotics-ER 1.5模型专门针对机器人关键能力进行了优化，包括：\n\n*   **视觉与空间理解**：快速而强大的空间推理能力，能够以Gemini Flash模型的低延迟生成语义精确的2D点，并基于物品的尺寸、重量和可操作性进行推理。\n*   **任务规划**：能够将复杂的请求（如“清理桌子”）分解为详细的计划。\n*   **进度评估**：有效估计任务执行的进展。\n*   **工具调用**：原生支持调用Google搜索等工具以获取信息，以及调用视觉-语言-动作模型（VLA）或任何第三方用户定义函数来执行任务。\n*   **具身推理优化**：它是首个为此类具身推理优化的“思考”模型，在学术和内部基准测试中均达到最先进的性能。\n\n## 解决复杂日常任务\n\n该模型旨在解决对机器人而言极具挑战性的任务。例如，要求机器人“将这些物品分类到正确的堆肥、回收和垃圾箱中”。这需要机器人：\n\n1.  在线查找当地回收指南。\n2.  理解面前的物体。\n3.  根据当地规则确定如何分类。\n4.  执行所有步骤完成分类。\n\nGemini Robotics-ER 1.5能够处理这类需要上下文信息和多步骤才能完成的日常任务。\n\n## Gemini Robotics-ER 1.5的新特性\n\n该模型针对机器人应用进行了专门调整，并引入了多项新功能：\n\n*   **快速强大的空间推理**：\n    *   生成语义精确的2D点，并基于物品尺寸、重量和可操作性进行推理。\n    *   支持“指向任何你可以拿起的东西”等命令，实现准确、响应迅速的交互。\n*   **编排高级智能体行为**：\n    *   利用高级空间和时间推理、规划和成功检测，实现可靠的长期任务执行循环（例如，“根据这张图片重新整理我的办公桌”）。\n    *   原生调用Google搜索工具和任何第三方用户定义函数（例如，“根据当地规则将垃圾分类到正确的垃圾箱中”）。\n*   **灵活的思考预算**：\n    *   开发者可以直接控制延迟与准确性之间的权衡。\n    *   对于复杂任务（如多步骤组装），模型可以“思考更长时间”以提高准确性；对于反应性任务（如检测或指向物体），则可以要求快速响应。\n*   **改进的安全过滤器**：\n    *   增强了语义安全性，更好地识别并拒绝生成违反物理约束（例如，超出机器人有效载荷能力）的计划。\n\n## 机器人的智能大脑\n\nGemini Robotics-ER 1.5可被视为机器人的高级大脑，能够理解复杂的自然语言指令，推理长期任务，并编排复杂的行为。它不仅擅长感知，还能理解场景中的内容以及如何处理。\n\n例如，它可以将“清理桌子”这样的复杂请求分解为计划，并调用合适的工具，无论是机器人的硬件API、专门的抓取模型，还是用于运动控制的视觉-语言-动作模型（VLA）。\n\n### 高级空间理解\n\n为了让机器人与物理世界互动，它们需要感知和理解环境。Gemini Robotics-ER 1.5经过微调，可生成高质量的空间结果，为物体提供精确的2D点。\n\n![Gemini Robotics-ER 1.5性能图](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/GeminiRoboticsER1.5_Graph-ERGen-RD3_V001.original.png)\n*Gemini Robotics-ER 1.5模型是我们最先进的具身推理模型，同时作为通用多模态基础模型也保持了强大的性能。*\n\n![Gemini Robotics-ER 1.5指向精度图](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/GeminiRoboticsER1.5_Graph-PointingBenchmark-RD3.original.png)\n*Gemini Robotics-ER 1.5是用于指向精度的最精确视觉-语言模型。*\n\n**2D点生成示例**：\n\n给定厨房场景图像，Gemini Robotics-ER 1.5可以提供每个物体（或物体的一部分）的位置。结合机器人的3D传感器，可以确定物体在空间中的精确位置，从而生成准确的运动计划。\n\n![厨房场景中物品的2D点生成](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/unnamed-2_2.original.png)\n*提示：指出图像中的以下物品：洗碗皂、碗碟架、水龙头、电饭煲、独角兽。点以[y, x]格式表示，归一化到0-1000。只包含图像中实际存在的物体。*\n\n模型只会包含图像中存在的物品，避免了幻觉（例如，不会为不存在的“独角兽”生成点），使其基于实际可见内容进行推理。\n\n### 时间推理\n\n除了定位物体，真正的时空推理还涉及理解物体和动作随时间展开的关系。Gemini Robotics-ER 1.5通过处理视频来理解物理世界中的因果关系。\n\n### 利用可操作性编排长期任务\n\n当启用“思考”功能时，模型可以推理复杂的指向和边界框查询。例如，在咖啡制作过程中，它能理解“如何”以及“在哪里”放置马克杯、咖啡胶囊，以及如何关闭咖啡机，甚至在清理时知道马克杯的放置位置。\n\n![咖啡制作示例：识别马克杯放置位置](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_bjgs08R.original.png)\n*提示：识别我应该把马克杯放在哪里来制作一杯咖啡。以JSON对象列表的形式返回：`[{\"box_2d\": [y_min, x_min, y_max, x_min], \"label\": <label>}]`，坐标在0-1000之间归一化。*\n\n![咖啡制作示例：识别咖啡胶囊放置位置](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image10_5vMuO9V.original.png)\n*提示：我应该把咖啡胶囊放在哪里？*\n\n![咖啡制作示例：关闭咖啡机轨迹规划](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image15_kFakYKn.original.png)\n*提示：现在，我需要关闭咖啡机。绘制8个点轨迹，指示盖子把手应如何移动以关闭它。从把手开始。点为[Y,X]归一化坐标[0 - 1000]。请输出所有点，包括轨迹点，格式为：`[{\"point\": [Y, X], \"label\": }, {\"point\": [Y, X], \"label\": }, ...]`。*\n\n![咖啡制作示例：清理后马克杯放置位置](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image1_PCWZQMD.original.png)\n*提示：我喝完咖啡了。现在我应该把马克杯放在哪里清理？以JSON对象列表的形式返回：`[{\"point\": [y, x], \"label\": <label>}]`，坐标在0-1000之间归一化。*\n\n![垃圾分类示例](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image12_O0rW8PY.original.png)\n*另一个结合规划和空间定位的例子，生成一个“空间定位”的计划。*\n\n### 灵活的思考预算\n\n![思考预算对性能的影响](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/GeminiRoboticsER1.5_Graph-TTB-RD2_V001.original.png)\n*Gemini Robotics-ER 1.5使用推理时计算扩展来提高性能。思考令牌预算是可调的。这允许开发者在延迟敏感任务和高精度推理任务之间取得平衡。*\n\n模型性能随思考令牌预算的增加而提高。对于简单的空间理解任务（如物体检测），在非常短的思考预算下也能获得高性能；而更复杂的推理则受益于更大的预算。开发者可以通过`thinking_config`选项来设置或禁用思考预算。\n\n## 安全注意事项\n\nGemini Robotics-ER 1.5在安全性方面取得了显著改进，增强了以下方面的过滤器：\n\n*   **语义安全**：模型能够理解并拒绝生成危险或有害任务的计划，并已通过ASIMOV基准等严格评估。\n*   **物理约束感知**：模型现在能更好地识别请求是否会违反定义的物理约束（例如，机器人的有效载荷能力或工作空间限制）。\n\n然而，这些模型层面的安全保障不能替代物理系统所需的严格安全工程。Google倡导“瑞士奶酪”安全方法，即多层保护协同工作。开发者仍有责任实施标准的机器人安全最佳实践，包括紧急停止、避障和彻底的风险评估。\n\n## 立即开始构建\n\nGemini Robotics-ER 1.5现已提供预览版。它提供了构建机器人推理引擎所需的感知和规划能力。\n\n*   通过[Google AI Studio](https://aistudio.google.com/app/prompts/new_chat)开始体验模型。\n*   查阅[开发者文档](https://developers.google.com/gemini/reference/rest/v1beta/models/generateContent)以获取快速入门和API参考。\n*   探索[Colab笔记本](https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/tutorials/robotics_er_1_5_quickstart.ipynb)以查看实际实现。\n\n## 深入研究\n\n该模型是更广泛的Gemini Robotics系统的基础推理组件。要了解其背后的科学原理，包括端到端动作模型（VLA）和跨具身学习，请阅读[研究博客](https://ai.google.dev/research/blog/gemini-robotics-er-1-5)和[完整技术报告](https://arxiv.org/abs/2409.09117)。",
      "shortSummary": "Google AI发布了Gemini Robotics-ER 1.5，这是一款面向开发者的先进机器人具身推理模型。它作为机器人的高级“大脑”，专注于视觉与空间理解、任务规划和进度评估。该模型能原生调用工具，处理复杂的日常任务，并具备灵活的思考预算和改进的安全过滤器。Gemini Robotics-ER 1.5在空间和时间推理方面表现出色，能理解物理世界中的因果关系，并支持长期任务的编排。开发者现可通过Google AI Studio等平台进行预览和构建。",
      "translated_title": "使用Gemini Robotics-ER 1.5构建下一代物理智能体",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/GeminiRoboticsER1.5_Graph-ERGen-RD3_V001.original.png",
          "alt": "GeminiRoboticsER1.5_Graph-ER+Gen-RD3_V001",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/GeminiRoboticsER1.5_Graph-PointingBenchmark-RD3.original.png",
          "alt": "GeminiRoboticsER1.5_Graph-PointingBenchmark-RD3_V001 (1)",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/unnamed-2_2.original.png",
          "alt": "image9",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_bjgs08R.original.png",
          "alt": "image15",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image10_5vMuO9V.original.png",
          "alt": "image10",
          "title": "",
          "position": 5
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image15_kFakYKn.original.png",
          "alt": "image15",
          "title": "",
          "position": 6
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image1_PCWZQMD.original.png",
          "alt": "image1",
          "title": "",
          "position": 7
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image12_O0rW8PY.original.png",
          "alt": "image12",
          "title": "",
          "position": 8
        }
      ],
      "contentSource": "完整文章",
      "content": "Gemini Robotics-ER 1.5, now available to developers, is a state-of-the-art embodied reasoning model for robots. It excels in visual, spatial understanding, task planning, and progress estimation, allowing robots to perform complex, multi-step tasks."
    },
    {
      "title": "LiteRT-LM赋能Chrome、Chromebook Plus和Pixel Watch上的设备端生成式AI (原标题: On-device GenAI in Chrome, Chromebook Plus, and Pixel Watch with LiteRT-LM)",
      "link": "https://developers.googleblog.com/en/on-device-genai-in-chrome-chromebook-plus-and-pixel-watch-with-litert-lm/",
      "pubDate": "Tue, 23 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-23T16:00:00.000Z",
      "creator": "Google",
      "summary": "# LiteRT-LM：赋能设备端生成式AI\n\n本文介绍了LiteRT-LM，一个生产就绪的推理框架，旨在解决在用户设备上运行大型语言模型（LLM）所面临的挑战，从而提升产品体验。\n\n## 设备端LLM的优势与挑战\n\n*   **优势**：\n    *   **离线可用性**：随时可用，无需网络连接。\n    *   **成本效益**：无按API调用付费，适用于高频任务如文本摘要或校对。\n*   **挑战**：\n    *   **部署复杂性**：在各种边缘硬件上部署千兆字节规模的模型。\n    *   **性能要求**：实现亚秒级的首个Token生成时间（TTFT）延迟。\n    *   **质量保证**：同时满足所需的输出质量。\n\n## LiteRT-LM的推出与应用\n\n*   **核心功能**：LiteRT-LM是一个生产就绪的推理框架，已为Google产品中Gemini Nano最广泛的部署提供支持。\n*   **赋能产品**：它使Gemini Nano和Gemma等模型能够在Chrome、Chromebook Plus和Pixel Watch等产品上实现设备端运行，并通过MediaPipe LLM推理API支持其他开放模型。\n*   **开发者访问**：\n    *   现有高级API：MediaPipe LLM推理API、Chrome内置AI API和Android AICore。\n    *   **新开放**：首次提供LiteRT-LM引擎的底层C++接口（预览版），允许开发者构建定制化、高性能的AI管道。\n\n## LiteRT-LM在Google AI Edge堆栈中的定位\n\nLiteRT-LM是一个经过生产测试的推理框架，用于在各种边缘设备上高性能运行Gemini Nano、Gemma等LLM。它是一个完全开源的项目，提供易于集成的API和可重用模块，帮助开发者构建定制化的LLM管道。\n\nGoogle AI Edge堆栈从低到高抽象层级包括：\n\n1.  **LiteRT**：在设备上高效执行单个ML/AI模型的基础运行时。\n2.  **LiteRT-LM**：基于C++的LLM管道框架，利用LiteRT协同运行多个模型和处理步骤（如会话克隆、KV缓存管理、提示缓存/评分、有状态推理），以完成复杂的生成式AI任务。\n3.  **LLM Inference API**：由LiteRT-LM提供支持的高级原生GenAI API（Kotlin、Swift、JS）。\n\n这种分层结构提供了灵活性，LiteRT-LM则为开发者提供了核心能力和适应性，以大规模部署设备端LLM。\n\n## LiteRT-LM的关键亮点\n\n*   **跨平台**：支持Android、Linux、macOS、Windows和Raspberry Pi。\n*   **硬件加速**：利用LiteRT运行时，支持CPU、GPU和NPU加速，充分发挥设备端硬件潜力。\n*   **增强灵活性**：模块化设计和开源代码库提供最大的推理管道定制灵活性，支持多模态、开放权重模型以及跨各种移动平台和加速器的生产级大型模型推理。\n\n## 案例研究\n\n### 1. Chrome和Chromebook Plus中的多LLM功能\n\n现代LLM的千兆字节规模带来了独特的部署挑战。LiteRT-LM通过以下架构和优化克服了这一挑战：\n\n*   **共享基础模型与LoRA**：允许多个功能共享一个基础模型，并使用轻量级LoRA进行功能特定定制。\n*   **Engine/Session架构**：\n    *   **Engine（单例）**：作为应用程序功能共享的单一实例，管理所有昂贵、共享的资源（如基础模型和多模态编码器），并根据运行时环境智能地处理资源的加载和卸载。\n    *   **Session（有状态接口）**：应用程序功能与之交互的接口，每个Session代表一个独立的对话或任务，管理其自身的状态、历史和上下文。Session可以通过小型、任务特定的适配器（LoRA权重）来定制基础模型的行为。\n\n    ![LiteRT-LM引擎/会话系统架构图](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/engine_session.original.png)\n    *LiteRT-LM引擎/会话系统架构图：Engine（底部）作为中央资源管理器，生成两个不同的Session（顶部）；一个用于摘要，一个用于图像理解。两个Session共享通用资源，如基础文本解码器和分词器，而图像理解Session额外请求视觉编码器。由于没有活动的Session需要音频编码器，Engine将其从内存中卸载。*\n\n*   **关键优化**：\n    *   **上下文切换**：每个Session封装其完整的“上下文”（包括Transformer的KV缓存、LoRA权重等）。切换任务时，LiteRT-LM保存传出Session的状态并恢复传入Session的状态，确保共享LLM始终具有活动任务的正确状态。\n    *   **会话克隆**：用户可以克隆Session以避免重新计算共享的提示前缀，从而有效地缓存特定点的KV缓存状态，允许从该状态分支出多个新任务，节省大量计算。\n    *   **写时复制（CoW）KV缓存**：KV缓存可能非常大。CoW机制使得克隆的Session不会立即复制KV缓存，而是创建对原始缓冲区的引用。只有当Session即将覆盖与另一个Session内容冲突的新数据时，才会执行实际复制。这使得克隆速度极快（<10ms），并通过重用KV缓存缓冲区最大限度地减少内存占用。\n\n这些架构和优化能力对于在Chrome和Chromebook Plus中成功实现多个高性能设备端LLM功能至关重要。LiteRT-LM还通过LiteRT作为底层运行时进行后端委托，并抽象平台特定组件，实现了广泛的平台兼容性。\n\n### 2. Pixel Watch上的语言模型部署\n\n在Pixel Watch等资源严重受限的设备上部署LLM，需要将重点从支持多功能共享模型转向部署具有最小二进制大小和内存占用的单一专用功能。\n\n*   **模块化设计**：LiteRT-LM的模块化设计在此发挥关键作用。它允许开发者直接从其核心组件构建定制管道。\n*   **Pixel Watch优化**：为Pixel Watch选择了最少必需的模块（如执行器、分词器和采样器），并组装了一个专门的管道，从而最大限度地减小了二进制大小和内存使用，以满足设备的资源限制。\n\n    ![为Pixel Watch优化的轻量级LLM管道](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/litert_lm_pixel_watch.original.png)\n    *为Pixel Watch优化的轻量级LLM管道*\n\n这个案例展示了LiteRT-LM的灵活性，其模块化组件使开发者能够根据任何目标设备（从强大的智能手机到受限的可穿戴设备）的特定资源和功能要求，精确定制LLM部署。\n\n## 如何开始\n\n*   **探索社区**：访问LiteRT HuggingFace社区，发现兼容的开放模型如Gemma和Qwen。\n*   **访问代码**：查看GitHub仓库，获取C++预览版和示例代码。\n*   **阅读文档**：深入了解使用LiteRT-LM运行时在设备上构建和执行LLM的必要步骤。",
      "shortSummary": "LiteRT-LM是一个生产就绪的推理框架，旨在赋能Chrome、Chromebook Plus和Pixel Watch等设备上的生成式AI。它解决了在边缘设备上部署大型语言模型（LLM）的性能和资源挑战，提供离线可用性和成本效益。LiteRT-LM支持跨平台部署，利用硬件加速，并通过模块化设计和Engine/Session架构实现多功能共享模型及资源受限设备的定制化部署。开发者现可通过C++接口访问其底层能力，构建高性能设备端AI应用。",
      "translated_title": "LiteRT-LM赋能Chrome、Chromebook Plus和Pixel Watch上的设备端生成式AI",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/engine_session.original.png",
          "alt": "engine_session",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/litert_lm_pixel_watch.original.png",
          "alt": "litert_lm_pixel_watch",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "Google AI Edge provides the tools to run AI features on-device, and its new LiteRT-LM runtime is a significant leap forward for generative AI. LiteRT-LM is an open-source C++ API, cross-platform compatibility, and hardware acceleration designed to efficiently run large language models like Gemma and Gemini Nano across a vast range of hardware. Its key innovation is a flexible, modular architecture that can scale to power complex, multi-task features in Chrome and Chromebook Plus, while also being lean enough for resource-constrained devices like the Pixel Watch. This versatility is already enabling a new wave of on-device generative AI, bringing capabilities like WebAI and smart replies to users."
    }
  ],
  "lastUpdated": "2025-10-02T10:29:23.089Z"
}