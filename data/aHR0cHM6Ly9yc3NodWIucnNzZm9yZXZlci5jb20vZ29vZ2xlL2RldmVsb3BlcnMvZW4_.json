{
  "sourceUrl": "https://rsshub.rssforever.com/google/developers/en",
  "title": "Google Developers Blog",
  "description": "Google Developers Blog - Powered by RSSHub",
  "link": "https://developers.googleblog.com",
  "items": [
    {
      "title": "Gemini CLI 🤝 FastMCP：简化 MCP 服务器开发 (原标题: Gemini CLI 🤝 FastMCP: Simplifying MCP server development)",
      "link": "https://developers.googleblog.com/en/gemini-cli-fastmcp-simplifying-mcp-server-development/",
      "pubDate": "Sun, 21 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-21T16:00:00.000Z",
      "creator": "Google",
      "summary": "## Gemini CLI 与 FastMCP 集成：简化 MCP 服务器开发\n\n本文宣布 Gemini CLI 已与 FastMCP（Python 中用于构建模型上下文协议 (MCP) 服务器的领先库）实现无缝集成。此项集成旨在简化 MCP 服务器的开发过程，使用户能够轻松地将自定义 MCP 工具和提示直接连接到 Gemini CLI。\n\n### 核心集成功能\n\n*   **安装与配置**\n    *   从 FastMCP v2.12.3 版本开始，用户可以使用 `fastmcp install gemini-cli server.py` 命令来安装使用 FastMCP 构建的本地 STDIO 传输 MCP 服务器。\n    *   此命令大大简化了流程，使 FastMCP 服务器的功能能够立即在 Gemini CLI 中可用并配置。\n    *   它会自动处理配置、依赖管理，并调用 Gemini CLI 内置的 MCP 管理系统（`gemini mcp add`）。\n    *   对于本地服务器开发而言，此集成提供了一种便捷的入门方式。\n\n### MCP 服务器如何增强 Gemini CLI 体验\n\nMCP 服务器能够显著改善 Gemini CLI 的使用体验，通过允许代理执行操作并访问其原本无法获得的上下文。\n\n*   **Pythonic 设计**：FastMCP 采用 Pythonic 设计，利用装饰器和类型提示，使得创建这些组件变得简单直观。\n*   **关键功能**：\n    *   **工具 (Tools)**：允许 Gemini CLI 执行操作、计算，或与外部 API 和数据库交互，使其成为工作流中的积极参与者。\n    *   **提示 (Prompts)**：用户可以将 FastMCP 定义的提示作为斜杠命令（例如 `/promptname`）在 Gemini CLI 中使用。这简化了交互，并使常用提示在终端环境中感觉原生。\n\n### 快速入门指南\n\n想要体验此集成非常简单，只需几个步骤即可开始使用 Gemini CLI 和 FastMCP：\n\n1.  **安装 Gemini CLI**：运行命令 `npm install -g @google/gemini-cli@latest`。\n2.  **安装 FastMCP**：确保安装 v2.12.3 或更高版本：`pip install fastmcp>=2.12.3`。\n3.  **创建服务器文件**：编写包含自定义工具和提示的 `server.py` 文件。\n4.  **集成**：执行命令 `fastmcp install gemini-cli server.py`。\n5.  **验证**：启动 Gemini CLI 并使用 `/mcp` 命令进行验证。\n\n### 更多信息与社区支持\n\n*   有关更详细的信息和高级配置，请参阅官方的 FastMCP 集成文档。\n*   Gemini CLI 和 FastMCP 均为开源项目。用户可以通过直接在 GitHub 上为 FastMCP 或 Gemini CLI 提交新的 issue，来提出功能请求或改进建议。",
      "shortSummary": "Gemini CLI 现已与 FastMCP（Python 的 MCP 服务器构建库）无缝集成，旨在简化模型上下文协议 (MCP) 服务器的开发。用户可通过 `fastmcp install gemini-cli` 命令轻松连接自定义 MCP 工具和提示到 Gemini CLI。此集成使 Gemini CLI 代理能执行更多操作并访问更多上下文，通过提供工具和可作为斜杠命令使用的提示来增强功能。用户可快速安装并开始利用此集成，并通过 GitHub 参与开源项目。",
      "translated_title": "Gemini CLI 🤝 FastMCP：简化 MCP 服务器开发",
      "images": [],
      "contentSource": "完整文章",
      "content": "Gemini CLI now seamlessly integrates with FastMCP, Python's leading library for building MCP servers. We’re thrilled to announce this integration between two open-source projects that empowers you to effortlessly connect your custom MCP tools and prompts, directly to Gemini CLI!"
    },
    {
      "title": "ADK for Java 通过 LangChain4j 集成向第三方语言模型开放 (原标题: ADK for Java opening up to third-party language models via LangChain4j integration)",
      "link": "https://developers.googleblog.com/en/adk-for-java-opening-up-to-third-party-language-models-via-langchain4j-integration/",
      "pubDate": "Mon, 15 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-15T16:00:00.000Z",
      "creator": "Google",
      "summary": "## ADK for Java 通过 LangChain4j 集成向第三方语言模型开放\n\nGoogle 的 Java 代理开发工具包（ADK）在 2025 年 9 月 11 日发布的 0.2.0 版本中，新增了与 LangChain4j LLM 框架的集成。这一重要更新旨在为开发者提供更广泛的大型语言模型（LLM）选择，以构建强大的 AI 代理。\n\n### LangChain4j 集成的核心优势\n\n*   **模型多样性**：除了 ADK 内置的 Google Gemini 和 Anthropic Claude 集成外，开发者现在可以通过 LangChain4j 访问来自 OpenAI、Anthropic、GitHub、Mistral 等第三方提供商的多种模型。\n*   **本地模型支持**：还支持通过 Ollama 或 Docker Model Runner 等工具运行的本地开源模型。\n*   **易于集成**：LangChain4j LLM 框架支持的模型列表可在其官方文档中查询。\n\n### 如何通过 LangChain4j 集成 LLM\n\n当使用 `LlmAgent` 构建器声明 ADK 代理时，可以通过 `model()` 构建器方法指定 LLM。通常，这会传入一个表示模型名称的字符串（例如 \"gemini-2.5-flash\"）。\n\nLangChain4j 集成通过使用一个扩展 `BaseLlm` 抽象类的新 `LangChain4j` 类，在两个框架之间建立了桥梁。这意味着开发者可以实例化一个 LangChain4j 聊天模型，然后将其包装在 `new LangChain4j()` 实例中，传递给 ADK 代理。\n\n#### 示例一：使用 Docker Model Runner 运行 Gemma 3\n\n1.  **安装与拉取模型**：在机器上安装并启用 Docker Model Runner 后，可通过命令 `docker model pull ai/gemma3` 轻松拉取 Gemma 3 模型。\n2.  **Maven 依赖**：在 `pom.xml` 中添加 `google-adk-contrib-langchain4j` 和 `langchain4j-open-ai` 依赖。\n3.  **配置模型**：由于 Docker Model Runner 模型暴露了 OpenAI 兼容的 API 接口，可以使用 `OpenAiChatModel` 进行配置，指定 `baseUrl`（例如 `http://localhost:12434/engines/llama.cpp/v1`）和 `modelName`（例如 `ai/gemma3n`）。\n4.  **配置代理**：将配置好的 `OpenAiChatModel` 实例通过 `new LangChain4j(dmrChatModel)` 传递给 `LlmAgent`，例如构建一个国际象棋教练代理。\n\n#### 示例二：使用 Ollama 运行 Qwen 3\n\n1.  **Maven 依赖**：在 `pom.xml` 中添加 `google-adk`、`google-adk-contrib-langchain4j` 和 `langchain4j-ollama` 依赖。\n2.  **安装与运行模型**：假设已在机器上安装 Ollama 并拉取了 Qwen 3 模型（例如 `qwen3:1.7b`），并在端口 11434 运行。\n3.  **配置模型**：使用 `OllamaChatModel` 进行配置，指定 `modelName` 和 `baseUrl`（例如 `http://127.0.0.1:11434`）。\n4.  **配置代理**：将配置好的 `OllamaChatModel` 实例通过 `new LangChain4j(ollamaChatModel)` 传递给 `LlmAgent`，例如构建一个友好的科学老师代理。\n\n### 工具调用能力\n\n如果模型支持函数调用，开发者还可以为代理提供工具访问权限，例如访问 MCP 服务器或本地代码驱动的函数。ADK 文档和相关文章提供了更多关于可用工具的信息。\n\n### 0.2.0 版本的其他重要增强功能\n\n除了 LangChain4j 集成，0.2.0 版本还带来了多项强大的代理开发工作流改进：\n\n*   **扩展的工具能力**：\n    *   **基于实例的 `FunctionTools`**：现在可以从对象实例创建 `FunctionTools`，而非仅限于静态方法，提高了代理架构的灵活性。\n    *   **改进的异步支持**：`FunctionTools` 现在支持返回 `Single` 的方法，增强了异步操作支持和代理响应能力。\n    *   **更好的循环控制**：`Event Actions` 中新增的 `endInvocation` 字段允许在工具调用后以编程方式中断或停止代理循环，提供更精细的执行控制。\n*   **高级代理逻辑和内存**：\n    *   **链式回调**：增加了对模型、代理和工具执行前后事件的链式回调支持，使得代理生命周期内的逻辑更加复杂和精细。\n    *   **新的内存和检索**：引入了 `InMemoryMemoryService` 用于简单快速的内存管理，并实现了使用 AI Platform API 的 `VertexAiRagRetrieval` 以支持更高级的 RAG 模式。\n*   **其他关键增强**：包括父 POM 和 Maven Wrapper (`./mvnw`)，确保所有贡献者都能获得一致且直接的构建过程。\n\n### 总结与展望\n\nLangChain4j 的集成是 ADK for Java 迈向更开放、更灵活框架的重要一步，旨在帮助开发者构建更强大的 AI 代理。鼓励开发者查阅 GitHub 发布说明、ADK for Java 文档、入门指南或 GitHub 模板项目以获取更多信息。文章作者期待听到社区关于 ADK for Java 和 LangChain4j 集成的使用案例。",
      "shortSummary": "Google ADK for Java 0.2.0 版本通过集成 LangChain4j LLM 框架，显著扩展了其对第三方语言模型的支持。开发者现在除了内置的 Gemini 和 Claude，还能利用 LangChain4j 访问 OpenAI、Mistral 等多种模型，包括通过 Docker Model Runner 或 Ollama 运行的本地模型。此次更新还带来了工具能力、异步支持、代理逻辑和内存管理等多方面的增强，旨在使 ADK for Java 成为一个更开放、灵活的 AI 代理构建框架。",
      "translated_title": "ADK for Java 通过 LangChain4j 集成向第三方语言模型开放",
      "images": [],
      "contentSource": "完整文章",
      "content": "The recent 0.2.0 release of Google’s Agent Development Kit (ADK) for Java adds an integration with t..."
    },
    {
      "title": "Gemini 批量 API 现已支持嵌入和 OpenAI 兼容性 (原标题: Gemini Batch API now supports Embeddings and OpenAI Compatibility)",
      "link": "https://developers.googleblog.com/en/gemini-batch-api-now-supports-embeddings-and-openai-compatibility/",
      "pubDate": "Tue, 09 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-09T16:00:00.000Z",
      "creator": "Google",
      "summary": "## Gemini 批量 API 新功能：支持嵌入和 OpenAI 兼容性\n\nGoogle Cloud 于 2025 年 9 月 10 日宣布，对 Gemini 批量 API 进行重大扩展，新增了两项关键功能：\n\n1.  **支持新推出的 Gemini 嵌入模型。**\n2.  **提供 OpenAI SDK 兼容性，方便开发者提交和处理批量请求。**\n\n### Gemini 批量 API 概述\n\nGemini 批量 API 最初旨在实现异步处理，特别适用于高吞吐量和对延迟容忍的用例。其核心优势在于：\n\n*   **成本效益：** 处理费率比实时 API 降低 50%。\n*   **效率提升：** 能够处理大规模数据请求。\n\n### 批量 API 嵌入支持\n\n新推出的 Gemini 嵌入模型已广泛应用于生产部署。现在，通过批量 API 使用该模型，开发者可以获得更多优势：\n\n*   **更高的速率限制。**\n*   **更低的价格：** 每 100 万输入 token 仅需 $0.075，价格减半。\n*   **解锁新用例：** 特别适用于对成本敏感、对延迟容忍或需要异步处理的场景。\n\n**使用示例 (Python 代码):**\n\n文章提供了使用 `google.genai` 客户端进行批量嵌入的详细代码示例，包括：\n\n1.  创建包含请求的 JSONL 文件。\n2.  通过 `client.files.upload` 上传请求文件。\n3.  使用 `client.batches.create_embeddings` 创建批量嵌入作业。\n4.  等待作业完成并下载结果。\n\n### 批量 API 的 OpenAI 兼容性\n\n对于已经使用 OpenAI SDK 兼容层的开发者，切换到 Gemini 批量 API 变得异常简单，只需更新少量代码即可：\n\n*   **便捷性：** 开发者可以通过指定 Gemini API 密钥和特定的 `base_url` (`https://generativelanguage.googleapis.com/v1beta/openai/`)，继续使用 OpenAI SDK 来提交和管理批量请求。\n\n**使用示例 (Python 代码):**\n\n文章展示了如何使用 `openai` 客户端创建批量作业，包括：\n\n1.  上传符合 OpenAI 批量输入格式的 JSONL 文件。\n2.  使用 `openai_client.batches.create` 方法创建批量作业，指定 `input_file_id`、`endpoint` 和 `completion_window`。\n3.  轮询作业状态，并在完成后下载结果。\n\n### 未来展望\n\nGoogle 表示将持续扩展其批量处理服务，以进一步优化 Gemini API 的使用成本，鼓励开发者关注后续更新。",
      "shortSummary": "Gemini 批量 API 现已扩展，支持新推出的 Gemini 嵌入模型和 OpenAI SDK 兼容性。此更新允许开发者以更低的成本（费率降低 50%，嵌入模型价格减半）和更高的速率限制，异步处理大规模、对延迟不敏感的任务。通过简单的代码修改，使用 OpenAI SDK 的开发者也能轻松切换。Google 将继续优化批量服务以降低成本。",
      "translated_title": "Gemini 批量 API 现已支持嵌入和 OpenAI 兼容性",
      "images": [],
      "contentSource": "完整文章",
      "content": "Batch API now supports Embeddings and OpenAI CompatibilityToday we are extending the Gemini Batch AP..."
    },
    {
      "title": "宣布 Genkit Go 1.0 及增强型 AI 辅助开发 (原标题: Announcing Genkit Go 1.0 and Enhanced AI-Assisted Development)",
      "link": "https://developers.googleblog.com/en/announcing-genkit-go-10-and-enhanced-ai-assisted-development/",
      "pubDate": "Mon, 08 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-08T16:00:00.000Z",
      "creator": "Google",
      "summary": "Google 激动地宣布发布 **Genkit Go 1.0**，这是其针对 Go 生态系统的开源 AI 开发框架的首个稳定、生产就绪版本。同时，还推出了 **`genkit init:ai-tools`** 命令，旨在极大提升 AI 辅助开发工作流。\n\n### Genkit 简介\nGenkit 是一个开源框架，用于构建全栈 AI 驱动的应用程序。它提供：\n*   统一的多种模型提供商接口。\n*   简化的 API，支持多模态内容、结构化输出、工具调用、检索增强生成（RAG）和代理工作流。\n*   通过 Genkit Go，开发者现在可以利用 Go 语言的速度、安全性和可靠性，构建和部署生产就绪的 AI 应用程序。\n\n### Genkit Go 1.0 的新特性\n1.  **生产就绪：**\n    *   Genkit Go 1.0 致力于为 Go 中构建 AI 应用程序提供稳定、可靠的基础。\n    *   API 稳定且经过良好测试，确保部署到生产环境的应用程序的可靠性。\n    *   遵循 Go 的兼容性承诺，Genkit 1.* 版本的程序即使在未来的“点”发布（如 1.1、1.2）出现时，也将继续正确编译和运行。\n2.  **类型安全的 AI 流程：**\n    *   “流程”（Flows）是 Genkit 最强大的功能之一，它们是用于 AI 用例的函数，提供可观察性、易测试性和简化的部署。\n    *   开发者可以使用 Go 结构体和 JSON Schema 验证来定义类型安全的 AI 流程，例如生成结构化食谱，确保数据的一致性和准确性。\n3.  **统一的模型接口：**\n    *   Genkit Go 提供了一个单一、一致的接口，用于与多个 AI 模型提供商协作，包括 Google AI、Vertex AI、OpenAI、Anthropic 和 Ollama 等。\n4.  **工具调用：**\n    *   Genkit Go 使得为 AI 模型提供访问外部函数和 API 的能力变得轻而易举，例如定义一个用于获取天气信息的工具。\n5.  **简易部署：**\n    *   开发者可以通过最少的设置，将 AI 流程部署为 HTTP 端点。\n\n### 丰富的开发者工具\nGenkit Go 配备了一套全面的开发工具，旨在使 AI 应用程序的构建快速而直观：\n1.  **独立 CLI 安装：**\n    *   通过一个独立的 CLI 二进制文件，可以快速开始，无需其他运行时安装（支持 macOS、Linux 和 Windows）。\n    *   CLI 支持任何 Genkit 应用程序（JavaScript、Go、Python），并提供运行和评估流程等便捷命令。\n2.  **交互式开发者 UI：**\n    *   提供可视化界面，用于测试和调试 AI 应用程序。\n    *   功能包括：交互式运行流程、通过详细跟踪进行调试、监控性能（延迟、令牌使用、成本）以及试验提示和模型配置。\n    *   可以通过 CLI 命令 `genkit start -- go run .` 与 Go 应用程序一同启动。\n\n### 介绍 `genkit init:ai-tools` 命令\n*   **革新 AI 辅助开发：** 此命令为 JavaScript 和 Go 开发者提供了无缝集成 Genkit 框架和工具的 AI 编码助手配置。\n*   **运行效果：**\n    *   检测并保留现有的 AI 助手配置。\n    *   安装 Genkit MCP 服务器，其中包含强大的开发工具，如 `lookup_genkit_docs`（搜索 Genkit 文档）、`list_flows`（列出当前 Genkit 应用中的所有流程）、`run_flow`（执行流程）和 `get_trace`（获取执行跟踪）。\n    *   创建一个 `GENKIT.md` 文件，其中包含针对 AI 助手的全面语言特定指令。\n*   **支持的 AI 工具：**\n    *   Gemini CLI (Google 的 CLI AI 编码助手)\n    *   Firebase Studio (Firebase 的代理式云开发环境)\n    *   Claude Code (Anthropic 的编码助手)\n    *   Cursor (AI 驱动的代码编辑器)\n    *   对于其他工具，可以选择“generic”选项以获取可手动集成的 `GENKIT.md` 文件。\n*   **增强的开发体验：** 通过 AI 助手集成，开发者可以：\n    *   询问 Genkit Go API 相关问题并获得准确、最新的答案。\n    *   生成符合最佳实践的 Go 特定 Genkit 代码。\n    *   通过 AI 助手分析跟踪来调试流程。\n    *   使用 AI 生成的输入来测试应用程序。\n    *   获取 Go 特定模式和惯用代码的帮助。\n\n### 快速入门\n文章提供了快速创建项目、安装 Genkit 和 CLI、设置 AI 助手集成、创建第一个流程以及启动开发者 UI 的步骤。\n\n### 更多资源\n开发者可以通过官方文档、入门指南、Go API 参考、GitHub 示例和 Genkit Discord 社区获取更多信息和支持。\n\nGenkit Go 1.0 将 Go 的性能和可靠性与 Genkit 生产就绪的 AI 框架和工具相结合，旨在赋能开发者构建强大的 AI 应用程序。",
      "shortSummary": "Google 发布 Genkit Go 1.0，这是 Go 生态系统的首个稳定、生产就绪的开源 AI 开发框架。它提供类型安全的 AI 流程、统一的模型接口、工具调用和丰富的开发者工具。同时推出的 `genkit init:ai-tools` 命令，能自动配置 AI 编码助手，无缝集成 Genkit，提供文档查询、流程管理、调试和代码生成等功能，显著提升 AI 辅助开发体验。Genkit Go 1.0 结合了 Go 的性能与 Genkit 的 AI 框架，助力开发者构建强大的 AI 应用。",
      "translated_title": "宣布 Genkit Go 1.0 及增强型 AI 辅助开发",
      "images": [],
      "contentSource": "完整文章",
      "content": "We are launching 1.0 stable release of Genkit Go, empowering Go developers to build performant, production-ready AI-powered applications with Genkit. Recent enhancements include support for integrating and building MCP tools, expanding third-party model provider support, and production AI monitoring with Firebase.\n\nAdditionally, we are announcing a new feature in the Genkit CLI to provide AI development tools, like the Gemini CLI and Cursor, with the latest knowledge of Genkit - supercharging Genkit development experience when using AI assistance."
    },
    {
      "title": "A2A 扩展：赋能自定义代理功能 (原标题: A2A Extensions: Empowering Custom Agent Functionality)",
      "link": "https://developers.googleblog.com/en/a2a-extensions-empowering-custom-agent-functionality/",
      "pubDate": "Mon, 08 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-08T16:00:00.000Z",
      "creator": "Google",
      "summary": "# A2A 扩展：赋能自定义代理功能\n\nA2A 协议提供了一个强大且标准化的代理间通信框架，但它也认识到“一刀切”的方法可能不适用于所有场景。为此，A2A 引入了“扩展”功能，允许开发者在 A2A 服务器中添加自定义的、特定领域的功能和方法，超越核心协议。此外，它还使客户端能够添加远程代理应支持的额外要求，这在代理需要提供专业化功能时非常有用。\n\n## 如何开始使用？\n\n入门非常简单。一个代理只需在其“代理卡”（一个概述其功能的 JSON 文件）中声明对某个扩展的支持。由于任何人都可以定义、发布和实现扩展（每个扩展都由一个唯一的 URI 标识），因此 A2A 生态系统非常开放和社区驱动。这种方法确保 A2A 协议保持高度灵活性和适应性，使开发者能够构建更强大、更专业的多种代理系统。\n\n![A2A 扩展](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Screenshot_2025-09-04_at_13.03.31.original.png)\n图 1: A2A 扩展\n\n## A2A 扩展的实际应用案例\n\n随着 A2A 协议的普及，我们看到了广泛的扩展用例：\n\n### 1. A2A 中的可追溯性扩展\n\n*   **目的**：确保不同的 AI 代理能够可靠、无误地协作，通过提供深度可见性来跟踪交互并有效诊断问题，从而实现对其通信的评估。\n*   **工作原理：ResponseTrace**\n    *   **核心**：ResponseTrace 消息是此扩展的核心，它是一个结构化日志，捕获代理执行的操作序列。它被设计为轻量级且独立于主 A2A 协议，仅专注于记录代理发出的请求。\n    *   **组成**：ResponseTrace 由一系列“步骤”（Steps）组成，每个步骤代表一个操作，可以是以下两种类型之一：\n        1.  **工具调用 (ToolInvocation)**：对特定工具（如函数或 API）的调用。\n        2.  **代理调用 (AgentInvocation)**：对另一个代理的调用，可以是另一个 A2A 代理或不同类型的代理。\n    *   **层级结构**：这些步骤是分层的。如果一个代理调用了另一个也支持可追溯性的代理，则第二个代理的跟踪可以嵌套在第一个代理内部，从而提供整个工作流的完整端到端视图。\n\n### 2. Twilio 的延迟扩展\n\n*   **目的**：Twilio 构建了一个延迟感知扩展，以帮助其基于 Twilio ConversationRelay 构建的语音代理进行模型选择。\n*   **特点**：这是一个领域特定功能的经典示例。广播代理的延迟信息不属于核心协议的代理卡规范，但 Twilio 为此构建了一个扩展，以帮助选择最合适的代理或优雅地进行适应。\n\n### 3. 其他有趣的应用程序\n\n*   **Identity Machines**：通过 A2A 扩展实现代理间调用的零信任握手。它们展示了代理任务委托如何基于信任和独立的策略门（如目的、预算、能力、模型、PII 状态等自定义参数）进行锚定，在代理接受任务之前强制执行这些策略。\n*   **Ethereum 的 ERC-8004 提案**：旨在为 AI 代理创建一个信任层，允许它们跨不同的组织边界进行交互。该标准引入了链上注册表，用于身份、声誉和验证，以实现来自各种生态系统的代理之间的安全和可互操作的通信。",
      "shortSummary": "A2A 扩展通过允许开发者添加自定义的、特定领域的功能和方法，增强了 A2A 代理间通信协议。它使代理能够超越核心协议，支持专业化需求，并通过“代理卡”声明实现开放和社区驱动的生态系统。实际应用包括用于评估和诊断的通信可追溯性、Twilio 的延迟感知模型选择，以及 Identity Machines 实现的零信任握手和 Ethereum 提出的代理信任层（ERC-8004）。这极大地提升了多代理系统的灵活性和适应性。",
      "translated_title": "A2A 扩展：赋能自定义代理功能",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Screenshot_2025-09-04_at_13.03.31.original.png",
          "alt": "A2A Extensions",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "A2A Extensions provide a flexible way to add custom functionalities to agent-to-agent communication, going beyond the core A2A protocol. They enable specialized features and are openly defined and implemented."
    },
    {
      "title": "超越反向传播：JAX 的符号能力开启科学计算新领域 (原标题: Beyond backpropagation: JAX's symbolic power unlocks new frontiers in scientific computing)",
      "link": "https://developers.googleblog.com/en/jax-symbolic-power-unlocks-new-frontiers-in-scientific-computing/",
      "pubDate": "Mon, 08 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-08T16:00:00.000Z",
      "creator": "Google",
      "summary": "# JAX 在科学计算中的新突破：超越反向传播\n\nJAX 不仅是大型 AI 模型开发的热门框架，其在更广泛的科学领域，特别是计算密集型领域（如物理信息机器学习）的应用也日益增长。JAX 的核心优势在于其可组合的变换（higher-order functions），例如 `grad` 函数可以接收一个函数并返回其梯度计算函数，并且这些变换可以自由嵌套和组合，这使得 JAX 在处理高阶导数和其他复杂变换时表现出色。\n\n## 解决偏微分方程 (PDEs) 的挑战\n\n新加坡国立大学和 Sea AI Lab 的研究员 Zekun Shi 和 Min Lin 的经验表明，JAX 如何解决科学研究中的根本挑战，尤其是在求解复杂偏微分方程 (PDEs) 时面临的计算瓶颈。他们的工作专注于使用神经网络求解高阶 PDEs。尽管神经网络作为通用函数逼近器具有巨大潜力，但其主要障碍在于需要评估高阶导数，有时高达四阶甚至更高，包括混合偏导数。\n\n### 传统框架的局限性\n\n标准深度学习框架主要针对通过反向传播（backward mode AD）进行模型训练而优化，因此不适合计算高阶导数。重复应用反向传播来计算高阶导数的成本与导数阶数 (k) 呈指数级增长，与域维度 (d) 呈多项式增长。这种“维度诅咒”和导数阶数的指数级扩展使得处理大型、复杂的实际问题几乎不可能。\n\n![维度诅咒](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image1_LSGbsBj.original.jpg)\n*计算图随导数阶数 k 呈指数级增长*\n\n### JAX 的独特解决方案：泰勒模式自动微分 (Taylor Mode AD)\n\n研究人员发现，JAX 是一个“游戏规则改变者”，因为它提供了其他深度学习库所不具备的泰勒模式自动微分 (Taylor mode AD) 能力。JAX 的关键架构特点是其强大的函数表示和变换机制，通过追踪 Python 代码并编译以实现高性能。这种通用设计支持从即时编译到标准导数计算的广泛应用，其底层灵活性使得在其他框架中难以实现的高级操作成为可能。\n\n对于研究人员而言，最关键的应用是 JAX 对泰勒模式 AD 的支持。泰勒模式 AD 通过前向推进函数的泰勒级数展开，并在单次通过中高效计算高阶导数，而不是通过重复、昂贵的反向传播。\n\n![二阶导数的泰勒模式](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image2_z0BG98v.original.jpg)\n*二阶导数的泰勒模式 - 无指数级扩展*\n\n### 随机泰勒导数估计器 (STDE)\n\n利用 JAX 的泰勒模式 AD，研究人员开发了一种算法——随机泰勒导数估计器 (STDE)，以高效地随机化和估计任何微分算子。该方法的核心思想是利用泰勒模式 AD 高效计算 PDEs 中出现的高阶导数张量的收缩。通过构建特殊的随机切向量（或“jets”），他们可以在一次高效的前向传播中获得任意复杂微分算子的无偏估计。\n\n该方法在 NeurIPS 2024 上获得了最佳论文奖，名为“Stochastic Taylor Derivative Estimator: Efficient amortization for arbitrary differential operators”。\n\n### 显著的成果\n\n使用 JAX 中的 STDE 方法，研究人员实现了：\n*   **速度提升：** 相比基线方法，速度提升超过 1000 倍。\n*   **内存减少：** 内存使用减少超过 30 倍。\n*   **问题解决能力：** 能够在单个 NVIDIA A100 GPU 上，在短短 8 分钟内解决一个 100 万维的 PDE，这在以前是无法完成的任务。\n\n这些成果在仅针对标准机器学习工作负载的框架中是无法实现的，因为其他框架虽然高度优化反向传播，但对端到端计算图表示的关注不如 JAX。\n\n## JAX 的通用能力与未来展望\n\n除了泰勒模式 AD，JAX 的模块化设计以及对通用数据类型和函数变换的支持对研究至关重要。在另一项工作“Automatic Functional Differentiation in JAX”中，研究人员甚至将 JAX 推广到处理无限维向量（希尔伯特空间中的函数），通过将其描述为自定义数组并注册到 JAX 中。这使得他们能够重用现有机制来计算泛函和算子的变分导数，这是其他框架完全无法实现的功能。\n\n因此，研究团队不仅在本项目中，还在量子化学等广泛研究领域采用了 JAX。其作为通用、可扩展和符号强大的系统的基本设计，使其成为推动科学计算前沿的理想选择。\n\nJAX 团队鼓励科学界探索 JAX 的科学计算生态系统，分享使用 JAX 解决科学难题的故事，并提供功能需求以指导 JAX 的发展路线图。",
      "shortSummary": "JAX正从AI领域扩展到科学计算，尤其在解决复杂偏微分方程(PDEs)方面展现出巨大潜力。传统框架因高阶导数计算成本高昂而受限，而JAX凭借其独特的泰勒模式自动微分(AD)能力，能高效计算高阶导数。研究人员利用JAX开发了随机泰勒导数估计器(STDE)，实现了超过1000倍的速度提升和30倍内存减少，成功解决了百万维PDE等此前无法处理的问题。JAX的通用性和符号能力使其成为推动科学计算前沿的强大工具。",
      "translated_title": "超越反向传播：JAX 的符号能力开启科学计算新领域",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image1_LSGbsBj.original.jpg",
          "alt": "curse of dimensionality",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image2_z0BG98v.original.jpg",
          "alt": "Taylor-mode for second-order derivative",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "JAX, a framework known for large-scale AI model development, is proving to be a powerful tool in scientific computing, particularly for solving complex Partial Differential Equations (PDEs), now being leveraged by researchers to achieve significant speed-ups and memory reductions in solving high-order PDEs and demonstrating its potential to unlock new frontiers in scientific discovery."
    },
    {
      "title": "Veo 3 和 Veo 3 Fast – 新定价、新配置和更高分辨率 (原标题: Veo 3 and Veo 3 Fast – new pricing, new configurations and better resolution)",
      "link": "https://developers.googleblog.com/en/veo-3-and-veo-3-fast-new-pricing-new-configurations-and-better-resolution/",
      "pubDate": "Sun, 07 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-07T16:00:00.000Z",
      "creator": "Google",
      "summary": "## Veo 3 和 Veo 3 Fast 更新概览\n\n2025年9月8日，Veo平台宣布了三项重大更新，旨在提升视频生成能力并降低成本，同时确保Veo 3和Veo 3 Fast模型在Gemini API中稳定用于规模化生产。\n\n### 主要更新点：\n*   **支持垂直格式输出**：新增9:16宽高比，适用于移动优先和社交媒体场景。\n*   **1080p高清输出**：提供更高分辨率的视频生成选项。\n*   **全新更低定价**：Veo 3和Veo 3 Fast的视频生成价格大幅下调。\n\n### 价格调整\n此次更新显著降低了使用Veo 3和Veo 3 Fast生成高质量视频的成本：\n*   **Veo 3**：价格从每秒$0.75降至**每秒$0.40**。\n*   **Veo 3 Fast**：价格从每秒$0.40降至**每秒$0.15**。\n\n### 新功能：垂直视频和1080p高清输出\n\n**1. 垂直视频支持 (9:16 宽高比)**\n*   现在，用户可以设置`aspectRatio`参数为`9:16`，以生成专为移动设备和社交平台优化的垂直格式视频。\n\n**2. 1080p高清分辨率**\n*   通过将`resolution`参数设置为`1080p`，用户可以生成更高质量的Veo 3和Veo 3 Fast视频。\n\n### MediaSim 演示应用\n\nGoogle AI Studio中提供了一个名为MediaSim的演示应用，展示了如何结合Gemini的多模态理解、生成和图像编辑能力，以及Veo 3的视频和音频生成能力，来创建多模态媒体模拟。该应用集成了tldraw SDK，提供了一个交互式画布。\n\n### 开发者资源与示例\n\n此次发布旨在让开发者更经济高效地构建支持高质量视频生成的应用程序。所有通过Veo生成的视频将继续包含数字SynthID水印。\n\n**Python 示例代码：**\n以下是一个使用`google-genai`库创建视频的基本Python示例，演示了如何设置模型、提示词、负面提示词、宽高比和分辨率：\n\n```python\nimport time\nfrom google import genai\nfrom google.genai import types\n\nclient = genai.Client()\noperation = client.models.generate_videos(\n    model=\"veo-3.0-fast-generate-001\",\n    prompt=\"a close-up shot of a golden retriever playing in a field of sunflowers\",\n    config=types.GenerateVideosConfig(\n        negative_prompt=\"barking, woofing\",\n        aspect_ratio=\"9:16\",\n        resolution=\"720p\",\n    ),\n)\n\n# Waiting for the video(s) to be generated\nwhile not operation.done:\n    time.sleep(20)\n    operation = client.operations.get(operation)\n\nprint(operation)\ngenerated_video = operation.response.generated_videos[0]\nclient.files.download(file=generated_video.video)\ngenerated_video.video.save(\"golden_retriever.mp4\")\n```\n\n**获取更多信息：**\n*   查阅官方文档。\n*   访问Google AI Studio中的Veo 3入门应用。\n*   参考Veo cookbook。",
      "shortSummary": "Veo 3和Veo 3 Fast宣布重大更新：价格大幅下调，Veo 3降至$0.40/秒，Veo 3 Fast降至$0.15/秒。同时，新增支持9:16垂直视频格式和1080p高清输出，以满足移动和社交媒体需求。这些模型已在Gemini API中稳定用于生产，并提供了开发者资源和Python示例，鼓励开发者利用更经济、更高质量的视频生成能力进行创新。",
      "translated_title": "Veo 3 和 Veo 3 Fast – 新定价、新配置和更高分辨率",
      "images": [],
      "contentSource": "完整文章",
      "content": "Today, we’re launching three big Veo updates: support for vertical format outputs (9:16 aspect ratio..."
    },
    {
      "title": "Google AI Edge Gallery：现已支持音频并登陆Google Play (原标题: Google AI Edge Gallery: Now with audio and on Google Play)",
      "link": "https://developers.googleblog.com/en/google-ai-edge-gallery-now-with-audio-and-on-google-play/",
      "pubDate": "Thu, 04 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-04T16:00:00.000Z",
      "creator": "Google",
      "summary": "Google AI Edge 团队已对其 Gemma 3n 预览版进行了扩展，现在包含了音频支持功能。\n\n主要更新和特点：\n\n*   **Gemma 3n 预览版扩展**：Google AI Edge 扩展了其 Gemma 3n 预览版的功能。\n*   **新增音频支持**：此次扩展的核心是增加了对音频的支持，使用户能够体验与音频相关的功能。\n*   **通过 Google AI Edge Gallery 体验**：用户可以通过 Google AI Edge Gallery 应用程序在自己的移动设备上直接体验这些新功能。\n*   **在 Play Store 上推出公开测试版**：Google AI Edge Gallery 现已在 Google Play 商店上作为公开测试版（Open Beta）提供，方便用户下载和使用。",
      "shortSummary": "Google AI Edge 的 Gemma 3n 预览版已扩展，现支持音频功能。用户可通过 Google Play 商店上线的 Google AI Edge Gallery 公开测试版，在自己的手机上体验这些新特性。",
      "translated_title": "Google AI Edge Gallery：现已支持音频并登陆Google Play",
      "images": [],
      "contentSource": "RSS",
      "content": "Google AI Edge has expanded the Gemma 3n preview to include audio support. Users can play with it on their own mobile phone using the Google AI Edge Gallery, which is now available in Open Beta on Play Store."
    },
    {
      "title": "从微调到生产：使用 Dataflow 构建可扩展的嵌入式管道 (原标题: From Fine-Tuning to Production: A Scalable Embedding Pipeline with Dataflow)",
      "link": "https://developers.googleblog.com/en/deploying-embeddinggemma-at-scale-with-dataflow/",
      "pubDate": "Wed, 03 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-03T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 从微调到生产：使用 Dataflow 构建可扩展的嵌入式管道\n\n本文深入探讨了如何利用 Google 的高效开源嵌入模型 EmbeddingGemma，结合 Google Cloud Dataflow 和向量数据库（如 AlloyDB），构建一个可扩展、实时的知识摄取管道。该管道旨在支持语义搜索和检索增强生成（RAG）等现代 AI 应用，实现从模型微调到生产环境的无缝过渡。\n\n## 嵌入式技术与 Dataflow 的强大结合\n\n### 嵌入式技术的核心作用\n*   **数据表示**：嵌入（Embeddings）是数据的数值向量表示，能够捕捉词语和概念之间的深层语义关系。\n*   **AI 应用基石**：它们是理解信息、进行语义搜索以及为大型语言模型（LLMs）提供相关上下文（RAG系统）的基础。\n*   **知识摄取需求**：构建这些应用需要一个强大的知识摄取管道，能够处理非结构化数据，将其转换为嵌入，并加载到专门的向量数据库中。\n\n### EmbeddingGemma 的优势\n*   **高效与开源**：EmbeddingGemma 是 Google 新推出的高效、308M 参数的开源嵌入模型。\n*   **多场景适用**：其小巧的尺寸使其非常适合设备端应用，同时在云端也通过微调解锁了强大的定制能力。\n*   **自包含与安全性**：作为开源模型，EmbeddingGemma 可以完全托管在 Dataflow 内部，消除了对外部网络调用的需求，简化了管理，并能安全处理大规模私有数据集。\n*   **可微调性**：允许用户根据特定数据嵌入需求进行定制。\n*   **卓越的质量**：在 Massive Text Embedding Benchmark (MTEB) 多语言排行榜上，它是 500M 参数以下文本专用多语言嵌入模型中排名最高的。\n\n### Dataflow 的核心优势\nDataflow 是一个全托管、自动扩缩的平台，用于统一的批处理和流式数据处理。将 EmbeddingGemma 直接集成到 Dataflow 管道中，可带来以下优势：\n*   **数据局部性带来的效率**：处理直接在 Dataflow 工作器上进行，避免了对独立推理服务的远程过程调用（RPC），解决了配额和多系统自动扩缩的问题，减少了资源占用。\n*   **统一系统**：单一系统处理自动扩缩、观察和监控，简化了运维开销。\n*   **可扩展性和简洁性**：Dataflow 根据需求自动扩缩管道，Apache Beam 的转换功能减少了样板代码。\n\n## 使用 Dataflow ML 构建摄取管道\n\n一个典型的知识摄取管道包含四个阶段：从数据源读取、数据预处理、生成嵌入和写入向量数据库。\n\n![Dataflow 的 MLTransform](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/EGDataflow_Graph_RD1-V01.original.jpg)\n\n借助 Dataflow 的 `MLTransform`（一个用于数据准备的强大 `PTransform`），整个工作流只需几行代码即可实现。\n\n### 使用 MLTransform 生成 Gemma 嵌入\n\n文章提供了一个示例，展示如何配置 `MLTransform` 以使用 Hugging Face 模型（如 EmbeddingGemma），然后将结果写入 AlloyDB，以便进行语义搜索。AlloyDB 等数据库能够将语义搜索与结构化搜索结合，提供高质量和相关的结果。\n\n核心步骤包括：\n1.  **定义模型**：指定用于生成嵌入的模型名称，例如 `google/embeddinggemma-300m`。用户可以轻松将其替换为自己的微调模型。\n2.  **定义嵌入转换**：使用 `SentenceTransformerEmbeddings` 定义转换，指定要嵌入的列。\n3.  **定义写入配置**：使用 `AlloyDBVectorWriterConfig` 定义写入到向量数据库的配置。Dataflow 支持通过配置对象写入多种向量数据库，包括 AlloyDB、CloudSQL 和 BigQuery。\n4.  **构建并运行管道**：通过 Apache Beam 管道，将数据创建、嵌入生成（通过 `MLTransform`）和写入向量数据库（通过 `VectorDatabaseWriteTransform`）串联起来。\n\n这种简单而强大的模式允许用户在单个、可扩展、经济高效且托管的管道中并行处理海量数据集，使用 EmbeddingGemma 生成嵌入，并填充向量数据库。\n\n## 立即开始\n\n结合最新的 Gemma 模型、Dataflow 的可扩展性以及 AlloyDB 等向量数据库的向量搜索能力，用户可以轻松构建复杂的下一代 AI 应用。文章鼓励读者探索 Dataflow ML 文档，特别是关于数据准备和生成嵌入的部分，并尝试使用 EmbeddingGemma 的示例 notebook。对于大规模服务器端应用，建议通过 Gemini API 探索最先进的 Gemini Embedding 模型以获得最大性能和容量。",
      "shortSummary": "Google Cloud Dataflow与EmbeddingGemma模型结合，提供了一个可扩展、实时的知识摄取管道。EmbeddingGemma是Google高效、可微调的开源嵌入模型，在MTEB排行榜表现卓越。Dataflow作为全托管、自动扩缩的数据处理平台，可将EmbeddingGemma直接集成，实现高效的数据局部性处理和统一管理。通过Dataflow的MLTransform，用户能轻松将非结构化数据转换为嵌入，并写入AlloyDB等向量数据库，支持语义搜索和RAG等AI应用，简化从微调到生产的流程。",
      "translated_title": "从微调到生产：使用 Dataflow 构建可扩展的嵌入式管道",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/EGDataflow_Graph_RD1-V01.original.jpg",
          "alt": "Dataflow's MLTransform",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "Learn how to use Google's EmbeddingGemma, an efficient open model, with Google Cloud's Dataflow and vector databases like AlloyDB to build scalable, real-time knowledge ingestion pipelines."
    },
    {
      "title": "介绍 EmbeddingGemma：用于设备端嵌入的最佳开源模型 (原标题: Introducing EmbeddingGemma: The Best-in-Class Open Model for On-Device Embeddings)",
      "link": "https://developers.googleblog.com/en/introducing-embeddinggemma/",
      "pubDate": "Wed, 03 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-03T16:00:00.000Z",
      "creator": "Google",
      "summary": "Google DeepMind 推出了 **EmbeddingGemma**，这是一款全新的开源嵌入模型，旨在为设备端 AI 提供同类最佳的性能。\n\n### 核心特性与优势\n\n*   **卓越性能与效率**：\n    *   拥有 3.08 亿参数，其设计高度高效，使其成为同等规模模型中的佼佼者。\n    *   在 Massive Text Embedding Benchmark (MTEB) 上，是 5 亿参数以下排名最高的开源多语言文本嵌入模型。\n    *   基于 Gemma 3 架构，经过 100 多种语言训练，量化后可在不到 200MB 的 RAM 中运行。\n    *   性能可与参数量近乎两倍的流行模型相媲美。\n    ![MTEB Score](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/EmbeddingGemma_Chart01_RD3-V01.original.png)\n\n*   **灵活的离线工作能力**：\n    *   体积小巧、速度快、效率高，支持可定制的输出维度（通过 Matryoshka 表示从 768 到 128）。\n    *   具有 2K 令牌上下文窗口，可在手机、笔记本电脑、台式机等日常设备上运行。\n    *   与 Gemma 3n 协同工作，为移动 RAG 管道和语义搜索等应用解锁新用例。\n\n*   **广泛的工具集成**：\n    *   已与 sentence-transformers, llama.cpp, MLX, Ollama, LiteRT, transformers.js, LMStudio, Weaviate, Cloudflare, LlamaIndex, LangChain 等流行工具集成，方便开发者快速上手。\n\n### 如何赋能移动优先的 RAG 管道\n\n*   EmbeddingGemma 生成文本的数值表示（嵌入），将其转换为高维空间中的向量以表示含义。\n*   在 RAG 管道中，嵌入的质量对于检索相关上下文至关重要。高质量的嵌入能确保检索到准确的文档，从而生成上下文相关的答案。\n*   EmbeddingGemma 的高性能确保了高质量的表示，为准确可靠的设备端应用提供支持。\n    ![MTEB Multilingual v2](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/EmbeddingGemma_Chart02_RD3-V01.original.png)\n\n### 卓越的尺寸性能\n\n*   在检索、分类和聚类等任务上，EmbeddingGemma 在同等大小的流行嵌入模型中表现出色，尤其在多语言嵌入生成方面性能强劲。\n\n### 小巧、快速、高效的设计\n\n*   3.08 亿参数模型由约 1 亿模型参数和 2 亿嵌入参数组成，专为性能和最小资源消耗而设计。\n*   利用 Matryoshka 表示学习 (MRL) 提供多种嵌入尺寸（768、512、256 或 128 维度），兼顾质量、速度和存储成本。\n*   在 EdgeTPU 上，256 个输入令牌的嵌入推理时间小于 15 毫秒，实现实时响应。\n*   通过量化感知训练 (QAT)，显著降低 RAM 使用量至 200MB 以下，同时保持模型质量。\n\n### 离线优先的设计理念\n\n*   赋能开发者构建设备端、灵活且注重隐私的应用，直接在设备硬件上生成文档嵌入，确保敏感用户数据安全。\n*   使用与 Gemma 3n 相同的分词器进行文本处理，减少 RAG 应用的内存占用。\n\n### 新的解锁能力\n\n*   无需互联网连接即可同时搜索个人文件、文本、电子邮件和通知。\n*   通过与 Gemma 3n 结合，实现个性化、行业特定和离线启用的聊天机器人。\n*   分类用户查询以进行相关函数调用，帮助移动代理理解。\n*   支持针对特定领域、任务或语言进行微调。\n\n### 如何选择合适的嵌入模型\n\n*   **设备端、离线用例**：EmbeddingGemma 是最佳选择，针对隐私、速度和效率进行了优化。\n*   **大多数大规模、服务器端应用**：通过 Gemini API 探索最先进的 Gemini Embedding 模型，以获得最高质量和最大性能。\n\n### 立即开始使用 EmbeddingGemma\n\n*   模型权重可在 Hugging Face、Kaggle 和 Vertex AI 上获取。\n*   提供详细文档、推理和微调指南，以及 Gemma Cookbook 中的快速入门 RAG 示例。\n*   支持 transformers.js, MLX, llama.cpp, LiteRT, Ollama, LMStudio, Weaviate 等流行设备端 AI 工具。",
      "shortSummary": "Google DeepMind 发布了 EmbeddingGemma，这是一款同类最佳的开源嵌入模型，专为设备端 AI 设计。它拥有 3.08 亿参数，高效、多语言，能在离线状态下运行，并支持 RAG 和语义搜索。EmbeddingGemma 性能卓越，可与更大模型媲美，且内存占用极低（<200MB RAM），提供灵活的嵌入尺寸。它确保数据隐私，并已集成到多种流行工具中，为移动优先的 AI 应用提供了强大支持。",
      "translated_title": "介绍 EmbeddingGemma：用于设备端嵌入的最佳开源模型",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/EmbeddingGemma_Chart01_RD3-V01.original.png",
          "alt": "MTEB Score",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/EmbeddingGemma_Chart02_RD3-V01.original.png",
          "alt": "MTEB Multilingual v2",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "Introducing EmbeddingGemma: a new embedding model designed for efficient on-device AI applications from Google. This open model is the highest-ranking text-only multilingual embedding model under 500M parameters on the MTEB benchmark, enabling powerful features like RAG and semantic search directly on mobile devices without an internet connection."
    }
  ],
  "lastUpdated": "2025-09-23T10:29:01.548Z"
}