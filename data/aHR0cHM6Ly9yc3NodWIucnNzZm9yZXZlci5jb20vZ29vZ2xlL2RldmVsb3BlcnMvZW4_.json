{
  "sourceUrl": "https://rsshub.rssforever.com/google/developers/en",
  "title": "Google Developers Blog",
  "description": "Google Developers Blog - Powered by RSSHub",
  "link": "https://developers.googleblog.com",
  "items": [
    {
      "title": "使用 Veo 3 进行构建，现已在 Gemini API 中推出 (原标题: Build with Veo 3, now available in the Gemini API)",
      "link": "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
      "pubDate": "Wed, 16 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-16T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 使用 Veo 3 进行构建，现已在 Gemini API 中推出\n\nGoogle 于2025年7月17日宣布，通过 Gemini API 和 Vertex AI 向开发者提供 Veo 3 的付费预览。Veo 3 是 Google 在2025年 Google I/O 大会上首次亮相的视频模型，全球用户已使用它生成了数千万高质量视频。\n\n### Veo 3 的核心能力\nVeo 3 是 Google 首个集成高保真视频输出和原生音频的视频模型，目前支持文本到视频生成，并将很快支持图像到视频。\n\n### 开发者应用案例\n开发者已开始利用 Veo 3 进行实验，探索其在内容构思、快速迭代和提高效率方面的潜力：\n\n*   **Cartwheel:** 开发了一个系统，能将人类的2D视频转换为生产就绪的3D动画。Cartwheel 利用 Veo 3 生成逼真流畅的人体动作，然后将其转化为客户所需的3D动画。\n*   **Volley:** 使用 Veo 3 制作游戏内的过场动画以推进故事。Veo 3 帮助 Volley 的设计师快速迭代游戏内容，为即将推出的 RPG 游戏《Wit's End》提供最佳输出。\n\n### Veo 3 的主要功能\nVeo 3 旨在处理各种视频生成任务，从电影叙事到动态角色动画，不仅能生成令人惊叹的视觉效果，还能生成对话和音效等音频，从而创造更沉浸式的体验：\n\n*   **同步音频 (Synchronized Sound):** 原生生成丰富的音频（对话、效果和音乐），并与视频同步。\n*   **电影级质量 (Cinematic Quality):** 制作高质量、高清晰度的视频，捕捉提示中的创意细节，包括复杂的纹理和微妙的光影效果。\n*   **真实物理效果 (Realistic Physics):** 模拟真实世界物理，实现真实的运动，如自然的角色动作、准确的水流和阴影投射。\n\n### 获取与定价\n\n*   **可用性:** Veo 3 可通过 Gemini API（付费预览）和 Vertex AI 获得。它也在 Google AI Studio 中作为 SDK 模板和交互式 Starter App 提供，方便付费层级用户快速原型开发。此外，Google AI 订阅者可在 Gemini 应用和 Flow 中使用 Veo 3，企业客户则可通过 Vertex AI 访问。\n*   **定价:** 视频和音频输出的定价为每秒0.75美元。未来将推出 Veo 3 Fast，提供更快速、更经济的视频创作选项。\n*   **负责任的构建:** 所有由 Veo 3 模型生成的视频都将包含数字 SynthID 水印。\n*   **入门资源:** 开发者可以通过查阅文档、教程（cookbook）和 Veo 3 入门应用（付费层级）来开始使用。",
      "shortSummary": "Google已通过Gemini API和Vertex AI向开发者推出Veo 3的付费预览。Veo 3是首个集成高保真视频和原生音频的视频模型，支持文本到视频，并即将支持图像到视频。它提供同步音频、电影级质量和真实物理效果，帮助开发者快速迭代和高效创作。定价为每秒0.75美元，未来将推出更快的Veo 3 Fast。所有生成视频均含SynthID水印，可通过Google AI Studio等平台访问。",
      "translated_title": "使用 Veo 3 进行构建，现已在 Gemini API 中推出",
      "images": [],
      "contentSource": "完整文章",
      "content": "Veo 3, Google’s latest AI video generation model, is now available in paid preview via the Gemini API and Google AI Studio. Unveiled at Google I/O 2025, Veo 3 can generate both video and synchronized audio, including dialogue, background sounds, and even animal noises. This model delivers realistic visuals, natural lighting, and physics, with accurate lip syncing and sound that matches on-screen action."
    },
    {
      "title": "解锁 Gemini 的推理：Vertex AI 上 Logprobs 的分步指南 (原标题: Unlock Gemini’s reasoning: A step-by-step guide to logprobs on Vertex AI)",
      "link": "https://developers.googleblog.com/en/unlock-gemini-reasoning-with-logprobs-on-vertex-ai/",
      "pubDate": "Tue, 15 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-15T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 解锁 Gemini 的推理：Vertex AI 上 Logprobs 的分步指南\n\n本文详细介绍了如何在 Vertex AI 上的 Gemini API 中使用 Logprobs 功能，以深入理解模型的决策过程，并将其应用于构建更智能、更可靠的应用程序。\n\n## 什么是 Logprobs？\n\nLogprobs（对数概率）是模型分配给某个词元（token）的概率分数的自然对数。理解其关键概念有助于解释模型置信度：\n\n*   **概率**：介于 0 到 1 之间。\n*   **自然对数**：0 到 1 之间任何数的自然对数都是负数。\n*   **对数概率为 0**：表示 100% 的确定性（即概率为 1）。\n*   **解释**：Logprob 分数越接近 0，表示模型对其选择的置信度越高。\n\n## 环境设置\n\n在开始使用 Logprobs 之前，需要进行以下环境配置：\n\n1.  **安装 Google GenAI SDK**：使用 `pip install -U -q google-genai` 命令安装或升级 Python SDK。\n2.  **配置 Google Cloud 项目**：设置您的 `PROJECT_ID` 和 `location`（例如 `global`）以进行身份验证。\n3.  **初始化客户端和模型**：创建 `genai.Client` 实例，并指定要使用的 Gemini 模型（例如 `gemini-2.5-flash`）。\n\n## 启用和处理 Logprobs\n\n通过在请求的 `generation_config` 中设置以下两个参数来启用 Logprobs：\n\n*   `response_logprobs=True`：指示模型返回其输出中选择的词元的对数概率（默认为 `False`）。\n*   `logprobs=[integer]`：要求模型同时返回指定数量（1 到 20）的每个步骤中排名靠前的替代词元的对数概率。\n\n文章提供了一个示例，展示了如何使用 Logprobs 对句子进行分类，并提供了一个 `print_logprobs` 辅助函数来以可读格式打印结果，详细展示了模型对每个词元的预测及其替代词元。\n\n**输出解读示例**：\n\n如果模型返回 `Token: 'Neutral' (-0.0214)`，以及替代词元如 `'Positive': (-4.8219)` 和 `'Negative': (-5.6293)`，这表明模型对“Neutral”的置信度极高（-0.0214 非常接近 0），而对其他选项的置信度则低得多。\n\n## Logprobs 的应用场景\n\nLogprobs 不仅仅是一个调试工具，它能帮助开发者构建更智能、更可靠、更具上下文感知能力的应用程序。\n\n### 1. 更智能的分类\n\nLogprobs 将分类从简单的答案转变为透明的决策过程，从而构建更强大的系统：\n\n*   **检测歧义以进行人工审查**：当模型对分类结果的置信度不高时（例如，前两个选项的 Logprobs 差异很小），可以将其标记为需要人工审查。\n*   **基于置信度的阈值设置**：将所选词元的 Logprob 转换回原始概率分数（使用 `math.exp()`），然后根据预设的置信度阈值（例如 90%）决定是否接受分类结果。\n\n### 2. 动态自动补全\n\n通过反复查询模型并检查其顶级候选词元的 Logprobs，可以构建一个动态的自动补全功能，实时提供最相关的下一个词建议。\n\n文章通过模拟用户输入“The best thing about living in Toronto is the”来展示此功能。随着上下文的增加，模型对下一个词的预测变得越来越具体和准确。\n\n![自动补全分析](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image1_Gl9YFyT.original.png)\n\n**分析**：\n\n*   对于“The”这样的通用词，建议是通用的起始词。\n*   当上下文完整为“The best thing about living in Toronto is the”时，模型能自信地预测出多伦多的具体属性，如“diversity”（多样性）和“food”（美食），这展示了其深层的上下文理解能力。\n\n### 3. 定量 RAG 评估\n\nLogprobs 可以用于评估检索增强生成（RAG）系统，衡量其答案与检索到的上下文的一致性。\n\n当大型语言模型（LLM）拥有相关上下文时，其生成事实一致答案的置信度会提高，这反映在更高的 Logprobs 上。通过计算生成答案的平均 Logprob，可以获得一个“基础性”或“置信度”分数。\n\n文章设置了虚构的知识库，并测试了三种场景：良好检索、不良检索和无检索。\n\n![Logprobs 图表](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/logprobs_chart.original.jpg)\n\n**分析**：\n\n结果显示出清晰的相关性。“良好检索”场景的得分最高（最接近零），因为模型对其答案得到文本支持的置信度很高。这使得 Logprobs 成为自动化评估和改进 RAG 系统的强大指标。\n\n## 展望\n\nLogprobs 功能为开发者提供了深入了解模型决策过程的能力，并可应用于分析分类结果、构建动态自动补全功能和评估 RAG 系统。这只是该功能众多应用场景中的一小部分，鼓励开发者进一步探索。更多信息可查阅入门笔记本和 Gemini API 官方文档。",
      "shortSummary": "本文介绍了 Vertex AI 上 Gemini API 中新推出的 Logprobs 功能。该功能允许开发者查看模型选择的词元及其替代词元的概率分数，从而深入理解模型的推理过程。文章详细讲解了 Logprobs 的概念、启用方法，并展示了其在智能分类（检测歧义、置信度阈值）、动态自动补全和定量 RAG 评估等关键应用场景中的实际价值，帮助开发者构建更智能、更可靠的 AI 应用。",
      "translated_title": "解锁 Gemini 的推理：Vertex AI 上 Logprobs 的分步指南",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image1_Gl9YFyT.original.png",
          "alt": "auto-complete analysis",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/logprobs_chart.original.jpg",
          "alt": "logprobs_chart",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "The `logprobs` feature has been officially introduced in the Gemini API on Vertex AI, provides insight into the model's decision-making by showing probability scores for chosen and alternative tokens. This step-by-step guide will walk you through how to enable and interpret this feature and apply it to powerful use cases such as confident classification, dynamic autocomplete, and quantitative RAG evaluation."
    },
    {
      "title": "斯坦福大学的Marin基础模型：首个使用JAX开发的完全开放模型 (原标题: Stanford’s Marin foundation model: The first fully open model developed using JAX)",
      "link": "https://developers.googleblog.com/en/stanfords-marin-foundation-model-first-fully-open-model-developed-using-jax/",
      "pubDate": "Tue, 15 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-15T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 斯坦福大学Marin基础模型：首个使用JAX开发的完全开放模型\n\n## 引言：扩展AI模型“开放”的定义\n\n当前AI时代，强大的基础模型被开放共享，加速了创新。斯坦福大学CRFM（基础模型研究中心）的Marin项目旨在扩展“开放”的定义，使其涵盖模型背后的整个科学过程。该项目被设计为一个“开放实验室”，目标不仅是共享模型本身，还要公开完整的研发历程，包括代码、数据集、数据方法、实验、超参数和训练日志。这种高水平的透明度通过提供一个独特、完全可复现的资源，补充了现有生态系统，使研究人员能够深入审查、构建和信任所开发的模型。Marin项目致力于为基础模型研究营造一个更透明、更易于访问的未来。\n\n![AI模型开放性谱系](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/ai-model-openness-spectrum-stanford-marin-open-.original_4oMY0az.png)\n\n## 核心发布与技术栈\n\n该“开放实验室”的首批发布是Marin-8B-Base和Marin-8B-Instruct模型。秉承项目原则，模型、数据、代码和分词器均在宽松的Apache 2.0许可下发布。实现完全可复现性是一个艰巨的工程问题，需要控制大规模分布式系统中的每一个变异源。项目的成功取决于一个能够在大规模下提供可复现性保证，并最大限度提高效率以训练具有领先性价比的基础模型的技术栈。团队选择了JAX作为基础，并构建了一个新的框架——Levanter，以充分利用JAX的强大功能。\n\n## 构建开放基础模型的核心挑战与解决方案\n\n为了成功创建真正开放、可扩展和可复现的基础模型，CRFM团队解决了以下几个工程挑战：\n\n### 1. 在单个加速器上实现最大速度\n\n*   **问题**：核心训练循环执行数十亿次，Python等解释型语言的开销会造成巨大的性能瓶颈。操作逐步调度还会导致过多的内存流量和开销，尤其是在TPU等硬件上，吞吐量取决于高效执行融合操作。\n*   **解决方案**：\n    *   Levanter将整个多阶段训练步骤（前向传播、损失、反向传播和更新）封装成一个单一函数，并使用`@jax.jit`装饰器。JAX的XLA编译器将整个过程转换为一个高度优化的机器码内核，融合操作以最大限度地利用硬件。\n    *   使用`jax.value_and_grad`在一次传递中计算损失及其梯度，避免冗余计算。\n    *   JAX还支持梯度检查点等高级技术，节省内存并允许使用更大的批次大小。\n    *   Levanter还使用了JAX强大的Pallas-based Splash Attention内核，这是几乎所有大型语言模型核心的关键操作——点积注意力的高度优化实现。\n\n### 2. 管理大规模并行计算的复杂性\n\n*   **问题**：训练最先进的模型需要扩展到数千个加速器芯片。手动管理模型和数据的分区以及设备间的通信极其复杂，代码很快变得难以阅读、调试和适应。\n*   **解决方案**：\n    *   JAX的`@jax.jit`装饰器无缝支持单程序多数据（SPMD）并行化，自动化底层数据分片和通信。XLA编译器自动调度加速器之间的通信，最大限度地减少网络等待时间，最大化计算时间。\n    *   为了使`jit`的功能更易于安全使用，Levanter开发了Haliax库，用于命名张量。通过使用人类可读的名称（如“embed”或“batch”）而不是位置索引来引用张量轴，代码变得自文档化且健壮。\n    *   这种抽象允许通过更改配置文件中的几行代码，轻松定义和修改复杂的分片策略，如完全分片数据并行（FSDP）和张量并行，而无需触及模型代码。\n\n### 3. 构建和管理弹性、高成本效益的计算集群\n\n*   **问题**：大规模训练需要灵活访问大型计算集群。项目严重依赖抢占式TPU实例来管理成本，这意味着需要一种方法来轻松地将许多较小、分散的TPU切片组合成一个逻辑集群，并对频繁中断具有弹性。\n*   **解决方案**：\n    *   利用Google Cloud TPU Multislice技术，允许训练作业使用多个TPU切片，如同一个大型系统。\n    *   Levanter使用Ray来编排此过程，在训练作业期间无缝扩展或缩减TPU切片数量，并确保在任何单个切片被抢占时作业仍保持弹性。\n    *   得益于JAX和XLA，Levanter和Marin在GPU上也获得了类似的高性能结果。\n\n### 4. 通过完美可复现性培养科学信任\n\n*   **问题**：Marin项目的核心目标是实现可验证的科学。这要求即使在训练暂停、重启或在不同机器配置之间移动时也能获得可复现的结果，这是一个重大的技术障碍。\n*   **解决方案**：\n    *   这是驱动Levanter设计的根本要求。项目选择JAX正是因为其强大的可复现性保证，例如其默认使用确定性伪随机数生成器（PRNGs）。\n    *   Marin-8B的训练验证了这一选择，该训练涉及在不同TPU切片和硬件类型之间迁移，同时在抢占发生时成功保持了逐比特的可复现性。\n    *   Levanter还包括一个基于Google Tensorstore库的健壮数据加载系统。Levanter的数据存储提供对任何批次训练数据的确定性、随机访问，无论作业重启或数据源更改如何，这对于支持中期训练等高级训练策略至关重要。JAX的确定性和Levanter的数据存储也使可解释性研究人员能够轻松理解特定数据在训练期间如何影响模型。\n\n### 5. 创建一个内聚的框架\n\n*   **问题**：虽然JAX提供了强大的引擎，但没有现有高级框架能满足项目对可读性、大规模可扩展性和逐比特确定性的严格综合要求。需要一个完整、有主见的系统来编排整个训练过程。\n*   **解决方案**：\n    *   从头开始构建了Levanter，一个JAX原生的框架，旨在成为所需系统：逐比特确定性、可扩展并具有高级分布式策略，以及弹性。\n    *   JAX不仅仅是一个库，它是一个用于构建新工具的“元框架”。Levanter建立在其成熟、高性能的TPU支持以及高级抽象（jit）与低级控制（Pallas）的无缝集成之上。\n    *   这种方法在JAX社区中很常见，该社区已经产生了Flax、Equinox、Orbax和Optax等充满活力的库生态系统，它们协同工作，使像Marin这样的团队能够构建强大的解决方案。\n\n## Marin-8B的训练历程：“Tootsie”过程\n\n上述原则、工具和库在Marin-8B的训练运行中得到了实施和应用。模型架构是Llama风格的Transformer。\n\n![Marin 8B-Base模型架构一览](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Marin-8B-Base-model-architecture_2.original.png)\n\nMarin-8B的训练是一个自适应的旅程，而非静态、单一的运行，内部称之为“Tootsie”过程。这一真实研究工作流程的诚实描绘已公开。该过程涵盖了超过12万亿个token，涉及多个阶段，适应了新数据、技术，甚至不同的硬件配置——在训练过程中在大型、多切片TPU配置（2x v5e-256到1x v4-2048 pods）之间迁移。团队持续优化数据混合，纳入更高质量的来源，并调整学习率和批次大小等超参数以优化性能。这种“混乱”的现实是一个强大的教育工具，而JAX和Levanter堆栈在处理这些重大转变同时保持逐比特可复现性的能力，有力地证明了其鲁棒性。\n\n## 加入Marin社区\n\nMarin项目是参与基础模型开发和为JAX生态系统做出贡献的公开邀请。Marin的旅程回答了“开放的下一步是什么？”这个问题。这项创建“开放实验室”的努力得益于JAX生态系统的技术能力。其性能、可移植性和为可复现性而设计的底层架构是使研究“完整旅程”可访问的关键要素。\n\n通过共享从数据方法到训练日志的一切，项目旨在提供一个完全可复现的资源——一个赋能研究人员深入审查、构建和信任AI工作的资源。项目相信这是迈向AI更透明未来的协作一步。邀请您加入这个“开放实验室”——使用Marin，为研究做出贡献，并帮助构建下一波创新和值得信赖的基础模型。\n\n项目的核心资源是官方网站marin.community。在那里，您可以找到Hugging Face上发布的模型，在GitHub上探索“开放实验室”，阅读Marin文档，并深入了解Levanter训练框架。您还可以在Colab中通过一个简单的推理示例试用Marin。活跃的讨论正在Discord频道中进行，您可以在那里直接与其他开发者互动。对于JAX生态系统的新手，官方JAX文档提供了优秀的资源，包括快速入门指南。",
      "shortSummary": "斯坦福大学的Marin项目发布了首个使用JAX开发的完全开放基础模型，旨在将AI模型的“开放”定义扩展至整个科学研究过程。该项目发布了Marin-8B模型和Levanter框架，强调代码、数据和训练日志的完全可复现性与透明度。JAX的强大功能解决了速度、并行、集群管理和确定性复现等核心挑战，实现了“开放实验室”愿景。此举旨在促进AI研究的信任、协作与透明化，并邀请社区参与贡献。",
      "translated_title": "斯坦福大学的Marin基础模型：首个使用JAX开发的完全开放模型",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/ai-model-openness-spectrum-stanford-marin-open-.original_4oMY0az.png",
          "alt": "The Spectrum of AI Model Openness",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Marin-8B-Base-model-architecture_2.original.png",
          "alt": "Marin 8B-Base model architecture at a glance",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "The Marin project aims to expand the definition of 'open' in AI to include the entire scientific process, not just the model itself, by making the complete development journey accessible and reproducible. This effort, powered by the JAX framework and its Levanter tool, allows for deep scrutiny, trust in, and building upon foundation models, fostering a more transparent future for AI research."
    },
    {
      "title": "使用 ADK 和 Gemini CLI 简化你的 Agent ‘氛围构建’流程 (原标题: Simplify your Agent \"vibe building\" flow with ADK and Gemini CLI)",
      "link": "https://developers.googleblog.com/en/simplify-agent-building-adk-gemini-cli/",
      "pubDate": "Tue, 15 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-15T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 使用 ADK 和 Gemini CLI 简化 Agent 开发流程\n\n本文介绍了 Agent Development Kit (ADK) 的最新更新，以及其与 Gemini CLI 结合使用如何显著简化和加速 AI Agent 的开发过程，旨在消除开发中的摩擦，提升开发者的“直觉编程”（vibe coding）体验。\n\n### 消除开发摩擦\n\n对于开发者而言，进入“心流”状态至关重要，而文档阅读、框架选择和上下文切换等因素常常会打断这种状态。ADK 与 Gemini CLI 的结合旨在解决这些痛点，通过在开发者与 Gemini 之间建立无缝对话，将高层级想法迅速转化为功能性 Agent，从而让开发者能将更多时间用于解决实际问题，而非从零开始编写代码。\n\n### 核心机制：优化的 llms-full.txt 文件\n\n此次升级的核心是 ADK 仓库中经过改进的 `llms-full.txt` 文件。该文件可视为 ADK 框架的精简指南，详细列出了所有组件、功能和最佳实践。其主要改进包括：\n\n*   **文件大小缩减**：长度缩短了 50% 以上，减少了 token 消耗。\n*   **LLM 友好性**：更易于大型语言模型（LLMs）理解。\n\n这些改进使得 Gemini CLI 能够**全面理解 ADK 框架**，而不会占用过多的上下文窗口或出现“上下文腐烂”问题。这显著降低了成本，并提高了代码生成的准确性。Gemini CLI 获得了对框架的深度原生理解，能够将高层级计划直接转化为准确、符合习惯的多 Agent 代码，从而**大幅加速原型开发速度**，使开发者能够快速实验和迭代。\n\n### 实战演示：构建 AI GitHub 问题标签 Agent\n\n文章通过一个实际案例展示了 ADK 和 Gemini CLI 的强大功能：构建一个 AI Agent 来自动标记 GitHub 问题，以便于优先级排序和分配。整个过程仅需几个简单步骤：\n\n1.  **步骤 0：下载 llms-full.txt**\n    *   将 `llms-full.txt` 文件从 `adk-python` 仓库下载到工作目录。\n\n2.  **步骤 1：与 Gemini CLI 构思计划**\n    *   开发者向 Gemini CLI 描述构建 AI Agent 的高层级目标（例如，使用 ADK 框架和 Gemini 为 GitHub 仓库问题打标签）。\n    *   Gemini CLI 随即生成一个详细的四阶段计划：\n        *   **阶段 1：项目设置与配置**：初始化项目、安装依赖（`google-adk`、`PyGithub`）、配置 GitHub 认证（PAT）。\n        *   **阶段 2：开发 GitHub 自定义工具**：创建 `github_tools.py` 文件，包含 `get_issue`、`get_available_labels` 和 `apply_label` 等函数，供 Agent 调用。\n        *   **阶段 3：构建标签 Agent**：在 `main.py` 中定义 Agent，配置 Gemini 模型（如 `gemini-1.5-flash`），集成自定义工具，并编写核心指令（prompt）指导 Agent 的工作流程（读取问题 -> 获取标签 -> 分析并选择 -> 应用标签）。\n        *   **阶段 4：创建应用程序入口点**：在 `main.py` 中创建简单的 CLI 接口，接受仓库名和问题编号作为输入，触发 Agent 执行标签任务。\n    *   在此阶段，开发者无需编写任何代码，即可定义完整的逻辑。\n\n3.  **步骤 2：将计划转化为 ADK Agent 代码**\n    *   开发者指示 Gemini CLI 使用 `llms-full.txt` 生成基于上述计划的 Python ADK 代码。\n    *   Gemini CLI 立即生成了完整的、功能性的 Agent 应用程序代码，包括 `labeling_agent` 的定义（使用 `google.adk.agents`、`google.genai` 等）和 `github_tools.py` 中的具体函数实现。文章提到还生成了 `requirements.txt` 和 `label_config.py`。\n    *   由于 `llms-full.txt` 提供了必要的上下文，CLI 自动处理了所有样板代码。\n\n4.  **步骤 3 & 4：在心流中测试和改进**\n    *   代码生成后，开发者可以立即进行本地测试。如果发现 bug 或有改进想法（例如，添加新的标签或在 Agent 完成后生成摘要），可以直接向 Gemini CLI 提出修改请求。\n    *   Gemini CLI 会在保持上下文的情况下重构代码，无需手动编辑，从而保持开发者的心流状态。\n\n5.  **步骤 5：循环迭代**\n    *   新的工作流程形成了一个高效的迭代循环：\n        1.  构思 Agent 逻辑。\n        2.  使用 Gemini CLI 生成 ADK 代码。\n        3.  即时测试。\n        4.  通过简单的对话请求进行改进。\n    *   这种循环允许开发者持续优化 Agent，而不会受到传统开发中常见的摩擦干扰。\n\n### 总结\n\nADK 与 Gemini CLI 的结合提供了一种前所未有的快速、流畅的 Agent 开发体验。通过利用优化的 `llms-full.txt` 文件，Gemini CLI 能够深度理解 ADK 框架，将高层级想法迅速转化为可运行的代码，并支持在开发过程中进行无缝的迭代和改进，从而让开发者能够更专注于解决实际问题。",
      "shortSummary": "ADK (Agent Development Kit) 与 Gemini CLI 的结合显著简化了 AI Agent 开发。通过优化的 `llms-full.txt` 文件，Gemini CLI 能深度理解 ADK 框架，将高层级想法迅速转化为准确的 Agent 代码。这加速了原型开发，并支持在开发过程中通过对话进行无缝迭代。开发者能将更多精力放在解决实际问题上，而非繁琐的编码工作，从而实现更流畅、高效的“直觉编程”体验。",
      "translated_title": "使用 ADK 和 Gemini CLI 简化你的 Agent ‘氛围构建’流程",
      "images": [],
      "contentSource": "完整文章",
      "content": "The updated Agent Development Kit (ADK) simplifies and accelerates the process of building AI agents by providing the CLI with a deep, cost-effective understanding of the ADK framework, allowing developers to quickly ideate, generate, test, and improve functional agents through conversational prompts, eliminating friction and keeping them in a productive \"flow\" state."
    },
    {
      "title": "Gemini Embedding 现已在 Gemini API 中普遍可用 (原标题: Gemini Embedding now generally available in the Gemini API)",
      "link": "https://developers.googleblog.com/en/gemini-embedding-available-gemini-api/",
      "pubDate": "Sun, 13 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-13T16:00:00.000Z",
      "creator": "Google",
      "summary": "## Gemini Embedding 现已普遍可用\n\nGoogle 于2025年7月14日宣布，其首个 Gemini Embedding 文本模型（`gemini-embedding-001`）现已在 Gemini API 和 Vertex AI 中向开发者普遍可用。\n\n### 核心亮点与性能\n\n*   **领先地位**：自2025年3月实验性发布以来，该嵌入模型在 Massive Text Embedding Benchmark (MTEB) 多语言排行榜上一直名列前茅。\n*   **卓越表现**：在检索、分类等多种任务中，其性能超越了 Google 此前的文本嵌入模型以及其他商业产品。\n*   **统一体验**：`gemini-embedding-001` 在科学、法律、金融和编码等多个领域提供统一的尖端体验。\n\n以下是 Gemini Embedding 与其他商业专有模型的比较：\n\n![嵌入模型对比图](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/EmbedingsChart_16x9_RD2-V01.original.jpg)\n\n*注：图表中“传统 Google 模型”结合了3个 Gemini API 和 Vertex AI 模型（`text-embedding-004`、`text-embedding-005` 和 `text-multilingual-embedding-002`）的最高分数。更详细的结果可在技术报告中查阅。*\n\n### 模型详情\n\n*   **多功能性**：支持超过100种语言。\n*   **输入长度**：最大输入令牌长度为2048。\n*   **MRL 技术**：利用 Matryoshka Representation Learning (MRL) 技术，允许开发者将输出维度从默认的3072进行缩减，以优化性能和存储成本。\n*   **推荐维度**：为获得最高质量结果，建议使用3072、1536或768的输出维度。\n\n### 费率限制与定价\n\n*   **免费与付费层级**：Gemini API 提供免费和付费层级，开发者可以免费试用 `gemini-embedding-001`，或为生产需求选择更高限制的付费服务。\n*   **定价**：该模型定价为每100万输入令牌0.15美元。\n\n### 如何开始使用\n\n*   开发者现在可以通过 Gemini API 访问 `gemini-embedding-001` 模型。\n*   可以通过 Google AI Studio 免费开始使用。\n*   兼容现有的 `embed_content` 端点。\n*   提供了 Python 代码示例和详细的开发者文档及快速入门指南。\n\n### 模型弃用与迁移建议\n\n*   **实验版弃用**：实验性模型 `gemini-embedding-exp-03-07` 将于2025年8月14日停止支持，用户无需重新嵌入内容。\n*   **传统模型弃用**：\n    *   `embedding-001` 将于2025年8月14日弃用。\n    *   `text-embedding-004` 将于2026年1月14日弃用。\n*   **强烈建议**：Google 强烈建议用户尽快将项目迁移到最新的模型。\n\n### 未来展望\n\n*   Gemini Embedding 即将支持 Batch API，以实现异步数据处理和更低的成本。\n*   未来将发布更多具有更广泛模态和功能的嵌入模型。",
      "shortSummary": "Google 宣布其首个 Gemini Embedding 文本模型（`gemini-embedding-001`）现已在 Gemini API 和 Vertex AI 中普遍可用。该模型在多语言基准测试中表现出色，支持100多种语言，并提供灵活的输出维度。它提供免费和付费层级，定价为每100万输入令牌0.15美元。Google 鼓励开发者尽快迁移到此新模型，因旧版和实验性模型将陆续弃用。未来还将支持 Batch API，并推出更多功能。",
      "translated_title": "Gemini Embedding 现已在 Gemini API 中普遍可用",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/EmbedingsChart_16x9_RD2-V01.original.jpg",
          "alt": "Embedings Chart",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "The Gemini Embedding text model is now generally available in the Gemini API and Vertex AI. This versatile model has consistently ranked #1 on the MTEB Multilingual leaderboard since its experimental launch in March, supports over 100 languages, has a 2048 maximum input token length, and is priced at $0.15 per 1M input tokens."
    },
    {
      "title": "企业真相在行动：Apigee API hub 赋能强大的开发者门户 (原标题: Enterprise truth in action: Apigee API hub fueling powerful Developer Portals)",
      "link": "https://developers.googleblog.com/en/apigee-api-hub-fueling-developer-portals/",
      "pubDate": "Sun, 13 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-13T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 企业真相在行动：Apigee API hub 赋能强大的开发者门户\n\nGoogle Cloud 的 Apigee 团队致力于提供策略和工具，帮助客户构建强大有效的 API 生态系统。其中，API 的发现机制至关重要。文章详细阐述了 Apigee 平台中两个既独立又紧密相连的组件——Apigee API hub 和开发者门户——如何满足不同角色的需求。Apigee API hub 被视为核心的“真相承载引擎”，它赋能并提升了开发者门户的功能，是企业代理（agentic）策略的基石。\n\n## Apigee API hub：企业所有 API 的真相来源\n\nApigee API hub 被比作 API 程序的“中枢神经系统”。它提供一个集中化的位置，用于编目组织内所有 API 及其数据（无论 API 风格、生命周期阶段或托管环境），从而成为“企业真相”的权威来源。这为治理和一致性奠定了基础，帮助团队防止蔓延、构建更好的 API，并最终为用户提供更优质的体验。\n\n![Apigee API hub metrics](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image5_VZss8uJ.original.png)\n\nApigee API hub 主要关注组织内部 API 生产者的需求，包括平台工程师、API 架构师、安全专家以及负责构建和管理 API 的开发人员。对于这些用户，Apigee API hub 是唯一的真相来源，其功能包括：\n\n*   **全面的 API 概览和元数据：** Apigee API hub 能够自动发现和编目所有 API，创建一个包含丰富元数据的统一存储库。这包括 API 生命周期阶段、版本、规范和文档等信息，提供 API 生态系统的完整准确视图。这些全面且机器可读的数据正是 AI 代理理解和与企业系统交互所需的，作为服务的核心注册表。\n*   **基于标准的集中式治理：** 该 hub 允许在整个 API 生命周期中建立和执行一致的治理。通过维护单一的真相来源，可以有效确保质量一致性、遵守标准、管理安全性、跟踪 API 成熟度，并就 API 策略做出明智决策，例如识别可货币化的 API。这种统一的真相来源对于建立治理至关重要，不仅适用于人类开发者，也适用于控制和保护 AI 代理如何访问和利用 API 组合，确保安全性、API 风格和数据格式的一致性。\n*   **增强内部协作：** 该 hub 通过提供集中且一致的 API 信息来源，促进内部协作。这有助于促进 API 重用，减少冗余，并简化开发工作流程。\n\n![Apigee API hub refund](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image1_05Kg96Y.original.png)\n\n## 开发者门户：为 API 消费者量身定制的体验\n\n相比之下，开发者门户就像一个“店面”，API 消费者（即构建应用程序的开发人员）通过它发现并与 API 交互。并非组织内生产的每个 API 都旨在供外部消费。Apigee API hub 作为超集管理所有 API，而开发者门户则暴露其中经过精心选择的子集，并使用自定义的样式和品牌。\n\n![Developer Portals](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image2_7JWBY5z.original.png)\n\n为了真正有效，这些门户必须由 Apigee API hub 中存在的“企业真相”提供支持。门户提供 hub 功能的精选且用户友好的子集，专注于无缝的 API 消费：\n\n*   **真实有效的 API 发现：** 开发者门户依赖于 Apigee API hub 元数据的准确性和完整性，使开发人员能够轻松找到所需的 API。\n*   **API 消费的“出口”：** Apigee API hub 充当控制中心，提供一个受控的“出口”，有选择地将特定 API 及其相关的“企业真相”发布到开发者门户。这确保只有经过批准和相关的 API 才能供消费。\n\n![Developer Portals 2](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image7_2RPZWOQ.original.png)\n\n## 释放 API 潜力\n\nApigee API hub 和开发者门户虽然独立，但共同构成一个统一的系统。hub 的全面目录和“企业真相”直接赋能并驱动开发人员的门户体验，并且现在，更关键的是，它们成为新兴 AI 代理策略不可或缺的基础。\n\n通过理解 Apigee API hub 作为 API 生产者和代理的核心存储库的作用，以及门户作为面向消费者的子集和受控“出口”的作用，组织可以充分释放其 API 的潜力并加速创新。\n\n文章最后鼓励用户开始使用 Apigee API hub 和开发者门户，并提及一个 GitHub 示例，展示如何将非托管 API 导入 hub 并将其文档发布到开发者门户。此外，还预告了 2025 年 7 月 31 日的线上活动，以深入了解 hub 和门户的功能。",
      "shortSummary": "Apigee API hub 和开发者门户是 Google Cloud Apigee 平台中两个关键组件。Apigee API hub 是企业所有 API 的“真相来源”，面向 API 生产者，提供集中编目、治理和内部协作，也是 AI 代理策略的基石。开发者门户则是面向 API 消费者的“店面”，展示并允许交互 Apigee API hub 中精选的 API 子集。两者虽独立但紧密相连，共同构成统一系统，赋能 API 发现、管理和消费，帮助组织释放 API 潜力并加速创新。",
      "translated_title": "企业真相在行动：Apigee API hub 赋能强大的开发者门户",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image5_VZss8uJ.original.png",
          "alt": "Apigee API hub metrics",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image1_05Kg96Y.original.png",
          "alt": "Apigee API hub refund",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image2_7JWBY5z.original.png",
          "alt": "Developer Portals",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image7_2RPZWOQ.original.png",
          "alt": "Developer Portals 2",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "完整文章",
      "content": "The Apigee API hub and Developer Portals are distinct but interconnected parts of the Apigee platform that help organizations discover and manage APIs for different personas, unlocking API potential and accelerating innovation."
    },
    {
      "title": "使用 Firebase Studio 推进代理式 AI 开发 (原标题: Advancing agentic AI development with Firebase Studio)",
      "link": "https://developers.googleblog.com/en/advancing-agentic-ai-development-with-firebase-studio/",
      "pubDate": "Wed, 09 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-09T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 使用 Firebase Studio 推进代理式 AI 开发\n\n在 Google Cloud Summit London 大会上，Google Cloud 宣布了 Firebase Studio 的重大进展。Firebase Studio 是一个基于云的 AI 工作区，旨在通过集成强大的 AI 功能，重新定义 AI 辅助开发，并帮助开发者构建下一代代理式应用程序。\n\n## 主要更新亮点\n\nFirebase Studio 的最新更新包括：\n\n*   **多功能代理模式 (Agent modes)**：提供与 Gemini 交互的不同方式。\n*   **模型上下文协议 (Model Context Protocol, MCP) 的基础支持**：增强工作流的可扩展性。\n*   **Gemini CLI 集成**：将强大的 AI 功能直接融入开发者的工作流程。\n\n## 1. 引入自主代理模式：您的 AI 伙伴，由您定义\n\nFirebase Studio 现已提供三种与 Gemini 交互的模式，包括全新的自主代理模式。开发者可以根据需求在这些模式之间无缝切换，以加速开发任务，这得益于 Gemini 2.5 强大的代码理解和推理能力。\n\n![Agent Mode](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image5_xvxrYmH.original.png)\n\n### 模式详解：\n\n*   **Ask 模式 (对话与规划)**\n    *   **用途：** 用于与 Gemini 进行讨论和规划，非常适合头脑风暴、代码规划和协作讨论复杂问题。\n    *   **特点：** 纯粹的对话模式，不会对文件进行任何修改。\n*   **Agent 模式 (建议与批准)**\n    *   **用途：** Gemini 可以提出对应用程序的修改建议。\n    *   **特点：** 开发者始终处于控制之中，必须在任何文件被修改之前批准所有建议的更改。这提供了完整的监督，并允许轻松地在更改集成到项目之前进行代码审查。\n*   **Agent (Auto-run) 模式 (自主运行)**\n    *   **用途：** Gemini 可以自主推理并生成整个应用程序，或向现有应用程序添加功能。\n    *   **示例：** 它可以跨多个文件进行代码更改、编写测试、修复错误和重构组件，所有这些都只需一个提示。\n    *   **安全：** 为了安全起见，在删除文件、运行终端命令或使用外部工具之前，始终需要获得开发者的许可。\n\n### 个性化指导：\n\n代理模式可以利用项目级别的规则文件（如 `.idx/airules.md`、`GEMINI.md` 或 `.cursorrules`）提供个性化指导，确保 Gemini 遵循特定的设计模式和偏好。系统会自动检测并加载这些文件中的指令，提供高度可定制的体验。\n\n## 2. 通过模型上下文协议 (MCP) 释放可扩展性\n\n除了新的代理模式，Firebase Studio 还在预览对模型上下文协议 (MCP) 的基础支持。现在，开发者可以将 MCP 服务器添加到工作区，从而扩展和个性化 Firebase 中 Gemini 的工作流程。例如，可以使用 Firebase MCP 服务器在构建或调试应用程序时，通过自然语言探索 Cloud Firestore 中的数据；或者使用 Context7 获取特定库的上下文（如使用 MediaPipe 的设备端机器学习构建应用程序）。\n\n## 3. Gemini CLI 集成到 Firebase Studio\n\nGemini CLI 是一个功能强大且免费的工具，可用于代码生成、内容生成和研究等多种任务。它提供慷慨的使用层级、高级 AI 功能、与 Google Search 的实时上下文集成以及开源架构，支持定制和贡献。现在，Gemini CLI 已直接集成到 Firebase Studio 中。\n\n如果开发者在终端上花费大量时间进行代码生成、调试、执行命令或管理项目文件等任务，Gemini CLI 提供无缝的 AI 驱动体验，无需切换到单独的聊天窗口。\n\n## 实际影响：Firebase Studio 的应用案例\n\nFirebase Studio 的 AI 功能已被广泛应用于简化工作流程并加速产品发布。实际案例包括：\n\n*   为氢经济创建采购平台。\n*   为用户提供个性化时尚造型师。\n*   帮助宝可梦爱好者通过 AI 驱动的卡片扫描和识别管理卡片收藏。\n*   通过引人注目的可视化和以人为本的分析，将建筑设计变为现实。\n\nGoogle Cloud 承诺将继续推出新功能和更新，帮助开发者快速轻松地交付专业品质的应用程序。鼓励开发者探索 Firebase Studio 的新功能，并在 X 和 LinkedIn 上使用 #FirebaseStudio 分享他们的作品。",
      "shortSummary": "Firebase Studio 推出重大更新，旨在通过集成 AI 推进代理式 AI 开发。主要亮点包括：三种多功能代理模式（Ask、Agent、Agent Auto-run），提供从对话到自主代码生成的能力；对模型上下文协议 (MCP) 的基础支持，增强工作流可扩展性；以及 Gemini CLI 的直接集成，实现终端内无缝的 AI 辅助开发。这些更新旨在简化开发流程，加速应用发布，并已在多个实际案例中展现出显著效果。",
      "translated_title": "使用 Firebase Studio 推进代理式 AI 开发",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image5_xvxrYmH.original.png",
          "alt": "Agent Mode",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "Updates in Firebase Studio include new Agent modes, foundational support for the Model Context Protocol (MCP), and Gemini CLI integration, all designed to redefine AI-assisted development allow developers to create full-stack applications from a single prompt and integrate powerful AI capabilities directly into their workflow."
    },
    {
      "title": "发布 GenAI Processors：构建强大灵活的 Gemini 应用 (原标题: Announcing GenAI Processors: Build powerful and flexible Gemini applications)",
      "link": "https://developers.googleblog.com/en/genai-processors/",
      "pubDate": "Wed, 09 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-09T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 发布 GenAI Processors：构建强大灵活的 Gemini 应用\n\nGoogle DeepMind 于 2025 年 7 月 10 日发布了 GenAI Processors，这是一个新的开源 Python 库。该库旨在解决使用大型语言模型（LLM）构建复杂 AI 应用（特别是处理多模态输入和需要实时响应的应用）时面临的挑战。这类应用通常涉及复杂的拼接数据处理步骤、异步 API 调用和自定义逻辑，随着复杂性增加，代码可能变得脆弱且难以维护。\n\n## GenAI Processors 核心概念\n\nGenAI Processors 提供了一个抽象层，定义了一个统一的 `Processor` 接口，用于处理从输入处理、预处理到模型调用和输出处理的所有环节。其核心是将所有输入和输出视为 `ProcessorParts` 的异步流（即双向流）。`ProcessorParts` 是标准化的数据部分（例如，一段音频、文本转录、图像帧）及其相关元数据，它们在管道中流动。这种基于流的 API 允许无缝地链接和组合不同的操作，从低级数据操作到高级模型调用。\n\n![GenAI Processors library](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image2_kBE2xzY.original.png)\n\nGenAI Processors 库旨在优化 `Processor` 的并发执行。在执行流中，任何部分都可以在其图中所有祖先计算完成后并发生成。该流程保持输出流相对于输入流的顺序，并将以最小化首个令牌时间（TTFT）的方式执行。这种并发优化是自动进行的：将 `Processor` 应用于输入流将尽可能自动触发并发执行。\n\n## 应用示例与优势\n\n该库使得构建实时应用变得简单，例如：\n\n*   **实时代理（Live Agent）**：只需几行代码，即可使用 Gemini Live API 构建能够实时处理音频和视频流的代理。文章提供了结合摄像头和麦克风输入、通过 Gemini Live API 处理并播放音频输出的代码示例。\n*   **基于标准文本 LLM 的实时代理**：利用 GenAI Processor 库的双向流能力和 Google Speech API，可以构建一个将麦克风输入转换为文本、通过 LLM 生成响应、再将文本转换为音频并播放的代理。\n\n即使对于非流式用例，一旦数据可用就立即处理也能显著减少延迟和首个令牌时间（TTFT），这对于提供良好用户体验至关重要。GenAI Processors 通过利用 Python 的原生特性，提供了一种在不增加代码复杂性的情况下编写响应式应用程序的方法。例如，旅行规划器和研究代理等基于回合的代理可以利用 GenAI Processors 的并发功能来提高响应速度。\n\n## 核心设计原则\n\nGenAI Processors 的核心是 `Processor` 概念：一个封装特定工作单元的基本构建块。它接收输入流，执行操作，并输出结果流。这种简单、一致的 API 是该库强大和灵活的基石。\n\n其核心设计决策及其为开发者带来的好处包括：\n\n*   **模块化设计**：将复杂工作流分解为自包含的 `Processor` 单元，确保代码可重用性、可测试性，并显著简化复杂管道的维护。\n*   **异步与并发**：充分利用 Python 的 `asyncio` 来高效处理 I/O 密集型和计算密集型任务，从而实现响应式应用程序，无需手动线程或复杂的并发管理。\n*   **与 Gemini API 集成**：专用的处理器，如 `GenaiModel`（用于基于回合的交互）和 `LiveProcessor`（用于实时流式传输），简化了与 Gemini API 的交互，包括 Live API 的复杂性，减少了样板代码并加速了集成。\n*   **可扩展性**：通过继承基类或使用装饰器，可以轻松创建自定义处理器，将自己的数据处理逻辑、外部 API 或专业操作无缝集成到管道中。\n*   **统一的多模态处理**：`ProcessorPart` 包装器提供了一个一致的接口，用于在管道中处理各种数据类型（文本、图像、音频、JSON 等）。\n*   **流操作实用程序**：内置用于分割、连接和合并异步流的实用程序，为复杂管道中的数据流提供精细控制。\n\n## 如何开始\n\n开始使用 GenAI Processors 非常简单。可以通过 pip 安装：`pip install genai-processors`。\n\n为了帮助用户熟悉该库，官方提供了一系列 Colab 笔记本，引导用户了解核心概念并演示如何构建各种类型的处理器和应用程序。建议从 Content API Colab 和 Processor Intro Colab 开始。用户还可以探索仓库中的 `examples/` 目录，获取构建更复杂应用程序（如研究代理和实时评论代理）的实际演示。\n\n## 展望与社区贡献\n\nGenAI Processors 目前处于早期阶段，但它为解决 AI 应用中复杂工作流和编排挑战提供了坚实的基础。虽然 Google GenAI SDK 支持多种语言，但 GenAI Processors 目前仅支持 Python。\n\n`core/` 目录包含基本处理器，官方积极鼓励社区为 `contrib/` 目录贡献更专业的功能。Google DeepMind 期待与开发者社区合作，扩展该库并构建更复杂的 AI 系统。\n\n鼓励开发者查看 GitHub 上的 GenAI Processors 仓库：[https://github.com/google-gemini/genai-processors](https://github.com/google-gemini/genai-processors)。\n\n## 致谢\n\nGenAI Processors 是一个优秀团队奉献和辛勤工作的结果。文章对 Juliette Love, KP Sawhney, Antoine He, Will Thompson, Arno Eigenwillig, Ke Wang, Parth Kothari, Tim Blyth, Philipp Schmid, Patrick Löber, Omar Sanseviero, Alexey Kolganov, Adam Langley, Evan Senter, Seth Odoom, Thierry Coppey, 和 Murat Ozturk 等在将此库变为现实中发挥关键作用的个人表示感谢。",
      "shortSummary": "Google DeepMind 发布了 GenAI Processors，这是一个新的开源 Python 库，旨在简化复杂、多模态、实时 LLM 应用的构建。它提供了一个统一的 `Processor` 接口，将所有输入输出视为异步流，支持无缝链接和并发执行。该库通过模块化设计、异步处理和与 Gemini API 的深度集成，帮助开发者构建更强大、响应更快的 AI 应用，尤其适用于实时代理等场景。目前支持 Python，并鼓励社区贡献。",
      "translated_title": "发布 GenAI Processors：构建强大灵活的 Gemini 应用",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image2_kBE2xzY.original.png",
          "alt": "GenAI Processors library",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "GenAI Processors is a new open-source Python library from Google DeepMind designed to simplify the development of AI applications, especially those handling multimodal input and requiring real-time responsiveness, by providing a consistent \"Processor\" interface for all steps from input handling to model calls and output processing, for seamless chaining and concurrent execution."
    },
    {
      "title": "T5Gemma：一种新的编码器-解码器Gemma模型集合 (原标题: T5Gemma: A new collection of encoder-decoder Gemma models)",
      "link": "https://developers.googleblog.com/en/t5gemma/",
      "pubDate": "Tue, 08 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-08T16:00:00.000Z",
      "creator": "Google",
      "summary": "### T5Gemma：编码器-解码器架构的新篇章\n\n**引言**\n\n在大型语言模型（LLMs）快速发展的背景下，虽然解码器模型占据了主导地位并展现出强大的生成能力，但经典的编码器-解码器架构（如T5）因其高推理效率、设计灵活性和更丰富的编码器输入理解能力，在摘要、翻译和问答等实际应用中仍是热门选择。然而，这种强大的架构相对受到的关注较少。\n\n本文介绍了 **T5Gemma**，这是一个新的编码器-解码器LLM集合。它通过一种名为“适应性”（adaptation）的技术，将预训练的解码器模型转换为编码器-解码器架构。T5Gemma基于Gemma 2框架，包括适应后的Gemma 2 2B和9B模型，以及一系列新训练的T5尺寸模型（Small、Base、Large和XL）。项目团队发布了预训练和指令微调的T5Gemma模型，以期为社区研究和开发开启新机遇。\n\n**从解码器到编码器-解码器：模型适应性**\n\nT5Gemma的核心问题是：能否基于预训练的解码器模型构建顶级的编码器-解码器模型？答案是肯定的，通过探索“模型适应性”技术。其核心思想是利用已预训练的解码器模型权重来初始化编码器-解码器模型的参数，然后通过UL2或PrefixLM预训练进一步适应它们。\n\n![模型适应性概览](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Chart-1.original.png)\n\n这种适应方法具有高度灵活性，允许创造性地组合模型尺寸。例如，可以搭配一个大型编码器和一个小型解码器（如9B编码器与2B解码器）来创建“不平衡”模型。这使得模型能够针对特定任务（如摘要，其中对输入的深度理解比生成输出的复杂性更关键）微调质量-效率权衡。\n\n**性能表现：质量与效率的平衡**\n\n实验结果表明，T5Gemma模型在多个基准测试（如SuperGLUE，衡量学习到的表示质量）中，其性能与解码器Gemma模型相当或更优，几乎主导了质量-推理效率的帕累托前沿。\n\n![编码器-解码器模型基准测试](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Encoder-decoder_models_benchmarks.original.png)\n\n这种性能优势不仅是理论上的，也转化为实际的质量和速度。在测量GSM8K（数学推理）的实际延迟时，T5Gemma展现出明显优势。例如，T5Gemma 9B-9B在相似延迟下实现了比Gemma 2 9B更高的精度。更令人印象深刻的是，T5Gemma 9B-2B在精度上比2B-2B模型有显著提升，而其延迟几乎与小得多的Gemma 2 2B模型相同。这些实验最终表明，编码器-解码器适应性提供了一种灵活而强大的方式来平衡质量和推理速度。\n\n**基础与微调能力**\n\nT5Gemma在预训练和指令微调后均展现出强大的能力：\n\n*   **预训练后**：T5Gemma在需要推理的复杂任务上取得了显著进展。例如，T5Gemma 9B-9B在GSM8K（数学推理）上比原始Gemma 2 9B模型高出9分以上，在DROP（阅读理解）上高出4分。这表明，通过适应性初始化的编码器-解码器架构有潜力创建更强大、性能更好的基础模型。\n\n    ![预训练模型详细结果](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/pretrained-model-results.original.png)\n\n*   **指令微调后**：与Gemma 2 IT相比，T5Gemma IT的性能差距全面显著扩大。例如，T5Gemma 2B-2B IT的MMLU分数比Gemma 2 2B跃升近12点，其GSM8K分数从58.0%增至70.7%。适应性架构不仅可能提供更好的起点，还能更有效地响应指令微调，最终形成一个更强大、更有用的最终模型。\n\n    ![微调+RLHF模型结果](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/results-for-fine-tuned-RLHFed-models.original.png)\n\n**T5Gemma检查点发布**\n\n项目团队非常高兴地发布了构建强大、通用编码器-解码器模型的新方法，即通过适应预训练的解码器LLM（如Gemma 2）。为加速进一步研究并允许社区在此基础上进行开发，团队发布了一套T5Gemma检查点。发布内容包括：\n\n*   **多种尺寸**：T5尺寸模型（Small、Base、Large和XL）、Gemma 2模型（2B和9B），以及介于T5 Large和T5 XL之间的一个额外模型。\n*   **多种变体**：预训练和指令微调模型。\n*   **灵活配置**：一个强大且高效的不平衡9B-2B检查点，用于探索编码器和解码器尺寸之间的权衡。\n*   **不同训练目标**：使用PrefixLM或UL2目标训练的模型，分别提供最先进的生成性能或表示质量。\n\n希望这些检查点能为研究模型架构、效率和性能提供宝贵资源。\n\n**入门指南**\n\n文章提供了相关链接，包括研究论文、Hugging Face和Kaggle上的模型权重下载、Colab notebook以及Vertex AI上的模型推理指南。",
      "shortSummary": "T5Gemma是基于Gemma 2框架的新型编码器-解码器大语言模型集合，通过将预训练的解码器模型转换为编码器-解码器架构实现。它在推理效率和质量上表现出色，在多个基准测试中优于或媲美同类解码器模型，尤其在数学推理等复杂任务上展现显著提升。该项目发布了多种尺寸和配置的预训练及指令微调模型，旨在促进编码器-解码器架构的研究与应用。",
      "translated_title": "T5Gemma：一种新的编码器-解码器Gemma模型集合",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Chart-1.original.png",
          "alt": "decoder-only model",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Encoder-decoder_models_benchmarks.original.png",
          "alt": "Encoder-decoder models benchmarks",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/pretrained-model-results.original.png",
          "alt": "Detailed results for pretrained models",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/results-for-fine-tuned-RLHFed-models.original.png",
          "alt": "Results for fine-tuned + RLHFed models",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "完整文章",
      "content": "T5Gemma is a new family of encoder-decoder LLMs developed by converting and adapting pretrained decoder-only models based on the Gemma 2 framework, offering superior performance and efficiency compared to its decoder-only counterparts, particularly for tasks requiring deep input understanding, like summarization and translation."
    },
    {
      "title": "Gemini API 中的批处理模式：事半功倍 (原标题: Batch Mode in the Gemini API: Process more for less)",
      "link": "https://developers.googleblog.com/en/scale-your-ai-workloads-batch-mode-gemini-api/",
      "pubDate": "Sun, 06 Jul 2025 16:00:00 GMT",
      "isoDate": "2025-07-06T16:00:00.000Z",
      "creator": "Google",
      "summary": "# Gemini API 批处理模式：事半功倍\n\nGoogle 于2025年7月7日推出了 Gemini API 的批处理模式（Batch Mode），这是一个全新的异步端点，专为高吞吐量、对延迟不敏感的工作负载设计。该模式允许用户提交大型作业，将调度和处理工作卸载给API，并在24小时内获取结果，同时享受比同步API低50%的折扣。\n\n## 核心优势\n\n批处理模式是处理那些数据已准备就绪且无需即时响应任务的理想工具。通过将这些大型作业与实时流量分离，用户可以获得三大关键益处：\n\n*   **成本节约**：批处理作业的定价比给定模型的标准费率低50%。\n*   **更高吞吐量**：批处理模式具有更高的速率限制，能够处理更多请求。\n*   **简便的API调用**：无需管理复杂的客户端排队或重试逻辑。可用结果将在24小时内返回。\n\n## 简单的工作流程\n\nAPI设计简洁直观。用户将所有请求打包到一个文件中，提交作业，并在作业完成后检索结果。以下是开发者目前利用批处理模式完成任务的一些方式：\n\n*   **批量内容生成与处理**：\n    Reforged Labs 专注于深度视频理解，利用 Gemini 2.5 Pro 每月分析和标记大量视频广告。实施批处理模式彻底改变了他们的运营，显著降低了成本，加快了客户交付速度，并实现了获取有意义市场洞察所需的大规模可扩展性。\n    ![批量内容生成与处理](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image2_qhTloW2.original.png)\n\n*   **模型评估**：\n    Vals AI 对基础模型进行基准测试，涵盖法律、金融、税务和医疗保健等实际用例。他们正在使用批处理模式提交大量评估查询，而不受速率限制的约束。\n    ![模型评估](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_uvTbuTT.original.png)\n\n## 快速上手\n\n用户可以使用 Google GenAI Python SDK 立即开始使用批处理模式。基本流程包括：\n\n1.  创建一个包含多个请求的 JSONL 文件。\n2.  使用 `client.files.upload()` 方法上传该文件。\n3.  使用 `client.batches.create()` 方法创建批处理作业，指定模型和源文件。\n4.  等待作业完成（最长24小时），然后通过 `client.files.download()` 方法下载结果文件并处理其内容。\n\n## 展望\n\nGemini API 的批处理模式正在逐步向所有用户推出。这仅仅是批处理能力的开始，Google 正在积极努力扩展其功能，未来将提供更强大、更灵活的选项。",
      "shortSummary": "Gemini API 推出批处理模式，专为高吞吐量、非延迟敏感型任务设计。它允许用户提交大型作业，在24小时内获取结果，并享受50%的成本折扣。主要优势包括显著的成本节约、更高的吞吐量和简化的API调用。开发者可利用其进行批量内容生成、模型评估等。该模式已通过Python SDK提供，未来将进一步扩展功能。",
      "translated_title": "Gemini API 中的批处理模式：事半功倍",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image2_qhTloW2.original.png",
          "alt": "Bulk content generation and processing",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_uvTbuTT.original.png",
          "alt": "Model evaluations",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "The new batch mode in the Gemini API is designed for high-throughput, non-latency-critical AI workloads, simplifying large jobs by handling scheduling and processing, and making tasks like data analysis, bulk content creation, and model evaluation more cost-effective and scalable, so developers can process large volumes of data efficiently."
    }
  ],
  "lastUpdated": "2025-07-20T10:33:05.139Z"
}