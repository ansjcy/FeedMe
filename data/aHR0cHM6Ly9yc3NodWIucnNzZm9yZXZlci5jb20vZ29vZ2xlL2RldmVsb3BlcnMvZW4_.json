{
  "sourceUrl": "https://rsshub.rssforever.com/google/developers/en",
  "title": "Google Developers Blog",
  "description": "Google Developers Blog - Powered by RSSHub",
  "link": "https://developers.googleblog.com",
  "items": [
    {
      "title": "利用AG-UI将ADK智能体与精美前端结合，提升用户体验 (原标题: Delight users by combining ADK Agents with Fancy Frontends using AG-UI)",
      "link": "https://developers.googleblog.com/en/delight-users-by-combining-adk-agents-with-fancy-frontends-using-ag-ui/",
      "pubDate": "Thu, 25 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-25T16:00:00.000Z",
      "creator": "Google",
      "summary": "## 通过将ADK智能体与AG-UI的精美前端结合来取悦用户\n\n在生成式AI应用开发领域，过去一年取得了飞速进展。后端智能体系统在推理、规划和执行复杂任务方面展现出惊人的能力。然而，一个强大的智能体只是成功的一半，如何将其智能无缝连接到应用程序前端，并创建真正协作的用户体验，一直是开发者面临的挑战。\n\n### 解决方案：ADK与AG-UI的集成\n本文重点介绍了Agent Development Kit (ADK) 与 AG-UI 的新集成。AG-UI是一个用于构建丰富、交互式AI用户体验的开放协议。这种结合使开发者能够轻松构建复杂的后端AI智能体，并为其提供生产就绪、协作式的前端。\n\n### 您的智能体后端：ADK\nADK（智能体开发工具包）是一个开箱即用的开源工具包，旨在构建能够“做事”而非仅仅“聊天”的AI智能体。它抽象了智能体工程中最困难的部分，让开发者能够专注于应用程序的独特逻辑。\n\nADK为您的智能体提供以下核心能力：\n*   **多步规划：** 能够推理问题，将其分解为多个步骤并按顺序执行。\n*   **工具使用：** 与外部API、服务和数据源无缝集成，赋予智能体真实世界的能力。\n*   **状态管理：** 强大的进度和记忆跟踪，无需从头构建复杂的链式逻辑。\n\n通过ADK，您可以在数小时内将想法转化为可工作的智能体原型，同时完全控制智能体的角色及其可访问的工具。尽管ADK提供了本地开发和调试UI（`adk web`），但它并非面向用户的界面。此前，任何UI与智能体的集成都是定制化的，缺乏标准化的通信方式。\n\n### 连接前端：AG-UI\nCopilotKit团队发布了AG-UI，这是一个开放协议和UI层，旨在标准化智能体与用户通过丰富UI直接对话的方式。AG-UI专注于直接面向用户的智能体（而非后台智能体），并利用中间件和客户端集成来泛化支持任何前端和后端。它提供了一种标准化的方式，让后端智能体与前端应用程序通信，从而实现AI与人类用户之间的实时、有状态的协作。\n\nAG-UI的创建者还提供了CopilotKit，这是一个开源库，包含可直接使用的React组件和Hooks，它们与AG-UI协同工作。这意味着您可以在几分钟内让应用程序运行起一个精致的聊天界面、侧边栏及其他UI元素。\n\n![ADK + AG UI Blog Sep 25 2025](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/ADK__AG_UI_Blog_Sep_25_2025.original.png)\n\n### 强强联合：ADK + AG-UI 集成解锁的功能\n通过将强大的后端（ADK）与灵活的前端协议（AG-UI）结合，您现在可以构建真正交互式的AI应用程序。ADK运行智能体的“大脑”和工具编排，而AG-UI则提供与UI组件的通信通道，以实现无缝的用户体验。\n\n此集成开箱即用，解锁了一系列新功能：\n*   **生成式UI：** 超越文本。您的智能体可以直接在聊天中生成和渲染UI组件，提供丰富的上下文信息和操作。\n*   **共享状态：** 前端和后端智能体共享对应用程序状态的共同理解，允许智能体响应UI中的用户操作，反之亦然。\n*   **人机协作（Human-in-the-Loop）：** 用户可以在智能体执行操作之前对其进行监督、批准或纠正，有助于确保安全性和控制。\n*   **前端工具：** 赋能智能体直接与前端交互，例如代表用户填写表单、导航页面或批注文档。\n\n### 开始构建：快速入门指南\n入门非常简单，只需运行一个命令即可克隆一个全栈启动仓库，其中包含预配置的ADK后端和使用CopilotKit的Next.js前端：\n\n`npx copilotkit@latest create -f adk`\n\n获得启动项目后，核心逻辑也很简单：\n\n1.  **在后端定义您的智能体（`/backend/agent.ts`）：**\n    在ADK后端中，您定义智能体的指令并为其提供工具。例如，一个获取股票价格的工具。\n    ```javascript\n    // backend/agent.ts\n    import { adk } from \"@copilotkit/adk\";\n    adk.setSystemMessage(\n      \"You are a helpful assistant that can fetch stock prices.\"\n    );\n    adk.addTool(\"getStockPrice\", {\n      description: \"Get the current stock price for a given ticker symbol.\",\n      parameters: {\n        type: \"object\",\n        properties: {\n          ticker: { type: \"string\", description: \"The stock ticker symbol (e.g., GOOGL).\" },\n        },\n        required: [\"ticker\"],\n      },\n      handler: async ({ ticker }) => {\n        console.log(`Fetching stock price for ${ticker}...`);\n        const price = (Math.random() * 1000).toFixed(2);\n        return `The current price of ${ticker} is $${price}.`;\n      },\n    });\n    export default adk;\n    ```\n\n2.  **连接前端（`/frontend/src/app/page.tsx`）：**\n    在React/Next.js前端中，您使用`CopilotKit`提供程序包装您的应用程序，并使用UI组件。\n    ```javascript\n    // frontend/src/app/page.tsx\n    \"use client\";\n    import { CopilotKit } from \"@copilotkit/react-core\";\n    import { CopilotChat } from \"@copilotkit/react-ui\";\n    import \"@copilotkit/react-ui/styles.css\";\n    export default function Home() {\n      return (\n        <CopilotKit url=\"http://localhost:5001/api/adk\"> {/* URL to your ADK backend */}\n          <main>\n            <h1>Welcome to Your ADK + AG-UI App!</h1>\n            <p>Ask me to get a stock price.</p>\n          </main>\n          <CopilotChat />\n        </CopilotKit>\n      );\n    }\n    ```\n\n这样，您就拥有了一个功能齐全的AI智能体，带有一个精致的前端，随时可以进行定制和扩展。\n\n### 立即开始构建\n停止纠结于将智能体逻辑连接到用户界面的复杂性。通过ADK和AG-UI的集成，您可以专注于最重要的事情：交付强大、智能且协作的AI应用程序。\n\n### 资源：\n*   快速入门仓库：`npx copilotkit@latest create -f adk`\n*   官方文档：`https://docs.copilotkit.ai/adk`\n*   GitHub上的启动项目：`https://github.com/copilotkit/with-adk`\n*   AG-UI Dojo教程：`https://dojo.ag-ui.com/adk-middleware/feature/shared_state`",
      "shortSummary": "ADK（Agent Development Kit）与AG-UI（开放协议）的集成，旨在简化生成式AI应用的开发。ADK提供多步规划、工具使用和状态管理等后端智能体核心能力。AG-UI则标准化了智能体与用户UI的直接通信，支持生成式UI、共享状态、人机协作及前端工具等高级功能。此集成使开发者能快速构建具有强大智能体和精美前端的交互式、协作性AI应用，显著提升用户体验和开发效率。",
      "translated_title": "利用AG-UI将ADK智能体与精美前端结合，提升用户体验",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/ADK__AG_UI_Blog_Sep_25_2025.original.png",
          "alt": "ADK + AG UI Blog Sep 25 2025",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "Delight users by combining ADK Agents with Fancy Frontends using AG-UIFor developers building genera..."
    },
    {
      "title": "Apigee Operator for Kubernetes 与 GKE Inference Gateway 集成，用于认证和 AI/LLM 策略 (原标题: Apigee Operator for Kubernetes and GKE Inference Gateway integration for Auth and AI/LLM policies)",
      "link": "https://developers.googleblog.com/en/apigee-operator-for-kubernetes-and-gke-inference-gateway-integration-for-auth-and-aillm-policies/",
      "pubDate": "Thu, 25 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-25T16:00:00.000Z",
      "creator": "Google",
      "summary": "# Apigee Operator for Kubernetes 与 GKE Inference Gateway 集成，用于认证和 AI/LLM 策略\n\n## 引言：API 在 AI/Agent 中的关键作用\n文章强调了 API 在生成式 AI 和自动化代理工作流中的核心地位，指出没有 API，AI 功能将难以被广泛访问和利用。API 通过使模型对自动化代理和人类用户可用，从而释放了生成式 AI 的力量。\n\n## GKE Inference Gateway 概述\nGKE Inference Gateway 是 Google Kubernetes Engine (GKE) Gateway 的一个扩展，专为服务生成式人工智能 (AI) 工作负载提供优化的路由和负载均衡。它简化了 AI 推理工作负载的部署、管理和可观测性。其主要功能包括：\n*   **推理优化负载均衡**：根据模型服务器的指标智能分配请求，以优化 AI 模型服务。\n*   **动态 LoRA 微调模型服务**：支持在通用加速器上服务动态 LoRA (Low-Rank Adaptation) 微调模型，通过多路复用减少 GPU 和 TPU 需求。\n*   **推理优化自动扩缩**：利用模型服务器指标实现 GKE Horizontal Pod Autoscaler (HPA) 的自动扩缩。\n*   **模型感知路由**：根据 GKE 集群内 OpenAI API 规范中定义的模型名称路由推理请求。\n*   **模型特定服务关键性**：允许指定 AI 模型的服务关键性，优先处理对延迟敏感的请求，而非对延迟容忍的批量推理作业。\n*   **集成 AI 安全**：与 Google Cloud Model Armor 集成，对模型提示和响应进行 AI 安全检查。\n*   **推理可观测性**：提供请求速率、延迟、错误和饱和度等推理请求的可观测性指标。\n\n## 面临的挑战\n大多数使用 GKE Inference Gateway 的企业客户希望保护和优化其代理/AI 工作负载。他们希望发布和商业化其代理 API，同时利用 Apigee 提供的高质量 API 治理功能，作为其代理 API 商业化策略的一部分。\n\n## 解决方案：GKE Inference Gateway 与 Apigee Operator for Kubernetes 的集成\nGKE Inference Gateway 通过引入 `GCPTrafficExtension` 资源解决了这一挑战。该资源使得 GKE Gateway 能够通过服务扩展（或 ext-proc）机制，向策略决策点 (PDP) 发起“侧向”调用。Apigee Operator for Kubernetes 利用这一服务扩展机制，对流经 GKE Inference Gateway 的 API 流量强制执行 Apigee 策略，从而为 GKE Inference Gateway 用户带来了 Apigee API 治理的优势。\n\n### 集成工作流程\nGKE Inference Gateway 和 Apigee Operator for Kubernetes 的协同工作步骤如下：\n1.  **配置 Apigee 实例**：GKE Inference Gateway 管理员在 Google Cloud 上配置一个 Apigee 实例。\n2.  **安装 Apigee Operator for Kubernetes**：管理员在其 GKE 集群中安装 Apigee Operator for Kubernetes，并将其连接到新配置的 Apigee 实例。\n3.  **创建 ApigeeBackendService**：创建一个 `ApigeeBackendService` 资源，该资源充当 Apigee 数据平面的代理。\n4.  **应用流量扩展**：将 `ApigeeBackendService` 作为 `GCPTrafficExtension` 中的 `backendRef` 进行引用。\n5.  **强制执行策略**：将 `GCPTrafficExtension` 应用到 GKE Inference Gateway，使 Apigee 能够对流经网关的 API 流量强制执行策略。\n\n![Apigee + GKE IG Diagram (2)](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Apigee__GKE_IG_Diagram_2.original.png)\n\n## Apigee Operator for Kubernetes：LLM 的 API 管理\nApigee 提供了一个全面的 API 管理层，适用于 Google Cloud、其他公共云和本地基础设施上的传统事务性 API 和大型语言模型 (LLM)。该平台提供强大的策略引擎、完整的 API 生命周期管理以及先进的 AI/ML 驱动分析功能。Apigee 在 Gartner Magic Quadrant 中被公认为 API 管理领域的领导者，服务于具有复杂 API 需求的大型企业。\n\n通过与 GKE Inference Gateway 的新集成，GKE 用户可以利用 Apigee 的全套功能来管理、治理和商业化其通过 API 提供的 AI 工作负载。这包括 API 生产者能够将 API 打包成 API 产品，并通过自助服务开发者门户提供给开发者。用户还可以获得 Apigee 的增值服务，如 API 安全和详细的 API 分析。\n\n### 集成后可用的 Apigee 策略\nGKE 用户可以通过集成访问 Apigee 管理的以下策略：\n*   API 密钥\n*   配额\n*   速率限制\n*   Google 访问令牌\n*   键值存储\n*   OpenAPI 规范验证\n*   流量峰值管理\n*   自定义 JavaScript\n*   响应缓存\n*   外部服务调用\n\nApigee Operator for Kubernetes 还支持管理员模板规则，允许组织管理员在其整个组织中强制执行策略规则。\n\n### 未来计划：Apigee AI 策略\n未来计划包括支持 Apigee AI 策略，涵盖：\n*   Model Armor 安全\n*   语义缓存\n*   令牌计数和强制\n*   基于提示的模型路由\n\n## 总结\n通过 GKE Inference Gateway 利用 Apigee 一流的 API 管理和安全能力，企业现在可以统一其 AI 服务和 API 治理层。借助 Apigee 全功能的 API 管理平台，企业可以专注于其核心任务：在 GKE 上运行推理引擎，利用公共云中一流的 AI 基础设施。",
      "shortSummary": "Apigee Operator for Kubernetes 与 GKE Inference Gateway 实现了集成，为 AI/LLM 工作负载提供了全面的 API 管理和治理。GKE Inference Gateway 优化了 AI 推理服务，而 Apigee Operator 则通过 `GCPTrafficExtension` 机制，将 Apigee 的认证、配额、速率限制等策略应用于流经网关的 API 流量。此集成使企业能够安全地发布、管理和商业化其代理/AI API，统一了 AI 服务与 API 治理层，并计划支持更多 AI 相关的策略。",
      "translated_title": "Apigee Operator for Kubernetes 与 GKE Inference Gateway 集成，用于认证和 AI/LLM 策略",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Apigee__GKE_IG_Diagram_2.original.png",
          "alt": "Apigee + GKE IG Diagram (2)",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "No AI/Agents without APIs!Many users interact with generative AI daily without realizing the crucial..."
    },
    {
      "title": "你的AI现在是本地专家：通过Google地图进行基础信息关联现已全面推出 (原标题: Your AI is now a local expert: Grounding with Google Maps is now GA)",
      "link": "https://developers.googleblog.com/en/your-ai-is-now-a-local-expert-grounding-with-google-maps-is-now-ga/",
      "pubDate": "Thu, 25 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-25T16:00:00.000Z",
      "creator": "Google",
      "summary": "## Vertex AI中Google地图基础信息关联功能全面推出\n\nGoogle于2025年9月26日宣布，Vertex AI中的“通过Google地图进行基础信息关联”（Grounding with Google Maps）功能现已全面推出（GA）。此功能旨在帮助开发者构建与Google地图实时、最新信息连接的生成式AI应用。\n\n### 重要性与定义\n\n*   **重要性：** 开发者在构建生成式AI应用时面临的一个关键挑战是确保模型输出的事实性和可靠性，尤其是在人们依赖这些信息做出实际决策时。此功能旨在解决这一问题。\n*   **“基础信息关联”（Grounding）定义：** 这是一个使大型语言模型（LLM）能够利用来自Google或客户自身数据的可验证信息源的过程。\n\n### 功能亮点与优势\n\n通过Google地图进行基础信息关联，开发者现在可以构建基于全球超过2.5亿商家和地点地理空间上下文的AI应用，这些信息来自Google地图及其活跃贡献者社区分享的本地洞察。\n\n该功能以多种方式帮助开发者构建事实可靠的生成式AI输出，提供更优质、更个性化的结果：\n\n*   **提供及时细节：** AI应用可访问每日更新的Google地图信息，从而向用户提供餐厅当前营业时间或商店是否暂时关闭等及时细节。\n*   **处理主观问题：** 结合Google地图用户评论的上下文，使用户能够轻松询问关于地点的主观问题，例如“这家咖啡馆的氛围如何？”或“这里适合安静的晚餐吗？”（*此功能目前仅在美国和印度可用。*）\n*   **加速商业发现：** 将Google地图数据与您的业务数据结合。例如，房地产公司可以为不同客户档案创建物业摘要，为有小孩的家庭突出附近的公园和学校，而为年轻专业人士则可能侧重于当地夜生活和餐厅。\n*   **提供全面查询结果：** 结合Google搜索和Google地图的基础信息关联功能。当用户询问“今晚公园里的音乐会我们可以带孩子去吗？”时，Gemini可以动态选择使用Google搜索查找表演者和场地政策，并使用Google地图提供儿童友好度信息以及附近建议。\n\n### 跨行业应用\n\nGoogle地图基础信息关联功能可在多个行业中发挥作用，早期采用的领域包括：\n\n*   **旅游和观光：** 帮助旅行者获得酒店、餐厅和景点推荐，涵盖从决定住宿地点、行程规划到当地探索的整个旅行体验。\n    *   万豪国际执行副总裁兼首席客户官Peggy Roe表示，这有助于提供真实的优质服务，深化与客人的互动。\n*   **房地产：** 为潜在买家提供关于社区的详细信息，包括定制房源摘要和类似社区的建议。\n    *   Compass工程副总裁Yotam Lemberger指出，这将通过生成式AI改善购房和销售体验，提供更个性化的位置洞察。\n*   **设备：** 个人助理可帮助用户发现附近景点并回答关于社区的问题。\n*   **社交媒体：** 帮助朋友共同研究活动和餐饮，包括集体探索区域、提供附近餐厅建议或帮助用户了解当地夜生活。\n\n### 如何开始\n\nVertex AI中的Google地图基础信息关联功能现已与Gemini一同提供，并包含一个便于实验的层级（Gemini Pro 10K提示）。要了解更多信息并开始使用，请访问相关文档、查看交互式演示，并访问Google Cloud Console中的Vertex AI Studio。开发者只需将Google Maps工具添加到Gemini API即可实现Google地图基础信息关联。",
      "shortSummary": "Google宣布Vertex AI中的“通过Google地图进行基础信息关联”功能现已全面推出。此功能使开发者能够构建与Google地图实时、最新信息连接的生成式AI应用，确保模型输出的事实性和可靠性。它允许AI利用全球超过2.5亿商家和地点的地理空间上下文，提供个性化、及时的信息，并支持旅游、房地产、设备和社交媒体等多个行业。开发者可通过Gemini API轻松集成此功能，并享有免费试用层级。",
      "translated_title": "你的AI现在是本地专家：通过Google地图进行基础信息关联现已全面推出",
      "images": [],
      "contentSource": "完整文章",
      "content": "We are excited to announce Grounding with Google Maps in Vertex AI is now Generally Available (GA). ..."
    },
    {
      "title": "持续推出我们的最新模型，发布改进版 Gemini 2.5 Flash 和 Flash-Lite (原标题: Continuing to bring you our latest models, with an improved Gemini 2.5 Flash and Flash-Lite release)",
      "link": "https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/",
      "pubDate": "Wed, 24 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-24T16:00:00.000Z",
      "creator": "Google",
      "summary": "# Gemini 2.5 Flash 和 Flash-Lite 更新发布\n\n*   **发布日期与目标**\n    *   **发布日期**：2025年9月25日\n    *   **发布内容**：Gemini 2.5 Flash 和 2.5 Flash-Lite 的更新版本。\n    *   **可用平台**：Google AI Studio 和 Vertex AI。\n    *   **核心目标**：持续提升模型质量和效率。\n\n*   **关键改进概览**\n    *   **质量与速度提升**：与当前稳定模型相比，Gemini 2.5 Flash 和 2.5 Flash-Lite 预览模型的质量和速度均有提升。\n        *   ![Intelligence vs End-to-End response time](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/rev_21_benchmarks_2.original.png)\n    *   **输出令牌效率**：\n        *   Gemini 2.5 Flash-Lite 的输出令牌（及成本）减少50%。\n        *   Gemini 2.5 Flash 的输出令牌（及成本）减少24%。\n        *   ![Output token efficiency](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/rev_21_benchmarks_1.original.png)\n\n*   **更新版 Gemini 2.5 Flash-Lite 详情**\n    *   基于三个关键主题进行训练和构建：\n        1.  **更好的指令遵循能力**：显著提升了遵循复杂指令和系统提示的能力。\n        2.  **减少冗余**：生成更简洁的答案，有效降低高吞吐量应用的令牌成本和延迟。\n        3.  **更强的多模态与翻译能力**：提供更准确的音频转录、更好的图像理解和改进的翻译质量。\n    *   **测试模型字符串**：`gemini-2.5-flash-lite-preview-09-2025`\n\n*   **更新版 Gemini 2.5 Flash 详情**\n    *   主要在两个收到持续反馈的领域进行了改进：\n        1.  **更好的智能体工具使用**：改进了模型使用工具的方式，在更复杂、智能体化和多步骤应用中表现更佳。在关键智能体基准测试中表现显著提升，SWE-Bench Verified 提升5%（从48.9%到54%）。\n        2.  **更高效率**：在“思考开启”模式下，模型成本效率显著提高，以更少的令牌实现更高质量的输出，降低延迟和成本。\n    *   **早期测试者反馈**：Manus 联合创始人兼首席科学家 Yichao ‘Peak’ Ji 表示，新模型在长周期智能体任务中性能提升15%，卓越的成本效率使其能够以前所未有的规模扩展。\n    *   **测试模型字符串**：`gemini-2.5-flash-preview-09-2025`\n\n*   **使用 Gemini 进行构建**\n    *   发布预览版本旨在让开发者测试最新改进、提供反馈并构建生产级体验。\n    *   本次发布并非稳定版本，而是为未来的稳定版本奠定基础。\n\n*   **引入 `-latest` 别名**\n    *   为每个模型系列引入 `-latest` 别名（如 `gemini-flash-latest` 和 `gemini-flash-lite-latest`），始终指向最新模型版本。\n    *   方便开发者试验新功能，无需为每次发布更新代码。\n    *   **重要提示**：在通过 `-latest` 更新或弃用特定版本前，将提前两周通过电子邮件通知。\n    *   **注意**：使用 `-latest` 别名时，速率限制、成本和可用功能可能在不同版本之间波动。\n    *   **稳定性应用**：对于需要更高稳定性的应用，建议继续使用 `gemini-2.5-flash` 和 `gemini-2.5-flash-lite`。\n\n*   **未来展望**\n    *   Google 将继续推动 Gemini 的前沿发展，未来将分享更多信息。",
      "shortSummary": "Google于2025年9月25日发布了Gemini 2.5 Flash和Flash-Lite的更新版本，旨在提升模型质量和效率。新版Flash-Lite在指令遵循、简洁性及多模态/翻译能力上有所增强。Flash模型则改进了智能体工具使用和整体效率，并在SWE-Bench Verified等基准测试中取得显著进展。为简化最新模型访问，Google还引入了`-latest`别名。这些预览版将为未来的稳定版本提供基础，并允许开发者提前测试和反馈。",
      "translated_title": "持续推出我们的最新模型，发布改进版 Gemini 2.5 Flash 和 Flash-Lite",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/rev_21_benchmarks_2.original.png",
          "alt": "Intelligence vs End-to-End response time",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/rev_21_benchmarks_1.original.png",
          "alt": "Output token efficiency",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "Google is releasing updated Gemini 2.5 Flash and Flash-Lite preview models with improved quality, speed, and efficiency. These releases introduce a \"-latest\" alias for easy access to the newest versions, allowing developers to test and provide feedback to shape future stable releases."
    },
    {
      "title": "使用Gemini Robotics-ER 1.5构建下一代物理智能体 (原标题: Building the Next Generation of Physical Agents with Gemini Robotics-ER 1.5)",
      "link": "https://developers.googleblog.com/en/building-the-next-generation-of-physical-agents-with-gemini-robotics-er-15/",
      "pubDate": "Wed, 24 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-24T16:00:00.000Z",
      "creator": "Google",
      "summary": "# Gemini Robotics-ER 1.5：下一代机器人具身推理模型\n\nGoogle AI于2025年9月25日发布了其最先进的机器人具身推理模型Gemini Robotics-ER 1.5，并向所有开发者开放预览。这是首个广泛可用的Gemini Robotics模型，旨在作为机器人的高级推理“大脑”。\n\n## 核心能力与优势\n\nGemini Robotics-ER 1.5模型专门针对机器人关键能力进行了优化，包括：\n\n*   **视觉与空间理解**：快速而强大的空间推理能力，能够以Gemini Flash模型的低延迟生成语义精确的2D点，并基于物品的尺寸、重量和可操作性进行推理。\n*   **任务规划**：能够将复杂的请求（如“清理桌子”）分解为详细的计划。\n*   **进度评估**：有效估计任务执行的进展。\n*   **工具调用**：原生支持调用Google搜索等工具以获取信息，以及调用视觉-语言-动作模型（VLA）或任何第三方用户定义函数来执行任务。\n*   **具身推理优化**：它是首个为此类具身推理优化的“思考”模型，在学术和内部基准测试中均达到最先进的性能。\n\n## 解决复杂日常任务\n\n该模型旨在解决对机器人而言极具挑战性的任务。例如，要求机器人“将这些物品分类到正确的堆肥、回收和垃圾箱中”。这需要机器人：\n\n1.  在线查找当地回收指南。\n2.  理解面前的物体。\n3.  根据当地规则确定如何分类。\n4.  执行所有步骤完成分类。\n\nGemini Robotics-ER 1.5能够处理这类需要上下文信息和多步骤才能完成的日常任务。\n\n## Gemini Robotics-ER 1.5的新特性\n\n该模型针对机器人应用进行了专门调整，并引入了多项新功能：\n\n*   **快速强大的空间推理**：\n    *   生成语义精确的2D点，并基于物品尺寸、重量和可操作性进行推理。\n    *   支持“指向任何你可以拿起的东西”等命令，实现准确、响应迅速的交互。\n*   **编排高级智能体行为**：\n    *   利用高级空间和时间推理、规划和成功检测，实现可靠的长期任务执行循环（例如，“根据这张图片重新整理我的办公桌”）。\n    *   原生调用Google搜索工具和任何第三方用户定义函数（例如，“根据当地规则将垃圾分类到正确的垃圾箱中”）。\n*   **灵活的思考预算**：\n    *   开发者可以直接控制延迟与准确性之间的权衡。\n    *   对于复杂任务（如多步骤组装），模型可以“思考更长时间”以提高准确性；对于反应性任务（如检测或指向物体），则可以要求快速响应。\n*   **改进的安全过滤器**：\n    *   增强了语义安全性，更好地识别并拒绝生成违反物理约束（例如，超出机器人有效载荷能力）的计划。\n\n## 机器人的智能大脑\n\nGemini Robotics-ER 1.5可被视为机器人的高级大脑，能够理解复杂的自然语言指令，推理长期任务，并编排复杂的行为。它不仅擅长感知，还能理解场景中的内容以及如何处理。\n\n例如，它可以将“清理桌子”这样的复杂请求分解为计划，并调用合适的工具，无论是机器人的硬件API、专门的抓取模型，还是用于运动控制的视觉-语言-动作模型（VLA）。\n\n### 高级空间理解\n\n为了让机器人与物理世界互动，它们需要感知和理解环境。Gemini Robotics-ER 1.5经过微调，可生成高质量的空间结果，为物体提供精确的2D点。\n\n![Gemini Robotics-ER 1.5性能图](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/GeminiRoboticsER1.5_Graph-ERGen-RD3_V001.original.png)\n*Gemini Robotics-ER 1.5模型是我们最先进的具身推理模型，同时作为通用多模态基础模型也保持了强大的性能。*\n\n![Gemini Robotics-ER 1.5指向精度图](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/GeminiRoboticsER1.5_Graph-PointingBenchmark-RD3.original.png)\n*Gemini Robotics-ER 1.5是用于指向精度的最精确视觉-语言模型。*\n\n**2D点生成示例**：\n\n给定厨房场景图像，Gemini Robotics-ER 1.5可以提供每个物体（或物体的一部分）的位置。结合机器人的3D传感器，可以确定物体在空间中的精确位置，从而生成准确的运动计划。\n\n![厨房场景中物品的2D点生成](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/unnamed-2_2.original.png)\n*提示：指出图像中的以下物品：洗碗皂、碗碟架、水龙头、电饭煲、独角兽。点以[y, x]格式表示，归一化到0-1000。只包含图像中实际存在的物体。*\n\n模型只会包含图像中存在的物品，避免了幻觉（例如，不会为不存在的“独角兽”生成点），使其基于实际可见内容进行推理。\n\n### 时间推理\n\n除了定位物体，真正的时空推理还涉及理解物体和动作随时间展开的关系。Gemini Robotics-ER 1.5通过处理视频来理解物理世界中的因果关系。\n\n### 利用可操作性编排长期任务\n\n当启用“思考”功能时，模型可以推理复杂的指向和边界框查询。例如，在咖啡制作过程中，它能理解“如何”以及“在哪里”放置马克杯、咖啡胶囊，以及如何关闭咖啡机，甚至在清理时知道马克杯的放置位置。\n\n![咖啡制作示例：识别马克杯放置位置](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_bjgs08R.original.png)\n*提示：识别我应该把马克杯放在哪里来制作一杯咖啡。以JSON对象列表的形式返回：`[{\"box_2d\": [y_min, x_min, y_max, x_min], \"label\": <label>}]`，坐标在0-1000之间归一化。*\n\n![咖啡制作示例：识别咖啡胶囊放置位置](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image10_5vMuO9V.original.png)\n*提示：我应该把咖啡胶囊放在哪里？*\n\n![咖啡制作示例：关闭咖啡机轨迹规划](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image15_kFakYKn.original.png)\n*提示：现在，我需要关闭咖啡机。绘制8个点轨迹，指示盖子把手应如何移动以关闭它。从把手开始。点为[Y,X]归一化坐标[0 - 1000]。请输出所有点，包括轨迹点，格式为：`[{\"point\": [Y, X], \"label\": }, {\"point\": [Y, X], \"label\": }, ...]`。*\n\n![咖啡制作示例：清理后马克杯放置位置](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image1_PCWZQMD.original.png)\n*提示：我喝完咖啡了。现在我应该把马克杯放在哪里清理？以JSON对象列表的形式返回：`[{\"point\": [y, x], \"label\": <label>}]`，坐标在0-1000之间归一化。*\n\n![垃圾分类示例](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image12_O0rW8PY.original.png)\n*另一个结合规划和空间定位的例子，生成一个“空间定位”的计划。*\n\n### 灵活的思考预算\n\n![思考预算对性能的影响](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/GeminiRoboticsER1.5_Graph-TTB-RD2_V001.original.png)\n*Gemini Robotics-ER 1.5使用推理时计算扩展来提高性能。思考令牌预算是可调的。这允许开发者在延迟敏感任务和高精度推理任务之间取得平衡。*\n\n模型性能随思考令牌预算的增加而提高。对于简单的空间理解任务（如物体检测），在非常短的思考预算下也能获得高性能；而更复杂的推理则受益于更大的预算。开发者可以通过`thinking_config`选项来设置或禁用思考预算。\n\n## 安全注意事项\n\nGemini Robotics-ER 1.5在安全性方面取得了显著改进，增强了以下方面的过滤器：\n\n*   **语义安全**：模型能够理解并拒绝生成危险或有害任务的计划，并已通过ASIMOV基准等严格评估。\n*   **物理约束感知**：模型现在能更好地识别请求是否会违反定义的物理约束（例如，机器人的有效载荷能力或工作空间限制）。\n\n然而，这些模型层面的安全保障不能替代物理系统所需的严格安全工程。Google倡导“瑞士奶酪”安全方法，即多层保护协同工作。开发者仍有责任实施标准的机器人安全最佳实践，包括紧急停止、避障和彻底的风险评估。\n\n## 立即开始构建\n\nGemini Robotics-ER 1.5现已提供预览版。它提供了构建机器人推理引擎所需的感知和规划能力。\n\n*   通过[Google AI Studio](https://aistudio.google.com/app/prompts/new_chat)开始体验模型。\n*   查阅[开发者文档](https://developers.google.com/gemini/reference/rest/v1beta/models/generateContent)以获取快速入门和API参考。\n*   探索[Colab笔记本](https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/tutorials/robotics_er_1_5_quickstart.ipynb)以查看实际实现。\n\n## 深入研究\n\n该模型是更广泛的Gemini Robotics系统的基础推理组件。要了解其背后的科学原理，包括端到端动作模型（VLA）和跨具身学习，请阅读[研究博客](https://ai.google.dev/research/blog/gemini-robotics-er-1-5)和[完整技术报告](https://arxiv.org/abs/2409.09117)。",
      "shortSummary": "Google AI发布了Gemini Robotics-ER 1.5，这是一款面向开发者的先进机器人具身推理模型。它作为机器人的高级“大脑”，专注于视觉与空间理解、任务规划和进度评估。该模型能原生调用工具，处理复杂的日常任务，并具备灵活的思考预算和改进的安全过滤器。Gemini Robotics-ER 1.5在空间和时间推理方面表现出色，能理解物理世界中的因果关系，并支持长期任务的编排。开发者现可通过Google AI Studio等平台进行预览和构建。",
      "translated_title": "使用Gemini Robotics-ER 1.5构建下一代物理智能体",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/GeminiRoboticsER1.5_Graph-ERGen-RD3_V001.original.png",
          "alt": "GeminiRoboticsER1.5_Graph-ER+Gen-RD3_V001",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/GeminiRoboticsER1.5_Graph-PointingBenchmark-RD3.original.png",
          "alt": "GeminiRoboticsER1.5_Graph-PointingBenchmark-RD3_V001 (1)",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/unnamed-2_2.original.png",
          "alt": "image9",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_bjgs08R.original.png",
          "alt": "image15",
          "title": "",
          "position": 4
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image10_5vMuO9V.original.png",
          "alt": "image10",
          "title": "",
          "position": 5
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image15_kFakYKn.original.png",
          "alt": "image15",
          "title": "",
          "position": 6
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image1_PCWZQMD.original.png",
          "alt": "image1",
          "title": "",
          "position": 7
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image12_O0rW8PY.original.png",
          "alt": "image12",
          "title": "",
          "position": 8
        }
      ],
      "contentSource": "完整文章",
      "content": "Gemini Robotics-ER 1.5, now available to developers, is a state-of-the-art embodied reasoning model for robots. It excels in visual, spatial understanding, task planning, and progress estimation, allowing robots to perform complex, multi-step tasks."
    },
    {
      "title": "LiteRT-LM赋能Chrome、Chromebook Plus和Pixel Watch上的设备端生成式AI (原标题: On-device GenAI in Chrome, Chromebook Plus, and Pixel Watch with LiteRT-LM)",
      "link": "https://developers.googleblog.com/en/on-device-genai-in-chrome-chromebook-plus-and-pixel-watch-with-litert-lm/",
      "pubDate": "Tue, 23 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-23T16:00:00.000Z",
      "creator": "Google",
      "summary": "# LiteRT-LM：赋能设备端生成式AI\n\n本文介绍了LiteRT-LM，一个生产就绪的推理框架，旨在解决在用户设备上运行大型语言模型（LLM）所面临的挑战，从而提升产品体验。\n\n## 设备端LLM的优势与挑战\n\n*   **优势**：\n    *   **离线可用性**：随时可用，无需网络连接。\n    *   **成本效益**：无按API调用付费，适用于高频任务如文本摘要或校对。\n*   **挑战**：\n    *   **部署复杂性**：在各种边缘硬件上部署千兆字节规模的模型。\n    *   **性能要求**：实现亚秒级的首个Token生成时间（TTFT）延迟。\n    *   **质量保证**：同时满足所需的输出质量。\n\n## LiteRT-LM的推出与应用\n\n*   **核心功能**：LiteRT-LM是一个生产就绪的推理框架，已为Google产品中Gemini Nano最广泛的部署提供支持。\n*   **赋能产品**：它使Gemini Nano和Gemma等模型能够在Chrome、Chromebook Plus和Pixel Watch等产品上实现设备端运行，并通过MediaPipe LLM推理API支持其他开放模型。\n*   **开发者访问**：\n    *   现有高级API：MediaPipe LLM推理API、Chrome内置AI API和Android AICore。\n    *   **新开放**：首次提供LiteRT-LM引擎的底层C++接口（预览版），允许开发者构建定制化、高性能的AI管道。\n\n## LiteRT-LM在Google AI Edge堆栈中的定位\n\nLiteRT-LM是一个经过生产测试的推理框架，用于在各种边缘设备上高性能运行Gemini Nano、Gemma等LLM。它是一个完全开源的项目，提供易于集成的API和可重用模块，帮助开发者构建定制化的LLM管道。\n\nGoogle AI Edge堆栈从低到高抽象层级包括：\n\n1.  **LiteRT**：在设备上高效执行单个ML/AI模型的基础运行时。\n2.  **LiteRT-LM**：基于C++的LLM管道框架，利用LiteRT协同运行多个模型和处理步骤（如会话克隆、KV缓存管理、提示缓存/评分、有状态推理），以完成复杂的生成式AI任务。\n3.  **LLM Inference API**：由LiteRT-LM提供支持的高级原生GenAI API（Kotlin、Swift、JS）。\n\n这种分层结构提供了灵活性，LiteRT-LM则为开发者提供了核心能力和适应性，以大规模部署设备端LLM。\n\n## LiteRT-LM的关键亮点\n\n*   **跨平台**：支持Android、Linux、macOS、Windows和Raspberry Pi。\n*   **硬件加速**：利用LiteRT运行时，支持CPU、GPU和NPU加速，充分发挥设备端硬件潜力。\n*   **增强灵活性**：模块化设计和开源代码库提供最大的推理管道定制灵活性，支持多模态、开放权重模型以及跨各种移动平台和加速器的生产级大型模型推理。\n\n## 案例研究\n\n### 1. Chrome和Chromebook Plus中的多LLM功能\n\n现代LLM的千兆字节规模带来了独特的部署挑战。LiteRT-LM通过以下架构和优化克服了这一挑战：\n\n*   **共享基础模型与LoRA**：允许多个功能共享一个基础模型，并使用轻量级LoRA进行功能特定定制。\n*   **Engine/Session架构**：\n    *   **Engine（单例）**：作为应用程序功能共享的单一实例，管理所有昂贵、共享的资源（如基础模型和多模态编码器），并根据运行时环境智能地处理资源的加载和卸载。\n    *   **Session（有状态接口）**：应用程序功能与之交互的接口，每个Session代表一个独立的对话或任务，管理其自身的状态、历史和上下文。Session可以通过小型、任务特定的适配器（LoRA权重）来定制基础模型的行为。\n\n    ![LiteRT-LM引擎/会话系统架构图](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/engine_session.original.png)\n    *LiteRT-LM引擎/会话系统架构图：Engine（底部）作为中央资源管理器，生成两个不同的Session（顶部）；一个用于摘要，一个用于图像理解。两个Session共享通用资源，如基础文本解码器和分词器，而图像理解Session额外请求视觉编码器。由于没有活动的Session需要音频编码器，Engine将其从内存中卸载。*\n\n*   **关键优化**：\n    *   **上下文切换**：每个Session封装其完整的“上下文”（包括Transformer的KV缓存、LoRA权重等）。切换任务时，LiteRT-LM保存传出Session的状态并恢复传入Session的状态，确保共享LLM始终具有活动任务的正确状态。\n    *   **会话克隆**：用户可以克隆Session以避免重新计算共享的提示前缀，从而有效地缓存特定点的KV缓存状态，允许从该状态分支出多个新任务，节省大量计算。\n    *   **写时复制（CoW）KV缓存**：KV缓存可能非常大。CoW机制使得克隆的Session不会立即复制KV缓存，而是创建对原始缓冲区的引用。只有当Session即将覆盖与另一个Session内容冲突的新数据时，才会执行实际复制。这使得克隆速度极快（<10ms），并通过重用KV缓存缓冲区最大限度地减少内存占用。\n\n这些架构和优化能力对于在Chrome和Chromebook Plus中成功实现多个高性能设备端LLM功能至关重要。LiteRT-LM还通过LiteRT作为底层运行时进行后端委托，并抽象平台特定组件，实现了广泛的平台兼容性。\n\n### 2. Pixel Watch上的语言模型部署\n\n在Pixel Watch等资源严重受限的设备上部署LLM，需要将重点从支持多功能共享模型转向部署具有最小二进制大小和内存占用的单一专用功能。\n\n*   **模块化设计**：LiteRT-LM的模块化设计在此发挥关键作用。它允许开发者直接从其核心组件构建定制管道。\n*   **Pixel Watch优化**：为Pixel Watch选择了最少必需的模块（如执行器、分词器和采样器），并组装了一个专门的管道，从而最大限度地减小了二进制大小和内存使用，以满足设备的资源限制。\n\n    ![为Pixel Watch优化的轻量级LLM管道](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/litert_lm_pixel_watch.original.png)\n    *为Pixel Watch优化的轻量级LLM管道*\n\n这个案例展示了LiteRT-LM的灵活性，其模块化组件使开发者能够根据任何目标设备（从强大的智能手机到受限的可穿戴设备）的特定资源和功能要求，精确定制LLM部署。\n\n## 如何开始\n\n*   **探索社区**：访问LiteRT HuggingFace社区，发现兼容的开放模型如Gemma和Qwen。\n*   **访问代码**：查看GitHub仓库，获取C++预览版和示例代码。\n*   **阅读文档**：深入了解使用LiteRT-LM运行时在设备上构建和执行LLM的必要步骤。",
      "shortSummary": "LiteRT-LM是一个生产就绪的推理框架，旨在赋能Chrome、Chromebook Plus和Pixel Watch等设备上的生成式AI。它解决了在边缘设备上部署大型语言模型（LLM）的性能和资源挑战，提供离线可用性和成本效益。LiteRT-LM支持跨平台部署，利用硬件加速，并通过模块化设计和Engine/Session架构实现多功能共享模型及资源受限设备的定制化部署。开发者现可通过C++接口访问其底层能力，构建高性能设备端AI应用。",
      "translated_title": "LiteRT-LM赋能Chrome、Chromebook Plus和Pixel Watch上的设备端生成式AI",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/engine_session.original.png",
          "alt": "engine_session",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/litert_lm_pixel_watch.original.png",
          "alt": "litert_lm_pixel_watch",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "Google AI Edge provides the tools to run AI features on-device, and its new LiteRT-LM runtime is a significant leap forward for generative AI. LiteRT-LM is an open-source C++ API, cross-platform compatibility, and hardware acceleration designed to efficiently run large language models like Gemma and Gemini Nano across a vast range of hardware. Its key innovation is a flexible, modular architecture that can scale to power complex, multi-task features in Chrome and Chromebook Plus, while also being lean enough for resource-constrained devices like the Pixel Watch. This versatility is already enabling a new wave of on-device generative AI, bringing capabilities like WebAI and smart replies to users."
    },
    {
      "title": "Google Colab 推出更多返校季功能改进！ (原标题: Google Colab Adds More Back to School Improvements!)",
      "link": "https://developers.googleblog.com/en/google-colab-adds-more-back-to-school-improvements/",
      "pubDate": "Tue, 23 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-23T16:00:00.000Z",
      "creator": "Google",
      "summary": "# Google Colab 推出更多返校季功能改进\n\nGoogle Colab 于2025年9月24日宣布了一系列新功能，旨在简化和增强基于笔记本的教学材料。这些更新主要包括在笔记本层面冻结运行时版本、无缝演示和复制笔记本。\n\n## 引入运行时版本选择器\n\n为了解决笔记本环境中代码重现性差的问题（例如，因依赖项或兼容性错误导致旧笔记本无法运行），Colab 推出了运行时版本选择器。此功能旨在确保用户的工作稳定、可共享且可重现。\n\n*   **挑战**：Colab 运行时会持续更新最新的软件包、安全补丁和功能，这虽然提供了现代工具，但也可能导致旧代码因核心依赖项变化而中断。\n*   **解决方案**：用户现在可以将笔记本固定到特定的、版本化的 Colab 运行时。这意味着无论何时运行代码，都能保证在相同的运行时环境和相同的软件包版本下执行。\n*   **如何使用**：\n    *   在“更改运行时类型”对话框中找到新的运行时版本选择器。\n    *   新笔记本默认使用“最新”版本。\n    *   用户可以选择特定版本（例如 2025.07），以确保项目、课程或出版物的长期稳定性。\n*   **适用场景**：\n    *   需要确保结果可验证的研究人员。\n    *   需要为所有学生提供一致工作体验的教育工作者。\n    *   依赖特定依赖栈构建复杂项目的开发人员。\n*   **透明度**：每个可用的运行时版本都对应 Colab 的 `backend-info` GitHub 仓库中的一个分支，用户可以在其中查看完整的运行时版本快照，包括 Ubuntu 操作系统版本、Python/Julia/R 版本以及已安装的软件包详情等。\n*   **维护计划**：Colab 计划维护过去的运行时版本一年，并欢迎用户就所需维护时长提供反馈。\n\n![运行时选择器](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Runtime_Selector_V2.original.png)\n\n## 幻灯片模式改进\n\nColab 对其幻灯片模式功能进行了增强，使其更加灵活和易用。\n\n*   **从任意位置开始幻灯片**：\n    *   选择“开始幻灯片”将从当前聚焦的单元格开始演示。\n    *   选择“从头开始幻灯片”则会从笔记本的第一个单元格开始。\n\n![开始幻灯片菜单](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Start_Slideshow_Menu_V2.original.png)\n\n*   **带有单元格配对的幻灯片**：如果创建一个可折叠标题，并在其下方放置一个代码单元格，它们将被配对到一张幻灯片中。这非常适合在同一张幻灯片上同时展示简短的解释和可运行的示例。\n\n![访问幻灯片元素](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Accessing_A_Slideshow_Element_V2.original.png)\n\n## 新增 URL 链接功能\n\n为了方便分享笔记本内容，Colab 引入了新的 URL 链接功能。\n\n*   **复制链接**：在任何笔记本 URL 的末尾添加 `#copy=true`，当打开该链接时，Colab 将自动弹出复制对话框。例如：`https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/Getting_started_with_google_colab_ai.ipynb#copy=true`\n\n![复制对话框截图](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Screenshot_2025-09-12_at_3.36.42PM.original.png)\n\n*   **幻灯片模式链接**：类似地，在 URL 末尾添加 `#slideshowMode=true` 可以自动以幻灯片模式打开笔记本。例如：`https://colab.sandbox.google.com/drive/1gCqFEquqNvEoTDX3SNhR2PZkXWPHKXnc?usp=sharing#slideshowMode=true`\n\n这些功能旨在使课程材料的分享变得更加便捷。\n\n## 展望\n\n运行时版本是 Colab 提升工作健壮性和可重现性的基础性一步。结合更灵活的幻灯片模式（可从任意单元格开始演示，实现解释与实时演示同步）和新的 URL 链接功能（简化内容分享），Colab 希望这些改进能极大地提升课堂工作流程的效率。Colab 鼓励用户尝试这些新功能，并通过 X 或 GitHub 提供反馈。",
      "shortSummary": "Google Colab 推出多项返校季改进，旨在提升教学和研究体验。核心更新包括：引入**运行时版本选择器**，允许用户将笔记本固定到特定运行时版本，解决代码重现性问题；增强**幻灯片模式**，支持从任意单元格开始演示，并能将标题与代码单元格配对显示；新增**URL 链接功能**，通过在链接末尾添加参数（如 `#copy=true` 或 `#slideshowMode=true`）即可实现快速复制或自动进入幻灯片模式，极大方便了内容分享和课堂教学。",
      "translated_title": "Google Colab 推出更多返校季功能改进！",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Runtime_Selector_V2.original.png",
          "alt": "Runtime Selector",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Start_Slideshow_Menu_V2.original.png",
          "alt": "Start Slideshow Menu",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Accessing_A_Slideshow_Element_V2.original.png",
          "alt": "Accessing A Slideshow Element",
          "title": "",
          "position": 3
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Screenshot_2025-09-12_at_3.36.42PM.original.png",
          "alt": "Screenshot 2025-09-12 at 3.36.42 PM",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "完整文章",
      "content": "Colab is back with a few new and exciting features meant to simplify and enhance your notebook-based..."
    },
    {
      "title": "推出Data Commons模型上下文协议（MCP）服务器：为AI开发者简化公共数据访问 (原标题: Introducing the Data Commons Model Context Protocol (MCP) Server: Streamlining Public Data Access for AI Developers)",
      "link": "https://developers.googleblog.com/en/datacommonsmcp/",
      "pubDate": "Tue, 23 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-23T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 推出Data Commons模型上下文协议（MCP）服务器：为AI开发者简化公共数据访问\n\n2025年9月24日，Data Commons宣布正式发布其模型上下文协议（MCP）服务器。这一里程碑式的发布旨在让Data Commons庞大且相互关联的公共数据集能够即时被全球的AI开发者、数据科学家和组织访问并利用。此举进一步支持了Data Commons的宏伟目标：利用真实世界的统计信息作为锚点，帮助减少大型语言模型（LLM）的“幻觉”现象。\n\n## MCP服务器的优势\n\nMCP服务器为AI代理提供了一种标准化方式，使其能够原生消费Data Commons数据。这使得开发者无需学习或直接与复杂的底层API交互，即可利用Data Commons全面的数据。它显著加速了数据丰富、代理驱动型应用程序的创建，从而有效降低LLM产生幻觉的几率。\n\n开发者可以比以往更快地部署AI代理和应用程序，向最终用户提供可信、有来源的Data Commons信息。MCP服务器使代理能够处理各种数据驱动的查询，从初步探索到生成性报告：\n\n*   **探索性查询：** 例如，“非洲有哪些健康数据？”\n*   **分析性查询：** 例如，“比较金砖国家的人均寿命、经济不平等和GDP增长。”\n*   **生成性查询：** 例如，“生成一份关于美国各县收入与糖尿病之间关系的简明报告。”\n\n## 真实世界用例：ONE数据代理\n\n自2023年以来，Google的Data Commons与全球倡导组织ONE Campaign建立了合作关系，该组织致力于为非洲的经济机会和健康生活争取投资。此次合作促成了ONE数据平台的创建，该平台将ONE组织的全球发展数据和政策专业知识与Data Commons提供的广泛公共数据集相结合。\n\n作为首个用例，ONE数据利用MCP服务器和代理驱动的探索能力，开发了“ONE数据代理”——一个用于健康融资数据的交互式平台。这个新工具允许用户在几秒钟内使用自然语言搜索数千万个健康融资数据点，然后可视化这些数据并下载干净的数据集，从而节省时间，并有助于改进倡导、报告和政策制定。\n\n这对于全球健康领域的工作者来说是一项关键创新。发展中国家迫切需要加强卫生系统，但寻找可靠的健康融资数据却是一项重大挑战——如同大海捞针。这些信息分散在数千个不同的信息孤岛中，埋藏在不同的报告格式里，由技术术语组织，并存储在多个独立的数据库中。现在，例如，如果您想识别哪些国家面临捐助者削减资金的风险，您可以快速搜索那些最依赖外部健康资金、因此最容易受到援助削减或债务冲击影响的国家。\n\n要从传统数据库中编制一份可靠的报告，用户需要跨数据集工作并手动提取数据。然而，代理能够理解复杂的查询，并能够快速获取和编译所需数据。ONE数据代理正在为可访问、有影响力的数据驱动型倡导开辟新时代。\n\n![ONE数据代理截图](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/ONE_Data_Agent_Screenshot.original.jpg)\n\n## 如何开始\n\n无论您是正在原型设计新的AI代理、为产品添加数据功能，还是简化组织的分析工作流程，Data Commons MCP服务器都将帮助您更快地行动。\n\n它旨在实现无缝集成和最小化的入门摩擦。Data Commons MCP服务器自然地融入了Google Cloud Platform最新的代理开发工作流程，例如代理开发工具包（ADK）和包括Gemini CLI在内的客户端。该服务器还可以轻松集成到任何其他代理工作流程或平台中。\n\n为了帮助您入门，我们提供了一个Colab笔记本中的ADK示例代理，以及使用Gemini CLI服务器的说明：\n\n*   通过安装PyPi包，在Gemini CLI或您喜欢的MCP客户端中试用。\n*   使用Google Colab开始开发ADK代理。\n*   探索我们的GitHub存储库，查看示例代理并开始构建您自己的代理。",
      "shortSummary": "Data Commons发布了模型上下文协议（MCP）服务器，旨在为AI开发者简化公共数据访问，并减少大型语言模型（LLM）的幻觉。该服务器提供标准化方式，使AI代理能原生消费Data Commons数据，加速数据驱动应用开发。它支持探索性、分析性和生成性查询。ONE数据代理是一个成功案例，通过MCP服务器简化了全球健康融资数据的获取和分析，提升了倡导和政策制定效率。开发者可通过Gemini CLI、Colab和GitHub资源开始使用。",
      "translated_title": "推出Data Commons模型上下文协议（MCP）服务器：为AI开发者简化公共数据访问",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/ONE_Data_Agent_Screenshot.original.jpg",
          "alt": "ONE Data Agent Screenshot",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "Data Commons announces the availability of its MCP Server, which is a major milestone in making all of Data Commons’ vast public datasets instantly accessible and actionable for AI developers worldwide."
    },
    {
      "title": "Gemini CLI 🤝 FastMCP：简化 MCP 服务器开发 (原标题: Gemini CLI 🤝 FastMCP: Simplifying MCP server development)",
      "link": "https://developers.googleblog.com/en/gemini-cli-fastmcp-simplifying-mcp-server-development/",
      "pubDate": "Sun, 21 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-21T16:00:00.000Z",
      "creator": "Google",
      "summary": "## Gemini CLI 与 FastMCP 集成：简化 MCP 服务器开发\n\n本文宣布 Gemini CLI 已与 FastMCP（Python 中用于构建模型上下文协议 (MCP) 服务器的领先库）实现无缝集成。此项集成旨在简化 MCP 服务器的开发过程，使用户能够轻松地将自定义 MCP 工具和提示直接连接到 Gemini CLI。\n\n### 核心集成功能\n\n*   **安装与配置**\n    *   从 FastMCP v2.12.3 版本开始，用户可以使用 `fastmcp install gemini-cli server.py` 命令来安装使用 FastMCP 构建的本地 STDIO 传输 MCP 服务器。\n    *   此命令大大简化了流程，使 FastMCP 服务器的功能能够立即在 Gemini CLI 中可用并配置。\n    *   它会自动处理配置、依赖管理，并调用 Gemini CLI 内置的 MCP 管理系统（`gemini mcp add`）。\n    *   对于本地服务器开发而言，此集成提供了一种便捷的入门方式。\n\n### MCP 服务器如何增强 Gemini CLI 体验\n\nMCP 服务器能够显著改善 Gemini CLI 的使用体验，通过允许代理执行操作并访问其原本无法获得的上下文。\n\n*   **Pythonic 设计**：FastMCP 采用 Pythonic 设计，利用装饰器和类型提示，使得创建这些组件变得简单直观。\n*   **关键功能**：\n    *   **工具 (Tools)**：允许 Gemini CLI 执行操作、计算，或与外部 API 和数据库交互，使其成为工作流中的积极参与者。\n    *   **提示 (Prompts)**：用户可以将 FastMCP 定义的提示作为斜杠命令（例如 `/promptname`）在 Gemini CLI 中使用。这简化了交互，并使常用提示在终端环境中感觉原生。\n\n### 快速入门指南\n\n想要体验此集成非常简单，只需几个步骤即可开始使用 Gemini CLI 和 FastMCP：\n\n1.  **安装 Gemini CLI**：运行命令 `npm install -g @google/gemini-cli@latest`。\n2.  **安装 FastMCP**：确保安装 v2.12.3 或更高版本：`pip install fastmcp>=2.12.3`。\n3.  **创建服务器文件**：编写包含自定义工具和提示的 `server.py` 文件。\n4.  **集成**：执行命令 `fastmcp install gemini-cli server.py`。\n5.  **验证**：启动 Gemini CLI 并使用 `/mcp` 命令进行验证。\n\n### 更多信息与社区支持\n\n*   有关更详细的信息和高级配置，请参阅官方的 FastMCP 集成文档。\n*   Gemini CLI 和 FastMCP 均为开源项目。用户可以通过直接在 GitHub 上为 FastMCP 或 Gemini CLI 提交新的 issue，来提出功能请求或改进建议。",
      "shortSummary": "Gemini CLI 现已与 FastMCP（Python 的 MCP 服务器构建库）无缝集成，旨在简化模型上下文协议 (MCP) 服务器的开发。用户可通过 `fastmcp install gemini-cli` 命令轻松连接自定义 MCP 工具和提示到 Gemini CLI。此集成使 Gemini CLI 代理能执行更多操作并访问更多上下文，通过提供工具和可作为斜杠命令使用的提示来增强功能。用户可快速安装并开始利用此集成，并通过 GitHub 参与开源项目。",
      "translated_title": "Gemini CLI 🤝 FastMCP：简化 MCP 服务器开发",
      "images": [],
      "contentSource": "完整文章",
      "content": "Gemini CLI now seamlessly integrates with FastMCP, Python's leading library for building MCP servers. We’re thrilled to announce this integration between two open-source projects that empowers you to effortlessly connect your custom MCP tools and prompts, directly to Gemini CLI!"
    },
    {
      "title": "ADK for Java 通过 LangChain4j 集成向第三方语言模型开放 (原标题: ADK for Java opening up to third-party language models via LangChain4j integration)",
      "link": "https://developers.googleblog.com/en/adk-for-java-opening-up-to-third-party-language-models-via-langchain4j-integration/",
      "pubDate": "Mon, 15 Sep 2025 16:00:00 GMT",
      "isoDate": "2025-09-15T16:00:00.000Z",
      "creator": "Google",
      "summary": "## ADK for Java 通过 LangChain4j 集成向第三方语言模型开放\n\nGoogle 的 Java 代理开发工具包（ADK）在 2025 年 9 月 11 日发布的 0.2.0 版本中，新增了与 LangChain4j LLM 框架的集成。这一重要更新旨在为开发者提供更广泛的大型语言模型（LLM）选择，以构建强大的 AI 代理。\n\n### LangChain4j 集成的核心优势\n\n*   **模型多样性**：除了 ADK 内置的 Google Gemini 和 Anthropic Claude 集成外，开发者现在可以通过 LangChain4j 访问来自 OpenAI、Anthropic、GitHub、Mistral 等第三方提供商的多种模型。\n*   **本地模型支持**：还支持通过 Ollama 或 Docker Model Runner 等工具运行的本地开源模型。\n*   **易于集成**：LangChain4j LLM 框架支持的模型列表可在其官方文档中查询。\n\n### 如何通过 LangChain4j 集成 LLM\n\n当使用 `LlmAgent` 构建器声明 ADK 代理时，可以通过 `model()` 构建器方法指定 LLM。通常，这会传入一个表示模型名称的字符串（例如 \"gemini-2.5-flash\"）。\n\nLangChain4j 集成通过使用一个扩展 `BaseLlm` 抽象类的新 `LangChain4j` 类，在两个框架之间建立了桥梁。这意味着开发者可以实例化一个 LangChain4j 聊天模型，然后将其包装在 `new LangChain4j()` 实例中，传递给 ADK 代理。\n\n#### 示例一：使用 Docker Model Runner 运行 Gemma 3\n\n1.  **安装与拉取模型**：在机器上安装并启用 Docker Model Runner 后，可通过命令 `docker model pull ai/gemma3` 轻松拉取 Gemma 3 模型。\n2.  **Maven 依赖**：在 `pom.xml` 中添加 `google-adk-contrib-langchain4j` 和 `langchain4j-open-ai` 依赖。\n3.  **配置模型**：由于 Docker Model Runner 模型暴露了 OpenAI 兼容的 API 接口，可以使用 `OpenAiChatModel` 进行配置，指定 `baseUrl`（例如 `http://localhost:12434/engines/llama.cpp/v1`）和 `modelName`（例如 `ai/gemma3n`）。\n4.  **配置代理**：将配置好的 `OpenAiChatModel` 实例通过 `new LangChain4j(dmrChatModel)` 传递给 `LlmAgent`，例如构建一个国际象棋教练代理。\n\n#### 示例二：使用 Ollama 运行 Qwen 3\n\n1.  **Maven 依赖**：在 `pom.xml` 中添加 `google-adk`、`google-adk-contrib-langchain4j` 和 `langchain4j-ollama` 依赖。\n2.  **安装与运行模型**：假设已在机器上安装 Ollama 并拉取了 Qwen 3 模型（例如 `qwen3:1.7b`），并在端口 11434 运行。\n3.  **配置模型**：使用 `OllamaChatModel` 进行配置，指定 `modelName` 和 `baseUrl`（例如 `http://127.0.0.1:11434`）。\n4.  **配置代理**：将配置好的 `OllamaChatModel` 实例通过 `new LangChain4j(ollamaChatModel)` 传递给 `LlmAgent`，例如构建一个友好的科学老师代理。\n\n### 工具调用能力\n\n如果模型支持函数调用，开发者还可以为代理提供工具访问权限，例如访问 MCP 服务器或本地代码驱动的函数。ADK 文档和相关文章提供了更多关于可用工具的信息。\n\n### 0.2.0 版本的其他重要增强功能\n\n除了 LangChain4j 集成，0.2.0 版本还带来了多项强大的代理开发工作流改进：\n\n*   **扩展的工具能力**：\n    *   **基于实例的 `FunctionTools`**：现在可以从对象实例创建 `FunctionTools`，而非仅限于静态方法，提高了代理架构的灵活性。\n    *   **改进的异步支持**：`FunctionTools` 现在支持返回 `Single` 的方法，增强了异步操作支持和代理响应能力。\n    *   **更好的循环控制**：`Event Actions` 中新增的 `endInvocation` 字段允许在工具调用后以编程方式中断或停止代理循环，提供更精细的执行控制。\n*   **高级代理逻辑和内存**：\n    *   **链式回调**：增加了对模型、代理和工具执行前后事件的链式回调支持，使得代理生命周期内的逻辑更加复杂和精细。\n    *   **新的内存和检索**：引入了 `InMemoryMemoryService` 用于简单快速的内存管理，并实现了使用 AI Platform API 的 `VertexAiRagRetrieval` 以支持更高级的 RAG 模式。\n*   **其他关键增强**：包括父 POM 和 Maven Wrapper (`./mvnw`)，确保所有贡献者都能获得一致且直接的构建过程。\n\n### 总结与展望\n\nLangChain4j 的集成是 ADK for Java 迈向更开放、更灵活框架的重要一步，旨在帮助开发者构建更强大的 AI 代理。鼓励开发者查阅 GitHub 发布说明、ADK for Java 文档、入门指南或 GitHub 模板项目以获取更多信息。文章作者期待听到社区关于 ADK for Java 和 LangChain4j 集成的使用案例。",
      "shortSummary": "Google ADK for Java 0.2.0 版本通过集成 LangChain4j LLM 框架，显著扩展了其对第三方语言模型的支持。开发者现在除了内置的 Gemini 和 Claude，还能利用 LangChain4j 访问 OpenAI、Mistral 等多种模型，包括通过 Docker Model Runner 或 Ollama 运行的本地模型。此次更新还带来了工具能力、异步支持、代理逻辑和内存管理等多方面的增强，旨在使 ADK for Java 成为一个更开放、灵活的 AI 代理构建框架。",
      "translated_title": "ADK for Java 通过 LangChain4j 集成向第三方语言模型开放",
      "images": [],
      "contentSource": "完整文章",
      "content": "The recent 0.2.0 release of Google’s Agent Development Kit (ADK) for Java adds an integration with t..."
    }
  ],
  "lastUpdated": "2025-09-28T10:25:44.801Z"
}