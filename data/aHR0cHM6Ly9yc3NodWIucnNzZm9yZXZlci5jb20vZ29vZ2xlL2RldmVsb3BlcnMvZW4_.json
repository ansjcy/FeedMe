{
  "sourceUrl": "https://rsshub.rssforever.com/google/developers/en",
  "title": "Google Developers Blog",
  "description": "Google Developers Blog - Powered by RSSHub",
  "link": "https://developers.googleblog.com",
  "items": [
    {
      "title": "宣布ADK评估中的用户模拟功能 (原标题: Announcing User Simulation in ADK Evaluation)",
      "link": "https://developers.googleblog.com/en/announcing-user-simulation-in-adk-evaluation/",
      "pubDate": "Thu, 06 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-06T16:00:00.000Z",
      "creator": "Google",
      "summary": "### ADK评估中用户模拟功能的发布\n\n2025年11月7日，Agent的本质是对话式的。用户可能需要提出后续问题、完善之前的请求并根据需要提供额外信息。然而，为Agent手动编写多轮对话测试脚本是一个脆弱且耗时的过程，容易因Agent行为的微小变化而失效，使测试维护变得令人沮丧。\n\n为了解决这个问题，Agent开发工具包（ADK）中推出了一项新功能：**用户模拟（User Simulation）**。这项功能旨在帮助开发者摆脱测试僵化的实现路径，转而评估Agent实际达成用户意图的能力。\n\n#### 什么是用户模拟器？\n\n用户模拟器本质上是一个由大型语言模型（LLM）驱动的用户提示生成器。它直接集成到ADK评估框架中，允许在本地运行。开发者只需提供一个高层次的目标，模拟器就会动态生成对话的用户端，以追求该目标。它不是一个独立的服务，而是ADK内部的一个本地工具，支持快速、迭代的“内循环”工作流程。\n\n#### 工作原理\n\n1.  **定义对话场景（Conversation Scenario）**\n    *   不再需要僵化的逐轮脚本，而是提供一个`ConversationScenario`。\n    *   这是一个简单的JSON对象，包含两个关键部分：\n        *   `starting_prompt`：一个固定的初始提示，用于开始对话。\n        *   `conversation_plan`：一个自然语言指南，告诉模拟器其目标。\n    *   **示例：**\n        ```json\n        {\n          \"scenarios\": [\n            {\n              \"starting_prompt\": \"What can you do for me?",
      "shortSummary": "ADK（Agent开发工具包）推出“用户模拟”新功能，旨在解决手动编写多轮对话测试脚本的耗时和脆弱性问题。该功能是一个由LLM驱动的用户提示生成器，允许开发者通过定义高层次的“对话场景”和“评估配置”来动态模拟用户交互，从而评估Agent达成用户意图的能力。它能显著减少测试创建时间，构建更具弹性的测试，并创建可靠的回归测试套件，是构建可靠AI Agent的关键一步。",
      "translated_title": "宣布ADK评估中的用户模拟功能",
      "images": [],
      "contentSource": "完整文章",
      "content": "The new **User Simulation** feature in the Agent Development Kit (ADK) replaces rigid, brittle manual test scripts with dynamic, LLM-powered conversation generation. Developers define a high-level `conversation_plan`, and the simulator handles the multi-turn interaction to achieve the goal. This dramatically reduces test creation time, builds more resilient tests, and creates a reliable regression suite for AI agents."
    },
    {
      "title": "宣布推出Go语言版Agent开发工具包：用你喜爱的语言构建强大的AI智能体 (原标题: Announcing the Agent Development Kit for Go: Build Powerful AI Agents with Your Favorite Languages)",
      "link": "https://developers.googleblog.com/en/announcing-the-agent-development-kit-for-go-build-powerful-ai-agents-with-your-favorite-languages/",
      "pubDate": "Thu, 06 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-06T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 宣布推出Go语言版Agent开发工具包（ADK）\n\nGoogle于2025年11月7日宣布，将Go语言加入Agent开发工具包（ADK）支持语言家族，旨在让开发者能够使用Go语言构建强大且复杂的AI智能体。ADK是一个开源、代码优先的工具包，专为需要对AI智能体进行精细控制的开发者设计，它将大型语言模型（LLM）编排、智能体行为和工具使用的复杂性直接融入代码中。\n\n## ADK的核心优势\nADK通过将复杂逻辑直接编码，为开发者带来以下关键优势：\n*   **强大的调试能力：** 能够像对待所有服务一样严谨地定义和调试智能体逻辑。\n*   **可靠的版本控制：** 轻松跟踪更改并自信地部署应用。\n*   **部署自由：** 应用程序可以部署到任何地方，无论是本地笔记本电脑还是云端。\n\n## Go语言版ADK的特点\n对于Go语言开发者而言，ADK for Go提供了一种地道且高性能的方式来构建智能体，充分利用Go语言的并发性和强类型特性，创建健壮且可扩展的智能体应用。此外，ADK Go通过MCP Toolbox for Databases开箱即用地支持30多种数据库，使数据集成变得无缝且简单。\n\n## ADK的通用关键特性\nGo语言版ADK与Python和Java版ADK共享相同的核心设计原则和特性，确保一致的开发体验：\n*   **丰富的工具生态系统：** 通过预构建工具、自定义函数、OpenAPI规范以及与Google生态系统的紧密集成来增强智能体功能。\n*   **代码优先开发：** 直接在首选语言中定义智能体逻辑、工具和编排，实现极致的灵活性、可测试性和版本控制。\n*   **模块化多智能体系统：** 通过将多个专业智能体组合成灵活的层次结构，设计可扩展的应用程序。\n*   **开发UI：** 内置的开发用户界面可加速工作流程，允许开发者测试、评估、调试和展示其智能体。\n\n## Go语言版ADK对Agent2Agent (A2A) 协议的支持\nADK Go现在包含对Agent2Agent (A2A) 协议的支持，这使得开发者能够构建强大的多智能体系统，其中智能体可以协作解决复杂问题。通过A2A协议，主智能体可以无缝地编排和委托任务给专业的子智能体（无论是本地服务还是远程部署），确保安全且不透明的交互，而无需暴露内部内存或专有逻辑。Google已将A2A Go SDK贡献给A2A项目仓库。\n\n## 立即开始\n准备好利用Go语言的速度和ADK的控制力了吗？您的下一个颠覆性智能体只需一个命令即可实现。\n\n*   **ADK for Go 获取命令：** `go get google.golang.org/adk`\n*   **源代码：** [https://github.com/google/adk-go](https://github.com/google/adk-go)\n*   **示例：** [https://github.com/google/adk-samples](https://github.com/google/adk-samples)\n*   **文档：** [https://google.github.io/adk-docs/](https://google.github.io/adk-docs/)\n\n加入社区，提问、分享项目并与其他开发者联系：\n*   **Reddit：** `r/agentdevelopmentkit`\n*   **报告Bug：** [https://github.com/google/adk-go/issues](https://github.com/google/adk-go/issues)",
      "shortSummary": "Google宣布推出Go语言版Agent开发工具包（ADK），这是一个开源、代码优先的工具包，旨在帮助Go开发者构建强大且可扩展的AI智能体。ADK将LLM编排、智能体行为和工具使用直接融入代码，提供强大的调试、版本控制和部署自由。它支持丰富的工具生态系统、模块化多智能体系统，并集成了Agent2Agent (A2A) 协议，允许智能体协作。开发者可利用Go的并发性和强类型特性，高效构建AI应用。",
      "translated_title": "宣布推出Go语言版Agent开发工具包：用你喜爱的语言构建强大的AI智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "The Agent Development Kit (ADK), an open-source, code-first toolkit for building powerful and sophisticated AI agents, now supports Go. ADK moves LLM orchestration and agent behavior directly into your code, giving you robust debugging, versioning, and deployment freedom. ADK for Go is idiomatic and performant, leveraging Go's strengths, and includes support for over 30+ databases and the Agent-to-Agent (A2A) protocol for collaborative multi-agent systems. Start building today!"
    },
    {
      "title": "Agent Garden - 学习、发现和构建的示例 (原标题: Agent Garden - Samples for learning, discovering and building)",
      "link": "https://developers.googleblog.com/en/agent-garden-samples-for-learning-discovering-and-building/",
      "pubDate": "Mon, 03 Nov 2025 16:00:00 GMT",
      "isoDate": "2025-11-03T16:00:00.000Z",
      "creator": "Google",
      "summary": "## Agent Garden 简介\n\nAgent Garden 现已向所有用户开放，不再仅限于 Google Cloud 用户。它旨在解决创建复杂 AI 代理（特别是多代理系统）所面临的主要挑战，这些挑战通常涉及大量研究、精确协调和巨大的开发投入以整合各种工具和框架。\n\n**Agent Garden 帮助开发者解决以下挑战：**\n*   开发能够应对复杂业务挑战的 AI 代理。\n*   创建将生成式 AI 与确定性逻辑相结合的复杂多代理工作流。\n*   将代理无缝部署并集成到现有系统和数据源中。\n\n## 核心功能与优势\n\nAgent Garden 提供了一个不断增长的精选代理示例、解决方案和工具库，旨在利用 Agent Development Kit (ADK) 加速强大 AI 代理的开发和部署。它提供独特的学习体验和资源，以启动代理开发之旅。\n\n**Agent Garden 的能力包括：**\n*   **丰富的示例集合**：提供针对各种用例量身定制的强大功能集合。这些代码示例使用 ADK 开发，并与 BigQuery 和 Vertex AI Search 等云服务进行了广泛集成。\n*   **详细的示例信息**：每个代理示例都包含详细的概述、适用的用例、架构洞察及其功能描述。开发者还可以直接在 GitHub 上访问示例代码进行深入审查。\n\n## 简化部署与定制\n\nAgent Garden 简化了代理的部署和定制过程，使开发者能够更高效地工作。\n\n*   **一键部署**：通过 Agent Starter Pack（一个帮助无缝部署到生产环境的开源启动包），开发者现在只需点击一下即可将示例代理部署到其项目中的 Agent Engine，并通过 Agent Engine Playground UI 进行测试和实验。\n*   **代理定制**：Agent Garden 通过 Firebase Studio 提供选项，允许开发者打开和定制代理，以满足其特定的需求和用例。\n\n这种探索、学习、部署和定制的开发者旅程旨在使 ADK 代理开发对您的用例而言无缝衔接。\n\n## 成功案例\n\n早期测试者已经成功利用 Agent Garden 提供的创新示例，展示了成功的采用和切实的商业价值。雷诺集团（Renault Group）在 Google Cloud Next’2025 上分享了他们的成功经验，详细介绍了如何将一个复杂的数据科学家代理集成到其电动汽车 (EV) 充电器平台中。这一集成显著增强了他们的运营和用户体验，赋予业务团队直接利用其数据的自主权。\n\n## 常见问题与资源\n\n**常见问题解答：**\n*   **代理是否可以直接部署到生产环境？** adk-samples 和 Agent Starter Pack 都是开源软件，受适用源代码中规定的条款约束。\n*   **部署的代理是否可以访问和共享？** 代理部署在 Agent Engine 中，并受 Agent Engine 访问管理选项的约束。\n*   **如何删除部署的代理？** 代理部署在 Agent Engine 中，可以在 Agent Engine 内部删除。\n*   **在哪里可以找到更多示例？** Agent Garden 将提供高影响力用例的精选示例，更多示例可在 ADK samples、ADK community repo 和 ADK docs 中找到。\n*   **如何与社区保持互动？** 请积极参与 ADK 社区。\n\n**相关资源：**\n*   Agent Garden\n*   Agent Starter Pack\n*   Agent Development Kit (ADK Docs, ADK samples, ADK Python community)",
      "shortSummary": "Agent Garden 现已向所有用户开放，旨在简化 AI 代理（特别是多代理系统）的开发和部署。它提供精选的代理示例、解决方案和工具，结合 Agent Development Kit (ADK)，帮助开发者应对复杂业务挑战，实现生成式 AI 与确定性逻辑的融合。通过一键部署和 Firebase Studio 定制，开发者可以轻松学习、部署和个性化代理，加速 AI 解决方案的落地，如雷诺集团的成功案例所示。",
      "translated_title": "Agent Garden - 学习、发现和构建的示例",
      "images": [],
      "contentSource": "完整文章",
      "content": "Agent Garden is now available to all users to simplify AI agent creation and deployment using the Agent Development Kit (ADK). It provides curated agent samples, one-click deployment via Agent Starter Pack, and customization through Firebase Studio. It helps developers with complex business challenges and multi-agent workflows, with Renault Group cited as an early success story."
    },
    {
      "title": "超越请求-响应：构建实时双向流式多智能体系统 (原标题: Beyond Request-Response: Architecting Real-time Bidirectional Streaming Multi-agent System)",
      "link": "https://developers.googleblog.com/en/beyond-request-response-architecting-real-time-bidirectional-streaming-multi-agent-system/",
      "pubDate": "Wed, 29 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-29T16:00:00.000Z",
      "creator": "Google",
      "summary": "本文探讨了传统请求-响应模型在构建复杂AI智能体时的局限性，并提出了实时双向流式架构作为多智能体系统的下一步发展方向。文章详细分析了这种“无回合”模型引入的主要工程挑战，并介绍了Agent开发工具包（ADK）如何通过流原生优先的方法来解决这些问题。\n\n### 请求-响应模型智能体的架构局限性\n\n传统的请求-响应通信模式在AI智能体开发中存在以下关键局限性，阻碍了真正交互式和智能体验的实现：\n\n*   **感知延迟**：智能体必须等待用户完整输入后才能开始处理，导致不自然的、基于回合的延迟，破坏了对话的流畅性。\n*   **脱节的工具集成**：在请求-响应模型中，调用工具通常会中断交互流程，导致用户体验碎片化。\n*   **笨拙的多模态处理**：处理音频和视频等同步数据流需要复杂且脆弱的逻辑来拼接独立输入，难以实现统一体验。\n\n### 实时双向流式智能体范式的愿景\n\n通过从基于回合的事务转向持久的、双向的流，可以解锁新一类智能体能力，使其更像协作伙伴而非简单工具：\n\n*   **真正的并发性和可中断性**：智能体可以在用户仍在输入时处理信息并采取行动，实现非阻塞交互和自然中断（“插话”）功能。\n*   **通过流式工具提供主动协助**：工具不再局限于单一请求-响应周期，可以作为持久的后台进程，随时间向用户或智能体流式传输信息。\n*   **统一的多模态处理**：流式架构通过原生处理连续、并行的流作为单一、统一的上下文来解决多模态问题，实现实时的环境和情境感知。\n\n### 实时双向流式多智能体系统的工程挑战\n\n构建一个健壮的实时双向多智能体应用并非易事，开发者必须解决一系列新的复杂工程问题：\n\n*   **无回合世界中的上下文管理**：在连续流中，需要设计新的机制来将流分割成逻辑事件，用于调试、分析和恢复对话。在没有明确“回合结束”信号的情况下，如何存储和传输连续的上下文流是一个挑战。\n*   **并发性和性能问题**：流式智能体是高度并发的系统，必须以低延迟处理多个异步I/O流。在多智能体系统中，这种固有的并发性变得指数级复杂。\n*   **开发者体验和可扩展性**：流式系统的底层复杂性必须通过简单、强大的抽象来隐藏。框架需要提供直观的开发者体验，例如定义可随时间产生多个结果的工具，并提供钩子和回调以注入自定义逻辑。\n\n### 双向流式范式：ADK的架构深度解析\n\nADK（Agent Development Kit）通过其开源架构，旨在实现“实时”智能体范式，该架构基于以下核心设计：\n\n![ADK架构图](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/unnamed.original_CLUn4eI.png)\n\n1.  **异步实时I/O管理**：\n    *   ADK引入了`LiveRequestQueue`，一个基于`asyncio`的队列，允许客户端应用程序无缝地将各种数据类型（文本、音频/视频blob）排队。智能体的异步运行器（`run_live`）从该队列中消费，实现近实时的数据处理，并以事件形式流式响应。\n\n2.  **多智能体的有状态、可转移会话**：\n    *   ADK会话在实时交互中持续存在，不仅包含历史记录，还包括工具调用、工具响应和各种系统信号。\n    *   通过信号（如中断、明确的“完成”信号或智能体转移）来划分连续流为离散事件。\n    *   将大型媒体blob存储在对象存储中，并在事务性数据库中引用它们。\n    *   从音频/视频流生成文本转录，作为单独的、带时间戳的事件捕获。\n    *   这种有状态的会话在智能体交接时（例如，从分流智能体到专家智能体）被完整传输，确保无缝衔接。\n\n3.  **事件驱动回调实现实时定制**：\n    *   ADK实现了回调机制，如`before_tool_callback`和`after_tool_callback`，允许开发者在工具执行前后注入自定义逻辑，实现动态控制（如日志记录、内容审核或信息注入）。\n\n4.  **流原生工具**：\n    *   ADK支持“流式工具”，这些工具被定义为异步生成器（`AsyncGenerator`）。它们可以接受标准输入并随时间产生多个结果，可选地接受`LiveRequestQueue`直接处理用户输入流，并在长时间运行任务在后台执行时向用户/模型提供中间更新。\n\n### 前进之路：挑战与未来研究\n\n该架构是深入探索和研究的起点。未来的工作将专注于提高性能（启动和智能体转移时间），并提供更丰富的回调类型（如`before-model-callback`和`after-model-callback`），以实现对智能体生命周期更深层次的控制。",
      "shortSummary": "传统请求-响应模型因延迟、工具集成脱节和多模态处理笨拙而限制了AI智能体。本文提出实时双向流式架构，以实现真正的并发性、可中断性、主动协助和统一多模态处理。该模型面临上下文管理、并发性能和开发者体验等工程挑战。Agent开发工具包（ADK）通过异步I/O管理、有状态可转移会话、事件驱动回调和流原生工具来解决这些问题，为构建更具交互性的多智能体系统铺平道路。",
      "translated_title": "超越请求-响应：构建实时双向流式多智能体系统",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/unnamed.original_CLUn4eI.png",
          "alt": "unnamed",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "The blog post argues the request-response model fails for advanced multi-agent AI. It advocates for a real-time bidirectional streaming architecture, implemented by the Agent Development Kit (ADK). This streaming model enables true concurrency, natural interruptibility, and unified multimodal processing. ADK's core features are real-time I/O management, stateful sessions for agent handoffs, and streaming-native tools."
    },
    {
      "title": "介绍 Gemini CLI 的 Jules 扩展 (原标题: Introducing the Jules extension for Gemini CLI)",
      "link": "https://developers.googleblog.com/en/introducing-the-jules-extension-for-gemini-cli/",
      "pubDate": "Tue, 28 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-28T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 介绍 Gemini CLI 的 Jules 扩展\n\n本文宣布推出 Jules 扩展，为 Gemini CLI 带来了一个自主的助手，旨在异步处理编码任务，从而为开发者提供强大的新工作流程。\n\n## 核心工作流程\n\nJules 扩展与 Gemini CLI 协同工作，加速创意编码流程，允许开发者将任务委派给 Jules，同时保持在 Gemini CLI 中的工作流。\n\n*   **Gemini CLI**：作为终端中的协作器和协调器，用于用户主动关注的任务。\n*   **Jules 扩展**：作为自主助手，在后台的虚拟机 (VM) 中工作，负责克隆代码、安装依赖项和修改文件。\n\n## Jules 扩展可处理的任务类型\n\nJules 扩展能够处理多种异步任务，从而提高开发效率：\n\n*   **异步任务**：可以直接从 Gemini CLI 指派 Jules 扩展独立完成任务。\n*   **后台错误修复**：在用户处理 Gemini CLI 中的其他同步任务时，可以指派 Jules 在后台修复多个错误。\n*   **新分支中的更改**：在解决 GitHub 上的问题后，Jules 扩展可以将更改提交到一个新分支。\n\n## 开始使用\n\n### 先决条件\n\n在使用 Jules 扩展之前，需要满足以下条件：\n\n1.  **安装 Gemini CLI**：确保已安装 Gemini CLI，且版本为 v0.4.0 或更新。\n2.  **拥有 Jules 账户**：在 jules.google.com 注册一个 Jules 账户。\n3.  **连接仓库**：在 Jules 控制台中将您的 GitHub 仓库连接到您的 Jules 账户。\n\n### 安装\n\n通过在终端中运行以下命令来安装 Jules 扩展：\n\n```shell\ngemini extensions install https://github.com/gemini-cli-extensions/jules --auto-update\n```\n\n*   `--auto-update` 参数是可选的，如果指定，扩展将在新版本发布时自动更新。\n\n### 使用\n\n*   **启动 Jules 任务**：使用 `/jules` 命令，后跟您的提示。例如：\n\n    ```shell\n    /jules Convert commonJS modules to ES modules\n    ```\n    一旦任务启动，扩展将在后台完成它。\n\n*   **检查任务状态**：使用 `/jules` 命令并查询任务状态。例如：\n\n    ```shell\n    /jules what is the status of my last task?\n    ```\n\n## 加速您的编码工作流\n\n鼓励开发者尝试 Gemini CLI 中的 Jules 扩展，并在 Jules 扩展仓库中分享其如何帮助加速编码工作流的反馈。如果支持该项目，请考虑给仓库点赞。\n\n## 更多信息\n\n*   Jules extension for Gemini CLI\n*   所有 Gemini CLI 扩展\n*   Jules 文档\n*   Gemini CLI 文档",
      "shortSummary": "本文宣布推出 Gemini CLI 的 Jules 扩展，这是一个自主助手，旨在异步处理编码任务。Jules 在后台虚拟机中工作，负责克隆代码、安装依赖项和修改文件，从而让开发者能够将任务（如错误修复和分支更改）委派给它，同时专注于 Gemini CLI 中的主要工作。用户需安装 Gemini CLI，注册 Jules 账户并连接 GitHub 仓库，然后通过 `/jules` 命令启动和管理任务。",
      "translated_title": "介绍 Gemini CLI 的 Jules 扩展",
      "images": [],
      "contentSource": "完整文章",
      "content": "Introducing the Jules extension for Gemini CLI, an autonomous sidekick for developers. It accelerates coding workflows by offloading tasks like asynchronous work, bug fixes, and changes in new branches to Jules, while you stay in flow with Gemini CLI. Get started by installing the extension and using the /jules command to initiate and check task statuses."
    },
    {
      "title": "推出Coral NPU：面向边缘AI的全栈平台 (原标题: Introducing Coral NPU: A full-stack platform for Edge AI)",
      "link": "https://developers.googleblog.com/en/introducing-coral-npu-a-full-stack-platform-for-edge-ai/",
      "pubDate": "Tue, 14 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-14T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 推出Coral NPU：面向边缘AI的全栈平台\n\nGoogle于2025年10月14日推出了Coral NPU，这是一个全栈、开源平台，旨在解决限制低功耗边缘设备和可穿戴设备上强大、始终在线AI的核心性能、碎片化和隐私挑战。\n\n## 边缘AI面临的挑战\n\n生成式AI极大地改变了我们对技术的期望，但下一个技术飞跃是将智能直接嵌入到我们的个人环境中。为了实现真正有帮助的、主动的AI（如实时翻译、理解物理环境），它必须在可穿戴和随身设备上运行。这带来了三个核心挑战：\n\n*   **性能差距：** 复杂的机器学习模型需要大量计算，远超边缘设备的有限功耗、散热和内存预算。\n*   **碎片化成本：** 为各种专有处理器编译和优化ML模型既困难又昂贵，阻碍了设备间的一致性能。\n*   **用户信任赤字：** 个人AI必须优先考虑个人数据的隐私和安全。\n\n## Coral NPU解决方案\n\nCoral NPU平台基于Google原有的Coral项目，与Google Research和Google DeepMind合作设计，为硬件设计师和ML开发者提供了构建下一代私密、高效边缘AI设备所需的工具。它是一个AI优先的硬件架构，旨在实现超低功耗、始终在线的边缘AI。\n\n*   **统一的开发体验：** 简化了环境感知等应用的部署。\n*   **低功耗设计：** 专为可穿戴设备的全天候AI而设计，最大限度地减少电池使用，并可配置以支持更高性能用例。\n*   **开放可用：** 相关的文档和工具已发布，供开发者和设计师立即开始构建。\n\n## AI优先的架构\n\n传统的芯片设计在通用CPU（灵活但ML效率低）和专用加速器（ML效率高但缺乏灵活性）之间存在权衡。Coral NPU通过颠覆传统芯片设计来解决这一问题，它优先考虑ML矩阵引擎而非标量计算，从芯片层面优化AI架构，从而创建了一个专为更高效的设备端推理而设计的平台。\n\n![Coral NPU全平台](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/The_full_platform_v4_Fin_Alpha.original.png)\n\n作为一个完整的参考神经网络处理单元（NPU）架构，Coral NPU为下一代节能、ML优化片上系统（SoC）提供了构建模块。\n\n*   **RISC-V基础：** 架构基于一套符合RISC-V ISA的架构IP块，设计用于最小功耗，非常适合始终在线的环境感知。\n*   **性能表现：** 基础设计提供512 GOPS的性能，同时仅消耗几毫瓦，为边缘设备、听戴设备、AR眼镜和智能手表提供强大的设备端AI。\n*   **开放可扩展：** 基于RISC-V的开放可扩展架构为SoC设计师提供了修改基础设计或将其用作预配置NPU的灵活性。\n\nCoral NPU架构包含以下组件：\n\n*   **标量核心：** 轻量级、C可编程的RISC-V前端，管理数据流到后端核心，采用简单的“运行到完成”模型，实现超低功耗和传统CPU功能。\n*   **向量执行单元：** 强大的单指令多数据（SIMD）协处理器，符合RISC-V向量指令集（RVV）v1.0，支持对大型数据集的并行操作。\n*   **矩阵执行单元：** 高效的量化外积乘累加（MAC）引擎，专为加速基本神经网络操作而构建（该矩阵核心仍在开发中，将于今年晚些时候在GitHub上发布）。\n\n![Coral NPU设计灵感](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Inspiration_of_Coral_v4_Fin_Alpha.original.png)\n\n## 统一的开发者体验\n\nCoral NPU架构是一个简单、C可编程的目标，可以与IREE和TFLM等现代编译器无缝集成，从而轻松支持TensorFlow、JAX和PyTorch等ML框架。\n\n*   **全面的软件工具链：** 包括TFLM编译器、通用MLIR编译器、C编译器、自定义内核和模拟器，为开发者提供灵活的路径。\n*   **优化编译流程：** 例如，JAX框架的模型首先使用StableHLO方言导入MLIR格式，然后输入IREE编译器，该编译器应用硬件特定插件识别Coral NPU架构，并进行渐进式降低优化，最终生成紧凑的二进制文件，可在边缘设备上高效执行。\n*   **行业标准工具：** 简化ML模型的编程，并在各种硬件目标上提供一致的体验。\n\n![Coral NPU编译器工具链](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/The_compiler_toolchain_v4_Fin_Alpha.original.png)\n\nCoral NPU的协同设计过程侧重于两个关键领域：\n\n1.  高效加速当前设备端视觉和音频应用中领先的基于编码器的架构。\n2.  与Gemma团队密切合作，优化Coral NPU以支持小型Transformer模型，确保加速器架构支持边缘生成式AI的下一代发展。\n\n这种双重关注意味着Coral NPU有望成为第一个开放、基于标准、低功耗的NPU，能够将大型语言模型（LLMs）引入可穿戴设备。\n\n## 目标应用\n\nCoral NPU旨在实现超低功耗、始终在线的边缘AI应用，特别关注环境感知系统。其主要目标是在可穿戴设备、手机和物联网（IoT）设备上实现全天候AI体验，同时最大限度地减少电池使用。\n\n潜在用例包括：\n\n*   **情境感知：** 检测用户活动（如步行、跑步）、接近度或环境（如室内/室外、移动中），以启用“请勿打扰”模式或其他情境感知功能。\n*   **音频处理：** 语音和语音检测、关键词识别、实时翻译、转录和基于音频的辅助功能。\n*   **图像处理：** 人物和物体检测、面部识别、手势识别和低功耗视觉搜索。\n*   **用户交互：** 通过手势、音频提示或其他传感器驱动输入实现控制。\n\n## 硬件强制隐私\n\nCoral NPU的核心原则是通过硬件强制安全来建立用户信任。其架构旨在支持CHERI等新兴技术，提供细粒度的内存级安全和可扩展的软件分区。通过这种方法，敏感的AI模型和个人数据可以被隔离在硬件强制的沙盒中，从而减轻基于内存的攻击。\n\n## 构建生态系统\n\n开放硬件项目的成功离不开强大的合作伙伴关系。Google正与Synaptics合作，Synaptics是其首个战略硅合作伙伴，也是物联网嵌入式计算、无线连接和多模态传感领域的领导者。Synaptics已宣布推出其新的Astra™ SL2610系列AI原生物联网处理器，该产品线采用了其Torq™ NPU子系统，这是Coral NPU架构的业界首个生产实现。Torq™ NPU设计支持Transformer模型和动态操作符，使开发者能够为消费和工业物联网构建面向未来的边缘AI系统。\n\n此次合作支持了Google对统一开发者体验的承诺。Synaptics Torq™边缘AI平台基于IREE和MLIR的开源编译器和运行时构建。这一合作是为智能、情境感知设备构建共享开放标准的重要一步。\n\n## 总结\n\n通过Coral NPU，Google正在为个人AI的未来构建一个基础层。目标是提供一个通用、开源且安全的平台，供行业在此基础上进行构建，从而促进一个充满活力的生态系统。这将使开发者和芯片供应商能够超越当前碎片化的局面，在边缘计算的共享标准上进行协作，从而加速创新。",
      "shortSummary": "Google推出Coral NPU，一个全栈开源平台，解决低功耗边缘AI的性能、碎片化和隐私挑战。它采用AI优先的RISC-V架构，提供512 GOPS性能，功耗仅数毫瓦。平台提供统一开发体验，支持主流ML框架，赋能可穿戴设备、手机及IoT的全天候私密AI应用，如情境感知和实时翻译。Coral NPU强调硬件强制隐私，并与Synaptics等伙伴共建开放生态，加速边缘AI创新。",
      "translated_title": "推出Coral NPU：面向边缘AI的全栈平台",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/The_full_platform_v4_Fin_Alpha.original.png",
          "alt": "The full platform_v4_Fin_Alpha",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Inspiration_of_Coral_v4_Fin_Alpha.original.png",
          "alt": "Inspiration of Coral_v4_Fin_Alpha",
          "title": "",
          "position": 2
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/The_compiler_toolchain_v4_Fin_Alpha.original.png",
          "alt": "The compiler toolchain_v4_Fin_Alpha",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "完整文章",
      "content": "Coral NPU is a full-stack platform for Edge AI, addressing performance, fragmentation, and user trust deficits. It's an AI-first architecture, prioritizing ML matrix engines, and offers a unified developer experience. Designed for ultra-low-power, always-on AI in wearables and IoT, it enables contextual awareness, audio/image processing, and user interaction with hardware-enforced privacy. Synaptics is the first partner to implement Coral NPU."
    },
    {
      "title": "Gemini CLI 迎来全新交互体验 (原标题: Say hello to a new level of interactivity in Gemini CLI)",
      "link": "https://developers.googleblog.com/en/say-hello-to-a-new-level-of-interactivity-in-gemini-cli/",
      "pubDate": "Tue, 14 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-14T16:00:00.000Z",
      "creator": "Google",
      "summary": "# Gemini CLI 交互性迈上新台阶\n\nGoogle 于 2025 年 10 月 15 日宣布，Gemini CLI 迎来重大增强，旨在提升用户工作流程的强大性和熟悉度。此次升级允许用户直接在 Gemini CLI 内部运行复杂的交互式命令，例如用于编辑的 `vim`、用于监控的 `top`，甚至是交互式 `git rebase -i`。\n\n## 告别终端切换，保持上下文一致\n\n过去，用户需要退出 Gemini CLI 才能运行交互式 shell 命令，且这些命令在 Gemini CLI 的上下文之外执行，导致工作流程中断。现在，通过引入伪终端（PTY）支持，所有需要丰富功能（如文本编辑器、系统监视器或依赖终端控制代码）的命令都可以在 Gemini CLI 内部及其上下文中运行，从而实现：\n\n*   **保持上下文：** 所有操作都在 Gemini CLI 内部完成，无需切换到单独的终端。\n*   **避免“挂起”：** 解决了代理式 CLI 在处理交互式命令时可能出现的“挂起”问题。\n\n## 工作原理：终端状态序列化与实时交互\n\n当用户在 Gemini CLI 中运行 shell 命令时，Gemini CLI 会在后台的伪终端中生成一个新进程，并利用 `node-pty` 库。PTY 充当中间层，为操作系统提供必要的接口，使其将该会话识别为标准终端。\n\n为了在屏幕上显示这个在后台运行的虚拟终端，Gemini CLI 采用了创新的序列化技术：\n\n1.  **快照捕获：** 新的序列化器会实时捕获伪终端的每一个瞬间快照，包括所有文本、颜色甚至光标位置。\n2.  **实时流传输：** 这些快照随后被流式传输给用户，实现用户与终端应用程序的实时查看和交互，就像观看视频流一样，提供实时的动态反馈。\n\n![Gemini CLI - 交互式 Shell 界面](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemini_CLI_-_interactive_shell_UoV7sHm.original.png)\n\n## 全面双向交互体验\n\n新的架构支持完整的双向通信，极大地提升了用户体验：\n\n*   **输入写入：** 用户键入的按键会发送到正在运行的进程。\n*   **动态调整大小：** 当用户调整窗口大小时，Gemini shell 内部的应用程序会像在原生终端中一样自适应布局。\n*   **焦点控制：** 用户可以通过按下 `Ctrl+F` 键将焦点切换到终端。\n*   **彩色输出：** 改进了输出处理，能够正确渲染丰富多彩的终端输出，让用户可以充分体验喜爱的命令行工具。\n\n## 如何开始使用交互式 Shell\n\n交互式 shell 功能已在 Gemini CLI v0.9.0 及更高版本中默认启用。用户可以通过以下命令升级到最新版本：\n\n```bash\nnpm install -g @google/gemini-cli@latest\n```\n\n更多详细信息请参考官方 Gemini CLI 文档。\n\n## 可运行的交互式命令示例\n\n现在，用户可以在 Gemini CLI 中运行多种类型的交互式命令，包括：\n\n*   使用 `vim`、`nvim` 或 `nano` 编辑代码。\n*   使用交互式 `git` 命令管理提交。\n*   运行常用语言的交互式 REPL（读取-求值-打印循环）。\n*   运行 `htop` 或 `mc` 等全屏终端应用程序。\n*   轻松导航 `npm init` 或 `ng new` 等交互式设置脚本。\n*   响应某些 `gcloud` 命令的交互式提示。\n\n此次更新是 shell 集成方面的重要一步。团队正在积极完善所有平台上的输入处理，并鼓励用户在 GitHub 仓库分享遇到的任何问题和反馈。",
      "shortSummary": "Gemini CLI 现已支持直接运行复杂的交互式命令，如 `vim`、`git rebase -i` 和 `top`。通过引入伪终端（PTY）支持和终端状态序列化技术，用户无需切换终端，所有操作都在 Gemini CLI 上下文内完成。该功能提供实时、双向交互，支持输入、窗口调整和彩色输出。用户只需升级到 v0.9.0+ 版本即可体验，显著提升了命令行工作效率和用户体验。",
      "translated_title": "Gemini CLI 迎来全新交互体验",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemini_CLI_-_interactive_shell_UoV7sHm.original.png",
          "alt": "Gemini CLI - interactive shell",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "We're excited to announce an enhancement to Gemini CLI that makes your workflow more powerful a..."
    },
    {
      "title": "介绍 Veo 3.1 和 Gemini API 中的新创意功能 (原标题: Introducing Veo 3.1 and new creative capabilities in the Gemini API)",
      "link": "https://developers.googleblog.com/en/introducing-veo-3-1-and-new-creative-capabilities-in-the-gemini-api/",
      "pubDate": "Tue, 14 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-14T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 介绍 Veo 3.1 和 Gemini API 中的新创意功能\n\nGoogle 于2025年10月15日发布了 Veo 3.1 和 Veo 3.1 Fast，目前在 Gemini API 中提供付费预览。这些更新的模型旨在通过显著升级，赋能开发者创建更具吸引力的内容，特别是在从图像生成视频方面提供了改进的输出。\n\n## 主要改进和新功能\n\nVeo 3.1 和 Veo 3.1 Fast 带来了多项关键改进和全新的模型功能：\n\n*   **改进的视频生成模型**\n    *   **更丰富的原生音频**：模型现在能生成更丰富的原生音频，从自然的对话到同步的音效。\n    *   **更强的叙事控制**：通过对电影风格的更好理解，提供更大的叙事控制能力。\n    *   **增强的图像到视频功能**：确保更好的提示遵循性，提供卓越的音视频质量，并能在多个场景中保持角色一致性。\n\n*   **新的模型能力**\n    *   **“视频配料”（Ingredients to video）**：\n        *   用户现在可以通过提供最多3张角色、物体或场景的参考图像来指导生成过程。\n        *   这有助于在多个镜头中保持角色一致性，或将特定风格应用于视频。\n        *   ![Screenshot 2025-10-14 9.45.50 AM](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Screenshot_2025-10-14_9.45.50_AM.original.png)\n    *   **“场景扩展”（Scene extension）**：\n        *   通过生成与前一个视频连接的新片段，用户可以创建更长的视频，甚至持续一分钟或更长时间。\n        *   每个新视频都基于前一个片段的最后一秒生成，从而保持视觉连续性，非常适合扩展带有背景音频的镜头。\n    *   **“首尾帧”（First and last frame）**：\n        *   通过提供起始图像和结束图像，用户可以指示 Veo 3.1 在两者之间生成平滑自然的过渡场景，并配有相应的音频。\n        *   ![Screenshot 2025-10-13 4.09.05 PM](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Screenshot_2025-10-13_4.09.05_PM.original.png)\n\n## 可用性\n\nVeo 3.1 和 Veo 3.1 Fast 可通过以下途径获取：\n*   Gemini API (在 Google AI Studio 和 Vertex AI 中)\n*   Gemini 应用\n*   Flow\n\n## Veo 3.1 的应用案例\n\n*   **Promise Studios**：一家 GenAI 电影工作室，在其 MUSE 平台中使用 Veo 3.1 来增强生成式故事板和预可视化，以实现导演驱动的制作级叙事。\n*   **Latitude**：正在其生成式叙事引擎中试验 Veo 3.1，以即时将用户创建的故事变为现实。\n\n## 开始使用\n\nVeo 3.1 及这些新功能现已通过 Gemini API 提供付费预览。\n*   查阅文档以获取详细参数信息和视频长度控制。\n*   在 Veo Studio（新的 AI Studio 演示应用，需要付费的 Gemini API 密钥）中开始使用 Veo 3.1 和新功能。\n*   通过更新的 cookbook 指南直接进入代码。\n\nVeo 3.1 的价格与 Veo 3 相同。",
      "shortSummary": "Google 发布了 Veo 3.1 和 Veo 3.1 Fast，在 Gemini API 中提供付费预览。这些模型显著提升了视频生成能力，包括更丰富的原生音频、更强的叙事控制和增强的图像到视频功能，确保角色一致性。新增功能包括使用参考图像指导生成、延长现有视频（场景扩展）以及在首尾帧之间生成过渡。Veo 3.1 已在 Gemini API、Gemini 应用和 Flow 中可用，价格与 Veo 3 相同。",
      "translated_title": "介绍 Veo 3.1 和 Gemini API 中的新创意功能",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Screenshot_2025-10-14_9.45.50_AM.original.png",
          "alt": "Screenshot 2025-10-14 9.45.50 AM",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Screenshot_2025-10-13_4.09.05_PM.original.png",
          "alt": "Screenshot 2025-10-13 4.09.05 PM",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "Google is releasing Veo 3.1 and Veo 3.1 Fast, an updated video generation model, in paid preview via the Gemini API. This version offers richer native audio, greater narrative control, and enhanced image-to-video capabilities. New features include guiding generation with reference images, extending existing Veo videos, and generating transitions between frames. Companies like Promise Studios, Latitude, and Whering are already using Veo 3.1 for various applications."
    },
    {
      "title": "掌控你的AI：学习如何微调Gemma 3 270M并在设备上运行 (原标题: Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device)",
      "link": "https://developers.googleblog.com/en/own-your-ai-fine-tune-gemma-3-270m-for-on-device/",
      "pubDate": "Tue, 07 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-07T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 掌控你的AI：学习如何微调Gemma 3 270M并在设备上运行\n\n本文介绍了如何利用Gemma 3 270M这一轻量级、最先进的开放模型，进行快速微调并将其部署到设备上，从而创建高度专业化的自定义AI应用。Gemma模型系列源自与Gemini模型相同的技术，因其性能和可访问性，已获得超过2.5亿次下载和85,000个社区变体。\n\n## Gemma 3 270M的优势\nGemma 3 270M的紧凑尺寸使其无需昂贵硬件即可快速针对新用例进行微调，并部署到设备上。这为模型开发提供了灵活性，并让用户完全掌控这一强大工具。\n\n## 实践示例：文本转表情符号应用\n文章通过一个具体的例子展示了如何训练一个模型将文本翻译成表情符号，并在Web应用中进行测试。用户甚至可以教授模型使用自己常用的特定表情符号，从而生成一个个性化的表情符号生成器。\n\n## 端到端流程（一小时内完成）\n整个过程分为三个主要步骤，可在不到一小时内完成：\n\n### 1. 使用微调定制模型行为\n*   **问题：** 预训练的大型语言模型（LLM）是通用型模型。如果直接要求Gemma将文本翻译成表情符号，它可能会输出多余的对话填充内容。\n*   **解决方案：** 微调是强制模型输出特定格式并教授新知识最可靠的方法。通过在包含文本和表情符号示例的数据集上训练模型，可以教会它使用特定的表情符号。\n*   **数据集创建：** 提供更多示例能让模型学习得更好。可以使用AI生成不同文本短语对应相同表情符号的示例，以增强数据集的鲁棒性。\n    *   ![创建微调数据集](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/creating-dataset-for-finetuning_2.original.png)\n*   **QLoRA技术：** 过去微调模型需要大量VRAM，但通过量化低秩适应（QLoRA）这一参数高效微调（PEFT）技术，只需更新少量权重，大大降低了内存需求。这使得在Google Colab中使用免费T4 GPU加速，可以在几分钟内完成Gemma 3 270M的微调。\n*   用户可以使用示例数据集或自定义数据集，运行微调笔记本进行模型训练和性能测试。\n\n### 2. 量化并转换模型以用于Web\n*   **部署需求：** 考虑到表情符号通常在移动设备或计算机上使用，将模型部署到设备上的应用中是合理的。\n*   **模型优化：** 原始模型虽然小巧，但仍超过1GB。为了确保快速加载的用户体验，需要通过**量化**来缩小模型。量化过程通过降低模型权重的精度（例如从16位到4位整数），显著减小文件大小，同时对许多任务的性能影响最小。\n    *   ![Gemma设备上量化](https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/gemma-quantization-for-ondevice.original.png)\n*   **Web应用准备：** 使用LiteRT转换笔记本（与MediaPipe配合使用）或ONNX转换笔记本（与Transformers.js配合使用），可以一步完成模型的量化和转换。\n*   **WebGPU：** 这些框架利用WebGPU（一种现代Web API，允许应用访问本地设备的硬件进行计算），使得LLM能够在浏览器客户端运行，从而无需复杂的服务器设置和每次调用的推理成本。\n\n### 3. 在浏览器中运行模型\n*   **部署：** 用户可以直接在浏览器中运行定制模型。下载示例Web应用，只需修改一行代码即可接入新模型。\n*   **简化集成：** MediaPipe和Transformers.js都使这一过程变得简单。文章提供了一个MediaPipe worker中推理任务的JavaScript示例代码。\n*   **优势：** 一旦模型在用户设备上缓存，后续请求将在本地以低延迟运行，用户数据保持完全私密，并且应用即使在离线状态下也能正常运行。\n*   **分享：** 可以将应用上传到Hugging Face Spaces进行分享。\n\n## 展望\n创建专业化的AI模型不再需要成为AI专家或数据科学家。通过相对较小的数据集，可以在几分钟内提升Gemma模型的性能。这些技术使用户能够构建强大且高度定制化的AI应用，提供快速、私密且无处不在的卓越用户体验。\n\n## 资源\n文章提供了完整的项目源代码和资源链接，包括：\n*   在Colab中使用QLoRA高效微调Gemma\n*   将Gemma 3 270M转换为MediaPipe LLM Inference API使用\n*   将Gemma 3 270M转换为Transformers.js使用\n*   GitHub上的演示代码下载\n*   Gemma Cookbook和chrome.dev上的更多Web AI演示\n*   Gemma 3系列模型及其设备上功能的更多信息",
      "shortSummary": "本文介绍了如何利用Gemma 3 270M这一轻量级开放模型，通过微调创建自定义AI应用并在设备上运行。借助QLoRA技术，用户可在Google Colab中快速微调模型（例如用于文本转表情符号），然后通过量化将其优化至300MB以下，实现在Web应用中客户端运行。这使得无需昂贵硬件或专业知识，即可构建快速、私密且可离线使用的个性化AI体验。",
      "translated_title": "掌控你的AI：学习如何微调Gemma 3 270M并在设备上运行",
      "images": [
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/creating-dataset-for-finetuning_2.original.png",
          "alt": "creating-dataset-for-finetuning (2)",
          "title": "",
          "position": 1
        },
        {
          "url": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/gemma-quantization-for-ondevice.original.png",
          "alt": "gemma-quantization-for-ondevice",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "This guide shows you how to fine-tune the Gemma 3 270M model for custom tasks, like an emoji translator. Learn to quantize and convert the model for on-device use, deploying it in a web app with MediaPipe or Transformers.js for a fast, private, and offline-capable user experience."
    },
    {
      "title": "宣布推出适用于 Gemini CLI 的 Genkit 扩展 (原标题: Announcing the Genkit Extension for Gemini CLI)",
      "link": "https://developers.googleblog.com/en/announcing-the-genkit-extension-for-gemini-cli/",
      "pubDate": "Tue, 07 Oct 2025 16:00:00 GMT",
      "isoDate": "2025-10-07T16:00:00.000Z",
      "creator": "Google",
      "summary": "# 宣布推出适用于 Gemini CLI 的 Genkit 扩展\n\nGoogle 于2025年10月8日宣布推出 **Genkit 扩展**，旨在简化使用 Genkit 和 Gemini CLI 构建 AI 应用程序的流程。此扩展为 Gemini CLI 提供了对 Genkit 架构、模式和工具的深入理解，使用户能够直接从终端构建、调试和迭代 AI 应用程序。这是同期推出的一系列 Gemini CLI 扩展之一，旨在增强 AI 辅助开发工作流程。\n\n## 安装\n\n您可以通过以下命令安装 Genkit 扩展：\n`gemini extensions install https://github.com/gemini-cli-extensions/genkit`\n\n## Genkit 扩展是什么？\n\nGenkit 扩展是一个官方的 Gemini CLI 扩展，它在 Gemini CLI 和您的 Genkit 项目之间提供了深度集成。通过将 Genkit 的 MCP（模型上下文协议）服务器与专门的上下文文件捆绑在一起，此扩展使 Gemini CLI 能够全面理解如何使用 Genkit SDK。\n\n安装此扩展后，Gemini CLI 将获得以下能力：\n*   **Genkit MCP 工具**：与 Genkit 开发工具直接集成。\n*   **上下文感知辅助**：理解 Genkit 概念、最佳实践和工作流程。\n*   **智能代码生成**：专门为 Genkit 应用程序开发量身定制的 AI 辅助。\n\n## 主要功能\n\n### Genkit 专用 MCP 工具\n\n该扩展提供了几个强大的工具，将 Gemini CLI 直接连接到您的 Genkit 应用程序：\n*   `get_usage_guide`：在实现 AI 功能之前获取特定语言的 Genkit 使用说明和最佳实践。\n*   `lookup_genkit_docs`：直接从聊天界面访问最新的、特定语言的 Genkit 文档。\n*   `list_flows`：发现并探索当前 Genkit 应用程序中定义的流程。\n*   `run_flow`：直接执行流程以进行调试和分析。\n*   `get_trace`：逐步分析 OpenTelemetry 跟踪，以了解流程执行情况。\n\n### 智能开发辅助\n\n安装 Genkit 扩展后，Gemini CLI 能够理解 Genkit 的架构，并可以帮助您：\n*   **添加新的 AI 功能**：使用适当的 Genkit 模式生成针对特定用例量身定制的 AI 功能。\n*   **调试应用程序**：通过上下文感知的建议分析跟踪并解决问题。\n*   **遵循最佳实践**：确保您的代码遵循 Genkit 推荐的模式和约定。\n\n### 无缝工作流程集成\n\n该扩展旨在与您现有的 Genkit 开发工作流程协同工作。它理解您的项目结构，尊重您选择的提供商（在新项目开始时优先选择 Google Gen AI），并与 Genkit Developer UI 无缝集成。\n\n## 入门\n\n### 先决条件\n\n在安装扩展之前，请确保您已安装 Gemini CLI。\n\n### 安装\n\n1.  安装扩展：`gemini extensions install https://github.com/gemini-cli-extensions/genkit`\n2.  扩展将自动配置 Gemini CLI，使其具备 Genkit 特定的知识和工具。\n\n### 使用示例\n\n安装后，您可以立即开始利用该扩展：\n*   **创建新流程**：例如，输入 `> Write a flow that generates a structured workout program based on goals, experience, and time`，Gemini CLI 将参考最新的 Genkit 开发指南和文档来编写准确的代码并遵循最佳实践。\n*   **调试和分析**：例如，输入 `> Help me understand why my flow is returning inconsistent results`，扩展将确保 Gemini CLI 提供专门针对 Genkit 模式、使用适当 API 并遵循既定最佳实践的响应。\n\n## 对开发工作流程的影响\n\nGenkit 扩展弥合了通用 AI 辅助与 Genkit 生态系统专业知识之间的鸿沟。这种集成代表着朝着更智能、上下文感知的开发工具迈出了一步。它提供了理解 Genkit 架构、了解其功能并能指导您应对构建生产级 AI 应用程序特定挑战的辅助。\n\n## 立即开始\n\n鼓励用户试用该扩展，并在 Genkit 扩展仓库中提供反馈。如果觉得有用，请考虑为仓库加星以示支持。\n\n## 了解更多\n\n*   Genkit 文档\n*   Genkit AI 辅助开发指南\n*   Gemini CLI 扩展文档\n*   Genkit 扩展仓库",
      "shortSummary": "Google 宣布推出适用于 Gemini CLI 的 Genkit 扩展，旨在简化 AI 应用程序开发。该扩展将 Genkit 的架构和工具深度集成到 Gemini CLI 中，使用户能够直接从终端构建、调试和迭代 AI 应用。它提供上下文感知辅助、智能代码生成和 Genkit 专用工具，帮助开发者遵循最佳实践并无缝集成到现有工作流程。此举显著提升了 Genkit 应用程序的开发效率和智能化水平。",
      "translated_title": "宣布推出适用于 Gemini CLI 的 Genkit 扩展",
      "images": [],
      "contentSource": "完整文章",
      "content": "The new Genkit Extension for Gemini CLI gives the command line deep knowledge of Genkit's architecture. It helps you build, debug, and iterate on AI apps with intelligent code generation, context-aware assistance, and tools to run flows and analyze traces directly from your terminal."
    }
  ],
  "lastUpdated": "2025-11-10T10:33:38.485Z"
}