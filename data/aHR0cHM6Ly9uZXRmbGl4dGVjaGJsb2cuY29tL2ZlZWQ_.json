{
  "sourceUrl": "https://netflixtechblog.com/feed",
  "title": "Netflix TechBlog - Medium",
  "description": "Learn about Netflix’s world class engineering efforts, company culture, product developments and more. - Medium",
  "link": "https://netflixtechblog.com?source=rss----2615bd06b42e---4",
  "items": [
    {
      "title": "一次建模，随处呈现：Netflix 的 UDA（统一数据架构） (原标题: Model Once, Represent Everywhere: UDA (Unified Data Architecture) at Netflix)",
      "link": "https://netflixtechblog.com/uda-unified-data-architecture-6a6aee261d8d?source=rss----2615bd06b42e---4",
      "pubDate": "Thu, 12 Jun 2025 14:56:32 GMT",
      "isoDate": "2025-06-12T14:56:32.000Z",
      "creator": "Netflix Technology Blog",
      "summary": "## Netflix 的 UDA（统一数据架构）：一次建模，随处呈现\n\n### 引言：挑战与需求\n随着 Netflix 业务（电影、剧集、游戏、直播活动、广告）的不断增长，支撑其运营的系统复杂性也随之增加。核心业务概念（如“演员”或“电影”）在多个系统中被独立建模，例如内部应用所依赖的 Enterprise GraphQL Gateway、媒体资产管理平台以及媒体计算平台等。这些系统各自为政，缺乏协调和共享理解，导致一系列挑战：\n\n*   **模型重复与不一致：** 团队在不同系统中重复建模相同的业务实体，导致定义冲突，难以协调。\n*   **术语不一致：** 即使在同一系统内，团队也可能对同一概念使用不同术语，或对不同概念使用相同术语，阻碍协作。\n*   **数据质量问题：** 跨微服务的数据差异和损坏引用难以检测，标识符和外键建模不一致且文档不足，需要人工干预。\n*   **连接性受限：** 系统内部数据关系受限，跨系统则几乎不存在。\n\n为解决这些问题，Netflix 需要新的基础架构，以在概念层面一次定义模型，并在所有地方重用这些定义。这不仅需要文档化概念，更要将其连接到实际系统和数据，并将这些定义投射出去，生成模式并强制系统间的一致性。这些核心思想促成了 UDA 的诞生。\n\n![Spider-Man Pointing meme with each Spider-Man labelled as: “it’s a movie”, “it’s a tv show”, “it’s a game”.](https://cdn-images-1.medium.com/max/1024/0*wNYAhebbErEdYROL)\n\n### UDA 简介\nUDA（统一数据架构）是内容工程中连接数据的基础。它使团队能够一次性建模领域，并在不同系统间保持一致的表示，从而实现自动化、可发现性和语义互操作性。\n\n**UDA 赋能用户和系统实现以下功能：**\n\n*   **注册和连接领域模型：** 形式化地概念化联邦业务领域，避免混淆和重复建模。\n*   **编目并将领域模型映射到数据容器：** 将领域模型表示为图，便于查找业务概念的实际数据位置和结构（如 GraphQL 类型解析器、Data Mesh 源、Iceberg 表）。\n*   **将领域模型转译为模式定义语言：** 自动为各种系统（如 GraphQL、Avro、SQL、RDF、Java）创建一致的技术数据结构，减少手动工作和错误。\n*   **在数据容器之间忠实地移动数据：** 自动处理数据在不同系统间的移动和正确转换，确保数据一致性和准确性。\n*   **通过搜索和图遍历发现和探索领域概念：** 更容易找到所需业务信息，理解概念和数据之间的关系，并确信访问的是正确信息。\n*   **通过编程方式内省知识图：** 开发者可以构建更智能的应用，自动化复杂的数据依赖工作流，并从数据关系中发现新洞察。\n\nUDA 的基础是一个知识图谱，通过映射将领域模型连接到数据容器，并以内部元模型 Upper 为基础。Upper 定义了 UDA 中的领域建模语言，并支持跨系统自动生成模式和数据管道的投射。\n\n![Image of the UDA knowledge graph. A central node representing a domain model is connected to other nodes representing Data Mesh, GraphQL, and Iceberg data containers.](https://cdn-images-1.medium.com/max/1024/1*j1I2cLD0vtfE9IQfNiUwVQ.png)\n\n### UDA 在生产系统中的应用案例\n\n*   **Primary Data Management (PDM)：** Netflix 权威参考数据和分类管理平台。PDM 将领域模型转换为平面或分层分类，驱动生成的 UI，并投射到 Avro 和 GraphQL 模式，自动在数据仓库和 Enterprise Gateway 中配置数据产品和 GraphQL API。\n*   **Sphere：** 业务用户的自助式运营报告工具。Sphere 使用 UDA 编目和关联跨系统的业务概念，通过熟悉的术语（如“演员”或“电影”）实现发现。一旦概念被选中，Sphere 会遍历知识图谱并生成 SQL 查询，从数据仓库中检索数据，无需手动连接或技术中介。\n\n### UDA 作为知识图谱\nUDA 旨在解决数据集成问题，它将数据目录与模式注册表统一，并具有语义集成这一硬性要求。将业务概念与模式和数据容器以图状结构连接，并以强大的语义基础为支撑，自然而然地促使 Netflix 采用了知识图谱方法。\n\nNetflix 选择 RDF 和 SHACL 作为 UDA 知识图谱的基础，但在企业规模化应用中遇到了以下挑战：\n\n*   **RDF 缺乏可用的信息模型：** RDF 提供了灵活的图结构，但在如何组织命名图、管理本体所有权或定义治理边界方面指导不足。\n*   **SHACL 不适用于企业数据建模：** SHACL 旨在验证原生 RDF，假设全局唯一 URI 和单一数据图。但企业数据围绕本地模式和类型化键构建，难以表达和验证异构系统中的真实数据。\n*   **团队缺乏共享的创作实践：** 缺乏强有力的指导方针，导致团队本体建模不一致，破坏了语义互操作性。\n*   **本体工具缺乏协作建模支持：** 与 GraphQL Federation 不同，本体框架没有内置对模块化贡献、团队所有权或安全联邦的支持。\n\n为应对这些挑战，UDA 采用了“命名图优先”的信息模型。每个命名图都遵循一个管理模型（其本身也是知识图谱中的一个命名图），确保了整个图的解析、模块化和治理。\n\n### Upper：领域建模语言\nUpper 是一种用于正式描述领域（业务或系统）及其概念的语言。这些概念被组织成领域模型：定义带键实体、其属性以及与其他实体（可能带键或嵌套，在同一领域或跨领域）关系的受控词汇表。领域模型中的带键概念可以组织成类型分类，并可从其他领域模型进行扩展。Upper 还提供了一套丰富的数据类型。\n\n![Visualization of the UDA graph representation of a One Piece character. The Character node in the graph is connected to a Devil Fruit node. The Devil Fruit node is connected to a Devil Fruit Type node.](https://cdn-images-1.medium.com/max/1024/0*A_-GpZLvqbxuVdkH)\n\nUpper 领域模型本身就是数据，以概念性 RDF 形式表达，组织成命名图，可在 UDA 知识图谱中进行内省、查询和版本控制。这个图谱不仅统一了领域模型本身，还统一了它们转译成的模式（GraphQL、Avro、Iceberg、Java）以及连接领域概念与具体数据容器的映射。Upper 提升了抽象级别，超越了传统的本体语言，它定义了 W3C 语义技术的一个严格子集，专为领域建模量身定制和泛化。\n\n![Screenshot of UDA UI showing domain model for One Piece serialized as Turtle.](https://cdn-images-1.medium.com/max/1024/1*SGMUpJucEWhdlZsd4blz3A.png)\n\nUpper 是 UDA 中连接数据的元模型——所有模型的模型。它被设计为一个自举上层本体，这意味着 Upper 能够自我引用（因为它将自身建模为一个领域模型）、自我描述（因为它定义了领域模型这一概念）和自我验证（因为它符合自身模型）。这种方法使 UDA 能够自举其基础设施：Upper 本身被投射到生成的基于 Jena 的 Java API 和 GraphQL 模式中，这些 API 和模式被用于 Netflix 的 Enterprise GraphQL 网关中的 GraphQL 服务。由于所有领域模型都是 Upper 的保守扩展，其他系统领域模型（包括 GraphQL、Avro、Data Mesh 和映射）都能无缝集成到同一运行时中，从而实现跨模式的一致数据语义和互操作性。\n\n![Screenshot of an IDE. It shows Java code using the generated API from the Upper metamodel to traverse and print terms from a domain domain in the top while the bottom contains the output of an execution.](https://cdn-images-1.medium.com/max/1024/0*5tJcW2A6lLrNi257)\n\n### 数据容器表示\n数据容器是信息存储库，包含符合其自身模式语言或类型系统的实例数据：来自 GraphQL 服务的联邦实体、来自 Data Mesh 源的 Avro 记录、来自 Iceberg 表的行或来自 Java API 的对象。每个容器都在一个系统中运行，该系统施加其自身的结构和操作约束。\n\n![Screenshot of a UI showing details for a Data Mesh Source containing One Piece Characters.](https://cdn-images-1.medium.com/max/1024/1*qUzAb6-TC2HL8qAWAW1Xlw.png)\n\n数据容器表示本身也是数据，它们是对数据系统成员作为图数据的忠实解释。UDA 将这些系统的定义捕获为它们自己的领域模型（即系统领域），这些模型编码了系统的信息架构和其中数据容器的模式。它们为将系统转换为图表示提供了蓝图。\n\n![Screenshot of an IDE showing two files open side by side. On the left is a system domain model for Data Mesh. On the right is a representation of a Data Mesh source containing One Piece Character data.](https://cdn-images-1.medium.com/max/1024/0*6QzelmSRrIj1G881)\n\nUDA 将数据容器表示编目到知识图谱中。它记录了底层数据资产的坐标和元数据，但与传统目录不同的是，它只跟踪与领域模型语义连接的资产。这使得用户和系统能够将领域模型中的概念连接到可以访问相应实例数据的具体位置。这些连接被称为映射。\n\n### 映射\n映射是连接领域模型和数据容器的数据。领域模型中的每个元素（从领域模型本身到特定属性和关系）都可寻址。同样，数据容器表示使所有组件都可寻址，从 Iceberg 表到单个列，或从 GraphQL 类型到特定字段。映射连接领域模型子图中的节点与容器表示子图中的节点。从视觉上看，映射是连接这两个图的一组弧线。\n\n![Screenshot of UDA UI showing a mapping between a concept in UDA and a Data Mesh Source.](https://cdn-images-1.medium.com/max/1024/1*it3X5Vu8plWX5QvN_AJkgw.png)\n\n映射实现了发现功能。从一个领域概念开始，用户和系统可以遍历知识图谱，找到该概念在何处具体化。",
      "shortSummary": "Netflix 的 UDA（统一数据架构）旨在解决其日益增长的系统间数据不一致和复杂性问题。UDA 通过采用知识图谱方法，在概念层面一次性建模核心业务概念，并利用名为 Upper 的元模型，实现跨系统的统一数据表示、自动化模式生成和无缝数据移动。它显著提升了“电影”或“演员”等概念在 GraphQL、Data Mesh 和 Iceberg 等平台上的可发现性和语义互操作性，从而简化了内部运营和数据访问。",
      "translated_title": "一次建模，随处呈现：Netflix 的 UDA（统一数据架构）",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*wNYAhebbErEdYROL",
          "alt": "Spider-Man Pointing meme with each Spider-Man labelled as: “it’s a movie”, “it’s a tv show”, “it’s a game”.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*j1I2cLD0vtfE9IQfNiUwVQ.png",
          "alt": "Image of the UDA knowledge graph. A central node representing a domain model is connected to other nodes representing Data Mesh, GraphQL, and Iceberg data containers.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*A_-GpZLvqbxuVdkH",
          "alt": "Visualization of the UDA graph representation of a One Piece character. The Character node in the graph is connected to a Devil Fruit node. The Devil Fruit node is connected to a Devil Fruit Type node.",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*SGMUpJucEWhdlZsd4blz3A.png",
          "alt": "Screenshot of UDA UI showing domain model for One Piece serialized as Turtle.",
          "title": "",
          "position": 4
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*5tJcW2A6lLrNi257",
          "alt": "Screenshot of an IDE. It shows Java code using the generated API from the Upper metamodel to traverse and print terms from a domain domain in the top while the bottom contains the output of an execution.",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<p>By <a href=\"https://www.linkedin.com/in/ahutter/\">Alex Hutter</a>, <a href=\"https://www.linkedin.com/in/bertails/\">Alexandre Bertails</a>, <a href=\"https://www.linkedin.com/in/clairezwang0612/\">Claire Wang</a>, <a href=\"https://www.linkedin.com/in/haoyuan-h-98b587134/\">Haoyuan He</a>, <a href=\"https://www.linkedin.com/in/kishore-banala/\">Kishore Banala</a>, <a href=\"https://www.linkedin.com/in/peterroyal/\">Peter Royal</a>, <a href=\"https://www.linkedin.com/in/shervinafshar/\">Shervin Afshar</a></p><p>As Netflix’s offerings grow — across films, series, games, live events, and ads — so does the complexity of the systems that support it. Core business concepts like ‘actor’ or ‘movie’ are modeled in many places: in our Enterprise GraphQL Gateway powering internal apps, in our asset management platform storing media assets, in our media computing platform that powers encoding pipelines, to name a few. Each system models these concepts differently and in isolation, with little coordination or shared understanding. While they often operate on the same concepts, these systems remain largely unaware of that fact, and of each other.</p><figure><img alt=\"Spider-Man Pointing meme with each Spider-Man labelled as: “it’s a movie”, “it’s a tv show”, “it’s a game”.\" src=\"https://cdn-images-1.medium.com/max/1024/0*wNYAhebbErEdYROL\" /></figure><p>As a result, several challenges emerge:</p><ul><li><strong>Duplicated and Inconsistent Models</strong> — Teams re-model the same business entities in different systems, leading to conflicting definitions that are hard to reconcile.</li><li><strong>Inconsistent Terminology</strong> — Even within a single system, teams may use different terms for the same concept, or the same term for different concepts, making collaboration harder.</li><li><strong>Data Quality Issues</strong> — Discrepancies and broken references are hard to detect across our many microservices. While identifiers and foreign keys exist, they are inconsistently modeled and poorly documented, requiring manual work from domain experts to find and fix any data issues.</li><li><strong>Limited Connectivity</strong> — Within systems, relationships between data are constrained by what each system supports. Across systems, they are effectively non-existent.</li></ul><p>To address these challenges, we need new foundations that allow us to define a model once, at the conceptual level, and reuse those definitions everywhere. But it isn’t enough to just document concepts; we need to connect them to real systems and data. And more than just connect, we have to project those definitions outward, generating schemas and enforcing consistency across systems. The conceptual model must become part of the control plane.</p><p>These were the core ideas that led us to build UDA.</p><h3>Introducing UDA</h3><p><strong>UDA (Unified Data Architecture)</strong> is the foundation for connected data in <a href=\"https://netflixtechblog.com/netflix-studio-engineering-overview-ed60afcfa0ce\">Content Engineering</a>. It enables teams to model domains once and represent them consistently across systems — powering automation, discoverability, and <a href=\"https://en.wikipedia.org/wiki/Semantic_interoperability\">semantic interoperability</a>.</p><p><strong>Using UDA, users and systems can:</strong></p><p><strong>Register and connect domain models </strong>— formal conceptualizations of federated business domains expressed as data.</p><ul><li><strong>Why? </strong>So everyone uses the same official definitions for business concepts, which avoids confusion and stops different teams from rebuilding similar models in conflicting ways.</li></ul><p><strong>Catalog and map domain models to data containers</strong>, such as GraphQL type resolvers served by a <a href=\"https://netflixtechblog.com/open-sourcing-the-netflix-domain-graph-service-framework-graphql-for-spring-boot-92b9dcecda18\">Domain Graph Service</a>, <a href=\"https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873\">Data Mesh sources</a>, or Iceberg tables, through their representation as a graph.</p><ul><li><strong>Why?</strong> To make it easy to find where the actual data for these business concepts lives (e.g., in which specific database, table, or service) and understand how it’s structured there.</li></ul><p><strong>Transpile domain models into schema definition languages</strong> like GraphQL, Avro, SQL, RDF, and Java, while preserving semantics.</p><ul><li><strong>Why? </strong>To automatically create consistent technical data structures (schemas) for various systems directly from the domain models, saving developers manual effort and reducing errors caused by out-of-sync definitions.</li></ul><p><strong>Move data faithfully between data containers</strong>, such as from federated GraphQL entities to <a href=\"https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873\">Data Mesh</a> (a general purpose data movement and processing platform for moving data between Netflix systems at scale), Change Data Capture (CDC) sources to joinable Iceberg Data Products.</p><ul><li><strong>Why? </strong>To save developer time by automatically handling how data is moved and correctly transformed between different systems. This means less manual work to configure data movement, ensuring data shows up consistently and accurately wherever it’s needed.</li></ul><p><strong>Discover and explore domain concepts </strong>via search and graph traversal.</p><ul><li><strong>Why? </strong>So anyone can more easily find the specific business information they’re looking for, understand how different concepts and data are related, and be confident they are accessing the correct information.</li></ul><p><strong>Programmatically introspect the knowledge graph</strong> using Java, GraphQL, or SPARQL.</p><ul><li><strong>Why?</strong> So developers can build smarter applications that leverage this connected business information, automate more complex data-dependent workflows, and help uncover new insights from the relationships in the data.</li></ul><p><strong>This post introduces the foundations of UDA</strong> as a knowledge graph, connecting domain models to data containers through mappings, and grounded in an in-house <a href=\"https://en.wikipedia.org/wiki/Metamodeling#:~:text=A%20metamodel%2F%20surrogate%20model%20is,representing%20input%20and%20output%20relations\">metamodel</a>, or model of models, called Upper. Upper defines the language for domain modeling in UDA and enables projections that automatically generate schemas and pipelines across systems.</p><figure><img alt=\"Image of the UDA knowledge graph. A central node representing a domain model is connected to other nodes representing Data Mesh, GraphQL, and Iceberg data containers.\" src=\"https://cdn-images-1.medium.com/max/1024/1*j1I2cLD0vtfE9IQfNiUwVQ.png\" /><figcaption>The same domain model can be connected to semantically equivalent data containers in the UDA knowledge graph.</figcaption></figure><p><strong>This post also highlights two systems</strong> that leverage UDA in production:</p><p><strong>Primary Data Management (PDM)</strong> is our platform for managing authoritative reference data and taxonomies. PDM turns domain models into flat or hierarchical taxonomies that drive a generated UI for business users. These taxonomy models are projected into Avro and GraphQL schemas, automatically provisioning data products in the Warehouse and GraphQL APIs in the <a href=\"https://netflixtechblog.com/how-netflix-scales-its-api-with-graphql-federation-part-1-ae3557c187e2\">Enterprise Gateway</a>.</p><p><strong>Sphere</strong> is our self-service operational reporting tool for business users. Sphere uses UDA to catalog and relate business concepts across systems, enabling discovery through familiar terms like ‘actor’ or ‘movie.’ Once concepts are selected, Sphere walks the knowledge graph and generates SQL queries to retrieve data from the warehouse, no manual joins or technical mediation required.</p><h4>UDA is a Knowledge Graph</h4><p><strong>UDA needs to solve the </strong><a href=\"https://en.wikipedia.org/wiki/Data_integration\"><strong>data integration</strong></a><strong> problem. </strong>We needed a data catalog unified with a schema registry, but with a hard requirement for <a href=\"https://en.wikipedia.org/wiki/Semantic_integration#:~:text=Semantic%20integration%20is%20the%20process,from%20diverse%20sources\">semantic integration</a>. Connecting business concepts to schemas and data containers in a graph-like structure, grounded in strong semantic foundations, naturally led us to consider a <a href=\"https://en.wikipedia.org/wiki/Knowledge_graph\">knowledge graph</a> approach.</p><p><strong>We chose RDF and SHACL as the foundation for UDA’s knowledge graph</strong>. But operationalizing them at enterprise scale surfaced several challenges:</p><ul><li><strong>RDF lacked a usable information model.</strong> While RDF offers a flexible graph structure, it provides little guidance on how to organize data into <a href=\"https://www.w3.org/TR/rdf12-concepts/#dfn-named-graph\">named graphs</a>, manage ontology ownership, or define governance boundaries. Standard <a href=\"https://www.w3.org/2001/sw/wiki/Linking_patterns\">follow-your-nose mechanisms</a> like owl:imports apply only to ontologies and don’t extend to named graphs; we needed a generalized mechanism to express and resolve dependencies between them.</li><li><strong>SHACL is not a modeling language for enterprise data.</strong> Designed to validate native RDF, SHACL assumes globally unique URIs and a single data graph. But enterprise data is structured around local schemas and typed keys, as in GraphQL, Avro, or SQL. SHACL could not express these patterns, making it difficult to model and validate real-world data across heterogeneous systems.</li><li><strong>Teams lacked shared authoring practices.</strong> Without strong guidelines, teams modeled their ontologies inconsistently breaking semantic interoperability. Even subtle differences in style, structure, or naming led to divergent interpretations and made transpilation harder to define consistently across schemas.</li><li><strong>Ontology tooling lacked support for collaborative modeling.</strong> Unlike GraphQL Federation, ontology frameworks had no built-in support for modular contributions, team ownership, or safe federation. Most engineers found the tools and concepts unfamiliar, and available authoring environments lacked the structure needed for coordinated contributions.</li></ul><p><strong>To address these challenges, UDA adopts a named-graph-first information model.</strong> Each named graph conforms to a governing model, itself a named graph in the knowledge graph. This systematic approach ensures resolution, modularity, and enables governance across the entire graph. While a full description of UDA’s information infrastructure is beyond the scope of this post, the next sections explain how UDA bootstraps the knowledge graph with its metamodel and uses it to model data container representations and mappings.</p><h4>Upper is Domain Modeling</h4><p><strong>Upper is a language for formally describing domains — business or system — and their concepts</strong>. <a href=\"https://en.wikipedia.org/wiki/Conceptualization_(information_science)\">These concepts are organized into domain models</a>: controlled vocabularies that define classes of keyed entities, their attributes, and their relationships to other entities, which may be keyed or nested, within the same domain or across domains. Keyed concepts within a domain model can be organized in taxonomies of types, which can be as complex as the business or the data system needs them to be. Keyed concepts can also be extended from other domain models — that is, new attributes and relationships can be <a href=\"https://tomgruber.org/writing/onto-design.pdf#page=4\">contributed monotonically</a>. Finally, Upper ships with a rich set of datatypes for attribute values, which can also be customized per domain.</p><figure><img alt=\"Visualization of the UDA graph representation of a One Piece character. The Character node in the graph is connected to a Devil Fruit node. The Devil Fruit node is connected to a Devil Fruit Type node.\" src=\"https://cdn-images-1.medium.com/max/1024/0*A_-GpZLvqbxuVdkH\" /><figcaption><em>The graph representation of the onepiece: domain model from our UI. Depicted here you can see how Characters are related to Devil Fruit, and that each Devil Fruit has a type.</em></figcaption></figure><p><strong>Upper domain models are data</strong>. They are expressed as <a href=\"https://www.w3.org/TR/rdf12-concepts/\">conceptual RDF</a> and organized into named graphs, making them introspectable, queryable, and versionable within the UDA knowledge graph. This graph unifies not just the domain models themselves, but also the schemas they transpile to — GraphQL, Avro, Iceberg, Java — and the mappings that connect domain concepts to concrete data containers, such as GraphQL type resolvers served by a <a href=\"https://netflixtechblog.com/open-sourcing-the-netflix-domain-graph-service-framework-graphql-for-spring-boot-92b9dcecda18\">Domain Graph Service</a>, <a href=\"https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873\">Data Mesh sources</a>, or Iceberg tables, through their representations. Upper raises the level of abstraction above traditional ontology languages: it defines a strict subset of <a href=\"https://www.w3.org/2001/sw/wiki/Main_Page\">semantic technologies</a> from the W3C tailored and generalized for domain modeling. It builds on ontology frameworks like RDFS, OWL, and SHACL so domain authors can model effectively without even needing to learn what an ontology is.</p><figure><img alt=\"Screenshot of UDA UI showing domain model for One Piece serialized as Turtle.\" src=\"https://cdn-images-1.medium.com/max/1024/1*SGMUpJucEWhdlZsd4blz3A.png\" /><figcaption>UDA domain model for One Piece. <a href=\"https://github.com/Netflix-Skunkworks/uda/blob/9627a97fcd972a41ec910be3f928ea7692d38714/uda-intro-blog/onepiece.ttl\">Link to full definition</a>.</figcaption></figure><p><strong>Upper is the metamodel for Connected Data in UDA — the model for all models</strong>. It is designed as a bootstrapping <a href=\"https://en.wikipedia.org/wiki/Upper_ontology\">upper ontology</a>, which means that Upper is <em>self-referencing</em>, because it models itself as a domain model; <em>self-describing</em>, because it defines the very concept of a domain model; and <em>self-validating</em>, because it conforms to its own model. This approach enables UDA to bootstrap its own infrastructure: Upper itself is projected into a generated Jena-based Java API and GraphQL schema used in GraphQL service federated into Netflix’s Enterprise GraphQL gateway. These same generated APIs are then used by the projections and the UI. Because all domain models are <a href=\"https://en.wikipedia.org/wiki/Conservative_extension\">conservative extensions</a> of Upper, other system domain models — including those for GraphQL, Avro, Data Mesh, and Mappings — integrate seamlessly into the same runtime, enabling consistent data semantics and interoperability across schemas.</p><figure><img alt=\"Screenshot of an IDE. It shows Java code using the generated API from the Upper metamodel to traverse and print terms from a domain domain in the top while the bottom contains the output of an execution.\" src=\"https://cdn-images-1.medium.com/max/1024/0*5tJcW2A6lLrNi257\" /><figcaption>Traversing a domain model programmatically using the Java API generated from the Upper metamodel.</figcaption></figure><h4>Data Container Representations</h4><p><strong>Data containers are repositories of information. </strong>They contain instance data that conform to their own schema languages or type systems: federated entities from GraphQL services, Avro records from Data Mesh sources, rows from Iceberg tables, or objects from Java APIs. Each container operates within the context of a system that imposes its own structural and operational constraints.</p><figure><img alt=\"Screenshot of a UI showing details for a Data Mesh Source containing One Piece Characters.\" src=\"https://cdn-images-1.medium.com/max/1024/1*qUzAb6-TC2HL8qAWAW1Xlw.png\" /><figcaption>A Data Mesh source is a data container.</figcaption></figure><p><strong>Data container </strong><a href=\"https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning\"><strong>representations</strong></a><strong> are data.</strong> They are faithful interpretations of the members of data systems as graph data. UDA captures the definition of these systems as their own domain models, the system domains. These models encode both the information architecture of the systems and the schemas of the data containers within. They provide a blueprint for translating the systems into graph representations.</p><figure><img alt=\"Screenshot of an IDE showing two files open side by side. On the left is a system domain model for Data Mesh. On the right is a representation of a Data Mesh source containing One Piece Character data.\" src=\"https://cdn-images-1.medium.com/max/1024/0*6QzelmSRrIj1G881\" /><figcaption><em>Side by side/super imposed image of data container schema and representation. </em><a href=\"https://github.com/Netflix-Skunkworks/uda/blob/9627a97fcd972a41ec910be3f928ea7692d38714/uda-intro-blog/onepiece_character_data_container.ttl\"><em>Link to full data container representation</em></a><em>.</em></figcaption></figure><p><strong>UDA catalogs the data container representations into the knowledge graph.</strong> It records the coordinates and metadata of the underlying data assets, but unlike a traditional catalog, it only tracks assets that are semantically connected to domain models. This enables users and systems to connect concepts from domain models to the concrete locations where corresponding instance data can be accessed. Those connections are called <em>Mappings</em>.</p><h4>Mappings</h4><p><strong>Mappings are data that connect domain models to data containers.</strong> Every element in a domain model is addressable, from the domain model itself down to specific attributes and relationships. Likewise, data container representations make all components addressable, from an Iceberg table to an individual column, or from a GraphQL type to a specific field. A Mapping connects nodes in a subgraph of the domain model to nodes in a subgraph of a container representation. Visually, the Mapping is the set of arcs that link those two graphs together.</p><figure><img alt=\"Screenshot of UDA UI showing a mapping between a concept in UDA and a Data Mesh Source.\" src=\"https://cdn-images-1.medium.com/max/1024/1*it3X5Vu8plWX5QvN_AJkgw.png\" /><figcaption><em>A mapping between a domain model and a Data Mesh Source from the UDA UI. </em><a href=\"https://github.com/Netflix-Skunkworks/uda/blob/9627a97fcd972a41ec910be3f928ea7692d38714/uda-intro-blog/onepiece_character_mappings.ttl\"><em>Link to full mapping</em></a><em>.</em></figcaption></figure><p><strong>Mappings enable discovery.</strong> Starting from a domain concept, users and systems can walk the knowledge graph to find where that concept is materialized — in which data system, in which container, and even how a specific attribute or relationship is physically accessed. The inverse is also supported: given a data container, one can trace back to the domain concepts it participates in.</p><p><strong>Mappings shape UDA’s approach to semantic data integration.</strong> Most existing schema languages are not expressive enough in capturing richer semantics of a domain to address requirements for data integration (<a href=\"https://doi.org/10.1007/978-3-319-49340-4_8\">for example</a>, “accessibility of data, providing semantic context to support its interpretation, and establishing meaningful links between data”). A trivial example of this could be seen in the lack of built-in facilities in Avro to represent foreign keys, making it very hard to express how entities relate across Data Mesh sources. Mappings, together with the corresponding system domain models, allow for such relationships, and many other constraints, to be defined in the domain models and used programmatically in actual data systems.</p><p><strong>Mappings enable intent-based automation.</strong> Data is not always available in the systems where consumers need it. Because Mappings encode both meaning and location, UDA can reason about how data should move, preserving semantics, without requiring the consumer to specify how it should be done. Beyond the cataloging use case, connecting to existing containers, UDA automatically derives <em>canonical Mappings</em> from registered domain models as part of the projection process.</p><h4>Projections</h4><p><strong>A projection produces a concrete data container.</strong> These containers, such as a GraphQL schema or a Data Mesh source, implement the characteristics derived from a registered domain model. Each projection is a concrete realization of Upper’s denotational semantics, ensuring <a href=\"https://en.wikipedia.org/wiki/Semantic_interoperability\">semantic interoperability</a> across all containers projected from the same domain model.</p><p><strong>Projections produce consistent public contracts across systems.</strong> The data containers generated by projections encode data contracts in the form of schemas, derived by transpiling a domain model into the target container’s schema language. UDA currently supports transpilation to GraphQL and Avro schemas.</p><p>The GraphQL transpilation produces a schema that adheres to the <a href=\"https://spec.graphql.org/October2021/#sec-Overview\">official GraphQL spec</a> with the ability to generate all GraphQL types defined in the spec. Given that the UDA domain model can be federated, it also supports generating federated graphQL schemas. Below is an example of a transpiled GraphQL schema.</p><figure><img alt=\"Screenshot of an IDE showing two files open side by side. On the left is the definition of a Character in UDA. On the right is transpiled GraphQL schema.\" src=\"https://cdn-images-1.medium.com/max/1024/0*NPXB3ujnUGSIklei\" /><figcaption><em>Domain model on the left, with transpiled GraphQL schema on the right. </em><a href=\"https://github.com/Netflix-Skunkworks/uda/blob/9627a97fcd972a41ec910be3f928ea7692d38714/uda-intro-blog/onepiece.graphqls\"><em>Link to full transpiled GraphQL schema</em></a><em>.</em></figcaption></figure><p>The Avro transpilation produces a schema that is a Data Mesh flavor of Avro, which includes some customization on top of the <a href=\"https://avro.apache.org/docs/1.12.0/specification/\">official Avro spec</a>. This schema is used to automatically create a Data Mesh source container. Below is an example of a transpiled Avro schema.</p><figure><img alt=\"Screenshot of an IDE showing two files open side by side. On the left is the definition of a Devil Fruit in UDA. On the right is transpiled Avro schema.\" src=\"https://cdn-images-1.medium.com/max/1024/0*uVInkj5S3PYTqNA-\" /><figcaption><em>Domain model on the left, with transpiled Avro schema on the right. </em><a href=\"https://github.com/Netflix-Skunkworks/uda/blob/9627a97fcd972a41ec910be3f928ea7692d38714/uda-intro-blog/onepiece.avro\"><em>Link to full transpiled Avro schema</em></a><em>.</em></figcaption></figure><p><strong>Projections can automatically populate data containers. </strong>Some projections, such as those to GraphQL schemas or Data Mesh sources produce empty containers that require developers to populate the data. This might be creating GraphQL APIs or pushing events onto Data Mesh sources. Conversely, other containers, like Iceberg Tables, are automatically created and populated by UDA. For Iceberg Tables, UDA leverages the Data Mesh platform to automatically create data streams to move data into tables. This process utilizes much of the same infrastructure detailed in this blog post <a href=\"https://netflixtechblog.com/data-movement-in-netflix-studio-via-data-mesh-3fddcceb1059\">here</a>.</p><p><strong>Projections have mappings. </strong>UDA automatically generates and manages mappings between the newly created data containers and the projected domain model.</p><h3>Early Adopters</h3><h4>Controlled Vocabularies (PDM)</h4><p>The full range of Netflix’s business activities relies on a sprawling data model that captures the details of our many business processes. Teams need to be able to coordinate operational activities to ensure that content production is complete, advertising campaigns are in place, and promotional assets are ready to deploy. We implicitly depend upon a singular definition of shared concepts, such as content production is complete. Multiple definitions create coordination challenges. Software (and humans) don’t know that the definitions mean the same thing.</p><p>We started the Primary Data Management (PDM) initiative to create unified and consistent definitions for the core concepts in our data model. These definitions form <strong>controlled vocabularies</strong>, standardized and governed lists for what values are permitted within certain fields in our data model.</p><p><strong>Primary Data Management (PDM) is a single place where business users can manage controlled vocabularies. </strong>Our data model governance has been scattered across different tools and teams creating coordination challenges. This is an information management problem relating to the definition, maintenance and consistent use of reference data and taxonomies. This problem is not unique to Netflix, so we looked outward for existing solutions to this problem.</p><figure><img alt=\"Screenshot of PDM UI\" src=\"https://cdn-images-1.medium.com/max/1024/0*GJMad4GU29YxPONf\" /><figcaption>Managing the taxonomy of One Piece characters in PDM.</figcaption></figure><p><strong>PDM uses the Simple Knowledge Organization System (</strong><a href=\"https://www.w3.org/TR/skos-primer\"><strong>SKOS</strong></a><strong>)</strong> <strong>model</strong>. It is a W3C data standard designed for modeling knowledge. Its terminology is abstract, with Concepts that can be organized into ConceptSchemes and properties to describe various types of relationships. Every system is hardcoded against <em>something</em>, that’s how software knows how to manipulate data. We want a system that can work with a data model as its input, so we still need <em>something</em> concrete to build the software against. This is what SKOS provides, a generic basis for modeling knowledge that our system can understand.</p><p><strong>PDM uses Domain Models to integrate SKOS into the rest of Content Engineering’s ecosystem. </strong>A core premise of the system is that it takes a domain model as input, and everything that <em>can</em> be derived <em>is</em> derived from that model. PDM builds a user interface based upon the model definition and leverages UDA to project this model into type-safe interfaces for other systems to use. The system will provision a Domain Graph Service (DGS) within our federated GraphQL API environment using a GraphQL schema that UDA projects from the domain model. UDA is also used to provision data movement pipelines which are able to feed our <a href=\"https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-5c0c1c7d7eaf\">GraphSearch</a> infrastructure as well as move data into the warehouse. The data movement systems use Avro schemas, and UDA creates a projection from the domain model to Avro.</p><p><strong>Consumers of controlled vocabularies never know they’re using SKOS. </strong>Domain models use terms that fit in with the domain. SKOS’s generic notion of <em>broader</em> and <em>narrower</em> to define a hierarchy are hidden from consumers as super-properties within the model. This allows consumers to work with language that is familiar to them while enabling PDM to work with any model. The best of both worlds.</p><h4>Operational Reporting (Sphere)</h4><p><strong>Operational reporting serves the detailed day-to-day activities and processes of a business domain.</strong> It is a reporting paradigm specialized in covering high-resolution, low-latency data sets.</p><p><strong>Operational reporting systems should generate reports without relying on technical intermediaries. </strong>Operational reporting systems need to address the persistent challenge of empowering business users to explore and obtain the data they need, when they need it. Without such self-service systems, requests for new reports or data extracts often result in back-and-forth exchanges, where the initial query may not exactly meet business users’ expectations, requiring further clarification and refinement.</p><p><strong>Data discovery and query generation are two relevant aspects of data integration. </strong>Supplying end-users with an accurate, contextual, and user-friendly data discovery experience provides a basis for query generation mechanism which produces syntactically correct and semantically reliable queries.</p><p><strong>Operational reports are predominantly run on data hydrated from GraphQL services into the Data Warehouse. </strong>You can read about our journey from conventional data movement to streaming data pipelines based on CDC and GraphQL hydration in <a href=\"https://netflixtechblog.com/data-movement-in-netflix-studio-via-data-mesh-3fddcceb1059\">this blog post</a>. Among the challenging byproducts of this approach was that a single, distinct data concept is now present in two places (GraphQL and data warehouse), with some disparity in semantic context to guide and support the interpretations and connectivity of that data. To address this, we formulate a mechanism to use the syntax and semantics captured in the federated schema from <a href=\"https://netflixtechblog.com/how-netflix-scales-its-api-with-graphql-federation-part-1-ae3557c187e2\">Netflix’s Enterprise GraphQL</a> and populate <em>representational domain models</em> in UDA to preserve those details and add more.</p><p><strong>Domain models enable the data discovery experience. </strong>Metadata aggregated from various data-producing systems is captured in UDA domain models using a unified vocabulary. This metadata is surfaced for the users’ search and discovery needs; instead of specifying exact tables and join keys, users simply can search for familiar business concepts such as ‘actors’ or ‘movies’. We use UDA models to disambiguate and resolve the intended concepts and their related data entities.</p><p><strong>UDA knowledge graph is the data landscape for query generation. </strong>Once concepts are discovered and their mappings to corresponding data containers are identified and located in the knowledge graph, we use them to establish join strategies. Through graph traversal, we identify <em>boundaries</em> and <em>islands</em> within the data landscape. This ensures only feasible, joinable combinations are selected while weeding out semantically incorrect and non-executable query candidates.</p><figure><img alt=\"Screenshot of Sphere’s UI\" src=\"https://cdn-images-1.medium.com/max/1024/0*EFEfzwY-3Tb6521O\" /><figcaption>Generating a report in Sphere.</figcaption></figure><p><strong>Sphere is a UDA-powered self-service operational reporting system. </strong>The solution based on knowledge graphs described above is called Sphere. Seeing self-service operational reporting through this lens, we can improve business users’ agency in access to operational data. They are empowered to explore, assemble, and refine reports at the conceptual level, while technical complexities are managed by the system.</p><h3>Stay Tuned</h3><p>UDA marks a fundamental shift in how we approach data modeling within Content Engineering. By providing a unified knowledge graph composed of what we know about our various data systems and the business concepts within them, we’ve made information more consistent, connected, and discoverable across our organization. We’re excited about future applications of these ideas such as:</p><ul><li>Supporting additional projections like Protobuf/gRPC</li><li>Materializing the knowledge graph of instance data for querying, profiling, and management</li><li>Finally solving some of the initial <a href=\"https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-5c0c1c7d7eaf\">challenges</a> posed by Graph Search (that actually inspired some of this work)</li></ul><p>If you’re interested in this space, we’d love to connect — whether you’re exploring new roles down the road or just want to swap ideas.</p><p>Expect to see future blog posts exploring PDM and Sphere in more detail soon!</p><h4>Credits</h4><p>Thanks to <a href=\"https://www.linkedin.com/in/andreaslegenbauer/\">Andreas Legenbauer</a>, <a href=\"https://www.linkedin.com/in/bernardo-g-4414b41/\">Bernardo Gomez Palacio Valdes</a>, <a href=\"https://www.linkedin.com/in/czhao/\">Charles Zhao</a>, <a href=\"https://www.linkedin.com/in/christopherchonguw/\">Christopher Chong</a>, <a href=\"https://www.linkedin.com/in/deepa-krishnan-593b60/\">Deepa Krishnan</a>, <a href=\"https://www.linkedin.com/in/gpesma/\">George Pesmazoglou</a>, <a href=\"https://www.linkedin.com/in/jsilvax/\">Jessica Silva</a>, <a href=\"https://www.linkedin.com/in/katherine-anderson-77074159/\">Katherine Anderson</a>, <a href=\"https://www.linkedin.com/in/malikday/\">Malik Day</a>, <a href=\"https://www.linkedin.com/in/ritabogdanovashapkina/\">Rita Bogdanova</a>, <a href=\"https://www.linkedin.com/in/ruoyunzheng/\">Ruoyun Zheng</a>, <a href=\"https://www.linkedin.com/in/shawn-s-b80821b0/\">Shawn Stedman</a>, <a href=\"https://www.linkedin.com/in/suchitagoyal/\">Suchita Goyal</a>, <a href=\"http://www.linkedin.com/in/utkarshshrivastava/\">Utkarsh Shrivastava</a>, <a href=\"https://www.linkedin.com/in/yoomikoh/\">Yoomi Koh</a>, <a href=\"https://www.linkedin.com/in/yuliashmeleva/\">Yulia Shmeleva</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6a6aee261d8d\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://netflixtechblog.com/uda-unified-data-architecture-6a6aee261d8d\">Model Once, Represent Everywhere: UDA (Unified Data Architecture) at Netflix</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "FM-Intent：使用分层多任务学习预测用户会话意图 (原标题: FM-Intent: Predicting User Session Intent with Hierarchical Multi-Task Learning)",
      "link": "https://netflixtechblog.com/fm-intent-predicting-user-session-intent-with-hierarchical-multi-task-learning-94c75e18f4b8?source=rss----2615bd06b42e---4",
      "pubDate": "Wed, 21 May 2025 16:28:07 GMT",
      "isoDate": "2025-05-21T16:28:07.000Z",
      "creator": "Netflix Technology Blog",
      "summary": "### FM-Intent：使用分层多任务学习预测用户会话意图\n\n本文介绍了Netflix开发的一种新型推荐模型——FM-Intent，旨在通过分层多任务学习预测用户会话意图，从而增强其现有的基础推荐模型（FM）的性能。\n\n#### 动机与背景\n*   推荐系统在电商、流媒体和社交网络等数字服务中至关重要，Netflix的推荐系统对产品和业务影响巨大。\n*   Netflix现有的基础模型（FM）在理解用户偏好方面已取得显著进展，但仍有机会通过整合用户意图预测来进一步提升其能力，从而提供更全面、更细致的推荐体验。\n*   现有意图预测方法通常采用简单的多任务学习，将意图预测头添加到下一项预测模型中，但未能建立任务间的层级关系。\n*   FM-Intent旨在解决这些局限性，它通过分层多任务学习增强了基础模型，利用短期和长期隐式信号捕获用户潜在的会话意图，并利用这些意图预测来改进下一项推荐。\n\n#### 主要贡献\n1.  一个新颖的推荐模型，能够捕获Netflix平台上的用户意图，并利用这些意图信息增强下一项推荐。\n2.  一种分层多任务学习方法，有效建模用户短期和长期兴趣。\n3.  全面的实验验证，表明其性能显著优于包括Netflix基础模型在内的现有最先进模型。\n\n#### 理解Netflix中的用户意图\n在Netflix生态系统中，用户意图通过各种交互元数据体现。FM-Intent利用这些隐式信号来预测用户意图和下一项推荐。\n\n*   **图1：Netflix用户参与数据概述。**\n    ![Netflix用户参与数据概述](https://cdn-images-1.medium.com/max/1024/0*3pMgS5u3TepefPLB)\n    用户意图可以与多种交互元数据相关联。我们利用各种隐式信号来预测用户意图和下一项。\n\n*   **用户意图类型示例：**\n    *   **行为类型：** 用户在Netflix上打算做什么，例如发现新内容或继续观看之前的内容（如“继续观看”）。\n    *   **类型偏好：** 用户在会话期间的内容偏好（如动作、惊悚、喜剧），这些偏好在不同会话间可能显著变化。\n    *   **电影/节目类型：** 用户是在寻找电影（通常是单次、较长的观看体验）还是电视剧（可能是多集、较短的观看时间）。\n    *   **发布时间：** 用户偏好新发布、近期（如一周到一个月内）还是常青目录内容。\n    这些维度作为潜在用户意图的代理，对于提供相关推荐至关重要。\n\n#### FM-Intent 模型架构\nFM-Intent采用分层多任务学习方法，包含三个主要组件。\n\n*   **图2：FM-Intent分层多任务学习模型架构图。**\n    ![FM-Intent分层多任务学习模型架构图](https://cdn-images-1.medium.com/max/624/1*TO3z9jPiu2QZR-xnL7erMQ.png)\n    我们使用真实意图和项目ID标签来优化预测。\n\n*   **主要组件：**\n    1.  **输入特征序列形成：** 通过结合交互元数据构建丰富的输入特征，全面表示用户行为。\n    2.  **用户意图预测：**\n        *   通过Transformer编码器处理输入特征序列，生成多个意图信号的预测。\n        *   Transformer编码器通过多头注意力机制有效建模用户长期兴趣。\n        *   通过全连接层将意图编码转换为预测分数。\n        *   创新点：基于注意力的聚合生成全面的意图嵌入，捕获不同意图信号的相对重要性。\n    3.  **基于分层多任务学习的下一项预测：**\n        *   将输入特征与用户意图嵌入结合，进行更准确的下一项推荐。\n        *   分层关系：意图预测首先进行，其结果作为下一项预测任务的输入特征。\n\n#### 离线实验结果\n在Netflix抽样用户参与数据上进行了全面的离线实验。\n\n*   **下一项和下一意图预测准确性：**\n    *   **图3：基线模型和FM-Intent在Netflix用户参与数据集上的下一项和下一意图预测结果。**\n        ![基线模型和FM-Intent在Netflix用户参与数据集上的下一项和下一意图预测结果](https://cdn-images-1.medium.com/max/1024/0*7h7aSQhq7U_heAUu)\n        所有指标均表示为相对于SOTA基线TransAct的相对百分比改进。N/A表示模型无法预测特定意图。\n    *   FM-Intent在下一项预测准确性方面比最佳基线（TransAct）有统计学上显著的7.4%改进。\n    *   大多数基线模型表现有限，因为它们无法预测或整合用户意图。\n    *   Netflix的生产模型（FM-Intent-V0）表现良好，但缺乏预测和利用用户意图的能力。\n\n#### 定性分析：用户聚类\nFM-Intent生成有意义的用户意图嵌入，可用于对具有相似意图的用户进行聚类。\n\n*   **图4：FM-Intent发现的用户意图嵌入的K-means++（K=10）聚类。**\n    ![FM-Intent发现的用户意图嵌入的K-means++（K=10）聚类](https://cdn-images-1.medium.com/max/1024/0*nBqH-ZHRXRfR3-e0)\n    FM-Intent发现了具有相似意图的独特用户群。\n*   识别出10个不同的聚类，揭示了具有独特观看模式的用户群：\n    *   主要发现新内容的用户与继续观看近期/喜爱内容的用户。\n    *   特定类型爱好者（如动漫/儿童内容观看者）。\n    *   具有特定观看模式的用户（如重复观看者与休闲观看者）。\n\n#### FM-Intent的潜在应用\nFM-Intent已成功集成到Netflix的推荐生态系统中，可用于多种下游应用：\n*   **个性化UI优化：** 预测的用户意图可指导Netflix主页的布局和内容选择。\n*   **分析和用户理解：** 意图嵌入和聚类为观看模式和偏好提供有价值的洞察。\n*   **增强推荐信号：** 意图预测可作为其他推荐模型的特征，提高其准确性和相关性。\n*   **搜索优化：** 实时意图预测根据用户当前会话意图优先排序搜索结果。\n\n#### 结论\nFM-Intent通过分层多任务学习增强了用户意图预测，代表了Netflix推荐能力的一大进步。它显著优于现有最先进模型。通过理解用户潜在意图，FM-Intent能够提供更个性化、更相关、更令人满意的推荐。",
      "shortSummary": "FM-Intent是Netflix开发的新型推荐模型，通过分层多任务学习预测用户会话意图，以增强其现有基础模型。它利用用户短期和长期隐式信号捕获潜在意图，并将其直接用于改进下一项内容推荐。实验证明，FM-Intent在下一项预测准确性上显著优于现有模型，并能识别有意义的用户意图聚类。该模型已集成到Netflix推荐生态系统，可用于个性化UI、用户分析和搜索优化，提供更精准的推荐。",
      "translated_title": "FM-Intent：使用分层多任务学习预测用户会话意图",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*3pMgS5u3TepefPLB",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/624/1*TO3z9jPiu2QZR-xnL7erMQ.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*7h7aSQhq7U_heAUu",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*nBqH-ZHRXRfR3-e0",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=94c75e18f4b8",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<p>Authors: <a href=\"https://www.linkedin.com/in/sejoon-oh/\">Sejoon Oh</a>, <a href=\"https://www.linkedin.com/in/moumitab/\">Moumita Bhattacharya</a>, <a href=\"https://www.linkedin.com/in/yesufeng/\">Yesu Feng</a>, <a href=\"https://www.linkedin.com/in/sudarshanlamkhede/\">Sudarshan Lamkhede</a>, <a href=\"https://www.linkedin.com/in/markhsiao/\">Ko-Jen Hsiao</a>, and <a href=\"https://www.linkedin.com/in/jbasilico/\">Justin Basilico</a></p><h3>Motivation</h3><p>Recommender systems have become essential components of digital services across e-commerce, streaming media, and social networks [1, 2]. At Netflix, these systems drive significant product and business impact by connecting members with relevant content at the right time [3, 4]. While our recommendation <strong>foundation model (FM)</strong> has made substantial progress in understanding user preferences through large-scale learning from interaction histories (please refer to this <a href=\"https://netflixtechblog.medium.com/foundation-model-for-personalized-recommendation-1a0bd8e02d39\"><strong><em>article</em></strong></a> about FM @ Netflix), there is an opportunity to further enhance its capabilities. By extending FM to incorporate the prediction of underlying user intents, we aim to enrich its understanding of user sessions beyond next-item prediction, thereby offering a more comprehensive and nuanced recommendation experience.</p><p>Recent research has highlighted the importance of understanding user intent in online platforms [5, 6, 7, 8]. As Xia et al. [8] demonstrated at Pinterest, predicting a user’s future intent can lead to more accurate and personalized recommendations. However, existing intent prediction approaches typically employ simple multi-task learning that adds intent prediction heads to next-item prediction models without establishing a hierarchical relationship between these tasks.</p><p>To address these limitations, we introduce <strong><em>FM-Intent</em></strong>, a novel recommendation model that enhances our foundation model through hierarchical multi-task learning. FM-Intent captures a user’s latent session intent using both short-term and long-term implicit signals as proxies, then leverages this intent prediction to improve next-item recommendations. Unlike conventional approaches, FM-Intent establishes a clear hierarchy where intent predictions directly inform item recommendations, creating a more coherent and effective recommendation pipeline.</p><p>FM-Intent makes three key contributions:</p><ol><li>A novel recommendation model that captures user intent on the Netflix platform and enhances next-item prediction using this intent information.</li><li>A hierarchical multi-task learning approach that effectively models both short-term and long-term user interests.</li><li>Comprehensive experimental validation showing significant performance improvements over state-of-the-art models, including our foundation model.</li></ol><h3>Understanding User Intent in Netflix</h3><p>In the Netflix ecosystem, user intent manifests through various interaction metadata, as illustrated in Figure 1. FM-Intent leverages these implicit signals to predict both user intent and next-item recommendations.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*3pMgS5u3TepefPLB\" /></figure><p><em>Figure 1: Overview of user engagement data in Netflix. User intent can be associated with several interaction metadata. We leverage various implicit signals to predict user intent and next-item.</em></p><p>In Netflix, there can be multiple types of user intents. For instance,</p><blockquote><strong><em>Action Type</em></strong>: Categories reflecting what users intend to do on Netflix, such as discovering new content versus continuing previously started content. For example, when a member plays a follow-up episode of something they were already watching, this can be categorized as “continue watching” intent.</blockquote><blockquote><strong><em>Genre Preference</em></strong>: The pre-defined genre labels (e.g., Action, Thriller, Comedy) that indicate a user’s content preferences during a session. These preferences can shift significantly between sessions, even for the same user.</blockquote><blockquote><strong><em>Movie/Show Type</em></strong>: Whether a user is looking for a movie (typically a single, longer viewing experience) or a TV show (potentially multiple episodes of shorter duration).</blockquote><blockquote><strong><em>Time-since-release</em></strong>: Whether the user prefers newly released content, recent content (e.g., between a week and a month), or evergreen catalog titles.</blockquote><p>These dimensions serve as proxies for the latent user intent, which is often not directly observable but crucial for providing relevant recommendations.</p><h3>FM-Intent Model Architecture</h3><p>FM-Intent employs a hierarchical multi-task learning approach with three major components, as illustrated in Figure 2.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/624/1*TO3z9jPiu2QZR-xnL7erMQ.png\" /></figure><p><em>Figure 2: An architectural illustration of our hierarchical multi-task learning model FM-Intent for user intent and item predictions. We use ground-truth intent and item-ID labels to optimize predictions.</em></p><h4>1. Input Feature Sequence Formation</h4><p>The first component constructs rich input features by combining interaction metadata. The input feature for each interaction combines categorical embeddings and numerical features, creating a comprehensive representation of user behavior.</p><h4>2. User Intent Prediction</h4><p>The intent prediction component processes the input feature sequence through a Transformer encoder and generates predictions for multiple intent signals.</p><p>The Transformer encoder effectively models the long-term interest of users through multi-head attention mechanisms. For each prediction task, the intent encoding is transformed into prediction scores via fully-connected layers.</p><p>A key innovation in FM-Intent is the attention-based aggregation of individual intent predictions. This approach generates a comprehensive intent embedding that captures the relative importance of different intent signals for each user, providing valuable insights for personalization and explanation.</p><h4>3. Next-Item Prediction with Hierarchical Multi-Task Learning</h4><p>The final component combines the input features with the user intent embedding to make more accurate next-item recommendations.</p><p>FM-Intent employs hierarchical multi-task learning where intent predictions are conducted first, and their results are used as input features for the next-item prediction task. This hierarchical relationship ensures that the next-item recommendations are informed by the predicted user intent, creating a more coherent and effective recommendation model.</p><h3>Offline Results</h3><p>We conducted comprehensive offline experiments on sampled Netflix user engagement data to evaluate FM-Intent’s performance. Note that FM-Intent uses a much smaller dataset for training compared to the FM production model due to its complex hierarchical prediction architecture.</p><h4>Next-Item and Next-Intent Prediction Accuracy</h4><p>Table 1 compares FM-Intent with several state-of-the-art sequential recommendation models, including our production model (FM-Intent-V0).</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*7h7aSQhq7U_heAUu\" /></figure><p><em>Table 1: Next-item and next-intent prediction results of baselines and our proposed method FM-Intent on the Netflix user engagement dataset.</em></p><p>All metrics are represented as relative % improvements compared to the SOTA baseline: TransAct. N/A indicates that a model is not capable of predicting a certain intent. Note that we added additional fully-connected layers to LSTM, GRU, and Transformer baselines in order to predict user intent, while we used original implementations for other baselines. FM-Intent demonstrates statistically significant improvement of 7.4% in next-item prediction accuracy compared to the best baseline (TransAct).</p><p>Most baseline models show limited performance as they either cannot predict user intent or cannot incorporate intent predictions into next-item recommendations. Our production model (FM-Intent-V0) performs well but lacks the ability to predict and leverage user intent. Note that FM-Intent-V0 is trained with a smaller dataset for a fair comparison with other models; the actual production model is trained with a much larger dataset.</p><h3>Qualitative Analysis: User Clustering</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*nBqH-ZHRXRfR3-e0\" /></figure><p><em>Figure 3: K-means++ (K=10) clustering of user intent embeddings found by FM-Intent; FM-Intent finds unique clusters of users that share the similar intent.</em></p><p>FM-Intent generates meaningful user intent embeddings that can be used for clustering users with similar intents. Figure 3 visualizes 10 distinct clusters identified through K-means++ clustering.<em> </em>These clusters reveal meaningful user segments with distinct viewing patterns:</p><ul><li>Users who primarily discover new content versus those who continue watching recent/favorite content.</li><li>Genre enthusiasts (e.g., <em>anime/kids content viewers</em>).</li><li>Users with specific viewing patterns (e.g., <em>Rewatchers</em> versus <em>casual viewers</em>).</li></ul><h3>Potential Applications of FM-Intent</h3><p>FM-Intent has been successfully integrated into Netflix’s recommendation ecosystem, can be leveraged for several downstream applications:</p><blockquote><strong>Personalized UI Optimization</strong>: The predicted user intent could inform the layout and content selection on the Netflix homepage, emphasizing different rows based on whether users are in discovery mode, continue-watching mode, or exploring specific genres.</blockquote><blockquote><strong>Analytics and User Understanding</strong>: Intent embeddings and clusters provide valuable insights into viewing patterns and preferences, informing content acquisition and production decisions.</blockquote><blockquote><strong>Enhanced Recommendation Signals</strong>: Intent predictions serve as features for other recommendation models, improving their accuracy and relevance.</blockquote><blockquote><strong>Search Optimization</strong>: Real-time intent predictions help prioritize search results based on the user’s current session intent.</blockquote><h3>Conclusion</h3><p>FM-Intent represents an advancement in Netflix’s recommendation capabilities by enhancing them with hierarchical multi-task learning for user intent prediction. Our comprehensive experiments demonstrate that FM-Intent significantly outperforms state-of-the-art models, including our prior foundation model that focused solely on next-item prediction. By understanding not just what users might watch next but what underlying intents users have, we can provide more personalized, relevant, and satisfying recommendations.</p><h3>Acknowledgements</h3><p>We thank our stunning colleagues in the Foundation Model team &amp; AIMS org. for their valuable feedback and discussions. We also thank our partner teams for getting this up and running in production.</p><h3>References</h3><p>[1] Amatriain, X., &amp; Basilico, J. (2015). Recommender systems in industry: A netflix case study. In Recommender systems handbook (pp. 385–419). Springer.</p><p>[2] Gomez-Uribe, C. A., &amp; Hunt, N. (2015). The netflix recommender system: Algorithms, business value, and innovation. ACM Transactions on Management Information Systems (TMIS), 6(4), 1–19.</p><p>[3] Jannach, D., &amp; Jugovac, M. (2019). Measuring the business value of recommender systems. ACM Transactions on Management Information Systems (TMIS), 10(4), 1–23.</p><p>[4] Bhattacharya, M., &amp; Lamkhede, S. (2022). Augmenting Netflix Search with In-Session Adapted Recommendations. In Proceedings of the 16th ACM Conference on Recommender Systems (pp. 542–545).</p><p>[5] Chen, Y., Liu, Z., Li, J., McAuley, J., &amp; Xiong, C. (2022). Intent contrastive learning for sequential recommendation. In Proceedings of the ACM Web Conference 2022 (pp. 2172–2182).</p><p>[6] Ding, Y., Ma, Y., Wong, W. K., &amp; Chua, T. S. (2021). Modeling instant user intent and content-level transition for sequential fashion recommendation. IEEE Transactions on Multimedia, 24, 2687–2700.</p><p>[7] Liu, Z., Chen, H., Sun, F., Xie, X., Gao, J., Ding, B., &amp; Shen, Y. (2021). Intent preference decoupling for user representation on online recommender system. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence (pp. 2575–2582).</p><p>[8] Xia, X., Eksombatchai, P., Pancha, N., Badani, D. D., Wang, P. W., Gu, N., Joshi, S. V., Farahpour, N., Zhang, Z., &amp; Zhai, A. (2023). TransAct: Transformer-based Realtime User Action Model for Recommendation at Pinterest. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (pp. 5249–5259).</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=94c75e18f4b8\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://netflixtechblog.com/fm-intent-predicting-user-session-intent-with-hierarchical-multi-task-learning-94c75e18f4b8\">FM-Intent: Predicting User Session Intent with Hierarchical Multi-Task Learning</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "幕后：构建强大的广告事件处理管道 (原标题: Behind the Scenes: Building a Robust Ads Event Processing Pipeline)",
      "link": "https://netflixtechblog.com/behind-the-scenes-building-a-robust-ads-event-processing-pipeline-e4e86caf9249?source=rss----2615bd06b42e---4",
      "pubDate": "Fri, 09 May 2025 19:44:04 GMT",
      "isoDate": "2025-05-09T19:44:04.000Z",
      "creator": "Netflix Technology Blog",
      "summary": "## Netflix 广告事件处理管道的演进\n\n### 引言\n在数字广告平台中，一个强大的反馈系统对于广告活动的生命周期和成功至关重要。Netflix 着手构建了一个能够满足当前需求并可扩展以应对未来需求的强大事件处理平台。文章将广告服务比作“大脑”，而广告事件则是“心跳”，持续提供实时反馈，以支持更好的决策、优化、报告、衡量和计费。\n\n### 试点阶段 (2022年11月)\nNetflix 与微软合作推出了全新的基础广告计划。最初的系统设计理念是简单、安全、高效，并以设备发起和服务器代理操作为基础。该系统由以下三个主要组件构成：\n\n*   **Microsoft Ad Server**\n*   **Netflix Ads Manager**\n*   **Ad Event Handler**\n\n该系统旨在有效跟踪每个广告的展示、频次限制和货币化过程。其关键功能包括：\n\n*   **客户端请求**：设备从 Netflix 播放系统请求广告，广告管理器会添加信息后向广告服务器请求广告。\n*   **服务器端广告插入**：广告服务器使用 VAST (Video Ad Serving Template) 格式发送广告响应。\n*   **Netflix Ads Manager**：解析 VAST 文档，提取跟踪事件信息，并创建简化的响应结构。跟踪信息被打包成结构化的 protobuf 数据模型，并加密为不透明令牌，告知客户端何时发送事件及对应的令牌。\n*   **客户端设备**：在广告播放期间，客户端设备发送带有令牌的事件，Netflix 遥测系统将其排队到 Kafka 进行异步处理。\n*   **Ads Event Handler**：作为 Kafka 消费者，读取/解密事件负载，并将编码的跟踪信息转发给广告服务器和其他供应商。\n\n![图片 1：基本广告事件处理系统](https://cdn-images-1.medium.com/max/1024/1*S_6xv6LxoRyL8KtUXWueP.png)\n\n### 扩展阶段\n随着第三方广告供应商（用于衡量、跟踪和验证）的不断增加，Netflix 发现不透明令牌中封装的数据量持续增长。这些缓存在客户端设备上的令牌可能导致内存使用量升高，影响设备性能。为应对这些挑战，Netflix 引入了一个新的持久层——**Ads Metadata Registry**（广告元数据注册表），作为广告服务和事件处理系统之间的键值抽象存储服务。\n\n*   **机制**：该服务存储每个已投放广告的元数据。当事件回调时，事件处理器会读取跟踪信息并将其转发给供应商。客户端设备和广告系统之间的契约继续使用每个事件的不透明令牌，但现在令牌中包含的是引用标识符（广告ID、注册表中对应的元数据记录ID和事件名称），而非完整的跟踪信息。\n*   **优势**：这种方法使系统能够应对未来数据量的增长。\n\n![图片 2：广告服务与报告之间的存储服务](https://cdn-images-1.medium.com/max/1024/1*0wP5cyAj84Vju7ryabJE1A.png)\n\n### 演进阶段 (2024年1月)\nNetflix 决定投资自研广告技术平台，这意味着事件处理管道必须进行重大演进，以实现与现有产品的对等，并支持使用 Netflix 自研广告服务器进行新产品的快速迭代。这需要重新评估整个广告工程团队的架构。\n\n**需要支持的用例包括：**\n\n*   通过 Netflix 广告服务器支持自研广告的频次限制。\n*   纳入展示的定价信息，为计费事件奠定基础。\n*   一个强大的报告系统，用于与广告商共享活动报告，并结合指标数据收集，评估活动的交付和效果。\n*   扩展事件处理器以执行跨不同供应商的跟踪信息查找。\n\n**战略规划：**\n\n1.  **集中式广告事件收集系统 (Ads Event Publisher)**：该系统将常见的操作（如令牌解密、丰富、哈希标识符）整合到单一执行步骤中，并向消费者提供一个高度可扩展（例如与广告服务器和广告媒体无关）的统一数据契约。\n2.  将所有广告遥测的消费者移至集中式服务下游，在广告工程中实现上游系统和消费者之间的清晰分离。\n3.  广告会话化过程被定位在原始广告事件的下游。\n4.  所有用于广告报告/分析/指标的广告数据处理管道都将使用由集中式系统发布的数据。\n\n![图片 3：广告事件处理管道](https://cdn-images-1.medium.com/max/1024/1*aPL3RHeFEzlw_psaLddWKw.png)\n\n**演进后管道的关键组件：**\n\n*   **Ads Event Publisher**：负责收集广告遥测数据，并向广告工程团队提供统一的广告事件，支持衡量、财务/计费、报告、频次限制以及向广告服务器提供关键反馈循环等功能。\n*   **实时消费者**：\n    *   **Frequency Capping**：跟踪每个广告活动、用户画像和任何其他设定的频次限制参数，供广告服务器在每次广告决策时使用。\n    *   **Ads Metrics**：一个 Flink 作业，将原始数据转换为一系列维度和指标，随后写入 Apache Druid OLAP 数据库，提供实时指标并应用预算限制功能。\n    *   **Ads Sessionizer**：一个 Apache Flink 作业，将与单个广告相关的所有事件整合为广告会话，提供实时广告播放信息和重要的业务洞察及报告。\n    *   **Ads Event Handler**：持续向广告供应商发送信息。\n*   **Billing/Revenue**：离线工作流，用于整理展示数据，支持计费和收入确认流程。\n*   **Ads Reporting & Metrics**：为客户经理提供报告模块，并提供一个集中式指标 API，帮助评估广告活动的交付情况。\n\n### 结论\n这些系统显著加速了 Netflix 推出新业务功能的能力。通过与微软的合作，展示广告事件被整合到新的管道中，确保了在通过 Netflix 广告系统发布时覆盖所有用例。程序化购买能力现在支持大量跟踪器和展示事件上的动态出价。分享退出信号有助于确保欧洲广告业务的隐私和 GDPR 合规性。新的事件类型，如广告点击和二维码扫描事件，也通过该管道流动，确保所有指标和报告得到一致跟踪。\n\n### 关键启示\n\n*   **战略性、增量式演进**：广告事件处理系统的开发是一个精心策划的旅程，每次迭代都通过解决现有挑战、预测未来需求以及展示团队合作、规划和协调来精心规划。\n*   **数据契约**：清晰的数据契约对于确保系统间解释的一致性和互操作性至关重要。通过标准化数据模型和建立广告服务与集中事件收集之间清晰的数据交换，团队能够以极快的速度迭代并按时交付许多发布。\n*   **关注点分离**：消费者无需理解每个广告遥测源或管理更新和迁移。相反，集中式系统处理这些任务，使消费者能够专注于其核心业务逻辑。\n\nNetflix 未来还有一系列令人兴奋的项目，包括管理 Netflix 直播流上的广告事件、去重过程、丰富数据信号以提供增强的报告和洞察。此外，还在推进原生广告策略，集成转化 API 以改进转化跟踪等。",
      "shortSummary": "Netflix 详细阐述了其广告事件处理管道的构建与演进历程。该系统最初是一个与微软合作的试点项目，旨在简单高效地处理广告事件。随着数据量增长和业务需求变化，Netflix 引入了元数据注册服务以优化数据处理。最终，该平台演进为自研的集中式系统，统一了广告事件的收集、处理和分发，以支持频次限制、实时指标、会话化、计费和报告等多种关键功能，确保了系统的可扩展性、效率和未来适应性。",
      "translated_title": "幕后：构建强大的广告事件处理管道",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*S_6xv6LxoRyL8KtUXqWueQ.png",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*0wP5cyAj84Vju7ryabJE1A.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*aPL3RHeFEzlw_psaLddWKw.png",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e4e86caf9249",
          "alt": "",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<p><a href=\"https://www.linkedin.com/in/kineshsatiya/\">Kinesh Satiya</a></p><h4>Introduction</h4><p>In a digital advertising platform, a robust feedback system is essential for the lifecycle and success of an ad campaign. This system comprises of diverse sub-systems designed to monitor, measure, and optimize ad campaigns. At Netflix, we embarked on a journey to build a robust event processing platform that not only meets the current demands but also scales for future needs. This blog post delves into the architectural evolution and technical decisions that underpin our Ads event processing pipeline.</p><p>Ad serving acts like the “brain” — making decisions, optimizing delivery and ensuring right Ad is shown to the right member at the right time. Meanwhile, ad events, after an Ad is rendered, function like “heartbeats”, continuously providing real-time feedback (oxygen/nutrients) that fuels better decision-making, optimizations, reporting, measurement, and billing. Expanding on this analogy:</p><ul><li>Just as the brain relies on continuous blood flow, ad serving depends on a steady stream of ad events to adjust next ad serving decision, frequency capping, pacing, and personalization.</li><li>If the nervous system stops sending signals (ad events stop flowing), the brain (ad serving) lacks critical insights and starts making poor decisions or even fails.</li><li>The healthier and more accurate the event stream (just like strong heart function), the better the ad serving system can adapt, optimize, and drive business outcomes.</li></ul><p>Let’s dive into the journey of building this pipeline.</p><h4>The Pilot</h4><p>In November 2022, we launched a brand <a href=\"https://about.netflix.com/en/news/announcing-basic-with-ads-us\">new basic ads plan</a>, in partnership with Microsoft. The software systems extended the existing Netflix playback systems to play ads. Initially, the system was designed to be simple, secure, and efficient, with an underlying ethos of device-originated and server-proxied operations. The system consisted of three main components: the Microsoft Ad Server, Netflix Ads Manager, and Ad Event Handler. Each ad served required tracking to ensure the feedback loop functioned effectively, providing the external ad server with insights on impressions, frequency capping (advertiser policy that limits the number of times a user sees a specific ad), and monetization processes.</p><p>Key features of this system include:</p><ol><li><strong>Client Request: </strong>Client devices request for ads during an ad break from Netflix playback systems, which is then decorated with information by ads manager to request ads from the ad server.</li><li><strong>Server-Side Ad Insertion:</strong> The Ad Server sends ad responses using the VAST (Video Ad Serving Template) format.</li><li><strong>Netflix Ads Manager:</strong> This service parses VAST documents, extracts tracking event information, and creates a simplified response structure for Netflix playback systems and client devices. <br> — The tracking information is packed into a structured protobuf data model.<br> — This structure is encrypted to create an opaque token.<br> — The final response, informs the client devices, when to send an event and the corresponding token.</li><li><strong>Client Device:</strong> During ad playback, client devices send events accompanied by a token. The Netflix telemetry system then enqueues all these events in Kafka for asynchronous processing.</li><li><strong>Ads Event Handler:</strong> This component is a Kafka consumer, that reads/decrypts the event payload and forwards the tracking information encoded back to the ad server and other vendors.</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*S_6xv6LxoRyL8KtUXqWueQ.png\" /><figcaption>Fig 1: Basic Ad Event Handling System</figcaption></figure><p>There is an <a href=\"https://netflixtechblog.com/ensuring-the-successful-launch-of-ads-on-netflix-f99490fdf1ba\">excellent prior blog</a> post that explains how this systems was tested end-to-end at scale. This system design allowed us to quickly add new integrations for verification with vendors like DV, IAS and Nielsen for measurement.</p><h4>The Expansion</h4><p>As we continued to expand our third-party (3P) advertising vendors for measurement, tracking and verification, we identified a critical trend: growth in the volume of data encapsulated within opaque tokens. These tokens, which are cached on client devices, present a risk of elevated memory usage, potentially impacting device performance. We also anticipated increase in third-party tracking URLs, metadata needs, and more event types as our business added new capabilities.</p><p>To strategically address these challenges, we introduced a new persistence layer using <a href=\"https://netflixtechblog.com/introducing-netflixs-key-value-data-abstraction-layer-1ea8a0a11b30\">Key-Value abstraction</a>, between ad serving and event handling system: Ads Metadata Registry. This transient storage service stores metadata for each Ad served, and upon callback, event handler would read the tracking information to relay information to the vendors. The contract between the client device and Ads systems continues to use the opaque token per event, but now, instead of tracking information, it contains reference identifiers — Ad ID, the corresponding metadata record ID in the registry and the event name. This approach future proofed our systems to handle any growth in data that needs to pass from ad serving to event handling systems.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*0wP5cyAj84Vju7ryabJE1A.png\" /><figcaption>Fig 2: Storage service between Ad Serving &amp; Reporting</figcaption></figure><h4>The Evolution</h4><p>In January of 2024, we decided to invest in in-house advertising technology platform. This implied that the event processing pipeline had to evolve significantly — attain parity with existing offerings and continue to support new product launches with rapid iterations using in-house Netflix Ad Server. This required re-evaluation of the entire architecture across all of Ads engineering teams.</p><p>First, we made an inventory of the use-cases that would need to be supported through ad events.</p><ol><li>We’d need to start supporting frequency capping in-house for all ads through Netflix Ad server.</li><li>Incorporate pricing information for impressions to set the stage for billing events, which are used to charge advertisers.</li><li>A robust reporting system to share campaign reports with advertisers, combined with metrics data collection, helps assess the delivery and effectiveness of the campaign.</li><li>Scale event handler to perform tracking information look-ups across different vendors.</li></ol><p>Next, we examined upcoming launches, such as Pause/Display ads, to gain deeper insights into our strategic initiatives. We recognized that Display Ads would utilize a distinct logging framework, suggesting that different upstream pipelines might deliver ad telemetry. However, the downstream use-cases were expected to remain largely consistent. Additionally, by reviewing the goals of our telemetry teams, we saw large initiatives aimed at upgrading the platform, indicating potential future migrations.</p><p>Keeping the above insights &amp; challenges in mind,</p><ul><li>We planned a centralized ad event collection system. This centralized service would consolidate common operations like decryption of tokens, enrichment, hashing identifiers into a single step execution and provide a single unified data contract to consumers that is highly extensible (like being agnostic to ad server &amp; ad media).</li><li>We proposed moving all consumers of ad telemetry downstream of the centralized service. This creates a clean separation between upstream systems and consumers in Ads Engineering.</li><li>In the initial development phase of our advertising system, a crucial component was the creation of ad sessions based on individual ad events. This system was constructed using ad playback telemetry, which allowed us to gather essential metrics from these ad sessions. A significant decision in this plan was to position the ad sessionization process downstream of the raw ad events.</li><li>The proposal also recommended moving all our Ads data processing pipelines for reporting/analytics/metrics for Ads using the data published by the centralized system.</li></ul><p>Putting together all the components in our vision -</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*aPL3RHeFEzlw_psaLddWKw.png\" /><figcaption>Fig 3: Ad Event processing pipeline</figcaption></figure><p>Key components on event processing pipeline -</p><p><strong>Ads Event Publisher:</strong> This centralized system is responsible for collecting ads telemetry and providing unified ad events to the ads engineering teams. It supports various functions such as measurement, finance/billing, reporting, frequency capping, and maintaining an essential feedback loop back to the ad server.</p><p><strong>Realtime Consumers</strong></p><ol><li><strong>Frequency Capping: </strong>This system tracks impressions for each campaign, profile, and any other frequency capping parameters set up for the campaign. It is utilized by the Ad Server during each ad decision to ensure ads are served with frequency limits.</li><li><strong>Ads Metrics: </strong>This component is a Flink job that transforms raw data to a set of dimensions and metrics, subsequently writing to Apache Druid OLAP database. The streaming data is further backed by an offline process that corrects any inaccuracy during streaming ingestion and providing accurate metrics. It provides real-time metrics to assess the delivery health of campaigns and applies budget capping functionality.</li><li><strong>Ads Sessionizer: </strong>An Apache Flink job that consolidates all events related to a single ad into an Ad Session. This session provides real-time information about ad playback, offering essential business insights and reporting. It is a crucial job that supports all downstream analytical and reporting processes.</li><li><strong>Ads Event Handler: </strong>This service continuously sends information to ad vendors by reading tracking information from ad events, ensuring accurate and timely data exchange.</li></ol><p><strong>Billing/Revenue: </strong>These are offline workflows designed to curate impressions, supporting billing and revenue recognition processes.</p><p><strong>Ads Reporting &amp; Metrics: </strong>This service powers reporting module for our account managers and provides a centralized metrics API that help assess the delivery of a campaign.</p><p>This was a massive multi-quarter effort across different engineering teams. With extensive planning (kudos to our TPM team!) and coordination, we were able to iterate fast, build several services and execute the vision above, to power our in-house ads technology platform.</p><h4>Conclusion</h4><p>These systems have significantly accelerated our ability to launch new capabilities for the business.</p><ul><li>Through our partnership with Microsoft, Display Ad events were integrated into the new pipeline for reusability and ensuring when launching through Netflix ads systems, all use-cases were covered.</li><li>Programmatic buying capabilities now support the exchange of numerous trackers and dynamic bid prices on impression events.</li><li>Sharing opt-out signals helps ensure privacy and compliance with GDPR regulations for Ads business in Europe, supporting accurate reporting and measurement.</li><li>New event types like Ad clicks and scanning of QR codes events also flow through the pipeline, ensuring all metrics and reporting are tracked consistently.</li></ul><p><strong>Key Takeways</strong></p><ul><li><strong>Strategic, incremental evolution:</strong> The development of our ads event processing systems has been a carefully orchestrated journey. Each iteration was meticulously planned by addressing existing challenges, anticipating future needs, and showcasing teamwork, planning, and coordination across various teams. These pillars have been fundamental to the success of this journey.</li><li><strong>Data contract:</strong> A clear data contract has been pivotal in ensuring consistency in interpretation and interoperability across our systems. By standardizing the data models, and establishing a clear data exchange between ad serving, and centralized event collection, our teams have been able to iterate at exceptional speed and continue to deliver many launches on time.</li><li><strong>Separation of concerns: </strong>Consumers are relieved from the need to understand each source of ad telemetry or manage updates and migrations. Instead, a centralized system handles these tasks, allowing consumers to focus on their core business logic.</li></ul><p>We have an exciting list of projects on the horizon. These include managing ad events from ads on Netflix live streams, de-duplication processes, and enriching data signals to deliver enhanced reporting and insights. Additionally, we are advancing our Native Ads strategy, integrating Conversion API for improved conversion tracking, among many others.</p><p>This is definitely not a season finale; it’s just the beginning of our journey to create a best-in-class ads technology platform. We warmly invite you to share your thoughts and comments with us. If you’re interested in learning more or becoming a part of this innovative journey, <a href=\"https://jobs.netflix.com/\">Ads Engineering is hiring</a>!</p><h4><em>Acknowledgements</em></h4><p><em>A special thanks to our amazing colleagues and teams who helped build our foundational post-impression system: </em><a href=\"https://www.linkedin.com/in/simonspencer1/\"><em>Simon Spencer</em></a><em>, </em><a href=\"https://www.linkedin.com/in/priyankaavj/\"><em>Priyankaa Vijayakumar,</em></a><em> </em><a href=\"https://www.linkedin.com/in/indrajit-roy-choudhury-5b011754/\"><em>Indrajit Roy Choudhury</em></a><em>; Ads TPM team — </em><a href=\"https://www.linkedin.com/in/sonyabellamy/\"><em>Sonya Bellamy</em></a><em>; the Ad Serving Team —</em><a href=\"https://www.linkedin.com/in/andrewjsweeney/\"><em> Andrew Sweeney</em></a><em>, </em><a href=\"https://www.linkedin.com/in/tim-z-b9112034/\"><em>Tim Zheng</em></a>, <a href=\"https://www.linkedin.com/in/haidongt/\"><em>Haidong Tang</em></a><em> and </em><a href=\"https://www.linkedin.com/in/edhbarker/\"><em>Ed Barker</em></a><em>; the Ads Data Engineering Team — </em><a href=\"https://www.linkedin.com/in/sonalisharma/\"><em>Sonali Sharma</em></a><em>, </em><a href=\"https://www.linkedin.com/in/harshavardhan-arepalli-3a9b0992/\"><em>Harsha Arepalli</em></a><em>, and </em><a href=\"https://www.linkedin.com/in/winifredtran/\"><em>Wini Tran</em></a><em>; Product Data Systems — </em><a href=\"https://www.linkedin.com/in/d3cay/\"><em>David Klosowski;</em></a><em> and the entire Ads Reporting and Measurement team!</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e4e86caf9249\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://netflixtechblog.com/behind-the-scenes-building-a-robust-ads-event-processing-pipeline-e4e86caf9249\">Behind the Scenes: Building a Robust Ads Event Processing Pipeline</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "衡量Netflix内容的对话清晰度 (原标题: Measuring Dialogue Intelligibility for Netflix Content)",
      "link": "https://netflixtechblog.com/measuring-dialogue-intelligibility-for-netflix-content-58c13d2a6f6e?source=rss----2615bd06b42e---4",
      "pubDate": "Thu, 08 May 2025 00:40:17 GMT",
      "isoDate": "2025-05-08T00:40:17.000Z",
      "creator": "Netflix Technology Blog",
      "summary": "### 引言：提升会员体验\n\nNetflix致力于为会员提供最佳体验，深知这离不开与技术伙伴的紧密合作。通过结合外部专业知识与Netflix的创意和运营洞察，双方共同探索新想法、开发实用工具，并推动技术边界以服务于故事讲述。这种合作不仅赋能创作者，也帮助Netflix为会员创新。其中一个重要领域便是改善从拍摄到屏幕的对话清晰度，Netflix称之为“对话完整性管道”（Dialogue Integrity Pipeline）。\n\n### 对话完整性管道：挑战与痛点\n\n观众在观看内容时，常常会遇到难以听清对话的情况，这会严重影响观影体验。制作复杂的现代影视作品需要巨大的艺术和技术投入，而对话清晰度可能在“对话完整性管道”的多个环节出现问题，导致观众难以理解。这些问题包括：\n\n*   自然主义的表演风格、多样的语速和口音。\n*   嘈杂的拍摄地点、麦克风放置不当。\n*   电影化（高动态范围）的混音风格、过度的对话处理、不合格的设备。\n*   通过分发管道产生的音频妥协。\n*   电视扬声器不足、家庭环境噪音。\n\n解决这些问题对于维护Netflix内容应有的卓越标准至关重要。\n\n### 大规模测量：量化对话清晰度\n\nNetflix利用行业标准响度计来测量内容并确保其符合核心响度规范。这些工具也提供关于音频动态范围（从响到轻）的反馈，这会影响对话清晰度。Netflix的音频算法团队希望进一步深化这些测量，并对给定内容的整个运行时长内的对话清晰度形成全面的理解。\n\n该团队开发了一个基于短时客观清晰度（STOI）指标的语音清晰度测量系统。首先，一个语音活动检测器分析对话主音轨以识别语音片段，然后将其与混音中的非语音声音（通常是音乐和效果）进行比较。接着，系统计算每个语音频段的信噪比，结果以每句对话0到1.0的范围进行简洁总结，以量化竞争性音乐和效果对听众的干扰程度。\n\n*   **图片 1**：此图展示了eSTOI（扩展短时客观清晰度）方法如何测量对话（图中fg [前景] 音轨）与非语音（图中bg [背景] 音轨）的对比，以根据竞争性非语音声音判断清晰度。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/0*WSViFfuvT8pcZshi)\n\n### 交付前优化对话：合作与工具\n\n了解Netflix内容中的对话清晰度至关重要，但Netflix的使命不止于分析——它致力于赋能创作者，使其能够制作出在家中与观众无缝共鸣的混音。鉴于数字音频工作站（DAW）缺乏专用的对话清晰度测量插件，Netflix与行业领导者Fraunhofer数字媒体技术研究所（Fraunhofer IDMT）和Nugen Audio合作，开创了一种解决方案，以增强创意控制并确保从混音到最终交付的对话清晰度。\n\nNetflix与Fraunhofer IDMT合作，将其基于机器学习的语音清晰度解决方案适配为跨平台插件标准，并引入Nugen Audio开发DAW兼容插件。\n\n*   **Fraunhofer IDMT的贡献**\n    Fraunhofer听觉、语音和音频技术部门（HSA）在测量语音清晰度的媒体处理工具方面进行了大量研究和开发。2020年，其基于机器学习的方法被集成到Steinberg的Nuendo数字音频工作站中。Netflix向Fraunhofer工程团队提出合作建议，希望通过跨平台VST（虚拟工作室技术）和AAX（Avid音频扩展）插件标准，使他们的技术能够被其他音频工作站使用。科学家们对该项目充满热情，并提供了他们的对话清晰度库。\n\n    *   **图片 2**：Fraunhofer IDMT对话清晰度测量仪集成到Steinberg Nuendo数字音频工作站中。\n\n    ![图片 2](https://cdn-images-1.medium.com/max/1024/0*wuapXe2lajcx3tTj)\n\n*   **Nugen Audio的贡献**\n    Nugen Audio创建了VisLM插件，为声音团队提供了一种高效准确的方法来测量混音是否符合传统的广播和流媒体规范——总混音响度、对话响度和真实峰值。此后，VisLM已成为全球后期制作行业广泛使用的工具。Nugen Audio与Fraunhofer合作，将Fraunhofer IDMT对话清晰度库集成到一个全新的行业首创工具——Nugen DialogCheck中。该工具为重新录音混音师提供实时洞察，帮助他们在混音过程中最关键的时刻调整对话清晰度，确保每一个字都清晰可懂。\n\n    *   **图片 3**：Nugen DialogCheck插件界面。\n\n    ![图片 3](https://cdn-images-1.medium.com/max/1024/0*gGt-DpKR806J2jqT)\n\n### 通过合作实现更清晰的对话\n\n制作清晰的对话不仅仅是技术挑战，更是一门需要持续创新和强大行业合作的艺术。为了赋能创作者，Netflix及其合作伙伴正在将先进的清晰度测量工具直接嵌入到DAW中，使声音团队能够：\n\n*   在混音早期检测并解决对话清晰度问题。\n*   在不损害艺术意图的情况下微调语音清晰度。\n*   在任何收听环境下，为每位观众提供沉浸式、无障碍的故事体验。\n\nNetflix致力于推动音频卓越的边界，从创新大规模清晰度测量到与Fraunhofer和Nugen Audio合作开发DialogCheck插件等尖端工具，Netflix正在为对话清晰度设定新标准——确保每一个字都如创作者所愿被听到。创新并非孤立发生。通过与合作伙伴携手合作，Netflix可以继续突破可能的极限，激发创造力，推动故事讲述的未来。",
      "shortSummary": "Netflix与Fraunhofer IDMT和Nugen Audio合作，旨在提升其内容的对话清晰度。通过开发基于STOI的语音清晰度测量系统，并将其集成到DAW插件（如Nugen DialogCheck）中，Netflix使创作者能在混音阶段早期识别并解决对话模糊问题。这不仅确保了声音团队在不影响艺术意图的前提下优化对话，也为全球观众提供了更沉浸、易懂的观影体验，体现了Netflix对音频卓越和行业协作的承诺。",
      "translated_title": "衡量Netflix内容的对话清晰度",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*WSViFfuvT8pcZshi",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*wuapXe2lajcx3tTj",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*gGt-DpKR806J2jqT",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=58c13d2a6f6e",
          "alt": "",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<p><em>Enhancing Member Experience Through Strategic Collaboration</em></p><p><a href=\"https://www.linkedin.com/in/ozziesutherland/\">Ozzie Sutherland</a>, <a href=\"https://www.linkedin.com/in/iroroorife/\">Iroro Orife</a>, <a href=\"https://www.linkedin.com/in/chih-wei-wu-73081689/\">Chih-Wei Wu</a>, <a href=\"https://www.linkedin.com/in/bhanusrikanth/\">Bhanu Srikanth</a></p><p>At Netflix, delivering the best possible experience for our members is at the heart of everything we do, and we know we can’t do it alone. That’s why we work closely with a diverse ecosystem of technology partners, combining their deep expertise with our creative and operational insights. Together, we explore new ideas, develop practical tools, and push technical boundaries in service of storytelling. This collaboration not only empowers the talented creatives working on our shows with better tools to bring their vision to life, but also helps us innovate in service of our members. By building these partnerships on trust, transparency, and shared purpose, we’re able to move faster and more meaningfully, always with the goal of making our stories more immersive, accessible, and enjoyable for audiences everywhere. One area where this collaboration is making a meaningful impact is in improving dialogue intelligibility, from set to screen. We call this the Dialogue Integrity Pipeline.</p><h4>Dialogue Integrity Pipeline</h4><p>We’ve all been there, settling in for a night of entertainment, only to find ourselves straining to catch what was just said on screen. You’re wrapped up in the story, totally invested, when suddenly a key line of dialogue vanishes into thin air. “Wait, what did they say? I can’t understand the dialogue! What just happened?”</p><p>You may pick up the remote and rewind, turn up the volume, or try to stay with it and hope this doesn’t happen again. Creating sophisticated, modern series and films requires an incredible artistic &amp; technical effort. At Netflix, we strive to ensure those great stories are easy for the audience to enjoy. Dialogue intelligibility can break down at multiple points in what we call the <strong>Dialogue Integrity Pipeline</strong>, the journey from on-set capture to final playback at home. Many facets of the process can contribute to dialogue that’s difficult to understand:</p><ul><li>Naturalistic acting styles, diverse speech patterns, and accents</li><li>Noisy locations, microphone placement problems on set</li><li>Cinematic (high dynamic range) mixing styles, excessive dialogue processing, substandard equipment</li><li>Audio compromises through the distribution pipeline</li><li>TVs with inadequate speakers, noisy home environments</li></ul><p>Addressing these issues is critical to maintaining the standard of excellence our content deserves.</p><h4>Measurement at Scale</h4><p>Netflix utilizes industry-standard loudness meters to measure content and its adherence to our core loudness specifications. This tool also provides feedback on audio dynamic range (loud to soft) which impacts dialogue intelligibility. The Audio Algorithms team at Netflix wanted to take these measurements further and develop a holistic understanding of dialogue intelligibility throughout the runtime of a given title.</p><p>The team developed a Speech Intelligibility measurement system based on the Short-time Objective Intelligibility (STOI) metric [<a href=\"https://www.researchgate.net/profile/Cees-Taal/publication/224219052_An_Algorithm_for_Intelligibility_Prediction_of_Time-Frequency_Weighted_Noisy_Speech/links/0deec51da9fbbc5eea000000/An-Algorithm-for-Intelligibility-Prediction-of-Time-Frequency-Weighted-Noisy-Speech.pdf\">Taal et al.</a> (IEEE <em>Transactions on Audio, Speech, and Language Processing</em>)]. Firstly, a speech activity detector analyses the dialogue stem to render speech utterances, which are then compared to non-speech sounds in the mix, typically Music and Effects. Then the system calculates the Signal-to-Noise ratio, in each speech frequency band, the results of which are summarized succinctly, per-utterance on the range [0, 1.0], to quantify the degree to which competing Music and Effects can distract the listener.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*WSViFfuvT8pcZshi\" /><figcaption>This chart shows how eSTOI (extended Short-Time Objective Intelligibility) method measures dialogue (fg [foreground] stem in the graphic) against non-speech (bg [background] stem in the graphic) to judge intelligibility based on competing non-speech sound.</figcaption></figure><h4>Optimizing Dialogue Prior to Delivery</h4><p>Understanding dialogue intelligibility across Netflix titles is invaluable, but our mission goes beyond analysis — we strive to empower creators with the tools to craft mixes that resonate seamlessly with audiences at home.</p><p>Seeing the lack of dedicated Dialogue Intelligibility Meter plugins for Digital Audio Workstations, we teamed up with industry leaders, Fraunhofer Institute for Digital Media Technology IDMT (Fraunhofer IDMT) and Nugen Audio to pioneer a solution that enhances creative control and ensures crystal-clear dialogue from mix to final delivery.</p><p>We collaborated with Fraunhofer IDMT to adapt their machine-learning-based speech intelligibility solution for cross-platform plugin standards and brought in Nugen Audio to develop DAW-compatible plugins.</p><h4>Fraunhofer IDMT</h4><p>The Fraunhofer Department of Hearing, Speech, and Audio Technology HSA has done significant research and development on media processing tools that measure speech intelligibility. In 2020, the machine learning-based method was integrated into Steinberg’s Nuendo Digital Audio Workstation. We approached the Fraunhofer engineering team with a collaboration proposal to make their technology accessible to other audio workstations through the cross-platform VST (Virtual Studio Technology) and AAX (Avid Audio Extension) plugin standards. The scientists were keen on the project and provided their dialogue intelligibility library.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*wuapXe2lajcx3tTj\" /><figcaption>The Fraunhofer IDMT Dialogue Intelligibility Meter integrated into the Steinberg Nuendo Digital Audio Workstation.</figcaption></figure><h4>Nugen Audio</h4><p>Nugen Audio created the VisLM plugin to provide sound teams with an efficient and accurate way to measure mixes for conformance to traditional broadcast &amp; streaming specifications — Full Mix Loudness, Dialogue Loudness, and True Peak. Since then, VisLM has become a widely used tool throughout the global post-production industry. Nugen Audio partnered with Fraunhofer, integrating the Fraunhofer IDMT Dialogue Intelligibility libraries into a new industry-first tool — Nugen DialogCheck. This tool gives <strong>re-recording mixers</strong> real-time insights, helping them adjust dialogue clarity at the most crucial points in the mixing process, ensuring every word is clear and understood.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*gGt-DpKR806J2jqT\" /></figure><h4>Clearer Dialogue Through Collaboration</h4><p>Crafting crystal-clear dialogue isn’t just a technical challenge — it’s an art that requires continuous innovation and strong industry collaboration. To empower creators, Netflix and its partners are embedding advanced intelligibility measurement tools directly into DAWs, giving sound teams the ability to:</p><ul><li>Detect and resolve dialogue clarity issues early in the mix.</li><li>Fine-tune speech intelligibility without compromising artistic intent.</li><li>Deliver immersive, accessible storytelling to every viewer, in any listening environment.</li></ul><p>At Netflix, we’re committed to pushing the boundaries of audio excellence, from innovating scaled intelligibility measurements to collaborating with Fraunhofer and Nugen Audio on cutting-edge tools like the DialogCheck Plugin, we’re setting a new standard for dialogue clarity — ensuring every word is heard exactly as creators intended. But innovation doesn’t happen in isolation. By working together with our partners, we can continue to push the limits of what’s possible, fueling creativity and driving the future of storytelling.</p><p>Finally, we’d like to extend a heartfelt thanks to Scott Kramer for his contributions to this initiative.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=58c13d2a6f6e\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://netflixtechblog.com/measuring-dialogue-intelligibility-for-netflix-content-58c13d2a6f6e\">Measuring Dialogue Intelligibility for Netflix Content</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "Netflix 如何准确归因 eBPF 流日志 (原标题: How Netflix Accurately Attributes eBPF Flow Logs)",
      "link": "https://netflixtechblog.com/how-netflix-accurately-attributes-ebpf-flow-logs-afe6d644a3bc?source=rss----2615bd06b42e---4",
      "pubDate": "Tue, 08 Apr 2025 17:50:58 GMT",
      "isoDate": "2025-04-08T17:50:58.000Z",
      "creator": "Netflix Technology Blog",
      "summary": "# Netflix 如何准确归因 eBPF 流日志\n\nNetflix 在之前的博客文章中介绍了如何使用 eBPF 大规模捕获 TCP 流日志，以增强云网络洞察。本文深入探讨了 Netflix 如何解决一个核心问题：将流 IP 地址准确归因于工作负载身份。\n\n## 简要回顾\n\n*   **FlowExporter**: 作为所有 Netflix 工作负载的 sidecar 运行在 AWS 云中，利用 eBPF 和 TCP tracepoints 监控 TCP socket 状态变化。当 TCP socket 关闭时，FlowExporter 生成包含 IP 地址、端口、时间戳和额外 socket 统计信息的流日志记录。平均每秒产生 500 万条记录。\n*   **归因挑战**: 在云环境中，IP 地址会随着工作负载实例的创建和终止而重新分配，因此仅凭 IP 地址无法提供关于通信工作负载的洞察。\n*   **FlowCollector**: 后端服务，负责从 FlowExporter 实例收集流日志，归因 IP 地址，并将归因后的流发送到 Netflix 的 Data Mesh 进行后续处理。\n*   **eBPF 流日志的价值**: 提供 Netflix 广泛微服务集群的服务拓扑和网络健康状况的全面视图，不受编程语言、RPC 机制或应用层协议的限制，尤其适用于服务网格尚未覆盖的区域。\n\n## 误归因问题\n\n自 eBPF 流日志引入以来，将流 IP 地址准确归因于工作负载身份一直是一个重大挑战。\n\n*   **原有方法**: 依赖内部 IP 地址跟踪服务 Sonar，该服务在 Netflix AWS VPC 中的 IP 地址分配或解除分配时发出事件。FlowCollector 实时消费这些事件流进行归因。\n    ![图片 1](https://cdn-images-1.medium.com/max/1024/0*QIn-JibEFM2CLans)\n*   **根本缺陷**: 分布式系统中的延迟和故障可能导致 IP 地址变更事件延迟到达 FlowCollector，从而导致误归因（例如，IP 地址已重新分配给工作负载 Y，但 FlowCollector 仍认为属于工作负载 X）。此外，事件时间戳可能不准确。\n*   **影响**: 误归因使流数据不可靠，影响决策；用户难以验证工作负载依赖关系；对于关键服务，误归因频繁发生；导致全舰队范围的依赖分析不切实际。\n*   **临时解决方案**: FlowCollector 收到流后，会将其保留 15 分钟再进行归因，以等待延迟的 IP 地址变更事件。这减少了误归因，但未能完全消除，且数据新鲜度降低，影响实时分析。\n*   **重要性**: 彻底消除误归因至关重要，因为即使一个误归因的流也可能导致不正确的依赖关系。Netflix 过去一年开发了一种新的归因方法，最终消除了误归因。\n\n## 归因本地 IP 地址\n\n每个 socket 都有一个本地 IP 地址和一个远程 IP 地址。以前两者使用相同方法归因。现在，本地 IP 地址的归因被简化。\n\n*   **EC2 实例**: FlowExporter 直接从本地磁盘读取 Netflix Metatron 在启动时提供的身份证书，即可确定本地工作负载身份。\n*   **容器工作负载 (Titus)**:\n    *   **挑战**: FlowExporter 运行在容器宿主级别，一个宿主管理多个具有不同身份的容器工作负载。\n    *   **解决方案**: 利用 Netflix 的容器 IP 地址分配服务 IPMan。IPManAgent（运行在每个容器宿主上的守护进程）将 IP 地址到工作负载 ID 的映射写入 eBPF map，FlowExporter 的 eBPF 程序可使用此 map 查找 socket 本地 IP 地址关联的工作负载 ID。\n    ![图片 2](https://cdn-images-1.medium.com/max/1024/0*fyfZ6m2NrMq1NgRQ)\n    *   **IPv6 到 IPv4 转换机制**: Netflix 的 IPv6 到 IPv4 转换机制可能导致内核为不同容器工作负载创建的 socket 报告相同的本地 IPv4 地址。为解决此问题，修改 Titus，在拦截 `connect` 系统调用时，将 (本地 IPv4 地址, 本地端口) 到工作负载 ID 的映射写入 eBPF map。FlowExporter 的 eBPF 程序使用此 map 正确归因由转换机制创建的 socket。\n\n通过解决这些问题，现在可以准确归因每个流的本地 IP 地址。\n\n## 归因远程 IP 地址\n\n解决了本地 IP 地址归因问题后，准确归因远程 IP 地址变得可行。\n\n*   **信息**: FlowExporter 报告的每个流现在包含本地 IP 地址、本地工作负载身份以及连接开始/结束时间戳。\n*   **FlowCollector 学习**: FlowCollector 接收这些流后，可以学习每个工作负载拥有给定 IP 地址的时间范围。例如，如果 FlowCollector 看到一个本地 IP 地址为 10.0.0.1 且与工作负载 X 关联的流，其开始时间为 t1，结束时间为 t2，则可以推断 10.0.0.1 在 t1 到 t2 期间属于工作负载 X。Netflix 使用 Amazon Time Sync，时间戳可靠。\n*   **数据结构**: 每个 FlowCollector 节点维护一个内存中的哈希表，将 IP 地址映射到时间范围列表。\n    ```go\ntype IPAddressTracker struct {\n    ipToTimeRanges map[netip.Addr]timeRanges\n}\ntype timeRanges []timeRange\ntype timeRange struct {\n    workloadID string\n    start      time.Time\n    end        time.Time\n}\n    ```\n*   **填充哈希表**: FlowCollector 从接收到的流中提取本地 IP 地址、本地工作负载身份、开始时间和结束时间，并在 map 中创建/扩展相应的时间范围。时间范围按升序排序且不重叠。\n*   **广播机制**: 由于每个流只发送到一个 FlowCollector 节点，因此每个节点必须与其他节点共享其学习到的时间范围。Netflix 使用 Kafka 实现广播机制。\n*   **远程 IP 归因**: FlowCollector 在填充的 map 中查找远程 IP 地址，返回时间范围列表。然后使用流的开始时间戳确定相应的时间范围和关联的工作负载身份。如果开始时间不在任何时间范围内，FlowCollector 会在延迟后重试。\n    ![图片 3](https://cdn-images-1.medium.com/max/1024/0*o8tJzaxRlWDBIYBS)\n*   **新方法的优势**: 通过连续的心跳（每个都关联可靠的 IP 地址所有权时间范围）实现准确归因，优雅处理瞬时问题，少量延迟或丢失的心跳不会导致误归因。与旧方法（依赖离散事件）相比，新方法更健壮。\n*   **延迟**: FlowCollector 暂时将接收到的流存储在磁盘上 1 分钟，然后归因其远程 IP 地址。这引入了 1 分钟的延迟，但远短于旧方法的 15 分钟。\n*   **成本效益**: 新方法简单，使用内存查找，无需持久存储，启动时状态可快速重建。30 个 c7i.2xlarge 实例可处理每秒 500 万个流。\n\n## 归因跨区域 IP 地址\n\n*   **区域化**: Netflix 的云微服务跨多个 AWS 区域运行，每个主要区域运行一个 FlowCollector 集群。\n*   **广播限制**: 广播机制仅限于同一区域内的 FlowCollector 节点。\n*   **跨区域流**: 远程 IP 地址在不同区域的跨区域流会被接收的 FlowCollector 节点转发到相应区域的节点。FlowCollector 通过查找所有 Netflix VPC CIDR 构建的 trie 来确定远程 IP 地址的区域。这种方法比跨所有区域广播 IP 地址时间范围更新更高效，因为只有 1% 的 Netflix 流是跨区域的。\n\n## 归因非工作负载 IP 地址\n\n并非所有流 IP 地址都属于 Netflix 的云工作负载。\n\n*   **示例**: 大部分流通过 AWS ELB。这些流的远程 IP 地址与 ELB 关联，无法运行 FlowExporter。\n*   **方法**: 继续使用 Sonar 的 IP 地址变更事件进行归因。尽管数据流可能包含不准确的时间戳并有延迟，但由于 ELB IP 地址重新分配极不频繁，误归因不是主要问题。\n\n## 验证正确性\n\n验证新方法是否消除了误归因具有挑战性，因为缺乏确定的工作负载依赖关系真相来源。\n\n*   **验证方法**: 分析了 Netflix 云网关 Zuul 的流日志。Zuul 具有广泛的足迹（处理所有云入口流量）、大量下游依赖关系，并且可以通过其路由配置作为真相来源来推导其依赖关系。\n*   **结果**: 在两周的窗口期内，通过 Zuul 的流没有发现误归因。这提供了强有力的信心，表明新的归因方法已消除误归因。在旧方法中，Zuul 约 40% 的依赖关系被流日志报告为误归因。\n\n## 结论\n\n误归因问题解决后，eBPF 流日志现在能为 Netflix 的服务拓扑和网络健康提供可靠的全舰队洞察。这一进步为服务依赖审计、安全分析和事件分类等领域带来了许多令人兴奋的机会，同时帮助 Netflix 工程师更好地理解不断演进的分布式系统。",
      "shortSummary": "Netflix 解决了 eBPF 流日志中 IP 地址误归因的核心问题。通过改进 FlowExporter 和 FlowCollector，为本地和远程 IP 地址开发了新的归因方法。本地 IP 通过 eBPF map 和 IPManAgent 准确识别，远程 IP 则通过 FlowCollector 节点间共享的 IP 拥有时间范围信息进行归因。新方法消除了误归因，将数据延迟从 15 分钟缩短至 1 分钟，并经验证准确性。这使得 eBPF 流日志能提供可靠的全舰队网络洞察，支持服务依赖审计和安全分析。",
      "translated_title": "Netflix 如何准确归因 eBPF 流日志",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*QIn-JibEFM2CLans",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*fyfZ6m2NrMq1NgRQ",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*o8tJzaxRlWDBIYBS",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=afe6d644a3bc",
          "alt": "",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<p>By <a href=\"https://www.linkedin.com/in/chengxie90/\">Cheng Xie</a>, <a href=\"https://www.linkedin.com/in/bryan-shultz-85983829/\">Bryan Shultz</a>, and <a href=\"https://www.linkedin.com/in/christine-xu-1b77191b/\">Christine Xu</a></p><p>In a previous <a href=\"https://netflixtechblog.com/how-netflix-uses-ebpf-flow-logs-at-scale-for-network-insight-e3ea997dca96\">blog post</a>, we described how Netflix uses eBPF to capture TCP flow logs at scale for enhanced cloud network insights. In this post, we delve deeper into how Netflix solved a core problem: accurately attributing flow IP addresses to workload identities.</p><h3>A Brief Recap</h3><p><strong>FlowExporter</strong> is a sidecar that runs alongside all Netflix workloads in the AWS Cloud. It uses eBPF and <a href=\"https://www.brendangregg.com/blog/2018-03-22/tcp-tracepoints.html\">TCP tracepoints</a> to monitor TCP socket state changes. When a TCP socket closes, FlowExporter generates a flow log record that includes the IP addresses, ports, timestamps, and additional socket statistics. On average, 5 million records are produced per second.</p><p>In cloud environments, IP addresses are reassigned to different workloads as workload instances are created and terminated, so IP addresses alone cannot provide insights on which workloads are communicating. To make the flow logs useful, each IP address must be attributed to its corresponding workload identity. <strong>FlowCollector</strong>, a backend service, collects flow logs from FlowExporter instances across the fleet, attributes the IP addresses, and sends these attributed flows to Netflix’s <a href=\"https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873\">Data Mesh</a> for subsequent stream and batch processing.</p><p>The eBPF flow logs provide a comprehensive view of service topology and network health across Netflix’s extensive microservices fleet, regardless of the programming language, RPC mechanism, or application-layer protocol used by individual workloads. This is especially useful for reaching the corners where our <a href=\"https://netflixtechblog.com/zero-configuration-service-mesh-with-on-demand-cluster-discovery-ac6483b52a51\">Service Mesh</a> does not yet have coverage.</p><h3>The Problem with Misattribution</h3><p>Accurately attributing flow IP addresses to workload identities has been a significant challenge since our eBPF flow logs were introduced.</p><p>As noted in our previous blog post, our initial attribution approach relied on <a href=\"https://youtu.be/8C9xNVYbCVk?si=Mqic7typcyB-v3JR&amp;t=1687\">Sonar</a>, an internal IP address tracking service that emits an event whenever an IP address in Netflix’s AWS VPCs is assigned or unassigned to a workload. FlowCollector consumes a stream of IP address change events from Sonar and uses this information to attribute flow IP addresses in real-time.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*QIn-JibEFM2CLans\" /></figure><p>The fundamental drawback of this method is that it can lead to misattribution. Delays and failures are inevitable in distributed systems, which may delay IP address change events from reaching FlowCollector. For instance, an IP address may initially be assigned to workload X but later reassigned to workload Y. However, if the change event for this reassignment is delayed, FlowCollector will continue to assume that the IP address belongs to workload X, resulting in misattributed flows. Additionally, event timestamps may be inaccurate depending on how they are captured.</p><p>Misattribution rendered the flow data unreliable for decision-making. Users often depend on flow logs to validate workload dependencies, but misattribution creates confusion. Without expert knowledge of expected dependencies, users would struggle to identify or confirm misattribution. Moreover, misattribution occurred frequently for critical services with a large footprint due to frequent IP address changes. Overall, misattribution makes fleet-wide dependency analysis impractical.</p><p>As a workaround, we made FlowCollector hold received flows for 15 minutes before attribution, allowing time for delayed IP address change events. While this approach reduced misattribution, it did not eliminate it. Moreover, the waiting period made the data less fresh, reducing its utility for real-time analysis.</p><p>Fully eliminating misattribution is crucial because it only takes a single misattributed flow to produce an incorrect workload dependency. Solving this problem required a complete rethinking of our approach. Over the past year, Netflix developed a new attribution method that has finally eliminated misattribution, as detailed in the rest of this post.</p><h3>Attributing Local IP Addresses</h3><p>Each socket has two IP addresses: a local IP address and a remote IP address. Previously, we used the same method to attribute both. However, attributing the local IP address should be a simpler task since the local IP address belongs to the instance where FlowExporter captures the socket. Therefore, FlowExporter should determine the local workload identity from its environment and attribute the local IP address before sending the flow to FlowCollector.</p><p>This is straightforward for workloads running directly on EC2 instances, as Netflix’s <a href=\"https://www.youtube.com/watch?v=-mmOT9I6JlY\">Metatron</a> provisions workload identity certificates to each EC2 instance at boot time. FlowExporter can simply read these certificates from the local disk to determine the local workload identity.</p><p>Attributing local IP addresses for container workloads running on Netflix’s container platform, <a href=\"https://netflixtechblog.com/titus-the-netflix-container-management-platform-is-now-open-source-f868c9fb5436\">Titus</a>, is more challenging. FlowExporter runs at the container host level, where each host manages multiple container workloads with different identities. When FlowExporter’s eBPF programs receive a socket event from TCP tracepoints in the kernel, the socket may have been created by one of the container workloads or by the host itself. Therefore, FlowExporter must determine which workload to attribute the socket’s local IP address to. To solve this problem, we leveraged <a href=\"https://www.youtube.com/watch?v=fmUM9bMoCNE\">IPMan</a>, Netflix’s container IP address assignment service. IPManAgent, a daemon running on every container host, is responsible for assigning and unassigning IP addresses. As container workloads are launched, IPManAgent writes an IP-address-to-workload-ID mapping to an eBPF map, which FlowExporter’s eBPF programs can then use to look up the workload ID associated with a socket local IP address.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*fyfZ6m2NrMq1NgRQ\" /></figure><p>Another challenge was to accommodate Netflix’s <a href=\"https://lpc.events/event/11/contributions/932/attachments/908/1764/LPC%202021_%20Talking%20IPv6%20to%20IPv4%20Without%20NAT_2.pdf\">IPv6 to IPv4 translation mechanism</a> on Titus. To facilitate IPv6 migration, Netflix developed a mechanism that enables IPv6-only containers to communicate with IPv4 destinations without incurring NAT64 overhead. This mechanism intercepts <em>connect</em> syscalls and replaces the underlying socket with one that uses a shared IPv4 address assigned to the container host. This confuses FlowExporter because the kernel reports the same local IPv4 address for sockets created by different container workloads. To disambiguate, local port information is additionally required. We modified Titus to write a mapping of (local IPv4 address, local port) to the workload ID into an eBPF map whenever a <em>connect</em> syscall is intercepted. FlowExporter’s eBPF programs then use this map to correctly attribute sockets created by the translation mechanism.</p><p>With these problems solved, we can now accurately attribute the local IP address of every flow.</p><h3>Attributing Remote IP Addresses</h3><p>Once the local IP address attribution problem is solved, accurately attributing remote IP addresses becomes feasible. Now, each flow reported by FlowExporter includes the local IP address, the local workload identity, and connection start/end timestamps. As FlowCollector receives these flows, it can learn the time ranges during which each workload owns a given IP address. For instance, if FlowCollector sees a flow with local IP address 10.0.0.1 associated with workload X that starts at t1 and ends at t2, it can deduce that 10.0.0.1 belonged to workload X from t1 to t2. Since Netflix uses <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/set-time.html\">Amazon Time Sync</a> across its fleet, the timestamps (captured by FlowExporter) are reliable.</p><p>The FlowCollector service cluster consists of many nodes. Every node must be capable of attributing arbitrary remote IP addresses and, therefore, requires knowledge of all workload IP addresses and their recent ownership records. To represent this knowledge, each node maintains an in-memory hashmap that maps an IP address to a list of time ranges, as illustrated by the following Go structs:</p><pre>type IPAddressTracker struct {<br>    ipToTimeRanges map[netip.Addr]timeRanges<br>}<br><br>type timeRanges []timeRange<br><br>type timeRange struct {<br>    workloadID   string<br>    start        time.Time<br>    end          time.Time<br>}</pre><p>To populate the hashmap, FlowCollector extracts the local IP address, local workload identity, start time, and end time from each received flow and creates/extends the corresponding time ranges in the map. The time ranges for each IP address are sorted in ascending order, and they are non-overlapping since an IP address cannot belong to two different workloads simultaneously.</p><p>Since each flow is only sent to one FlowCollector node, each node must share the time ranges it learned from received flows with other nodes. We implemented a broadcasting mechanism using Kafka, where each node publishes learned time ranges to all other nodes. Although more efficient broadcasting implementations exist, the Kafka-based approach is simple and has worked well for us.</p><p>Now, FlowCollector can attribute remote IP addresses by looking them up in the populated map, which returns a list of time ranges. It then uses the flow’s start timestamp to determine the corresponding time range and associated workload identity. If the start time does not fall within any time range, FlowCollector will retry after a delay, eventually giving up if the retry fails. Such failures may occur when flows are lost or broadcast messages are delayed. For our use cases, it is acceptable to leave a small percentage of flows unattributed, but any misattribution is unacceptable.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*o8tJzaxRlWDBIYBS\" /></figure><p>This new method achieves accurate attribution thanks to the continuous heartbeats, each associated with a reliable time range of IP address ownership. It handles transient issues gracefully — a few delayed or lost heartbeats do not lead to misattribution. In contrast, the previous method relied solely on discrete IP address assignment and unassignment events. Lacking heartbeats, it had to presume an IP address remained assigned until notified otherwise (which can be hours or days later), making it vulnerable to misattribution when the notifications were delayed.</p><p>One detail is that when FlowCollector receives a flow, it cannot attribute its remote IP address right away because it requires the latest observed time ranges for the remote IP address. Since FlowExporter reports flows in batches every minute, FlowCollector must wait until it receives the flow batch from the remote workload FlowExporter for the last minute, which may not have arrived yet. To address this, FlowCollector temporarily stores received flows on disk for one minute before attributing their remote IP addresses. This introduces a 1-minute delay, but it is much shorter than the 15-minute delay with the previous approach.</p><p>In addition to producing accurate attribution, the new method is also cost-effective thanks to its simplicity and in-memory lookups. Because the in-memory state can be quickly rebuilt when a FlowCollector node starts up, no persistent storage is required. With 30 c7i.2xlarge instances, we can process 5 million flows per second for the entire Netflix fleet.</p><h3>Attributing Cross-Regional IP Addresses</h3><p>For simplicity, we have so far glossed over one topic: regionalization. Netflix’s cloud microservices operate across multiple AWS regions. To optimize flow reporting and minimize cross-regional traffic, a FlowCollector cluster runs in each major region, and FlowExporter agents send flows to their corresponding regional FlowCollector. When FlowCollector receives a flow, its local IP address is guaranteed to be within the region.</p><p>To minimize cross-region traffic, the broadcasting mechanism is limited to FlowCollector nodes within the same region. Consequently, the IP address time ranges map contains only IP addresses from that region. However, cross-regional flows have a remote IP address in a different region. To attribute these flows, the receiving FlowCollector node forwards them to nodes in the corresponding region. FlowCollector determines the region for a remote IP address by looking up a trie built from all Netflix VPC CIDRs. This approach is more efficient than broadcasting IP address time range updates across all regions, as only 1% of Netflix flows are cross-regional.</p><h3>Attributing Non-Workload IP Addresses</h3><p>So far, FlowCollector can accurately attribute IP addresses belonging to Netflix’s cloud workloads. However, not all flow IP addresses fall into this category. For instance, a significant portion of flows goes through AWS ELBs. For these flows, their remote IP addresses are associated with the ELBs, where we cannot run FlowExporter. Consequently, FlowCollector cannot determine their identities by simply observing the received flows. To attribute these remote IP addresses, we continue to use IP address change events from Sonar, which crawls AWS resources to detect changes in IP address assignments. Although this data stream may contain inaccurate timestamps and be delayed, misattribution is not a main concern since ELB IP address reassignment occurs very infrequently.</p><h3>Verifying Correctness</h3><p>Verifying that the new method has eliminated misattribution is challenging due to the lack of a definitive source of truth for workload dependencies to validate flow logs against; the flow logs themselves are intended to serve as this source of truth, after all. To build confidence, we analyzed the flow logs of a large service with well-understood dependencies. A large footprint is necessary, as misattribution is more prevalent in services with numerous instances, and there must be a reliable method to determine the dependencies for this service without relying on flow logs.</p><p>Netflix’s cloud gateway, <a href=\"https://github.com/Netflix/zuul\">Zuul</a>, served this purpose perfectly due to its extensive footprint (handling all cloud ingress traffic), its large number of downstream dependencies, and our ability to derive its dependencies from its routing configurations as the source of truth for comparison with flow logs. We found no misattribution for flows through Zuul over a two-week window. This provided strong confidence that the new attribution method has eliminated misattribution. In the previous approach, approximately 40% of Zuul’s dependencies reported by the flow logs were misattributed.</p><h3>Conclusion</h3><p>With misattribution solved, eBPF flow logs now deliver dependable, fleet-wide insights into Netflix’s service topology and network health. This advancement unlocks numerous exciting opportunities in areas such as service dependency auditing, security analysis, and incident triage, while helping Netflix engineers develop a better understanding of our ever-evolving distributed systems.</p><h3>Acknowledgments</h3><p>We would like to thank <a href=\"https://www.linkedin.com/in/mdubcovsky/\">Martin Dubcovsky</a>, <a href=\"https://www.linkedin.com/in/joannekoong/\">Joanne Koong</a>, <a href=\"https://www.linkedin.com/in/troshko/\">Taras Roshko</a>, <a href=\"https://www.linkedin.com/in/nabilschear/\">Nabil Schear</a>, <a href=\"https://www.linkedin.com/in/jacobmeyers35/\">Jacob Meyers</a>, <a href=\"https://www.linkedin.com/in/parshap/\">Parsha Pourkhomami</a>, <a href=\"https://www.linkedin.com/in/hechaoli/\">Hechao Li</a>, <a href=\"https://www.linkedin.com/in/donavanfritz/\">Donavan Fritz</a>, <a href=\"https://www.linkedin.com/in/rob-gulewich-0335b52/\">Rob Gulewich</a>, <a href=\"https://www.linkedin.com/in/amanda-li-410286166/\">Amanda Li</a>, <a href=\"https://www.linkedin.com/in/jdsalem/\">John Salem</a>, <a href=\"https://www.linkedin.com/in/haananth/\">Hariharan Ananthakrishnan</a>, <a href=\"https://www.linkedin.com/in/joshmachine/\">Keerti Lakshminarayan</a>, and other stunning colleagues for their feedback, inspiration, and contributions to the success of this effort.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=afe6d644a3bc\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://netflixtechblog.com/how-netflix-accurately-attributes-ebpf-flow-logs-afe6d644a3bc\">How Netflix Accurately Attributes eBPF Flow Logs</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "Netflix媒体制作套件助力全球化制作 (原标题: Globalizing Productions with Netflix’s Media Production Suite)",
      "link": "https://netflixtechblog.com/globalizing-productions-with-netflixs-media-production-suite-fc3c108c0a22?source=rss----2615bd06b42e---4",
      "pubDate": "Mon, 31 Mar 2025 16:21:48 GMT",
      "isoDate": "2025-03-31T16:21:48.000Z",
      "creator": "Netflix Technology Blog",
      "summary": "## Netflix媒体制作套件（MPS）：赋能全球化制作\n\n### 引言\n\n在快速发展的影视行业中，从剧本到荧幕的制作过程充满挑战。尽管行业不断创新并向云端工作流迈进，但要实现全球范围内的云创新及其全部优势却异常困难。Netflix认识到，简化复杂的媒体管理物流、消除繁琐的非创意任务，并让制作团队专注于核心的创意叙事，是行业发展的关键。为此，Netflix开发了一套由电影制作人专为电影制作人打造的工具套件——媒体制作套件（Media Production Suite, MPS）。\n\n### 面临的挑战\n\n*   **媒体管理效率低下：** 制作生命周期中，大量时间与资源耗费在媒体物流管理上。一部Netflix作品平均产生约200TB的原始摄影文件（OCF），最高可达700TB。传统的LTO磁带工作流虽然成本效益高，但存在物理运输、难以搜索、播放和共享媒体资产等缺点，尤其在全球协作时，严重影响媒体的可访问性。\n*   **数字化流程瓶颈：** 即使是全数字化工作流，缺乏自动化和标准化也导致后期制作和VFX环节劳动密集，易引入人为错误和安全风险。与全球多样化供应商的协作成为巨大的技术难题，文件尺寸增大和工作流复杂化进一步加剧了这些问题，降低了效率，减少了创意工作时间。\n*   **云迁移新挑战：** 将媒体迁移到云端带来了新的操作和技术障碍。许多后期制作机构仍依赖物理硬盘，媒体需手动在供应商之间传递，急需一个超越这些障碍的集中式云解决方案。\n*   **技术可及性限制：** Netflix认为，优秀的故事可以来自任何地方，但传统工作流的技术限制阻碍了媒体访问和创作者获取人才。艺术家不仅需要强大的云存储，还需要高性能工作站和实时播放能力，这些在某些市场或预算下可能无法获得或负担。\n*   **行业趋势：** 行业正从“将媒体带给艺术家/应用程序”（传统工作流）转向“将人与应用程序带给媒体”（云工作流和远程工作站）。Netflix的目标是在每年数百部作品的规模上实现这一愿景。\n\n### 构建全球化技术的挑战\n\n在全球范围内构建解决方案面临显著挑战。电影和剧集制作在技术可及性、最佳实践和全球标准化方面存在不平等。不同国家和地区基于当地需求和细微差别，处于不同的创新阶段。这导致开发出的全球技术必须同时满足成熟和新兴市场，并兼顾不同的语言和工作流。\n\n全球人才和供应商需求的多样性带来了标准化挑战。许多成熟的后期制作和VFX机构已建立内部脚本和自动化流程，但定制化耗时。引入新工作流可能扰乱既有流程，增加供应商的利润压力。因此，创新必须为作品带来显著益处才能被大规模采纳。可靠性、良好记录、强大支持和极低的容错率是成熟市场的首要考量。\n\n### Netflix的解决方案：MPS理念\n\n在开发MPS时，Netflix认识到必须在不大幅扩张运营实体的情况下，处理海量作品。因此，自动化变得至关重要。色彩和构图管理以及交付物必须由用户无缝控制和轻松管理，无需人工干预。通过拥抱开放标准（如ACES、AMF、ASC MHL、ASC FDL和OTIO），Netflix不仅简化了这些流程，还促进了跨市场和国家的顺畅协作，确保全球制作以无与伦比的效率和凝聚力运行。例如，FDL标准使得将不同格式、分辨率、镜头和安全框的素材标准化，并提取中心构图，可以轻松自动化，将控制权直接交给用户，而这在过去通常只为高端作品提供。\n\n### 内容中心媒体制作套件（MPS）\n\nNetflix致力于提供可定制、功能丰富的工具，既能满足高级用户的需求，又能对经验不足的电影制作人保持直观和简化。通过与全球Netflix团队、供应商和人才的协作，Netflix在内容中心（Content Hub）内推出了媒体制作套件（MPS），实现了技术的民主化。MPS利用Netflix的规模经济和资源，释放全球人才库，大幅减少非创意任务，简化工作流，平衡市场差异，最终最大化创意工作时间。\n\n**MPS的核心组成：**\n\n1.  **Netflix混合基础设施：** 结合了云端和物理分布式能力，在全球多个地点运行，靠近制作团队以优化用户性能。该基础设施通过Netflix Open Connect网络连接到亚马逊网络服务（AWS），旨在高速处理大量摄影和声音媒体。为确保制作团队有足够的上传速度将媒体上传至云端，Netflix已在全球推出内容中心摄取中心（Content Hub Ingest Centers），提供所需的高速互联网连接。所有媒体集中化后，MPS消除了物理媒体传输的需要，降低了人为错误的风险，同时增强了安全性和可访问性。\n    \n    ![图片 1](https://cdn-images-1.medium.com/max/1024/0*CUGxiNprXnLcOmhI)\n    \n2.  **自动化工具套件：** 在Netflix混合基础设施层之上，MPS包含一套利用Netflix生态系统中媒体的工具：\n    *   **素材摄取（Footage Ingest）：** 允许用户将媒体/文件上传到内容中心的应用程序。\n    *   **媒体库（Media Library）：** 允许用户搜索、预览、共享和下载媒体的中央库。\n    *   **每日样片（Dailies）：** 由运营团队支持的工作流，提供素材的自动化质量控制、声音同步、色彩应用、渲染，并将每日样片直接交付给剪辑团队。\n    *   **远程工作站（Remote Workstations）：** 提供远程剪辑工作站和存储，满足后期制作需求。\n    *   **VFX素材提取（VFX Pulls）：** 自动化转换和交付视觉效果板、相关色彩和构图文件给VFX供应商。\n    *   **套底素材提取（Conform Pulls）：** 自动化整合、修剪和交付所有OCF给画面精修供应商。\n    *   **媒体下载器（Media Downloader）：** 一旦媒体在Netflix云中可用，便启动下载的自动化下载工具。\n\n虽然MPS中的每个工具成熟度不同，但已有超过350部作品使用了其中至少一个工具。在开发过程中，Netflix收集了来自全球各地（包括美国/加拿大、欧洲/中东/非洲、东南亚、拉丁美洲和亚太地区）用户的反馈。\n\n### 案例研究：Senna 系列\n\n![图片 2](https://cdn-images-1.medium.com/max/1024/0*NLeFw4FDGx2jsZg7)\n\n巴西制作的剧集《Senna》（讲述F1传奇车手埃尔顿·塞纳生平）利用MPS重塑了内容创作工作流，克服了地理障碍，并释放了创新潜力，为全球观众提供世界级的叙事。该剧集不仅在故事叙事上具有开创性，其制作过程也跨越了阿根廷、乌拉圭、巴西和英国。剪辑团队分布在阿雷格里港和西班牙，VFX工作室则在巴西、加拿大、美国和印度协作，所有这些都由Netflix的子公司Scanline VFX协调。该剧集完美体现了现代电影制作的全球化特性，也与Netflix新的内容中心媒体制作套件（MPS）完美契合。\n\nMPS是《Senna》工作流编排的核心。尽管MPS中的每个工具都基于选择加入模式，但要使用许多下游服务，第一步是确保原始摄影文件（OCF）和原始声音文件（OSF）已上传。制片人表示，将所有素材基于云端是至关重要的，这避免了从阿根廷等地物理运输媒体到巴西所需的大量时间。通过使用MPS，制作团队不再需要通过飞机运输硬盘、制作LTO磁带或管理物理运输。值得一提的是，使用MPS时，除非有特定需求，否则不再强制写入LTO磁带。\n\n《Senna》于2023年6月开始制作时，Netflix对MPS的投入仍处于早期阶段，工具被认为是测试版。然而，在制作团队的帮助、反馈和合作下，Netflix很快意识到这项投资是值得加倍投入的。自《Senna》使用的早期版本以来，Netflix已在全球范围内建立了摄取中心，硬盘可以在那里被接收，并在数小时内将所有原始摄影文件上传到Netflix生态系统。虽然上传能力并非新颖概念，但其背后远非简单。一旦硬盘插入并打开Netflix素材摄取应用程序，系统会运行验证，确保硬盘上包含所有预期的现场媒体。媒体上传并运行校验和验证媒体完整性后，所有媒体都会被检查，提取元数据，并创建用于查看/共享/下载的资产，包括可播放的代理文件。所有媒体随后会自动备份到第二层云存储，用于最终归档。\n\n传统上，如果想了解媒体管理步骤的进展，或者在未收到完成通知时是否可以清空现场摄影机卡，需要打电话给供应商。对于《Senna》，任何想了解进度的人只需登录内容中心，即可在素材摄取仪表板中查看所有活动，并查询过去上传的任何信息。\n\n![图片 3](https://cdn-images-1.medium.com/max/1024/0*7gaSH8-YpnOTKnpu)",
      "shortSummary": "Netflix推出了媒体制作套件（MPS），旨在解决全球影视制作中复杂的媒体管理、协作和技术可及性挑战。MPS结合了混合云基础设施和自动化工具，实现了媒体的云端化、流程的标准化和自动化，从而大幅减少了物理运输和非创意任务，提升了全球制作效率和创意自由度。巴西剧集《Senna》的成功应用证明了MPS在跨国协作和简化工作流方面的巨大潜力，它消除了对物理媒体传输的依赖，并提供了媒体上传和管理的高度透明性。",
      "translated_title": "Netflix媒体制作套件助力全球化制作",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*CUGxiNprXnLcOmhI",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*NLeFw4FDGx2jsZg7",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*7gaSH8-YpnOTKnpu",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*_CzN6mkamROqqxjo",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*V01AyZdu0si2N0z5",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<p><a href=\"https://www.linkedin.com/in/jesse-korosi-44790985/\"><strong>Jesse Korosi</strong></a>,<strong> </strong><a href=\"https://www.linkedin.com/in/thijsvdkamp/\"><strong>Thijs van de Kamp</strong></a><strong>, </strong><a href=\"https://www.linkedin.com/in/mayralvega/\"><strong>Mayra Vega</strong></a>,<strong> </strong><a href=\"https://www.linkedin.com/in/laurafuturo/\"><strong>Laura Futuro</strong></a>,<strong> </strong><a href=\"https://www.linkedin.com/in/margoline/\"><strong>Anton Margoline</strong></a></p><p>The journey from script to screen is full of challenges in the ever-evolving world of film and television. The industry has always innovated, and over the last decade, it started moving towards cloud-based workflows. However, unlocking cloud innovation and all its benefits on a global scale has proven to be difficult. The opportunity is clear: streamline complex media management logistics, eliminate tedious, non-creative task-based work and enable productions to focus on what matters most–creative storytelling. With these challenges in mind, Netflix has developed a suite of tools by filmmakers for filmmakers: the Media Production Suite (MPS).</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*CUGxiNprXnLcOmhI\" /></figure><h3><strong>What are we solving for?</strong></h3><p>Significant time and resources are devoted to managing media logistics throughout the production lifecycle. An average Netflix title produces around ~200 Terabytes of Original Camera Files (OCF), with outliers up to 700 Terabytes, not including any work-in-progress files, VFX assets, 3D assets, etc. The data produced on set is traditionally copied to physical tape stock like LTO. This workflow has been considered the industry norm for a long time and may be cost-effective, but comes with trade-offs. Aside from needing to physically ship and track all movement of the tape stock, storing media on a physical tape makes it harder to search, play and share media assets; slowing down accessibility to production media when needed, especially when titles need to collaborate with talent and vendors all over the world.</p><p>Even when workflows are fully digital, the distribution of media between multiple departments and vendors can still be challenging. A lack of automation and standardization often results in a labour-intensive process across post-production and VFX with a lot of dependencies that introduce potential human errors and security risks. Many productions utilize a large variety of vendors, making this collaboration a large technical puzzle. As file sizes grow and workflows become more complex, these issues are magnified, leading to inefficiencies that slow down post-production and reduce the available time spent on creative work.</p><p>Moving media into the cloud introduces new challenges for production and post ramping up to meet the operational and technological hurdles this poses. For some post-production facilities, it’s not uncommon to see a wall of portable hard drives at their facility, with media being hand-carried between vendors because alternatives are not available. The need for a centralized, cloud-based solution that transcends these barriers is more pressing than ever. This results in a willingness to embrace new and innovative ideas, even if exploratory, and introduce drastic workflow changes to productions in pursuit of creative evolution.</p><p>At Netflix, we believe that great stories can come from anywhere, but we have seen that technical limitations in traditional workflows reduce access to media and restrict filmmakers’ access to talent. Besides the need for robust cloud storage for their media, artists need access to powerful workstations and real-time playback. Depending on the market, or production budget, cutting-edge technology might not be available or affordable.</p><p>What if we started charting a course to break free from many of these technical limitations and found ways to enhance creativity? Industry trade shows like the International Broadcast Convention (IBC) and the National Association of Broadcasters Show (NAB) highlight a strong global trend: instead of bringing media to the artist/applications (traditional workflow) we see the shift towards bringing people and applications to the media (cloud workflows and remote workstations). The concept of cloud-based workflows is not new, as many technology leaders in our industry have been experimenting in this space for more than a decade. However, executing this vision at a Netflix scale with hundreds of titles a year has not been done before…</p><h3><strong>The challenge of building a global technology to solve this</strong></h3><p>Building solutions at a global scale poses significant challenges. The art of making movies and series lacks equal access to technology, best practices, and global standardization. Different countries worldwide are at different phases of innovation based on local needs and nuances. While some regions boast over a century of cinematic history and have a strong industry, others are just beginning to carve their niche. This vast gap presents a unique challenge: developing global technology that caters to both established and emerging markets, each with distinct languages and workflows.</p><p>The large diversity of needs by talent and vendors globally creates a standardization challenge and can be seen when productions use a global talent pool. Many mature post-production and VFX facilities have built scripts and automation that flow between various artists and personnel within their facility; allowing a more streamlined workflow, even though the customization is time-consuming. E.g., Transcoding, or transcriptions that automatically run when files are dropped in a hot folder, with the expectation that certain sidecar metadata files will accompany them with a specific organizational structure. Embracing and integrating new workflows introduces the fear of disrupting a well-established process, increasing additional pressure on the profit margins of vendors. Small workflow changes that may seem arbitrary may actually have a large impact on vendors. Therefore, innovation should provide meaningful benefits to a title in order to get adopted at scale. Reliability, a proven track record, strong support, and an incredibly low tolerance for bugs, or issues are top of mind in well-established markets.</p><p>In developing this suite, we recognized the necessity of addressing the vast array of titles that flow through Netflix without the luxury of expanding into a massive operational entity. Consequently, automation became imperative. The intricacies of color and framing management, along with deliverables, must be seamlessly controlled and effortlessly managed by the user, without the need for manual intervention. Therefore, we cannot lean into humans configuring JSON files behind the scenes to map camera formats into deliverables. By embracing open standards, we not only streamline these processes but also facilitate smoother collaboration across diverse markets and countries, ensuring that our global productions can operate with unparalleled efficiency and cohesion. To ensure this, we’ve decided to lean heavily into standards like <a href=\"https://www.oscars.org/science-technology/sci-tech-projects/aces\">ACES</a>, <a href=\"https://acescentral.com/knowledge-base-2/when-is-amf-used/\">AMF</a>, <a href=\"https://theasc.com/society/ascmitc/asc-media-hash-list\">ASC MHL</a>, <a href=\"https://theasc.com/society/ascmitc/asc-framing-decision-list\">ASC FDL</a>, and <a href=\"https://github.com/OpenTimelineIO\">OTIO</a>. ACES and AMF for color pipeline management. ASC MHL for any file management/verifications. ASC FDL will serve as our framing interoperability and OTIO for any timeline interchange. Leaning into standards like this means that many things can be automated at scale and more importantly, high-complexity workflows can be offered to markets or shows that don’t normally have access to them. As an example, if a show is shot on various camera formats all framed and recorded at different resolutions, with different lenses and different safeties on each frame. The task of normalizing all of these for a VFX vendor into one common container with a normalized center extracted frame is often only offered to very high-end titles, considering it takes a human behind the curtain to create all of these mappings. But by leaning into a standard like the FDL, it means this can now easily be automated, and the control for these mappings, put directly in the hands of users.</p><h3><strong>Our Answer — Content Hub’s Media Production Suite (MPS)</strong></h3><iframe src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FQi9N7z9IgMU%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DQi9N7z9IgMU&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FQi9N7z9IgMU%2Fhqdefault.jpg&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"854\" height=\"480\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/20f44e6583c1fc3db670399bdd288444/href\">https://medium.com/media/20f44e6583c1fc3db670399bdd288444/href</a></iframe><p>Building a global scalable solution that could be utilized in a diversity of markets has been an exciting challenge. We set out to provide customizable and feature-rich tooling for advanced users while remaining intuitive and streamlined enough for less experienced filmmakers. With collaboration from Netflix teams, vendors, and talent across the globe, we’ve taken a bold step forward in enabling a suite of tools inside Netflix Content Hub that democratizes technology: the Media Production Suite. While leveraging our scale economies and access to resources, we can now unlock global talent pools for our productions, drastically reduce non-creative task-based work, streamline workflows, and level the playing field between our markets, ultimately maximizing the time available for what matters most; creative work!</p><h3>So what is it?</h3><p>1. <strong>Netflix Hybrid Infrastructure</strong>: Netflix has invested in a hybrid infrastructure, a mix of cloud-based and physically distributed capabilities operating in multiple locations across the world and close to our productions to optimize user performance. This infrastructure is available for Netflix shows and is foundational under Content Hub’s Media Production Suite tooling. Local storage and compute services are connected through the Netflix Open Connect network (Netflix Content Delivery Network) to the infrastructure of Amazon Web Services (AWS). The system facilitates large volumes of camera and sound media and is built for speed. In order to ensure that productions have sufficient upload speeds to get their media into the cloud, Netflix has started to roll out Content Hub Ingest Centers globally to provide high-speed internet connectivity where required. With all media centralized, MPS eliminates the need for physical media transport and reduces the risk of human error. This approach not only streamlines operations but also enhances security and accessibility.</p><p>2. <strong>Automation and Tooling</strong>: In addition to the Netflix Hybrid infrastructure layer, MPS consists of a suite of tools that tap into the media in the Netflix ecosystem.</p><p><strong>Footage Ingest</strong> — An application that allows users to upload media/files into Content Hub.</p><p><strong>Media Library</strong> — A central library that allows users to search, preview, share and download media.</p><p><strong>Dailies</strong> — A workflow, backed by an operational team, offering automated Quality Control of your footage, sound sync, application of color, rendering, and delivering dailies directly to editorial.</p><p><strong>Remote Workstations</strong> — Offering access to remote editorial workstations and storage for post-production needs.</p><p><strong>VFX Pulls</strong> — An automated method for converting and delivering visual effects plates, associated color, and framing files to VFX vendors.</p><p><strong>Conform Pulls</strong> — An automated method for consolidating, trimming, and delivering all OCF to picture-finishing vendors.</p><p><strong>Media Downloader</strong> — An automated download tool that initiates a download once media has been made available in the Netflix cloud.</p><p>While each of the individual tools within MPS is at different states of maturity, over 350 titles have made use of at least one of the tools noted above. Input has been taken from all over the world while developing, with users ranging from UCAN (United States/Canada), EMEA (Europe, Middle East, and Africa), SEA (South East Asia), LATAM (Latin America), and APAC (Asia Pacific).</p><h3><strong>Senna: Early Adoption and Insightful Feedback Driving MPS Evolution</strong></h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*NLeFw4FDGx2jsZg7\" /><figcaption><em>Media from the Brazilian-produced series ‘Senna’ being reviewed in MPS</em></figcaption></figure><p>The Brazilian-produced series <em>Senna</em>, which follows the life of legendary Formula 1 driver Ayrton Senna, utilized MPS to reshape their content creation workflow, overcome geographical barriers, and unlock innovation to support world-class storytelling for a global audience. <em>Senna</em> is a groundbreaking series, not just for its storytelling but for its production journey across Argentina, Uruguay, Brazil, and the United Kingdom. With editorial teams spread across Porto Alegre and Spain, and VFX studios collaborating across locations in Brazil, Canada, the United States, and India, all orchestrated by our subsidiary Scanline VFX. The series exemplifies the global nature of modern filmmaking and was the perfect fit for Netflix’s new Content Hub Media Production Suite (MPS).</p><p>At the heart of <em>Senna’s</em> workflow orchestration is MPS. While each of the tools within MPS is based on an opt-in model, in order to use many of the downstream services, the first step is ensuring that the original camera files (OCF) and original sound files (OSF) are uploaded. “<em>We knew we were going to shoot in different places,</em>” said Post Supervisor Gabriel Queiroz,<em>“to have all this material cloud-based, it’s definitely one of the most important things for us. It would be hard to bring all this media physically from Argentina or wherever to Brazil. It will take us a lot of time.”</em> With <em>Senna</em> shooting across locations, allowing production the capability of uploading their OCF and OSF resulted in no longer requiring shuttling hard drives on airplanes, creating LTO tapes, &amp; managing physical shipments for their negative. And yes, you read that correctly; when utilizing MPS, we don’t require LTO tapes to be written unless there are title-specific needs.</p><p>With <em>Senna</em> beginning production back in June of 2023, our investment in MPS was still very early stages, and the tooling was considered beta. However, with the help, feedback, and partnership from this production, it was quickly realized that the investment was worth doubling down on. Since the early version used on <em>Senna</em>, Netflix has been spinning up ingest centers around the world, where drives can be dropped off, and within a matter of hours, all original camera files are uploaded into the Netflix ecosystem. While creating the ability to upload is not a novel concept, behind the scenes, it’s far from simple. Once a drive has been plugged in and our Netflix Footage Ingest application is opened, a validation is run, ensuring all expected media from set is on the drive. After media has been uploaded and checksums are run validating media integrity, all media is inspected, metadata is extracted, and assets are created for viewing/sharing/downloading with playable proxies. All media is then automatically backed up to a second tier of cloud-based storage for the final archive.</p><p>Traditionally, if you wanted to check in with your post vendor on how things are going for each of these media management steps noted above, or whether or not you can clear on set camera cards if you haven’t gotten a completion notification, you would have to pick up the phone and call your vendor. For <em>Senna</em>, anyone who wanted visibility on progress, simply logged in to Content Hub and could see any activity in the Footage Ingest dashboard, as well as look up any information needed on past uploads.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*7gaSH8-YpnOTKnpu\" /><figcaption><em>Remote monitoring media being uploaded and archived using the MPS Footage Ingest workflow</em></figcaption></figure><p>While many services in MPS are available once media has been uploaded, <em>Senna’s</em> use of MPS focused on VFX. With <em>Senna</em> shooting a high volume of footage and the show having a high volume of VFX shots, according to Post Supervisor Gabriel Queiroz <em>“Using MPS was basically a no-brainer, </em>[having]<em> used the tool before, I knew what it could bring to the project. And to be honest, with the amount of footage that we have, it was just so much material and with the amount of vendors we have, knowing that we would have to deliver all this footage to all these kinds of vendors, including outside of Brazil and to different parts of the world.”</em></p><p>With a traditional workflow, utilizing available resources in Latin America, VFX Pulls would have been done manually. This process is prone to human error and more importantly, for a show like <em>Senna</em>, too slow and would have resulted in different I/O methods for every vendor.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*_CzN6mkamROqqxjo\" /><figcaption><em>Illustrating a traditional VFX Editor having to manage various I/O methods</em></figcaption></figure><p>By utilizing MPS, the Assistant Editor was able to log into Content Hub, upload an EDL, and have their VFX Pulls automatically transcoded, color files consolidated and all media placed into a Google Drive style folder built directly in Content Hub (called Workspaces). The VFX Editor was able to make any additional tweaks they wanted to the directory before farming out each of the shots to whichever vendor they were meant for. When it came time for the VFX vendors to then send shots back to editorial or DI, this was also done through MPS. Having one standard method for I/O for all VFX file sharing meant that Editorial and DI did not have to manage a different file transfer/workflow for every single vendor that was onboarded.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*V01AyZdu0si2N0z5\" /><figcaption><em>Illustrating a more streamlined workflow for VFX vendors when using MPS</em></figcaption></figure><p>After picture was locked and it was time for <em>Senna</em> to do their Online, the DI facility Quanta was able to utilize the Conform Pull service within MPS. The Conform Pull service allowed their team to upload an EDL, which ran a QC on all of the media from within their edit to ensure a smooth conform and then consolidated, trimmed, and packaged up all of the media they needed for the online. Since this early beta and thanks to learnings from many shows like Senna, advancements have been made in the system’s ability to match back to source media for both Conform and VFX Pulls. Rather than requiring an exact match between EDL and source OCF, there are several variations of fuzzy matching that can take place, as well as a current investigation in using one of our perceptual matching algorithms, allowing for a perceptual conform using computer vision, instead of solely relying on metadata.</p><iframe src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F6LFMBsuRmtA%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D6LFMBsuRmtA&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F6LFMBsuRmtA%2Fhqdefault.jpg&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"854\" height=\"480\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/894002e879e7fa87d8223112d1afd762/href\">https://medium.com/media/894002e879e7fa87d8223112d1afd762/href</a></iframe><h3>Conclusion</h3><p>The Media Production Suite (MPS) represents a transformative leap in how we approach media production at Netflix. By embracing open standards, we have crafted a scalable solution that not only makes economic sense but also democratizes access to advanced production tools across the globe. This approach allows us to eliminate tedious tasks, enabling our teams to focus on what truly matters: creative storytelling. By fostering global collaboration and leveraging the power of cloud-based workflows, we’re not just enhancing efficiency but also elevating the quality of our productions. As we continue to innovate and refine our processes, we remain committed to breaking down barriers and unlocking the full potential of creative talent worldwide. The future of filmmaking is here, and with MPS, we are leading the charge toward a more connected and creatively empowered industry.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=fc3c108c0a22\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://netflixtechblog.com/globalizing-productions-with-netflixs-media-production-suite-fc3c108c0a22\">Globalizing Productions with Netflix’s Media Production Suite</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "个性化推荐基础模型 (原标题: Foundation Model for Personalized Recommendation)",
      "link": "https://netflixtechblog.com/foundation-model-for-personalized-recommendation-1a0bd8e02d39?source=rss----2615bd06b42e---4",
      "pubDate": "Sat, 29 Mar 2025 00:51:25 GMT",
      "isoDate": "2025-03-29T00:51:25.000Z",
      "creator": "Netflix Technology Blog",
      "summary": "Netflix正在开发一种**个性化推荐基础模型**，旨在解决现有推荐系统面临的复杂性、高维护成本以及模型间创新难以迁移的问题。该模型借鉴了自然语言处理（NLP）领域大型语言模型（LLM）的成功经验，特别是其数据中心方法和半监督学习能力。\n\n### 动机与目标\n*   **现有系统挑战**：Netflix的推荐系统由多个专门模型组成，维护成本高昂，且模型间难以共享学习成果。\n*   **灵感来源**：受LLM范式转变的启发，即从众多小型专业模型转向一个大型通用模型。\n*   **核心洞察**：\n    *   **数据中心方法**：强调积累大规模、高质量数据，并尽可能实现端到端学习，而非依赖特征工程。\n    *   **利用半监督学习**：LLM的“下一词预测”目标在无标签数据上表现出色，并能习得深层世界知识。\n*   **模型目标**：构建一个能够大规模整合用户完整互动历史和内容信息的模型，并通过共享模型权重或直接嵌入的方式，将学习成果分发给其他下游模型。\n\n### 数据处理\nNetflix拥有超过3亿用户和数千亿次互动，数据量庞大。为了有效利用这些数据，模型采用了以下策略：\n\n*   **互动分词（Interaction Tokenization）**：\n    *   并非所有原始用户行为都同等重要，需要定义有意义的“分词”。\n    *   类似于NLP中的字节对编码（BPE），通过合并相邻动作来形成更高级别的分词，同时保留关键信息（如总观看时长、互动类型）。\n    *   目标是在互动历史长度和单个分词细节之间取得平衡，避免信息丢失或序列过长。\n    *   ![图片 1](https://cdn-images-1.medium.com/max/1024/0*1dhdoLxKnf_fcZOq)\n    *   **挑战与解决方案**：活跃用户的历史记录可能包含数千个事件，超出Transformer模型的处理能力，且推理时需要毫秒级延迟。\n        *   **稀疏注意力机制**：通过低秩压缩等技术扩展上下文窗口，提高计算效率。\n        *   **滑动窗口采样**：训练时对完整序列进行重叠窗口采样，使模型能从整个序列中学习。\n        *   **KV缓存**：推理时使用KV缓存以保持低延迟。\n\n*   **每个“分词”中的信息**：\n    *   与LLM不同，推荐系统的互动事件包含异构细节，如动作属性（地点、时间、时长、设备）和内容信息（项目ID、元数据如类型、发布国家）。\n    *   大多数特征（尤其是类别特征）直接嵌入模型中，采用端到端学习。\n    *   时间戳需特殊处理以捕捉绝对和相对时间。\n    *   分词特征分为两类：\n        *   **请求时特征（Request-Time Features）**：预测时可用（如登录时间、设备）。\n        *   **动作后特征（Post-Action Features）**：互动发生后可用（如具体观看的节目、时长）。\n    *   预测下一个互动时，结合当前步骤的请求时特征和前一步骤的动作后特征。\n\n### 模型目标与架构考量\n*   **核心目标**：采用类似GPT的自回归“下一分词预测”目标，利用大规模无标签用户互动数据。\n*   **关键修改**：\n    *   **加权目标**：并非所有用户互动都同等重要（例如，5分钟预告片与2小时电影的权重不同）。\n    *   **多分词预测目标**：训练时预测接下来的N个分词，而非单个，以捕捉长期依赖关系。\n    *   **辅助预测目标**：除了预测下一个项目ID，还使用输入数据中的多个字段作为辅助目标（如从项目派生出的类型序列）。这有助于正则化、减少过拟合、提供用户意图洞察，并通过分层预测提高准确性（如先预测类型再预测项目ID）。\n\n### 推荐基础模型的独特挑战\n除了训练大型模型的基础设施挑战外，推荐系统还有其独特难题，特别是**实体冷启动**。\n\n*   **冷启动问题**：Netflix频繁添加新内容，模型需要预测用户对新内容的偏好，即使这些内容尚未被互动。\n*   **解决方案**：\n    *   **增量训练**：\n        *   由于数据集庞大，频繁重新训练不切实际。\n        *   通过重用旧模型参数并为新标题初始化新参数来“热启动”新模型。\n        *   新标题嵌入可以通过对现有平均嵌入添加随机噪声或基于元数据加权组合相似标题的嵌入来初始化。\n    *   **处理未见实体**：\n        *   即使增量训练也无法保证高效学习所有新实体。\n        *   基础模型必须结合实体元数据信息，而不仅仅是用户互动数据。\n        *   模型结合了可学习的项目ID嵌入和可学习的元数据嵌入。\n        *   ![图片 2](https://cdn-images-1.medium.com/max/1024/0*7qnfUGWgXtVUjhP9)\n        *   最终标题嵌入通过混合层结合元数据嵌入和ID嵌入，并使用基于实体“年龄”的注意力机制：新标题更多依赖元数据，成熟标题更多依赖ID嵌入。训练中引入随机性鼓励模型从元数据中学习。\n\n### 下游应用与挑战\n该推荐基础模型旨在理解用户的长期偏好，并可用于多种下游应用：\n\n*   **直接作为预测模型**：模型主要训练用于预测用户下一个互动实体，包含多个预测头以满足不同业务需求（如预测用户对不同类型的偏好）。\n*   **利用嵌入**：\n    *   生成用户和实体（视频、游戏、类型）的嵌入，用于离线和在线应用。\n    *   可作为其他模型的特征或用于候选生成（如检索吸引用户的标题）。\n    *   高质量的标题嵌入支持标题到标题的推荐。\n    *   **挑战**：嵌入空间维度任意且不可解释，且在不同模型训练运行中不兼容。\n    *   **解决方案**：应用正交低秩变换来稳定用户/项目嵌入空间，确保嵌入维度含义的一致性。\n*   **特定数据微调**：模型具有适应性，可使用特定应用数据进行微调（整个模型或子图）。",
      "shortSummary": "Netflix正在开发一个个性化推荐基础模型，旨在解决现有推荐系统的高维护成本和创新迁移难题。该模型借鉴LLM的数据中心方法和半监督学习，通过对用户互动进行分词和处理异构信息，并采用多目标预测。为应对新内容冷启动问题，模型支持增量训练，并结合ID嵌入和元数据嵌入。其输出的嵌入可直接用于预测或作为下游应用的特征，并通过正交变换确保嵌入空间稳定性。",
      "translated_title": "个性化推荐基础模型",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*1dhdoLxKnf_fcZOq",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*7qnfUGWgXtVUjhP9",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*dEypYqp643q6GcVzn3IIww.png",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1a0bd8e02d39",
          "alt": "",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<p>By <a href=\"https://www.linkedin.com/in/markhsiao/\">Ko-Jen Hsiao</a>, <a href=\"https://www.linkedin.com/in/yesufeng/\">Yesu Feng</a> and <a href=\"https://www.linkedin.com/in/sudarshanlamkhede/\">Sudarshan Lamkhede</a></p><h3>Motivation</h3><p>Netflix’s personalized recommender system is a complex system, boasting a variety of specialized machine learned models each catering to distinct needs including “Continue Watching” and “Today’s Top Picks for You.” (Refer to our recent <a href=\"https://videorecsys.com/slides/mark_talk3.pdf\">overview</a> for more details). However, as we expanded our set of personalization algorithms to meet increasing business needs, maintenance of the recommender system became quite costly. Furthermore, it was difficult to transfer innovations from one model to another, given that most are independently trained despite using common data sources. This scenario underscored the need for a new recommender system architecture where member preference learning is centralized, enhancing accessibility and utility across different models.</p><p>Particularly, these models predominantly extract features from members’ recent interaction histories on the platform. Yet, many are confined to a brief temporal window due to constraints in serving latency or training costs. This limitation has inspired us to develop a foundation model for recommendation. This model aims to assimilate information both from members’ comprehensive interaction histories and our content at a very large scale. It facilitates the distribution of these learnings to other models, either through shared model weights for fine tuning or directly through embeddings.</p><p>The impetus for constructing a foundational recommendation model is based on the paradigm shift in natural language processing (NLP) to large language models (LLMs). In NLP, the trend is moving away from numerous small, specialized models towards a single, large language model that can perform a variety of tasks either directly or with minimal fine-tuning. Key insights from this shift include:</p><ol><li><strong>A Data-Centric Approach</strong>: Shifting focus from model-centric strategies, which heavily rely on feature engineering, to a data-centric one. This approach prioritizes the accumulation of large-scale, high-quality data and, where feasible, aims for end-to-end learning.</li><li><strong>Leveraging Semi-Supervised Learning</strong>: The next-token prediction objective in LLMs has proven remarkably effective. It enables large-scale semi-supervised learning using unlabeled data while also equipping the model with a surprisingly deep understanding of world knowledge.</li></ol><p>These insights have shaped the design of our foundation model, enabling a transition from maintaining numerous small, specialized models to building a scalable, efficient system. By scaling up semi-supervised training data and model parameters, we aim to develop a model that not only meets current needs but also adapts dynamically to evolving demands, ensuring sustainable innovation and resource efficiency.</p><h3>Data</h3><p>At Netflix, user engagement spans a wide spectrum, from casual browsing to committed movie watching. With over 300 million users at the end of 2024, this translates into hundreds of billions of interactions — an immense dataset comparable in scale to the token volume of large language models (LLMs). However, as in LLMs, the quality of data often outweighs its sheer volume. To harness this data effectively, we employ a process of interaction tokenization, ensuring meaningful events are identified and redundancies are minimized.</p><p><strong>Tokenizing User Interactions</strong>: Not all raw user actions contribute equally to understanding preferences. Tokenization helps define what constitutes a meaningful “token” in a sequence. Drawing an analogy to Byte Pair Encoding (BPE) in NLP, we can think of tokenization as merging adjacent actions to form new, higher-level tokens. However, unlike language tokenization, creating these new tokens requires careful consideration of what information to retain. For instance, the total watch duration might need to be summed or engagement types aggregated to preserve critical details.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*1dhdoLxKnf_fcZOq\" /><figcaption><strong>Figure 1.</strong>Tokenization of user interaction history by merging actions on the same title, preserving important information.</figcaption></figure><p>This tradeoff between granular data and sequence compression is akin to the balance in LLMs between vocabulary size and context window. In our case, the goal is to balance the length of interaction history against the level of detail retained in individual tokens. Overly lossy tokenization risks losing valuable signals, while too granular a sequence can exceed practical limits on processing time and memory.</p><p>Even with such strategies, interaction histories from active users can span thousands of events, exceeding the capacity of transformer models with standard self attention layers. In recommendation systems, context windows during inference are often limited to hundreds of events — not due to model capability but because these services typically require millisecond-level latency. This constraint is more stringent than what is typical in LLM applications, where longer inference times (seconds) are more tolerable.</p><p>To address this during training, we implement two key solutions:</p><ol><li><strong>Sparse Attention Mechanisms</strong>: By leveraging sparse attention techniques such as low-rank compression, the model can extend its context window to several hundred events while maintaining computational efficiency. This enables it to process more extensive interaction histories and derive richer insights into long-term preferences.</li><li><a href=\"https://arxiv.org/abs/2409.14517\"><strong>Sliding Window Sampling</strong></a>: During training, we sample overlapping windows of interactions from the full sequence. This ensures the model is exposed to different segments of the user’s history over multiple epochs, allowing it to learn from the entire sequence without requiring an impractically large context window.</li></ol><p>At inference time, when multi-step decoding is needed, we can deploy KV caching to efficiently reuse past computations and maintain low latency.</p><p>These approaches collectively allow us to balance the need for detailed, long-term interaction modeling with the practical constraints of model training and inference, enhancing both the precision and scalability of our recommendation system.</p><p><strong>Information in Each ‘Token’</strong>: While the first part of our tokenization process focuses on structuring sequences of interactions, the next critical step is defining the rich information contained within each token. Unlike LLMs, which typically rely on a single embedding space to represent input tokens, our interaction events are packed with heterogeneous details. These include attributes of the action itself (such as locale, time, duration, and device type) as well as information about the content (such as item ID and metadata like genre and release country). Most of these features, especially categorical ones, are directly embedded within the model, embracing an end-to-end learning approach. However, certain features require special attention. For example, timestamps need additional processing to capture both absolute and relative notions of time, with absolute time being particularly important for understanding time-sensitive behaviors.</p><p>To enhance prediction accuracy in sequential recommendation systems, we organize token features into two categories:</p><ol><li><strong>Request-Time Features</strong>: These are features available at the moment of prediction, such as log-in time, device, or location.</li><li><strong>Post-Action Features</strong>: These are details available after an interaction has occurred, such as the specific show interacted with or the duration of the interaction.</li></ol><p>To predict the next interaction, we combine request-time features from the current step with post-action features from the <a href=\"https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/18140\">previous step</a>. This blending of contextual and historical information ensures each token in the sequence carries a comprehensive representation, capturing both the immediate context and user behavior patterns over time.</p><h3>Considerations for Model Objective and Architecture</h3><p>As previously mentioned, our default approach employs the autoregressive next-token prediction objective, similar to GPT. This strategy effectively leverages the vast scale of unlabeled user interaction data. The adoption of this objective in recommendation systems has shown multiple successes [1–3]. However, given the distinct differences between language tasks and recommendation tasks, we have made several critical modifications to the objective.</p><p>Firstly, during the pretraining phase of typical LLMs, such as GPT, every target token is generally treated with equal weight. In contrast, in our model, not all user interactions are of equal importance. For instance, a 5-minute trailer play should not carry the same weight as a 2-hour full movie watch. A greater challenge arises when trying to align long-term user satisfaction with specific interactions and recommendations. To address this, we can adopt a multi-token prediction objective during training, where the model predicts the next <em>n</em> tokens at each step instead of a single token[4]. This approach encourages the model to capture longer-term dependencies and avoid myopic predictions focused solely on immediate next events.</p><p>Secondly, we can use multiple fields in our input data as auxiliary prediction objectives in addition to predicting the next item ID, which remains the primary target. For example, we can derive genres from the items in the original sequence and use this genre sequence as an auxiliary target. This approach serves several purposes: it acts as a regularizer to reduce overfitting on noisy item ID predictions, provides additional insights into user intentions or long-term genre preferences, and, when structured hierarchically, can improve the accuracy of predicting the target item ID. By first predicting auxiliary targets, such as genre or original language, the model effectively narrows down the candidate list, simplifying subsequent item ID prediction.</p><h3>Unique Challenges for Recommendation FM</h3><p>In addition to the infrastructure challenges posed by training bigger models with substantial amounts of user interaction data that are common when trying to build foundation models, there are several unique hurdles specific to recommendations to make them viable. One of unique challenges is entity cold-starting.</p><p>At Netflix, our mission is to entertain the world. New titles are added to the catalog frequently. Therefore the recommendation foundation models require a cold start capability, which means the models need to estimate members’ preferences for newly launched titles before anyone has engaged with them. To enable this, our foundation model training framework is built with the following two capabilities: Incremental training and being able to do inference with unseen entities.</p><ol><li><strong>Incremental training </strong>: Foundation models are trained on extensive datasets, including every member’s history of plays and actions, making frequent retraining impractical. However, our catalog and member preferences continually evolve. Unlike large language models, which can be incrementally trained with stable token vocabularies, our recommendation models require new embeddings for new titles, necessitating expanded embedding layers and output components. To address this, we warm-start new models by reusing parameters from previous models and initializing new parameters for new titles. For example, new title embeddings can be initialized by adding slight random noise to existing average embeddings or by using a weighted combination of similar titles’ embeddings based on metadata. This approach allows new titles to start with relevant embeddings, facilitating faster fine-tuning. In practice, the initialization method becomes less critical when more member interaction data is used for fine-tuning.</li><li><strong>Dealing with unseen entities </strong>: Even with incremental training, it’s not always guaranteed to learn efficiently on new entities (ex: newly launched titles). It’s also possible that there will be some new entities that are not included/seen in the training data even if we fine-tune foundation models on a frequent basis. Therefore, it’s also important to let foundation models use metadata information of entities and inputs, not just member interaction data. Thus, our foundation model combines both learnable item id embeddings and learnable embeddings from metadata. The following diagram demonstrates this idea.</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*7qnfUGWgXtVUjhP9\" /><figcaption><strong>Figure 2. </strong>Titles are associated with various metadata, such as genres, storylines, and tones. Each type of metadata could be represented by averaging its respective embeddings, which are then concatenated to form the overall metadata-based embedding for the title.</figcaption></figure><p>To create the final title embedding, we combine this metadata-based embedding with a fully-learnable ID-based embedding using a mixing layer. Instead of simply summing these embeddings, we use an attention mechanism based on the “age” of the entity. This approach allows new titles with limited interaction data to rely more on metadata, while established titles can depend more on ID-based embeddings. Since titles with similar metadata can have different user engagement, their embeddings should reflect these differences. Introducing some randomness during training encourages the model to learn from metadata rather than relying solely on ID embeddings. This method ensures that newly-launched or pre-launch titles have reasonable embeddings even with no user interaction data.</p><h3>Downstream Applications and Challenges</h3><p>Our recommendation foundation model is designed to understand long-term member preferences and can be utilized in various ways by downstream applications:</p><ol><li><strong>Direct Use as a Predictive Model </strong>The model is primarily trained to predict the next entity a user will interact with. It includes multiple predictor heads for different tasks, such as forecasting member preferences for various genres. These can be directly applied to meet diverse business needs..</li><li><strong>Utilizing embeddings </strong>The model generates valuable embeddings for members and entities like videos, games, and genres. These embeddings are calculated in batch jobs and stored for use in both offline and online applications. They can serve as features in other models or be used for candidate generation, such as retrieving appealing titles for a user. High-quality title embeddings also support title-to-title recommendations. However, one important consideration is that the embedding space has arbitrary, uninterpretable dimensions and is incompatible across different model training runs. This poses challenges for downstream consumers, who must adapt to each retraining and redeployment, risking bugs due to invalidated assumptions about the embedding structure. To address this, we apply an orthogonal low-rank transformation to stabilize the user/item embedding space, ensuring consistent meaning of embedding dimensions, even as the base foundation model is retrained and redeployed.</li><li><strong>Fine-Tuning with Specific Data </strong>The model’s adaptability allows for fine-tuning with application-specific data. Users can integrate the full model or subgraphs into their own models, fine-tuning them with less data and computational power. This approach achieves performance comparable to previous models, despite the initial foundation model requiring significant resources.</li></ol><h3>Scaling Foundation Models for Netflix Recommendations</h3><p>In scaling up our foundation model for Netflix recommendations, we draw inspiration from the success of large language models (LLMs). Just as LLMs have demonstrated the power of scaling in improving performance, we find that scaling is crucial for enhancing generative recommendation tasks. Successful scaling demands robust evaluation, efficient training algorithms, and substantial computing resources. Evaluation must effectively differentiate model performance and identify areas for improvement. Scaling involves data, model, and context scaling, incorporating user engagement, external reviews, multimedia assets, and high-quality embeddings. Our experiments confirm that the scaling law also applies to our foundation model, with consistent improvements observed as we increase data and model size.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*dEypYqp643q6GcVzn3IIww.png\" /><figcaption><strong>Figure 3. </strong>The relationship between model parameter size and relative performance improvement. The plot demonstrates the scaling law in recommendation modeling, showing a trend of increased performance with larger model sizes. The x-axis is logarithmically scaled to highlight growth across different magnitudes.</figcaption></figure><h3>Conclusion</h3><p>In conclusion, our Foundation Model for Personalized Recommendation represents a significant step towards creating a unified, data-centric system that leverages large-scale data to increase the quality of recommendations for our members. This approach borrows insights from Large Language Models (LLMs), particularly the principles of semi-supervised learning and end-to-end training, aiming to harness the vast scale of unlabeled user interaction data. Addressing unique challenges, like cold start and presentation bias, the model also acknowledges the distinct differences between language tasks and recommendation. The Foundation Model allows various downstream applications, from direct use as a predictive model to generate user and entity embeddings for other applications, and can be fine-tuned for specific canvases. We see promising results from downstream integrations. This move from multiple specialized models to a more comprehensive system marks an exciting development in the field of personalized recommendation systems.</p><h3>Acknowledgements</h3><p>Contributors to this work (name in alphabetical order): <a href=\"https://www.linkedin.com/in/aileisun/\">Ai-Lei Sun</a> <a href=\"https://www.linkedin.com/in/aishafenton/\">Aish Fenton</a> <a href=\"https://www.linkedin.com/in/annecocos/\">Anne Cocos</a> <a href=\"https://www.linkedin.com/in/foranuj/\">Anuj Shah</a> <a href=\"https://www.linkedin.com/in/arashaghevli/\">Arash Aghevli</a> <a href=\"https://www.linkedin.com/in/baolin-li-659426115/\">Baolin Li</a> <a href=\"https://www.linkedin.com/in/bowei-yan-0080a326/\">Bowei Yan</a> <a href=\"https://www.linkedin.com/in/danielzheng256/\">Dan Zheng</a> <a href=\"https://www.linkedin.com/in/dwliang/\">Dawen Liang</a> <a href=\"https://www.linkedin.com/in/ding-tong-2812785a/\">Ding Tong</a> <a href=\"https://www.linkedin.com/in/divya-gadde-3ba01551/\">Divya Gadde</a> <a href=\"https://www.linkedin.com/in/emma-yanyang-kong-6904b457/\">Emma Kong</a> <a href=\"https://www.linkedin.com/in/gary-y-62175170/\">Gary Yeh</a> <a href=\"https://www.linkedin.com/in/inbar-naor-6b973a50/\">Inbar Naor</a> <a href=\"https://www.linkedin.com/in/jinwangw/\">Jin Wang</a> <a href=\"https://www.linkedin.com/in/jbasilico/\">Justin Basilico</a> <a href=\"https://www.linkedin.com/in/kabir-nagrecha/overlay/about-this-profile/\">Kabir Nagrecha</a> <a href=\"https://www.linkedin.com/in/kzielnicki/\">Kevin Zielnicki</a> <a href=\"https://www.linkedin.com/in/linasbaltrunas/\">Linas Baltrunas</a> <a href=\"https://www.linkedin.com/in/lingyi-liu-4b866016/\">Lingyi Liu</a> <a href=\"https://www.linkedin.com/in/lequn-luke-wang-9226b2129/\">Luke Wang</a> <a href=\"https://www.linkedin.com/in/matan-appelbaum-39472b96/\">Matan Appelbaum</a> <a href=\"https://www.linkedin.com/in/tuzhucheng/\">Michael Tu</a> <a href=\"https://www.linkedin.com/in/moumitab/\">Moumita Bhattacharya</a> <a href=\"https://www.linkedin.com/in/pabloadelgado/\">Pablo Delgado</a> <a href=\"https://www.linkedin.com/in/qiuling-xu-a445b815a/\">Qiuling Xu</a> <a href=\"https://www.linkedin.com/in/rakeshkomuravelli/\">Rakesh Komuravelli</a> <a href=\"https://www.linkedin.com/in/raveeshbhalla/\">Raveesh Bhalla</a> <a href=\"https://www.linkedin.com/in/rob-story-b21a4912/\">Rob Story</a> <a href=\"https://www.linkedin.com/in/rogermenezes/\">Roger Menezes</a> <a href=\"https://www.linkedin.com/in/sejoon-oh/\">Sejoon Oh</a> <a href=\"https://www.linkedin.com/in/shahrzad-naseri-1b988760/\">Shahrzad Naseri</a> <a href=\"https://www.linkedin.com/in/swanandjoshi7/\">Swanand Joshi</a> <a href=\"https://www.linkedin.com/in/trungnguyen324/\">Trung Nguyen</a> <a href=\"https://www.linkedin.com/in/vito-ostuni-0b576027/\">Vito Ostuni </a><a href=\"https://www.linkedin.com/in/thomasweiwang/\">Wei Wang</a> <a href=\"https://www.linkedin.com/in/zhezhangncsu/\">Zhe Zhang</a></p><h3>Reference</h3><ol><li>C. K. Kang and J. McAuley, “Self-Attentive Sequential Recommendation,” <em>2018 IEEE International Conference on Data Mining (ICDM)</em>, Singapore, 2018, pp. 197–206, doi: 10.1109/ICDM.2018.00035.</li><li>F. Sun et al., “BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer,” <em>Proceedings of the 28th ACM International Conference on Information and Knowledge Management (CIKM ‘19)</em>, Beijing, China, 2019, pp. 1441–1450, doi: 10.1145/3357384.3357895.</li><li>J. Zhai et al., “Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations,” <em>arXiv preprint arXiv:2402.17152</em>, 2024.</li><li>F. Gloeckle, B. Youbi Idrissi, B. Rozière, D. Lopez-Paz, and G. Synnaeve, “Better &amp; Faster Large Language Models via Multi-token Prediction,” arXiv preprint arXiv:2404.19737, Apr. 2024.</li></ol><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1a0bd8e02d39\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://netflixtechblog.com/foundation-model-for-personalized-recommendation-1a0bd8e02d39\">Foundation Model for Personalized Recommendation</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "HDR10+ 现已在 Netflix 上线 (原标题: HDR10+ Now Streaming on Netflix)",
      "link": "https://netflixtechblog.com/hdr10-now-streaming-on-netflix-c9ab1f4bd72b?source=rss----2615bd06b42e---4",
      "pubDate": "Mon, 24 Mar 2025 18:39:15 GMT",
      "isoDate": "2025-03-24T18:39:15.000Z",
      "creator": "Netflix Technology Blog",
      "summary": "# Netflix 现已上线 HDR10+ 流媒体服务\n\nNetflix 宣布，其服务现已开始为支持 AV1 的设备提供 HDR10+ 内容流媒体服务，旨在增强经认证的 HDR10+ 设备的观看体验。这些设备此前仅能接收 HDR10 内容。HDR10+ 内容中包含的动态元数据，能够显著提升在这些设备上观看时的画面质量和准确性。\n\n## 提升会员画质体验\n\n*   **HDR 技术的先驱：** 近十年前，Netflix 率先采用了高动态范围（HDR）技术，该技术能使图像呈现更多细节、更鲜艳的色彩和更高的真实感。\n*   **HDR 生态系统增长：**\n    *   Netflix 开始制作、编码并为会员流式传输 HDR 节目和电影。\n    *   过去五年中，HDR 流媒体播放量增长超过 300%，观看 Netflix 的 HDR 配置设备数量翻了一倍多。\n    *   自《马可·波罗》第一季推出 HDR 以来，Netflix 现有超过 11,000 小时的 HDR 影片供会员观看。\n*   **引入 HDR10+ 的目的：**\n    *   进一步增强 Netflix 不断增长的 HDR 生态系统。\n    *   在更多设备上保留创作者的艺术意图。\n    *   提供更具沉浸感的观看体验。\n\n## 技术实现：AV1 视频编解码器\n\n*   **AV1 的应用：** Netflix 使用由开放媒体联盟 (AOM) 于 2018 年标准化的 AV1 视频编解码器启用 HDR10+。AV1 是目前最高效的编解码器之一。\n*   **AV1 的优势：**\n    *   此前已为 SDR 内容启用 AV1 编码，为会员带来了更高的视觉质量、更低的播放延迟和更高分辨率的流媒体播放。\n    *   AV1-SDR 已成为 Netflix 第二大流媒体编解码器，仅次于 H.264/AVC。\n    *   随着 HDR10+ 流媒体的加入，预计 AV1 将很快成为 Netflix 最主要的流媒体编解码器。\n*   **HDR10+ 内容扩展：**\n    *   Netflix 已将 HDR10+ 流媒体添加到新发布和现有热门 HDR 影片中。\n    *   目前，AV1-HDR10+ 占所有符合条件观看时长的 50%。\n    *   目标是在今年年底前，为所有 HDR 影片提供 HDR10+ 体验。\n\n## 行业主流 HDR 格式\n\n目前，行业公认的三种主流 HDR 格式是：Dolby Vision、HDR10 和 HDR10+。所有这三种格式都在内容中嵌入元数据，作为指导播放设备（如电视、移动设备或电脑）如何显示图像的指令。\n\n*   **HDR10：**\n    *   最广泛采用的 HDR 格式，所有 HDR 设备都支持。\n    *   使用静态元数据，对整个内容定义一次，包括最大内容光照水平 (MaxCLL)、最大帧平均光照水平 (MaxFALL) 以及用于色彩分级的母版显示器特性。\n    *   这种元数据仅允许对显示设备进行“一刀切”的色调映射，无法处理大多数内容中存在的动态对比度变化。\n*   **HDR10+ 和 Dolby Vision：**\n    *   通过动态元数据改进了 HDR10。\n    *   动态元数据提供逐帧的图像统计信息，从而实现对每个场景的优化色调映射调整。\n    *   这能实现更高的视觉保真度，更忠实地保留创作者的艺术意图。\n\n## HDR10 与 HDR10+ 的对比\n\n以下图片展示了同一 AV1 编码内容在 HDR10（上图）和 HDR10+（下图）下显示时的屏幕截图。\n\n![HDR10 图像](https://cdn-images-1.medium.com/max/1024/1*AjnQRaY7VFZoonX5SI36IA.png)\n*图1：使用 HDR10 元数据显示的设备照片。*\n\n![HDR10+ 图像](https://cdn-images-1.medium.com/max/1024/1*gsW42hweG6RMbWwQjy1etg.png)\n*图2：使用 HDR10+ 元数据显示的设备照片。请注意 HDR10+ 捕获中手电筒细节的保留，以及 HDR10 中手电筒下方区域的过度曝光。*\n\n从图片中桌子上的手电筒可以看出，HDR10 内容中的高光细节被裁剪，但在 HDR10+ 中得到了恢复。此外，HDR10 内容中手电筒下方的区域过度曝光，而 HDR10+ 则以更高的保真度渲染了该区域。HDR10+ 凭借其动态元数据在此示例中表现出色，原因在于该帧前后场景的亮度统计数据明显不同。静态的 HDR10 元数据无法适应内容的变化。尽管这是一个简单的示例，但 HDR10+ 中的动态元数据在任何场景中都展现了其价值。这种一致性使会员能够沉浸在内容中，并更好地保留创作者的艺术意图。\n\n## 接收 HDR10+ 的要求\n\n要接收 HDR10+ 内容，需满足以下条件：\n\n1.  会员必须订阅 Netflix 高级套餐。\n2.  影片必须提供 HDR10+ 格式。\n3.  会员设备必须支持 AV1 和 HDR10+。兼容设备示例包括：\n    *   符合 Netflix HDR10+ 认证的智能电视、手机和平板电脑。\n    *   通过 HDMI 连接到符合 HDR10+ 标准显示器的源设备（如机顶盒、流媒体设备等），并符合 Netflix HDR10+ 认证。\n4.  对于电视或流媒体设备，请确保在 Netflix 应用程序设置中启用了 HDR 开关。\n\n## 总结\n\nNetflix 上每天观看的 HDR 内容越来越多。将 Netflix HDR 生态系统扩展到包含 HDR10+，增加了更多会员访问带有动态元数据的 HDR 内容的机会，改善了观看体验，并保留了内容创作者的艺术意图。对创新和质量的承诺，彰显了 Netflix 为所有会员提供沉浸式和真实观看体验的决心。\n\n**内容摄取说明：** 尽管 Netflix 已为分发启用 HDR10+，但其在内容摄取端（即所有内容交付给 Netflix 时）仍仅接受 Dolby Vision 母版，并从中高效地大规模派生出 HDR10+、HDR10 和 Dolby Vision 格式以供分发。",
      "shortSummary": "Netflix 现已为支持 AV1 的设备上线 HDR10+ 流媒体服务，以提升观看体验。HDR10+ 采用动态元数据，相比 HDR10 的静态元数据，能更精确地呈现画面细节并保留创作者意图。Netflix 的 HDR 内容已超 1.1 万小时，AV1-HDR10+ 已占符合条件观看时长的 50%，目标是年底前所有 HDR 影片都支持 HDR10+。观看 HDR10+ 需高级套餐和兼容设备。此举旨在扩展 HDR 生态系统，提供更沉浸的体验。",
      "translated_title": "HDR10+ 现已在 Netflix 上线",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*AjnQRaY7VFZoonX5SI36IA.png",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*gsW42hweG6RMbWwQjy1etg.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c9ab1f4bd72b",
          "alt": "",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<p><a href=\"https://www.linkedin.com/in/rquero\">Roger Quero</a>, <a href=\"https://www.linkedin.com/in/liwei-guo\">Liwei Guo</a>, <a href=\"https://www.linkedin.com/in/jeffrwatts/\">Jeff Watts</a>, <a href=\"https://www.linkedin.com/in/joseph-mccormick-7b386026\">Joseph McCormick</a>, <a href=\"https://www.linkedin.com/in/agataopalach/\">Agata Opalach</a>, <a href=\"https://www.linkedin.com/in/anush-moorthy-b8451142/\">Anush Moorthy</a></p><p>We are excited to announce that we are now streaming HDR10+ content on our service for AV1-enabled devices, enhancing the viewing experience for certified HDR10+ devices, which previously only received HDR10 content. The dynamic metadata included in our HDR10+ content improves the quality and accuracy of the picture when viewed on these devices.</p><h3>Delighting Members with Even Better Picture Quality</h3><p>Nearly a decade ago, we made a bold move to be a pioneering adopter of High Dynamic Range (HDR) technology. HDR enables images to have more details, vivid colors, and improved realism. We began producing our shows and movies in HDR, encoding them in HDR, and streaming them in HDR for our members. We were confident that it would greatly enhance our members’ viewing experience, and unlock new creative visions — and we were right! In the last five years, HDR streaming has increased by more than 300%, while the number of HDR-configured devices watching Netflix has more than doubled. Since launching HDR with season one of <em>Marco Polo</em>, Netflix now has over 11,000 hours of HDR titles for members to immerse themselves in.</p><p>We continue to enhance member joy while maintaining creative vision by adding support for HDR10+. This will further augment Netflix’s growing HDR ecosystem, preserve creative intent on even more devices, and provide a more immersive viewing experience.</p><p>We enabled HDR10+ on Netflix using the <a href=\"https://aomedia.org/specifications/av1/\">AV1 video codec</a> that was standardized by the Alliance for Open Media (AOM) in 2018. AV1 is one of the most efficient codecs available today. We <a href=\"https://netflixtechblog.com/bringing-av1-streaming-to-netflix-members-tvs-b7fc88e42320\">previously enabled</a> AV1 encoding for SDR content, and saw tremendous value for our members, including higher and more consistent visual quality, lower play delay and increased streaming at the highest resolution. AV1-SDR is already the second most streamed codec at Netflix, behind H.264/AVC, which has been around for over 20 years! With the addition of HDR10+ streams to AV1, we expect the day is not far when AV1 will be the most streamed codec at Netflix.</p><p>To enhance our offering, we have been adding HDR10+ streams to both new releases and existing popular HDR titles. AV1-HDR10+ now accounts for 50% of all eligible viewing hours. We will continue expanding our HDR10+ offerings with the goal of providing an HDR10+ experience for all HDR titles by the end of this year¹.</p><h3><strong>Industry Adopted Formats</strong></h3><p>Today, the industry recognizes three prevalent HDR formats: Dolby Vision, HDR10, and HDR10+. For all three HDR Formats, metadata is embedded in the content, serving as instructions to guide the playback device — whether it’s a TV, mobile device, or computer — on how to display the image.</p><p>HDR10 is the most widely adopted HDR format, supported by all HDR devices. HDR10 uses static metadata that is defined once for the entire content detailing aspects such as the maximum content light level (MaxCLL), maximum frame average light level (MaxFALL), as well as characteristics of the mastering display used for color grading. This metadata only allows for a one-size-fits-all tone mapping of the content for display devices. It cannot account for dynamic contrast across scenes, which most content contains.</p><p>HDR10+ and Dolby Vision improve on this with dynamic metadata that provides content image statistics on a per-frame basis, enabling optimized tone mapping adjustments for each scene. This achieves greater perceptual fidelity to the original, preserving creative intent.</p><h3><strong>HDR10 vs. HDR10+</strong></h3><p>The figure below shows screen grabs of two AV1-encoded frames of the same content displayed using HDR10 (top) and HDR10+ (bottom).</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*AjnQRaY7VFZoonX5SI36IA.png\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*gsW42hweG6RMbWwQjy1etg.png\" /></figure><p><em>Photographs of devices displaying the same frame with HDR10 metadata (top) and HDR10+ metadata (bottom). Notice the preservation of the flashlight detail in the HDR10+ capture, and the over-exposure of the region under the flashlight in the HDR10 one².</em></p><p>As seen in the flashlight on the table, the highlight details are clipped in the HDR10 content, but are recovered in HDR10+. Further, the region under the flashlight is overexposed in the HDR10 content, while HDR10+ renders that region with greater fidelity to the source. The reason HDR10+, with its dynamic metadata, shines in this example is that the scenes preceding and following the scene with this frame have markedly different luminance statistics. The static HDR10 metadata is unable to account for the change in the content. While this is a simple example, the dynamic metadata in HDR10+ demonstrates such value across any set of scenes. This consistency allows our members to stay immersed in the content, and better preserves creative intent.</p><h3><strong>Receiving HDR10+</strong></h3><p>At the time of launch, these requirements must be satisfied to receive HDR10+:</p><p>1.Member must have a Netflix Premium plan subscription</p><p>2. Title must be available in HDR10+ format</p><p>3. Member device must support AV1 &amp; HDR10+. Here are some examples of compatible devices:</p><ul><li>SmartTVs, mobile phones, and tablets that meet Netflix certification for HDR10+</li><li>Source device (such as set-top boxes, streaming devices, MVPDs, etc.) that meets Netflix certification for HDR10+, connected to an HDR10+ compliant display via HDMI</li></ul><p>4. For TV or streaming devices, ensure that the HDR toggle is enabled in our Netflix application settings: <a href=\"https://help.netflix.com/en/node/100220\">https://help.netflix.com/en/node/100220</a></p><p>Additional guidance: <a href=\"https://help.netflix.com/en/node/13444\">https://help.netflix.com/en/node/13444</a></p><h3>Summary</h3><p>More HDR content is watched every day on Netflix. Expanding the Netflix HDR ecosystem to include HDR10+ increases the accessibility of HDR content with dynamic metadata to more members, improves the viewing experience, and preserves the creative intent of our content creators. The commitment to innovation and quality underscores our dedication to delivering an immersive and authentic viewing experience for all our members.</p><h3>Acknowledgements</h3><p>Launching HDR10+ was a collaborative effort involving multiple teams at Netflix, and we are grateful to everyone who contributed to making this idea a reality. We would like to extend our thanks to the following teams for their crucial roles in this launch:</p><ul><li>The various Client and Partner Engineering teams at Netflix that manage the Netflix experience across different device platforms.<br>Special acknowledgments: <a href=\"https://www.linkedin.com/in/akshaygarg05/\">Akshay Garg</a>, <a href=\"https://www.linkedin.com/in/dashap/\">Dasha Polyakova</a>, <a href=\"https://www.linkedin.com/in/wei-vivian-li/\">Vivian Li</a>, <a href=\"https://www.linkedin.com/in/benjamintoofer/\">Ben Toofer</a>, <a href=\"https://www.linkedin.com/in/allanzp/\">Allan Zhou</a>, <a href=\"https://www.linkedin.com/in/artemdanylenko/\">Artem Danylenko</a></li><li>The Encoding Technologies team that is responsible for producing optimized encodings to enable high-quality experiences for our members. Special acknowledgments: <a href=\"https://www.linkedin.com/in/adithyaprakash/\">Adithya Prakash</a>, <a href=\"https://www.linkedin.com/in/carvalhovinicius/\">Vinicius Carvalho</a></li><li>The Content Operations &amp; Innovation teams responsible for producing and delivering HDR content to Netflix, maintaining the intent of creative vision from production to streaming. Special acknowledgements: <a href=\"https://www.linkedin.com/in/michael-keegan-072a4950/\">Michael Keegan</a></li><li>The Product Discover team that enables seamless UI discovery journey for our members. Special acknowledgments: <a href=\"https://www.linkedin.com/in/chad-mckee/\">Chad McKee</a> <a href=\"https://www.linkedin.com/in/ramyasomaskandan/\">Ramya Somaskandan</a></li><li>The Playback Experience team that delivers the best possible experience to our members. Special acknowledgments: <a href=\"https://www.linkedin.com/in/nate-santti/\">Nate Santti</a></li></ul><h4>Footnotes</h4><ol><li>While we have enabled HDR10+ for distribution i.e., for what our members consume on their devices, we continue to accept only Dolby Vision masters on the ingest side, i.e., for all content delivery to Netflix as per our <a href=\"https://partnerhelp.netflixstudios.com/hc/en-us/sections/360012197873-Branded-Delivery-Specifications\">delivery specification</a>. In addition to HDR10+, we continue to serve HDR10 and DolbyVision. Our encoding pipeline is designed with flexibility and extensibility where all these HDR formats could be derived from a single DolbyVision deliverable efficiently at scale.</li><li>We recognize that it is hard to convey visual improvements in HDR video using still photographs converted to SDR. We encourage the reader to stream Netflix content in HDR10+ and check for yourself!</li></ol><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c9ab1f4bd72b\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://netflixtechblog.com/hdr10-now-streaming-on-netflix-c9ab1f4bd72b\">HDR10+ Now Streaming on Netflix</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "Netflix 规模化影片发布可观测性 (原标题: Title Launch Observability at Netflix Scale)",
      "link": "https://netflixtechblog.com/title-launch-observability-at-netflix-scale-8efe69ebd653?source=rss----2615bd06b42e---4",
      "pubDate": "Wed, 05 Mar 2025 01:24:53 GMT",
      "isoDate": "2025-03-05T01:24:53.000Z",
      "creator": "Netflix Technology Blog",
      "summary": "本文是关于 Netflix 影片发布可观测性系列文章的第三部分，重点探讨了实现大规模影片全面可观测性的策略、工具和方法。",
      "shortSummary": "Netflix 实现了大规模影片发布可观测性，通过引入“标题健康”端点，遵循准确反映生产行为、标准化和“洞察三元组”原则。其系统架构包括主动监控、实时曝光量跟踪、基于 Hollow Feeds 的数据存储、可观测性仪表盘以及独特的“时间旅行”预发布验证功能，旨在确保影片成功发布并提升用户体验。",
      "translated_title": "Netflix 规模化影片发布可观测性",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/760/0*8s2gCb2Pqw2Q0Frq",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*P-0nxUAHve77yBtv",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*aeo7vs3h2Z5JKH5t",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*1h2cwZDfmz8nis_h",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*dBFS1pBlqNoCUHwV",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<h4>Part 3: System Strategies and Architecture</h4><p><strong>By:</strong> <a href=\"https://www.linkedin.com/in/varun-khaitan/\">Varun Khaitan</a></p><p>With special thanks to my stunning colleagues: <a href=\"https://www.linkedin.com/in/mallikarao/\">Mallika Rao</a>, <a href=\"https://www.linkedin.com/in/esmir-mesic/\">Esmir Mesic</a>, <a href=\"https://www.linkedin.com/in/hugodesmarques/\">Hugo Marques</a></p><p>This blog post is a continuation of <a href=\"https://netflixtechblog.com/title-launch-observability-at-netflix-scale-19ea916be1ed\">Part 2</a>, where we cleared the ambiguity around title launch observability at Netflix. In this installment, we will explore the strategies, tools, and methodologies that were employed to achieve comprehensive title observability at scale.</p><h3>Defining the observability endpoint</h3><p>To create a comprehensive solution, we decided to introduce observability endpoints first. Each microservice involved in our <strong>Personalization stack</strong> that integrated with our observability solution had to introduce a new “Title Health” endpoint. Our goal was for each new endpoint to adhere to a few principles:</p><ol><li>Accurate reflection of production behavior</li><li>Standardization across all endpoints</li><li>Answering the Insight Triad: “Healthy” or not, why not and how to fix it.</li></ol><p><strong>Accurately Reflecting Production Behavior</strong></p><p>A key part of our solution is insights into production behavior, which necessitates our requests to the endpoint result in traffic to the real service functions that mimics the same pathways the traffic would take if it came from the usual callers.</p><p>In order to allow for this mimicking, many systems implement an “event” handling, where they convert our request into a call to the real service with properties enabled to log when titles are filtered out of their response and why. Building services that adhere to software best practices, such as Object-Oriented Programming (OOP), the SOLID principles, and modularization, is crucial to have success at this stage. Without these practices, service endpoints may become tightly coupled to business logic, making it challenging and costly to add a new endpoint that seamlessly integrates with the observability solution while following the same production logic.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/760/0*8s2gCb2Pqw2Q0Frq\" /><figcaption><em>A service with modular business logic facilitates the seamless addition of an observability endpoint.</em></figcaption></figure><p><strong>Standardization</strong></p><p>To standardize communication between our observability service and the personalization stack’s observability endpoints, we’ve developed a stable proto request/response format. This centralized format, defined and maintained by our team, ensures all endpoints adhere to a consistent protocol. As a result, requests are uniformly handled, and responses are processed cohesively. This standardization enhances adoption within the personalization stack, simplifies the system, and improves understanding and debuggability for engineers.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*P-0nxUAHve77yBtv\" /><figcaption><em>The request schema for the observability endpoint.</em></figcaption></figure><p><strong>The Insight Triad API</strong></p><p>To efficiently understand the health of a title and triage issues quickly, all implementations of the observability endpoint must answer: is the title eligible for this phase of promotion, if not — why is it not eligible, and what can be done to fix any problems.</p><p>The end-users of this observability system are Launch Managers, whose job it is to ensure smooth title launches. As such, they must be able to quickly see whether there is a problem, what the problem is, and how to solve it. Teams implementing the endpoint must provide as much information as possible so that a non-engineer (Launch Manager) can understand the root cause of the issue and fix any title setup issues as they arise. They must also provide enough information for partner engineers to identify the problem with the underlying service in cases of system-level issues.</p><p>These requirements are captured in the following protobuf object that defines the endpoint response.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*aeo7vs3h2Z5JKH5t\" /><figcaption><em>The response schema for the observability endpoint.</em></figcaption></figure><h3>High level architecture</h3><p>We’ve distilled our comprehensive solution into the following key steps, capturing the essence of our approach:</p><ol><li>Establish observability endpoints across all services within our Personalization and Discovery Stack.</li><li>Implement proactive monitoring for each of these endpoints.</li><li>Track real-time title impressions from the Netflix UI.</li><li>Store the data in an optimized, highly distributed datastore.</li><li>Offer easy-to-integrate APIs for our dashboard, enabling stakeholders to track specific titles effectively.</li><li>“Time Travel” to validate ahead of time.</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*1h2cwZDfmz8nis_h\" /><figcaption><em>Observability stack high level architecture diagram</em></figcaption></figure><p>In the following sections, we will explore each of these concepts and components as illustrated in the diagram above.</p><h3>Key Features</h3><h4>Proactive monitoring through scheduled collectors jobs</h4><p>Our Title Health microservice runs a scheduled collector job every 30 minutes for most of our personalization stack.</p><p>For each Netflix row we support (such as Trending Now, Coming Soon, etc.), there is a dedicated collector. These collectors retrieve the relevant list of titles from our catalog that qualify for a specific row by interfacing with our catalog services. These services are informed about the expected subset of titles for each row, for which we are assessing title health.</p><p>Once a collector retrieves its list of candidate titles, it orchestrates batched calls to assigned row services using the above standardized schema to retrieve all the relevant health information of the titles. Additionally, some collectors will instead poll our kafka queue for impressions data.</p><h4>Real-time Title Impressions and Kafka Queue</h4><p>In addition to evaluating title health via our personalization stack services, we also keep an eye on how our recommendation algorithms treat titles by reviewing impressions data. It’s essential that our algorithms treat all titles equitably, for each one has limitless potential.</p><p>This data is processed from a real-time impressions stream into a Kafka queue, which our title health system regularly polls. Specialized collectors access the Kafka queue every two minutes to retrieve impressions data. This data is then aggregated in minute(s) intervals, calculating the number of impressions titles receive in near-real-time, and presented as an additional health status indicator for stakeholders.</p><h4>Data storage and distribution through Hollow Feeds</h4><p><a href=\"https://hollow.how/\">Netflix Hollow</a> is an Open Source java library and toolset for disseminating in-memory datasets from a single producer to many consumers for high performance read-only access. Given the shape of our data, hollow feeds are an excellent strategy to distribute the data across our service boxes.</p><p>Once collectors gather health data from partner services in the personalization stack or from our impressions stream, this data is stored in a dedicated Hollow feed for each collector. Hollow offers numerous features that help us monitor the overall health of a Netflix row, including ensuring there are no large-scale issues across a feed publish. It also allows us to track the history of each title by maintaining a per-title data history, calculate differences between previous and current data versions, and roll back to earlier versions if a problematic data change is detected.</p><h4>Observability Dashboard using Health Check Engine</h4><p>We maintain several dashboards that utilize our title health service to present the status of titles to stakeholders. These user interfaces access an endpoint in our service, enabling them to request the current status of a title across all supported rows. This endpoint efficiently reads from all available Hollow Feeds to obtain the current status, thanks to Hollow’s in-memory capabilities. The results are returned in a standardized format, ensuring easy support for future UIs.</p><p>Additionally, we have other endpoints that can summarize the health of a title across subsets of sections to highlight specific member experiences.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*dBFS1pBlqNoCUHwV\" /><figcaption>Message depicting a dashboard request.</figcaption></figure><h4>Time Traveling: Catching before launch</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Zz2Y8yjPAsbG5WVR\" /></figure><p>Titles launching at Netflix go through several phases of pre-promotion before ultimately launching on our platform. For each of these phases, the first several hours of promotion are critical for the reach and effective personalization of a title, especially once the title has launched. Thus, to prevent issues as titles go through the launch lifecycle, our observability system needs to be capable of simulating traffic ahead of time so that relevant teams can catch and fix issues before they impact members. We call this capability <strong>“Time Travel”</strong>.</p><p>Many of the metadata and assets involved in title setup have specific timelines for when they become available to members. To determine if a title will be viewable at the start of an experience, we must simulate a request to a partner service as if it were from a future time when those specific metadata or assets are available. This is achieved by including a future timestamp in our request to the observability endpoint, corresponding to when the title is expected to appear for a given experience. The endpoint then communicates with any further downstream services using the context of that future timestamp.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*jrdqpJmp0lzna6Zc\" /><figcaption>An example request with a future timestamp.</figcaption></figure><h3>Conclusion</h3><p>Throughout this series, we’ve explored the journey of enhancing title launch observability at Netflix. In <a href=\"https://netflixtechblog.com/title-launch-observability-at-netflix-scale-c88c586629eb\">Part 1</a>, we identified the challenges of managing vast content launches and the need for scalable solutions to ensure each title’s success. <a href=\"https://netflixtechblog.com/title-launch-observability-at-netflix-scale-19ea916be1ed\">Part 2</a> highlighted the strategic approach to navigating ambiguity, introducing “Title Health” as a framework to align teams and prioritize core issues. In this final part, we detailed the sophisticated system strategies and architecture, including observability endpoints, proactive monitoring, and “Time Travel” capabilities; all designed to ensure a thrilling viewing experience.</p><p>By investing in these innovative solutions, we enhance the discoverability and success of each title, fostering trust with content creators and partners. This journey not only bolsters our operational capabilities but also lays the groundwork for future innovations, ensuring that every story reaches its intended audience and that every member enjoys their favorite titles on Netflix.</p><p>Thank you for joining us on this exploration, and stay tuned for more insights and innovations as we continue to entertain the world.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8efe69ebd653\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://netflixtechblog.com/title-launch-observability-at-netflix-scale-8efe69ebd653\">Title Launch Observability at Netflix Scale</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "Netflix 印象系统介绍 (原标题: Introducing Impressions at Netflix)",
      "link": "https://netflixtechblog.com/introducing-impressions-at-netflix-e2b67c88c9fb?source=rss----2615bd06b42e---4",
      "pubDate": "Sat, 15 Feb 2025 01:13:20 GMT",
      "isoDate": "2025-02-15T01:13:20.000Z",
      "creator": "Netflix Technology Blog",
      "summary": "# Netflix 印象系统介绍：构建印象数据的真实来源\n\n本文是Netflix关于其印象（Impressions）系统系列博客的第一部分，重点介绍了如何为印象数据建立一个“真实来源”（Source-of-Truth, SOT）数据集。在Netflix，用户界面上展示的每个电影海报或宣传横幅都被称为“印象”，它们是驱动个性化引擎的关键数据点，旨在将简单的浏览转化为沉浸式的观看体验。\n\n## 印象历史的重要性\n\nNetflix需要印象历史数据来：\n\n*   **增强个性化：** 追踪用户已看到但未互动的内，从而提供新鲜且更具吸引力的推荐。\n*   **频率限制（Frequency Capping）：** 避免重复展示相同内容，保持观看体验的活力，减少用户疲劳。\n*   **突出新发布内容：** 监控新内容的初始用户互动，调整推广策略以提高可见性和参与度。\n*   **分析洞察：** 提供平台相关的分析信息，例如评估主页特定行的表现或营销策略的有效性。\n\n## 架构概览：构建真实来源\n\n管理印象的第一步是创建SOT数据集，它是支持各种下游工作流和用例的基础。\n\n### 1. 收集原始印象事件\n\n*   当Netflix用户与界面互动时，会产生大量原始事件。\n*   这些事件从客户端迅速传输到服务器，进入一个集中的事件处理队列。\n*   自定义事件提取器处理队列中的数据，识别并提取所有印象事件。\n*   提取的事件同时路由到：\n    *   **Apache Kafka 主题：** 用于即时处理需求。\n    *   **Apache Iceberg 表：** 用于长期保留和历史分析。\n*   这种双路径方法利用了Kafka的低延迟流处理能力和Iceberg对大规模、不可变数据集的高效管理，确保了实时响应性和全面的历史数据可用性。\n    ![图片 1](https://cdn-images-1.medium.com/max/1024/0*T6tQiUj-VDtyEhd1)\n    *印象在主页上的展示*\n    ![图片 2](https://cdn-images-1.medium.com/max/1024/0*4NRQp10pg9KK_GKU)\n    *收集原始印象事件*\n\n### 2. 过滤和丰富原始印象\n\n*   无状态的Apache Flink作业负责处理排队后的原始印象事件。\n*   它过滤掉无效条目，并用额外的元数据（如影视剧标题、展示页面和行位置）丰富有效数据。\n*   精炼后的输出使用Avro schema进行结构化，成为Netflix印象数据的权威真实来源。\n*   丰富后的数据通过Kafka（实时应用）和Apache Iceberg表（历史分析）无缝访问。\n    ![图片 3](https://cdn-images-1.medium.com/max/1024/0*Lhs-gvhMuIyKylHt)\n    *印象真实来源架构*\n\n### 3. 确保高质量印象\n\n*   通过收集详细的列级指标（如验证标识符、检查关键列填充情况）来确保印象数据的高质量。\n*   收集到的数据输入到全面的质量仪表板，并支持分层阈值警报系统。\n*   警报系统及时通知潜在问题，以便迅速解决回归。\n*   在数据丰富过程中，确保所有列之间的一致性，并尽可能进行就地修正以提供准确数据。\n    ![图片 4](https://cdn-images-1.medium.com/max/736/0*VWssCnOIabEqo02H)\n    *显示两个列（entityId和videoId）之间不匹配计数的仪表板*\n\n## 配置\n\n*   **数据量：** 全球每秒处理100万到150万个印象事件，每个事件约1.2KB。\n*   **技术栈：** 使用Apache Flink进行低延迟流处理，支持批处理和流处理，实现历史数据回填和实时与历史分析的一致性。\n*   **Flink 配置：** 每个区域8个任务管理器，每个配备8个CPU核心和32GB内存，并行度为48。\n*   **数据输出：** Flink作业的sink配备数据网格连接器，输出到Kafka（实时数据流）和Iceberg（历史数据保存）。\n*   **部署模型：** 采用“孤岛模型”（island model），所有依赖项位于单个区域内，确保高可用性，一个区域降级不影响其他区域。\n    ![图片 5](https://cdn-images-1.medium.com/max/1024/0*B-hm-UJMBV7-WOb6)\n    *每秒原始印象记录数*\n\n## 未来工作\n\n*   **解决非模式化事件的挑战：** 引入模式管理，在保持灵活性的同时提供数据清晰度。\n*   **使用自动伸缩器自动化性能调优：** 集成自动伸缩器，根据工作负载动态调整资源，优化性能和资源利用。\n*   **改进数据质量警报：** 构建更智能的数据质量平台，识别印象流中的异常，跟踪数据血缘和数据治理，并向生产者发送回归警报，减少误报。\n\n## 结论\n\n为印象数据创建可靠的真实来源是一项复杂但至关重要的任务，它能显著增强个性化和内容发现体验。本系列下一部分将深入探讨如何利用SOT数据集创建提供印象历史的微服务。",
      "shortSummary": "Netflix利用“印象”（用户界面上的视觉内容曝光）作为关键数据点，驱动其个性化推荐引擎。他们构建了一个强大的系统，每天处理数十亿印象事件，创建了一个“真实来源”数据集。该系统通过Apache Kafka、Flink和Iceberg收集、过滤和丰富数据，确保实时处理和历史分析。印象历史能增强个性化、实现频率限制、突出新内容并提供分析洞察，从而优化用户体验。未来计划包括引入模式管理、自动化性能调优和改进数据质量警报。",
      "translated_title": "Netflix 印象系统介绍",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*T6tQiUj-VDtyEhd1",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*4NRQp10pg9KK_GKU",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*Lhs-gvhMuIyKylHt",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/736/0*VWssCnOIabEqo02H",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*B-hm-UJMBV7-WOb6",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<h4>Part 1: Creating the Source of Truth for Impressions</h4><p><strong>By:</strong> <a href=\"https://www.linkedin.com/in/tulikabhatt/\">Tulika Bhatt</a></p><p>Imagine scrolling through Netflix, where each movie poster or promotional banner competes for your attention. Every image you hover over isn’t just a visual placeholder; it’s a critical data point that fuels our sophisticated personalization engine. At Netflix, we call these images ‘impressions,’ and they play a pivotal role in transforming your interaction from simple browsing into an immersive binge-watching experience, all tailored to your unique tastes.</p><p>Capturing these moments and turning them into a personalized journey is no simple feat. It requires a state-of-the-art system that can track and process these impressions while maintaining a detailed history of each profile’s exposure. This nuanced integration of data and technology empowers us to offer bespoke content recommendations.</p><p>In this multi-part blog series, we take you behind the scenes of our system that processes billions of impressions daily. We will explore the challenges we encounter and unveil how we are building a resilient solution that transforms these client-side impressions into a personalized content discovery experience for every Netflix viewer.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*T6tQiUj-VDtyEhd1\" /><figcaption>Impressions on homepage</figcaption></figure><h3>Why do we need impression history?</h3><h4>Enhanced Personalization</h4><p>To tailor recommendations more effectively, it’s crucial to track what content a user has already encountered. Having impression history helps us achieve this by allowing us to identify content that has been displayed on the homepage but not engaged with, helping us deliver fresh, engaging recommendations.</p><h4>Frequency Capping</h4><p>By maintaining a history of impressions, we can implement frequency capping to prevent over-exposure to the same content. This ensures users aren’t repeatedly shown identical options, keeping the viewing experience vibrant and reducing the risk of frustration or disengagement.</p><h4>Highlighting New Releases</h4><p>For new content, impression history helps us monitor initial user interactions and adjust our merchandising efforts accordingly. We can experiment with different content placements or promotional strategies to boost visibility and engagement.</p><h4>Analytical Insights</h4><p>Additionally, impression history offers insightful information for addressing a number of platform-related analytics queries. Analyzing impression history, for example, might help determine how well a specific row on the home page is functioning or assess the effectiveness of a merchandising strategy.</p><h3>Architecture Overview</h3><p>The first pivotal step in managing impressions begins with the creation of a Source-of-Truth (SOT) dataset. This foundational dataset is essential, as it supports various downstream workflows and enables a multitude of use cases.</p><h4>Collecting Raw Impression Events</h4><p>As Netflix members explore our platform, their interactions with the user interface spark a vast array of raw events. These events are promptly relayed from the client side to our servers, entering a centralized event processing queue. This queue ensures we are consistently capturing raw events from our global user base.</p><p>After raw events are collected into a centralized queue, a custom event extractor processes this data to identify and extract all impression events. These extracted events are then routed to an Apache Kafka topic for immediate processing needs and simultaneously stored in an Apache Iceberg table for long-term retention and historical analysis. This dual-path approach leverages Kafka’s capability for low-latency streaming and Iceberg’s efficient management of large-scale, immutable datasets, ensuring both real-time responsiveness and comprehensive historical data availability.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*4NRQp10pg9KK_GKU\" /><figcaption>Collecting raw impression events</figcaption></figure><h4>Filtering &amp; Enriching Raw Impressions</h4><p>Once the raw impression events are queued, a stateless Apache Flink job takes charge, meticulously processing this data. It filters out any invalid entries and enriches the valid ones with additional metadata, such as show or movie title details, and the specific page and row location where each impression was presented to users. This refined output is then structured using an Avro schema, establishing a definitive source of truth for Netflix’s impression data. The enriched data is seamlessly accessible for both real-time applications via Kafka and historical analysis through storage in an Apache Iceberg table. This dual availability ensures immediate processing capabilities alongside comprehensive long-term data retention.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Lhs-gvhMuIyKylHt\" /><figcaption>Impression Source-of-Truth architecture</figcaption></figure><h4>Ensuring High Quality Impressions</h4><p>Maintaining the highest quality of impressions is a top priority. We accomplish this by gathering detailed column-level metrics that offer insights into the state and quality of each impression. These metrics include everything from validating identifiers to checking that essential columns are properly filled. The data collected feeds into a comprehensive quality dashboard and supports a tiered threshold-based alerting system. These alerts promptly notify us of any potential issues, enabling us to swiftly address regressions. Additionally, while enriching the data, we ensure that all columns are in agreement with each other, offering in-place corrections wherever possible to deliver accurate data.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/736/0*VWssCnOIabEqo02H\" /><figcaption>Dashboard showing mismatch count between two columns- entityId and videoId</figcaption></figure><h3>Configuration</h3><p>We handle a staggering volume of 1 to 1.5 million impression events globally every second, with each event approximately 1.2KB in size. To efficiently process this massive influx in real-time, we employ Apache Flink for its low-latency stream processing capabilities, which seamlessly integrates both batch and stream processing to facilitate efficient backfilling of historical data and ensure consistency across real-time and historical analyses. Our Flink configuration includes 8 task managers per region, each equipped with 8 CPU cores and 32GB of memory, operating at a parallelism of 48, allowing us to handle the necessary scale and speed for seamless performance delivery. The Flink job’s sink is equipped with a data mesh connector, as detailed in our <a href=\"https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873\">Data Mesh platform</a> which has two outputs: Kafka and Iceberg. This setup allows for efficient streaming of real-time data through Kafka and the preservation of historical data in Iceberg, providing a comprehensive and flexible data processing and storage solution.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*B-hm-UJMBV7-WOb6\" /><figcaption>Raw impressions records per second</figcaption></figure><p>We utilize the ‘island model’ for deploying our Flink jobs, where all dependencies for a given application reside within a single region. This approach ensures high availability by isolating regions, so if one becomes degraded, others remain unaffected, allowing traffic to be shifted between regions to maintain service continuity. Thus, all data in one region is processed by the Flink job deployed within that region.</p><h3>Future Work</h3><h4>Addressing the Challenge of Unschematized Events</h4><p>Allowing raw events to land on our centralized processing queue unschematized offers significant flexibility, but it also introduces challenges. Without a defined schema, it can be difficult to determine whether missing data was intentional or due to a logging error. We are investigating solutions to introduce schema management that maintains flexibility while providing clarity.</p><h4>Automating Performance Tuning with Autoscalers</h4><p>Tuning the performance of our Apache Flink jobs is currently a manual process. The next step is to integrate with autoscalers, which can dynamically adjust resources based on workload demands. This integration will not only optimize performance but also ensure more efficient resource utilization.</p><h4>Improving Data Quality Alerts</h4><p>Right now, there’s a lot of business rules dictating when a data quality alert needs to be fired. This leads to a lot of false positives that require manual judgement. A lot of times it is difficult to track changes leading to regression due to inadequate data lineage information. We are investing in building a comprehensive data quality platform that more intelligently identifies anomalies in our impression stream, keeps track of data lineage and data governance, and also, generates alerts notifying producers of any regressions. This approach will enhance efficiency, reduce manual oversight, and ensure a higher standard of data integrity.</p><h3>Conclusion</h3><p>Creating a reliable source of truth for impressions is a complex but essential task that enhances personalization and discovery experience. Stay tuned for the next part of this series, where we’ll delve into how we use this SOT dataset to create a microservice that provides impression histories. We invite you to share your thoughts in the comments and continue with us on this journey of discovering impressions.</p><h3>Acknowledgments</h3><p>We are genuinely grateful to our amazing colleagues whose contributions were essential to the success of Impressions: Julian Jaffe, Bryan Keller, Yun Wang, Brandon Bremen, Kyle Alford, Ron Brown and Shriya Arora.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e2b67c88c9fb\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://netflixtechblog.com/introducing-impressions-at-netflix-e2b67c88c9fb\">Introducing Impressions at Netflix</a> was originally published in <a href=\"https://netflixtechblog.com\">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    }
  ],
  "lastUpdated": "2025-06-21T04:34:33.281Z"
}