{
  "sourceUrl": "https://slack.engineering/feed/",
  "title": "Engineering at Slack",
  "description": "Hear directly from Slack's engineers about what we build, why and how we build it, and how you can use it.",
  "link": "https://slack.engineering",
  "items": [
    {
      "title": "优化我们的端到端（E2E）流水线 (原标题: Optimizing Our E2E Pipeline)",
      "link": "https://slack.engineering/speedup-e2e-testing/",
      "pubDate": "Mon, 14 Apr 2025 09:00:30 +0000",
      "isoDate": "2025-04-14T09:00:30.000Z",
      "content": "<p>In the world of DevOps and Developer Experience (DevXP), speed and efficiency can make a big difference on an engineer’s day-to-day tasks. Today, we’ll dive into how Slack’s DevXP team took some existing tools and used them to optimize an end-to-end (E2E) testing pipeline. This lowered build times and reduced redundant processes, saving both time&#8230;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://slack.engineering/speedup-e2e-testing/\">Optimizing Our E2E Pipeline</a> appeared first on <a rel=\"nofollow\" href=\"https://slack.engineering\">Engineering at Slack</a>.</p>\n",
      "contentSnippet": "In the world of DevOps and Developer Experience (DevXP), speed and efficiency can make a big difference on an engineer’s day-to-day tasks. Today, we’ll dive into how Slack’s DevXP team took some existing tools and used them to optimize an end-to-end (E2E) testing pipeline. This lowered build times and reduced redundant processes, saving both time…\nThe post Optimizing Our E2E Pipeline appeared first on Engineering at Slack.",
      "creator": "Dan Carton",
      "encodedSnippet": "<p>In the world of DevOps and Developer Experience (DevXP), speed and efficiency can make a big difference on an engineer’s day-to-day tasks. Today, we’ll dive into how Slack’s DevXP team took some existing tools and used them to optimize an end-to-end (E2E) testing pipeline. This lowered build times and reduced redundant processes, saving both time&#8230;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://slack.engineering/speedup-e2e-testing/\">Optimizing Our E2E Pipeline</a> appeared first on <a rel=\"nofollow\" href=\"https://slack.engineering\">Engineering at Slack</a>.</p>\n\nIn the world of DevOps and Developer Experience (DevXP), speed and efficiency can make a big difference on an engineer’s day-to-day tasks. Today, we’ll dive into how Slack’s DevXP team took some existing tools and used them to optimize an end-to-end (E2E) testing pipeline. This lowered build times and reduced redundant processes, saving both time and resources for engineers at Slack.\nThe Problem: Unnecessary Frontend Builds\nFor one of our largest code repositories (a monolithic repository, or monorepo), Slack has a CI/CD pipeline that runs E2E tests before merging code into the <span style=\"font-weight: 400\">main</span> branch. This is critical for ensuring that changes are validated across the entire stack for the Slack application: frontend, backend, database, and the handful of services in between. However, we noticed a bottleneck: building the frontend code was taking longer than expected and occurred too frequently, even when there were no frontend-related changes. Here’s the breakdown:\nDeveloper Workflow: A developer makes changes and pushes to a branch.\nBuild Process: The frontend code is built (~5 minutes).\nDeployment: The build is deployed to a QA environment.\nTesting: We run over 200 E2E tests, taking another 5 minutes.\nThis entire process took about 10 minutes per run. Half of that time, around 5 minutes, was consumed by frontend builds, even when no frontend changes were involved.\n\nGiven that hundreds of pull requests (PRs) are merged daily, these redundant builds were not only time-consuming, but costly:\nThousands of frontend builds per week, storing nearly a gigabyte of data per build in AWS S3.\n\nHalf of these builds do not contain frontend changes compared to the last merge into <span style=\"font-weight: 400\">main</span>, causing terabytes of duplicate data.\n5 minutes per build, adding unnecessary delays to pipelines (thousands of hours a week).\nThe Solution: Smarter Build Strategy with Cached Frontend Assets\nTo tackle this, we leveraged existing tools to rethink our build strategy.\nStep 1: Conditional Frontend Builds\nOur first step was determining whether a fresh frontend build was necessary. We detected changes by utilizing <span style=\"font-weight: 400\">git diff</span> and its 3-dot notation to identify the difference between the latest common commit of the current checked-out branch and <span style=\"font-weight: 400\">main</span>. If changes were detected, we invoke a frontend build job. If no changes were detected, we skipped the build entirely and reused a prebuilt version. \nStep 2: Prebuilt Assets and Internal CDN\nWhen a frontend build is not needed, we locate an existing build from AWS S3. To be efficient, we use a recent frontend build that is still in Production. We delegate the task of serving the prebuilt frontend assets for our E2E tests to an internal CDN. This reduced the need for creating a new build on each PR, while still ensuring we test on current assets.\nThe Challenges: Efficiency at Scale\nWhile the approach seemed straightforward, scaling this solution to our monorepo presented a few challenges:\nIdentifying Frontend Changes: Our repository contains over 100,000 tracked files. Determining whether frontend changes were present required efficient file tracking, which git handled in just a couple of seconds. \nFinding Prebuilt Assets: With hundreds of PRs merged into this repository daily, identifying a prebuilt version that was fresh enough required robust asset management. By using straightforward S3 storage concepts, we were able to balance recency, coherent file naming, and performance to manage our assets.\nBeing Fast: We were able to distinguish if a frontend build was unnecessary and find a recent build artifact in just under 3 seconds on average.\n\n\n\nThe Results: A 60% Drop in Build Frequency and 50% Drop in Build Time\nOur efforts paid off, delivering remarkable improvements:\n60% Reduction in Build Frequency: By intelligently reusing prebuilt frontend assets, we reduced the number of unnecessary frontend builds by over half.\nHundreds of Hours Saved Monthly: Cloud compute time and developer wait times are reduced.\nSeveral Terabytes of Storage Savings: We reduced our AWS S3 storage by several terabytes each month. These duplicate assets would have otherwise been stored for one year.\n50% Build Time Improvement: This was the second major project by the Frontend DevXP Team and its partnering teams. The first project, which upgraded our Webpack setup, reduced the average build from ~10 minutes to ~5 minutes. This project took the average build from ~5 minutes down to just ~2 minutes. With both projects being successful, we reduced our average build time for E2E pipelines from ~10 minutes to ~2 minutes: a huge improvement for the year!\n\nTwo unexpected outcomes:\nMore Reliable and Trustworthy E2E Results: Our test flakiness, which refers to tests failing intermittently or inconsistently despite no code changes, was significantly reduced. This improvement resulted from the optimized pipeline, decreased likelihood of needing complex frontend builds, and consistent asset delivery. We observed our lowest percentage of test flakiness as a result, based on monthly measurements.\nRediscovering Legacy Code: Implementing this optimization required a deep dive into legacy code of multiple systems that hadn’t been significantly modified in a long time. This exploration yielded valuable insights, prompted new questions about the codebase’s behavior, and generated a backlog of tasks for future enhancements.\nConclusion: Rethinking Frontend Build Efficiency\nBy strategically utilizing existing tools like <span style=\"font-weight: 400\">git diff</span> and internal CDNs, we managed to save valuable developer time, reduce cloud costs, and improve overall build efficiency.\nFor teams in other companies facing similar bottlenecks in DevOps and DevXP, the lesson is to question what’s truly necessary in your pipeline and optimize accordingly. The improvement from this project seems obvious in hind-sight, but it’s common to overlook inefficiencies in systems that haven’t outright failed. In our case, rethinking how we handled frontend assets turned into a massive win for the organization.\nAcknowledgments\nThere are a lot of moving parts in a project like this: complex pipelines for building and testing, cloud infrastructure, an internal CDN, intricate build systems for frontend code, and existing custom setups throughout our entire system. It includes code written in Python, JavaScript, Bash, PHP/Hack, Rust, YAML, and Ruby. We achieved this without any downtime! Okay, almost. There was ten minutes of internal downtime for our deployment pipeline, but it was fixed pretty quickly.\nThis work was not possible without contributions from:\nAnirudh Janga, Josh Cartmell, Arminé Iradian, Anupama Jasthi, Matt Jennings, Zack Weeden, John Long, Issac Gerges, Andrew MacDonald, Vani Anantha and Dave Harrington\nInterested in taking on interesting projects, making people’s work lives easier, or just building some pretty cool forms? We’re hiring! \n Explore Opportunities\n\t\t\n \n \nThe post Optimizing Our E2E Pipeline appeared first on Engineering at Slack.",
      "summary": "本文介绍了 Slack 的 DevXP 团队如何优化端到端（E2E）测试流水线，从而降低构建时间和减少冗余流程，为工程师节省时间和资源。\n\n主要内容包括：\n\n*   **问题：** 不必要的前端构建导致耗时增加，即使没有前端代码变更也会触发构建。\n    *   每次运行耗时约 10 分钟，其中前端构建占用约 5 分钟。\n    *   每周数千次前端构建，产生大量的重复数据，浪费存储空间。\n\n    ![Stages of the end to end testing pipeline.](https://slack.engineering/wp-content/uploads/sites/7/2025/04/image4.png?w=640)\n*   **解决方案：** 采用更智能的构建策略，缓存前端资源。\n    *   **条件前端构建：** 使用 `git diff` 检测前端代码是否发生变更，仅在必要时才进行构建。\n    *   **预构建资源和内部 CDN：** 当不需要构建时，从 AWS S3 获取已有的构建版本，并使用内部 CDN 提供预构建的前端资源。\n*   **挑战：** 在大规模代码仓库中实现高效的文件变更跟踪和预构建资源管理。\n    *   使用 Git 快速跟踪文件变更。\n    *   通过 S3 存储管理实现快速查找预构建资源。\n    *   平均在 3 秒内完成判断是否需要构建和查找构建产物。\n\n    ![End to end pipeline depicting local pre-built frontends at the third stage.](https://slack.engineering/wp-content/uploads/sites/7/2025/04/image3.png?w=640)\n*   **结果：** 构建频率降低 60%，构建时间缩短 50%。\n    *   每月节省数百小时的云计算时间和开发者等待时间。\n    *   每月减少数 TB 的 AWS S3 存储。\n    *   E2E 流水线的平均构建时间从 ~10 分钟减少到 ~2 分钟。\n\n    ![January baseline was an average build time of 10 minutes and 6 minutes for testing. By August average build time was 2 minutes, an 80% decrease for the year.](https://slack.engineering/wp-content/uploads/sites/7/2025/04/image1.png?w=640)\n*   **意外收获：** E2E 测试结果的可靠性和可信度提高，并重新发现了遗留代码。\n\n*   **结论：** 通过优化前端构建效率，可以节省开发者时间，降低云成本，并提高整体构建效率。",
      "translated_title": "优化我们的端到端（E2E）流水线",
      "images": [
        {
          "url": "https://slack.engineering/wp-content/uploads/sites/7/2025/04/image4.png?w=640",
          "alt": "Stages of the end to end testing pipeline.",
          "title": "",
          "position": 1
        },
        {
          "url": "https://slack.engineering/wp-content/uploads/sites/7/2025/04/image3.png?w=640",
          "alt": "End to end pipeline depicting local pre-built frontends at the third stage.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://slack.engineering/wp-content/uploads/sites/7/2025/04/image1.png?w=640",
          "alt": "January baseline was an average build time of 10 minutes and 6 minutes for testing. By August average build time was 2 minutes, an 80% decrease for the year.",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "完整文章"
    },
    {
      "title": "我们如何构建安全和私有的企业搜索 (原标题: How we built enterprise search to be secure and private)",
      "link": "https://slack.engineering/how-we-built-enterprise-search-to-be-secure-and-private/",
      "pubDate": "Fri, 07 Mar 2025 01:14:37 +0000",
      "isoDate": "2025-03-07T01:14:37.000Z",
      "content": "<p>Many don’t know that “Slack” is in fact a backronym—it stands for “Searchable Log of all Communication and Knowledge”. And these days, it’s not just a searchable log: with Slack AI, Slack is now an intelligent log, leveraging the latest in generative AI to securely surface powerful, time-saving insights. We built Slack AI from the&#8230;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://slack.engineering/how-we-built-enterprise-search-to-be-secure-and-private/\">How we built enterprise search to be secure and private</a> appeared first on <a rel=\"nofollow\" href=\"https://slack.engineering\">Engineering at Slack</a>.</p>\n",
      "contentSnippet": "Many don’t know that “Slack” is in fact a backronym—it stands for “Searchable Log of all Communication and Knowledge”. And these days, it’s not just a searchable log: with Slack AI, Slack is now an intelligent log, leveraging the latest in generative AI to securely surface powerful, time-saving insights. We built Slack AI from the…\nThe post How we built enterprise search to be secure and private appeared first on Engineering at Slack.",
      "creator": "Ian Hoffman",
      "encodedSnippet": "<p>Many don’t know that “Slack” is in fact a backronym—it stands for “Searchable Log of all Communication and Knowledge”. And these days, it’s not just a searchable log: with Slack AI, Slack is now an intelligent log, leveraging the latest in generative AI to securely surface powerful, time-saving insights. We built Slack AI from the&#8230;</p>\n<p>The post <a rel=\"nofollow\" href=\"https://slack.engineering/how-we-built-enterprise-search-to-be-secure-and-private/\">How we built enterprise search to be secure and private</a> appeared first on <a rel=\"nofollow\" href=\"https://slack.engineering\">Engineering at Slack</a>.</p>\n\nMany don’t know that “Slack” is in fact a backronym—it stands for “Searchable Log of all Communication and Knowledge”. And these days, it’s not just a searchable log: with Slack AI, Slack is now an intelligent log, leveraging the latest in generative AI to securely surface powerful, time-saving insights. We built Slack AI from the ground up to be secure and private following principles that mirror our existing enterprise grade compliance standards:\n\n\nCustomer data never leaves Slack.\nWe do not train large language models (LLMs) on customer data.\nSlack AI only operates on the data that the user can already see.\nSlack AI integrates seamlessly with our existing enterprise grade compliance and security offerings.\nNow, with enterprise search, Slack is more than a log of all content and knowledge inside Slack—it also includes knowledge from your key applications. Users can now surface up-to-date, relevant content that is permissioned to them directly in Slack’s search. We’re starting with Google Drive and GitHub, and you’ll see many more of your connected apps as the year goes on. With these new apps, Slack search and AI Answers are all the more powerful, pulling in context from across key tools to satisfy your queries.\nWe built enterprise search to uphold the same Enterprise-grade security and privacy standards as Slack AI:\nWe never store data from external sources in our databases.\nExternal data and permissions are up to date with the external system.\nUsers and admins must explicitly grant Slack access to external sources and may revoke that access at any time.\nWe uphold the principle of least privilege, only requesting the authorizations we need to satisfy search queries.\nThis blog post will explain how these principles guided the architecture of enterprise search.\nHow enterprise search upholds the Slack AI principles\nFirst, a refresher: how does Slack AI uphold our security principles?\n\n\nSlack uses AWS to host closed-source large language models (LLMs) in an escrow VPC. This structure ensures that the model provider never has access to Slack customer data and customer data never leaves Slack’s trust boundary—whether it’s Slack messages, enterprise search results, or anything in between.\nWe use Retrieval Augmented Generation (RAG) instead of training LLMs. Using RAG, we supply an LLM with only the content needed to complete the task. This content is permissioned to the user and only available to the LLM at runtime, meaning the LLM doesn’t retain any of your data, ever.\nTo provide a private, permissions-aware AI product, Slack uses the requesting user’s Access Control List (ACL) to ensure that the LLM only receives data the user can already access in Slack.\nFinally, we re-use all our existing compliance infrastructure (such as Encryption Key Management and International Data Residency) when storing and processing LLM-generated content. And we don’t even store Search Answer summaries—we just show them to the requesting user and immediately discard them.\nEnterprise search is built atop Slack AI and benefits from many of the innovations we developed for Slack AI. We use the same LLMs in the same escrow VPC; we use RAG to avoid training LLMs on user data; and we don’t store Search Answers in the database (whether or not they contain external content). However, enterprise search adds a new twist. We can now provide permissioned content from external sources to the LLM and in your search results. \nHow enterprise search upholds our security principles\nWe never store data from external sources\nWhen developing enterprise search, we decided not to store external source data in our database. Instead, we opted for a federated, real-time approach. Building atop Slack’s app platform, we use public search APIs from our partners to return the most up-to-date, permissioned results for a given user. Note that the Slack client may cache data between reloads to performantly serve product features like filtering and previews.\n\nExternal data and permissions are up to date \nWhen searching for external data, it’s essential that we only fetch data which the user can access in the external system (this mirrors our Slack AI principle #3, “Slack AI only operates on the data that the user can already see”) and that this data is up to date.\nUsing a real-time instead of an index-based approach helps us uphold this principle. Because we’re always fetching data from external sources in response to a user query, we never risk that data getting stale. There’s nothing stored on our side, so staleness simply isn’t possible. \nBut how do we scope down queries to just data that the querying user can access in the external system? The Slack platform already provides powerful primitives for connecting external systems to Slack, chief among them being OAuth. The OAuth protocol allows a user to securely authorize Slack to take agreed-upon actions on their behalf, like reading files the user can access in the external system. By leveraging OAuth, we ensure that enterprise search can never perform an action the user did not authorize the system to perform in the external system, and that the actions we perform are a subset of those the user could themselves perform.\nUsers must explicitly grant access to external sources\nWe believe that your external data should be yours to control. As such, Slack admins must opt in each external source for use in their organization’s search results and Search Answers. They can also revoke this access (for both search results and Search Answers) at any time.\nNext, Slack users also explicitly grant access before we integrate any external sources in their search. Users may also revoke access to any source at any time. This level of control is possible due to the OAuth-based approach mentioned above.\nPrinciple of Least Privilege\nAn important security principle is that a system should never request more privileges than it requires. For enterprise search, this means that when we connect to an external system, we only request the OAuth scopes which are necessary to satisfy search queries—specifically read scopes.\nNot only do we adhere to the principle of least privilege, we show admins and end users the scopes we plan to request when they enable an external source for use in enterprise search. This means that admins and end users always know which authorizations Slack requires to integrate with an external source.\nConclusion\nAt Salesforce, trust is our #1 value. We’re proud to have built an enterprise search experience that puts security and privacy front and center, building atop the robust security principles already instilled by Slack AI. We’re excited to see how our customers use this powerful new functionality, secure in the knowledge that their external data is always in good hands.\nThe post How we built enterprise search to be secure and private appeared first on Engineering at Slack.",
      "summary": "本文介绍了Slack如何构建企业搜索功能，以确保安全和隐私，并将其与Slack AI集成。\n\n*   **主要观点：** Slack构建企业搜索时，遵循了与Slack AI相同的安全和隐私原则，包括：\n    *   客户数据永远不会离开Slack。\n    *   不在客户数据上训练大型语言模型（LLM）。\n    *   Slack AI仅在用户已经可以看到的数据上运行。\n    *   与现有的企业级合规性和安全产品无缝集成。\n\n*   **企业搜索的构建方式：**\n    *   使用AWS托管闭源LLM，确保模型提供商无法访问Slack客户数据。\n    *   使用检索增强生成（RAG），仅向LLM提供完成任务所需的内容，避免LLM保留用户数据。\n    *   使用用户的访问控制列表（ACL），确保LLM仅接收用户可以访问的数据。\n    *   重用现有的合规性基础设施。\n    *   不存储搜索答案摘要。\n\n*   **企业搜索如何维护安全原则：**\n    *   不存储来自外部来源的数据，而是采用联合的实时方法，使用合作伙伴的公共搜索API。\n    *   确保外部数据和权限是最新的，始终从外部来源获取数据。\n    *   用户必须明确授予对外部来源的访问权限，管理员也可以随时撤销访问权限。\n    *   坚持最小权限原则，仅请求满足搜索查询所需的授权。\n\n*   **集成应用：**\n    *   目前支持Google Drive和GitHub，未来将支持更多连接的应用。\n\n*   **结论：** Slack的企业搜索体验将安全和隐私放在首位，并建立在Slack AI已经灌输的强大的安全原则之上。",
      "translated_title": "我们如何构建安全和私有的企业搜索",
      "contentSource": "RSS"
    }
  ],
  "lastUpdated": "2025-05-28T21:13:54.378Z",
  "processingStats": {
    "totalItems": 2,
    "newItems": 0,
    "existingItems": 2,
    "lastProcessed": "2025-05-28T21:13:54.378Z"
  }
}