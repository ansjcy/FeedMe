{
  "sourceUrl": "https://medium.com/feed/airbnb-engineering",
  "title": "The Airbnb Tech Blog - Medium",
  "description": "Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io - Medium",
  "link": "https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4",
  "items": [
    {
      "title": "在 Airbnb 使用 Impulse 进行负载测试 (原标题: Load Testing with Impulse at Airbnb)",
      "link": "https://medium.com/airbnb-engineering/load-testing-with-impulse-at-airbnb-f466874d03d2?source=rss----53c7c27702d5---4",
      "pubDate": "Mon, 09 Jun 2025 17:45:39 GMT",
      "isoDate": "2025-06-09T17:45:39.000Z",
      "creator": "Chenhao Yang",
      "summary": "# Airbnb 的 Impulse 负载测试实践\n\n## 引言\n系统级负载测试对于确保系统可靠性和效率至关重要，它能帮助识别瓶颈、评估峰值流量下的容量、建立性能基线并检测错误。在 Airbnb 这样规模和复杂度的公司，负载测试需要具备健壮性、灵活性和去中心化特性，以支持工程团队进行与 CI 无缝集成的自助式负载测试。\n\nImpulse 是 Airbnb 内部的“负载测试即服务”框架之一。它提供工具来生成合成负载、模拟依赖项以及从生产环境中收集流量数据。本文介绍了 Impulse 的架构，以及它如何最大限度地减少手动工作、无缝集成到可观测性堆栈中，并赋能团队主动解决潜在问题。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/1*3LijjQrJDLVA_ptfeRe83g.jpeg)\n\n## 架构\nImpulse 是一个全面的负载测试框架，允许服务所有者进行上下文感知的负载测试、模拟依赖项并收集流量数据，以确保系统在各种条件下的性能。它包含以下核心组件：\n\n![图片 2](https://cdn-images-1.medium.com/max/1024/1*SFDblGijyiLQfI7C5RpPXw.png)\n*图 1: Impulse 框架及其四个主要组件*\n\n1.  **负载生成器 (Load Generator)**：\n    *   用于根据合成或收集的流量，动态生成上下文感知的请求，以测试不同场景。\n    *   **上下文感知**：允许用户使用 Java 或 Kotlin 编写任意测试逻辑，并大规模启动容器来运行这些测试。选择代码而非 DSL/配置的原因在于：\n        *   **灵活性**：编程语言比 DSL 更具表达力，能更好地支持复杂的上下文场景。\n        *   **可重用性**：相同的测试代码可用于其他测试（如集成测试）。\n        *   **开发者熟练度**：学习曲线低，无需学习新的测试逻辑编写方式。\n        *   **开发者体验**：支持 IDE、测试、调试等。\n    *   **去中心化与容器化**：每次触发负载测试时，都会创建一组新的容器来运行测试，这带来了：\n        *   **隔离性**：不同服务间的负载测试相互隔离，消除干扰。\n        *   **可伸缩性**：容器数量可根据流量需求进行伸缩。\n        *   **成本效益**：容器生命周期短，仅在负载测试运行期间存在。\n    *   Impulse 的负载生成器能将工作负载均匀分布到所有数据中心，并确保总的每秒触发次数 (TPS) 符合配置，从而更好地模拟生产环境中的真实流量分布。\n    *   **执行**：设计用于在 CI/CD 流水线中自动触发。开发者可以配置多个测试阶段（如预热、稳定、峰值），每个阶段可配置测试用例、TPS 和持续时间。\n\n    ![图片 3](https://cdn-images-1.medium.com/max/977/1*WD4EWyWHDQMf_7nDIGkAuA.png)\n    *图 2: 容器化负载生成器*\n\n2.  **依赖模拟器 (Dependency Mocker)**：\n    *   用于模拟下游服务的响应，包括延迟，从而使被测服务 (SUT) 的负载测试无需涉及某些依赖服务。这对于不支持负载测试的供应商服务或在日常部署中不影响下游服务进行回归测试尤为重要。\n    *   Impulse 是一个去中心化框架，每个服务都有自己的依赖模拟器，以消除服务间的干扰并降低通信成本。\n    *   模拟器是进程外服务，独立运行，避免影响 SUT 性能。它们是短生命周期的，仅在测试运行前启动，测试结束后关闭，以节省成本和维护工作。\n    *   响应延迟和异常可配置，模拟器实例数量可按需调整以支持大量流量。\n    *   **其他值得注意的特性**：\n        *   可选择性地模拟部分依赖项，目前支持 HTTP JSON、Airbnb Thrift 和 Airbnb GraphQL 依赖。\n        *   支持负载测试之外的用例，如集成测试。\n    *   **两种模拟响应生成选项**：\n        *   **合成响应 (Synthetic response)**：由用户逻辑生成，类似于集成测试，但响应来自远程（进程外）服务器，并模拟延迟。\n        *   **重放响应 (Replay response)**：从生产下游记录中重放响应，由流量收集器组件支持。\n\n    ![图片 4](https://cdn-images-1.medium.com/max/1024/1*QXMa3Nj3-aSUE_EOvlv1xw.png)\n    *图 3: 依赖模拟器*\n\n3.  **流量收集器 (Traffic Collector)**：\n    *   旨在捕获生产环境中的上游和下游流量及其之间的关系。\n    *   通过复制下游响应（包括生产环境般的延迟和错误），Impulse 能够准确地重放生产流量进行负载测试，避免下游数据或行为的不一致。\n    *   确保测试环境中的服务行为与生产环境中的服务行为一致，从而实现更真实、更可靠的性能评估。\n\n    ![图片 5](https://cdn-images-1.medium.com/max/1024/1*EImg3JUEGzbos5r3_6U-FQ.png)\n    *图 4: 流量收集器*\n\n4.  **测试 API 生成器 (Testing API Generator)**：\n    *   解决事件驱动、异步工作流（如消息队列事件、延迟作业）的测试难题。\n    *   在 CI 阶段根据事件或作业模式创建 HTTP API。这些 API 作为底层异步流的包装器，并专门注册在测试环境中。\n    *   使负载测试工具（如负载生成器）能够向这些合成 API 发送流量，从而使异步流像同步流一样被执行。\n    *   目标是帮助开发者识别异步流实现中的性能瓶颈和潜在问题，以及在高流量条件下的表现，通过绕过消息队列等中间件组件，简化负载测试过程。\n\n    ![图片 6](https://cdn-images-1.medium.com/max/660/1*F3qllm7qqMu4N2k0bBFbdQ.png)\n    *图 5: 异步流的测试 API 生成器*\n\n## 与其他测试框架的集成\nImpulse 的模块化设计促进了其与其他内部测试框架（如集成测试和 API 测试）的集成，为系统服务测试提供了系统化的方法。\n\n![图片 7](https://cdn-images-1.medium.com/max/1024/1*649CFxbpASxHotVVbqdQkQ.png)\n*图 6: Impulse 如何与 Airbnb 其他内部测试框架交互*\n\n## 结论\nImpulse 及其四个核心组件帮助 Airbnb 的开发者进行自助式负载测试。它已在多个客户支持后端服务中实施，并获得了积极反馈，例如：它帮助识别并解决了线程池压力导致的 `ApiClientThreadToolExhaustionException`、客户端 API 调用中的偶发超时错误以及主服务容器中的高内存使用问题，从而优化了资源分配。Impulse 被高度推荐作为开发和测试流程不可或缺的一部分。",
      "shortSummary": "Airbnb 开发了 Impulse，一个内部的“负载测试即服务”框架，旨在实现健壮、灵活和去中心化的自助式负载测试。它包含负载生成器、依赖模拟器、流量收集器和测试 API 生成器四个核心组件。Impulse 能够生成上下文感知负载、模拟依赖、重放生产流量并测试异步工作流。通过与 CI/CD 无缝集成，Impulse 帮助团队主动识别并解决性能瓶颈、评估系统容量，从而提高服务可靠性和效率，并最大限度地减少手动工作。",
      "translated_title": "在 Airbnb 使用 Impulse 进行负载测试",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*3LijjQrJDLVA_ptfeRe83g.jpeg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*SFDblGijyiLQfI7C5RpPXw.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/977/1*WD4EWyWHDQMf_7nDIGkAuA.png",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*QXMa3Nj3-aSUE_EOvlv1xw.png",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*EImg3JUEGzbos5r3_6U-FQ.png",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<p>Comprehensive Load Testing with Load Generator, Dependency Mocker, Traffic Collector, and More</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*3LijjQrJDLVA_ptfeRe83g.jpeg\" /></figure><p>Authors: <a href=\"https://www.linkedin.com/in/chenhao-yang-9799b022/\">Chenhao Yang</a>, <a href=\"https://www.linkedin.com/in/haoyue-wang-a722509a/\">Haoyue Wang</a>, <a href=\"https://www.linkedin.com/in/xiaoyawei/\">Xiaoya Wei</a>, <a href=\"https://www.linkedin.com/in/zhijie-guan/\">Zay Guan</a>, <a href=\"https://www.linkedin.com/in/yaolin-chen-591a31339/\">Yaolin Chen</a> and <a href=\"https://www.linkedin.com/in/fei-yuan/\">Fei Yuan</a></p><p>System-level load testing is crucial for reliability and efficiency. It identifies bottlenecks, evaluates capacity for peak traffic, establishes performance baselines, and detects errors. At a company of Airbnb’s size and complexity, we’ve learned that load testing needs to be robust, flexible, and decentralized. This requires the right set of tools to enable engineering teams to do self-service load tests that integrate seamlessly with CI.</p><p>Impulse is one of our internal load-testing-as-a-service frameworks. It provides tools that can generate synthetic loads, mock dependencies, and collect traffic data from production environments. In this blog post, we’ll share how Impulse is architected to minimize manual effort, seamlessly integrate with our observability stack, and empower teams to proactively address potential issues.</p><h3>Architecture</h3><p>Impulse is a comprehensive load testing framework that allows service owners to conduct context-aware load tests, mock dependencies, and collect traffic data to ensure the system’s performance under various conditions. It includes the following components:</p><ol><li><strong>Load generator</strong> to generate context-aware requests on the fly, for testing different scenarios with synthetic or collected traffic.</li><li><strong>Dependency mocker</strong> to mock the downstream responses with latency, so that the load testing on the service under test (SUT) doesn’t need to involve certain dependent services. This is especially crucial when the dependencies are vendor services that don’t support load testing, or if the team wants to regression load test their service during day-to-day deployment without affecting downstreams.</li><li><strong>Traffic collector</strong> to collect both the upstream and downstream traffic from the production environment, and then apply the resulting data to the test environment.</li><li><strong>Testing API generator</strong> to wrap asynchronous workflows into synchronous API calls for load testing.</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*SFDblGijyiLQfI7C5RpPXw.png\" /><figcaption>Figure 1: The Impulse framework and its four main components</figcaption></figure><p>Each of these four tools are independent, allowing service owners the flexibility to select one or more components for their load testing needs.</p><h4>Load generator</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/977/1*WD4EWyWHDQMf_7nDIGkAuA.png\" /><figcaption>Figure 2: Containerized load generator</figcaption></figure><p><em>Context aware</em></p><p>When load testing, requests made to the SUT often require some information from the previous response or need to be sent in a specific order. For example, if an update API needs to provide an <em>entity_id</em> to update, we must ensure the entity already exists in the testing environment context.</p><p>Our load generator tool allows users to write arbitrary testing logic in Java or Kotlin and launch containers to run these tests at scale against the SUT. Why write code instead of DSL/configuration logic?</p><ul><li>Flexibility: Programming languages are more expressive than DSL and can better support complex contextual scenarios.</li><li>Reusability: The same testing code can be used in other tests, e.g., integration tests.</li><li>Developer proficiency: Low/no learning curve to onboard, don’t need to learn how to write testing logic.</li><li>Developer experience: IDE support, testing, debugging, etc.</li></ul><p>Here is an example of synthetic context-aware test case:</p><pre>class HelloWorldLoadGenerator : LoadGenerator {<br>   override suspend fun run() {<br>       val createdEntity = sutApiClient.create(CreateRequest(name=&quot;foo&quot;, ...)).data<br><br>       // request with id from previous response (context)<br>       val updateResponse = sutApiClient.update(UpdateRequest(id=createdEntity.id, name=&quot;bar&quot;))<br>       <br>       // ... other operations<br>       <br>       // clean up<br>       sutApiClient.delete(DeleteRequest(id=createdEntity.id))<br>   }<br>}</pre><p><em>Decentralized</em></p><p>The load generator is decentralized and containerized, which means each time a load test is triggered, a set of new containers will be created to run the test. This design has several benefits:</p><ul><li>Isolation: Load testing runs between different services are isolated from each other, eliminating any interference.</li><li>Scalability: The number of containers can be scaled up or down according to the traffic requirements.</li><li>Cost efficiency: The containers are short-lived, as they only exist during the load testing run.</li></ul><p>What’s more, as our services are cloud based, a subtle point is that the Impulse framework will evenly distribute the workers among all our data centers, and the load will be emitted evenly from all the workers. Impulse’s load generator ensures the overall trigger per second (TPS) is as configured. Based on this, we can better leverage the locality settings in load balancers, which can better mimic the real traffic distribution in production.</p><p><em>Execution</em></p><p>The load generator is designed to be executed in the CI/CD pipeline, which means we can trigger load testing automatically. Developers can configure the testing spec in multiple phases, e.g., a warm up phase, a steady state phase, a peak phase, etc. Each phase can be configured with:</p><ul><li>Test cases to run</li><li>TPS (trigger per second) of each test case</li><li>Test duration</li></ul><h4>Dependency mocker</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*QXMa3Nj3-aSUE_EOvlv1xw.png\" /><figcaption>Figure 3: Dependency mocker</figcaption></figure><p>Impulse is a decentralized framework where each service has its own dependency mocker. This can eliminate interference between services and reduce communication costs. Each dependency mocker is an out-of-process service, which means the SUT behaves just as it does in production. We run the mockers in separate instances to avoid any impact on the performance of the SUT. The mock servers are all short lived — they only start before tests run and shut down afterwards to save costs and maintenance effort. The response latency and exceptions are configurable and the number of mocker instances can be adjusted on demand to support large amounts of traffic.</p><p>Other noteworthy features:</p><ul><li>You can selectively stub some of the dependencies. Currently, stubbing is supported for HTTP JSON, Airbnb Thrift, and Airbnb GraphQL dependencies.</li><li>The dependency mockers support use cases beyond load testing. For instance, integration tests often rely on other services or third-party API calls, which may not guarantee a stable testing environment or might only support ideal scenarios. Dependency mockers can address this by offering predefined responses or exceptions to fully test those flows.</li></ul><p>Impulse supports two options for generating mock responses:</p><ol><li>Synthetic response: The response is generated by user logic, as in integration testing; the difference is that the response comes from a remote (out-of-process) server with simulated latency.<br>- Similar to the load generator, the logic is written in Java/Kotlin code and contains request matching and response generation.<br>- Latency can be simulated using p95/p99 metrics.</li><li>Replay response: The response is replayed from the production downstream recording, supported by the traffic collector component.</li></ol><p>Here is an example of a synthetic response with latency in Kotlin:</p><pre>downstreamsMocking.every(<br>      thriftRequest&lt;FooRequest&gt;().having { it.message == &quot;hello&quot; }<br>    ).returns { request -&gt;<br>      ThriftDownstream.Response.thriftEncoded(<br>        HttpStatus.OK,<br>        FooResponse.builder.reply(&quot;${request.message} world&quot;).build()<br>      )<br>    }.with {<br>      delay = latencyFromP95(p95=500.miliseconds, min=200.miliseconds, max=2000.miliseconds)<br>    }</pre><h4>Traffic collector</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*EImg3JUEGzbos5r3_6U-FQ.png\" /><figcaption>Figure 4: Traffic collector</figcaption></figure><p>The traffic collector component is designed to capture both upstream and downstream traffic, along with the relationships between them. This approach allows Impulse to accurately replay production traffic during load testing, avoiding inconsistencies in downstream data or behavior. By replicating downstream responses — including production-like latency and errors — via the dependency mocker, the system ensures high-fidelity load testing. As a result, services in the testing environment behave identically to those in production, enabling more realistic and reliable performance evaluations.</p><h4>Testing API generator</h4><p>We rely heavily on event-driven, asynchronous workflows that are critical to our business operations. These include processing events from a message queue (MQ) and executing delayed jobs. Most of the MQ events/jobs are emitted from synchronous flows (e.g., API calls), so theoretically they can be covered by API load testing. However, the real world is more complex. These asynchronous flows often involve long chains of event and job emissions originating from various sources, making it difficult to replicate and test them accurately using only API-based methods.</p><p>To address this, the testing API generator component creates HTTP APIs during the CI stage according to the event or job schema. These APIs act as wrappers around the underlying asynchronous flows and are registered exclusively in the testing environment. This setup enables load testing tools — such as load generators — to send traffic to these synthetic APIs, allowing asynchronous flows to be exercised as if they were synchronous. As a result, it’s possible to perform targeted, realistic load testing on asynchronous logic that would otherwise be hard to simulate.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/660/1*F3qllm7qqMu4N2k0bBFbdQ.png\" /><figcaption>Figure 5: Testing API generator for async flows</figcaption></figure><p>The goal of the testing API generator is to help developers identify performance bottlenecks and potential issues in their async flow implementations and under high traffic conditions. It does this by enabling direct load testing of async flows without involving middleware components like MQs. The rationale is that developers typically aim to evaluate the behavior of their own logic, not the middleware, which is usually already well-tested. By bypassing these components, this approach simplifies the load testing process and empowers developers to independently manage and execute their own tests.</p><h4>Integration with other testing frameworks</h4><p>Airbnb emphasizes product quality, utilizing versatile testing frameworks that cover integration and API tests across development, staging, and production environments, and integrate smoothly into CI/CD pipelines. The modular design of Impulse facilitates its integration with these frameworks, offering systematic service testing.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*649CFxbpASxHotVVbqdQkQ.png\" /><figcaption>Figure 6: How Impulse interfaces with other internal testing frameworks</figcaption></figure><h3>Conclusion</h3><p>In this blog post, we shared how Impulse and its four core components help developers perform self-service load testing at Airbnb. As of this writing, Impulse has been implemented in several customer support backend services and is currently under review with different teams across the company who are planning to leverage Impulse to conduct load testing.</p><p>We’ve received a lot of good feedback in the process. For example: “<em>Impulse helps us to identify and address potential issues in our service. During testing, it detected an ApiClientThreadToolExhaustionException caused by thread pool pressure. Additionally, it alerted us about occasional timeout errors in client API calls during service deployments. Impulse helped us identify high memory usage in the main service container, enabling us to fine-tune the memory allocation and optimize our service’s resource usage. Highly recommend utilizing Impulse as an integral part of the development and testing processes.</em>”</p><h3>Acknowledgments</h3><p>Thanks to Jeremy Werner, Yashar Mehdad, Raj Rajagopal, Claire Cheng, Tim L., Wei Ji, Jay Wu, Brian Wallace for support on the Impulse project.</p><p>Does this type of work interest you? Check out our open roles <a href=\"https://careers.airbnb.com/\">here</a>.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f466874d03d2\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/load-testing-with-impulse-at-airbnb-f466874d03d2\">Load Testing with Impulse at Airbnb</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "大规模倾听、学习与帮助：机器学习如何改变爱彼迎的语音支持体验 (原标题: Listening, Learning, and Helping at Scale: How Machine Learning Transforms Airbnb’s Voice Support…)",
      "link": "https://medium.com/airbnb-engineering/listening-learning-and-helping-at-scale-how-machine-learning-transforms-airbnbs-voice-support-b71f912d4760?source=rss----53c7c27702d5---4",
      "pubDate": "Thu, 29 May 2025 17:29:46 GMT",
      "isoDate": "2025-05-29T17:29:46.000Z",
      "creator": "Yuanpei Cao",
      "summary": "# 大规模倾听、学习与帮助：机器学习如何改变爱彼迎的语音支持体验\n\n爱彼迎致力于提供流畅、直观且有帮助的社区支持体验，无论是协助房客处理预订变更，还是帮助房东解决房源问题。尽管帮助中心和客服聊天机器人能高效解决许多咨询，但部分用户更倾向于与支持代表进行即时语音对话。为了使这些互动更快、更有效，爱彼迎通过机器学习显著改进了其交互式语音应答（IVR）系统。\n\n## 重新构想语音支持旅程\n\n传统的IVR系统通常依赖于僵硬的菜单树，要求呼叫者按键并导航预设路径。爱彼迎设计了一个自适应、会话式的IVR系统，能够实时倾听、理解和响应。当呼叫者联系爱彼迎支持时，通常会发生以下流程：\n\n1.  **呼叫与问候**：IVR接听并提示：“请用几句话告诉我您今天致电的原因。”\n2.  **自动语音识别（ASR）**：呼叫者的回答通过爱彼迎专用的ASR系统转录成文本，保留关键的领域特定术语。\n3.  **理解意图**：一个“联系原因检测”模型将问题分类为取消、退款、账户问题等类别。\n4.  **决策制定**：\n    *   如果可以自助服务，系统会通过短信或应用通知发送相关的帮助文章或智能工作流程。\n    *   如果呼叫者明确请求代理支持或问题需要人工干预，呼叫将路由到客户支持代理，并附带相关详细信息。\n5.  **澄清响应**：一个“释义模型”生成用户意图的摘要，IVR在提供解决方案之前与用户分享。这确保用户理解他们收到的资源的上下文。\n6.  **解决或升级**：呼叫者收到包含相关爱彼迎帮助中心文章链接的短信或应用通知。如果需要进一步帮助，他们可以按0连接客户服务代表。\n\n通过从僵硬的菜单转向自然语言理解，爱彼迎允许房客和房东用自己的话表达问题，从而提高满意度和解决效率。\n\n![爱彼迎IVR核心服务与机器学习组件交互的高级架构](https://cdn-images-1.medium.com/max/1024/1*4MMPeZJDlFDELNNSZ8dGPA.png)\n*图1：爱彼迎IVR核心服务如何与核心机器学习组件交互以通过电话解决用户问题的高级架构。*\n\n## 机器学习驱动的IVR系统核心组件\n\n爱彼迎的IVR系统由以下关键机器学习组件驱动：\n\n### 1. 自动语音识别（ASR）：精准转录\n\n在语音驱动的支持系统中，实现高转录准确性至关重要，尤其是在嘈杂的电话环境中。通用语音识别模型在处理爱彼迎特定术语时常遇到困难。为了提高ASR准确性，爱彼迎：\n\n*   从通用预训练模型转向专门为嘈杂电话音频调整的模型。\n*   引入了领域特定短语列表优化，确保爱彼迎术语被正确识别。\n*   结果：词错误率（WER）从33%显著降低到约10%。这提高了下游帮助文章推荐的准确性，增加了用户参与度，改善了与ASR菜单交互用户的客户NPS，同时减少了对人工代理的依赖并缩短了客户服务处理时间。\n\n### 2. 联系原因预测：理解“为什么”\n\n转录呼叫者陈述后，下一步是识别其意图。爱彼迎通过创建详细的“联系原因分类法”来实现这一点，该分类法涵盖了所有潜在的爱彼迎咨询。一个意图检测模型将呼叫分类到相应的“联系原因”类别。\n\n*   **部署**：在生产环境中，爱彼迎部署了“问题检测服务”来托管意图检测模型，并行运行以实现最佳的可扩展性、灵活性和效率。平均意图检测延迟保持在50毫秒以下，确保了无缝的实时体验。\n*   **特殊情况**：对于呼叫者明确要求与人工代理通话的情况，系统使用不同的意图检测模型来识别此意图，并将呼叫路由到合适的团队。\n\n![意图检测架构和问题检测服务](https://cdn-images-1.medium.com/max/1024/0*oj7NCOlBGYOnNBOx)\n*图2：意图检测架构和问题检测服务。*\n\n### 3. 帮助文章检索：提供正确信息\n\n许多常见的爱彼迎问题可以通过提供清晰、相关的教育信息快速解决。爱彼迎使用“帮助文章检索和排名系统”来自动识别用户查询中的问题，并通过短信和爱彼迎应用通知发送最相关的帮助文章链接。该过程包含两个机器学习阶段：\n\n*   **语义检索和排名**：将爱彼迎帮助文章嵌入索引到向量数据库中，使用余弦相似度高效检索多达30篇相关文章（通常在60毫秒内）。一个基于LLM的排名模型随后对这些检索到的文章进行重新排名，将排名最高的文章直接呈现给用户。\n*   **效果**：该双阶段系统不仅支持IVR互动，还支持客户支持聊天机器人和帮助中心搜索。其有效性通过Precision@N等指标持续评估。\n\n![帮助文章检索和排名系统架构图](https://cdn-images-1.medium.com/max/1024/1*wlkl-CT0czyGqNdKx-ffIw.png)\n*图3：帮助文章检索和排名系统架构图。*\n\n### 4. 释义模型：增强用户理解\n\nIVR客户支持的一个关键挑战是确保用户在接收帮助文章链接之前清楚理解解决方案。为了解决这个问题，爱彼迎实施了一种轻量级的释义方法，利用一组精选的标准化摘要。\n\n*   **实现**：UX撰稿人创建了常见爱彼迎场景的简洁明了的释义。在线服务时，用户查询通过基于文本嵌入相似度的最近邻匹配映射到这些精选摘要。\n*   **效果**：手动评估端到端模型输出证实精度超过90%。在针对联系客户支持的英语房东进行的实验中，在发送文章链接前呈现释义摘要增加了用户对文章内容的参与度，从而提高了自助解决率，减少了对直接客户支持协助的需求。\n\n## 结论\n\n通过结合自动语音识别、联系原因检测系统、帮助文章检索系统和释义模型，爱彼迎创建了一个IVR系统，该系统简化了支持互动并提高了用户满意度。该解决方案使呼叫者能够自然地描述问题，减少了常见查询对人工代理的依赖，并通过自助服务提供即时、相关的支持。当需要人工协助时，系统通过将用户路由到合适的代理并提供必要的上下文，确保了平稳的过渡。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/1*zyT9hDwGkSCvZ-wKEx669w.jpeg)",
      "shortSummary": "爱彼迎利用机器学习彻底改造其语音支持（IVR）系统。通过智能IVR，用户可以自然表达问题，系统通过自动语音识别（ASR）、意图检测、帮助文章检索和释义模型提供自助服务或智能路由。这显著提高了支持效率和用户满意度，减少了对人工代理的依赖，并确保了需要时与合适代理的无缝连接。",
      "translated_title": "大规模倾听、学习与帮助：机器学习如何改变爱彼迎的语音支持体验",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*zyT9hDwGkSCvZ-wKEx669w.jpeg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*4MMPeZJDlFDELNNSZ8dGPA.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*oj7NCOlBGYOnNBOx",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*wlkl-CT0czyGqNdKx-ffIw.png",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b71f912d4760",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<h3><strong>Listening, Learning, and Helping at Scale: How Machine Learning Transforms Airbnb’s Voice Support Experience</strong></h3><p>A look into how Airbnb uses speech recognition, intent detection, and language models to understand users and assist agents more effectively.</p><p><em>By </em><a href=\"https://www.linkedin.com/in/yuanpei-cao-792b103b/\"><em>Yuanpei Cao</em></a><em>, </em><a href=\"https://www.linkedin.com/in/heng-j-1a44a711/\"><em>H</em>eng Ji</a><em>, </em><a href=\"https://www.linkedin.com/in/elaineliu5/\"><em>Elaine Liu</em></a><em>, </em><a href=\"https://www.linkedin.com/in/peng-wang-13117371/\"><em>Peng Wang</em></a><em>, and </em><a href=\"https://www.linkedin.com/in/tiantian-zhang-a4208726/\"><em>Tiantian Zhang</em></a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*zyT9hDwGkSCvZ-wKEx669w.jpeg\" /></figure><p>At Airbnb, we aim to provide a smooth, intuitive, and helpful community support experience, whether it’s helping a guest navigate a booking change or helping a host with a listing issue. While our Help Center and customer support chatbot helps resolve many inquiries efficiently, some users prefer the immediacy of a voice conversation with a support representative. To make these interactions faster and more effective, we’ve significantly improved our Interactive Voice Response (IVR) system via machine learning.</p><p>Over the years, Airbnb has invested in conversational AI to enhance customer support. In our previous blog posts <a href=\"https://medium.com/airbnb-engineering/task-oriented-conversational-ai-in-airbnb-customer-support-5ebf49169eaa\"><em>Task-Oriented Conversational AI in Airbnb Customer Support</em></a> and<a href=\"https://medium.com/airbnb-engineering/using-chatbots-to-provide-faster-covid-19-community-support-567c97c5c1c9\"> <em>Using Chatbots to Provide Faster COVID-19 Community Support</em></a>, we explored how AI-driven chatbots streamline guest and host interactions through automated messaging. This post explains how we extend that work to voice-based support, leveraging machine learning to improve real-time phone interactions with our intelligent IVR system.</p><p>We’ll take you through the end-to-end IVR journey, the key machine learning components that power it, and how we designed a system that delivers faster, more human-like, and more intuitive voice support for our community.</p><h3>Reimagining the voice support journey</h3><p>Traditional IVR systems often rely on rigid menu trees, requiring callers to press buttons and navigate pre-set paths. Instead, we designed an adaptive, conversational IVR that listens, understands, and responds in real time. Here’s normally what happens when a caller reaches out to Airbnb support:</p><ol><li><strong>Call and greeting: </strong>IVR picks up and prompts, <em>“In a few sentences, please tell us why you’re calling today.”</em></li><li><strong>Automated speech recognition (ASR):</strong> The caller’s response is transcribed with Airbnb-specific ASR. For example, if a caller says, <em>“I need to request a refund for my reservation,”</em> ASR accurately converts this speech into text, preserving key domain-specific terms.</li><li><strong>Understanding intent:</strong> A Contact Reason Detection model classifies the issue into a category like cancellations, refunds, account issues, etc.</li><li><strong>Decision-making:</strong> If self-service is possible, the system retrieves and sends a relevant help article or an intelligent workflow via SMS or app notification. If the caller explicitly requests agent support or the issue requires human intervention, the call is routed to a customer support agent with relevant details attached.</li><li><strong>Clarifying response:</strong> A Paraphrasing model generates a summary of the user intent, which IVR shares with the user before delivering the solution. This ensures that users understand the context of the resource they receive. Continuing our example, the system would respond, “<em>I understand your issue is regarding a refund request.</em> <em>We have sent you a link to resources about this topic. Follow the instructions to find answers. If you need to speak with an agent, press 0 to be connected to our customer service representative.</em>” The underscored Paraphrasing component enhances engagement by bridging the gap between system-generated responses and user comprehension, making the self-service experience more intuitive.</li><li><strong>Resolution or escalation:</strong> The caller receives an SMS or app notification with a direct link to a relevant <a href=\"https://www.airbnb.com/help\">Airbnb Help Center</a> article. If further assistance is needed, they can press 0 to connect with a customer service representative.</li></ol><p>By moving away from rigid menus to natural language understanding, we allow guests and hosts to express their issues in their own words, helping to increase satisfaction and resolution efficiency.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*BNyYIWtMRorDGJ935DcQJw.png\" /><figcaption>Figure 1: High-level architecture of how Airbnb IVR Core Service interacts with core machine learning components to resolve user issues over the phone.</figcaption></figure><h3>Breaking down our ML-powered IVR system</h3><h4>1. Automated speech recognition (ASR): transcribing with precision</h4><p>In a voice-driven support system, achieving high transcription accuracy is essential, particularly in noisy phone environments where speech can be unclear. General speech recognition models often struggle with Airbnb-specific terminology, leading to errors like misinterpreting “listing” as “lifting” or “help with my stay” as “happy Christmas Day.” These inaccuracies create challenges in understanding user intent and impact downstream processes.</p><p>To enhance ASR accuracy, we transitioned from a generic high-quality pretrained model to one specifically adapted for noisy phone audio. Additionally, we introduced a domain-specific phrase list optimization that ensures Airbnb terms are properly recognized. Based on a sample of hundreds of clips, this significantly <strong>reduced the word error rate (WER) from 33% to approximately 10%</strong>. The reduced WER significantly enhanced the accuracy of downstream help article recommendations, increasing user engagement, improving customer NPS among users who interacted with the ASR menu, while reducing reliance on human agents and lowering customer service handling time.</p><h4>2. Contact Reason prediction: understanding the why</h4><p>After transcribing the caller’s statements, the next step involves identifying their intent. We accomplished this by creating a detailed Contact Reason taxonomy that categorizes all potential Airbnb inquiries, as elaborated in “<a href=\"https://medium.com/airbnb-engineering/t-leaf-taxonomy-learning-and-evaluation-framework-30ae19ce8c52\">T-LEAF: Taxonomy Learning and EvaluAtion Framework</a>.” We then use an intent detection model to classify calls into a Contact Reason category, ensuring each inquiry is handled appropriately. For example, if a caller mentions “I haven’t received my refund yet,” the model predicts the Contact Reason as Missing Refund and forwards it to the relevant downstream components.</p><p>In production, we deploy the Issue Detection Service to host the intent detection models, running them in parallel to achieve optimal scalability, flexibility, and efficiency. Parallel computing ensures that intent detection <strong>latency remains under 50ms on average</strong>, making the process imperceptible to IVR users and ensuring a seamless real-time experience. The detected intent is then analyzed within the IVR workflow to determine the next action, whether it’s guiding the user through a self-service resolution or escalating directly to a human agent.</p><p>Occasionally, callers prefer to speak directly with a human agent instead of describing their issues, using terms like “agent” or “escalation.” For such scenarios, we use a different intent detection model to recognize when a caller wants to escalate to a human agent. If this intent is detected, the IVR system honors the caller’s request and routes the call to the suitable support team.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Q7LxFhZitCrNdqKAy_ffOw.png\" /><figcaption>Figure 2. Intent detection architecture and Issue Detection Service.</figcaption></figure><h4>3. Help article retrieval: delivering the right information</h4><p>Many common Airbnb issues can be quickly resolved by providing clear and relevant educational information. To help provide useful information to users and minimize the need for human customer support, we use the Help Article Retrieval and Ranking system. This advanced system automatically identifies the issue in a user’s inquiry and delivers the most relevant help article link via SMS text message and Airbnb app notification. Our process incorporates two machine learning stages.</p><p><strong>Semantic retrieval and ranking:</strong> We index Airbnb Help Article embeddings into a vector database, enabling efficient retrieval of up to 30 relevant articles per user query using cosine similarity, typically within 60ms. An LLM-based ranking model then re-ranks these retrieved articles, with the top-ranked article directly presented to users via IVR channels. This dual-stage system not only powers IVR interactions but also supports our customer support chatbot and Help Center search. Across these platforms, its effectiveness is continuously evaluated using metrics like Precision@N, facilitating ongoing improvements and refinements.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*wlkl-CT0czyGqNdKx-ffIw.png\" /><figcaption>Figure 3. Architecture diagram for the Help Article Retrieval and Ranking system.</figcaption></figure><h4>4. Paraphrasing model: enhancing user understanding</h4><p>A key challenge in IVR-based customer support is ensuring users clearly understand the resolution before receiving help article links, as they typically lack visibility into the article’s contents or title. To address this, we implemented a lightweight paraphrasing approach leveraging a curated set of standardized summaries.</p><p>UX writers created concise and clear paraphrases for common Airbnb scenarios. During online serving, user inquiries are mapped to these curated summaries via nearest-neighbor matching based on text embedding similarity. We calibrated a similarity threshold to ensure high-quality matches. Manual evaluation of end-to-end model outputs confirmed precision exceeding 90%.</p><p>The outcome was a finite-state solution delivering the most appropriate paraphrased IVR prompt before presenting a help article link. For example, if a caller states, “I need to cancel my reservation and request a refund,” the model generates a response like “I understand your issue is about a refund request” before sending the retrieved help article link.</p><p>Integrating this model ensures users receive clear, contextually relevant summaries prior to accessing help articles. In an experiment targeting English hosts who contacted customer support, we found that presenting a paraphrased summary before sending the article link increases user engagement with article content, resulting in improvement in self-resolution rates, helping to reduce the need for direct customer support assistance.</p><h3>Conclusion</h3><p>By combining Automated Speech Recognition and Contact Reason Detection systems with a help article retrieval system, and a paraphrasing model, we have created an IVR system that streamlines support interactions and improves user satisfaction. Our solution enables callers to describe issues naturally, reduces dependency on human agents for common inquiries, and provides instant, relevant support through self-service. When human assistance is necessary, the system ensures a smooth transition by routing users to the right agent with essential context.</p><p>Interested in working at Airbnb? Check out our <a href=\"https://careers.airbnb.com/\">open roles</a>.</p><h3><strong>Acknowledgements</strong></h3><p>Thanks to Zhenyu Zhao, Mia Zhao, Wayne Zhang, Lucca Siaudzionis, Lulu Chen, Sukey Xu, Floria Wan, Michael Zhou, Can Yang, Yaolin Chen, Shuaihu Wang, Huifan Qu, Ming Shang,Yu Jiang, Wanting Chen, Elena Zhao, Shanna Su, Cassie Cao, Hao Wang, Haoran Zhu, Xirui Liu, Ying Tan, Xiaohan Zeng, Xiaoyu Meng, Gavin Li, Gaurav Rai, Hemanth Kolla, Ihor Hordiienko, Matheus Scharf, and Stepan Sydoruk who helped bring this vision to life. Also thanks to Paige Schwartz, Stephanie Chu, Neal Cohen, Becky Ajuonuma, Iman Saleh, Dani Normanm, Javier Salido, and Lauren Mackevich for the review and editing.</p><p>Thanks to Jeremy Werner, Joy Zhang, Claire Cheng, Yashar Mehdad, Shuohao Zhang, Shawn Yan, Kelvin Xiong, Michael Lubavin, Teng Wang, Wei Ji, and Chenhao Yang’s leadership support on building conversational AI products at Airbnb.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b71f912d4760\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/listening-learning-and-helping-at-scale-how-machine-learning-transforms-airbnbs-voice-support-b71f912d4760\">Listening, Learning, and Helping at Scale: How Machine Learning Transforms Airbnb’s Voice Support…</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "Airbnb 如何衡量房源生命周期价值 (原标题: How Airbnb Measures Listing Lifetime Value)",
      "link": "https://medium.com/airbnb-engineering/how-airbnb-measures-listing-lifetime-value-a603bf05142c?source=rss----53c7c27702d5---4",
      "pubDate": "Wed, 26 Mar 2025 15:46:46 GMT",
      "isoDate": "2025-03-26T15:46:46.000Z",
      "creator": "Carlos Sanchez Martinez",
      "summary": "## Airbnb 如何衡量房源生命周期价值\n\n### 引言\n\nAirbnb 致力于为用户提供最佳体验，这需要深入理解哪些房源对房客具有高价值。为此，Airbnb 计算并使用房源生命周期价值（Listing Lifetime Value, LTV）的估算值。这些估算不仅有助于识别最受房客欢迎的房源类型，还能帮助 Airbnb 为房东提供资源和建议，以提升其房源所带来的价值。与传统单边销售模式（如零售商向顾客销售商品）不同，本文重点阐述了 Airbnb 如何在多卖家和多买家的平台环境中建模 LTV。\n\n### 房源生命周期价值（LTV）框架\n\nAirbnb 的房源 LTV 框架估算三个关键量：基线 LTV、增量 LTV 和营销诱导增量 LTV。\n\n#### 1. 基线 LTV (Baseline LTV)\n\n基线 LTV 被定义并估算为房源在未来 365 天内将在 Airbnb 上获得的预订总数。Airbnb 依靠机器学习和丰富的房源信息来估算每个独立房源的这一数值。在实践中，还会遵循财务指导，通过预测未来结果并应用相关折现率来计算现值。\n\n![图片 2: 房源LTV估算示例](https://cdn-images-1.medium.com/max/874/1*PLgiegXaNpY8nthDFZpmpA.png)\n\n*   **用途：** 基线 LTV 估算用于对房源进行细分，识别最受房客欢迎的房源类型，从而指导供应拓展策略。它还用于识别那些预计未能充分发挥其预订潜力的房源，这些房源可能受益于额外的指导。\n\n#### 2. 增量 LTV (Incremental LTV)\n\n在估算生命周期价值时，多边市场面临一个共同挑战：一个房源的交易可能以牺牲另一个房源的交易为代价，即“蚕食效应”。例如，当一个新房源加入市场时，它可能会从原本预订其他房源的房客那里获得一些预订。为了准确衡量每个房源所增加的价值，需要考虑这种动态。\n\nAirbnb 通过创建“增量 LTV”估算来解决这一挑战。增量价值是指若无该房源参与，则不会发生的额外交易；而蚕食价值是指即使没有该房源参与，也会发生的交易。增量 LTV 是通过从基线 LTV 中减去蚕食价值估算得出的。\n\n![图片 3: 蚕食效应示意图](https://cdn-images-1.medium.com/max/1024/1*UH0hKFiaFYB-l_LL-CkSRQ.png)\n\n#### 3. 营销诱导增量 LTV (Marketing-induced incremental LTV)\n\n生命周期价值并非静态不变，LTV 模型需要反映内部举措如何带来额外的房源价值。例如，如果 Airbnb 开展一项营销活动，向房东提供成功改善房源的技巧，为了理解该活动的回报，需要衡量因活动而产生的价值，以及如果没有营销干预，自然会产生的价值。营销诱导增量 LTV 用于衡量内部举措创造了多少额外的房源 LTV。\n\n![图片 4: 房源LTV框架概览](https://cdn-images-1.medium.com/max/1024/1*RIUqYmgP_5JWfAohtdCdBQ.png)\n\n### 衡量房源生命周期价值的挑战\n\n#### 1. 准确衡量基线 LTV\n\n框架最重要的要求是准确估算基线 LTV。估算设置利用在估算时点 `t` 捕获的房源特征（包括房源和房东的丰富信息），然后使用这些特征训练机器学习模型。价值标签是未来 365 天内发生的预订数量，在 `t + 365` 日观察到。\n\n![图片 5: 标签与特征收集示意图](https://cdn-images-1.medium.com/max/1024/1*6EanAK-Y42jbcWyATva8GA.png)\n\n*   **影响：** 这种设置意味着需要等待 365 天才能完全评估预测的准确性。此外，如果训练数据捕获时间与模型评分时间之间发生剧烈冲击（如 COVID-19 大流行期间旅行停滞），初始训练数据可能无法进行准确预测。\n*   **应对策略：** 缩短训练窗口以减少模型漂移；向模型输入精细的地理数据和关于外部因素的人工信息；采用 LightGBM 模型处理高基数特征。\n\n#### 2. 衡量增量性\n\n衡量增量性具有挑战性，因为无法观察到真实情况。虽然可以观察到每个房源的预订数量，但无法区分哪些预订是增量的，哪些是从其他房源中“蚕食”而来的。\n\n由于没有增量性标签来直接估算这一结果，Airbnb 转而估算一个“生产函数”。直观地讲，增量性在很大程度上取决于连接市场两端的能力。生产函数有助于识别房源供应和房客需求何时连接并提供增量价值。当某个细分市场房客需求高而房源供应相对较低时，增量性估算值会很高；反之，当房源供应量大而需求相对较低时，增量性会很低。\n\n![图片 6: 生产函数公式](https://cdn-images-1.medium.com/max/1024/1*ccUD00N5xq2IfTiMkZHNGA.png)\n\n#### 3. 处理不确定性\n\n为了应对疫情期间经历的不确定性，Airbnb 开始根据房源实际获得的预订数量与最初预期数量的差异来更新 LTV 估算值。这种方法有助于捕捉初始预测后发生的任何冲击。\n\n在实践中，Airbnb 根据房源的已发生价值、更新后的房源特征以及使用历史数据估算的类似房源的价值到达模式，每日调整房源的预期价值。通过这种方式，预期预订量和已发生预订量会随着时间推移趋近于最终的预订量。\n\n![图片 7: 房源生命周期价值估算更新示例](https://cdn-images-1.medium.com/max/1024/1*vNQ0046lY7rfWrHIJK6Oww.png)\n\n### 结论\n\n估算每个房源的生命周期价值至关重要，因为它有助于 Airbnb 更有效地服务社区。其用例包括：\n\n*   识别独特的房源细分市场，让新房东能向大量房客展示其待客之道。\n*   找出房源有更多预订机会并可能受益于额外需求的地点。\n*   识别哪些内部营销举措为社区带来了最大价值。\n\n该衡量框架还可能扩展到其他应用，例如 Airbnb 体验（Experiences）的生命周期价值，其中体验房源的价值将严重依赖于旅行趋势和房客发现这些体验的能力。\n\nAirbnb 团队持续解决 LTV 相关的有趣问题，并鼓励有志之士探索其团队的开放职位。",
      "shortSummary": "Airbnb通过其房源生命周期价值（LTV）框架，识别并优化对房客有价值的房源。该框架包含基线LTV（预测未来365天预订量）、增量LTV（考虑新房源对现有房源的“蚕食效应”）和营销诱导增量LTV（衡量内部活动带来的价值）。文章详细阐述了LTV的衡量方法，并探讨了准确性、增量性评估和不确定性处理等挑战，例如疫情期间的市场变化。通过机器学习和动态更新，LTV估算帮助Airbnb优化供应策略、指导房东并评估营销效果，从而提升社区体验。",
      "translated_title": "Airbnb 如何衡量房源生命周期价值",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*JSoY6CDkTMQEFXgP",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/874/1*PLgiegXaNpY8nthDFZpmpA.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*UH0hKFiaFYB-l_LL-CkSRQ.png",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*RIUqYmgP_5JWfAohtdCdBQ.png",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*6EanAK-Y42jbcWyATva8GA.png",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<p>A deep dive on the framework that lets us identify the most valuable listings for our guests.</p><p><strong>By:</strong> <a href=\"https://www.linkedin.com/in/carlossanchezmartinez/\">Carlos Sanchez-Martinez</a>, <a href=\"https://www.linkedin.com/in/seanmk2/\">Sean O’Donnell</a>, <a href=\"https://www.linkedin.com/in/lohua-yuan/\">Lo-Hua Yuan</a>, <a href=\"https://www.linkedin.com/in/yunshanz/\">Yunshan Zhu</a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*JSoY6CDkTMQEFXgP\" /></figure><p>At Airbnb, we always strive to provide our community with the best experience. To do so, it’s important to understand what kinds of accommodation listings are valuable to our guests. We achieve this by calculating and using estimates of <strong>listing lifetime value</strong>. These estimates not only allow us to identify which types of listings resonate best with guests, but also help us develop resources and recommendations for hosts to increase the value driven by their listings.</p><p>Most of the existing literature on lifetime value focuses on traditional sales channels in which a single seller transacts with many buyers (e.g. a retailer selling clothing to a customer). In contrast, this blog post explains how we model lifetime value in a platform like Airbnb, with multiple sellers and buyers. In the first section, we describe our general listings lifetime value framework. In the second section, we discuss relevant challenges when putting this framework into practice.</p><h3>Our Listing Lifetime Value Framework</h3><p>Our listing lifetime value (LTV) framework estimates three different quantities of interest: baseline LTV, incremental LTV, and marketing-induced incremental LTV.</p><h4>(1) Baseline LTV</h4><p>To measure LTV, we need to define what we mean by “value” and what time horizon constitutes a “lifetime.” Simplifying slightly for the purposes of this blog post, we define and estimate our baseline listing LTV as the total number of bookings that a listing will make on Airbnb over the next 365 days.</p><p>We rely on machine learning and the rich information we have about our listings to estimate this quantity for each individual listing. In practice, we also follow financial guidance to arrive at present value by projecting outcomes into the future and applying a relevant discount rate to future value.</p><p>Table 1 shows some hypothetical baseline LTV estimates. As you can see from the examples, LTV is not static, and can evolve as we improve the accuracy of our estimates, observe changes in our marketplace, or even develop a listing (e.g., by providing guidance that helps hosts improve the listing to get more bookings).</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/874/1*PLgiegXaNpY8nthDFZpmpA.png\" /><figcaption><strong>Table 1. Example Listing LTV Estimates</strong></figcaption></figure><p>We use baseline LTV estimates to segment our listings and identify which types of listings resonate best with our guests. This informs our supply expansion strategy. We also use baseline LTV to identify listings that are not expected to reach their full booking potential and may benefit from additional guidance.</p><h4>(2) Incremental LTV</h4><p>When estimating lifetime value, we face a challenge that is common across multi-sided marketplaces: the transactions made by one listing might come at the expense of another listing’s transactions. For example, when a new listing joins our marketplace, this listing will get some bookings from guests who were previously booking other listings. We need to account for this dynamic if we want to accurately measure how much value is <em>added</em> by each listing.</p><p>We address this challenge by creating “incremental<em> </em>LTV” estimates. We refer to the additional transactions that would not have occurred without the listing’s participation as “incremental value,” and the transactions that would have occurred even without the listing’s participation as “cannibalized value.” We estimate the incremental LTV for a listing by subtracting cannibalized value estimates from the baseline LTV. We explain this adjustment in more detail when discussing measurement challenges.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*UH0hKFiaFYB-l_LL-CkSRQ.png\" /><figcaption><strong>Figure 1. Cannibalization.</strong> In this context, cannibalization refers to the transactions that would have occurred even without a listing’s participation in the marketplace. For example, when a new listing joins the platform, some bookings obtained by that listing would have been made at other listings on the platform had the new listing not joined.</figcaption></figure><h4>(3) Marketing-induced incremental LTV</h4><p>Lifetime value is not static, and our LTV model needs to tell us how our internal initiatives bring additional listing value. For example, suppose we run a marketing campaign that provides hosts with tips on how to successfully improve their listings. To understand the return from the campaign, we need to measure how much value is accrued due to the campaign, and how much value would have been organically accrued without our marketing intervention. We calculate “marketing-induced incremental LTV” to measure how much additional listing LTV is created by our internal initiatives.</p><p>Having outlined our measurement framework (summarized in Figure 2), we now cover some of the technical challenges we faced when putting this framework into practice.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*RIUqYmgP_5JWfAohtdCdBQ.png\" /><figcaption><strong>Figure 2.</strong> <strong>Listing LTV Framework</strong></figcaption></figure><h3>Challenges when measuring Listing Lifetime Value</h3><h4>Challenge (1): Accurately measuring baseline LTV</h4><p>The most important requirement for our framework is accurate estimation of baseline LTV. Figure 3 illustrates our estimation setup. First, we leverage listing features snapshotted at estimation time t. This data includes rich knowledge we have about each listing and host (availability, price, location, host tenure, etc). We then use these features to train our machine learning model. As a value label, we use the number of bookings made within the next 365-day period, which is observed on date t + 365.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*6EanAK-Y42jbcWyATva8GA.png\" /><figcaption><strong>Figure 3. Label vs. Feature Collection. </strong>Our label lands 365 days after we collect the initial set of features for our model.</figcaption></figure><p>This setup has two important implications that impact accuracy and evaluation:</p><ul><li>We have to wait 365 days to fully evaluate the accuracy of a prediction.</li><li>Our initial training data might not allow us to make accurate predictions if we observe shocks between the time when the training data was captured, and the time when we score the model.</li></ul><p>In practice, we felt the full consequences of these implications during the COVID-19 pandemic, when travel came to a halt and marketplace dynamics changed drastically. Our model’s training data from before the pandemic had dramatically different characteristics relative to the scoring data we collected after the pandemic. When dealing with this shock, we implemented various strategies that helped us improve model accuracy:</p><ul><li>Reducing training windows, allowing us to reduce model drift.</li><li>Feeding the model with granular geographic data and human-provided information about external factors as borders closed and reopened due to the pandemic.</li><li>Adopting <a href=\"http://lightgbm.readthedocs.io\">LightGBM</a>, which handles high cardinality features like the geographic variables mentioned previously.</li></ul><h4>Challenge (2): Measuring incrementality</h4><p>Accounting for incrementality is challenging because we never observe the ground truth. While we observe how many bookings are made per listing, we cannot tell which bookings are incremental and which bookings are cannibalized from other listings.</p><p>Since we don’t have an incrementality label to estimate this outcome directly, we instead estimate a production function. Intuitively, incrementality is heavily dependent on our ability to connect both sides of our marketplace. Production functions allow us to identify when our supply of listings and demand from guests connect and provide incremental value. Incrementality estimates will be high when a segment has high guest demand and relatively low listing supply. In contrast, incrementality will be low when segments have a large volume of listing supply and relatively low demand, meaning guests have an easy time finding a place to stay and a new listing is more likely to cannibalize bookings from other listings.</p><p>Specifically, we model how our total supply of listings (S) and total demand from guests (D) impacts our target outcome bookings (O), as in equation (1):</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ccUD00N5xq2IfTiMkZHNGA.png\" /></figure><p>We estimate this model with historical supply, demand, and outcome data aggregated across internally-defined segments that have little overlapping demand. Having estimated model (1), we calculate how extra supply of listings results in additional bookings in the given segment: this is our estimate of incrementality.</p><h4>Challenge (3): Handling uncertainty</h4><p>To handle the uncertainty we experienced during the pandemic, we began updating our LTV estimates as listings received greater or fewer numbers of bookings than initially expected. This approach has helped us capture any shocks that occur after making our initial predictions.</p><p>To show how this can be useful, let’s go back to our marketing campaign example. Assume that we run this campaign for six months, and that we measure the success of this campaign by comparing marketing-induced incremental LTV against our total marketing investment in the campaign. As a first approach, we could use the initial baseline LTV figures (which feed into marketing-induced LTV) estimated at the time when the listing was first targeted by our initiative. However, listings targeted on day 1 of the marketing campaign will have six months of booking history by the time the campaign ends and we evaluate success. A more accurate approach uses realized bookings after the initial prediction to start correcting for model error.</p><p>Table 2 illustrates how this works. Suppose that on 2024–01–01, we expect that Listing A will get a total of 16 bookings by the end of the year. If six months into the 365 day period, Listing A has received 16 bookings, we should adjust its expected value upward to, say, 21 bookings. In fact, every day for 365 days after 2024–01–01, we can look at the bookings that Listing A has accrued and adjust the expected bookings accordingly. By construction, the expected and accrued bookings converge to the final bookings 365 days after the initial booking date. Going back to our marketing example, if Listing A ultimately receives 20 bookings, updating the initial estimate means we went from 20% underprediction on day 0 to a more reasonable 5% overprediction as of month 6.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*vNQ0046lY7rfWrHIJK6Oww.png\" /><figcaption><strong>Table 2.</strong> <strong>Example of how we update listing lifetime value estimates.</strong></figcaption></figure><p>In practice, we make daily adjustments to a listing’s expected value based on the listing’s accrued value, updated listing features, and value arrival patterns for similar listings estimated using historical data.</p><h3>Conclusion</h3><p>In this blog post, we explained how we approach listing lifetime value at Airbnb. We covered our measurement framework, including baseline LTV, incremental LTV, and marketing-induced incremental LTV. We also zoomed into measurement challenges, like when travel patterns changed drastically during the COVID pandemic and accurately estimating LTV became more difficult.</p><p>Estimating the lifetime value for each listing is important because it helps us serve our community more effectively. Use cases include:</p><ul><li>Identifying unique listing segments through which new hosts can showcase their hospitality to a large guest audience.</li><li>Pinpointing locations where listings have an opportunity to get more bookings, and might benefit from additional demand.</li><li>Identifying which internal marketing initiatives bring the most value to our community.</li></ul><p>It’s also worth noting that our measurement framework may extend to other applications, such as the lifetime value for Airbnb Experiences listings, where the value of an experience listing will heavily depend on travel trends and on guests’ ability to discover these experiences.</p><p>We continue to solve interesting problems around LTV every day (and as more insights come up, we’ll keep sharing them on our blog). Can you see yourself making an impact here? If so, we encourage you to explore the <a href=\"https://careers.airbnb.com/positions/?_departments=data-science\">open roles on our team</a>.</p><h3>Acknowledgments</h3><p>Finally, we need to give special thanks to Airfam and alumni Sam Barrows, Robert Chang, Linsha Chen, Richard Dear, Andrey Fradkin, Ruben Lobel, Brian De Luna, Dan T. Nguyen, Vaughn Quoss, Jason Ting, and Peng Ye. Without their foundational work, these LTV models would not have been possible.</p><p>Thanks as well to Rebecca Ajuonuma, Carolina Barcenas, Nathan Brixius, Jenny Chen, Peter Coles, Lauren Mackevich, Dan Schmierer, Yvonne Wang, Shanni Weilert, and Jane Zhang for their valuable feedback when writing this blog post.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a603bf05142c\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/how-airbnb-measures-listing-lifetime-value-a603bf05142c\">How Airbnb Measures Listing Lifetime Value</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "基于嵌入的Airbnb搜索召回系统 (原标题: Embedding-Based Retrieval for Airbnb Search)",
      "link": "https://medium.com/airbnb-engineering/embedding-based-retrieval-for-airbnb-search-aabebfc85839?source=rss----53c7c27702d5---4",
      "pubDate": "Wed, 19 Mar 2025 17:02:45 GMT",
      "isoDate": "2025-03-19T17:02:45.000Z",
      "creator": "Huiji Gao",
      "summary": "## 基于嵌入的Airbnb搜索召回系统\n\n**引言**\n\nAirbnb搜索旨在为用户提供最相关的房源，但面对数百万房源、大地理区域搜索（如加利福尼亚、法国）和高需求目的地（如巴黎、伦敦）的挑战，以及灵活日期搜索带来的复杂性，实现这一目标并非易事。为了解决这些问题，Airbnb构建了首个基于嵌入的召回（EBR）搜索系统。该系统的目标是将初始符合条件的房源集缩小到一个更小的池子，以便后续更计算密集型的机器学习模型进行排序。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/0*dhEL1kHnOpCWnqJa)\n\n本文探讨了构建EBR系统的三个关键挑战：\n1.  训练数据构建\n2.  模型架构设计\n3.  使用近似最近邻（ANN）解决方案的在线服务策略\n\n![图片 2](https://cdn-images-1.medium.com/max/924/0*TCeRWXyWhaTJeGfp)\n*图1：Airbnb搜索中各类排序模型的通用阶段和规模。*\n\n**训练数据构建**\n\n构建EBR系统的第一步是训练一个机器学习模型，将房源和去标识化的搜索查询映射为数值向量（嵌入）。为此，团队构建了一个利用对比学习的训练数据管道，该策略涉及为给定查询识别正负样本对。模型在训练过程中学习将查询、正样本房源和负样本房源映射到数值向量，使得查询与正样本房源的相似度远高于与负样本房源的相似度。\n\n为了构建这些样本对，团队设计了一种基于用户行程的采样方法，这对于捕捉用户多阶段搜索旅程至关重要。数据表明，用户在最终预订前会进行多次搜索并采取各种操作。因此，该策略能够捕获整个多阶段旅程并考虑用户可能探索的各种房源类型。\n\n具体而言，团队首先根据位置、客人数量和入住时长等关键查询参数，对所有已预订用户的历史查询进行分组，定义为“行程”。对于每个行程，分析用户执行的所有搜索，并将最终预订的房源作为正样本。负样本则从用户在搜索结果中看到但未预订的房源中选择，以及那些用户曾有更强交互（如加入心愿单）但最终未预订的房源。这种负样本的选择至关重要，因为随机采样房源会使问题过于简单，导致模型性能不佳。\n\n![图片 3](https://cdn-images-1.medium.com/max/878/0*3rXf0K0bJoObo17-)\n*图2：为给定用户旅程构建（正，负）样本对的示例。预订的房源始终被视为正样本。负样本从出现在搜索结果中（并可能被交互过）但用户最终未预订的房源中选择。*\n\n![图片 4](https://cdn-images-1.medium.com/max/558/0*AUApsIPfEFdmx_S-)\n*图3：用于构建EBR模型训练数据的整体数据管道示例。*\n\n**模型架构**\n\n模型架构遵循传统的双塔网络设计：\n*   **房源塔**：处理房源本身的特征，如历史互动、设施和客人容量。\n*   **查询塔**：处理与搜索查询相关的特征，如地理搜索位置、客人数量和入住时长。\n\n这两个塔共同生成房源和搜索查询的嵌入。一个关键的设计决策是选择特征，使得房源塔可以每天离线计算。这使得团队能够通过每日批处理作业预计算房源嵌入，显著降低了在线延迟，因为只有查询塔需要在实时处理传入的搜索请求时进行评估。\n\n![图片 5](https://cdn-images-1.medium.com/max/827/0*rLXZmgqFE5BiSOGS)\n*图4：EBR模型中使用的双塔架构。请注意，房源塔每天离线计算所有房源的嵌入。*\n\n**在线服务**\n\n构建EBR系统的最后一步是选择在线服务的基础设施。团队探索了多种近似最近邻（ANN）解决方案，并最终选择了倒排文件索引（IVF）作为最佳权衡方案，尽管分层可导航小世界（HNSW）在召回率方面略胜一筹。\n\n选择IVF的主要原因包括：\n*   **实时更新量大**：Airbnb房源的定价和可用性数据频繁更新，导致HNSW索引的内存占用过大。\n*   **过滤器兼容性**：大多数Airbnb搜索包含过滤器（尤其是地理过滤器），HNSW与过滤器的并行检索导致延迟性能不佳。\n\n相比之下，IVF解决方案预先对房源进行聚类，只需在搜索索引中存储聚类中心和聚类分配。在服务时，通过将聚类分配视为标准搜索过滤器，从最接近查询嵌入的聚类中检索房源，这使得与现有搜索系统的集成非常直接。\n\n![图片 6](https://cdn-images-1.medium.com/max/603/0*WH3lbXvph3aBkPBY)\n*图5：使用IVF的整体服务流程。房源预先聚类，在在线服务期间，房源从最接近查询嵌入的聚类中检索。*\n\n在EBR模型中选择相似度函数也产生了有趣的影响。团队探索了点积和欧几里得距离；虽然两者在模型性能上相似，但使用欧几里得距离平均产生了更均衡的聚类。这是一个关键的发现，因为IVF检索的质量对聚类大小的均匀性高度敏感：如果一个聚类包含太多房源，将大大降低检索系统的区分能力。团队推测，点积相似度产生这种不平衡是因为它本质上只考虑特征向量的方向而忽略其大小，而许多底层特征是基于历史计数的，使得大小成为一个重要因素。\n\n![图片 7](https://cdn-images-1.medium.com/max/695/0*RlVEZwdCwA5j4cwo)\n*图6：使用点积与欧几里得距离作为相似度度量时聚类大小分布的示例。我们发现欧几里得距离产生了更均衡的聚类大小。*\n\n**成果**\n\n本文描述的EBR系统已在搜索和邮件营销生产环境中全面上线，并通过A/B测试带来了总预订量的统计显著增长。值得注意的是，这一新召回系统带来的预订量提升与过去两年搜索排名中一些最大的机器学习改进相当。与基线相比，EBR系统的主要改进在于它有效地融入了查询上下文，从而在召回阶段更准确地对房源进行排名。这最终帮助Airbnb向用户展示了更相关的结果，特别是对于符合条件结果数量较多的查询。",
      "shortSummary": "Airbnb成功构建并部署了首个基于嵌入的召回（EBR）系统，以优化其搜索体验。该系统通过将房源和查询映射为数值向量，有效缩小了初始房源池。其核心在于基于用户行程构建训练数据、采用双塔模型架构（房源嵌入离线计算），并选择IVF作为在线服务方案以应对实时更新和过滤器挑战。EBR系统已在搜索和邮件营销中全面上线，A/B测试显示总预订量显著增长，效果与重大机器学习改进相当，显著提升了搜索结果的相关性。",
      "translated_title": "基于嵌入的Airbnb搜索召回系统",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*dhEL1kHnOpCWnqJa",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/924/0*TCeRWXyWhaTJeGfp",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/878/0*3rXf0K0bJoObo17-",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/558/0*AUApsIPfEFdmx_S-",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://cdn-images-1.medium.com/max/827/0*rLXZmgqFE5BiSOGS",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*dhEL1kHnOpCWnqJa\" /></figure><p>Our journey in applying embedding-based retrieval techniques to build an accurate and scalable candidate retrieval system for Airbnb Homes search</p><p>Authors: <a href=\"https://www.linkedin.com/in/mustafa-moose-abdool-8aab037a/\">Mustafa (Moose) Abdool</a>, <a href=\"https://www.linkedin.com/in/soumyadip-banerjee-75991b42/\">Soumyadip Banerjee</a>, <a href=\"https://www.linkedin.com/in/kouyang1/\">Karen Ouyang</a>, <a href=\"https://www.linkedin.com/in/do-kyum-kim-9a810417/\">Do-Kyum Kim</a>, <a href=\"https://www.linkedin.com/in/moutupsi-paul/\">Moutupsi Paul</a>, <a href=\"https://www.linkedin.com/in/xiaowei-liu-60415841/\">Xiaowei Liu</a>, <a href=\"https://www.linkedin.com/in/bin-xu-96253aa5/\">Bin Xu</a>, <a href=\"https://www.linkedin.com/in/tracy-xiaoxi-yu/\">Tracy Yu</a>, <a href=\"https://www.linkedin.com/in/hui-gao-275a924/\">Hui Gao</a>, <a href=\"https://www.linkedin.com/in/yangbo-zhu/\">Yangbo Zhu</a>, <a href=\"https://www.linkedin.com/in/huiji-gao/\">Huiji Gao</a>, <a href=\"https://www.linkedin.com/in/liweihe/\">Liwei He</a>, <a href=\"https://www.linkedin.com/in/sanjeevkatariya/\">Sanjeev Katariya</a></p><h3>Introduction</h3><p>Search plays a crucial role in helping Airbnb guests find the perfect stay. The goal of Airbnb Search is to surface the most relevant listings for each user’s query — but with millions of available homes, that’s no easy task. It’s especially difficult when searches include large geographic areas (like California or France) or high-demand destinations (like Paris or London). Recent innovations — such as <em>flexible date search</em>, which allows guests to explore stays without fixed check-in and check-out dates — have added yet another layer of complexity to ranking and finding the right results.</p><p>To tackle these challenges, we need a system that can retrieve relevant homes while also being scalable enough (in terms of latency and compute) to handle queries with a large candidate count. In this blog post, we share our journey in building Airbnb’s first-ever Embedding-Based Retrieval (EBR) search system. The goal of this system is to narrow down the initial set of eligible homes into a smaller pool, which can then be scored by more compute-intensive machine learning models later in the search ranking process.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/924/0*TCeRWXyWhaTJeGfp\" /></figure><p><strong>Figure 1:</strong> The general stages and scale for the various types of ranking models used in Airbnb Search</p><p>We’ll explore three key challenges in building this EBR system: (1) constructing training data, (2) designing the model architecture, and (3) developing an online serving strategy using Approximate Nearest Neighbor (ANN) solutions.</p><h3>Training Data Construction</h3><p>The first step in building our EBR system was training a machine learning model to map both homes and de-identified search queries into numerical vectors. To achieve this, we built a training data pipeline (Figure 3) that leveraged contrastive learning — a strategy that involves identifying pairs of positive- and negative-labeled homes for a given query. During training, the model learns to map a query, a positive home, and a negative home into a numerical vector, such that the similarity between the query and the positive home is much higher than the similarity between the query and the negative home.</p><p>To construct these pairs, we devised a sampling method based on user trips. This was an important design decision, since users on Airbnb generally undergo a multi-stage search journey. Data shows that before making a final booking, users tend to perform multiple searches and take various actions — such as clicking into a home’s details, reading reviews, or adding a home to a wishlist. As such, it was crucial to develop a strategy that captures this entire multi-stage journey and accounts for the diverse types of listings a user might explore.</p><p>Diving deeper, we first grouped all historical queries of users who made bookings, using key query parameters such as location, number of guests, and length of stay — our definition of a “trip.” For each trip, we analyzed all searches performed by the user, with the final booked listing as the positive label. To construct (positive, negative) pairs, we paired this booked listing with other homes the user had seen but not booked. Negative labels were selected from homes the user encountered in search results, along with those they had interacted with more intentfully — such as by wishlisting — but ultimately did not book. This choice of negative labels was key: Randomly sampling homes made the problem too easy and resulted in poor model performance.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/878/0*3rXf0K0bJoObo17-\" /></figure><p><strong>Figure 2: </strong>Example of constructing (positive, negative) pairs for a given user journey. The booked home is always treated as a positive. Negatives are selected from homes that appeared in the search result (and were potentially interacted with) but that the user did not end up booking.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/558/0*AUApsIPfEFdmx_S-\" /></figure><p><strong>Figure 3: </strong>Example of overall data pipeline used to construct training data for the EBR model.</p><h3>Model Architecture</h3><p>The model architecture followed a traditional two-tower network design. One tower (the <em>listing tower</em>) processes features about the home listing itself — such as historical engagement, amenities, and guest capacity. The other tower (the <em>query tower</em>) processes features related to the search query — such as the geographic search location, number of guests, and length of stay. Together, these towers generate the embeddings for home listings and search queries, respectively.</p><p>A key design decision here was choosing features such that the listing tower could be computed offline on a daily basis. This enabled us to pre-compute the home embeddings in a daily batch job, significantly reducing online latency, since only the query tower had to be evaluated in real-time for incoming search requests.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/827/0*rLXZmgqFE5BiSOGS\" /></figure><p><strong>Figure 4: </strong>Two-tower architecture as used in the EBR model. Note that the listing tower is computed offline daily for all homes.</p><h3>Online Serving</h3><p>The final step in building our EBR system was choosing the infrastructure for online serving. We explored a number of approximate nearest neighbor (ANN) solutions and narrowed them down to two main candidates: inverted file index (IVF) and hierarchical navigable small worlds (HNSW). While HNSW performed slightly better in terms of evaluation metrics — using recall as our main evaluation metric — we ultimately found that IVF offered the best trade-off between speed and performance.</p><p>The core reason for this is the high volume of real-time updates per second for Airbnb home listings, as pricing and availability data is frequently updated. This caused the memory footprint of the HNSW index to grow too large. In addition, most Airbnb searches include filters, especially geographic filters. We found that parallel retrieval with HNSW alongside filters resulted in poor latency performance.</p><p>In contrast, the IVF solution, where listings are clustered beforehand, only required storing cluster centroids and cluster assignments within our search index. At serving time, we simply retrieve listings from the top clusters by treating the cluster assignments as a standard search filter, making integration with our existing search system quite straightforward.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/603/0*WH3lbXvph3aBkPBY\" /></figure><p><strong>Figure 5: </strong>Overall serving flow using IVF. Homes are clustered beforehand and, during online serving, homes are retrieved from the closest clusters to the query embedding.</p><p>In this approach, our choice of similarity function in the EBR model itself ended up having interesting implications. We explored both dot product and Euclidean distance; while both performed similarly from a model perspective, using Euclidean distance produced much more balanced clusters on average. This was a key insight, as the quality of IVF retrieval is highly sensitive to cluster size uniformity: If one cluster had too many homes, it would greatly reduce the discriminative power of our retrieval system.</p><p>We hypothesize that this imbalance arises with dot product similarity because it inherently only considers the direction of feature vectors while ignoring their magnitudes — whereas many of our underlying features are based on historical counts, making magnitude an important factor.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/695/0*RlVEZwdCwA5j4cwo\" /></figure><p><strong>Figure 6: </strong>Example of the distribution of cluster sizes when using dot product vs. Euclidean distance as a similarity measure. We found that Euclidean distance produced much more balanced cluster sizes.</p><h3>Results</h3><p>The EBR system described in this post was fully launched in both Search and Email Marketing production and led to a statistically-significant gain in overall bookings when A/B tested. Notably, the bookings lift from this new retrieval system was on par with some of the largest machine learning improvements to our search ranking in the past two years.</p><p>The key improvement over the baseline was that our EBR system effectively incorporated query context, allowing homes to be ranked more accurately during retrieval. This ultimately helped us display more relevant results to users, especially for queries with a high number of eligible results.</p><h3>Acknowledgments</h3><p>We would like to especially thank the entire Search and Knowledge Infrastructure &amp; ML Infrastructure org (led by <a href=\"https://www.linkedin.com/in/yi-li-755a6b24/\">Yi Li</a>) and Marketing Technology org (led by <a href=\"https://www.linkedin.com/in/michael-kinoti-7a309215/\">Michael Kinoti</a>) for their great collaborations throughout this project!</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=aabebfc85839\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/embedding-based-retrieval-for-airbnb-search-aabebfc85839\">Embedding-Based Retrieval for Airbnb Search</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "使用大型语言模型加速大规模测试迁移 (原标题: Accelerating Large-Scale Test Migration with LLMs)",
      "link": "https://medium.com/airbnb-engineering/accelerating-large-scale-test-migration-with-llms-9565c208023b?source=rss----53c7c27702d5---4",
      "pubDate": "Thu, 13 Mar 2025 17:01:05 GMT",
      "isoDate": "2025-03-13T17:01:05.000Z",
      "creator": "Charles Covey-Brandt",
      "summary": "Airbnb最近完成了一项由大型语言模型（LLM）驱动的大规模代码迁移，将近3500个React组件测试文件从Enzyme更新为使用React Testing Library (RTL)。这项工作原预计需要1.5年的工程时间，但通过结合前沿模型和强大的自动化，仅用6周就完成了整个迁移。\n\n**背景与挑战**\n\n*   **技术栈演进：** Airbnb自2020年起为所有新的React组件测试开发采用了RTL，逐步淘汰Enzyme。\n*   **Enzyme的局限性：** Enzyme设计用于早期React版本，其对组件内部的深度访问不再符合现代React测试实践。\n*   **迁移难题：** 由于Enzyme和RTL之间的根本差异，无法简单替换或删除Enzyme文件，这会造成代码覆盖率的显著缺失。需要一种自动化方法来重构测试文件，同时保留原始测试意图和代码覆盖率。\n\n**LLM驱动的迁移方案**\n\n2023年的一次黑客马拉松证明LLM能够成功转换Enzyme文件。在此基础上，Airbnb于2024年开发了一个可扩展的LLM驱动迁移管道，其核心策略包括：\n\n1.  **文件验证与重构步骤**\n    *   将迁移分解为一系列自动化的验证和重构步骤，类似于生产流水线。\n    *   流程被建模为状态机，文件只有在通过前一阶段的验证后才能进入下一阶段，失败时LLM会介入修复。\n    *   这种分步方法提供了坚实的基础，便于跟踪进度、改进特定步骤的失败率，并支持并发处理数百个文件。\n    *   ![图片 2](https://cdn-images-1.medium.com/max/587/0*cHwpLgo6nzx8bROe)\n        *   图示：从Enzyme重构、修复Jest、修复lint和tsc，到标记文件完成的重构步骤。\n\n2.  **重试循环与动态提示**\n    *   最有效的改进策略是“暴力”重试：多次重试步骤直到通过或达到限制。\n    *   每次重试都使用动态提示，将验证错误和文件的最新版本提供给LLM。\n    *   ![图片 3](https://cdn-images-1.medium.com/max/962/0*XtBaesbBgYOBY_uP)\n        *   图示：重试循环。对于给定步骤N，如果文件有错误，则重试验证并尝试修复，直到达到最大重试次数或文件不再包含错误。\n    *   此方法成功迁移了大量中低复杂度的测试文件，多数在10次尝试内完成。\n\n3.  **增加上下文**\n    *   对于复杂文件，通过向提示中注入尽可能多的相关上下文来提高成功率。\n    *   提示扩展到4万至10万个token，包含多达50个相关文件、手动编写的少量示例以及项目中现有的优秀测试文件。\n    *   每个提示包含：被测组件的源代码、待迁移的测试文件、当前步骤的验证失败信息、同目录下的相关测试（保持团队特定模式）、通用迁移指南和常见解决方案。\n    *   这种丰富的上下文方法对于复杂文件非常有效，帮助LLM更好地理解团队模式和代码库架构。\n    *   首次批量运行在4小时内成功迁移了75%的目标文件。\n\n4.  **从75%到97%：系统性改进**\n    *   为解决剩余的“长尾”文件（约900个），构建了系统性改进工具。\n    *   **可见性：** 自动生成代码注释，记录每个迁移步骤的状态，便于识别常见问题。\n    *   **灵活重跑：** 能够轻松重跑单个文件或按特定步骤过滤的文件。\n    *   通过“采样、调整、扫描”的反馈循环：运行所有失败文件，找出LLM常见问题；选择代表性样本；更新提示和脚本；对样本验证修复；再次对所有剩余文件运行。\n    *   经过4天的此循环，完成率从75%提升到97%，仅剩不到100个文件。\n\n**结果与影响**\n\n*   **效率显著提升：** 自动化迁移了3500个Enzyme文件中的97%，仅用6周时间，而手动预计需1.5年。\n*   **手动干预：** 剩余3%的文件（约100个）通过自动化生成的基线进行手动修复，额外耗时一周。\n*   **质量保证：** 成功替换Enzyme，同时保持了原始测试意图和整体代码覆盖率。\n*   **成本效益：** 总成本（包括LLM API使用和6周工程时间）远低于手动迁移的估计。\n*   ![图片 1](https://cdn-images-1.medium.com/max/1024/1*j0QXnA13Sy5ruaIAU5C_eg.jpeg)\n    *   图示：Airbnb使用LLM将3500个React组件测试文件从Enzyme迁移到React Testing Library，将预计1.5年的工作量缩短至6周。\n\n**展望**\n\n此次迁移凸显了LLM在大规模代码转换中的强大潜力。Airbnb计划扩展此方法，开发更复杂的迁移工具，并探索LLM驱动自动化在提升开发者生产力方面的新应用。",
      "shortSummary": "Airbnb利用大型语言模型（LLMs）成功地将近3500个React组件测试文件从Enzyme迁移到React Testing Library。通过构建自动化管道、采用重试机制和增加上下文，他们将原预计1.5年的手动工作量缩短至仅6周，并实现了97%的自动化迁移率。此举显著提升了开发效率，并展示了LLM在大规模代码转换中的巨大潜力。",
      "translated_title": "使用大型语言模型加速大规模测试迁移",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*j0QXnA13Sy5ruaIAU5C_eg.jpeg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/587/0*cHwpLgo6nzx8bROe",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/962/0*XtBaeswbgYOBY_uP",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9565c208023b",
          "alt": "",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*j0QXnA13Sy5ruaIAU5C_eg.jpeg\" /></figure><p>By: <a href=\"https://www.linkedin.com/in/chazcb\">Charles Covey-Brandt</a></p><p>Airbnb recently completed our first large-scale, LLM-driven code migration, updating nearly 3.5K React component test files from Enzyme to use React Testing Library (RTL) instead. We’d originally estimated this would take 1.5 years of engineering time to do by hand, but — using a combination of frontier models and robust automation — we finished the entire migration in just 6 weeks.</p><p>In this blog post, we’ll highlight the unique challenges we faced migrating from Enzyme to RTL, how LLMs excel at solving this particular type of challenge, and how we structured our migration tooling to run an LLM-driven migration at scale.</p><h3>Background</h3><p>In 2020, Airbnb adopted React Testing Library (RTL) for all new React component test development, marking our first steps away from Enzyme. Although Enzyme had served us well since 2015, it was designed for earlier versions of React, and the framework’s deep access to component internals no longer aligned with modern React testing practices.</p><p>However, because of the fundamental differences between these frameworks, we couldn’t easily swap out one for the other (read more about the differences <a href=\"https://kentcdodds.com/blog/introducing-the-react-testing-library\">here</a>). We also couldn’t just delete the Enzyme files, as analysis showed this would create significant gaps in our code coverage. To complete this migration, we needed an automated way to refactor test files from Enzyme to RTL while preserving the intent of the original tests <em>and</em> their code coverage.</p><h3>How We Did It</h3><p>In mid-2023, an Airbnb hackathon team demonstrated that large language models could successfully convert hundreds of Enzyme files to RTL in just a few days.</p><p>Building on this promising result, in 2024 we developed a scalable pipeline for an LLM-driven migration. We broke the migration into discrete, per-file steps that we could parallelize, added configurable retry loops, and significantly expanded our prompts with additional context. Finally, we performed breadth-first prompt tuning for the long tail of complex files.</p><h3>1. File Validation and Refactor Steps</h3><p>We started by breaking down the migration into a series of automated validation and refactor steps. Think of it like a production pipeline: each file moves through stages of validation, and when a check fails, we bring in the LLM to fix it.</p><p>We modeled this flow like a state machine, moving the file to the next state only after validation on the previous state passed:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/587/0*cHwpLgo6nzx8bROe\" /><figcaption>Diagram shows refactor steps from Enzyme refactor, fixing Jest, fixing lint and tsc, and marking file as complete.</figcaption></figure><p>This step-based approach provided a solid foundation for our automation pipeline. It enabled us to track progress, improve failure rates for specific steps, and rerun files or steps when needed. The step-based approach also made it simple to run migrations on hundreds of files concurrently, which was critical for both quickly migrating simple files, and chipping away at the long tail of files later in the migration.</p><h3>2. Retry Loops &amp; Dynamic Prompting</h3><p>Early on in the migration, we experimented with different prompt engineering strategies to improve our per-file migration success rate. However, building on the stepped approach, we found the most effective route to improve outcomes was simply brute force: retry steps multiple times until they passed or we reached a limit. We updated our steps to use dynamic prompts for each retry, giving the validation errors and the most recent version of the file to the LLM, and built a loop runner that ran each step up to a configurable number of attempts.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/962/0*XtBaeswbgYOBY_uP\" /><figcaption><em>Diagram of a retry loop. For a given step N, if the file has errors, we retry validation and attempt to fix errors unless we hit the max retries or the file no longer contains errors.</em></figcaption></figure><p>With this simple retry loop, we found we could successfully migrate a large number of our simple-to-medium complexity test files, with some finishing successfully after a few retries, and most by 10 attempts.</p><h3>3. Increasing the Context</h3><p>For test files up to a certain complexity, just increasing our retry attempts worked well. However, to handle files with intricate test state setups or excessive indirection, we found the best approach was to push as much relevant context as possible into our prompts.</p><p>By the end of the migration, our prompts had expanded to anywhere between 40,000 to 100,000 tokens, pulling in as many as 50 related files, a whole host of manually written <a href=\"https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/multishot-prompting\">few-shot</a> examples, as well as examples of existing, well-written, passing test files from within the same project.</p><p>Each prompt included:</p><ul><li>The source code of the component under test</li><li>The test file we were migrating</li><li>Validation failures for the step</li><li>Related tests from the same directory (maintaining team-specific patterns)</li><li>General migration guidelines and common solutions</li></ul><p>Here’s how that looked in practice (significantly trimmed down for readability):</p><pre>// Code example shows a trimmed down version of a prompt <br>// including the raw source code from related files, imports, <br>// examples, the component source itself, and the test file to migrate.<br><br>const prompt = [<br>  &#39;Convert this Enzyme test to React Testing Library:&#39;,<br>  `SIBLING TESTS:\\n${siblingTestFilesSourceCode}`,<br>  `RTL EXAMPLES:\\n${reactTestingLibraryExamples}`,<br>  `IMPORTS:\\n${nearestImportSourceCode}`,<br>  `COMPONENT SOURCE:\\n${componentFileSourceCode}`,<br>  `TEST TO MIGRATE:\\n${testFileSourceCode}`,<br>].join(&#39;\\n\\n&#39;);</pre><p>This rich context approach proved highly effective for these more complex files — the LLM could better understand team-specific patterns, common testing approaches, and the overall architecture of the codebase.</p><p>We should note that, although we did some prompt engineering at this step, the main success driver we saw was choosing the <em>right</em> related files (finding nearby files, good example files from the same project, filtering the dependencies for files that were relevant to the component, etc.), rather than getting the prompt engineering perfect.</p><p>After building and testing our migration scripts with retries and rich contexts, when we ran our first bulk run, <strong>we successfully migrated 75% of our target files in just four hours</strong>.</p><h3>4. From 75% to 97%: Systematic Improvement</h3><p>That 75% success rate was really exciting to get to, but it still left us with nearly 900 files failing our step-based validation criteria. To tackle this long tail, we needed a systematic way to understand where remaining files were getting stuck and improve our migration scripts to address these issues. We also wanted to do this <em>breadth first</em> to aggressively chip away at our remaining files without getting stuck on the most difficult migration cases.</p><p>To do this, we built two features into our migration tooling.</p><p>First, we built a simple system to give us visibility into common issues our scripts were facing by stamping files with an automatically-generated comment to record the status of each migration step. Here’s what that code comment looked like:</p><pre>// MIGRATION STATUS: {&quot;enyzme&quot;:&quot;done&quot;,&quot;jest&quot;:{&quot;passed&quot;:8,&quot;failed&quot;:2,&quot;total&quot;:10,&quot;skipped&quot;:0,&quot;successRate&quot;:80},&quot;eslint&quot;:&quot;pending&quot;,&quot;tsc&quot;:&quot;pending&quot;,}</pre><p>And second, we added the ability to easily re-run single files or path patterns, filtered by the specific step they were stuck on:</p><pre>$ llm-bulk-migration --step=fix-jest --match=project-abc/**</pre><p>Using these two features, we could quickly run a feedback loop to improve our prompts and tooling:</p><ol><li>Run all remaining failing files to find common issues the LLM is getting stuck on</li><li>Select a sample of files (5 to 10) that exemplify a common issue</li><li>Update our prompts and scripts to address that issue</li><li>Re-run against the sample of failing files to validate our fix</li><li>Repeat by running against all remaining files again</li></ol><p>After running this “sample, tune, sweep” loop for 4 days, we had pushed our completed files from 75% to 97% of the total files, and had just under 100 files remaining. By this point, we had retried many of these long tail files anywhere between 50 to 100 times, and it seemed we were pushing into a ceiling of what we could fix via automation. Rather than invest in more tuning, we opted to manually fix the remaining files, working from the baseline (failing) refactors to reduce the time to get those files over the finish line.</p><h3>Results and Impact</h3><p>With the validation and refactor pipeline, retry loops, and expanded context in place, we were able to automatically migrate 75% of our target files in 4 hours.</p><p>After four days of prompt and script refinement using the “sample, tune, and sweep” strategy, we reached 97% of the 3.5K original Enzyme files.</p><p>And for the remaining 3% of files that didn’t complete through automation, our scripts provided a great baseline for manual intervention, allowing us to complete the migration for those remaining files in another week of work.</p><p>Most importantly, we were able to replace Enzyme while maintaining original test intent and our overall code coverage. And even with high retry counts on the long tail of the migration, the total cost — including LLM API usage and six weeks of engineering time — proved far more efficient than our original manual migration estimate.</p><h3>What’s Next</h3><p>This migration underscores the power of LLMs for large-scale code transformation. We plan to expand this approach, develop more sophisticated migration tools, and explore new applications of LLM-powered automation to enhance developer productivity.</p><p>Want to help shape the future of developer tools? We’re hiring engineers who love solving complex problems at scale. Check out our <a href=\"https://careers.airbnb.com\">careers page</a> to learn more.</p><h3>****************</h3><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9565c208023b\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/accelerating-large-scale-test-migration-with-llms-9565c208023b\">Accelerating Large-Scale Test Migration with LLMs</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "改进地图搜索排名 (原标题: Improving Search Ranking for Maps)",
      "link": "https://medium.com/airbnb-engineering/improving-search-ranking-for-maps-13b03f2c2cca?source=rss----53c7c27702d5---4",
      "pubDate": "Wed, 18 Dec 2024 18:02:37 GMT",
      "isoDate": "2024-12-18T18:02:37.000Z",
      "creator": "Malay Haldar",
      "summary": "## 改进Airbnb地图搜索排名\n\nAirbnb的搜索结果通过两种界面展示：列表视图（list-results）和地图视图（map-results）。最初，两者的核心排名算法相同，都基于房源的预订概率。然而，针对列表视图设计的排名假设在地图视图中不再适用，因为地图上的房源是分散的，没有固定的排名顺序，用户注意力也不会像列表那样从上到下衰减。\n\n### 地图视图的独特之处\n\n*   **列表视图：** 用户注意力从列表顶部开始衰减，越往下关注度越低。排名算法通过预订概率对房源进行排序，以最大化用户与房东的连接。 \n    ![图1：按房源搜索排名划分的点击率](https://cdn-images-1.medium.com/max/1024/0*Y9drAzLenJ9GAYEA)\n*   **地图视图：** 房源以图钉形式散布在地图上，没有固定的排名列表，用户注意力也不会因排名位置而衰减。因此，直接按预订概率排序的策略不再适用。 \n    ![图2：地图结果](https://cdn-images-1.medium.com/max/624/0*6iaMrBpbSQjVnsLF)\n\n### 逐步优化用户注意力模型\n\n为了适应地图界面，Airbnb逐步改进了用户注意力模型：\n\n#### 1. 均匀用户注意力模型\n\n*   **假设：** 初始假设用户注意力均匀分布在所有地图图钉上。然而，用户点击的图钉数量有限，大量图钉可能导致用户错过最佳选择。 \n    ![图3：按搜索者百分比划分的点击不同地图图钉数量](https://cdn-images-1.medium.com/max/1024/0*Vi5l4XPrl3YdHsP0)\n*   **解决方案：** 引入一个参数，限制所选地图图钉中最高预订概率与最低预订概率的比率。限制越严格，显示的地图图钉的平均预订概率越高。\n*   **效果：** 通过A/B测试，限制版本显著提升了预订量，尤其是高质量预订（5星评价的行程增加），并减少了用户发现所需房源所需的印象数和点击数。 \n    ![图4：通过在线A/B实验进行探索](https://cdn-images-1.medium.com/max/1024/0*trGxNfKu4rHa4Gpx)\n\n#### 2. 分层用户注意力模型\n\n*   **背景：** 在桌面搜索中，左侧通常显示固定数量（例如18个）的列表结果，每个结果都需要一个对应的地图图钉，因此无法简单限制图钉数量。 \n    ![图6：桌面搜索结果](https://cdn-images-1.medium.com/max/1024/0*A83SEjyDlyTUCI06)\n*   **解决方案：** 将地图图钉分为两层：\n    *   **常规椭圆形图钉（带价格）：** 用于预订概率最高的房源。\n    *   **迷你图钉（不带价格）：** 用于预订概率相对较低的房源。迷你图钉的点击率比常规图钉低约8倍，从而引导用户注意力。 \n        ![图5：带价格的椭圆形图钉和迷你图钉](https://cdn-images-1.medium.com/max/1024/0*pkL4ovuWpR1Rz9z-)\n*   **效果：** 通过A/B实验，该方法成功将用户注意力优先引向预订概率最高的房源。 \n    ![图7：分层地图图钉的实验结果](https://cdn-images-1.medium.com/max/842/0*1V-XbGegLzPch25O)\n\n#### 3. 折扣用户注意力模型\n\n*   **洞察：** 通过绘制地图上不同坐标处图钉的点击率，发现用户注意力并非完全均匀，而是随位置变化（移动端和桌面端有所不同）。 \n    ![图8：地图坐标上地图图钉的点击率](https://cdn-images-1.medium.com/max/598/0*rDDubemWn97XvCN2)\n    ![图8：地图坐标上地图图钉的点击率](https://cdn-images-1.medium.com/max/602/0*I9GtvJEw5BGfHn96)\n*   **解决方案：** 设计算法重新定位地图中心，使预订概率最高的房源更靠近中心。算法评估一系列潜在坐标，选择最接近高预订概率房源的作为新中心。 \n    ![图9：寻找最优中心的算法](https://cdn-images-1.medium.com/max/1024/0*IqlsENiSd-9IdQ5v)\n*   **效果：** 在线A/B实验显示，该算法使未取消预订量提升0.27%，地图移动次数减少1.5%，表明用户使用地图的努力程度降低。\n\n### 结论与展望\n\n用户与地图的交互方式与列表截然不同。通过逐步精细化用户与地图的交互模型，Airbnb显著改善了用户的实际体验。然而，当前方法仍面临挑战：如何在地图上展示所有可用房源？这将是未来的工作重点。更深入的技术细节已在KDD '24会议上发表的研究论文中讨论。",
      "shortSummary": "Airbnb改进了其地图搜索排名算法，以适应地图界面与列表界面的根本差异。针对地图上用户注意力分布的特点，团队迭代开发了三种模型：首先，通过限制图钉预订概率范围，提升了预订量和预订质量；其次，在固定图钉数量的场景下，引入分层图钉（常规图钉和迷你图钉）以引导用户注意力；最后，通过算法重新定位地图中心，使高预订概率房源更靠近中心，进一步优化了用户体验并减少了操作。这些改进显著提升了用户体验和预订转化率，但如何在地图上展示所有房源仍是未来的挑战。",
      "translated_title": "改进地图搜索排名",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*DO7m1JZFPSvVRlBG",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*Y9drAzLenJ9GAYEA",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/624/0*6iaMrBpbSQjVnsLF",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*Vi5l4XPrl3YdHsP0",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*trGxNfKu4rHa4Gpx",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<p>How Airbnb is adapting ranking for our map interface.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*DO7m1JZFPSvVRlBG\" /></figure><p><a href=\"https://www.linkedin.com/in/malayhaldar/\">Malay Haldar</a>, <a href=\"https://www.linkedin.com/in/hongwei-zhang-86b15624/\">Hongwei Zhang</a>, <a href=\"https://www.linkedin.com/in/kedar-bellare-3048128a/\">Kedar Bellare</a> <a href=\"https://www.linkedin.com/in/sherrytchen/\">Sherry Chen</a></p><p>Search is the core mechanism that connects guests with Hosts at Airbnb. Results from a guest’s search for listings are displayed through two interfaces: (1) as a list of rectangular cards that contain the listing image, price, rating, and other details on it, referred to as <em>list-results</em> and (2) as oval pins on a map showing the listing price, called <em>map-results</em>. Since its inception, the core of the ranking algorithm that powered both these interfaces was the same — ordering listings by their booking probabilities and selecting the top listings for display.</p><p>But some of the basic assumptions underlying ranking, built for a world where search results are presented as lists, simply break down for maps.</p><h3>What Is Different About Maps?</h3><p>The central concept that drives ranking for list-results is that <em>user attention decays</em> starting from the top of the list, going down towards the bottom. A plot of rank vs click-through rates in Figure 1 illustrates this concept. X-axis represents the rank of listings in search results. Y-axis represents the click-through rate (CTR) for listings at the particular rank.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Y9drAzLenJ9GAYEA\" /><figcaption>Figure 1: Click-through rates by listing search rank</figcaption></figure><p>To maximize the connections between guests and Hosts, the ranking algorithm sorts listings by their booking probabilities based on a <a href=\"https://www.airbnb.com/help/article/39\">number of factors</a> and sequentially assigns their position in the list-results. This often means that the larger a listing’s booking probability, the more attention it receives from searchers.</p><p>But in map-results, listings are scattered as pins over an area (see Figure 2). There is no ranked list, and there is no decay of user attention by ranking position. Therefore, for listings that are shown on the map, the strategy of sorting by booking probabilities is no longer applicable.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/624/0*6iaMrBpbSQjVnsLF\" /><figcaption>Figure 2: Map results</figcaption></figure><h3>Uniform User Attention</h3><p>To adapt ranking to the map interface, we look at new ways of modeling user attention flow across a map. We start with the most straightforward assumption that user attention is spread equally across the map pins. User attention is a very precious commodity and most searchers only click through a few map pins (see Figure 3). A large number of pins on the map means those limited clicks may miss discovering the best options available. Conversely, limiting the number of pins to the topmost choices increases the probability of the searcher finding something suitable, but runs the risk of removing their preferred choice.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Vi5l4XPrl3YdHsP0\" /><figcaption>Figure 3: Number of distinct map pins clicked by percentage of searchers</figcaption></figure><p>We test this hypothesis, controlled by a parameter . The parameter serves as an upper bound on the ratio of the highest booking probability vs the lowest booking probability when selecting the map pins. The bounds set by the parameter controls the booking probability of the listings behind the map pins. The more restricted the bounds, the higher the average booking probability of the listings presented as map pins. Figure 4 summarizes the results from A/B testing a range of parameters.</p><p>The reduction in the average impressions to discovery metric in Figure 4 denotes the fewer number of map pins a searcher has to process before clicking the listing that they eventually book. Similarly, the reduction in average clicks to discovery shows the fewer number of map pins a searcher has to click through to find the listing they booked.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*trGxNfKu4rHa4Gpx\" /><figcaption>Figure 4: Exploring through online A/B experiments</figcaption></figure><p>Launching the restricted version resulted in one of the largest bookings improvement in Airbnb ranking history. More importantly, the gains were not only for bookings, but for quality bookings. This could be seen by the increase in trips that resulted in 5-star rating after the stay from the treatment group, in comparison to trips from the control group.</p><h3>Tiered User Attention</h3><p>In our next iteration of modeling user attention, we separate the map pins into two tiers. The listings with the highest booking probabilities are displayed as regular oval pins with price. Listings with comparatively lower booking probabilities are displayed as smaller ovals without price, referred to as mini-pins (Figure 5). By design, mini-pins draw less user attention, with click-through rates about 8x less than regular pins.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*pkL4ovuWpR1Rz9z-\" /><figcaption>Figure 5: Oval pins with price and mini-pins</figcaption></figure><p>This comes in handy particularly for searches on desktop where 18 results are shown in a grid on the left, each of them requiring a map pin on the right (Figure 6).</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*A83SEjyDlyTUCI06\" /><figcaption>Figure 6: Search results on desktop</figcaption></figure><p>The number of map pins is fixed in this case, and limiting them, as we did in the previous section, is not an option. Creating the two tiers prioritizes user attention towards the map pins with the highest probabilities of getting booked. Figure 7 shows the results of testing the idea through an online A/B experiment.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/842/0*1V-XbGegLzPch25O\" /><figcaption>Figure 7: Experiment results for tiered map pins</figcaption></figure><h3>Discounted User Attention</h3><p>In our final iteration, we refine our understanding of how user attention is distributed over the map by plotting the click-through rate of map pins located at different coordinates on the map. Figure 8 shows these plots for the mobile (top) and the desktop apps (bottom).</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/598/0*rDDubemWn97XvCN2\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/602/0*I9GtvJEw5BGfHn96\" /><figcaption>Figure 8: Click-through rates of map pins across map coordinates.</figcaption></figure><p>To maximize the chances that a searcher will discover the listings with the highest booking probabilities, we design an algorithm that re-centers the map such that the listings with the highest booking probabilities appear closer to the center. The steps of this algorithm are illustrated in Figure 9, where a range of potential coordinates are evaluated and the one which is closer to the listings with the highest booking probabilities is chosen as the new center.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*IqlsENiSd-9IdQ5v\" /><figcaption>Figure 9: Algorithm for finding optimal center</figcaption></figure><p>When tested in an online A/B experiment, the algorithm improved uncancelled bookings by 0.27%. We also observed a reduction of 1.5% in map moves, indicating less effort from the searchers to use the map.</p><h3>Conclusion</h3><p>Users interact with maps in a way that’s fundamentally different from interacting with items in a list. By modeling the user interaction with maps in a progressively sophisticated manner, we were able to improve the user experience for guests in the real world. However, the current approach has a challenge that remains unsolved: how can we represent the full range of available listings on the map? This is part of our future work. A more in-depth discussion of the topics covered here, along with technical details, is presented in our research paper that was <a href=\"https://arxiv.org/pdf/2407.00091\">published at the <strong>KDD ’24</strong> conference</a>. We welcome all feedback and suggestions.</p><p>If this type of work interests you, we encourage you to apply for an<a href=\"https://careers.airbnb.com/\"> open position</a> today.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=13b03f2c2cca\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/improving-search-ranking-for-maps-13b03f2c2cca\">Improving Search Ranking for Maps</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "爱彼迎在 KDD 2024 大会 (原标题: Airbnb at KDD 2024)",
      "link": "https://medium.com/airbnb-engineering/airbnb-at-kdd-2024-d5c2fa81a119?source=rss----53c7c27702d5---4",
      "pubDate": "Tue, 17 Dec 2024 18:02:43 GMT",
      "isoDate": "2024-12-17T18:02:43.000Z",
      "creator": "Huiji Gao",
      "summary": "# 爱彼迎在 KDD 2024 大会\n\n## 引言\n爱彼迎在西班牙巴塞罗那举行的 KDD 2024 大会上表现突出。爱彼迎的数据科学家和工程师就以下主题进行了演讲：\n*   深度学习与搜索排名\n*   在线实验与测量\n*   产品质量与客户旅程\n*   双边市场\n\n本文总结了爱彼迎在 KDD 2024 的贡献，并提供了相关学术论文的访问途径。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/0*JnSzLDm3Uh2hY6c2)\n\n## KDD 大会概览\nKDD（知识发现与数据挖掘）是数据挖掘和机器学习领域最负盛名的全球会议之一，由计算机协会 (ACM) 的一个特殊兴趣小组每年举办。第 30 届 KDD 大会在西班牙巴塞罗那举行，吸引了数千名学术界和工业界的研究人员和科学家。参会公司包括谷歌、Meta、苹果、亚马逊、爱彼迎等。会议共接受了 151 篇应用数据科学 (ADS) 论文和 411 篇研究论文，以及 34 场教程和 30 场研讨会。\n\n## 爱彼迎在 KDD 2024 的重要贡献\n爱彼迎在 KDD 2024 拥有显著影响力，具体贡献包括：\n*   三篇完整的 ADS 论文（接受率低于 20%）\n*   一个研讨会\n*   七篇研讨会论文和受邀演讲被主会议收录\n\n爱彼迎的工作主题涵盖：\n*   深度学习与搜索排名\n*   在线实验与测量\n*   因果推断与机器学习\n*   双边市场\n\n## 主要贡献领域\n\n### 1. 深度学习与搜索排名\n智能搜索排名旨在根据房客偏好、房源特征和额外搜索上下文来准确匹配房客与房源，这在双边市场中仍是一个复杂挑战。爱彼迎发布了多篇论文来解决搜索排名问题。\n\n*   **《爱彼迎地图学习排名》**\n    *   **问题：** 爱彼迎的搜索结果主要通过列表和地图两种界面显示，但传统的排名算法是为列表设计，在地图界面上存在基本假设失效的问题。\n    *   **解决方案：** 重构了地图结果的排名算法，修订了用户与地图搜索结果互动方式的数学基础，并提出了统一两种界面的理论。\n    *   **影响：** 通过迭代和实验驱动的方法，显著提升了用户体验。未来的研究方向包括地图结果的无偏学习排名和引导用户关注更相关房源。\n\n*   **《通过模型蒸馏实现多目标学习排名》**\n    *   **问题：** 在线市场中的搜索排名不仅要优化主要目标（如转化率），还要兼顾次要目标（如订单取消、评价、客服咨询、平台长期增长）。传统方法面临参数调整昂贵、数据稀疏不平衡和兼容性差等挑战。\n    *   **解决方案：** 提出了一种基于蒸馏的排名解决方案，优化了爱彼迎跨多个排名模型的端到端排名系统，并考虑了训练和服务效率。\n    *   **影响：** 该方案显著提升了主要目标转化率，同时满足了次要目标约束并提高了模型稳定性。此外，该系统可通过模型自蒸馏进一步简化，并能高效地将非可微分的业务目标注入排名系统。\n\n### 2. 在线实验与测量\n在线实验（如 A/B 测试）是爱彼迎等组织进行数据驱动决策的常用方式，但高方差是一个常见挑战。\n\n*   **《A/B 测试中的指标分解》**\n    *   **问题：** CUPED（利用实验前数据进行对照实验）等方差降低方法存在理论限制，且在实验中利用数据进行处理效应增强的实践指导不足。\n    *   **解决方案：** 提出通过处理效应增强来提高敏感度的新方向，将目标指标分解为两个或更多组件，以分离高信号低噪声和低信号高噪声的部分。\n    *   **影响：** 理论、模拟和实证示例表明，如果存在这种分解，可以提高敏感度。提供了三个真实世界应用案例，展示了指标分解相对于未分解分析的敏捷性提升。未来的工作包括在频率派和贝叶斯框架下确定样本量。\n\n### 3. 双边市场优化\n爱彼迎员工主持了“双边市场优化：搜索、定价、匹配与增长”研讨会，讨论了在双边市场中解决生产者（房东）和消费者（房客）需求的演变。机器学习方法在爱彼迎这类复杂的互联网规模双边市场中至关重要。爱彼迎的贡献旨在优化房客体验、寻找房源价格均衡点、减少不良互动（降低客服成本）、大规模检测运营人员跟进活动等。\n\n*   **《用于个性化的房客意图建模》**\n    *   **问题：** 理解用户意图对提供无缝和个性化体验至关重要，但有限的用户数据和不可预测的房客行为使其难以推断房客的真实意图。\n    *   **解决方案：** 采用深度学习方法预测用户旅行计划中难以推断的细节，如下一个目的地和旅行日期。该框架分析用户应用内浏览历史、预订历史、搜索查询和其他互动信号，生成多个用户意图信号。\n    *   ![图片 2](https://cdn-images-1.medium.com/max/1024/0*9QYBiZCmirD4aV1o)\n    *   **影响：** 有助于营销邮件、灵活旅行搜索和应用首页推荐，也帮助房东优化房源以提高满意度和预订量。\n\n*   **《房客需求理解》**\n    *   **问题：** 大多数房东不是专业酒店从业者，难以正确为其房源定价，需要数据和建议。\n    *   **解决方案：** 结合经济建模和因果推断技术，将房客分段并估计其价格敏感度，通过小规模定向实验和大规模自然实验进行微调。\n    *   ![图片 3](https://cdn-images-1.medium.com/max/656/0*-3Mj0gacSPA56ZQx)\n    *   **影响：** 模型输出帮助房东在更高入住率和更高每晚价格之间做出权衡。\n\n*   **《用于房东侧产品的房源嵌入》**\n    *   **问题：** 爱彼迎提供许多基于房源比较的工具（寻找相似或等效替代品）。\n    *   **解决方案：** 研究了房源嵌入在爱彼迎双边市场中的应用和学习。讨论了使用房客互动数据训练神经网络嵌入模型的架构和训练，并将其应用于房东侧产品界面。解决了负训练样本构建、训练数据采样偏差校正以及通过模型内缓存进行扩展和加速训练等技术挑战。\n    *   **影响：** 评估方法包括批内指标、基于词汇的评估和相似房源属性。已应用于爱彼迎产品，如房东日历相似房源。\n\n*   **《搜索排名中的客户支持优化》**\n    *   **问题：** 爱彼迎致力于防止不良体验，同时保持增长。\n    *   **解决方案：** 利用大量累积的客服数据建模用户预订后需要客服支持的概率。模型发现搜索者、房源和房东的多个特征能准确预测客服需求（例如，当天预订需要更多支持，响应迅速的房东能减少支持需求）。\n    *   **影响：** 将客服模型输出纳入搜索结果排名，预测可能导致负面体验的房源排名会降低。\n\n*   **《使用活动日志进行 LLM 预训练》**\n    *   （文章内容在此处截断）旨在通过分析用户活动日志来预训练大型语言模型，以便在大规模用户互动后进行跟进，确保高质量体验。",
      "shortSummary": "爱彼迎在 KDD 2024 大会上表现突出，展示了其在数据挖掘和机器学习领域的最新研究成果。爱彼迎贡献了三篇 ADS 论文、一个研讨会及多篇研讨会论文，涵盖深度学习与搜索排名、在线实验与测量、因果推断与机器学习以及双边市场优化等核心领域。具体工作包括地图排名算法重构、多目标学习排名、A/B 测试指标分解、房客意图建模、房客需求理解、房源嵌入及客户支持优化等，旨在提升用户体验、优化市场效率并解决复杂业务挑战。",
      "translated_title": "爱彼迎在 KDD 2024 大会",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*JnSzLDm3Uh2hY6c2",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*9QYBiZCmirD4aV1o",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/656/0*-3Mj0gacSPA56ZQx",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d5c2fa81a119",
          "alt": "",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<p>Airbnb had a large presence at the 2024 KDD conference hosted in Barcelona, Spain. Our Data Scientist and Engineers presented on topics like Deep Learning &amp; Search Ranking, Online Experimentation &amp; Measurement, Product Quality &amp; Customer Journey, and Two-sided Marketplaces. This blog post summarizes our contributions to KDD for 2024 and provides access to the academic papers presented during the conference.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*JnSzLDm3Uh2hY6c2\" /></figure><p>Authors: <a href=\"mailto:huiji.gao@airbnb.com\">Huiji Gao</a>, <a href=\"mailto:peter.coles@airbnb.com\">Peter Coles</a>, <a href=\"mailto:carolina.barcenas@airbnb.com\">Carolina Barcenas</a>, <a href=\"mailto:sanjeev.katariya@airbnb.com\">Sanjeev Katariya</a></p><p><a href=\"https://kdd.org/\">KDD</a> (Knowledge and Data Mining) is one of the most prestigious global conferences in data mining and machine learning. Hosted annually by a special interest group of the Association for Computing Machinery (ACM), it’s where attendees learn about some of the most ground-breaking AI developments in data mining, machine learning, knowledge discovery, and large-scale data analytics.</p><p>This year, the 30th KDD conference was held at Barcelona, Spain, attracting thousands of researchers and scientists from academia and industry. Various companies contributed to and attended the conference including Google, Meta, Apple, Amazon, Airbnb, Pinterest, LinkedIn, Booking, Expedia, ByteDance etc. There were 151 Applied Data Science (ADS) track papers and 411 Research track papers accepted, 34 tutorials, and 30 workshops.</p><p>Airbnb had a significant presence at KDD 2024 with three full <a href=\"https://kdd2024.kdd.org/applied-data-science-track-papers\">ADS track</a> papers (acceptance rate under 20%), one workshop, and seven workshop papers and invited talks accepted into the main conference proceedings. The topics of our work spanned Deep learning &amp; Search Ranking, Online Experimentation &amp; Measurement, Causal Inference &amp; Machine Learning, and Two-sided Marketplaces.</p><p>In this blog post, we will summarize our teams’ contributions and share highlights from an exciting week-long conference with research and industry talks, workshops, panel discussions, and more.</p><h3><strong>Deep Learning and Search Ranking</strong></h3><p>Intelligent search ranking — the process of accurately matching a guest with a listing based on their preference, a listing’s features, and additional search context — still remains a nuanced challenge that researchers are constantly trying to solve.</p><p>Making optimal guest-host matches has remained an issue in a two-sided marketplace for a variety of reasons — the timespan of guest searches (ranging between days and weeks), unpredictable host behavior and ratings (the potential for hosts to cancel a booking or receive low ratings), and limited understanding of guest preference across multiple interfaces. We published several papers addressing the issue of search ranking as part of our presence at KDD.</p><p><a href=\"https://arxiv.org/abs/2407.00091\"><strong>Learning to Rank for Maps at Airbnb</strong></a></p><p>Airbnb brings together hosts who rent listings to prospective guests from around the globe. Results from a guest’s search for listings are displayed primarily through two interfaces: (1) as a list of rectangular cards that contain on them the listing image, price, rating, and other details, referred to as list-results, and (2) as oval pins on a map showing the listing price, called map-results. Both these interfaces, since their inception, have used the same ranking algorithm that orders listings by their booking probabilities and selects the top listings for display.</p><p>However, some of the basic assumptions underlying ranking are built for a world where search results are presented as lists and simply break down for map-results. In this work, we rebuilt ranking for maps by revising the mathematical foundations of how users interact with map search results. Our iterative and experiment-driven approach led us through a path full of twists and turns, ending in a unified theory for the two interfaces.</p><p>Our journey shows how assumptions taken for granted when designing machine learning algorithms may not apply equally across all user interfaces, and how they can be adapted. The net impact was one of the largest improvements in user experience for Airbnb which we discuss as a series of experimental validations. The work introduced in this paper is merely the beginning of future exciting research projects, such as making learning to rank unbiased for map-results and demarcating the map pins to direct the user attention towards more relevant ones.</p><p><a href=\"https://arxiv.org/abs/2407.07181\"><strong>Multi-objective Learning to Rank by Model Distillation</strong></a></p><p>In online marketplaces, the objective of search ranking is not only on optimizing purchasing or conversion rate (primary objective), but also the purchase outcomes (secondary objectives), e.g. order cancellation, review rating, customer service inquiries, platform long term growth. To balance these primary and secondary objectives, several multi-objective learning to rank approaches have been widely studied</p><p>Traditional approaches in industrial search and recommender systems encounter challenges such as expensive parameter tuning that leads to sub-optimal solutions, suffering from imbalanced data sparsity issues, and lack of compatibility with ad-hoc objectives. In this work, we propose a distillation-based ranking solution for multi-objective ranking, which optimizes the end-to-end ranking system at Airbnb across multiple ranking models on different objectives, along with various considerations to optimize training and serving efficiency that meets industry standards.</p><p>Compared with traditional approaches, the proposed solution not only significantly meets and increases the primary objective of conversion by a large margin, but also addresses the secondary objective constraints while improving model stability. Furthermore, we demonstrated the proposed system could be further simplified by model self-distillation. We also did additional simulations to show that this approach could help us efficiently inject ad-hoc non-differentiable business objectives into the ranking system, while enabling us to balance our optimization objectives.</p><h3><strong>Online Experimentation and Measurement</strong></h3><p>Online experimentation (e.g., A/B testing) is a common way for organizations like Airbnb to make data-driven decisions. But high variance is frequently a challenge. For example, it’s hard to prove that a change in our search UX will drive value because bookings can be infrequent and depend on a large number of interactions over a long period of time.</p><p><a href=\"https://dl.acm.org/doi/pdf/10.1145/3637528.3671556\"><strong>Metric Decomposition in A/B Tests</strong></a></p><p>More than a decade ago, CUPED (Controlled Experiments Utilizing Pre-Experiment Data) mainstreamed the idea of variance reduction leveraging pre-experiment covariates. Since its introduction, it has been implemented, extended, and modernized by major online experimentation platforms. Despite the wide adoption, it is known by practitioners that the variance reduction rate from CUPED, utilizing pre-experimental data, varies case by case and has a theoretical limit. In theory, CUPED can be extended to augment a treatment effect estimator utilizing in-experiment data, but practical guidance on how to construct such an augmentation is lacking.</p><p>In this work, we fill this gap by proposing a new direction for sensitivity improvement via treatment effect augmentation, whereby a target metric of interest is decomposed into</p><p>two or more components in an attempt to isolate those with high signal and low noise from those with low signal and high noise. We show through theory, simulation, and empirical examples that if such a decomposition exists (or can be engineered), sensitivity may be increased via approximately null augmentation (in a frequentist setting) and reduced posterior variance (in a Bayesian setting).</p><p>We provide three real world applications demonstrating different flavors of metric decomposition. These applications illustrate the gain in agility metric decomposition yields relative to an un-decomposed analysis, indicating both empirically and theoretically the value of this practice in both frequentist and Bayesian settings. An important extension to this work would be to next consider sample size determination in both the frequentist or Bayesian contexts; while a boost in sensitivity typically means less data is required for a given analysis, a methodology that determines the smallest sample size required to control various operating characteristics in this context would be of practical value.</p><h3>Two-sided Marketplace Optimization</h3><p>Airbnb employees hosted a workshop on <a href=\"https://sites.google.com/view/tsmo2024/home?authuser=0\">Two-sided Marketplace Optimization: Search, Pricing, Matching &amp; Growth</a>. This workshop brought practitioners of two-sided marketplaces together and discussed the evolution of content ranking, recommendation systems, and data mining when solving for producers and consumers on these platforms.</p><p>Two-sided marketplaces have recently emerged as viable business models for many real-world applications. They model transactions as a network with two distinct types of participants: one type to represent the supply and another the demand of a specific good. Traditionally, research related to online marketplaces focused on how to better satisfy demand. But with two-sided marketplaces, there is more nuance at play. Modern global examples, like Airbnb, operate platforms where users provide services; users may be hosts,or guests. Such platforms must develop models that address all their users’ needs and goals at scale. Machine learning-powered methods and algorithms are essential in every aspect of such complex, internet-scale-sized, two-sided marketplaces.</p><p>Airbnb is a community based on connection and belonging–we strive to connect people and places. Our contributions to this workshop showcase the work we’re doing to support this mission by optimizing guest experiences, finding equilibrium spots for listing prices, reducing the incidence of poor interactions (and customer support costs as a side effect), detecting when operational staff should follow up on activity at scale, and more.</p><p><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Understanding-User-Booking-Intent-at-Airbnb.pdf\"><strong>Guest Intention Modeling for Personalization</strong></a></p><p>Airbnb has transformed the way people travel by offering unique and personalized stays in destinations worldwide. To provide a seamless and tailored experience, understanding user intent plays an important role.</p><p>However, limited user data and unpredictable guest behavior can make it difficult to understand the essential intent from guests on listings from hosts. Our work shows how we approach this challenging problem. We describe how we apply a deep learning approach to predict difficult-to-infer details for a user’s travel plan, such as the next destination and travel dates. The framework analyzes high-level information from users’ in-app browsing history, booking history, search queries, and other engagement signals, and produces multiple user intent signals.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*9QYBiZCmirD4aV1o\" /></figure><p>Marketing emails, flexible travel search (e.g., for “Europe in the summer”), and recommendations on the app home page are three guest interactions that benefit from correct intention modeling. Hosts also benefit, since a clear understanding of guest demand can help them optimize listings to increase satisfaction and bookings.</p><p><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Understanding-Guest-Preferences-and-Optimizing-.pdf\"><strong>Guest Demand Understanding</strong></a></p><p>Hosts can find it difficult to correctly price their listings in two-sided marketplaces serviced by end users. Most hosts are not professional hospitality workers, and would benefit from access to data and advice on how guests see their listings and how they compare to other listings in their neighborhood. We constantly look for ways to give guidance on how hosts can optimally price their listings. The same information can then be used to help guests find their ideal stay.</p><p>In our paper, we presented an example of how this problem can be solved in general.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/656/0*-3Mj0gacSPA56ZQx\" /></figure><p>As illustrated above, both demand and supply change over time, influencing the equilibrium price for a property at a specific point. A historical optimum (such as A above) has to be adjusted to find the current optimum (point C). It is difficult to run experiments since any large-scale experiment we might run will cause the environment to change in complex ways. We tackle this problem by combining economic modeling with causal inference techniques. We segment guests and estimate how price-sensitive each guest segment is, and fine-tune them with empirical data from small targeted experiments and larger-scale natural ones, which are used to adjust estimates for the price sensitivity of each guest segment. Hosts can then use the models’ output to make informed tradeoffs between higher occupancy and higher nightly rates.</p><p><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Learning-and-Applying-Airbnb-Listing-Embeddings-.pdf\"><strong>Listing Embedding for Host-side Products</strong></a></p><p>In order to facilitate the matching of listings and guests, Airbnb provides numerous products and services to both hosts and guests. Many of these tools are based on the ability to compare listings, i.e. finding similar listings or listings that may be viewed as equivalent substitutes. Our work presents a study on the application and learning of listing embeddings in Airbnb’s two-sided marketplace. Specifically, we discuss the architecture and training of a neural network embedding model using guest side engagement data, which is then applied to host-side product surfaces. We address the key technical challenges we encountered, including the formulation of negative training examples, correction of training data sampling bias, and the scaling and speeding up training with the help of in-model caching. Additionally, we discuss our comprehensive approach to evaluation, which ranges from in-batch metrics and vocabulary-based evaluation to the properties of similar listings. Finally, we share our insights from utilizing listing embeddings in Airbnb products, such as host calendar similar listings.</p><p><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Predicting-Potential-Customer-Support-Needs-and-OptimizingSearch-Ranking.pdf\"><strong>Customer Support Optimization in Search Ranking</strong></a></p><p>As of the date of the paper, Airbnb had more than 7.7 million listings from more than 5 million hosts worldwide. Airbnb is investing both in rapid growth and in making sure that the booking experience is pleasant for hosts and guests. It would, however, be ideal to avoid poor experiences in the first place. Our work highlights how we prevent poor experiences without significantly reducing growth.</p><p>We use the mass of accumulated support data at Airbnb to model the probability that, if the current user were to book a listing, they would require CS support. Our model discovered multiple features about the searcher, home, and hosts that accurately predict CS requirements. For example, same-day bookings tend to require more support, and a responsive host tends to reduce support needs. So, if a guest chooses a same-day booking, matching them with a highly responsive host can lead to a better experience overall. We incorporate the output of our CS support model in search result rankings; booked homes will sometimes rank lower if we predict a booking will lead to a negative experience.</p><p><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Can-Language-Models-Accelerate-Prototyping-for-Non-Language-Data-Classification-Summarization-of-Activity-Logs-as-Text.pdf\"><strong>LLM Pretraining using Activity Logs</strong></a></p><p>It’s often important to follow up with users after they’ve had a long series of interactions with a two-sided marketplace to help make sure that their experiences are of high quality. When user interactions meet certain business criteria, operations agents create tickets to follow up with them. For example, user retention and reactivation agents might review user activity logs and decide to follow up with the user, to encourage them to re-engage with the platform.</p><p>We propose transforming structured data (activity logs) into a more manageable text format and then leveraging modern language models (i.e., BERT) to pretrain a large language model based on user activities. We then performed fine-tuning on the model using historical data about which users were followed up with and checked its predictions. Our work demonstrates the large language model trained on pre-processed activity can successfully identify when a user should be followed up with, at an experimentally significant rate. Our preliminary results suggest that our framework may outperform by 80% the average precision of a similar model that was designed relying heavily on feature engineering.</p><h3>Product Quality and Customer Journey Optimization</h3><p>Typically, product quality is evaluated based on structured data. Customer ratings, types of support issues, resolution times, and other factors are used as a proxy for how someone booking on Airbnb might value a listing. This kind of data has limitations — more popular listings have more data, often users don’t leave feedback, and feedback is usually biased towards the positive (users with negative experiences tend to churn and not give feedback).</p><p>In the Workshop on Causal Inference and Machine Learning in Practice, we highlighted an example of how we push the boundaries of product quality assessment techniques and applications, mixing traditional casual inference with cutting-edge machine learning research. In our work “<a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Understanding-Product-Quality-with-Unstructured-Data.pdf\">Understanding Product Quality with Unstructured Data: An Application of LLMs and Embeddings at Airbnb</a>”, we presented how an approach based on text embeddings and LLMs can be combined with approaches based on structured data to significantly improve product quality evaluations. We generate text embeddings on a mix of listing and review texts, then cluster the embeddings based on rebooking and churn rates. Once we have clear clusters, we extract keywords from the original data, and use these keywords to calculate a listing quality score, based on their similarity to the keyword list.</p><p>In addition, we were invited to give a talk <a href=\"https://sites.google.com/view/kdd-workshop-2023\">on Quality Foundations at Airbnb</a>, at KDD’s 3r<a href=\"https://sites.google.com/view/kdd-workshop-2023\">d Workshop on End-End Customer Journey Optimization.</a> It’s often hard to differentiate the quality of customer experiences using simple review ratings, in part due to the tightness of their distribution. In this talk, we present an alternative notion of quality based on customer revealed preference: did a customer return to use the platform again after their experience? We describe how a metric — Guest Return Propensity (GRP) — leverages this concept and can differentiate quality, capture platform externalities, and predict future returns.</p><p>In practice, this measure may not be suited to many common business use cases due to its lagging nature and an inability to easily explain why it has changed. We describe a quality measurement system that builds on the conceptual foundation of GRP by modeling it as an outcome of upstream realized quality signals. These signals — from sources like reviews and customer support — are weighted by their impact on return propensity and mapped to a quality taxonomy to aid in explainability. The resulting score is capable of finely differentiating the quality of customer experiences, aiding tradeoff decisions, and providing timely insights.</p><h3>Conclusion</h3><p>The 2024 edition of KDD was an amazing opportunity for data scientists and machine learning engineers from across the globe and industry, government, and academia, to connect and exchange learnings and discoveries. We were honored to have the opportunity to share some of our knowledge and techniques, generalizing what we have been learning when we apply machine learning to problems we see at Airbnb. We continue to focus on improving our customers’ experience and growing our business, and the information we’ve shared has been crucial to our success. We’re excited to continue learning from peers and contribute our work back to our community. We eagerly await advancements and improvements that might come about as others build upon the work we’ve shared.</p><p>Below, you’ll find a complete list of the talks and papers shared in this article along with the team members who contributed. If this type of work interests you, we encourage you to apply for an<a href=\"https://careers.airbnb.com/\"> open position</a> today.</p><h3>List of papers and talks</h3><p><strong>Learning to Rank for Maps at Airbnb (</strong><a href=\"https://dl.acm.org/doi/10.1145/3637528.3671648\"><strong>link</strong></a><strong>)</strong></p><p>Authors: Malay Haldar, Hongwei Zhang, Kedar Bellare, Sherry Chen, Soumyadip Banerjee, Xiaotang Wang, Mustafa Abdool, Huiji Gao, Pavan Tapadia, Liwei He, Sanjeev Katariya</p><p><strong>Multi-objective Learning to Rank by Model Distillation (</strong><a href=\"https://dl.acm.org/doi/10.1145/3637528.3671597\"><strong>link</strong></a><strong>)</strong></p><p>Authors: Jie Tang, Huiji Gao, Liwei He, Sanjeev Katariya</p><p><strong>Metric Decomposition in A/B Tests (</strong><a href=\"https://dl.acm.org/doi/10.1145/3637528.3671556\"><strong>link</strong></a><strong>)</strong></p><p>Authors: Alex Deng (former employee at Airbnb), Luke Hagar (University of Waterloo), Nathaniel T. Stevens (University of Waterloo), Tatiana Xifara (Airbnb), Amit Gandhi (University of Pennsylvania)</p><p><strong>Understanding Guest Preferences and Optimizing Two-sided Marketplaces: Airbnb as an Example (</strong><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Understanding-Guest-Preferences-and-Optimizing-.pdf\"><strong>link</strong></a><strong>)</strong></p><p>Authors: Yufei Wu, Daniel Schmierer</p><p><strong>Predicting Potential Customer Support Needs and Optimizing Search Ranking in a Two-Sided Marketplace (</strong><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Predicting-Potential-Customer-Support-Needs-and-OptimizingSearch-Ranking.pdf\"><strong>link</strong></a><strong>)</strong></p><p>Authors: Do-kyum Kim, Han Zhao, Huiji Gao, Liwei He, Malay Haldar, Sanjeev Katariya</p><p><strong>​​Understanding User Booking Intent at Airbnb (</strong><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Understanding-User-Booking-Intent-at-Airbnb.pdf\"><strong>link</strong></a><strong>)</strong></p><p>Authors: Xiaowei Liu, Weiwei Guo, Jie Tang, Sherry Chen, Huiji Gao, Liwei He, Pavan Tapadia, Sanjeev Katariya</p><p><strong>Can Language Models Accelerate Prototyping for Non-Language Data? Classification &amp; Summarization of Activity Logs as Text (</strong><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Can-Language-Models-Accelerate-Prototyping-for-Non-Language-Data-Classification-Summarization-of-Activity-Logs-as-Text.pdf\"><strong>link</strong></a><strong>)</strong></p><p>Authors: José González-Brenes</p><p><strong>Learning and Applying Airbnb Listing Embeddings in Two-Sided Marketplace (</strong><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Learning-and-Applying-Airbnb-Listing-Embeddings-.pdf\"><strong>link</strong></a><strong>)</strong></p><p>Authors: Siarhei Bykau, Dekun Zou</p><p><strong>Understanding Product Quality with Unstructured Data: An Application of LLMs and Embeddings at Airbnb (</strong><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Understanding-Product-Quality-with-Unstructured-Data.pdf\"><strong>link</strong></a><strong>)</strong></p><p>Authors: Jikun Zhu, Zhiying Gu, Brad Li, Linsha Chen</p><p><strong>Invited Talk: Quality Foundations at Airbnb</strong></p><p>Speakers: Peter Coles, Mike Egesdal</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d5c2fa81a119\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/airbnb-at-kdd-2024-d5c2fa81a119\">Airbnb at KDD 2024</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "我的Airbnb之旅 | Vijaya Kaza (原标题: My Journey To Airbnb | Vijaya Kaza)",
      "link": "https://medium.com/airbnb-engineering/my-journey-to-airbnb-vijaya-kaza-8f06543b38d5?source=rss----53c7c27702d5---4",
      "pubDate": "Thu, 05 Dec 2024 18:36:44 GMT",
      "isoDate": "2024-12-05T18:36:44.000Z",
      "creator": "Lauren Mackevich",
      "summary": "# 我的Airbnb之旅 | Vijaya Kaza\n\n本文是Vijaya Kaza的自述，讲述了她如何成为Airbnb首席安全官（CSO）兼信任与安全工程主管的职业历程。她目前负责开发技术（平台、工具和AI模型）以保护Airbnb社区，并确保Airbnb基础设施和信息资产的安全。她还是Airbnb技术多元化委员会的执行联合发起人。\n\n## 早期教育与职业起步\n*   **学术背景：** Vijaya Kaza在印度一个大家庭中长大，从小在科学和数学方面表现出色。她自然而然地选择了工程学，获得了电气工程学士学位和两个硕士学位，为她在技术领域的工作奠定了基础。\n*   **意外进入网络安全领域：**\n    *   她的第一份工作是在思科担任软件工程师。\n    *   她进入安全领域纯属偶然，只是追随了一位她喜欢的经理，后者调到了一个新的安全业务部门。她从此爱上了安全工作，并在此领域深耕。\n    *   在思科工作17年，领导了价值10亿美元的安全产品组合开发。\n    *   随后加入FireEye，负责帮助公司从本地部署模式转型为云/SaaS业务模式，并增加了云安全产品组合的收入。\n    *   之后在旧金山的一家移动安全初创公司Lookout领导产品开发，这段消费者安全经验为她日后在Airbnb的角色提供了宝贵基础。\n    *   她指出，网络安全是一个不断创新、充满活力的领域，每一次技术变革（从云到移动再到AI）都带来新的安全挑战。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/1*FfdFpfjGZODh7ltZ2W2Mrg.jpeg)\n\n## 意外的Airbnb机会\n*   **最初的犹豫：** 当Airbnb首次接触她担任首席安全官（CSO）一职时，她感到惊讶，因为她一直从事工程和产品开发工作。\n*   **被愿景打动：** 在与Airbnb首席技术官（CTO）Ari Balogh非正式会面后，她被Ari改造工程和技术组织的愿景深深打动，并认同其专注于工程技艺的理念。\n*   **身兼二职：** 鉴于她兼具工程和安全领域的专业知识，Airbnb也正在寻找信任与安全组织的工程负责人，她最终同时承担了这两个角色，这让她能够回归工程团队领导的本源，同时利用她的安全经验担任CSO。\n*   **加入Airbnb的感受：** 她于2019年加入Airbnb，对公司致力于提供积极用户体验的奉献精神印象深刻。\n\n![图片 2](https://cdn-images-1.medium.com/max/998/1*7dLlEcMROo22Bn4C244qbw.jpeg)\n\n## 选择Airbnb的理由\n*   **巨大影响力：** Airbnb近6000名员工服务全球数百万用户，提供了产生巨大影响力的机会。\n*   **使命驱动：** 公司创始人通过Airbnb.org等倡议致力于行善的激情和承诺，以及其使命驱动的方法，与她产生了强烈共鸣。\n*   **人才与价值观：** 她持续被这里才华横溢、富有爱心的人们所打动，他们被Airbnb的愿景所团结。她认为，很少有公司能如此有效地将卓越技术与社会责任感结合起来。\n\n## 信任与安全团队与安全团队：共同使命，不同侧重\n*   **共同使命：** 两个团队都致力于保护用户和平台。\n*   **信任与安全团队：** 专注于为房客和房东提供安心体验，通过技术降低安全和隐私风险。例如，创新的预订筛选技术旨在识别高风险预订，从而减少全球范围内破坏性派对的风险。\n*   **网络安全团队：** 专注于保护Airbnb的资产、数据、员工和基础设施，实施强大的安全控制和威胁检测能力。\n\n## 业余爱好带来的领导力启示\n*   **即兴喜剧：** 她尝试了即兴喜剧，这项爱好意外地为她带来了宝贵的领导力经验。\n*   **即兴思维与领导力：** 即兴表演的本质要求人们在当下快速思考，对新问题和情景做出反应，这训练了实时处理信息并形成连贯、引人注目的回应的关键技能，对领导力至关重要。\n\n## 职业建议：保持冷静\n*   **面对挫折：** 职业生涯中不可避免地会遇到挫折、失望和烦恼。\n*   **关键在于应对：** 重要的是如何应对这些短期障碍，而不是过度纠结。\n*   **保持专注与坚持：** 保持专注，一步一个脚印，坚定不移地向前推进。她从大家庭中长女的早期领导角色中学到了这些经验。她的建议是保持冷静，保持视角，坚定信念地前进。",
      "shortSummary": "Vijaya Kaza分享了她成为Airbnb首席安全官和信任与安全工程主管的职业历程。她从电气工程起步，偶然进入网络安全领域，先后在思科、FireEye和Lookout积累经验。被Airbnb的愿景和巨大影响力吸引，她于2019年加入，领导保护用户和平台安全的团队。她强调了适应性、持续学习以及从即兴喜剧中获得的领导力洞察，并建议在职业生涯中保持冷静和专注。",
      "translated_title": "我的Airbnb之旅 | Vijaya Kaza",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*FfdFpfjGZODh7ltZ2W2Mrg.jpeg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/998/1*7dLlEcMROo22Bn4C244qbw.jpeg",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8f06543b38d5",
          "alt": "",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*FfdFpfjGZODh7ltZ2W2Mrg.jpeg\" /></figure><p><a href=\"https://www.linkedin.com/in/vkaza/\"><em>Vijaya Kaza</em></a><em> is the Chief Security Officer and Head of Engineering for Trust and Safety at Airbnb. She leads teams responsible for developing the technology (Platforms, tools and AI models), to safeguard the Airbnb community, as well as for securing Airbnb’s infrastructure and information assets. She is also the executive co-sponsor of Airbnb Tech’s Diversity Council.</em></p><p><em>Here’s Vijaya’s story of how she got to Airbnb, in her own words.</em></p><p><strong>Straight shot to science and engineering</strong></p><p>I grew up in a modest, multi-generational family in India with 30 to 40 family members under one roof on any given day. As the oldest child in that house, I was expected to excel academically and set an example for the other children to follow.</p><p>In our culture back then, being “good at school” was synonymous with shining in science and math. As luck would have it, I had a strong affinity for those subjects and enjoyed studying and diving deep into them. I followed a natural path that combined math and science, studying engineering in college, and got a bachelor’s and two master’s in electrical engineering. This foundation paved the way for my future work in technology.</p><p><strong>Stumbling into cybersecurity</strong></p><p>After college, I landed my first job as a Software Engineer at Cisco. However, my entry into the security field was accidental. I simply followed a manager I liked who was moving into a new security business unit, fell in love with Security and never looked back after that! There was no grand plan or calculated career strategy.</p><p>After 17 years at Cisco leading product development for a $1B security product portfolio, I headed to FireEye, another well-known name in the Cybersecurity space. There I had the responsibility for helping the company transition from on-prem to a cloud/SaaS business model, and growing the revenue of their cloud security portfolio. That role gave me the experience of working on different areas of security, as well as leading both Product Management and Engineering in a General Manager capacity.</p><p>Next, I led Product Development at Lookout, a startup in San Francisco focused on mobile security. At the time, it was a consumer security company pivoting to building for enterprise customers. I didn’t know it then, but that glimpse of consumer security was a great primer for my eventual role at Airbnb.</p><p>Cybersecurity is an ever-changing domain with constant innovation and I’ve thoroughly enjoyed working and learning in such a dynamic space. Each major technological transformation — from cloud to mobile to AI — brings novel security challenges to solve for businesses and end users.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/998/1*7dLlEcMROo22Bn4C244qbw.jpeg\" /></figure><p><strong>An unexpected opportunity at Airbnb</strong></p><p>When I was initially approached for the Chief Security Officer (CSO) role at Airbnb, I was taken aback. I had always worked in the field doing engineering and product development work, so I wasn’t sure about this role. But after meeting Ari Balogh, Airbnb’s Chief Technology Officer (CTO) for an informal coffee chat, we really hit it off and I was thoroughly impressed by his vision to transform the engineering and technology organization within the company. Ari shared his philosophy of focusing on the <a href=\"https://medium.com/airbnb-engineering/commitment-to-craft-e36d5a8efe2a\">craft of engineering</a> and that really resonated with me. The prospect of molding Airbnb’s engineering culture was very enticing.</p><p>Turns out Airbnb was also looking for someone to lead engineering for its Trust and Safety organization. Given my background with a blend of engineering and security domain expertise, I ended up taking on both of these roles. This unique opportunity allowed me to go back to my roots leading engineering teams while taking advantage of my security experience in the capacity of a CSO.</p><p>When I joined the company in 2019, I was struck by Airbnb’s dedication and effort to deliver a positive user experience. Our attention and design focus that go into helping guests and hosts have a seamless experience is unlike anything I’ve experienced before.</p><p><strong>Why Airbnb?</strong></p><p>I joined Airbnb because of the opportunity to have an outsized impact — Airbnb’s almost 6,000 employees serve millions of people worldwide. Our mission-driven approach, powered by our founders’ unmatched passion and commitment to doing good through initiatives like Airbnb.org really resonated with me. I’ve been continuously impressed by the caliber of talented, caring people here who are united by Airbnb’s vision. It’s rare to find a company that so effectively combines technical excellence with social consciousness.</p><p><strong>Two teams, one mission</strong></p><p>While both the Trust and Safety and Security teams share the common mission of safeguarding users and the platform, the actual techniques, threats, and focus areas are completely different. Trust is at the core of our business and we’re deeply focused on providing our guests and hosts peace of mind as they live, work, travel and host on Airbnb. We build technology to help lower safety and privacy risks for our community.</p><p>For example, our innovative reservation screening technology aims to help reduce the risk of disruptive parties on Airbnb globally by taking steps to identify higher-risk reservations and potentially prevent these bookings from being made.</p><p>On the cybersecurity front, we focus on securing Airbnb’s assets, our data, our employees, and our infrastructure. We implement robust security controls and threat detection capabilities to safeguard Airbnb’s internal resources.</p><p><strong>Embracing the improv mindset</strong></p><p>Outside of work, I’ve pursued some hobbies that have surprisingly imparted invaluable leadership lessons. A few years ago, I decided to try improv comedy, something that’s entirely outside my wheelhouse. I had never done anything close to theater or acting in my entire life. What started as a fun experiment quickly became a passion project and I progressed through the levels and eventually performed live in front of an audience with friends and family in attendance.</p><p>It may seem unrelated at first, but the very nature of improv is profoundly relevant for leadership. You’re constantly put on the spot and need to think on your feet, responding to new questions and scenarios in the moment. Improv trains this vital skill of processing information in real time and formulating a coherent, compelling reaction.</p><p><strong>Keep a steady head</strong></p><p>Over any career, including mine, there are inevitable professional setbacks, disappointments, and annoyances along the way. The key is to not make much of these short-term hurdles; it’s how you respond that matters most. The less you agonize over the bumps in the road, the better. Maintain your focus, keep one foot in front of the other, and persist forward undeterred. I learned these lessons early, having taken on leadership roles from a young age as the eldest child in a large household. My advice is to keep a steady head, maintain perspective, and plow forward with conviction.</p><p>We’re currently expanding the Airbnb team and hiring for several roles. Check out our open positions <a href=\"https://careers.airbnb.com/\">here</a>.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8f06543b38d5\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/my-journey-to-airbnb-vijaya-kaza-8f06543b38d5\">My Journey To Airbnb | Vijaya Kaza</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "从数据到洞察：细分爱彼迎的房源供应 (原标题: From Data to Insights: Segmenting Airbnb’s Supply)",
      "link": "https://medium.com/airbnb-engineering/from-data-to-insights-segmenting-airbnbs-supply-c88aa2bb9399?source=rss----53c7c27702d5---4",
      "pubDate": "Mon, 25 Nov 2024 18:02:22 GMT",
      "isoDate": "2024-11-25T18:02:22.000Z",
      "creator": "Alexandre Salama",
      "summary": "爱彼迎（Airbnb）通过数据驱动的细分方法，深入理解其房源供应的可用性模式。与传统酒店不同，爱彼迎的房源由具有不同收入目标和日程限制的房东提供，导致房源可用性差异巨大。理解这些差异对于产品开发、营销活动和运营至关重要。\n\n## 引言\n\n爱彼迎的房源供应具有非标准化和可用性多变的特点。过去，单独使用“可用性率”等单一特征无法全面反映房东的真实可用性模式。例如，30%的可用性可能代表周末房东或季节性房东。为了解决这一问题，爱彼迎引入了细分方法，通过结合多个特征来创建离散类别，从而揭示房东不同的可用性模式。\n\n传统的细分方法（如RFM）侧重于客户价值，且通常限于小数据集的一次性分析，不适用于处理日历数据和每日对数百万房源进行推断。本文探讨了爱彼迎如何通过丰富可用性数据、引入新颖特征并应用机器学习技术，开发出一种实用且可扩展的方法，每日对数百万房源的可用性进行细分。\n\n**案例：区分具有相似资料的房东**\n\n以Alice和Max为例，他们都有相同的两居室公寓。Alice只在夏季出租，而Max全年可用。这反映了两种截然不同的出租风格，需要爱彼迎提供差异化的支持：\n\n*   **Alice（季节性房东）**：可能大部分时间自住，只在夏季出租。爱彼迎可提供季节性定价建议、针对偶尔房东的入门指南和设置建议。\n*   **Max（全年房东）**：可能将出租作为主要收入来源。爱彼迎可提供高级预订分析、多预订管理工具以及收入和税务指导。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/1*Lqx5Zg187zZY1UewaG7Q7Q.jpeg)\n\n![图片 2](https://cdn-images-1.medium.com/max/1024/1*3OwRiPtxF8IMHd0ESTAemg.png)\n\n## 数据集构建\n\n为了捕捉房东行为的这些关键差异，爱彼迎构建了一个包含以下特征的数据集：\n\n### 1. 可用性率（Availability Rate）\n\n*   **定义**：房东在特定夜晚“有意可用”的程度。本文主要关注回顾性可用性（已发生）。\n*   **计算**：\n    *   **空置之夜（Nights Vacant）**：房源被列为可用但未被预订的夜晚。\n    *   **已预订之夜（Nights Booked）**：房源被列为可用且已被预订的夜晚。\n    *   **总可用之夜（Nights Available）**：在365天回顾期内，空置之夜与已预订之夜的总和。\n    *   **可用性率** = 总可用之夜 / 365。\n\n![图片 3](https://cdn-images-1.medium.com/max/1024/1*tC4E7RzzTyX50VPOjfMoIg.png)\n\n从分布中观察到，大量房源的可用性接近0%或100%，而中间部分没有明显的断点，需要进一步区分。\n\n### 2. 连续性（Streakiness）\n\n对于可用性率处于中间范围的房源，仅凭可用性率不足以捕捉其日历模式的细微差别。例如，两个房源可用性率均为50%，但一个集中可用，一个分散可用。\n\n![图片 4](https://cdn-images-1.medium.com/max/1024/1*y1TNNHhfZPlc_R9y8zdBhg.png)\n\n为了捕捉这种区别，引入了“连续性”特征。一个“连续可用序列”被定义为至少2晚的连续可用，随后是至少2晚的连续不可用。\n\n![图片 5](https://cdn-images-1.medium.com/max/1024/1*lOXpLC6TyG_HEhwjg4hEfA.png)\n\n**连续性特征** = 连续可用序列数 / 总可用之夜。至此，爱彼迎拥有了两个相对正交的特征：可用性率和连续性。\n\n![图片 6](https://cdn-images-1.medium.com/max/1024/1*Qr4Z7Kl8FLuw4YMzCqYf3A.png)\n\n### 3. 季节性（Seasonality）\n\n可用性率和连续性无法捕捉日历的“紧凑性”或季节性。例如，两个房源可用性率和连续性相似，但一个集中在夏季可用，另一个则分散在多个季度。\n\n![图片 7](https://cdn-images-1.medium.com/max/1024/1*s0eOYP6Nzlmb0fNJLwoNSA.png)\n\n鉴于季节性在爱彼迎业务中的关键作用，引入了以下特征：\n\n*   **至少一晚可用的季度数（Quarters with at Least One Night of Availability）**\n*   **最长连续可用月份数（Maximum Consecutive Months）**\n\n这些特征共同提供了对季节性模式更清晰的洞察。\n\n### 最终数据集\n\n最终的特征集包括所有在平台上列出的房源，计算上述设计好的特征。随后，从这些日期中抽取一个大的随机样本，并对数值特征进行缩放以确保可比性。\n\n![图片 8](https://cdn-images-1.medium.com/max/1024/1*3s1H1osFkh-GNQpHgBB5wA.png)\n\n## 细分模型\n\n爱彼迎应用K-means聚类算法来识别细分，测试K值从2到10。通过“肘部法则”确定最佳聚类数量为8个。\n\n聚类命名过程包括：\n\n1.  检查每个特征在不同聚类中的分布，识别显著差异。\n2.  从每个聚类中随机抽样房源并可视化其日历。\n3.  与跨职能内部工作组迭代命名。\n\n![图片 9](https://cdn-images-1.medium.com/max/1024/1*yayTaaAXJaANqK3ORFvl9A.png)\n\n![图片 10](https://cdn-images-1.medium.com/max/1024/1*gudh_qoE2suaIibj7O2K9Q.png)\n\n## 细分验证\n\n由于房东行为模式是潜在属性，没有“真实标签”，爱彼迎通过多种方法确保细分在业务上“合理”并可靠反映真实房东行为：\n\n### 1. A/B测试\n\n评估不同细分如何使用鼓励房东完成“推荐操作”（如允许客人最后一刻预订）以获得奖励的功能。\n\n![图片 11](https://cdn-images-1.medium.com/max/1024/1*hwUsT0koNgogJ_gEsAIppw.png)\n\n结果与直觉一致：特定场合或很少使用的房东可能对激励不感兴趣；“Always On”房东已高度参与，倾向自主策略；中等参与度的房东是激励的理想目标。\n\n![图片 12](https://cdn-images-1.medium.com/max/982/1*NuJpZ4btpXn23btHI8M2RQ.png)\n\n### 2. 可用性细分的相关性\n\n验证聚类与已知属性的相关性，例如“Always On”房源多由专业人士管理，“Short Seasonal”房源多见于滑雪或海滩目的地。此外，“Event Motivated”房源在大型活动期间数量会增加。\n\n![图片 13](https://cdn-images-1.medium.com/max/1024/1*QHD7knLqv1-BgqwEkp_6cQ.png)\n\n### 3. 用户体验（UX）研究\n\n将细分结果与UX研究团队通过房东调查创建的定性用户画像进行比较，确保与真实世界行为一致。\n\n## 规模化与生产化\n\n为了将细分方法推广到所有房源，爱彼迎使用决策树算法。该模型以4个特征为输入，K-means聚类标签为输出，提供了一组简单、可解释的if-else规则来分类房源。通过将决策树逻辑转换为SQL查询（CASE WHEN语句），模型被集成到爱彼迎的数据仓库中，实现每日的房源细分。\n\n![图片 14](https://cdn-images-1.medium.com/max/1024/1*vl5fzKGg253W85-P5pqTfA.png)\n\n## 爱彼迎及其他领域的应用\n\n这些细分在爱彼迎内部被广泛应用：\n\n*   **产品团队**：指导策略，分析A/B测试中的异质处理效应。\n*   **营销团队**：进行精准消息推送。\n*   **UX研究团队**：深入洞察房东动机。\n\n例如，通过细分发现“Event Motivated”房东在即时预订采纳方面有提升空间，通过增加“只接受特定评分客人”的选项，可以提高他们的采纳意愿。\n\n此外，该细分方法论已扩展到房东活动数据，用于区分“偶尔设置调整者”和“频繁设置优化者”。这种方法也可推广到其他需要理解时间性参与的行业，如社交媒体（潜水者 vs. 活跃内容创作者）、网约车（高峰期兼职司机 vs. 全职司机）、流媒体服务（夜间观看者 vs. 持续观看者）和电商（促销/假日购物者 vs. 全年购物者）。",
      "shortSummary": "爱彼迎利用数据驱动的细分方法，通过结合可用性率、连续性和季节性等特征，并应用K-means聚类和决策树算法，将数百万房源细分为8种不同的可用性模式。这些细分通过A/B测试、相关性分析和UX研究进行验证，确保其业务合理性。该方法论已在爱彼迎内部广泛应用于产品、营销和UX研究，并可推广至社交媒体、网约车等其他行业，以理解用户的时间性参与行为。",
      "translated_title": "从数据到洞察：细分爱彼迎的房源供应",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*Lqx5Zg187zZY1UewaG7Q7Q.jpeg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*3OwRiPtxF8IMHd0ESTAemg.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*tC4E7RzzTyX50VPOjfMoIg.png",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*y1TNNHhfZPlc_R9y8zdBhg.png",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*lOXpLC6TyG_HEhwjg4hEfA.png",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<h4>How Airbnb uses data-driven segmentation to understand supply availability patterns.</h4><p><strong>By:</strong> <a href=\"https://www.linkedin.com/in/alexandre-salama\">Alexandre Salama</a>, <a href=\"https://www.linkedin.com/in/timabraham\">Tim Abraham</a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Lqx5Zg187zZY1UewaG7Q7Q.jpeg\" /></figure><h3>Introduction</h3><p>At Airbnb, our supply comes from hosts who decide to list their spaces on our platform. Unlike traditional hotels, these spaces are not all interchangeable units in a building that are available to book year-round. Our hosts are people, with different earnings objectives and schedule constraints — leading to different levels of availability to host. Understanding these differences is a key input into how we develop our products, campaigns, and operations.</p><p>Over the years, we’ve created various ways to measure host availability, developing “features” that capture different aspects of how and when listings are available. However, these features provide an incomplete picture when viewed in isolation. For example, a ~30% availability rate could indicate two very different scenarios: a host who only accepts bookings on weekends, or a host whose listing is only available during a specific season, such as summer.</p><p>This is where segmentation comes in.</p><blockquote>By combining multiple features, segmentation allows us to create discrete categories that represent the different availability patterns of hosts.</blockquote><p>But traditional segmentation methodologies, such as “<a href=\"https://en.wikipedia.org/wiki/RFM_(market_research)\">RFM</a>” (Recency, Frequency, Monetary), are focused on customer value rather than calendar dynamics, and are often limited to one-off analyses on small datasets. In contrast, we need an approach that can handle calendar data and daily inference for millions of listings.</p><p>To address the above challenges, this blog post explores how Airbnb used segmentation to better understand host behavior at scale. <strong>By enriching availability data with novel features and applying machine learning techniques, we developed a practical and scalable approach to segment availability for millions of listings daily.</strong></p><h3>Example: Distinguishing Hosts with Similar Profiles</h3><p>Consider Alice and Max, two hosts with identical 2-bedroom apartments on Airbnb. However, Alice only lists her property in the summer, while Max has it available year-round — reflecting two distinct hosting styles.</p><p>Alice’s seasonal availability suggests that she might live in the property most of the time, only renting it out during the summer months. Airbnb can support her with seasonal pricing tips, onboarding guides for occasional hosts, and settings suggestions.</p><p>Conversely, Max’s full-time availability indicates a more professional hosting style, possibly his primary income source. Airbnb can provide him with advanced booking analytics, tools for managing multiple reservations, and guidance on earnings and tax implications.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*3OwRiPtxF8IMHd0ESTAemg.png\" /><figcaption>Two Hosts with Similar Profiles (Illustrative)</figcaption></figure><p><strong>How can we create a dataset that captures these crucial differences in hosting behavior?</strong></p><h3>Dataset</h3><h4>Availability Rate</h4><p>A first step is to capture the host’s “intention to be available” on a specific night. Availability can be both analyzed from a backward-looking (in the past) or forward-looking (in the future) perspective. For simplicity, this post focuses on backward-looking availability, as it reflects the final state of a calendar after all changes in inventory, bookings and cancellations have occurred. Forward-looking availability is not as straightforward because changes can still happen between the analysis date and the future dates being analyzed.</p><p>We consider both:</p><ul><li><strong>Nights Vacant:</strong> nights when the listing was listed as available for booking on Airbnb, and remained vacant.</li><li><strong>Nights Booked: </strong>nights when the listing was listed as available for booking on Airbnb, and was later booked on Airbnb.</li></ul><p>Consequently, we can calculate the corresponding Nights Intended to be Available, or Nights Available, for the 365-day look-back period as the sum of Nights Vacant and Nights Booked. We then divide it by 365, to obtain the corresponding <strong>Availability Rate</strong>.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*tC4E7RzzTyX50VPOjfMoIg.png\" /><figcaption>Distribution of Listings by Availability Rate in the Previous Year (Illustrative)</figcaption></figure><p>From this distribution we observe:</p><ul><li>A considerable proportion of listings has little-to-no availability (~0% availability rate).</li><li>Conversely, a significant proportion of listings has near full availability (~100% availability rate).</li><li>Between these extremes, a significant set of listings emerges without strong breakpoints.</li></ul><p>How can we further differentiate these listings that fall in the middle range?</p><h4>Streakiness</h4><p>For listings that are not at either end of the spectrum, availability rate on its own is insufficient for capturing the nuances of how a listing is made available throughout the month. Consider listings A and B, which both have a 50% availability rate in a given month.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*y1TNNHhfZPlc_R9y8zdBhg.png\" /><figcaption>Two Listings with Similar Availability Rates but Distinct Calendar Patterns</figcaption></figure><p>Although these listings have distinct availability patterns, they both have the same availability rate (50%)!</p><p>Listing A’s concentrated, block-like availability could lend itself to recommendations for weekly stay discounts, or advice for hosts who are away for a longer stretch — guidance which may not be suitable for Listing B.</p><p>To capture this distinction, we introduce “Streakiness”. In the example above, Listing A had 1 long streak of availability which was interrupted on night 16, while Listing B had 8 short streaks of availability, each lasting 2 nights before a 2-night break.</p><p>We define a streak as a consecutive sequence of availability with a minimum of 2 consecutive nights, followed by a subsequent period of at least 2 consecutive nights of unavailability, as described in the diagram below. Note that we initially considered using a single night of availability/unavailability as a threshold but found it to be a less reliable signal of the consistency that streakiness aims to measure.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*lOXpLC6TyG_HEhwjg4hEfA.png\" /><figcaption>Streak Definition</figcaption></figure><p>This leads us to the corresponding <strong>Streakiness</strong> feature, computed as the ratio of Streaks divided by the number of Nights Available (computed in the previous section). At this point, we now have two relatively orthogonal features for our analysis: availability rate and streakiness.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Qr4Z7Kl8FLuw4YMzCqYf3A.png\" /><figcaption>Combining Availability + Streakiness</figcaption></figure><h4>Seasonality</h4><p>We found that while availability and streakiness provide a solid basis for measuring volume and consistency, they don’t capture a calendar’s “compactness” — in other words, its <strong>seasonality</strong>. As an example, consider Listings C and D, which both have around 15% availability and 14 streaks:</p><ul><li>Listing C concentrates its availability within a narrower block of time (summer season) — see first calendar below.</li><li>Listing D distributes its availability more evenly across multiple quarters — see second calendar below.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*s0eOYP6Nzlmb0fNJLwoNSA.png\" /><figcaption>Two Listings with Similar Availability Rates / Streakiness but Distinct Calendar Patterns</figcaption></figure><p>Seasonality plays a crucial role in Airbnb’s business, as guest demand and host availability fluctuate with changes in seasonal appeal, holidays, and local events. Given this, we propose to create a <strong>Quarters with at Least One Night of Availability</strong> feature.</p><p>Additionally, we create a <strong>Maximum Consecutive Months</strong> feature which captures streakiness at a yearly scale, highlighting the longest continuous period a listing is available. Together, these features give clearer insight into seasonal patterns.</p><h4>Final dataset</h4><p>The final feature set includes all listings that were listed on the platform as of a broad set of dates. For each listing, we calculate the features we’ve designed in the previous sections. Then, we take a large, random sample across these dates. Finally, we scale the numerical features to ensure they are on a comparable scale.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*3s1H1osFkh-GNQpHgBB5wA.png\" /><figcaption>Sample Listings Depicting our Feature Set</figcaption></figure><h3>Segmentation Model</h3><p>We can now apply a <a href=\"https://en.wikipedia.org/wiki/K-means_clustering\"><strong>K-means clustering algorithm</strong></a> to identify segments, testing models with K values from 2 to 10. Using the <a href=\"https://en.wikipedia.org/wiki/Elbow_method_(clustering)\">elbow plot</a> to find the optimal number of clusters, we select 8 clusters as the best representation of our data.</p><p>We now have our clusters, but they don’t have names yet. Our cluster naming process involves several steps:</p><ul><li>Checking the distribution of each feature by cluster to identify strong differences (e.g., “cluster 1 has the highest availability rate”)</li><li>Randomly sampling listings from each cluster and visualizing their calendars</li><li>Iterating on naming with a cross-functional internal working group</li></ul><p>The output of this process is summarized in the table below, while the following diagram displays a “typical” calendar for each cluster.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*yayTaaAXJaANqK3ORFvl9A.png\" /><figcaption>From Cluster Intuition to Cluster Name</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*gudh_qoE2suaIibj7O2K9Q.png\" /><figcaption>Examples of Calendars by Cluster</figcaption></figure><h3>Segment Validation</h3><p><strong>Since we are measuring a latent attribute </strong>— underlying host behavior patterns that don’t have “ground truth” labels — <strong>there is no perfectly accurate way to validate our segmentation</strong>. However, we can use various methodologies to ensure that it “makes sense” from a business perspective, and reliably reflects real-life host behaviors.</p><p>We do so in three steps:</p><ul><li>A/B Testing</li><li>Correlates of Availability Segments</li><li>User Experience (UX) Research</li></ul><h4>A/B Testing</h4><p>In an A/B test, we assessed how the different segments previously used a feature that encouraged hosts to complete “recommended actions” (e.g., letting guests book their home last-minute) so they may earn a monetary incentive.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*hwUsT0koNgogJ_gEsAIppw.png\" /><figcaption>Example of Host-Facing Recommended Actions</figcaption></figure><p>We show the use of the feature by each segment below. These results align with our intuition: hosts who use Airbnb for specific occasions or rarely may not be interested in following recommendations, even when incentivized. Similarly, “Always On” hosts, who are already highly engaged and proactive in managing their listings, might prefer to rely on their own strategies rather than follow Airbnb’s suggestions. Hosts who fall somewhere in between, with moderate levels of engagement, may be the ideal target for incentives, as they are likely open to adjustments that could boost their performance.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/982/1*NuJpZ4btpXn23btHI8M2RQ.png\" /><figcaption>Example of Heterogeneous Treatment Effects by Availability Segment<br>(“CI” = Confidence Interval)</figcaption></figure><h4>Correlates of Availability Segments</h4><p>We also validate our clusters by checking correlations with known attributes. For instance, we confirm that “Always On” listings are likely more managed by professionals, or that “Short Seasonal” listings are likely more common in ski or beach destinations.</p><p>Furthermore, we know it is common to observe an increase in the number of listings around big events. As expected, we observe a rise in “Event Motivated” listings leading up to and during major events periods, reflecting hosts’ responsiveness to increased demand.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*QHD7knLqv1-BgqwEkp_6cQ.png\" /><figcaption>Impact of an Event on the % of Event-Motivated Listings (Illustrative)</figcaption></figure><h4>UX Research</h4><p>Finally, we know the UX Research team conducts host surveys to create qualitative personas, which we compare against our clusters to ensure they align with real-world behavior. For instance, we verify if segments with high weekend availability match hosts who self-report preferring weekend rentals.</p><h3>Scaling and Productionization</h3><p>Now, we need to scale this segmentation to all our listings.</p><p>To achieve this, we use a<strong> </strong><a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning\"><strong>decision tree algorithm</strong></a>. We train a model using our 4 features, with cluster labels from our K-means model as outputs. We also perform a train-test split to make sure the model accurately predicts each cluster.</p><p>This new model provides a simple, interpretable set of if-else rules to classify listings into clusters. <strong>Using the decision tree structure, we translate the model’s logic into a SQL query by converting the decision tree’s “IF” conditions into “CASE WHEN” statements</strong>. This integration enables the model to be propagated in our data warehouse.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*vl5fzKGg253W85-P5pqTfA.png\" /><figcaption>Decision Tree Structure</figcaption></figure><h3>Applications at Airbnb and Beyond</h3><p>At Airbnb, various teams leverage these segments: product teams to inform strategy and analyze heterogeneous treatment effects in A/B tests, marketing teams for targeted messaging, and UX research teams for insights into hosts’ motivations.</p><p>For instance, we revealed an opportunity to boost <a href=\"https://www.airbnb.com/help/article/523\">Instant Book</a> adoption among “Event Motivated” hosts, who may occasionally list their primary residence and prefer manual guest screening. Adding an option for hosts to only accept guests with a certain rating may make Instant Book more appealing to them, offering a balance between host control and booking efficiency.</p><p>Initially designed for listing availability data, this segmentation methodology has also been adapted to host activity data. We developed a second segmentation focused on days with “host engagement” (e.g., adjusting prices, updating policies, revising listing descriptions) to differentiate occasional “Settings Tinkerers” from frequent “Settings Optimizers.”</p><p><strong>This approach can also be adapted to other industries where understanding temporal engagement is essential</strong>, for instance, to distinguish:</p><ul><li>Social Media: casual lurkers vs. active content creators</li><li>Ridesharing: occasional drivers during peak demand vs. full-time drivers</li><li>Streaming Services: nighttime streamers vs. continuous streamers</li><li>E-commerce: sales/holidays enthusiasts vs. year-round shoppers</li></ul><h3>Acknowledgments</h3><p>This blog post was a collaborative effort, with significant contributions from Tim Abraham, the main co-author. We’d also like to acknowledge the invaluable support of team members from multiple organizations, including (but not limited to) Regina Wu, Maggie Jarley, and Peter Coles.</p><p><em>Does this type of work interest you? We’re </em><a href=\"https://careers.airbnb.com/\"><em>hiring</em></a><em>!</em></p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c88aa2bb9399\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/from-data-to-insights-segmenting-airbnbs-supply-c88aa2bb9399\">From Data to Insights: Segmenting Airbnb’s Supply</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "在 Airbnb 构建用户信号平台 (原标题: Building a User Signals Platform at Airbnb)",
      "link": "https://medium.com/airbnb-engineering/building-a-user-signals-platform-at-airbnb-b236078ec82b?source=rss----53c7c27702d5---4",
      "pubDate": "Wed, 20 Nov 2024 19:27:27 GMT",
      "isoDate": "2024-11-20T19:27:27.000Z",
      "creator": "Kidai Kwon",
      "summary": "Airbnb 团队构建了一个大规模、近实时流处理平台——用户信号平台（User Signals Platform, USP），旨在捕捉和理解用户行为，从而提供更个性化的产品体验。该平台使多个团队能够轻松利用实时用户活动数据，并解决了大规模流处理操作中的挑战。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/0*ZDusO7LglpaC7sF7)\n\n### 背景与目标\n\nAirbnb 连接着全球数百万房客与独特的房源和体验。在预订流程的各个阶段（如浏览、规划、收藏、比较和预订），提供个性化体验至关重要。这需要强大的基础设施来处理海量用户互动数据并近实时地提供洞察。同时，平台化基础设施也十分重要，以便不熟悉流处理的工程团队也能贡献用户洞察。\n\nUSP 的设计目标包括：\n\n*   存储用户在网站上的实时和历史互动数据。\n*   支持在线用例和离线数据分析的查询。\n*   支持实时数据在线服务用例，端到端流处理延迟低于 1 秒。\n*   支持异步计算以获取用户理解数据，如用户细分和会话参与度。\n*   允许各团队轻松定义捕捉用户活动的管道。\n\n### USP 系统架构\n\nUSP 包含数据管道层和在线服务层。数据管道层基于 Lambda 架构，其中在线流处理组件近实时处理 Kafka 事件，离线组件用于数据校正和回填。在线服务层通过查询数据管道层写入的键值（KV）存储来执行读取操作。\n\n![图片 2](https://cdn-images-1.medium.com/max/1008/0*1zHh1jsXPpJ6MTKm)\n\n**关键设计选择：**\n\n*   **选择 Flink 而非 Spark Streaming**：Flink 基于事件流处理，而 Spark Streaming 基于微批处理，Flink 在处理事件延迟方面表现更优。\n*   **KV 存储的追加写入模式**：数据以追加方式存储在 KV 存储中，事件处理时间戳作为版本，即使事件被多次处理也能保证幂等性，大大降低了复杂性。\n*   **基于配置的开发工作流**：通过配置生成作业模板，并允许开发人员定义 Flink 和批处理作业共享的转换逻辑，使 USP 对不熟悉 Flink 的团队更友好。\n\n### USP 功能\n\nUSP 支持多种用户事件处理类型，其详细流程如下：\n\n![图片 3](https://cdn-images-1.medium.com/max/1024/0*MgmfxGTHkspd_Npc)\n\n1.  **用户信号 (User Signals)**：\n    *   对应于可按信号类型、开始时间和结束时间查询的近期用户活动列表（如搜索、房源浏览、预订）。\n    *   开发人员通过定义配置和转换类来创建新的用户信号，并运行脚本自动生成 Flink 配置、回填批处理文件和告警文件。\n    *   支持通过 RocksDB 作为状态存储，近实时地连接多个源 Kafka 事件。\n\n2.  **用户细分 (User Segments)**：\n    *   能够近实时地定义用户群组，具有不同的计算触发标准以及开始和过期条件。\n    *   开发人员只需实现抽象方法中的业务逻辑，无需关注流处理组件。\n    *   例如，“活跃行程规划者”细分：用户搜索后立即加入，14 天不活跃或预订后移除。\n\n3.  **会话参与度 (Session Engagements)**：\n    *   Flink 作业能够对一系列短期用户行为进行分组和分析，以获取特定时间范围内的整体用户行为洞察。\n    *   将转换后的 Kafka 事件按用户 ID 分割成键控流，实现并行计算。\n    *   采用滑动窗口和会话窗口等多种窗口技术，根据窗口内聚合的用户行为触发计算。\n    *   异步计算模式允许执行资源密集型操作（如运行机器学习模型或服务调用），而不会中断实时处理管道，确保计算出的用户理解数据高效存储并可从 KV 存储快速查询。\n\n![图片 4](https://cdn-images-1.medium.com/max/1024/0*r7kzHxNg10NYsMAt)\n\n### Flink 运维经验\n\nUSP 是一个为开发人员构建的流处理平台，以下是运维数百个 Flink 作业的一些经验：\n\n1.  **指标 (Metrics)**：\n    *   **事件延迟 (Event Latency)**：从用户事件生成到转换事件写入 KV 存储的端到端延迟。受客户端网络或日志批处理影响，难以控制。优先使用服务器端事件。\n    *   **摄入延迟 (Ingestion Latency)**：从用户事件到达 Kafka 集群到转换事件写入 KV 存储的延迟。这是主要监控指标，涵盖 Kafka 过载、KV 存储写入延迟等问题。\n    *   **作业延迟 (Job Latency)**：从 Flink 作业开始处理源 Kafka 事件到转换事件写入 KV 存储的延迟。\n    *   **转换延迟 (Transform Latency)**：从 Flink 作业开始处理源 Kafka 事件到 Flink 作业完成转换的延迟。\n\n![图片 5](https://cdn-images-1.medium.com/max/1024/0*10I3qsGy0L0HPL7d)\n\n2.  **通过热备 Task Managers 提高 Flink 作业稳定性**：\n    *   **问题**：Flink 将 Kafka 主题的不同分区分配给不同的 Task Manager。如果一个 Task Manager 失败，分配给它的 Kafka 分区事件将被阻塞，直到新的 Task Manager 启动。\n    *   **解决方案**：预置额外的热备 Pod。当 Task Manager 失败时，热备 Task Manager 会立即开始处理终止 Pod 的任务，而不是等待新 Pod 启动，从而减少停机时间，提高稳定性。\n\n![图片 6](https://cdn-images-1.medium.com/max/1024/0*xhxfJ_KyoOuZ06Dv)\n\n### 结论\n\n在过去几年中，USP 作为平台在赋能众多团队实现产品个性化方面发挥了关键作用。目前，USP 每秒处理超过 100 万个事件，运行 100 多个 Flink 作业，并且 USP 服务每秒处理 7 万次查询。未来，团队将探索不同类型的异步计算模式以进一步提高性能。",
      "shortSummary": "Airbnb 构建了用户信号平台（USP），这是一个大规模、近实时的流处理平台，旨在通过捕捉和理解用户行为来提供个性化产品体验。该平台基于 Flink，利用 Kafka 和 KV 存储处理用户事件，并支持用户信号、用户细分和会话参与度等功能。为提高稳定性，USP 采用了热备 Task Manager 机制。目前，USP 每秒处理超过 100 万个事件，服务 7 万次查询，在 Airbnb 的产品个性化中发挥着关键作用。",
      "translated_title": "在 Airbnb 构建用户信号平台",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*ZDusO7LglpaC7sF7",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1008/0*1zHh1jsXPpJ6MTKm",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*MgmfxGTHkspd_Npc",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*r7kzHxNg10NYsMAt",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*10I3qsGy0L0HPL7d",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<p>How Airbnb built a stream processing platform to power user personalization.</p><p><strong>By:</strong> Kidai Kwon, Pavan Tambay, Xinrui Hua, Soumyadip (Soumo) Banerjee, Phanindra (Phani) Ganti</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*ZDusO7LglpaC7sF7\" /></figure><h3>Overview</h3><p>Understanding user actions is critical for delivering a more personalized product experience. In this blog, we will explore how Airbnb developed a large-scale, near real-time stream processing platform for capturing and understanding user actions, which enables multiple teams to easily leverage real-time user activities. Additionally, we will discuss the challenges encountered and valuable insights gained from operating a large-scale stream processing platform.</p><h3>Background</h3><p>Airbnb connects millions of guests with unique homes and experiences worldwide. To help guests make the best travel decisions, providing personalized experiences throughout the booking process is essential. Guests may move through various stages — browsing destinations, planning trips, wishlisting, comparing listings, and finally booking. At each stage, Airbnb can enhance the guest experience through tailored interactions, both within the app and through notifications.</p><p>This personalization can range from understanding recent user activities, like searches and viewed homes, to segmenting users based on their trip intent and stage. A robust infrastructure is essential for processing extensive user engagement data and delivering insights in near real-time. Additionally, it’s important to platformize the infrastructure so that other teams can contribute to deriving user insights, especially since many engineering teams are not familiar with stream processing.</p><p>Airbnb’s User Signals Platform (USP) is designed to leverage user engagement data to provide personalized product experiences with many goals:</p><ul><li>Ability to store both real-time and historic data about users’ engagement across the site.</li><li>Ability to query data for both online use cases and offline data analyses.</li><li>Ability to support online serving use cases with real-time data, with an end-to-end streaming latency of less than 1 second.</li><li>Ability to support asynchronous computations to derive user understanding data, such as user segments and session engagement.</li><li>Ability to allow various teams to easily define pipelines to capture user activities.</li></ul><h3>USP System Architecture</h3><p>USP consists of a data pipeline layer and an online serving layer. The data pipeline layer is based on the <a href=\"https://en.wikipedia.org/wiki/Lambda_architecture\">Lambda architecture</a> with an online streaming component that processes <a href=\"https://kafka.apache.org/\">Kafka</a> events near real-time and an offline component for data correction and backfill. The online serving layer performs read time operations by querying the <a href=\"https://en.wikipedia.org/wiki/Key%E2%80%93value_database\">Key Value</a> (KV) store, written at the data pipeline layer. At a high-level, the below diagram demonstrates the lifecycle of user events produced by Airbnb applications that are transformed via <a href=\"https://flink.apache.org/\">Flink</a>, stored in the KV store, then served via the service layer:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1008/0*1zHh1jsXPpJ6MTKm\" /><figcaption><strong>Figure 1. USP System Architecture Overview</strong></figcaption></figure><p>Key design choices that were made:</p><ul><li>We chose <a href=\"https://flink.apache.org/\">Flink</a> streaming over <a href=\"https://spark.apache.org/\">Spark</a> streaming because we previously experienced event delays with Spark due to the difference between micro-batch streaming (Spark streaming), which processes data streams as a series of small batch jobs, and event-based streaming (Flink), which processes event by event.</li><li>We decided to store transformed data in an append-only manner in the KV store with the event processing timestamp as a version. This greatly reduces complexity because with at-least once processing, it guarantees idempotency even if the same events are processed multiple times via stream processing or batch processing.</li><li>We used a config based developer workflow to generate job templates and allow developers to define transforms, which are shared between Flink and batch jobs in order to make the USP developer friendly, especially to other teams that are not familiar with Flink operations.</li></ul><h3>USP Capabilities</h3><p>USP supports several types of user event processing based on the above streaming architecture. The diagram below is a detailed view of various user event processing flows within USP. Source Kafka events from user activities are first transformed into User Signals, which are written to the KV store for querying purposes and also emitted as Kafka events. These transform Kafka events are consumed by user understanding jobs (such as User Segments, Session Engagements) to trigger asynchronous computations. The USP service layer handles online query requests by querying the KV store and performing any other query time operations.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*MgmfxGTHkspd_Npc\" /><figcaption><strong>Figure 2. USP Capabilities Flow</strong></figcaption></figure><h4>User Signals</h4><p>User signals correspond to a list of recent user activities that are queryable by signal type, start time, and end time. Searches, home views, and bookings are example signal types. When creating a new User Signal, the developer defines a config that specifies the source Kafka event and the transform class. Below is an example User Signal definition with a config and a user-defined transform class.</p><pre>- name: example_signal<br>  type: simple<br>  signal_class: com.airbnb.usp.api.ExampleSignal<br>  event_sources:<br>  - kafka_topic: example_source_event<br>    transform: com.airbnb.usp.transforms.ExampleSignalTransform</pre><pre>public class ExampleSignalTransform extends AbstractSignalTransform {<br>  @Override<br>  public boolean isValidEvent(ExampleSourceEvent event) {<br>  }<br><br>  @Override<br>  public ExampleSignal transform(ExampleSourceEvent event) {<br>  }<br>}</pre><p>Developers can also specify a join signal, which allows joining multiple source Kafka events with a specified join key near real-time via stateful streaming with RocksDB as a state store.</p><pre>- name: example_join_signal<br>  type: left_join<br>  signal_class: com.airbnb.usp.api.ExampleJoinSignal<br>  transform: com.airbnb.usp.transforms.ExampleJoinSignalTransform<br>  left_event_source:<br>    kafka_topic: example_left_source_event<br>    join_key_field: example_join_key<br>  right_event_source:<br>    kafka_topic: example_right_source_event<br>    join_key_field: example_join_key</pre><p>Once the config and the transform class are defined for a signal, developers run a script to auto-generate Flink configurations, backfill batch files, and alert files like below:</p><pre>$ python3 setup_signal.py --signal example_signal<br><br>Generates:<br><br># Flink configuration related<br>[1] ../flink/signals/flink-jobs.yaml<br>[2] ../flink/signals/example_signal-streaming.conf<br><br># Backfill related files<br>[3] ../batch/example_signal-batch.py<br><br># Alerts related files<br>[4] ../alerts/example_signal-events_written_anomaly.yaml<br>[5] ../alerts/example_signal-overall_latency_high.yaml<br>[6] ../alerts/example_signal-overall_success_rate_low.yaml</pre><h4>User Segments</h4><p>User Segments provide the ability to define user cohorts near real-time with different triggering criteria for compute and various start and expiration conditions. The user-defined transform exposes several abstract methods which developers can simply implement the business logic without having to worry about streaming components.</p><p>For example, the active trip planner is a User Segment that assigns guests into the segment as soon as the guest performs a search and removes the guests from the segment after 14 days of inactivity or once the guest makes a booking. Below are abstract methods that the developer will implement to create the active trip planner User Segment:</p><ul><li><strong>inSegment</strong>: Given the triggered User Signals, check if the given user is in the segment.</li><li><strong>getStartTimestamp</strong>: Define the start time when the given user will be in the segment. For example, when the user starts a search on Airbnb, the start time will be set to the search timestamp and the user will be immediately placed in this user segment.</li><li><strong>getExpirationTimestamp</strong>: Define the end time when the given user will be out of the segment. For example, when the user performs a search, the user will be in the segment for the next 14 days until the next triggering User Signal arrives, then the expiration time will be updated accordingly.</li></ul><pre>public class ExampleSegmentTransform extends AbstractSegmentTransform {<br>  @Override<br>  protected boolean inSegment(List&lt;Signal&gt; inputSignals) {<br>  }<br><br>  @Override<br>  public Instant getStartTimestamp(List&lt;Signal&gt; inputSignals) {<br>  }<br><br>  @Override<br>  public Instant getExpirationTimestamp(List&lt;Signal&gt; inputSignals) {<br>  }<br>}</pre><h4>Session Engagements</h4><p>The session engagement Flink job enables developers to group and analyze a series of short-term user actions, known as session engagements, to gain insights into holistic user behavior within a specific timeframe. For example, understanding the photos of homes the guest viewed in the current session would be useful to derive the guest preference for the upcoming trip.</p><p>As transform Kafka events from User Signals get ingested, the job splits the stream into keyed streams by user id as a key to allow the computation to be performed in parallel.</p><p>The job employs various windowing techniques, such as sliding windows and session windows, to trigger computations based on aggregated user actions within these windows. Sliding windows continuously advance by a specified time interval, while session windows dynamically adjust based on user activity patterns. For example, as a user browses multiple listings on the Airbnb app, a sliding window of size 10 minutes that slides every 5 minutes is used to analyze the user’s short term engagement to generate the user’s short term trip preference.</p><p>The asynchronous compute pattern empowers developers to execute resource intensive operations, such as running ML models or making service calls, without disrupting the real-time processing pipeline. This approach ensures that computed user understanding data is efficiently stored and readily available for rapid querying from the KV store.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*r7kzHxNg10NYsMAt\" /><figcaption><strong>Figure 3. Session Engagements Flow</strong></figcaption></figure><h3>Flink Operations</h3><p>USP is a stream processing platform built for developers. Below are some of the learnings from operating hundreds of Flink jobs.</p><h4>Metrics</h4><p>We use various latency metrics to measure the performance of streaming jobs.</p><ul><li><strong>Event Latency</strong>: From when the user events are generated from applications to when the transformed events are written to the KV store.</li><li><strong>Ingestion Latency</strong>: From when the user events arrive at the Kafka cluster to when the transformed events are written to the KV store.</li><li><strong>Job Latency</strong>: From when the Flink job starts processing source Kafka events to when the transformed events are written to the KV store.</li><li><strong>Transform Latency</strong>: From when the Flink job starts processing source Kafka events to when the Flink job finishes the transformation.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*10I3qsGy0L0HPL7d\" /><figcaption><strong>Figure 4. Flink Job Metrics</strong></figcaption></figure><p>Event Latency is the end-to-end latency measuring when the generated user action becomes queryable. This metric can be difficult to control because if the Flink job relies on client side events, the events themselves may not be readily ingestible due to the slow network on the client device or the batching of the logs on the client device for performance. With these reasons, it’s also preferable to rely on server side events over client side events for the source user events, only if the comparables are available.</p><p>Ingestion Latency is the main metric we monitor. This also covers various issues that can happen in different stages such as overloaded Kafka topics and latency issues when writing to the KV store (from client pool issues, rate limits, service instability).</p><h4>Improving Flink Job stability with standby Task Managers</h4><p>Flink is a distributed system that runs on a single Job Manager that orchestrates tasks in different Task Managers that act as actual workers. When a Flink job is ingesting a Kafka topic, different partitions of the Kafka topic are assigned to different Task Managers. If one Task Manager fails, incoming Kafka events from the partitions assigned to that task manager will be blocked until a new replacement task manager is created. Unlike the online service horizontal scaling where pods can be simply replaced with traffic rebalancing, Flink assigns fixed partitions of input Kafka topics to Task Managers without auto reassignment. This creates large backlogs of events from those Kafka partitions from the failed Task Manager, while other Task Managers are still processing events from other partitions.</p><p>In order to reduce this downtime, we provision extra hot-standby pods. In the diagram below, on the left side, the job is running at a stable state with four Task Managers with one Task Manager (Task Manager 5) as a hot-standby. On the right side, in case of the Task Manager 4 failure, the standby Task Manager 5 immediately starts processing tasks for the terminated pod, instead of waiting for the new pod to spin up. Eventually another standby pod will be created. In this way, we can achieve better stability with a small cost of having standby pods.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*xhxfJ_KyoOuZ06Dv\" /><figcaption><strong>Figure 5. Flink Job Manager And Task Manager Setup</strong></figcaption></figure><h3>Conclusion</h3><p>Over the last several years, USP has played a crucial role as a platform empowering numerous teams to achieve product personalization. Currently, USP processes over 1 million events per second across 100+ Flink jobs and the USP service serves 70k queries per second. For future work, we are looking into different types of asynchronous compute patterns via Flink to improve performance.</p><h3>Acknowledgments</h3><p>USP is a collaborative effort between Airbnb’s Search Infrastructure and Stream Infrastructure, particularly Derrick Chie, Ran Zhang, Yi Li. Big thanks to our former teammates who contributed to this work: Emily Hsia, Youssef Francis, Swaroop Jagadish, Brandon Bevans, Zhi Feng, Wei Sun, Alex Tian, Wei Hou.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b236078ec82b\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/building-a-user-signals-platform-at-airbnb-b236078ec82b\">Building a User Signals Platform at Airbnb</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    }
  ],
  "lastUpdated": "2025-06-17T04:38:19.700Z"
}