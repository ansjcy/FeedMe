{
  "sourceUrl": "https://medium.com/feed/airbnb-engineering",
  "title": "The Airbnb Tech Blog - Medium",
  "description": "Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io - Medium",
  "link": "https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4",
  "items": [
    {
      "title": "Pay As a Local",
      "link": "https://medium.com/airbnb-engineering/pay-as-a-local-bef469b72f32?source=rss----53c7c27702d5---4",
      "pubDate": "Mon, 12 Jan 2026 18:02:56 GMT",
      "isoDate": "2026-01-12T18:02:56.000Z",
      "creator": "Gerum Haile",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Pay As a Local",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*6K6D4WxFwqlwdtBc6uzczw.jpeg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*ajAC0o7YNT-L-TgI2YVPnw.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*RVumwnLlf9S8DcwfjZxQAg.png",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/265/1*xmEalXsiRw5vzv0cMyCe5g.gif",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://cdn-images-1.medium.com/max/864/1*jqRjCcvFGvlIw1T3NT13Wg.gif",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<h3><strong>How Airbnb rolled out 20+ locally relevant payment methods worldwide in just 14 months</strong></h3><p><strong>By: </strong><a href=\"https://www.linkedin.com/in/gerumhaile\"><strong>Gerum Haile</strong></a><strong>, </strong><a href=\"https://www.linkedin.com/in/bo-shi-0321a693\"><strong>Bo Shi,</strong></a><strong> </strong><a href=\"https://www.linkedin.com/in/yujialiu1992\"><strong>Yujia Liu</strong></a><strong>, </strong><a href=\"https://cn.linkedin.com/in/yanwei-bai-ba5bb315b\"><strong>Yanwei Bai</strong></a><strong>, </strong><a href=\"https://www.linkedin.com/in/boyuan-dev/\"><strong>Bo Yuan</strong></a><strong>, </strong><a href=\"https://www.linkedin.com/in/rory-macqueen-28242ba2/\"><strong>Rory MacQueen</strong></a><strong>, </strong><a href=\"https://www.linkedin.com/in/yixiamao\"><strong>Yixia Mao</strong></a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*6K6D4WxFwqlwdtBc6uzczw.jpeg\" /></figure><p>Across the more than 220 global markets that Airbnb operates in, cards are the primary way that guests pay for stays, experiences, and services. However, to help make our platform accessible to more people, reduce friction at checkout, and drive more adoption, we introduced trusted, locally preferred payment methods — called local payment methods or LPMs. By offering and supporting these payment methods, Airbnb enables guests everywhere to choose what works best for them.</p><p>In this blog post, we’ll discuss the implementation details behind our Pay as a Local initiative, which allowed us to launch 20+ local payment methods across multiple markets in just over one year.</p><h3>LPMs: What they are, why they matter, and our discovery and selection process</h3><p>Local payment methods go beyond traditional cards and include:</p><ul><li>Country or region-specific digital wallets (such as M-Pesa or MTN, MoMo)</li><li>Online bank transfers (such as Online Banking Czech, Online Banking Slovakia)</li><li>Real-time or instant bank payments (such as PIX, UPI)</li><li>Local payment schemes (such as EFTPOS, Cartes Bancaires)</li></ul><p>By embracing LPMs, Airbnb helps make travel more inclusive and seamless for people around the world. LPMs help the platform to:</p><ul><li><strong>Boost conversion and bookings</strong> by offering guests familiar, trusted payment options.</li><li><strong>Unlock new markets</strong> where credit card usage is low or non-existent.</li><li><strong>Build accessibility</strong> for guests without credit cards or traditional banking access.</li></ul><p>Through our research on local payment methods (LPMs), we identified over 300 unique payment options worldwide. For the initial phase of the LPM initiative, we used a structured qualification framework to select which local payment methods we would support. We evaluated the top 75 travel markets and selected the top one to two payment methods per market — excluding those without a clear travel use case — and arrived at a shortlist of just over 20 LPMs best suited for integration into our payment platform.</p><h3>Background on Airbnb’s payment platform</h3><p>Airbnb’s payments platform is designed to decouple payment logic from the core business (i.e., stays, experiences, and services), allowing for greater flexibility and scalability. The platform efficiently coordinates both guest pay-ins and host payouts by working with regulated payment service providers and financial partners.</p><p>Beyond payment processing, the system also supports robust payment trust and compliance functions.</p><h3>Modernization</h3><p>As part of a multi-year replatforming initiative for our payments architecture called Payments LTA (long-term architecture), we shifted from a monolithic system to a capability-oriented services system structured by domains, using a domain-driven decomposition approach. This modernization approach reduced our time to market, increased reusability and extensibility, and empowered greater team autonomy.</p><p>The core payment domain delivers essential capabilities for pay-in, payout, and payment intermediation. It consists of multiple subdomains, including Pay-in, Payout, Transaction Fulfillment, Processing, Wallet &amp; instruments, Ledger, Incentives &amp; Stored Value, Issuing, and Settlement &amp; Reconciliation.</p><h3>Replatforming as an enabler for local payment method expansion</h3><p>The processing subdomain enables integration with third-party payment service providers (PSPs) and supports API and file-based vendor integration, as well as switching and routing capabilities. As part of our replatforming initiative, we adopted a connector and plugin-based architecture for onboarding new third-party payment service providers. This strategy has significantly reduced the time required to integrate new PSPs in different markets.</p><p>During this replatforming effort, we also introduced <strong>Multi-Step Transactions (MST)</strong>: a processor-agnostic framework that supports payment flows completed across multiple stages. MST defines a PSP-agnostic transaction language to describe the intermediate steps required in a payment, such as submitting supplemental data or handling dynamic interactions. These steps, called Actions<strong>,</strong> can include:</p><ul><li>Redirects</li><li>Strong customer authentication (SCA) frictions (challenges, fingerprinting)</li><li>Payment method — specific flows</li></ul><p>When a PSP indicates that an additional user action is required, its vendor plugin normalizes the request into an ActionPayload and returns it with a transaction intent status of ACTION_REQUIRED. This architecture ensures consistent handling of complex, multi-step payment experiences across diverse PSPs and markets.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ajAC0o7YNT-L-TgI2YVPnw.png\" /></figure><h3>LPM integration architecture</h3><p>While our modernized payment platform laid the foundation for enabling LPMs, these payment methods come with a unique set of challenges. Many local methods require users to complete transactions in third-party wallet apps. This introduces complexity in app switching, session hand-off, and synchronization between Airbnb and external digital wallets.</p><p>Each local payment vendor also exposes different APIs and behaviors across charge, refund, and settlement flows, making integration and standardization difficult.</p><h3>Technical approach</h3><p>We analyzed the end-to-end behavior of our 20+ LPMs, and identified three foundational payment flows that capture the full spectrum of user and system interactions. By distilling LPM behaviors into these standardized payment flow archetypes, we established a unified framework for integration:</p><ol><li><strong>Redirect flow:</strong> Guests are redirected to a third-party site or app to complete the payment, then return to Airbnb to finalize their booking (e.g., Naver Pay, GoPay, FPX).</li><li><strong>Async flow:</strong> Guests complete payment externally after receiving a prompt (such as a QR code or push notification), and Airbnb receives payment confirmation asynchronously via webhooks (e.g., Pix, MB Way, Blik).</li><li><strong>Direct flow:</strong> Guests enter their payment credentials directly within Airbnb’s interface, allowing real-time processing similar to traditional card payments (e.g., Carte Bancaires, Apple Pay).</li></ol><p>This standardized approach has enabled significant reusability across integrations and substantially reduced the engineering effort required to support new payment methods.</p><h3>Asynchronous payment orchestration</h3><p>Since many guests complete payments through external providers, we redesigned our payment orchestration — building on top of MST — to support payment flows that require user actions outside Airbnb (redirect flows and async flows).</p><p>For redirect flows, where guests complete the payment on a third-party app or website:</p><ul><li>Airbnb’s payments platform sends a charge request to the local payment vendor, whose response includes a redirectUrl.</li><li>Our platform redirects the user to the external app or website to complete the payment.</li><li>Once the payment is successfully completed, the user is redirected back to Airbnb with a result token. Airbnb’s payments platform then uses this token to securely confirm and finalize the payment with the local processor.</li></ul><p>For async flows (which typically involve scanning a QR code):</p><ul><li>Airbnb’s payments platform sends a charge request to the local payment vendor, whose response includes a qrCodeData.</li><li>The checkout page displays the QR code for the user to scan and complete the payment in their wallet app.</li><li>After the payment succeeds, the vendor sends a webhook notification to Airbnb’s payments platform, which updates the payment status to success and confirms the user’s order.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*RVumwnLlf9S8DcwfjZxQAg.png\" /></figure><h4>Naver Pay: Redirect To Naver Pay Website</h4><p>Naver Pay is one of the fastest-growing digital payment methods in South Korea. As of early 2025, it has reached over 30.6 million active users, representing approximately 60% of the South Korean population. Enabling Naver Pay in the South Korean market not only helps deliver a more seamless and familiar payment experience for local guests, but also expands Airbnb’s reach to new users who prefer using Naver Pay as their primary payment method.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/265/1*xmEalXsiRw5vzv0cMyCe5g.gif\" /></figure><h4>Pix: Scan A QR Code</h4><p><a href=\"https://news.airbnb.com/br/airbnb-anuncia-parcelamento-sem-juros-para-pagamento-de-estadias/\">Pix</a> is an instant payment system developed by the Central Bank of Brazil, enabling 24/7 real-time money transfers through methods such as QR codes or Pix keys. Its adoption has been extraordinary — by late 2024, more than 76% of Brazil’s population was using Pix, making it the country’s most popular payment method, surpassing cash, credit, and debit cards. In 2024 alone, Pix processed over BRL 26.4 trillion (approximately USD 4.6 trillion) in transaction volume, underscoring its pivotal role in Brazil’s digital payment ecosystem.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/864/1*jqRjCcvFGvlIw1T3NT13Wg.gif\" /></figure><h3>Config-driven payment method integration</h3><p>Airbnb embraced a config-driven approach, powered by a central YAML-based Payment Method Config that acts as a single source of truth for flows, eligibility, input fields, refund rules, and more. Instead of scattering payment method logic across the frontend, backend, and various services, we consolidate all relevant details in this config. Both core payment services and frontend experiences dynamically reference this single source of truth, ensuring consistency for eligibility checks, UI rendering, and business rules. This unified approach dramatically reduces duplication, manual updates, and errors across the stack, making integration and maintenance faster and more reliable.</p><p>These configs also drive automated code generation for backend services using code generation tools, producing Java classes, DTOs, enums, schema, and integration scaffolding. As a result, integrating or updating a payment method is largely declarative — just a config change. This streamlines launches from months to weeks and makes ongoing maintenance far simpler.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*2elXD-3BHco1AqeSAXPQDg.png\" /></figure><h4>Payment widget</h4><p>Our payment widget — the payment method UI embedded into the checkout page — includes the list of available payment methods and handles the user’s inputs. Local payment methods often require specialized input forms (such as CPF for Pix) and have unique country/currency eligibility.</p><p>Rather than hardcoding forms and rules into the client, we centralize both form-field specification and eligibility checks in the backend. Servers send configuration payloads to clients defining exactly which fields to collect, which validation rules to apply, and which payment options to render. This empowers the frontend to dynamically adapt UI and validation for each payment method, accelerating launches and keeping user experiences fresh without frequent client releases.</p><p>For example, Pix in Brazil requires the guest’s first name, last name, and CPF (tax ID), which we collect and transmit as required to complete the payment.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/764/1*h58IqdWRxO_5U7ZrD_cMDA.png\" /></figure><p>Below is a diagram illustrating how dynamic payment method configurations are delivered from the backend to the frontend, enabling tailored checkout presentations for each payment method.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*-K2aydrTL6jJGHouKu1M2Q.png\" /></figure><h3>Building confidence through better testability</h3><p>Testing local payment methods can be difficult, because developers often don’t have access to local wallets. Yet with such a broad range of payment methods and complex flows, comprehensive testing is essential to prevent regressions and ensure seamless functionality.</p><p>To address this, we enhanced Airbnb’s in-house <strong>Payment Service Provider (PSP) Emulator</strong>, enabling realistic simulation of PSP interactions for both redirect and asynchronous payment methods. The Emulator allows developers to test end-to-end payment scenarios without relying on unstable (or nonexistent) PSP sandboxes. For redirect payments, the Emulator provides a simple UI mirroring PSP acquirer pages, allowing testers to explicitly approve or decline transactions for precise scenario control. For async methods, it returns QR code details and automatically schedules webhook emission tasks upon receiving a /payments request — delivering a complete, reliable testing environment across diverse LPMs.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ATFCaiOr1uS11DfoO2q5_Q.png\" /></figure><h3>Scaling observability for local payment methods</h3><p>Maintaining high reliability and availability is critical for Airbnb’s global payment system. As we expand to support many new local payment methods, we face increasing complexity: greater dependencies on external PSPs and wide variations in payment behaviors. For example, a real-time card payment and a redirect flow like Naver Pay follow completely different technical paths. That diversity makes observability difficult — a single “payment success rate” may represent card health well, but say little about an asynchronous LPM. Without proper visibility, regressions can go unnoticed until they affect real users. As dozens of new LPMs go live, observability has become the foundation of reliability.</p><p>To address this, we built a centralized monitoring framework that unifies metrics across all layers, from client to PSP. When launching a new LPM, onboarding now requires a single config change; add the method name, and metrics begin streaming automatically:</p><ul><li><strong>Client metrics</strong> — user-level flow health from clients</li><li><strong>Payment backend metrics</strong> — API-level metrics for payment flows</li><li><strong>PSP metrics</strong> — API-level visibility between Airbnb and the PSP</li><li><strong>Webhook metrics</strong> — async completion status for redirect methods or refunds</li></ul><p>We have also standardized the alerting rules across our platform’s Client, Backend, PSP, and Webhook layers using composite alerts and anomaly detection. Each alert follows a consistent pattern (failure count, rate, time window), e.g., “Naver Pay resume failures &gt; 5 and failure rate &gt; 20% in 30 minutes.” This design minimizes false positives during low-traffic periods.</p><p>This framework scales effectively, providing end-to-end visibility from user click to PSP confirmation. It enables engineers to trace issues in minutes rather than hours, whether those issues were caused by internal changes or external outages. By turning observability into a shared, automated layer, we were able to strengthen the backbone of payment reliability while accelerating the rollout of new LPMs worldwide.</p><h3>Impact</h3><p>The Pay as a Local initiative delivered significant business and technical impact:</p><ul><li><strong>Meaningful booking uplift:</strong> We observed<strong> meaningful uplift </strong>in bookings and new users in markets where we launched local payment methods</li><li><strong>Faster integrations:</strong> Reduced integration time <strong>significantly </strong>through reusable flows and config-driven automation.</li><li><strong>Stronger reliability:</strong> Improved observability for early outage detection, standardized testing to prevent regressions, and streamlined vendor escalation and on-call processes for global resilience.</li></ul><h3>Conclusion</h3><p>Supporting local payment methods helps Airbnb to stay competitive and relevant in the global travel industry. These payment options help improve checkout conversion, drive adoption, and unlock new growth opportunities.</p><p>This post outlined how the<strong> </strong>Airbnb payment platform has evolved to support local payment methods at scale — through asynchronous payment orchestration, config-driven onboarding, centralized observability, and<strong> </strong>robust testability. Together, these capabilities enable faster integrations, lower maintenance overhead, and offer a more seamless, localized checkout experience for guests worldwide.</p><p>As Airbnb continues to expand globally, our payments platform will keep evolving with the same principles of extensibility, reliability, and scalability, ensuring that guests everywhere can pay confidently, using the methods they know and trust.</p><h3>Acknowledgments</h3><p>We had many people at Airbnb contributing to this big rearchitecture, but countless thanks to <a href=\"mailto:mini.atwal@airbnb.com\">Mini Atwal</a>, <a href=\"mailto:ashish.singla@airbnb.com\">Ashish Singla</a>, <a href=\"mailto:musaab.attaras@airbnb.com\">Musaab At-Taras</a>,<a href=\"mailto:linmin.yang@airbnb.com\">Linmin Yang</a>, <a href=\"mailto:yong.rhyu@airbnb.com\">Yong Rhyu</a>, <a href=\"mailto:yohannes.tsegay@airbnb.com\">Yohannes Tsegay</a>, <a href=\"mailto:livar.cunha@airbnb.com\">Livar Cunha</a>, <a href=\"mailto:praveena.subrahmanyam@airbnb.com\">Praveena Subrahmanyam</a>, <a href=\"mailto:steve.ickes@airbnb.com\">Steve Ickes</a>, <a href=\"mailto:vijaykumar.borkar@airbnb.com\">Vijaykumar Borkar</a>, Vibhu Ramani, Aashna Jain, Abhishek Ghosh, Abhishek Patel, Adithya Tammavarapu, Akai Hsieh, Akash Budhia, Amar Parkash, Amee Mewada, Ankita Balakrushan Tate, Bharath Kumar Chandramouli, Bo Shi, Bo Yuan. Callum Li. Carlos Townsend Pico, Chanakya Daparthy, Charles Tang, Cibi Pari, Cindy Jaimez, Cindy Shi, Dan Yo, Daniela Nobre, Danielle Zegelstein, David Cordoba, David Drinan, Dawei Wang, Dechuan Xu, Denise Francisco, Denny Liang, Dimi Matcovschi, Divya Verma, Feifeng Yang, Gabriel Siqueira, Sunny Wallia, Prashant Jamlakar, Daniel Kriske, Giovanni Iniguez, Haojie Zhang, Haokun Chen, Haoti Zhong, Harriet Russell, Harshit Gupta, Henrique Moreira Indio do Brasil, Ishan Ishan, Jenny Shen, Jerroid Marks, Jiafang Jiang, Joey Yin, Jon Chew, Karen Kuo, Katie Turley, Letian Zhang, Maneesh Lall, Manish Singhal, Maria Daneri, Mark Jang, Mengfei Ren, Michelle Desiderio, Mohit Dhawan, Nam Kim, Nerea Ruiz Alvarez, Nikita Kapoor, Oliver Zhang, Omer Faruk Gul, Pallavi Sharma, Prateek Sri, Rae Huang, Rohit Krishnan Dandayudham, Rory MacQueen, Ruize Liu, Sam Bitter, Sam Tang, Saran Singh. Sardana Sai Anil, Serdar Yildirim, Shwetha Saibanna, Silvia Crespo Sanchez, Simon Xia, Stella Dong, Stella Su, Stephanie Leung, Steve Cao, Sumit Ranjan, Tay Rauch, Thanigaivelan Manickavelu, Tiffany Selby, Toland Hon, Trish Burgess,Vishal Garg, Vivian Lue, Vyom Rastogi, William Betz, Xi Wen, Xing Xing, Xuanxuan Wu, Yangguang Li, Yanwei Bai, Yeung Song, Yixia Mao, Yujia Liu. Yun Cho, Zhenhui Zhu, Ziyun Ye</p><h3>****************</h3><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bef469b72f32\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/pay-as-a-local-bef469b72f32\">Pay As a Local</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "使用大型语言模型和 @generateMock 实现大规模 GraphQL 数据模拟 (原标题: GraphQL Data Mocking at Scale with LLMs and @generateMock)",
      "link": "https://medium.com/airbnb-engineering/graphql-data-mocking-at-scale-with-llms-and-generatemock-30b380f12bd6?source=rss----53c7c27702d5---4",
      "pubDate": "Thu, 30 Oct 2025 17:01:54 GMT",
      "isoDate": "2025-10-30T17:01:54.000Z",
      "creator": "Michael Rebello",
      "summary": "# 使用大型语言模型和 @generateMock 实现大规模 GraphQL 数据模拟\n\nAirbnb 重新构想了 GraphQL 数据模拟，通过结合 GraphQL 基础设施、丰富的产品上下文和大型语言模型（LLM），生成并维护令人信服且类型安全的模拟数据。其核心是一个名为 `@generateMock` 的新 GraphQL 客户端指令。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/1*wHF0IXmtHzIfzv-IZh_CKA.jpeg)\n\n## 关键挑战\n\nAirbnb 工程师在 GraphQL 模拟方面面临以下主要痛点：\n\n1.  **手动创建模拟数据耗时：** GraphQL 查询可能包含数百行，手动编写模拟响应数据非常繁琐且容易出错。工程师通常手动编写 JSON 文件或实例化类型，或修改复制粘贴的服务器响应。\n2.  **在没有服务器的情况下进行原型设计和演示困难：** 客户端工程师在服务器完全实现 GraphQL 模式之前无法测试 UI。他们通常通过硬编码数据、使用代理或修改网络层来解决问题，导致时间和精力浪费。\n3.  **模拟数据随时间与 GraphQL 查询不同步：** 手动编写的模拟数据与底层查询和模式没有紧密耦合。随着查询的演变，模拟数据往往会过时，降低测试质量。现有工具（如随机值生成器）缺乏生成真实、有意义数据所需的领域知识。\n\n## 目标\n\n为解决这些挑战，Airbnb 设定了三个核心目标：\n\n*   **消除手动编写模拟数据的需求：** 模拟数据应自动生成，将工程师从手动编写和维护中解放出来。\n*   **创建高度真实的模拟数据：** 模拟数据应与用户界面设计匹配，并看起来像真实的生产数据，以支持高质量的演示。\n*   **让工程师保持在本地开发流程中：** 解决方案应无缝集成到工程师当前的开发流程中，无需切换上下文。\n\n## @generateMock：模式 + 上下文 + LLM = 魔法\n\nAirbnb 引入了新的客户端 GraphQL 指令 `@generateMock`，工程师可以将其添加到任何操作、片段或字段中，以自动生成模拟数据。\n\n![图片 2](https://cdn-images-1.medium.com/max/1024/1*llh_79Y2eUguxztCsbzIkg.png)\n\n该指令接受可选参数来自定义生成的模拟数据，并可重复使用以生成不同变体：\n\n*   `id`：模拟的标识符，用于命名生成的辅助函数。\n*   `hints`：提供给 LLM 的额外上下文或指令，影响模拟数据的外观和字段填充密度。\n*   `designURL`：设计稿的 URL，帮助 LLM 生成与设计匹配的模拟数据。\n\n工程师在本地修改 `.graphql` 文件并添加 `@generateMock` 后，运行 Niobe 命令行工具（GraphQL 代码生成器）。Niobe 会生成一个包含实际模拟数据的 JSON 文件，以及一个提供加载和使用模拟数据函数的源文件。\n\n![图片 3](https://cdn-images-1.medium.com/max/1024/1*6stJEGGeLtkfKK_D6ulaoA.png)\n\n工程师也可以修改生成的模拟 JSON 数据，Niobe 在后续生成时会避免覆盖这些修改。\n\n## 模拟数据是如何生成的？\n\nNiobe 收集以下信息作为上下文传递给 LLM，以生成高度真实的模拟数据：\n\n*   被模拟的查询/片段/字段的定义。\n*   被查询的 GraphQL 模式子集及其相关文档（Niobe 会剥离不相关的类型和字段）。\n*   `designURL` 指定的设计文档的图像 URL（通过内部 API 生成快照）。\n*   `@generateMock` 中指定的额外 `hints`。\n*   生成模拟数据的平台（如“iOS”、“Android”、“Web”）。\n*   Airbnb 托管的图像 URL 列表及其简短描述，防止 LLM 虚构不存在的 URL。\n\n![图片 4](https://cdn-images-1.medium.com/max/1024/1*yPJB6saOgVZ7kQErYjGtWQ.png)\n\n所有这些信息被整合到一个针对 Gemini 2.5 Pro 微调的提示中。该模型因其 100 万令牌的上下文窗口和在内部测试中显著更快的性能而被选中。这种方法能够生成高度真实的 JSON 模拟数据，加载到应用程序中后效果非常逼真。\n\n![图片 5](https://cdn-images-1.medium.com/max/1024/1*1PXeYCxy9BVSWYtOI7edHQ.png)\n\n## 工作原理\n\n当工程师使用 Niobe CLI 生成 GraphQL 代码时，模拟数据生成是该过程的最后一步：\n\n1.  如果 `@generateMock` 包含 `designURL`，Niobe 会验证 URL，并使用内部 API 生成设计节点的图像快照，获取其 URL。\n2.  CLI 聚合所有上下文信息（包括设计快照 URL），并构建发送给 LLM 的提示。\n3.  提示发送到 Gemini 2.5 Pro 模型，结果流式传输回客户端。\n4.  收到 LLM 的模拟 JSON 响应后，Niobe 使用 `graphql NPM package` 的 `graphqlSync` 函数对数据进行验证。\n5.  如果验证产生错误（例如，LLM 虚构了无效的枚举值或未填充必填字段），Niobe 会聚合这些错误并连同初始模拟数据一起反馈给 LLM，进行“自我修复”的重试机制。这确保了生成的模拟数据完全有效。\n6.  最终，Niobe 将验证后的模拟数据写入 JSON 文件和伴随的源文件。\n\n![图片 6](https://cdn-images-1.medium.com/max/1024/1*i1gTnZfrP0qZmQrn3N-jkQ.png)\n\n## @respondWithMock：解除客户端开发阻塞\n\n除了 `@generateMock`，Airbnb 还引入了 `@respondWithMock` 指令，使客户端工程师无需等待后端服务器实现即可迭代功能。\n\n![图片 7](https://cdn-images-1.medium.com/max/1024/1*GDhohfDu-AgMKgN3yBH4LA.png)\n\n当存在此指令时，Niobe 会修改生成的代码以包含此注解的详细信息。在运行时，GraphQL 客户端会加载生成的模拟数据，并无缝返回模拟响应，而不是使用服务器数据。这使得客户端工程师能够使用本地模拟数据来查询未实现的字段。\n\n`@respondWithMock` 也可以应用于单个字段。在这种情况下，GraphQL 客户端会从服务器请求所有字段，除了那些带有 `@respondWithMock` 注解的字段，然后为这些字段打上本地模拟数据补丁，从而生成生产数据和模拟数据的混合体。工程师甚至可以重复使用此指令，并使用查询输入变量来决定何时在运行时返回特定的生成模拟数据。\n\n![图片 8](https://cdn-images-1.medium.com/max/1024/1*6JRec3pvDGoc8_nMYS7HIg.png)\n\n## 模式演进：保持模拟数据真实性\n\n为解决模拟数据与查询随时间不同步的问题，Niobe 在每个生成的 JSON 文件中嵌入了两个额外的哈希值：\n\n*   被模拟客户端实体（即 GraphQL 查询文档）的哈希。\n*   `@generateMock` 输入参数的哈希。\n\n![图片 9](https://cdn-images-1.medium.com/max/852/1*P0Fps2QTSpsnTP_shlqNfQ.png)\n\n每次代码生成运行时，Niobe 会根据 GraphQL 文档检查现有模拟的哈希是否与当前哈希匹配。如果匹配，则跳过这些类型的模拟生成。如果哈希发生变化，Niobe 会将现有模拟数据包含在提供给 LLM 的上下文中，并附带修改指令，从而智能地更新模拟数据。这确保了 Niobe 不会不必要地修改未更改且仍然有效的字段的模拟数据。",
      "shortSummary": "Airbnb 推出了一套创新的 GraphQL 数据模拟方案，利用大型语言模型（LLM）和两个新的指令 `@generateMock` 和 `@respondWithMock`。`@generateMock` 结合 GraphQL 模式、产品上下文和 LLM 自动生成高度真实、类型安全的模拟数据，解决了手动创建耗时、数据不同步的痛点。它支持通过 `hints` 和 `designURL` 定制模拟内容。`@respondWithMock` 则允许客户端工程师在后端未实现时，无缝使用本地模拟数据进行开发和测试，甚至可以混合生产和模拟数据。Niobe 工具通过哈希值管理模拟数据的版本，确保其随模式演进而保持同步和有效。",
      "translated_title": "使用大型语言模型和 @generateMock 实现大规模 GraphQL 数据模拟",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*wHF0IXmtHzIfzv-IZh_CKA.jpeg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*llh_79Y2eUguxztCsbzIkg.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*6stJEGGeLtkfKK_D6ulaoA.png",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*yPJB6saOgVZ7kQErYjGtWQ.png",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*1PXeYCxy9BVSWYtOI7edHQ.png",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<p><em>How Airbnb combines GraphQL infra, product context, and LLMs to generate and maintain convincing, type-safe mock data using a new directive.</em></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*wHF0IXmtHzIfzv-IZh_CKA.jpeg\" /></figure><h3>Introduction</h3><p>Producing valid and realistic mock data for testing and prototyping with GraphQL has been a persistent challenge across the industry for years. Mock data is tedious to write and maintain, and attempts to improve the process, such as random value generation and field-level stubbing, fall short because they lack essential domain context to make test data realistic and meaningful. The time spent on this manual work ultimately takes away from what most engineers would like to focus on: building features.</p><p>In this post, we’ll explore how we’ve reimagined mocking GraphQL data at Airbnb by combining GraphQL validation, rich product and schema context, and LLMs to generate and maintain convincing, type-safe mock data. Our solution centers around a simple new GraphQL client directive — @generateMock — that engineers can add to any operation, fragment, or field. This approach eliminates the need for engineers to manually write and maintain mocks as queries evolve, freeing up time to focus on building the product.</p><h3>Key challenges</h3><p>After meeting with Airbnb product engineers and analyzing results from internal surveys, we distilled the most common pain points around GraphQL mocking down into three key challenges:</p><ol><li><strong>Manually creating mocks is time consuming.</strong> GraphQL queries can grow to hundreds of lines, and hand-crafting mock response data is extremely tedious. Most engineers manually write mocks as either raw JSON files or by instantiating types generated from the GraphQL schema, while others modify copy-and-pasted JSON responses from the server. Although both of these methods can yield realistic-looking data that can be used for demos and snapshot tests, they require significant time investment and are prone to subtle mistakes.</li><li><strong>Prototyping &amp; demoing features without the server is hard.</strong> Typically, server and client engineers agree on a GraphQL schema early on in the feature development process. Once the schema has been established, however, the two groups split off and start working in parallel: Server engineers implement the logic to back the new schema and client engineers build the frontend UI, logic, and the queries that power them. This parallelization is particularly challenging for client engineers, since they can’t actually test the UI they’re building until the server has fully implemented the schema. To unblock themselves, client engineers often hardcode data into views, leverage proxies to manipulate responses, or hack custom logic into the networking layer locally, resulting in wasted time and effort.</li><li><strong>Mocks get out of sync with GraphQL queries over time.</strong> Since most mocks are hand-written, they are not tightly coupled to the underlying queries and schema they are supposed to represent. If a team builds a new feature, then comes back a few months later to add new functionality backed by additional GraphQL fields, engineers must remember to manually update their mock data. As there is no forcing function to guarantee mocks stay in sync with queries, mock data tends to shift further away from the production reality as time passes — degrading the quality of tests.</li></ol><p>These challenges are not unique to Airbnb and are common across the industry. Although tooling like random value generators and local field resolvers can provide some assistance, they lack the domain knowledge and context needed to produce realistic, meaningful data for high-quality demos, quick product iteration, and reliable testing.</p><h3>Goals</h3><p>When setting out to solve these challenges at Airbnb, we established three north-star goals:</p><ol><li><strong>Eliminate the need to hand-write mock data.</strong> Mock data should be generated automatically to free up engineers from needing to hand-craft and maintain mock GraphQL data.</li><li><strong>Create highly realistic mock data.</strong> Mock data should match the user interface designs and look like real production data in order to support high-quality demos, which are highly valued at Airbnb for early feedback.</li><li><strong>Keep engineers in their local focus loops.</strong> Our solution should seamlessly integrate into engineers’ current development processes so they can generate mocks without context-switching to a website, separate repository, or unfamiliar tool.</li></ol><h3>@generateMock: Schema + context + LLMs = magic</h3><p>To generate mock data while keeping engineers in their local focus loops, we introduced a new client GraphQL directive called @generateMock, which engineers can use to automatically generate mock data for a given GraphQL operation, fragment, or field:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*llh_79Y2eUguxztCsbzIkg.png\" /><figcaption>Example of @generateMock being specified on a GraphQL query.</figcaption></figure><p>This directive accepts a few optional arguments that engineers can use to customize the generated mock data, and the directive itself can be repeated with different input arguments to generate different mock variations:</p><ul><li><em>id</em>: The identifier to use for the mock, as well as for naming generated helper functions. Useful when repeating the @generateMock directive to produce multiple mocks.</li><li><em>hints</em>: Additional context or instructions on how the mock should look. For example, a hint might be “Include travel entries for Barcelona, Paris, and Kyoto.” Under the hood, this information is fed to an LLM and heavily influences what the generated mock data looks like and how densely populated its fields are.</li><li><em>designURL</em>: The URL of a design mockup of the screen that will render the mock data. Specifying this argument helps the LLM produce mock data that matches the design by generating matching names, addresses, and other similar content.</li></ul><p>At Airbnb, engineers use a command line tool we call Niobe to generate code for their GraphQL queries and fragments. After modifying a .graphql file locally, engineers run this code generator, then use the generated TypeScript/Kotlin/Swift files to send GraphQL requests. To generate mock data using @generateMock, engineers simply need to run Niobe code generation after adding the directive — just as they would after making any other GraphQL change.</p><p>During code generation, Niobe produces both a JSON file containing the actual mock data for each @generateMock directive, as well as a source file that provides functions for loading and consuming mock data from demo apps, snapshot tests, and unit tests. As shown in the Swift code below, the <em>mockMixedStatusIndicators()</em> function is generated on the InboxSyncQuery’s root <em>Data</em> type. It provides access to an instantiated type that’s populated with the generated mock data for <em>mixed_status_indicators</em>, allowing engineers to use the mock without having to load the JSON data manually:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*6stJEGGeLtkfKK_D6ulaoA.png\" /><figcaption>Using a generated mock in a Swift unit test.</figcaption></figure><p>Engineers are free to modify the generated mock JSON data as well — as we’ll see below, Niobe will avoid overwriting their modifications on subsequent generation invocations.</p><h3>What does mock data look like?</h3><p>The context that we provide to the LLM is vital to generating data that is realistic enough to use in demos. To this end, Niobe collects the following information and includes it in the context passed to the LLM:</p><ul><li>The definitions of the query/fragment/fields being mocked (i.e., those marked with @generateMock and their dependencies).</li><li>The <em>subset</em> of the GraphQL schema being queried, as well as any associated documentation that is present as inline comments. This information enables the LLM to infer the types that are used by the query being mocked. Importantly, this isn’t the <em>whole</em> schema, because including the full schema would likely overload the context window — Niobe traverses the schema and strips out types and fields that are not needed to resolve the query, along with any extra whitespace.</li><li>The URL for the image representation of the design document specified within <em>designURL</em>, if any. Niobe integrates with an internal API to generate a snapshot image of the provided node in the design document. The API pushes this snapshot to a storage bucket and provides a URL that Niobe feeds to the LLM, along with specialized instructions on how to use it.</li><li>The additional <em>hints</em> specified in @generateMock.</li><li>The platform (e.g., “iOS”, “Android”, or “Web”) for which the mock data is being generated (for style specificity).</li><li>A list of Airbnb-hosted image URLs that the LLM can choose from if needed, along with short textual descriptions of each. This prevents the LLM from hallucinating image URLs that don’t exist and ensures that the mock data contains valid URLs which can be properly loaded at runtime when prototyping or demoing.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*yPJB6saOgVZ7kQErYjGtWQ.png\" /><figcaption>Illustration of the various pieces of context that are passed to the LLM during mock generation.</figcaption></figure><p>All this information is consolidated into a prompt we fine-tuned against Gemini 2.5 Pro. We chose this model because of its 1-million token context window, plus the fact that in our internal tests this configuration performed significantly faster than comparable models while producing mock data of similar quality. Using this approach, we’re able to produce highly realistic JSON mocks which, when loaded into the application, yield very convincing results as shown below:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1PXeYCxy9BVSWYtOI7edHQ.png\" /><figcaption>Screenshot of a design mockup compared to a mock that was generated using @generateMock.</figcaption></figure><p>The data in the screenshot on the right looks quite realistic, but if you look closely you may notice that the data is indeed mocked — all the photos are coming from the seed data set that we feed the LLM.</p><h3>How it works</h3><p>When an engineer uses the Niobe CLI to generate code for their GraphQL files, Niobe automatically performs mock generation as the final step of this process, as shown in the flowchart below:</p><ul><li>If the @generateMock directive includes a <em>designURL</em>, Niobe validates the URL to ensure it includes a <em>node-id</em>, then uses an internal API to produce an image snapshot of that particular node. The API, in turn, pushes this snapshot to a storage bucket and provides Niobe with its URL.</li><li>Next, the CLI aggregates all the context described in the section above — including the URL of the design snapshot — and crafts a prompt to send to the LLM. This prompt is then sent to the Gemini 2.5 Pro model, and results are streamed back to the client in order to show a progress indicator in the CLI.</li><li>Once the mock JSON response has been received from the LLM, Niobe performs a validation step against this data by passing the GraphQL schema, client GraphQL document, and JSON data to the graphql <a href=\"https://www.npmjs.com/package/graphql\">NPM package</a>’s <em>graphqlSync</em> function.</li><li>If the validation produces errors (for example, if the LLM hallucinated an invalid enum value or failed to populate a required field), Niobe aggregates these errors and feeds them back into the LLM along with the initial mock data. This retry mechanism is used to essentially “self-heal” and fix invalid mock data.<br>– This step is <em>critical</em> to reliably generating mock data. By placing the LLM within our existing GraphQL infrastructure, we’re able to enforce a set of guardrails through this validation step and provide strong guarantees that the mock data produced at the end of the pipeline is fully valid — something that wouldn’t be possible by using a tool outside our GraphQL infrastructure like ChatGPT.<br>– Finally, once the mock data has been validated, Niobe writes it to a JSON file, alongside a companion source file which provides functions for loading the mock from application code.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*i1gTnZfrP0qZmQrn3N-jkQ.png\" /><figcaption>Flowchart of how mock generation works under the hood.</figcaption></figure><h3>@respondWithMock: Unblocking client development</h3><p>In addition to generating realistic mock data with @generateMock, we also wanted to empower client engineers to iterate on features without waiting for the backend server implementation. A second directive, @respondWithMock, works alongside @generateMock to make this possible:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*GDhohfDu-AgMKgN3yBH4LA.png\" /><figcaption>Simple example of using the @respondWithMock directive.</figcaption></figure><p>When this directive is present, Niobe alters the code that’s generated alongside the mock data to include extra details about this annotation. At runtime, the GraphQL client uses this to load the generated mock data, then seamlessly returns the mocked response <em>instead</em> of using data from the server. This effectively allows client engineers to unblock themselves from waiting on the server implementation, since they can easily use locally mocked data when querying unimplemented fields. The screenshot of the inbox screen earlier in this post is actually a real screenshot that was taken by generating with these two directives and running the Airbnb app in an iOS simulator — no manual mocking, proxying, or response modification needed!</p><p>@respondWithMock can also be specified on <em>individual fields</em>. When used on fields within a query instead of on the query itself, the GraphQL client will actually request all fields from the server <em>except those annotated with @respondWithMock</em>, then patch in locally mocked data for the remaining fields — producing a hybrid of production and mock data, and making it possible for client engineers to develop against new (unimplemented) fields in existing queries. Engineers can even repeat this directive and use query input variables to decide if and when to return a specific generated mock at runtime, as shown below:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*6JRec3pvDGoc8_nMYS7HIg.png\" /><figcaption>Using @respondWithMock with conditionals and on individual fields.</figcaption></figure><h3>Schema evolution: Keeping mocks truthful</h3><p>The final challenge we addressed was the issue of keeping mocks in sync with queries as they evolve over time. Since Niobe manages mock data that is generated via the @generateMock directive, it can be smart about maintaining that mock data. As part of mock generation, Niobe embeds two extra keys in each generated JSON file:</p><ol><li>A hash of the client entity being mocked (i.e., the GraphQL query document).</li><li>A hash of the input arguments to @generateMock.</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/852/1*P0Fps2QTSpsnTP_shlqNfQ.png\" /><figcaption>Niobe embeds version hashes in mock data in order to determine when a given mock needs to be updated.</figcaption></figure><p>Each time code generation runs, Niobe determines whether existing mocks’ hashes differ from what their current hashes should be based on the GraphQL document. If they match, it skips mock generation for those types. On the other hand, if one of the hashes changed, Niobe intelligently updates that mock by including the existing mock in the context provided to the LLM, along with instructions on how to modify it.</p><p>It’s important that Niobe doesn’t unnecessarily modify existing mock data for fields that are unchanged and still valid, since doing so could overwrite manual tweaks that were made to the JSON by engineers or break existing tests that rely on this data. To avoid this, we provide the LLM with a diff of what changed in the query, and tuned the prompt to focus on that diff and avoid making spurious changes to unrelated fields.</p><p>Finally, each client codebase includes an automated check that ensures mock version hashes are up to date when code is submitted. This provides a guarantee that all generated mocks stay in sync with queries as they evolve over time. When engineers encounter these validation failures, they just re-run code generation locally — no manual updates required.</p><h3>Conclusion</h3><p><em>“@generateMock has significantly sped up my local development and made working with local data much more enjoyable.” — Senior Software Engineer</em></p><p>By integrating highly contextualized LLMs — informed by the GraphQL schema, product context, and UX designs — directly into existing GraphQL tooling, we’ve unlocked the ability to generate valid and realistic mock data while eliminating the need for engineers to manually hand-write and maintain mocks. The directive-driven approach of @generateMock and @respondWithMock allows engineers to build clients before the server implementation is complete while keeping them in their focus loops and providing a guarantee that mock data stays in sync as queries evolve.</p><p>In just the past few months, Airbnb engineers have generated and merged over 700 mocks across iOS, Android, and Web using @generateMock, and we plan to roll out internal support for backend services soon. These tools have fundamentally changed how engineers mock GraphQL data for tests and prototypes at Airbnb, allowing them to focus on building product features rather than crafting and maintaining mock data.</p><h3>Acknowledgments</h3><p>Special thanks to Raymond Wang and Virgil King for their contributions bringing @generateMock support to Web and Android clients, as well as to many other engineers and teams at Airbnb who participated in design reviews, built supporting infrastructure, and provided usage feedback.</p><p>Does this type of work interest you? Check out our open roles <a href=\"https://careers.airbnb.com/\">here</a>.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=30b380f12bd6\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/graphql-data-mocking-at-scale-with-llms-and-generatemock-30b380f12bd6\">GraphQL Data Mocking at Scale with LLMs and @generateMock</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "从静态限流到Airbnb键值存储中的自适应流量管理 (原标题: From Static Rate Limiting to Adaptive Traffic Management in Airbnb’s Key-Value Store)",
      "link": "https://medium.com/airbnb-engineering/from-static-rate-limiting-to-adaptive-traffic-management-in-airbnbs-key-value-store-29362764e5c2?source=rss----53c7c27702d5---4",
      "pubDate": "Thu, 09 Oct 2025 16:01:55 GMT",
      "isoDate": "2025-10-09T16:01:55.000Z",
      "creator": "Shravan Gaonkar",
      "summary": "Airbnb的键值存储Mussel已从简单的静态限流系统演进为多层级的自适应流量管理方案，以确保在流量高峰、批量上传、机器人攻击或DDoS攻击等复杂场景下的快速和可靠性。\n\n# Mussel流量管理演进概述\n\nMussel作为Airbnb核心服务的关键组件，处理所有请求。其早期QoS系统依赖于基于Redis的静态QPS限流，主要目标是防止服务崩溃。然而，随着服务成熟，目标转向了最大化“有效吞吐量”（goodput），即在不降低性能的前提下完成最多有用的工作。这促使Airbnb开发了一个自适应的QoS系统，该系统能够自动施加优先背压。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/1*kBx_8QLd7El4TZ2nrouYUw.jpeg)\n\n# 新增的QoS层级\n\n为实现自适应流量管理和最大化有效吞吐量，Mussel的QoS系统增加了以下几个新层级：\n\n1.  **资源感知限流 (RARC)**：根据请求的实际资源消耗（如行数、字节和延迟）而非简单的请求计数来计费。\n2.  **分级负载卸载**：在容量不足时，优先保障高优先级流量（如客户支持、信任与安全）的响应性。\n3.  **热键检测与DDoS缓解**：实时检测访问模式倾斜，并通过缓存或合并重复请求来保护后端存储层。\n\n# 静态客户端配额限流的局限性\n\nMussel最初的限流系统简单且易于操作，为每个调用者设置静态的每分钟QPS配额。然而，随着系统采用率的增长，其局限性逐渐显现：\n\n*   **成本差异**：一个单行查询和一个十万行扫描被视为相同的成本，未能反映其对后端负载的巨大差异。\n*   **流量倾斜**：无法识别“热键”问题。当一个键被大量不同调用者同时访问时，即使每个调用者都在其配额内，聚合流量仍可能使底层存储分片过载，导致局部瓶颈并影响整个集群的性能。\n\n这些局限性促使Airbnb从“请求计数”思维转向“资源核算”思维。\n\n![图片 2](https://cdn-images-1.medium.com/max/1024/1*XVDMQb8i2pQEUogiZipFbQ.jpeg)\n\n# 资源感知限流 (RARC)\n\nRARC通过引入“请求单位”（RU）来解决成本差异问题。RU的计算综合了四个可观测因素：\n\n*   固定的每次调用开销\n*   处理的行数\n*   负载字节数\n*   **关键的延迟**：延迟能够反映缓存命中与磁盘访问等不同成本。\n\nRU的计算公式示例：\n*   `RU_read = 1 + w_r × bytes_read + w_l × latency_ms`\n*   `RU_write = 6 + w_b × bytes_written / 4096 bytes + w_l × latency_ms`\n\n每个调度器使用一个本地令牌桶来管理RU配额，并根据请求的RU成本进行扣除。当令牌桶为空时，请求将被拒绝（HTTP 419）。后端延迟会影响负载卸载层，而非RU的周期性补充，这使得限流核算保持简单，同时能快速响应存储层的压力。\n\n![图片 3](https://cdn-images-1.medium.com/max/1024/1*VuIeZSMzyRqXuXpfSzM_yg.jpeg)\n\n# 负载卸载：应对快速变化的容量问题\n\nRARC在秒级尺度上调整，但无法应对更快的负载变化。负载卸载层通过结合三个实时信号来弥补这一不足：\n\n1.  **流量关键性**：根据预定义的优先级，确保高优先级流量在容量紧张时仍能响应。\n2.  **延迟比率**：实时系统压力指标，通过长期p95延迟除以短期p95延迟计算。比率趋近0.3表示延迟急剧上升。当达到阈值时，调度器会暂时增加指定客户端类别的RU成本，加速其令牌桶耗尽，从而降低请求速率。该比率使用P²算法计算。\n3.  **CoDel启发式排队策略**：监控请求在调度器内部队列的等待时间。如果等待时间过长表明系统已饱和，请求将提前失败，释放内存和线程。\n\n这些机制协同工作，使Mussel在后端出现问题时能将恢复时间缩短约一半，并使调度器无需人工干预即可保持稳定。\n\n![图片 4](https://cdn-images-1.medium.com/max/1024/1*GWv-z2hXM6RiW6sRJKzYCg.png)\n\n# 热键检测与DDoS防御\n\nRU限流和负载卸载无法有效阻止针对单个记录的大量相同读取请求（如热门列表、爬虫或DDoS攻击）。Mussel通过三步热键防御层来解决此问题：\n\n1.  **实时检测**：每个调度器使用内存中的top-k计数器（Space-Saving算法变体）实时追踪最热门的键。\n2.  **本地缓存**：当一个键达到“热”阈值时，调度器会从进程本地的LRU缓存中提供服务，条目在大约三秒后过期。\n3.  **请求合并**：对于缓存未命中，调度器会跟踪正在进行的读取请求，新到达的相同请求会附加到待处理的Future上，第一个后端响应会扇出给所有等待者。这确保了通常每个热键每个调度器Pod只有一个请求到达存储层。\n\n在一次模拟DDoS演练中，针对少量键的百万QPS规模突发流量被热键层压缩到极低水平，后端未受到影响。\n\n![图片 5](https://cdn-images-1.medium.com/max/1024/1*DuwP-cqJEnQcbD64RGUTYA.png)\n\n# 回顾与关键启示\n\n*   **早期可见成果的价值**：RARC的早期部署验证了概念，为后续更深层次的改进奠定了基础。\n*   **偏好本地控制循环**：所有关键信号（P²延迟分位数、Space-Saving top-k计数器、CoDel队列延迟）都在每个调度器内部运行，无需跨节点协调，实现了线性扩展，即使控制平面受压也能保护容量。\n*   **分时尺度的有效保护**：RU定价处理微突发，而延迟比率和CoDel队列阈值响应宏观减速。两者结合才能有效应对各种冲击。\n*   **QoS是一个活的系统**：流量模式、后端能力和新工作负载不断演变。未来的计划包括数据库原生资源组和基于历史使用曲线的自动配额调整。指导原则是：衡量真实成本、本地快速响应、分层防御。",
      "shortSummary": "Airbnb的键值存储Mussel已从静态限流演进为自适应流量管理，以应对高峰流量和DDoS攻击。最初的QPS限流因无法区分请求成本和处理热键而受限。新系统引入了资源感知限流（RARC），根据请求的实际资源消耗（包括延迟）计费。此外，它还通过分级负载卸载（基于流量关键性、实时延迟比率和排队策略）和热键检测与防御（实时缓存和请求合并）来确保服务可靠性。这些改进显著提升了系统在压力下的韧性，并减少了恢复时间。",
      "translated_title": "从静态限流到Airbnb键值存储中的自适应流量管理",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*kBx_8QLd7El4TZ2nrouYUw.jpeg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*XVDMQb8i2pQEUogiZipFbQ.jpeg",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*VuIeZSMzyRqXuXpfSzM_yg.jpeg",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*GWv-z2hXM6RiW6sRJKzYCg.png",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*DuwP-cqJEnQcbD64RGUTYA.png",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<p>How Airbnb hardened Mussel, our key-value store, with smarter traffic controls to stay fast and reliable during traffic spikes.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*kBx_8QLd7El4TZ2nrouYUw.jpeg\" /></figure><p>By <a href=\"https://www.linkedin.com/in/shravangaonkar/\">Shravan Gaonkar</a>, <a href=\"https://www.linkedin.com/in/caseygetz/\">Casey Getz</a>, <a href=\"https://www.linkedin.com/in/wonheec/\">Wonhee Cho</a></p><h3>Introduction</h3><p>Every request lookup on Airbnb, from stays, experiences, and services search to customer support inquiries ultimately hits <a href=\"https://medium.com/airbnb-engineering/mussel-airbnbs-key-value-store-for-derived-data-406b9fa1b296\">Mussel</a>, our multi-tenant key-value store for derived data. Mussel operates as a proxy service, deployed as a fleet of stateless dispatchers — each a Kubernetes pod. On a typical day, this fleet handles millions of predictable point and range reads. During peak events, however, it must absorb several-fold higher volume, terabyte-scale bulk uploads, and sudden bursts from automated bots or DDoS attacks. Its ability to reliably serve this volatile mix of traffic is therefore critical to both the Airbnb user experience and the stability of the many services that power our platform.</p><p>Given Mussel’s traffic volume and its role in core Airbnb flows, <a href=\"https://en.wikipedia.org/wiki/Quality_of_service\">quality of service</a> (QoS) is one of the product’s defining features. The first-generation QoS system was primarily an isolation tool. It relied on a Redis-backed counter, client quota based rate-limiter, that checked a caller’s requests per second (QPS) against a configurable fixed quota. The goal was to prevent a single misbehaving client from overwhelming the service and causing a complete outage. For this purpose, it was simple and effective.</p><p>However, as the service matured, our goal shifted from merely preventing meltdowns to maximizing goodput — that is, getting the most useful work done without degrading performance. A system of fixed, manually configured quotas can’t achieve this, as it can’t adapt in real time to shifting traffic patterns, new query shapes, or sudden threats like a DDoS attack. A truly effective QoS system needs to be adaptive, automatically exerting prioritized backpressure when it senses the system has reached its useful capacity.</p><p>To better match our QoS system to the realities of online traffic and maximize goodput, over time we evolved it to add several new layers.</p><ul><li><strong>Resource-aware rate control (RARC)</strong>: Charges each request in <em>request units</em> (RU) that reflect rows, bytes, and latency, not just counts.</li><li><strong>Load shedding with criticality tiers</strong>: Guarantees that high-priority traffic (e.g., customer support, trust and safety) stays responsive when capacity evaporates.</li><li><strong>Hot-key detection &amp; DDoS mitigation</strong>: Detects skewed access patterns in real time and then shields the backend — whether the surge is legitimate or a DDoS burst — by caching or coalescing the duplicate requests before they reach the storage layer.</li></ul><p>What follows is an engineer’s view of how these layers were designed, deployed, and battle-tested, and why the same ideas may apply to any multi-tenant system that has outgrown simple QPS limits.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*XVDMQb8i2pQEUogiZipFbQ.jpeg\" /><figcaption>Progression Timeline</figcaption></figure><h3>Background: Life with Client Quota Rate Limiter</h3><p>When Mussel launched, rate-limiting was entirely handled via simple QPS rate-limiting using a Redis-based distributed counter service. Each caller received a static, per-minute quota, and the dispatcher incremented a Redis key for every incoming request. If the key’s value exceeded the caller’s quota, the dispatcher returned an HTTP 429. The design was simple, predictable, and easy to operate.</p><p>Two architectural details made this feasible. First, Mussel and its storage engine were tightly coupled; backend effort correlated reasonably well with the number of calls at the front door. Second, the traffic mix was modest in size and variety, so a single global limit per caller rarely caused trouble.</p><p>As adoption grew, two limitations became clear.</p><ol><li><strong>Cost variance:</strong> A one-row lookup and a 100,000-row scan were treated equally, even though their load on the backend differed by orders of magnitude. The system couldn’t distinguish high-value cheap work from low-value expensive work.</li><li><strong>Traffic skew:</strong> Per-caller rate limits provided isolation at the client level, but were blind to the data’s access pattern. When a single key became “hot” — for example, a popular listing accessed by thousands of different callers simultaneously — the aggregate traffic could overwhelm the underlying storage shard, even if each individual caller remained within its quota. This created a localized bottleneck that degraded performance for the entire cluster, impacting clients requesting completely unrelated data. Isolation by <em>caller</em> was insufficient to prevent this kind of resource contention.</li></ol><p>Addressing these gaps meant shifting from a <em>request-counting</em> mindset to a <em>resource-accounting</em> mindset and designing controls that reflect the real cost of each operation.</p><h3>Resource-aware rate control</h3><p>A fair quota system must account for the real work a request imposes on the storage layer. Resource-aware rate control (RARC) meets this need by charging operations in <em>request units</em> (RU) rather than raw requests per second.</p><p>A request unit blends four observable factors: fixed per-call overhead, rows processed, payload bytes, and — crucially — latency. Latency captures effects that rows and bytes alone miss: two one-megabyte reads can differ greatly in cost if one hits cache and the other triggers disk. In practice, we use a linear model. For both reads and writes, the cost is:</p><pre><br>RU_read = 1 + w_r × bytes_read + w_l × latency_ms<br>RU_write = 6 + w_b × bytes_written / 4096 bytes + w_l × latency_ms<br><br>Weight factors w_r, w_b, and w_l come from load-test calibration <br>based on the compute, network and disk I/O. <br>bytes_read, bytes_written and latency is measured per request</pre><p>Although approximate, the formula separates operations whose surface metrics look similar yet load the backend very differently.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*VuIeZSMzyRqXuXpfSzM_yg.jpeg\" /><figcaption>Impact of Latency on RU computation</figcaption></figure><p>Each dispatcher continues to rely on rate-limiter for distributed counting, but the counter now represents request-unit tokens instead of raw QPS. At the start of every epoch, the dispatcher adds the caller’s static RU quota to a local token bucket and immediately debits that bucket by the RU cost of each incoming request. When the bucket is empty, the request is rejected with HTTP 419. Because all dispatchers follow the same procedure and epochs are short, their buckets remain closely aligned without additional coordination.</p><p>Adaptive protection is handled in the separate load-shedding layer; backend latency influences which traffic is dropped or delayed, not the size of the periodic RU refill. This keeps rate accounting straightforward — static quotas expressed in request units — while still reacting quickly when the storage layer shows signs of stress.</p><h3>Load shedding: Staying healthy when capacity evaporates or develops hotspots</h3><p>Rate limits based on request units excel at smoothing normal traffic, but they adjust on a scale of seconds. When the workload shifts faster — a bot floods a key, a shard stalls, or a batch job begins a full-table scan — those seconds are enough for queues to balloon and service-level objectives to slip. To bridge this reaction-time gap, Mussel uses a load-shedding safety net that combines three real-time signals: (1) traffic criticality, (2) a latency ratio, and (3) a CoDel-inspired queueing policy.</p><p>The latency ratio is a ratio that serves as a real-time indicator of stress on the system stress. Each dispatcher computes this ratio by dividing the long-term p95 latency by the short-term p95 latency. A stable system has a ratio near 1.0; a value dropping towards 0.3 indicates that latency is rising sharply. When that threshold is crossed, the dispatcher temporarily increases the RU cost applied to a designated client class so that its token bucket drains faster and the request rate naturally backs off. If the ratio keeps falling, the same penalty can be expanded to additional classes until latency returns to a safe range.</p><p>The estimate uses the constant-memory P² algorithm [1], requiring no raw sample storage or cross-node coordination.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*GWv-z2hXM6RiW6sRJKzYCg.png\" /><figcaption>Latency response over time and illustration of throttling</figcaption></figure><p>The Control-Delay (CoDel) thread pool tackles the second hazard: queue buildup <em>inside the dispatcher itself</em> [2]. It monitors the time a request <em>waits</em> in the queue. If that sojourn time proves the system is already saturated, the request fails early, freeing up memory and threads for higher-priority work. An optional latency penalty can also be applied to RU accounting, charging more for queries from callers that persistently trigger the latency ratio.</p><p>Together, these layers — criticality, a real-time latency ratio, and adaptive queueing — form a shield that lets guest-facing traffic ride out backend hiccups. In practice, this system has cut recovery times by about half and keeps dispatchers stable without human intervention.</p><h3>Hot-key detection and DDoS defence</h3><p>Request-unit limits and load shedding keep client usage fair, but they cannot stop a stampede of identical reads aimed at one record. Imagine a listing that hits the front page of a major news outlet: tens of thousands of guests refresh their browser, all asking for the same key. A misconfigured crawler — or a deliberate botnet — can generate the same access pattern, only faster. The result is shard overload, a full dispatcher queue, and rising latency for unrelated work.</p><p>Mussel neutralises this amplification with a three-step<strong> </strong>hot-key defence layer<strong>:</strong> real-time detection, local caching, and request coalescing.</p><h3>Real-time detection in constant space</h3><p>Every dispatcher streams incoming keys into an in-memory <em>top-k</em> counter. The counter is a variant of the Space-Saving algorithm [2] popularized in Brian Hayes’s “Britney Spears Problem” essay [4]. In just a few megabytes, it tracks approximate hit counts, maintains a frequency-ordered heap, and surfaces the hottest keys in real time in each individual dispatcher.</p><h3>Local caching and request coalescing</h3><p>When a key crosses the hot threshold, the dispatcher serves it from a process-local LRU cache. Entries expire after roughly three seconds, so they vanish as soon as demand cools; no global cache is required. A cache miss can still arrive multiple times in the same millisecond, so the dispatcher tracks in-flight reads for hot keys. New arrivals attach to the pending future; the first backend response then fans out to all waiters. In most cases only <em>one</em> request per hot key per dispatcher pod ever reaches the storage layer.</p><h3>Impact in production</h3><p>In a controlled DDoS drill that targeted a small set of keys at ≈ million-QPS scale, the hot-key layer collapsed the burst to a trickle — each dispatcher forwarded only an occasional request, well below the capacity of any individual shard — so the backend never felt the surge.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*DuwP-cqJEnQcbD64RGUTYA.png\" /><figcaption>Hotkeys detected and served from dispatcher cache in real time</figcaption></figure><h3>Retrospective and key takeaways</h3><p>The journey from a single QPS counter to a layered, cost-aware QoS stack has reshaped how Mussel handles traffic and, just as importantly, how engineers think about fairness and resilience. A few themes surface when we look back across the stages described above.</p><p>The first is the value of early, visible impact. The initial release of request-unit accounting went live well before load shedding or hot-key defence. Soon after deployment it automatically throttled a caller whose range scans had been quietly inflating cluster latency. That early win validated the concept and built momentum for the deeper changes that followed.</p><p>A second lesson is to prefer to keep control loops local. All the key signals — P² latency quantiles, the Space-Saving top-k counter, and CoDel queue delay — run entirely inside each dispatcher. Because no cross-node coordination is required, the system scales linearly and continues to protect capacity even if the control plane is itself under stress.</p><p>Third, effective protection works on<strong> </strong>two different time-scales<strong>.</strong> Per-call RU pricing catches micro-spikes; the latency ratio and CoDel queue thresholds respond to macro slow-downs. Neither mechanism alone would have kept latency flat during the last controlled DDoS drill, but in concert they absorbed the shock and recovered within seconds.</p><p>Finally, QoS is a living system. Traffic patterns evolve, back-end capabilities improve, and new workloads appear. Planned next steps include database-native resource groups and automatic quota tuning from thirty-day usage curves. The principles that guided this project — measure true cost, react locally and quickly, layer defences — form a durable template, but the implementation will continue to grow with the platform it protects.</p><p>Does this type of work interest you? We’re hiring, check out open roles <a href=\"https://careers.airbnb.com/\">here</a>.</p><h3>📚 References</h3><ol><li>Raj Jain and Imrich Chlamtac. 1985. The P² algorithm for dynamic calculation of quantiles and histograms without storing observations. <em>Communications of the ACM</em>, <strong>28</strong>(10), 1076–1085.<a href=\"https://doi.org/10.1145/4372.4378\"> https://doi.org/10.1145/4372.4378</a></li><li>Erik D. Demaine, Alejandro López-Ortiz, and J. Ian Munro. 2002. Frequency estimation of internet packet streams with limited space. In <em>Algorithms — ESA 2002: 10th Annual European Symposium</em>, Rome, Italy, September 17–21, 2002. Rolf H. Möhring and Rajeev Raman (Eds.). <em>Lecture Notes in Computer Science</em>, Vol. <strong>2461</strong>. Springer, 348–360.</li><li>Kathleen M. Nichols and Van Jacobson. 2012. Controlling queue delay. <em>Communications of the ACM</em>, <strong>55</strong>(7), 42–50.<a href=\"https://doi.org/10.1145/2209249.2209264\"> https://doi.org/10.1145/2209249.2209264</a></li><li>Brian Hayes. 2008. Computing science: The Britney Spears problem. <em>American Scientist</em>, <strong>96</strong>(4), 274–279. <a href=\"https://www.americanscientist.org/article/the-britney-spears-problem\">https://www.americanscientist.org/article/the-britney-spears-problem</a></li></ol><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=29362764e5c2\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/from-static-rate-limiting-to-adaptive-traffic-management-in-airbnbs-key-value-store-29362764e5c2\">From Static Rate Limiting to Adaptive Traffic Management in Airbnb’s Key-Value Store</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "在 Airbnb 构建下一代键值存储 (原标题: Building a Next-Generation Key-Value Store at Airbnb)",
      "link": "https://medium.com/airbnb-engineering/building-a-next-generation-key-value-store-at-airbnb-0de8465ba354?source=rss----53c7c27702d5---4",
      "pubDate": "Wed, 24 Sep 2025 16:02:09 GMT",
      "isoDate": "2025-09-24T16:02:09.000Z",
      "creator": "Shravan Gaonkar",
      "summary": "# Airbnb 下一代键值存储 Mussel 的构建\n\nAirbnb 重新设计了其核心键值存储 Mussel，从 v1 升级到 v2，以满足现代数据需求。Mussel 旨在连接离线和在线工作负载，提供高可扩展的批量加载能力和个位数毫秒级的读取速度。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/1*sUS0d9nGKa-WQupVS8Wb4g.jpeg)\n\n## 为什么进行重新架构？\n\nMussel v1 曾可靠支持 Airbnb 多年，但新的业务需求（如实时欺诈检查、即时个性化、动态定价和海量数据处理）要求一个能结合实时流处理与批量摄取、且易于管理的平台。\n\n## v1 的主要挑战\n\nMussel v2 旨在解决 v1 的以下问题：\n\n*   **操作复杂性：** v1 扩展或替换节点需手动执行多步 Chef 脚本；v2 采用 Kubernetes manifests 和自动化部署，将数小时工作缩短至数分钟。\n*   **容量与热点：** v1 的静态哈希分区可能导致节点过载和延迟峰值；v2 的动态范围分片和预分片确保了即使对于 100TB+ 的表，读取速度依然快速（p99 < 25ms）。\n*   **一致性灵活性：** v1 提供有限的一致性控制；v2 允许团队根据 SLA 需求选择即时或最终一致性。\n*   **成本与透明度：** v1 的资源使用不透明；v2 增加了命名空间租用、配额强制和仪表板，提供成本可见性和控制。\n\n## 新架构：Mussel v2\n\n![图片 2](https://cdn-images-1.medium.com/max/1024/1*yjXZcYHPpQdl-peEhfEAmA.png)\n\nMussel v2 是一个彻底的重新架构，旨在解决 v1 的操作和可扩展性挑战。它被设计为自动化、可维护和可扩展，同时确保功能对等，并为 100 多个现有用例提供简便的迁移。\n\n### 调度器 (Dispatcher)\n\n*   无状态、可水平扩展的 Kubernetes 服务，取代了 v1 紧耦合、协议特定的设计。\n*   将客户端 API 调用转换为后端查询/修改。\n*   支持双写和影子读模式以进行迁移。\n*   管理重试和速率限制，并与 Airbnb 的服务网格集成以实现安全和服务发现。\n*   **读取：** 每个数据名称映射到一个逻辑表，支持优化的点查找、范围/前缀查询，以及从本地副本进行陈旧读取以降低延迟。动态限流和优先级管理确保了在流量变化下的性能。\n*   **写入：** 首先持久化到 Kafka 以确保持久性，然后由 Replayer 和 Write Dispatcher 按顺序应用到后端。这种事件驱动模型吸收突发流量，确保一致性，并消除了 v1 的操作开销。Kafka 也支持升级、引导和迁移。\n\n### 批量加载 (Bulk Load)\n\n*   保留了 v1 的语义，支持“合并”（添加到现有表）和“替换”（交换数据集）操作。\n*   使用基于 Airflow 的现有接口进行数据导入，将数据仓库数据转换为标准化格式并上传到 S3。\n*   无状态控制器协调作业，分布式有状态工作节点集群（Kubernetes StatefulSets）并行执行摄取，将 S3 中的记录加载到表中。\n*   通过去重、增量合并和插入时重复键忽略等优化，确保了 Airbnb 规模下的高吞吐量和高效写入。\n\n### 数据生命周期管理 (TTL)\n\n*   v2 引入了拓扑感知的过期服务，将数据命名空间分片为基于范围的子任务，由多个工作节点并发处理。\n*   过期记录被并行扫描和删除，最大限度地减少了大型数据集的清理时间。\n*   子任务的调度旨在限制对实时查询的影响。\n*   写入密集型表使用最大版本强制和定向删除来维护性能和数据卫生。\n\n## 迁移过程\n\n### 挑战\n\nMussel 存储海量数据，服务于 Airbnb 数千个表，承载着关键任务的读写流量。迁移目标是：将所有数据和流量从 Mussel v1 迁移到 v2，实现零数据丢失和对客户可用性零影响。\n\n### 过程\n\n采用了蓝/绿迁移策略，但由于 v1 不提供表级快照或 CDC 流，因此增加了复杂性。\n\n*   **自定义迁移管道：** 开发了自定义管道，能够引导表到 v2。\n*   **双写：** 一旦表被引导，就启用双写，使 v2 与 v1 保持同步。\n*   **阶段划分：**\n    1.  **蓝色区域 (Blue Zone)：** 所有流量流向 v1。\n    2.  **影子模式 (Shadowing - Green)：** v2 开始影子 v1，并行处理读写，但仅 v1 响应。用于检查 v2 的正确性和性能。\n    3.  **反向模式 (Reverse)：** v2 接管活跃流量，v1 保持待机。内置自动断路器和回退逻辑。\n    4.  **切换 (Cutover)：** v2 通过所有检查后，按数据名称逐一完成切换，Kafka 作为写入可靠性的中间件。\n*   **风险降低：** 迁移逐表进行，每一步都可逆，并可根据表的风险配置文件进行微调。\n\n### 迁移管道详情\n\n![图片 3](https://cdn-images-1.medium.com/max/1024/1*4Q-yjBQu8jwWSv0pkkNIHQ.jpeg)\n\nv1 架构使用 Kafka 作为复制日志。在数据迁移到 v2 期间，利用相同的 Kafka 流来维护 v1 和 v2 之间的最终一致性。\n\n自定义管道步骤：\n\n1.  **源数据采样：** 从 v1 下载备份数据，提取相关表并采样以了解数据分布。\n2.  **在 v2 上创建预分片表：** 根据采样结果，创建具有预定义分片布局的 v2 表，以最小化迁移期间的数据重排。\n3.  **引导 (Bootstrap)：** 最耗时步骤，使用 Kubernetes StatefulSets 持久化本地状态并定期检查点进度。\n4.  **校验和验证：** 验证 v1 备份中的所有数据是否已正确摄取到 v2。\n5.  **追赶 (Catch-up)：** 应用在引导阶段 Kafka 中累积的任何滞后消息。\n6.  **双写：** v1 和 v2 从相同的 Kafka 主题消费，确保最终一致性。\n7.  **读流量迁移：** 调度器动态配置，逐步将读请求从 v1 切换到 v2，并进行影子请求和回退。\n\n## 经验教训\n\n*   **一致性复杂性：** 从最终一致性 (v1) 到强一致性 (v2) 引入了新的挑战，需要写入去重、热键阻塞和延迟写入修复。\n*   **预分片至关重要：** 从基于哈希 (v1) 到基于范围 (v2) 的分区，需要准确采样 v1 数据并预分片，以确保迁移期间后端节点的均衡摄取流量。\n*   **查询模型调整：** v2 不像 v1 那样有效地向下推送范围过滤器，需要实现客户端分页。\n*   **新鲜度与成本：** 不同用例需要不同的权衡，例如使用主副本获取最新数据或使用辅助副本平衡陈旧度与成本/性能。\n*   **Kafka 的作用：** Kafka 稳定的 p99 毫秒级延迟使其成为迁移过程中不可或缺的一部分。\n*   **构建灵活性：** 客户重试和常规批量作业提供了安全网，迁移设计允许按表分配阶段和即时可逆性，这对于大规模风险管理至关重要。\n\n最终，Airbnb 成功迁移了超过 PB 级的数据，涉及数千个表，实现了零停机和零数据丢失。\n\n## 结论与展望\n\nMussel v2 融合了通常仅限于独立专业系统的功能。它能够同时：\n\n*   批量上传数十 TB 数据。\n*   在同一集群中每秒处理 100k+ 流式写入。\n*   保持 p99 读取延迟低于 25 毫秒。\n*   允许调用者按命名空间切换陈旧读取。\n\n通过将 NewSQL 后端与 Kubernetes 原生控制平面结合，Mussel v2 提供了对象存储的弹性、低延迟缓存的响应能力和现代服务网格的可操作性。工程师无需再拼凑缓存、队列和数据存储来满足 SLA，Mussel 开箱即用，让团队专注于产品创新。\n\n未来，Airbnb 将分享更多关于 Mussel 中 QoS 管理的演进以及大规模批量加载优化的见解。",
      "shortSummary": "Airbnb 重新架构了其核心键值存储 Mussel，从 v1 升级到基于 NewSQL 后端和 Kubernetes 的 v2。此举旨在解决 v1 的操作复杂性、可扩展性、一致性及成本透明度问题。Mussel v2 实现了高可扩展性、低延迟读取和高效批量加载，并支持灵活的一致性模型。通过蓝/绿部署、双写和自定义迁移管道，Airbnb 成功迁移了数 PB 数据，实现了零停机和零数据丢失，极大地简化了数据基础设施管理，使工程师能专注于产品创新。",
      "translated_title": "在 Airbnb 构建下一代键值存储",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*sUS0d9nGKa-WQupVS8Wb4g.jpeg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*yjXZcYHPpQdl-peEhfEAmA.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*4Q-yjBQu8jwWSv0pkkNIHQ.jpeg",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=0de8465ba354",
          "alt": "",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*sUS0d9nGKa-WQupVS8Wb4g.jpeg\" /></figure><p>How we completely rearchitected Mussel, our storage engine for derived data, and lessons learned from the migration from Mussel V1 to V2.</p><p>By <a href=\"https://www.linkedin.com/in/shravangaonkar/\">Shravan Gaonkar</a>, <a href=\"https://www.linkedin.com/in/chandramoulir/\">Chandramouli Rangarajan</a>, <a href=\"https://www.linkedin.com/in/yanhan-zhang/\">Yanhan Zhang</a></p><p>How we completely rearchitected Mussel, our storage engine for derived data, and lessons learned from the migration from Mussel V1 to V2.</p><p>Airbnb’s core key-value store, internally known as Mussel, bridges offline and online workloads, providing highly scalable bulk load capabilities combined with single-digit millisecond reads.</p><p>Since first writing about Mussel in a 2022 <a href=\"https://medium.com/airbnb-engineering/mussel-airbnbs-key-value-store-for-derived-data-406b9fa1b296\">blog post</a>, we have completely deprecated the storage backend of the original system (what we now call Mussel v1) and have replaced it with a NewSQL backend which we are referring to as Mussel v2. Mussel v2 has been running successfully in production for a year, and we wanted to share why we undertook this rearchitecture, what the challenges were, and what benefits we got from it.</p><h3>Why rearchitect</h3><p>Mussel v1 reliably supported Airbnb for years, but new requirements — real-time fraud checks, instant personalization, dynamic pricing, and massive data — demand a platform that combines real-time streaming with bulk ingestion, all while being easy to manage.</p><h3>Key Challenges with v1</h3><p>Mussel v2 solves a number of issues with v1, delivering a scalable, cloud-native key-value store with predictable performance and minimal operational overhead.</p><ul><li><strong>Operational complexity:</strong> Scaling or replacing nodes required multi-step Chef scripts on EC2; v2 uses Kubernetes manifests and automated rollouts, reducing hours of manual work to minutes.</li><li><strong>Capacity &amp; hotspots:</strong> Static hash partitioning sometimes overloaded nodes, leading to latency spikes. V2’s dynamic range sharding and presplitting keep reads fast (p99 &lt; 25ms), even for 100TB+ tables.</li><li><strong>Consistency flexibility:</strong> v1 offered limited consistency control. v2 lets teams choose between immediate or eventual consistency based on their SLA needs.</li><li><strong>Cost &amp; Transparency:</strong> Resource usage in v1 was opaque. v2 adds namespace tenancy, quota enforcement, and dashboards, providing cost visibility and control.</li></ul><h3>New architecture</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*yjXZcYHPpQdl-peEhfEAmA.png\" /></figure><p>Mussel v2 is a complete re-architecture addressing v1’s operational and scalability challenges. It’s designed to be automated, maintainable, and scalable, while ensuring feature parity and an easy migration for 100+ existing user cases.</p><h3>Dispatcher</h3><p>In Mussel v2, the Dispatcher is a stateless, horizontally-scalable Kubernetes service that replaces the tightly coupled, protocol-specific design of v1. It translates client API calls into backend queries/mutations, supports dual-write and shadow-read modes for migration, manages retries and rate limits, and integrates with Airbnb’s service mesh for security and service discovery.</p><p><strong>Reads</strong> are simplified: Each dataname maps to a logical table, enabling optimized point lookups, range/prefix queries, and stale reads from local replicas to reduce latency. Dynamic throttling and prioritization maintain performance under changing traffic.</p><p><strong>Writes</strong> are persisted in Kafka for durability first, with the Replayer and Write Dispatcher applying them in order to the backend. This event-driven model absorbs bursts, ensures consistency, and removes v1’s operational overhead. Kafka also underpins upgrades, bootstrapping, and migrations until CDC and snapshotting mature.</p><p>The architecture suits derived data and replay-heavy use cases today, with a long-term goal of shifting ingestion and replication fully to the distributed backend database to bring down latency and simplify operations.</p><p>Bulk load<strong><br></strong>Bulk load remains essential for moving large datasets from offline warehouses into Mussel for low-latency queries. v2 preserves v1 semantics, supporting both “merge” (add to existing tables) and “replace” (swap datasets) semantics.</p><p>To maintain a familiar interface, v2 keeps the existing Airflow-based onboarding and transforms warehouse data into a standardized format, uploading to S3 for ingestion. <a href=\"https://airbnb.io/projects/airflow/\">Airflow</a> is an open-source platform for authoring, scheduling, and monitoring data pipelines. Created at Airbnb, it lets users define workflows in code as directed acyclic graphs (DAGs), enabling quick iteration and easy orchestration of tasks for data engineers and scientists worldwide.</p><p>A stateless controller orchestrates jobs, while a distributed, stateful worker fleet (Kubernetes StatefulSets) performs parallel ingestion, loading records from S3 into tables. Optimizations — like deduplication for replace jobs, delta merges, and insert-on-duplicate-key-ignore — ensure high throughput and efficient writes at Airbnb scale.</p><h3>TTL</h3><p>Automated data expiration (TTL) can help support data governance goals and storage efficiency. In v1, expiration relied on the storage engine’s compaction cycle, which struggled at scale.</p><p>Mussel v2 introduces a topology-aware expiration service that shards data namespaces into range-based subtasks processed concurrently by multiple workers. Expired records are scanned and deleted in parallel, minimizing sweep time for large datasets. Subtasks are scheduled to limit impact on live queries, and write-heavy tables use max-version enforcement with targeted deletes to maintain performance and data hygiene.</p><p>These enhancements provide the same retention functionality as v1 but with far greater efficiency, transparency, and scalability, meeting Airbnb’s modern data platform demands and enabling future use cases.</p><h3>The migration process</h3><h3>Challenge</h3><p>Mussel stores vast amounts of data and serves thousands of tables across a wide array of Airbnb services, sustaining mission-critical read and write traffic at high scale. Given the criticality of Mussel to Airbnb’s online traffic, our migration goal was straightforward but challenging: Move all data and traffic from Mussel v1 to v2 with zero data loss and no impact on availability to our customers.</p><h3>Process</h3><p>We adopted a blue/green migration strategy, but with notable complexities. Mussel v1 didn’t provide table-level snapshots or CDC streams, which are standard in many datastores. To bridge this gap, we developed a custom migration pipeline capable of bootstrapping tables to v2, selected by usage patterns and risk profiles. Once bootstrapped, dual writes were enabled on a per-table basis to keep v2 in sync as the migration progressed.</p><p>The migration itself followed several distinct stages:</p><ul><li><strong>Blue Zone:</strong> All traffic initially flowed to v1 (“Blue”). This provided a stable baseline as we migrated data behind the scenes.</li><li><strong>Shadowing (Green):</strong> Once tables were bootstrapped, v2 (“Green”) began shadowing v1 — handling reads/writes in parallel, but only v1 responded. This allowed us to check v2’s correctness and performance without risk.</li><li><strong>Reverse:</strong> After building confidence, v2 took over active traffic while v1 remained on standby. We built automatic circuit breakers and fallback logic: If v2 showed elevated error rates or lagged behind v1, we could instantly return traffic to v1 or revert to shadowing.</li><li><strong>Cutover:</strong> When v2 passed all checks, we completed the cutover on a dataname-by-dataname basis, with Kafka serving as a robust intermediary for write reliability throughout.</li></ul><p>To further de-risk the process, migration was performed one table at a time. Every step was reversible and could be fine-tuned per table or group of tables based on their risk profile. This granular, staged approach allowed for rapid iteration, safe rollbacks, and continuous progress without impacting the business.</p><h3>Migration pipeline</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*4Q-yjBQu8jwWSv0pkkNIHQ.jpeg\" /></figure><p>As described in our previous blog post, the v1 architecture uses Kafka as a replication log — data is first written to Kafka, then consumed by the v1 backend. During the data migration to v2, we leveraged the same Kafka stream to maintain eventual consistency between v1 and v2.</p><p>To migrate any given table from v1 to v2, we built a custom pipeline consisting of the following steps:</p><ol><li><strong>Source data sampling</strong>: We download backup data from v1, extract the relevant tables, and sample the data to understand its distribution.</li><li><strong>Create pre-split table on v2</strong>: Based on the sampling results, we create a corresponding v2 table with a pre-defined shard layout to minimize data reshuffling during migration.</li><li><strong>Bootstrap</strong>: This is the most time-consuming step, taking hours or even days depending on table size. To bootstrap efficiently, we use Kubernetes StatefulSets to persist local state and periodically checkpoint progress.</li><li><strong>Checksum verification</strong>: We verify that all data from the v1 backup has been correctly ingested into v2.</li><li><strong>Catch-up</strong>: We apply any lagging messages that accumulated in Kafka during the bootstrap phase.</li><li><strong>Dual writes</strong>: At this stage, both v1 and v2 consume from the same Kafka topic. We ensure eventual consistency between the two, with replication lag typically within tens of milliseconds.</li></ol><p>Once data migration is complete and we enter dual write mode, we can begin the read traffic migration phase. During this phase, our dispatcher can be dynamically configured to serve read requests for specific tables from v1, while sending shadow requests to v2 for consistency checks. We then gradually shift to serving reads from v2, accompanied by reverse shadow requests to v1 for consistency checks, which also enables quick fallback to v1 responses if v2 becomes unstable. Eventually, we fully transition to serving all read traffic from v2.</p><h3>Lessons learned</h3><p>Several key insights emerged from this migration:</p><ul><li><strong>Consistency complexity:</strong> Migrating from an eventually consistent (v1) to a strongly consistent (v2) backend introduced new challenges, particularly around write conflicts. Addressing these required features like write deduplication, hotkey blocking, and lazy write repair — sometimes trading off storage cost or read performance.</li><li><strong>Presplitting is critical:</strong> As we shifted from hash-based (v1) to range-based partitioning (v2), inserting large consecutive data could cause hotspots and disrupt our v2 backend. To prevent this, we needed to accurately sample the v1 data and presplit it into multiple shards based on v2’s topology, ensuring balanced ingestion traffic across backend nodes during data migration.</li><li><strong>Query model adjustments:</strong> v2 doesn’t push down range filters as effectively, requiring us to implement client-side pagination for prefix and range queries.</li><li><strong>Freshness vs. cost:</strong> Different use cases required different tradeoffs. Some prioritized data freshness and used primary replicas for the latest reads, while others leveraged secondary replicas to balance staleness with cost and performance.</li><li><strong>Kafka’s role:</strong> Kafka’s proven stable p99 millisecond latency made it an invaluable part of our migration process.</li><li><strong>Building in flexibility:</strong> Customer retries and routine bulk jobs provided a safety net for the rare inconsistencies, and our migration design allowed for per-table stage assignments and instant reversibility — key for managing risk at scale.</li></ul><p>As a result, we migrated more than a petabyte of data across thousands of tables with zero downtime or data loss, thanks to a blue/green rollout, dual-write pipeline, and automated fallbacks — so the product teams could keep shipping features while the engine under them evolved.</p><h3>Conclusion and next steps</h3><p>What sets Mussel v2 apart is the way it fuses capabilities that are usually confined to separate, specialized systems. In our deployment of Mussel V2, we observe that this system can simultaneously</p><ol><li>ingest tens of terabytes in bulk data upload,</li><li>sustain 100 k+ streaming writes per second in the same cluster, and</li><li>keep p99 reads under 25 ms</li></ol><p>— all while giving callers a simple dial to toggle stale reads on a per-namespace basis. By pairing a NewSQL backend with a Kubernetes-native control plane, Mussel v2 delivers the elasticity of object storage, the responsiveness of a low-latency cache, and the operability of modern service meshes — rolled into one platform. Engineers no longer need to stitch together a cache, a queue, and a datastore to hit their SLAs; Mussel provides those guarantees out of the box, letting teams focus on product innovation instead of data plumbing.</p><p>Looking ahead, we’ll be sharing deeper insights into how we’re evolving quality of service (QoS) management within Mussel, now orchestrated cleanly from the Dispatcher layer. We’ll also describe our journey in optimizing bulk loading at scale — unlocking new performance and reliability wins for complex data pipelines. If you’re passionate about building large-scale distributed systems and want to help shape the future of data infrastructure at Airbnb, take a look at our <a href=\"https://careers.airbnb.com/\">Careers page</a> — we’re always looking for talented engineers to join us on this mission.</p><h3>References</h3><ol><li><a href=\"https://medium.com/airbnb-engineering/mussel-airbnbs-key-value-store-for-derived-data-406b9fa1b296\">https://medium.com/airbnb-engineering/mussel-airbnbs-key-value-store-for-derived-data-406b9fa1b296</a></li></ol><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=0de8465ba354\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/building-a-next-generation-key-value-store-at-airbnb-0de8465ba354\">Building a Next-Generation Key-Value Store at Airbnb</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "Viaduct，五年回顾：数据导向型服务网格的现代化 (原标题: Viaduct, Five Years On: Modernizing the Data-Oriented Service Mesh)",
      "link": "https://medium.com/airbnb-engineering/viaduct-five-years-on-modernizing-the-data-oriented-service-mesh-e66397c9e9a9?source=rss----53c7c27702d5---4",
      "pubDate": "Wed, 17 Sep 2025 17:01:58 GMT",
      "isoDate": "2025-09-17T17:01:58.000Z",
      "creator": "Adam Miskiewicz",
      "summary": "![图片 1](https://cdn-images-1.medium.com/max/1024/1*q_owFEOLfQlioFXHP7UqBA.avif)\n\n本文回顾了Airbnb数据导向型服务网格Viaduct五年来的发展，并宣布其已开源。Viaduct自2020年以来取得了显著增长，流量增长了八倍，托管代码的团队数量翻倍至130多个，代码库规模扩大了两倍多，同时保持了运营开销不变、事故时间减半，并使成本与QPS线性增长。\n\n## Viaduct 的核心原则（保持不变）\n\nViaduct自成立以来一直遵循三个核心原则：\n\n*   **中心化 Schema：** Viaduct提供一个单一、集成的Schema，连接公司所有领域。尽管Schema由多个团队分散开发，但它是一个高度互联的图。超过75%的Viaduct请求是内部请求，因为它已成为连接开发者与所有数据和能力的“一站式”数据网格。\n*   **托管业务逻辑：** 团队被鼓励直接在Viaduct中托管业务逻辑，这与GraphQL服务器作为微服务薄层的常见做法不同。Viaduct提供了一个无服务器平台，让开发者专注于业务逻辑编写，而非运维问题。\n*   **可重入性（Re-entrancy）：** Viaduct上托管的逻辑可以通过发布GraphQL片段和查询来与其他逻辑组合。这对于维护大型代码库的模块化和避免传统单体架构的风险至关重要。\n\n## Viaduct 的演变：“Viaduct Modern”现代化计划\n\nViaduct在大部分历史中是自下而上、响应开发者需求演进的，这导致了多种实现方式、混乱的开发者体验和架构完整性不足。为解决这些问题，Airbnb启动了“Viaduct Modern”计划，对面向开发者的API和执行引擎进行了彻底改造。\n\n### 租户 API 简化\n\n*   **旧模型：** 编程模型复杂，开发者面临多种实现功能的机制（见下图）。\n    ![图片 2](https://cdn-images-1.medium.com/max/1024/1*0Gujn6vAspuKh3ZuPjlK-g.png)\n    *Viaduct 原始的复杂编程模型*\n*   **新模型：** 显著简化，只提供两种机制：节点解析器（node resolvers）和字段解析器（field resolvers）。选择由Schema本身驱动，而非基于功能行为的随意区分（见下图）。\n    ![图片 3](https://cdn-images-1.medium.com/max/1024/1*xuL1H_ylQR5PU2eh2JuXEA.png)\n    *Viaduct Modern 的简化模型*\n*   API在两种解析器类型之间尽可能统一，提升了开发体验。\n\n### 租户模块化\n\n*   通过模块和可重入性实现强大的抽象边界。\n*   从模糊的代码组织约定演变为正式的“租户模块”概念：一个Schema单元及其实现代码，由单个团队拥有。\n*   模块之间通过GraphQL片段和查询进行组合，而非直接的代码依赖。\n*   **示例：** “核心用户”团队定义`User`类型，而“消息”团队可以通过`extend type User { displayName: String @resolver}`扩展`User`类型，并声明其`displayName`字段的解析器，无需了解`firstName`和`lastName`字段的来源。\n\n### 框架模块化\n\n*   目标是使框架本身更具模块化，以便更快地改进性能和可靠性，同时减少对应用代码的修改。\n*   Viaduct由GraphQL执行引擎、租户API和托管应用代码三层组成。\n*   **新设计：** 在这些层之间创建了强大的抽象边界（见下图）。\n    ![图片 4](https://cdn-images-1.medium.com/max/1024/1*qvOOmI-Hs7-PMa52kSACqQ.png)\n    *Viaduct Modern 的模块化设计*\n*   最显著的变化是引擎与面向开发者的租户API之间的边界。引擎API的核心是GraphQL值的动态类型表示，而租户API是静态类型的（通过生成的Kotlin类）。生成的类型是动态表示的薄封装。这种分离允许引擎和租户API独立演进。\n\n### 平滑迁移\n\n*   为实现渐进式迁移，新的Modern API和现有的Classic API在新引擎之上并行运行。\n*   团队可以立即获得引擎带来的性能/成本优势，并逐步采用Modern API的改进。\n*   同时构建两个API也促使引擎API设计更加通用和清晰。\n\n## 其他改进\n\n自2020年以来，Viaduct还进行了其他改进：\n\n*   **可观测性：** 新架构清晰的边界使得所有权和归因更加精确。\n*   **构建时间：** 采用Schema优先、直接生成字节码等方式优化构建时间，Viaduct Modern的模块化也有助于控制构建时间。\n*   **调度器：** 作为水平扩展的Kubernetes应用运行，使用调度器将操作路由到部署分片，实现混洗分片，并简化离线/在线流量隔离（目前不开源）。\n\n## Viaduct 开源\n\nAirbnb从“Viaduct Modern”项目启动之初就计划将其开源，旨在提升软件质量、回馈开源社区，并希望通过更广泛社区的贡献来改进Viaduct。目前，新引擎已全面投入生产，而新的租户API仍处于Alpha阶段。Airbnb鼓励社区参与，共同塑造API。\n\n## Viaduct 适用性\n\nViaduct已被证明可以扩展到大规模图，同时也适合作为初学者友好的GraphQL服务器。它从一开始就强调开发者体验，并提供了一个优秀的GraphQL解决方案构建环境。",
      "shortSummary": "Airbnb宣布将其数据导向型服务网格Viaduct开源。过去五年，Viaduct在Airbnb内部实现了显著增长，流量增长八倍，团队数量翻倍。为解决早期演进中的复杂性，Viaduct推出了“Viaduct Modern”计划，彻底改造了开发者API和执行引擎，简化了编程模型，增强了模块化，并实现了平滑迁移。Viaduct基于中心化Schema、托管业务逻辑和可重入性三大原则，旨在提供强大的、开发者友好的GraphQL解决方案。开源旨在提升软件质量并吸纳社区贡献。",
      "translated_title": "Viaduct，五年回顾：数据导向型服务网格的现代化",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*q_owFEOLfQlioFXHP7UqBA.avif",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*0Gujn6vAspuKh3ZuPjlK-g.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*xuL1H_ylQR5PU2eh2JuXEA.png",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*qvOOmI-Hs7-PMa52kSACqQ.png",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e66397c9e9a9",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*q_owFEOLfQlioFXHP7UqBA.avif\" /></figure><h4>A more powerful engine and a simpler API for our data-oriented mesh</h4><p>By: Adam Miskiewicz, Raymie Stata</p><p>In November 2020 we <a href=\"https://medium.com/airbnb-engineering/taming-service-oriented-architecture-using-a-data-oriented-service-mesh-da771a841344\">published</a> a post about Viaduct, our data-oriented service mesh. Today, we’re excited to announce Viaduct is available as open-source software (OSS) at <a href=\"https://github.com/airbnb/viaduct\">https://github.com/airbnb/viaduct</a>.</p><p>Before we talk about OSS, here’s a quick update on Viaduct’s adoption and evolution at Airbnb over the last five years. Since 2020, traffic through Viaduct has grown by a factor of eight. The number of teams hosting code in Viaduct has doubled to 130+ (with hundreds of weekly active developers). The codebase hosted by Viaduct has tripled to over <strong>1.5M</strong> lines (plus about the same in test code). We’ve achieved all this while keeping operational overhead constant, halving incident-minutes, and keeping costs growing linearly with QPS.</p><h3>What’s the same?</h3><p>Three principles have guided Viaduct since day one and still anchor the project: a <strong>central schema</strong> served by <strong>hosted business logic</strong> via a <strong>re-entrant</strong> API.</p><p><strong>Central schema <br></strong>Viaduct serves our central schema: a single, integrated schema connecting all of our domains across the company. While that schema is developed in a <em>decentralized</em> manner by many teams, it’s one, highly connected graph. Over 75% of Viaduct requests are internal because Viaduct has become a “one‑stop” data-oriented mesh connecting developers to all of our data and capabilities.</p><p><strong>Hosted business logic <br></strong>From the beginning, we’ve encouraged teams to host their business logic directly in Viaduct. This runs counter to what many consider to be best practices in GraphQL, which is that GraphQL servers should be a thin layer over microservices that host the real business logic. We’ve created a serverless platform for hosting business logic, allowing our developers to focus on writing business logic rather than on operational issues. As noted by Katie, an engineer on our Media team:</p><blockquote>“As we migrate our media APIs into Viaduct, we’re looking forward to retiring a handful of standalone services. Centralizing everything means less overhead, fewer moving parts, and a much smoother developer experience!”</blockquote><p><strong>Re-entrancy</strong></p><p>At the heart of our developer experience is what we call <em>re-entrancy</em>: Logic hosted on Viaduct composes with other logic hosted on Viaduct by issuing GraphQL fragments and queries. Re-entrancy has been crucial for maintaining modularity in a large codebase and avoiding classic monolith hazards.</p><h3>What’s changed?</h3><p>For most of Viaduct’s history, evolution has been bottom-up and reactive to immediate developer needs. We added capabilities incrementally, which helped us move fast, but also produced multiple ways to accomplish similar tasks (some well‑supported, others not) and created a confusing developer experience, especially for new teams. Another side-effect of this reactive approach has been a lack of architectural integrity. The interfaces between the layers of Viaduct, described in more detail below, are loose and often arbitrary, and the abstraction boundary between the Viaduct framework and the code that it hosts is weak. As a result, it has become increasingly difficult to make changes to Viaduct without disrupting our customer base.</p><p>To address these issues, over a year ago we launched a major initiative we call <strong>“Viaduct Modern”</strong>, a ground-up overhaul of both the developer-facing API and the execution engine.</p><h3>Tenant API</h3><p>One driving principle of Viaduct Modern has been to simplify and rationalize the API we provide to developers in Viaduct, which we call the <strong>“Tenant API”</strong>. The following diagram captures the decision tree one faced when deciding how to implement functionality in the old API:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*0Gujn6vAspuKh3ZuPjlK-g.png\" /><figcaption>Viaduct’s original complex programming model</figcaption></figure><p>Each oval in this diagram represents a different mechanism for writing code. In contrast, the new API offers just two mechanisms: node resolvers and field resolvers.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*xuL1H_ylQR5PU2eh2JuXEA.png\" /><figcaption>Viaduct Modern’s simpler model</figcaption></figure><p>The choice between the two is driven by the schema itself, not ad‑hoc distinctions based on a feature’s behavior. We unified the APIs for both resolver types wherever possible, which simplifies dev experience. After four years evolving the API in a use‑case‑driven manner, we distilled the best ideas into a single simple surface (and left the mistakes behind).</p><h3>Tenant modularity</h3><p>Strong abstraction boundaries are essential in any large codebase. Microservices achieve this via service definitions and RPC API boundaries; Viaduct achieves it via <strong>modules</strong> plus <strong>re‑entrancy</strong>.</p><p>Modularity in the central schema and hosted code has evolved. Initially, all we had was a vague set of conventions for organizing code into team-owned directories. There was no formal concept of a module, and schema and code were kept in separate source directories with unenforced naming conventions to connect the two. Over time, we evolved that into a more formal abstraction we call a “tenant module.” A tenant module is a unit of schema together with the code that implements that schema, and crucially, is owned by a single team. While we encourage rich graph‑level connections across modules, we <strong>discourage direct code dependencies</strong> between modules. Instead, modules compose via GraphQL fragments and queries. Viaduct Modern extends and simplifies these re‑entrancy tools.</p><p>Let’s look at an example. Imagine two teams, a “Core User” team that owns and manages the basic profile data of users, and then a “Messaging” team that operates a messaging platform for users to interact with each other. In our example, the Messaging team would like to define a <em>displayName</em> field on a <em>User</em>, which is used in their user interface. This would look something like this:</p><p><strong>Core User team</strong></p><pre>type User implements Node {<br>  id: ID!<br>  firstName: String<br>  lastName: String<br>  … <br>}</pre><pre>class UserResolver : Nodes.User() {<br>    @Inject<br>    val userClient: UserServiceClient<br><br>    @Inject<br>    val userResponseMapper: UserResponseMapper<br><br>    override suspend fun resolve(ctx.Context): User {<br>        val r = userClient.fetch(ctx.id)<br>        return userResponseMapper(r)<br>    } <br>}</pre><p>This is the base definition of the <em>User</em> type that lives in the Core User team’s module. This base definition defines the first- and last-name fields (among many others), and it’s the Core User team’s responsibility to materialize those fields.</p><p><strong>Messaging team</strong></p><pre>extend type User {<br>  displayName: String @resolver<br>}</pre><pre>@Resolver(&quot;firstName lastName&quot;)<br>class DisplayNameResolver : UserResolvers.DisplayName() {<br>    override suspend fun resolve(ctx: Context): String {<br>        val f = ctx.objectValue.getFirstName()<br>        val l = ctx.objectValue.getLastName()<br>        return &quot;$f ${l.first()}.&quot;<br>    }<br>}</pre><p>The Messaging team can then extend the <em>User </em>type with the display name field, and also indicates that they intend to provide a resolver for it. The code has an <em>@Resolver </em>annotation that indicates which fields of the <em>User</em> object it needs to implement the <em>displayName</em> field. The Messaging team doesn’t need to understand which module these fields come from, and their code doesn’t depend on code from the Core User team. Instead, the Messaging team states their data needs in a declarative fashion.</p><h3>Framework modularity</h3><p>A major goal of Viaduct Modern has been to make the framework itself more modular. We want to enable faster improvements to Viaduct — especially regarding performance and reliability — without extensive changes to application code. Viaduct is composed of three main layers: the GraphQL execution engine, the tenant API, and the hosted application code. While these layers made sense, the interfaces between them were weak, making Viaduct difficult to update. The new design is focused on creating strong abstraction boundaries between these layers to improve flexibility and maintainability:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*qvOOmI-Hs7-PMa52kSACqQ.png\" /><figcaption>Viaduct Modern’s modular design</figcaption></figure><p>The most significant change is the boundary between the <strong>engine</strong> and the developer-facing <strong>tenant API</strong>. In the previous system, that boundary hardly existed. Viaduct Modern defines a strong <strong>engine API</strong> whose core is a <strong>dynamically‑typed</strong> representation of GraphQL values (input and output objects as simple maps from field name → value). The tenant API, by contrast, is <strong>statically typed</strong>: we generate Kotlin classes for every GraphQL type in the central schema. In the new architecture, generated types are thin wrappers over the dynamic representation. The tenant API forms the bridge between the engine’s untyped world and tenants’ typed world.</p><p>This separation lets us evolve the engine in relative isolation (to improve latency, throughput, and reliability) and evolve the tenant API in relative isolation (to improve dev experience). Large changes will still cross the boundary, but as Viaduct Modern stabilizes, that should be rare.</p><h3>Migration without a “big bang”</h3><p>Viaduct Modern would be a non‑starter if it required a step‑function migration of a million+ lines of code. To enable gradual migration, we’re shipping <strong>two tenant APIs</strong> side‑by‑side — the new <strong>Modern</strong> API and the existing <strong>Classic</strong> API — both on top of the new engine. This lets teams realize performance/cost wins from the engine immediately, while adopting the ergonomic wins of the Modern API over time.</p><p>Shipping two APIs has also improved the engine API design: building two tenant runtimes simultaneously forced us to keep the engine’s concerns clean and general. Over time, we expect to build additional tenant APIs on the engine (e.g., <strong>TypeScript</strong>).</p><h3>Other improvements</h3><p>Viaduct has seen a lot of other improvements since 2020. The story is too long to be told in this post, but to list some of the highlights:</p><ul><li><strong>Observability.</strong> Hosting software from 100+ teams means our framework has to make ownership and attribution crystal clear. In the old system, there is no clear dividing line between tenant code and framework code, so our instrumentation includes a bit of guesswork in its attribution to parties. The new architecture draws a crisp boundary, which enables deeper, more accurate attribution.</li><li><strong>Build time.</strong> We’re schema‑first: Developers write schema as source, and Viaduct generates code to provide a strongly typed, ergonomic surface. At our scale, build time is a constant battle. Over the years we’ve made numerous investments to improve build time, including direct-to-bytecode code generation that bypasses lengthy compilation of generated code. We anticipate that the improved modularity in Viaduct Modern will keep build times in check as the codebase grows.</li><li><strong>Dispatcher.</strong> We run Viaduct as a horizontally-scaled Kubernetes app. To mitigate blast radius, we use a dispatcher that routes operations to deployment shards, applying <a href=\"https://aws.amazon.com/blogs/architecture/shuffle-sharding-massive-and-magical-fault-isolation/\">shuffle sharding</a>. It also simplifies isolating offline vs. online traffic and hosting experimental framework builds. (We don’t currently plan to open‑source the dispatcher as it’s tightly tied to Airbnb’s serving framework, but we may talk more about our strategies here in the future!)</li></ul><h3>Open-sourcing Viaduct</h3><p>Our intent from the start of Viaduct Modern was to open‑source it. We believe that setting out to build software for the world will result in higher quality software for ourselves. Also, as a significant consumer of open-source software, we feel an obligation to give back. Last but not least, while we’ve learned a lot by working with Airbnb developers, we think Viaduct can be massively improved by incorporating ideas and contributions from the wider community.</p><p>We’re open‑sourcing at an interesting moment. Viaduct is <strong>mature and battle‑tested</strong> <em>and also</em> <strong>new and evolving</strong>. The new engine is now in full production, while the new tenant API is still in alpha. We’ve implemented a small but robust kernel of the new API and are using it in a few (demanding) use cases. We’re investing heavily in the Modern API and migrating our own workloads to it, but it’s early days. By open‑sourcing early, we hope to grow a community who will shape the API with us together.</p><h3>Is Viaduct for you?</h3><p>Although our use case proves Viaduct can scale to massive graphs, we think it’s also a great GraphQL server when you’re just starting. We’ve emphasized developer ergonomics from day one, and we believe Viaduct provides one of the best environments for building GraphQL solutions. Whether you’re operating a super-graph today or just kicking the tires, we’d love for you to try Viaduct Modern and tell us what works — and what doesn’t.</p><p>—</p><p>Thanks to the entire Viaduct team, and especially Aileen Chen and Raymie Stata, for the tireless work on Viaduct Modern.</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e66397c9e9a9\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/viaduct-five-years-on-modernizing-the-data-oriented-service-mesh-e66397c9e9a9\">Viaduct, Five Years On: Modernizing the Data-Oriented Service Mesh</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "使用数据导向的服务网格驯服面向服务的架构 (原标题: Taming Service-Oriented Architecture Using A Data-Oriented Service Mesh)",
      "link": "https://medium.com/airbnb-engineering/taming-service-oriented-architecture-using-a-data-oriented-service-mesh-da771a841344?source=rss----53c7c27702d5---4",
      "pubDate": "Tue, 16 Sep 2025 18:37:58 GMT",
      "isoDate": "2025-09-16T18:37:58.000Z",
      "creator": "Adam Miskiewicz",
      "summary": "# 使用数据导向的服务网格驯服面向服务的架构：Airbnb 的 Viaduct\n\nAirbnb 推出了一款名为 Viaduct 的数据导向服务网格，旨在显著提升其基于微服务的面向服务架构（SOA）的模块化水平。本文阐述了 Viaduct 的设计理念和工作原理。\n\n## 大规模 SOA 依赖图的挑战\n现代应用程序通常包含数千甚至数万个微服务，它们以不受约束的方式相互连接，导致复杂的依赖图。这种“意大利面条式 SOA”使得系统修改日益困难。为了管理庞大的微服务数量，需要新的组织原则和技术措施。Airbnb 的研究发现，数据导向的服务网格能够为 SOA 带来新的模块化水平。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/1*KUevf-1aGcLQyit3wzmh8w.jpeg)\n\n*图示：Airbnb 工程师 Raymie Stata, Arun Vijayvergiya, Adam Miskiewicz 介绍 Viaduct*\n\n![图片 2](https://cdn-images-1.medium.com/max/896/1*y2E1EmsS3paNhoChYGnJIw.png)\n\n*图示：一个典型的微服务依赖图，展示了复杂的服务间关系*\n\n## 过程导向与数据导向设计\n软件工程中模块化组织程序并非新问题：\n*   **过程导向设计（1970年代前）**：以过程和模块为中心，模块通过公共 API 暴露功能，隐藏内部实现（如 Pascal 和 C）。\n*   **数据导向设计（1980年代后）**：以数据为中心，模块定义对象类，通过公共方法 API 访问封装的内部数据（如 Simula 和 Clu）。\n\n当前的 SOA 及其微服务本质上是过程导向的，类似于1970年代的模块。Airbnb 认为 SOA 需要向数据导向设计演进，而服务网格从过程导向转向数据导向是实现这一目标的关键。\n\n## Viaduct：数据导向的服务网格\n现代可扩展 SOA 应用的核心是服务网格（如 Istio, Linkerd），负责将服务调用路由到相应的微服务实例。然而，行业标准的服务网格通常只关注远程过程调用，对构成应用架构的数据一无所知。Viaduct 的愿景是用数据导向的服务网格取代这些过程导向的服务网格。\n\nAirbnb 使用 GraphQL 构建了 Viaduct，一个数据导向的服务网格。Viaduct 服务网格通过 GraphQL schema 定义，包含：\n*   **类型（Types）和接口（Interfaces）**：描述服务网格内管理的数据。\n*   **查询（Queries）和订阅（Subscriptions）**：提供访问数据的方式，这些方式从提供数据的服务入口点中抽象出来。\n*   **变更（Mutations）**：提供更新数据的方式，同样从服务入口点中抽象出来。\n\nSchema 中的类型和接口定义了服务网格中所有数据的一个统一图。例如，一个电商公司的 schema 可能定义 `productById(id: ID)` 字段返回 `Product` 类型结果，数据消费者可以通过一个查询从产品导航到制造商、评论甚至评论作者，而无需了解底层数据来自哪些微服务。Viaduct 负责管理服务依赖，将其从数据消费者中抽象出来。\n\n## 以 Schema 为中心\nViaduct 将 schema 视为一个单一的工件，并实现了多项原语，允许团队在保持统一 schema 的同时高效协作。随着 Viaduct 逐渐取代底层的过程导向服务网格，其 schema 将更完整地捕捉应用管理的数据。Airbnb 利用这个“中心 schema”来定义部分微服务的 API，这些微服务的 GraphQL schema 是中心 schema 的子集。未来，计划进一步利用中心 schema 来定义数据库中存储的数据的 schema。\n\n这种以中心 schema 定义 API 和数据库 schema 的方法，将解决大规模 SOA 应用面临的一大挑战：**数据敏捷性**。目前，数据库 schema 的变更往往需要手动反映在两层、三层甚至更多层微服务的 API 中，才能暴露给客户端代码，这可能需要数周的多团队协调。通过从单一的中心 schema 派生服务 API 和数据库 schema，数据库 schema 的变更可以通过一次更新传播到客户端代码。\n\n## 无服务器化（Serverless）\n在大型 SOA 应用中，存在许多无状态的“派生数据服务”和“前端后端服务”，它们从底层服务获取原始数据并转换为适合客户端展示的数据。这种无状态逻辑非常适合无服务器计算模型，它消除了微服务的操作开销，将逻辑托管在“云函数”平台中。\n\nViaduct 拥有一种机制，可以使用无服务器云函数计算“派生字段”，这些函数在图之上运行，无需了解底层服务。这使得转换逻辑可以从服务网格中移出，放入无状态容器，从而保持图的整洁，并减少所需服务的数量和复杂性。\n\n## 结论\nViaduct 基于 `graphql-java` 构建，支持通过 GraphQL 选择集进行细粒度字段选择。它采用现代数据加载技术，实现短路和软依赖等可靠性技术，并包含请求内缓存。Viaduct 提供数据可观察性，允许在字段级别理解哪些服务消费了哪些数据。作为一个 GraphQL 接口，Viaduct 利用了庞大的开源工具生态系统，包括实时 IDE、模拟服务器和 schema 可视化工具。\n\nViaduct 已在 Airbnb 生产环境中运行一年多，从少量核心实体开始，现已发展到包含80个核心实体，能够支持 Airbnb 75% 的现代 API 流量。",
      "shortSummary": "Airbnb 推出了 Viaduct，一个基于 GraphQL 的数据导向服务网格，旨在解决大规模面向服务架构（SOA）中微服务依赖图复杂性问题。Viaduct 将服务网格从传统的过程导向转变为数据导向，通过统一的 GraphQL schema 抽象数据访问和底层服务依赖。这显著提升了模块化，简化了数据敏捷性，并支持无服务器函数处理派生数据。Viaduct 已在 Airbnb 生产环境中运行一年多，有效管理了75%的现代 API 流量。",
      "translated_title": "使用数据导向的服务网格驯服面向服务的架构",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*KUevf-1aGcLQyit3wzmh8w.jpeg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/896/1*y2E1EmsS3paNhoChYGnJIw.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=da771a841344",
          "alt": "",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<p>Introducing Viaduct, Airbnb’s data-oriented service mesh</p><p>By: Raymie Stata, Arun Vijayvergiya, Adam Miskiewicz</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*KUevf-1aGcLQyit3wzmh8w.jpeg\" /></figure><p>At Hasura’s <a href=\"https://hasura.io/enterprisegraphql/\">Enterprise GraphQL Conf</a> on October 22, we presented Viaduct, what we’re calling a <em>data-oriented service mesh </em>that we believe will bring a step function improvement in the modularity of our microservices-based Service-Oriented Architecture (SOA). In this blog post, we describe the philosophy behind Viaduct and provide a rough sketch of how it works. Please <a href=\"https://www.youtube.com/watch?v=xxk9MWCk7cM\">watch the presentation</a> for a more detailed look.</p><h3>Massive SOA Dependency Graphs</h3><p>For a while, <strong>S</strong>ervice-<strong>O</strong>riented <strong>A</strong>rchitectures have been moving towards ever larger numbers of small microservices. Modern applications can consist of thousands to tens of thousands of microservices connected in unconstrained ways. As a result, it’s not uncommon to see dependency graphs like the following:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/896/1*y2E1EmsS3paNhoChYGnJIw.png\" /></figure><p>This particular dependency graph happens to be from Airbnb, but it’s not uncommon. <a href=\"https://twitter.com/werner/status/741673514567143424\">Amazon</a>, <a href=\"https://medium.com/refraction-tech-everything/how-netflix-works-the-hugely-simplified-complex-stuff-that-happens-every-time-you-hit-play-3a40c9be254b\">Netflix</a>, and <a href=\"https://eng.uber.com/microservice-architecture/\">Uber</a> are examples of those that shared similarly tangled dependency graphs.</p><p>These dependency graphs are reminiscent of <a href=\"https://en.wikipedia.org/wiki/Spaghetti_code\">spaghetti code</a>, just at the microservices level. Similar to how spaghetti code becomes harder and harder to modify over time, so does spaghetti SOA. To help manage the larger number of services inherent in a microservices-based architecture, we need organizing principles as well as technical measures to implement those principles. At Airbnb, we undertook an effort to find such principles and measures. Our investigations led us to the concept of a <em>data-oriented service mesh</em>,<em> </em>which we believe brings a new level of modularity to SOA.</p><h3>Procedure- vs Data-Oriented Design</h3><p>Organizing large programs into modular units is not a new problem in software engineering. Up until the 1970s, the main paradigm of software organization focused on grouping code into procedures and procedures into modules. In this approach, modules publish a public API to be used by code outside of the module; behind this public API, modules hide their internal, helper procedures and other implementation details. Languages such as Pascal and C are based on this paradigm.</p><p>Starting in the ’80s, the paradigm shifted to organizing software primarily around data, not procedures. In this approach, modules define classes of objects that encapsulate an internal representation of an object accessed via a public API of methods<em> </em>on the object. Languages such as Simula and Clu pioneered this form of organization.</p><p>SOA is a step back to more procedure-oriented designs. Today’s microservice is a collection of procedural endpoints — a classic, 1970s-style module. We believe that SOA needs to evolve to support data-oriented design, and that this evolution can be enabled by transitioning our service mesh from a procedural orientation to a data orientation.</p><h3>Viaduct: A Data-Oriented Service Mesh</h3><p>Central to modern, scalable SOA applications is a <em>service mesh</em> (e.g., <a href=\"https://istio.io/\">Istio</a>, <a href=\"https://linkerd.io/\">Linkerd</a>), which routes service invocations to instances of microservices that can handle them. The current industry standard for service meshes is to organize exclusively around remote procedure invocations without knowing anything about the data that makes up the application architecture. Our vision is to replace these procedure-oriented service meshes with service meshes organized around <em>data.</em></p><p>At Airbnb, we are using <a href=\"https://graphql.org/\">GraphQL</a>™️ to build a data-oriented service mesh called <em>Viaduct.</em> A Viaduct service mesh is defined in terms of a GraphQL schema consisting of:</p><ul><li><em>Types</em> (and <em>interfaces</em>) describing data managed within your service mesh</li><li><em>Queries</em> (and <em>subscriptions</em>) providing means to access that data, which is abstracted from the service entry points that provide the data</li><li><em>Mutations </em>providing ways to update data, again abstracted from service entry points</li></ul><p>The types (and interfaces) in the schema define a single graph across all of the data managed within the service mesh. For example, at an eCommerce company, a service mesh’s schema may define a field productById(id: ID) that returns results of type Product. From this starting point, a single query allows a data consumer to navigate to information about the product’s manufacturer, e.g., productById { manufacturer }; reviews of the product, e.g. productById { reviews }; and even the authors of those reviews, e.g., productById { reviews { author } }.</p><p>The data elements requested by such a query may come from many different microservices. In a procedure-oriented service mesh, the data consumer would need to take these services as explicit dependencies. In our data-oriented service mesh, it is the service mesh, i.e., Viaduct, not the data consumer, that knows which services provide which data element. Viaduct abstracts away the service dependencies from any single consumer.</p><h3>Putting Schema at the Center</h3><p>In our talk we discuss how, unlike other distributed GraphQL systems like <a href=\"https://graphql-modules.com/\">GraphQL Modules</a> or <a href=\"https://www.apollographql.com/docs/federation/\">Apollo Federation</a>, Viaduct deals with the schema as a single artifact and has implemented several primitives that allow us to keep a unified schema while still allowing for many teams to collaborate on that schema productively. As Viaduct replaces more and more of our underlying procedure-oriented service mesh, its schema captures the data managed by our application more and more completely. We have taken advantage of this “central schema,” as we call it, as a place to define the APIs of some of our microservices. In particular, we have started using GraphQL for the API of some microservices. For these microservices, their GraphQL schemas are defined as a subset of the central schema. In the future, we want to take this idea further, using the central schema to define the schema of data stored in our database.</p><p>Among other things, using the central schema to define our APIs and database schemas will solve one of the bigger challenges of large-scale SOA applications: data agility. In today’s SOA applications, a change to a database schema often needs to be manually reflected in the APIs of two, three, and sometimes even more layers of microservices before it can be exposed to client code. Such changes can require weeks of coordinating among multiple teams. By deriving service APIs and database schemas from a single, central schema, a database schema change like this can be propagated to client code with a single update.</p><h3>Going Serverless</h3><p>Often in large SOA applications, there are many stateless “derived-data” services and “backend-for-frontend” services that take raw data from lower-level services and transform it into data that’s more appropriate for presentation in clients. Stateless logic like this is a good fit for the serverless computing model, which eliminates the operational overhead of microservices altogether and instead hosts logic in a “cloud functions” fabric.</p><p>Viaduct has a mechanism for computing what we call “derived fields” using serverless cloud functions that operate on top of<em> </em>the graph without knowledge of the underlying services. These functions allow us to move transformational logic out of the service mesh and into stateless containers, keeping our graph clean and reducing the number and complexity of services we need.</p><h3>Conclusion</h3><p>Viaduct is built on <a href=\"https://www.graphql-java.com/\">graphql-java</a> and supports fine-grained field selection via GraphQL selection sets. It uses modern data-loading techniques, employs reliability techniques such as short-circuiting and soft dependencies, and implements an intra-request cache. Viaduct provides <em>data observability, </em>allowing us to understand, down to the field level, what services consume what data. As a GraphQL interface, Viaduct allows us to take advantage of a large ecosystem of open source tooling, including live IDEs, mock servers, and schema visualizers.</p><p>Viaduct started powering production workflows at Airbnb over a year ago. We started from scratch with a clean schema consisting of a handful of entities and have grown it to include 80 core entities that are able to power 75% of our modern API traffic.</p><p>As mentioned in the introduction, more details on the motivation and technology behind Viaduct can be found in our <a href=\"https://www.youtube.com/watch?v=xxk9MWCk7cM\">presentation</a>.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=da771a841344\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/taming-service-oriented-architecture-using-a-data-oriented-service-mesh-da771a841344\">Taming Service-Oriented Architecture Using A Data-Oriented Service Mesh</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "将Airbnb的JVM单体仓库迁移到Bazel (原标题: Migrating Airbnb’s JVM Monorepo to Bazel)",
      "link": "https://medium.com/airbnb-engineering/migrating-airbnbs-jvm-monorepo-to-bazel-33f90eda51ec?source=rss----53c7c27702d5---4",
      "pubDate": "Wed, 13 Aug 2025 17:01:58 GMT",
      "isoDate": "2025-08-13T17:01:58.000Z",
      "creator": "Thomas Bao",
      "summary": "# 将Airbnb的JVM单体仓库迁移到Bazel\n\nAirbnb近期成功将其最大的代码仓库——JVM单体仓库迁移到了Bazel构建系统。该仓库包含数千万行Java、Kotlin和Scala代码，为airbnb.com背后的众多后端服务和数据管道提供支持。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/1*rSkIrKYhc8Bwcv2xLE1mxA.jpeg)\n\n**迁移成果概览（历时4.5年）：**\n*   构建CSAT（客户满意度）：从38%提升至68%\n*   本地构建和测试时间：提速3-5倍\n*   IntelliJ同步时间：提速2-3倍\n*   部署到开发环境时间：提速2-3倍\n\n本文将探讨迁移的原因、分享关键的实施亮点，并总结主要经验教训。\n\n## 为什么选择Bazel？\n\n在迁移之前，JVM单体仓库使用Gradle作为构建系统。选择迁移到Bazel是因为它提供了三大核心优势：速度、可靠性和统一的构建基础设施层。\n\n### 1. 速度\n\nBazel的可缓存、可移植操作允许通过远程执行扩展性能。\n*   **Gradle的局限性：** 2021年，大型服务的本地构建通常需要20多分钟，预合并CI的p90时间为35分钟。Gradle构建已接近极限，即使在CI上垂直扩展到高端AWS机器并使用启发式方法拆分项目构建和测试，效率依然低下，存在机器利用率不足和共享任务重复的问题。\n*   **Bazel的优势：**\n    *   **远程执行（RBE）：** 允许扩展到数千个并行操作，比Gradle的分片启发式方法效率更高。RBE工作节点是短生命周期的，提高了机器利用率和成本效益。\n    *   **“无字节构建”（Build without the Bytes）：** 只下载文件子集，大幅减少下载量（Gradle中每个缓存的工件都需要下载）。\n    *   **本地构建加速：** RBE显著加快了本地构建速度。\n    ![图片 2](https://cdn-images-1.medium.com/max/1024/0*y__bh8jYeKCNGmCW)\n    ![图片 3](https://cdn-images-1.medium.com/max/1024/0*qEJJw_dqvH--5nS1)\n    *   **并行分析：** Gradle大型项目的配置通常需要数分钟，因为它单线程运行。Bazel的分析阶段并行运行，部分原因是其配置语言Starlark被限制为无副作用。\n\n### 2. 可靠性\n\nBazel的密封性确保了可靠、可重复的构建。\n*   **Gradle的问题：** Gradle任务可以访问完整的文件系统，这在大规模应用中可能导致严重的意外后果。例如，开发者更新任务清理`/tmp/`目录中的文件，可能与其他使用该目录的Gradle任务产生竞态条件，导致CI失败。\n*   **Bazel的解决方案：**\n    *   **沙箱机制：** Bazel通过沙箱解决此问题，确保构建操作只能访问指定的输入。未声明为输入的任何文件在沙箱环境中均不存在。\n    *   **资源隔离：** Gradle任务隐式依赖于机器资源，在不同大小的机器上运行可能导致资源争用。RBE通过在具有严格资源限制的相同容器中运行操作来解决此问题。本地和CI构建都配置为使用RBE，大大减少了环境差异。\n\n### 3. 共享基础设施\n\nAirbnb目前拥有多个特定语言和平台的代码仓库（如web、iOS、Python、Go），所有这些现在都已迁移到Bazel。统一使用Bazel可以在所有仓库之间实现统一的构建基础设施层，包括：\n*   远程缓存\n*   远程构建执行\n*   受影响目标计算\n*   通过构建事件协议进行检测和日志记录\n\n## 我们如何迁移？\n\n### 1. 概念验证\n\n首个里程碑是证明可以在Bazel中构建服务并运行其单元测试。为了最大程度地减少对工程师的干扰，Bazel构建与Gradle并行存在，开发者可以选择使用Gradle或Bazel。\n*   **选择Viaduct服务：** 选择了Airbnb的GraphQL单体平台Viaduct进行概念验证，因为它：\n    *   是Airbnb最大、最复杂的服务之一，其成功迁移预示着整个单体仓库的迁移可行性。\n    *   构建速度慢是主要痛点，Bazel能产生巨大影响。\n    *   每月有300名产品工程师修改其代码，提高构建速度能显著提升生产力。\n    *   Viaduct的核心基础设施团队渴望合作。\n*   **实现方式：**\n    *   将Viaduct的大部分构建逻辑从Gradle移植到Bazel。\n    *   构建了一个自动化构建文件生成器，以应对Gradle构建图的持续变化和共存需求。\n    ![图片 4](https://cdn-images-1.medium.com/max/1024/0*sleFOnx1XS43xzAI)\n*   **初期挑战与解决：** 尽管Bazel本地构建速度提升了2-4倍，但许多开发者最初不愿切换。经过与服务所有者沟通，发现了许多缺失的集成和错误。又花了几个月时间解决这些痛点后，Viaduct开发者才自愿切换到Bazel。\n\n### 2. 使用Bazel扩展构建和测试\n\n概念验证成功后，团队开始将Bazel扩展到JVM单体仓库的其余部分。\n*   **广度优先策略：** 首先让所有仓库在Bazel中编译和测试。\n*   **共存的好处：**\n    *   开发者仍可使用Bazel进行本地开发，即使部署仍使用Gradle构建。\n    *   在Bazel基础设施出现问题时，可以禁用Bazel，让用户回退到Gradle。\n*   **共存的缺点：** 必须维护Gradle和Bazel两个构建图。手动维护Bazel构建图会降低开发体验，因此团队进一步投入自动化构建文件生成。\n\n### 3. 自动化构建文件生成\n\n团队受到Gazelle的启发，Gazelle通过解析源文件生成Bazel构建文件。\n*   **自研生成器：** 考虑到严格的性能要求和处理依赖循环的需求，团队决定构建自己的自动化构建文件生成器。\n*   **性能优化：** 为了不显著降低开发者体验，生成器必须在每次提交合并到主线前快速运行。通过实现外部缓存来加速生成过程。\n*   **工作原理：** 类似Gazelle，生成器解析Java、Kotlin和Scala源文件中的包、导入语句和符号声明，构建文件级依赖图。\n*   **CI集成：** 在CI中，每个主线提交都会发布一个仓库的缓存索引。用户运行`sync-configs`时，会下载此缓存，并仅重新扫描自合并基础以来更改的目录，大大提高了常见情况下的性能。\n*   **更细粒度的构建图：** 生成器支持更细粒度的构建图，Bazel目标数量是Gradle项目的10倍以上，通过更多并行构建和更少缓存失效实现更快的构建。它还能自动检测编译循环并在必要时合并编译单元。\n![图片 5](https://cdn-images-1.medium.com/max/1024/0*zmnMMjLRC2a-XCO0)\n*   **持续价值：** 即使在迁移后，构建文件生成器仍在使用，通过自动修复构建文件配置和移除未使用的依赖项来改善开发者体验。相比之下，Gradle时代用户手动维护约4500个Gradle文件，导致未使用的依赖项、臃肿的依赖图和更慢的构建。\n\n### 4. 移植构建逻辑\n\n除了JVM源文件，仓库还有大量由多个团队拥有的构建逻辑（如代码生成），通常以Gradle插件的形式存在。\n*   **与生成器集成：** 移植构建逻辑时，必须将其与构建文件生成器集成。由于目标更细粒度，Gradle中的一行配置可能需要应用于10多个Bazel构建文件。\n*   **插件和指令：** 构建文件生成器架构支持类似于Gazelle扩展的插件，这些插件由特定文件（如Thrift或GraphQL文件）的存在触发，并能生成新的构建目标（如代码生成操作）。对于手动添加或不易从文件结构推断的Gradle逻辑，支持类似于Gazelle的生成器指令（如添加依赖项或设置属性）。\n*   **团队协作：** 最初由核心团队移植大部分构建逻辑。随着Bazel的普及，复杂构建逻辑的所有者被激励迁移到Bazel，因为其更快、更可靠，他们也常编写自己的构建文件生成器插件，体现了生成器的可扩展性。\n\n### 5. 第三方库多版本支持\n\n迁移过程中遇到的一个主要问题是同一第三方库的多个版本。\n*   **问题：** 最初只指定每个库的单一版本，但Gradle中每个子项目可以使用不同版本的第三方库。这导致在针对单一版本编译时，可能因缺少符号而编译或测试失败。\n*   **解决方案：** 构建了工具来生成多个`maven_install`规则，并添加了自定义方面来在目标级别解决冲突。\n![图片 6](https://cdn-images-1.medium.com/max/1024/0*_U1LAeex52squvmt)\n*   **后续改进：** 将冲突解决移至分析时以获得更好的IDE支持，并添加了更友好的库更新工具。\n\n### 6. 迁移部署\n\n在绝大多数项目在CI中通过Bazel构建并单元测试通过后，团队开始专注于迁移部署。Bazel构建的`.jar`与Gradle构建的`.jar`不完全相同，因此需要策略确保部署安全。\n\n#### 服务\n*   **验证：** 使用启动和集成测试验证服务的部署正确性。约700个服务中，约100个遇到启动或集成测试失败。\n*   **主要问题及解决：**\n    *   **缺失依赖：** 大多数失败是由于Java反射加载的缺失依赖（通常来自配置文件或其他文件）。通过解析文件以查找将通过反射加载的类，然后添加所需依赖来解决。\n    *   **库版本差异：** 另一个主要错误源是库版本不同，可能导致运行时缺少符号错误。Gradle中用户手动指定依赖和版本，而Bazel中构建文件从源生成，依赖从导入语句推断（不指定版本）。通过强制第三方库版本与Gradle项目匹配来解决。\n*   **结果：** 考虑到反射类和版本同步后，只有个位数百分比的服务在生产环境中遇到运行时问题，需要更深入的手动修复。\n\n#### 数据管道和其他项目\n*   **规模：** 除了服务，还有450个数据管道和约50个其他项目部署到Spark集群或Flink运行时。\n*   **验证：** 类似服务，通过测试捕获了许多问题。Airbnb数据工程的“铺路”数据管道有CI测试，可在本地运行小版本Spark管道。\n*   **结果：** 对于约400个“铺路”管道，通过CI测试后，只有约3%在生产环境中出现运行时问题，因此可以非常快速地迁移。\n*   **剩余挑战：** 少数更定制化的可部署项目需要单独部署和监控以验证正确性。\n\n## 我们学到了什么？\n\n### 1. 客户合作\n\n在迁移早期，团队确定了具有巨大影响潜力和愿意投资新构建系统的关键试点服务。例如，Viaduct具有复杂的构建逻辑，导致构建缓慢和可靠性问题，且许多开发者贡献该服务，改进其构建对Airbnb的开发者体验影响巨大。\n*   与试点团队的合作非常有价值。他们是早期采用者，在报告和调试问题、分析性能瓶颈和提出功能方面做出了重大贡献。试点团队也成为了倡导者，提供内部支持，帮助激励Airbnb其他开发者社区。\n\n### 2. 过早优化的危险\n\n这次迁移历时4.5年。事后看来，如果团队先迁移再优化，可以大幅缩短迁移时间。\n*   虽然增加构建粒度缩短了构建时间，但却增加了迁移时间。具体来说，增加构建粒度大大增加了配置文件数量，使得手动管理配置变得更加困难。这迫使更多功能集成到自动化构建中。",
      "shortSummary": "Airbnb历时4.5年成功将其庞大的JVM单体仓库从Gradle迁移到Bazel。此次迁移显著提升了构建CSAT、本地构建和测试速度（3-5倍）、IntelliJ同步速度（2-3倍）及部署速度（2-3倍）。选择Bazel是因其卓越的速度、可靠性和统一的构建基础设施。迁移过程涉及概念验证、自动化构建文件生成、构建逻辑移植及第三方库多版本支持，最终成功迁移了服务和数据管道的部署。关键经验是客户合作的重要性，以及避免过早优化可能带来的迁移时间延长。",
      "translated_title": "将Airbnb的JVM单体仓库迁移到Bazel",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*rSkIrKYhc8Bwcv2xLE1mxA.jpeg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*y__bh8jYeKCNGmCW",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*qEJJw_dqvH--5nS1",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*sleFOnx1XS43xzAI",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*zmnMMjLRC2a-XCO0",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*rSkIrKYhc8Bwcv2xLE1mxA.jpeg\" /></figure><p><strong>By: </strong>Jack Dai, Howard Ho, Loc Dinh, Stepan Goncharov, Ted Tenedorio, and Thomas Bao</p><p>At Airbnb, we recently completed migrating our largest repo, the JVM monorepo, to Bazel. This repo contains <strong>tens of millions of lines</strong> of Java, Kotlin, and Scala code that power the vast array of backend services and data pipelines behind airbnb.com.</p><p><strong>Migration in numbers (4.5 years of work):</strong></p><ul><li>Build CSAT: 38% → 68%</li><li><strong>3–5x </strong>faster local build and test times</li><li><strong>2–3x </strong>faster IntelliJ syncs</li><li><strong>2–3x</strong> faster deploys to the development environment</li></ul><p>In this blog post, we’ll discuss the <strong>why</strong>, share some highlights on the <strong>how</strong>, and finish off with <strong>key learnings</strong>.</p><h3>Why Bazel?</h3><p>Before the migration, our JVM monorepo used Gradle as its build system. We decided to migrate to<strong> </strong>Bazel because it offered three key advantages: speed, reliability, and a uniform build infrastructure layer.</p><h4>Speed</h4><p><em>Bazel’s cacheable, portable actions allow us to scale performance with remote execution</em></p><p>In 2021, builds of large services often took &gt;20 minutes locally and pre-merge CI p90 was 35 minutes.</p><p>Building with Gradle was near its limit. We had already vertically scaled to high-end AWS machines on CI and remote development machines for developers of large services. In CI, we also used heuristics to split project builds and tests across multiple machines. However, this was inefficient, because of machine underutilization and duplication of shared tasks.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*y__bh8jYeKCNGmCW\" /></figure><p>Bazel remote execution allowed us to scale to thousands of parallel actions. This was far more efficient than our sharding heuristics. Remote build execution (RBE) workers are also short-lived, which results in better machine utilization and cost efficiency. In addition, <a href=\"https://blog.bazel.build/2021/04/07/build-without-the-bytes.html\">Build without the Bytes</a> allows downloading only a subset of files, greatly reducing download volume (in Gradle, every cached artifact needs to be downloaded). Finally and most importantly, local builds are significantly faster thanks to RBE.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*qEJJw_dqvH--5nS1\" /></figure><p>In addition, <a href=\"https://docs.gradle.org/current/userguide/build_lifecycle.html#sec:configuration\">Gradle configuration</a> of some large projects often took minutes due to it being single-threaded. Bazel analysis, in contrast, runs in parallel, in part because its configuration language, Starlark, is constrained to be side-effect-free.</p><h4>Reliability</h4><p><em>Bazel’s hermeticity ensures reliable, repeatable builds</em></p><p>Gradle tasks have access to the full file system, which can lead to serious unintended consequences at scale. One example we ran into was when a developer updated a task to clean up recent files in the /tmp/ directory. This created a race condition with other Gradle tasks that used the /tmp/ directory and caused CI to fail when thousands of Gradle tasks had to be rerun.</p><p>Bazel solves this issue with sandboxing, which ensures that only specified inputs are available to a build action. If a file isn’t declared as an input, it simply doesn’t exist in the sandboxed environment.</p><p>Gradle tasks also implicitly depend on the machine’s resources. Gradle builds run on local machines and CI machines of different sizes. This can lead to resource contention when a task is run on a smaller machine or when the cache is cold and thousands of tasks are run on the same machine.</p><p>Remote build execution (RBE) solves this by running actions in identical containers with strict resource limits. We also configured both local and CI builds to use RBE, which greatly reduces environment differences.</p><h4>Shared infrastructure</h4><p>Airbnb currently has a collection of language- and platform-specific repos, such as <a href=\"https://medium.com/airbnb-engineering/adopting-bazel-for-web-at-scale-a784b2dbe325\">web</a>, <a href=\"https://medium.com/airbnb-engineering/migrating-our-ios-build-system-from-buck-to-bazel-ddd6f3f25aa3\">iOS</a>, Python, and Go, all of which are now on Bazel. Unifying on Bazel enables a uniform build infrastructure layer across repos, which includes:</p><ul><li>Remote caching</li><li>Remote build execution</li><li>Affected targets calculation</li><li>Instrumentation &amp; logging from the <a href=\"https://bazel.build/docs/build-event-protocol\">Build Event Protocol</a></li></ul><h3>How did we migrate?</h3><h4>Proof of concept</h4><p>As a first milestone, we wanted to show that we could build a service and run its unit tests in Bazel. Because this was a proof of concept, we wanted to <strong>minimize disruption to engineers</strong>. Therefore, the Bazel build <em>co-existed </em>with Gradle<strong>. </strong>As a result, developers could choose between using Gradle or Bazel locally<em>.</em></p><p>We needed to prove that developers would <em>choose</em> to opt in to using Bazel over Gradle. It wasn’t enough for Bazel to be faster, developers had to willingly opt in to using Bazel.</p><p>For this proof of concept, we chose Airbnb’s GraphQL monolith platform, <a href=\"https://medium.com/airbnb-engineering/taming-service-oriented-architecture-using-a-data-oriented-service-mesh-da771a841344\">Viaduct</a>, which had the following important properties:</p><ol><li>It was one of Airbnb’s largest and most complex services. If we could migrate Viaduct to Bazel, then we could likely migrate the rest of the monorepo.</li><li>Slow builds were a major pain point, so Bazel could have a large impact.</li><li>Viaduct has 300 product engineers modifying its code every month, so improving Viaduct’s build speed would be a substantial productivity win.</li><li>Because of (2) and (3) above, Viaduct’s core infrastructure team was eager to partner with us.</li></ol><p>To achieve a working Viaduct build with Bazel, we did two things. First, we had to port much of Viaduct’s build logic from Gradle to Bazel. Second, because we decided to maintain co-existing builds and the Gradle build graph was still changing, we decided to build an automated build file generator (which we’ll cover in detail in a separate section).</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8d_f127RDGo6fHBHcqmN3g.png\" /></figure><p>Importantly, even though we were able to locally build the service 2–4x faster with Bazel, many developers did not yet want to switch.</p><p>In talking with the service’s owners, we discovered a number of missing integrations and bugs. It took us an additional few months to address these pain points, after which Viaduct developers willingly switched from Gradle to Bazel.</p><h4>Scaling builds and tests with Bazel</h4><p>The proof of concept showed that Bazel was superior to Gradle for one of Airbnb’s largest services and a large audience of developers. Now we wanted to scale it to the rest of Airbnb’s JVM monorepo.</p><p>We decided to scale breadth-first, getting all of the repo compiling and testing in Bazel. Again, to minimize disruption, Bazel builds co-existed with Gradle, which had two important benefits.</p><p>First, developers could still use Bazel for local development and get most of its benefits even though their code was still built with Gradle for deployment. Second, we could always disable Bazel if it was negatively impacting developers. For example, when Bazel infrastructure like the remote cache or remote execution cluster experienced an incident, we could and did disable Bazel, letting users fall back to Gradle.</p><p>However, a major downside was that both Gradle and Bazel build graphs had to be maintained. Manually maintaining a Bazel build graph would have degraded the developer experience. As a result, we invested further in automated build file generation, so that developers didn’t need to manually maintain Bazel build files.</p><h4>Automated build file generation</h4><p>For our build file generator, we were heavily inspired by <a href=\"https://github.com/bazel-contrib/bazel-gazelle\">Gazelle</a>, which generates Bazel build files by parsing source files to build a dependency graph.</p><p>Although we considered extending Gazelle to support JVM languages, we had very strict performance requirements and needed to handle dependency cycles. This ultimately led us to build our own automated build file generator.</p><p>Because we had to maintain <em>co-existing build graphs</em>, we needed to run the build file generator on every commit before merging into mainline. This meant it had to run as fast as possible to not significantly degrade the developer experience. To achieve this, we implemented external caching to speed up the automated build file generation.</p><p>Similar to Gazelle, the build file generator parses Java, Kotlin, and Scala source files for package and import statements and symbol declarations to build a file-level dependency graph.</p><p>In CI, we publish a cached index of the repository at each mainline commit. When a user runs sync-configs, it downloads this cache and only re-scans directories which have changed since the merge base. This greatly improves performance for the common case where users only modify a small set of files.</p><p>In addition, with this build file generator, we were able to support a more fine-grained build graph, which resulted in &gt;10x more Bazel targets than Gradle projects. This enabled faster builds through more parallel builds and less cache invalidation. However, one challenge of moving to a more fine-grained build graph is the possibility of introducing compilation cycles; sync-configs is able to detect this automatically and merge compilation units when necessary.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*UnuZOwRc8X-lT3onIEYHiA.png\" /></figure><p>Even after the migration, the build file generator remains in use. It improves the developer experience by automatically fixing build file configurations and removing unused dependencies. In contrast, when we were on Gradle, users manually maintained ~4,500 Gradle files, which led to unused dependencies, a bloated dependency graph, and slower builds with fewer cache hits.</p><h4>Porting build logic</h4><p>In addition to JVM source files, our repo has a large amount of build logic such as code generation owned by multiple teams. These often took the form of Gradle plugins.</p><p>Because we now had automated build file generation, when porting the build logic, we also had to integrate it with the build file generator. Also, because we had more granular targets, a single line of Gradle config now might need to apply to 10+ Bazel build files.</p><p>As a result, our build file generator architecture supported plugins similar to Gazelle extensions. These plugins were triggered by the presence of specific files such as Thrift or GraphQL files. These plugins could also generate new build targets such as codegen actions.</p><p>In some cases, the Gradle logic was manually added as a one-off or not easily inferred from the file structure. As a result, we also supported generator <em>directives</em> similar to Gazelle, such as adding dependencies or setting attributes.</p><p>Initially, our team ported much of this build logic ourselves with minimal help from service owners. As Bazel adoption grew, owners of complex build logic were incentivized to migrate to Bazel, because it was faster and more reliable than Gradle. In the process, they often wrote their own build file generator plugins, highlighting the extensibility of our generator.</p><h4>Third-party library multi-version support</h4><p>Another major issue we hit on the road to 100% compilation and testing with Bazel was multiple versions of the same third-party library.</p><p>Initially, we specified a single version of each library. The build file generator would add dependencies from this universe.</p><p>However, in Gradle, each sub-project within the monorepo could use different versions of third-party libraries. As a result, when compiling against a single version, compilation or testing could fail with a missing symbol.</p><p>To bring multi-version support to our Bazel system, we built tooling to generate multiple maven_install rules and added a custom <a href=\"https://bazel.build/extending/aspects\">aspect</a> to resolve conflicts at the target level.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*_U1LAeex52squvmt\" /></figure><p><em>Multiple versions of Guava in Bazel before we added conflict resolution</em></p><p>Once we had this capability, we systematically synced library versions from Gradle so that each build target’s classpath more closely matched its Gradle counterpart.</p><p>To learn more about our approach, see our <a href=\"https://www.youtube.com/watch?v=Ui4YtqWhqYU\">BazelCon 2022 talk</a>. Since giving this talk, we have made improvements like moving the resolution to analysis-time for better IDE support and adding more user-friendly tooling for updating libraries.</p><h4>Migrating the deploys</h4><p>As we got the vast majority of projects building and passing unit tests with Bazel in CI, we began to focus on migrating deployments. The Bazel-built jars were not identical to the Gradle-built jars. As a result, we needed a strategy to ensure the deployments were safe. We started with services.</p><h4>Services</h4><p>To verify the correctness of deploying services with Bazel, we used startup and integration tests. Of ~700 services, ~100 encountered startup or integration test failures. The majority of failures were missing dependencies that were loaded via Java reflection, usually from config files or other files. As a result, we were able to fix a number of these issues by parsing files for classes that would be loaded via reflection, and then adding the required dependency.</p><p>Another major source of errors was differing library versions, which could lead to missing symbol errors at runtime. In Gradle, users manually specified dependencies and their versions. However, in Bazel, build files were generated from source and dependencies were inferred from import statements, which didn’t specify the version. We solved many of these errors forcing third-party library versions to match those of the Gradle project.</p><p>After taking into account reflected classes and syncing versions, only a single-digit percentage of services hit production runtime issues that required more in-depth manual work to fix.</p><h4>Data pipelines and other projects</h4><p>In addition to services, we had 450 data pipelines and ~50 other projects that were deployed to either a Spark cluster or a Flink runtime.</p><p>Similar to services, we were able to catch a number of issues using tests. In particular, data pipelines on Airbnb’s data engineering paved path have CI tests that run a small version of the Spark pipeline locally. For these ~400 paved-path pipelines, after passing CI tests, only about 3% had production issues at runtime. As a result, we were able to very quickly migrate the paved-path pipelines.</p><p>As with services, we had a few remaining deployables that were a bit more bespoke and had to be individually deployed and monitored to verify correctness.</p><h3>What did we learn?</h3><h4>Customer partnership</h4><p>Early in the migration, we identified key pilot services that had large opportunities for impact and an appetite to invest in migrating to a new build system. For example, Viaduct had complex build logic leading to slow builds and reliability issues. In addition, many developers contributed to the service, so improving their builds had a large impact on Airbnb’s developer experience.</p><p>Partnering with pilot teams was incredibly valuable. They were early adopters and made significant contributions in the form of reporting and debugging issues, profiling performance bottlenecks, and suggesting features. The pilot teams also became advocates and provided internal support, helping motivate the rest of the Airbnb developer community.</p><h4>Dangers of premature optimization</h4><p>This migration took 4.5 years. With the benefit of hindsight we think we could have drastically improved the migration timeline if we had <strong>migrated first, before improving</strong><em>.</em></p><p>Although increasing build granularity improved build times, it increased the time to migrate. Specifically, increased build granularity greatly increased the number of configuration files, making it much harder to manage configurations manually. This forced more functionality into automated build file generation, which increased its complexity.</p><p>If we had migrated first and <em>then</em> optimized the build granularity, we believe we could have migrated sooner, enabling users to get benefits from Bazel sooner and reducing the time spent maintaining two co-existing builds.</p><p>Similarly, build granularity also made it harder to match deploy jars between Gradle and Bazel. This led to spending more time testing deployments and fixing runtime issues.</p><p>On a more positive note, we accelerated the migration by deciding to support multiple third-party library versions and implementing version resolution. This enabled us to sync versions from Gradle to Bazel, which fixed a large number of build and runtime issues.</p><p>Towards the end, one major takeaway in the migration was, <strong>by default, we should try to imitate what was there before</strong><em>. </em>In our case, deviating from Gradle usually added technical risk, and should be carefully considered, especially its downstream consequences.</p><p>As engineers, we often want to improve things. However, during migrations, improvements can have non-obvious consequences and potentially significantly slow down the migration.</p><h3>🏁 Conclusion</h3><p>After 4.5 years, we fully migrated Airbnb’s biggest repo from Gradle to Bazel, achieving:</p><ul><li>Build CSAT: 38% → 68%</li><li><strong>3–5x </strong>faster local build and test times</li><li><strong>2–3x</strong> faster IntelliJ syncs</li><li><strong>2–3x</strong> faster deploys to the development environment</li></ul><p>Finally, now that several Airbnb repos are on Bazel, we’re able to share <a href=\"https://www.youtube.com/watch?v=RpSVBtyoYCY\">common infrastructure</a> such as remote build caching, remote build execution, affected targets calculation, and more.</p><p>Interested in helping us solve problems like these at Airbnb? Learn more about our open engineering roles <a href=\"https://careers.airbnb.com/\">here</a>.</p><h3>Acknowledgments</h3><p>Additionally, thank you Janusz Kudelka, Kumail Hussain, Meghan Dow, Pawel Lipski, Peimin Liu, Tomasz Pacuszka, and various other internal and external partners.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=33f90eda51ec\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/migrating-airbnbs-jvm-monorepo-to-bazel-33f90eda51ec\">Migrating Airbnb’s JVM Monorepo to Bazel</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "大规模 Istio 无缝升级 (原标题: Seamless Istio Upgrades at Scale)",
      "link": "https://medium.com/airbnb-engineering/seamless-istio-upgrades-at-scale-bcb0e49c5cf8?source=rss----53c7c27702d5---4",
      "pubDate": "Thu, 07 Aug 2025 17:01:42 GMT",
      "isoDate": "2025-08-07T17:01:42.000Z",
      "creator": "Rushy R. Panchal",
      "summary": "# 大规模 Istio 无缝升级\n\n## 引言\n\nAirbnb 自2019年起大规模运行 Istio®，支持数万个 Pod、数十个 Kubernetes 集群和数千个虚拟机 (VM)，峰值期间通过 Istio 处理数千万 QPS。Istio 是 Airbnb 架构的基础组成部分，这使得其持续维护和升级成为一项挑战。尽管如此，Airbnb 已成功升级 Istio 14 次。本文将探讨 Airbnb 服务网格团队如何在保持高可用性的同时安全地升级 Istio。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/1*44AVFDg8R66nWj4cAU8vCA.jpeg)\n\n## 面临的挑战与升级目标\n\nAirbnb 工程师运行着数千种不同的工作负载，无法合理协调每个团队。因此，升级过程必须独立于单个团队运行，且由于无法同时监控所有工作负载，必须通过渐进式发布将风险降至最低。\n\n基于此，Airbnb 设计的升级流程旨在实现以下目标：\n\n*   **工作负载和用户零停机**：这是无缝升级的关键，工作负载所有者无需参与 Istio 升级。\n*   **渐进式发布**：能够控制哪些工作负载被升级或回滚。\n*   **全面回滚能力**：无需协调每个工作负载团队，即可在所有工作负载上回滚升级。\n*   **规定时间内完成升级**：所有工作负载应在定义的时间内完成升级。\n\n## 架构概览\n\nAirbnb 的 Istio 部署包含一个**管理集群**，运行 Istiod 并包含所有网格的工作负载配置（如 VirtualServices、DestinationRules），以及多个**工作负载集群**，运行用户工作负载。VM 独立运行，但其 Istio Manifests 仍部署到管理集群的独立命名空间中。Airbnb 独占使用 **Sidecar 模式**，即每个工作负载都运行 istio-proxy，尚未运行 Ambient 模式。\n\n![Istio 部署架构图](https://cdn-images-1.medium.com/max/1024/0*-qAuZZZxdsi-oJSO)\n\n## 升级流程\n\nAirbnb 遵循 Istio 的 **Canary 升级模型**，即同时运行两个版本的 Istiod（当前版本和新版本）。这两个版本共同构成一个逻辑服务网格，允许连接到不同 Istiod 的工作负载相互通信。Istiod 版本通过不同的修订标签进行管理（例如，1-24-5 用于 Istio 1.24.5，1-25-2 用于 Istio 1.25.2）。\n\n升级涉及控制平面 Istiod 和数据平面 Sidecar istio-proxy。尽管 Istio 支持旧版 istio-proxy 连接新版 Istiod，但 Airbnb 不采用此方式。相反，他们原子性地将新版 istio-proxy 及其连接的 Istiod 配置一同发布到工作负载。例如，为 1.24 版本构建的 istio-proxy 只会连接到 1.24 的 Istiod，这降低了升级期间的复杂性（跨版本数据平面-控制平面兼容性）。\n\n升级过程的第一步是在管理集群上部署带有新修订标签的新版 Istiod。由于所有工作负载都明确绑定到某个修订版，因此没有工作负载会连接到这个新的 Istiod，这一步没有影响。升级的其余部分（也是主要工作和风险所在）是逐步将工作负载切换到运行新版 istio-proxy 并连接到新版 Istiod。\n\n![多修订版 Istio 示意图](https://cdn-images-1.medium.com/max/1024/0*k7_eTtEPqBiDKM3t)\n\n## 发布规范 (`rollouts.yml`)\n\nAirbnb 通过一个名为 `rollouts.yml` 的文件控制工作负载运行的 istio-proxy 版本。该文件指定了工作负载命名空间（作为模式）和 Istio 版本的百分比分布，例如：\n\n```yaml\n# \"production\" 是默认值；任何不匹配其他模式的都将匹配此项。\nproduction: 1-24-5: 100\n\n\".*-staging\": 1-24-5: 75 1-25-2: 25\n\n# 一个固定的命名空间；我们的端到端验证工作负载。\nistio-e2e: 1-25-2: 100\n```\n\n此规范规定了所有命名空间的期望状态。给定命名空间首先映射到一个桶（基于最长匹配模式），然后根据该桶的分布选择一个版本。这种分布应用于命名空间级别，而非 Pod 或 VM 级别，且通过一致性哈希实现确定性分配。大部分升级过程仅涉及更新 `rollouts.yml` 并进行监控。这种方式允许选择性升级工作负载，并能独立升级环境，确保只有一定比例的环境运行新版本，从而有时间进行“烘焙”并发现潜在的回归问题。\n\n## Kubernetes 工作负载升级机制\n\n每个 Istio 修订版在每个工作负载集群上都有一个对应的 `MutatingAdmissionWebhook` 用于 Sidecar 注入。此 Webhook 选择带有 `istio.io/rev=<revision>` 标签的 Pod，并向其注入 istio-proxy 和 istio-init 容器。值得注意的是，istio-proxy 容器包含 `PROXY_CONFIG` 环境变量，该变量将 `discoveryAddress` 设置为特定的 Istiod 修订版。这确保了 istio-proxy 版本和其连接的 Istiod 配置的原子性部署，完全由 Sidecar 注入器完成。\n\n每个工作负载的 Deployment 都带有此修订标签。然而，传统方法要求每个团队手动更新此标签并部署其工作负载，这使得在所有工作负载上执行回滚或确保 100% 完成升级变得不切实际。\n\n### Krispr：内部突变框架\n\n为避免单独更新工作负载，Airbnb 使用内部构建的突变框架 **Krispr** 来注入修订标签，从而将基础设施组件升级与工作负载部署解耦。Airbnb 在 Kubernetes 上运行的工作负载使用内部 API 定义，而非直接指定 Kubernetes Manifests。此抽象在 CI 期间编译为 Kubernetes Manifests，Krispr 作为编译过程的一部分运行，并突变这些 Manifests。其中一项突变就是根据 `rollouts.yml` 决定并注入 Istio 修订标签到每个 Deployment 的 Pod 规范中。如果团队在部署时发现任何问题，他们可以回滚，从而也回滚 Istio 升级，而无需服务网格团队的介入。\n\n此外，Krispr 还在 Pod 准入阶段运行。如果一个 Pod 来自超过两周的 Deployment，Krispr 将重新突变该 Pod，并在需要时更新其修订标签。结合 Kubernetes 节点最长两周的生命周期（从而确保任何给定 Pod 的最长生命周期也是两周），Airbnb 可以保证 Istio 升级最终完成。大多数工作负载将在部署时（CI 中的 Krispr 运行期间）升级，对于不定期部署的工作负载，自然的 Pod 循环和重新突变将确保它们在最长四周内完成升级。\n\n**总结 Kubernetes 工作负载升级流程：**\n\n1.  **CI 阶段**：Krispr 根据 `rollouts.yml` 突变工作负载的 Kubernetes Manifests，添加 Istio 修订标签。\n2.  **Pod 准入阶段**：如果 Pod 的 Deployment 超过两周，Krispr 会重新突变 Pod，并在需要时更新 Istio 修订标签。\n3.  **Webhook 注入**：特定修订版的 Istio `MutatingAdmissionWebhook` 会注入 Sidecar 和关联的 `discoveryAddress`。\n\n## 虚拟机 (VM) 工作负载升级机制\n\n在 VM 上，Airbnb 部署一个包含 istio-proxy、运行 istio-iptables 的脚本（类似于 istio-init 容器）和 Istiod `discoveryAddress` 的工件。通过将 istio-proxy 和 `discoveryAddress` 打包在同一工件中，可以原子性地升级两者。此工件的安装由主机守护进程 **mxagent** 负责。它通过轮询 VM 上的键值标签（如 AWS 上的 EC2 标签或 GCP 上的资源标签，这些标签模仿 Kubernetes 工作负载的 `istio.io/rev` 标签）来确定要安装的版本。每当这些标签改变时，mxagent 就会下载并安装对应版本的工件。因此，升级 VM 上的 istio-proxy 只需更新该 VM 上的这些标签，mxagent 将处理其余部分。\n\nAirbnb 的 VM 工作负载主要是基础设施平台，通常不定期部署代码。因此，VM 不支持部署时升级（不像 Kubernetes 工作负载）。同样，团队无法自行回滚这些工作负载，但考虑到此类基础设施平台数量有限，这是可以接受的。\n\n标签更新由中央控制器 **mxrc** 管理，它扫描过时的 VM。如果 `rollouts.yml` 会导致 VM 的资源标签集发生变化，控制器将相应地更新标签。这大致对应于 Krispr 的 Pod 准入时突变，但需要注意的是，VM 是可变的且生命周期长，因此是原地升级。\n\n为了安全起见，mxrc 会考虑 VM 的健康状况，特别是 WorkloadEntry 上的就绪探针状态。类似于 Kubernetes 的 `maxUnavailable` 语义，mxrc 旨在将不可用 VM 的数量（即不健康的 VM 加上正在升级的 VM）保持在定义的百分比以下。它逐步执行这些升级，目标是在两周内完成一个工作负载所有 VM 的升级。\n\n## 结论\n\n跟上开源软件的更新步伐是一项挑战，尤其是在大规模环境下。升级和其他 Day-2 操作常常被忽视，这在最终需要升级时（为了引入安全补丁、保持在支持窗口内、利用新功能等）增加了负担。对于 Istio 而言尤其如此，其版本很快就会达到生命周期结束。\n\n尽管服务网格的复杂性和规模巨大，Airbnb 已成功升级 Istio 14 次。这得益于：\n\n*   **可维护性设计**\n*   **确保零停机的流程**\n*   **通过渐进式发布降低风险**\n\n类似流程也应用于 Airbnb 的其他多个基础架构系统。\n\n## 未来工作\n\n随着 Airbnb 基础设施的不断发展和壮大，服务网格团队正在关注几个关键项目：\n\n*   **利用 Ambient 模式**：作为一种更具成本效益且更易于管理的 Istio 部署模型。特别是，它通过完全无需触及工作负载部署来简化升级。\n*   **将单一生产网格拆分为多个网格**：以分离故障域、提供更好的安全隔离边界并进一步扩展 Istio。对于升级而言，这将进一步缩小影响范围，因为一些只运行低风险工作负载（如 staging）的网格可以首先升级。",
      "shortSummary": "Airbnb 详细介绍了其大规模 Istio 无缝升级实践。面对数万个 Pod 和 VM 的复杂环境，他们通过 Istio 的 Canary 升级模型，结合内部工具 Krispr（用于 Kubernetes）和 mxagent/mxrc（用于 VM），实现了控制平面和数据平面的原子性升级。这套系统确保了零停机、渐进式发布和无需人工协调的全面回滚能力，使所有工作负载能在数周内完成升级。未来计划包括采用 Ambient 模式和拆分服务网格以提升效率和安全性。",
      "translated_title": "大规模 Istio 无缝升级",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*44AVFDg8R66nWj4cAU8vCA.jpeg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*-qAuZZZxdsi-oJSO",
          "alt": "A diagram of Istio deployment architecture at Airbnb. There is a single configuration cluster containing Istio custom resources like VirtualServices, Istiod, and Istiod’s ConfigMaps. There are two workload clusters, each running user workloads, and alongside those a number of VMs. Workloads and VMs communicate with each other.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*k7_eTtEPqBiDKM3t",
          "alt": "Istio with multiple revisions. There is a config cluster with Istio custom resources like VirtualServices, multiple Istiod deployments, and the Istiod ConfigMaps for each of those. Workloads can connect to either Istiod.",
          "title": "",
          "position": 3
        },
        {
          "url": "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bcb0e49c5cf8",
          "alt": "",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<h4><strong>How Airbnb upgrades tens of thousands of pods on dozens of Kubernetes clusters to new Istio versions</strong></h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*44AVFDg8R66nWj4cAU8vCA.jpeg\" /></figure><p>Airbnb has been running Istio® at scale since 2019. We support workloads running on both Kubernetes and virtual machines (using <a href=\"https://istio.io/latest/docs/ops/deployment/vm-architecture/\">Istio’s mesh expansion</a>). Across these two environments, we run tens of thousands of pods, dozens of Kubernetes clusters, and thousands of VMs. These workloads send tens of millions of QPS at peak through Istio. Our <a href=\"https://www.youtube.com/watch?v=6kDiDQW5YXQ\">IstioCon 2021 talk</a> describes our journey onto Istio and our <a href=\"https://www.youtube.com/watch?v=1D8lg36ZNHs\">KubeCon 2021 talk</a> goes into further detail on our architecture.</p><p>Istio is a foundational piece of our architecture, which makes ongoing maintenance and upgrades a challenge. Despite that, we have upgraded Istio a total of 14 times. This blog post will explore how the Service Mesh team at Airbnb safely upgrades Istio while maintaining high availability.</p><h4>Challenges</h4><p>Airbnb engineers collectively run thousands of different workloads. We cannot reasonably coordinate the teams that own these, so our upgrades must function independently of individual teams. We also cannot monitor all of these at once, and so we must minimize risk through gradual rollouts.</p><p>With that in mind, we designed our upgrade process with the following goals:</p><ol><li>Zero downtime for workloads and users. This is the <em>seamless</em> part of the upgrade — a workload owner doesn’t need to be in the loop for Istio upgrades.</li><li>Gradual rollouts with the ability to control which workloads are upgraded or reverted.</li><li>We must be able to roll back an upgrade across all workloads, without coordinating every workload team.</li><li>All workloads should be upgraded within some defined time.</li></ol><h4>Architecture</h4><p>Our deployment consists of one management cluster, which runs Istiod and contains all workload configuration for the mesh (VirtualServices, DestinationRules, and so forth), and multiple workload clusters, which run user workloads. VMs run separately, but their Istio manifests are still deployed to the management cluster in their own namespaces. We use Sidecar mode exclusively, meaning that every workload runs istio-proxy — we do not yet run <a href=\"https://istio.io/latest/docs/ambient/overview/\">Ambient</a>.</p><figure><img alt=\"A diagram of Istio deployment architecture at Airbnb. There is a single configuration cluster containing Istio custom resources like VirtualServices, Istiod, and Istiod’s ConfigMaps. There are two workload clusters, each running user workloads, and alongside those a number of VMs. Workloads and VMs communicate with each other.\" src=\"https://cdn-images-1.medium.com/max/1024/0*-qAuZZZxdsi-oJSO\" /></figure><h4>Upgrade Process</h4><p>At a high level, we follow <a href=\"https://istio.io/latest/docs/setup/upgrade/canary/\">Istio’s canary upgrade model</a>. This involves running two versions (or Istio revisions) of Istiod simultaneously: the current version and the new version that we are upgrading to. Both form one logical service mesh, so workloads connected to one Istiod can communicate with workloads connected to another Istiod and vice versa. Istiod versions are managed using different revision labels — for example, 1–24–5 for Istio 1.24.5 and 1–25–2 for Istio 1.25.2.</p><p>An upgrade involves both Istiod, the control plane, and istio-proxy, the data plane sidecar, running on all pods and VMs. While Istio supports connecting an <a href=\"https://istio.io/latest/docs/releases/supported-releases/#control-planedata-plane-skew\">older istio-proxy to a newer Istiod</a>, we do not use this. Instead, we atomically roll out the new istio-proxy version to a workload along with the configuration of which Istiod to connect to. For example, the istio-proxy built for version 1.24 will only connect to 1.24’s Istiod and the istio-proxy built for 1.25 will only connect to 1.25’s Istiod. This reduces a dimension of complexity during upgrades (cross-version data plane — control plane compatibility).</p><p>The first step of our upgrade process is to deploy the new Istiod, with a new revision label, onto the management cluster. Because all workloads are explicitly pinned to a revision, no workload will connect to this new Istiod, so this first step has no impact.</p><p>The rest of the upgrade comprises all of the effort and risk — workloads are gradually shifted to run the new istio-proxy version and connect to the new Istiod.</p><figure><img alt=\"Istio with multiple revisions. There is a config cluster with Istio custom resources like VirtualServices, multiple Istiod deployments, and the Istiod ConfigMaps for each of those. Workloads can connect to either Istiod.\" src=\"https://cdn-images-1.medium.com/max/1024/0*k7_eTtEPqBiDKM3t\" /><figcaption><em>Multiple Istio revisions, with some workloads connected to different revisions.</em></figcaption></figure><h3>Rollout specification</h3><p>We control what version of istio-proxy workloads run through a file called rollouts.yml. This file specifies workload namespaces (as patterns) and the percentage distribution of Istio versions:</p><pre># &quot;production&quot; is the default; anything not matching a different pattern will match this.<br>production:<br>  1-24-5: 100<br><br>&quot;.*-staging&quot;:<br>  1-24-5: 75<br>  1-25-2: 25<br><br># A pinned namespace; our end-to-end verification workload.<br>istio-e2e:<br>  1-25-2: 100</pre><p>This spec dictates the desired state of all namespaces. A given namespace is first mapped to a bucket (based on the longest pattern that matches) and then a version is chosen based on the distribution for that bucket. The distribution applies at the namespace level, not the pod (or VM) level. For example,</p><pre>&quot;.*-staging&quot;:<br>  1-24-5: 75<br>  1-25-2: 25</pre><p>means that 75% of the namespaces with the suffix -stagingwill be assigned to 1–24–5 and the remaining 25% will be assigned to 1–25–2. This assignment is deterministic, using consistent hashing. The majority of our upgrade process involves updating rollouts.yml and then monitoring.</p><p>This process allows us to selectively upgrade workloads. We can also upgrade environments separately and ensure that only a certain percentage of those environments are on the new version. This gives us time to bake an upgrade and learn of potential regressions.</p><p>The rest of this post will describe the mechanism through which a change to rollouts.yml is applied to thousands of workloads, for both Kubernetes and VMs.</p><h3>Kubernetes</h3><p>Each Istio revision has a corresponding <a href=\"https://istio.io/latest/docs/setup/additional-setup/sidecar-injection/#automatic-sidecar-injection\">MutatingAdmissionWebhook for sidecar injection</a> on every workload cluster. This webhook selects pods specifying the label istio.io/rev=&lt;revision&gt; and injects the istio-proxy and istio-init containers into those pods. Notably, the istio-proxy container contains the PROXY_CONFIG environment variable, which sets the discoveryAddress to the Istiod revision. This is how the istio-proxyversion and the configuration for which Istiod to connect to are deployed atomically — entirely by the sidecar injector.</p><p>Every workload’s Deployment has this revision label. For example, a workload configured to use Istio 1.24.5 will have the label istio.io/rev=1–24–5in its pod template; thus pods for that Deployment will be mutated by the MutatingAdmissionWebhook for Istio 1.24.5.</p><p>This setup is the standard method of upgrading Istio, but requires that every Deployment specifies a revision label. To perform an upgrade across thousands of workloads, every team would have to update this label and deploy their workload. We could neither perform a rollback across all workloads nor reasonably expect an upgrade to complete to 100%, both for the same reason — relying on every workload to deploy.</p><h4>Krispr</h4><p>To avoid having to update workloads individually, a workload’s configuration never directly specifies the revision label in source code. Instead, we use <a href=\"https://medium.com/airbnb-engineering/a-krispr-approach-to-kubernetes-infrastructure-a0741cff4e0c\">Krispr, a mutation framework built in-house</a>, to inject the revision label. Krispr gives us the ability to decouple infrastructure component upgrades from workload deployments.</p><p>Airbnb workloads that run on Kubernetes use an internal API to define their workload, instead of specifying Kubernetes manifests. This abstraction is then compiled into Kubernetes manifests during CI. Krispr runs as part of this compilation and mutates those Kubernetes manifests. One of those mutations injects the Istio revision label into the pod specification of each Deployment, reading rollouts.ymlto decide which label to inject. If a team sees any issue with their workload when they deploy, they can roll back and thus also roll back the Istio upgrade — all without involving the Service Mesh team.</p><p>In addition, Krispr runs during pod admission. If a pod is being admitted from a Deployment that is more than two weeks old, Krispr will re-mutate the pod and accordingly update the pod’s revision label if needed. Combined with the fact that our Kubernetes nodes have a maximum lifetime of two weeks, thus ensuring that any given pod’s maximum lifetime is also two weeks, we can guarantee that an Istio upgrade completes. A majority of workloads will be upgraded when they deploy (during the Krispr run in CI) and for those that don’t deploy regularly, the natural pod cycling and re-mutation will ensure they are upgraded in at most four weeks.</p><p>In summary, per workload:</p><ol><li>During CI, Krispr mutates the Kubernetes manifests of a workload to add the Istio revision label, based on rollouts.yml.</li><li>When a pod is admitted to a cluster, Krispr will re-mutate the pod if its Deployment is more than two weeks old and update the Istio revision label if needed.</li><li>The revision-specific Istio MutatingAdmissionWebhook will mutate the pod by injecting the sidecar and associated discoveryAddress.</li></ol><h3>Virtual machines</h3><p>On VMs, we deploy an artifact that contains istio-proxy, a script to run istio-iptables (similar to the istio-init container), and the Istiod discoveryAddress. By packaging istio-proxy and the discoveryAddress in the same artifact, we can atomically upgrade both.</p><p>Installation of this artifact is the responsibility of an on-host daemon called mxagent. It determines what version to install by polling a set of key-value tags on the VM (such as <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html\">EC2 tags on AWS</a> or <a href=\"https://cloud.google.com/compute/docs/tag-resources\">resource tags on GCP</a>). These tags mimic the istio.io/rev label for Kubernetes-based workloads. Whenever they change, mxagent will download and install the artifact corresponding to that version. Thus, upgrading istio-proxy on a VM just involves updating these tags on that VM; mxagent will take care of the rest.</p><p>Our VM workloads are largely infrastructure platforms that don’t typically have code deployed at regular intervals. As such, VMs don’t support a deploy-time upgrade (in the way that Kubernetes workloads can be upgraded when they deploy). Similarly, teams cannot roll back these workloads themselves, but this has been acceptable, given that there are just a handful of such infrastructure platforms.</p><p>The tag updates are managed by a central controller, mxrc, which scans for outdated VMs. If rollouts.yml would result in a different set of resource tags for a VM, the controller will update the tags accordingly. This roughly corresponds to Krispr’s pod admission-time mutation — however, with the caveat that VMs are mutable and long-lived, and thus are upgraded in-place.</p><p>For safety, mxrc takes into account the health of the VM, namely in the form of the <a href=\"https://istio.io/latest/docs/reference/config/networking/workload-group/#ReadinessProbe\">readiness probe status on the WorkloadEntry</a>. Similar to Kubernetes’ maxUnavailable semantics, mxrc aims to keep the number of unavailable VMs (that is, unhealthy VMs plus those with in-progress upgrades) below a defined percentage. It gradually performs these upgrades, aiming to upgrade all the VMs for a workload in two weeks.</p><p>At the end of two weeks, all VMs will match the desired state in rollouts.yml.</p><h3>Conclusion</h3><p>Keeping up-to-date with open-source software is a challenge, especially at scale. Upgrades and other Day-2 operations often become an afterthought, which furthers the burden when upgrades are eventually necessary (to bring in security patches, remain within support windows, utilize new features, and so forth). This is particularly true with Istio, where a version reaches end-of-life support rapidly.</p><p>Even with the complexity and scale of our service mesh, we have successfully upgraded Istio 14 times. This was made possible due to designing for maintainability, building a process that ensures zero downtime, and derisking through the use of gradual rollouts. Similar processes are in use for a number of other foundational infrastructure systems at Airbnb.</p><h3>Future work</h3><p>As Airbnb’s infrastructure continues to evolve and grow, we’re looking at a few key projects to evolve our service mesh:</p><ul><li>Utilizing <a href=\"https://istio.io/latest/docs/ambient/overview/\">Ambient mode</a> as a more cost-effective and easier-to-manage deployment model of Istio. In particular, this simplifies upgrades by not needing to touch workload deployments at all.</li><li>Splitting our singular production mesh into multiple meshes in order to separate fault domains, provide better security isolation boundaries, and scale Istio further. For upgrades, this would further reduce the blast radius, as some meshes that only run low-risk workloads (such as staging) could be upgraded first.</li></ul><p>If this type of work interests you, we encourage you to apply for an <a href=\"https://careers.airbnb.com/\">open position</a> today.</p><h3>Acknowledgements</h3><p>All of our work with Istio is thanks to many different people, including: Jungho Ahn, Stephen Chan, Weibo He, Douglas Jordan, Brian Wolfe, Edie Yang, Dasol Yoon, and Ying Zhu.</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bcb0e49c5cf8\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/seamless-istio-upgrades-at-scale-bcb0e49c5cf8\">Seamless Istio Upgrades at Scale</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "Airbnb 在 Kubernetes 上通过分布式数据库实现高可用性 (原标题: Achieving High Availability with distributed database on Kubernetes at Airbnb)",
      "link": "https://medium.com/airbnb-engineering/achieving-high-availability-with-distributed-database-on-kubernetes-at-airbnb-58cc2e9856f4?source=rss----53c7c27702d5---4",
      "pubDate": "Mon, 28 Jul 2025 17:57:46 GMT",
      "isoDate": "2025-07-28T17:57:46.000Z",
      "creator": "Artem Danilov",
      "summary": "## Airbnb 在 Kubernetes 上实现分布式数据库高可用性\n\n### 引言\n\n传统上，组织通过分片策略在高成本的独立服务器上部署数据库以实现扩展。然而，随着数据需求的增长，这种策略的局限性日益明显，维护项目变得越来越长和复杂。分布式横向可扩展数据库已不罕见，其中许多是开源的。但在云环境中以高可用性、低延迟、可扩展性且成本合理的方式可靠运行这些数据库，是许多公司面临的挑战。\n\nAirbnb 采取了一种创新策略：在云环境中跨多个 Kubernetes 集群部署分布式数据库集群。尽管这种设计模式因其复杂性目前尚不常见，但它使 Airbnb 实现了目标系统可靠性和可操作性。本文分享了 Airbnb 如何克服挑战并开发出适用于任何强一致性分布式存储系统的最佳实践。\n\n### 在 Kubernetes 上管理数据库\n\nAirbnb 将一个开源的横向可扩展分布式 SQL 数据库集成到其基础设施中。虽然 Kubernetes 是运行无状态服务的优秀工具，但将其用于数据库等有状态服务具有挑战性，尤其是在节点替换和升级方面。\n\n*   **数据处理挑战**：Kubernetes 缺乏对节点间数据分布的了解，因此每次节点替换都需要仔细的数据处理，以防止数据仲裁丢失和服务中断，包括在替换节点前复制数据。\n*   **存储解决方案**：Airbnb 选择使用 AWS EBS 将存储卷附加到节点，这使得在节点替换时可以快速将卷重新附加到新的虚拟机。得益于 Kubernetes 的持久卷声明（PVC），这种重新附加是自动发生的。\n*   **自定义操作符**：为了确保新的存储节点有足够时间赶上集群的当前状态，Airbnb 依赖于自定义的 Kubernetes 操作符（k8s operator），该操作符允许根据应用程序的特定需求定制各种 Kubernetes 操作。\n\n### 协调节点替换\n\n节点替换的原因多种多样，包括 AWS 实例退役、Kubernetes 升级或配置更改。Airbnb 将节点替换事件分为三类：\n\n1.  **数据库发起的事件**：如配置更改或版本升级。\n2.  **主动基础设施事件**：如实例退役或节点升级。\n3.  **计划外基础设施故障**：如节点无响应。\n\n*   **安全管理**：\n    *   对于数据库发起的事件，k8s-operator 中实现了自定义检查，以在删除任何 Pod 之前验证所有节点是否正常运行。\n    *   为了与基础设施发起的事件串行化，在 Kubernetes 中实现了一个准入钩子（admission hook），用于拦截 Pod 驱逐。该钩子拒绝任何驱逐 Pod 的尝试，但会在 Pod 上分配一个自定义注解，自定义数据库 k8s-operator 会监听并根据该注解安全地删除 Pod，从而与任何数据库发起的节点替换串行化。\n    *   对于计划外基础设施故障（如硬件故障），无法协调。但可以通过确保在故障硬件被替换之前，阻止前两类事件的任何节点替换来提高可用性。\n*   **k8s 操作符的作用**：在 Airbnb 的基础设施中，k8s 操作符处理主动和基础设施触发的节点替换，在节点替换时保持数据一致性，并确保计划外事件不会影响正在进行的维护。\n\n### Kubernetes 升级\n\n定期的 Kubernetes 升级至关重要，但可能是高风险操作，特别是对于数据库而言。云托管的 Kubernetes 在控制平面升级后可能不提供回滚功能，如果出现问题，会带来潜在的灾难恢复挑战。Airbnb 采用自管理 Kubernetes 集群，允许回滚控制平面，但糟糕的 Kubernetes 升级仍可能导致服务中断，直到回滚完成。\n\n### 通过多 Kubernetes 集群确保容错性\n\nAirbnb 认为实现高区域可用性的最佳方式是将每个数据库部署在三个独立的 Kubernetes 集群中，每个集群位于不同的 AWS 可用区（AZ）。\n\n*   **隔离性**：AWS 可用区不仅提供独立的电源、网络和连接，还进行逐区域的发布。Kubernetes 集群与 AWS AZ 的对齐意味着任何底层基础设施问题或错误部署的爆炸半径都有限，因为它们仅限于单个 AZ。\n*   **分阶段部署**：在内部，Airbnb 还会将新配置或新数据库版本首先部署到位于单个 Kubernetes 集群中一个 AZ 内的逻辑集群的一部分。\n*   **可用性提升**：尽管这种设置增加了复杂性，但它通过限制来自每一层（无论是数据库、Kubernetes 还是 AWS 基础设施）的故障部署所引起问题的爆炸半径，显著提高了可用性。\n*   **弹性示例**：最近，基础设施中一次错误的配置部署突然终止了暂存 Kubernetes 集群中特定类型的所有虚拟机，删除了大部分查询层 Pod。然而，由于中断被隔离到单个 Kubernetes 集群，三分之二的查询层节点保持运行，从而避免了任何影响。\n*   **容量冗余**：Airbnb 还对数据库集群进行过量配置，以确保即使整个 AZ、Kubernetes 集群或区域内的所有存储节点发生故障，仍有足够的容量来处理流量。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/0*X2h18kiTtfcbRXQo)\n\n![图片 2](https://cdn-images-1.medium.com/max/1024/0*1hSoKkktPABPkTj)\n\n### 利用 AWS EBS 实现可靠性和延迟处理\n\nEBS 为 Airbnb 的部署提供了两个关键优势：\n\n1.  **快速重新附加**：在节点替换期间能够快速重新附加。\n2.  **卓越的持久性**：与本地磁盘相比具有更强的持久性。\n\n*   **副本数量**：借助 EBS，Airbnb 仅使用三个副本即可自信地运行高可用集群，无需额外冗余即可保持可靠性。\n*   **延迟缓解**：EBS 偶尔会出现尾部延迟峰值，p99 延迟可能达到 1 秒。为缓解此问题，Airbnb 实施了存储读取超时会话变量，允许查询在 EBS 延迟峰值期间透明地重试其他存储节点。\n    *   **读策略优化**：默认情况下，所使用的数据库将所有请求和重试发送给领导者。为了在 EBS 健康的存储节点上启用重试，必须允许从领导者和副本进行读取，但原始请求优先选择最近的副本。这带来了降低延迟和避免跨 AZ 网络成本的额外好处，因为每个 AZ 都有一个副本。\n    *   **陈旧读取**：对于允许的用例，利用陈旧读取功能，使副本能够独立提供读取服务，而无需对领导者进行同步调用（领导者可能在读取时遇到 EBS 延迟峰值）。\n\n### 结论：探索 Kubernetes 上的开源数据库\n\nAirbnb 在 Kubernetes 上运行分布式数据库的实践使其实现了高可用性、低延迟、可扩展性和更低的维护成本。通过利用操作符模式、多集群部署、AWS EBS 和陈旧读取，Airbnb 证明了即使是开源分布式存储系统也能在云环境中蓬勃发展。\n\nAirbnb 已在生产环境中运行多个数据库集群，其中最大的一个集群在 150 个存储节点上处理 300 万 QPS，存储超过 300TB 数据，分布在 400 万个内部分片中。得益于本文描述的技术，所有这些都实现了 99.95% 的可用性。\n\n对于其他考虑在 Kubernetes 上运行开源数据库的公司来说，机遇是巨大的。拥抱挑战，运行开源数据库以塑造这些工具以供企业使用。云中可扩展、可靠数据管理的未来在于协作和开源创新。",
      "shortSummary": "Airbnb 通过在多个 AWS 可用区内的 Kubernetes 集群上部署分布式数据库，实现了高可用性。他们利用自定义 Kubernetes 操作符、AWS EBS 进行存储和可靠性，并协调节点替换，以确保数据一致性和故障容错。这种策略使 Airbnb 的生产数据库实现了 99.95% 的可用性、低延迟、高可扩展性及更低的维护成本，证明了开源分布式存储系统在云环境中的可行性。",
      "translated_title": "Airbnb 在 Kubernetes 上通过分布式数据库实现高可用性",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*X2h18kiTtfcbRXQo",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*1hSoKkktmPABPkTj",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=58cc2e9856f4",
          "alt": "",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*X2h18kiTtfcbRXQo\" /></figure><h3>Introduction</h3><p>Traditionally, organizations have deployed databases on costly, high-end standalone servers using sharding for scaling as a strategy. As data demands grew, the limitations of this strategy became increasingly evident with increasingly longer and more complex maintenance projects.</p><p>Increasingly distributed horizontally scalable databases are not uncommon and many of them are open source. However, running these databases reliably in the cloud with high availability, low latency and scalability, all at a reasonable cost is a problem many companies are trying to solve.</p><p>We chose an innovative strategy of deploying<strong> a distributed database cluster across multiple Kubernetes clusters in a cloud environment</strong>. Although currently an uncommon design pattern due to its complexity, this strategy allowed us to achieve target system reliability and operability.</p><p>In this post, we’ll share how we overcame challenges and the best practices we’ve developed for this strategy and we believe these best practices should be applicable to any other strongly consistent, distributed storage systems.</p><h3>Managing Databases on Kubernetes</h3><p>Earlier this year, we integrated an open source horizontally scalable, distributed SQL database into our infrastructure.</p><p>While Kubernetes is a great tool for running stateless services, the use of Kubernetes for stateful services — like databases — is challenging, particularly around node replacement and upgrades.</p><p>Since Kubernetes lacks knowledge of data distribution across nodes, each node replacement requires careful data handling to prevent data quorum loss and service disruption, this includes copying the data before replacing a node.</p><p>At Airbnb, we opted to attach storage volumes to nodes using AWS EBS, this allows quick volume reattachment to new virtual machines upon node replacement. Thanks to Kubernetes’ Persistent Volume Claims (<a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#binding\">PVC</a>), this reattachment happens automatically. In addition we need to allow time for a new storage node to catch up with the cluster’s current state before moving to the next node replacement. For this, we rely on the custom <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/operator/\">k8s operator</a><a href=\"https://github.com/pingcap/tidb-operator\">,</a> which allows us to customize various Kubernetes operations according to specifics of the application.</p><h3>Coordinating Node Replacement</h3><p>Node replacements occur for various reasons, from <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-retirement.html\">AWS instance retirement</a> to Kubernetes upgrades or configuration changes. To address these cases, we categorize node replacement events into three groups:</p><ol><li><strong>Database-initiated events:</strong> Such as config changes or version upgrades.</li><li><strong>Proactive infrastructure events:</strong> Like instance retirements or node upgrades.</li><li><strong>Unplanned infrastructure failures:</strong> Such as a node becoming unresponsive.</li></ol><p>To safely manage node replacements for database-initiated events, we implemented a a custom check in the k8s-operator that verifies that all nodes are up and running before deleting any pod.</p><p>In order to serialize it with the second group initiated by infrastructure, we implemented <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/\">an admission hook</a> in k8s to intercept pod eviction. This admission hook rejects any attempt to evict the pod, but assigns a custom annotation on the pod which our customer database k8s-operator watches and acts on to safely delete the pod serializing it with any database-initiated node replacements described above.</p><p>Node replacements due to unplanned infrastructure failure events like hardware failure, can’t be coordinated. But we can still improve availability by ensuring that any node replacement event from the first two groups will be blocked until the failed hardware is replaced.</p><p>In our infrastructure the k8s operator handles both proactive and infrastructure-triggered node replacements, maintaining data consistency in the presence of node replacements and ensuring that unplanned events don’t impact ongoing maintenance.</p><h3>Kubernetes Upgrades</h3><p>Regular Kubernetes upgrades are essential but can be high-risk operations, especially for databases. Cloud managed Kubernetes might not offer rollbacks once the control plane is upgraded, posing a potential disaster recovery challenge if something goes wrong. While our approach involves using self-managed Kubernetes clusters, which does allow rolling back the control plane, a bad Kubernetes upgrade could still cause service disruption till rollback is completed.</p><h3>Ensuring Fault Tolerance with Multiple Kubernetes clusters</h3><p>At Airbnb, we think the best way to achieve high regional availability is to deploy each database across three independent Kubernetes clusters, each within a different AWS availability zone (<a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-fault-isolation-boundaries/availability-zones.htm\">AZ</a>). AWS uses availability zones not just for independent power, networking, and connectivity, but they also do rollouts zone by zone. Our Kubernetes cluster alignment with AWS AZ also means that any underlying infrastructure issues or bad deployments have a limited blast radius as they are restricted to a single AZ. Internally, we also deploy a new configuration or a new database version to a part of the logical cluster running in a single Kubernetes cluster in one AZ first.</p><p>While this setup adds complexity, it significantly boosts availability by limiting the blast radius of any issues stemming from faulty deployments at every layer — whether database, Kubernetes, or AWS infrastructure.</p><p>For instance, recently, a faulty config deployment in our infrastructure abruptly terminated all VMs of a specific type in our staging Kubernetes cluster, deleting most of the query layer pods. However, since the disruption was isolated to a single Kubernetes cluster, two-thirds of our query layer nodes remained operational, preventing any impact.</p><p>We also overprovision our database clusters to ensure that, even if an entire AZ, Kubernetes cluster, or all storage nodes within a zone goes down, we still have sufficient capacity to handle traffic.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*1hSoKkktmPABPkTj\" /></figure><h3>Leveraging AWS EBS for Reliability and Latency Handling</h3><p>EBS offers two key benefits for our deployment: rapid reattachment during node replacements and superior durability compared to local disks. With EBS, we confidently run a highly available cluster using only three replicas, maintaining reliability without needing additional redundancy.</p><p>However, EBS can occasionally experience tail latency spikes, with p99 latency reaching up to 1 second. To mitigate this, we implemented a storage read timeout session variable, allowing queries to transparently retry against other storage nodes during EBS latency spikes. By default the database we use sends all requests and retries to the leader. To enable retries on storage nodes with healthy EBS, we have to allow reads from both leader and replica reads, but prefer the closest one for the original request. This brings the added benefit of reduced latency and no cross-AZ network costs, as we have a replica in each AZ. Finally, for use cases that permit it, we leverage stale reads feature, enabling reads to be served independently by the replica without requiring synchronous calls to the leader, which may be experiencing an EBS latency spike at the time of the read.</p><h3>Conclusion: Exploring Open Source Databases on Kubernetes</h3><p>Our journey running a distributed database on Kubernetes has empowered us to achieve high availability, low latency, scalability, and lower maintenance costs. By leveraging the operator pattern, multi-cluster deployments, AWS EBS, and stale reads, we’ve demonstrated that even open source distributed storage systems can thrive in cloud environments.</p><p>We already operate several database clusters in production in the described setup, with the largest one handling 3M QPS across 150 storage nodes, storing over 300+ TB of data spread across 4M internal shards. All this with 99.95% availability thanks to techniques described in this post.</p><p>For other companies considering to run open-source databases on Kubernetes, the opportunities are immense. Embrace the challenge, run open-source databases to shape these tools for enterprise use. The future of scalable, reliable data management in the cloud lies in collaboration and open-source innovation — now is the time to lead and participate.</p><h3>Acknowledgments</h3><p>Thanks to Abhishek Parmar, Brian Wolfe, Chen Ding, Daniel Low, Hao Luo, Xiaomou Wang for collaboration and Shylaja Ramachandra for editing.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=58cc2e9856f4\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/achieving-high-availability-with-distributed-database-on-kubernetes-at-airbnb-58cc2e9856f4\">Achieving High Availability with distributed database on Kubernetes at Airbnb</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "理解和改进 SwiftUI 性能 (原标题: Understanding and Improving SwiftUI Performance)",
      "link": "https://medium.com/airbnb-engineering/understanding-and-improving-swiftui-performance-37b77ac61896?source=rss----53c7c27702d5---4",
      "pubDate": "Tue, 24 Jun 2025 16:43:07 GMT",
      "isoDate": "2025-06-24T16:43:07.000Z",
      "creator": "Cal Stephens",
      "summary": "## 理解和改进 SwiftUI 性能\n\n### 引言\n\nAirbnb 自2022年起采用 SwiftUI，显著提升了工程师的生产力。然而，大规模应用 SwiftUI 也带来了性能挑战，例如常见的低效代码模式和累积的性能损耗。为解决这些问题，Airbnb 开发了新的工具，用于主动识别和静态验证代码的正确性。\n\n### Airbnb 的 SwiftUI 功能架构\n\nAirbnb 多年来一直使用基于 UIKit 的 Epoxy 库和单向数据流系统。在 SwiftUI 屏幕层中，他们选择继续沿用现有的单向数据流库，这简化了在大型代码库中逐步采用 SwiftUI 的过程，并提高了功能质量和可维护性。然而，他们发现使用该库的 SwiftUI 功能性能未达预期，且问题原因并不明显。\n\n### 理解 SwiftUI 视图差异化（Diffing）\n\n在 SwiftUI 等声明式 UI 系统中，确保框架知道何时需要重新评估和重新渲染视图至关重要。当父视图更新时，SwiftUI 通过比较视图的存储属性来检测变化。理想情况下，视图的 `body` 仅在其属性实际改变时才会被重新评估。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/1*qBYJ9abMpZyuODmbHkYD5Q.jpeg)\n\n![图片 2: 理想的视图重新评估流程](https://cdn-images-1.medium.com/max/1024/1*LvNBJSor0RDThlW3Oq7mWw.png)\n\n然而，实际情况并非总是如此，不必要的视图 `body` 评估会因执行不必要的工作而损害性能。为了可视化视图 `body` 在实际应用中被重新评估的频率，Airbnb 使用了一个修饰符，每次渲染时给视图应用一个随机颜色。测试发现，许多视图被不必要地重新评估和重新渲染。\n\n![图片 3: 不必要的视图重新评估示例](https://cdn-images-1.medium.com/max/1024/1*yNrr8e8yI9RcKg6Z-VUhEA.gif)\n\n### SwiftUI 视图差异化算法\n\nSwiftUI 内置的差异化算法虽然未正式文档化，但对性能影响巨大。它使用基于反射的算法来比较视图的每个存储属性：\n\n*   **`Equatable` 类型：** SwiftUI 使用其 `Equatable` 一致性比较新旧值。\n*   **值类型（如结构体）：** 递归比较每个实例属性。\n*   **引用类型（如类）：** 使用引用标识进行比较。\n*   **闭包：** 尝试按标识比较，但大多数非简单闭包无法可靠比较。\n\n如果视图的所有属性与前一个值比较后都相等，则 `body` 不会被重新评估，内容也不会重新渲染。使用 `@State` 和 `@Environment` 等 SwiftUI 属性包装器修饰的值不参与此差异化算法，而是通过其他机制触发视图更新。\n\nAirbnb 在其代码库中发现了几种常见的模式会混淆 SwiftUI 的差异化算法：\n\n*   **闭包：** 某些类型（如闭包）本质上不受支持，几乎总是比较为不相等，导致不必要的 `body` 评估。\n*   **简单数据类型：** 存储在视图上的简单数据类型可能意外地按引用而非值进行比较，例如包装了内部引用类型的写时复制（copy-on-write）结构体。\n\n如果视图包含任何不可差异化的值，整个视图将变得不可差异化。这揭示了 Airbnb 单向数据流库的性能问题：其动作处理是基于闭包的，而 SwiftUI 无法差异化闭包。在许多情况下，使值可差异化需要大量侵入性且可能不理想的架构更改，且难以防止后续回归。\n\n### 控制 SwiftUI 视图差异化\n\n幸运的是，有另一种选择：如果视图遵循 `Equatable` 协议，SwiftUI 将使用其 `Equatable` 一致性进行差异化，而非默认的基于反射的算法。这允许开发者选择性地决定哪些属性应参与比较。例如，对于动作处理程序，如果它不影响视图内容或标识，则可以将其排除在 `Equatable` 比较之外。\n\n然而，手动编写和维护 `Equatable` 一致性会带来大量样板代码，且容易出错（例如，添加新属性时忘记更新 `Equatable`）。\n\n为此，Airbnb 创建了一个新的 **`@Equatable` 宏**，自动生成 `Equatable` 一致性。该宏会比较视图的所有存储实例属性（排除 `@State` 和 `@Environment` 等属性包装器）。不影响视图 `body` 输出且不可 `Equatable` 的属性可以使用 `@SkipEquatable` 标记，将其排除在生成的实现之外。这使得 Airbnb 可以继续使用基于闭包的动作处理程序，而不会影响 SwiftUI 的差异化过程。\n\n采用 `@Equatable` 宏后，视图保证可差异化。如果工程师后续添加了不可 `Equatable` 的属性，构建将失败，从而突出潜在的差异化行为回归。这使得 `@Equatable` 宏成为一个复杂的代码检查工具（linter），对于在大规模代码库中扩展性能改进非常有价值。\n\n### 管理视图体（View Body）大小\n\nSwiftUI 差异化的另一个重要方面是理解 SwiftUI 只能差异化真正的 `View` 结构体。任何其他代码，例如计算属性或生成 SwiftUI 视图的辅助函数，都无法被差异化。\n\n例如，将复杂的视图 `body` 拆分为单独的计算属性（如 `headerSection` 和 `actionCardSection`）是常见的组织方式，但运行时 SwiftUI 会将这些视图内联到主视图 `body` 中。这意味着，当屏幕的任何部分状态改变时，整个视图 `body` 都会被重新评估，随着视图变得更大更复杂，这将导致大量的非必要工作，损害性能。\n\n为提高性能，应将布局代码实现为独立的 SwiftUI 视图。这允许 SwiftUI 正确差异化每个子视图，仅在必要时重新评估其 `body`。例如，将 `MyScreen` 拆分为独立的 `HeaderSection` 和 `CardSection` 视图，并为它们应用 `@Equatable`，可以确保 `HeaderSection` 仅在 `title` 改变时重新评估，而 `CardSection` 仅在 `isCardSelected` 改变时重新评估。通过将视图分解为更小、可差异化的部分，SwiftUI 可以高效地仅更新实际改变的部分。\n\n### 视图体复杂度 Lint 规则\n\n大型、复杂的视图在开发过程中并不总是显而易见的。为了帮助工程师了解何时需要将视图重构为更小、可差异化的部分，Airbnb 创建了一个自定义的 SwiftLint 规则。该规则使用 SwiftSyntax 解析视图 `body` 并测量其复杂度。复杂度指标定义为每次使用计算属性、函数或闭包组合视图时增加的值。当视图复杂度超过可配置的限制（目前为10）时，该规则会在 Xcode 中自动触发警告。\n\n![图片 4: SwiftLint 视图复杂度警告示例](https://cdn-images-1.medium.com/max/1024/1*Tdo1L8qZf81FWFeJaNY6yQ.png)\n\n### 结论\n\n通过理解 SwiftUI 视图差异化机制，Airbnb 运用了三项关键技术：\n\n1.  **`@Equatable` 宏：** 确保视图 `body` 仅在视图内部的值实际改变时才被重新评估。\n2.  **拆分视图：** 将视图分解为更小的部分，以实现更快的重新评估。\n3.  **复杂度 Lint 规则：** 鼓励开发者在视图变得过大和复杂之前进行重构。\n\n将这些技术应用于 Airbnb 应用中的 SwiftUI 视图，显著减少了不必要的视图重新评估和重新渲染。例如，在搜索栏和筛选面板中，重新渲染的次数明显减少。\n\n![图片 5: 应用技术后减少重新渲染的示例](https://cdn-images-1.medium.com/max/1024/1*tWhEXK5kyFP5KYPCUSvp6Q.gif)\n\n根据页面性能评分系统的数据，在最复杂的 SwiftUI 屏幕中采用这些技术确实改善了用户体验。例如，通过在主要搜索屏幕上最重要的视图中采用 `@Equatable` 并将大型视图体拆分为更小的可差异化部分，滚动卡顿减少了15%。这些技术还提供了灵活性，允许 Airbnb 采用最适合其需求的功能架构，而不会牺牲性能或施加繁重的限制（例如，完全避免在 SwiftUI 视图中使用闭包）。\n\n当然，这些技术并非万能药，并非所有 SwiftUI 功能都必须使用它们，它们本身也不足以保证出色的性能。然而，理解它们的工作原理和原因，为构建高性能 SwiftUI 功能奠定了宝贵的基础，并使得在代码中发现和避免问题模式变得更容易。",
      "shortSummary": "Airbnb 团队为解决 SwiftUI 性能问题，主要聚焦于优化视图差异化（diffing）和管理视图体大小。他们开发了 `@Equatable` 宏，自动生成 `Equatable` 一致性，确保视图仅在关键属性变化时更新，并允许跳过不可差异化的属性（如闭包）。同时，他们提倡将大型视图拆分为更小的独立组件，并创建了 SwiftLint 规则来检测并限制视图复杂度，鼓励及时重构。这些措施显著减少了不必要的视图重新渲染，提升了应用性能，例如搜索页面滚动卡顿减少15%。",
      "translated_title": "理解和改进 SwiftUI 性能",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*qBYJ9abMpZyuODmbHkYD5Q.jpeg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*LvNBJSor0RDThlW3Oq7mWw.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*yNrr8e8yI9RcKg6Z-VUhEA.gif",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*Tdo1L8qZf81FWFeJaNY6yQ.png",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*tWhEXK5kyFP5KYPCUSvp6Q.gif",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<p>New techniques we’re using at Airbnb to improve and maintain performance of SwiftUI features at scale</p><p>By <a href=\"https://www.linkedin.com/in/calstephens/\">Cal Stephens</a>, <a href=\"https://www.linkedin.com/in/miguel-jimenez-b98216112\">Miguel Jimenez</a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*qBYJ9abMpZyuODmbHkYD5Q.jpeg\" /></figure><p>Airbnb <a href=\"https://medium.com/airbnb-engineering/unlocking-swiftui-at-airbnb-ea58f50cde49\">first adopted SwiftUI in 2022</a>, starting with individual components and later expanding to entire screens and features. We’ve seen major improvements to engineers’ productivity thanks to its declarative, flexible, and composable architecture. However, adopting SwiftUI has brought new challenges related to performance. For example, there are many common code patterns in SwiftUI that can be inefficient, and many small papercuts can add up to a large cumulative performance hit. To begin addressing some of these issues at scale, we’ve created new tooling for proactively identifying these cases and statically validating correctness.</p><h3>SwiftUI feature architecture at Airbnb</h3><p>We’ve been leveraging declarative UI patterns at Airbnb for many years, using our UIKit-based <a href=\"https://medium.com/airbnb-engineering/introducing-epoxy-for-ios-6bf062be1670\">Epoxy library</a> and <a href=\"https://medium.com/airbnb-engineering/introducing-epoxy-for-ios-6bf062be1670#fbe0\">unidirectional data flow</a> systems. When adopting SwiftUI in our screen layer, we decided to continue using our existing unidirectional data flow library. This simplified the process of incrementally adopting SwiftUI within our large codebase, and we find it improves the quality and maintainability of features.</p><p>However, we noticed that SwiftUI features using our unidirectional data flow library didn’t perform as well as we expected, and it wasn’t immediately obvious to us what the problem was. Understanding SwiftUI’s performance characteristics is an important requirement for building performant features, especially when venturing outside of the “standard” SwiftUI toolbox.</p><h3>Understanding SwiftUI view diffing</h3><p>When working with declarative UI systems like SwiftUI, it’s important to ensure the framework knows which views need to be re-evaluated and re-rendered when the state of the screen changes. Changes are detected by diffing the view’s stored properties any time its parent is updated. Ideally the view’s body will only be re-evaluated when its properties actually change:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*LvNBJSor0RDThlW3Oq7mWw.png\" /></figure><p>However, this behavior is not always the reality (more on why in a moment). Unnecessary view body evaluations hurt performance by performing unnecessary work.</p><p>How do you know how often a view’s body is re-evaluated in a real app? An easy way to visualize this is with a modifier that applies a random color to the view every time it’s rendered. When testing this on various views in our app’s most performance-sensitive screens, we quickly found that many views were re-evaluated and re-rendered more often than necessary:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*yNrr8e8yI9RcKg6Z-VUhEA.gif\" /></figure><h4>The SwiftUI view diffing algorithm</h4><p>SwiftUI’s built-in diffing algorithm is often overlooked and not officially documented, but it has a huge impact on performance. To determine if a view’s body needs to be re-evaluated, SwiftUI uses a reflection-based diffing algorithm to compare each of the view’s stored properties:</p><ol><li>If a type is <em>Equatable</em>, SwiftUI compares the old and new values using the type’s <em>Equatable</em> conformance. Otherwise:</li><li>SwiftUI compares value types (e.g., structs) by recursively comparing each instance property.</li><li>SwiftUI compares reference types (e.g., classes) using reference identity.</li><li>SwiftUI attempts to compare closures by identity. However, most closures cannot be compared reliably.</li></ol><p>If all of the view’s properties compare as equal to the previous value, then the body isn’t re-evalulated and the content isn’t re-rendered. Values using SwiftUI property wrappers like<em> @State </em>and <em>@Environment</em> don’t participate in this diffing algorithm, and instead trigger view updates through different mechanisms.</p><p>When reviewing different views in our codebase, we found several common patterns that confounded SwiftUI’s diffing algorithm:</p><ol><li>Some types are inherently not supported, like closures.</li><li>Simple data types stored on the view may be unexpectedly compared by reference instead of by value.</li></ol><p>Here’s an example SwiftUI view with properties that interact poorly with the diffing algorithm:</p><pre>struct MyView: View {<br>  /// A generated data model that is a struct with value semantics,<br>  /// but is copy-on-write and wraps an internal reference type.<br>  /// Compared by reference, not by value, which could cause unwanted body evaluations.<br>  let dataModel: CopyOnWriteDataModel<br><br>  /// Other miscellaneous properties used by the view. Typically structs, but sometimes a class.<br>  /// Unexpected comparisons by reference could cause unwanted body evaluations.<br>  let requestState: MyFeatureRequestState<br><br>  /// An action handler for this view, part of our unidirectional data flow library. <br>  /// Wraps a closure that routes the action to the screen&#39;s action handler.<br>  /// Closures almost always compare as not-equal, and typically cause unwanted body evaluations. <br>  let handler: Handler&lt;MyViewAction&gt;<br><br>  var body: some View { ... }<br>}</pre><p>If a view contains any value that isn’t diffable, the entire view becomes non-diffable. Preventing this in a scalable way is almost impossible with existing tools. This finding also reveals the performance issue caused by our unidirectional data flow library: action handling is closure-based, but SwiftUI can’t diff closures!</p><p>In some cases, like with the action handlers from our unidirectional data flow library, making the value diffable would require large, invasive, and potentially undesirable architecture changes. Even in simpler cases, this process is still time consuming, and there’s no easy way to prevent a regression from creeping in later on. This is a big obstacle when trying to improve and maintain performance at scale in large codebases with many different contributors.</p><h3>Controlling SwiftUI view diffing</h3><p>Fortunately, we have another option: If a view conforms to Equatable, SwiftUI will diff it using its Equatable conformance <em>instead</em> of using the default reflection-based diffing algorithm.</p><p>The advantage of this approach is that it lets us selectively decide which properties should be compared when diffing our view. In our case, we know that the handler object doesn’t affect the content or identity of our view. We only want our view to be re-evalulated and re-rendered when the <em>dataModel</em> and <em>requestState</em> values are updated. We can express that with a custom <em>Equatable</em> implementation:</p><pre>// An Equatable conformance that makes the above SwiftUI view diffable.<br>extension MyView: Equatable {<br>  static func ==(lhs: MyView, rhs: MyView) -&gt; Bool {<br>    lhs.dataModel == rhs.dataModel<br>      &amp;&amp; lhs.requestState == rhs.requestState<br>      // Intentionally not comparing handler, which isn&#39;t Equatable.<br>  }<br>}</pre><p>However:</p><ol><li>This is a lot of additional boilerplate for engineers to write, especially for views with lots of properties.</li><li>Writing and maintaining a custom conformance is error-prone. You can easily forget to update the <em>Equatable</em> conformance when adding new properties later, which would cause bugs.</li></ol><p>So, instead of manually writing and maintaining <em>Equatable</em> conformances, we created a new<em> @Equatable </em>macro that generates conformances for us.</p><pre>// A sample SwiftUI view that has adopted @Equatable<br>// and is now guaranteed to be diffable.<br>@Equatable<br>struct MyView: View {<br>  // Simple data types must be Equatable, or the build will fail.<br>  let dataModel: CopyOnWriteDataModel<br>  let requestState: MyFeatureRequestState<br><br>  // Types that aren&#39;t Equatable can be excluded from the<br>  // generated Equatable conformance using @SkipEquatable,<br>  // as long as they don’t affect the output of the view body.<br>  @SkipEquatable let handler: Handler&lt;MyViewAction&gt;<br><br>  var body: some View { ... }<br>}</pre><p>The <em>@Equatable</em> macro generates an <em>Equatable</em> implementation that compares all of the view’s stored instance properties, excluding properties with SwiftUI property wrappers like<em>@State </em>and <em>@Environment</em> that trigger view updates through other mechanisms. Properties that aren’t <em>Equatable</em> and don’t affect the output of the view body can be marked with <em>@SkipEquatable</em> to exclude them from the generated implementation. This allows us to continue using the closure-based action handlers from our unidirectional data flow library without impacting the SwiftUI diffing process!</p><p>After adopting the <em>@Equatable</em> macro on a view, that view is guaranteed to be diffable. If an engineer adds a non-<em>Equatable</em> property later, the build will fail, highlighting a potential regression in the diffing behavior. This effectively makes the <em>@Equatable</em> macro a sophisticated linter — which is really valuable for scaling these performance improvements in a codebase with many components and many contributors, since it makes it less likely for regressions to slip in later.</p><h3>Managing the size of view bodies</h3><p>Another essential aspect of SwiftUI diffing is understanding that SwiftUI can only diff proper View structs. Any other code, such as computed properties or helper functions that generate a SwiftUI view, cannot be diffed.</p><p>Consider the following example:</p><pre>// Complex SwiftUI views are often simplified by<br>// splitting the view body into separate computed properties.<br>struct MyScreen: View {<br>  /// The unidirectional data flow state store for this feature.<br>  @ObservedObject var store: StateStore&lt;MyState, MyAction&gt;<br><br>  var body: some View {<br>    VStack {<br>      headerSection<br>      actionCardSection<br>    }<br>  }<br><br>  private var headerSection: some View {<br>    Text(store.state.titleString)<br>      .textStyle(.title)<br>  }<br><br>  private var actionCardSection: some View {<br>    VStack {<br>      Image(store.state.cardSelected ? &quot;enabled&quot; : &quot;disabled&quot;)<br>      Text(&quot;This is a selectable card&quot;)<br>    }<br>    .strokedCard(.roundedRect_mediumCornerRadius_12)<br>    .scaleEffectButton(action: {<br>      store.handle(.cardTapped) <br>    })<br>  }<br>}</pre><p>This is a common way to organize complex view bodies, since it makes the code easier to read and maintain. However, at runtime, SwiftUI effectively inlines the views returned from the properties into the main view body, as if we instead wrote:</p><pre>// At runtime, computed properties are no different<br>// from just having a single, large view body!<br>struct MyScreen: View {<br>  @ObservedObject var store: StateStore&lt;MyState, MyAction&gt;<br><br>  // Re-evaluated every time the state of the screen is updated.<br>  var body: some View {<br>    VStack {<br>      Text(store.state.titleString)<br>        .textStyle(.title)<br><br>      VStack {<br>        Image(store.state.cardSelected ? &quot;enabled&quot; : &quot;disabled&quot;)<br>        Text(&quot;This is a selectable card&quot;)<br>      }<br>      .strokedCard(.roundedRect_mediumCornerRadius_12)<br>      .scaleEffectButton(action: {<br>        store.handle(.cardTapped) <br>      })<br>    }<br>  }<br>}</pre><p>Since all of this code is part of the same view body, all of it will be re-evaluated when any part of the screen’s state changes. While this specific example is simple, as the view grows larger and more complicated, re-evaluating it will become more expensive. Eventually there would be a large amount of unnecessary work happening on every screen update, hurting performance.</p><p>To improve performance, we can implement the layout code in separate SwiftUI views. This allows SwiftUI to properly diff each child view, only re-evaluating their bodies when necessary:</p><pre>struct MyScreen: View {<br>  @ObservedObject var store: StateStore&lt;MyState, MyAction&gt;<br><br>  var body: some View {<br>    VStack {<br>      HeaderSection(title: store.state.titleString)<br>      CardSection(<br>       isCardSelected: store.state.isCardSelected,<br>       handler: store.handler,<br>      )<br>    }<br>  }<br>}<br><br>/// Only re-evaluated and re-rendered when the title property changes.<br>@Equatable<br>struct HeaderSection: View {<br>  let title: String<br><br>  var body: some View {<br>    Text(title)<br>      .textStyle(.title)<br>  }<br>}<br><br>/// Only re-evaluated and re-rendered when the isCardSelected property changes.<br>@Equatable<br>struct CardSection: View {<br>  let isCardSelected: Bool<br>  @SkipEquatable let handler: Handler&lt;MyAction&gt;<br><br>  var body: some View {<br>    VStack {<br>      Image(store.state.isCardSelected ? &quot;enabled&quot; : &quot;disabled&quot;)<br>      Text(&quot;This is a selectable card&quot;)<br>    }<br>    .strokedCard(.roundedRect_mediumCornerRadius_12)<br>    .scaleEffectButton(action: {<br>      handler.handle(.cardTapped) <br>    })<br>  }<br>}</pre><p>By breaking the view into smaller, diffable pieces, SwiftUI can efficiently update only the parts of the view that actually changed. This approach helps maintain performance as a feature grows more complex.</p><h4>View body complexity lint rule</h4><p>Large, complex views aren’t always obvious during development. Easily available metrics like total line count aren’t a good proxy for complexity. To help engineers know when it’s time to refactor a view into smaller, diffable pieces, we created a custom <a href=\"https://github.com/realm/SwiftLint\">SwiftLint</a> rule that parses the view body using <a href=\"https://github.com/swiftlang/swift-syntax\">SwiftSyntax</a> and measures its complexity. We defined the view complexity metric as a value that increases every time you compose views using computed properties, functions, or closures. With this rule we automatically trigger an alert in Xcode when a view is getting too complex. (The complexity limit is configurable, and we currently allow a maximum complexity level of 10.)</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Tdo1L8qZf81FWFeJaNY6yQ.png\" /><figcaption>The rule shows as a warning during local Xcode builds alerting engineers as early as possible. In this screenshot, the complexity limit is set to 3, and this specific view has a complexity of 5.</figcaption></figure><h3>Conclusion</h3><p>With an understanding of how SwiftUI view diffing works, we can use an <em>@Equatable</em> macro to ensure view bodies are only re-evaluated when the values inside views actually change, break views into smaller parts for faster re-evaluation, and encourage developers to refactor views before they get too large and complex.</p><p>Applying these three techniques to SwiftUI views in our app has led to a large reduction in unnecessary view re-evaluation and re-renders. Revisiting the examples from earlier, you see far fewer re-renders in the search bar and filter panel:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*tWhEXK5kyFP5KYPCUSvp6Q.gif\" /></figure><p>Using results from our <a href=\"https://medium.com/airbnb-engineering/airbnbs-page-performance-score-on-ios-36d5f200bc73\">page performance score</a> system, we’ve found that adopting these techniques in our most complicated SwiftUI screens really does improve performance for our users. For example, we reduced <a href=\"https://medium.com/airbnb-engineering/airbnbs-page-performance-score-on-ios-36d5f200bc73#4c63\">scroll hitches</a> by<strong> </strong>15% on our main Search screen by adopting <em>@Equatable</em> on its most important views, and breaking apart large view bodies into smaller diffable pieces. These techniques also give us the flexibility to use a feature architecture that best suits our needs without compromising performance or imposing burdensome limitations (e.g., completely avoiding closures in SwiftUI views).</p><p>Of course, these techniques aren’t a silver bullet. It’s not necessary for all SwiftUI features to use them, and these techniques by themselves aren’t enough to guarantee great performance. However, understanding how and why they work serves as a valuable foundation for building performant SwiftUI features, and makes it easier to spot and avoid problematic patterns in your own code.</p><p>If you’re interested in joining us on our quest to make the best iOS app in the App Store, please see our <a href=\"https://careers.airbnb.com/\">careers</a> page for open iOS roles.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=37b77ac61896\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/understanding-and-improving-swiftui-performance-37b77ac61896\">Understanding and Improving SwiftUI Performance</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    }
  ],
  "lastUpdated": "2026-01-13T04:49:47.298Z"
}