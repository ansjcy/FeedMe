{
  "sourceUrl": "https://medium.com/feed/airbnb-engineering",
  "title": "The Airbnb Tech Blog - Medium",
  "description": "Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io - Medium",
  "link": "https://medium.com/airbnb-engineering?source=rss----53c7c27702d5---4",
  "items": [
    {
      "title": "大规模 Istio 无缝升级 (原标题: Seamless Istio Upgrades at Scale)",
      "link": "https://medium.com/airbnb-engineering/seamless-istio-upgrades-at-scale-bcb0e49c5cf8?source=rss----53c7c27702d5---4",
      "pubDate": "Thu, 07 Aug 2025 17:01:42 GMT",
      "isoDate": "2025-08-07T17:01:42.000Z",
      "creator": "Rushy R. Panchal",
      "summary": "# 大规模 Istio 无缝升级\n\n## 引言\n\nAirbnb 自2019年起大规模运行 Istio®，支持数万个 Pod、数十个 Kubernetes 集群和数千个虚拟机 (VM)，峰值期间通过 Istio 处理数千万 QPS。Istio 是 Airbnb 架构的基础组成部分，这使得其持续维护和升级成为一项挑战。尽管如此，Airbnb 已成功升级 Istio 14 次。本文将探讨 Airbnb 服务网格团队如何在保持高可用性的同时安全地升级 Istio。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/1*44AVFDg8R66nWj4cAU8vCA.jpeg)\n\n## 面临的挑战与升级目标\n\nAirbnb 工程师运行着数千种不同的工作负载，无法合理协调每个团队。因此，升级过程必须独立于单个团队运行，且由于无法同时监控所有工作负载，必须通过渐进式发布将风险降至最低。\n\n基于此，Airbnb 设计的升级流程旨在实现以下目标：\n\n*   **工作负载和用户零停机**：这是无缝升级的关键，工作负载所有者无需参与 Istio 升级。\n*   **渐进式发布**：能够控制哪些工作负载被升级或回滚。\n*   **全面回滚能力**：无需协调每个工作负载团队，即可在所有工作负载上回滚升级。\n*   **规定时间内完成升级**：所有工作负载应在定义的时间内完成升级。\n\n## 架构概览\n\nAirbnb 的 Istio 部署包含一个**管理集群**，运行 Istiod 并包含所有网格的工作负载配置（如 VirtualServices、DestinationRules），以及多个**工作负载集群**，运行用户工作负载。VM 独立运行，但其 Istio Manifests 仍部署到管理集群的独立命名空间中。Airbnb 独占使用 **Sidecar 模式**，即每个工作负载都运行 istio-proxy，尚未运行 Ambient 模式。\n\n![Istio 部署架构图](https://cdn-images-1.medium.com/max/1024/0*-qAuZZZxdsi-oJSO)\n\n## 升级流程\n\nAirbnb 遵循 Istio 的 **Canary 升级模型**，即同时运行两个版本的 Istiod（当前版本和新版本）。这两个版本共同构成一个逻辑服务网格，允许连接到不同 Istiod 的工作负载相互通信。Istiod 版本通过不同的修订标签进行管理（例如，1-24-5 用于 Istio 1.24.5，1-25-2 用于 Istio 1.25.2）。\n\n升级涉及控制平面 Istiod 和数据平面 Sidecar istio-proxy。尽管 Istio 支持旧版 istio-proxy 连接新版 Istiod，但 Airbnb 不采用此方式。相反，他们原子性地将新版 istio-proxy 及其连接的 Istiod 配置一同发布到工作负载。例如，为 1.24 版本构建的 istio-proxy 只会连接到 1.24 的 Istiod，这降低了升级期间的复杂性（跨版本数据平面-控制平面兼容性）。\n\n升级过程的第一步是在管理集群上部署带有新修订标签的新版 Istiod。由于所有工作负载都明确绑定到某个修订版，因此没有工作负载会连接到这个新的 Istiod，这一步没有影响。升级的其余部分（也是主要工作和风险所在）是逐步将工作负载切换到运行新版 istio-proxy 并连接到新版 Istiod。\n\n![多修订版 Istio 示意图](https://cdn-images-1.medium.com/max/1024/0*k7_eTtEPqBiDKM3t)\n\n## 发布规范 (`rollouts.yml`)\n\nAirbnb 通过一个名为 `rollouts.yml` 的文件控制工作负载运行的 istio-proxy 版本。该文件指定了工作负载命名空间（作为模式）和 Istio 版本的百分比分布，例如：\n\n```yaml\n# \"production\" 是默认值；任何不匹配其他模式的都将匹配此项。\nproduction: 1-24-5: 100\n\n\".*-staging\": 1-24-5: 75 1-25-2: 25\n\n# 一个固定的命名空间；我们的端到端验证工作负载。\nistio-e2e: 1-25-2: 100\n```\n\n此规范规定了所有命名空间的期望状态。给定命名空间首先映射到一个桶（基于最长匹配模式），然后根据该桶的分布选择一个版本。这种分布应用于命名空间级别，而非 Pod 或 VM 级别，且通过一致性哈希实现确定性分配。大部分升级过程仅涉及更新 `rollouts.yml` 并进行监控。这种方式允许选择性升级工作负载，并能独立升级环境，确保只有一定比例的环境运行新版本，从而有时间进行“烘焙”并发现潜在的回归问题。\n\n## Kubernetes 工作负载升级机制\n\n每个 Istio 修订版在每个工作负载集群上都有一个对应的 `MutatingAdmissionWebhook` 用于 Sidecar 注入。此 Webhook 选择带有 `istio.io/rev=<revision>` 标签的 Pod，并向其注入 istio-proxy 和 istio-init 容器。值得注意的是，istio-proxy 容器包含 `PROXY_CONFIG` 环境变量，该变量将 `discoveryAddress` 设置为特定的 Istiod 修订版。这确保了 istio-proxy 版本和其连接的 Istiod 配置的原子性部署，完全由 Sidecar 注入器完成。\n\n每个工作负载的 Deployment 都带有此修订标签。然而，传统方法要求每个团队手动更新此标签并部署其工作负载，这使得在所有工作负载上执行回滚或确保 100% 完成升级变得不切实际。\n\n### Krispr：内部突变框架\n\n为避免单独更新工作负载，Airbnb 使用内部构建的突变框架 **Krispr** 来注入修订标签，从而将基础设施组件升级与工作负载部署解耦。Airbnb 在 Kubernetes 上运行的工作负载使用内部 API 定义，而非直接指定 Kubernetes Manifests。此抽象在 CI 期间编译为 Kubernetes Manifests，Krispr 作为编译过程的一部分运行，并突变这些 Manifests。其中一项突变就是根据 `rollouts.yml` 决定并注入 Istio 修订标签到每个 Deployment 的 Pod 规范中。如果团队在部署时发现任何问题，他们可以回滚，从而也回滚 Istio 升级，而无需服务网格团队的介入。\n\n此外，Krispr 还在 Pod 准入阶段运行。如果一个 Pod 来自超过两周的 Deployment，Krispr 将重新突变该 Pod，并在需要时更新其修订标签。结合 Kubernetes 节点最长两周的生命周期（从而确保任何给定 Pod 的最长生命周期也是两周），Airbnb 可以保证 Istio 升级最终完成。大多数工作负载将在部署时（CI 中的 Krispr 运行期间）升级，对于不定期部署的工作负载，自然的 Pod 循环和重新突变将确保它们在最长四周内完成升级。\n\n**总结 Kubernetes 工作负载升级流程：**\n\n1.  **CI 阶段**：Krispr 根据 `rollouts.yml` 突变工作负载的 Kubernetes Manifests，添加 Istio 修订标签。\n2.  **Pod 准入阶段**：如果 Pod 的 Deployment 超过两周，Krispr 会重新突变 Pod，并在需要时更新 Istio 修订标签。\n3.  **Webhook 注入**：特定修订版的 Istio `MutatingAdmissionWebhook` 会注入 Sidecar 和关联的 `discoveryAddress`。\n\n## 虚拟机 (VM) 工作负载升级机制\n\n在 VM 上，Airbnb 部署一个包含 istio-proxy、运行 istio-iptables 的脚本（类似于 istio-init 容器）和 Istiod `discoveryAddress` 的工件。通过将 istio-proxy 和 `discoveryAddress` 打包在同一工件中，可以原子性地升级两者。此工件的安装由主机守护进程 **mxagent** 负责。它通过轮询 VM 上的键值标签（如 AWS 上的 EC2 标签或 GCP 上的资源标签，这些标签模仿 Kubernetes 工作负载的 `istio.io/rev` 标签）来确定要安装的版本。每当这些标签改变时，mxagent 就会下载并安装对应版本的工件。因此，升级 VM 上的 istio-proxy 只需更新该 VM 上的这些标签，mxagent 将处理其余部分。\n\nAirbnb 的 VM 工作负载主要是基础设施平台，通常不定期部署代码。因此，VM 不支持部署时升级（不像 Kubernetes 工作负载）。同样，团队无法自行回滚这些工作负载，但考虑到此类基础设施平台数量有限，这是可以接受的。\n\n标签更新由中央控制器 **mxrc** 管理，它扫描过时的 VM。如果 `rollouts.yml` 会导致 VM 的资源标签集发生变化，控制器将相应地更新标签。这大致对应于 Krispr 的 Pod 准入时突变，但需要注意的是，VM 是可变的且生命周期长，因此是原地升级。\n\n为了安全起见，mxrc 会考虑 VM 的健康状况，特别是 WorkloadEntry 上的就绪探针状态。类似于 Kubernetes 的 `maxUnavailable` 语义，mxrc 旨在将不可用 VM 的数量（即不健康的 VM 加上正在升级的 VM）保持在定义的百分比以下。它逐步执行这些升级，目标是在两周内完成一个工作负载所有 VM 的升级。\n\n## 结论\n\n跟上开源软件的更新步伐是一项挑战，尤其是在大规模环境下。升级和其他 Day-2 操作常常被忽视，这在最终需要升级时（为了引入安全补丁、保持在支持窗口内、利用新功能等）增加了负担。对于 Istio 而言尤其如此，其版本很快就会达到生命周期结束。\n\n尽管服务网格的复杂性和规模巨大，Airbnb 已成功升级 Istio 14 次。这得益于：\n\n*   **可维护性设计**\n*   **确保零停机的流程**\n*   **通过渐进式发布降低风险**\n\n类似流程也应用于 Airbnb 的其他多个基础架构系统。\n\n## 未来工作\n\n随着 Airbnb 基础设施的不断发展和壮大，服务网格团队正在关注几个关键项目：\n\n*   **利用 Ambient 模式**：作为一种更具成本效益且更易于管理的 Istio 部署模型。特别是，它通过完全无需触及工作负载部署来简化升级。\n*   **将单一生产网格拆分为多个网格**：以分离故障域、提供更好的安全隔离边界并进一步扩展 Istio。对于升级而言，这将进一步缩小影响范围，因为一些只运行低风险工作负载（如 staging）的网格可以首先升级。",
      "shortSummary": "Airbnb 详细介绍了其大规模 Istio 无缝升级实践。面对数万个 Pod 和 VM 的复杂环境，他们通过 Istio 的 Canary 升级模型，结合内部工具 Krispr（用于 Kubernetes）和 mxagent/mxrc（用于 VM），实现了控制平面和数据平面的原子性升级。这套系统确保了零停机、渐进式发布和无需人工协调的全面回滚能力，使所有工作负载能在数周内完成升级。未来计划包括采用 Ambient 模式和拆分服务网格以提升效率和安全性。",
      "translated_title": "大规模 Istio 无缝升级",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*44AVFDg8R66nWj4cAU8vCA.jpeg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*-qAuZZZxdsi-oJSO",
          "alt": "A diagram of Istio deployment architecture at Airbnb. There is a single configuration cluster containing Istio custom resources like VirtualServices, Istiod, and Istiod’s ConfigMaps. There are two workload clusters, each running user workloads, and alongside those a number of VMs. Workloads and VMs communicate with each other.",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*k7_eTtEPqBiDKM3t",
          "alt": "Istio with multiple revisions. There is a config cluster with Istio custom resources like VirtualServices, multiple Istiod deployments, and the Istiod ConfigMaps for each of those. Workloads can connect to either Istiod.",
          "title": "",
          "position": 3
        },
        {
          "url": "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bcb0e49c5cf8",
          "alt": "",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<h4><strong>How Airbnb upgrades tens of thousands of pods on dozens of Kubernetes clusters to new Istio versions</strong></h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*44AVFDg8R66nWj4cAU8vCA.jpeg\" /></figure><p>Airbnb has been running Istio® at scale since 2019. We support workloads running on both Kubernetes and virtual machines (using <a href=\"https://istio.io/latest/docs/ops/deployment/vm-architecture/\">Istio’s mesh expansion</a>). Across these two environments, we run tens of thousands of pods, dozens of Kubernetes clusters, and thousands of VMs. These workloads send tens of millions of QPS at peak through Istio. Our <a href=\"https://www.youtube.com/watch?v=6kDiDQW5YXQ\">IstioCon 2021 talk</a> describes our journey onto Istio and our <a href=\"https://www.youtube.com/watch?v=1D8lg36ZNHs\">KubeCon 2021 talk</a> goes into further detail on our architecture.</p><p>Istio is a foundational piece of our architecture, which makes ongoing maintenance and upgrades a challenge. Despite that, we have upgraded Istio a total of 14 times. This blog post will explore how the Service Mesh team at Airbnb safely upgrades Istio while maintaining high availability.</p><h4>Challenges</h4><p>Airbnb engineers collectively run thousands of different workloads. We cannot reasonably coordinate the teams that own these, so our upgrades must function independently of individual teams. We also cannot monitor all of these at once, and so we must minimize risk through gradual rollouts.</p><p>With that in mind, we designed our upgrade process with the following goals:</p><ol><li>Zero downtime for workloads and users. This is the <em>seamless</em> part of the upgrade — a workload owner doesn’t need to be in the loop for Istio upgrades.</li><li>Gradual rollouts with the ability to control which workloads are upgraded or reverted.</li><li>We must be able to roll back an upgrade across all workloads, without coordinating every workload team.</li><li>All workloads should be upgraded within some defined time.</li></ol><h4>Architecture</h4><p>Our deployment consists of one management cluster, which runs Istiod and contains all workload configuration for the mesh (VirtualServices, DestinationRules, and so forth), and multiple workload clusters, which run user workloads. VMs run separately, but their Istio manifests are still deployed to the management cluster in their own namespaces. We use Sidecar mode exclusively, meaning that every workload runs istio-proxy — we do not yet run <a href=\"https://istio.io/latest/docs/ambient/overview/\">Ambient</a>.</p><figure><img alt=\"A diagram of Istio deployment architecture at Airbnb. There is a single configuration cluster containing Istio custom resources like VirtualServices, Istiod, and Istiod’s ConfigMaps. There are two workload clusters, each running user workloads, and alongside those a number of VMs. Workloads and VMs communicate with each other.\" src=\"https://cdn-images-1.medium.com/max/1024/0*-qAuZZZxdsi-oJSO\" /></figure><h4>Upgrade Process</h4><p>At a high level, we follow <a href=\"https://istio.io/latest/docs/setup/upgrade/canary/\">Istio’s canary upgrade model</a>. This involves running two versions (or Istio revisions) of Istiod simultaneously: the current version and the new version that we are upgrading to. Both form one logical service mesh, so workloads connected to one Istiod can communicate with workloads connected to another Istiod and vice versa. Istiod versions are managed using different revision labels — for example, 1–24–5 for Istio 1.24.5 and 1–25–2 for Istio 1.25.2.</p><p>An upgrade involves both Istiod, the control plane, and istio-proxy, the data plane sidecar, running on all pods and VMs. While Istio supports connecting an <a href=\"https://istio.io/latest/docs/releases/supported-releases/#control-planedata-plane-skew\">older istio-proxy to a newer Istiod</a>, we do not use this. Instead, we atomically roll out the new istio-proxy version to a workload along with the configuration of which Istiod to connect to. For example, the istio-proxy built for version 1.24 will only connect to 1.24’s Istiod and the istio-proxy built for 1.25 will only connect to 1.25’s Istiod. This reduces a dimension of complexity during upgrades (cross-version data plane — control plane compatibility).</p><p>The first step of our upgrade process is to deploy the new Istiod, with a new revision label, onto the management cluster. Because all workloads are explicitly pinned to a revision, no workload will connect to this new Istiod, so this first step has no impact.</p><p>The rest of the upgrade comprises all of the effort and risk — workloads are gradually shifted to run the new istio-proxy version and connect to the new Istiod.</p><figure><img alt=\"Istio with multiple revisions. There is a config cluster with Istio custom resources like VirtualServices, multiple Istiod deployments, and the Istiod ConfigMaps for each of those. Workloads can connect to either Istiod.\" src=\"https://cdn-images-1.medium.com/max/1024/0*k7_eTtEPqBiDKM3t\" /><figcaption><em>Multiple Istio revisions, with some workloads connected to different revisions.</em></figcaption></figure><h3>Rollout specification</h3><p>We control what version of istio-proxy workloads run through a file called rollouts.yml. This file specifies workload namespaces (as patterns) and the percentage distribution of Istio versions:</p><pre># &quot;production&quot; is the default; anything not matching a different pattern will match this.<br>production:<br>  1-24-5: 100<br><br>&quot;.*-staging&quot;:<br>  1-24-5: 75<br>  1-25-2: 25<br><br># A pinned namespace; our end-to-end verification workload.<br>istio-e2e:<br>  1-25-2: 100</pre><p>This spec dictates the desired state of all namespaces. A given namespace is first mapped to a bucket (based on the longest pattern that matches) and then a version is chosen based on the distribution for that bucket. The distribution applies at the namespace level, not the pod (or VM) level. For example,</p><pre>&quot;.*-staging&quot;:<br>  1-24-5: 75<br>  1-25-2: 25</pre><p>means that 75% of the namespaces with the suffix -stagingwill be assigned to 1–24–5 and the remaining 25% will be assigned to 1–25–2. This assignment is deterministic, using consistent hashing. The majority of our upgrade process involves updating rollouts.yml and then monitoring.</p><p>This process allows us to selectively upgrade workloads. We can also upgrade environments separately and ensure that only a certain percentage of those environments are on the new version. This gives us time to bake an upgrade and learn of potential regressions.</p><p>The rest of this post will describe the mechanism through which a change to rollouts.yml is applied to thousands of workloads, for both Kubernetes and VMs.</p><h3>Kubernetes</h3><p>Each Istio revision has a corresponding <a href=\"https://istio.io/latest/docs/setup/additional-setup/sidecar-injection/#automatic-sidecar-injection\">MutatingAdmissionWebhook for sidecar injection</a> on every workload cluster. This webhook selects pods specifying the label istio.io/rev=&lt;revision&gt; and injects the istio-proxy and istio-init containers into those pods. Notably, the istio-proxy container contains the PROXY_CONFIG environment variable, which sets the discoveryAddress to the Istiod revision. This is how the istio-proxyversion and the configuration for which Istiod to connect to are deployed atomically — entirely by the sidecar injector.</p><p>Every workload’s Deployment has this revision label. For example, a workload configured to use Istio 1.24.5 will have the label istio.io/rev=1–24–5in its pod template; thus pods for that Deployment will be mutated by the MutatingAdmissionWebhook for Istio 1.24.5.</p><p>This setup is the standard method of upgrading Istio, but requires that every Deployment specifies a revision label. To perform an upgrade across thousands of workloads, every team would have to update this label and deploy their workload. We could neither perform a rollback across all workloads nor reasonably expect an upgrade to complete to 100%, both for the same reason — relying on every workload to deploy.</p><h4>Krispr</h4><p>To avoid having to update workloads individually, a workload’s configuration never directly specifies the revision label in source code. Instead, we use <a href=\"https://medium.com/airbnb-engineering/a-krispr-approach-to-kubernetes-infrastructure-a0741cff4e0c\">Krispr, a mutation framework built in-house</a>, to inject the revision label. Krispr gives us the ability to decouple infrastructure component upgrades from workload deployments.</p><p>Airbnb workloads that run on Kubernetes use an internal API to define their workload, instead of specifying Kubernetes manifests. This abstraction is then compiled into Kubernetes manifests during CI. Krispr runs as part of this compilation and mutates those Kubernetes manifests. One of those mutations injects the Istio revision label into the pod specification of each Deployment, reading rollouts.ymlto decide which label to inject. If a team sees any issue with their workload when they deploy, they can roll back and thus also roll back the Istio upgrade — all without involving the Service Mesh team.</p><p>In addition, Krispr runs during pod admission. If a pod is being admitted from a Deployment that is more than two weeks old, Krispr will re-mutate the pod and accordingly update the pod’s revision label if needed. Combined with the fact that our Kubernetes nodes have a maximum lifetime of two weeks, thus ensuring that any given pod’s maximum lifetime is also two weeks, we can guarantee that an Istio upgrade completes. A majority of workloads will be upgraded when they deploy (during the Krispr run in CI) and for those that don’t deploy regularly, the natural pod cycling and re-mutation will ensure they are upgraded in at most four weeks.</p><p>In summary, per workload:</p><ol><li>During CI, Krispr mutates the Kubernetes manifests of a workload to add the Istio revision label, based on rollouts.yml.</li><li>When a pod is admitted to a cluster, Krispr will re-mutate the pod if its Deployment is more than two weeks old and update the Istio revision label if needed.</li><li>The revision-specific Istio MutatingAdmissionWebhook will mutate the pod by injecting the sidecar and associated discoveryAddress.</li></ol><h3>Virtual machines</h3><p>On VMs, we deploy an artifact that contains istio-proxy, a script to run istio-iptables (similar to the istio-init container), and the Istiod discoveryAddress. By packaging istio-proxy and the discoveryAddress in the same artifact, we can atomically upgrade both.</p><p>Installation of this artifact is the responsibility of an on-host daemon called mxagent. It determines what version to install by polling a set of key-value tags on the VM (such as <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html\">EC2 tags on AWS</a> or <a href=\"https://cloud.google.com/compute/docs/tag-resources\">resource tags on GCP</a>). These tags mimic the istio.io/rev label for Kubernetes-based workloads. Whenever they change, mxagent will download and install the artifact corresponding to that version. Thus, upgrading istio-proxy on a VM just involves updating these tags on that VM; mxagent will take care of the rest.</p><p>Our VM workloads are largely infrastructure platforms that don’t typically have code deployed at regular intervals. As such, VMs don’t support a deploy-time upgrade (in the way that Kubernetes workloads can be upgraded when they deploy). Similarly, teams cannot roll back these workloads themselves, but this has been acceptable, given that there are just a handful of such infrastructure platforms.</p><p>The tag updates are managed by a central controller, mxrc, which scans for outdated VMs. If rollouts.yml would result in a different set of resource tags for a VM, the controller will update the tags accordingly. This roughly corresponds to Krispr’s pod admission-time mutation — however, with the caveat that VMs are mutable and long-lived, and thus are upgraded in-place.</p><p>For safety, mxrc takes into account the health of the VM, namely in the form of the <a href=\"https://istio.io/latest/docs/reference/config/networking/workload-group/#ReadinessProbe\">readiness probe status on the WorkloadEntry</a>. Similar to Kubernetes’ maxUnavailable semantics, mxrc aims to keep the number of unavailable VMs (that is, unhealthy VMs plus those with in-progress upgrades) below a defined percentage. It gradually performs these upgrades, aiming to upgrade all the VMs for a workload in two weeks.</p><p>At the end of two weeks, all VMs will match the desired state in rollouts.yml.</p><h3>Conclusion</h3><p>Keeping up-to-date with open-source software is a challenge, especially at scale. Upgrades and other Day-2 operations often become an afterthought, which furthers the burden when upgrades are eventually necessary (to bring in security patches, remain within support windows, utilize new features, and so forth). This is particularly true with Istio, where a version reaches end-of-life support rapidly.</p><p>Even with the complexity and scale of our service mesh, we have successfully upgraded Istio 14 times. This was made possible due to designing for maintainability, building a process that ensures zero downtime, and derisking through the use of gradual rollouts. Similar processes are in use for a number of other foundational infrastructure systems at Airbnb.</p><h3>Future work</h3><p>As Airbnb’s infrastructure continues to evolve and grow, we’re looking at a few key projects to evolve our service mesh:</p><ul><li>Utilizing <a href=\"https://istio.io/latest/docs/ambient/overview/\">Ambient mode</a> as a more cost-effective and easier-to-manage deployment model of Istio. In particular, this simplifies upgrades by not needing to touch workload deployments at all.</li><li>Splitting our singular production mesh into multiple meshes in order to separate fault domains, provide better security isolation boundaries, and scale Istio further. For upgrades, this would further reduce the blast radius, as some meshes that only run low-risk workloads (such as staging) could be upgraded first.</li></ul><p>If this type of work interests you, we encourage you to apply for an <a href=\"https://careers.airbnb.com/\">open position</a> today.</p><h3>Acknowledgements</h3><p>All of our work with Istio is thanks to many different people, including: Jungho Ahn, Stephen Chan, Weibo He, Douglas Jordan, Brian Wolfe, Edie Yang, Dasol Yoon, and Ying Zhu.</p><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bcb0e49c5cf8\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/seamless-istio-upgrades-at-scale-bcb0e49c5cf8\">Seamless Istio Upgrades at Scale</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "Airbnb 在 Kubernetes 上通过分布式数据库实现高可用性 (原标题: Achieving High Availability with distributed database on Kubernetes at Airbnb)",
      "link": "https://medium.com/airbnb-engineering/achieving-high-availability-with-distributed-database-on-kubernetes-at-airbnb-58cc2e9856f4?source=rss----53c7c27702d5---4",
      "pubDate": "Mon, 28 Jul 2025 17:57:46 GMT",
      "isoDate": "2025-07-28T17:57:46.000Z",
      "creator": "Artem Danilov",
      "summary": "## Airbnb 在 Kubernetes 上实现分布式数据库高可用性\n\n### 引言\n\n传统上，组织通过分片策略在高成本的独立服务器上部署数据库以实现扩展。然而，随着数据需求的增长，这种策略的局限性日益明显，维护项目变得越来越长和复杂。分布式横向可扩展数据库已不罕见，其中许多是开源的。但在云环境中以高可用性、低延迟、可扩展性且成本合理的方式可靠运行这些数据库，是许多公司面临的挑战。\n\nAirbnb 采取了一种创新策略：在云环境中跨多个 Kubernetes 集群部署分布式数据库集群。尽管这种设计模式因其复杂性目前尚不常见，但它使 Airbnb 实现了目标系统可靠性和可操作性。本文分享了 Airbnb 如何克服挑战并开发出适用于任何强一致性分布式存储系统的最佳实践。\n\n### 在 Kubernetes 上管理数据库\n\nAirbnb 将一个开源的横向可扩展分布式 SQL 数据库集成到其基础设施中。虽然 Kubernetes 是运行无状态服务的优秀工具，但将其用于数据库等有状态服务具有挑战性，尤其是在节点替换和升级方面。\n\n*   **数据处理挑战**：Kubernetes 缺乏对节点间数据分布的了解，因此每次节点替换都需要仔细的数据处理，以防止数据仲裁丢失和服务中断，包括在替换节点前复制数据。\n*   **存储解决方案**：Airbnb 选择使用 AWS EBS 将存储卷附加到节点，这使得在节点替换时可以快速将卷重新附加到新的虚拟机。得益于 Kubernetes 的持久卷声明（PVC），这种重新附加是自动发生的。\n*   **自定义操作符**：为了确保新的存储节点有足够时间赶上集群的当前状态，Airbnb 依赖于自定义的 Kubernetes 操作符（k8s operator），该操作符允许根据应用程序的特定需求定制各种 Kubernetes 操作。\n\n### 协调节点替换\n\n节点替换的原因多种多样，包括 AWS 实例退役、Kubernetes 升级或配置更改。Airbnb 将节点替换事件分为三类：\n\n1.  **数据库发起的事件**：如配置更改或版本升级。\n2.  **主动基础设施事件**：如实例退役或节点升级。\n3.  **计划外基础设施故障**：如节点无响应。\n\n*   **安全管理**：\n    *   对于数据库发起的事件，k8s-operator 中实现了自定义检查，以在删除任何 Pod 之前验证所有节点是否正常运行。\n    *   为了与基础设施发起的事件串行化，在 Kubernetes 中实现了一个准入钩子（admission hook），用于拦截 Pod 驱逐。该钩子拒绝任何驱逐 Pod 的尝试，但会在 Pod 上分配一个自定义注解，自定义数据库 k8s-operator 会监听并根据该注解安全地删除 Pod，从而与任何数据库发起的节点替换串行化。\n    *   对于计划外基础设施故障（如硬件故障），无法协调。但可以通过确保在故障硬件被替换之前，阻止前两类事件的任何节点替换来提高可用性。\n*   **k8s 操作符的作用**：在 Airbnb 的基础设施中，k8s 操作符处理主动和基础设施触发的节点替换，在节点替换时保持数据一致性，并确保计划外事件不会影响正在进行的维护。\n\n### Kubernetes 升级\n\n定期的 Kubernetes 升级至关重要，但可能是高风险操作，特别是对于数据库而言。云托管的 Kubernetes 在控制平面升级后可能不提供回滚功能，如果出现问题，会带来潜在的灾难恢复挑战。Airbnb 采用自管理 Kubernetes 集群，允许回滚控制平面，但糟糕的 Kubernetes 升级仍可能导致服务中断，直到回滚完成。\n\n### 通过多 Kubernetes 集群确保容错性\n\nAirbnb 认为实现高区域可用性的最佳方式是将每个数据库部署在三个独立的 Kubernetes 集群中，每个集群位于不同的 AWS 可用区（AZ）。\n\n*   **隔离性**：AWS 可用区不仅提供独立的电源、网络和连接，还进行逐区域的发布。Kubernetes 集群与 AWS AZ 的对齐意味着任何底层基础设施问题或错误部署的爆炸半径都有限，因为它们仅限于单个 AZ。\n*   **分阶段部署**：在内部，Airbnb 还会将新配置或新数据库版本首先部署到位于单个 Kubernetes 集群中一个 AZ 内的逻辑集群的一部分。\n*   **可用性提升**：尽管这种设置增加了复杂性，但它通过限制来自每一层（无论是数据库、Kubernetes 还是 AWS 基础设施）的故障部署所引起问题的爆炸半径，显著提高了可用性。\n*   **弹性示例**：最近，基础设施中一次错误的配置部署突然终止了暂存 Kubernetes 集群中特定类型的所有虚拟机，删除了大部分查询层 Pod。然而，由于中断被隔离到单个 Kubernetes 集群，三分之二的查询层节点保持运行，从而避免了任何影响。\n*   **容量冗余**：Airbnb 还对数据库集群进行过量配置，以确保即使整个 AZ、Kubernetes 集群或区域内的所有存储节点发生故障，仍有足够的容量来处理流量。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/0*X2h18kiTtfcbRXQo)\n\n![图片 2](https://cdn-images-1.medium.com/max/1024/0*1hSoKkktPABPkTj)\n\n### 利用 AWS EBS 实现可靠性和延迟处理\n\nEBS 为 Airbnb 的部署提供了两个关键优势：\n\n1.  **快速重新附加**：在节点替换期间能够快速重新附加。\n2.  **卓越的持久性**：与本地磁盘相比具有更强的持久性。\n\n*   **副本数量**：借助 EBS，Airbnb 仅使用三个副本即可自信地运行高可用集群，无需额外冗余即可保持可靠性。\n*   **延迟缓解**：EBS 偶尔会出现尾部延迟峰值，p99 延迟可能达到 1 秒。为缓解此问题，Airbnb 实施了存储读取超时会话变量，允许查询在 EBS 延迟峰值期间透明地重试其他存储节点。\n    *   **读策略优化**：默认情况下，所使用的数据库将所有请求和重试发送给领导者。为了在 EBS 健康的存储节点上启用重试，必须允许从领导者和副本进行读取，但原始请求优先选择最近的副本。这带来了降低延迟和避免跨 AZ 网络成本的额外好处，因为每个 AZ 都有一个副本。\n    *   **陈旧读取**：对于允许的用例，利用陈旧读取功能，使副本能够独立提供读取服务，而无需对领导者进行同步调用（领导者可能在读取时遇到 EBS 延迟峰值）。\n\n### 结论：探索 Kubernetes 上的开源数据库\n\nAirbnb 在 Kubernetes 上运行分布式数据库的实践使其实现了高可用性、低延迟、可扩展性和更低的维护成本。通过利用操作符模式、多集群部署、AWS EBS 和陈旧读取，Airbnb 证明了即使是开源分布式存储系统也能在云环境中蓬勃发展。\n\nAirbnb 已在生产环境中运行多个数据库集群，其中最大的一个集群在 150 个存储节点上处理 300 万 QPS，存储超过 300TB 数据，分布在 400 万个内部分片中。得益于本文描述的技术，所有这些都实现了 99.95% 的可用性。\n\n对于其他考虑在 Kubernetes 上运行开源数据库的公司来说，机遇是巨大的。拥抱挑战，运行开源数据库以塑造这些工具以供企业使用。云中可扩展、可靠数据管理的未来在于协作和开源创新。",
      "shortSummary": "Airbnb 通过在多个 AWS 可用区内的 Kubernetes 集群上部署分布式数据库，实现了高可用性。他们利用自定义 Kubernetes 操作符、AWS EBS 进行存储和可靠性，并协调节点替换，以确保数据一致性和故障容错。这种策略使 Airbnb 的生产数据库实现了 99.95% 的可用性、低延迟、高可扩展性及更低的维护成本，证明了开源分布式存储系统在云环境中的可行性。",
      "translated_title": "Airbnb 在 Kubernetes 上通过分布式数据库实现高可用性",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*X2h18kiTtfcbRXQo",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*1hSoKkktmPABPkTj",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=58cc2e9856f4",
          "alt": "",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*X2h18kiTtfcbRXQo\" /></figure><h3>Introduction</h3><p>Traditionally, organizations have deployed databases on costly, high-end standalone servers using sharding for scaling as a strategy. As data demands grew, the limitations of this strategy became increasingly evident with increasingly longer and more complex maintenance projects.</p><p>Increasingly distributed horizontally scalable databases are not uncommon and many of them are open source. However, running these databases reliably in the cloud with high availability, low latency and scalability, all at a reasonable cost is a problem many companies are trying to solve.</p><p>We chose an innovative strategy of deploying<strong> a distributed database cluster across multiple Kubernetes clusters in a cloud environment</strong>. Although currently an uncommon design pattern due to its complexity, this strategy allowed us to achieve target system reliability and operability.</p><p>In this post, we’ll share how we overcame challenges and the best practices we’ve developed for this strategy and we believe these best practices should be applicable to any other strongly consistent, distributed storage systems.</p><h3>Managing Databases on Kubernetes</h3><p>Earlier this year, we integrated an open source horizontally scalable, distributed SQL database into our infrastructure.</p><p>While Kubernetes is a great tool for running stateless services, the use of Kubernetes for stateful services — like databases — is challenging, particularly around node replacement and upgrades.</p><p>Since Kubernetes lacks knowledge of data distribution across nodes, each node replacement requires careful data handling to prevent data quorum loss and service disruption, this includes copying the data before replacing a node.</p><p>At Airbnb, we opted to attach storage volumes to nodes using AWS EBS, this allows quick volume reattachment to new virtual machines upon node replacement. Thanks to Kubernetes’ Persistent Volume Claims (<a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#binding\">PVC</a>), this reattachment happens automatically. In addition we need to allow time for a new storage node to catch up with the cluster’s current state before moving to the next node replacement. For this, we rely on the custom <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/operator/\">k8s operator</a><a href=\"https://github.com/pingcap/tidb-operator\">,</a> which allows us to customize various Kubernetes operations according to specifics of the application.</p><h3>Coordinating Node Replacement</h3><p>Node replacements occur for various reasons, from <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-retirement.html\">AWS instance retirement</a> to Kubernetes upgrades or configuration changes. To address these cases, we categorize node replacement events into three groups:</p><ol><li><strong>Database-initiated events:</strong> Such as config changes or version upgrades.</li><li><strong>Proactive infrastructure events:</strong> Like instance retirements or node upgrades.</li><li><strong>Unplanned infrastructure failures:</strong> Such as a node becoming unresponsive.</li></ol><p>To safely manage node replacements for database-initiated events, we implemented a a custom check in the k8s-operator that verifies that all nodes are up and running before deleting any pod.</p><p>In order to serialize it with the second group initiated by infrastructure, we implemented <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/\">an admission hook</a> in k8s to intercept pod eviction. This admission hook rejects any attempt to evict the pod, but assigns a custom annotation on the pod which our customer database k8s-operator watches and acts on to safely delete the pod serializing it with any database-initiated node replacements described above.</p><p>Node replacements due to unplanned infrastructure failure events like hardware failure, can’t be coordinated. But we can still improve availability by ensuring that any node replacement event from the first two groups will be blocked until the failed hardware is replaced.</p><p>In our infrastructure the k8s operator handles both proactive and infrastructure-triggered node replacements, maintaining data consistency in the presence of node replacements and ensuring that unplanned events don’t impact ongoing maintenance.</p><h3>Kubernetes Upgrades</h3><p>Regular Kubernetes upgrades are essential but can be high-risk operations, especially for databases. Cloud managed Kubernetes might not offer rollbacks once the control plane is upgraded, posing a potential disaster recovery challenge if something goes wrong. While our approach involves using self-managed Kubernetes clusters, which does allow rolling back the control plane, a bad Kubernetes upgrade could still cause service disruption till rollback is completed.</p><h3>Ensuring Fault Tolerance with Multiple Kubernetes clusters</h3><p>At Airbnb, we think the best way to achieve high regional availability is to deploy each database across three independent Kubernetes clusters, each within a different AWS availability zone (<a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-fault-isolation-boundaries/availability-zones.htm\">AZ</a>). AWS uses availability zones not just for independent power, networking, and connectivity, but they also do rollouts zone by zone. Our Kubernetes cluster alignment with AWS AZ also means that any underlying infrastructure issues or bad deployments have a limited blast radius as they are restricted to a single AZ. Internally, we also deploy a new configuration or a new database version to a part of the logical cluster running in a single Kubernetes cluster in one AZ first.</p><p>While this setup adds complexity, it significantly boosts availability by limiting the blast radius of any issues stemming from faulty deployments at every layer — whether database, Kubernetes, or AWS infrastructure.</p><p>For instance, recently, a faulty config deployment in our infrastructure abruptly terminated all VMs of a specific type in our staging Kubernetes cluster, deleting most of the query layer pods. However, since the disruption was isolated to a single Kubernetes cluster, two-thirds of our query layer nodes remained operational, preventing any impact.</p><p>We also overprovision our database clusters to ensure that, even if an entire AZ, Kubernetes cluster, or all storage nodes within a zone goes down, we still have sufficient capacity to handle traffic.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*1hSoKkktmPABPkTj\" /></figure><h3>Leveraging AWS EBS for Reliability and Latency Handling</h3><p>EBS offers two key benefits for our deployment: rapid reattachment during node replacements and superior durability compared to local disks. With EBS, we confidently run a highly available cluster using only three replicas, maintaining reliability without needing additional redundancy.</p><p>However, EBS can occasionally experience tail latency spikes, with p99 latency reaching up to 1 second. To mitigate this, we implemented a storage read timeout session variable, allowing queries to transparently retry against other storage nodes during EBS latency spikes. By default the database we use sends all requests and retries to the leader. To enable retries on storage nodes with healthy EBS, we have to allow reads from both leader and replica reads, but prefer the closest one for the original request. This brings the added benefit of reduced latency and no cross-AZ network costs, as we have a replica in each AZ. Finally, for use cases that permit it, we leverage stale reads feature, enabling reads to be served independently by the replica without requiring synchronous calls to the leader, which may be experiencing an EBS latency spike at the time of the read.</p><h3>Conclusion: Exploring Open Source Databases on Kubernetes</h3><p>Our journey running a distributed database on Kubernetes has empowered us to achieve high availability, low latency, scalability, and lower maintenance costs. By leveraging the operator pattern, multi-cluster deployments, AWS EBS, and stale reads, we’ve demonstrated that even open source distributed storage systems can thrive in cloud environments.</p><p>We already operate several database clusters in production in the described setup, with the largest one handling 3M QPS across 150 storage nodes, storing over 300+ TB of data spread across 4M internal shards. All this with 99.95% availability thanks to techniques described in this post.</p><p>For other companies considering to run open-source databases on Kubernetes, the opportunities are immense. Embrace the challenge, run open-source databases to shape these tools for enterprise use. The future of scalable, reliable data management in the cloud lies in collaboration and open-source innovation — now is the time to lead and participate.</p><h3>Acknowledgments</h3><p>Thanks to Abhishek Parmar, Brian Wolfe, Chen Ding, Daniel Low, Hao Luo, Xiaomou Wang for collaboration and Shylaja Ramachandra for editing.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=58cc2e9856f4\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/achieving-high-availability-with-distributed-database-on-kubernetes-at-airbnb-58cc2e9856f4\">Achieving High Availability with distributed database on Kubernetes at Airbnb</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "理解和改进 SwiftUI 性能 (原标题: Understanding and Improving SwiftUI Performance)",
      "link": "https://medium.com/airbnb-engineering/understanding-and-improving-swiftui-performance-37b77ac61896?source=rss----53c7c27702d5---4",
      "pubDate": "Tue, 24 Jun 2025 16:43:07 GMT",
      "isoDate": "2025-06-24T16:43:07.000Z",
      "creator": "Cal Stephens",
      "summary": "## 理解和改进 SwiftUI 性能\n\n### 引言\n\nAirbnb 自2022年起采用 SwiftUI，显著提升了工程师的生产力。然而，大规模应用 SwiftUI 也带来了性能挑战，例如常见的低效代码模式和累积的性能损耗。为解决这些问题，Airbnb 开发了新的工具，用于主动识别和静态验证代码的正确性。\n\n### Airbnb 的 SwiftUI 功能架构\n\nAirbnb 多年来一直使用基于 UIKit 的 Epoxy 库和单向数据流系统。在 SwiftUI 屏幕层中，他们选择继续沿用现有的单向数据流库，这简化了在大型代码库中逐步采用 SwiftUI 的过程，并提高了功能质量和可维护性。然而，他们发现使用该库的 SwiftUI 功能性能未达预期，且问题原因并不明显。\n\n### 理解 SwiftUI 视图差异化（Diffing）\n\n在 SwiftUI 等声明式 UI 系统中，确保框架知道何时需要重新评估和重新渲染视图至关重要。当父视图更新时，SwiftUI 通过比较视图的存储属性来检测变化。理想情况下，视图的 `body` 仅在其属性实际改变时才会被重新评估。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/1*qBYJ9abMpZyuODmbHkYD5Q.jpeg)\n\n![图片 2: 理想的视图重新评估流程](https://cdn-images-1.medium.com/max/1024/1*LvNBJSor0RDThlW3Oq7mWw.png)\n\n然而，实际情况并非总是如此，不必要的视图 `body` 评估会因执行不必要的工作而损害性能。为了可视化视图 `body` 在实际应用中被重新评估的频率，Airbnb 使用了一个修饰符，每次渲染时给视图应用一个随机颜色。测试发现，许多视图被不必要地重新评估和重新渲染。\n\n![图片 3: 不必要的视图重新评估示例](https://cdn-images-1.medium.com/max/1024/1*yNrr8e8yI9RcKg6Z-VUhEA.gif)\n\n### SwiftUI 视图差异化算法\n\nSwiftUI 内置的差异化算法虽然未正式文档化，但对性能影响巨大。它使用基于反射的算法来比较视图的每个存储属性：\n\n*   **`Equatable` 类型：** SwiftUI 使用其 `Equatable` 一致性比较新旧值。\n*   **值类型（如结构体）：** 递归比较每个实例属性。\n*   **引用类型（如类）：** 使用引用标识进行比较。\n*   **闭包：** 尝试按标识比较，但大多数非简单闭包无法可靠比较。\n\n如果视图的所有属性与前一个值比较后都相等，则 `body` 不会被重新评估，内容也不会重新渲染。使用 `@State` 和 `@Environment` 等 SwiftUI 属性包装器修饰的值不参与此差异化算法，而是通过其他机制触发视图更新。\n\nAirbnb 在其代码库中发现了几种常见的模式会混淆 SwiftUI 的差异化算法：\n\n*   **闭包：** 某些类型（如闭包）本质上不受支持，几乎总是比较为不相等，导致不必要的 `body` 评估。\n*   **简单数据类型：** 存储在视图上的简单数据类型可能意外地按引用而非值进行比较，例如包装了内部引用类型的写时复制（copy-on-write）结构体。\n\n如果视图包含任何不可差异化的值，整个视图将变得不可差异化。这揭示了 Airbnb 单向数据流库的性能问题：其动作处理是基于闭包的，而 SwiftUI 无法差异化闭包。在许多情况下，使值可差异化需要大量侵入性且可能不理想的架构更改，且难以防止后续回归。\n\n### 控制 SwiftUI 视图差异化\n\n幸运的是，有另一种选择：如果视图遵循 `Equatable` 协议，SwiftUI 将使用其 `Equatable` 一致性进行差异化，而非默认的基于反射的算法。这允许开发者选择性地决定哪些属性应参与比较。例如，对于动作处理程序，如果它不影响视图内容或标识，则可以将其排除在 `Equatable` 比较之外。\n\n然而，手动编写和维护 `Equatable` 一致性会带来大量样板代码，且容易出错（例如，添加新属性时忘记更新 `Equatable`）。\n\n为此，Airbnb 创建了一个新的 **`@Equatable` 宏**，自动生成 `Equatable` 一致性。该宏会比较视图的所有存储实例属性（排除 `@State` 和 `@Environment` 等属性包装器）。不影响视图 `body` 输出且不可 `Equatable` 的属性可以使用 `@SkipEquatable` 标记，将其排除在生成的实现之外。这使得 Airbnb 可以继续使用基于闭包的动作处理程序，而不会影响 SwiftUI 的差异化过程。\n\n采用 `@Equatable` 宏后，视图保证可差异化。如果工程师后续添加了不可 `Equatable` 的属性，构建将失败，从而突出潜在的差异化行为回归。这使得 `@Equatable` 宏成为一个复杂的代码检查工具（linter），对于在大规模代码库中扩展性能改进非常有价值。\n\n### 管理视图体（View Body）大小\n\nSwiftUI 差异化的另一个重要方面是理解 SwiftUI 只能差异化真正的 `View` 结构体。任何其他代码，例如计算属性或生成 SwiftUI 视图的辅助函数，都无法被差异化。\n\n例如，将复杂的视图 `body` 拆分为单独的计算属性（如 `headerSection` 和 `actionCardSection`）是常见的组织方式，但运行时 SwiftUI 会将这些视图内联到主视图 `body` 中。这意味着，当屏幕的任何部分状态改变时，整个视图 `body` 都会被重新评估，随着视图变得更大更复杂，这将导致大量的非必要工作，损害性能。\n\n为提高性能，应将布局代码实现为独立的 SwiftUI 视图。这允许 SwiftUI 正确差异化每个子视图，仅在必要时重新评估其 `body`。例如，将 `MyScreen` 拆分为独立的 `HeaderSection` 和 `CardSection` 视图，并为它们应用 `@Equatable`，可以确保 `HeaderSection` 仅在 `title` 改变时重新评估，而 `CardSection` 仅在 `isCardSelected` 改变时重新评估。通过将视图分解为更小、可差异化的部分，SwiftUI 可以高效地仅更新实际改变的部分。\n\n### 视图体复杂度 Lint 规则\n\n大型、复杂的视图在开发过程中并不总是显而易见的。为了帮助工程师了解何时需要将视图重构为更小、可差异化的部分，Airbnb 创建了一个自定义的 SwiftLint 规则。该规则使用 SwiftSyntax 解析视图 `body` 并测量其复杂度。复杂度指标定义为每次使用计算属性、函数或闭包组合视图时增加的值。当视图复杂度超过可配置的限制（目前为10）时，该规则会在 Xcode 中自动触发警告。\n\n![图片 4: SwiftLint 视图复杂度警告示例](https://cdn-images-1.medium.com/max/1024/1*Tdo1L8qZf81FWFeJaNY6yQ.png)\n\n### 结论\n\n通过理解 SwiftUI 视图差异化机制，Airbnb 运用了三项关键技术：\n\n1.  **`@Equatable` 宏：** 确保视图 `body` 仅在视图内部的值实际改变时才被重新评估。\n2.  **拆分视图：** 将视图分解为更小的部分，以实现更快的重新评估。\n3.  **复杂度 Lint 规则：** 鼓励开发者在视图变得过大和复杂之前进行重构。\n\n将这些技术应用于 Airbnb 应用中的 SwiftUI 视图，显著减少了不必要的视图重新评估和重新渲染。例如，在搜索栏和筛选面板中，重新渲染的次数明显减少。\n\n![图片 5: 应用技术后减少重新渲染的示例](https://cdn-images-1.medium.com/max/1024/1*tWhEXK5kyFP5KYPCUSvp6Q.gif)\n\n根据页面性能评分系统的数据，在最复杂的 SwiftUI 屏幕中采用这些技术确实改善了用户体验。例如，通过在主要搜索屏幕上最重要的视图中采用 `@Equatable` 并将大型视图体拆分为更小的可差异化部分，滚动卡顿减少了15%。这些技术还提供了灵活性，允许 Airbnb 采用最适合其需求的功能架构，而不会牺牲性能或施加繁重的限制（例如，完全避免在 SwiftUI 视图中使用闭包）。\n\n当然，这些技术并非万能药，并非所有 SwiftUI 功能都必须使用它们，它们本身也不足以保证出色的性能。然而，理解它们的工作原理和原因，为构建高性能 SwiftUI 功能奠定了宝贵的基础，并使得在代码中发现和避免问题模式变得更容易。",
      "shortSummary": "Airbnb 团队为解决 SwiftUI 性能问题，主要聚焦于优化视图差异化（diffing）和管理视图体大小。他们开发了 `@Equatable` 宏，自动生成 `Equatable` 一致性，确保视图仅在关键属性变化时更新，并允许跳过不可差异化的属性（如闭包）。同时，他们提倡将大型视图拆分为更小的独立组件，并创建了 SwiftLint 规则来检测并限制视图复杂度，鼓励及时重构。这些措施显著减少了不必要的视图重新渲染，提升了应用性能，例如搜索页面滚动卡顿减少15%。",
      "translated_title": "理解和改进 SwiftUI 性能",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*qBYJ9abMpZyuODmbHkYD5Q.jpeg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*LvNBJSor0RDThlW3Oq7mWw.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*yNrr8e8yI9RcKg6Z-VUhEA.gif",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*Tdo1L8qZf81FWFeJaNY6yQ.png",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*tWhEXK5kyFP5KYPCUSvp6Q.gif",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<p>New techniques we’re using at Airbnb to improve and maintain performance of SwiftUI features at scale</p><p>By <a href=\"https://www.linkedin.com/in/calstephens/\">Cal Stephens</a>, <a href=\"https://www.linkedin.com/in/miguel-jimenez-b98216112\">Miguel Jimenez</a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*qBYJ9abMpZyuODmbHkYD5Q.jpeg\" /></figure><p>Airbnb <a href=\"https://medium.com/airbnb-engineering/unlocking-swiftui-at-airbnb-ea58f50cde49\">first adopted SwiftUI in 2022</a>, starting with individual components and later expanding to entire screens and features. We’ve seen major improvements to engineers’ productivity thanks to its declarative, flexible, and composable architecture. However, adopting SwiftUI has brought new challenges related to performance. For example, there are many common code patterns in SwiftUI that can be inefficient, and many small papercuts can add up to a large cumulative performance hit. To begin addressing some of these issues at scale, we’ve created new tooling for proactively identifying these cases and statically validating correctness.</p><h3>SwiftUI feature architecture at Airbnb</h3><p>We’ve been leveraging declarative UI patterns at Airbnb for many years, using our UIKit-based <a href=\"https://medium.com/airbnb-engineering/introducing-epoxy-for-ios-6bf062be1670\">Epoxy library</a> and <a href=\"https://medium.com/airbnb-engineering/introducing-epoxy-for-ios-6bf062be1670#fbe0\">unidirectional data flow</a> systems. When adopting SwiftUI in our screen layer, we decided to continue using our existing unidirectional data flow library. This simplified the process of incrementally adopting SwiftUI within our large codebase, and we find it improves the quality and maintainability of features.</p><p>However, we noticed that SwiftUI features using our unidirectional data flow library didn’t perform as well as we expected, and it wasn’t immediately obvious to us what the problem was. Understanding SwiftUI’s performance characteristics is an important requirement for building performant and outside of the “standard” SwiftUI toolbox.</p><h3>Understanding SwiftUI view diffing</h3><p>When working with declarative UI systems like SwiftUI, it’s important to ensure the framework knows which views need to be re-evaluated and re-rendered when the state of the screen changes. Changes are detected by diffing the view’s stored properties any time its parent is updated. Ideally the view’s body will only be re-evaluated when its properties actually change:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*LvNBJSor0RDThlW3Oq7mWw.png\" /></figure><p>However, this behavior is not always the reality (more on why in a moment). Unnecessary view body evaluations hurt performance by performing unnecessary work.</p><p>How do you know how often a view’s body is re-evaluated in a real app? An easy way to visualize this is with a modifier that applies a random color to the view every time it’s rendered. When testing this on various views in our app’s most performance-sensitive screens, we quickly found that many views were re-evaluated and re-rendered more often than necessary:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*yNrr8e8yI9RcKg6Z-VUhEA.gif\" /></figure><h4>The SwiftUI view diffing algorithm</h4><p>SwiftUI’s built-in diffing algorithm is often overlooked and not officially documented, but it has a huge impact on performance. To determine if a view’s body needs to be re-evaluated, SwiftUI uses a reflection-based diffing algorithm to compare each of the view’s stored properties:</p><ol><li>If a type is <em>Equatable</em>, SwiftUI compares the old and new values using the type’s <em>Equatable</em> conformance. Otherwise:</li><li>SwiftUI compares value types (e.g., structs) by recursively comparing each instance property.</li><li>SwiftUI compares reference types (e.g., classes) using reference identity.</li><li>SwiftUI attempts to compare closures by identity. However, most non-trivial closures cannot be compared reliably.</li></ol><p>If all of the view’s properties compare as equal to the previous value, then the body isn’t re-evalulated and the content isn’t re-rendered. Values using SwiftUI property wrappers like<em> @State </em>and <em>@Environment</em> don’t participate in this diffing algorithm, and instead trigger view updates through different mechanisms.</p><p>When reviewing different views in our codebase, we found several common patterns that confounded SwiftUI’s diffing algorithm:</p><ol><li>Some types are inherently not supported, like closures.</li><li>Simple data types stored on the view may be unexpectedly compared by reference instead of by value.</li></ol><p>Here’s an example SwiftUI view with properties that interact poorly with the diffing algorithm:</p><pre>struct MyView: View {<br>  /// A generated data model that is a struct with value semantics,<br>  /// but is copy-on-write and wraps an internal reference type.<br>  /// Compared by reference, not by value, which could cause unwanted body evaluations.<br>  let dataModel: CopyOnWriteDataModel<br><br>  /// Other miscellaneous properties used by the view. Typically structs, but sometimes a class.<br>  /// Unexpected comparisons by reference could cause unwanted body evaluations.<br>  let requestState: MyFeatureRequestState<br><br>  /// An action handler for this view, part of our unidirectional data flow library. <br>  /// Wraps a closure that routes the action to the screen&#39;s action handler.<br>  /// Closures almost always compare as not-equal, and typically cause unwanted body evaluations. <br>  let handler: Handler&lt;MyViewAction&gt;<br><br>  var body: some View { ... }<br>}</pre><p>If a view contains any value that isn’t diffable, the entire view becomes non-diffable. Preventing this in a scalable way is almost impossible with existing tools. This finding also reveals the performance issue caused by our unidirectional data flow library: action handling is closure-based, but SwiftUI can’t diff closures!</p><p>In some cases, like with the action handlers from our unidirectional data flow library, making the value diffable would require large, invasive, and potentially undesirable architecture changes. Even in simpler cases, this process is still time consuming, and there’s no easy way to prevent a regression from creeping in later on. This is a big obstacle when trying to improve and maintain performance at scale in large codebases with many different contributors.</p><h3>Controlling SwiftUI view diffing</h3><p>Fortunately, we have another option: If a view conforms to Equatable, SwiftUI will diff it using its Equatable conformance <em>instead</em> of using the default reflection-based diffing algorithm.</p><p>The advantage of this approach is that it lets us selectively decide which properties should be compared when diffing our view. In our case, we know that the handler object doesn’t affect the content or identity of our view. We only want our view to be re-evalulated and re-rendered when the <em>dataModel</em> and <em>requestState</em> values are updated. We can express that with a custom <em>Equatable</em> implementation:</p><pre>// An Equatable conformance that makes the above SwiftUI view diffable.<br>extension MyView: Equatable {<br>  static func ==(lhs: MyView, rhs: MyView) -&gt; Bool {<br>    lhs.dataModel == rhs.dataModel<br>      &amp;&amp; lhs.requestState == rhs.requestState<br>      // Intentionally not comparing handler, which isn&#39;t Equatable.<br>  }<br>}</pre><p>However:</p><ol><li>This is a lot of additional boilerplate for engineers to write, especially for views with lots of properties.</li><li>Writing and maintaining a custom conformance is error-prone. You can easily forget to update the <em>Equatable</em> conformance when adding new properties later, which would cause bugs.</li></ol><p>So, instead of manually writing and maintaining <em>Equatable</em> conformances, we created a new<em> @Equatable </em>macro that generates conformances for us.</p><pre>// A sample SwiftUI view that has adopted @Equatable<br>// and is now guaranteed to be diffable.<br>@Equatable<br>struct MyView: View {<br>  // Simple data types must be Equatable, or the build will fail.<br>  let dataModel: CopyOnWriteDataModel<br>  let requestState: MyFeatureRequestState<br><br>  // Types that aren&#39;t Equatable can be excluded from the<br>  // generated Equatable conformance using @SkipEquatable,<br>  // as long as they don’t affect the output of the view body.<br>  @SkipEquatable let handler: Handler&lt;MyViewAction&gt;<br><br>  var body: some View { ... }<br>}</pre><p>The <em>@Equatable</em> macro generates an <em>Equatable</em> implementation that compares all of the view’s stored instance properties, excluding properties with SwiftUI property wrappers like<em>@State </em>and <em>@Environment</em> that trigger view updates through other mechanisms. Properties that aren’t <em>Equatable</em> and don’t affect the output of the view body can be marked with <em>@SkipEquatable</em> to exclude them from the generated implementation. This allows us to continue using the closure-based action handlers from our unidirectional data flow library without impacting the SwiftUI diffing process!</p><p>After adopting the <em>@Equatable</em> macro on a view, that view is guaranteed to be diffable. If an engineer adds a non-<em>Equatable</em> property later, the build will fail, highlighting a potential regression in the diffing behavior. This effectively makes the <em>@Equatable</em> macro a sophisticated linter — which is really valuable for scaling these performance improvements in a codebase with many components and many contributors, since it makes it less likely for regressions to slip in later.</p><h3>Managing the size of view bodies</h3><p>Another essential aspect of SwiftUI diffing is understanding that SwiftUI can only diff proper View structs. Any other code, such as computed properties or helper functions that generate a SwiftUI view, cannot be diffed.</p><p>Consider the following example:</p><pre>// Complex SwiftUI views are often simplified by<br>// splitting the view body into separate computed properties.<br>struct MyScreen: View {<br>  /// The unidirectional data flow state store for this feature.<br>  @ObservedObject var store: StateStore&lt;MyState, MyAction&gt;<br><br>  var body: some View {<br>    VStack {<br>      headerSection<br>      actionCardSection<br>    }<br>  }<br><br>  private var headerSection: some View {<br>    Text(store.state.titleString)<br>      .textStyle(.title)<br>  }<br><br>  private var actionCardSection: some View {<br>    VStack {<br>      Image(store.state.cardSelected ? &quot;enabled&quot; : &quot;disabled&quot;)<br>      Text(&quot;This is a selectable card&quot;)<br>    }<br>    .strokedCard(.roundedRect_mediumCornerRadius_12)<br>    .scaleEffectButton(action: {<br>      store.handle(.cardTapped) <br>    })<br>  }<br>}</pre><p>This is a common way to organize complex view bodies, since it makes the code easier to read and maintain. However, at runtime, SwiftUI effectively inlines the views returned from the properties into the main view body, as if we instead wrote:</p><pre>// At runtime, computed properties are no different<br>// from just having a single, large view body!<br>struct MyScreen: View {<br>  @ObservedObject var store: StateStore&lt;MyState, MyAction&gt;<br><br>  // Re-evaluated every time the state of the screen is updated.<br>  var body: some View {<br>    Text(store.state.titleString)<br>      .textStyle(.title)<br><br>    VStack {<br>      Image(store.state.cardSelected ? &quot;enabled&quot; : &quot;disabled&quot;)<br>      Text(&quot;This is a selectable card&quot;)<br>    }<br>    .strokedCard(.roundedRect_mediumCornerRadius_12)<br>    .scaleEffectButton(action: {<br>      store.handle(.cardTapped) <br>    })<br>  }<br>}</pre><p>Since all of this code is part of the same view body, all of it will be re-evaluated when any part of the screen’s state changes. While this specific example is simple, as the view grows larger and more complicated, re-evaluating it will become more expensive. Eventually there would be a large amount of unnecessary work happening on every screen update, hurting performance.</p><p>To improve performance, we can implement the layout code in separate SwiftUI views. This allows SwiftUI to properly diff each child view, only re-evaluating their bodies when necessary:</p><pre>struct MyScreen: View {<br>  @ObservedObject var store: StateStore&lt;MyState, MyAction&gt;<br><br>  var body: some View {<br>    VStack {<br>      HeaderSection(title: store.state.titleString)<br>      CardSection(<br>       isCardSelected: store.state.isCardSelected,<br>       handler: store.handler,<br>      )<br>    }<br>  }<br>}<br><br>/// Only re-evaluated and re-rendered when the title property changes.<br>@Equatable<br>struct HeaderSection: View {<br>  let title: String<br><br>  var body: some View {<br>    Text(title)<br>      .textStyle(.title)<br>  }<br>}<br><br>/// Only re-evaluated and re-rendered when the isCardSelected property changes.<br>@Equatable<br>struct CardSection: View {<br>  let isCardSelected: Bool<br>  @SkipEquatable let handler: Handler&lt;MyAction&gt;<br><br>  var body: some View {<br>    VStack {<br>      Image(store.state.isCardSelected ? &quot;enabled&quot; : &quot;disabled&quot;)<br>      Text(&quot;This is a selectable card&quot;)<br>    }<br>    .strokedCard(.roundedRect_mediumCornerRadius_12)<br>    .scaleEffectButton(action: {<br>      handler.handle(.cardTapped) <br>    })<br>  }<br>}</pre><p>By breaking the view into smaller, diffable pieces, SwiftUI can efficiently update only the parts of the view that actually changed. This approach helps maintain performance as a feature grows more complex.</p><h4>View body complexity lint rule</h4><p>Large, complex views aren’t always obvious during development. Easily available metrics like total line count aren’t a good proxy for complexity. To help engineers know when it’s time to refactor a view into smaller, diffable pieces, we created a custom <a href=\"https://github.com/realm/SwiftLint\">SwiftLint</a> rule that parses the view body using <a href=\"https://github.com/swiftlang/swift-syntax\">SwiftSyntax</a> and measures its complexity. We defined the view complexity metric as a value that increases every time you compose views using computed properties, functions, or closures. With this rule we automatically trigger an alert in Xcode when a view is getting too complex. (The complexity limit is configurable, and we currently allow a maximum complexity level of 10.)</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Tdo1L8qZf81FWFeJaNY6yQ.png\" /><figcaption>The rule shows as a warning during local Xcode builds alerting engineers as early as possible. In this screenshot, the complexity limit is set to 3, and this specific view has a complexity of 5.</figcaption></figure><h3>Conclusion</h3><p>With an understanding of how SwiftUI view diffing works, we can use an <em>@Equatable</em> macro to ensure view bodies are only re-evaluated when the values inside views actually change, break views into smaller parts for faster re-evaluation, and encourage developers to refactor views before they get too large and complex.</p><p>Applying these three techniques to SwiftUI views in our app has led to a large reduction in unnecessary view re-evaluation and re-renders. Revisiting the examples from earlier, you see far fewer re-renders in the search bar and filter panel:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*tWhEXK5kyFP5KYPCUSvp6Q.gif\" /></figure><p>Using results from our <a href=\"https://medium.com/airbnb-engineering/airbnbs-page-performance-score-on-ios-36d5f200bc73\">page performance score</a> system, we’ve found that adopting these techniques in our most complicated SwiftUI screens really does improve performance for our users. For example, we reduced <a href=\"https://medium.com/airbnb-engineering/airbnbs-page-performance-score-on-ios-36d5f200bc73#4c63\">scroll hitches</a> by<strong> </strong>15% on our main Search screen by adopting <em>@Equatable</em> on its most important views, and breaking apart large view bodies into smaller diffable pieces. These techniques also give us the flexibility to use a feature architecture that best suits our needs without compromising performance or imposing burdensome limitations (e.g., completely avoiding closures in SwiftUI views).</p><p>Of course, these techniques aren’t a silver bullet. It’s not necessary for all SwiftUI features to use them, and these techniques by themselves aren’t enough to guarantee great performance. However, understanding how and why they work serves as a valuable foundation for building performant SwiftUI features, and makes it easier to spot and avoid problematic patterns in your own code.</p><p>If you’re interested in joining us on our quest to make the best iOS app in the App Store, please see our <a href=\"https://careers.airbnb.com/\">careers</a> page for open iOS roles.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=37b77ac61896\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/understanding-and-improving-swiftui-performance-37b77ac61896\">Understanding and Improving SwiftUI Performance</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "在 Airbnb 使用 Impulse 进行负载测试 (原标题: Load Testing with Impulse at Airbnb)",
      "link": "https://medium.com/airbnb-engineering/load-testing-with-impulse-at-airbnb-f466874d03d2?source=rss----53c7c27702d5---4",
      "pubDate": "Mon, 09 Jun 2025 17:45:39 GMT",
      "isoDate": "2025-06-09T17:45:39.000Z",
      "creator": "Chenhao Yang",
      "summary": "# Airbnb 的 Impulse 负载测试实践\n\n## 引言\n系统级负载测试对于确保系统可靠性和效率至关重要，它能帮助识别瓶颈、评估峰值流量下的容量、建立性能基线并检测错误。在 Airbnb 这样规模和复杂度的公司，负载测试需要具备健壮性、灵活性和去中心化特性，以支持工程团队进行与 CI 无缝集成的自助式负载测试。\n\nImpulse 是 Airbnb 内部的“负载测试即服务”框架之一。它提供工具来生成合成负载、模拟依赖项以及从生产环境中收集流量数据。本文介绍了 Impulse 的架构，以及它如何最大限度地减少手动工作、无缝集成到可观测性堆栈中，并赋能团队主动解决潜在问题。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/1*3LijjQrJDLVA_ptfeRe83g.jpeg)\n\n## 架构\nImpulse 是一个全面的负载测试框架，允许服务所有者进行上下文感知的负载测试、模拟依赖项并收集流量数据，以确保系统在各种条件下的性能。它包含以下核心组件：\n\n![图片 2](https://cdn-images-1.medium.com/max/1024/1*SFDblGijyiLQfI7C5RpPXw.png)\n*图 1: Impulse 框架及其四个主要组件*\n\n1.  **负载生成器 (Load Generator)**：\n    *   用于根据合成或收集的流量，动态生成上下文感知的请求，以测试不同场景。\n    *   **上下文感知**：允许用户使用 Java 或 Kotlin 编写任意测试逻辑，并大规模启动容器来运行这些测试。选择代码而非 DSL/配置的原因在于：\n        *   **灵活性**：编程语言比 DSL 更具表达力，能更好地支持复杂的上下文场景。\n        *   **可重用性**：相同的测试代码可用于其他测试（如集成测试）。\n        *   **开发者熟练度**：学习曲线低，无需学习新的测试逻辑编写方式。\n        *   **开发者体验**：支持 IDE、测试、调试等。\n    *   **去中心化与容器化**：每次触发负载测试时，都会创建一组新的容器来运行测试，这带来了：\n        *   **隔离性**：不同服务间的负载测试相互隔离，消除干扰。\n        *   **可伸缩性**：容器数量可根据流量需求进行伸缩。\n        *   **成本效益**：容器生命周期短，仅在负载测试运行期间存在。\n    *   Impulse 的负载生成器能将工作负载均匀分布到所有数据中心，并确保总的每秒触发次数 (TPS) 符合配置，从而更好地模拟生产环境中的真实流量分布。\n    *   **执行**：设计用于在 CI/CD 流水线中自动触发。开发者可以配置多个测试阶段（如预热、稳定、峰值），每个阶段可配置测试用例、TPS 和持续时间。\n\n    ![图片 3](https://cdn-images-1.medium.com/max/977/1*WD4EWyWHDQMf_7nDIGkAuA.png)\n    *图 2: 容器化负载生成器*\n\n2.  **依赖模拟器 (Dependency Mocker)**：\n    *   用于模拟下游服务的响应，包括延迟，从而使被测服务 (SUT) 的负载测试无需涉及某些依赖服务。这对于不支持负载测试的供应商服务或在日常部署中不影响下游服务进行回归测试尤为重要。\n    *   Impulse 是一个去中心化框架，每个服务都有自己的依赖模拟器，以消除服务间的干扰并降低通信成本。\n    *   模拟器是进程外服务，独立运行，避免影响 SUT 性能。它们是短生命周期的，仅在测试运行前启动，测试结束后关闭，以节省成本和维护工作。\n    *   响应延迟和异常可配置，模拟器实例数量可按需调整以支持大量流量。\n    *   **其他值得注意的特性**：\n        *   可选择性地模拟部分依赖项，目前支持 HTTP JSON、Airbnb Thrift 和 Airbnb GraphQL 依赖。\n        *   支持负载测试之外的用例，如集成测试。\n    *   **两种模拟响应生成选项**：\n        *   **合成响应 (Synthetic response)**：由用户逻辑生成，类似于集成测试，但响应来自远程（进程外）服务器，并模拟延迟。\n        *   **重放响应 (Replay response)**：从生产下游记录中重放响应，由流量收集器组件支持。\n\n    ![图片 4](https://cdn-images-1.medium.com/max/1024/1*QXMa3Nj3-aSUE_EOvlv1xw.png)\n    *图 3: 依赖模拟器*\n\n3.  **流量收集器 (Traffic Collector)**：\n    *   旨在捕获生产环境中的上游和下游流量及其之间的关系。\n    *   通过复制下游响应（包括生产环境般的延迟和错误），Impulse 能够准确地重放生产流量进行负载测试，避免下游数据或行为的不一致。\n    *   确保测试环境中的服务行为与生产环境中的服务行为一致，从而实现更真实、更可靠的性能评估。\n\n    ![图片 5](https://cdn-images-1.medium.com/max/1024/1*EImg3JUEGzbos5r3_6U-FQ.png)\n    *图 4: 流量收集器*\n\n4.  **测试 API 生成器 (Testing API Generator)**：\n    *   解决事件驱动、异步工作流（如消息队列事件、延迟作业）的测试难题。\n    *   在 CI 阶段根据事件或作业模式创建 HTTP API。这些 API 作为底层异步流的包装器，并专门注册在测试环境中。\n    *   使负载测试工具（如负载生成器）能够向这些合成 API 发送流量，从而使异步流像同步流一样被执行。\n    *   目标是帮助开发者识别异步流实现中的性能瓶颈和潜在问题，以及在高流量条件下的表现，通过绕过消息队列等中间件组件，简化负载测试过程。\n\n    ![图片 6](https://cdn-images-1.medium.com/max/660/1*F3qllm7qqMu4N2k0bBFbdQ.png)\n    *图 5: 异步流的测试 API 生成器*\n\n## 与其他测试框架的集成\nImpulse 的模块化设计促进了其与其他内部测试框架（如集成测试和 API 测试）的集成，为系统服务测试提供了系统化的方法。\n\n![图片 7](https://cdn-images-1.medium.com/max/1024/1*649CFxbpASxHotVVbqdQkQ.png)\n*图 6: Impulse 如何与 Airbnb 其他内部测试框架交互*\n\n## 结论\nImpulse 及其四个核心组件帮助 Airbnb 的开发者进行自助式负载测试。它已在多个客户支持后端服务中实施，并获得了积极反馈，例如：它帮助识别并解决了线程池压力导致的 `ApiClientThreadToolExhaustionException`、客户端 API 调用中的偶发超时错误以及主服务容器中的高内存使用问题，从而优化了资源分配。Impulse 被高度推荐作为开发和测试流程不可或缺的一部分。",
      "shortSummary": "Airbnb 开发了 Impulse，一个内部的“负载测试即服务”框架，旨在实现健壮、灵活和去中心化的自助式负载测试。它包含负载生成器、依赖模拟器、流量收集器和测试 API 生成器四个核心组件。Impulse 能够生成上下文感知负载、模拟依赖、重放生产流量并测试异步工作流。通过与 CI/CD 无缝集成，Impulse 帮助团队主动识别并解决性能瓶颈、评估系统容量，从而提高服务可靠性和效率，并最大限度地减少手动工作。",
      "translated_title": "在 Airbnb 使用 Impulse 进行负载测试",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*3LijjQrJDLVA_ptfeRe83g.jpeg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*SFDblGijyiLQfI7C5RpPXw.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/977/1*WD4EWyWHDQMf_7nDIGkAuA.png",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*QXMa3Nj3-aSUE_EOvlv1xw.png",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*EImg3JUEGzbos5r3_6U-FQ.png",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<p>Comprehensive Load Testing with Load Generator, Dependency Mocker, Traffic Collector, and More</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*3LijjQrJDLVA_ptfeRe83g.jpeg\" /></figure><p>Authors: <a href=\"https://www.linkedin.com/in/chenhao-yang-9799b022/\">Chenhao Yang</a>, <a href=\"https://www.linkedin.com/in/haoyue-wang-a722509a/\">Haoyue Wang</a>, <a href=\"https://www.linkedin.com/in/xiaoyawei/\">Xiaoya Wei</a>, <a href=\"https://www.linkedin.com/in/zhijie-guan/\">Zay Guan</a>, <a href=\"https://www.linkedin.com/in/yaolin-chen-591a31339/\">Yaolin Chen</a> and <a href=\"https://www.linkedin.com/in/fei-yuan/\">Fei Yuan</a></p><p>System-level load testing is crucial for reliability and efficiency. It identifies bottlenecks, evaluates capacity for peak traffic, establishes performance baselines, and detects errors. At a company of Airbnb’s size and complexity, we’ve learned that load testing needs to be robust, flexible, and decentralized. This requires the right set of tools to enable engineering teams to do self-service load tests that integrate seamlessly with CI.</p><p>Impulse is one of our internal load-testing-as-a-service frameworks. It provides tools that can generate synthetic loads, mock dependencies, and collect traffic data from production environments. In this blog post, we’ll share how Impulse is architected to minimize manual effort, seamlessly integrate with our observability stack, and empower teams to proactively address potential issues.</p><h3>Architecture</h3><p>Impulse is a comprehensive load testing framework that allows service owners to conduct context-aware load tests, mock dependencies, and collect traffic data to ensure the system’s performance under various conditions. It includes the following components:</p><ol><li><strong>Load generator</strong> to generate context-aware requests on the fly, for testing different scenarios with synthetic or collected traffic.</li><li><strong>Dependency mocker</strong> to mock the downstream responses with latency, so that the load testing on the service under test (SUT) doesn’t need to involve certain dependent services. This is especially crucial when the dependencies are vendor services that don’t support load testing, or if the team wants to regression load test their service during day-to-day deployment without affecting downstreams.</li><li><strong>Traffic collector</strong> to collect both the upstream and downstream traffic from the production environment, and then apply the resulting data to the test environment.</li><li><strong>Testing API generator</strong> to wrap asynchronous workflows into synchronous API calls for load testing.</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*SFDblGijyiLQfI7C5RpPXw.png\" /><figcaption>Figure 1: The Impulse framework and its four main components</figcaption></figure><p>Each of these four tools are independent, allowing service owners the flexibility to select one or more components for their load testing needs.</p><h4>Load generator</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/977/1*WD4EWyWHDQMf_7nDIGkAuA.png\" /><figcaption>Figure 2: Containerized load generator</figcaption></figure><p><em>Context aware</em></p><p>When load testing, requests made to the SUT often require some information from the previous response or need to be sent in a specific order. For example, if an update API needs to provide an <em>entity_id</em> to update, we must ensure the entity already exists in the testing environment context.</p><p>Our load generator tool allows users to write arbitrary testing logic in Java or Kotlin and launch containers to run these tests at scale against the SUT. Why write code instead of DSL/configuration logic?</p><ul><li>Flexibility: Programming languages are more expressive than DSL and can better support complex contextual scenarios.</li><li>Reusability: The same testing code can be used in other tests, e.g., integration tests.</li><li>Developer proficiency: Low/no learning curve to onboard, don’t need to learn how to write testing logic.</li><li>Developer experience: IDE support, testing, debugging, etc.</li></ul><p>Here is an example of synthetic context-aware test case:</p><pre>class HelloWorldLoadGenerator : LoadGenerator {<br>   override suspend fun run() {<br>       val createdEntity = sutApiClient.create(CreateRequest(name=&quot;foo&quot;, ...)).data<br><br>       // request with id from previous response (context)<br>       val updateResponse = sutApiClient.update(UpdateRequest(id=createdEntity.id, name=&quot;bar&quot;))<br>       <br>       // ... other operations<br>       <br>       // clean up<br>       sutApiClient.delete(DeleteRequest(id=createdEntity.id))<br>   }<br>}</pre><p><em>Decentralized</em></p><p>The load generator is decentralized and containerized, which means each time a load test is triggered, a set of new containers will be created to run the test. This design has several benefits:</p><ul><li>Isolation: Load testing runs between different services are isolated from each other, eliminating any interference.</li><li>Scalability: The number of containers can be scaled up or down according to the traffic requirements.</li><li>Cost efficiency: The containers are short-lived, as they only exist during the load testing run.</li></ul><p>What’s more, as our services are cloud based, a subtle point is that the Impulse framework will evenly distribute the workers among all our data centers, and the load will be emitted evenly from all the workers. Impulse’s load generator ensures the overall trigger per second (TPS) is as configured. Based on this, we can better leverage the locality settings in load balancers, which can better mimic the real traffic distribution in production.</p><p><em>Execution</em></p><p>The load generator is designed to be executed in the CI/CD pipeline, which means we can trigger load testing automatically. Developers can configure the testing spec in multiple phases, e.g., a warm up phase, a steady state phase, a peak phase, etc. Each phase can be configured with:</p><ul><li>Test cases to run</li><li>TPS (trigger per second) of each test case</li><li>Test duration</li></ul><h4>Dependency mocker</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*QXMa3Nj3-aSUE_EOvlv1xw.png\" /><figcaption>Figure 3: Dependency mocker</figcaption></figure><p>Impulse is a decentralized framework where each service has its own dependency mocker. This can eliminate interference between services and reduce communication costs. Each dependency mocker is an out-of-process service, which means the SUT behaves just as it does in production. We run the mockers in separate instances to avoid any impact on the performance of the SUT. The mock servers are all short lived — they only start before tests run and shut down afterwards to save costs and maintenance effort. The response latency and exceptions are configurable and the number of mocker instances can be adjusted on demand to support large amounts of traffic.</p><p>Other noteworthy features:</p><ul><li>You can selectively stub some of the dependencies. Currently, stubbing is supported for HTTP JSON, Airbnb Thrift, and Airbnb GraphQL dependencies.</li><li>The dependency mockers support use cases beyond load testing. For instance, integration tests often rely on other services or third-party API calls, which may not guarantee a stable testing environment or might only support ideal scenarios. Dependency mockers can address this by offering predefined responses or exceptions to fully test those flows.</li></ul><p>Impulse supports two options for generating mock responses:</p><ol><li>Synthetic response: The response is generated by user logic, as in integration testing; the difference is that the response comes from a remote (out-of-process) server with simulated latency.<br>- Similar to the load generator, the logic is written in Java/Kotlin code and contains request matching and response generation.<br>- Latency can be simulated using p95/p99 metrics.</li><li>Replay response: The response is replayed from the production downstream recording, supported by the traffic collector component.</li></ol><p>Here is an example of a synthetic response with latency in Kotlin:</p><pre>downstreamsMocking.every(<br>      thriftRequest&lt;FooRequest&gt;().having { it.message == &quot;hello&quot; }<br>    ).returns { request -&gt;<br>      ThriftDownstream.Response.thriftEncoded(<br>        HttpStatus.OK,<br>        FooResponse.builder.reply(&quot;${request.message} world&quot;).build()<br>      )<br>    }.with {<br>      delay = latencyFromP95(p95=500.miliseconds, min=200.miliseconds, max=2000.miliseconds)<br>    }</pre><h4>Traffic collector</h4><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*EImg3JUEGzbos5r3_6U-FQ.png\" /><figcaption>Figure 4: Traffic collector</figcaption></figure><p>The traffic collector component is designed to capture both upstream and downstream traffic, along with the relationships between them. This approach allows Impulse to accurately replay production traffic during load testing, avoiding inconsistencies in downstream data or behavior. By replicating downstream responses — including production-like latency and errors — via the dependency mocker, the system ensures high-fidelity load testing. As a result, services in the testing environment behave identically to those in production, enabling more realistic and reliable performance evaluations.</p><h4>Testing API generator</h4><p>We rely heavily on event-driven, asynchronous workflows that are critical to our business operations. These include processing events from a message queue (MQ) and executing delayed jobs. Most of the MQ events/jobs are emitted from synchronous flows (e.g., API calls), so theoretically they can be covered by API load testing. However, the real world is more complex. These asynchronous flows often involve long chains of event and job emissions originating from various sources, making it difficult to replicate and test them accurately using only API-based methods.</p><p>To address this, the testing API generator component creates HTTP APIs during the CI stage according to the event or job schema. These APIs act as wrappers around the underlying asynchronous flows and are registered exclusively in the testing environment. This setup enables load testing tools — such as load generators — to send traffic to these synthetic APIs, allowing asynchronous flows to be exercised as if they were synchronous. As a result, it’s possible to perform targeted, realistic load testing on asynchronous logic that would otherwise be hard to simulate.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/660/1*F3qllm7qqMu4N2k0bBFbdQ.png\" /><figcaption>Figure 5: Testing API generator for async flows</figcaption></figure><p>The goal of the testing API generator is to help developers identify performance bottlenecks and potential issues in their async flow implementations and under high traffic conditions. It does this by enabling direct load testing of async flows without involving middleware components like MQs. The rationale is that developers typically aim to evaluate the behavior of their own logic, not the middleware, which is usually already well-tested. By bypassing these components, this approach simplifies the load testing process and empowers developers to independently manage and execute their own tests.</p><h4>Integration with other testing frameworks</h4><p>Airbnb emphasizes product quality, utilizing versatile testing frameworks that cover integration and API tests across development, staging, and production environments, and integrate smoothly into CI/CD pipelines. The modular design of Impulse facilitates its integration with these frameworks, offering systematic service testing.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*649CFxbpASxHotVVbqdQkQ.png\" /><figcaption>Figure 6: How Impulse interfaces with other internal testing frameworks</figcaption></figure><h3>Conclusion</h3><p>In this blog post, we shared how Impulse and its four core components help developers perform self-service load testing at Airbnb. As of this writing, Impulse has been implemented in several customer support backend services and is currently under review with different teams across the company who are planning to leverage Impulse to conduct load testing.</p><p>We’ve received a lot of good feedback in the process. For example: “<em>Impulse helps us to identify and address potential issues in our service. During testing, it detected an ApiClientThreadToolExhaustionException caused by thread pool pressure. Additionally, it alerted us about occasional timeout errors in client API calls during service deployments. Impulse helped us identify high memory usage in the main service container, enabling us to fine-tune the memory allocation and optimize our service’s resource usage. Highly recommend utilizing Impulse as an integral part of the development and testing processes.</em>”</p><h3>Acknowledgments</h3><p>Thanks to Jeremy Werner, Yashar Mehdad, Raj Rajagopal, Claire Cheng, Tim L., Wei Ji, Jay Wu, Brian Wallace for support on the Impulse project.</p><p>Does this type of work interest you? Check out our open roles <a href=\"https://careers.airbnb.com/\">here</a>.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f466874d03d2\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/load-testing-with-impulse-at-airbnb-f466874d03d2\">Load Testing with Impulse at Airbnb</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "大规模倾听、学习与帮助：机器学习如何改变爱彼迎的语音支持体验 (原标题: Listening, Learning, and Helping at Scale: How Machine Learning Transforms Airbnb’s Voice Support…)",
      "link": "https://medium.com/airbnb-engineering/listening-learning-and-helping-at-scale-how-machine-learning-transforms-airbnbs-voice-support-b71f912d4760?source=rss----53c7c27702d5---4",
      "pubDate": "Thu, 29 May 2025 17:29:46 GMT",
      "isoDate": "2025-05-29T17:29:46.000Z",
      "creator": "Yuanpei Cao",
      "summary": "# 大规模倾听、学习与帮助：机器学习如何改变爱彼迎的语音支持体验\n\n爱彼迎致力于提供流畅、直观且有帮助的社区支持体验，无论是协助房客处理预订变更，还是帮助房东解决房源问题。尽管帮助中心和客服聊天机器人能高效解决许多咨询，但部分用户更倾向于与支持代表进行即时语音对话。为了使这些互动更快、更有效，爱彼迎通过机器学习显著改进了其交互式语音应答（IVR）系统。\n\n## 重新构想语音支持旅程\n\n传统的IVR系统通常依赖于僵硬的菜单树，要求呼叫者按键并导航预设路径。爱彼迎设计了一个自适应、会话式的IVR系统，能够实时倾听、理解和响应。当呼叫者联系爱彼迎支持时，通常会发生以下流程：\n\n1.  **呼叫与问候**：IVR接听并提示：“请用几句话告诉我您今天致电的原因。”\n2.  **自动语音识别（ASR）**：呼叫者的回答通过爱彼迎专用的ASR系统转录成文本，保留关键的领域特定术语。\n3.  **理解意图**：一个“联系原因检测”模型将问题分类为取消、退款、账户问题等类别。\n4.  **决策制定**：\n    *   如果可以自助服务，系统会通过短信或应用通知发送相关的帮助文章或智能工作流程。\n    *   如果呼叫者明确请求代理支持或问题需要人工干预，呼叫将路由到客户支持代理，并附带相关详细信息。\n5.  **澄清响应**：一个“释义模型”生成用户意图的摘要，IVR在提供解决方案之前与用户分享。这确保用户理解他们收到的资源的上下文。\n6.  **解决或升级**：呼叫者收到包含相关爱彼迎帮助中心文章链接的短信或应用通知。如果需要进一步帮助，他们可以按0连接客户服务代表。\n\n通过从僵硬的菜单转向自然语言理解，爱彼迎允许房客和房东用自己的话表达问题，从而提高满意度和解决效率。\n\n![爱彼迎IVR核心服务与机器学习组件交互的高级架构](https://cdn-images-1.medium.com/max/1024/1*4MMPeZJDlFDELNNSZ8dGPA.png)\n*图1：爱彼迎IVR核心服务如何与核心机器学习组件交互以通过电话解决用户问题的高级架构。*\n\n## 机器学习驱动的IVR系统核心组件\n\n爱彼迎的IVR系统由以下关键机器学习组件驱动：\n\n### 1. 自动语音识别（ASR）：精准转录\n\n在语音驱动的支持系统中，实现高转录准确性至关重要，尤其是在嘈杂的电话环境中。通用语音识别模型在处理爱彼迎特定术语时常遇到困难。为了提高ASR准确性，爱彼迎：\n\n*   从通用预训练模型转向专门为嘈杂电话音频调整的模型。\n*   引入了领域特定短语列表优化，确保爱彼迎术语被正确识别。\n*   结果：词错误率（WER）从33%显著降低到约10%。这提高了下游帮助文章推荐的准确性，增加了用户参与度，改善了与ASR菜单交互用户的客户NPS，同时减少了对人工代理的依赖并缩短了客户服务处理时间。\n\n### 2. 联系原因预测：理解“为什么”\n\n转录呼叫者陈述后，下一步是识别其意图。爱彼迎通过创建详细的“联系原因分类法”来实现这一点，该分类法涵盖了所有潜在的爱彼迎咨询。一个意图检测模型将呼叫分类到相应的“联系原因”类别。\n\n*   **部署**：在生产环境中，爱彼迎部署了“问题检测服务”来托管意图检测模型，并行运行以实现最佳的可扩展性、灵活性和效率。平均意图检测延迟保持在50毫秒以下，确保了无缝的实时体验。\n*   **特殊情况**：对于呼叫者明确要求与人工代理通话的情况，系统使用不同的意图检测模型来识别此意图，并将呼叫路由到合适的团队。\n\n![意图检测架构和问题检测服务](https://cdn-images-1.medium.com/max/1024/0*oj7NCOlBGYOnNBOx)\n*图2：意图检测架构和问题检测服务。*\n\n### 3. 帮助文章检索：提供正确信息\n\n许多常见的爱彼迎问题可以通过提供清晰、相关的教育信息快速解决。爱彼迎使用“帮助文章检索和排名系统”来自动识别用户查询中的问题，并通过短信和爱彼迎应用通知发送最相关的帮助文章链接。该过程包含两个机器学习阶段：\n\n*   **语义检索和排名**：将爱彼迎帮助文章嵌入索引到向量数据库中，使用余弦相似度高效检索多达30篇相关文章（通常在60毫秒内）。一个基于LLM的排名模型随后对这些检索到的文章进行重新排名，将排名最高的文章直接呈现给用户。\n*   **效果**：该双阶段系统不仅支持IVR互动，还支持客户支持聊天机器人和帮助中心搜索。其有效性通过Precision@N等指标持续评估。\n\n![帮助文章检索和排名系统架构图](https://cdn-images-1.medium.com/max/1024/1*wlkl-CT0czyGqNdKx-ffIw.png)\n*图3：帮助文章检索和排名系统架构图。*\n\n### 4. 释义模型：增强用户理解\n\nIVR客户支持的一个关键挑战是确保用户在接收帮助文章链接之前清楚理解解决方案。为了解决这个问题，爱彼迎实施了一种轻量级的释义方法，利用一组精选的标准化摘要。\n\n*   **实现**：UX撰稿人创建了常见爱彼迎场景的简洁明了的释义。在线服务时，用户查询通过基于文本嵌入相似度的最近邻匹配映射到这些精选摘要。\n*   **效果**：手动评估端到端模型输出证实精度超过90%。在针对联系客户支持的英语房东进行的实验中，在发送文章链接前呈现释义摘要增加了用户对文章内容的参与度，从而提高了自助解决率，减少了对直接客户支持协助的需求。\n\n## 结论\n\n通过结合自动语音识别、联系原因检测系统、帮助文章检索系统和释义模型，爱彼迎创建了一个IVR系统，该系统简化了支持互动并提高了用户满意度。该解决方案使呼叫者能够自然地描述问题，减少了常见查询对人工代理的依赖，并通过自助服务提供即时、相关的支持。当需要人工协助时，系统通过将用户路由到合适的代理并提供必要的上下文，确保了平稳的过渡。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/1*zyT9hDwGkSCvZ-wKEx669w.jpeg)",
      "shortSummary": "爱彼迎利用机器学习彻底改造其语音支持（IVR）系统。通过智能IVR，用户可以自然表达问题，系统通过自动语音识别（ASR）、意图检测、帮助文章检索和释义模型提供自助服务或智能路由。这显著提高了支持效率和用户满意度，减少了对人工代理的依赖，并确保了需要时与合适代理的无缝连接。",
      "translated_title": "大规模倾听、学习与帮助：机器学习如何改变爱彼迎的语音支持体验",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*zyT9hDwGkSCvZ-wKEx669w.jpeg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*4MMPeZJDlFDELNNSZ8dGPA.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*oj7NCOlBGYOnNBOx",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*wlkl-CT0czyGqNdKx-ffIw.png",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b71f912d4760",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<h3><strong>Listening, Learning, and Helping at Scale: How Machine Learning Transforms Airbnb’s Voice Support Experience</strong></h3><p>A look into how Airbnb uses speech recognition, intent detection, and language models to understand users and assist agents more effectively.</p><p><em>By </em><a href=\"https://www.linkedin.com/in/yuanpei-cao-792b103b/\"><em>Yuanpei Cao</em></a><em>, </em><a href=\"https://www.linkedin.com/in/heng-j-1a44a711/\"><em>H</em>eng Ji</a><em>, </em><a href=\"https://www.linkedin.com/in/elaineliu5/\"><em>Elaine Liu</em></a><em>, </em><a href=\"https://www.linkedin.com/in/peng-wang-13117371/\"><em>Peng Wang</em></a><em>, and </em><a href=\"https://www.linkedin.com/in/tiantian-zhang-a4208726/\"><em>Tiantian Zhang</em></a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*zyT9hDwGkSCvZ-wKEx669w.jpeg\" /></figure><p>At Airbnb, we aim to provide a smooth, intuitive, and helpful community support experience, whether it’s helping a guest navigate a booking change or helping a host with a listing issue. While our Help Center and customer support chatbot helps resolve many inquiries efficiently, some users prefer the immediacy of a voice conversation with a support representative. To make these interactions faster and more effective, we’ve significantly improved our Interactive Voice Response (IVR) system via machine learning.</p><p>Over the years, Airbnb has invested in conversational AI to enhance customer support. In our previous blog posts <a href=\"https://medium.com/airbnb-engineering/task-oriented-conversational-ai-in-airbnb-customer-support-5ebf49169eaa\"><em>Task-Oriented Conversational AI in Airbnb Customer Support</em></a> and<a href=\"https://medium.com/airbnb-engineering/using-chatbots-to-provide-faster-covid-19-community-support-567c97c5c1c9\"> <em>Using Chatbots to Provide Faster COVID-19 Community Support</em></a>, we explored how AI-driven chatbots streamline guest and host interactions through automated messaging. This post explains how we extend that work to voice-based support, leveraging machine learning to improve real-time phone interactions with our intelligent IVR system.</p><p>We’ll take you through the end-to-end IVR journey, the key machine learning components that power it, and how we designed a system that delivers faster, more human-like, and more intuitive voice support for our community.</p><h3>Reimagining the voice support journey</h3><p>Traditional IVR systems often rely on rigid menu trees, requiring callers to press buttons and navigate pre-set paths. Instead, we designed an adaptive, conversational IVR that listens, understands, and responds in real time. Here’s normally what happens when a caller reaches out to Airbnb support:</p><ol><li><strong>Call and greeting: </strong>IVR picks up and prompts, <em>“In a few sentences, please tell us why you’re calling today.”</em></li><li><strong>Automated speech recognition (ASR):</strong> The caller’s response is transcribed with Airbnb-specific ASR. For example, if a caller says, <em>“I need to request a refund for my reservation,”</em> ASR accurately converts this speech into text, preserving key domain-specific terms.</li><li><strong>Understanding intent:</strong> A Contact Reason Detection model classifies the issue into a category like cancellations, refunds, account issues, etc.</li><li><strong>Decision-making:</strong> If self-service is possible, the system retrieves and sends a relevant help article or an intelligent workflow via SMS or app notification. If the caller explicitly requests agent support or the issue requires human intervention, the call is routed to a customer support agent with relevant details attached.</li><li><strong>Clarifying response:</strong> A Paraphrasing model generates a summary of the user intent, which IVR shares with the user before delivering the solution. This ensures that users understand the context of the resource they receive. Continuing our example, the system would respond, “<em>I understand your issue is regarding a refund request.</em> <em>We have sent you a link to resources about this topic. Follow the instructions to find answers. If you need to speak with an agent, press 0 to be connected to our customer service representative.</em>” The underscored Paraphrasing component enhances engagement by bridging the gap between system-generated responses and user comprehension, making the self-service experience more intuitive.</li><li><strong>Resolution or escalation:</strong> The caller receives an SMS or app notification with a direct link to a relevant <a href=\"https://www.airbnb.com/help\">Airbnb Help Center</a> article. If further assistance is needed, they can press 0 to connect with a customer service representative.</li></ol><p>By moving away from rigid menus to natural language understanding, we allow guests and hosts to express their issues in their own words, helping to increase satisfaction and resolution efficiency.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*BNyYIWtMRorDGJ935DcQJw.png\" /><figcaption>Figure 1: High-level architecture of how Airbnb IVR Core Service interacts with core machine learning components to resolve user issues over the phone.</figcaption></figure><h3>Breaking down our ML-powered IVR system</h3><h4>1. Automated speech recognition (ASR): transcribing with precision</h4><p>In a voice-driven support system, achieving high transcription accuracy is essential, particularly in noisy phone environments where speech can be unclear. General speech recognition models often struggle with Airbnb-specific terminology, leading to errors like misinterpreting “listing” as “lifting” or “help with my stay” as “happy Christmas Day.” These inaccuracies create challenges in understanding user intent and impact downstream processes.</p><p>To enhance ASR accuracy, we transitioned from a generic high-quality pretrained model to one specifically adapted for noisy phone audio. Additionally, we introduced a domain-specific phrase list optimization that ensures Airbnb terms are properly recognized. Based on a sample of hundreds of clips, this significantly <strong>reduced the word error rate (WER) from 33% to approximately 10%</strong>. The reduced WER significantly enhanced the accuracy of downstream help article recommendations, increasing user engagement, improving customer NPS among users who interacted with the ASR menu, while reducing reliance on human agents and lowering customer service handling time.</p><h4>2. Contact Reason prediction: understanding the why</h4><p>After transcribing the caller’s statements, the next step involves identifying their intent. We accomplished this by creating a detailed Contact Reason taxonomy that categorizes all potential Airbnb inquiries, as elaborated in “<a href=\"https://medium.com/airbnb-engineering/t-leaf-taxonomy-learning-and-evaluation-framework-30ae19ce8c52\">T-LEAF: Taxonomy Learning and EvaluAtion Framework</a>.” We then use an intent detection model to classify calls into a Contact Reason category, ensuring each inquiry is handled appropriately. For example, if a caller mentions “I haven’t received my refund yet,” the model predicts the Contact Reason as Missing Refund and forwards it to the relevant downstream components.</p><p>In production, we deploy the Issue Detection Service to host the intent detection models, running them in parallel to achieve optimal scalability, flexibility, and efficiency. Parallel computing ensures that intent detection <strong>latency remains under 50ms on average</strong>, making the process imperceptible to IVR users and ensuring a seamless real-time experience. The detected intent is then analyzed within the IVR workflow to determine the next action, whether it’s guiding the user through a self-service resolution or escalating directly to a human agent.</p><p>Occasionally, callers prefer to speak directly with a human agent instead of describing their issues, using terms like “agent” or “escalation.” For such scenarios, we use a different intent detection model to recognize when a caller wants to escalate to a human agent. If this intent is detected, the IVR system honors the caller’s request and routes the call to the suitable support team.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Q7LxFhZitCrNdqKAy_ffOw.png\" /><figcaption>Figure 2. Intent detection architecture and Issue Detection Service.</figcaption></figure><h4>3. Help article retrieval: delivering the right information</h4><p>Many common Airbnb issues can be quickly resolved by providing clear and relevant educational information. To help provide useful information to users and minimize the need for human customer support, we use the Help Article Retrieval and Ranking system. This advanced system automatically identifies the issue in a user’s inquiry and delivers the most relevant help article link via SMS text message and Airbnb app notification. Our process incorporates two machine learning stages.</p><p><strong>Semantic retrieval and ranking:</strong> We index Airbnb Help Article embeddings into a vector database, enabling efficient retrieval of up to 30 relevant articles per user query using cosine similarity, typically within 60ms. An LLM-based ranking model then re-ranks these retrieved articles, with the top-ranked article directly presented to users via IVR channels. This dual-stage system not only powers IVR interactions but also supports our customer support chatbot and Help Center search. Across these platforms, its effectiveness is continuously evaluated using metrics like Precision@N, facilitating ongoing improvements and refinements.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*wlkl-CT0czyGqNdKx-ffIw.png\" /><figcaption>Figure 3. Architecture diagram for the Help Article Retrieval and Ranking system.</figcaption></figure><h4>4. Paraphrasing model: enhancing user understanding</h4><p>A key challenge in IVR-based customer support is ensuring users clearly understand the resolution before receiving help article links, as they typically lack visibility into the article’s contents or title. To address this, we implemented a lightweight paraphrasing approach leveraging a curated set of standardized summaries.</p><p>UX writers created concise and clear paraphrases for common Airbnb scenarios. During online serving, user inquiries are mapped to these curated summaries via nearest-neighbor matching based on text embedding similarity. We calibrated a similarity threshold to ensure high-quality matches. Manual evaluation of end-to-end model outputs confirmed precision exceeding 90%.</p><p>The outcome was a finite-state solution delivering the most appropriate paraphrased IVR prompt before presenting a help article link. For example, if a caller states, “I need to cancel my reservation and request a refund,” the model generates a response like “I understand your issue is about a refund request” before sending the retrieved help article link.</p><p>Integrating this model ensures users receive clear, contextually relevant summaries prior to accessing help articles. In an experiment targeting English hosts who contacted customer support, we found that presenting a paraphrased summary before sending the article link increases user engagement with article content, resulting in improvement in self-resolution rates, helping to reduce the need for direct customer support assistance.</p><h3>Conclusion</h3><p>By combining Automated Speech Recognition and Contact Reason Detection systems with a help article retrieval system, and a paraphrasing model, we have created an IVR system that streamlines support interactions and improves user satisfaction. Our solution enables callers to describe issues naturally, reduces dependency on human agents for common inquiries, and provides instant, relevant support through self-service. When human assistance is necessary, the system ensures a smooth transition by routing users to the right agent with essential context.</p><p>Interested in working at Airbnb? Check out our <a href=\"https://careers.airbnb.com/\">open roles</a>.</p><h3><strong>Acknowledgements</strong></h3><p>Thanks to Zhenyu Zhao, Mia Zhao, Wayne Zhang, Lucca Siaudzionis, Lulu Chen, Sukey Xu, Floria Wan, Michael Zhou, Can Yang, Yaolin Chen, Shuaihu Wang, Huifan Qu, Ming Shang,Yu Jiang, Wanting Chen, Elena Zhao, Shanna Su, Cassie Cao, Hao Wang, Haoran Zhu, Xirui Liu, Ying Tan, Xiaohan Zeng, Xiaoyu Meng, Gavin Li, Gaurav Rai, Hemanth Kolla, Ihor Hordiienko, Matheus Scharf, and Stepan Sydoruk who helped bring this vision to life. Also thanks to Paige Schwartz, Stephanie Chu, Neal Cohen, Becky Ajuonuma, Iman Saleh, Dani Normanm, Javier Salido, and Lauren Mackevich for the review and editing.</p><p>Thanks to Jeremy Werner, Joy Zhang, Claire Cheng, Yashar Mehdad, Shuohao Zhang, Shawn Yan, Kelvin Xiong, Michael Lubavin, Teng Wang, Wei Ji, and Chenhao Yang’s leadership support on building conversational AI products at Airbnb.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b71f912d4760\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/listening-learning-and-helping-at-scale-how-machine-learning-transforms-airbnbs-voice-support-b71f912d4760\">Listening, Learning, and Helping at Scale: How Machine Learning Transforms Airbnb’s Voice Support…</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "Airbnb 如何衡量房源生命周期价值 (原标题: How Airbnb Measures Listing Lifetime Value)",
      "link": "https://medium.com/airbnb-engineering/how-airbnb-measures-listing-lifetime-value-a603bf05142c?source=rss----53c7c27702d5---4",
      "pubDate": "Wed, 26 Mar 2025 15:46:46 GMT",
      "isoDate": "2025-03-26T15:46:46.000Z",
      "creator": "Carlos Sanchez Martinez",
      "summary": "## Airbnb 如何衡量房源生命周期价值\n\n### 引言\n\nAirbnb 致力于为用户提供最佳体验，这需要深入理解哪些房源对房客具有高价值。为此，Airbnb 计算并使用房源生命周期价值（Listing Lifetime Value, LTV）的估算值。这些估算不仅有助于识别最受房客欢迎的房源类型，还能帮助 Airbnb 为房东提供资源和建议，以提升其房源所带来的价值。与传统单边销售模式（如零售商向顾客销售商品）不同，本文重点阐述了 Airbnb 如何在多卖家和多买家的平台环境中建模 LTV。\n\n### 房源生命周期价值（LTV）框架\n\nAirbnb 的房源 LTV 框架估算三个关键量：基线 LTV、增量 LTV 和营销诱导增量 LTV。\n\n#### 1. 基线 LTV (Baseline LTV)\n\n基线 LTV 被定义并估算为房源在未来 365 天内将在 Airbnb 上获得的预订总数。Airbnb 依靠机器学习和丰富的房源信息来估算每个独立房源的这一数值。在实践中，还会遵循财务指导，通过预测未来结果并应用相关折现率来计算现值。\n\n![图片 2: 房源LTV估算示例](https://cdn-images-1.medium.com/max/874/1*PLgiegXaNpY8nthDFZpmpA.png)\n\n*   **用途：** 基线 LTV 估算用于对房源进行细分，识别最受房客欢迎的房源类型，从而指导供应拓展策略。它还用于识别那些预计未能充分发挥其预订潜力的房源，这些房源可能受益于额外的指导。\n\n#### 2. 增量 LTV (Incremental LTV)\n\n在估算生命周期价值时，多边市场面临一个共同挑战：一个房源的交易可能以牺牲另一个房源的交易为代价，即“蚕食效应”。例如，当一个新房源加入市场时，它可能会从原本预订其他房源的房客那里获得一些预订。为了准确衡量每个房源所增加的价值，需要考虑这种动态。\n\nAirbnb 通过创建“增量 LTV”估算来解决这一挑战。增量价值是指若无该房源参与，则不会发生的额外交易；而蚕食价值是指即使没有该房源参与，也会发生的交易。增量 LTV 是通过从基线 LTV 中减去蚕食价值估算得出的。\n\n![图片 3: 蚕食效应示意图](https://cdn-images-1.medium.com/max/1024/1*UH0hKFiaFYB-l_LL-CkSRQ.png)\n\n#### 3. 营销诱导增量 LTV (Marketing-induced incremental LTV)\n\n生命周期价值并非静态不变，LTV 模型需要反映内部举措如何带来额外的房源价值。例如，如果 Airbnb 开展一项营销活动，向房东提供成功改善房源的技巧，为了理解该活动的回报，需要衡量因活动而产生的价值，以及如果没有营销干预，自然会产生的价值。营销诱导增量 LTV 用于衡量内部举措创造了多少额外的房源 LTV。\n\n![图片 4: 房源LTV框架概览](https://cdn-images-1.medium.com/max/1024/1*RIUqYmgP_5JWfAohtdCdBQ.png)\n\n### 衡量房源生命周期价值的挑战\n\n#### 1. 准确衡量基线 LTV\n\n框架最重要的要求是准确估算基线 LTV。估算设置利用在估算时点 `t` 捕获的房源特征（包括房源和房东的丰富信息），然后使用这些特征训练机器学习模型。价值标签是未来 365 天内发生的预订数量，在 `t + 365` 日观察到。\n\n![图片 5: 标签与特征收集示意图](https://cdn-images-1.medium.com/max/1024/1*6EanAK-Y42jbcWyATva8GA.png)\n\n*   **影响：** 这种设置意味着需要等待 365 天才能完全评估预测的准确性。此外，如果训练数据捕获时间与模型评分时间之间发生剧烈冲击（如 COVID-19 大流行期间旅行停滞），初始训练数据可能无法进行准确预测。\n*   **应对策略：** 缩短训练窗口以减少模型漂移；向模型输入精细的地理数据和关于外部因素的人工信息；采用 LightGBM 模型处理高基数特征。\n\n#### 2. 衡量增量性\n\n衡量增量性具有挑战性，因为无法观察到真实情况。虽然可以观察到每个房源的预订数量，但无法区分哪些预订是增量的，哪些是从其他房源中“蚕食”而来的。\n\n由于没有增量性标签来直接估算这一结果，Airbnb 转而估算一个“生产函数”。直观地讲，增量性在很大程度上取决于连接市场两端的能力。生产函数有助于识别房源供应和房客需求何时连接并提供增量价值。当某个细分市场房客需求高而房源供应相对较低时，增量性估算值会很高；反之，当房源供应量大而需求相对较低时，增量性会很低。\n\n![图片 6: 生产函数公式](https://cdn-images-1.medium.com/max/1024/1*ccUD00N5xq2IfTiMkZHNGA.png)\n\n#### 3. 处理不确定性\n\n为了应对疫情期间经历的不确定性，Airbnb 开始根据房源实际获得的预订数量与最初预期数量的差异来更新 LTV 估算值。这种方法有助于捕捉初始预测后发生的任何冲击。\n\n在实践中，Airbnb 根据房源的已发生价值、更新后的房源特征以及使用历史数据估算的类似房源的价值到达模式，每日调整房源的预期价值。通过这种方式，预期预订量和已发生预订量会随着时间推移趋近于最终的预订量。\n\n![图片 7: 房源生命周期价值估算更新示例](https://cdn-images-1.medium.com/max/1024/1*vNQ0046lY7rfWrHIJK6Oww.png)\n\n### 结论\n\n估算每个房源的生命周期价值至关重要，因为它有助于 Airbnb 更有效地服务社区。其用例包括：\n\n*   识别独特的房源细分市场，让新房东能向大量房客展示其待客之道。\n*   找出房源有更多预订机会并可能受益于额外需求的地点。\n*   识别哪些内部营销举措为社区带来了最大价值。\n\n该衡量框架还可能扩展到其他应用，例如 Airbnb 体验（Experiences）的生命周期价值，其中体验房源的价值将严重依赖于旅行趋势和房客发现这些体验的能力。\n\nAirbnb 团队持续解决 LTV 相关的有趣问题，并鼓励有志之士探索其团队的开放职位。",
      "shortSummary": "Airbnb通过其房源生命周期价值（LTV）框架，识别并优化对房客有价值的房源。该框架包含基线LTV（预测未来365天预订量）、增量LTV（考虑新房源对现有房源的“蚕食效应”）和营销诱导增量LTV（衡量内部活动带来的价值）。文章详细阐述了LTV的衡量方法，并探讨了准确性、增量性评估和不确定性处理等挑战，例如疫情期间的市场变化。通过机器学习和动态更新，LTV估算帮助Airbnb优化供应策略、指导房东并评估营销效果，从而提升社区体验。",
      "translated_title": "Airbnb 如何衡量房源生命周期价值",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*JSoY6CDkTMQEFXgP",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/874/1*PLgiegXaNpY8nthDFZpmpA.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*UH0hKFiaFYB-l_LL-CkSRQ.png",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*RIUqYmgP_5JWfAohtdCdBQ.png",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*6EanAK-Y42jbcWyATva8GA.png",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<p>A deep dive on the framework that lets us identify the most valuable listings for our guests.</p><p><strong>By:</strong> <a href=\"https://www.linkedin.com/in/carlossanchezmartinez/\">Carlos Sanchez-Martinez</a>, <a href=\"https://www.linkedin.com/in/seanmk2/\">Sean O’Donnell</a>, <a href=\"https://www.linkedin.com/in/lohua-yuan/\">Lo-Hua Yuan</a>, <a href=\"https://www.linkedin.com/in/yunshanz/\">Yunshan Zhu</a></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*JSoY6CDkTMQEFXgP\" /></figure><p>At Airbnb, we always strive to provide our community with the best experience. To do so, it’s important to understand what kinds of accommodation listings are valuable to our guests. We achieve this by calculating and using estimates of <strong>listing lifetime value</strong>. These estimates not only allow us to identify which types of listings resonate best with guests, but also help us develop resources and recommendations for hosts to increase the value driven by their listings.</p><p>Most of the existing literature on lifetime value focuses on traditional sales channels in which a single seller transacts with many buyers (e.g. a retailer selling clothing to a customer). In contrast, this blog post explains how we model lifetime value in a platform like Airbnb, with multiple sellers and buyers. In the first section, we describe our general listings lifetime value framework. In the second section, we discuss relevant challenges when putting this framework into practice.</p><h3>Our Listing Lifetime Value Framework</h3><p>Our listing lifetime value (LTV) framework estimates three different quantities of interest: baseline LTV, incremental LTV, and marketing-induced incremental LTV.</p><h4>(1) Baseline LTV</h4><p>To measure LTV, we need to define what we mean by “value” and what time horizon constitutes a “lifetime.” Simplifying slightly for the purposes of this blog post, we define and estimate our baseline listing LTV as the total number of bookings that a listing will make on Airbnb over the next 365 days.</p><p>We rely on machine learning and the rich information we have about our listings to estimate this quantity for each individual listing. In practice, we also follow financial guidance to arrive at present value by projecting outcomes into the future and applying a relevant discount rate to future value.</p><p>Table 1 shows some hypothetical baseline LTV estimates. As you can see from the examples, LTV is not static, and can evolve as we improve the accuracy of our estimates, observe changes in our marketplace, or even develop a listing (e.g., by providing guidance that helps hosts improve the listing to get more bookings).</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/874/1*PLgiegXaNpY8nthDFZpmpA.png\" /><figcaption><strong>Table 1. Example Listing LTV Estimates</strong></figcaption></figure><p>We use baseline LTV estimates to segment our listings and identify which types of listings resonate best with our guests. This informs our supply expansion strategy. We also use baseline LTV to identify listings that are not expected to reach their full booking potential and may benefit from additional guidance.</p><h4>(2) Incremental LTV</h4><p>When estimating lifetime value, we face a challenge that is common across multi-sided marketplaces: the transactions made by one listing might come at the expense of another listing’s transactions. For example, when a new listing joins our marketplace, this listing will get some bookings from guests who were previously booking other listings. We need to account for this dynamic if we want to accurately measure how much value is <em>added</em> by each listing.</p><p>We address this challenge by creating “incremental<em> </em>LTV” estimates. We refer to the additional transactions that would not have occurred without the listing’s participation as “incremental value,” and the transactions that would have occurred even without the listing’s participation as “cannibalized value.” We estimate the incremental LTV for a listing by subtracting cannibalized value estimates from the baseline LTV. We explain this adjustment in more detail when discussing measurement challenges.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*UH0hKFiaFYB-l_LL-CkSRQ.png\" /><figcaption><strong>Figure 1. Cannibalization.</strong> In this context, cannibalization refers to the transactions that would have occurred even without a listing’s participation in the marketplace. For example, when a new listing joins the platform, some bookings obtained by that listing would have been made at other listings on the platform had the new listing not joined.</figcaption></figure><h4>(3) Marketing-induced incremental LTV</h4><p>Lifetime value is not static, and our LTV model needs to tell us how our internal initiatives bring additional listing value. For example, suppose we run a marketing campaign that provides hosts with tips on how to successfully improve their listings. To understand the return from the campaign, we need to measure how much value is accrued due to the campaign, and how much value would have been organically accrued without our marketing intervention. We calculate “marketing-induced incremental LTV” to measure how much additional listing LTV is created by our internal initiatives.</p><p>Having outlined our measurement framework (summarized in Figure 2), we now cover some of the technical challenges we faced when putting this framework into practice.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*RIUqYmgP_5JWfAohtdCdBQ.png\" /><figcaption><strong>Figure 2.</strong> <strong>Listing LTV Framework</strong></figcaption></figure><h3>Challenges when measuring Listing Lifetime Value</h3><h4>Challenge (1): Accurately measuring baseline LTV</h4><p>The most important requirement for our framework is accurate estimation of baseline LTV. Figure 3 illustrates our estimation setup. First, we leverage listing features snapshotted at estimation time t. This data includes rich knowledge we have about each listing and host (availability, price, location, host tenure, etc). We then use these features to train our machine learning model. As a value label, we use the number of bookings made within the next 365-day period, which is observed on date t + 365.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*6EanAK-Y42jbcWyATva8GA.png\" /><figcaption><strong>Figure 3. Label vs. Feature Collection. </strong>Our label lands 365 days after we collect the initial set of features for our model.</figcaption></figure><p>This setup has two important implications that impact accuracy and evaluation:</p><ul><li>We have to wait 365 days to fully evaluate the accuracy of a prediction.</li><li>Our initial training data might not allow us to make accurate predictions if we observe shocks between the time when the training data was captured, and the time when we score the model.</li></ul><p>In practice, we felt the full consequences of these implications during the COVID-19 pandemic, when travel came to a halt and marketplace dynamics changed drastically. Our model’s training data from before the pandemic had dramatically different characteristics relative to the scoring data we collected after the pandemic. When dealing with this shock, we implemented various strategies that helped us improve model accuracy:</p><ul><li>Reducing training windows, allowing us to reduce model drift.</li><li>Feeding the model with granular geographic data and human-provided information about external factors as borders closed and reopened due to the pandemic.</li><li>Adopting <a href=\"http://lightgbm.readthedocs.io\">LightGBM</a>, which handles high cardinality features like the geographic variables mentioned previously.</li></ul><h4>Challenge (2): Measuring incrementality</h4><p>Accounting for incrementality is challenging because we never observe the ground truth. While we observe how many bookings are made per listing, we cannot tell which bookings are incremental and which bookings are cannibalized from other listings.</p><p>Since we don’t have an incrementality label to estimate this outcome directly, we instead estimate a production function. Intuitively, incrementality is heavily dependent on our ability to connect both sides of our marketplace. Production functions allow us to identify when our supply of listings and demand from guests connect and provide incremental value. Incrementality estimates will be high when a segment has high guest demand and relatively low listing supply. In contrast, incrementality will be low when segments have a large volume of listing supply and relatively low demand, meaning guests have an easy time finding a place to stay and a new listing is more likely to cannibalize bookings from other listings.</p><p>Specifically, we model how our total supply of listings (S) and total demand from guests (D) impacts our target outcome bookings (O), as in equation (1):</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ccUD00N5xq2IfTiMkZHNGA.png\" /></figure><p>We estimate this model with historical supply, demand, and outcome data aggregated across internally-defined segments that have little overlapping demand. Having estimated model (1), we calculate how extra supply of listings results in additional bookings in the given segment: this is our estimate of incrementality.</p><h4>Challenge (3): Handling uncertainty</h4><p>To handle the uncertainty we experienced during the pandemic, we began updating our LTV estimates as listings received greater or fewer numbers of bookings than initially expected. This approach has helped us capture any shocks that occur after making our initial predictions.</p><p>To show how this can be useful, let’s go back to our marketing campaign example. Assume that we run this campaign for six months, and that we measure the success of this campaign by comparing marketing-induced incremental LTV against our total marketing investment in the campaign. As a first approach, we could use the initial baseline LTV figures (which feed into marketing-induced LTV) estimated at the time when the listing was first targeted by our initiative. However, listings targeted on day 1 of the marketing campaign will have six months of booking history by the time the campaign ends and we evaluate success. A more accurate approach uses realized bookings after the initial prediction to start correcting for model error.</p><p>Table 2 illustrates how this works. Suppose that on 2024–01–01, we expect that Listing A will get a total of 16 bookings by the end of the year. If six months into the 365 day period, Listing A has received 16 bookings, we should adjust its expected value upward to, say, 21 bookings. In fact, every day for 365 days after 2024–01–01, we can look at the bookings that Listing A has accrued and adjust the expected bookings accordingly. By construction, the expected and accrued bookings converge to the final bookings 365 days after the initial booking date. Going back to our marketing example, if Listing A ultimately receives 20 bookings, updating the initial estimate means we went from 20% underprediction on day 0 to a more reasonable 5% overprediction as of month 6.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*vNQ0046lY7rfWrHIJK6Oww.png\" /><figcaption><strong>Table 2.</strong> <strong>Example of how we update listing lifetime value estimates.</strong></figcaption></figure><p>In practice, we make daily adjustments to a listing’s expected value based on the listing’s accrued value, updated listing features, and value arrival patterns for similar listings estimated using historical data.</p><h3>Conclusion</h3><p>In this blog post, we explained how we approach listing lifetime value at Airbnb. We covered our measurement framework, including baseline LTV, incremental LTV, and marketing-induced incremental LTV. We also zoomed into measurement challenges, like when travel patterns changed drastically during the COVID pandemic and accurately estimating LTV became more difficult.</p><p>Estimating the lifetime value for each listing is important because it helps us serve our community more effectively. Use cases include:</p><ul><li>Identifying unique listing segments through which new hosts can showcase their hospitality to a large guest audience.</li><li>Pinpointing locations where listings have an opportunity to get more bookings, and might benefit from additional demand.</li><li>Identifying which internal marketing initiatives bring the most value to our community.</li></ul><p>It’s also worth noting that our measurement framework may extend to other applications, such as the lifetime value for Airbnb Experiences listings, where the value of an experience listing will heavily depend on travel trends and on guests’ ability to discover these experiences.</p><p>We continue to solve interesting problems around LTV every day (and as more insights come up, we’ll keep sharing them on our blog). Can you see yourself making an impact here? If so, we encourage you to explore the <a href=\"https://careers.airbnb.com/positions/?_departments=data-science\">open roles on our team</a>.</p><h3>Acknowledgments</h3><p>Finally, we need to give special thanks to Airfam and alumni Sam Barrows, Robert Chang, Linsha Chen, Richard Dear, Andrey Fradkin, Ruben Lobel, Brian De Luna, Dan T. Nguyen, Vaughn Quoss, Jason Ting, and Peng Ye. Without their foundational work, these LTV models would not have been possible.</p><p>Thanks as well to Rebecca Ajuonuma, Carolina Barcenas, Nathan Brixius, Jenny Chen, Peter Coles, Lauren Mackevich, Dan Schmierer, Yvonne Wang, Shanni Weilert, and Jane Zhang for their valuable feedback when writing this blog post.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a603bf05142c\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/how-airbnb-measures-listing-lifetime-value-a603bf05142c\">How Airbnb Measures Listing Lifetime Value</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "基于嵌入的Airbnb搜索召回系统 (原标题: Embedding-Based Retrieval for Airbnb Search)",
      "link": "https://medium.com/airbnb-engineering/embedding-based-retrieval-for-airbnb-search-aabebfc85839?source=rss----53c7c27702d5---4",
      "pubDate": "Wed, 19 Mar 2025 17:02:45 GMT",
      "isoDate": "2025-03-19T17:02:45.000Z",
      "creator": "Huiji Gao",
      "summary": "## 基于嵌入的Airbnb搜索召回系统\n\n**引言**\n\nAirbnb搜索旨在为用户提供最相关的房源，但面对数百万房源、大地理区域搜索（如加利福尼亚、法国）和高需求目的地（如巴黎、伦敦）的挑战，以及灵活日期搜索带来的复杂性，实现这一目标并非易事。为了解决这些问题，Airbnb构建了首个基于嵌入的召回（EBR）搜索系统。该系统的目标是将初始符合条件的房源集缩小到一个更小的池子，以便后续更计算密集型的机器学习模型进行排序。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/0*dhEL1kHnOpCWnqJa)\n\n本文探讨了构建EBR系统的三个关键挑战：\n1.  训练数据构建\n2.  模型架构设计\n3.  使用近似最近邻（ANN）解决方案的在线服务策略\n\n![图片 2](https://cdn-images-1.medium.com/max/924/0*TCeRWXyWhaTJeGfp)\n*图1：Airbnb搜索中各类排序模型的通用阶段和规模。*\n\n**训练数据构建**\n\n构建EBR系统的第一步是训练一个机器学习模型，将房源和去标识化的搜索查询映射为数值向量（嵌入）。为此，团队构建了一个利用对比学习的训练数据管道，该策略涉及为给定查询识别正负样本对。模型在训练过程中学习将查询、正样本房源和负样本房源映射到数值向量，使得查询与正样本房源的相似度远高于与负样本房源的相似度。\n\n为了构建这些样本对，团队设计了一种基于用户行程的采样方法，这对于捕捉用户多阶段搜索旅程至关重要。数据表明，用户在最终预订前会进行多次搜索并采取各种操作。因此，该策略能够捕获整个多阶段旅程并考虑用户可能探索的各种房源类型。\n\n具体而言，团队首先根据位置、客人数量和入住时长等关键查询参数，对所有已预订用户的历史查询进行分组，定义为“行程”。对于每个行程，分析用户执行的所有搜索，并将最终预订的房源作为正样本。负样本则从用户在搜索结果中看到但未预订的房源中选择，以及那些用户曾有更强交互（如加入心愿单）但最终未预订的房源。这种负样本的选择至关重要，因为随机采样房源会使问题过于简单，导致模型性能不佳。\n\n![图片 3](https://cdn-images-1.medium.com/max/878/0*3rXf0K0bJoObo17-)\n*图2：为给定用户旅程构建（正，负）样本对的示例。预订的房源始终被视为正样本。负样本从出现在搜索结果中（并可能被交互过）但用户最终未预订的房源中选择。*\n\n![图片 4](https://cdn-images-1.medium.com/max/558/0*AUApsIPfEFdmx_S-)\n*图3：用于构建EBR模型训练数据的整体数据管道示例。*\n\n**模型架构**\n\n模型架构遵循传统的双塔网络设计：\n*   **房源塔**：处理房源本身的特征，如历史互动、设施和客人容量。\n*   **查询塔**：处理与搜索查询相关的特征，如地理搜索位置、客人数量和入住时长。\n\n这两个塔共同生成房源和搜索查询的嵌入。一个关键的设计决策是选择特征，使得房源塔可以每天离线计算。这使得团队能够通过每日批处理作业预计算房源嵌入，显著降低了在线延迟，因为只有查询塔需要在实时处理传入的搜索请求时进行评估。\n\n![图片 5](https://cdn-images-1.medium.com/max/827/0*rLXZmgqFE5BiSOGS)\n*图4：EBR模型中使用的双塔架构。请注意，房源塔每天离线计算所有房源的嵌入。*\n\n**在线服务**\n\n构建EBR系统的最后一步是选择在线服务的基础设施。团队探索了多种近似最近邻（ANN）解决方案，并最终选择了倒排文件索引（IVF）作为最佳权衡方案，尽管分层可导航小世界（HNSW）在召回率方面略胜一筹。\n\n选择IVF的主要原因包括：\n*   **实时更新量大**：Airbnb房源的定价和可用性数据频繁更新，导致HNSW索引的内存占用过大。\n*   **过滤器兼容性**：大多数Airbnb搜索包含过滤器（尤其是地理过滤器），HNSW与过滤器的并行检索导致延迟性能不佳。\n\n相比之下，IVF解决方案预先对房源进行聚类，只需在搜索索引中存储聚类中心和聚类分配。在服务时，通过将聚类分配视为标准搜索过滤器，从最接近查询嵌入的聚类中检索房源，这使得与现有搜索系统的集成非常直接。\n\n![图片 6](https://cdn-images-1.medium.com/max/603/0*WH3lbXvph3aBkPBY)\n*图5：使用IVF的整体服务流程。房源预先聚类，在在线服务期间，房源从最接近查询嵌入的聚类中检索。*\n\n在EBR模型中选择相似度函数也产生了有趣的影响。团队探索了点积和欧几里得距离；虽然两者在模型性能上相似，但使用欧几里得距离平均产生了更均衡的聚类。这是一个关键的发现，因为IVF检索的质量对聚类大小的均匀性高度敏感：如果一个聚类包含太多房源，将大大降低检索系统的区分能力。团队推测，点积相似度产生这种不平衡是因为它本质上只考虑特征向量的方向而忽略其大小，而许多底层特征是基于历史计数的，使得大小成为一个重要因素。\n\n![图片 7](https://cdn-images-1.medium.com/max/695/0*RlVEZwdCwA5j4cwo)\n*图6：使用点积与欧几里得距离作为相似度度量时聚类大小分布的示例。我们发现欧几里得距离产生了更均衡的聚类大小。*\n\n**成果**\n\n本文描述的EBR系统已在搜索和邮件营销生产环境中全面上线，并通过A/B测试带来了总预订量的统计显著增长。值得注意的是，这一新召回系统带来的预订量提升与过去两年搜索排名中一些最大的机器学习改进相当。与基线相比，EBR系统的主要改进在于它有效地融入了查询上下文，从而在召回阶段更准确地对房源进行排名。这最终帮助Airbnb向用户展示了更相关的结果，特别是对于符合条件结果数量较多的查询。",
      "shortSummary": "Airbnb成功构建并部署了首个基于嵌入的召回（EBR）系统，以优化其搜索体验。该系统通过将房源和查询映射为数值向量，有效缩小了初始房源池。其核心在于基于用户行程构建训练数据、采用双塔模型架构（房源嵌入离线计算），并选择IVF作为在线服务方案以应对实时更新和过滤器挑战。EBR系统已在搜索和邮件营销中全面上线，A/B测试显示总预订量显著增长，效果与重大机器学习改进相当，显著提升了搜索结果的相关性。",
      "translated_title": "基于嵌入的Airbnb搜索召回系统",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*dhEL1kHnOpCWnqJa",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/924/0*TCeRWXyWhaTJeGfp",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/878/0*3rXf0K0bJoObo17-",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/558/0*AUApsIPfEFdmx_S-",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://cdn-images-1.medium.com/max/827/0*rLXZmgqFE5BiSOGS",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*dhEL1kHnOpCWnqJa\" /></figure><p>Our journey in applying embedding-based retrieval techniques to build an accurate and scalable candidate retrieval system for Airbnb Homes search</p><p>Authors: <a href=\"https://www.linkedin.com/in/mustafa-moose-abdool-8aab037a/\">Mustafa (Moose) Abdool</a>, <a href=\"https://www.linkedin.com/in/soumyadip-banerjee-75991b42/\">Soumyadip Banerjee</a>, <a href=\"https://www.linkedin.com/in/kouyang1/\">Karen Ouyang</a>, <a href=\"https://www.linkedin.com/in/do-kyum-kim-9a810417/\">Do-Kyum Kim</a>, <a href=\"https://www.linkedin.com/in/moutupsi-paul/\">Moutupsi Paul</a>, <a href=\"https://www.linkedin.com/in/xiaowei-liu-60415841/\">Xiaowei Liu</a>, <a href=\"https://www.linkedin.com/in/bin-xu-96253aa5/\">Bin Xu</a>, <a href=\"https://www.linkedin.com/in/tracy-xiaoxi-yu/\">Tracy Yu</a>, <a href=\"https://www.linkedin.com/in/hui-gao-275a924/\">Hui Gao</a>, <a href=\"https://www.linkedin.com/in/yangbo-zhu/\">Yangbo Zhu</a>, <a href=\"https://www.linkedin.com/in/huiji-gao/\">Huiji Gao</a>, <a href=\"https://www.linkedin.com/in/liweihe/\">Liwei He</a>, <a href=\"https://www.linkedin.com/in/sanjeevkatariya/\">Sanjeev Katariya</a></p><h3>Introduction</h3><p>Search plays a crucial role in helping Airbnb guests find the perfect stay. The goal of Airbnb Search is to surface the most relevant listings for each user’s query — but with millions of available homes, that’s no easy task. It’s especially difficult when searches include large geographic areas (like California or France) or high-demand destinations (like Paris or London). Recent innovations — such as <em>flexible date search</em>, which allows guests to explore stays without fixed check-in and check-out dates — have added yet another layer of complexity to ranking and finding the right results.</p><p>To tackle these challenges, we need a system that can retrieve relevant homes while also being scalable enough (in terms of latency and compute) to handle queries with a large candidate count. In this blog post, we share our journey in building Airbnb’s first-ever Embedding-Based Retrieval (EBR) search system. The goal of this system is to narrow down the initial set of eligible homes into a smaller pool, which can then be scored by more compute-intensive machine learning models later in the search ranking process.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/924/0*TCeRWXyWhaTJeGfp\" /></figure><p><strong>Figure 1:</strong> The general stages and scale for the various types of ranking models used in Airbnb Search</p><p>We’ll explore three key challenges in building this EBR system: (1) constructing training data, (2) designing the model architecture, and (3) developing an online serving strategy using Approximate Nearest Neighbor (ANN) solutions.</p><h3>Training Data Construction</h3><p>The first step in building our EBR system was training a machine learning model to map both homes and de-identified search queries into numerical vectors. To achieve this, we built a training data pipeline (Figure 3) that leveraged contrastive learning — a strategy that involves identifying pairs of positive- and negative-labeled homes for a given query. During training, the model learns to map a query, a positive home, and a negative home into a numerical vector, such that the similarity between the query and the positive home is much higher than the similarity between the query and the negative home.</p><p>To construct these pairs, we devised a sampling method based on user trips. This was an important design decision, since users on Airbnb generally undergo a multi-stage search journey. Data shows that before making a final booking, users tend to perform multiple searches and take various actions — such as clicking into a home’s details, reading reviews, or adding a home to a wishlist. As such, it was crucial to develop a strategy that captures this entire multi-stage journey and accounts for the diverse types of listings a user might explore.</p><p>Diving deeper, we first grouped all historical queries of users who made bookings, using key query parameters such as location, number of guests, and length of stay — our definition of a “trip.” For each trip, we analyzed all searches performed by the user, with the final booked listing as the positive label. To construct (positive, negative) pairs, we paired this booked listing with other homes the user had seen but not booked. Negative labels were selected from homes the user encountered in search results, along with those they had interacted with more intentfully — such as by wishlisting — but ultimately did not book. This choice of negative labels was key: Randomly sampling homes made the problem too easy and resulted in poor model performance.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/878/0*3rXf0K0bJoObo17-\" /></figure><p><strong>Figure 2: </strong>Example of constructing (positive, negative) pairs for a given user journey. The booked home is always treated as a positive. Negatives are selected from homes that appeared in the search result (and were potentially interacted with) but that the user did not end up booking.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/558/0*AUApsIPfEFdmx_S-\" /></figure><p><strong>Figure 3: </strong>Example of overall data pipeline used to construct training data for the EBR model.</p><h3>Model Architecture</h3><p>The model architecture followed a traditional two-tower network design. One tower (the <em>listing tower</em>) processes features about the home listing itself — such as historical engagement, amenities, and guest capacity. The other tower (the <em>query tower</em>) processes features related to the search query — such as the geographic search location, number of guests, and length of stay. Together, these towers generate the embeddings for home listings and search queries, respectively.</p><p>A key design decision here was choosing features such that the listing tower could be computed offline on a daily basis. This enabled us to pre-compute the home embeddings in a daily batch job, significantly reducing online latency, since only the query tower had to be evaluated in real-time for incoming search requests.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/827/0*rLXZmgqFE5BiSOGS\" /></figure><p><strong>Figure 4: </strong>Two-tower architecture as used in the EBR model. Note that the listing tower is computed offline daily for all homes.</p><h3>Online Serving</h3><p>The final step in building our EBR system was choosing the infrastructure for online serving. We explored a number of approximate nearest neighbor (ANN) solutions and narrowed them down to two main candidates: inverted file index (IVF) and hierarchical navigable small worlds (HNSW). While HNSW performed slightly better in terms of evaluation metrics — using recall as our main evaluation metric — we ultimately found that IVF offered the best trade-off between speed and performance.</p><p>The core reason for this is the high volume of real-time updates per second for Airbnb home listings, as pricing and availability data is frequently updated. This caused the memory footprint of the HNSW index to grow too large. In addition, most Airbnb searches include filters, especially geographic filters. We found that parallel retrieval with HNSW alongside filters resulted in poor latency performance.</p><p>In contrast, the IVF solution, where listings are clustered beforehand, only required storing cluster centroids and cluster assignments within our search index. At serving time, we simply retrieve listings from the top clusters by treating the cluster assignments as a standard search filter, making integration with our existing search system quite straightforward.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/603/0*WH3lbXvph3aBkPBY\" /></figure><p><strong>Figure 5: </strong>Overall serving flow using IVF. Homes are clustered beforehand and, during online serving, homes are retrieved from the closest clusters to the query embedding.</p><p>In this approach, our choice of similarity function in the EBR model itself ended up having interesting implications. We explored both dot product and Euclidean distance; while both performed similarly from a model perspective, using Euclidean distance produced much more balanced clusters on average. This was a key insight, as the quality of IVF retrieval is highly sensitive to cluster size uniformity: If one cluster had too many homes, it would greatly reduce the discriminative power of our retrieval system.</p><p>We hypothesize that this imbalance arises with dot product similarity because it inherently only considers the direction of feature vectors while ignoring their magnitudes — whereas many of our underlying features are based on historical counts, making magnitude an important factor.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/695/0*RlVEZwdCwA5j4cwo\" /></figure><p><strong>Figure 6: </strong>Example of the distribution of cluster sizes when using dot product vs. Euclidean distance as a similarity measure. We found that Euclidean distance produced much more balanced cluster sizes.</p><h3>Results</h3><p>The EBR system described in this post was fully launched in both Search and Email Marketing production and led to a statistically-significant gain in overall bookings when A/B tested. Notably, the bookings lift from this new retrieval system was on par with some of the largest machine learning improvements to our search ranking in the past two years.</p><p>The key improvement over the baseline was that our EBR system effectively incorporated query context, allowing homes to be ranked more accurately during retrieval. This ultimately helped us display more relevant results to users, especially for queries with a high number of eligible results.</p><h3>Acknowledgments</h3><p>We would like to especially thank the entire Search and Knowledge Infrastructure &amp; ML Infrastructure org (led by <a href=\"https://www.linkedin.com/in/yi-li-755a6b24/\">Yi Li</a>) and Marketing Technology org (led by <a href=\"https://www.linkedin.com/in/michael-kinoti-7a309215/\">Michael Kinoti</a>) for their great collaborations throughout this project!</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=aabebfc85839\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/embedding-based-retrieval-for-airbnb-search-aabebfc85839\">Embedding-Based Retrieval for Airbnb Search</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "使用大型语言模型加速大规模测试迁移 (原标题: Accelerating Large-Scale Test Migration with LLMs)",
      "link": "https://medium.com/airbnb-engineering/accelerating-large-scale-test-migration-with-llms-9565c208023b?source=rss----53c7c27702d5---4",
      "pubDate": "Thu, 13 Mar 2025 17:01:05 GMT",
      "isoDate": "2025-03-13T17:01:05.000Z",
      "creator": "Charles Covey-Brandt",
      "summary": "Airbnb最近完成了一项由大型语言模型（LLM）驱动的大规模代码迁移，将近3500个React组件测试文件从Enzyme更新为使用React Testing Library (RTL)。这项工作原预计需要1.5年的工程时间，但通过结合前沿模型和强大的自动化，仅用6周就完成了整个迁移。\n\n**背景与挑战**\n\n*   **技术栈演进：** Airbnb自2020年起为所有新的React组件测试开发采用了RTL，逐步淘汰Enzyme。\n*   **Enzyme的局限性：** Enzyme设计用于早期React版本，其对组件内部的深度访问不再符合现代React测试实践。\n*   **迁移难题：** 由于Enzyme和RTL之间的根本差异，无法简单替换或删除Enzyme文件，这会造成代码覆盖率的显著缺失。需要一种自动化方法来重构测试文件，同时保留原始测试意图和代码覆盖率。\n\n**LLM驱动的迁移方案**\n\n2023年的一次黑客马拉松证明LLM能够成功转换Enzyme文件。在此基础上，Airbnb于2024年开发了一个可扩展的LLM驱动迁移管道，其核心策略包括：\n\n1.  **文件验证与重构步骤**\n    *   将迁移分解为一系列自动化的验证和重构步骤，类似于生产流水线。\n    *   流程被建模为状态机，文件只有在通过前一阶段的验证后才能进入下一阶段，失败时LLM会介入修复。\n    *   这种分步方法提供了坚实的基础，便于跟踪进度、改进特定步骤的失败率，并支持并发处理数百个文件。\n    *   ![图片 2](https://cdn-images-1.medium.com/max/587/0*cHwpLgo6nzx8bROe)\n        *   图示：从Enzyme重构、修复Jest、修复lint和tsc，到标记文件完成的重构步骤。\n\n2.  **重试循环与动态提示**\n    *   最有效的改进策略是“暴力”重试：多次重试步骤直到通过或达到限制。\n    *   每次重试都使用动态提示，将验证错误和文件的最新版本提供给LLM。\n    *   ![图片 3](https://cdn-images-1.medium.com/max/962/0*XtBaesbBgYOBY_uP)\n        *   图示：重试循环。对于给定步骤N，如果文件有错误，则重试验证并尝试修复，直到达到最大重试次数或文件不再包含错误。\n    *   此方法成功迁移了大量中低复杂度的测试文件，多数在10次尝试内完成。\n\n3.  **增加上下文**\n    *   对于复杂文件，通过向提示中注入尽可能多的相关上下文来提高成功率。\n    *   提示扩展到4万至10万个token，包含多达50个相关文件、手动编写的少量示例以及项目中现有的优秀测试文件。\n    *   每个提示包含：被测组件的源代码、待迁移的测试文件、当前步骤的验证失败信息、同目录下的相关测试（保持团队特定模式）、通用迁移指南和常见解决方案。\n    *   这种丰富的上下文方法对于复杂文件非常有效，帮助LLM更好地理解团队模式和代码库架构。\n    *   首次批量运行在4小时内成功迁移了75%的目标文件。\n\n4.  **从75%到97%：系统性改进**\n    *   为解决剩余的“长尾”文件（约900个），构建了系统性改进工具。\n    *   **可见性：** 自动生成代码注释，记录每个迁移步骤的状态，便于识别常见问题。\n    *   **灵活重跑：** 能够轻松重跑单个文件或按特定步骤过滤的文件。\n    *   通过“采样、调整、扫描”的反馈循环：运行所有失败文件，找出LLM常见问题；选择代表性样本；更新提示和脚本；对样本验证修复；再次对所有剩余文件运行。\n    *   经过4天的此循环，完成率从75%提升到97%，仅剩不到100个文件。\n\n**结果与影响**\n\n*   **效率显著提升：** 自动化迁移了3500个Enzyme文件中的97%，仅用6周时间，而手动预计需1.5年。\n*   **手动干预：** 剩余3%的文件（约100个）通过自动化生成的基线进行手动修复，额外耗时一周。\n*   **质量保证：** 成功替换Enzyme，同时保持了原始测试意图和整体代码覆盖率。\n*   **成本效益：** 总成本（包括LLM API使用和6周工程时间）远低于手动迁移的估计。\n*   ![图片 1](https://cdn-images-1.medium.com/max/1024/1*j0QXnA13Sy5ruaIAU5C_eg.jpeg)\n    *   图示：Airbnb使用LLM将3500个React组件测试文件从Enzyme迁移到React Testing Library，将预计1.5年的工作量缩短至6周。\n\n**展望**\n\n此次迁移凸显了LLM在大规模代码转换中的强大潜力。Airbnb计划扩展此方法，开发更复杂的迁移工具，并探索LLM驱动自动化在提升开发者生产力方面的新应用。",
      "shortSummary": "Airbnb利用大型语言模型（LLMs）成功地将近3500个React组件测试文件从Enzyme迁移到React Testing Library。通过构建自动化管道、采用重试机制和增加上下文，他们将原预计1.5年的手动工作量缩短至仅6周，并实现了97%的自动化迁移率。此举显著提升了开发效率，并展示了LLM在大规模代码转换中的巨大潜力。",
      "translated_title": "使用大型语言模型加速大规模测试迁移",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/1*j0QXnA13Sy5ruaIAU5C_eg.jpeg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/587/0*cHwpLgo6nzx8bROe",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/962/0*XtBaeswbgYOBY_uP",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9565c208023b",
          "alt": "",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*j0QXnA13Sy5ruaIAU5C_eg.jpeg\" /></figure><p>By: <a href=\"https://www.linkedin.com/in/chazcb\">Charles Covey-Brandt</a></p><p>Airbnb recently completed our first large-scale, LLM-driven code migration, updating nearly 3.5K React component test files from Enzyme to use React Testing Library (RTL) instead. We’d originally estimated this would take 1.5 years of engineering time to do by hand, but — using a combination of frontier models and robust automation — we finished the entire migration in just 6 weeks.</p><p>In this blog post, we’ll highlight the unique challenges we faced migrating from Enzyme to RTL, how LLMs excel at solving this particular type of challenge, and how we structured our migration tooling to run an LLM-driven migration at scale.</p><h3>Background</h3><p>In 2020, Airbnb adopted React Testing Library (RTL) for all new React component test development, marking our first steps away from Enzyme. Although Enzyme had served us well since 2015, it was designed for earlier versions of React, and the framework’s deep access to component internals no longer aligned with modern React testing practices.</p><p>However, because of the fundamental differences between these frameworks, we couldn’t easily swap out one for the other (read more about the differences <a href=\"https://kentcdodds.com/blog/introducing-the-react-testing-library\">here</a>). We also couldn’t just delete the Enzyme files, as analysis showed this would create significant gaps in our code coverage. To complete this migration, we needed an automated way to refactor test files from Enzyme to RTL while preserving the intent of the original tests <em>and</em> their code coverage.</p><h3>How We Did It</h3><p>In mid-2023, an Airbnb hackathon team demonstrated that large language models could successfully convert hundreds of Enzyme files to RTL in just a few days.</p><p>Building on this promising result, in 2024 we developed a scalable pipeline for an LLM-driven migration. We broke the migration into discrete, per-file steps that we could parallelize, added configurable retry loops, and significantly expanded our prompts with additional context. Finally, we performed breadth-first prompt tuning for the long tail of complex files.</p><h3>1. File Validation and Refactor Steps</h3><p>We started by breaking down the migration into a series of automated validation and refactor steps. Think of it like a production pipeline: each file moves through stages of validation, and when a check fails, we bring in the LLM to fix it.</p><p>We modeled this flow like a state machine, moving the file to the next state only after validation on the previous state passed:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/587/0*cHwpLgo6nzx8bROe\" /><figcaption>Diagram shows refactor steps from Enzyme refactor, fixing Jest, fixing lint and tsc, and marking file as complete.</figcaption></figure><p>This step-based approach provided a solid foundation for our automation pipeline. It enabled us to track progress, improve failure rates for specific steps, and rerun files or steps when needed. The step-based approach also made it simple to run migrations on hundreds of files concurrently, which was critical for both quickly migrating simple files, and chipping away at the long tail of files later in the migration.</p><h3>2. Retry Loops &amp; Dynamic Prompting</h3><p>Early on in the migration, we experimented with different prompt engineering strategies to improve our per-file migration success rate. However, building on the stepped approach, we found the most effective route to improve outcomes was simply brute force: retry steps multiple times until they passed or we reached a limit. We updated our steps to use dynamic prompts for each retry, giving the validation errors and the most recent version of the file to the LLM, and built a loop runner that ran each step up to a configurable number of attempts.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/962/0*XtBaeswbgYOBY_uP\" /><figcaption><em>Diagram of a retry loop. For a given step N, if the file has errors, we retry validation and attempt to fix errors unless we hit the max retries or the file no longer contains errors.</em></figcaption></figure><p>With this simple retry loop, we found we could successfully migrate a large number of our simple-to-medium complexity test files, with some finishing successfully after a few retries, and most by 10 attempts.</p><h3>3. Increasing the Context</h3><p>For test files up to a certain complexity, just increasing our retry attempts worked well. However, to handle files with intricate test state setups or excessive indirection, we found the best approach was to push as much relevant context as possible into our prompts.</p><p>By the end of the migration, our prompts had expanded to anywhere between 40,000 to 100,000 tokens, pulling in as many as 50 related files, a whole host of manually written <a href=\"https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/multishot-prompting\">few-shot</a> examples, as well as examples of existing, well-written, passing test files from within the same project.</p><p>Each prompt included:</p><ul><li>The source code of the component under test</li><li>The test file we were migrating</li><li>Validation failures for the step</li><li>Related tests from the same directory (maintaining team-specific patterns)</li><li>General migration guidelines and common solutions</li></ul><p>Here’s how that looked in practice (significantly trimmed down for readability):</p><pre>// Code example shows a trimmed down version of a prompt <br>// including the raw source code from related files, imports, <br>// examples, the component source itself, and the test file to migrate.<br><br>const prompt = [<br>  &#39;Convert this Enzyme test to React Testing Library:&#39;,<br>  `SIBLING TESTS:\\n${siblingTestFilesSourceCode}`,<br>  `RTL EXAMPLES:\\n${reactTestingLibraryExamples}`,<br>  `IMPORTS:\\n${nearestImportSourceCode}`,<br>  `COMPONENT SOURCE:\\n${componentFileSourceCode}`,<br>  `TEST TO MIGRATE:\\n${testFileSourceCode}`,<br>].join(&#39;\\n\\n&#39;);</pre><p>This rich context approach proved highly effective for these more complex files — the LLM could better understand team-specific patterns, common testing approaches, and the overall architecture of the codebase.</p><p>We should note that, although we did some prompt engineering at this step, the main success driver we saw was choosing the <em>right</em> related files (finding nearby files, good example files from the same project, filtering the dependencies for files that were relevant to the component, etc.), rather than getting the prompt engineering perfect.</p><p>After building and testing our migration scripts with retries and rich contexts, when we ran our first bulk run, <strong>we successfully migrated 75% of our target files in just four hours</strong>.</p><h3>4. From 75% to 97%: Systematic Improvement</h3><p>That 75% success rate was really exciting to get to, but it still left us with nearly 900 files failing our step-based validation criteria. To tackle this long tail, we needed a systematic way to understand where remaining files were getting stuck and improve our migration scripts to address these issues. We also wanted to do this <em>breadth first</em> to aggressively chip away at our remaining files without getting stuck on the most difficult migration cases.</p><p>To do this, we built two features into our migration tooling.</p><p>First, we built a simple system to give us visibility into common issues our scripts were facing by stamping files with an automatically-generated comment to record the status of each migration step. Here’s what that code comment looked like:</p><pre>// MIGRATION STATUS: {&quot;enyzme&quot;:&quot;done&quot;,&quot;jest&quot;:{&quot;passed&quot;:8,&quot;failed&quot;:2,&quot;total&quot;:10,&quot;skipped&quot;:0,&quot;successRate&quot;:80},&quot;eslint&quot;:&quot;pending&quot;,&quot;tsc&quot;:&quot;pending&quot;,}</pre><p>And second, we added the ability to easily re-run single files or path patterns, filtered by the specific step they were stuck on:</p><pre>$ llm-bulk-migration --step=fix-jest --match=project-abc/**</pre><p>Using these two features, we could quickly run a feedback loop to improve our prompts and tooling:</p><ol><li>Run all remaining failing files to find common issues the LLM is getting stuck on</li><li>Select a sample of files (5 to 10) that exemplify a common issue</li><li>Update our prompts and scripts to address that issue</li><li>Re-run against the sample of failing files to validate our fix</li><li>Repeat by running against all remaining files again</li></ol><p>After running this “sample, tune, sweep” loop for 4 days, we had pushed our completed files from 75% to 97% of the total files, and had just under 100 files remaining. By this point, we had retried many of these long tail files anywhere between 50 to 100 times, and it seemed we were pushing into a ceiling of what we could fix via automation. Rather than invest in more tuning, we opted to manually fix the remaining files, working from the baseline (failing) refactors to reduce the time to get those files over the finish line.</p><h3>Results and Impact</h3><p>With the validation and refactor pipeline, retry loops, and expanded context in place, we were able to automatically migrate 75% of our target files in 4 hours.</p><p>After four days of prompt and script refinement using the “sample, tune, and sweep” strategy, we reached 97% of the 3.5K original Enzyme files.</p><p>And for the remaining 3% of files that didn’t complete through automation, our scripts provided a great baseline for manual intervention, allowing us to complete the migration for those remaining files in another week of work.</p><p>Most importantly, we were able to replace Enzyme while maintaining original test intent and our overall code coverage. And even with high retry counts on the long tail of the migration, the total cost — including LLM API usage and six weeks of engineering time — proved far more efficient than our original manual migration estimate.</p><h3>What’s Next</h3><p>This migration underscores the power of LLMs for large-scale code transformation. We plan to expand this approach, develop more sophisticated migration tools, and explore new applications of LLM-powered automation to enhance developer productivity.</p><p>Want to help shape the future of developer tools? We’re hiring engineers who love solving complex problems at scale. Check out our <a href=\"https://careers.airbnb.com\">careers page</a> to learn more.</p><h3>****************</h3><p><em>All product names, logos, and brands are property of their respective owners. All company, product and service names used in this website are for identification purposes only. Use of these names, logos, and brands does not imply endorsement.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9565c208023b\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/accelerating-large-scale-test-migration-with-llms-9565c208023b\">Accelerating Large-Scale Test Migration with LLMs</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "改进地图搜索排名 (原标题: Improving Search Ranking for Maps)",
      "link": "https://medium.com/airbnb-engineering/improving-search-ranking-for-maps-13b03f2c2cca?source=rss----53c7c27702d5---4",
      "pubDate": "Wed, 18 Dec 2024 18:02:37 GMT",
      "isoDate": "2024-12-18T18:02:37.000Z",
      "creator": "Malay Haldar",
      "summary": "## 改进Airbnb地图搜索排名\n\nAirbnb的搜索结果通过两种界面展示：列表视图（list-results）和地图视图（map-results）。最初，两者的核心排名算法相同，都基于房源的预订概率。然而，针对列表视图设计的排名假设在地图视图中不再适用，因为地图上的房源是分散的，没有固定的排名顺序，用户注意力也不会像列表那样从上到下衰减。\n\n### 地图视图的独特之处\n\n*   **列表视图：** 用户注意力从列表顶部开始衰减，越往下关注度越低。排名算法通过预订概率对房源进行排序，以最大化用户与房东的连接。 \n    ![图1：按房源搜索排名划分的点击率](https://cdn-images-1.medium.com/max/1024/0*Y9drAzLenJ9GAYEA)\n*   **地图视图：** 房源以图钉形式散布在地图上，没有固定的排名列表，用户注意力也不会因排名位置而衰减。因此，直接按预订概率排序的策略不再适用。 \n    ![图2：地图结果](https://cdn-images-1.medium.com/max/624/0*6iaMrBpbSQjVnsLF)\n\n### 逐步优化用户注意力模型\n\n为了适应地图界面，Airbnb逐步改进了用户注意力模型：\n\n#### 1. 均匀用户注意力模型\n\n*   **假设：** 初始假设用户注意力均匀分布在所有地图图钉上。然而，用户点击的图钉数量有限，大量图钉可能导致用户错过最佳选择。 \n    ![图3：按搜索者百分比划分的点击不同地图图钉数量](https://cdn-images-1.medium.com/max/1024/0*Vi5l4XPrl3YdHsP0)\n*   **解决方案：** 引入一个参数，限制所选地图图钉中最高预订概率与最低预订概率的比率。限制越严格，显示的地图图钉的平均预订概率越高。\n*   **效果：** 通过A/B测试，限制版本显著提升了预订量，尤其是高质量预订（5星评价的行程增加），并减少了用户发现所需房源所需的印象数和点击数。 \n    ![图4：通过在线A/B实验进行探索](https://cdn-images-1.medium.com/max/1024/0*trGxNfKu4rHa4Gpx)\n\n#### 2. 分层用户注意力模型\n\n*   **背景：** 在桌面搜索中，左侧通常显示固定数量（例如18个）的列表结果，每个结果都需要一个对应的地图图钉，因此无法简单限制图钉数量。 \n    ![图6：桌面搜索结果](https://cdn-images-1.medium.com/max/1024/0*A83SEjyDlyTUCI06)\n*   **解决方案：** 将地图图钉分为两层：\n    *   **常规椭圆形图钉（带价格）：** 用于预订概率最高的房源。\n    *   **迷你图钉（不带价格）：** 用于预订概率相对较低的房源。迷你图钉的点击率比常规图钉低约8倍，从而引导用户注意力。 \n        ![图5：带价格的椭圆形图钉和迷你图钉](https://cdn-images-1.medium.com/max/1024/0*pkL4ovuWpR1Rz9z-)\n*   **效果：** 通过A/B实验，该方法成功将用户注意力优先引向预订概率最高的房源。 \n    ![图7：分层地图图钉的实验结果](https://cdn-images-1.medium.com/max/842/0*1V-XbGegLzPch25O)\n\n#### 3. 折扣用户注意力模型\n\n*   **洞察：** 通过绘制地图上不同坐标处图钉的点击率，发现用户注意力并非完全均匀，而是随位置变化（移动端和桌面端有所不同）。 \n    ![图8：地图坐标上地图图钉的点击率](https://cdn-images-1.medium.com/max/598/0*rDDubemWn97XvCN2)\n    ![图8：地图坐标上地图图钉的点击率](https://cdn-images-1.medium.com/max/602/0*I9GtvJEw5BGfHn96)\n*   **解决方案：** 设计算法重新定位地图中心，使预订概率最高的房源更靠近中心。算法评估一系列潜在坐标，选择最接近高预订概率房源的作为新中心。 \n    ![图9：寻找最优中心的算法](https://cdn-images-1.medium.com/max/1024/0*IqlsENiSd-9IdQ5v)\n*   **效果：** 在线A/B实验显示，该算法使未取消预订量提升0.27%，地图移动次数减少1.5%，表明用户使用地图的努力程度降低。\n\n### 结论与展望\n\n用户与地图的交互方式与列表截然不同。通过逐步精细化用户与地图的交互模型，Airbnb显著改善了用户的实际体验。然而，当前方法仍面临挑战：如何在地图上展示所有可用房源？这将是未来的工作重点。更深入的技术细节已在KDD '24会议上发表的研究论文中讨论。",
      "shortSummary": "Airbnb改进了其地图搜索排名算法，以适应地图界面与列表界面的根本差异。针对地图上用户注意力分布的特点，团队迭代开发了三种模型：首先，通过限制图钉预订概率范围，提升了预订量和预订质量；其次，在固定图钉数量的场景下，引入分层图钉（常规图钉和迷你图钉）以引导用户注意力；最后，通过算法重新定位地图中心，使高预订概率房源更靠近中心，进一步优化了用户体验并减少了操作。这些改进显著提升了用户体验和预订转化率，但如何在地图上展示所有房源仍是未来的挑战。",
      "translated_title": "改进地图搜索排名",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*DO7m1JZFPSvVRlBG",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*Y9drAzLenJ9GAYEA",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/624/0*6iaMrBpbSQjVnsLF",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*Vi5l4XPrl3YdHsP0",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*trGxNfKu4rHa4Gpx",
          "alt": "",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "RSS",
      "content": "<p>How Airbnb is adapting ranking for our map interface.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*DO7m1JZFPSvVRlBG\" /></figure><p><a href=\"https://www.linkedin.com/in/malayhaldar/\">Malay Haldar</a>, <a href=\"https://www.linkedin.com/in/hongwei-zhang-86b15624/\">Hongwei Zhang</a>, <a href=\"https://www.linkedin.com/in/kedar-bellare-3048128a/\">Kedar Bellare</a> <a href=\"https://www.linkedin.com/in/sherrytchen/\">Sherry Chen</a></p><p>Search is the core mechanism that connects guests with Hosts at Airbnb. Results from a guest’s search for listings are displayed through two interfaces: (1) as a list of rectangular cards that contain the listing image, price, rating, and other details on it, referred to as <em>list-results</em> and (2) as oval pins on a map showing the listing price, called <em>map-results</em>. Since its inception, the core of the ranking algorithm that powered both these interfaces was the same — ordering listings by their booking probabilities and selecting the top listings for display.</p><p>But some of the basic assumptions underlying ranking, built for a world where search results are presented as lists, simply break down for maps.</p><h3>What Is Different About Maps?</h3><p>The central concept that drives ranking for list-results is that <em>user attention decays</em> starting from the top of the list, going down towards the bottom. A plot of rank vs click-through rates in Figure 1 illustrates this concept. X-axis represents the rank of listings in search results. Y-axis represents the click-through rate (CTR) for listings at the particular rank.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Y9drAzLenJ9GAYEA\" /><figcaption>Figure 1: Click-through rates by listing search rank</figcaption></figure><p>To maximize the connections between guests and Hosts, the ranking algorithm sorts listings by their booking probabilities based on a <a href=\"https://www.airbnb.com/help/article/39\">number of factors</a> and sequentially assigns their position in the list-results. This often means that the larger a listing’s booking probability, the more attention it receives from searchers.</p><p>But in map-results, listings are scattered as pins over an area (see Figure 2). There is no ranked list, and there is no decay of user attention by ranking position. Therefore, for listings that are shown on the map, the strategy of sorting by booking probabilities is no longer applicable.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/624/0*6iaMrBpbSQjVnsLF\" /><figcaption>Figure 2: Map results</figcaption></figure><h3>Uniform User Attention</h3><p>To adapt ranking to the map interface, we look at new ways of modeling user attention flow across a map. We start with the most straightforward assumption that user attention is spread equally across the map pins. User attention is a very precious commodity and most searchers only click through a few map pins (see Figure 3). A large number of pins on the map means those limited clicks may miss discovering the best options available. Conversely, limiting the number of pins to the topmost choices increases the probability of the searcher finding something suitable, but runs the risk of removing their preferred choice.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Vi5l4XPrl3YdHsP0\" /><figcaption>Figure 3: Number of distinct map pins clicked by percentage of searchers</figcaption></figure><p>We test this hypothesis, controlled by a parameter . The parameter serves as an upper bound on the ratio of the highest booking probability vs the lowest booking probability when selecting the map pins. The bounds set by the parameter controls the booking probability of the listings behind the map pins. The more restricted the bounds, the higher the average booking probability of the listings presented as map pins. Figure 4 summarizes the results from A/B testing a range of parameters.</p><p>The reduction in the average impressions to discovery metric in Figure 4 denotes the fewer number of map pins a searcher has to process before clicking the listing that they eventually book. Similarly, the reduction in average clicks to discovery shows the fewer number of map pins a searcher has to click through to find the listing they booked.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*trGxNfKu4rHa4Gpx\" /><figcaption>Figure 4: Exploring through online A/B experiments</figcaption></figure><p>Launching the restricted version resulted in one of the largest bookings improvement in Airbnb ranking history. More importantly, the gains were not only for bookings, but for quality bookings. This could be seen by the increase in trips that resulted in 5-star rating after the stay from the treatment group, in comparison to trips from the control group.</p><h3>Tiered User Attention</h3><p>In our next iteration of modeling user attention, we separate the map pins into two tiers. The listings with the highest booking probabilities are displayed as regular oval pins with price. Listings with comparatively lower booking probabilities are displayed as smaller ovals without price, referred to as mini-pins (Figure 5). By design, mini-pins draw less user attention, with click-through rates about 8x less than regular pins.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*pkL4ovuWpR1Rz9z-\" /><figcaption>Figure 5: Oval pins with price and mini-pins</figcaption></figure><p>This comes in handy particularly for searches on desktop where 18 results are shown in a grid on the left, each of them requiring a map pin on the right (Figure 6).</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*A83SEjyDlyTUCI06\" /><figcaption>Figure 6: Search results on desktop</figcaption></figure><p>The number of map pins is fixed in this case, and limiting them, as we did in the previous section, is not an option. Creating the two tiers prioritizes user attention towards the map pins with the highest probabilities of getting booked. Figure 7 shows the results of testing the idea through an online A/B experiment.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/842/0*1V-XbGegLzPch25O\" /><figcaption>Figure 7: Experiment results for tiered map pins</figcaption></figure><h3>Discounted User Attention</h3><p>In our final iteration, we refine our understanding of how user attention is distributed over the map by plotting the click-through rate of map pins located at different coordinates on the map. Figure 8 shows these plots for the mobile (top) and the desktop apps (bottom).</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/598/0*rDDubemWn97XvCN2\" /></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/602/0*I9GtvJEw5BGfHn96\" /><figcaption>Figure 8: Click-through rates of map pins across map coordinates.</figcaption></figure><p>To maximize the chances that a searcher will discover the listings with the highest booking probabilities, we design an algorithm that re-centers the map such that the listings with the highest booking probabilities appear closer to the center. The steps of this algorithm are illustrated in Figure 9, where a range of potential coordinates are evaluated and the one which is closer to the listings with the highest booking probabilities is chosen as the new center.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*IqlsENiSd-9IdQ5v\" /><figcaption>Figure 9: Algorithm for finding optimal center</figcaption></figure><p>When tested in an online A/B experiment, the algorithm improved uncancelled bookings by 0.27%. We also observed a reduction of 1.5% in map moves, indicating less effort from the searchers to use the map.</p><h3>Conclusion</h3><p>Users interact with maps in a way that’s fundamentally different from interacting with items in a list. By modeling the user interaction with maps in a progressively sophisticated manner, we were able to improve the user experience for guests in the real world. However, the current approach has a challenge that remains unsolved: how can we represent the full range of available listings on the map? This is part of our future work. A more in-depth discussion of the topics covered here, along with technical details, is presented in our research paper that was <a href=\"https://arxiv.org/pdf/2407.00091\">published at the <strong>KDD ’24</strong> conference</a>. We welcome all feedback and suggestions.</p><p>If this type of work interests you, we encourage you to apply for an<a href=\"https://careers.airbnb.com/\"> open position</a> today.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=13b03f2c2cca\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/improving-search-ranking-for-maps-13b03f2c2cca\">Improving Search Ranking for Maps</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "爱彼迎在 KDD 2024 大会 (原标题: Airbnb at KDD 2024)",
      "link": "https://medium.com/airbnb-engineering/airbnb-at-kdd-2024-d5c2fa81a119?source=rss----53c7c27702d5---4",
      "pubDate": "Tue, 17 Dec 2024 18:02:43 GMT",
      "isoDate": "2024-12-17T18:02:43.000Z",
      "creator": "Huiji Gao",
      "summary": "# 爱彼迎在 KDD 2024 大会\n\n## 引言\n爱彼迎在西班牙巴塞罗那举行的 KDD 2024 大会上表现突出。爱彼迎的数据科学家和工程师就以下主题进行了演讲：\n*   深度学习与搜索排名\n*   在线实验与测量\n*   产品质量与客户旅程\n*   双边市场\n\n本文总结了爱彼迎在 KDD 2024 的贡献，并提供了相关学术论文的访问途径。\n\n![图片 1](https://cdn-images-1.medium.com/max/1024/0*JnSzLDm3Uh2hY6c2)\n\n## KDD 大会概览\nKDD（知识发现与数据挖掘）是数据挖掘和机器学习领域最负盛名的全球会议之一，由计算机协会 (ACM) 的一个特殊兴趣小组每年举办。第 30 届 KDD 大会在西班牙巴塞罗那举行，吸引了数千名学术界和工业界的研究人员和科学家。参会公司包括谷歌、Meta、苹果、亚马逊、爱彼迎等。会议共接受了 151 篇应用数据科学 (ADS) 论文和 411 篇研究论文，以及 34 场教程和 30 场研讨会。\n\n## 爱彼迎在 KDD 2024 的重要贡献\n爱彼迎在 KDD 2024 拥有显著影响力，具体贡献包括：\n*   三篇完整的 ADS 论文（接受率低于 20%）\n*   一个研讨会\n*   七篇研讨会论文和受邀演讲被主会议收录\n\n爱彼迎的工作主题涵盖：\n*   深度学习与搜索排名\n*   在线实验与测量\n*   因果推断与机器学习\n*   双边市场\n\n## 主要贡献领域\n\n### 1. 深度学习与搜索排名\n智能搜索排名旨在根据房客偏好、房源特征和额外搜索上下文来准确匹配房客与房源，这在双边市场中仍是一个复杂挑战。爱彼迎发布了多篇论文来解决搜索排名问题。\n\n*   **《爱彼迎地图学习排名》**\n    *   **问题：** 爱彼迎的搜索结果主要通过列表和地图两种界面显示，但传统的排名算法是为列表设计，在地图界面上存在基本假设失效的问题。\n    *   **解决方案：** 重构了地图结果的排名算法，修订了用户与地图搜索结果互动方式的数学基础，并提出了统一两种界面的理论。\n    *   **影响：** 通过迭代和实验驱动的方法，显著提升了用户体验。未来的研究方向包括地图结果的无偏学习排名和引导用户关注更相关房源。\n\n*   **《通过模型蒸馏实现多目标学习排名》**\n    *   **问题：** 在线市场中的搜索排名不仅要优化主要目标（如转化率），还要兼顾次要目标（如订单取消、评价、客服咨询、平台长期增长）。传统方法面临参数调整昂贵、数据稀疏不平衡和兼容性差等挑战。\n    *   **解决方案：** 提出了一种基于蒸馏的排名解决方案，优化了爱彼迎跨多个排名模型的端到端排名系统，并考虑了训练和服务效率。\n    *   **影响：** 该方案显著提升了主要目标转化率，同时满足了次要目标约束并提高了模型稳定性。此外，该系统可通过模型自蒸馏进一步简化，并能高效地将非可微分的业务目标注入排名系统。\n\n### 2. 在线实验与测量\n在线实验（如 A/B 测试）是爱彼迎等组织进行数据驱动决策的常用方式，但高方差是一个常见挑战。\n\n*   **《A/B 测试中的指标分解》**\n    *   **问题：** CUPED（利用实验前数据进行对照实验）等方差降低方法存在理论限制，且在实验中利用数据进行处理效应增强的实践指导不足。\n    *   **解决方案：** 提出通过处理效应增强来提高敏感度的新方向，将目标指标分解为两个或更多组件，以分离高信号低噪声和低信号高噪声的部分。\n    *   **影响：** 理论、模拟和实证示例表明，如果存在这种分解，可以提高敏感度。提供了三个真实世界应用案例，展示了指标分解相对于未分解分析的敏捷性提升。未来的工作包括在频率派和贝叶斯框架下确定样本量。\n\n### 3. 双边市场优化\n爱彼迎员工主持了“双边市场优化：搜索、定价、匹配与增长”研讨会，讨论了在双边市场中解决生产者（房东）和消费者（房客）需求的演变。机器学习方法在爱彼迎这类复杂的互联网规模双边市场中至关重要。爱彼迎的贡献旨在优化房客体验、寻找房源价格均衡点、减少不良互动（降低客服成本）、大规模检测运营人员跟进活动等。\n\n*   **《用于个性化的房客意图建模》**\n    *   **问题：** 理解用户意图对提供无缝和个性化体验至关重要，但有限的用户数据和不可预测的房客行为使其难以推断房客的真实意图。\n    *   **解决方案：** 采用深度学习方法预测用户旅行计划中难以推断的细节，如下一个目的地和旅行日期。该框架分析用户应用内浏览历史、预订历史、搜索查询和其他互动信号，生成多个用户意图信号。\n    *   ![图片 2](https://cdn-images-1.medium.com/max/1024/0*9QYBiZCmirD4aV1o)\n    *   **影响：** 有助于营销邮件、灵活旅行搜索和应用首页推荐，也帮助房东优化房源以提高满意度和预订量。\n\n*   **《房客需求理解》**\n    *   **问题：** 大多数房东不是专业酒店从业者，难以正确为其房源定价，需要数据和建议。\n    *   **解决方案：** 结合经济建模和因果推断技术，将房客分段并估计其价格敏感度，通过小规模定向实验和大规模自然实验进行微调。\n    *   ![图片 3](https://cdn-images-1.medium.com/max/656/0*-3Mj0gacSPA56ZQx)\n    *   **影响：** 模型输出帮助房东在更高入住率和更高每晚价格之间做出权衡。\n\n*   **《用于房东侧产品的房源嵌入》**\n    *   **问题：** 爱彼迎提供许多基于房源比较的工具（寻找相似或等效替代品）。\n    *   **解决方案：** 研究了房源嵌入在爱彼迎双边市场中的应用和学习。讨论了使用房客互动数据训练神经网络嵌入模型的架构和训练，并将其应用于房东侧产品界面。解决了负训练样本构建、训练数据采样偏差校正以及通过模型内缓存进行扩展和加速训练等技术挑战。\n    *   **影响：** 评估方法包括批内指标、基于词汇的评估和相似房源属性。已应用于爱彼迎产品，如房东日历相似房源。\n\n*   **《搜索排名中的客户支持优化》**\n    *   **问题：** 爱彼迎致力于防止不良体验，同时保持增长。\n    *   **解决方案：** 利用大量累积的客服数据建模用户预订后需要客服支持的概率。模型发现搜索者、房源和房东的多个特征能准确预测客服需求（例如，当天预订需要更多支持，响应迅速的房东能减少支持需求）。\n    *   **影响：** 将客服模型输出纳入搜索结果排名，预测可能导致负面体验的房源排名会降低。\n\n*   **《使用活动日志进行 LLM 预训练》**\n    *   （文章内容在此处截断）旨在通过分析用户活动日志来预训练大型语言模型，以便在大规模用户互动后进行跟进，确保高质量体验。",
      "shortSummary": "爱彼迎在 KDD 2024 大会上表现突出，展示了其在数据挖掘和机器学习领域的最新研究成果。爱彼迎贡献了三篇 ADS 论文、一个研讨会及多篇研讨会论文，涵盖深度学习与搜索排名、在线实验与测量、因果推断与机器学习以及双边市场优化等核心领域。具体工作包括地图排名算法重构、多目标学习排名、A/B 测试指标分解、房客意图建模、房客需求理解、房源嵌入及客户支持优化等，旨在提升用户体验、优化市场效率并解决复杂业务挑战。",
      "translated_title": "爱彼迎在 KDD 2024 大会",
      "images": [
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*JnSzLDm3Uh2hY6c2",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://cdn-images-1.medium.com/max/1024/0*9QYBiZCmirD4aV1o",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://cdn-images-1.medium.com/max/656/0*-3Mj0gacSPA56ZQx",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d5c2fa81a119",
          "alt": "",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "RSS",
      "content": "<p>Airbnb had a large presence at the 2024 KDD conference hosted in Barcelona, Spain. Our Data Scientist and Engineers presented on topics like Deep Learning &amp; Search Ranking, Online Experimentation &amp; Measurement, Product Quality &amp; Customer Journey, and Two-sided Marketplaces. This blog post summarizes our contributions to KDD for 2024 and provides access to the academic papers presented during the conference.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*JnSzLDm3Uh2hY6c2\" /></figure><p>Authors: <a href=\"mailto:huiji.gao@airbnb.com\">Huiji Gao</a>, <a href=\"mailto:peter.coles@airbnb.com\">Peter Coles</a>, <a href=\"mailto:carolina.barcenas@airbnb.com\">Carolina Barcenas</a>, <a href=\"mailto:sanjeev.katariya@airbnb.com\">Sanjeev Katariya</a></p><p><a href=\"https://kdd.org/\">KDD</a> (Knowledge and Data Mining) is one of the most prestigious global conferences in data mining and machine learning. Hosted annually by a special interest group of the Association for Computing Machinery (ACM), it’s where attendees learn about some of the most ground-breaking AI developments in data mining, machine learning, knowledge discovery, and large-scale data analytics.</p><p>This year, the 30th KDD conference was held at Barcelona, Spain, attracting thousands of researchers and scientists from academia and industry. Various companies contributed to and attended the conference including Google, Meta, Apple, Amazon, Airbnb, Pinterest, LinkedIn, Booking, Expedia, ByteDance etc. There were 151 Applied Data Science (ADS) track papers and 411 Research track papers accepted, 34 tutorials, and 30 workshops.</p><p>Airbnb had a significant presence at KDD 2024 with three full <a href=\"https://kdd2024.kdd.org/applied-data-science-track-papers\">ADS track</a> papers (acceptance rate under 20%), one workshop, and seven workshop papers and invited talks accepted into the main conference proceedings. The topics of our work spanned Deep learning &amp; Search Ranking, Online Experimentation &amp; Measurement, Causal Inference &amp; Machine Learning, and Two-sided Marketplaces.</p><p>In this blog post, we will summarize our teams’ contributions and share highlights from an exciting week-long conference with research and industry talks, workshops, panel discussions, and more.</p><h3><strong>Deep Learning and Search Ranking</strong></h3><p>Intelligent search ranking — the process of accurately matching a guest with a listing based on their preference, a listing’s features, and additional search context — still remains a nuanced challenge that researchers are constantly trying to solve.</p><p>Making optimal guest-host matches has remained an issue in a two-sided marketplace for a variety of reasons — the timespan of guest searches (ranging between days and weeks), unpredictable host behavior and ratings (the potential for hosts to cancel a booking or receive low ratings), and limited understanding of guest preference across multiple interfaces. We published several papers addressing the issue of search ranking as part of our presence at KDD.</p><p><a href=\"https://arxiv.org/abs/2407.00091\"><strong>Learning to Rank for Maps at Airbnb</strong></a></p><p>Airbnb brings together hosts who rent listings to prospective guests from around the globe. Results from a guest’s search for listings are displayed primarily through two interfaces: (1) as a list of rectangular cards that contain on them the listing image, price, rating, and other details, referred to as list-results, and (2) as oval pins on a map showing the listing price, called map-results. Both these interfaces, since their inception, have used the same ranking algorithm that orders listings by their booking probabilities and selects the top listings for display.</p><p>However, some of the basic assumptions underlying ranking are built for a world where search results are presented as lists and simply break down for map-results. In this work, we rebuilt ranking for maps by revising the mathematical foundations of how users interact with map search results. Our iterative and experiment-driven approach led us through a path full of twists and turns, ending in a unified theory for the two interfaces.</p><p>Our journey shows how assumptions taken for granted when designing machine learning algorithms may not apply equally across all user interfaces, and how they can be adapted. The net impact was one of the largest improvements in user experience for Airbnb which we discuss as a series of experimental validations. The work introduced in this paper is merely the beginning of future exciting research projects, such as making learning to rank unbiased for map-results and demarcating the map pins to direct the user attention towards more relevant ones.</p><p><a href=\"https://arxiv.org/abs/2407.07181\"><strong>Multi-objective Learning to Rank by Model Distillation</strong></a></p><p>In online marketplaces, the objective of search ranking is not only on optimizing purchasing or conversion rate (primary objective), but also the purchase outcomes (secondary objectives), e.g. order cancellation, review rating, customer service inquiries, platform long term growth. To balance these primary and secondary objectives, several multi-objective learning to rank approaches have been widely studied</p><p>Traditional approaches in industrial search and recommender systems encounter challenges such as expensive parameter tuning that leads to sub-optimal solutions, suffering from imbalanced data sparsity issues, and lack of compatibility with ad-hoc objectives. In this work, we propose a distillation-based ranking solution for multi-objective ranking, which optimizes the end-to-end ranking system at Airbnb across multiple ranking models on different objectives, along with various considerations to optimize training and serving efficiency that meets industry standards.</p><p>Compared with traditional approaches, the proposed solution not only significantly meets and increases the primary objective of conversion by a large margin, but also addresses the secondary objective constraints while improving model stability. Furthermore, we demonstrated the proposed system could be further simplified by model self-distillation. We also did additional simulations to show that this approach could help us efficiently inject ad-hoc non-differentiable business objectives into the ranking system, while enabling us to balance our optimization objectives.</p><h3><strong>Online Experimentation and Measurement</strong></h3><p>Online experimentation (e.g., A/B testing) is a common way for organizations like Airbnb to make data-driven decisions. But high variance is frequently a challenge. For example, it’s hard to prove that a change in our search UX will drive value because bookings can be infrequent and depend on a large number of interactions over a long period of time.</p><p><a href=\"https://dl.acm.org/doi/pdf/10.1145/3637528.3671556\"><strong>Metric Decomposition in A/B Tests</strong></a></p><p>More than a decade ago, CUPED (Controlled Experiments Utilizing Pre-Experiment Data) mainstreamed the idea of variance reduction leveraging pre-experiment covariates. Since its introduction, it has been implemented, extended, and modernized by major online experimentation platforms. Despite the wide adoption, it is known by practitioners that the variance reduction rate from CUPED, utilizing pre-experimental data, varies case by case and has a theoretical limit. In theory, CUPED can be extended to augment a treatment effect estimator utilizing in-experiment data, but practical guidance on how to construct such an augmentation is lacking.</p><p>In this work, we fill this gap by proposing a new direction for sensitivity improvement via treatment effect augmentation, whereby a target metric of interest is decomposed into</p><p>two or more components in an attempt to isolate those with high signal and low noise from those with low signal and high noise. We show through theory, simulation, and empirical examples that if such a decomposition exists (or can be engineered), sensitivity may be increased via approximately null augmentation (in a frequentist setting) and reduced posterior variance (in a Bayesian setting).</p><p>We provide three real world applications demonstrating different flavors of metric decomposition. These applications illustrate the gain in agility metric decomposition yields relative to an un-decomposed analysis, indicating both empirically and theoretically the value of this practice in both frequentist and Bayesian settings. An important extension to this work would be to next consider sample size determination in both the frequentist or Bayesian contexts; while a boost in sensitivity typically means less data is required for a given analysis, a methodology that determines the smallest sample size required to control various operating characteristics in this context would be of practical value.</p><h3>Two-sided Marketplace Optimization</h3><p>Airbnb employees hosted a workshop on <a href=\"https://sites.google.com/view/tsmo2024/home?authuser=0\">Two-sided Marketplace Optimization: Search, Pricing, Matching &amp; Growth</a>. This workshop brought practitioners of two-sided marketplaces together and discussed the evolution of content ranking, recommendation systems, and data mining when solving for producers and consumers on these platforms.</p><p>Two-sided marketplaces have recently emerged as viable business models for many real-world applications. They model transactions as a network with two distinct types of participants: one type to represent the supply and another the demand of a specific good. Traditionally, research related to online marketplaces focused on how to better satisfy demand. But with two-sided marketplaces, there is more nuance at play. Modern global examples, like Airbnb, operate platforms where users provide services; users may be hosts,or guests. Such platforms must develop models that address all their users’ needs and goals at scale. Machine learning-powered methods and algorithms are essential in every aspect of such complex, internet-scale-sized, two-sided marketplaces.</p><p>Airbnb is a community based on connection and belonging–we strive to connect people and places. Our contributions to this workshop showcase the work we’re doing to support this mission by optimizing guest experiences, finding equilibrium spots for listing prices, reducing the incidence of poor interactions (and customer support costs as a side effect), detecting when operational staff should follow up on activity at scale, and more.</p><p><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Understanding-User-Booking-Intent-at-Airbnb.pdf\"><strong>Guest Intention Modeling for Personalization</strong></a></p><p>Airbnb has transformed the way people travel by offering unique and personalized stays in destinations worldwide. To provide a seamless and tailored experience, understanding user intent plays an important role.</p><p>However, limited user data and unpredictable guest behavior can make it difficult to understand the essential intent from guests on listings from hosts. Our work shows how we approach this challenging problem. We describe how we apply a deep learning approach to predict difficult-to-infer details for a user’s travel plan, such as the next destination and travel dates. The framework analyzes high-level information from users’ in-app browsing history, booking history, search queries, and other engagement signals, and produces multiple user intent signals.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*9QYBiZCmirD4aV1o\" /></figure><p>Marketing emails, flexible travel search (e.g., for “Europe in the summer”), and recommendations on the app home page are three guest interactions that benefit from correct intention modeling. Hosts also benefit, since a clear understanding of guest demand can help them optimize listings to increase satisfaction and bookings.</p><p><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Understanding-Guest-Preferences-and-Optimizing-.pdf\"><strong>Guest Demand Understanding</strong></a></p><p>Hosts can find it difficult to correctly price their listings in two-sided marketplaces serviced by end users. Most hosts are not professional hospitality workers, and would benefit from access to data and advice on how guests see their listings and how they compare to other listings in their neighborhood. We constantly look for ways to give guidance on how hosts can optimally price their listings. The same information can then be used to help guests find their ideal stay.</p><p>In our paper, we presented an example of how this problem can be solved in general.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/656/0*-3Mj0gacSPA56ZQx\" /></figure><p>As illustrated above, both demand and supply change over time, influencing the equilibrium price for a property at a specific point. A historical optimum (such as A above) has to be adjusted to find the current optimum (point C). It is difficult to run experiments since any large-scale experiment we might run will cause the environment to change in complex ways. We tackle this problem by combining economic modeling with causal inference techniques. We segment guests and estimate how price-sensitive each guest segment is, and fine-tune them with empirical data from small targeted experiments and larger-scale natural ones, which are used to adjust estimates for the price sensitivity of each guest segment. Hosts can then use the models’ output to make informed tradeoffs between higher occupancy and higher nightly rates.</p><p><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Learning-and-Applying-Airbnb-Listing-Embeddings-.pdf\"><strong>Listing Embedding for Host-side Products</strong></a></p><p>In order to facilitate the matching of listings and guests, Airbnb provides numerous products and services to both hosts and guests. Many of these tools are based on the ability to compare listings, i.e. finding similar listings or listings that may be viewed as equivalent substitutes. Our work presents a study on the application and learning of listing embeddings in Airbnb’s two-sided marketplace. Specifically, we discuss the architecture and training of a neural network embedding model using guest side engagement data, which is then applied to host-side product surfaces. We address the key technical challenges we encountered, including the formulation of negative training examples, correction of training data sampling bias, and the scaling and speeding up training with the help of in-model caching. Additionally, we discuss our comprehensive approach to evaluation, which ranges from in-batch metrics and vocabulary-based evaluation to the properties of similar listings. Finally, we share our insights from utilizing listing embeddings in Airbnb products, such as host calendar similar listings.</p><p><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Predicting-Potential-Customer-Support-Needs-and-OptimizingSearch-Ranking.pdf\"><strong>Customer Support Optimization in Search Ranking</strong></a></p><p>As of the date of the paper, Airbnb had more than 7.7 million listings from more than 5 million hosts worldwide. Airbnb is investing both in rapid growth and in making sure that the booking experience is pleasant for hosts and guests. It would, however, be ideal to avoid poor experiences in the first place. Our work highlights how we prevent poor experiences without significantly reducing growth.</p><p>We use the mass of accumulated support data at Airbnb to model the probability that, if the current user were to book a listing, they would require CS support. Our model discovered multiple features about the searcher, home, and hosts that accurately predict CS requirements. For example, same-day bookings tend to require more support, and a responsive host tends to reduce support needs. So, if a guest chooses a same-day booking, matching them with a highly responsive host can lead to a better experience overall. We incorporate the output of our CS support model in search result rankings; booked homes will sometimes rank lower if we predict a booking will lead to a negative experience.</p><p><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Can-Language-Models-Accelerate-Prototyping-for-Non-Language-Data-Classification-Summarization-of-Activity-Logs-as-Text.pdf\"><strong>LLM Pretraining using Activity Logs</strong></a></p><p>It’s often important to follow up with users after they’ve had a long series of interactions with a two-sided marketplace to help make sure that their experiences are of high quality. When user interactions meet certain business criteria, operations agents create tickets to follow up with them. For example, user retention and reactivation agents might review user activity logs and decide to follow up with the user, to encourage them to re-engage with the platform.</p><p>We propose transforming structured data (activity logs) into a more manageable text format and then leveraging modern language models (i.e., BERT) to pretrain a large language model based on user activities. We then performed fine-tuning on the model using historical data about which users were followed up with and checked its predictions. Our work demonstrates the large language model trained on pre-processed activity can successfully identify when a user should be followed up with, at an experimentally significant rate. Our preliminary results suggest that our framework may outperform by 80% the average precision of a similar model that was designed relying heavily on feature engineering.</p><h3>Product Quality and Customer Journey Optimization</h3><p>Typically, product quality is evaluated based on structured data. Customer ratings, types of support issues, resolution times, and other factors are used as a proxy for how someone booking on Airbnb might value a listing. This kind of data has limitations — more popular listings have more data, often users don’t leave feedback, and feedback is usually biased towards the positive (users with negative experiences tend to churn and not give feedback).</p><p>In the Workshop on Causal Inference and Machine Learning in Practice, we highlighted an example of how we push the boundaries of product quality assessment techniques and applications, mixing traditional casual inference with cutting-edge machine learning research. In our work “<a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Understanding-Product-Quality-with-Unstructured-Data.pdf\">Understanding Product Quality with Unstructured Data: An Application of LLMs and Embeddings at Airbnb</a>”, we presented how an approach based on text embeddings and LLMs can be combined with approaches based on structured data to significantly improve product quality evaluations. We generate text embeddings on a mix of listing and review texts, then cluster the embeddings based on rebooking and churn rates. Once we have clear clusters, we extract keywords from the original data, and use these keywords to calculate a listing quality score, based on their similarity to the keyword list.</p><p>In addition, we were invited to give a talk <a href=\"https://sites.google.com/view/kdd-workshop-2023\">on Quality Foundations at Airbnb</a>, at KDD’s 3r<a href=\"https://sites.google.com/view/kdd-workshop-2023\">d Workshop on End-End Customer Journey Optimization.</a> It’s often hard to differentiate the quality of customer experiences using simple review ratings, in part due to the tightness of their distribution. In this talk, we present an alternative notion of quality based on customer revealed preference: did a customer return to use the platform again after their experience? We describe how a metric — Guest Return Propensity (GRP) — leverages this concept and can differentiate quality, capture platform externalities, and predict future returns.</p><p>In practice, this measure may not be suited to many common business use cases due to its lagging nature and an inability to easily explain why it has changed. We describe a quality measurement system that builds on the conceptual foundation of GRP by modeling it as an outcome of upstream realized quality signals. These signals — from sources like reviews and customer support — are weighted by their impact on return propensity and mapped to a quality taxonomy to aid in explainability. The resulting score is capable of finely differentiating the quality of customer experiences, aiding tradeoff decisions, and providing timely insights.</p><h3>Conclusion</h3><p>The 2024 edition of KDD was an amazing opportunity for data scientists and machine learning engineers from across the globe and industry, government, and academia, to connect and exchange learnings and discoveries. We were honored to have the opportunity to share some of our knowledge and techniques, generalizing what we have been learning when we apply machine learning to problems we see at Airbnb. We continue to focus on improving our customers’ experience and growing our business, and the information we’ve shared has been crucial to our success. We’re excited to continue learning from peers and contribute our work back to our community. We eagerly await advancements and improvements that might come about as others build upon the work we’ve shared.</p><p>Below, you’ll find a complete list of the talks and papers shared in this article along with the team members who contributed. If this type of work interests you, we encourage you to apply for an<a href=\"https://careers.airbnb.com/\"> open position</a> today.</p><h3>List of papers and talks</h3><p><strong>Learning to Rank for Maps at Airbnb (</strong><a href=\"https://dl.acm.org/doi/10.1145/3637528.3671648\"><strong>link</strong></a><strong>)</strong></p><p>Authors: Malay Haldar, Hongwei Zhang, Kedar Bellare, Sherry Chen, Soumyadip Banerjee, Xiaotang Wang, Mustafa Abdool, Huiji Gao, Pavan Tapadia, Liwei He, Sanjeev Katariya</p><p><strong>Multi-objective Learning to Rank by Model Distillation (</strong><a href=\"https://dl.acm.org/doi/10.1145/3637528.3671597\"><strong>link</strong></a><strong>)</strong></p><p>Authors: Jie Tang, Huiji Gao, Liwei He, Sanjeev Katariya</p><p><strong>Metric Decomposition in A/B Tests (</strong><a href=\"https://dl.acm.org/doi/10.1145/3637528.3671556\"><strong>link</strong></a><strong>)</strong></p><p>Authors: Alex Deng (former employee at Airbnb), Luke Hagar (University of Waterloo), Nathaniel T. Stevens (University of Waterloo), Tatiana Xifara (Airbnb), Amit Gandhi (University of Pennsylvania)</p><p><strong>Understanding Guest Preferences and Optimizing Two-sided Marketplaces: Airbnb as an Example (</strong><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Understanding-Guest-Preferences-and-Optimizing-.pdf\"><strong>link</strong></a><strong>)</strong></p><p>Authors: Yufei Wu, Daniel Schmierer</p><p><strong>Predicting Potential Customer Support Needs and Optimizing Search Ranking in a Two-Sided Marketplace (</strong><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Predicting-Potential-Customer-Support-Needs-and-OptimizingSearch-Ranking.pdf\"><strong>link</strong></a><strong>)</strong></p><p>Authors: Do-kyum Kim, Han Zhao, Huiji Gao, Liwei He, Malay Haldar, Sanjeev Katariya</p><p><strong>​​Understanding User Booking Intent at Airbnb (</strong><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Understanding-User-Booking-Intent-at-Airbnb.pdf\"><strong>link</strong></a><strong>)</strong></p><p>Authors: Xiaowei Liu, Weiwei Guo, Jie Tang, Sherry Chen, Huiji Gao, Liwei He, Pavan Tapadia, Sanjeev Katariya</p><p><strong>Can Language Models Accelerate Prototyping for Non-Language Data? Classification &amp; Summarization of Activity Logs as Text (</strong><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Can-Language-Models-Accelerate-Prototyping-for-Non-Language-Data-Classification-Summarization-of-Activity-Logs-as-Text.pdf\"><strong>link</strong></a><strong>)</strong></p><p>Authors: José González-Brenes</p><p><strong>Learning and Applying Airbnb Listing Embeddings in Two-Sided Marketplace (</strong><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Learning-and-Applying-Airbnb-Listing-Embeddings-.pdf\"><strong>link</strong></a><strong>)</strong></p><p>Authors: Siarhei Bykau, Dekun Zou</p><p><strong>Understanding Product Quality with Unstructured Data: An Application of LLMs and Embeddings at Airbnb (</strong><a href=\"https://airbnb.tech/wp-content/uploads/sites/19/2024/12/Understanding-Product-Quality-with-Unstructured-Data.pdf\"><strong>link</strong></a><strong>)</strong></p><p>Authors: Jikun Zhu, Zhiying Gu, Brad Li, Linsha Chen</p><p><strong>Invited Talk: Quality Foundations at Airbnb</strong></p><p>Speakers: Peter Coles, Mike Egesdal</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d5c2fa81a119\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/airbnb-engineering/airbnb-at-kdd-2024-d5c2fa81a119\">Airbnb at KDD 2024</a> was originally published in <a href=\"https://medium.com/airbnb-engineering\">The Airbnb Tech Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    }
  ],
  "lastUpdated": "2025-08-11T04:52:34.529Z"
}