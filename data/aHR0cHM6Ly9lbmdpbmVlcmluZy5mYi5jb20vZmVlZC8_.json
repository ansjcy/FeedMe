{
  "sourceUrl": "https://engineering.fb.com/feed/",
  "title": "Engineering at Meta",
  "description": "Engineering at Meta Blog",
  "link": "https://engineering.fb.com/",
  "items": [
    {
      "title": "将马尔贝克海底电缆延伸至巴西南部 (原标题: Extending the Malbec subsea cable to Southern Brazil)",
      "link": "https://engineering.fb.com/2025/05/22/connectivity/extending-malbec-subsea-cable-southern-brazil/",
      "pubDate": "Thu, 22 May 2025 16:00:08 +0000",
      "isoDate": "2025-05-22T16:00:08.000Z",
      "content": "<p>Meta is partnering with V.tal to extend the Malbec subsea cable to Porto Alegre, Brazil by 2027. With this new extension, Malbec will become the first subsea cable to land in the state of Rio Grande do Sul, bringing more connectivity to millions of people in Southern Brazil and neighboring countries. Malbec will improve the [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/05/22/connectivity/extending-malbec-subsea-cable-southern-brazil/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/05/22/connectivity/extending-malbec-subsea-cable-southern-brazil/\">Extending the Malbec subsea cable to Southern Brazil</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n",
      "contentSnippet": "Meta is partnering with V.tal to extend the Malbec subsea cable to Porto Alegre, Brazil by 2027. With this new extension, Malbec will become the first subsea cable to land in the state of Rio Grande do Sul, bringing more connectivity to millions of people in Southern Brazil and neighboring countries. Malbec will improve the [...]\nRead More...\nThe post Extending the Malbec subsea cable to Southern Brazil appeared first on Engineering at Meta.",
      "creator": "",
      "encodedSnippet": "<p>Meta is partnering with V.tal to extend the Malbec subsea cable to Porto Alegre, Brazil by 2027. With this new extension, Malbec will become the first subsea cable to land in the state of Rio Grande do Sul, bringing more connectivity to millions of people in Southern Brazil and neighboring countries. Malbec will improve the [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/05/22/connectivity/extending-malbec-subsea-cable-southern-brazil/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/05/22/connectivity/extending-malbec-subsea-cable-southern-brazil/\">Extending the Malbec subsea cable to Southern Brazil</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n\nMeta is partnering with V.tal to extend the Malbec subsea cable to Porto Alegre, Brazil by 2027. \nWith this new extension, Malbec will become the first subsea cable to land in the state of Rio Grande do Sul, bringing more connectivity to millions of people in Southern Brazil and neighboring countries.\nMalbec will improve the scale and reliability of digital infrastructure in Porto Alegre, establishing it as a digital hub and improving online experiences across Southern Brazil, Argentina, Chile, Paraguay, and Uruguay. \nToday, we’re announcing the extension of the Malbec subsea cable to the city of Porto Alegre, Brazil. Developed by Meta, in partnership with V.tal, Malbec is a 2,500 km cable that entered service in 2021 to provide connectivity between the Southern Cone of South America and Brazil. The new extension will be operational in 2027 and will link Porto Alegre to the cities of Rio de Janeiro and São Paulo, Brazil and Buenos Aires, Argentina. \n“The expansion of Malbec to Porto Alegre is a milestone for connectivity in South America, benefiting millions of people in Brazil and positioning the capital of Rio Grande do Sul as the first major international digital hub in the south of the country,” explained Ana Luiza Valadares, Meta’s Public Policy Director, Connectivity & Infra, LatAm. “It will contribute to attracting digital infrastructure companies, lowering costs for companies and improving consumer services.”\nFelipe Campos, CEO of V.tal, added, “The impact of this project will be significant for the local digital economy, positioning Porto Alegre as a new connectivity hub. It will be a unique infrastructure that will attract the interest of operators and internet providers, as well as other submarine cable companies.\nIn addition, all the Southern Cone countries will benefit from this new ecosystem, not to mention the end users and companies who will have a better experience when using the internet and digital applications.”\n\nThis extension is one of the latest in Meta’s digital infrastructure investments to support growing demand for digital capacity, resilience, and global reach. Earlier this year, Meta also activated a Point of Presence (PoP) in Porto Alegre. PoPs facilitate the efficient delivery of  content locally, which reduces the network management costs for internet service providers while improving the quality of experience for their customers. With the advent of AI and increasing demand for online services, digital infrastructure deployments play an important role in ensuring that the benefits of AI and other emerging technologies are available to everyone, regardless of where they live or work. \n“This investment in submarine connectivity, fully aligned with our Economic, Inclusive and Sustainable Development Plan, represents a strategic milestone for the state’s future,” said Rio Grand do Sul Governor, Eduardo Leite. “Furthermore, it fosters artificial intelligence projects, technologies that are already transforming the present and will define the future of innovation, a sector in which Rio Grande do Sul is a leader in Brazil, according to the ranking of state competitiveness.”\nMalbec will be the first international subsea cable to land in Rio Grande do Sul, bringing with it over 84 terabits of international capacity and direct connectivity to northern Brazil and Argentina. Like most subsea cables, local service providers will be able to acquire capacity on Malbec to serve additional bandwidth to millions of people in Brazil’s southern states. The providers will also extend Malbec’s capacity by connecting with providers in the neighboring countries of Argentina, Chile, Paraguay, and Uruguay, further positioning Brazil as a South American connectivity hub.\nThe post Extending the Malbec subsea cable to Southern Brazil appeared first on Engineering at Meta.",
      "summary": "Meta与V.tal合作，计划在2027年前将马尔贝克海底电缆延伸至巴西阿雷格里港。\n\n*   **主要内容：**\n    *   马尔贝克将成为首个登陆南里奥格兰德州的海底电缆，为巴西南部及周边国家数百万人提供更多连接。\n    *   该延伸项目将改善阿雷格里港的数字基础设施规模和可靠性，将其打造为数字中心，并改善巴西南部、阿根廷、智利、巴拉圭和乌拉圭的在线体验。\n    *   马尔贝克是一条2500公里的电缆，于2021年投入使用，旨在连接南美洲南部锥体和巴西。新的延伸段将于2027年投入运营，将阿雷格里港与巴西的里约热内卢和圣保罗以及阿根廷的布宜诺斯艾利斯连接起来。\n    *   Meta还在阿雷格里港启用了一个存在点 (PoP)，以促进在本地高效地交付内容，从而降低互联网服务提供商的网络管理成本，同时提高客户的体验质量。\n    *   该项目将为当地数字经济带来重大影响，将阿雷格里港定位为新的连接中心，并吸引运营商、互联网提供商和其他海底电缆公司的兴趣。\n    *   马尔贝克将成为首个登陆南里奥格兰德州的国际海底电缆，带来超过84太比特的国际容量，并直接连接巴西北部和阿根廷。\n\n*   **各方观点：**\n    *   Meta认为，马尔贝克延伸至阿雷格里港是南美洲连接的一个里程碑，将使巴西数百万人受益，并将南里奥格兰德州的首府定位为该国南部第一个主要的国际数字中心。\n    *   V.tal认为，该项目将吸引运营商和互联网提供商的兴趣，并将使包括阿根廷、智利、巴拉圭和乌拉圭在内的南锥体国家受益。\n    *   南里奥格兰德州州长认为，这项投资与该州的经济、包容和可持续发展计划完全一致，代表着该州未来发展的一个战略里程碑，并将促进人工智能项目的发展。\n\nThe post Extending the Malbec subsea cable to Southern Brazil appeared first on Engineering at Meta.",
      "translated_title": "将马尔贝克海底电缆延伸至巴西南部",
      "contentSource": "RSS"
    },
    {
      "title": "迈向1000个模型：扩展Instagram的推荐系统 (原标题: Journey to 1000 models: Scaling Instagram’s recommendation system)",
      "link": "https://engineering.fb.com/2025/05/21/production-engineering/journey-to-1000-models-scaling-instagrams-recommendation-system/",
      "pubDate": "Wed, 21 May 2025 16:00:38 +0000",
      "isoDate": "2025-05-21T16:00:38.000Z",
      "content": "<p>In this post, we explore how Instagram has successfully scaled its algorithm to include over 1000 ML models without sacrificing recommendation quality or reliability.  We delve into the intricacies of managing such a vast array of models, each with its own performance characteristics and product goals.  We share insights and lessons learned along the way—from [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/05/21/production-engineering/journey-to-1000-models-scaling-instagrams-recommendation-system/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/05/21/production-engineering/journey-to-1000-models-scaling-instagrams-recommendation-system/\">Journey to 1000 models: Scaling Instagram&#8217;s recommendation system</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n",
      "contentSnippet": "In this post, we explore how Instagram has successfully scaled its algorithm to include over 1000 ML models without sacrificing recommendation quality or reliability.  We delve into the intricacies of managing such a vast array of models, each with its own performance characteristics and product goals.  We share insights and lessons learned along the way—from [...]\nRead More...\nThe post Journey to 1000 models: Scaling Instagram’s recommendation system appeared first on Engineering at Meta.",
      "creator": "",
      "encodedSnippet": "<p>In this post, we explore how Instagram has successfully scaled its algorithm to include over 1000 ML models without sacrificing recommendation quality or reliability.  We delve into the intricacies of managing such a vast array of models, each with its own performance characteristics and product goals.  We share insights and lessons learned along the way—from [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/05/21/production-engineering/journey-to-1000-models-scaling-instagrams-recommendation-system/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/05/21/production-engineering/journey-to-1000-models-scaling-instagrams-recommendation-system/\">Journey to 1000 models: Scaling Instagram&#8217;s recommendation system</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n\nIn this post, we explore how Instagram has successfully scaled its algorithm to include over 1000 ML models without sacrificing recommendation quality or reliability. \nWe delve into the intricacies of managing such a vast array of models, each with its own performance characteristics and product goals. \nWe share insights and lessons learned along the way—from the initial realization that our infrastructure maturity was lagging behind our ambitious scaling goals, to the innovative solutions we implemented to bridge these gaps.\nIn the ever-evolving landscape of social media, Instagram serves as a hub for creative expression and connection, continually adapting to meet the dynamic needs of its global community. At the heart of this adaptability lies a web of machine learning (ML) models, each playing a crucial role in personalizing experiences. As Instagram’s reach and influence has grown, so too has the complexity of its algorithmic infrastructure. This growth, while exciting, presents a unique set of challenges, particularly in terms of reliability and scalability.\nJoin us as we uncover the strategies and tools that have enabled Instagram to maintain its position at the forefront of social media innovation, ensuring a seamless and engaging experience for billions of users worldwide.\n\nAre there really that many ML models in Instagram?\nThough what shows up in Feed, Stories, and Reels is personally ranked, the number of ranked surfaces goes much deeper—to which comments surface in Feed, which notifications are “important,” or whom you might tag in a post. These are all driven by ML recommendations. \nWithin a given surface, we’ll have different layers of the ranking funnel: sourcing (retrieval), early-stage ranking (ESR), and late-stage ranking (LSR). We operate on fewer candidates as we progress through the funnel, as the underlying operations grow more expensive (see Figure 1 below):\nFigure 1: The ranking funnel.\nWithin each surface and layer, there is constant experimentation, and these permutations create a severe infrastructure challenge. We need to allow room for our ML engineers to experiment with changes such as adjusting weights for a given prediction. The net result, depicted below in Figure 2, is a large number of models serving user traffic in production:\nFigure 2: An expression of the factors behind the fleet’s numerical growth.\nHow did we realize infra maturity wasn’t going to catch up?\nIdentified risks\nWe identified several risks associated with scaling our algorithm, rooted in complaints about ML productivity and repeating patterns of issues:\nDiscovery: Even as a team focused on one app — Instagram — we couldn’t stay on top of the growth, and product ML teams were maintaining separate sources of truth, if any, for their models in production.\nRelease: We didn’t have a consistent way to launch new models safely, and the process was slow, impacting ML velocity and, therefore, product innovation.\nHealth: We lacked a consistent definition of model prediction quality, and with the diversity of surfaces and subtlety of degraded ranking, quality issues went unnoticed.\nSolution overview\nTo address these risks, we implemented several solutions:\nModel registry: We built a registry that serves as a ledger for production model importance and business function foremost, among other metadata. This registry serves as our foundational source of truth, upon which we can leverage automation to uplevel system-wide observability, change management, and model health.\nModel launch tooling: We developed a more ideal flow for launching new models that includes estimation, approval, prep, scale-up, and finalization. This process is now automated, and we’ve reduced the time it takes to launch a new model from days to hours.\nModel stability: We defined and operationalized model stability, a pioneering metric that measures the accuracy of our model predictions. We’ve leveraged model stability to produce SLOs for all models in the model registry, which enables simple understanding of the entire product surface’s ML health.\nModel registry\nWhat did model investigations look like prior to the registry?\nBefore we created the model registry, the investigation process was a time-consuming and error-prone experience for on-call engineers and model owners. An on-call engineer had to ask multiple questions to model owners to gather information, as depicted Figure 3 below, about the context of what this model does in the stack and to clarify how important it is to the business.\nFigure 3: A fictional but typical non-productive investigation.\nUnderstanding this context is extremely important to the operational response: Depending on the importance of the model and the criticality of the surface it’s supporting, the response is going to differ in kind. When a model is an experiment serving a small percentage of the traffic, an appropriate response can be to end the experiment and reroute the traffic back to the main model (the baseline). But if there’s a problem with the baseline model that needs to be handled with urgency, it’s not possible to “just turn it off.” The engineer on call has to loop in the model owner, defeating the purpose of having a dedicated on-call.\nTo avoid holding up an operational response on a single POC, we needed a central source of truth for model importance and business function. What if the model is not available? What if 10 of these issues happen concurrently? \nWith the development of the model registry, we standardized the collection of model importance and business function information, ensuring most of our operational resources were going towards the most important models.\nWhat problems did the model registry solve?\nThe model registry is a system of record built on top of Configerator, Meta’s distributed configuration suite . This schematized ledger (see an example in Figure 4 and detailed further below) provides read-and-write access to operational data based on the inventory of production models. It’s a flexible and extensible foundation upon which one can build automation and tools to solve problems that are specific to individual organizations within Meta that are not served by the general tooling. \nFigure 4: An abridged example of what a model registry entry looks like.\nAs Instagram scaled its investment in AI through rapid innovation in content recommendations, the number of models and AI assets grew; as a result, it has been increasingly important — but also increasingly difficult — to maintain a minimum standard for all of our models, as we lacked an authoritative source for the business context as well as for a model’s importance. \nIn creating the model registry, we set out to provide a structured interface for collecting business context via model types, importance via criticality, and additional metadata that would enable model understanding. Below, we’ll get into the model types, criticality, and automation we’ve built for this purpose.\nModel types\nAt a high level, model type describes the purpose for the ML workload where it represents a category or class of models that share a common purpose or are used in similar contexts. For example, we have “ig_stories_tray_mtml” which is a string attached to training flows, model checkpoints, inference services, and more. Put simply, a model type identifies for the reader this model’s purpose in the ranking funnel.\nLet’s break it down: \n“ig_stories_tray_mtml” → “ig” “stories” “tray” “mtml”\n“ig”: This model is an “ig” model as opposed to “fb” or “whatsapp”.\n“stories”: This model serves IG Stories.\n“tray”: This model serves in the main IG Stories tray (as opposed to stories in some other surface).\n “mtml”: This model is a multi-task-multi-label model, commonly used in late-stage ranking.\nWe can then use these model type strings to tag AI assets, and since they serve as proxies for business context, we can use them also for asset management, policy enforcement, analytics, and more.\nThe metadata entries in the model registry are anchored on two main types that describe model instances (ModelMetadata) as well as model types (ModelTypeMetadata). These types are made up of “core” attributes that are universally applicable, as well as “extended” attributes that allow different teams to encode their opinions about how these entries will inform operations. For example, in Instagram our extended attributes encode “baseline” and “holdout” model IDs, which are used in our ranking infrastructure to orchestrate ranking funnel execution. \nCriticality\nIn addition to defining business function, we had to establish clear guidelines for model importance. Within Meta, SEVs and services have a unified-importance tier system where the Global Service Index (GSI) records a criticality from TIER0 to TIER4 based on the maximum incident severity level the service can cause, from SEV0 as the most critical to SEV4 as simply a “heads up.” Since GSI criticality had social proof at the company, and infra engineers were familiar with this system, we adopted these criticalities for models and now annotate them at the model type and model level.\nNo longer would each team decide to raise their own model services to TIER1 for themselves, increasing the burden on all teams that support these models. Teams needed to provide an immediate response (available 24/7) on call and be able to prove that their models contributed meaningfully to critical business metrics to qualify for elevated monitoring.\nConfiguration structure as a foundation for automation \nOnce we had onboarded a critical mass of Instagram models to the model registry, we could begin to fully integrate with our monitoring and observability suite using our Meta-wide configuration solution, Configerator. With this, we could now have model performance monitoring and alerts that are fully automated and integrated with our tooling for SLIs called SLICK, dashboards that allow us to monitor models across many time series dimensions, and a suite of alerting specific to the model that is driven from the entries in the model registry.\nThis provided all our teams confidence that our monitoring coverage was complete and automated.\nLaunching\nWhile a point-in-time snapshot of models in production is great for static systems, Instagram’s ML landscape is constantly shifting. With the rapid increase of iteration on the recommendation system driving an increased number of launches, it became clear our infrastructure support to make this happen was not adequate. Time-to-launch was a bottleneck in ML velocity, and we needed to drive it down.\nWhat did the process look like?\nConventionally, services were longstanding systems that had engineers supporting them to tune. Even when new changes would introduce new capacity regression risks, we could gate this behind change safety mechanisms. \nHowever, our modeling and experimentation structure was unique in that we were planning for more rapid iteration, and our options were insufficient. To safely test the extent of load a new service could support, we would clone the entire service, send shadow traffic (i.e., cloned traffic that isn’t processed by our clients), and run multiple overload tests until we found a consistent peak throughput. But this wasn’t a perfect science. Sometimes we didn’t send enough traffic, and sometimes we’d send too much, and the amount could change throughout the day due to variations in global user behavior. \nThis could easily take two days to get right, including actually debugging the performance itself when the results weren’t expected. Once we got the result, we’d then have to estimate the final cost. Below (in Figure 5) is the formula we landed on.\nFigure 5: A formula calculating capacity estimations for a new launch.\nThe actual traffic shifting portion was tedious as well. For example, when we managed to fully estimate that we needed 500 replicas to host the new service, we might not actually have 500 spares lying around to do a full replacement, so launching was a delicate process of partially sizing up by approximately 20%, sending 20% of traffic over, and then scaling down the old service by 20% to reclaim and recycle the capacity. Rinse, repeat. Inefficient!\nAnd by the time we got to the end of this arduous process, the ordeal still wasn’t over. Each team was responsible for correctly setting up new alerts for their baseline in a timely fashion, or else their old models could and did trigger false alarms. \nHow does forcing virtual pools aid product growth?\nOne of the prerequisites for fixing competition for resources and unblocking productivity was to put up guardrails. Prior to this, it was “first come first served,” with no clear way to even “reserve” future freed capacity. It was also hard to reason about fairness from an infra perspective: Would it make sense to give each team equal pools, or give each individual person a maximum limit? \nAs it turned out, not all MLEs are experimenting at the same time, due to staggered progress on their work, so individual (per-engineer) limits were not ideal. One member might be in the experimentation stage and another might be training. So our solution was to provide bandwidth to each team. \nOnce each team — and therefore product — had quotas distributed, their launch policy became more clear cut. Some teams established free launching as long as the team was within quota. Others required no regressions in capacity usage. But mostly this unlocked our ability to run launches in parallel, since each one required much less red tape, and prioritization was no longer done at the org level.\nWhat other tooling improved launching?\nAs mentioned earlier, preplanning with capacity estimations was critical to understanding cost and ensuring reliability. We were often asked, Why not let autoscaling take care of everything? The problem was that each service could be configured slightly differently than a previously optimized service, or some architectural change could have affected the performance of the model. We didn’t have an infinite amount of supply to work with, so by the time we fully traffic-shifted everything over, we might find that we didn’t have enough supply. Reverting is costly, taking hours to get through each stage.\nBy doing capacity estimations in advance, this also allowed us and each team to accurately evaluate metric improvement versus cost. It might be worthwhile to double our costs if something would increase time spent on the app by 1%, but likely not for a 0.05% improvement where we could better spend that capacity funding another initiative.  \nWith partners in AI Infra, we developed two major solutions to this process: offline performance evaluation and an automated launching platform.\nWe simplified determining performance of a new service using recorded traffic. Pre-recorded traffic was continuously collected into a data warehouse that the benchmarker could read from, and we’d spin up temporary jobs with this automation. One job would replay different levels of traffic continuously and send it to another job that was a clone of the existing experiment. By putting stoppers on desired latency and error rates, the tooling would eventually output a converged stable number that we could understand as the max load (see Figure 6).\nFigure 6: Load tests converging on an accurate measure of load.\nThe launch platform itself would input the numbers we captured from these tests, automatically collect demand data as defined, and run that same formula to calculate a cost. The platform would then perform the upscaling/downscaling cycle for teams as we shifted traffic.\nAnd finally, by leveraging the model registry, we were able to land this model change in code (see example in Figure 6), to help us better maintain and understand the 1000+ models within our fleet. Likewise, this bolstered our trust in the model registry, which was now directly tied to the model launch lifecycle.\nFigure 7: A theoretical model registry change during launch.\nThis suite of launch automation has dramatically reduced the class of SEVs related to model launches, improved our pace of innovation from a few to more than 10 launches per week, and reduced the amount of time engineers spend conducting a launch by more than two days.\nModel stability\nAs the number of models in production increased, our organization started to feel the effects of an inconsistent measure of model health. While ranking models are run like any other distributed backend system (receive a request, produce a response), one may think a universal SLO that measures request success rate can suffice to capture holistic health. This is not the case for ranking models, as the accuracy of recommendations received carries significant importance to the end-user experience. If we consider a user who is a huge fan of golf but does not enjoy cooking content (see the “available & irrelevant” case in Figure 8 below), we see an example of this inaccuracy in practice. This is precisely what the model stability metric sought to capture.\nFigure 8: Different types of responses that can be provided to an end user.\nWhy is measuring ranking model reliability unique?\nRanking models, unlike traditional idempotent request/response backends, produce scores predicting user action given a set of candidates (PLIKE, PCOMMENT, PFOLLOW, etc.). These scores then combine and are used to determine which candidates are most relevant to an end user. It’s important that these scores accurately reflect user interest, as their accuracy is directly correlated to user engagement. If we recommend irrelevant content, user engagement suffers. The model stability metric was designed to make it easy to measure this accuracy and detect inaccuracy at our scale. \nLet’s discuss how this works.\nDefining model stability\nModels are complex, and they produce multiple output predictions. Let’s take a simplified example (shown in Figure 9 below) of a multi-task-multi-label (MTML) model predicting three actions:\nFigure 9: A simplified MTML model predicting three actions.\nFor us to claim this model is stable, we must also claim that each underlying prediction is stable.\nWhen evaluating the accuracy of a ranking model’s predictions, we typically look at two metrics:\nModel calibration, which is based on observed real-world outcomes and answers the question, “Are we over- or under-predicting user action?” It is calculated as a ratio of predicted click-through-rate (CTR) and empirical CTR. A perfect predictor will have calibration centered at 1.\nModel normalized entropy (NE), which measures the discriminative power of a predictor, and answers the question, “How well can this predictor separate action from inaction?” It is calculated as a ratio of the average log-loss per impression to what the average log-loss per impression would be if we always predicted the empirical CTR. With NE, lower values are better, and an NE of 1 is equivalent to random predictions.\n(For more information regarding our choice of prediction evaluation metrics, please refer to the paper, “Practical Lessons from Predicting Clicks on Ads at Facebook.”)\nA model’s predictions are unstable when either calibration or NE are out of their expected healthy ranges. To determine what a healthy range is, we must look at each metric in real time, and Figure 10 below shows what these time series can look like:\nFigure 10: Example predictions of calibration and NE over a period of time.\nBy observing the trend of a healthy prediction, we can apply thresholds for our evaluation metrics. When these thresholds are breached, the underlying prediction is considered unstable.\nFrom here, we can define model stability as a binary indicator across a model’s predictions. It is 1 if all underlying predictions are stable, and 0 if any prediction is unstable. This is an extremely powerful method of reacting to real-time prediction instability as well as a tool for understanding trends in predictive health per model or across distinct products ranking funnels.\nOperationalizing model stability\nWith a real-time view on model predictive health, we can leverage this unified definition of model stability and apply it to all of our models in production, once again leveraging the model registry as a ledger to hold this important data. In Figure 11 below, we can see the addition of model stability metric metadata after we determined the expected thresholds.\nFigure 11: Model stability definitions stored in the model registry.\nGiven the large number of models in production, each producing many predictions, building a portable definition of model health applicable to all of our ranking models represented an important milestone toward upleveling Instagram’s ML infrastructure maturity. This has unlocked our ability to build generic alerting to guarantee detection of our most important models becoming unstable, thereby moving us closer to mitigation when our recommendation system is at risk. \nSince the addition of these metrics and alerting, ML teams have discovered previously hidden issues within their models and addressed them faster than before, leading to higher-quality recommendations.\nKey takeaways\nIn our journey to scale Instagram’s algorithm to manage over 1000 models, we have learned several critical lessons that have shaped our approach and infrastructure. These takeaways not only highlight the challenges we faced but also underscore the strategies that led to our success.\nInfra understanding is the foundation to building the right tools\nA unified understanding of our infrastructure footprint was essential in developing the right tools to support our scaling efforts. By identifying the gaps and potential risks in our existing systems, we were able to implement solutions such as the model registry that significantly improved our operational efficiency and reliability posture.\nHelping colleagues move fast means we all move faster\nBy addressing the model iteration bottleneck, we enabled our teams to innovate more rapidly. Our focus on creating a seamless, self-service process for model iteration empowered client teams to take ownership of their workflows. This not only accelerated their progress but also reduced the operational burden on our infrastructure team. As a result, the entire organization benefited from increased agility and productivity.\nReliability must consider quality\nEnsuring the reliability of our models required us to redefine how we measure and maintain model quality. By operationalizing model stability and establishing clear metrics for model health, we were able to proactively manage the performance of our models. This approach enables us to maintain high standards of quality across our recommendation systems, ultimately enhancing user engagement and satisfaction.\nOur experience in scaling Instagram’s recommendation system has reinforced the importance of infrastructure understanding, collaboration, and a focus on quality. By building robust tools and processes, we have not only improved our own operations but also empowered our colleagues to drive innovation and growth across the platform.\nThe post Journey to 1000 models: Scaling Instagram’s recommendation system appeared first on Engineering at Meta.",
      "summary": "以下是Instagram如何扩展其推荐系统至1000多个机器学习模型的摘要，同时保持推荐质量和可靠性：\n\n*   **背景：** Instagram的算法基础设施随着其影响力的增长而变得复杂，带来了可靠性和可扩展性方面的挑战。\n*   **问题：**\n    *   模型数量庞大，需要管理每个模型的性能特征和产品目标。\n    *   基础设施成熟度滞后于雄心勃勃的扩展目标。\n    *   ML工程师在实验中需要调整权重等参数，导致模型数量快速增长。\n    *   模型发现、发布和健康状况监控方面存在风险。\n\n*   **解决方案：**\n    *   **模型注册表：** 作为生产模型重要性和业务功能的账本，提供系统范围内的可观察性、变更管理和模型健康状况。\n\n    *   **模型发布工具：** 自动化新模型发布流程，缩短发布时间。\n\n    *   **模型稳定性：** 定义了模型稳定性指标，用于衡量模型预测的准确性，并为所有模型设定SLO。\n\n*   **模型注册表详情：**\n    *   解决了模型调查过程耗时且容易出错的问题。\n\n    *   标准化了模型重要性和业务功能的收集，确保运营资源用于最重要的模型。\n\n    *   基于Configerator构建，提供对运营数据的读写访问。\n\n    *   通过模型类型（例如，ig_stories_tray_mtml）和重要性（基于GSI criticality）来组织模型。\n\n    *   与监控和可观察性套件集成，实现模型性能的自动化监控和警报。\n\n*   **模型发布流程改进：**\n    *   传统发布流程耗时且效率低下，需要克隆服务、发送影子流量和进行过载测试。\n\n    *   引入容量估算公式（如图5所示）来预先规划容量需求。\n\n    *   实施虚拟池来解决资源竞争问题，允许多个团队并行发布。\n\n    *   改进了容量估算工具，以便准确评估指标改进与成本之间的关系。\n\n*   **容量估算公式：**\n\n    ![Figure 5: A formula calculating capacity estimations for a new launch.](https://example.com/image5.png)  *(请注意：这里需要替换成文章中实际的图片链接，由于文章中没有提供实际的图片链接，这里用一个占位符代替)*\n\n*   **排名漏斗：**\n\n    ![Figure 1: The ranking funnel.](https://example.com/image1.png) *(请注意：这里需要替换成文章中实际的图片链接，由于文章中没有提供实际的图片链接，这里用一个占位符代替)*\n\n*   **模型增长因素：**\n\n    ![Figure 2: An expression of the factors behind the fleet’s numerical growth.](https://example.com/image2.png) *(请注意：这里需要替换成文章中实际的图片链接，由于文章中没有提供实际的图片链接，这里用一个占位符代替)*\n\n*   **低效的调查过程：**\n\n    ![Figure 3: A fictional but typical non-productive investigation.](https://example.com/image3.png) *(请注意：这里需要替换成文章中实际的图片链接，由于文章中没有提供实际的图片链接，这里用一个占位符代替)*\n\n*   **模型注册表示例：**\n\n    ![Figure 4: An abridged example of what a model registry entry looks like.](https://example.com/image4.png) *(请注意：这里需要替换成文章中实际的图片链接，由于文章中没有提供实际的图片链接，这里用一个占位符代替)*\n}\n```",
      "translated_title": "迈向1000个模型：扩展Instagram的推荐系统",
      "contentSource": "RSS"
    }
  ],
  "lastUpdated": "2025-05-28T21:13:55.212Z",
  "processingStats": {
    "totalItems": 2,
    "newItems": 0,
    "existingItems": 2,
    "lastProcessed": "2025-05-28T21:13:55.212Z"
  }
}