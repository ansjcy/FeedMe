{
  "sourceUrl": "https://engineering.fb.com/feed/",
  "title": "Engineering at Meta",
  "description": "Engineering at Meta Blog",
  "link": "https://engineering.fb.com/",
  "items": [
    {
      "title": "扩展LLM推理：张量并行、上下文并行和专家并行方面的创新 (原标题: Scaling LLM Inference: Innovations in Tensor Parallelism, Context Parallelism, and Expert Parallelism)",
      "link": "https://engineering.fb.com/2025/10/17/ai-research/scaling-llm-inference-innovations-tensor-parallelism-context-parallelism-expert-parallelism/",
      "pubDate": "Fri, 17 Oct 2025 16:00:50 +0000",
      "isoDate": "2025-10-17T16:00:50.000Z",
      "creator": "",
      "summary": "Meta公司正在不断推动大型语言模型（LLM）推理系统的边界，以支持Meta AI应用等。为了优化资源效率、吞吐量和延迟等关键性能指标，Meta开发并实施了先进的并行化技术。\n\n## LLM推理的关键性能指标\nMeta的主要目标是优化以下指标：\n*   **资源效率**：最大化GPU利用率，提高运营效率。\n*   **吞吐量（查询/秒）**：处理更高容量的请求，服务更多用户。\n*   **延迟**：最小化响应时间，提供无缝用户体验，包括：\n    *   **预填充（Prefill）阶段的首个令牌时间（TTFT）**：理想情况下低于350毫秒。\n    *   **解码（Decoding）阶段的增量令牌时间（TTIT）**：目标低于25毫秒。\n\n这些指标突出了LLM推理的不同计算需求：预填充是计算密集型的，而解码是内存带宽密集型的。\n\n## LLM推理的两个阶段\n典型的LLM生成推理任务分为两个阶段：\n1.  **预填充阶段**：处理输入提示（可能长达数千个令牌），为LLM的每个Transformer层生成键值（KV）缓存。由于注意力机制随序列长度呈二次方增长，此阶段受计算限制。\n2.  **解码阶段**：利用并逐步更新KV缓存，逐个生成令牌。此阶段受内存限制，因为读取内存的I/O时间占据主导地位，模型权重和KV缓存占据大部分内存。\n\n## 通过并行化解决瓶颈\n为了有效扩展LLM推理，特别是处理长上下文和大型模型，Meta采用了三种主要的推理并行化技术：\n\n### 1. 张量并行（Tensor Parallelism, TP）\n*   **目的**：将大型模型分布到多个GPU上，实现单个设备无法提供的高吞吐量。它将模型的单个层（如注意力块和多层感知器层）分片成更小的独立块，在不同设备上执行。\n*   **挑战**：`allreduce`通信操作可能占端到端延迟的30%。\n*   **解决方案：直接数据访问（DDA）算法**：\n    *   **DDA flat算法**：通过允许每个rank直接从其他rank加载内存并执行本地reduce操作，改善小消息大小的`allreduce`延迟，将延迟从O(N)降低到O(1)。\n    *   **DDA tree算法**：将`allreduce`分解为reduce-scatter和all-gather两个阶段，并在每一步中使用直接数据访问，将延迟降低到常数因子，适用于稍大的消息大小。\n*   **成果**：DDA解决方案在AMD MI300X上与Nvidia H100实现了整体性能持平，DDA在解码（小消息）方面比RCCL基线快10-50%，在预填充方面快10-30%，从而使TTIT减少约10%。\n\n### 2. 上下文并行（Context Parallelism, CP）\n*   **目的**：管理和处理极长的上下文，例如Llama 4引入的1M/10M令牌能力。\n*   **挑战**：\n    *   **计算**：密集注意力FLOPs随上下文长度呈二次方增长。\n    *   **内存**：KV缓存随上下文线性增长。\n    *   **通信**：跨多个主机并行化时通信延迟增加。\n*   **解决方案：“环形注意力”（ring attention）变体**：\n    *   **Pass-KV**：输入令牌在多个CP rank之间分割。每个rank计算其查询、键和值张量的一部分，然后交换键和值张量以实现完整上下文的注意力交互。\n    *   **Pass-Q**：类似Pass-KV，但交换查询张量。\n*   **成果**：结合快速注意力内核，在单个H100主机上实现了100万令牌的预填充时间少于一分钟，在32个H100主机上实现了1000万令牌的分布式推理时间少于一分钟。使用Llama 3 405B，在16个节点上通过CP实现了128K令牌预填充3.8秒，1M令牌预填充77秒的近线性扩展。\n\n### 3. 专家并行（Expert Parallelism, EP）\n*   **目的**：扩展混合专家（MoE）模型，其中大量“专家”模块无法完全适应单个主机。\n*   **机制**：利用两阶段的`all-to-all`通信模式，根据路由在数据并行和专家并行rank之间交换令牌。\n*   **挑战**：`all-to-all`通信可能占端到端延迟的10-30%，尤其对于解码消息（100KB到2MB）。\n*   **正在探索的解决方案**：\n    *   **动态`all-to-all`**：向远程邻居发送数据子块。\n    *   **持久`all-to-all`**：解决主要由内存句柄交换、网络负载平衡和CPU开销引起的减速。\n\n## 展望：解耦推理和未来挑战\n为了进一步优化LLM推理，Meta正朝着N维并行（CP、PP、EP、TP跨节点，以及独立的DP）和解耦预填充与解码层级发展。这将实现更好的资源平衡，并可能使用异构硬件（计算密集型硬件用于预填充，内存带宽密集型硬件用于解码）。\n\n未来的挑战包括：\n*   **云架构设计**：优化底层云基础设施以适应LLM工作负载。\n*   **通信集成到内核（fused kernel）**：将通信操作直接集成到计算内核中以提高效率。\n*   **设备发起内核（device-initiated kernel）**：使设备能够直接发起操作，减少CPU开销。\n\n这些并行化和系统级改进有助于实现下一代AI应用，并推动LLM能力的边界。",
      "shortSummary": "Meta公司通过创新并行化技术，优化LLM推理系统的资源效率、吞吐量和延迟。文章详细介绍了三种关键并行策略：张量并行（TP）通过DDA算法加速大型模型通信；上下文并行（CP）利用“环形注意力”处理超长上下文；专家并行（EP）通过优化`all-to-all`通信扩展MoE模型。这些技术显著提升了LLM推理性能，并为未来N维并行和解耦推理奠定了基础，以应对云架构和内核集成等挑战。",
      "translated_title": "扩展LLM推理：张量并行、上下文并行和专家并行方面的创新",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>At Meta, we are constantly pushing the boundaries of LLM inference systems to power applications such as the Meta AI App. We&#8217;re sharing how we developed and implemented advanced parallelism techniques to optimize key performance metrics related to resource efficiency, throughput, and latency. The rapid evolution of large language models (LLMs) has ushered in a [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/17/ai-research/scaling-llm-inference-innovations-tensor-parallelism-context-parallelism-expert-parallelism/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/17/ai-research/scaling-llm-inference-innovations-tensor-parallelism-context-parallelism-expert-parallelism/\">Scaling LLM Inference: Innovations in Tensor Parallelism, Context Parallelism, and Expert Parallelism</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "Sapling 单体仓库中的分支管理 (原标题: Branching in a Sapling Monorepo)",
      "link": "https://engineering.fb.com/2025/10/16/developer-tools/branching-in-a-sapling-monorepo/",
      "pubDate": "Thu, 16 Oct 2025 17:10:43 +0000",
      "isoDate": "2025-10-16T17:10:43.000Z",
      "creator": "",
      "summary": "Sapling 是 Meta 公司用于其大型单体仓库的可扩展、用户友好且开源的源代码控制系统。文章探讨了在大型单体仓库中设计和实现分支工作流所面临的挑战，以及如何在可扩展性和开发者体验之间进行权衡。\n\n**Meta 的源代码控制方法与挑战**\n\n*   **单体仓库优势**：Meta 的工程团队在一个大型单体仓库中工作，采用单一主分支，这有助于统一依赖管理、大规模重构、简化协作和代码复用。\n*   **版本管理挑战**：然而，这种方法给需要管理多个代码版本的团队带来了挑战。在多仓库设置中，团队可以依赖仓库分支来管理不同版本，并使用 cherry-pick 和 merge 等工具。但在单体仓库中，全仓库分支（full-repo branching）效果不佳。\n*   **全仓库分支的局限性**：\n    *   **冻结无关项目**：创建全仓库分支会冻结无关项目和依赖，导致它们迅速过时。\n    *   **可扩展性问题**：对于需要合并回主分支的开发工作流，全仓库分支不可扩展。它会创建具有多个父级的合并提交，使提交图变得宽泛且非线性，从而导致 `sl log` 和 `sl blame` 等操作的性能问题。\n    *   **“全有或全无”**：全仓库分支是“全有或全无”的，无法为特定项目或部分代码创建分支，导致团队复制代码，失去标准开发工具的优势，增加重复工作和错误。\n\n**Sapling 的单体仓库分支解决方案：目录分支**\n\n为解决上述挑战，Sapling 引入了一套新的源代码控制工具，实现了“目录分支”（directory branching）。\n\n*   **核心理念**：目录分支弥合了使用多个仓库分支和将代码副本作为单独目录维护之间的差距。它允许将单体仓库中的目录视为传统的仓库分支进行管理。\n*   **工作流**：用户可以通过复制代码来创建分支，通过 cherry-pick 维护代码，并在目录之间合并更改，就像它们是分支一样。\n*   **可扩展性优势**：目录分支支持目录间的合并，但在单体仓库的提交图层面，它们表现为线性提交。这解决了仓库级别合并提交带来的可扩展性挑战，同时仍提供目录级别的合并工作流。\n\n**目录分支的实现**\n\n目录分支在 Sapling 中通过一系列围绕 `sl subtree` 命令的操作实现：\n\n*   **`sl subtree copy`**：用于复制目录（或文件），可以从当前版本或任何历史版本复制到仓库中的新位置。Sapling 在提交中记录元数据，跟踪源目录、源修订版本和复制关系，从而恢复新分支中所有文件的完整历史记录。\n*   **`sl subtree import`**：如果代码尚未在单体仓库中，可以使用此命令创建外部仓库分支的目录分支。\n*   **`sl subtree graft` 和 `sl subtree merge`**：用于在目录分支之间进行 cherry-pick 或合并更改。这些操作利用存储的复制/合并元数据来重建目录之间的关系，从而实现三方合并。\n\n**构建系统和开发者工具集成**\n\n*   **优势**：所有目录分支的最新版本同时可见，使得持续集成（CI）可以通过一次检出测试多个分支。\n*   **构建系统集成**：Meta 使用 Buck2 作为构建系统，通过 Buck 配置修饰符（`-m` 标志）来选择使用的分支。\n*   **缺点**：代码搜索可能导致多个分支的重复结果，需要能够对结果进行排名的代码搜索系统来解决。\n\n**用户反馈与采纳原因**\n\n目录分支的引入取得了成功，Meta 内部的许多工程团队已采用它来管理多个代码版本。常见的采纳原因包括：\n\n1.  **CI 成本高昂或可能造成重大中断**：团队使用目录分支有效分离开发和生产版本的代码，更好地控制代码部署。\n2.  **长期实验性开发**：大量开发者协作数月，但更改可能扰乱生产版本，且使用大型差异堆栈模拟分支不切实际。\n3.  **解除 Git 迁移障碍**：在迁移到单体仓库期间，需要 Git 分支的等效功能，以便完成迁移和整合。\n\n尽管单体仓库的默认假设是单一版本，但当上述原因适用时，目录分支提供了一种解决方案，在不牺牲单体仓库优势的情况下实现分支工作流。\n\n**未来工作**\n\nSapling 计划利用目录分支更好地集成 Git 仓库，开发一种轻量级仓库迁移机制。这将通过 `sl subtree import` 命令的一个选项提供，允许创建指向外部仓库的软链接，并在用户请求时动态加载 Git 历史记录，从而降低 Git 仓库进入单体仓库的门槛。",
      "shortSummary": "Sapling 是 Meta 的开源源代码控制系统，用于其大型单体仓库。文章讨论了在大型单体仓库中管理分支的挑战，特别是全仓库分支在可扩展性方面的局限性。为解决此问题，Sapling 引入了“目录分支”解决方案。该方案允许将目录视为传统仓库分支进行复制、挑选和合并，同时在提交图上保持线性，从而解决了性能问题。目录分支通过 `sl subtree` 命令实现，已成功被 Meta 团队采纳，并计划用于更好地集成 Git 仓库，降低迁移障碍。",
      "translated_title": "Sapling 单体仓库中的分支管理",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>Sapling is a scalable, user-friendly, and open-source source control system that powers Meta&#8217;s monorepo. As discussed at the GitMerge 2024 conference session on branching, designing and implementing branching workflows for large monorepos is a challenging problem with multiple tradeoffs between scalability and the developer experience. After the conference, we designed, implemented, and open sourced our [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/16/developer-tools/branching-in-a-sapling-monorepo/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/16/developer-tools/branching-in-a-sapling-monorepo/\">Branching in a Sapling Monorepo</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "10倍骨干网：Meta 如何为 AI 扩展骨干网络连接 (原标题: 10X Backbone: How Meta Is Scaling Backbone Connectivity for AI)",
      "link": "https://engineering.fb.com/2025/10/16/data-center-engineering/10x-backbone-how-meta-is-scaling-backbone-connectivity-for-ai/",
      "pubDate": "Thu, 16 Oct 2025 16:30:02 +0000",
      "isoDate": "2025-10-16T16:30:02.000Z",
      "creator": "",
      "summary": "Meta正在详细阐述其如何扩展骨干网络以支持日益增长的AI工作负载，并将其AI集群扩展到多个数据中心。这一“10倍骨干网”项目旨在通过新技术和设计应对十倍的扩展需求。\n\n### Meta的骨干网络概述\nMeta的骨干网络由互连的路由平台组成，提供广域网（WAN）连接。它分为两种主要网络：\n\n*   **经典骨干网 (CBB)**：用于实现从数据中心（DC）到连接外部运营商的接入点（POP）的全球覆盖。CBB灵活多变，支持多种地理位置和连接需求，采用传统的IP/MPLS-TE技术。\n*   **快速骨干网 (EBB)**：专为可扩展的DC到DC互连而构建。EBB灵活性较低，但运行高度定制的软件栈，如Open/R路由协议和内部流量工程栈。EBB是本次扩展面临最大挑战的网络。\n\n自2015年EBB开始承载流量以来，其DC到DC流量增长显著。连接DC需要大量的光纤，且新DC的建设速度快，给网络连接带来了挑战。\n\n### 实现10倍骨干网的三大技术\n\n1.  **DC城域架构**\n    *   **问题**：为新DC构建数百英里长的长途光纤连接非常困难。\n    *   **解决方案**：预构建DC城域架构组件，包括两条提供可扩展容量的光纤环和两个提供远程站点连接的POP。DC连接到这些环，从而增加或启用DC与POP之间的容量。\n    *   **优势**：简化DC连接和WAN拓扑，标准化可扩展的物理设计，分离城域和长途网络。\n\n2.  **IP平台扩展**\n    *   **向上扩展（依赖供应商）**：\n        *   **更大机箱**：如12槽机箱比8槽机箱提供50%的容量，但引入了机械、散热、功耗、空间、ASIC、基础设施和网络操作系统（NOS）复杂性等挑战。\n        *   **更快接口**：通过从400G升级到800G平台，容量可以翻倍，但同样面临散热、功耗、新ASIC、基础设施和NOS复杂性等挑战，并需要支持800G-ZR+收发器。\n    *   **向外扩展（Meta可控）**：\n        *   **增加骨干网平面**：从四个增加到八个平面可以使全球容量翻倍，但这具有高度中断性，需要大量规划，并增加全局功耗和空间需求。\n        *   **每个平面增加多台设备**：允许在特定位置扩展容量，但对目标站点具有中断性，引入新的故障模式，并使网络操作更加复杂。\n    *   Meta在10倍骨干网的旅程中同时采用了向上和向外扩展技术。\n\n3.  **IP与光纤集成**\n    *   **ZR技术**：通过利用ZR技术，Meta改变了网络中每太比特的功耗足迹。\n        *   **变化**：移除了独立的转发器，其功能现在集成到路由器插头中。每个插头仅增加10-15W的功耗，而每个转发器曾消耗高达2kW。\n        *   **总体效益**：聚合功耗降低80%到90%。\n    *   **高层变化**：\n        *   **成本和能效**：以更小的空间和功耗包络部署相同的骨干网容量。\n        *   **机架分配**：光纤与IP设备的机架分配从90/10（ZR之前）变为60/40（ZR之后）。\n        *   **光纤对数**：每个机架可连接的光纤对数从1x增加到4x。\n        *   **简化部署**：安装可插拔模块比安装独立的转发器更简单、更可预测。\n        *   **简化运维**：减少活动设备数量。\n        *   **互操作性和供应商多样性**。\n    *   **挑战**：IP与光纤功能界限变得模糊，光通道的遥测和数据收集会增加IP设备的CPU消耗。\n\n### AI骨干网的构建\n过去18个月，对构建更大GPU集群的需求日益增长，超出了现有数据中心园区的容量。Meta正在寻找地理位置邻近的扩展地点，并与光纤采购团队合作，确定连接现有地点的时间、可行性和技术。\n\n根据所需距离，Meta提出了三种解决方案：\n*   **FR插头**：适用于3公里范围内的建筑连接。\n*   **LR插头**：将距离增加到10公里范围。\n*   **ZR插头 + 光纤DWDM技术**：用于超过10公里的距离，需要有源光组件进行复用和放大信号。这种方法将光纤数量减少了64倍，并采用了最新一代C+L波段800G ZR技术。保护切换虽然增加了操作挑战，但减少了IP平台端口的消耗。每个光纤对可承载64x 800G (51.2T) 的容量，通过水平扩展实现所需总容量。\n\n目前，单个AI骨干网站点对的规模是Meta过去十年构建的全球骨干网的两倍，这带来了部署和配置方面的巨大挑战。\n\n### 经验教训与未来展望\nEBB的扩展在过去八九年中经历了意想不到的加速，原计划2028年实现的扩展目标提前到2024年。\n\n**主要经验教训**：\n*   10倍骨干网的实现得益于向上和向外扩展的创新。\n*   预构建可扩展的城域设计能够更快地响应网络增长。\n*   IP/光纤集成减少了活动设备数量、空间和功耗，并实现了进一步扩展。\n*   复用10倍骨干网技术使得AI骨干网的构建成为可能。\n\n**未来展望**：Meta计划建设城市级数据中心，骨干网必须随之演进和扩展。叶脊架构被视为下一步扩展平台的关键，它能以更少的中断步骤提供所需的规模。Meta将继续执行AI骨干网的初始计划，并在部署更多站点和成熟运营过程中，深入理解AI在光网络中的复杂性。",
      "shortSummary": "Meta正通过“10倍骨干网”项目扩展其骨干网络，以支持AI工作负载和跨数据中心AI集群。该项目主要关注Express Backbone (EBB)网络，并采用了三大核心技术：预构建的DC城域架构、IP平台（向上和向外）扩展，以及利用ZR技术实现IP与光纤集成，大幅提升了能效和部署效率。为AI骨干网，Meta还开发了针对不同距离的连接方案，并计划采用叶脊架构应对未来城市级数据中心的需求。单个AI骨干网站点对的规模已是过去十年全球骨干网的两倍。",
      "translated_title": "10倍骨干网：Meta 如何为 AI 扩展骨干网络连接",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>We&#8217;re sharing details on our journey to scale Meta&#8217;s Backbone network to support the increasing demands of new and existing AI workloads. We&#8217;ve developed new technologies and designs to address our 10x scaling needs and applying some of these same principles to help extend our AI clusters between multiple data centers. Meta’s Backbone network is [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/16/data-center-engineering/10x-backbone-how-meta-is-scaling-backbone-connectivity-for-ai/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/16/data-center-engineering/10x-backbone-how-meta-is-scaling-backbone-connectivity-for-ai/\">10X Backbone: How Meta Is Scaling Backbone Connectivity for AI</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "可持续性设计：降低IT硬件排放的新设计原则 (原标题: Design for Sustainability: New Design Principles for Reducing IT Hardware Emissions)",
      "link": "https://engineering.fb.com/2025/10/14/data-center-engineering/design-for-sustainability-new-design-principles-for-reducing-it-hardware-emissions/",
      "pubDate": "Tue, 14 Oct 2025 20:40:20 +0000",
      "isoDate": "2025-10-14T20:40:20.000Z",
      "creator": "",
      "summary": "## 可持续性设计：降低IT硬件排放的新设计原则\n\nMeta公司提出了“可持续性设计”原则，这是一套针对IT硬件新设计的技术设计原则，旨在通过重用、延长使用寿命和优化设计来减少排放和成本。Meta已通过整合模块化、重用、改造、去物质化、使用更环保材料以及延长硬件生命周期等多种设计策略，显著降低了其数据中心的碳足迹，并呼吁更广泛的行业采纳这些策略以实现可持续发展目标。\n\n### Meta的可持续发展目标与关注点\n\n*   **净零排放目标**：Meta致力于在2030年实现整个价值链的净零排放。\n*   **关注Scope 3排放**：主要关注来自数据中心建设、IT硬件（计算、存储、冷却设备）和网络光纤基础设施等物理来源的Scope 3（价值链）排放。\n*   **排放来源**：包括IT硬件的制造、交付以及报废处理、回收或转售相关的排放。\n*   **控制与减少策略**：\n    *   优化材料选择。\n    *   在设计中选择和开发低碳替代品。\n    *   与硬件供应商合作，减少上游排放。\n\n### 什么是可持续性设计？\n\n“可持续性设计”是Meta开发和提出的一套指导方针，旨在帮助硬件设计师减少IT机架的环境影响。它考虑了能效、材料的选择、减少、循环性以及硬件的报废处理等多种因素。可持续硬件设计需要硬件设计师、工程师和可持续性专家之间的协作，以创建既满足性能要求又限制环境影响的硬件。本指南特别关注数据中心机架的设计，并为各种组件（如机械、冷却、计算、存储和布线）提供可持续的选择。\n\n### 减少Scope 3排放的内部团队行动\n\nMeta内部团队为减少IT硬件的Scope 3排放采取了以下措施：\n\n*   **优化硬件设计**：以最低排放为目标，尽可能延长材料的使用寿命，或使用低碳材料。\n*   **提高效率**：通过延长IT机架的使用寿命，可能跳过新一代设备的采购。\n*   **组件回收**：回收不再可用的服务器组件作为备件，例如从报废机架中回收双列直插式内存模块（DIMM）并重新部署到新构建中。\n*   **排放概况分析**：了解供应商、组件和系统设计的排放概况，以指导未来的减排路线图。\n*   **供应商协作**：与供应商合作，推动其制造过程电气化，转向可再生能源，并利用低碳材料和设计。\n\n这些减少IT硬件Scope 3排放的行动，同时也有助于减少数据中心产生的电子垃圾（e-waste）。\n\n### Meta部署的机架类型与组件\n\nMeta数据中心部署了多种机架设计，以支持不同的工作负载和基础设施需求，主要包括：\n\n*   **AI**：AI训练和推理工作负载。\n*   **计算**：运行Meta产品和服务所需的通用计算。\n*   **存储**：存储和维护产品使用的数据。\n*   **网络**：提供服务器之间的低延迟互连。\n\n尽管这些机架类型在架构上存在差异，但大多数都应用通用的硬件设计原则，并包含来自类似供应商的活跃和被动组件。因此，相同的可持续性设计原则适用于这些不同的机架类型。每个机架内，有五大类组件是排放减少的目标：\n\n*   计算（即内存、HDD/SSD）\n*   存储\n*   网络\n*   电源\n*   机架基础设施（即机械和散热）\n\n### 减少排放的具体技术\n\nMeta主要通过以下四个类别来解决与硬件组件相关的排放问题：\n\n1.  **模块化机架设计**\n    *   允许旧机架组件在新机架中重用，例如Open Rack设计（ORv2和ORv3）。\n    *   **ORv3关键特点**：电源单元（PSU）和电池备份单元（BBU）分离，实现更可靠和灵活的配置；48V电源输出，允许电源货架在机架中任意放置；可根据平台和区域需求配置PSU和BBU货架；正在努力设计“通用化”ORv3机架框架以简化组装和降低成本；ORv3N是ORv3的衍生品，专为网络应用设计，提供效率和成本改进。\n    *   随着AI工作负载的扩展，新的专业机架设计需要设计师采纳最模块化的设计原则。\n\n2.  **重用/改造现有机架设计**\n    *   这是一种经济高效且可持续的方法，可以满足不断演进的数据中心需求，减少电子垃圾、降低成本并加速部署。\n    *   **优点**：成本节约、减少电子垃圾、加快部署、环境效益。\n    *   **挑战**：兼容性问题、电源和冷却要求、可扩展性和灵活性限制、测试和验证。\n    *   Meta认为，在每个新机架设计中都应充分考虑重用或改造现有机架的益处。\n\n3.  **绿色和回收材料**\n    *   **绿色钢铁**：通过电弧炉（EAF）而非传统高炉生产，使用清洁可再生电力和更高比例的回收含量，显著减少碳排放。Meta与提供100%清洁可再生能源生产绿色钢铁的供应商合作。\n    *   **回收钢铁、铝和铜**：回收这些材料可节省大量生产硬件所需的能源。Meta要求所有机架/底盘至少包含20%的回收钢铁，所有散热器必须完全由回收铝或铜制造。\n\n4.  **提高可靠性以延长使用寿命**\n    *   延长机架、服务器、内存和SSD的使用寿命有助于Meta减少硬件采购量，从而显著降低排放和成本。\n    *   可靠性基准测试是评估硬件寿命延长可行性的重要因素。\n    *   需要考虑备件和供应商支持的可用性可能下降，以及设备故障率增加的风险。\n\n5.  **去物质化**\n    *   减少和移除不必要的硬件组件可以显著减少原材料、水和/或能源的使用。\n    *   这包括减少机架上的钢铁使用，或在保持设计约束的同时移除服务器主板上不必要的组件。\n    *   去物质化还涉及将多个机架整合为更少、更高效的机架，从而减少整体物理占地面积。\n    *   硬件板上多余组件的原因可能包括：未来验证、灵活性、调试和测试、冗余、模块化、法规遵从性。\n    *   Meta强调，每个硬件设计都应优化组件填充，避免不必要的组件，例如未填充的集成电路（IC）插座、未使用的连接器、测试点、冗余电源或可选内存/存储组件。\n\n6.  **生产化低排放新技术**\n    *   内存和SSD/HDD通常是服务器机架中最大的隐含碳排放源。\n    *   新技术可以帮助Meta显著减少排放和成本，同时提供更高的功率归一化性能。\n    *   **示例**：\n        *   从HDD转向SSD：减少驱动器、服务器、机架、BBU和PSU的需求，并有助于降低整体能耗。\n        *   液冷：根据当地环境条件和数据中心工作负载，液冷在服务器机架中可能比传统空冷碳效率高17%。\n        *   探索替代技术：如相变存储器（PCM）或磁阻随机存取存储器（MRAM），它们具有相同性能但碳排放较低。\n        *   使用低功耗双倍数据速率（LPDDR）代替DDR，以实现低功耗和高带宽。",
      "shortSummary": "Meta推出了“可持续性设计”原则，旨在通过重用、延长使用寿命和优化设计，减少IT硬件的排放和成本。这些原则关注数据中心IT硬件的Scope 3排放，涵盖模块化设计、重用改造、使用绿色和回收材料、提高可靠性、去物质化以及采用低排放新技术。Meta呼吁全行业采纳这些策略，以实现2030年净零排放目标并减少电子垃圾。",
      "translated_title": "可持续性设计：降低IT硬件排放的新设计原则",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>We’re presenting Design for Sustainability,  a set of technical design principles for new designs of IT hardware to reduce emissions and cost through reuse, extending useful life, and optimizing design. At Meta, we’ve been able to significantly reduce the carbon footprint of our data centers by integrating several design strategies such as modularity, reuse, retrofitting, [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/14/data-center-engineering/design-for-sustainability-new-design-principles-for-reducing-it-hardware-emissions/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/14/data-center-engineering/design-for-sustainability-new-design-principles-for-reducing-it-hardware-emissions/\">Design for Sustainability: New Design Principles for Reducing IT Hardware Emissions</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "Meta 如何利用人工智能提高 IT 硬件范围 3 排放估算的质量 (原标题: How Meta Is Leveraging AI To Improve the Quality of Scope 3 Emission Estimates for IT Hardware)",
      "link": "https://engineering.fb.com/2025/10/14/data-center-engineering/how-meta-is-leveraging-ai-to-improve-the-quality-of-scope-3-emission-estimates-for-it-hardware/",
      "pubDate": "Tue, 14 Oct 2025 20:40:01 +0000",
      "isoDate": "2025-10-14T20:40:01.000Z",
      "creator": "",
      "summary": "Meta 致力于在 2030 年实现净零排放，并旨在为整个行业创建测量碳排放的通用分类法。为实现这一目标，Meta 在 2025 年 OCP 区域 EMEA 峰会上提出了一种新方法，该方法利用人工智能 (AI) 来改进对 IT 硬件范围 3 排放的理解。Meta 正与 OCP PCR 工作流合作，计划在 2025 年 OCP 全球峰会上开源此方法论。\n\n**IT 硬件范围 3 排放估算的挑战**\n\n*   理解服务器硬件的碳足迹对于可持续采购和设计至关重要。\n*   挑战在于复杂的供应链和供应商有限的数据，使得精确计算碳足迹变得困难。\n*   数据中心 IT 硬件的制造和运输所产生的隐含碳是主要的排放源，且难以量化。\n\n**Meta 的排放估算方法论**\n\nMeta 开发了一种方法来估算和追踪数据中心数亿个组件的碳排放。该方法结合了：\n\n*   **基于成本的估算**\n*   **模型估算**\n*   **组件特定的产品碳足迹 (PCF)**\n\n这些组件级别的估算根据数据质量进行排名，并聚合到服务器机架级别。这种方法允许 Meta 从单个螺丝到整个机架组件进行多级别粒度的排放分析，从而识别高影响的减排区域。最终目标是推动行业采用更可持续的制造实践，并生产低排放组件。\n\n**AI 在改进范围 3 排放估算中的应用**\n\nMeta 利用 AI 改进其数据库并理解 IT 硬件相关的范围 3 排放，主要通过以下方式：\n\n1.  **识别相似组件并应用现有 PCF**\n    *   **问题**：PCF 耗时且通常与特定标识符绑定，但库存中存在大量相似或变体组件。若不识别这些相似组件，其碳足迹估算数据质量会较低。\n    *   **方法**：Meta 采用自然语言处理 (NLP) 算法（如 TF-IDF 和余弦相似度）分析文本描述，识别相似组件。当收到新的 PCF 时，算法会识别同类别中描述相似的组件，并将其映射到代表性 PCF。对于数据质量低的组件，算法会寻找高质量的参考数据来改进估算。\n    *   **目的**：确保高质量 PCF 数据在所有相似组件中得到有效利用，提高数据准确性和一致性，并增强碳足迹估算的追溯性。\n\n2.  **从异构数据源提取数据**\n    *   **问题**：当 PCF 不可用时，Meta 避免使用基于支出的方法。参数化模型需要一致的输入数据，但信息可能存储在不同的表格、格式或单位中，导致难以应用模型。\n    *   **方法**：Meta 使用大型语言模型 (LLM)，特别是 Llama 3.1，从异构数据源中提取相关信息，并将其注入参数化模型。LLM 能够识别相同信息的不同表示形式。\n    *   **应用**：该方法已应用于内存（提取容量）和电缆（提取长度/类型）的排放计算，确保估算的一致性。\n\n3.  **创建 IT 硬件排放的组件级分类法**\n    *   **问题**：不同供应商的物料清单 (BOM) 结构不同，使得识别减排行动变得困难。\n    *   **方法**：Meta 使用 AI 将机架的描述性数据分类为两个层次：\n        *   **领域级**：机架的主要功能分组（如计算、网络、电源、机械、存储）。\n        *   **组件级**：主要排放源组件（如 CPU、GPU、DRAM、闪存）。\n    *   **过程**：在探索阶段，生成式 AI (GenAI) 模型自由识别潜在类别。与内部硬件专家审查后，确定了主要组件的固定列表，然后使用严格的 GenAI 分类器进行分组，生成互斥的层次结构。\n    *   **目的**：为碳足迹分析提供帮助，并推动行业采用通用的 IT 硬件碳足迹分类法，以便比较不同类型和世代硬件的排放。\n\n**未来计划：开源分类法和方法论**\n\nMeta 计划开源其学习成果和方法论，包括：\n\n*   服务器机架排放核算的分类法和方法论。\n*   使用 GenAI 分类器的分类法构建器。\n*   改进全行业设施报告流程的聚合方法论。\n\nMeta 致力于与 OCP PCR 组织合作，共同发展和分享这些方法论。",
      "shortSummary": "Meta 致力于在 2030 年实现净零排放，并正利用 AI 提高 IT 硬件范围 3 排放估算的质量。面对供应链复杂和数据有限的挑战，Meta 开发了一种结合成本、模型和产品碳足迹 (PCF) 的方法。AI 在此过程中发挥关键作用：通过 NLP 识别相似组件以应用 PCF；利用 LLM 从异构数据源提取信息；并使用生成式 AI 创建 IT 硬件的通用分类法，以识别减排热点。Meta 计划开源这些分类法和方法论，以推动行业标准化和可持续实践。",
      "translated_title": "Meta 如何利用人工智能提高 IT 硬件范围 3 排放估算的质量",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>As we focus on our goal of achieving net zero emissions in 2030, we also aim to create a common taxonomy for the entire industry to measure carbon emissions. We’re sharing details on a new methodology we presented at the 2025 OCP regional EMEA summit that leverages AI to improve our understanding of our IT [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/14/data-center-engineering/how-meta-is-leveraging-ai-to-improve-the-quality-of-scope-3-emission-estimates-for-it-hardware/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/14/data-center-engineering/how-meta-is-leveraging-ai-to-improve-the-quality-of-scope-3-emission-estimates-for-it-hardware/\">How Meta Is Leveraging AI To Improve the Quality of Scope 3 Emission Estimates for IT Hardware</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "OCP 峰会 2025：AI 网络硬件的开放未来 (原标题: OCP Summit 2025: The Open Future of Networking Hardware for AI)",
      "link": "https://engineering.fb.com/2025/10/13/data-infrastructure/ocp-summit-2025-the-open-future-of-networking-hardware-for-ai/",
      "pubDate": "Tue, 14 Oct 2025 00:00:20 +0000",
      "isoDate": "2025-10-14T00:00:20.000Z",
      "creator": "",
      "summary": "# OCP 峰会 2025：AI 网络硬件的开放未来\n\n在2025年开放计算项目（OCP）峰会上，Meta详细介绍了其AI训练集群下一代网络架构的发展方向。Meta扩展了其网络硬件产品组合，并向OCP贡献了新的解耦网络平台，旨在通过开放机架、服务器、存储盒和主板设计，促进整个行业的创新和协作。\n\n## 开放硬件与AI创新\nMeta坚信开放硬件是创新的催化剂，尤其是在数据中心基础设施日益支持新兴AI技术的情况下。开放硬件通过实现解耦，将传统数据中心技术分解为核心组件，从而构建更灵活、可扩展和高效的系统。自2011年共同创立OCP以来，Meta一直分享数据中心和组件设计，并开源其网络操作系统FBOSS，以推动行业内外的创新。\n\n## 数据中心网络新里程碑\nMeta宣布了其数据中心网络的几项重要进展：\n\n### 1. 解耦调度结构（DSF）的演进\n*   **规模扩展**：DSF已演进为支持跨越整个数据中心建筑的大型AI集群的横向扩展互连。\n*   **双阶段架构**：去年的DSF是基于VOQ的系统，支持开放标准的以太网RoCE接口。现在已发展为双阶段架构，可扩展支持多达18,432个XPU的无阻塞网络，是构建跨区域AI集群的基础。\n*   **大规模部署**：Meta已利用双阶段DSF构建了规模达整个数据中心建筑的18,000个GPU集群。\n\n### 2. 非调度结构（NSF）架构\n*   **新架构**：与DSF演进并行，Meta开发了新的非调度结构（NSF）架构。\n*   **关键特性**：\n    *   基于浅缓存OCP以太网交换机。\n    *   提供低往返延迟。\n    *   支持自适应路由，实现有效负载均衡，优化利用率并最小化拥塞。\n    *   作为Prometheus等千兆瓦级AI集群的基础构建模块。\n*   **三层NSF**：用于构建规模化AI集群。\n\n### 3. 新一代AI网络OCP交换机平台\n*   **现有平台**：去年Meta推出了Minipack3（基于Broadcom Tomahawk5）和Cisco 8501（基于Cisco Silicon One G200）两款51T以太网交换机，它们提供51.2 Tbps带宽，无需重定时器，并运行FBOSS。\n*   **Minipack3N**：今年推出基于NVIDIA Spectrum-4交换ASIC的新型51T以太网交换机Minipack3N，采用与Minipack3相同的系统设计，由Meta设计并由Accton制造。\n\n### 4. FBOSS和SAI的演进\n*   **开放基础**：Meta持续将OCP-SAI作为新网络结构、交换机硬件平台和光收发器集成到FBOSS的基础。\n*   **协作创新**：通过与供应商和OCP社区的紧密合作，SAI已演进以支持DSF、NSF以及为现代数据中心和AI工作负载量身定制的其他增强路由方案等高级功能。这种开放方法加速了整个行业的进步。\n\n### 5. 400G/800G光互连技术\n*   **现有部署**：去年推出的2x400G FR4 BASE（3公里）光模块已在Meta数据中心广泛部署。\n*   **新产品**：\n    *   **2x400G FR4 LITE（500米）**：为数据中心内部大多数用例优化，旨在加速光模块成本降低。\n    *   **400G DR4 OSFP-RHS**：Meta首代用于AI主机侧NIC连接的DR4解决方案。\n    *   **2x400G DR4 OSFP**：部署在交换机侧，提供从主机到交换机的连接。\n\n## OCP中的“以太网用于横向扩展网络”（ESUN）倡议\nMeta是OCP 2025全球峰会期间启动的“以太网用于横向扩展网络”（ESUN）倡议的创始参与者。\n\n*   **ESUN是什么？** ESUN是OCP网络项目中的一个新工作流，作为一个开放技术论坛，供行业运营商和领先供应商协作推进以太网技术的使用。其目标是利用和调整成熟的以太网生态系统，以满足现代AI系统中横向扩展领域独特的高性能需求。\n*   **关注点**：ESUN专注于横向扩展系统的网络功能方面，解决数据流量在网络交换机中管理和传输的技术挑战，包括定义协议头、错误处理机制和实现网络无损数据传输的最佳实践和标准。\n*   **合作与标准**：该倡议汇集了运营商、供应商和标准机构，共同开发适用于横向扩展网络的以太网解决方案，专注于以太网帧和交换层，确保强大、无损和容错的多跳拓扑，并与UEC和IEEE等组织紧密合作，与开放标准保持一致。\n*   **Meta的贡献**：Meta作为ESUN的初始成员之一，与AMD、Arista、ARM、Broadcom、Cisco、HPE、Marvell、Microsoft、NVIDIA、OpenAI和Oracle等行业领导者共同推动该倡议。Meta的贡献包括在定义AI集群ESUN需求方面的技术领导、与供应商和标准机构的开放协作，以及分享在Meta数据中心部署先进以太网网络方面的最佳实践。\n\n## 行业邀请：加入开放未来\nMeta邀请工程师、开发人员和行业合作伙伴加入OCP社区，共同塑造AI的下一代网络硬件。通过协作和分享想法，可以加速开放、面向未来的AI基础设施的开发，从而造福整个行业并支持未来技术的需求。",
      "shortSummary": "Meta在OCP峰会2025上发布了AI训练集群网络硬件的重大更新，包括DSF/NSF架构、Minipack3N交换机及新光模块。同时，Meta启动了“以太网用于横向扩展网络（ESUN）”倡议，旨在推动AI数据中心基础设施的创新、效率和可扩展性。",
      "translated_title": "OCP 峰会 2025：AI 网络硬件的开放未来",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>At Open Compute Project Summit (OCP) 2025, we’re sharing details about the direction of next-generation network fabrics for our AI training clusters. We’ve expanded our network hardware portfolio and are contributing new disaggregated network platforms to OCP. We look forward to continued collaboration with OCP to open designs for racks, servers, storage boxes, and motherboards [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/13/data-infrastructure/ocp-summit-2025-the-open-future-of-networking-hardware-for-ai/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/13/data-infrastructure/ocp-summit-2025-the-open-future-of-networking-hardware-for-ai/\">OCP Summit 2025: The Open Future of Networking Hardware for AI</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "介绍 React 基金会：React 和 React Native 的新家 (原标题: Introducing the React Foundation: The New Home for React & React Native)",
      "link": "https://engineering.fb.com/2025/10/07/open-source/introducing-the-react-foundation-the-new-home-for-react-react-native/",
      "pubDate": "Tue, 07 Oct 2025 18:00:01 +0000",
      "isoDate": "2025-10-07T18:00:01.000Z",
      "creator": "",
      "summary": "# 介绍 React 基金会：React 和 React Native 的新家\n\nMeta 宣布将 React、React Native 以及 JSX 等支持项目移交给新成立的 React 基金会，标志着 React 生态系统发展的一个重要里程碑。\n\n## React 的发展历程与影响力\n*   **起源与增长**：Meta 在十多年前开源了 React，旨在帮助开发者构建更好的用户体验。如今，React 已成为全球最受欢迎的开源项目之一，为超过 5000 万个网站和产品提供支持，这些产品来自微软、Shopify、彭博社、Discord、Coinbase、NFL 等众多公司。\n*   **平台扩展**：通过 React Native，React 的应用范围已超越了网页，扩展到移动设备、平板电脑、桌面应用、电视、游戏机乃至混合现实设备。\n*   **社区贡献**：React 的巨大成功离不开数千名教育工作者、公司和项目对其开发的贡献。社区是 React 的核心，推动了开源创新。\n\n## React 基金会的成立与使命\n*   **新家**：React 及其生态系统中的多个项目（包括 React、React Native 和 JSX）将过渡到 React 基金会。\n*   **使命**：React 基金会的任务是支持 React 社区及其成员，具体包括：\n    *   维护 React 的基础设施。\n    *   组织 React Conf 大会。\n    *   发起支持 React 生态系统的各项倡议。\n*   **隶属关系**：React 基金会将成为 Linux 基金会的一部分。Linux 基金会长期以来为开源项目提供了一个供应商中立的环境。\n\n## 治理结构\n*   **理事会**：React 基金会的理事会将由来自亚马逊、Callstack、Expo、Meta、微软、Software Mansion 和 Vercel 的代表组成，并计划未来进一步扩大。\n*   **业务与技术分离**：React 的业务治理和技术治理之间将有明确的分离。\n*   **技术治理**：发布、功能和技术方向将由一个新的结构来管理，该结构由 React 的维护者和贡献者驱动，并且独立于 React 基金会。React 团队正在积极制定这一新的技术治理结构，并将在未来的 React 博客文章中分享更多细节。\n\n## Meta 的承诺与未来展望\n*   **五年合作**：Meta 承诺与 React 基金会建立为期五年的合作伙伴关系。\n*   **资金与支持**：Meta 将提供超过 300 万美元的资金和专门的工程支持，以确保 React 平稳过渡到独立治理，同时保持社区所期望的稳定性和创新性。\n*   **持续投入**：Meta 将继续投资 React，并将其作为在网页和 Meta 众多应用中构建 UI 的主要工具。Meta 也将继续拥有一支全职致力于 React 和 React Native 的工程师团队。\n*   **未来机遇**：React 基金会的成立将为协作、创新和增长开启新的机遇，造福整个生态系统。通过强化的治理、更广泛的行业参与和持续的技术卓越，React 有望应对 UI 开发领域的下一代挑战。",
      "shortSummary": "Meta 宣布将 React、React Native 及相关项目移交给新成立的 React 基金会。该基金会隶属于 Linux 基金会，旨在维护 React 生态系统，并由亚马逊、Meta、微软等公司代表组成的理事会管理。Meta 承诺五年内提供超 300 万美元资金和工程支持，确保 React 的独立治理和持续发展。此举旨在促进更广泛的行业参与、协作与创新，应对 UI 开发的未来挑战。",
      "translated_title": "介绍 React 基金会：React 和 React Native 的新家",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>Meta open-sourced React over a decade ago to help developers build better user experiences. Since then, React has grown into one of the world’s most popular open source projects, powering over 50 million websites and products built by companies such as Microsoft, Shopify, Bloomberg, Discord, Coinbase, the NFL, and many others. With React Native, React [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/07/open-source/introducing-the-react-foundation-the-new-home-for-react-react-native/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/07/open-source/introducing-the-react-foundation-the-new-home-for-react-react-native/\">Introducing the React Foundation: The New Home for React &#038; React Native</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "介绍 OpenZL：一个开源的格式感知压缩框架 (原标题: Introducing OpenZL: An Open Source Format-Aware Compression Framework)",
      "link": "https://engineering.fb.com/2025/10/06/developer-tools/openzl-open-source-format-aware-compression-framework/",
      "pubDate": "Mon, 06 Oct 2025 16:00:53 +0000",
      "isoDate": "2025-10-06T16:00:53.000Z",
      "creator": "",
      "summary": "# 介绍 OpenZL：一个开源的格式感知压缩框架\n\nOpenZL 是 Meta 推出的一款新型开源数据压缩框架，专为结构化数据提供无损压缩。它旨在结合格式特定压缩器的卓越性能与单一可执行二进制文件的易于维护性。\n\n## OpenZL 的核心理念与背景\n\n*   **挑战与机遇：** 尽管 Zstandard 等通用压缩工具在数据中心工作负载中表现出色，但它们在处理结构化数据时，由于无法充分利用数据内部的特定模式，仍有大量压缩潜力未被挖掘。\n*   **定制化压缩器的困境：** 格式特定的压缩器能显著提升压缩比和速度，但为每种数据格式开发、部署和维护独立的压缩器和解压缩器会带来巨大的运维负担。\n*   **OpenZL 的解决方案：** OpenZL 应运而生，旨在解决这一矛盾，它通过一种创新的方法，在实现定制化性能的同时，保持了单一二进制文件的简洁性。\n\n## OpenZL 的工作原理\n\nOpenZL 的核心在于将数据结构作为显式输入参数，而非让压缩器猜测。\n\n1.  **显式结构输入：** 用户通过预设或简洁的格式描述（如 Simple Data Description Language, SDDL）向 OpenZL 提供数据形状。SDDL 仅用于解析，描述字节如何映射到字段（行、列、枚举、嵌套记录）。用户也可以编写自定义解析函数。\n2.  **离线训练器：** 一个离线优化组件（训练器）根据数据形状和样本，运行预算搜索，探索不同的转换选择和参数，以构建一个高效的压缩配置（“计划”）。它能提供一系列速度/压缩比的权衡，或直接针对特定速度约束下的最佳配置。\n3.  **编码时解析：** 在编码过程中，编码器将“计划”转换为具体的“解析图”（decode recipe），并将其嵌入到压缩帧中。如果“计划”包含控制点，编码器会根据数据特性选择最合适的路径并记录下来。\n4.  **通用解压缩器：** 所有的 OpenZL 文件都可以使用同一个通用解压缩器进行解压缩。解压缩器直接执行帧中嵌入的“解析图”，无需任何带外信息。\n\n## 性能示例：与通用压缩器的对比\n\n以压缩 Silesia 压缩语料库中的 `sao` 文件为例，该文件包含描述恒星的记录数组。通过向 OpenZL 提供其结构信息，OpenZL 展现出优于通用无损压缩器的性能：\n\n| 压缩器    | 压缩大小     | 压缩比 | 压缩速度 | 解压缩速度 |\n| :-------- | :----------- | :----- | :------- | :--------- |\n| zstd -3   | 5,531,935 B  | x1.31  | 220 MB/s | 850 MB/s   |\n| xz -9     | 4,414,351 B  | x1.64  | 3.5 MB/s | 45 MB/s    |\n| **OpenZL** | **3,516,649 B** | **x2.06** | **340 MB/s** | **1200 MB/s** |\n\nOpenZL 在实现更高压缩比的同时，保持甚至提升了压缩和解压缩速度，这对于数据中心处理管道至关重要。\n\n## 示例解析：OpenZL 如何利用结构\n\n在 `sao` 文件的压缩中，OpenZL 的处理流程包括：\n\n1.  **结构分解：** 将文件头与主体（大型结构表）分离。\n2.  **字段提取：** 将结构数组转换为数组结构，把每个字段提取到独立的流中。\n3.  **同质流处理：** 每个流包含同质数据，OpenZL 为其寻找最佳压缩策略：\n    *   **SRA0 (X轴位置)：** 索引大多有序，使用差分编码（delta）减小值范围。\n    *   **SDEC0 (Y轴位置)：** 虽不如X轴有序，但有明确的上下限，利用转置操作（transpose）使高位字节更可预测。\n    *   **其他字段 (IS, MAG, XRPM, XDPM)：** 基数远低于数量，且连续值间无关联，适合使用 `tokenize` 转换为字典和索引列表，再分别进行压缩。\n4.  **自动化与精细化：** 离线训练器在此阶段发挥作用，为每个流生成并优化特定的压缩策略。\n\n## 适应数据演变：再训练与运行时控制\n\n在实际应用中，数据结构和内容不断变化。OpenZL 通过其灵活的压缩计划来应对：\n\n*   **持续训练：** OpenZL 提供训练过程，根据提供的数据样本更新压缩计划，以维持或提升压缩性能。Meta 内部的“托管压缩”系统利用此功能，定期对注册用例进行监控、采样和再训练，并部署新的配置。\n*   **运行时自适应（控制点）：** 压缩配置可以包含控制点，在压缩时读取轻量级统计信息（如字符串重复、游程长度、直方图偏斜、差分方差），并选择“计划”中最佳的分支。这使得 OpenZL 能够动态适应数据变化、处理突发情况和异常值，而无需进行无限制的搜索，同时不增加解压缩器的复杂性。\n\n## 通用解压缩器的优势\n\nOpenZL 最大的运营优势在于其单一的通用解压缩器二进制文件，即使压缩配置改变，解压缩器也无需更新。\n\n*   **单一审计面：** 安全和正确性审查集中在一个二进制文件上，简化了审计流程。\n*   **全舰队范围的改进：** 解压缩器（如安全更新、性能优化）的任何更新都能惠及所有已压缩文件，包括旧数据。\n*   **操作清晰性：** 跨数据集使用相同的二进制文件、命令行接口、指标和仪表板，简化了修补和部署。\n*   **持续改进：** 可以在系统运行时不断训练和部署新的压缩计划，旧数据仍可解码，新数据则获得更好的压缩效果，实现了领域特定压缩而不碎片化生态系统。\n\n## OpenZL 的适用场景与局限性\n\n*   **适用场景：** OpenZL 特别适用于压缩向量、表格或树形结构数据，以及数值、字符串或二进制数据。常见示例包括时间序列数据集、机器学习张量和数据库表。前提是输入数据具有可被揭示的内部秩序。\n*   **局限性：** 当数据不包含可利用的结构时（例如纯文本文件），OpenZL 的优势不复存在，此时它会回退到 Zstandard，提供大致相同的性能水平。\n\n## 如何开始使用 OpenZL\n\n用户可以通过访问 OpenZL 网站和快速入门指南开始使用。源代码、文档和示例可在 GitHub 仓库中找到。OpenZL 欢迎社区的贡献和反馈。未来，OpenZL 将继续致力于简化结构暴露，并利用自动化压缩计划来处理不断演变的数据。",
      "shortSummary": "OpenZL 是一个开源的格式感知数据压缩框架，为结构化数据提供无损压缩。它通过显式利用数据结构，实现格式特定压缩器的性能，同时保持单一通用解压缩器的易维护性。OpenZL 包含离线训练器自动生成优化压缩计划，并支持运行时自适应。与通用压缩器相比，OpenZL 能在更高压缩比的同时保持或提升速度，特别适用于向量、表格和树形结构数据。其通用解压缩器简化了部署和维护，并支持持续改进。",
      "translated_title": "介绍 OpenZL：一个开源的格式感知压缩框架",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>OpenZL is a new open source data compression framework that offers lossless compression for structured data. OpenZL is designed to offer the performance of a format-specific compressor with the easy maintenance of a single executable binary. You can get started with OpenZL today by visiting our Quick Start guide and the OpenZL GitHub repository. Learn more [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/06/developer-tools/openzl-open-source-format-aware-compression-framework/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/06/developer-tools/openzl-open-source-format-aware-compression-framework/\">Introducing OpenZL: An Open Source Format-Aware Compression Framework</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "推出Candle海底电缆，更新我们在亚太地区的连接项目 (原标题: Introducing the Candle Subsea Cable, Updates to Our Asia-Pacific Connectivity Projects)",
      "link": "https://engineering.fb.com/2025/10/05/connectivity/introducing-the-candle-subsea-cable-updates-to-our-asia-pacific-connectivity-projects/",
      "pubDate": "Mon, 06 Oct 2025 02:30:07 +0000",
      "isoDate": "2025-10-06T02:30:07.000Z",
      "creator": "",
      "summary": "## Meta推出Candle海底电缆并更新亚太连接项目\n\nMeta近日宣布推出新的Candle海底电缆系统，并提供了其在亚太地区其他连接项目的最新进展，其中包括Bifrost电缆系统的竣工。此举旨在加强全球和区域连接，特别是在亚太地区，以支持Meta的服务、人工智能（AI）和新兴技术。\n\n### 亚太地区连接的重要性\n亚太地区拥有全球超过58%的互联网用户，他们高度依赖强大的全球基础设施来获取在线连接和创新技术（如AI）。Meta致力于构建世界级的网络基础设施，以提供足够的容量和弹性，支持全球用户的丰富在线体验，并实现AI和个人超级智能的未来愿景。\n\n### Candle海底电缆：亚太最大容量系统\n*   **名称：** Candle\n*   **连接范围：** 将连接东亚和东南亚国家，包括日本、台湾、菲律宾、印度尼西亚、马来西亚和新加坡。\n*   **上线时间：** 预计于2028年投入使用。\n*   **容量：** 提供570 Tbps的容量，使其成为亚太地区容量最大的电缆系统。\n*   **长度：** 跨越8,000公里。\n*   **覆盖人数：** 将连接超过5.8亿人。\n*   **技术：** 采用最新开发的24对光纤电缆技术，提供与Meta现有最大容量电缆Anjana相似的带宽。\n*   **合作：** Meta正与该地区领先的电信公司合作建设此系统。\n\n### 其他亚太地区海底电缆更新\nMeta还公布了其在亚太地区其他几个海底电缆项目的进展：\n\n*   **Bifrost电缆：**\n    *   该系统已完成建设。\n    *   目前连接新加坡、印度尼西亚、菲律宾和美国，预计2026年将延伸至墨西哥。\n    *   Bifrost将提供260 Tbps的冗余容量，为热门的跨太平洋数字路由开辟新路径。\n    *   与Echo电缆一起，Bifrost是Meta及其合作伙伴在2021年承诺将跨太平洋容量增加70%的关键组成部分。\n\n*   **Echo电缆：**\n    *   目前在关岛和加利福尼亚之间提供260 Tbps的容量。\n    *   未来可选择延伸至亚洲。\n\n*   **Apricot电缆：**\n    *   已在日本、台湾和关岛之间投入使用。\n    *   未来将延伸至菲律宾、印度尼西亚和新加坡。\n    *   长度：12,000公里。\n    *   容量：290 Tbps。\n    *   该系统将补充Bifrost和Echo系统，进一步增强区域连接。\n\n### 综合影响与未来展望\nCandle、Echo、Bifrost和Apricot系统将共同增强亚太地区的区域内连接，并建立通往美洲的跨太平洋桥梁。此外，Meta还投资了其他全球项目，如2Africa（通往印度、中东和欧洲）和Project Waterworth（预计在本十年末连接五大洲，包括亚洲），以实现更广泛的全球连接。\n\n这些数字基础设施的开发是Meta致力于将世界各地的人们连接起来的承诺的一部分。通过与合作伙伴的共同努力，这些投资将提升全球电信网络的规模和可靠性，确保Meta的服务能够快速高效地送达亚太地区及其他地区的商业和个人用户。\n\n**数据来源：** 根据Statista数据，截至2025年9月5日，全球56.5亿互联网用户中，亚太地区约占33亿。",
      "shortSummary": "Meta宣布推出Candle海底电缆，这是亚太地区容量最大的新系统，将于2028年连接日本、台湾、菲律宾、印尼、马来西亚和新加坡，提供570 Tbps容量。同时，Meta更新了其他亚太连接项目进展，包括已完成的Bifrost电缆（连接新加坡、印尼、菲律宾、美国）、Echo电缆（连接关岛和加州）以及已启用的Apricot电缆（连接日本、台湾、关岛）。这些投资旨在增强亚太地区及全球的连接性，支持Meta的服务、AI和新兴技术，以满足该地区庞大的互联网用户需求。",
      "translated_title": "推出Candle海底电缆，更新我们在亚太地区的连接项目",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>We’re introducing Candle, a new submarine cable connecting countries across East Asia and Southeast Asia. We’re also announcing several updates to our subsea cables across the Asia-Pacific, including the completion of the Bifrost cable system. The Asia-Pacific (APAC) region is home to over 58% of the world’s internet users1 – many who rely on robust [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/05/connectivity/introducing-the-candle-subsea-cable-updates-to-our-asia-pacific-connectivity-projects/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/05/connectivity/introducing-the-candle-subsea-cable-updates-to-our-asia-pacific-connectivity-projects/\">Introducing the Candle Subsea Cable, Updates to Our Asia-Pacific Connectivity Projects</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "使用基线配置文件加速我们的安卓应用 (原标题: Accelerating our Android apps with Baseline Profiles)",
      "link": "https://engineering.fb.com/2025/10/01/android/accelerating-our-android-apps-with-baseline-profiles/",
      "pubDate": "Wed, 01 Oct 2025 16:00:17 +0000",
      "isoDate": "2025-10-01T16:00:17.000Z",
      "creator": "",
      "summary": "# 使用基线配置文件加速Meta的安卓应用\n\nMeta公司致力于提升其数十亿安卓用户的应用体验。本文探讨了Meta如何利用Android的基线配置文件（Baseline Profiles）显著改善其安卓应用的性能，某些关键指标提升高达40%。\n\n## 应用性能的重要性与挑战\n\n良好的应用性能对用户体验至关重要。启动缓慢、掉帧和响应迟钝都会导致用户沮丧并最终流失。除了在开发过程中注重性能（如使用合适的数据结构、算法和缓存策略），理解编译代码的底层表示及其加载和执行方式也同样重要，以便优化构建工具和运行时。\n\nMeta在过去几年中开发了针对安卓应用的配置文件引导编译器和运行时优化基础设施，其中Android Runtime (ART) 的基线配置文件是核心组件。\n\n### ART性能考量\n\n*   **代码编译与执行：** Android应用主要使用Kotlin和Java，它们被编译成Dalvik字节码（Dex代码）。ART必须将Dex代码翻译成机器码才能执行。\n*   **默认执行流程：** 运行时，Dex方法会同时通过解释器执行并进行分析，以确定是否为“热点”方法。一旦确定为热点，ART的即时编译器（JIT）会将其编译成机器码，后续执行将使用编译后的版本（机器码通常比解释执行快得多）。\n*   **性能开销：** 类加载和Dex方法解释/分析阶段都会产生运行时开销，可能导致用户可感知的性能下降。\n*   **冷启动问题：** 应用首次启动（冷启动）时，类需要重新加载。每次应用版本更新后，即使编译过的方法也需要重新分析和编译。\n\n### Meta移动应用面临的挑战\n\n*   **用户访问主要途径：** Meta的大多数用户通过移动应用访问，其中大部分是安卓用户。\n*   **平衡开发速度与性能：** 在快速迭代新功能（如Instagram Reels、Messenger端到端加密）的同时，保持高性能是一个挑战。\n*   **启动性能：** 启动性能尤为关键，对用户体验影响巨大。Facebook和Instagram在启动时会加载超过20,000个类，滚动信息流时还会加载数千个。\n*   **用户旅程优化：** 除了启动，Meta还关注启动后的用户旅程性能，例如信息流滚动、照片加载和渲染时间。这些旅程根据用户行为和发生场景进行细分（例如，滚动信息流与滚动收件箱是不同的）。\n*   **代码库的动态性：** Meta的单体仓库每天有数千次提交。用户数据收集到的类加载配置文件差异巨大，即使是同一用户在不同时间、不同实验组下也可能不同。\n*   **解决方案需求：** 需要一个能够智能适应频繁代码变更、快速生成编译代码和配置文件，并能同时优化启动和其他用户旅程的解决方案。\n\n## ART安装时优化\n\n自Android 9以来，ART提供了以下安装时优化：\n\n1.  **AOT（Ahead of Time）编译：** 在应用首次运行前，将指定方法编译成机器码，消除初始执行时的解释和分析开销。\n2.  **应用镜像（App Image）创建：** 包含指定类的内存中ART数据结构的局部表示。应用启动时，应用镜像被映射到进程堆中，从而实现极快的类加载，并消除后续运行时加载这些类的成本。\n\n这些优化通过在应用安装时向ART提供特殊配置文件来触发，主要机制有两种：云配置文件（Cloud Profiles）和基线配置文件（Baseline Profiles）。\n\n### 云配置文件（Cloud Profiles）\n\n*   **生成方式：** 由Google Play在应用版本首次发布期间收集和聚合大量用户的分析数据。\n*   **应用方式：** 一旦云配置文件生成，所有后续通过Google Play安装该应用版本的用户都会收到该配置文件，ART会用它进行AOT编译和应用镜像创建。\n*   **缺点：**\n    *   早期用户无法受益，因为他们是数据提供者。\n    *   应用开发者无法观察或控制配置文件中的类和方法。\n    *   配置文件强烈倾向于早期启动性能改进。\n    *   仅通过Google Play提供，其他安装方式（如其他应用商店或侧载）无法使用。\n\n### 基线配置文件（Baseline Profiles）\n\n*   **核心特点：** 与云配置文件类似，也触发ART安装时优化，但由应用开发者生成和提供，并可打包在APK或AAB中。\n*   **与云配置文件的协同：** 当两者都可用时，可以协同使用。\n*   **优势：**\n    *   **开发者完全控制：** 开发者可以完全控制安装时优化，使其更符合应用的特定需求，包括优化启动之外的场景。\n    *   **即时可用：** 用户可以立即受益。\n    *   **生成方式灵活：** 可以通过基准测试（如Google的Macrobenchmark）生成，也可以通过`profgen`工具直接指定类和方法。\n\n## Meta如何创建基线配置文件\n\nMeta利用基线配置文件解决了其安卓应用性能面临的诸多挑战，尤其是在冷启动和每周更新导致编译代码被清除的问题上。\n\n### 数据收集与处理\n\nMeta最初使用AndroidX库附带的静态配置文件。如今，Meta采用了一套复杂的收集技术来生成应用配置文件：\n\n*   **基准测试：** 对部分应用使用本地基准测试，通过内部工具收集类和方法使用信息。\n*   **用户数据（针对复杂应用）：** 对于Facebook和Instagram等复杂应用，Meta还会从用户那里收集类和方法使用数据，以获得更全面的视图。\n    *   **类使用数据：** 使用自定义ClassLoader，插入代码记录加载的类，并以极低的采样率有条件地启用上传。收集到的类加载日志经过聚合，根据出现频率，超过特定阈值的类会被包含在下一版本的基线配置文件中。\n    *   **方法使用数据：** 通过应用内专门的遥测技术，识别用户通常调用的方法集群，并以类似方式进行采样和聚合。\n*   **配置文件生成：** 所有收集到的数据被组合成一个“人类可读配置文件”（Human Readable Profile），然后输入给`profgen`工具，生成最终的基线配置文件。\n\n### 人类可读配置文件示例（格式描述）\n\n人类可读配置文件采用特定格式，例如：\n*   `#` 用于注释行。\n*   类可以直接通过其描述符指定。\n*   方法可以直接指定，并可带可选标志。\n*   可以使用通配符匹配所有符合给定前缀的类或方法。\n\n### 调优与实验\n\n*   **初始策略：** Meta最初将冷启动作为主要优化目标，保守地设置了较高的类和方法包含频率阈值（要求在80%到90%的用户跟踪中出现）。\n*   **担忧：** 过大的基线配置文件可能导致性能下降，因为编译后的机器码通常比原始解释代码大10倍，会增加I/O成本。\n*   **持续优化：** 随着时间的推移，Meta尝试了不同的包含阈值，并将优化范围从冷启动扩展到其他用户交互。目前，对于大多数应用，只要在20%或更多的冷启动用户跟踪中出现的类和方法，都会被包含在内。\n*   **优化场景：** 已通过基线配置文件优化的交互包括Facebook和Instagram的信息流滚动，以及Messenger和Instagram私信中从聊天列表到聊天视图的导航。",
      "shortSummary": "Meta利用Android的基线配置文件（Baseline Profiles）显著提升了其安卓应用的性能，某些关键指标提升高达40%。面对启动慢、帧率下降等挑战，Meta发现传统的云配置文件存在局限。基线配置文件赋予开发者完全控制权，可根据用户数据（包括基准测试和真实用户行为）定制优化，预编译热点代码和类，从而加速应用启动和关键用户旅程。通过持续的实验和调整，Meta已将优化范围从冷启动扩展到新闻推送滚动等多种用户交互。",
      "translated_title": "使用基线配置文件加速我们的安卓应用",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>Key Takeways: With billions of Android app users, we’re always looking to improve the Meta app experience, and in this post, we explore the ways we’ve leveraged Android’s Baseline Profiles to significantly improve their performance. We discuss the performance challenges we’ve faced as Meta’s apps, how the needs of users have become more complex over [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/01/android/accelerating-our-android-apps-with-baseline-profiles/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/01/android/accelerating-our-android-apps-with-baseline-profiles/\">Accelerating our Android apps with Baseline Profiles</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    }
  ],
  "lastUpdated": "2025-10-19T05:25:01.540Z"
}