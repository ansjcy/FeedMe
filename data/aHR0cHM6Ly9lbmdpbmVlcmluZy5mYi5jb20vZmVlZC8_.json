{
  "sourceUrl": "https://engineering.fb.com/feed/",
  "title": "Engineering at Meta",
  "description": "Engineering at Meta Blog",
  "link": "https://engineering.fb.com/",
  "items": [
    {
      "title": "开源对环境有益 (原标题: Open Source Is Good for the Environment)",
      "link": "https://engineering.fb.com/2025/11/14/production-engineering/open-source-is-good-for-the-environment/",
      "pubDate": "Fri, 14 Nov 2025 20:54:14 +0000",
      "isoDate": "2025-11-14T20:54:14.000Z",
      "creator": "",
      "summary": "# 开源对环境的积极影响\n\nMeta Tech Podcast 的最新一集深入探讨了开源软件和开源硬件如何对环境产生积极影响。节目中，Pascal Hartig 与 Dharmesh 和 Lisa 进行了对话，讨论了开源硬件的方方面面，以及 Meta 在 2025 年开放计算项目 (OCP) 峰会上的主要发布。\n\n## 播客核心内容\n\n*   **主题聚焦**：节目重点介绍了开源硬件及其对环境的积极作用。\n*   **Meta 的重要发布**：在 OCP 峰会上，Meta 宣布了一项新的开放方法，该方法利用人工智能 (AI) 来理解和量化范围 3 排放（Scope 3 emissions）。\n*   **OCP 的发展**：播客回顾了开放计算项目 (OCP) 的历史，并强调了其显著增长，目前已有超过 400 家公司为其贡献力量。\n*   **Meta 的环保目标**：文章指出，AI 和开源硬件是 Meta 实现其在 2030 年达到净零排放目标的关键推动力。其中一个具体应用是利用 AI 技术开发用于数据中心建设的新型混凝土混合物，以减少环境影响。\n\n## 播客收听与互动\n\n*   **收听平台**：听众可以在 Spotify、Apple Podcasts 和 Pocket Casts 等主流播客平台找到并收听此节目。\n*   **节目宗旨**：Meta Tech Podcast 由 Meta 制作，旨在展示 Meta 工程师在各个技术层面（从底层框架到最终用户功能）所进行的创新工作。\n*   **反馈与职业机会**：听众可以通过 Instagram、Threads 或 X 向节目发送反馈。对 Meta 职业机会感兴趣的人士可以访问 Meta 招聘页面了解更多信息。\n\n## 相关图片\n\n以下是文章中包含的图片：\n\n*   ![图片 1](https://engineering.fb.com/wp-content/uploads/2025/11/MTP80_FBLI_1200_630-2-up-new.png)\n*   ![图片 2](https://engineering.fb.com/wp-content/uploads/2025/10/@Scale-Networking-Ron-Ankur-Scaling-AI-YT.png?w=580&h=326&crop=1)\n*   ![图片 3](https://engineering.fb.com/wp-content/uploads/2025/10/@Scale-Networking-Mark-Alberto-10x-Backbone-YT.png?w=580&h=326&crop=1)\n*   ![图片 4](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-data-center-interior-Hero-2.jpg?w=580&h=326&crop=1)\n*   ![图片 5](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-data-center-interior-Hero-1.jpg?w=580&h=326&crop=1)\n*   ![图片 6](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-OCP-2025-NSF-1.png?w=580&h=326&crop=1)\n*   ![图片 7](https://engineering.fb.com/wp-content/uploads/2025/09/CL2_7019-copy.png?w=580&h=326&crop=1)",
      "shortSummary": "Meta Tech Podcast 探讨了开源软件和硬件如何对环境产生积极影响。节目介绍了 Meta 在 2025 年 OCP 峰会上的重要发布，包括利用 AI 理解范围 3 排放的新开放方法。文章强调了 AI 和开源硬件如何帮助 Meta 在 2030 年实现净零排放，例如利用 AI 开发数据中心建设所需的新型混凝土。开放计算项目 (OCP) 现已发展到有 400 多家公司贡献。",
      "translated_title": "开源对环境有益",
      "images": [
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/11/MTP80_FBLI_1200_630-2-up-new.png",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/@Scale-Networking-Ron-Ankur-Scaling-AI-YT.png?w=580&h=326&crop=1",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/@Scale-Networking-Mark-Alberto-10x-Backbone-YT.png?w=580&h=326&crop=1",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-data-center-interior-Hero-2.jpg?w=580&h=326&crop=1",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-data-center-interior-Hero-1.jpg?w=580&h=326&crop=1",
          "alt": "",
          "title": "",
          "position": 5
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-OCP-2025-NSF-1.png?w=580&h=326&crop=1",
          "alt": "",
          "title": "",
          "position": 6
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/09/CL2_7019-copy.png?w=580&h=326&crop=1",
          "alt": "",
          "title": "",
          "position": 7
        }
      ],
      "contentSource": "完整文章",
      "content": "<p>Most people have heard of open-source software. But have you heard about open hardware? And did you know open source can have a positive impact on the environment? On this episode of the Meta Tech Podcast, Pascal Hartig sits down with Dharmesh and Lisa to talk about all things open hardware, and Meta’s biggest announcements [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/11/14/production-engineering/open-source-is-good-for-the-environment/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/11/14/production-engineering/open-source-is-good-for-the-environment/\">Open Source Is Good for the Environment</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "StyleX：大规模CSS样式库 (原标题: StyleX: A Styling Library for CSS at Scale)",
      "link": "https://engineering.fb.com/2025/11/11/web/stylex-a-styling-library-for-css-at-scale/",
      "pubDate": "Tue, 11 Nov 2025 17:58:20 +0000",
      "isoDate": "2025-11-11T17:58:20.000Z",
      "creator": "",
      "summary": "# StyleX：大规模CSS样式库\n\nStyleX 是 Meta 针对大规模应用开发的样式系统，旨在结合 CSS-in-JS 的人体工程学优势与静态 CSS 的高性能。它通过在构建时生成无冲突的原子 CSS，同时支持富有表现力且类型安全的样式编写。\n\n## 核心概述与发展\n\n*   **发布与采纳**：StyleX 于 2023 年底开源，现已成为 Meta 旗下产品（如 Facebook、Instagram、WhatsApp、Messenger 和 Threads）以及外部公司（如 Figma 和 Snowflake）的标准样式系统。\n*   **核心机制**：StyleX 本质上是一个编译器，在构建时提取样式并生成静态样式表。它也是一种理念，为大规模 CSS 的编写、共享和维护提供了一个框架。\n*   **解决的问题**：\n    *   Meta 早期在处理大规模 CSS 时面临诸多挑战，包括跨捆绑包的样式冲突、样式表依赖管理困难、特异性冲突（specificity wars）以及大型 CSS 捆绑包导致的性能问题（浏览器下载大量未使用的规则）。\n    *   早期的解决方案 `cx`（类似 CSS Modules）解决了命名空间冲突和依赖管理，但仍限于静态样式文件。\n    *   CSS-in-JS 运动兴起，开发者希望将样式与组件代码并置，编写基于运行时状态的动态样式。然而，早期的 CSS-in-JS 系统依赖运行时注入，引入了性能开销。\n*   **StyleX 的创新**：StyleX 吸取了 CSS-in-JS 的经验，但其样式会编译成静态 CSS，避免了运行时性能问题。它取代了 `cx` 系统，实现了样式在 JavaScript 中定义，支持组合、条件逻辑和构建时编译。这使得 CSS 大小减少了 80%，并显著提高了代码库的可维护性。\n\n## 编译器深入解析\n\nStyleX 的强大之处在于其抽象能力。它自动处理 CSS 特异性、变量生成和静态编译，从而生成可预测、无冲突的原子 CSS，让开发者专注于样式编写。\n\n*   **架构**：StyleX 是一个由多个集成包组成的 Monorepo。核心引擎是一个 Babel 插件，负责在项目上运行转换，提取 CSS。\n*   **编译流程**：\n    1.  编译器遍历文件，从样式对象中提取 CSS 元数据。\n    2.  将样式声明转换为原子 CSS 类。\n    3.  收集到的元数据经过值规范化、`@rules` 包装和旧版 polyfill 处理。\n    4.  最后，CSS 规则被排序并输出到静态样式表。\n\n## StyleX 的核心价值\n\n### 1. 可扩展性 (Scalability)\n\n*   **原子 CSS 编译**：StyleX 的核心是将其样式静态编译为原子 CSS。样式被转换为包含单个样式声明的类，以便在整个代码库中重用，从而使 CSS 大小随着应用程序的增长而趋于稳定。\n*   **构建时优化**：样式尽可能在构建时编译和缓存，系统可以分析所有可达样式，去重共享声明，并仅在运行时输出所需内容。\n*   **轻量级 API**：\n    *   `stylex.create()`：用于定义样式对象。这些对象在构建时被剥离，并转换为原子 CSS。每个 `property: value` 对都被哈希并输出为 CSS 类。此 API 专为可缓存性设计，只允许静态可解析的值。\n    *   `stylex.props()`：处理样式对象的合并和去重。每次调用都会被转译为一个对象，其中包含一个对应于每个原子样式的空格分隔的 `className` 字符串，以及一个用于动态样式的 `style` 属性。\n*   **编译示例**：\n    ```javascript\n    // 原始组件\n    import * as stylex from '@stylexjs/stylex';\n    const styles = stylex.create({\n      foo: { margin: 10 },\n      bar: { margin: 10, color: 'red' }\n    });\n    function MyComponent({style}) {\n      return (\n        <>\n          <div {...styles.props(styles.foo)}/>\n          <div {...styles.props(styles.bar)}/>\n          <div {...stylex.props(style)}/>\n        </>\n      )\n    }\n\n    // 编译后（简化）\n    import * as stylex from '@stylexjs/stylex';\n    function MyComponent({style}) {\n      return (\n        <>\n          <div className=\"m-10\" />\n          <div className=\"c-red m-10\" />\n          <div {...stylex.props(style)}/>\n        </>\n      )\n    }\n    // 对应的静态 CSS\n    .m-10 { margin: 10px }\n    .c-red { color: red }\n    ```\n*   **后处理**：转换完成后，系统会处理收集到的元数据，生成 LTR/RTL 变体，解析常量，并按优先级排序 CSS 规则，最终输出为静态样式表。\n\n### 2. 表现力 (Expressiveness)\n\nStyleX 将约束视为设计原则而非限制，禁止容易冲突的模式（如远距离样式），并强制执行静态可解析的模式。\n\n*   **可共享的值**：\n    *   `stylex.create()` 专为文件级可缓存性设计。\n    *   `stylex.defineVars()` 和 `stylex.defineConsts()`：提供 API 以在文件间重用值。它们根据变量名和导入路径生成确定性哈希，确保跨模块一致性。构建时，共享常量会被完全内联，而共享变量则成为可在组件间引用的全局 CSS 自定义属性。\n*   **样式封装**：StyleX 是一个组件样式系统，通过直接在元素上应用局部类名来避免全局样式。它禁止全局和复杂选择器，以防止远距离样式（即间接影响 DOM 中其他元素的规则），并促进样式封装。全局基线规则（如元素选择器或 CSS 重置）必须在单独的样式表中定义。\n*   **远距离观察 (Observing from a Distance)**：通过 `stylex.when` API，StyleX 允许使用关系选择器根据祖先、后代或兄弟元素的状态来样式化组件。被观察的元素必须使用 `stylex.defaultMarker()` 标记，以确保样式直接应用同时支持上下文行为。\n*   **保留 CSS 特性**：StyleX 通过构建时的静态转换保留了大部分 CSS 特性（媒体查询、伪类、关键帧等），尽可能模拟原生 CSS 行为。\n*   **动态样式支持**：当值在构建时未知时，编译器会发出 CSS 变量引用，运行时通过 `style` 属性将其内联写入。\n*   **主题化 API**：\n    *   `stylex.defineVars()`：创建变量分组。\n    *   `stylex.createTheme()`：通过在更高特异性下重新定义变量组来创建主题变体。\n*   **动画支持**：`stylex.keyframes()` 和 `stylex.viewTransitionClass()` API 支持动画，生成 `@keyframes` 和 `::view-transition-*` 规则。\n\n### 3. 可预测性 (Predictability)\n\nStyleX 的设计围绕可预测的样式合并展开：“最后一个样式总是胜出！”\n\n*   **确定性合并**：`stylex.props` 函数被视为样式对象的确定性合并：`stylex.props(styles.foo, styles.bar)` 中，`bar` 总是覆盖 `foo`。这使得跨文件共享和组合样式变得可预测。\n*   **CSS 特异性处理**：由于 StyleX 完全基于类，解决样式对象之间的冲突意味着确定要应用哪些类名并强制执行它们之间的优先级。在合并过程中，样式对象中重复的属性会被去重，只应用最后一个值。\n*   **优先级计算**：StyleX 为每个 CSS 规则计算一个数值优先级。这些优先级与用户配置的 `styleResolution` 一起，通过 `@layer` at-rule 或等效的 polyfill 来确定每个类选择器的特异性。\n*   **结果**：长手属性和短手属性可预测地合并，`:active` 状态覆盖 `:hover` 状态，媒体查询覆盖默认行为，并在可能的情况下尊重用户编写的顺序。这种幕后的特异性处理允许开发者组合和重用样式，而无需手动解决冲突。\n\n## 未来展望\n\nStyleX 由一群 CSS 爱好者维护，旨在让样式对所有人可用。\n\n*   **Monorepo 工具**：除了编译器，Monorepo 还包括 ESLint 插件（用于样式验证）、CLI（用于轻松生成样式表）、PostCSS 插件（用于后处理）和一个实验性的 CSS 解析器。\n*   **社区贡献**：开源社区在 StyleX 的发展方向上发挥了关键作用，贡献了社区构建的 Playground、VS Code 扩展、SWC 编译器、多个捆绑器集成等。\n*   **路线图亮点**：包括可共享函数 API、LLM 就绪的上下文文件、内联样式支持、开发者扩展、严格的编译器验证、逻辑样式工具等。",
      "shortSummary": "StyleX 是 Meta 开源的大规模应用样式系统，结合了 CSS-in-JS 的人体工程学和静态 CSS 的高性能。它通过构建时编译生成无冲突的原子 CSS，支持类型安全和富有表现力的样式编写。StyleX 解决了传统 CSS 的可扩展性、维护性和性能问题，通过其编译器实现样式封装、可预测的合并和高效的 CSS 输出。它已成为 Meta 产品及多家外部公司的标准，并持续通过社区贡献和路线图规划进行发展。",
      "translated_title": "StyleX：大规模CSS样式库",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>StyleX is Meta’s styling system for large-scale applications. It combines the ergonomics of CSS-in-JS with the performance of static CSS, generating collision-free atomic CSS while allowing for expressive, type-safe style authoring. StyleX was open sourced at the end of 2023 and has since become the standard styling system across Meta products like Facebook, Instagram, WhatsApp, [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/11/11/web/stylex-a-styling-library-for-css-at-scale/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/11/11/web/stylex-a-styling-library-for-css-at-scale/\">StyleX: A Styling Library for CSS at Scale</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "Meta的生成式广告模型（GEM）：加速广告推荐AI创新的核心大脑 (原标题: Meta’s Generative Ads Model (GEM): The Central Brain Accelerating Ads Recommendation AI Innovation)",
      "link": "https://engineering.fb.com/2025/11/10/ml-applications/metas-generative-ads-model-gem-the-central-brain-accelerating-ads-recommendation-ai-innovation/",
      "pubDate": "Mon, 10 Nov 2025 17:00:01 +0000",
      "isoDate": "2025-11-10T17:00:01.000Z",
      "creator": "",
      "summary": "Meta的生成式广告模型（GEM）：加速广告推荐AI创新的核心大脑\n\nMeta推出了其生成式广告推荐模型（GEM），这是一个基于大型语言模型（LLM）启发的新型基础模型，旨在通过增强现有广告推荐模型的能力来提供更相关的广告，从而显著提升广告效果和广告商的投资回报率（ROI）。GEM的创新架构使其能够随着参数数量的增加而高效扩展，并持续生成更精确的预测。\n\n**GEM的核心目标与成果**\n\n*   **提升广告性能与ROI：** GEM通过优化广告推荐，已在Instagram和Facebook上显著提升了广告转化率。\n*   **行业领先：** 它是目前业界规模最大的推荐系统（RecSys）基础模型，训练规模与大型语言模型相当。\n*   **实际效果：** 自今年早些时候推出以来，GEM在第二季度为Instagram带来了5%的广告转化率增长，为Facebook Feed带来了3%的增长。第三季度，模型架构的改进使相同数据和计算量下的性能效益翻倍。\n\n**GEM的三大关键创新**\n\nGEM通过以下三个核心创新，在推荐系统领域取得了重大进展：\n\n1.  **采用先进架构的模型扩展**\n    *   **挑战与应对：** Meta的广告推荐系统面临处理庞大且动态的特征空间、多样化的数据类型（广告主目标、创意格式、用户行为等）以及高效训练大规模模型的挑战。GEM通过其可扩展的架构克服了这些问题。\n    *   **可扩展的模型架构：** GEM的架构在给定数据和计算量下，驱动广告性能提升的效率比Meta原始的广告推荐排序模型高出4倍。它通过定制的注意力机制和跨特征学习，处理广告内容和用户互动数据（包括序列特征和非序列特征）。\n    *   **非序列特征交互建模：** 增强了Wukong架构，利用带有跨层注意力连接的可堆叠分解机，以学习最重要的特征组合。\n    *   **离线序列特征建模：** 采用金字塔并行结构，高效处理长达数千个事件的用户行为序列，从而从更长的用户互动历史中学习。\n    *   **跨特征学习（InterFormer）：** 采用并行摘要和交错结构，在序列学习和跨特征交互层之间交替，在保留完整序列信息的同时实现高效的跨特征学习。\n    *   **多领域学习与领域特定优化：** GEM能够从跨平台用户互动中学习，同时确保预测针对Facebook、Instagram和商业消息等不同Meta平台的独特特性进行优化。\n\n2.  **优化知识转移的后训练技术**\n    *   **知识转移的重要性：** GEM的知识必须高效地转移到数百个面向用户的垂直模型（VMs）中，才能产生实际影响。GEM采用直接和分层知识转移策略，并结合多种技术实现这一目标。\n    *   **知识蒸馏：** 通过使用“学生适配器”组件，利用最新的真实数据来优化教师模型（GEM）的输出，解决VMs中因延迟和领域不匹配导致的监督信号过时问题，使学生模型获得更及时和相关的监督。\n    *   **表示学习：** 自动从原始数据中提取有意义且紧凑的特征，支持教师模型到学生模型的知识高效转移，且不增加推理开销。\n    *   **参数共享：** 允许多个模型或组件重用相同的参数集，减少冗余，提高效率，并使较小的、对延迟敏感的VMs能够利用基础模型（FMs）丰富的表示和预学习模式，而无需承担其全部计算成本。\n\n3.  **增强的训练基础设施以支持可扩展性**\n    *   **训练规模与效率提升：** GEM的训练规模与现代LLM相当，Meta为此彻底改造了训练方案。重新设计的训练堆栈使有效训练FLOPs增加了23倍，同时使用16倍的GPU，硬件效率（MFU）提高了1.43倍。\n    *   **分布式训练：** 针对模型的密集部分采用混合分片分布式并行（HSDP），优化内存使用和通信成本；针对稀疏组件（如大型嵌入表）采用二维数据并行和模型并行方法。\n    *   **GPU吞吐量的系统级优化：** 实施了一系列技术以饱和GPU计算吞吐量并减少训练瓶颈，包括定制的内部GPU内核、PyTorch 2.0的图级编译、内存压缩技术（如FP8量化）以及不占用SM资源的GPU通信集合（NCCLX）。\n    *   **减少训练开销和作业启动时间：** 通过优化训练器初始化、数据读取器设置、检查点和PyTorch 2.0编译时间等，将作业启动时间缩短了5倍（PyTorch 2.0编译时间通过缓存策略缩短了7倍），从而提高了训练敏捷性。\n    *   **最大化开发生命周期中的GPU效率：** 在模型生命周期的所有阶段（从早期实验到大规模训练和后训练）都优化了GPU效率，例如在探索阶段使用轻量级模型变体加速迭代，并进行持续的在线训练以刷新基础模型。\n\n**GEM的未来展望**\n\nGEM的推出标志着Meta在广告推荐AI领域迈出了重要一步，通过持续的架构改进和训练优化，Meta将能够以更具吸引力的投资回报率继续扩展GEM的训练能力，进一步提升广告效果和用户体验。",
      "shortSummary": "Meta推出了生成式广告模型（GEM），这是一个基于LLM启发的新型基础模型，旨在通过增强广告推荐能力，提升广告效果和广告商投资回报率。GEM采用创新架构，实现了高效扩展和精准预测，并在Instagram和Facebook上显著提升了广告转化率（Q2分别增长5%和3%）。它通过先进的架构、后训练知识转移技术和优化的训练基础设施，克服了处理大规模、多样化广告数据的挑战，推动了Meta广告推荐系统的范式转变。",
      "translated_title": "Meta的生成式广告模型（GEM）：加速广告推荐AI创新的核心大脑",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>We’re sharing details about Meta’s Generative Ads Recommendation Model (GEM), a new foundation model that delivers increased ad performance and advertiser ROI by enhancing other ads recommendation models’ ability to serve relevant ads. GEM’s novel architecture allows it to scale with an increasing number of parameters while consistently generating more precise predictions efficiently. GEM propagates [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/11/10/ml-applications/metas-generative-ads-model-gem-the-central-brain-accelerating-ads-recommendation-ai-innovation/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/11/10/ml-applications/metas-generative-ads-model-gem-the-central-brain-accelerating-ads-recommendation-ai-innovation/\">Meta’s Generative Ads Model (GEM): The Central Brain Accelerating Ads Recommendation AI Innovation</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "视频隐形水印的规模化应用 (原标题: Video Invisible Watermarking at Scale)",
      "link": "https://engineering.fb.com/2025/11/04/video-engineering/video-invisible-watermarking-at-scale/",
      "pubDate": "Tue, 04 Nov 2025 18:00:52 +0000",
      "isoDate": "2025-11-04T18:00:52.000Z",
      "creator": "",
      "summary": "Meta公司在其平台上广泛应用隐形水印技术，以解决多种内容溯源问题。这项技术能够检测AI生成视频、验证视频首发者，并识别视频的来源和创作工具。\n\n### 什么是隐形水印？\n隐形水印是一种强大的媒体处理技术，它以人眼无法察觉的方式将信号嵌入到媒体中，但软件可以检测到。它通过巧妙地修改图像的像素值、音频的波形或大型语言模型（LLM）生成的文本标记来嵌入少量数据。与可能丢失的元数据标签不同，水印系统通过添加必要的冗余来确保嵌入的标识在转码和编辑后依然持久存在。\n\n### 隐形水印的应用场景\n1.  **谁首先发布了视频？** 隐形水印可以帮助识别视频的首次上传者，解决内容归属问题。\n2.  **内容是否由AI生成？** 随着生成式AI（GenAI）视频的日益逼真，区分真实和AI生成内容变得越来越困难。隐形水印可用于推断内容是否为AI生成。\n3.  **使用了什么工具创作？** 隐形水印可以直接推断出图像或视频的来源和创作工具。\n\n传统方法如可见水印（分散注意力）或元数据标签（编辑或重新编码时可能丢失）无法充分且稳健地解决这些挑战。隐形水印因其持久性和不可察觉性，提供了一种更优越的替代方案。\n\n### 隐形水印与相关概念的对比\n| 特征         | 数字水印           | 隐写术             | 隐形水印           |\n| :----------- | :----------------- | :----------------- | :----------------- |\n| **目的**     | 内容归属、保护、溯源 | 秘密通信           | 内容归属、保护、溯源 |\n| **可见性**   | 可见或不可见       | 不可见             | 不可见             |\n| **鲁棒性**   | 中到高             | 通常低             | 高（可抵抗编辑）   |\n| **载荷容量** | 中（可变）         | 可变               | 中（例如，>64位）  |\n| **计算成本** | 低（可见）到高（不可见） | 可变               | 高（高级机器学习模型） |\n\n### 规模化挑战与GPU到CPU的转变\n早期的数字水印研究（始于20世纪90年代）主要采用数字信号处理技术。然而，这些方法对社交媒体中常见的几何变换和过滤不够鲁棒。当前最先进的解决方案（如VideoSeal）使用机器学习（ML）技术，显著提高了鲁棒性。但将这些解决方案应用于视频领域（即逐帧水印）可能因计算成本过高而难以实现，除非进行必要的推理优化。\n\n尽管GPU似乎是部署基于ML的视频水印解决方案的显而易见的方案，但大多数GPU硬件专门用于大规模模型的训练和推理，对视频转码（压缩和解压缩）的支持有限或没有。这给Meta现有的视频处理软件（FFmpeg）和硬件堆栈带来了独特的挑战。\n\n**GPU优化尝试及转向CPU：**\nMeta的嵌入架构使用FFmpeg与自定义过滤器来计算和应用隐形水印掩码。在GPU上进行性能分析发现GPU利用率较低。即使实施了帧批处理和线程化，延迟或利用率也没有显著改善。主要瓶颈包括：\n*   **数据传输开销：** CPU与多个GPU之间高分辨率视频帧的来回传输导致线程和内存优化困难，GPU利用率不佳。\n*   **推理延迟：** 在同一主机上并行处理多个GPU上的隐形水印请求导致推理延迟显著增加。\n*   **模型加载时间：** 尽管模型尺寸小，但加载模型占用了总处理时间的很大一部分。依赖FFmpeg使得无法使用GPU上预加载的、已预热的模型。\n\n认识到这些限制后，Meta开始研究纯CPU推理方案。虽然初始基准测试显示CPU上的端到端（E2E）性能慢两倍以上，但通过调整编码器、解码器和PyTorch的线程参数，并优化隐形水印过滤器使用的采样参数，他们取得了显著改进。最终，经过适当调整的线程和嵌入参数，在单个进程中CPU上运行隐形水印的E2E延迟与GPU性能相差不到5%。更重要的是，他们可以在CPU上并行运行多个FFmpeg进程而不会增加延迟。这一突破使得他们能够以更高的运营效率部署解决方案。\n\n### 优化考量与权衡\n规模化部署隐形水印涉及在以下四个指标之间进行权衡：\n*   **延迟：** 水印处理的速度。\n*   **水印检测位准确性：** 检测嵌入水印的准确性。\n*   **视觉质量：** 确保嵌入的水印对人眼不可察觉。\n*   **压缩效率（通过BD-Rate衡量）：** 确保嵌入的水印不会显著增加比特率。\n\n优化一个指标可能会对其他指标产生负面影响。例如，更强的水印可能会提高位准确性，但可能导致可见伪影和比特率增加。\n\n**管理BD-Rate影响：** 隐形水印会引入熵，可能导致视频编码器产生更高的比特率。Meta的初始实现显示BD-Rate回归约20%。为缓解此问题，他们设计了一种新颖的帧选择水印方法，大大降低了BD-Rate影响，同时提高了视觉质量并最大限度地减少了水印位检测准确性的影响。\n\n**解决视觉质量回归：** 确保“隐形”水印真正隐形至关重要。尽管高质量指标得分（VMAF和SSIM）很高，但他们最初观察到明显的视觉伪影。通过实施自定义后处理技术和众包人工检查不同嵌入设置，他们解决了视觉质量问题。事实证明，传统视觉质量指标不足以检测隐形水印可能引入的伪影类型，因此主观评估至关重要。在调整算法以实现人眼不可见性的同时，他们密切监测了位准确性影响，以实现视觉质量和检测准确性之间的最佳平衡。\n\n### 经验教训与未来展望\n1.  **CPU效率：** 经过适当优化，纯CPU流水线在特定用例中可以达到与GPU流水线相当的性能，且成本更低。CPU为隐形水印系统提供了更具运营效率和可扩展性的解决方案。\n2.  **传统视频质量分数不足：** VMAF和SSIM等指标无法完全捕捉隐形水印引入的感知质量问题，需要人工检查。需要更多研究来开发一种能够以编程方式检测隐形水印造成的视觉质量损失的指标。\n3.  **生产质量标准高：** 水印技术可能因对BD-Rate和下游视频压缩的影响而无法直接应用于实际用例。需要扩展现有文献，以在保持卓越位准确性的同时，将BD-Rate影响降至最低。\n\nMeta已成功推出一个可扩展的水印解决方案，具有出色的延迟、视觉质量、检测位准确性和最小的BD-Rate影响。未来，他们将继续提高隐形水印检测的精度和复制检测召回率，并设想将隐形水印作为一个轻量级的“过滤器模块”，无缝集成到各种视频用例中，对用户体验影响最小，同时提供强大的内容溯源能力。",
      "shortSummary": "Meta公司已成功规模化部署隐形水印技术，用于内容溯源，包括检测AI生成视频、验证首发者和识别创作工具。该技术通过微妙修改媒体数据实现人眼不可察觉的嵌入。面对GPU在视频转码上的局限性，Meta通过优化FFmpeg和PyTorch参数，开发出CPU-based解决方案，其端到端性能与GPU相当，但运营效率更高。该方案在延迟、检测准确性、视觉质量和压缩效率（BD-Rate）之间取得了平衡，并通过人工检查解决了传统质量指标的不足，实现了鲁棒且高效的视频内容溯源。",
      "translated_title": "视频隐形水印的规模化应用",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>At Meta, we use invisible watermarking for a variety of content provenance use cases on our platforms. Invisible watermarking serves a number of use cases, including detecting AI-generated videos, verifying who posted a video first, and identifying the source and tools used to create a video. We&#8217;re sharing how we overcame the challenges of scaling [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/11/04/video-engineering/video-invisible-watermarking-at-scale/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/11/04/video-engineering/video-invisible-watermarking-at-scale/\">Video Invisible Watermarking at Scale</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "为生成式AI产品创新扩展隐私基础设施 (原标题: Scaling Privacy Infrastructure for GenAI Product Innovation)",
      "link": "https://engineering.fb.com/2025/10/23/security/scaling-privacy-infrastructure-for-genai-product-innovation/",
      "pubDate": "Thu, 23 Oct 2025 08:00:00 +0000",
      "isoDate": "2025-10-23T08:00:00.000Z",
      "creator": "",
      "summary": "# Meta为生成式AI产品创新扩展隐私基础设施\n\nMeta致力于赋能其产品团队负责任地利用生成式AI（GenAI）的力量。本文深入探讨了Meta如何通过扩展其**隐私感知基础设施（Privacy Aware Infrastructure, PAI）**来应对GenAI时代的数据保护挑战，并以Meta的AI眼镜作为GenAI用例的典范。Meta的目标是通过赋能产品团队获得血缘洞察和自动化隐私控制，加速GenAI产品创新，同时将用户信任和隐私作为基本原则。\n\n## GenAI面临的关键隐私挑战\n\nMeta在确保GenAI隐私方面遇到了三大主要挑战：\n\n*   **技术演进与数据爆炸式增长：** GenAI带来了新型数据类型和数据量的急剧增加，使得数据可观测性和管理变得更加复杂。\n*   **不断变化的合规要求：** 技术进步不断产生新的隐私和合规要求，Meta的竞争力取决于其适应这些需求的速度。\n*   **加速的创新周期：** GenAI驱动的功能加速了产品开发， necessitating infrastructure that can scale rapidly and enforce privacy controls automatically.\n\n## Meta AI眼镜：GenAI用例示例\n\nMeta的AI眼镜将可穿戴技术与GenAI结合，提供实时信息、个性化辅助和创意功能，所有这些都与佩戴者的环境相关联：\n\n*   **实时场景理解：** 利用先进的摄像头和传感器解释周围环境，实现即时、相关的问答。\n*   **情境叠加：** GenAI模型提供动态叠加和摘要，根据当前位置或活动提供量身定制的指导和信息。\n*   **自然直观的交互：** 通过Meta神经腕带等创新输入方法和Meta Ray-Ban显示眼镜中的先进输出技术，实现无缝、低延迟的全双工对话。\n\n这些前瞻性用例凸显了GenAI所实现的复杂数据流：连续的传感器输入、设备端和云端的实时处理，以及动态的用户反馈循环。\n\n## Meta隐私感知基础设施（PAI）概述\n\nMeta的PAI是其隐私策略的核心，它是一套基础设施服务、API和监控系统，旨在将隐私集成到产品开发的各个方面。PAI包括：\n\n*   **增强的可观测性：**\n    *   通过高级扫描和标记实现数据自动检测，识别摄取点处的相关数据。\n    *   通过数据血缘追踪进一步加强，维护数据来源、传播路径和使用情况的实时地图，提供数据在系统间流动的全面可见性。\n*   **高效的隐私控制：**\n    *   策略执行API，用于在数据存储、处理和访问层以编程方式强制执行隐私约束。\n    *   策略自动化，将区域和全球要求嵌入到自动化检查和工作流约束中。\n*   **可扩展性：** 支持Meta庞大生态系统中的数千个微服务和产品团队。\n\nPAI赋能工程师在创新的同时，自动确保策略合规性和安全性。\n\n### 图示：AI眼镜与Meta AI应用的系统上下文\n\n![图片 1](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-1.png)\n图1：AI眼镜与Meta AI应用交互的系统上下文。\n\n### 图示：关键隐私工作流\n\n![图片 2](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-2.png)\n图2：关键隐私工作流。\n\n## 深入探究PAI的“发现”阶段：大规模数据血缘\n\nPAI最具变革性的技术之一是其大规模数据血缘方法。该系统持续追踪和映射整个基础设施中的数据流。Meta的血缘解决方案必须跨越数百万数据和代码资产，涵盖数百个平台和多种编程语言。\n\n### 收集跨栈血缘以获取交互数据\n\n为了维护数据（例如AI眼镜的用户交互数据）的隐私要求，需要一个完整的数据移动图谱。跨栈血缘提供了这种可追溯性：\n\n*   **[A] 在Web内部：** 通过隐私探测器捕获交互数据进入Meta Web服务器以及Web组件之间的数据流，精确了解数据的收集和处理方式。\n*   **[B] Web -> 记录器 -> 数据仓库：** 当Web处理持久化数据时，血缘追踪写入数据仓库表的记录器。然后，在下游批量处理数据时，解析记录器配置、SQL查询和处理日志以提取数据血缘。\n*   **[C] Web <> 推理：** 对于大型语言模型（LLM）调用，在服务/RPC边界收集血缘信号，例如调用了哪些模型检查点、输入是什么以及返回给应用的响应是什么。\n*   **[D] 数据仓库 -> 训练：** 最后，血缘将数据仓库表链接到训练作业及其生成的检查点。这个边界可以强制执行和证明关于允许用途的隐私要求。\n\n### 图示：跨栈血缘\n\n![图片 3](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-3.png)\n图3：跨栈血缘。PAI收集所有堆栈的血缘信号，包括Web探测、记录器、批量处理血缘、RPC血缘和训练清单。它们共同构成了交互数据的端到端图。\n\n### 图示：AI眼镜交互的端到端血缘\n\n![图片 4](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-4.png)\n图4：AI眼镜交互的端到端血缘。通过这种可见性，Meta可以具体地推断隐私：精确知道哪些系统参与了，哪些没有，从而在边界强制执行数据流并证明策略合规性。\n\n### 构建全面的血缘可观测性\n\n一个健全的血缘可观测性系统必须全面捕获数据处理时所有实际的数据流或I/O操作：\n\n*   **捕获并链接所有读取操作到写入操作：** 写入数据资产时，确保使用与读取操作相同的关联键记录所有相关的写入操作。这适用于SQL和非SQL查询，以及分布式I/O操作。\n*   **创建通用隐私库以记录数据流信息：** Meta的隐私库（PrivacyLib）旨在初始化和传播隐私策略，为各种操作（如读取、写入、远程调用）提供通用抽象，并标准化日志记录等扩展。\n*   **在Meta所有相关数据系统中放置库集成点：** 该库已集成到所有相关数据系统，以各种编程语言实现，并确保全面覆盖I/O操作。\n\n### 图示：通过PrivacyLib构建血缘可观测性\n\n![图片 5](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-5.png)\n图5：通过PrivacyLib构建血缘可观测性。\n\n## 从血缘到AI眼镜的证明\n\n数据血缘揭示了哪些系统处理AI眼镜的交互数据。基于此，Meta可以保护数据：\n\n*   **使用血缘指导策略区域（Policy Zones）的放置**，以保护交互数据。\n*   **仅当所有训练数据资产都允许用于特定目的时**，才启动使用该区域数据资产的模型训练作业；否则，进行补救。\n*   **验证器持续监控这些边界**，以便在功能开发早期识别任何新增或更改的数据处理作业。\n\n### 图示：通过策略区域从血缘到证明\n\n![图片 6](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-6.png)\n图6：通过策略区域从血缘到证明。\n\n## 关键要点：PAI如何扩展GenAI隐私\n\nMeta的隐私方法是：扩展基础设施，而不仅仅是规则。通过将包括数据血缘在内的PAI技术嵌入到技术栈中，Meta赋能工程师安全、快速、全球化地交付下一波GenAI产品。\n\n*   GenAI及其驱动产品的飞速发展带来了新的隐私和策略挑战，需要快速开发隐私感知基础设施。\n*   隐私感知基础设施（PAI）提供可重用的工作流（理解 -> 发现 -> 执行 -> 证明），同样可以扩展GenAI产品的隐私执行。\n*   可扩展的数据血缘技术通过提供可审计的实时数据流洞察，促进隐私控制。\n*   自动防护措施和即时开发反馈帮助产品团队更快、更安全、更顺畅地推进。\n\n## 随着GenAI发展，隐私也在发展\n\n为GenAI扩展隐私是一个持续的旅程。随着AI能力的进步，隐私保护的复杂性和期望也在增加。Meta的PAI正在同步发展，整合更智能的血缘分析和更友好的开发者工具，以满足这些新需求。通过将隐私基础设施作为产品赋能者而非障碍，Meta正在为负责任的AI产品创新奠定基础。",
      "shortSummary": "Meta通过扩展其隐私感知基础设施（PAI）来负责任地推动生成式AI（GenAI）产品创新。PAI通过大规模数据血缘追踪、自动化隐私控制和增强的可观测性，应对GenAI带来的数据隐私挑战。以Meta AI眼镜为例，PAI确保数据从收集到训练的整个生命周期都符合隐私要求。其核心理念是扩展基础设施而非仅规则，从而赋能产品团队更快、更安全地开发新功能，同时维护用户信任和数据隐私。",
      "translated_title": "为生成式AI产品创新扩展隐私基础设施",
      "images": [
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-1.png",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-2.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-3.png",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-4.png",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-5.png",
          "alt": "",
          "title": "",
          "position": 5
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-6.png",
          "alt": "",
          "title": "",
          "position": 6
        }
      ],
      "contentSource": "完整文章",
      "content": "<p>How does Meta empower its product teams to harness GenAI’s power responsibly? In this post, we delve into how Meta addresses the challenges of safeguarding data in the GenAI era by scaling its Privacy Aware Infrastructure (PAI), with a particular focus on Meta’s AI glasses as an example GenAI use case.  We’ll describe in detail [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/23/security/scaling-privacy-infrastructure-for-genai-product-innovation/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/23/security/scaling-privacy-infrastructure-for-genai-product-innovation/\">Scaling Privacy Infrastructure for GenAI Product Innovation</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "解耦调度网络：助力Meta AI规模化发展 (原标题: Disaggregated Scheduled Fabric: Scaling Meta’s AI Journey)",
      "link": "https://engineering.fb.com/2025/10/20/data-center-engineering/disaggregated-scheduled-fabric-scaling-metas-ai-journey/",
      "pubDate": "Mon, 20 Oct 2025 16:00:37 +0000",
      "isoDate": "2025-10-20T16:00:37.000Z",
      "creator": "",
      "summary": "## 解耦调度网络 (DSF)：Meta的下一代AI网络架构\n\n### 引言\n\n解耦调度网络（DSF）是Meta为AI训练网络设计的下一代网络架构，旨在解决现有基于Clos网络的挑战。随着生成式AI（GenAI）的兴起，对高性能、低延迟、无损AI网络的需求激增。DSF通过将线卡和结构卡解耦为独立的硬件设备，突破了传统单片机箱交换机的物理限制，构建了可扩展的AI网络。DSF是一个基于虚拟输出队列（VOQ）的系统，由开放的OCP-SAI标准和FBOSS驱动，其模块化架构优化了负载均衡和拥塞控制，确保了集群内部和集群间流量的高性能。通过DSF，Meta已成功构建了互连数千个GPU的大型数据中心集群。\n\n### 传统IP网络的挑战\n\n在传统IP网络上运行AI训练作业（使用远程直接内存访问RDMA技术）时，Meta面临以下挑战：\n\n*   **大象流（Elephant flows）**：AI工作负载通常产生长时间、大流量的数据流，容易导致链路拥塞和队头阻塞。\n*   **低熵（Low entropy）**：集体操作中涉及的GPU数量可能导致IP流数量较少，从而造成哈希效率低下，即使网络容量充足也可能出现拥塞。\n*   **网络利用率低下（Suboptimal fabric utilization）**：上述问题共同导致网络链路带宽利用率严重不均。\n\nMeta尝试了多种解决方案，但都存在局限性，例如BGP策略无法处理故障，负载感知ECMP方案难以调优并产生乱序包，而集中式流量工程方案则随着网络规模增大而变得过于复杂且对故障反应缓慢。\n\n### DSF原理概述\n\nDSF的核心思想源于AI训练工作负载的“大象流”和“低熵”特性。其根本创新在于其**两域架构**：\n\n1.  **以太网域**：服务器和传统网络协议在此运行。\n2.  **结构域**：数据包被分解成单元，在结构中喷洒，然后在硬件中重组，最后返回以太网域。\n\nDSF由两个主要组件构成：\n\n*   **接口节点 (INs) / 机架解耦交换机 (RDSWs)**：处理外部连接和路由功能，与数据中心基础设施接口。\n*   **结构节点 (FNs) / 结构解耦交换机 (FDSWs)**：作为内部交换元件，专用于结构内的高速流量分发，无需L3路由能力。\n\n对外部网络而言，INs和FNs的分布式集合表现为一个统一的虚拟机箱交换机，其总外部端口数等同于所有INs的外部端口总和，从而突破了传统设计的物理限制。\n\nDSF的控制平面基于Meta的开源网络操作系统FBOSS，支持解耦网络的多ASIC控制需求，并通过FBOSS状态数据库（FSBD）实现节点间的实时状态同步。\n\nDSF通过以下机制实现流量管理：\n\n*   **数据包喷洒（Packet spraying）**：将流量分散到所有可用路径，硬件在接口节点重组数据包单元，确保有序交付。\n*   **基于信用的拥塞控制（Credit-based congestion control）**：入口INs动态向出口INs请求信用令牌，系统根据路径可用性、拥塞水平和带宽利用率进行实时决策。\n*   **虚拟输出队列（VOQ）**：确保无损交付，将传入数据包导向针对特定目的地端口和服务类别的虚拟输出队列，每个VOQ独立调度传输，提供精细的流量管理。\n\n这种方法使DSF能够实现近乎最优的负载均衡，充分利用网络带宽，灵活处理混合流量模式，并适应动态网络条件，无需手动重新配置或流量工程。\n\n### DSF在GenAI应用中的实践\n\nMeta利用DSF技术构建了大规模的GenAI集群：\n\n#### 1. DSF L1区域\n\n这是更大集群的构建块，互连数据中心区域内的数千个GPU。\n\n![图片 1](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-1.png)\n\n*   **拓扑结构**：一个AI区域包含多个扩展单元（SUx），每个单元内的GPU机架连接到RDSWs。所有RDSWs通过一层FDSWs互连。\n*   **硬件**：RDSWs由Jerico3-AI芯片驱动，FDSWs使用Ramon3芯片。FBOSS是所有角色的网络操作系统。\n*   **互连**：RDSW-FDSW连接使用2x400G FR4光纤。GPU到RDSW连接经过优化，有利于延迟敏感的分层集合操作。\n*   **规模**：为支持单个AI区域内高GPU规模，创建了两个相同的网络平面。\n\n#### 2. DSF双级网络 (L2区域)\n\n![图片 2](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-2.png)\n\n*   **互连**：通过第二级脊柱DSF交换机（SDSWs）互连4个DSF L1区域。SDSWs与FDSWs硬件相同，聚合L1区域，使其作为单个DSF网络。\n*   **规模**：这是一个无阻塞拓扑，提供18K x 800G GPU的互连规模。\n*   **特性**：所有RDSWs维护全网状FDSB会话以交换信息。引入了“输入平衡模式”（Input Balanced Mode），智能平衡跨层可达性信息，以避免故障时结构层和脊柱层的拥塞。\n\n#### 3. DSF区域 (L3)\n\n![图片 3](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-3.png)\n\n*   **互连**：通过L3超脊柱层连接5个DSF L2区域，以实现更大规模的GPU互连。\n*   **边缘PoD**：每个建筑使用特殊的边缘交付点（PoD），包含40个FDSWs和128个边缘DSF交换机（EDSWs）。EDSW与RDSW硬件相同，但功能是提供L3超脊柱连接。\n*   **端口**：每个EDSW连接到四个超脊柱设备，使用4x800G链路，每个边缘PoD提供总计2k x 800G端口。\n*   **流量与路由**：由于模型分片，L3超脊柱层预计流量较少，4.5:1的超额订阅率足够。通过iBGP（EDSW与RDSWs）和eBGP（EDSW与L3超脊柱）交换路由信息。\n*   **挑战**：在L3层，低熵和大象流问题可能再次出现，但由于流量较少，影响不那么显著。\n\n### 输入平衡模式 (Input Balanced Mode)\n\n输入平衡模式是DSF网络中的一项关键特性，旨在确保在远程链路故障时网络流量的平衡，避免DSF网络结构层和脊柱层出现严重拥塞。\n\n#### 机制\n\n该模式确保任何DSF设备的输入带宽等于或小于输出带宽，即使在远程链路故障时也不会发生过载。设备会传播减少的可达性信息，通知其他设备按比例减少发送到受影响设备的流量。\n\n![图片 4](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-4.png)\n\n*   **RDSW<->FDSW 链路故障**：\n    *   当RDSW与FDSW之间的链路发生故障时，RDSW会失去与FDSW的连接，FDSW也会停止通告该连接的可达性。\n    *   ![图片 5](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-5.png)\n    *   上游SDSW会发现到目标RDSW的输入容量大于输出容量。为避免过载，SDSW将随机选择部分输入链路，停止通告到该目标RDSW的可达性。\n    *   ![图片 6](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-6.png)\n    *   进一步，FDSW也会随机选择链路停止通告可达性，导致源RDSW向目标RDSW转发流量的能力减半。\n    *   ![图片 7](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-7.png)\n\n*   **FDSW<->SDSW 链路故障**：\n    *   当FDSW与SDSW之间的链路发生故障时，容量减少的信息会向两个方向传播。\n    *   ![图片 8](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-8.png)\n    *   **FDSW传播**：FDSW的输出容量减少，它会随机选择一个FDSW<->RDSW链路，停止向集群外的所有目的地通告可达性。但该链路仍可用于集群内流量。\n    *   ![图片 9](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-9.png)\n    *   **SDSW传播**：SDSW的输出容量减少，它会随机选择一个指向其他集群的链路，停止向目标集群的所有RDSW通告可达性。\n    *   ![图片 10](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-10.png)",
      "shortSummary": "Meta的解耦调度网络（DSF）是下一代AI训练网络架构，旨在解决传统Clos网络在大规模AI训练中遇到的“大象流”、低熵和利用率低下等问题。DSF通过解耦线卡和结构卡，采用两域架构、数据包喷洒和基于信用的虚拟输出队列（VOQ）机制，实现高效负载均衡和无损交付。该技术已用于构建从L1到L3的多层级GenAI集群，互连数万个GPU。DSF还引入了“输入平衡模式”，确保在链路故障时网络流量的平衡，显著提升了AI网络的弹性、性能和可扩展性。",
      "translated_title": "解耦调度网络：助力Meta AI规模化发展",
      "images": [
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-1.png",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-2.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-3.png",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-4.png",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-5.png",
          "alt": "",
          "title": "",
          "position": 5
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-6.png",
          "alt": "",
          "title": "",
          "position": 6
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-7.png",
          "alt": "",
          "title": "",
          "position": 7
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-8.png",
          "alt": "",
          "title": "",
          "position": 8
        }
      ],
      "contentSource": "完整文章",
      "content": "<p>Disaggregated Schedule Fabric (DSF) is Meta’s next-generation network fabric technology for AI training networks that addresses the challenges of existing Clos-based networks. We’re sharing the challenges and innovations surrounding DSF and discussing future directions, including the creation of mega clusters through DSF and non-DSF region interconnectivity, as well as the exploration of alternative switching technologies. [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/20/data-center-engineering/disaggregated-scheduled-fabric-scaling-metas-ai-journey/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/20/data-center-engineering/disaggregated-scheduled-fabric-scaling-metas-ai-journey/\">Disaggregated Scheduled Fabric: Scaling Meta’s AI Journey</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "扩展LLM推理：张量并行、上下文并行和专家并行方面的创新 (原标题: Scaling LLM Inference: Innovations in Tensor Parallelism, Context Parallelism, and Expert Parallelism)",
      "link": "https://engineering.fb.com/2025/10/17/ai-research/scaling-llm-inference-innovations-tensor-parallelism-context-parallelism-expert-parallelism/",
      "pubDate": "Fri, 17 Oct 2025 16:00:50 +0000",
      "isoDate": "2025-10-17T16:00:50.000Z",
      "creator": "",
      "summary": "Meta公司正在不断推动大型语言模型（LLM）推理系统的边界，以支持Meta AI应用等。为了优化资源效率、吞吐量和延迟等关键性能指标，Meta开发并实施了先进的并行化技术。\n\n## LLM推理的关键性能指标\nMeta的主要目标是优化以下指标：\n*   **资源效率**：最大化GPU利用率，提高运营效率。\n*   **吞吐量（查询/秒）**：处理更高容量的请求，服务更多用户。\n*   **延迟**：最小化响应时间，提供无缝用户体验，包括：\n    *   **预填充（Prefill）阶段的首个令牌时间（TTFT）**：理想情况下低于350毫秒。\n    *   **解码（Decoding）阶段的增量令牌时间（TTIT）**：目标低于25毫秒。\n\n这些指标突出了LLM推理的不同计算需求：预填充是计算密集型的，而解码是内存带宽密集型的。\n\n## LLM推理的两个阶段\n典型的LLM生成推理任务分为两个阶段：\n1.  **预填充阶段**：处理输入提示（可能长达数千个令牌），为LLM的每个Transformer层生成键值（KV）缓存。由于注意力机制随序列长度呈二次方增长，此阶段受计算限制。\n2.  **解码阶段**：利用并逐步更新KV缓存，逐个生成令牌。此阶段受内存限制，因为读取内存的I/O时间占据主导地位，模型权重和KV缓存占据大部分内存。\n\n## 通过并行化解决瓶颈\n为了有效扩展LLM推理，特别是处理长上下文和大型模型，Meta采用了三种主要的推理并行化技术：\n\n### 1. 张量并行（Tensor Parallelism, TP）\n*   **目的**：将大型模型分布到多个GPU上，实现单个设备无法提供的高吞吐量。它将模型的单个层（如注意力块和多层感知器层）分片成更小的独立块，在不同设备上执行。\n*   **挑战**：`allreduce`通信操作可能占端到端延迟的30%。\n*   **解决方案：直接数据访问（DDA）算法**：\n    *   **DDA flat算法**：通过允许每个rank直接从其他rank加载内存并执行本地reduce操作，改善小消息大小的`allreduce`延迟，将延迟从O(N)降低到O(1)。\n    *   **DDA tree算法**：将`allreduce`分解为reduce-scatter和all-gather两个阶段，并在每一步中使用直接数据访问，将延迟降低到常数因子，适用于稍大的消息大小。\n*   **成果**：DDA解决方案在AMD MI300X上与Nvidia H100实现了整体性能持平，DDA在解码（小消息）方面比RCCL基线快10-50%，在预填充方面快10-30%，从而使TTIT减少约10%。\n\n### 2. 上下文并行（Context Parallelism, CP）\n*   **目的**：管理和处理极长的上下文，例如Llama 4引入的1M/10M令牌能力。\n*   **挑战**：\n    *   **计算**：密集注意力FLOPs随上下文长度呈二次方增长。\n    *   **内存**：KV缓存随上下文线性增长。\n    *   **通信**：跨多个主机并行化时通信延迟增加。\n*   **解决方案：“环形注意力”（ring attention）变体**：\n    *   **Pass-KV**：输入令牌在多个CP rank之间分割。每个rank计算其查询、键和值张量的一部分，然后交换键和值张量以实现完整上下文的注意力交互。\n    *   **Pass-Q**：类似Pass-KV，但交换查询张量。\n*   **成果**：结合快速注意力内核，在单个H100主机上实现了100万令牌的预填充时间少于一分钟，在32个H100主机上实现了1000万令牌的分布式推理时间少于一分钟。使用Llama 3 405B，在16个节点上通过CP实现了128K令牌预填充3.8秒，1M令牌预填充77秒的近线性扩展。\n\n### 3. 专家并行（Expert Parallelism, EP）\n*   **目的**：扩展混合专家（MoE）模型，其中大量“专家”模块无法完全适应单个主机。\n*   **机制**：利用两阶段的`all-to-all`通信模式，根据路由在数据并行和专家并行rank之间交换令牌。\n*   **挑战**：`all-to-all`通信可能占端到端延迟的10-30%，尤其对于解码消息（100KB到2MB）。\n*   **正在探索的解决方案**：\n    *   **动态`all-to-all`**：向远程邻居发送数据子块。\n    *   **持久`all-to-all`**：解决主要由内存句柄交换、网络负载平衡和CPU开销引起的减速。\n\n## 展望：解耦推理和未来挑战\n为了进一步优化LLM推理，Meta正朝着N维并行（CP、PP、EP、TP跨节点，以及独立的DP）和解耦预填充与解码层级发展。这将实现更好的资源平衡，并可能使用异构硬件（计算密集型硬件用于预填充，内存带宽密集型硬件用于解码）。\n\n未来的挑战包括：\n*   **云架构设计**：优化底层云基础设施以适应LLM工作负载。\n*   **通信集成到内核（fused kernel）**：将通信操作直接集成到计算内核中以提高效率。\n*   **设备发起内核（device-initiated kernel）**：使设备能够直接发起操作，减少CPU开销。\n\n这些并行化和系统级改进有助于实现下一代AI应用，并推动LLM能力的边界。",
      "shortSummary": "Meta公司通过创新并行化技术，优化LLM推理系统的资源效率、吞吐量和延迟。文章详细介绍了三种关键并行策略：张量并行（TP）通过DDA算法加速大型模型通信；上下文并行（CP）利用“环形注意力”处理超长上下文；专家并行（EP）通过优化`all-to-all`通信扩展MoE模型。这些技术显著提升了LLM推理性能，并为未来N维并行和解耦推理奠定了基础，以应对云架构和内核集成等挑战。",
      "translated_title": "扩展LLM推理：张量并行、上下文并行和专家并行方面的创新",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>At Meta, we are constantly pushing the boundaries of LLM inference systems to power applications such as the Meta AI App. We&#8217;re sharing how we developed and implemented advanced parallelism techniques to optimize key performance metrics related to resource efficiency, throughput, and latency. The rapid evolution of large language models (LLMs) has ushered in a [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/17/ai-research/scaling-llm-inference-innovations-tensor-parallelism-context-parallelism-expert-parallelism/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/17/ai-research/scaling-llm-inference-innovations-tensor-parallelism-context-parallelism-expert-parallelism/\">Scaling LLM Inference: Innovations in Tensor Parallelism, Context Parallelism, and Expert Parallelism</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "Sapling 单体仓库中的分支管理 (原标题: Branching in a Sapling Monorepo)",
      "link": "https://engineering.fb.com/2025/10/16/developer-tools/branching-in-a-sapling-monorepo/",
      "pubDate": "Thu, 16 Oct 2025 17:10:43 +0000",
      "isoDate": "2025-10-16T17:10:43.000Z",
      "creator": "",
      "summary": "Sapling 是 Meta 公司用于其大型单体仓库的可扩展、用户友好且开源的源代码控制系统。文章探讨了在大型单体仓库中设计和实现分支工作流所面临的挑战，以及如何在可扩展性和开发者体验之间进行权衡。\n\n**Meta 的源代码控制方法与挑战**\n\n*   **单体仓库优势**：Meta 的工程团队在一个大型单体仓库中工作，采用单一主分支，这有助于统一依赖管理、大规模重构、简化协作和代码复用。\n*   **版本管理挑战**：然而，这种方法给需要管理多个代码版本的团队带来了挑战。在多仓库设置中，团队可以依赖仓库分支来管理不同版本，并使用 cherry-pick 和 merge 等工具。但在单体仓库中，全仓库分支（full-repo branching）效果不佳。\n*   **全仓库分支的局限性**：\n    *   **冻结无关项目**：创建全仓库分支会冻结无关项目和依赖，导致它们迅速过时。\n    *   **可扩展性问题**：对于需要合并回主分支的开发工作流，全仓库分支不可扩展。它会创建具有多个父级的合并提交，使提交图变得宽泛且非线性，从而导致 `sl log` 和 `sl blame` 等操作的性能问题。\n    *   **“全有或全无”**：全仓库分支是“全有或全无”的，无法为特定项目或部分代码创建分支，导致团队复制代码，失去标准开发工具的优势，增加重复工作和错误。\n\n**Sapling 的单体仓库分支解决方案：目录分支**\n\n为解决上述挑战，Sapling 引入了一套新的源代码控制工具，实现了“目录分支”（directory branching）。\n\n*   **核心理念**：目录分支弥合了使用多个仓库分支和将代码副本作为单独目录维护之间的差距。它允许将单体仓库中的目录视为传统的仓库分支进行管理。\n*   **工作流**：用户可以通过复制代码来创建分支，通过 cherry-pick 维护代码，并在目录之间合并更改，就像它们是分支一样。\n*   **可扩展性优势**：目录分支支持目录间的合并，但在单体仓库的提交图层面，它们表现为线性提交。这解决了仓库级别合并提交带来的可扩展性挑战，同时仍提供目录级别的合并工作流。\n\n**目录分支的实现**\n\n目录分支在 Sapling 中通过一系列围绕 `sl subtree` 命令的操作实现：\n\n*   **`sl subtree copy`**：用于复制目录（或文件），可以从当前版本或任何历史版本复制到仓库中的新位置。Sapling 在提交中记录元数据，跟踪源目录、源修订版本和复制关系，从而恢复新分支中所有文件的完整历史记录。\n*   **`sl subtree import`**：如果代码尚未在单体仓库中，可以使用此命令创建外部仓库分支的目录分支。\n*   **`sl subtree graft` 和 `sl subtree merge`**：用于在目录分支之间进行 cherry-pick 或合并更改。这些操作利用存储的复制/合并元数据来重建目录之间的关系，从而实现三方合并。\n\n**构建系统和开发者工具集成**\n\n*   **优势**：所有目录分支的最新版本同时可见，使得持续集成（CI）可以通过一次检出测试多个分支。\n*   **构建系统集成**：Meta 使用 Buck2 作为构建系统，通过 Buck 配置修饰符（`-m` 标志）来选择使用的分支。\n*   **缺点**：代码搜索可能导致多个分支的重复结果，需要能够对结果进行排名的代码搜索系统来解决。\n\n**用户反馈与采纳原因**\n\n目录分支的引入取得了成功，Meta 内部的许多工程团队已采用它来管理多个代码版本。常见的采纳原因包括：\n\n1.  **CI 成本高昂或可能造成重大中断**：团队使用目录分支有效分离开发和生产版本的代码，更好地控制代码部署。\n2.  **长期实验性开发**：大量开发者协作数月，但更改可能扰乱生产版本，且使用大型差异堆栈模拟分支不切实际。\n3.  **解除 Git 迁移障碍**：在迁移到单体仓库期间，需要 Git 分支的等效功能，以便完成迁移和整合。\n\n尽管单体仓库的默认假设是单一版本，但当上述原因适用时，目录分支提供了一种解决方案，在不牺牲单体仓库优势的情况下实现分支工作流。\n\n**未来工作**\n\nSapling 计划利用目录分支更好地集成 Git 仓库，开发一种轻量级仓库迁移机制。这将通过 `sl subtree import` 命令的一个选项提供，允许创建指向外部仓库的软链接，并在用户请求时动态加载 Git 历史记录，从而降低 Git 仓库进入单体仓库的门槛。",
      "shortSummary": "Sapling 是 Meta 的开源源代码控制系统，用于其大型单体仓库。文章讨论了在大型单体仓库中管理分支的挑战，特别是全仓库分支在可扩展性方面的局限性。为解决此问题，Sapling 引入了“目录分支”解决方案。该方案允许将目录视为传统仓库分支进行复制、挑选和合并，同时在提交图上保持线性，从而解决了性能问题。目录分支通过 `sl subtree` 命令实现，已成功被 Meta 团队采纳，并计划用于更好地集成 Git 仓库，降低迁移障碍。",
      "translated_title": "Sapling 单体仓库中的分支管理",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>Sapling is a scalable, user-friendly, and open-source source control system that powers Meta&#8217;s monorepo. As discussed at the GitMerge 2024 conference session on branching, designing and implementing branching workflows for large monorepos is a challenging problem with multiple tradeoffs between scalability and the developer experience. After the conference, we designed, implemented, and open sourced our [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/16/developer-tools/branching-in-a-sapling-monorepo/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/16/developer-tools/branching-in-a-sapling-monorepo/\">Branching in a Sapling Monorepo</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "10倍骨干网：Meta 如何为 AI 扩展骨干网络连接 (原标题: 10X Backbone: How Meta Is Scaling Backbone Connectivity for AI)",
      "link": "https://engineering.fb.com/2025/10/16/data-center-engineering/10x-backbone-how-meta-is-scaling-backbone-connectivity-for-ai/",
      "pubDate": "Thu, 16 Oct 2025 16:30:02 +0000",
      "isoDate": "2025-10-16T16:30:02.000Z",
      "creator": "",
      "summary": "Meta正在详细阐述其如何扩展骨干网络以支持日益增长的AI工作负载，并将其AI集群扩展到多个数据中心。这一“10倍骨干网”项目旨在通过新技术和设计应对十倍的扩展需求。\n\n### Meta的骨干网络概述\nMeta的骨干网络由互连的路由平台组成，提供广域网（WAN）连接。它分为两种主要网络：\n\n*   **经典骨干网 (CBB)**：用于实现从数据中心（DC）到连接外部运营商的接入点（POP）的全球覆盖。CBB灵活多变，支持多种地理位置和连接需求，采用传统的IP/MPLS-TE技术。\n*   **快速骨干网 (EBB)**：专为可扩展的DC到DC互连而构建。EBB灵活性较低，但运行高度定制的软件栈，如Open/R路由协议和内部流量工程栈。EBB是本次扩展面临最大挑战的网络。\n\n自2015年EBB开始承载流量以来，其DC到DC流量增长显著。连接DC需要大量的光纤，且新DC的建设速度快，给网络连接带来了挑战。\n\n### 实现10倍骨干网的三大技术\n\n1.  **DC城域架构**\n    *   **问题**：为新DC构建数百英里长的长途光纤连接非常困难。\n    *   **解决方案**：预构建DC城域架构组件，包括两条提供可扩展容量的光纤环和两个提供远程站点连接的POP。DC连接到这些环，从而增加或启用DC与POP之间的容量。\n    *   **优势**：简化DC连接和WAN拓扑，标准化可扩展的物理设计，分离城域和长途网络。\n\n2.  **IP平台扩展**\n    *   **向上扩展（依赖供应商）**：\n        *   **更大机箱**：如12槽机箱比8槽机箱提供50%的容量，但引入了机械、散热、功耗、空间、ASIC、基础设施和网络操作系统（NOS）复杂性等挑战。\n        *   **更快接口**：通过从400G升级到800G平台，容量可以翻倍，但同样面临散热、功耗、新ASIC、基础设施和NOS复杂性等挑战，并需要支持800G-ZR+收发器。\n    *   **向外扩展（Meta可控）**：\n        *   **增加骨干网平面**：从四个增加到八个平面可以使全球容量翻倍，但这具有高度中断性，需要大量规划，并增加全局功耗和空间需求。\n        *   **每个平面增加多台设备**：允许在特定位置扩展容量，但对目标站点具有中断性，引入新的故障模式，并使网络操作更加复杂。\n    *   Meta在10倍骨干网的旅程中同时采用了向上和向外扩展技术。\n\n3.  **IP与光纤集成**\n    *   **ZR技术**：通过利用ZR技术，Meta改变了网络中每太比特的功耗足迹。\n        *   **变化**：移除了独立的转发器，其功能现在集成到路由器插头中。每个插头仅增加10-15W的功耗，而每个转发器曾消耗高达2kW。\n        *   **总体效益**：聚合功耗降低80%到90%。\n    *   **高层变化**：\n        *   **成本和能效**：以更小的空间和功耗包络部署相同的骨干网容量。\n        *   **机架分配**：光纤与IP设备的机架分配从90/10（ZR之前）变为60/40（ZR之后）。\n        *   **光纤对数**：每个机架可连接的光纤对数从1x增加到4x。\n        *   **简化部署**：安装可插拔模块比安装独立的转发器更简单、更可预测。\n        *   **简化运维**：减少活动设备数量。\n        *   **互操作性和供应商多样性**。\n    *   **挑战**：IP与光纤功能界限变得模糊，光通道的遥测和数据收集会增加IP设备的CPU消耗。\n\n### AI骨干网的构建\n过去18个月，对构建更大GPU集群的需求日益增长，超出了现有数据中心园区的容量。Meta正在寻找地理位置邻近的扩展地点，并与光纤采购团队合作，确定连接现有地点的时间、可行性和技术。\n\n根据所需距离，Meta提出了三种解决方案：\n*   **FR插头**：适用于3公里范围内的建筑连接。\n*   **LR插头**：将距离增加到10公里范围。\n*   **ZR插头 + 光纤DWDM技术**：用于超过10公里的距离，需要有源光组件进行复用和放大信号。这种方法将光纤数量减少了64倍，并采用了最新一代C+L波段800G ZR技术。保护切换虽然增加了操作挑战，但减少了IP平台端口的消耗。每个光纤对可承载64x 800G (51.2T) 的容量，通过水平扩展实现所需总容量。\n\n目前，单个AI骨干网站点对的规模是Meta过去十年构建的全球骨干网的两倍，这带来了部署和配置方面的巨大挑战。\n\n### 经验教训与未来展望\nEBB的扩展在过去八九年中经历了意想不到的加速，原计划2028年实现的扩展目标提前到2024年。\n\n**主要经验教训**：\n*   10倍骨干网的实现得益于向上和向外扩展的创新。\n*   预构建可扩展的城域设计能够更快地响应网络增长。\n*   IP/光纤集成减少了活动设备数量、空间和功耗，并实现了进一步扩展。\n*   复用10倍骨干网技术使得AI骨干网的构建成为可能。\n\n**未来展望**：Meta计划建设城市级数据中心，骨干网必须随之演进和扩展。叶脊架构被视为下一步扩展平台的关键，它能以更少的中断步骤提供所需的规模。Meta将继续执行AI骨干网的初始计划，并在部署更多站点和成熟运营过程中，深入理解AI在光网络中的复杂性。",
      "shortSummary": "Meta正通过“10倍骨干网”项目扩展其骨干网络，以支持AI工作负载和跨数据中心AI集群。该项目主要关注Express Backbone (EBB)网络，并采用了三大核心技术：预构建的DC城域架构、IP平台（向上和向外）扩展，以及利用ZR技术实现IP与光纤集成，大幅提升了能效和部署效率。为AI骨干网，Meta还开发了针对不同距离的连接方案，并计划采用叶脊架构应对未来城市级数据中心的需求。单个AI骨干网站点对的规模已是过去十年全球骨干网的两倍。",
      "translated_title": "10倍骨干网：Meta 如何为 AI 扩展骨干网络连接",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>We&#8217;re sharing details on our journey to scale Meta&#8217;s Backbone network to support the increasing demands of new and existing AI workloads. We&#8217;ve developed new technologies and designs to address our 10x scaling needs and applying some of these same principles to help extend our AI clusters between multiple data centers. Meta’s Backbone network is [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/16/data-center-engineering/10x-backbone-how-meta-is-scaling-backbone-connectivity-for-ai/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/16/data-center-engineering/10x-backbone-how-meta-is-scaling-backbone-connectivity-for-ai/\">10X Backbone: How Meta Is Scaling Backbone Connectivity for AI</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "可持续性设计：降低IT硬件排放的新设计原则 (原标题: Design for Sustainability: New Design Principles for Reducing IT Hardware Emissions)",
      "link": "https://engineering.fb.com/2025/10/14/data-center-engineering/design-for-sustainability-new-design-principles-for-reducing-it-hardware-emissions/",
      "pubDate": "Tue, 14 Oct 2025 20:40:20 +0000",
      "isoDate": "2025-10-14T20:40:20.000Z",
      "creator": "",
      "summary": "## 可持续性设计：降低IT硬件排放的新设计原则\n\nMeta公司提出了“可持续性设计”原则，这是一套针对IT硬件新设计的技术设计原则，旨在通过重用、延长使用寿命和优化设计来减少排放和成本。Meta已通过整合模块化、重用、改造、去物质化、使用更环保材料以及延长硬件生命周期等多种设计策略，显著降低了其数据中心的碳足迹，并呼吁更广泛的行业采纳这些策略以实现可持续发展目标。\n\n### Meta的可持续发展目标与关注点\n\n*   **净零排放目标**：Meta致力于在2030年实现整个价值链的净零排放。\n*   **关注Scope 3排放**：主要关注来自数据中心建设、IT硬件（计算、存储、冷却设备）和网络光纤基础设施等物理来源的Scope 3（价值链）排放。\n*   **排放来源**：包括IT硬件的制造、交付以及报废处理、回收或转售相关的排放。\n*   **控制与减少策略**：\n    *   优化材料选择。\n    *   在设计中选择和开发低碳替代品。\n    *   与硬件供应商合作，减少上游排放。\n\n### 什么是可持续性设计？\n\n“可持续性设计”是Meta开发和提出的一套指导方针，旨在帮助硬件设计师减少IT机架的环境影响。它考虑了能效、材料的选择、减少、循环性以及硬件的报废处理等多种因素。可持续硬件设计需要硬件设计师、工程师和可持续性专家之间的协作，以创建既满足性能要求又限制环境影响的硬件。本指南特别关注数据中心机架的设计，并为各种组件（如机械、冷却、计算、存储和布线）提供可持续的选择。\n\n### 减少Scope 3排放的内部团队行动\n\nMeta内部团队为减少IT硬件的Scope 3排放采取了以下措施：\n\n*   **优化硬件设计**：以最低排放为目标，尽可能延长材料的使用寿命，或使用低碳材料。\n*   **提高效率**：通过延长IT机架的使用寿命，可能跳过新一代设备的采购。\n*   **组件回收**：回收不再可用的服务器组件作为备件，例如从报废机架中回收双列直插式内存模块（DIMM）并重新部署到新构建中。\n*   **排放概况分析**：了解供应商、组件和系统设计的排放概况，以指导未来的减排路线图。\n*   **供应商协作**：与供应商合作，推动其制造过程电气化，转向可再生能源，并利用低碳材料和设计。\n\n这些减少IT硬件Scope 3排放的行动，同时也有助于减少数据中心产生的电子垃圾（e-waste）。\n\n### Meta部署的机架类型与组件\n\nMeta数据中心部署了多种机架设计，以支持不同的工作负载和基础设施需求，主要包括：\n\n*   **AI**：AI训练和推理工作负载。\n*   **计算**：运行Meta产品和服务所需的通用计算。\n*   **存储**：存储和维护产品使用的数据。\n*   **网络**：提供服务器之间的低延迟互连。\n\n尽管这些机架类型在架构上存在差异，但大多数都应用通用的硬件设计原则，并包含来自类似供应商的活跃和被动组件。因此，相同的可持续性设计原则适用于这些不同的机架类型。每个机架内，有五大类组件是排放减少的目标：\n\n*   计算（即内存、HDD/SSD）\n*   存储\n*   网络\n*   电源\n*   机架基础设施（即机械和散热）\n\n### 减少排放的具体技术\n\nMeta主要通过以下四个类别来解决与硬件组件相关的排放问题：\n\n1.  **模块化机架设计**\n    *   允许旧机架组件在新机架中重用，例如Open Rack设计（ORv2和ORv3）。\n    *   **ORv3关键特点**：电源单元（PSU）和电池备份单元（BBU）分离，实现更可靠和灵活的配置；48V电源输出，允许电源货架在机架中任意放置；可根据平台和区域需求配置PSU和BBU货架；正在努力设计“通用化”ORv3机架框架以简化组装和降低成本；ORv3N是ORv3的衍生品，专为网络应用设计，提供效率和成本改进。\n    *   随着AI工作负载的扩展，新的专业机架设计需要设计师采纳最模块化的设计原则。\n\n2.  **重用/改造现有机架设计**\n    *   这是一种经济高效且可持续的方法，可以满足不断演进的数据中心需求，减少电子垃圾、降低成本并加速部署。\n    *   **优点**：成本节约、减少电子垃圾、加快部署、环境效益。\n    *   **挑战**：兼容性问题、电源和冷却要求、可扩展性和灵活性限制、测试和验证。\n    *   Meta认为，在每个新机架设计中都应充分考虑重用或改造现有机架的益处。\n\n3.  **绿色和回收材料**\n    *   **绿色钢铁**：通过电弧炉（EAF）而非传统高炉生产，使用清洁可再生电力和更高比例的回收含量，显著减少碳排放。Meta与提供100%清洁可再生能源生产绿色钢铁的供应商合作。\n    *   **回收钢铁、铝和铜**：回收这些材料可节省大量生产硬件所需的能源。Meta要求所有机架/底盘至少包含20%的回收钢铁，所有散热器必须完全由回收铝或铜制造。\n\n4.  **提高可靠性以延长使用寿命**\n    *   延长机架、服务器、内存和SSD的使用寿命有助于Meta减少硬件采购量，从而显著降低排放和成本。\n    *   可靠性基准测试是评估硬件寿命延长可行性的重要因素。\n    *   需要考虑备件和供应商支持的可用性可能下降，以及设备故障率增加的风险。\n\n5.  **去物质化**\n    *   减少和移除不必要的硬件组件可以显著减少原材料、水和/或能源的使用。\n    *   这包括减少机架上的钢铁使用，或在保持设计约束的同时移除服务器主板上不必要的组件。\n    *   去物质化还涉及将多个机架整合为更少、更高效的机架，从而减少整体物理占地面积。\n    *   硬件板上多余组件的原因可能包括：未来验证、灵活性、调试和测试、冗余、模块化、法规遵从性。\n    *   Meta强调，每个硬件设计都应优化组件填充，避免不必要的组件，例如未填充的集成电路（IC）插座、未使用的连接器、测试点、冗余电源或可选内存/存储组件。\n\n6.  **生产化低排放新技术**\n    *   内存和SSD/HDD通常是服务器机架中最大的隐含碳排放源。\n    *   新技术可以帮助Meta显著减少排放和成本，同时提供更高的功率归一化性能。\n    *   **示例**：\n        *   从HDD转向SSD：减少驱动器、服务器、机架、BBU和PSU的需求，并有助于降低整体能耗。\n        *   液冷：根据当地环境条件和数据中心工作负载，液冷在服务器机架中可能比传统空冷碳效率高17%。\n        *   探索替代技术：如相变存储器（PCM）或磁阻随机存取存储器（MRAM），它们具有相同性能但碳排放较低。\n        *   使用低功耗双倍数据速率（LPDDR）代替DDR，以实现低功耗和高带宽。",
      "shortSummary": "Meta推出了“可持续性设计”原则，旨在通过重用、延长使用寿命和优化设计，减少IT硬件的排放和成本。这些原则关注数据中心IT硬件的Scope 3排放，涵盖模块化设计、重用改造、使用绿色和回收材料、提高可靠性、去物质化以及采用低排放新技术。Meta呼吁全行业采纳这些策略，以实现2030年净零排放目标并减少电子垃圾。",
      "translated_title": "可持续性设计：降低IT硬件排放的新设计原则",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>We’re presenting Design for Sustainability,  a set of technical design principles for new designs of IT hardware to reduce emissions and cost through reuse, extending useful life, and optimizing design. At Meta, we’ve been able to significantly reduce the carbon footprint of our data centers by integrating several design strategies such as modularity, reuse, retrofitting, [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/14/data-center-engineering/design-for-sustainability-new-design-principles-for-reducing-it-hardware-emissions/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/14/data-center-engineering/design-for-sustainability-new-design-principles-for-reducing-it-hardware-emissions/\">Design for Sustainability: New Design Principles for Reducing IT Hardware Emissions</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    }
  ],
  "lastUpdated": "2025-11-17T05:27:50.971Z"
}