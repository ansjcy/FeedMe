{
  "sourceUrl": "https://engineering.fb.com/feed/",
  "title": "Engineering at Meta",
  "description": "Engineering at Meta Blog",
  "link": "https://engineering.fb.com/",
  "items": [
    {
      "title": "为生成式AI产品创新扩展隐私基础设施 (原标题: Scaling Privacy Infrastructure for GenAI Product Innovation)",
      "link": "https://engineering.fb.com/2025/10/23/security/scaling-privacy-infrastructure-for-genai-product-innovation/",
      "pubDate": "Thu, 23 Oct 2025 08:00:00 +0000",
      "isoDate": "2025-10-23T08:00:00.000Z",
      "creator": "",
      "summary": "# Meta为生成式AI产品创新扩展隐私基础设施\n\nMeta致力于赋能其产品团队负责任地利用生成式AI（GenAI）的力量。本文深入探讨了Meta如何通过扩展其**隐私感知基础设施（Privacy Aware Infrastructure, PAI）**来应对GenAI时代的数据保护挑战，并以Meta的AI眼镜作为GenAI用例的典范。Meta的目标是通过赋能产品团队获得血缘洞察和自动化隐私控制，加速GenAI产品创新，同时将用户信任和隐私作为基本原则。\n\n## GenAI面临的关键隐私挑战\n\nMeta在确保GenAI隐私方面遇到了三大主要挑战：\n\n*   **技术演进与数据爆炸式增长：** GenAI带来了新型数据类型和数据量的急剧增加，使得数据可观测性和管理变得更加复杂。\n*   **不断变化的合规要求：** 技术进步不断产生新的隐私和合规要求，Meta的竞争力取决于其适应这些需求的速度。\n*   **加速的创新周期：** GenAI驱动的功能加速了产品开发， necessitating infrastructure that can scale rapidly and enforce privacy controls automatically.\n\n## Meta AI眼镜：GenAI用例示例\n\nMeta的AI眼镜将可穿戴技术与GenAI结合，提供实时信息、个性化辅助和创意功能，所有这些都与佩戴者的环境相关联：\n\n*   **实时场景理解：** 利用先进的摄像头和传感器解释周围环境，实现即时、相关的问答。\n*   **情境叠加：** GenAI模型提供动态叠加和摘要，根据当前位置或活动提供量身定制的指导和信息。\n*   **自然直观的交互：** 通过Meta神经腕带等创新输入方法和Meta Ray-Ban显示眼镜中的先进输出技术，实现无缝、低延迟的全双工对话。\n\n这些前瞻性用例凸显了GenAI所实现的复杂数据流：连续的传感器输入、设备端和云端的实时处理，以及动态的用户反馈循环。\n\n## Meta隐私感知基础设施（PAI）概述\n\nMeta的PAI是其隐私策略的核心，它是一套基础设施服务、API和监控系统，旨在将隐私集成到产品开发的各个方面。PAI包括：\n\n*   **增强的可观测性：**\n    *   通过高级扫描和标记实现数据自动检测，识别摄取点处的相关数据。\n    *   通过数据血缘追踪进一步加强，维护数据来源、传播路径和使用情况的实时地图，提供数据在系统间流动的全面可见性。\n*   **高效的隐私控制：**\n    *   策略执行API，用于在数据存储、处理和访问层以编程方式强制执行隐私约束。\n    *   策略自动化，将区域和全球要求嵌入到自动化检查和工作流约束中。\n*   **可扩展性：** 支持Meta庞大生态系统中的数千个微服务和产品团队。\n\nPAI赋能工程师在创新的同时，自动确保策略合规性和安全性。\n\n### 图示：AI眼镜与Meta AI应用的系统上下文\n\n![图片 1](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-1.png)\n图1：AI眼镜与Meta AI应用交互的系统上下文。\n\n### 图示：关键隐私工作流\n\n![图片 2](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-2.png)\n图2：关键隐私工作流。\n\n## 深入探究PAI的“发现”阶段：大规模数据血缘\n\nPAI最具变革性的技术之一是其大规模数据血缘方法。该系统持续追踪和映射整个基础设施中的数据流。Meta的血缘解决方案必须跨越数百万数据和代码资产，涵盖数百个平台和多种编程语言。\n\n### 收集跨栈血缘以获取交互数据\n\n为了维护数据（例如AI眼镜的用户交互数据）的隐私要求，需要一个完整的数据移动图谱。跨栈血缘提供了这种可追溯性：\n\n*   **[A] 在Web内部：** 通过隐私探测器捕获交互数据进入Meta Web服务器以及Web组件之间的数据流，精确了解数据的收集和处理方式。\n*   **[B] Web -> 记录器 -> 数据仓库：** 当Web处理持久化数据时，血缘追踪写入数据仓库表的记录器。然后，在下游批量处理数据时，解析记录器配置、SQL查询和处理日志以提取数据血缘。\n*   **[C] Web <> 推理：** 对于大型语言模型（LLM）调用，在服务/RPC边界收集血缘信号，例如调用了哪些模型检查点、输入是什么以及返回给应用的响应是什么。\n*   **[D] 数据仓库 -> 训练：** 最后，血缘将数据仓库表链接到训练作业及其生成的检查点。这个边界可以强制执行和证明关于允许用途的隐私要求。\n\n### 图示：跨栈血缘\n\n![图片 3](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-3.png)\n图3：跨栈血缘。PAI收集所有堆栈的血缘信号，包括Web探测、记录器、批量处理血缘、RPC血缘和训练清单。它们共同构成了交互数据的端到端图。\n\n### 图示：AI眼镜交互的端到端血缘\n\n![图片 4](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-4.png)\n图4：AI眼镜交互的端到端血缘。通过这种可见性，Meta可以具体地推断隐私：精确知道哪些系统参与了，哪些没有，从而在边界强制执行数据流并证明策略合规性。\n\n### 构建全面的血缘可观测性\n\n一个健全的血缘可观测性系统必须全面捕获数据处理时所有实际的数据流或I/O操作：\n\n*   **捕获并链接所有读取操作到写入操作：** 写入数据资产时，确保使用与读取操作相同的关联键记录所有相关的写入操作。这适用于SQL和非SQL查询，以及分布式I/O操作。\n*   **创建通用隐私库以记录数据流信息：** Meta的隐私库（PrivacyLib）旨在初始化和传播隐私策略，为各种操作（如读取、写入、远程调用）提供通用抽象，并标准化日志记录等扩展。\n*   **在Meta所有相关数据系统中放置库集成点：** 该库已集成到所有相关数据系统，以各种编程语言实现，并确保全面覆盖I/O操作。\n\n### 图示：通过PrivacyLib构建血缘可观测性\n\n![图片 5](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-5.png)\n图5：通过PrivacyLib构建血缘可观测性。\n\n## 从血缘到AI眼镜的证明\n\n数据血缘揭示了哪些系统处理AI眼镜的交互数据。基于此，Meta可以保护数据：\n\n*   **使用血缘指导策略区域（Policy Zones）的放置**，以保护交互数据。\n*   **仅当所有训练数据资产都允许用于特定目的时**，才启动使用该区域数据资产的模型训练作业；否则，进行补救。\n*   **验证器持续监控这些边界**，以便在功能开发早期识别任何新增或更改的数据处理作业。\n\n### 图示：通过策略区域从血缘到证明\n\n![图片 6](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-6.png)\n图6：通过策略区域从血缘到证明。\n\n## 关键要点：PAI如何扩展GenAI隐私\n\nMeta的隐私方法是：扩展基础设施，而不仅仅是规则。通过将包括数据血缘在内的PAI技术嵌入到技术栈中，Meta赋能工程师安全、快速、全球化地交付下一波GenAI产品。\n\n*   GenAI及其驱动产品的飞速发展带来了新的隐私和策略挑战，需要快速开发隐私感知基础设施。\n*   隐私感知基础设施（PAI）提供可重用的工作流（理解 -> 发现 -> 执行 -> 证明），同样可以扩展GenAI产品的隐私执行。\n*   可扩展的数据血缘技术通过提供可审计的实时数据流洞察，促进隐私控制。\n*   自动防护措施和即时开发反馈帮助产品团队更快、更安全、更顺畅地推进。\n\n## 随着GenAI发展，隐私也在发展\n\n为GenAI扩展隐私是一个持续的旅程。随着AI能力的进步，隐私保护的复杂性和期望也在增加。Meta的PAI正在同步发展，整合更智能的血缘分析和更友好的开发者工具，以满足这些新需求。通过将隐私基础设施作为产品赋能者而非障碍，Meta正在为负责任的AI产品创新奠定基础。",
      "shortSummary": "Meta通过扩展其隐私感知基础设施（PAI）来负责任地推动生成式AI（GenAI）产品创新。PAI通过大规模数据血缘追踪、自动化隐私控制和增强的可观测性，应对GenAI带来的数据隐私挑战。以Meta AI眼镜为例，PAI确保数据从收集到训练的整个生命周期都符合隐私要求。其核心理念是扩展基础设施而非仅规则，从而赋能产品团队更快、更安全地开发新功能，同时维护用户信任和数据隐私。",
      "translated_title": "为生成式AI产品创新扩展隐私基础设施",
      "images": [
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-1.png",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-2.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-3.png",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-4.png",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-5.png",
          "alt": "",
          "title": "",
          "position": 5
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-PAI-for-GenAI-Figure-6.png",
          "alt": "",
          "title": "",
          "position": 6
        }
      ],
      "contentSource": "完整文章",
      "content": "<p>How does Meta empower its product teams to harness GenAI’s power responsibly? In this post, we delve into how Meta addresses the challenges of safeguarding data in the GenAI era by scaling its Privacy Aware Infrastructure (PAI), with a particular focus on Meta’s AI glasses as an example GenAI use case.  We’ll describe in detail [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/23/security/scaling-privacy-infrastructure-for-genai-product-innovation/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/23/security/scaling-privacy-infrastructure-for-genai-product-innovation/\">Scaling Privacy Infrastructure for GenAI Product Innovation</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "解耦调度网络：助力Meta AI规模化发展 (原标题: Disaggregated Scheduled Fabric: Scaling Meta’s AI Journey)",
      "link": "https://engineering.fb.com/2025/10/20/data-center-engineering/disaggregated-scheduled-fabric-scaling-metas-ai-journey/",
      "pubDate": "Mon, 20 Oct 2025 16:00:37 +0000",
      "isoDate": "2025-10-20T16:00:37.000Z",
      "creator": "",
      "summary": "## 解耦调度网络 (DSF)：Meta的下一代AI网络架构\n\n### 引言\n\n解耦调度网络（DSF）是Meta为AI训练网络设计的下一代网络架构，旨在解决现有基于Clos网络的挑战。随着生成式AI（GenAI）的兴起，对高性能、低延迟、无损AI网络的需求激增。DSF通过将线卡和结构卡解耦为独立的硬件设备，突破了传统单片机箱交换机的物理限制，构建了可扩展的AI网络。DSF是一个基于虚拟输出队列（VOQ）的系统，由开放的OCP-SAI标准和FBOSS驱动，其模块化架构优化了负载均衡和拥塞控制，确保了集群内部和集群间流量的高性能。通过DSF，Meta已成功构建了互连数千个GPU的大型数据中心集群。\n\n### 传统IP网络的挑战\n\n在传统IP网络上运行AI训练作业（使用远程直接内存访问RDMA技术）时，Meta面临以下挑战：\n\n*   **大象流（Elephant flows）**：AI工作负载通常产生长时间、大流量的数据流，容易导致链路拥塞和队头阻塞。\n*   **低熵（Low entropy）**：集体操作中涉及的GPU数量可能导致IP流数量较少，从而造成哈希效率低下，即使网络容量充足也可能出现拥塞。\n*   **网络利用率低下（Suboptimal fabric utilization）**：上述问题共同导致网络链路带宽利用率严重不均。\n\nMeta尝试了多种解决方案，但都存在局限性，例如BGP策略无法处理故障，负载感知ECMP方案难以调优并产生乱序包，而集中式流量工程方案则随着网络规模增大而变得过于复杂且对故障反应缓慢。\n\n### DSF原理概述\n\nDSF的核心思想源于AI训练工作负载的“大象流”和“低熵”特性。其根本创新在于其**两域架构**：\n\n1.  **以太网域**：服务器和传统网络协议在此运行。\n2.  **结构域**：数据包被分解成单元，在结构中喷洒，然后在硬件中重组，最后返回以太网域。\n\nDSF由两个主要组件构成：\n\n*   **接口节点 (INs) / 机架解耦交换机 (RDSWs)**：处理外部连接和路由功能，与数据中心基础设施接口。\n*   **结构节点 (FNs) / 结构解耦交换机 (FDSWs)**：作为内部交换元件，专用于结构内的高速流量分发，无需L3路由能力。\n\n对外部网络而言，INs和FNs的分布式集合表现为一个统一的虚拟机箱交换机，其总外部端口数等同于所有INs的外部端口总和，从而突破了传统设计的物理限制。\n\nDSF的控制平面基于Meta的开源网络操作系统FBOSS，支持解耦网络的多ASIC控制需求，并通过FBOSS状态数据库（FSBD）实现节点间的实时状态同步。\n\nDSF通过以下机制实现流量管理：\n\n*   **数据包喷洒（Packet spraying）**：将流量分散到所有可用路径，硬件在接口节点重组数据包单元，确保有序交付。\n*   **基于信用的拥塞控制（Credit-based congestion control）**：入口INs动态向出口INs请求信用令牌，系统根据路径可用性、拥塞水平和带宽利用率进行实时决策。\n*   **虚拟输出队列（VOQ）**：确保无损交付，将传入数据包导向针对特定目的地端口和服务类别的虚拟输出队列，每个VOQ独立调度传输，提供精细的流量管理。\n\n这种方法使DSF能够实现近乎最优的负载均衡，充分利用网络带宽，灵活处理混合流量模式，并适应动态网络条件，无需手动重新配置或流量工程。\n\n### DSF在GenAI应用中的实践\n\nMeta利用DSF技术构建了大规模的GenAI集群：\n\n#### 1. DSF L1区域\n\n这是更大集群的构建块，互连数据中心区域内的数千个GPU。\n\n![图片 1](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-1.png)\n\n*   **拓扑结构**：一个AI区域包含多个扩展单元（SUx），每个单元内的GPU机架连接到RDSWs。所有RDSWs通过一层FDSWs互连。\n*   **硬件**：RDSWs由Jerico3-AI芯片驱动，FDSWs使用Ramon3芯片。FBOSS是所有角色的网络操作系统。\n*   **互连**：RDSW-FDSW连接使用2x400G FR4光纤。GPU到RDSW连接经过优化，有利于延迟敏感的分层集合操作。\n*   **规模**：为支持单个AI区域内高GPU规模，创建了两个相同的网络平面。\n\n#### 2. DSF双级网络 (L2区域)\n\n![图片 2](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-2.png)\n\n*   **互连**：通过第二级脊柱DSF交换机（SDSWs）互连4个DSF L1区域。SDSWs与FDSWs硬件相同，聚合L1区域，使其作为单个DSF网络。\n*   **规模**：这是一个无阻塞拓扑，提供18K x 800G GPU的互连规模。\n*   **特性**：所有RDSWs维护全网状FDSB会话以交换信息。引入了“输入平衡模式”（Input Balanced Mode），智能平衡跨层可达性信息，以避免故障时结构层和脊柱层的拥塞。\n\n#### 3. DSF区域 (L3)\n\n![图片 3](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-3.png)\n\n*   **互连**：通过L3超脊柱层连接5个DSF L2区域，以实现更大规模的GPU互连。\n*   **边缘PoD**：每个建筑使用特殊的边缘交付点（PoD），包含40个FDSWs和128个边缘DSF交换机（EDSWs）。EDSW与RDSW硬件相同，但功能是提供L3超脊柱连接。\n*   **端口**：每个EDSW连接到四个超脊柱设备，使用4x800G链路，每个边缘PoD提供总计2k x 800G端口。\n*   **流量与路由**：由于模型分片，L3超脊柱层预计流量较少，4.5:1的超额订阅率足够。通过iBGP（EDSW与RDSWs）和eBGP（EDSW与L3超脊柱）交换路由信息。\n*   **挑战**：在L3层，低熵和大象流问题可能再次出现，但由于流量较少，影响不那么显著。\n\n### 输入平衡模式 (Input Balanced Mode)\n\n输入平衡模式是DSF网络中的一项关键特性，旨在确保在远程链路故障时网络流量的平衡，避免DSF网络结构层和脊柱层出现严重拥塞。\n\n#### 机制\n\n该模式确保任何DSF设备的输入带宽等于或小于输出带宽，即使在远程链路故障时也不会发生过载。设备会传播减少的可达性信息，通知其他设备按比例减少发送到受影响设备的流量。\n\n![图片 4](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-4.png)\n\n*   **RDSW<->FDSW 链路故障**：\n    *   当RDSW与FDSW之间的链路发生故障时，RDSW会失去与FDSW的连接，FDSW也会停止通告该连接的可达性。\n    *   ![图片 5](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-5.png)\n    *   上游SDSW会发现到目标RDSW的输入容量大于输出容量。为避免过载，SDSW将随机选择部分输入链路，停止通告到该目标RDSW的可达性。\n    *   ![图片 6](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-6.png)\n    *   进一步，FDSW也会随机选择链路停止通告可达性，导致源RDSW向目标RDSW转发流量的能力减半。\n    *   ![图片 7](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-7.png)\n\n*   **FDSW<->SDSW 链路故障**：\n    *   当FDSW与SDSW之间的链路发生故障时，容量减少的信息会向两个方向传播。\n    *   ![图片 8](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-8.png)\n    *   **FDSW传播**：FDSW的输出容量减少，它会随机选择一个FDSW<->RDSW链路，停止向集群外的所有目的地通告可达性。但该链路仍可用于集群内流量。\n    *   ![图片 9](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-9.png)\n    *   **SDSW传播**：SDSW的输出容量减少，它会随机选择一个指向其他集群的链路，停止向目标集群的所有RDSW通告可达性。\n    *   ![图片 10](https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-10.png)",
      "shortSummary": "Meta的解耦调度网络（DSF）是下一代AI训练网络架构，旨在解决传统Clos网络在大规模AI训练中遇到的“大象流”、低熵和利用率低下等问题。DSF通过解耦线卡和结构卡，采用两域架构、数据包喷洒和基于信用的虚拟输出队列（VOQ）机制，实现高效负载均衡和无损交付。该技术已用于构建从L1到L3的多层级GenAI集群，互连数万个GPU。DSF还引入了“输入平衡模式”，确保在链路故障时网络流量的平衡，显著提升了AI网络的弹性、性能和可扩展性。",
      "translated_title": "解耦调度网络：助力Meta AI规模化发展",
      "images": [
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-1.png",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-2.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-3.png",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-4.png",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-5.png",
          "alt": "",
          "title": "",
          "position": 5
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-6.png",
          "alt": "",
          "title": "",
          "position": 6
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-7.png",
          "alt": "",
          "title": "",
          "position": 7
        },
        {
          "url": "https://engineering.fb.com/wp-content/uploads/2025/10/Meta-DSF-Figure-8.png",
          "alt": "",
          "title": "",
          "position": 8
        }
      ],
      "contentSource": "完整文章",
      "content": "<p>Disaggregated Schedule Fabric (DSF) is Meta’s next-generation network fabric technology for AI training networks that addresses the challenges of existing Clos-based networks. We’re sharing the challenges and innovations surrounding DSF and discussing future directions, including the creation of mega clusters through DSF and non-DSF region interconnectivity, as well as the exploration of alternative switching technologies. [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/20/data-center-engineering/disaggregated-scheduled-fabric-scaling-metas-ai-journey/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/20/data-center-engineering/disaggregated-scheduled-fabric-scaling-metas-ai-journey/\">Disaggregated Scheduled Fabric: Scaling Meta’s AI Journey</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "扩展LLM推理：张量并行、上下文并行和专家并行方面的创新 (原标题: Scaling LLM Inference: Innovations in Tensor Parallelism, Context Parallelism, and Expert Parallelism)",
      "link": "https://engineering.fb.com/2025/10/17/ai-research/scaling-llm-inference-innovations-tensor-parallelism-context-parallelism-expert-parallelism/",
      "pubDate": "Fri, 17 Oct 2025 16:00:50 +0000",
      "isoDate": "2025-10-17T16:00:50.000Z",
      "creator": "",
      "summary": "Meta公司正在不断推动大型语言模型（LLM）推理系统的边界，以支持Meta AI应用等。为了优化资源效率、吞吐量和延迟等关键性能指标，Meta开发并实施了先进的并行化技术。\n\n## LLM推理的关键性能指标\nMeta的主要目标是优化以下指标：\n*   **资源效率**：最大化GPU利用率，提高运营效率。\n*   **吞吐量（查询/秒）**：处理更高容量的请求，服务更多用户。\n*   **延迟**：最小化响应时间，提供无缝用户体验，包括：\n    *   **预填充（Prefill）阶段的首个令牌时间（TTFT）**：理想情况下低于350毫秒。\n    *   **解码（Decoding）阶段的增量令牌时间（TTIT）**：目标低于25毫秒。\n\n这些指标突出了LLM推理的不同计算需求：预填充是计算密集型的，而解码是内存带宽密集型的。\n\n## LLM推理的两个阶段\n典型的LLM生成推理任务分为两个阶段：\n1.  **预填充阶段**：处理输入提示（可能长达数千个令牌），为LLM的每个Transformer层生成键值（KV）缓存。由于注意力机制随序列长度呈二次方增长，此阶段受计算限制。\n2.  **解码阶段**：利用并逐步更新KV缓存，逐个生成令牌。此阶段受内存限制，因为读取内存的I/O时间占据主导地位，模型权重和KV缓存占据大部分内存。\n\n## 通过并行化解决瓶颈\n为了有效扩展LLM推理，特别是处理长上下文和大型模型，Meta采用了三种主要的推理并行化技术：\n\n### 1. 张量并行（Tensor Parallelism, TP）\n*   **目的**：将大型模型分布到多个GPU上，实现单个设备无法提供的高吞吐量。它将模型的单个层（如注意力块和多层感知器层）分片成更小的独立块，在不同设备上执行。\n*   **挑战**：`allreduce`通信操作可能占端到端延迟的30%。\n*   **解决方案：直接数据访问（DDA）算法**：\n    *   **DDA flat算法**：通过允许每个rank直接从其他rank加载内存并执行本地reduce操作，改善小消息大小的`allreduce`延迟，将延迟从O(N)降低到O(1)。\n    *   **DDA tree算法**：将`allreduce`分解为reduce-scatter和all-gather两个阶段，并在每一步中使用直接数据访问，将延迟降低到常数因子，适用于稍大的消息大小。\n*   **成果**：DDA解决方案在AMD MI300X上与Nvidia H100实现了整体性能持平，DDA在解码（小消息）方面比RCCL基线快10-50%，在预填充方面快10-30%，从而使TTIT减少约10%。\n\n### 2. 上下文并行（Context Parallelism, CP）\n*   **目的**：管理和处理极长的上下文，例如Llama 4引入的1M/10M令牌能力。\n*   **挑战**：\n    *   **计算**：密集注意力FLOPs随上下文长度呈二次方增长。\n    *   **内存**：KV缓存随上下文线性增长。\n    *   **通信**：跨多个主机并行化时通信延迟增加。\n*   **解决方案：“环形注意力”（ring attention）变体**：\n    *   **Pass-KV**：输入令牌在多个CP rank之间分割。每个rank计算其查询、键和值张量的一部分，然后交换键和值张量以实现完整上下文的注意力交互。\n    *   **Pass-Q**：类似Pass-KV，但交换查询张量。\n*   **成果**：结合快速注意力内核，在单个H100主机上实现了100万令牌的预填充时间少于一分钟，在32个H100主机上实现了1000万令牌的分布式推理时间少于一分钟。使用Llama 3 405B，在16个节点上通过CP实现了128K令牌预填充3.8秒，1M令牌预填充77秒的近线性扩展。\n\n### 3. 专家并行（Expert Parallelism, EP）\n*   **目的**：扩展混合专家（MoE）模型，其中大量“专家”模块无法完全适应单个主机。\n*   **机制**：利用两阶段的`all-to-all`通信模式，根据路由在数据并行和专家并行rank之间交换令牌。\n*   **挑战**：`all-to-all`通信可能占端到端延迟的10-30%，尤其对于解码消息（100KB到2MB）。\n*   **正在探索的解决方案**：\n    *   **动态`all-to-all`**：向远程邻居发送数据子块。\n    *   **持久`all-to-all`**：解决主要由内存句柄交换、网络负载平衡和CPU开销引起的减速。\n\n## 展望：解耦推理和未来挑战\n为了进一步优化LLM推理，Meta正朝着N维并行（CP、PP、EP、TP跨节点，以及独立的DP）和解耦预填充与解码层级发展。这将实现更好的资源平衡，并可能使用异构硬件（计算密集型硬件用于预填充，内存带宽密集型硬件用于解码）。\n\n未来的挑战包括：\n*   **云架构设计**：优化底层云基础设施以适应LLM工作负载。\n*   **通信集成到内核（fused kernel）**：将通信操作直接集成到计算内核中以提高效率。\n*   **设备发起内核（device-initiated kernel）**：使设备能够直接发起操作，减少CPU开销。\n\n这些并行化和系统级改进有助于实现下一代AI应用，并推动LLM能力的边界。",
      "shortSummary": "Meta公司通过创新并行化技术，优化LLM推理系统的资源效率、吞吐量和延迟。文章详细介绍了三种关键并行策略：张量并行（TP）通过DDA算法加速大型模型通信；上下文并行（CP）利用“环形注意力”处理超长上下文；专家并行（EP）通过优化`all-to-all`通信扩展MoE模型。这些技术显著提升了LLM推理性能，并为未来N维并行和解耦推理奠定了基础，以应对云架构和内核集成等挑战。",
      "translated_title": "扩展LLM推理：张量并行、上下文并行和专家并行方面的创新",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>At Meta, we are constantly pushing the boundaries of LLM inference systems to power applications such as the Meta AI App. We&#8217;re sharing how we developed and implemented advanced parallelism techniques to optimize key performance metrics related to resource efficiency, throughput, and latency. The rapid evolution of large language models (LLMs) has ushered in a [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/17/ai-research/scaling-llm-inference-innovations-tensor-parallelism-context-parallelism-expert-parallelism/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/17/ai-research/scaling-llm-inference-innovations-tensor-parallelism-context-parallelism-expert-parallelism/\">Scaling LLM Inference: Innovations in Tensor Parallelism, Context Parallelism, and Expert Parallelism</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "Sapling 单体仓库中的分支管理 (原标题: Branching in a Sapling Monorepo)",
      "link": "https://engineering.fb.com/2025/10/16/developer-tools/branching-in-a-sapling-monorepo/",
      "pubDate": "Thu, 16 Oct 2025 17:10:43 +0000",
      "isoDate": "2025-10-16T17:10:43.000Z",
      "creator": "",
      "summary": "Sapling 是 Meta 公司用于其大型单体仓库的可扩展、用户友好且开源的源代码控制系统。文章探讨了在大型单体仓库中设计和实现分支工作流所面临的挑战，以及如何在可扩展性和开发者体验之间进行权衡。\n\n**Meta 的源代码控制方法与挑战**\n\n*   **单体仓库优势**：Meta 的工程团队在一个大型单体仓库中工作，采用单一主分支，这有助于统一依赖管理、大规模重构、简化协作和代码复用。\n*   **版本管理挑战**：然而，这种方法给需要管理多个代码版本的团队带来了挑战。在多仓库设置中，团队可以依赖仓库分支来管理不同版本，并使用 cherry-pick 和 merge 等工具。但在单体仓库中，全仓库分支（full-repo branching）效果不佳。\n*   **全仓库分支的局限性**：\n    *   **冻结无关项目**：创建全仓库分支会冻结无关项目和依赖，导致它们迅速过时。\n    *   **可扩展性问题**：对于需要合并回主分支的开发工作流，全仓库分支不可扩展。它会创建具有多个父级的合并提交，使提交图变得宽泛且非线性，从而导致 `sl log` 和 `sl blame` 等操作的性能问题。\n    *   **“全有或全无”**：全仓库分支是“全有或全无”的，无法为特定项目或部分代码创建分支，导致团队复制代码，失去标准开发工具的优势，增加重复工作和错误。\n\n**Sapling 的单体仓库分支解决方案：目录分支**\n\n为解决上述挑战，Sapling 引入了一套新的源代码控制工具，实现了“目录分支”（directory branching）。\n\n*   **核心理念**：目录分支弥合了使用多个仓库分支和将代码副本作为单独目录维护之间的差距。它允许将单体仓库中的目录视为传统的仓库分支进行管理。\n*   **工作流**：用户可以通过复制代码来创建分支，通过 cherry-pick 维护代码，并在目录之间合并更改，就像它们是分支一样。\n*   **可扩展性优势**：目录分支支持目录间的合并，但在单体仓库的提交图层面，它们表现为线性提交。这解决了仓库级别合并提交带来的可扩展性挑战，同时仍提供目录级别的合并工作流。\n\n**目录分支的实现**\n\n目录分支在 Sapling 中通过一系列围绕 `sl subtree` 命令的操作实现：\n\n*   **`sl subtree copy`**：用于复制目录（或文件），可以从当前版本或任何历史版本复制到仓库中的新位置。Sapling 在提交中记录元数据，跟踪源目录、源修订版本和复制关系，从而恢复新分支中所有文件的完整历史记录。\n*   **`sl subtree import`**：如果代码尚未在单体仓库中，可以使用此命令创建外部仓库分支的目录分支。\n*   **`sl subtree graft` 和 `sl subtree merge`**：用于在目录分支之间进行 cherry-pick 或合并更改。这些操作利用存储的复制/合并元数据来重建目录之间的关系，从而实现三方合并。\n\n**构建系统和开发者工具集成**\n\n*   **优势**：所有目录分支的最新版本同时可见，使得持续集成（CI）可以通过一次检出测试多个分支。\n*   **构建系统集成**：Meta 使用 Buck2 作为构建系统，通过 Buck 配置修饰符（`-m` 标志）来选择使用的分支。\n*   **缺点**：代码搜索可能导致多个分支的重复结果，需要能够对结果进行排名的代码搜索系统来解决。\n\n**用户反馈与采纳原因**\n\n目录分支的引入取得了成功，Meta 内部的许多工程团队已采用它来管理多个代码版本。常见的采纳原因包括：\n\n1.  **CI 成本高昂或可能造成重大中断**：团队使用目录分支有效分离开发和生产版本的代码，更好地控制代码部署。\n2.  **长期实验性开发**：大量开发者协作数月，但更改可能扰乱生产版本，且使用大型差异堆栈模拟分支不切实际。\n3.  **解除 Git 迁移障碍**：在迁移到单体仓库期间，需要 Git 分支的等效功能，以便完成迁移和整合。\n\n尽管单体仓库的默认假设是单一版本，但当上述原因适用时，目录分支提供了一种解决方案，在不牺牲单体仓库优势的情况下实现分支工作流。\n\n**未来工作**\n\nSapling 计划利用目录分支更好地集成 Git 仓库，开发一种轻量级仓库迁移机制。这将通过 `sl subtree import` 命令的一个选项提供，允许创建指向外部仓库的软链接，并在用户请求时动态加载 Git 历史记录，从而降低 Git 仓库进入单体仓库的门槛。",
      "shortSummary": "Sapling 是 Meta 的开源源代码控制系统，用于其大型单体仓库。文章讨论了在大型单体仓库中管理分支的挑战，特别是全仓库分支在可扩展性方面的局限性。为解决此问题，Sapling 引入了“目录分支”解决方案。该方案允许将目录视为传统仓库分支进行复制、挑选和合并，同时在提交图上保持线性，从而解决了性能问题。目录分支通过 `sl subtree` 命令实现，已成功被 Meta 团队采纳，并计划用于更好地集成 Git 仓库，降低迁移障碍。",
      "translated_title": "Sapling 单体仓库中的分支管理",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>Sapling is a scalable, user-friendly, and open-source source control system that powers Meta&#8217;s monorepo. As discussed at the GitMerge 2024 conference session on branching, designing and implementing branching workflows for large monorepos is a challenging problem with multiple tradeoffs between scalability and the developer experience. After the conference, we designed, implemented, and open sourced our [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/16/developer-tools/branching-in-a-sapling-monorepo/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/16/developer-tools/branching-in-a-sapling-monorepo/\">Branching in a Sapling Monorepo</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "10倍骨干网：Meta 如何为 AI 扩展骨干网络连接 (原标题: 10X Backbone: How Meta Is Scaling Backbone Connectivity for AI)",
      "link": "https://engineering.fb.com/2025/10/16/data-center-engineering/10x-backbone-how-meta-is-scaling-backbone-connectivity-for-ai/",
      "pubDate": "Thu, 16 Oct 2025 16:30:02 +0000",
      "isoDate": "2025-10-16T16:30:02.000Z",
      "creator": "",
      "summary": "Meta正在详细阐述其如何扩展骨干网络以支持日益增长的AI工作负载，并将其AI集群扩展到多个数据中心。这一“10倍骨干网”项目旨在通过新技术和设计应对十倍的扩展需求。\n\n### Meta的骨干网络概述\nMeta的骨干网络由互连的路由平台组成，提供广域网（WAN）连接。它分为两种主要网络：\n\n*   **经典骨干网 (CBB)**：用于实现从数据中心（DC）到连接外部运营商的接入点（POP）的全球覆盖。CBB灵活多变，支持多种地理位置和连接需求，采用传统的IP/MPLS-TE技术。\n*   **快速骨干网 (EBB)**：专为可扩展的DC到DC互连而构建。EBB灵活性较低，但运行高度定制的软件栈，如Open/R路由协议和内部流量工程栈。EBB是本次扩展面临最大挑战的网络。\n\n自2015年EBB开始承载流量以来，其DC到DC流量增长显著。连接DC需要大量的光纤，且新DC的建设速度快，给网络连接带来了挑战。\n\n### 实现10倍骨干网的三大技术\n\n1.  **DC城域架构**\n    *   **问题**：为新DC构建数百英里长的长途光纤连接非常困难。\n    *   **解决方案**：预构建DC城域架构组件，包括两条提供可扩展容量的光纤环和两个提供远程站点连接的POP。DC连接到这些环，从而增加或启用DC与POP之间的容量。\n    *   **优势**：简化DC连接和WAN拓扑，标准化可扩展的物理设计，分离城域和长途网络。\n\n2.  **IP平台扩展**\n    *   **向上扩展（依赖供应商）**：\n        *   **更大机箱**：如12槽机箱比8槽机箱提供50%的容量，但引入了机械、散热、功耗、空间、ASIC、基础设施和网络操作系统（NOS）复杂性等挑战。\n        *   **更快接口**：通过从400G升级到800G平台，容量可以翻倍，但同样面临散热、功耗、新ASIC、基础设施和NOS复杂性等挑战，并需要支持800G-ZR+收发器。\n    *   **向外扩展（Meta可控）**：\n        *   **增加骨干网平面**：从四个增加到八个平面可以使全球容量翻倍，但这具有高度中断性，需要大量规划，并增加全局功耗和空间需求。\n        *   **每个平面增加多台设备**：允许在特定位置扩展容量，但对目标站点具有中断性，引入新的故障模式，并使网络操作更加复杂。\n    *   Meta在10倍骨干网的旅程中同时采用了向上和向外扩展技术。\n\n3.  **IP与光纤集成**\n    *   **ZR技术**：通过利用ZR技术，Meta改变了网络中每太比特的功耗足迹。\n        *   **变化**：移除了独立的转发器，其功能现在集成到路由器插头中。每个插头仅增加10-15W的功耗，而每个转发器曾消耗高达2kW。\n        *   **总体效益**：聚合功耗降低80%到90%。\n    *   **高层变化**：\n        *   **成本和能效**：以更小的空间和功耗包络部署相同的骨干网容量。\n        *   **机架分配**：光纤与IP设备的机架分配从90/10（ZR之前）变为60/40（ZR之后）。\n        *   **光纤对数**：每个机架可连接的光纤对数从1x增加到4x。\n        *   **简化部署**：安装可插拔模块比安装独立的转发器更简单、更可预测。\n        *   **简化运维**：减少活动设备数量。\n        *   **互操作性和供应商多样性**。\n    *   **挑战**：IP与光纤功能界限变得模糊，光通道的遥测和数据收集会增加IP设备的CPU消耗。\n\n### AI骨干网的构建\n过去18个月，对构建更大GPU集群的需求日益增长，超出了现有数据中心园区的容量。Meta正在寻找地理位置邻近的扩展地点，并与光纤采购团队合作，确定连接现有地点的时间、可行性和技术。\n\n根据所需距离，Meta提出了三种解决方案：\n*   **FR插头**：适用于3公里范围内的建筑连接。\n*   **LR插头**：将距离增加到10公里范围。\n*   **ZR插头 + 光纤DWDM技术**：用于超过10公里的距离，需要有源光组件进行复用和放大信号。这种方法将光纤数量减少了64倍，并采用了最新一代C+L波段800G ZR技术。保护切换虽然增加了操作挑战，但减少了IP平台端口的消耗。每个光纤对可承载64x 800G (51.2T) 的容量，通过水平扩展实现所需总容量。\n\n目前，单个AI骨干网站点对的规模是Meta过去十年构建的全球骨干网的两倍，这带来了部署和配置方面的巨大挑战。\n\n### 经验教训与未来展望\nEBB的扩展在过去八九年中经历了意想不到的加速，原计划2028年实现的扩展目标提前到2024年。\n\n**主要经验教训**：\n*   10倍骨干网的实现得益于向上和向外扩展的创新。\n*   预构建可扩展的城域设计能够更快地响应网络增长。\n*   IP/光纤集成减少了活动设备数量、空间和功耗，并实现了进一步扩展。\n*   复用10倍骨干网技术使得AI骨干网的构建成为可能。\n\n**未来展望**：Meta计划建设城市级数据中心，骨干网必须随之演进和扩展。叶脊架构被视为下一步扩展平台的关键，它能以更少的中断步骤提供所需的规模。Meta将继续执行AI骨干网的初始计划，并在部署更多站点和成熟运营过程中，深入理解AI在光网络中的复杂性。",
      "shortSummary": "Meta正通过“10倍骨干网”项目扩展其骨干网络，以支持AI工作负载和跨数据中心AI集群。该项目主要关注Express Backbone (EBB)网络，并采用了三大核心技术：预构建的DC城域架构、IP平台（向上和向外）扩展，以及利用ZR技术实现IP与光纤集成，大幅提升了能效和部署效率。为AI骨干网，Meta还开发了针对不同距离的连接方案，并计划采用叶脊架构应对未来城市级数据中心的需求。单个AI骨干网站点对的规模已是过去十年全球骨干网的两倍。",
      "translated_title": "10倍骨干网：Meta 如何为 AI 扩展骨干网络连接",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>We&#8217;re sharing details on our journey to scale Meta&#8217;s Backbone network to support the increasing demands of new and existing AI workloads. We&#8217;ve developed new technologies and designs to address our 10x scaling needs and applying some of these same principles to help extend our AI clusters between multiple data centers. Meta’s Backbone network is [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/16/data-center-engineering/10x-backbone-how-meta-is-scaling-backbone-connectivity-for-ai/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/16/data-center-engineering/10x-backbone-how-meta-is-scaling-backbone-connectivity-for-ai/\">10X Backbone: How Meta Is Scaling Backbone Connectivity for AI</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "可持续性设计：降低IT硬件排放的新设计原则 (原标题: Design for Sustainability: New Design Principles for Reducing IT Hardware Emissions)",
      "link": "https://engineering.fb.com/2025/10/14/data-center-engineering/design-for-sustainability-new-design-principles-for-reducing-it-hardware-emissions/",
      "pubDate": "Tue, 14 Oct 2025 20:40:20 +0000",
      "isoDate": "2025-10-14T20:40:20.000Z",
      "creator": "",
      "summary": "## 可持续性设计：降低IT硬件排放的新设计原则\n\nMeta公司提出了“可持续性设计”原则，这是一套针对IT硬件新设计的技术设计原则，旨在通过重用、延长使用寿命和优化设计来减少排放和成本。Meta已通过整合模块化、重用、改造、去物质化、使用更环保材料以及延长硬件生命周期等多种设计策略，显著降低了其数据中心的碳足迹，并呼吁更广泛的行业采纳这些策略以实现可持续发展目标。\n\n### Meta的可持续发展目标与关注点\n\n*   **净零排放目标**：Meta致力于在2030年实现整个价值链的净零排放。\n*   **关注Scope 3排放**：主要关注来自数据中心建设、IT硬件（计算、存储、冷却设备）和网络光纤基础设施等物理来源的Scope 3（价值链）排放。\n*   **排放来源**：包括IT硬件的制造、交付以及报废处理、回收或转售相关的排放。\n*   **控制与减少策略**：\n    *   优化材料选择。\n    *   在设计中选择和开发低碳替代品。\n    *   与硬件供应商合作，减少上游排放。\n\n### 什么是可持续性设计？\n\n“可持续性设计”是Meta开发和提出的一套指导方针，旨在帮助硬件设计师减少IT机架的环境影响。它考虑了能效、材料的选择、减少、循环性以及硬件的报废处理等多种因素。可持续硬件设计需要硬件设计师、工程师和可持续性专家之间的协作，以创建既满足性能要求又限制环境影响的硬件。本指南特别关注数据中心机架的设计，并为各种组件（如机械、冷却、计算、存储和布线）提供可持续的选择。\n\n### 减少Scope 3排放的内部团队行动\n\nMeta内部团队为减少IT硬件的Scope 3排放采取了以下措施：\n\n*   **优化硬件设计**：以最低排放为目标，尽可能延长材料的使用寿命，或使用低碳材料。\n*   **提高效率**：通过延长IT机架的使用寿命，可能跳过新一代设备的采购。\n*   **组件回收**：回收不再可用的服务器组件作为备件，例如从报废机架中回收双列直插式内存模块（DIMM）并重新部署到新构建中。\n*   **排放概况分析**：了解供应商、组件和系统设计的排放概况，以指导未来的减排路线图。\n*   **供应商协作**：与供应商合作，推动其制造过程电气化，转向可再生能源，并利用低碳材料和设计。\n\n这些减少IT硬件Scope 3排放的行动，同时也有助于减少数据中心产生的电子垃圾（e-waste）。\n\n### Meta部署的机架类型与组件\n\nMeta数据中心部署了多种机架设计，以支持不同的工作负载和基础设施需求，主要包括：\n\n*   **AI**：AI训练和推理工作负载。\n*   **计算**：运行Meta产品和服务所需的通用计算。\n*   **存储**：存储和维护产品使用的数据。\n*   **网络**：提供服务器之间的低延迟互连。\n\n尽管这些机架类型在架构上存在差异，但大多数都应用通用的硬件设计原则，并包含来自类似供应商的活跃和被动组件。因此，相同的可持续性设计原则适用于这些不同的机架类型。每个机架内，有五大类组件是排放减少的目标：\n\n*   计算（即内存、HDD/SSD）\n*   存储\n*   网络\n*   电源\n*   机架基础设施（即机械和散热）\n\n### 减少排放的具体技术\n\nMeta主要通过以下四个类别来解决与硬件组件相关的排放问题：\n\n1.  **模块化机架设计**\n    *   允许旧机架组件在新机架中重用，例如Open Rack设计（ORv2和ORv3）。\n    *   **ORv3关键特点**：电源单元（PSU）和电池备份单元（BBU）分离，实现更可靠和灵活的配置；48V电源输出，允许电源货架在机架中任意放置；可根据平台和区域需求配置PSU和BBU货架；正在努力设计“通用化”ORv3机架框架以简化组装和降低成本；ORv3N是ORv3的衍生品，专为网络应用设计，提供效率和成本改进。\n    *   随着AI工作负载的扩展，新的专业机架设计需要设计师采纳最模块化的设计原则。\n\n2.  **重用/改造现有机架设计**\n    *   这是一种经济高效且可持续的方法，可以满足不断演进的数据中心需求，减少电子垃圾、降低成本并加速部署。\n    *   **优点**：成本节约、减少电子垃圾、加快部署、环境效益。\n    *   **挑战**：兼容性问题、电源和冷却要求、可扩展性和灵活性限制、测试和验证。\n    *   Meta认为，在每个新机架设计中都应充分考虑重用或改造现有机架的益处。\n\n3.  **绿色和回收材料**\n    *   **绿色钢铁**：通过电弧炉（EAF）而非传统高炉生产，使用清洁可再生电力和更高比例的回收含量，显著减少碳排放。Meta与提供100%清洁可再生能源生产绿色钢铁的供应商合作。\n    *   **回收钢铁、铝和铜**：回收这些材料可节省大量生产硬件所需的能源。Meta要求所有机架/底盘至少包含20%的回收钢铁，所有散热器必须完全由回收铝或铜制造。\n\n4.  **提高可靠性以延长使用寿命**\n    *   延长机架、服务器、内存和SSD的使用寿命有助于Meta减少硬件采购量，从而显著降低排放和成本。\n    *   可靠性基准测试是评估硬件寿命延长可行性的重要因素。\n    *   需要考虑备件和供应商支持的可用性可能下降，以及设备故障率增加的风险。\n\n5.  **去物质化**\n    *   减少和移除不必要的硬件组件可以显著减少原材料、水和/或能源的使用。\n    *   这包括减少机架上的钢铁使用，或在保持设计约束的同时移除服务器主板上不必要的组件。\n    *   去物质化还涉及将多个机架整合为更少、更高效的机架，从而减少整体物理占地面积。\n    *   硬件板上多余组件的原因可能包括：未来验证、灵活性、调试和测试、冗余、模块化、法规遵从性。\n    *   Meta强调，每个硬件设计都应优化组件填充，避免不必要的组件，例如未填充的集成电路（IC）插座、未使用的连接器、测试点、冗余电源或可选内存/存储组件。\n\n6.  **生产化低排放新技术**\n    *   内存和SSD/HDD通常是服务器机架中最大的隐含碳排放源。\n    *   新技术可以帮助Meta显著减少排放和成本，同时提供更高的功率归一化性能。\n    *   **示例**：\n        *   从HDD转向SSD：减少驱动器、服务器、机架、BBU和PSU的需求，并有助于降低整体能耗。\n        *   液冷：根据当地环境条件和数据中心工作负载，液冷在服务器机架中可能比传统空冷碳效率高17%。\n        *   探索替代技术：如相变存储器（PCM）或磁阻随机存取存储器（MRAM），它们具有相同性能但碳排放较低。\n        *   使用低功耗双倍数据速率（LPDDR）代替DDR，以实现低功耗和高带宽。",
      "shortSummary": "Meta推出了“可持续性设计”原则，旨在通过重用、延长使用寿命和优化设计，减少IT硬件的排放和成本。这些原则关注数据中心IT硬件的Scope 3排放，涵盖模块化设计、重用改造、使用绿色和回收材料、提高可靠性、去物质化以及采用低排放新技术。Meta呼吁全行业采纳这些策略，以实现2030年净零排放目标并减少电子垃圾。",
      "translated_title": "可持续性设计：降低IT硬件排放的新设计原则",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>We’re presenting Design for Sustainability,  a set of technical design principles for new designs of IT hardware to reduce emissions and cost through reuse, extending useful life, and optimizing design. At Meta, we’ve been able to significantly reduce the carbon footprint of our data centers by integrating several design strategies such as modularity, reuse, retrofitting, [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/14/data-center-engineering/design-for-sustainability-new-design-principles-for-reducing-it-hardware-emissions/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/14/data-center-engineering/design-for-sustainability-new-design-principles-for-reducing-it-hardware-emissions/\">Design for Sustainability: New Design Principles for Reducing IT Hardware Emissions</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "Meta 如何利用人工智能提高 IT 硬件范围 3 排放估算的质量 (原标题: How Meta Is Leveraging AI To Improve the Quality of Scope 3 Emission Estimates for IT Hardware)",
      "link": "https://engineering.fb.com/2025/10/14/data-center-engineering/how-meta-is-leveraging-ai-to-improve-the-quality-of-scope-3-emission-estimates-for-it-hardware/",
      "pubDate": "Tue, 14 Oct 2025 20:40:01 +0000",
      "isoDate": "2025-10-14T20:40:01.000Z",
      "creator": "",
      "summary": "Meta 致力于在 2030 年实现净零排放，并旨在为整个行业创建测量碳排放的通用分类法。为实现这一目标，Meta 在 2025 年 OCP 区域 EMEA 峰会上提出了一种新方法，该方法利用人工智能 (AI) 来改进对 IT 硬件范围 3 排放的理解。Meta 正与 OCP PCR 工作流合作，计划在 2025 年 OCP 全球峰会上开源此方法论。\n\n**IT 硬件范围 3 排放估算的挑战**\n\n*   理解服务器硬件的碳足迹对于可持续采购和设计至关重要。\n*   挑战在于复杂的供应链和供应商有限的数据，使得精确计算碳足迹变得困难。\n*   数据中心 IT 硬件的制造和运输所产生的隐含碳是主要的排放源，且难以量化。\n\n**Meta 的排放估算方法论**\n\nMeta 开发了一种方法来估算和追踪数据中心数亿个组件的碳排放。该方法结合了：\n\n*   **基于成本的估算**\n*   **模型估算**\n*   **组件特定的产品碳足迹 (PCF)**\n\n这些组件级别的估算根据数据质量进行排名，并聚合到服务器机架级别。这种方法允许 Meta 从单个螺丝到整个机架组件进行多级别粒度的排放分析，从而识别高影响的减排区域。最终目标是推动行业采用更可持续的制造实践，并生产低排放组件。\n\n**AI 在改进范围 3 排放估算中的应用**\n\nMeta 利用 AI 改进其数据库并理解 IT 硬件相关的范围 3 排放，主要通过以下方式：\n\n1.  **识别相似组件并应用现有 PCF**\n    *   **问题**：PCF 耗时且通常与特定标识符绑定，但库存中存在大量相似或变体组件。若不识别这些相似组件，其碳足迹估算数据质量会较低。\n    *   **方法**：Meta 采用自然语言处理 (NLP) 算法（如 TF-IDF 和余弦相似度）分析文本描述，识别相似组件。当收到新的 PCF 时，算法会识别同类别中描述相似的组件，并将其映射到代表性 PCF。对于数据质量低的组件，算法会寻找高质量的参考数据来改进估算。\n    *   **目的**：确保高质量 PCF 数据在所有相似组件中得到有效利用，提高数据准确性和一致性，并增强碳足迹估算的追溯性。\n\n2.  **从异构数据源提取数据**\n    *   **问题**：当 PCF 不可用时，Meta 避免使用基于支出的方法。参数化模型需要一致的输入数据，但信息可能存储在不同的表格、格式或单位中，导致难以应用模型。\n    *   **方法**：Meta 使用大型语言模型 (LLM)，特别是 Llama 3.1，从异构数据源中提取相关信息，并将其注入参数化模型。LLM 能够识别相同信息的不同表示形式。\n    *   **应用**：该方法已应用于内存（提取容量）和电缆（提取长度/类型）的排放计算，确保估算的一致性。\n\n3.  **创建 IT 硬件排放的组件级分类法**\n    *   **问题**：不同供应商的物料清单 (BOM) 结构不同，使得识别减排行动变得困难。\n    *   **方法**：Meta 使用 AI 将机架的描述性数据分类为两个层次：\n        *   **领域级**：机架的主要功能分组（如计算、网络、电源、机械、存储）。\n        *   **组件级**：主要排放源组件（如 CPU、GPU、DRAM、闪存）。\n    *   **过程**：在探索阶段，生成式 AI (GenAI) 模型自由识别潜在类别。与内部硬件专家审查后，确定了主要组件的固定列表，然后使用严格的 GenAI 分类器进行分组，生成互斥的层次结构。\n    *   **目的**：为碳足迹分析提供帮助，并推动行业采用通用的 IT 硬件碳足迹分类法，以便比较不同类型和世代硬件的排放。\n\n**未来计划：开源分类法和方法论**\n\nMeta 计划开源其学习成果和方法论，包括：\n\n*   服务器机架排放核算的分类法和方法论。\n*   使用 GenAI 分类器的分类法构建器。\n*   改进全行业设施报告流程的聚合方法论。\n\nMeta 致力于与 OCP PCR 组织合作，共同发展和分享这些方法论。",
      "shortSummary": "Meta 致力于在 2030 年实现净零排放，并正利用 AI 提高 IT 硬件范围 3 排放估算的质量。面对供应链复杂和数据有限的挑战，Meta 开发了一种结合成本、模型和产品碳足迹 (PCF) 的方法。AI 在此过程中发挥关键作用：通过 NLP 识别相似组件以应用 PCF；利用 LLM 从异构数据源提取信息；并使用生成式 AI 创建 IT 硬件的通用分类法，以识别减排热点。Meta 计划开源这些分类法和方法论，以推动行业标准化和可持续实践。",
      "translated_title": "Meta 如何利用人工智能提高 IT 硬件范围 3 排放估算的质量",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>As we focus on our goal of achieving net zero emissions in 2030, we also aim to create a common taxonomy for the entire industry to measure carbon emissions. We’re sharing details on a new methodology we presented at the 2025 OCP regional EMEA summit that leverages AI to improve our understanding of our IT [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/14/data-center-engineering/how-meta-is-leveraging-ai-to-improve-the-quality-of-scope-3-emission-estimates-for-it-hardware/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/14/data-center-engineering/how-meta-is-leveraging-ai-to-improve-the-quality-of-scope-3-emission-estimates-for-it-hardware/\">How Meta Is Leveraging AI To Improve the Quality of Scope 3 Emission Estimates for IT Hardware</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "OCP 峰会 2025：AI 网络硬件的开放未来 (原标题: OCP Summit 2025: The Open Future of Networking Hardware for AI)",
      "link": "https://engineering.fb.com/2025/10/13/data-infrastructure/ocp-summit-2025-the-open-future-of-networking-hardware-for-ai/",
      "pubDate": "Tue, 14 Oct 2025 00:00:20 +0000",
      "isoDate": "2025-10-14T00:00:20.000Z",
      "creator": "",
      "summary": "# OCP 峰会 2025：AI 网络硬件的开放未来\n\n在2025年开放计算项目（OCP）峰会上，Meta详细介绍了其AI训练集群下一代网络架构的发展方向。Meta扩展了其网络硬件产品组合，并向OCP贡献了新的解耦网络平台，旨在通过开放机架、服务器、存储盒和主板设计，促进整个行业的创新和协作。\n\n## 开放硬件与AI创新\nMeta坚信开放硬件是创新的催化剂，尤其是在数据中心基础设施日益支持新兴AI技术的情况下。开放硬件通过实现解耦，将传统数据中心技术分解为核心组件，从而构建更灵活、可扩展和高效的系统。自2011年共同创立OCP以来，Meta一直分享数据中心和组件设计，并开源其网络操作系统FBOSS，以推动行业内外的创新。\n\n## 数据中心网络新里程碑\nMeta宣布了其数据中心网络的几项重要进展：\n\n### 1. 解耦调度结构（DSF）的演进\n*   **规模扩展**：DSF已演进为支持跨越整个数据中心建筑的大型AI集群的横向扩展互连。\n*   **双阶段架构**：去年的DSF是基于VOQ的系统，支持开放标准的以太网RoCE接口。现在已发展为双阶段架构，可扩展支持多达18,432个XPU的无阻塞网络，是构建跨区域AI集群的基础。\n*   **大规模部署**：Meta已利用双阶段DSF构建了规模达整个数据中心建筑的18,000个GPU集群。\n\n### 2. 非调度结构（NSF）架构\n*   **新架构**：与DSF演进并行，Meta开发了新的非调度结构（NSF）架构。\n*   **关键特性**：\n    *   基于浅缓存OCP以太网交换机。\n    *   提供低往返延迟。\n    *   支持自适应路由，实现有效负载均衡，优化利用率并最小化拥塞。\n    *   作为Prometheus等千兆瓦级AI集群的基础构建模块。\n*   **三层NSF**：用于构建规模化AI集群。\n\n### 3. 新一代AI网络OCP交换机平台\n*   **现有平台**：去年Meta推出了Minipack3（基于Broadcom Tomahawk5）和Cisco 8501（基于Cisco Silicon One G200）两款51T以太网交换机，它们提供51.2 Tbps带宽，无需重定时器，并运行FBOSS。\n*   **Minipack3N**：今年推出基于NVIDIA Spectrum-4交换ASIC的新型51T以太网交换机Minipack3N，采用与Minipack3相同的系统设计，由Meta设计并由Accton制造。\n\n### 4. FBOSS和SAI的演进\n*   **开放基础**：Meta持续将OCP-SAI作为新网络结构、交换机硬件平台和光收发器集成到FBOSS的基础。\n*   **协作创新**：通过与供应商和OCP社区的紧密合作，SAI已演进以支持DSF、NSF以及为现代数据中心和AI工作负载量身定制的其他增强路由方案等高级功能。这种开放方法加速了整个行业的进步。\n\n### 5. 400G/800G光互连技术\n*   **现有部署**：去年推出的2x400G FR4 BASE（3公里）光模块已在Meta数据中心广泛部署。\n*   **新产品**：\n    *   **2x400G FR4 LITE（500米）**：为数据中心内部大多数用例优化，旨在加速光模块成本降低。\n    *   **400G DR4 OSFP-RHS**：Meta首代用于AI主机侧NIC连接的DR4解决方案。\n    *   **2x400G DR4 OSFP**：部署在交换机侧，提供从主机到交换机的连接。\n\n## OCP中的“以太网用于横向扩展网络”（ESUN）倡议\nMeta是OCP 2025全球峰会期间启动的“以太网用于横向扩展网络”（ESUN）倡议的创始参与者。\n\n*   **ESUN是什么？** ESUN是OCP网络项目中的一个新工作流，作为一个开放技术论坛，供行业运营商和领先供应商协作推进以太网技术的使用。其目标是利用和调整成熟的以太网生态系统，以满足现代AI系统中横向扩展领域独特的高性能需求。\n*   **关注点**：ESUN专注于横向扩展系统的网络功能方面，解决数据流量在网络交换机中管理和传输的技术挑战，包括定义协议头、错误处理机制和实现网络无损数据传输的最佳实践和标准。\n*   **合作与标准**：该倡议汇集了运营商、供应商和标准机构，共同开发适用于横向扩展网络的以太网解决方案，专注于以太网帧和交换层，确保强大、无损和容错的多跳拓扑，并与UEC和IEEE等组织紧密合作，与开放标准保持一致。\n*   **Meta的贡献**：Meta作为ESUN的初始成员之一，与AMD、Arista、ARM、Broadcom、Cisco、HPE、Marvell、Microsoft、NVIDIA、OpenAI和Oracle等行业领导者共同推动该倡议。Meta的贡献包括在定义AI集群ESUN需求方面的技术领导、与供应商和标准机构的开放协作，以及分享在Meta数据中心部署先进以太网网络方面的最佳实践。\n\n## 行业邀请：加入开放未来\nMeta邀请工程师、开发人员和行业合作伙伴加入OCP社区，共同塑造AI的下一代网络硬件。通过协作和分享想法，可以加速开放、面向未来的AI基础设施的开发，从而造福整个行业并支持未来技术的需求。",
      "shortSummary": "Meta在OCP峰会2025上发布了AI训练集群网络硬件的重大更新，包括DSF/NSF架构、Minipack3N交换机及新光模块。同时，Meta启动了“以太网用于横向扩展网络（ESUN）”倡议，旨在推动AI数据中心基础设施的创新、效率和可扩展性。",
      "translated_title": "OCP 峰会 2025：AI 网络硬件的开放未来",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>At Open Compute Project Summit (OCP) 2025, we’re sharing details about the direction of next-generation network fabrics for our AI training clusters. We’ve expanded our network hardware portfolio and are contributing new disaggregated network platforms to OCP. We look forward to continued collaboration with OCP to open designs for racks, servers, storage boxes, and motherboards [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/13/data-infrastructure/ocp-summit-2025-the-open-future-of-networking-hardware-for-ai/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/13/data-infrastructure/ocp-summit-2025-the-open-future-of-networking-hardware-for-ai/\">OCP Summit 2025: The Open Future of Networking Hardware for AI</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "介绍 React 基金会：React 和 React Native 的新家 (原标题: Introducing the React Foundation: The New Home for React & React Native)",
      "link": "https://engineering.fb.com/2025/10/07/open-source/introducing-the-react-foundation-the-new-home-for-react-react-native/",
      "pubDate": "Tue, 07 Oct 2025 18:00:01 +0000",
      "isoDate": "2025-10-07T18:00:01.000Z",
      "creator": "",
      "summary": "# 介绍 React 基金会：React 和 React Native 的新家\n\nMeta 宣布将 React、React Native 以及 JSX 等支持项目移交给新成立的 React 基金会，标志着 React 生态系统发展的一个重要里程碑。\n\n## React 的发展历程与影响力\n*   **起源与增长**：Meta 在十多年前开源了 React，旨在帮助开发者构建更好的用户体验。如今，React 已成为全球最受欢迎的开源项目之一，为超过 5000 万个网站和产品提供支持，这些产品来自微软、Shopify、彭博社、Discord、Coinbase、NFL 等众多公司。\n*   **平台扩展**：通过 React Native，React 的应用范围已超越了网页，扩展到移动设备、平板电脑、桌面应用、电视、游戏机乃至混合现实设备。\n*   **社区贡献**：React 的巨大成功离不开数千名教育工作者、公司和项目对其开发的贡献。社区是 React 的核心，推动了开源创新。\n\n## React 基金会的成立与使命\n*   **新家**：React 及其生态系统中的多个项目（包括 React、React Native 和 JSX）将过渡到 React 基金会。\n*   **使命**：React 基金会的任务是支持 React 社区及其成员，具体包括：\n    *   维护 React 的基础设施。\n    *   组织 React Conf 大会。\n    *   发起支持 React 生态系统的各项倡议。\n*   **隶属关系**：React 基金会将成为 Linux 基金会的一部分。Linux 基金会长期以来为开源项目提供了一个供应商中立的环境。\n\n## 治理结构\n*   **理事会**：React 基金会的理事会将由来自亚马逊、Callstack、Expo、Meta、微软、Software Mansion 和 Vercel 的代表组成，并计划未来进一步扩大。\n*   **业务与技术分离**：React 的业务治理和技术治理之间将有明确的分离。\n*   **技术治理**：发布、功能和技术方向将由一个新的结构来管理，该结构由 React 的维护者和贡献者驱动，并且独立于 React 基金会。React 团队正在积极制定这一新的技术治理结构，并将在未来的 React 博客文章中分享更多细节。\n\n## Meta 的承诺与未来展望\n*   **五年合作**：Meta 承诺与 React 基金会建立为期五年的合作伙伴关系。\n*   **资金与支持**：Meta 将提供超过 300 万美元的资金和专门的工程支持，以确保 React 平稳过渡到独立治理，同时保持社区所期望的稳定性和创新性。\n*   **持续投入**：Meta 将继续投资 React，并将其作为在网页和 Meta 众多应用中构建 UI 的主要工具。Meta 也将继续拥有一支全职致力于 React 和 React Native 的工程师团队。\n*   **未来机遇**：React 基金会的成立将为协作、创新和增长开启新的机遇，造福整个生态系统。通过强化的治理、更广泛的行业参与和持续的技术卓越，React 有望应对 UI 开发领域的下一代挑战。",
      "shortSummary": "Meta 宣布将 React、React Native 及相关项目移交给新成立的 React 基金会。该基金会隶属于 Linux 基金会，旨在维护 React 生态系统，并由亚马逊、Meta、微软等公司代表组成的理事会管理。Meta 承诺五年内提供超 300 万美元资金和工程支持，确保 React 的独立治理和持续发展。此举旨在促进更广泛的行业参与、协作与创新，应对 UI 开发的未来挑战。",
      "translated_title": "介绍 React 基金会：React 和 React Native 的新家",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>Meta open-sourced React over a decade ago to help developers build better user experiences. Since then, React has grown into one of the world’s most popular open source projects, powering over 50 million websites and products built by companies such as Microsoft, Shopify, Bloomberg, Discord, Coinbase, the NFL, and many others. With React Native, React [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/07/open-source/introducing-the-react-foundation-the-new-home-for-react-react-native/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/07/open-source/introducing-the-react-foundation-the-new-home-for-react-react-native/\">Introducing the React Foundation: The New Home for React &#038; React Native</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    },
    {
      "title": "介绍 OpenZL：一个开源的格式感知压缩框架 (原标题: Introducing OpenZL: An Open Source Format-Aware Compression Framework)",
      "link": "https://engineering.fb.com/2025/10/06/developer-tools/openzl-open-source-format-aware-compression-framework/",
      "pubDate": "Mon, 06 Oct 2025 16:00:53 +0000",
      "isoDate": "2025-10-06T16:00:53.000Z",
      "creator": "",
      "summary": "# 介绍 OpenZL：一个开源的格式感知压缩框架\n\nOpenZL 是 Meta 推出的一款新型开源数据压缩框架，专为结构化数据提供无损压缩。它旨在结合格式特定压缩器的卓越性能与单一可执行二进制文件的易于维护性。\n\n## OpenZL 的核心理念与背景\n\n*   **挑战与机遇：** 尽管 Zstandard 等通用压缩工具在数据中心工作负载中表现出色，但它们在处理结构化数据时，由于无法充分利用数据内部的特定模式，仍有大量压缩潜力未被挖掘。\n*   **定制化压缩器的困境：** 格式特定的压缩器能显著提升压缩比和速度，但为每种数据格式开发、部署和维护独立的压缩器和解压缩器会带来巨大的运维负担。\n*   **OpenZL 的解决方案：** OpenZL 应运而生，旨在解决这一矛盾，它通过一种创新的方法，在实现定制化性能的同时，保持了单一二进制文件的简洁性。\n\n## OpenZL 的工作原理\n\nOpenZL 的核心在于将数据结构作为显式输入参数，而非让压缩器猜测。\n\n1.  **显式结构输入：** 用户通过预设或简洁的格式描述（如 Simple Data Description Language, SDDL）向 OpenZL 提供数据形状。SDDL 仅用于解析，描述字节如何映射到字段（行、列、枚举、嵌套记录）。用户也可以编写自定义解析函数。\n2.  **离线训练器：** 一个离线优化组件（训练器）根据数据形状和样本，运行预算搜索，探索不同的转换选择和参数，以构建一个高效的压缩配置（“计划”）。它能提供一系列速度/压缩比的权衡，或直接针对特定速度约束下的最佳配置。\n3.  **编码时解析：** 在编码过程中，编码器将“计划”转换为具体的“解析图”（decode recipe），并将其嵌入到压缩帧中。如果“计划”包含控制点，编码器会根据数据特性选择最合适的路径并记录下来。\n4.  **通用解压缩器：** 所有的 OpenZL 文件都可以使用同一个通用解压缩器进行解压缩。解压缩器直接执行帧中嵌入的“解析图”，无需任何带外信息。\n\n## 性能示例：与通用压缩器的对比\n\n以压缩 Silesia 压缩语料库中的 `sao` 文件为例，该文件包含描述恒星的记录数组。通过向 OpenZL 提供其结构信息，OpenZL 展现出优于通用无损压缩器的性能：\n\n| 压缩器    | 压缩大小     | 压缩比 | 压缩速度 | 解压缩速度 |\n| :-------- | :----------- | :----- | :------- | :--------- |\n| zstd -3   | 5,531,935 B  | x1.31  | 220 MB/s | 850 MB/s   |\n| xz -9     | 4,414,351 B  | x1.64  | 3.5 MB/s | 45 MB/s    |\n| **OpenZL** | **3,516,649 B** | **x2.06** | **340 MB/s** | **1200 MB/s** |\n\nOpenZL 在实现更高压缩比的同时，保持甚至提升了压缩和解压缩速度，这对于数据中心处理管道至关重要。\n\n## 示例解析：OpenZL 如何利用结构\n\n在 `sao` 文件的压缩中，OpenZL 的处理流程包括：\n\n1.  **结构分解：** 将文件头与主体（大型结构表）分离。\n2.  **字段提取：** 将结构数组转换为数组结构，把每个字段提取到独立的流中。\n3.  **同质流处理：** 每个流包含同质数据，OpenZL 为其寻找最佳压缩策略：\n    *   **SRA0 (X轴位置)：** 索引大多有序，使用差分编码（delta）减小值范围。\n    *   **SDEC0 (Y轴位置)：** 虽不如X轴有序，但有明确的上下限，利用转置操作（transpose）使高位字节更可预测。\n    *   **其他字段 (IS, MAG, XRPM, XDPM)：** 基数远低于数量，且连续值间无关联，适合使用 `tokenize` 转换为字典和索引列表，再分别进行压缩。\n4.  **自动化与精细化：** 离线训练器在此阶段发挥作用，为每个流生成并优化特定的压缩策略。\n\n## 适应数据演变：再训练与运行时控制\n\n在实际应用中，数据结构和内容不断变化。OpenZL 通过其灵活的压缩计划来应对：\n\n*   **持续训练：** OpenZL 提供训练过程，根据提供的数据样本更新压缩计划，以维持或提升压缩性能。Meta 内部的“托管压缩”系统利用此功能，定期对注册用例进行监控、采样和再训练，并部署新的配置。\n*   **运行时自适应（控制点）：** 压缩配置可以包含控制点，在压缩时读取轻量级统计信息（如字符串重复、游程长度、直方图偏斜、差分方差），并选择“计划”中最佳的分支。这使得 OpenZL 能够动态适应数据变化、处理突发情况和异常值，而无需进行无限制的搜索，同时不增加解压缩器的复杂性。\n\n## 通用解压缩器的优势\n\nOpenZL 最大的运营优势在于其单一的通用解压缩器二进制文件，即使压缩配置改变，解压缩器也无需更新。\n\n*   **单一审计面：** 安全和正确性审查集中在一个二进制文件上，简化了审计流程。\n*   **全舰队范围的改进：** 解压缩器（如安全更新、性能优化）的任何更新都能惠及所有已压缩文件，包括旧数据。\n*   **操作清晰性：** 跨数据集使用相同的二进制文件、命令行接口、指标和仪表板，简化了修补和部署。\n*   **持续改进：** 可以在系统运行时不断训练和部署新的压缩计划，旧数据仍可解码，新数据则获得更好的压缩效果，实现了领域特定压缩而不碎片化生态系统。\n\n## OpenZL 的适用场景与局限性\n\n*   **适用场景：** OpenZL 特别适用于压缩向量、表格或树形结构数据，以及数值、字符串或二进制数据。常见示例包括时间序列数据集、机器学习张量和数据库表。前提是输入数据具有可被揭示的内部秩序。\n*   **局限性：** 当数据不包含可利用的结构时（例如纯文本文件），OpenZL 的优势不复存在，此时它会回退到 Zstandard，提供大致相同的性能水平。\n\n## 如何开始使用 OpenZL\n\n用户可以通过访问 OpenZL 网站和快速入门指南开始使用。源代码、文档和示例可在 GitHub 仓库中找到。OpenZL 欢迎社区的贡献和反馈。未来，OpenZL 将继续致力于简化结构暴露，并利用自动化压缩计划来处理不断演变的数据。",
      "shortSummary": "OpenZL 是一个开源的格式感知数据压缩框架，为结构化数据提供无损压缩。它通过显式利用数据结构，实现格式特定压缩器的性能，同时保持单一通用解压缩器的易维护性。OpenZL 包含离线训练器自动生成优化压缩计划，并支持运行时自适应。与通用压缩器相比，OpenZL 能在更高压缩比的同时保持或提升速度，特别适用于向量、表格和树形结构数据。其通用解压缩器简化了部署和维护，并支持持续改进。",
      "translated_title": "介绍 OpenZL：一个开源的格式感知压缩框架",
      "images": [],
      "contentSource": "RSS",
      "content": "<p>OpenZL is a new open source data compression framework that offers lossless compression for structured data. OpenZL is designed to offer the performance of a format-specific compressor with the easy maintenance of a single executable binary. You can get started with OpenZL today by visiting our Quick Start guide and the OpenZL GitHub repository. Learn more [...]</p>\n<p><a class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2025/10/06/developer-tools/openzl-open-source-format-aware-compression-framework/\">Read More...</a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://engineering.fb.com/2025/10/06/developer-tools/openzl-open-source-format-aware-compression-framework/\">Introducing OpenZL: An Open Source Format-Aware Compression Framework</a> appeared first on <a rel=\"nofollow\" href=\"https://engineering.fb.com\">Engineering at Meta</a>.</p>\n"
    }
  ],
  "lastUpdated": "2025-10-27T05:27:48.574Z"
}