{
  "sourceUrl": "https://machinelearningmastery.com/blog/feed/",
  "title": "MachineLearningMastery.com",
  "description": "Making developers awesome at machine learning",
  "link": "https://machinelearningmastery.com/blog/",
  "items": [
    {
      "title": "位置编码中的插值与使用 YaRN 扩展上下文窗口 (原标题: Interpolation in Positional Encodings and Using YaRN for Larger Context Window)",
      "link": "https://machinelearningmastery.com/interpolation-in-positional-encodings-and-using-yarn-for-larger-context-window/",
      "pubDate": "Tue, 17 Jun 2025 05:00:10 +0000",
      "isoDate": "2025-06-17T05:00:10.000Z",
      "creator": "Adrian Tam",
      "summary": "# 位置编码中的插值与使用 YaRN 扩展上下文窗口\n\n本文探讨了 Transformer 模型在处理不同长度序列时，如何应对位置编码带来的挑战。由于 Transformer 模型通常在固定序列长度下训练，但在推理时可能需要处理更长的序列，这使得模型难以处理未在训练中遇到的位置编码。\n\n## 概述\n\n文章分为三个主要部分：\n1.  正弦编码和 RoPE 中的插值与外推\n2.  学习型编码中的插值\n3.  使用 YaRN 扩展上下文窗口\n\n## 正弦编码和 RoPE 中的插值与外推\n\n正弦编码（Sinusoidal encodings）由于使用连续函数，在**外推**方面表现出色。对于更长的序列，可以直接将位置 `p` 替换为更大的值来获取位置编码。\n\n此外，也可以使用**插值**。将 `p` 视为浮点数，通过 `p = (L/L')p'` 的方式，将新序列 `L'` 中的位置 `p'` 映射到原始训练长度 `L` 的范围内。\n\n这些技术同样适用于旋转位置编码（RoPE）。生成正弦位置编码或 RoPE 的函数无需修改即可处理任意长度的序列。然而，为了确保模型能够处理新的编码而性能不下降，可能需要进行微调。例如，Llama 模型使用 RoPE，最大训练序列长度为 16K，而 Code Llama 仅通过 1000 步微调就将其序列长度扩展到了 100K token。\n\n## 学习型编码中的插值\n\n学习型位置编码（Learned positional encodings）通过查找表获取位置编码向量，这意味着序列长度由查找表大小固定，因此无法进行**外推**。\n\n然而，学习型编码仍然可以通过**插值**来处理比训练长度更长的序列。对于长度为 `L'` (大于 `L`) 的序列，位置 `p'` 的编码向量可以通过线性插值计算，即在原始训练长度 `L` 范围内找到对应的浮点位置 `p`，然后插值其相邻的两个整数位置 `n` 和 `m` 的编码向量。\n\n尽管可以实现插值，但文章指出，这种基本插值实现并不能保证模型在不进行再训练的情况下，处理更长序列时性能不会下降。\n\n## 使用 YaRN 扩展上下文窗口\n\n旋转位置编码（RoPE）是当前大型语言模型中最广泛使用的位置编码方法。YaRN 是一种旨在扩展 RoPE 以处理更长序列的方法，并且比上述基本插值方法更有效。\n\nYaRN 的核心创新在于，当序列长度从 `L` 扩展到 `L'` 时，它会**不均匀地缩放 RoPE 正弦频率**。这种方法被称为“NTK-by-parts”插值。YaRN 通过引入一个混合了插值和外推效果的 `θ_i'` 来修改原始 RoPE 公式。此外，YaRN 还引入了一个额外的缩放因子 `sqrt(1/t)`，进一步提升了模型在更长上下文长度下的性能（通过降低困惑度衡量）。\n\n## 总结\n\n本文介绍了模型如何处理比训练时更长的输入序列：\n*   正弦编码和 RoPE 易于进行外推。\n*   学习型编码仅支持插值。\n*   YaRN 提供了一种先进的方法，用于将 RoPE 扩展到更长的序列长度。\n\n这些方法的共同目标是使模型能够在不进行再训练的情况下处理更长的输入序列。\n\n![Building Transformer Models with Attention](https://machinelearningmastery.com/wp-content/uploads/2022/11/BTMA-400.png)",
      "shortSummary": "本文探讨了Transformer模型如何处理不同长度序列的位置编码挑战。正弦编码和RoPE擅长外推，也可通过插值处理更长序列，但可能需微调。学习型编码仅支持插值。YaRN是一种先进方法，通过不均匀缩放RoPE频率并引入额外缩放因子，有效扩展RoPE的上下文窗口，提升模型在长序列下的性能。目标是使模型无需再训练即可处理更长输入。",
      "translated_title": "位置编码中的插值与使用 YaRN 扩展上下文窗口",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/enkuu-smile_-kbHvA6oXP8E-unsplash-scaled.jpg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2022/11/BTMA-400.png",
          "alt": "Building Transformer Models with Attention",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "This post is divided into three parts; they are: • Interpolation and Extrapolation in Sinusoidal Encodings and RoPE • Interpolation in Learned Encodings • YaRN for Larger Context Window Sinusoidal encodings excel at extrapolation due to their use of continuous functions: $$ \\begin{aligned} PE(p, 2i) &amp;= \\sin\\left(\\frac{p}{10000^{2i/d}}\\right) \\\\ PE(p, 2i+1) &amp;= \\cos\\left(\\frac{p}{10000^{2i/d}}\\right) \\end{aligned} $$ You can simply substitute $p$ with a larger value to obtain the positional encoding for a longer sequence."
    },
    {
      "title": "如何结合Scikit-learn、CatBoost和SHAP构建可解释的树模型 (原标题: How to Combine Scikit-learn, CatBoost, and SHAP for Explainable Tree Models)",
      "link": "https://machinelearningmastery.com/how-to-combine-scikit-learn-catboost-and-shap-for-explainable-tree-models/",
      "pubDate": "Mon, 16 Jun 2025 12:00:01 +0000",
      "isoDate": "2025-06-16T12:00:01.000Z",
      "creator": "Vinod Chugani",
      "summary": "## 如何结合Scikit-learn、CatBoost和SHAP构建可解释的树模型\n\n### 引言\n\n在机器学习工作流中，高性能模型与可解释性之间存在着微妙的平衡。CatBoost等高性能算法虽然能取得卓越成果，但其“黑箱”特性使得利益相关者难以理解模型决策背后的“为什么”。本文旨在通过结合Scikit-learn、CatBoost和SHAP这三个互补的库来解决这一挑战，从而实现模型的准确性和可解释性。\n\n### 核心整合库\n\n*   **Scikit-learn**：提供预处理生态系统和评估框架，是大多数机器学习工作流的基础。\n*   **CatBoost**：提供最先进的梯度提升性能，并原生支持分类特征处理。\n*   **SHAP (SHapley Additive exPlanations)**：将高性能预测转化为透明、可量化的解释。\n\n### 实际应用场景\n\n本教程以Ames Housing数据集为例，预测房屋价格。这是一个完美的用例，因为在房地产领域，专业人士不仅需要知道模型预测的价格，还需要确切了解哪些特征以及它们如何驱动这些预测。\n\n### 学习目标\n\n通过本教程，您将学会：\n*   创建从Scikit-learn预处理到CatBoost建模再到SHAP详细解释的无缝数据管道。\n*   比较不同的特征重要性方法。\n*   解释复杂的特征交互。\n*   量化邻里效应等分类变量的影响。\n*   构建既准确又可解释的树模型框架。\n\n### 先决条件\n\n在开始本教程之前，您需要：\n*   安装Python 3.7或更高版本。\n*   熟悉Python语法和编程概念。\n*   安装以下库：\n    *   Pandas (1.3.0或更高版本)\n    *   NumPy (1.20.0或更高版本)\n    *   scikit-learn (1.0.0或更高版本)\n    *   CatBoost (1.0.0或更高版本)\n    *   SHAP (0.40.0或更高版本)\n    *   Matplotlib (3.3.0或更高版本) 用于可视化。\n*   对机器学习概念（如回归、训练/测试分割、模型评估）有基本了解。\n\n### 构建CatBoost基础模型\n\n在进行解释之前，首先需要一个高性能模型。文章基于之前对CatBoost的探索，重新构建了一个针对Ames Housing数据集的优化回归模型，实现了0.9310的R²分数。该模型展示了CatBoost在不进行大量预处理的情况下，原生处理缺失值和分类数据的能力。这个高性能模型为后续的解释工作奠定了可靠的基础。\n\n### 整合点1：Scikit-learn → CatBoost 工作流\n\n本节展示了Scikit-learn的预处理和评估工具如何与CatBoost无缝协作。尽管CatBoost可以自动处理许多预处理任务，但结合Scikit-learn可以利用更广泛的数据科学工具生态系统，并建立可扩展到更复杂管道的模式。通过Scikit-learn进行数据分割（训练集2063个样本，测试集516个样本），并在训练数据上训练CatBoost模型，最终在测试集上获得了0.9335的R²分数，验证了模型的强大性能，为SHAP解释提供了可靠的基础。\n\n### 整合点2：CatBoost → SHAP 解释\n\nSHAP将高性能的CatBoost模型转化为可解释的系统。与传统的特征重要性（平均而言哪些变量最重要）不同，SHAP通过量化每个特征对每个个体预测的贡献来提供更深入的洞察。这不仅揭示了哪些特征重要，还揭示了它们在不同上下文和值范围内的行为方式。\n\n**特征重要性比较：CatBoost vs. SHAP**\n\n*   **SHAP TreeExplainer**：计算所有516个测试预测的精确解释，涵盖84个特征。\n*   **共同点**：SHAP和CatBoost的重要性排名都将“GrLivArea”（居住面积）和“OverallQual”（整体质量）列为最重要的特征。\n*   **差异**：在中间排名中存在显著差异，例如“Neighborhood”（社区）在SHAP中排名第三，而在CatBoost中排名第四，反之“TotalBsmtSF”（地下室总面积）则相反。这突出了一个关键区别：CatBoost的重要性反映了特征在树分裂中被使用的频率，而SHAP的重要性则衡量了特征对最终预测的实际影响大小。\n*   **量化影响**：SHAP条形图清晰地可视化了特征影响的程度，显示“GrLivArea”的平均影响几乎是其他任何特征的两倍。这意味着居住面积的变化对预测的影响大约是整体质量变化的两倍，是社区效应的近三倍。\n\n![SHAP Global Feature Importance Plot](https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-chugani-combine-sklearn-catboost-shap-explainable-tree-models-1.png)\n\n### 通过依赖图理解特征交互\n\n全局重要性排名无法揭示特征在不同值范围内的行为或它们之间的交互。SHAP依赖图通过展示特征值与其对个体预测影响之间的关系来解决这些限制。这使得分析从“OverallQual很重要”深入到“OverallQual显示阶梯式增长，其影响取决于其他房屋特征”。\n\n**交互作用示例：**\n\n*   **居住面积（GrLivArea）与地下室总面积（TotalBsmtSF）的交互**：\n    *   当居住面积超过2300平方英尺时，地下室面积越大（红色越深），房屋的SHAP值（即对价格的贡献）越高。这表明买家重视全面的空间，而不仅仅是地上建筑面积。\n*   **整体质量（OverallQual）与建造年份（YearBuilt）的交互**：\n    *   虽然质量始终驱动价值，但颜色模式表明质量评级可能根据房屋建造年份具有不同的含义或市场价值。这反映了随着时间推移，建筑标准和买家期望的变化。\n\n![SHAP Dependence Plots: Standalone vs Interactive Effects](https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-chugani-combine-sklearn-catboost-shap-explainable-tree-models-2-1024x813.png)\n\n这些依赖图揭示了CatBoost学习到的关于特征关系的复杂模式，并展示了SHAP依赖分析如何超越基本的特征重要性，提供更深层次的洞察。",
      "shortSummary": "本文介绍了如何结合Scikit-learn、CatBoost和SHAP库来构建高性能且可解释的树模型。Scikit-learn用于预处理和评估，CatBoost提供强大的梯度提升性能，而SHAP则负责将模型预测转化为透明的解释。通过Ames Housing数据集的案例，文章展示了如何利用SHAP量化特征对个体预测的影响，比较特征重要性，并深入理解特征间的复杂交互，从而揭示“黑箱”模型内部的决策机制，为实际应用提供更深层次的洞察。",
      "translated_title": "如何结合Scikit-learn、CatBoost和SHAP构建可解释的树模型",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-chugani-combine-sklearn-catboost-shap-explainable-tree-models-feature.png",
          "alt": "How to Combine Scikit-learn, CatBoost, and SHAP for Explainable Tree Models",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-chugani-combine-sklearn-catboost-shap-explainable-tree-models-1.png",
          "alt": "How to Combine Scikit-learn, CatBoost, and SHAP for Explainable Tree Models",
          "title": "",
          "position": 2
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-chugani-combine-sklearn-catboost-shap-explainable-tree-models-2-1024x813.png",
          "alt": "How to Combine Scikit-learn, CatBoost, and SHAP for Explainable Tree Models",
          "title": "",
          "position": 3
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-chugani-combine-sklearn-catboost-shap-explainable-tree-models-3-1024x910.png",
          "alt": "How to Combine Scikit-learn, CatBoost, and SHAP for Explainable Tree Models",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "完整文章",
      "content": "Machine learning workflows often involve a delicate balance: you want models that perform exceptionally well, but you also need to understand and explain their predictions."
    },
    {
      "title": "Transformer模型中的位置编码 (原标题: Positional Encodings in Transformer Models)",
      "link": "https://machinelearningmastery.com/positional-encodings-in-transformer-models/",
      "pubDate": "Sun, 15 Jun 2025 05:06:17 +0000",
      "isoDate": "2025-06-15T05:06:17.000Z",
      "creator": "Adrian Tam",
      "summary": "# Transformer模型中的位置编码\n\n本文深入探讨了Transformer模型中至关重要的位置编码，它们是捕获语言序列特性的关键创新。\n\n![图片 1](https://machinelearningmastery.com/wp-content/uploads/2025/06/svetlana-gumerova-6fAgnT3Dhl4-unsplash-scaled.jpg)\n*语言模型中的位置编码*\n\n## 理解位置编码的必要性\n\n*   **问题背景**：传统的循环神经网络（RNN）按顺序处理词语，自然地捕捉了词序信息。然而，Transformer模型并行处理所有词元，导致其无法区分词元相同但顺序不同的句子（例如：“狐狸跳过狗”和“狗跳过狐狸”）。\n*   **解决方案**：位置编码通过为序列中的每个词元提供位置信息来解决这一问题。\n*   **工作原理**：\n    *   每个词元首先通过模型的嵌入层转换为一个向量，其大小称为“隐藏维度”。\n    *   位置编码会生成一个与隐藏维度相同大小的向量。\n    *   这些位置编码向量被添加到输入词元嵌入中，通常在注意力模块中进行。\n    *   在点积操作期间，这些编码强调了相邻词元之间的关系，帮助模型理解上下文，从而区分词序不同的句子。\n\n## 常见位置编码类型\n\n文章详细介绍了以下五种主要的位置编码类型：\n\n1.  **正弦位置编码 (Sinusoidal Positional Encodings)**：\n    *   **特点**：在原始Transformer论文中引入，使用正弦和余弦函数构建的确定性向量。\n    *   **公式**：\n        $$ PE(p, 2i) = \\sin\\left(\\frac{p}{10000^{2i/d}}\\right) $$\n        $$ PE(p, 2i+1) = \\cos\\left(\\frac{p}{10000^{2i/d}}\\right) $$\n        其中，$p$ 是位置，$i$ 是维度索引，$d$ 是隐藏维度。常数10000应大于最大序列长度。\n    *   **实现原理**：通过计算 `div_term` ($1/N^{2i/d}$)，然后将 `position` 矩阵与 `div_term` 相乘的结果分别代入正弦和余弦函数，最后交错填充到输出矩阵中。\n    *   **优点**：确定性，能够外推到训练时未见的更长序列；利用正弦函数的性质，可以轻松计算词元间的相对位置。\n    *   **局限性**：不适应数据特性，对非常长的序列可能效果不佳。\n\n2.  **学习位置编码 (Learned Positional Encodings)**：\n    *   **特点**：在BERT和GPT等模型中使用，位置向量在训练过程中学习得到。\n    *   **实现原理**：通过 `nn.Embedding` 层实现，该层充当一个查找表，将整数位置索引映射到学习到的向量。在 `forward` 函数中，根据输入序列长度生成位置索引，然后通过嵌入层查找对应的位置编码，并将其添加到输入 `x` 中。\n    *   **优点**：能够适应数据特性，如果训练得当，可能提供更好的性能。\n    *   **局限性**：无法外推到更长的序列；可能存在过拟合风险；作为模型参数的一部分，会增加模型大小。\n\n3.  **旋转位置编码 (Rotary Positional Encodings, RoPE)**：\n    *   **特点**：Llama模型等大多数现代大型语言模型使用，通过旋转矩阵编码相对位置，每个位置代表一个几何级数的角度。\n    *   **公式**：\n        $$ \\hat{x}_m^{(i)} = x_m^{(i)} \\cos(m\\theta_i) + x_m^{(d/2+i)} \\sin(m\\theta_i) $$\n        $$ \\hat{x}_m^{(d/2+i)} = x_m^{(d/2+i)} \\cos(m\\theta_i) – x_m^{(i)} \\sin(m\\theta_i) $$\n        其中 $\\theta_i = 10000^{-2i/d}$，$m$ 是位置索引。\n        矩阵形式：$\\mathbf{\\hat{x}}_m = \\mathbf{R}_m\\mathbf{x}_m$。\n    *   **实现原理**：`rotate_half` 函数将向量的后半部分取负并与前半部分交换位置。`apply_rotary_pos_emb` 函数将输入 `x` 与预先计算好的 `cos` 和 `sin` 值进行旋转操作。`RotaryPositionalEncoding` 类预计算并缓存正弦和余弦值。\n    *   **优点**：\n        *   旋转矩阵 $\\mathbf{R}_m$ 几何地旋转2D输入向量。\n        *   转置 $\\mathbf{R}_m^\\top = \\mathbf{R}_m^{-1}$ 表示反向旋转，因此相对位置可以轻松计算为 $\\mathbf{R}_{m-n} = \\mathbf{R}_m\\mathbf{R}_n^\\top$。\n        *   由于角度的几何级数，可以外推到更长的序列。\n        *   保留向量范数，有助于训练稳定性。\n\n4.  **相对位置编码 (Relative Positional Encodings)**：\n    *   **特点**：T5和MPT等模型使用，关注词元之间的相对距离而非绝对位置。\n    *   **实现原理**：计算 `context_position` 和 `memory_position` 矩阵的差值得到 `relative_position` 矩阵，表示词元 $i$ 和 $j$ 之间的相对位置。通过 `relative_position_bucket` 将值转换为非负，然后从 `relative_attention_bias` 张量中查找对应的位置编码向量。\n    *   **优点**：自然处理可变长度序列，适用于翻译等任务。\n    *   **相关方法**：**线性偏置注意力 (Attention with Linear Bias, ALiBi)** 是一种相关方法，它不是修改输入序列，而是将一个偏置矩阵添加到注意力分数中，该偏置矩阵基于词元距离按比例缩放。缩放因子为 $m_h=1/2^{8h/H}$。\n\n## 总结\n\n本文详细介绍了Transformer模型中位置编码的重要性及其不同类型：\n\n*   位置编码对于Transformer模型至关重要，因为它们以并行方式处理词元，需要额外的位置信息来理解序列顺序。\n*   不同类型的位置编码各有优缺点：\n    *   **正弦编码**：确定性，可外推到更长序列。\n    *   **学习编码**：简单，但无法外推。\n    *   **RoPE**：在长序列上表现更好，保留向量范数。\n    *   **相对位置编码**：关注词元间距离，适用于可变长度序列。\n\n![Building Transformer Models with Attention](https://machinelearningmastery.com/wp-content/uploads/2022/11/BTMA-400.png)\n*构建带有注意力的Transformer模型*",
      "shortSummary": "Transformer模型并行处理词元，缺乏序列顺序信息，因此需要位置编码。位置编码通过向词元嵌入添加位置向量来解决此问题，帮助模型理解上下文和词序。主要类型包括：原始Transformer使用的确定性正弦编码、BERT和GPT中学习到的编码、Llama模型中通过旋转矩阵实现的RoPE，以及T5等模型中基于词元间相对距离的相对位置编码。每种编码都有其独特优势，如外推能力或适应数据特性。",
      "translated_title": "Transformer模型中的位置编码",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/svetlana-gumerova-6fAgnT3Dhl4-unsplash-scaled.jpg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2022/11/BTMA-400.png",
          "alt": "Building Transformer Models with Attention",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "This post is divided into five parts; they are: • Understanding Positional Encodings • Sinusoidal Positional Encodings • Learned Positional Encodings • Rotary Positional Encodings (RoPE) • Relative Positional Encodings Consider these two sentences: \"The fox jumps over the dog\" and \"The dog jumps over the fox\"."
    },
    {
      "title": "使用Scikit-Learn管道、Pandas的ColumnTransformer和NumPy数组进行高级特征工程 (原标题: Advanced Feature Engineering Using Scikit-Learn Pipelines with Pandas’ ColumnTransformer and NumPy Arrays)",
      "link": "https://machinelearningmastery.com/advanced-feature-engineering-using-scikit-learn-pipelines-with-pandas-columntransformer-and-numpy-arrays/",
      "pubDate": "Fri, 13 Jun 2025 12:00:25 +0000",
      "isoDate": "2025-06-13T12:00:25.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "## 使用Scikit-Learn管道、Pandas的ColumnTransformer和NumPy数组进行高级特征工程\n\n![高级特征工程](https://machinelearningmastery.com/wp-content/uploads/2025/05/mlm-palomares-ollama-quantization-feature.png)\n\n本文介绍了如何利用Pandas、NumPy和Scikit-learn这三个强大的Python库，构建健壮、可扩展且模块化的高级特征工程工作流。文章通过一个实际示例，详细阐述了如何将这些工具结合起来，实现数据预处理和机器学习模型的无缝集成。\n\n### 核心组件\n\n*   **Scikit-learn 管道 (Pipelines)**：一种便捷的机制，用于定义一系列操作（通常是数据转换，也可包含机器学习模型），并将这些操作封装为一个单一的对象。这有助于简化工作流并提高代码的可读性。\n*   **Pandas ColumnTransformer**：一个专门的类，用于对数据集中特定列应用自定义的转换。它允许对不同类型的特征（如数值型和类别型）应用不同的预处理方法。\n*   **NumPy 数组 (Arrays)**：在目标问题中扮演关键角色，能够高效处理大量数据。Scikit-learn模型内部最终需要NumPy数组作为输入格式，而非Pandas DataFrame对象，因此NumPy数组是数据流转的关键。\n\n### 实践示例：构建特征工程管道\n\n文章通过一个自制数据集的例子，演示了如何构建一个完整的特征工程管道。\n\n#### 1. 导入必要的库\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\n```\n\n#### 2. 创建示例数据集\n\n为了演示，文章创建了一个包含20个实例的模拟公民数据DataFrame，包括数值型特征（`age`、`income`）和类别型特征（`gender`、`city`），以及一个二元目标标签（`label`）。\n\n```python\ndf = pd.DataFrame({\n    'age': np.random.randint(18, 70, size=20),\n    'income': np.random.randint(30000, 120000, size=20),\n    'gender': np.random.choice(['male', 'female'], size=20),\n    'city': np.random.choice(['NY', 'SF', 'LA'], size=20),\n    'label': np.random.choice([0, 1], size=20)\n})\n```\n\n#### 3. 分离特征与标签并定义特征列表\n\n将数据集分为特征 `X` 和标签 `y`，并分别定义数值型和类别型特征的名称列表，这对于后续 `ColumnTransformer` 的使用至关重要。\n\n```python\nX = df.drop('label', axis=1)\ny = df['label']\nnumeric_features = ['age', 'income']\ncategorical_features = ['gender', 'city']\n```\n\n![特征工程前的数据集特征](https://machinelearningmastery.com/wp-content/uploads/2025/05/Captura-de-pantalla-2025-05-29-a-las-9.59.41.png)\n\n#### 4. 定义预处理步骤\n\n文章定义了两个并行的管道，分别用于数值型和类别型特征的预处理，并通过 `ColumnTransformer` 将它们结合起来。\n\n*   **数值型转换器**：使用 `StandardScaler` 进行标准化。\n*   **类别型转换器**：使用 `OneHotEncoder` 进行独热编码，并设置 `handle_unknown='ignore'` 以处理未知类别。\n*   **预处理器 (preprocessor)**：一个 `ColumnTransformer` 对象，将上述两个管道并行应用于各自对应的特征列表。\n\n```python\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())\n])\ncategorical_transformer = Pipeline(steps=[\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ]\n)\n```\n\n#### 5. 应用预处理并查看结果\n\n通过 `preprocessor.fit_transform(X)` 应用定义的预处理管道，并将结果转换为DataFrame以便于可视化。\n\n```python\nX_preprocessed = preprocessor.fit_transform(X)\npreprocessed_feature_names = preprocessor.get_feature_names_out()\nX_preprocessed_df = pd.DataFrame(X_preprocessed, columns=preprocessed_feature_names)\nprint(\"Preprocessed Data:\")\nX_preprocessed_df.head()\n```\n\n![特征工程后的数据集特征](https://machinelearningmastery.com/wp-content/uploads/2025/05/Captura-de-pantalla-2025-05-29-a-las-9.59.48.png)\n\n#### 6. 将特征工程与机器学习模型结合\n\n最后，文章展示了如何将预处理管道 (`preprocessor`) 与一个机器学习模型（例如 `RandomForestClassifier`）组合成一个更高级别的端到端管道。这个管道包含两个顺序步骤：特征工程和模型训练。\n\n```python\nfeng_pipeline = Pipeline(steps=[\n    ('preprocessing', preprocessor),\n    ('classifier', RandomForestClassifier())\n])\n\nfeng_pipeline.fit(X, y)\npredictions = feng_pipeline.predict(X)\nprint(predictions)\n```\n\n这种集成方式使得数据从原始形式到模型训练的整个过程变得流畅，尤其得益于NumPy数组在预处理后作为数据结构传递给模型，无需额外的数据结构修改。\n\n### 总结\n\n本文详细阐述了如何利用Scikit-learn的 `Pipeline` 和 Pandas 的 `ColumnTransformer` 对象，结合 NumPy 数组，对包含不同类型特征的数据集执行高级且定制化的特征工程过程。这种方法不仅提高了工作流的效率和可维护性，也确保了数据在整个机器学习流程中的顺畅传递。",
      "shortSummary": "本文演示了如何使用Scikit-learn的Pipeline、Pandas的ColumnTransformer和NumPy数组进行高级特征工程。通过构建一个包含数据标准化、独热编码等预处理步骤的并行管道，并将其与机器学习模型（如随机森林）集成，文章展示了如何创建健壮、可扩展且模块化的端到端数据处理和建模工作流。NumPy数组在此过程中确保了数据在不同组件间的无缝传递。",
      "translated_title": "使用Scikit-Learn管道、Pandas的ColumnTransformer和NumPy数组进行高级特征工程",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/05/mlm-palomares-ollama-quantization-feature.png",
          "alt": "Advanced Feature Engineering Using Scikit-Learn Pipelines with Pandas’ ColumnTransformer and NumPy Arrays",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/05/Captura-de-pantalla-2025-05-29-a-las-9.59.41.png",
          "alt": "Dataset features before feature engineering",
          "title": "",
          "position": 2
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/05/Captura-de-pantalla-2025-05-29-a-las-9.59.48.png",
          "alt": "Dataset features after feature engineering",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "完整文章",
      "content": "Pandas , NumPy , and Scikit-learn ."
    },
    {
      "title": "使用 Pandas 和 Scikit-learn 处理不平衡数据集 (原标题: Navigating Imbalanced Datasets with Pandas and Scikit-learn)",
      "link": "https://machinelearningmastery.com/navigating-imbalanced-datasets-with-pandas-and-scikit-learn/",
      "pubDate": "Thu, 12 Jun 2025 12:00:56 +0000",
      "isoDate": "2025-06-12T12:00:56.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "# 使用 Pandas 和 Scikit-learn 处理不平衡数据集\n\n## 引言\n不平衡数据集在现实世界中并不少见，例如银行金融领域的欺诈检测（欺诈交易远少于合法交易）和医疗诊断（罕见疾病远少于常见健康状况）。这类数据集的主要问题在于，机器学习模型容易偏向多数类，导致模型性能下降，甚至可能退化为“虚拟分类器”。本文将介绍使用 Python 的 Pandas 和 Scikit-learn 库处理不平衡数据集的几种策略。\n\n![Navigating Imbalanced Datasets with Pandas and Scikit-learn](https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-ipc-navigating-imbalanced-datasets.png)\n*图：使用 Pandas 和 Scikit-learn 处理不平衡数据集*\n\n## 实践指南：银行营销数据集\n为了演示处理不平衡数据的实际方法，本文使用了公开可用的银行营销数据集。该数据集包含银行客户信息，并标记了客户是否在接到银行营销电话后订阅了定期存款（“是”或“否”）。\n\n**数据集不平衡性：**\n该数据集之所以不平衡，是因为只有约 11% 的客户订阅了定期存款，而约 89% 的客户拒绝了。具体来说：\n*   **未订阅（“no”）:** 39922 名客户 (88.3%)\n*   **已订阅（“yes”）:** 5289 名客户 (11.7%)\n这表明“yes”类是显著的少数类。\n\n## 处理不平衡数据集的策略\n\n### 策略 1：逆频率依赖加权 (Inverse Frequency-Dependent Weighting)\n这是 Scikit-learn 提供的一种策略，通过调整分类模型训练时的实例权重来处理不平衡数据。\n\n*   **原理：** 在 Scikit-learn 的某些分类器中（例如 `RandomForestClassifier`），设置 `class_weight='balanced'` 参数。这会使实例权重与类频率成反比，从而给予少数类更大的权重，以补偿类不平衡。\n*   **优点：** 模型在训练时会更加关注少数类，减少对多数类的偏向。\n*   **示例：** 使用 `RandomForestClassifier` 并设置 `class_weight='balanced'`，同时对分类特征进行独热编码（使用 Pandas 的 `pd.get_dummies()`）。\n\n### 策略 2：欠采样 (Undersampling)\n这是一种在模型训练前进行的数据预处理策略，主要通过 Pandas 实现。\n\n*   **原理：** 减少多数类实例的数量，使其与少数类实例的数量相匹配。\n*   **优点：** 减少模型对多数类的偏向。\n*   **缺点：**\n    *   可能导致信息丢失，特别是当多数类被大量欠采样时。\n    *   可能增加模型方差，有时甚至导致欠拟合。\n*   **适用场景：** 当数据集足够大，即使欠采样后仍能保留足够代表性和多样性的实例时。\n*   **示例：** 将多数类（“no”）的实例数量随机减少到与少数类（“yes”）相同的数量，然后将欠采样后的多数类与少数类合并，形成一个平衡的数据集。原始数据集约 45K 实例，欠采样后平衡数据集约 10.5K 实例。\n\n### 策略 3：过采样 (Oversampling)\n这也是一种在模型训练前进行的数据预处理策略，主要通过 Pandas 实现。\n\n*   **原理：** 通过随机复制少数类实例（带替换采样）来增加其数量，使其与多数类数量相匹配。\n*   **优点：** 有助于缓解模型对多数类的偏向。\n*   **缺点：**\n    *   如果少数类本身不具代表性，或者重复实例可能引入噪声，则不适用。\n    *   可能导致过拟合，因为模型可能会过度学习重复的少数类实例。\n*   **适用场景：** 当少数类规模较小但具有代表性，且添加重复实例不太可能引入噪声或导致过拟合时。\n*   **示例：** 将少数类（“yes”）的实例数量随机复制增加到与多数类（“no”）相同的数量，然后将多数类与过采样后的少数类合并，形成一个平衡的数据集。\n\n## 总结\n本文探讨了数据集中的类不平衡问题，并介绍了使用 Pandas 和 Scikit-learn 库处理该问题的三种常用策略：训练平衡分类模型（逆频率依赖加权）、欠采样和过采样。值得注意的是，还有其他更高级的策略，如 Scikit-learn 的重采样工具和 SMOTE（合成少数类过采样技术）等。",
      "shortSummary": "不平衡数据集（如欺诈检测）会导致机器学习模型偏向多数类，影响性能。本文介绍了使用 Pandas 和 Scikit-learn 处理此类问题的三种策略：一是通过 Scikit-learn 的 `class_weight='balanced'` 参数进行逆频率依赖加权，使模型更关注少数类；二是通过 Pandas 进行欠采样，减少多数类实例；三是通过 Pandas 进行过采样，复制少数类实例。这些方法旨在平衡数据分布，提升模型在不平衡数据集上的表现。",
      "translated_title": "使用 Pandas 和 Scikit-learn 处理不平衡数据集",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-ipc-navigating-imbalanced-datasets.png",
          "alt": "Navigating Imbalanced Datasets with Pandas and Scikit-learn",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "Imbalanced datasets, where a majority of the data samples belong to one class and the remaining minority belong to others, are not that rare."
    },
    {
      "title": "使用FastAPI和Docker部署机器学习模型的逐步指南 (原标题: Step-by-Step Guide to Deploying Machine Learning Models with FastAPI and Docker)",
      "link": "https://machinelearningmastery.com/step-by-step-guide-to-deploying-machine-learning-models-with-fastapi-and-docker/",
      "pubDate": "Wed, 11 Jun 2025 16:17:27 +0000",
      "isoDate": "2025-06-11T16:17:27.000Z",
      "creator": "Bala Priya C",
      "summary": "## 使用FastAPI和Docker部署机器学习模型的逐步指南\n\n![使用FastAPI和Docker部署机器学习模型的逐步指南](https://machinelearningmastery.com/wp-content/uploads/2025/06/step-by-step-deploying-ml-models-docker.png)\n\n本文提供了一个将机器学习模型部署到生产环境的详细指南，通过构建一个糖尿病进展预测器并将其封装为可部署的API。该教程涵盖了从模型训练到使用FastAPI构建REST API，再到使用Docker进行容器化，并最终发布到Docker Hub的完整流程。\n\n### 1. 项目概述与目标\n\n*   **目标**：将训练好的机器学习模型部署为可供实际用户使用的生产级API。\n*   **项目**：基于scikit-learn的样本数据集构建一个糖尿病进展预测器。\n*   **最终成果**：\n    *   一个训练好的随机森林模型，用于预测糖尿病进展分数。\n    *   一个使用FastAPI构建的REST API，接受患者数据并返回预测结果。\n    *   一个完全容器化的应用程序，可随时部署到云端。\n\n### 2. 开发环境设置\n\n*   **先决条件**：Python 3.11+ (或 3.9+), Docker已安装并运行，对Python和API有基本了解。\n*   **项目结构**：\n    *   `diabetes-predictor/` (根目录)\n        *   `app/` (FastAPI应用)\n            *   `__init__.py`\n            *   `main.py`\n        *   `models/` (存放训练好的模型)\n            *   `diabetes_model.pkl`\n        *   `train_model.py` (模型训练脚本)\n        *   `requirements.txt` (Python依赖)\n        *   `Dockerfile` (容器配置)\n*   **安装依赖**：\n    1.  创建并激活虚拟环境：`python -m venv diabetes-env`，然后激活。\n    2.  安装所需库：`pip install scikit-learn pandas fastapi uvicorn`。\n\n### 3. 构建机器学习模型 (`train_model.py`)\n\n*   **模型选择**：使用`RandomForestRegressor`，因其鲁棒性、对不同特征尺度的良好处理能力以及提供特征重要性洞察。\n*   **数据集**：加载scikit-learn的糖尿病数据集，包含442条患者记录和10个生理特征，目标是衡量一年后的疾病进展。\n*   **数据准备**：将数据集按80/20比例划分为训练集和测试集 (`random_state=42`确保结果可复现)。\n*   **模型训练**：\n    *   初始化`RandomForestRegressor`，设置`n_estimators=100`, `random_state=42`, `max_depth=10`（防止过拟合）。\n    *   使用训练数据拟合模型。\n*   **模型评估**：\n    *   在测试集上进行预测。\n    *   计算均方误差 (MSE) 和 R² 分数。R² 分数高于0.4被认为是该数据集的良好表现。\n*   **模型保存**：将训练好的模型保存为`models/diabetes_model.pkl`。\n*   **运行脚本**：`python3 train_model.py`。\n\n### 4. 创建FastAPI应用程序 (`app/main.py`)\n\n*   **API结构**：\n    *   创建`app`目录和`__init__.py`。\n    *   导入`FastAPI`, `BaseModel` (Pydantic), `pickle`, `numpy`, `os`。\n*   **输入数据结构**：\n    *   使用Pydantic的`BaseModel`定义`PatientData`类，包含10个浮点型生理特征（如age, sex, bmi等）。\n    *   提供`schema_extra`示例，帮助API用户理解输入格式。\n*   **FastAPI初始化与模型加载**：\n    *   初始化`FastAPI`应用，设置标题、描述和版本。\n    *   加载之前保存的`diabetes_model.pkl`。\n*   **预测端点 (`/predict`)**：\n    *   定义POST请求端点，接受`PatientData`作为输入。\n    *   将输入转换为NumPy数组。\n    *   使用加载的模型进行预测。\n    *   返回包含`predicted_progression_score`（四舍五入到两位小数）和`interpretation`（通过`get_interpretation`函数提供）的JSON响应。\n*   **解释函数 (`get_interpretation`)**：根据预测分数提供人类可读的解释（低于平均、平均、高于平均进展）。\n*   **健康检查端点 (`/`)**：定义GET请求端点，返回API状态和模型信息。\n\n### 5. 本地测试API\n\n*   **运行命令**：在项目根目录运行`uvicorn app.main:app --reload --port 8000`。\n*   **访问**：在浏览器中打开`http://localhost:8000/`。\n*   **测试预测**：使用`curl`命令发送POST请求及示例数据进行测试，验证API功能。\n\n### 6. 使用Docker进行容器化\n\n*   **创建`requirements.txt`**：列出所有Python依赖及其精确版本，确保环境一致性。\n*   **创建`Dockerfile`**：\n    *   基于`python:3.11-slim`镜像，保持容器小巧。\n    *   设置工作目录为`/app`。\n    *   安装系统依赖（如果需要）。\n    *   复制`requirements.txt`并安装Python依赖（使用`--no-cache-dir`减少镜像大小）。\n    *   复制`app/`和`models/`目录到容器中。\n    *   暴露端口8000。\n    *   定义容器启动命令：`uvicorn app.main:app --host 0.0.0.0 --port 8000`。\n*   **构建Docker镜像**：`docker build -t diabetes-predictor .`。\n*   **运行Docker容器**：`docker run -d -p 8000:8000 diabetes-predictor`。\n\n### 7. 发布到Docker Hub\n\n*   **目的**：将容器化应用共享，便于云平台部署。\n*   **设置Docker Hub**：注册Docker Hub账户。\n*   **登录**：在终端运行`docker login`并输入凭据。\n*   **标记镜像**：\n    *   `docker tag diabetes-predictor your-username/diabetes-predictor:v1.0`\n    *   `docker tag diabetes-predictor your-username/diabetes-predictor:latest`\n*   **推送镜像**：\n    *   `docker push your-username/diabetes-predictor:v1.0`\n    *   `docker push your-username/diabetes-predictor:latest`\n*   **验证**：停止本地容器，然后从Docker Hub拉取并运行镜像，再次测试API以确保其正常工作。\n\n### 8. 总结与展望\n\n本文成功构建了一个完整的机器学习部署流程：训练了随机森林模型，使用FastAPI创建了REST API，并用Docker进行了容器化。该模型现在已准备好部署到AWS ECS、Fargate、Google Cloud或Azure等云平台。未来的改进方向包括添加认证和速率限制、模型监控和日志记录以及批量预测端点。",
      "shortSummary": "本文提供了一个使用FastAPI和Docker部署机器学习模型的逐步指南。教程从训练一个预测糖尿病进展的随机森林模型开始，接着使用FastAPI构建一个REST API，实现数据输入和预测输出。随后，详细介绍了如何使用Dockerfile将整个应用容器化，并将其发布到Docker Hub，以便于云端部署。最终，用户将获得一个功能完整的、可生产部署的机器学习API。",
      "translated_title": "使用FastAPI和Docker部署机器学习模型的逐步指南",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/step-by-step-deploying-ml-models-docker.png",
          "alt": "Step-by-Step Guide to Deploying Machine Learning Models with FastAPI and Docker",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "You've trained your machine learning model, and it's performing great on test data."
    },
    {
      "title": "从零开始实现向量搜索：一步步教程 (原标题: Implementing Vector Search from Scratch: A Step-by-Step Tutorial)",
      "link": "https://machinelearningmastery.com/implementing-vector-search-from-scratch-a-step-by-step-tutorial/",
      "pubDate": "Tue, 10 Jun 2025 14:49:47 +0000",
      "isoDate": "2025-06-10T14:49:47.000Z",
      "creator": "Kanwal Mehreen",
      "summary": "# 从零开始实现向量搜索：一步步教程\n\n## 搜索的演进与向量搜索的兴起\n\n文章指出，搜索是计算领域最基本的问题之一。传统的关键词搜索方法（如基于词频和稀有度）存在局限性，它们过于字面化，无法理解用户查询的上下文和真实意图。例如，搜索“汽车修理”可能无法匹配到“车辆维护”的文档。为了弥补这一语义鸿沟，向量搜索应运而生。\n\n向量搜索通过将查询和文档转换为数值向量（高维数组，捕捉文本的语义本质）来匹配含义，而非精确的关键词。它在向量空间中寻找与查询向量距离最近的文档向量，从而返回上下文相关的结果。本教程旨在从头开始构建一个向量搜索系统，帮助读者深入理解其工作原理。\n\n## 向量搜索的核心工作原理\n\n向量搜索主要包含三个核心步骤：\n\n1.  **向量表示（Vector Representation）**：\n    *   将数据（如文本、图像）转换为数值向量。\n    *   常用技术包括词嵌入（Word Embeddings）或神经网络。\n    *   每个向量在高维空间中代表数据的语义。\n2.  **相似度计算（Similarity Calculation）**：\n    *   衡量查询向量与数据集中其他向量的“接近”程度。\n    *   常用度量包括余弦相似度（Cosine Similarity）或欧几里得距离（Euclidean Distance）。\n    *   向量越接近，表示相似度越高。\n3.  **检索（Retrieval）**：\n    *   根据相似度得分返回最相似的 Top-k 项。\n    *   例如，查询“机器学习”会找到与“人工智能”或“深度学习”相关的文档。\n\n## 从零开始构建向量搜索系统（Python 实现）\n\n文章通过一个 Python 示例，逐步演示了如何构建一个简单的向量搜索系统：\n\n### 步骤 1：环境设置\n\n*   使用 NumPy 进行向量操作，Matplotlib 进行可视化。\n*   为保持“从零开始”的理念，避免使用 FAISS 或 spaCy 等外部高级库。\n*   词嵌入将通过一个预定义的字典模拟，而非使用 Word2Vec、GloVe 或 BERT 等预训练模型。\n*   导入必要的库：`numpy`、`matplotlib.pyplot`、`collections.defaultdict`、`re`。\n\n### 步骤 2：创建示例数据集和词嵌入\n\n*   **示例数据集**：包含关于技术的几句话，例如：“Machine learning is powerful”、“Artificial intelligence advances rapidly”等。\n*   **简化 2D 词嵌入**：创建一个字典，将每个词映射到一个 2D 向量。这些向量是任意的，但设计上使得相关词（如“machine”和“neural”）在向量空间中彼此接近，便于可视化。\n\n### 步骤 3：将句子转换为向量\n\n*   定义 `tokenize` 函数：将文本转换为小写并分词。\n*   定义 `sentence_to_vector` 函数：通过平均句子中所有词的词向量来将句子转换为单个向量。如果词不在嵌入字典中，则使用零向量。\n*   将所有文档转换为对应的向量表示 (`doc_vectors`)。\n\n### 步骤 4：实现余弦相似度\n\n*   余弦相似度是衡量向量间角度的常用指标，非常适合比较文本嵌入的语义相似性。\n*   计算公式为两个向量的点积除以它们各自范数的乘积。\n*   函数会处理零向量以避免除以零的错误。\n\n### 步骤 5：构建向量搜索函数\n\n*   实现核心的 `vector_search` 函数，该函数接受查询、文档、嵌入和 `top_k` 参数。\n*   将查询转换为向量。\n*   计算查询向量与每个文档向量之间的余弦相似度。\n*   使用 `np.argsort` 对相似度进行排序，并返回 `top_k` 个最相似的文档及其得分。\n*   **示例查询**：“Machine learning technology”的搜索结果展示了语义相关性，即使文档中不包含确切的短语。\n\n```\nQuery: Machine learning technology\nTop results:\nScore: 1.000, Document: Machine learning is powerful\nScore: 0.999, Document: Deep learning transforms technology\nScore: 0.997, Document: Artificial intelligence advances rapidly\n```\n\n### 步骤 6：向量可视化\n\n*   通过 `plot_vectors` 函数将文档向量（蓝色点）和查询向量（红色星号）在 2D 空间中可视化。\n*   可视化结果清晰地展示了相似项如何在向量空间中聚类，以及查询向量如何靠近语义相关的文档向量。\n\n![从零开始实现向量搜索](https://machinelearningmastery.com/wp-content/uploads/2025/06/Vector-Search-from-Scratch.png)\n*图：从零开始实现向量搜索* \n\n![向量搜索：文档和查询向量的可视化](https://machinelearningmastery.com/wp-content/uploads/2025/06/plot.png)\n*图：向量搜索：文档和查询向量的可视化*\n\n## 向量搜索对 RAG 的重要性\n\n文章强调，在检索增强生成（RAG）系统中，向量搜索是检索步骤的基石。它通过将文档和查询转换为向量，即使对于复杂的查询也能获取上下文相关的准确信息。本教程的简单实现模拟了这一过程：查询向量检索语义上接近的文档，这些文档随后可供语言模型用于生成响应。在实际应用中，虽然需要更高维的嵌入和优化的搜索算法（如 HNSW 或 IVF），但核心思想保持不变。\n\n## 结论\n\n本教程成功从零开始实现了向量搜索。读者可以基于此扩展，使用真实的词嵌入（如来自 Hugging Face Transformers）或通过近似最近邻（ANN）技术优化搜索，从而加深对向量搜索的理解。",
      "shortSummary": "向量搜索通过将查询和文档转换为数值向量并匹配其语义含义，克服了传统关键词搜索的局限性。文章详细介绍了向量搜索的核心原理：向量表示、相似度计算和检索。它通过一个Python教程，从零开始实现了向量搜索系统，包括环境设置、数据与嵌入创建、句子向量化、余弦相似度计算、搜索功能构建及向量可视化。该技术是RAG（检索增强生成）系统的关键组成部分，用于高效检索上下文相关信息。",
      "translated_title": "从零开始实现向量搜索：一步步教程",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/Vector-Search-from-Scratch.png",
          "alt": "Implementing Vector Search from Scratch",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/plot.png",
          "alt": "Output",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "There’s no doubt that search is one of the most fundamental problems in computing."
    },
    {
      "title": "如何优化部署语言模型的大小 (原标题: How to Optimize Language Model Size for Deployment)",
      "link": "https://machinelearningmastery.com/how-to-optimize-language-model-size-for-deployment/",
      "pubDate": "Mon, 09 Jun 2025 16:40:47 +0000",
      "isoDate": "2025-06-09T16:40:47.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "## 如何优化部署语言模型的大小\n\n大型语言模型（LLMs）已广泛应用于现代AI应用，从聊天机器人到企业自动化。然而，在部署这些模型时，其庞大的体积带来了挑战，需要在性能、可访问性、能耗和计算资源消耗之间取得平衡。本文探讨了优化模型大小的概念和实践策略。\n\n![如何优化部署语言模型的大小](https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-ipc-optimize-language-model-size-deployment.png)\n\n### 架构层面的LLM简化方法\n\n语言模型的规模呈指数级增长，例如GPT-2到GPT-4的参数量从15亿激增至超过1750亿。这种规模的扩大虽然带来了卓越的能力，但也导致了在设备端、云端和实时环境中高效部署的挑战，包括训练/微调成本、推理速度（延迟影响用户体验）以及量化带来的精度/鲁棒性权衡。模型大小可以通过多种架构策略进行优化：\n\n1.  **模型蒸馏（知识蒸馏）**\n    *   **原理**：采用“教师-学生”范式，训练一个较小的“学生”模型，使其通过观察“教师”模型生成的输出（如迭代的下一词预测结果和最可能词的概率分布）来学习。教师模型对每个输出的置信度是关键。\n    *   **目标**：在准确性和紧凑性之间取得平衡，通常通过损失函数来指导简化模型的训练过程。\n    *   **示例**：\n        ```python\n        output = teacher_model(input)\n        loss = distillation_loss(student_model(input), output)\n        ```\n\n2.  **模型剪枝**\n    *   **原理**：移除对模型输出贡献最小的权重（即值最低的权重），类似于决策树剪枝以降低复杂性。\n    *   **效果**：训练期间的动态稀疏技术允许模型学习保留或丢弃哪些层间连接。剪枝后得到的稀疏模型可以减少内存使用并可能提升计算速度。\n    *   **示例**：\n        ```python\n        import torch\n\n        def prune_small_weights(model, threshold=1e-3):\n            with torch.no_grad():\n                for name, param in model.named_parameters():\n                    if \"weight\" in name:\n                        mask = param.abs() > threshold\n                        param.mul_(mask) # zero out small weights\n        ```\n\n3.  **层级缩减**\n    *   **原理**：通过减少LLM底层Transformer架构中编码器和解码器的层数，使整体神经网络组件更浅。\n    *   **适用场景**：当语言任务不需要深层上下文推理，或延迟和资源限制优先于额外深度带来的边际效益时。\n    *   **应用**：可以在较高层面应用，移除部分重复的编码器或解码器层。\n    *   **示例**：\n        ```python\n        from transformers import BertModel\n\n        model = BertModel.from_pretrained(\"bert-base-uncased\")\n        # Keeping only the first 6 encoder layers (this Bert Model has 12)\n        model.encoder.layer = model.encoder.layer[:6]\n        ```\n\n4.  **模块化方法（如LoRA - Low-Rank Adaptation）**\n    *   **原理**：通过向冻结权重的预训练模型中注入轻量级、可训练的组件来简化模型适应。\n    *   **优势**：在资源受限和多任务环境中特别有效，减少了为每个任务微调或部署多个完整大小模型的需要。\n\n### 权重层面的优化\n\n与改变模型结构的架构方法不同，权重层面的优化不侧重于组件的硬性简化或权重消除，而是尝试压缩或数值近似权重，以生成更高效、可用于生产的模型。这些方法包括量化、权重共享和压缩编解码器，它们在通常对准确性影响最小的情况下，减少内存占用并提高推理速度。\n\n1.  **量化**\n    *   **原理**：降低权重的精度表示（例如，从32位降至8位）。\n    *   **优势**：在边缘和受限设备上，能显著加速模型的微调和推理。\n    *   **示例**：\n        ```python\n        import torch\n\n        model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n        quantized_model = torch.quantization.prepare(model, inplace=False)\n        quantized_model = torch.quantization.convert(quantized_model, inplace=False)\n        ```\n\n2.  **权重共享**\n    *   **原理**：利用张量分解将大型权重矩阵近似为较小的组件，从而减少冗余值。\n    *   **示例**：\n        ```python\n        import torch.nn as nn\n\n        original = nn.Linear(512, 512)\n        factorized = nn.Sequential(nn.Linear(512, 64), nn.Linear(64, 512))\n        ```\n\n3.  **压缩编解码器**\n    *   **原理**：一种算法方法，用于在模型的特定操作阶段（通常是模型存储和加载）压缩或解压缩权重。\n    *   **特点**：与模型量化不同，它们不会移除权重精度表示的一部分，并且可以在之后完全解压。\n    *   **示例**：\n        ```python\n        import torch\n        import zipfile\n\n        torch.save(model.state_dict(), \"model.pt\")\n        with zipfile.ZipFile(\"model.zip",
      "shortSummary": "本文探讨了优化大型语言模型（LLMs）部署大小的策略，以平衡性能、资源消耗和可访问性。主要方法分为两大类：架构层面的简化和权重层面的优化。架构方法包括知识蒸馏、模型剪枝、层级缩减和模块化方法（如LoRA），旨在改变模型结构。权重优化则通过量化、权重共享和压缩编解码器来压缩或近似权重，以减少内存占用并提高推理速度。这些技术对于LLM在实际生产环境中的高效运行至关重要。",
      "translated_title": "如何优化部署语言模型的大小",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-ipc-optimize-language-model-size-deployment.png",
          "alt": "How to Optimize Language Model Size for Deployment",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "The rise of language models, and more specifically large language models (LLMs), has been of such a magnitude that it has permeated every aspect of modern AI applications &mdash; from chatbots and search engines to enterprise automation and coding assistants."
    },
    {
      "title": "策略性处理缺失数据：Pandas和Scikit-learn中的高级插补技术 (原标题: Dealing with Missing Data Strategically: Advanced Imputation Techniques in Pandas and Scikit-learn)",
      "link": "https://machinelearningmastery.com/dealing-with-missing-data-strategically-advanced-imputation-techniques-in-pandas-and-scikit-learn/",
      "pubDate": "Fri, 06 Jun 2025 12:00:05 +0000",
      "isoDate": "2025-06-06T12:00:05.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "## 策略性处理缺失数据：Pandas和Scikit-learn中的高级插补技术\n\n![文章主图](https://machinelearningmastery.com/wp-content/uploads/2025/05/456XdBNIRKqg8jpZSt_wGA.jpeg)\n\n### 引言\n\n在许多真实世界的数据集中，缺失值普遍存在，原因多样，包括人为错误、数据损坏或不完整的数据收集过程（例如，来自带有可选字段的调查）。虽然存在处理缺失值的基本策略，如完全删除行或列，或用默认值（通常是属性的均值或中位数）进行插补，但这些策略有时不足以满足需求。本文介绍了通过结合使用Pandas和Scikit-learn库实现的一些高级缺失数据插补技术。\n\n### 使用合成员工数据集\n\n为了演示根据特定上下文和问题需求进行缺失值插补的高级策略，文章使用了一个合成创建的员工数据集。该数据集可以从指定的URL轻松加载，并用于后续的插补示例。\n\n### 高级插补技术\n\n文章详细介绍了三种高级插补方法：\n\n#### 1. 链式方程多重插补 (MICE)\n\n*   **原理**：MICE是一种迭代插补方法，它使用多种估计器（如随机森林、贝叶斯岭等）来插补缺失值。默认情况下，使用贝叶斯岭回归方法，该方法将缺失值视为待学习的参数。\n*   **实现**：通过Scikit-learn的`IterativeImputer`实现。文章展示了使用默认贝叶斯岭回归器和指定随机森林回归器进行插补的示例代码。\n*   **结果**：经过MICE处理后，数据集中的所有缺失值均被成功插补。\n\n![插补后数据集样本](https://machinelearningmastery.com/wp-content/uploads/2025/05/Captura-de-pantalla-2025-05-28-a-las-12.34.05.png)\n\n#### 2. K近邻 (KNN) 插补\n\n*   **原理**：与标准K-NN算法类似，此方法通过计算和利用样本间的相似性来估计给定实例中的缺失值。可以利用加权相似度和自定义度量。\n*   **实现**：通过Scikit-learn的`KNNImputer`实现。\n*   **示例**：\n    *   设置K=5，权重为'distance'（邻居对缺失值估计的贡献与它们和目标实例之间的距离成反比）。\n    *   设置K=10，权重为'uniform'（所有选定的邻居对缺失值的估计贡献相等）。\n\n#### 3. 多估计器（集成）插补\n\n*   **原理**：这种策略是构建多个不同类型的插补估计器，每个估计器都会生成一个带有插补值的完整数据集版本。然后，通过检查每个数据集并关注包含缺失值的最关键属性，可以根据哪个（或哪些）估计器为特定数据上下文提供最真实或最一致的插补结果来决定选择其中一个版本，甚至对两个或更多版本进行聚合。\n*   **实现**：文章展示了如何使用贝叶斯岭、Extra Trees和随机森林回归器作为估计器来创建多个插补数据集的示例。\n\n### 总结\n\n文章最后通过一个表格总结了所探讨的三种方法的特点，并提供了何时使用（或避免）每种方法的建议：\n\n![插补策略选择指南](https://machinelearningmastery.com/wp-content/uploads/2025/05/imputing_Table-scaled.png)\n\n*   **KNN插补**：非常适合小型数值数据集，但对于大型数据集计算成本高昂。\n*   **集成估计器**：通常提供最佳质量，但它们是最复杂且计算成本最高的方法。\n*   **MICE**：通常是一种平衡的方法，适用于各种场景。",
      "shortSummary": "本文探讨了使用Pandas和Scikit-learn处理缺失数据的高级插补技术，以克服传统方法的局限性。文章详细介绍了三种主要策略：链式方程多重插补（MICE）、K近邻（KNN）插补和多估计器（集成）插补。MICE是一种平衡的方法，KNN适用于小型数据集，而集成方法能提供最佳质量但计算成本最高。选择哪种方法取决于具体的数据特性和需求。",
      "translated_title": "策略性处理缺失数据：Pandas和Scikit-learn中的高级插补技术",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/05/456XdBNIRKqg8jpZSt_wGA.jpeg",
          "alt": "Dealing with Missing Data Strategically: Advanced Imputation Techniques in Pandas and Scikit-learn",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/05/Captura-de-pantalla-2025-05-28-a-las-12.34.05.png",
          "alt": "Dataset sample with imputed missing values",
          "title": "",
          "position": 2
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/05/imputing_Table-scaled.png",
          "alt": "When to use one imputation strategy or another.",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "完整文章",
      "content": "Missing values appear more often than not in many real-world datasets."
    },
    {
      "title": "损失函数详解：每种只需2分钟即可理解其数学原理 (原标题: Loss Functions Explained: Understand the Maths in Just 2 Minutes Each)",
      "link": "https://machinelearningmastery.com/loss-functions-explained-understand-the-maths-in-just-2-minutes-each/",
      "pubDate": "Thu, 05 Jun 2025 13:59:36 +0000",
      "isoDate": "2025-06-05T13:59:36.000Z",
      "creator": "Kanwal Mehreen",
      "summary": "# 损失函数详解：每种只需2分钟即可理解其数学原理\n\n![损失函数详解](https://machinelearningmastery.com/wp-content/uploads/2025/05/Loss-Functions-Explained.png)\n\n## 引言\n\n在机器学习领域，理解模型如何评估其预测的准确性至关重要。损失函数（Loss Function）正是实现这一目标的核心工具，它量化了模型预测值与真实值之间的差异。通过最小化损失函数，模型能够学习并优化其内部参数（权重和偏差），从而提高预测性能。\n\n## 损失函数与成本函数\n\n文章首先澄清了损失函数和成本函数（Cost Function）之间的常见混淆：\n*   **损失函数**：衡量单个数据点的预测误差。\n*   **成本函数**：衡量所有训练样本的平均损失。\n在模型训练过程中，通常是最小化成本函数，因为它反映了模型在整体数据上的表现。\n\n## 常见损失函数类型\n\n文章详细介绍了六种常用的损失函数，包括它们的数学原理、直观解释、适用场景及优缺点。\n\n### 1. 均方误差 (Mean Squared Error, MSE)\n\n*   **用途**：主要用于回归任务。\n*   **公式**：\n    $$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i – \\hat{y}_i)^2 $$\n    其中 \\( n \\) 是数据点数量，\\( y_i \\) 是真实值，\\( \\hat{y}_i \\) 是预测值。\n*   **直观解释**：计算真实值与预测值之差的平方的平均值。由于误差被平方，较大的误差会受到更严厉的惩罚。\n*   **优点**：\n    *   简单易懂，广泛使用。\n    *   对大错误有较强的惩罚作用，适用于对大偏差敏感的场景（如医疗剂量、金融预测）。\n*   **缺点**：\n    *   对异常值非常敏感，单个异常值可能显著影响损失，导致模型偏向拟合异常值。\n    *   损失的单位是原始单位的平方，直接解释性较差。\n\n### 2. 平均绝对误差 (Mean Absolute Error, MAE)\n\n*   **用途**：主要用于回归任务。\n*   **公式**：\n    $$ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i – \\hat{y}_i \\right| $$\n    其中 \\( n \\) 是数据点数量，\\( y_i \\) 是真实值，\\( \\hat{y}_i \\) 是预测值。\n*   **直观解释**：计算真实值与预测值之差的绝对值的平均值。所有误差都以线性方式贡献，无论大小。\n*   **优点**：\n    *   对异常值具有更好的鲁棒性，因为它不会平方误差。\n    *   损失的单位与原始数据单位一致，更具可解释性。\n*   **缺点**：\n    *   在误差为零时不可微，这可能使基于梯度的优化方法在训练过程中略显困难或缓慢。\n\n### 3. Huber 损失 (Huber Loss)\n\n*   **用途**：回归任务，结合了 MSE 和 MAE 的优点。\n*   **公式**：\n    $$ L_\\delta(y, \\hat{y}) = \\begin{cases} \\frac{1}{2}(y – \\hat{y})^2 & \\text{if } |y – \\hat{y}| \\leq \\delta \\\\ \\delta \\cdot \\left(|y – \\hat{y}| – \\frac{1}{2} \\delta\\right) & \\text{if } |y – \\hat{y}| > \\delta \\end{cases} $$\n    其中 \\( \\delta \\) 是一个阈值。\n*   **直观解释**：当误差较小时，行为类似于 MSE（平方误差）；当误差较大时，行为类似于 MAE（线性误差）。\n*   **优点**：\n    *   对异常值具有鲁棒性，同时在误差较小时保持可微性。\n    *   可以通过调整 \\( \\delta \\) 来控制对大误差的容忍度。\n*   **缺点**：\n    *   引入了一个新的超参数 \\( \\delta \\) 需要调优。\n    *   对于非常干净或非常嘈杂的数据集，可能不如纯粹的 MSE 或 MAE 表现好。\n\n### 4. 铰链损失 (Hinge Loss)\n\n*   **用途**：主要用于二分类问题，特别是支持向量机 (SVMs)，旨在实现“自信地正确”分类。\n*   **公式**：\n    $$ L(y, \\hat{y}) = \\max(0, 1 – y \\cdot \\hat{y}) $$\n    其中 \\( y \\) 是真实标签（期望为 -1 或 +1），\\( \\hat{y} \\) 是模型预测的原始分数。\n*   **直观解释**：当模型正确分类且具有足够大的决策边界时，损失为零。否则，损失为正，促使模型提高置信度。\n*   **优点**：\n    *   适用于 SVM 等旨在最大化决策边界的模型。\n    *   鼓励模型不仅正确分类，而且要“自信地”正确分类。\n*   **缺点**：\n    *   不适用于输出概率的模型。\n    *   在“铰链点”不可微，可能使优化复杂化。\n    *   要求真实标签为 -1 或 +1。\n\n### 5. 二元交叉熵 (Binary Cross-Entropy, BCE)\n\n*   **用途**：二分类问题，适用于模型输出概率的场景。\n*   **公式**：\n    $$ L = – \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 – y_i) \\log(1 – \\hat{y}_i) \\right] $$\n    其中 \\( y_i \\in \\{0, 1\\} \\) 是真实标签，\\( \\hat{y}_i \\in (0, 1) \\) 是预测概率。\n*   **直观解释**：当真实标签为 1 时，损失仅在预测概率接近 1 时才低；当真实标签为 0 时，损失仅在预测概率接近 0 时才低。对“自信但错误”的预测施加重罚。\n*   **优点**：\n    *   产生平滑的损失曲面，有利于基于梯度的优化算法。\n    *   非常适合模型输出概率且任务严格为二分类的情况。\n*   **缺点**：\n    *   可能过度惩罚噪声或错误标记的数据点。\n    *   在类别高度不平衡的数据集上可能表现不佳，除非采取额外处理措施。\n\n### 6. 分类交叉熵 (Categorical Cross-Entropy, CCE)\n\n*   **用途**：多分类问题，是二元交叉熵的扩展。\n*   **公式**：\n    $$ \\text{Loss} = – \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c}) $$\n    其中 \\( C \\) 是类别总数，\\( y_{i,c} \\in \\{0,1\\} \\) 是真实标签（独热编码），\\( \\hat{y}_{i,c} \\in (0,1) \\) 是预测概率。\n*   **直观解释**：本质上是提取真实类别的对数概率，并惩罚模型对真实类别预测概率较低的情况。\n*   **优点**：\n    *   适用于具有清晰、互斥类别的多分类问题。\n    *   对“自信但错误”的预测进行重罚，促使模型准确且谨慎。\n*   **缺点**：\n    *   不适用于多标签问题（一个样本可属于多个类别）。\n    *   在类别不平衡的数据集上可能需要额外的处理。",
      "shortSummary": "损失函数是机器学习中衡量模型预测误差的关键工具，通过最小化它来优化模型。损失函数衡量单个数据点误差，而成本函数衡量平均误差。文章介绍了六种常见损失函数：回归任务的均方误差（MSE）、平均绝对误差（MAE）和 Huber 损失；以及分类任务的铰链损失、二元交叉熵（BCE）和分类交叉熵（CCE）。每种损失函数都有其独特的数学原理、适用场景及优缺点，选择合适的损失函数对模型性能至关重要。",
      "translated_title": "损失函数详解：每种只需2分钟即可理解其数学原理",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/05/Loss-Functions-Explained.png",
          "alt": "Loss Functions Explained",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "I must say, with the ongoing hype around machine learning, a lot of people jump straight to the application side without really understanding how things work behind the scenes."
    }
  ],
  "lastUpdated": "2025-06-19T09:29:13.714Z"
}