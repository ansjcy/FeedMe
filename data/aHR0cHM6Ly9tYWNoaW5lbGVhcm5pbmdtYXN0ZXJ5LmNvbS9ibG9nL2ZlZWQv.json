{
  "sourceUrl": "https://machinelearningmastery.com/blog/feed/",
  "title": "MachineLearningMastery.com",
  "description": "Making developers awesome at machine learning",
  "link": "https://machinelearningmastery.com/blog/",
  "items": [
    {
      "title": "词嵌入在表格数据特征工程中的应用 (原标题: Word Embeddings for Tabular Data Feature Engineering)",
      "link": "https://machinelearningmastery.com/word-embeddings-for-tabular-data-feature-engineering/",
      "pubDate": "Fri, 11 Jul 2025 12:00:16 +0000",
      "isoDate": "2025-07-11T12:00:16.000Z",
      "creator": "Matthew Mayo",
      "summary": "### 词嵌入在表格数据特征工程中的应用\n\n**引言**\n\n词嵌入（Word Embeddings）作为词语的密集向量表示，通过量化捕捉词语间的语义关系，彻底改变了自然语言处理（NLP）领域。尽管其主要应用于传统语言处理任务，但本教程探讨了其在表格数据特征工程中的创新应用。传统表格数据中，分类特征通常通过独热编码或标签编码处理，但这两种方法无法捕捉类别间的语义相似性。例如，在产品类别中，“电子产品”和“小工具”可能比“电子产品”和“家具”更相似，而传统编码无法体现这一点。词嵌入则能有效表示这种语义关联，从而可能提升模型性能。\n\n本教程旨在指导读者如何利用预训练词嵌入为表格数据集生成新特征，特别关注表格数据中包含可映射到现有词嵌入的描述性文本的分类列。\n\n**核心概念**\n\n*   **词嵌入（Word Embeddings）**：词语在向量空间中的数值表示。语义相似的词语在该空间中距离更近。\n*   **Word2Vec**：由Google开发的一种流行词嵌入算法，主要架构包括连续词袋模型（CBOW）和Skip-gram。\n*   **GloVe（Global Vectors for Word Representation）**：另一种广泛使用的词嵌入模型，利用语料库中的全局词-词共现统计信息。\n*   **特征工程（Feature Engineering）**：将原始数据转换为能更好代表潜在问题的新特征，以提高机器学习模型性能的过程。\n\n本方法的核心是使用预训练的Word2Vec模型（例如在Google News上训练的模型），将分类文本条目转换为其对应的词向量。这些向量随后成为表格数据的新数值特征。当分类值具有可利用的内在文本含义时，此技术尤其有用。\n\n**实际应用：使用Word2Vec进行特征工程**\n\n教程以一个假设数据集为例，其中包含一个名为`ItemDescription`的列，用于描述商品。目标是使用预训练的Word2Vec模型将这些描述转换为数值特征。\n\n1.  **导入所需库**：`pandas`、`numpy`和`gensim.models.KeyedVectors`。\n2.  **模拟数据集**：创建一个包含`ItemID`、`Price`、`ItemDescription`（如'electronics', 'gadget', 'appliance'等）和`Sales`的Pandas DataFrame。\n3.  **加载预训练Word2Vec模型**：\n    *   尝试加载大型预训练模型（如`GoogleNews-vectors-negative300.bin`）。\n    *   如果文件不存在，则创建一个小型虚拟模型用于演示目的。\n4.  **创建获取词嵌入的函数**：\n    *   定义`get_word_embedding(description, model)`函数。\n    *   该函数查询模型以获取描述的嵌入向量。\n    *   如果词语未找到，则返回一个零向量。\n5.  **应用函数并生成新特征**：\n    *   确定嵌入维度（`model.vector_size`）。\n    *   为每个维度创建新的列名（例如`desc_embedding_0`, `desc_embedding_1`）。\n    *   将`get_word_embedding`函数应用于DataFrame的`ItemDescription`列。\n    *   将生成的嵌入向量扩展为独立的列，并与原始DataFrame（去除`ItemDescription`列）进行拼接，形成新的特征工程后的DataFrame。\n\n**总结**\n\n通过利用预训练词嵌入，教程成功地将一个分类文本特征转换为丰富的数值表示，该表示捕捉了语义信息。这组新特征可以输入到机器学习模型中，从而可能提高模型性能，尤其是在分类值之间的关系微妙且具有文本含义的任务中。\n\n需要注意的是，嵌入的质量很大程度上取决于所使用的预训练模型及其训练语料库。这项技术不仅限于产品描述，还可以应用于任何包含描述性文本的分类列，例如“职位”、“流派”或“客户反馈”（在适当的文本处理以提取关键词之后）。关键在于分类列中的文本应具有足够的意义，以便通过词嵌入进行有效表示。\n\n![Word Embeddings for Tabular Data Feature Engineering](https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-word-embeddings-tabular-data-feature-engineering.png)",
      "shortSummary": "本教程介绍了如何将词嵌入技术应用于表格数据进行特征工程。传统方法无法捕捉分类特征间的语义相似性，而词嵌入能将描述性文本（如产品类别）转换为捕捉语义关系的数值向量。通过使用预训练的Word2Vec模型，文章演示了如何将文本描述转换为新的数值特征，并将其整合到表格数据中。这种方法能为机器学习模型提供更丰富的语义信息，从而潜在地提升模型性能，适用于任何包含有意义描述性文本的分类列。",
      "translated_title": "词嵌入在表格数据特征工程中的应用",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-word-embeddings-tabular-data-feature-engineering.png",
          "alt": "Word Embeddings for Tabular Data Feature Engineering",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "It would be difficult to argue that word embeddings &mdash; dense vector representations of words &mdash; have not dramatically revolutionized the field of natural language processing (NLP) by quantitatively capturing semantic relationships between words."
    },
    {
      "title": "决策树不仅仅适用于表格数据 (原标题: Decision Trees Aren’t Just for Tabular Data)",
      "link": "https://machinelearningmastery.com/decision-trees-arent-just-for-tabular-data/",
      "pubDate": "Thu, 10 Jul 2025 09:57:51 +0000",
      "isoDate": "2025-07-10T09:57:51.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "# 决策树不仅仅适用于表格数据\n\n## 引言\n决策树作为一种多功能、可解释且高效的机器学习技术，数十年来一直是分类和回归任务中广泛使用的模型。它们既可以作为独立模型，也可以作为随机森林和梯度提升机等更强大集成方法的核心组件。除了这些优点，决策树的另一个吸引人之处在于它们能够处理多种数据格式，而不仅仅是完全结构化的表格数据。本文将从理论和实践相结合的角度探讨决策树的这一特性。\n\n## 决策树概述\n决策树是一种用于预测任务（分类和回归）的监督学习模型。它们通过一组带有已知预测输出的标记示例进行训练，例如，一组收集到的动物标本属性及其所属物种。树的构建是一个迭代和递归地将训练数据集划分为子集的过程，旨在使每个子集尽可能地具有类别（或数值标签）的同质性。一旦训练完成，模型就学习到了一组应用于数据属性的层次化决策规则，这些规则可以直观地表示为一棵树。\n\n![决策树不仅仅适用于表格数据](https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-decision-trees-tabular-data-1.png)\n\n对未知标签的示例进行推理预测时，模型会从上到下检查这些规则或条件，最终根据问题是分类还是回归，到达一个指向类别或值预测的“叶节点”。\n\n![决策树概述](https://machinelearningmastery.com/wp-content/uploads/2025/07/decisiontreeexample.png)\n\n## 决策树超越表格数据\n大多数经典机器学习模型（包括决策树）通常处理的是结构化或表格数据，这些数据以实例（行）的形式组织，并通过数值和分类属性（列）进行描述。然而，决策树也能够处理非严格表格的数据集或其部分。\n\n常见的非表格数据示例包括文本、图像和时间序列。通过应用合适的预处理技术，这些数据格式可以转换为更结构化的形式。例如，一段文本序列（如客户对产品的评论）可以通过特征提取或嵌入转换为结构化数据，然后作为决策树分类器的输入，用于分析客户评论背后的积极或消极情绪。\n\n另一种在包含部分非结构化数据的预测任务中利用决策树的策略是使用混合解决方案，将深度学习模型与决策树结合起来。例如，一个卷积神经网络（CNN）可以被训练用于从图像中提取结构化特征（推断出尺寸、形状、颜色等属性），然后这些基于图像的特征被传递给基于树的模型（如随机森林）进行预测，例如估计产品销售额。\n\n在研究领域，已经有一些努力直接调整基于决策树的模型来处理图数据和层次数据等非表格数据，尽管它们在主流应用中仍然罕见。\n\n## 实际示例\n为了增加一些实践性，我们将演示如何在一个结合了纯表格数据和文本数据的数据集上训练一个基于决策树的模型。\n\n该代码示例的核心逻辑如下：\n1.  使用一个包含三个预测属性（其中一个是文本）的客户支持工单数据集。\n2.  文本数据需要预处理才能输入到决策树模型中。\n3.  使用TF-IDF向量化器获取每个文本的向量表示。\n4.  将这个新的文本特征与其它数值特征合并，以训练决策树分类器，并在测试集上进行评估。\n\n您可以通过执行此代码来训练模型，但可能会对其性能感到失望（正确预测和不正确预测的数量大致相同）。这是预期的结果，因为我们使用的是一个只有100个实例的小数据集，而从文本表示中学习通常需要更多的实例。\n\n## 结论\n本文讨论了决策树和基于决策树的机器学习模型（如随机森林）处理非严格表格数据的能力。从文本到图像再到时间序列，机器学习模型和数据可以通过预处理或组合的方式来适应那些乍一看似乎无法处理的数据。",
      "shortSummary": "决策树不仅限于处理表格数据，还能有效应对文本、图像和时间序列等非结构化数据。这主要通过两种方式实现：一是将非结构化数据通过预处理（如TF-IDF）转换为结构化特征；二是采用混合方案，结合深度学习模型提取特征后，再由决策树进行预测。文章通过一个结合文本和表格数据的实际案例，展示了决策树在处理多源异构数据方面的强大能力。",
      "translated_title": "决策树不仅仅适用于表格数据",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-decision-trees-tabular-data-1.png",
          "alt": "Decision Trees Aren’t Just for Tabular Data",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/decisiontreeexample.png",
          "alt": "Outline of a decision tree",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "Versatile, interpretable, and effective for a variety of use cases, decision trees have been among the most well-established machine learning techniques for decades, widely used for classification and regression tasks."
    },
    {
      "title": "10 个 NumPy 单行代码简化特征工程 (原标题: 10 NumPy One-Liners to Simplify Feature Engineering)",
      "link": "https://machinelearningmastery.com/10-numpy-one-liners-to-simplify-feature-engineering/",
      "pubDate": "Tue, 08 Jul 2025 12:00:29 +0000",
      "isoDate": "2025-07-08T12:00:29.000Z",
      "creator": "Bala Priya C",
      "summary": "## 10 个 NumPy 单行代码简化特征工程\n\n![10 NumPy One-Liners to Simplify Feature Engineering](https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-bala-feature-eng-one-liners.jpeg)\n\n在构建机器学习模型时，特征工程是获得竞争优势的关键。虽然 Pandas 和 Scikit-learn 提供了强大的工具，但 NumPy 的向量化操作能使特征工程更快、更优雅。本文探讨了 10 种强大的 NumPy 单行代码，利用其广播、高级索引和数学函数来高效创建新特征。\n\n### 1. 使用中位数绝对偏差 (MAD) 进行鲁棒缩放\n\n标准缩放对正态分布数据有效，但容易受异常值影响。MAD 缩放提供了一种鲁棒的替代方案，能够处理大量异常值，尤其适用于金融或网络分析等领域。该方法通过将数据围绕中位数居中，然后除以 MAD（中位数绝对偏差的中位数）来实现，从而提供不受异常值影响的尺度度量。\n\n### 2. 使用分位数对连续变量进行分箱\n\n将连续变量转换为分类箱对于许多算法至关重要，并有助于捕获非线性关系。等宽分箱可能导致组不平衡，而基于分位数的分箱能确保每个箱中样本数量大致相同。这对于树模型或创建可解释特征特别有用。`np.percentile()` 计算分位数边界，`np.digitize()` 将值分配到相应的箱中。\n\n### 3. 无循环的多项式特征\n\n多项式特征有助于捕获变量间的非线性关系和交互效应。传统的循环方法效率低下，而 NumPy 的列表推导式结合 `np.column_stack()` 可以高效生成所有可能的二次多项式组合（包括平方项和交互项），为模型提供非线性关系信息。\n\n### 4. 时间序列的滞后特征\n\n时间序列分析常需要捕获时间依赖性。滞后特征允许模型访问历史值，这对于预测和异常检测至关重要。`np.roll()` 函数可以平移数组元素，结合列表推导式和 `np.column_stack()` 可以同时生成多个滞后版本，并通过切片处理边缘情况，确保训练数据干净。\n\n### 5. 无 Pandas 的独热编码\n\n独热编码是处理机器学习中分类变量的必要步骤。虽然 Pandas 提供了便利方法，但纯 NumPy 实现对于大型数据集而言更快、更节省内存。该技术利用广播将每个类别值与所有可能的类别进行比较，然后将布尔结果转换为整数，创建二进制矩阵。\n\n### 6. 基于坐标的距离特征\n\n地理空间特征通常需要计算与参考点的距离，这在位置模型中很常见。NumPy 的广播功能可以同时从所有位置减去参考点，然后通过平方差求和并取平方根来高效计算欧几里得距离。这对于处理数百万个数据点的大型数据集至关重要。\n\n### 7. 变量对之间的交互特征\n\n特征交互有助于理解单个特征可能遗漏的隐藏模式。手动创建所有成对交互既繁琐又容易出错。该向量化方法通过嵌套列表推导式系统地生成所有唯一的特征对乘积，并使用 `.T` 转置结果，确保每行代表一个样本，每列代表一个交互项。\n\n### 8. 滚动窗口统计\n\n滚动统计可以平滑噪声数据并捕获局部趋势，这对于时间序列分析和信号处理至关重要。基于卷积的方法既优雅又高效。`np.convolve()` 函数结合均匀平均核 (`np.ones(window_size)/window_size`) 和 `mode='valid'` 参数，可以自然地实现滚动窗口操作，轻松扩展到其他窗口函数。\n\n### 9. 异常值指示特征\n\n与其移除异常值，不如创建特征来标记它们的存在，这可以为模型提供有价值的信息，尤其是在欺诈检测或质量控制等领域。该方法使用数据的百分位数（例如 5% 和 95%）作为异常值阈值，将超出此范围的值标记为 1，从而创建二进制特征，指示异常观测值。\n\n### 10. 分类变量的频率编码\n\n频率编码用分类值的出现次数替换原始值，这可能比任意标签编码更具信息量，尤其当类别频率与目标变量相关时。该方法首先使用 `np.unique()` 找到所有唯一类别及其计数，然后为每个原始类别值查找对应的频率计数，生成一个数值特征，表示该类别在数据集中出现的频率。\n\n### 特征工程的最佳实践\n\n在创建新的代表性特征时，应牢记以下几点：\n*   **内存效率：** 处理大型数据集时，考虑特征工程对内存的影响。\n*   **特征选择：** 并非越多特征越好，应使用相关性分析或特征重要性等技术选择最相关的特征。\n*   **验证：** 始终在保留集上验证工程特征，确保它们能提高模型性能且不会导致过拟合。\n*   **领域知识：** 最好的工程特征通常源于对问题领域的深入理解。NumPy 技术是高效实现这些领域洞察的工具。\n\n### 结论\n\n这些 NumPy 单行代码是解决常见特征工程挑战的实用方案。无论处理时间序列、地理空间数据还是传统表格数据集，这些技术都能帮助构建更高效、更易维护的特征工程管道。关键在于了解何时使用每种方法以及如何组合它们以从数据中提取最大信号。最佳的特征工程技术是能帮助模型学习特定问题领域模式的技术。将这些单行代码作为构建块，并通过适当的交叉验证和领域专业知识来验证其有效性。",
      "shortSummary": "本文介绍了10个NumPy单行代码，旨在简化和加速机器学习中的特征工程。这些技巧利用NumPy的向量化操作、广播和高级索引，高效处理数据预处理任务，包括鲁棒缩放、分位数分箱、生成多项式和滞后特征、独热编码、计算距离、创建交互特征、滚动统计、标记异常值及频率编码。文章强调NumPy在处理大规模数据集时的效率优势，并提供了特征工程的最佳实践，帮助构建更高效、可维护的机器学习管道。",
      "translated_title": "10 个 NumPy 单行代码简化特征工程",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-bala-feature-eng-one-liners.jpeg",
          "alt": "10 NumPy One-Liners to Simplify Feature Engineering",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "When building machine learning models, most developers focus on model architectures and hyperparameter tuning."
    },
    {
      "title": "你的第一个Python OpenAI API项目：分步指南 (原标题: Your First OpenAI API Project in Python Step-By-Step)",
      "link": "https://machinelearningmastery.com/your-first-openai-api-project-in-python-step-by-step/",
      "pubDate": "Mon, 07 Jul 2025 14:16:43 +0000",
      "isoDate": "2025-07-07T14:16:43.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "## 你的第一个Python OpenAI API项目：分步指南\n\n本文提供了一个详细的分步指南，教你如何使用Python和FastAPI构建第一个OpenAI API项目，以访问GPT-4等先进的大型语言模型（LLMs）。与之前介绍的本地LLM项目（如Ollama）不同，本文侧重于通过云端API调用OpenAI模型。\n\n### 准备工作和推荐阅读\n\n*   建议读者熟悉LLM相关工具和概念。\n*   推荐阅读文章包括《5个入门LLM的必备免费工具》、《10个大型语言模型关键概念解析》以及《Python中LLM的初学者指南（使用Ollama）》。\n*   还建议阅读本系列前一篇关于本地LLM API项目的文章。\n\n### 分步指南\n\n#### 1. 环境要求\n\n*   Python 3.9或更高版本。\n*   具备Python编程基础。\n*   推荐使用Visual Studio Code等IDE。\n*   不建议在Google Colab等云端笔记本环境运行，因为需要管理多个依赖和虚拟环境。\n\n#### 2. 获取OpenAI API密钥\n\n*   访问OpenAI官网注册或登录。\n*   进入“设置”（齿轮图标）->“账单”。\n\n![OpenAI billing settings](https://machinelearningmastery.com/wp-content/uploads/2025/06/Captura-de-pantalla-2025-06-24-a-las-11.57.35.png)\n\n*   访问GPT-4等模型需要有效的账单计划（订阅或按量付费），需提供支付信息并完成账户验证。\n*   前往“API密钥”页面，点击“+创建新密钥”生成并复制`sk-XXXXX`格式的密钥。\n\n#### 3. 设置Python项目和虚拟环境\n\n*   在VS Code中创建新项目文件夹，例如`openai_api_hello`。\n*   在文件夹内创建`main.py`（初始为空）和`requirements.txt`。\n*   `requirements.txt`内容：\n    ```\n    fastapi\n    uvicorn\n    openai\n    python-dotenv\n    ```\n*   强烈建议使用虚拟环境隔离项目依赖，避免版本冲突。\n*   在VS Code中设置虚拟环境：\n    1.  打开命令面板（`Command + Shift + P`）。\n    2.  选择`Python:Create Environment`，然后选择`venv`。\n    3.  选择创建新虚拟环境并选择合适的Python版本（如Python 3.11）。\n    4.  选择`requirements.txt`文件以安装列出的依赖。\n*   如果自动安装失败，可在IDE终端运行：`pip install fastapi uvicorn openai python-dotenv`。\n\n#### 4. 编写主Python程序 (`main.py`)\n\n将以下代码粘贴到`main.py`中：\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport os\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\n\n# IMPORTANT: PASTE YOUR OPENAI API KEY BETWEEN THE \"\" HERE:\nclient = OpenAI(api_key=\"sk-...\") # 将sk-...替换为你的实际API密钥\n\napp = FastAPI()\n\nclass ChatRequest(BaseModel):\n    message: str\n\n@app.post(\"/chat\")\nasync def chat(request: ChatRequest):\n    try:\n        completion = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                {\"role\": \"user\", \"content\": request.message}\n            ]\n        )\n        return {\"response\": completion.choices[0].message.content}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/\")\ndef root():\n    return {\"message\": \"Hello, World\"}\n```\n\n*   **代码说明:**\n    *   导入必要的库：`FastAPI`用于构建API，`BaseModel`用于数据验证，`OpenAI`用于与OpenAI API交互，`dotenv`用于加载环境变量。\n    *   `client = OpenAI(api_key=\"sk-...\")`：**务必将`sk-...`替换为你的OpenAI API密钥。**\n    *   **API密钥安全存储（可选但推荐）:**\n        *   在项目文件夹中创建`.env`文件，内容为`OPENAI_API_KEY=<YOUR KEY GOES HERE>`。\n        *   将`client = OpenAI(api_key=\"sk-...\")`替换为：\n            ```python\n            load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), \".env\"))\n            api_key = os.getenv(\"OPENAI_API_KEY\")\n            if not api_key:\n                raise RuntimeError(\"OPENAI_API_KEY not found in environment variables\")\n            client = OpenAI(api_key=api_key)\n            ```\n    *   `app = FastAPI()`：创建FastAPI应用实例。\n    *   `class ChatRequest(BaseModel): message: str`：定义Pydantic模型，用于验证传入请求的JSON体，确保包含一个名为`message`的字符串字段。\n    *   `@app.post(\"/chat\") async def chat(request: ChatRequest):`：定义POST请求的`/chat`端点。此函数调用OpenAI API的`chat.completions.create`方法，使用`gpt-4`模型，并传入系统消息和用户消息（来自请求的`message`字段）。返回模型的响应内容，并包含错误处理机制。\n\n#### 5. 运行和测试项目\n\n*   保存`main.py`文件。\n*   在IDE终端中运行：`uvicorn main:app --reload --port 8010`（确保在项目目录下且虚拟环境已激活）。\n*   成功运行后，终端会显示`INFO`消息，指示服务在`http://127.0.0.1:8010`运行。\n*   在浏览器中打开`http://127.0.0.1:8010/docs`，将看到FastAPI的交互式文档界面。\n\n![FastAPI Docs Interface for Your OpenAI API Project](https://machinelearningmastery.com/wp-content/uploads/2025/06/Captura-de-pantalla-2025-06-24-a-las-12.41.04.png)\n\n*   **测试API:**\n    1.  点击`POST /chat`旁边的箭头展开。\n    2.  点击“Try it out”按钮。\n    3.  在JSON参数值中输入你的问题，例如：`{\"message\": \"你的问题在这里\"}`。\n\n![Entering a prompt for GPT-4 in our API Project interface](https://machinelearningmastery.com/wp-content/uploads/2025/06/Captura-de-pantalla-2025-06-24-a-las-12.43.45.png)\n\n    4.  点击“Execute”按钮。\n    5.  稍等片刻，向下滚动即可看到GPT-4的响应。\n\n![Local LLM Response](https://machinelearningmastery.com/wp-content/uploads/2025/06/Captura-de-pantalla-2025-06-24-a-las-12.44.39.png)\n\n*   至此，你的API项目已成功与OpenAI的GPT-4模型进行对话。\n\n### 后续发展建议\n\n*   添加前端用户界面（如使用React、Vue、HTML/JavaScript或Streamlit）。\n*   改进错误处理机制。\n*   尝试其他OpenAI模型，如GPT-3.5。\n\n### 总结\n\n本文详细介绍了如何使用FastAPI构建并运行你的第一个本地OpenAI API项目，以利用GPT-4等先进模型进行快速推理。",
      "shortSummary": "本文提供了一个详细的分步指南，教你如何使用Python和FastAPI构建第一个OpenAI API项目。你将学习如何获取OpenAI API密钥，设置Python虚拟环境，编写与GPT-4等先进大型语言模型交互的API代码，并通过FastAPI的Web界面进行运行和测试。该项目使你能够通过云端API访问OpenAI的强大模型，而非本地部署。",
      "translated_title": "你的第一个Python OpenAI API项目：分步指南",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-first-openai-llm-project-step-by-step.png",
          "alt": "Your First OpenAI API Project in Python Step-By-Step",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/Captura-de-pantalla-2025-06-24-a-las-11.57.35.png",
          "alt": "OpenAI billing settings",
          "title": "",
          "position": 2
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/Captura-de-pantalla-2025-06-24-a-las-12.41.04.png",
          "alt": "FastAPI Docs Interface for Your OpenAI API Project",
          "title": "",
          "position": 3
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/Captura-de-pantalla-2025-06-24-a-las-12.43.45.png",
          "alt": "Entering a prompt for GPT-4 in our API Project interface",
          "title": "",
          "position": 4
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/Captura-de-pantalla-2025-06-24-a-las-12.44.39.png",
          "alt": "Local LLM Response",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "完整文章",
      "content": "In a <a href=\"https://machinelearningmastery."
    },
    {
      "title": "保护MLOps的FastAPI端点：认证指南 (原标题: Securing FastAPI Endpoints for MLOps: An Authentication Guide)",
      "link": "https://machinelearningmastery.com/securing-fastapi-endpoints-for-mlops-an-authentication-guide/",
      "pubDate": "Fri, 04 Jul 2025 12:14:51 +0000",
      "isoDate": "2025-07-04T12:14:51.000Z",
      "creator": "Abid Ali Awan",
      "summary": "### 保护MLOps的FastAPI端点：认证指南\n\n#### 引言\n在当今AI时代，数据科学家不仅关注模型训练和优化，还需要掌握机器学习操作（MLOps）技能，包括构建模型推理的REST API并将其部署到云端。虽然简单的API可用于测试，但生产环境中的模型部署对安全性有更高要求。本教程旨在通过FastAPI构建一个简单的机器学习应用，并指导如何为其设置认证，确保只有持有正确令牌的用户才能访问模型进行预测。\n\n#### 1. 项目设置\n本教程将构建一个“葡萄酒分类器”。\n*   **环境准备**：\n    *   创建Python虚拟环境。\n    *   安装必要的Python库：`fastapi`、`uvicorn`、`scikit-learn`、`pandas`、`joblib`、`python-dotenv`。\n*   **模型训练**：\n    *   创建`train_model.py`文件。\n    *   使用Scikit-learn的葡萄酒数据集训练一个随机森林分类器。\n    *   将训练好的模型保存为`wine_clf.joblib`。\n\n#### 2. 构建简单的FastAPI应用\n*   创建`main.py`文件以构建REST API。\n*   **模型加载与定义**：加载`wine_clf.joblib`模型和类别名称。\n*   **数据模型**：使用Pydantic定义`WineRequest`（输入数据）和`WineResponse`（预测结果）模型。\n*   **预测端点**：\n    *   定义`/predict` POST端点。\n    *   接收`WineRequest`负载，通过模型进行预测。\n    *   返回`WineResponse`格式的预测类别名称。\n*   **初始状态**：此时的`/predict`端点是未受保护的，任何人都可以访问。\n\n#### 3. 设置API密钥和自定义请求头\n为API实现认证，将使用API密钥。\n*   **API密钥配置**：\n    *   创建`.env`文件，并添加`API_KEY=abid1234`。\n    *   在`main.py`中，使用`python-dotenv`加载`.env`文件。\n    *   定义`API_KEY_NAME`为`X-API-Key`。\n    *   初始化`APIKeyHeader`，用于从请求头中提取API密钥。\n\n#### 4. 实现认证依赖\n*   定义异步函数`get_api_key`作为认证依赖。\n*   该函数从请求头中获取API密钥，并与预设的`API_KEY`进行比较。\n*   如果API密钥不匹配，则抛出`HTTPException`（状态码401 Unauthorized），并包含“Invalid API Key”的详细信息。\n\n#### 5. 使用认证保护端点\n*   通过在`/predict`端点的装饰器中添加`dependencies=[Depends(get_api_key)]`，将认证依赖应用于该端点。\n*   这确保了只有带有有效API密钥的请求才能访问预测服务。\n*   FastAPI会自动生成Swagger UI，可在`http://localhost:8000/docs`访问，用于交互式探索和测试API端点。\n\n![Securing FastAPI Endpoints for MLOps: An Authentication Guide](https://machinelearningmastery.com/wp-content/uploads/2025/07/awan_securing_fastapi_endpoints_2.png)\n![Securing FastAPI Endpoints](https://www.kdnuggets.com/wp-content/uploads/awan_securing_fastapi_endpoints_1.png)\n\n#### 6. 测试受保护的端点\n通过多种情况测试`/predict`端点，验证API密钥认证是否正常工作。\n*   **无API密钥测试**：发送不带`X-API-Key`头的请求，返回`{\"detail\":\"Invalid API Key\"}`。\n*   **错误API密钥测试**：发送带不正确`X-API-Key`的请求，返回`{\"detail\":\"Invalid API Key\"}`。\n*   **正确API密钥测试**：发送带正确`X-API-Key`的请求，成功返回预测结果`{\"predictions\":[\"Cultivar-0\"]}`。\n\n#### 总结\n本教程成功展示了如何使用FastAPI训练模型并提供服务，并通过实现API密钥认证增强了应用的安全性。FastAPI内置的安全特性（如OAuth2认证）使其成为构建安全、可扩展Web应用的优秀选择。",
      "shortSummary": "本教程详细介绍了如何使用FastAPI为MLOps应用构建安全的机器学习API。首先，文章指导用户训练一个葡萄酒分类模型并构建一个基础的FastAPI预测服务。随后，核心内容聚焦于通过API密钥实现认证，包括环境配置、API密钥头设置、实现认证依赖函数，并将其应用于`/predict`端点。最后，通过多种测试案例验证了认证机制的有效性，确保只有授权用户才能访问模型，从而提升了生产环境部署的安全性。",
      "translated_title": "保护MLOps的FastAPI端点：认证指南",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/awan_securing_fastapi_endpoints_2.png",
          "alt": "Securing FastAPI Endpoints for MLOps: An Authentication Guide",
          "title": "",
          "position": 1
        },
        {
          "url": "https://www.kdnuggets.com/wp-content/uploads/awan_securing_fastapi_endpoints_1.png",
          "alt": "Securing FastAPI Endpoints",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "In today's AI world, data scientists are not just focused on training and optimizing machine learning models."
    },
    {
      "title": "Transformer模型中的跳跃连接 (原标题: Skip Connections in Transformer Models)",
      "link": "https://machinelearningmastery.com/skip-connections-in-transformer-models/",
      "pubDate": "Fri, 04 Jul 2025 03:33:11 +0000",
      "isoDate": "2025-07-04T03:33:11.000Z",
      "creator": "Adrian Tam",
      "summary": "## Transformer模型中的跳跃连接\n\nTransformer模型由堆叠的Transformer层组成，每层包含一个注意力子层和一个前馈子层。这些子层并非直接连接，而是通过跳跃连接（Skip Connections）将输入与处理后的输出结合起来。本文深入探讨了Transformer模型中的跳跃连接，包括其必要性、如何实现梯度流以及预归一化（Pre-norm）和后归一化（Post-norm）架构的区别。\n\n### 为什么Transformer需要跳跃连接？\n\n深度学习模型，包括Transformer，在层数增加时，由于梯度消失问题，训练变得越来越困难。当梯度反向传播通过多层时，它们可能呈指数级减小，导致早期层难以有效学习。保持良好的梯度流是成功训练深度模型的关键。\n\n跳跃连接，也称为残差连接（Residual Connections），为信息和梯度在网络中流动创建了直接路径。它们允许模型学习残差函数，即期望输出与输入之间的差异，而不是从头开始学习完整的变换。这一概念最初在ResNet论文中提出。\n\n数学上表示为：\n$$y = F(x) + x$$\n其中$F(x)$是学习的函数。当$F(x) = 0$时，输出$y$等于输入$x$，这被称为恒等映射。模型可以从这个基线开始，逐渐将$y$塑造成不同于$x$的形状，而不是寻找一个全新的函数$F(x)$。这是使用跳跃连接的动机。\n\n在反向传播过程中，相对于输入的梯度变为：\n$$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\left(\\frac{\\partial F(x)}{\\partial x} + 1\\right)$$\n“+1”项确保即使$\\partial F/\\partial x$很小，梯度也不会消失。这就是跳跃连接缓解梯度消失问题的原因。在Transformer模型中，跳跃连接应用于每个子层（注意力层和前馈网络）周围，为梯度反向流动提供了路径，使Transformer模型能够更快地收敛，这对于其通常较深的架构和较长的训练时间来说是一个关键优势。\n\n![图片 1](https://machinelearningmastery.com/wp-content/uploads/2025/06/david-emrich-9a0S_8bU0lo-unsplash-scaled.jpg)\n\n下图展示了典型的跳跃连接用法：\n\n![图片 2](https://machinelearningmastery.com/wp-content/uploads/2025/06/bert.png)\n\n图中箭头表示绕过注意力子层和前馈子层的跳跃连接。这些连接将输入直接添加到每个子层的输出中，创建了一个残差学习框架。\n\n### Transformer模型中跳跃连接的实现\n\n残差连接在Transformer模型中通常通过将子层的输入直接加到其输出来实现，然后通常会进行层归一化。例如，在PyTorch中，一个BERT层的实现会像这样：\n\n```python\nclass BertLayer(nn.Module):\n    def __init__(self, dim, intermediate_dim, num_heads):\n        super().__init__()\n        # ... (attention, linear layers, activations, norms)\n\n    def forward(self, x):\n        # 跳跃连接围绕注意力子层\n        attn_output = self.attention(x, x, x)[0]\n        x = x + attn_output  # 残差连接\n        x = self.norm1(x)    # 层归一化\n\n        # 跳跃连接围绕MLP子层\n        mlp_output = self.linear1(x)\n        mlp_output = self.act(mlp_output)\n        mlp_output = self.linear2(mlp_output)\n        x = x + mlp_output   # 残差连接\n        x = self.norm2(x)    # 层归一化\n        return x\n```\n\n在`forward()`方法中，注意力模块的输出`attn_output`被保存并添加到输入`x`中，然后应用层归一化。同样，在MLP子层中，输入`x`（MLP子层的输入）被添加到`mlp_output`中。这种实现方式是后归一化（Post-norm）架构的例子，即归一化在残差连接之后应用。\n\n### 预归一化（Pre-norm）与后归一化（Post-norm）Transformer架构\n\n层归一化相对于跳跃连接的位置显著影响训练稳定性和模型性能。主要有两种架构：\n\n1.  **后归一化（Post-norm）**：\n    *   原始的“Attention Is All You Need”论文采用此架构，层归一化在残差连接之后应用。\n    *   训练稳定性：对于非常深的模型，训练可能不稳定，因为梯度方差可能随深度呈指数增长。\n    *   收敛速度：需要精心设计的学习率调度和预热期。\n    *   模型性能：如果成功训练，通常表现更好。\n\n2.  **预归一化（Pre-norm）**：\n    *   大多数现代Transformer模型采用此架构，归一化在子层之前应用。\n    *   训练稳定性：训练更鲁棒。\n    *   收敛速度：通常收敛更快，对学习率选择不那么敏感。\n    *   模型性能：尽管可能略逊于成功训练的后归一化模型，但对于非常深和大型的模型，更快的收敛速度和稳定性更有价值。\n\n预归一化实现的PyTorch代码示例：\n\n```python\nclass PreNormTransformerLayer(nn.Module):\n    def __init__(self, dim, intermediate_dim, num_heads):\n        super().__init__()\n        # ... (attention, linear layers, activations, norms)\n\n    def forward(self, x):\n        # 预归一化：在子层之前归一化\n        normalized_x = self.norm1(x)\n        attn_output = self.attention(normalized_x, normalized_x, normalized_x)[0]\n        x = x + attn_output  # 残差连接\n\n        normalized_x = self.norm2(x)\n        mlp_output = self.linear1(normalized_x)\n        mlp_output = self.act(mlp_output)\n        mlp_output = self.linear2(mlp_output)\n        x = x + mlp_output   # 残差连接\n        return x\n```\n\n### 总结\n\n跳跃连接是实现深度Transformer模型训练的基础组件。它们通过创建直接的梯度路径来缓解梯度消失问题，并允许模型学习残差函数。预归一化和后归一化架构的选择会显著影响训练稳定性和模型性能。对于大多数现代应用，尤其是深度和大型模型，预归一化因其更好的训练稳定性和更快的收敛速度而成为首选。\n\n![Building Transformer Models with Attention](https://machinelearningmastery.com/wp-content/uploads/2022/11/BTMA-400.png)",
      "shortSummary": "跳跃连接（残差连接）是Transformer模型中的关键组件，用于解决深度网络中的梯度消失问题。它们通过创建直接路径，使模型能够学习残差函数。Transformer模型中跳跃连接应用于每个子层。架构上，后归一化（Post-norm）在残差连接后进行归一化，可能不稳定但性能好；预归一化（Pre-norm）在子层前进行归一化，更稳定且收敛快，是现代深度Transformer模型的首选。",
      "translated_title": "Transformer模型中的跳跃连接",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/david-emrich-9a0S_8bU0lo-unsplash-scaled.jpg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/bert.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2022/11/BTMA-400.png",
          "alt": "Building Transformer Models with Attention",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "完整文章",
      "content": "This post is divided into three parts; they are: • Why Skip Connections are Needed in Transformers • Implementation of Skip Connections in Transformer Models • Pre-norm vs Post-norm Transformer Architectures Transformer models, like other deep learning models, stack many layers on top of each other."
    },
    {
      "title": "超越传统方法的5种高级RAG架构 (原标题: 5 Advanced RAG Architectures Beyond Traditional Methods)",
      "link": "https://machinelearningmastery.com/5-advanced-rag-architectures-beyond-traditional-methods/",
      "pubDate": "Wed, 02 Jul 2025 12:00:35 +0000",
      "isoDate": "2025-07-02T12:00:35.000Z",
      "creator": "Nahla Davies",
      "summary": "## 超越传统方法的5种高级RAG架构\n\n检索增强生成（RAG）通过结合信息检索和连贯、有根据的响应生成，彻底改变了语言模型领域。随着RAG的不断发展，一系列创新架构正在超越传统的简单检索和响应范式，重新定义了AI应用中上下文、准确性和动态信息的使用。\n\n![5 Advanced RAG Architectures Beyond Traditional Methods](https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-5-adv-rag-architectures-scaled.png)\n\n### 1. 双编码器多跳检索（Dual-Encoder Multi-Hop Retrieval）\n\n*   **核心理念**：通过动态分层查询，深入挖掘知识库，而非依赖单次、浅层检索。\n*   **工作原理**：将复杂问题（如“英伟达CEO在2023年对AI芯片短缺说了什么？”）分解为多个子查询。首先识别英伟达CEO，然后查询其公开声明，最后聚焦于与AI芯片短缺相关的评论。\n*   **技术细节**：使用双编码器处理初始和后续查询，以保持语义保真度并减少噪音。一个编码器处理不断演变的查询上下文，另一个则在每一步中搜索文档索引。\n*   **优势**：提供分层相关性，捕捉单一检索中可能丢失的细微差别，模仿人类研究行为，显著提高事实准确性和相关性，尤其适用于多方面、真实世界的问题。\n\n### 2. 上下文感知反馈循环（Context-Aware Feedback Loops）\n\n*   **核心理念**：引入迭代机制，模型根据检索到的文档评估自身响应。\n*   **工作原理**：如果置信度低或检测到矛盾，模型会循环返回，重新组织查询，检索更精确的来源，然后重新生成响应。\n*   **技术细节**：借鉴强化学习原则，通过轻量级置信度评估器和矛盾检查器（通常是小型Transformer模型）驱动反馈循环。\n*   **优势**：将静态生成转变为自适应系统，提高事实精确度、引用完整性，并在嘈杂或模糊数据环境中（特别是处理快速变化的数据时）产生更稳健的输出。\n\n### 3. 模块化记忆增强RAG（Modular Memory-Augmented RAG）\n\n*   **核心理念**：不仅扩展检索范围，更重要的是使上下文具有“粘性”，即能够长期存储和利用信息。\n*   **工作原理**：允许模型随时间存储、分类和优先处理检索到的信息块和生成的输出。记忆段被标记上下文元数据（用户ID、任务类型、日期、会话目标），检索模块选择性地访问相关模块。\n*   **技术细节**：记忆单元可以随时间重新排序或衰减，确保陈旧信息不会污染未来的生成。通过结构化记忆存储，而非将所有历史交互附加到每个新提示，实现跨会话的记忆持久化。\n*   **优势**：模型不再仅仅检索最相似的内容，而是检索当前最相关的内容。随着使用，系统能学习哪些数据对特定用户或工作流最有价值，使其更像一个具有历史、上下文和优先级的个性化助手。\n\n### 4. 具备工具集成能力的代理RAG（Agentic RAG with Tool-Use Integration）\n\n*   **核心理念**：将被动检索转变为主动推理，系统将子任务委托给外部工具或API。\n*   **工作原理**：单个输入可以触发一系列操作：查询搜索引擎、提取结构化数据、通过Python脚本过滤，最终生成基于静态文档和实时数据的响应。\n*   **技术细节**：严重依赖LangChain、ReAct或自定义路由模块等编排框架，让语言模型决定如何获取、分析和整合信息。\n*   **优势**：模型具有自主性和决策能力，能够根据任务类型、数据格式或用户意图规划下一步行动。例如，可以读取表格、使用算术推理工具，并将文本洞察与结构化输出结合，实现计划、执行和解释的能力，非常适合需要精确数据流管理的应用。\n\n### 5. 图结构上下文检索（Graph-Structured Context Retrieval）\n\n*   **核心理念**：在检索循环中引入知识图谱，不仅存储实体和关系，还主动驱动检索逻辑。\n*   **工作原理**：当查询被处理时，系统识别其锚定实体，并使用图遍历来获取语义链接的文档和上下文节点。它获取的是一个由关系、因果链或时间链接构成的文档网络，然后从该图诱导的上下文中重构连贯的叙述。\n*   **技术细节**：从“查找类似文档”转变为“映射出文档共同暗示的内容”。\n*   **优势**：在医学、法律或金融分析等复杂、相互关联的领域中尤其强大，因为所需答案可能并非明确写在单个文档中，而是从领域内多个分散的信息片段中推断出来。检索更智能，更能适应跨学科的复杂查询。\n\n**结论**\n\nRAG的演进展示了信息检索、推理和生成之间日益深化的协同作用。先进的RAG系统是分层的、记忆感知的、反馈驱动的，并具备代理能力。它们能够跨跳推理、从过去的会话中学习、动态使用工具，并像经验丰富的研究员一样导航知识。未来的AI系统将需要超越简单的top-k文档匹配，进入一个检索智能、上下文持久、生成既分析又富有创造力的世界。",
      "shortSummary": "本文介绍了超越传统方法的五种高级RAG（检索增强生成）架构。这些创新方法包括：双编码器多跳检索，通过分层查询深入挖掘信息；上下文感知反馈循环，使模型能自我评估和迭代优化响应；模块化记忆增强RAG，实现跨会话的上下文持久化和个性化；具备工具集成能力的代理RAG，将检索提升为主动推理和任务执行；以及图结构上下文检索，利用知识图谱进行更智能、关联性更强的检索。这些架构共同推动RAG系统向更智能、更精准、更具适应性的方向发展。",
      "translated_title": "超越传统方法的5种高级RAG架构",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-5-adv-rag-architectures-scaled.png",
          "alt": "5 Advanced RAG Architectures Beyond Traditional Methods",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "Retrieval-augmented generation (RAG) has shaken up the world of language models by combining the best of two worlds: <a href=\"https://www."
    },
    {
      "title": "Transformer模型中的专家混合架构 (原标题: Mixture of Experts Architecture in Transformer Models)",
      "link": "https://machinelearningmastery.com/mixture-of-experts-architecture-in-transformer-models/",
      "pubDate": "Tue, 01 Jul 2025 03:19:28 +0000",
      "isoDate": "2025-07-01T03:19:28.000Z",
      "creator": "Adrian Tam",
      "summary": "## Transformer模型中的专家混合架构\n\nTransformer模型在自然语言处理（NLP）任务中表现出色，但随着模型规模的扩大，计算复杂度也显著增加。专家混合（Mixture of Experts, MoE）架构通过引入稀疏性，提供了一种优雅的解决方案，使得模型能够高效扩展，而计算成本不会成比例地增加。\n\n![图片 1](https://machinelearningmastery.com/wp-content/uploads/2025/06/realfish-0MvkW2nYysk-unsplash-scaled.jpg)\n\n### 为什么Transformer需要专家混合架构\n\nMoE概念最早于1991年由Jacobs等人提出。它通过使用多个“专家”模型来处理输入，并利用一个“门控”机制来选择使用哪个专家。MoE架构在2021年的Switch Transformer和2024年的Mixtral模型中得到了复兴。\n\n在Transformer模型中，MoE的关键在于它只激活每个输入参数的一个子集进行计算，这使得可以定义非常大的模型，但在每次计算时只使用其中的一小部分。Transformer模型的“智能”主要体现在多层感知机（MLP）块中，这些块通常包含最多的参数和计算负载。训练MLP块以在各种任务中表现良好是具有挑战性的，因为不同的任务可能需要相互矛盾的行为。\n\n![图片 2](https://machinelearningmastery.com/wp-content/uploads/2025/06/mixtral.png)\n\nMoE通过拥有多个专家，并且每次只激活其中稀疏的子集来引入稀疏性。MoE架构仅修改MLP块，而所有专家共享相同的注意力块。每个Transformer层都拥有一组独立的专家，这使得跨层可以进行混合和匹配的组合。这种设计允许创建大量的专家，同时不会大幅增加参数数量，从而在保持低计算成本的同时实现模型扩展。核心思想是：不同的输入受益于不同的专业计算。通过拥有多个专家网络和选择使用哪些专家的路由机制，模型能够以更少的计算资源实现更好的性能。\n\n### 专家混合架构的工作原理\n\nMoE架构由三个关键组件组成：\n\n1.  **专家网络 (Expert Networks)**：多个独立的神经网络，类似于其他Transformer模型中的MLP块，用于处理输入。\n2.  **路由器 (Router)**：一个决定哪个专家应该处理每个输入的机制。它通常是一个线性层，后接softmax函数，生成一个关于N个专家的概率分布。通过“门控机制”选择top-k个专家。\n3.  **输出组合 (Output Combination)**：选定的top-k个专家处理输入，它们的输出通过路由器提供的归一化概率进行加权求和。\n\nMoE的基本操作流程如下：对于注意力块输出序列中的每个向量`x`，路由器将其与一个矩阵相乘以生成logits。经过softmax变换后，这些logits通过top-k操作进行筛选，生成k个索引和k个概率。这些索引激活相应的专家（在图中表示为MLP块），专家处理原始的注意力块输出。最后，专家输出通过归一化的路由器概率进行加权求和。\n\n概念上，MoE块的计算公式为：\n$$ \\text{MoE}(x) = \\sum_{i \\in \\text{TopK}(p)} p_i \\cdot \\text{Expert}_i(x) $$\n其中`k`是一个模型超参数，即使`k=2`也已被证明足以获得良好的性能。\n\n### Transformer模型中MoE的实现\n\n文章提供了一个PyTorch实现示例，展示了如何用MoE替换传统MLP块的Transformer层。主要包括三个类：\n\n*   **Expert类**：与标准的MLP块功能相同，但MoE子层会使用多个实例。\n*   **MoELayer类**：\n    *   初始化时定义专家数量(`num_experts`)、选择的专家数量(`top_k`)和维度(`dim`)。\n    *   创建`nn.ModuleList`来存储多个`Expert`实例，并定义一个`router`线性层。\n    *   `forward`方法处理输入：将输入重塑以进行专家处理，计算路由概率，使用`torch.topk`选择top-k专家及其归一化概率。\n    *   通过循环遍历选定的专家，处理每个输入向量，并根据路由概率进行加权求和，最后将输出重塑回原始形状。\n*   **MoETransformerLayer类**：\n    *   结合了多头注意力子层和MoE子层，MoE子层在此处替代了传统的MLP子层。\n    *   包含归一化层以确保模型稳定性。\n\n一个完整的MoE Transformer模型由一系列这样的Transformer层组成。文章还提供了简单的测试代码来验证MoE Transformer层的基本功能。\n\n### 进一步阅读\n\n文章推荐了多篇关于专家混合架构的进一步阅读材料，包括“What is mixture of experts?”、“Switch Transformers”、“Mixtral of Experts”和“GLaM”等，以供读者深入了解。\n\n![Building Transformer Models with Attention](https://machinelearningmastery.com/wp-content/uploads/2022/11/BTMA-400.png)\n\n### 总结\n\n本文详细介绍了Transformer模型中的专家混合架构，包括：\n\n*   MoE对于Transformer模型高效扩展的必要性。\n*   MoE如何通过专家模型、路由器和门控机制协同工作。\n*   如何实现MoE层以替代Transformer模型中的传统MLP层。",
      "shortSummary": "专家混合（MoE）架构通过引入稀疏性，使Transformer模型能够高效扩展，同时控制计算成本。它通过多个“专家”网络和一个“路由器”机制工作，路由器选择并组合top-k个专家的输出。MoE主要替代Transformer的MLP块，允许模型在不大幅增加参数的情况下处理不同输入，从而以更少的计算资源实现更好的性能。文章还提供了MoE在PyTorch中的实现示例。",
      "translated_title": "Transformer模型中的专家混合架构",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/realfish-0MvkW2nYysk-unsplash-scaled.jpg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/mixtral.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2022/11/BTMA-400.png",
          "alt": "Building Transformer Models with Attention",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "完整文章",
      "content": "This post covers three main areas: • Why Mixture of Experts is Needed in Transformers • How Mixture of Experts Works • Implementation of MoE in Transformer Models The Mixture of Experts (MoE) concept was first introduced in 1991 by <a href=\"https://www."
    },
    {
      "title": "你的第一个Python本地大语言模型API项目：分步指南 (原标题: Your First Local LLM API Project in Python Step-By-Step)",
      "link": "https://machinelearningmastery.com/your-first-local-llm-api-project-in-python-step-by-step/",
      "pubDate": "Mon, 30 Jun 2025 12:00:58 +0000",
      "isoDate": "2025-06-30T12:00:58.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "# 你的第一个Python本地大语言模型API项目：分步指南\n\n本文详细介绍了如何使用Python、Ollama和FastAPI在本地机器上搭建并运行一个大语言模型（LLM）API。通过此API，用户可以向本地下载的LLM发送提示并获取响应，实现类似ChatGPT但完全本地化的体验。\n\n## 核心目标与工具\n\n*   **目标**：在本地机器上设置一个LLM API，无需依赖外部云服务。\n*   **关键工具**：\n    *   **Ollama**：一个用户友好的框架，用于在本地运行Llama、Gemma或Mistral等开源LLM。\n    *   **FastAPI**：一个强大且轻量级的Python Web框架，用于构建RESTful API，实现用户与模型的HTTP交互。\n    *   **Python 3.9+**：作为开发语言。\n    *   **Requests**：用于发送HTTP请求。\n    *   **Uvicorn**：一个ASGI服务器，用于运行FastAPI应用。\n\n## 准备工作\n\n本教程假设您已安装Python 3.9或更高版本，并对Python语言有基本到中级的理解。代码设计用于在Visual Studio Code等IDE中实现，不适用于在线Notebook环境，因为需要本地下载和使用LLM。\n\n## 步骤指南\n\n### 1. 安装Ollama并本地下载LLM\n\n1.  **下载Ollama**：根据您的操作系统，从Ollama官网下载并启动相应的版本。\n2.  **下载LLM模型**：打开终端，运行命令 `ollama run llama3`。这将把Llama 3模型（默认是 `llama3:latest`）下载到本地。首次下载可能需要较长时间，具体取决于您的网络带宽。\n    *   **注意**：虽然此命令会自动启动一个对话助手，但本教程将采用不同的方法来构建基于Python的本地LLM API。\n\n### 2. 创建Python项目\n\n1.  **创建项目文件夹**：在文件目录中创建一个名为“local-llm-api”的文件夹。\n2.  **创建文件**：在文件夹内创建两个文件：`main.py` (暂时留空) 和 `requirements.txt`。\n3.  **配置 `requirements.txt`**：将以下内容添加到 `requirements.txt` 并保存：\n    ```\n    fastapi\n    uvicorn\n    requests\n    ```\n4.  **设置虚拟环境（推荐）**：\n    *   在VS Code中，通过 `Command + Shift + P` 打开命令面板。\n    *   输入或选择 `Python:Create Environment`，然后选择 `Venv`。\n    *   选择合适的Python版本（例如Python 3.11）。\n    *   系统会提示您选择 `requirements.txt` 文件以安装列出的依赖项（FastAPI、Uvicorn、Requests）。\n    *   如果上述步骤不成功，可以在IDE终端中运行：`pip install fastapi uvicorn requests`。\n\n### 3. 编写主Python程序 (`main.py`)\n\n将以下代码添加到 `main.py` 文件中：\n\n```python\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nimport requests\nimport json\nimport uvicorn\nimport os # Added for environment variable usage\n\napp = FastAPI()\n\nclass Prompt(BaseModel):\n    prompt: str\n\n@app.post(\"/generate\")\ndef generate_text(prompt: Prompt):\n    try:\n        # Use environment variables for host and model, with fallbacks\n        ollama_host = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n        ollama_model = os.getenv(\"OLLAMA_MODEL\", \"llama3:latest\")\n\n        response = requests.post(\n            f\"{ollama_host}/api/generate\", # f-string for host\n            json={\"model\": ollama_model, \"prompt\": prompt.prompt}, # Use ollama_model\n            stream=True,\n            timeout=120 # Give model time to respond\n        )\n        response.raise_for_status() # Raise an exception for HTTP errors (4xx or 5xx)\n\n        output = \"\"\n        for line in response.iter_lines():\n            if line:\n                data = line.decode(\"utf-8\").strip()\n                if data.startswith(\"data: \"):\n                    data = data[len(\"data: \"):]\n                if data == \"[DONE]\":\n                    break\n                try:\n                    chunk = json.loads(data)\n                    output += chunk.get(\"response\") or chunk.get(\"text\") or \"\"\n                except json.JSONDecodeError:\n                    print(f\"Warning: Could not decode JSON from line: {data}\") # Added for debugging\n                    continue\n        return {\"response\": output.strip() or \"(Empty response from model)\"}\n    except requests.RequestException as e:\n        return {\"error\": f\"Ollama request failed: {str(e)}\"}\n\nif __name__ == \"__main__\":\n    # For development, reload=True can be useful. For production, use reload=False.\n    uvicorn.run(\"main:app\", host=\"127.0.0.1\", port=8000, reload=False)\n```\n\n**代码解释**：\n\n*   `app = FastAPI()`：创建Web API实例，它将监听并处理请求。\n*   `class Prompt(BaseModel):`：定义JSON输入模式，用于接收LLM的提示。\n*   `@app.post(\"/generate\")`：定义一个API端点，用于发送提示并获取模型响应。\n*   `requests.post(...)`：此核心代码将提示发送到由Ollama获取的本地LLM。请确保 `model` 参数中的模型名称与您已下载的模型名称一致（可通过 `ollama list` 命令查看）。\n*   `for line in response.iter_lines():`：读取流式响应并将其格式化为可读的输出。\n\n### 4. 运行和测试API\n\n1.  **运行程序**：保存 `main.py` 文件，然后点击“运行”图标或在终端中运行 `python main.py`。\n2.  **确认运行**：您应该在IDE的输出中看到类似 `INFO: Uvicorn running on http://127.0.0.1:8000` 的信息，表示REST服务器已启动并运行。\n3.  **访问API文档**：在浏览器中打开 `http://127.0.0.1:8000/docs`。如果一切正常，您将看到FastAPI的交互式文档界面。\n    ![FastAPI Docs Interface for Your Local LLM API](https://machinelearningmastery.com/wp-content/uploads/2025/06/Captura-de-pantalla-2025-06-23-a-las-13.19.44-scaled.png)\n4.  **测试LLM**：\n    *   在界面中，点击 `POST /generate` 旁边的箭头展开。\n    *   点击“Try it out”按钮。\n    *   在JSON参数值中输入您的提示（例如，替换 `\"string\"` 为 `{\"prompt\": \"What is the capital of France?\"}`）。\n    ![Entering a prompt](https://machinelearningmastery.com/wp-content/uploads/2025/06/Captura-de-pantalla-2025-06-23-a-las-13.25.05.png)\n    *   点击“Execute”按钮。\n    *   稍等片刻，向下滚动即可看到LLM的响应。\n    ![Local LLM Response](https://machinelearningmastery.com/wp-content/uploads/2025/06/Captura-de-pantalla-2025-06-23-a-las-13.29.26-scaled.png)\n\n恭喜您，您已成功搭建了自己的本地LLM API！\n\n## 进阶改进思路\n\n*   **构建前端**：可以使用Streamlit等工具构建一个前端界面来消费这个基于FastAPI的API。\n*   **探索微调模型**：针对市场营销、保险、物流等特定领域或用例，探索使用微调模型。\n\n## 总结\n\n本文分步展示了如何利用Ollama下载本地大语言模型，并结合FastAPI构建一个基于REST服务的本地LLM API，从而在您自己的机器上通过Python程序实现快速模型推理。",
      "shortSummary": "本文提供了在Python中构建本地大语言模型（LLM）API的分步指南。它教你如何使用Ollama在本地运行LLM（如Llama 3），并利用FastAPI创建一个RESTful API来与模型交互。通过设置虚拟环境、编写Python代码并进行测试，用户可以在本地机器上发送提示并接收LLM响应，无需依赖外部云服务。该项目实现了本地化的LLM推理，并提供了进一步改进的建议。",
      "translated_title": "你的第一个Python本地大语言模型API项目：分步指南",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-first-local-llm-project-step-by-step.png",
          "alt": "Your First Local LLM API Project in Python Step-By-Step",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/Captura-de-pantalla-2025-06-23-a-las-13.19.44-scaled.png",
          "alt": "FastAPI Docs Interface for Your Local LLM API",
          "title": "",
          "position": 2
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/Captura-de-pantalla-2025-06-23-a-las-13.25.05.png",
          "alt": "Entering a prompt",
          "title": "",
          "position": 3
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/Captura-de-pantalla-2025-06-23-a-las-13.29.26-scaled.png",
          "alt": "Local LLM Response",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "完整文章",
      "content": "Interested in leveraging a large language model (LLM) API locally on your machine using Python and not-too-overwhelming tools frameworks? In this step-by-step article, you will set up a local API where you'll be able to send prompts to an LLM downloaded on your machine and obtain responses back."
    },
    {
      "title": "Transformer模型中的线性层和激活函数 (原标题: Linear Layers and Activation Functions in Transformer Models)",
      "link": "https://machinelearningmastery.com/linear-layers-and-activation-functions-in-transformer-models/",
      "pubDate": "Mon, 30 Jun 2025 01:45:34 +0000",
      "isoDate": "2025-06-30T01:45:34.000Z",
      "creator": "Adrian Tam",
      "summary": "# Transformer模型中的线性层和激活函数\n\n本文探讨了Transformer模型中线性层和激活函数的重要性，它们是实现非线性变换的关键组成部分。\n\n## 引言\n\n尽管注意力操作是Transformer模型的标志，但线性层和激活函数同样不可或缺。本文将深入讲解为何线性层和激活函数能够实现非线性变换、Transformer模型中前馈网络的典型设计以及常见的激活函数及其特性。\n\n![图片 1](https://machinelearningmastery.com/wp-content/uploads/2025/06/svetlana-gumerova-6fAgnT3Dhl4-unsplash-1-scaled.jpg)\n*Transformer模型中的线性层和激活函数*\n\n## 为何需要线性层和激活函数\n\nTransformer模型的核心功能——注意力层，对输入执行的是仿射变换（即线性变换），其输出是输入在每个序列元素上的加权和。然而，神经网络的力量并非仅来源于线性层，而是通过激活函数引入非线性，从而学习复杂的模式。\n\n在Transformer模型中，注意力层之后需要非线性来学习复杂的模式。这通过在每个注意力层之后添加一个前馈网络（FFN）或多层感知机（MLP）网络来实现。一个典型的Transformer块结构如下：\n\n![图片 2](https://machinelearningmastery.com/wp-content/uploads/2025/06/bert.png)\n*BERT模型架构*\n\n上图中的灰色框在Transformer模型中会重复多次。在每个块中（不包括归一化层），输入首先通过注意力层，然后通过前馈网络（在PyTorch中实现为`nn.Linear`）。前馈网络中的激活函数为变换增加了非线性，使模型能够学习更复杂的模式。\n\n前馈网络通常包含多个线性层：\n*   第一个线性层：扩展维度以探索不同的表示。\n*   最后一个线性层：将维度收缩回原始大小。\n*   激活函数：通常应用于第一个线性层的输出。\n\n由于这种设计，我们通常将块的前半部分称为“注意力子层”，后半部分称为“MLP子层”。\n\n## 前馈网络的典型设计\n\n以BERT模型为例，其MLP子层实现如下：\n\n```python\nimport torch.nn as nn\n\nclass BertMLP(nn.Module):\n    def __init__(self, dim, intermediate_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, intermediate_dim)\n        self.fc2 = nn.Linear(intermediate_dim, dim)\n        self.gelu = nn.GELU()\n\n    def forward(self, hidden_states):\n        hidden_states = self.fc1(hidden_states)\n        hidden_states = self.gelu(hidden_states)\n        hidden_states = self.fc2(hidden_states)\n        return hidden_states\n```\n\n该MLP子层包含两个线性模块。当输入序列进入MLP子层时，第一个线性模块扩展维度，然后应用GELU激活函数。结果通过第二个线性模块，将维度收缩回原始大小。中间维度通常是原始维度的4倍，这是Transformer模型中常见的设计模式。\n\n## 激活函数的变体\n\n激活函数向神经网络引入非线性，使其能够学习复杂的模式。虽然传统神经网络常用双曲正切（tanh）、Sigmoid和修正线性单元（ReLU），但Transformer模型通常采用GELU和SwiGLU激活函数。\n\n以下是一些常见激活函数的数学定义：\n$$ \\begin{aligned} \\text{Sigmoid}(x) &= \\frac{1}{1 + e^{-x}} \\\\ \\tanh(x) &= \\frac{e^x – e^{-x}}{e^x + e^{-x}} = 2\\text{Sigmoid}(2x) – 1 \\\\ \\text{ReLU}(x) &= \\max(0, x) \\\\ \\text{GELU}(x) &= x \\cdot \\Phi(x) \\approx \\frac{x}{2}\\Big(1 + \\tanh\\big(\\sqrt{\\frac{2}{\\pi}}(x + 0.044715x^3)\\big)\\Big) \\\\ \\text{Swish}_\\beta(x) &= x \\cdot \\text{Sigmoid}(\\beta x) = \\frac{x}{1 + e^{-\\beta x}} \\\\ \\text{SiLU}(x) &= \\frac{x}{1 + e^{-x}} = \\text{Swish}_1(x) \\\\ \\text{SwiGLU}(x) &= \\text{SiLU}(xW + b) \\cdot (xV + c) \\end{aligned} $$\n\n*   **ReLU (修正线性单元)**：在现代深度学习中很受欢迎，因为它避免了梯度消失问题且计算简单。\n*   **GELU (高斯误差线性单元)**：由于使用了标准正态分布的累积分布函数$\\Phi(x)$，计算成本更高。存在近似公式。GELU是非单调的，这可能需要更长的训练时间，但并非严格要求单调性。\n*   **Swish**：另一种非单调激活函数，参数$\\beta$控制$x=0$处的斜率。当$\\beta=1$时，称为SiLU（Sigmoid线性单元）。\n*   **SwiGLU (Swish-Gated Linear Unit)**：是现代Transformer模型中常见的一种新型激活函数。它是Swish函数与线性函数的乘积，其参数在训练过程中学习。其受欢迎源于其复杂性：展开公式会发现分子中包含二次项，这有助于模型在不增加额外层的情况下学习复杂的模式。\n\n![图片 3](https://machinelearningmastery.com/wp-content/uploads/2025/06/activations.png)\n*一些常见激活函数的图示*\n\n在Python代码中切换激活函数非常简单。PyTorch提供了内置的`nn.Sigmoid`、`nn.ReLU`、`nn.Tanh`和`nn.SiLU`。然而，SwiGLU需要特殊的实现。以下是Llama模型中使用的PyTorch代码：\n\n```python\nimport torch.nn as nn\n\nclass LlamaMLP(nn.Module):\n    def __init__(self, dim, intermediate_dim):\n        super().__init__()\n        self.gate_proj = nn.Linear(dim, intermediate_dim)\n        self.up_proj = nn.Linear(dim, intermediate_dim)\n        self.down_proj = nn.Linear(intermediate_dim, dim)\n        self.act = nn.SiLU()\n\n    def forward(self, hidden_states):\n        gate = self.gate_proj(hidden_states)\n        up = self.up_proj(hidden_states)\n        swish = self.act(up)\n        output = self.down_proj(swish * gate)\n        return hidden_states\n```\n\n此实现使用两个线性层处理输入`hidden_states`。一个输出通过SiLU函数，然后与另一个输出进行元素级乘法，最后通过一个最终的线性层进行处理。这些线性层被命名为“up”或“down”以表示维度扩展/收缩，而连接到SiLU的层被称为“gate”，因为它实现了门控机制。门控是神经网络的一种设计，意味着一个线性层输出与一个权重进行元素级乘法，这里这个权重由Swish激活函数产生。\n\nLlama模型架构如下所示，展示了MLP模块的双分支结构：\n\n![图片 4](https://machinelearningmastery.com/wp-content/uploads/2025/06/llama.png)\n*Llama模型架构*\n\n## 总结\n\n本文深入探讨了Transformer模型中的线性层和激活函数。具体来说，我们学习了：\n*   线性层和激活函数对于非线性变换的必要性。\n*   ReLU、GELU和SwiGLU激活函数的特性和实现。\n*   如何在Transformer模型中构建完整的前馈网络。\n\n![Building Transformer Models with Attention](https://machinelearningmastery.com/wp-content/uploads/2022/11/BTMA-400.png)\n*《使用注意力构建Transformer模型》电子书封面*",
      "shortSummary": "Transformer模型中的线性层和激活函数对于实现非线性变换至关重要。注意力层之后，前馈网络（FFN）通过线性层扩展和收缩维度，并结合激活函数引入非线性。Transformer模型常使用GELU和SwiGLU等激活函数来学习复杂模式。特别是SwiGLU（如Llama模型中所示）通过门控机制进一步增强了模型的表达能力。",
      "translated_title": "Transformer模型中的线性层和激活函数",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/svetlana-gumerova-6fAgnT3Dhl4-unsplash-1-scaled.jpg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/bert.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/activations.png",
          "alt": "",
          "title": "",
          "position": 3
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/llama.png",
          "alt": "",
          "title": "",
          "position": 4
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2022/11/BTMA-400.png",
          "alt": "Building Transformer Models with Attention",
          "title": "",
          "position": 5
        }
      ],
      "contentSource": "完整文章",
      "content": "This post is divided into three parts; they are: • Why Linear Layers and Activations are Needed in Transformers • Typical Design of the Feed-Forward Network • Variations of the Activation Functions The attention layer is the core function of a transformer model."
    }
  ],
  "lastUpdated": "2025-07-14T09:35:52.305Z"
}