{
  "sourceUrl": "https://machinelearningmastery.com/blog/feed/",
  "title": "MachineLearningMastery.com",
  "description": "Making developers awesome at machine learning",
  "link": "https://machinelearningmastery.com/blog/",
  "items": [
    {
      "title": "使用 Pandas 和 Scikit-learn 处理不平衡数据集 (原标题: Navigating Imbalanced Datasets with Pandas and Scikit-learn)",
      "link": "https://machinelearningmastery.com/navigating-imbalanced-datasets-with-pandas-and-scikit-learn/",
      "pubDate": "Thu, 12 Jun 2025 12:00:56 +0000",
      "isoDate": "2025-06-12T12:00:56.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "# 使用 Pandas 和 Scikit-learn 处理不平衡数据集\n\n## 引言\n不平衡数据集在现实世界中并不少见，例如银行金融领域的欺诈检测（欺诈交易远少于合法交易）和医疗诊断（罕见疾病远少于常见健康状况）。这类数据集的主要问题在于，机器学习模型容易偏向多数类，导致模型性能下降，甚至可能退化为“虚拟分类器”。本文将介绍使用 Python 的 Pandas 和 Scikit-learn 库处理不平衡数据集的几种策略。\n\n![Navigating Imbalanced Datasets with Pandas and Scikit-learn](https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-ipc-navigating-imbalanced-datasets.png)\n*图：使用 Pandas 和 Scikit-learn 处理不平衡数据集*\n\n## 实践指南：银行营销数据集\n为了演示处理不平衡数据的实际方法，本文使用了公开可用的银行营销数据集。该数据集包含银行客户信息，并标记了客户是否在接到银行营销电话后订阅了定期存款（“是”或“否”）。\n\n**数据集不平衡性：**\n该数据集之所以不平衡，是因为只有约 11% 的客户订阅了定期存款，而约 89% 的客户拒绝了。具体来说：\n*   **未订阅（“no”）:** 39922 名客户 (88.3%)\n*   **已订阅（“yes”）:** 5289 名客户 (11.7%)\n这表明“yes”类是显著的少数类。\n\n## 处理不平衡数据集的策略\n\n### 策略 1：逆频率依赖加权 (Inverse Frequency-Dependent Weighting)\n这是 Scikit-learn 提供的一种策略，通过调整分类模型训练时的实例权重来处理不平衡数据。\n\n*   **原理：** 在 Scikit-learn 的某些分类器中（例如 `RandomForestClassifier`），设置 `class_weight='balanced'` 参数。这会使实例权重与类频率成反比，从而给予少数类更大的权重，以补偿类不平衡。\n*   **优点：** 模型在训练时会更加关注少数类，减少对多数类的偏向。\n*   **示例：** 使用 `RandomForestClassifier` 并设置 `class_weight='balanced'`，同时对分类特征进行独热编码（使用 Pandas 的 `pd.get_dummies()`）。\n\n### 策略 2：欠采样 (Undersampling)\n这是一种在模型训练前进行的数据预处理策略，主要通过 Pandas 实现。\n\n*   **原理：** 减少多数类实例的数量，使其与少数类实例的数量相匹配。\n*   **优点：** 减少模型对多数类的偏向。\n*   **缺点：**\n    *   可能导致信息丢失，特别是当多数类被大量欠采样时。\n    *   可能增加模型方差，有时甚至导致欠拟合。\n*   **适用场景：** 当数据集足够大，即使欠采样后仍能保留足够代表性和多样性的实例时。\n*   **示例：** 将多数类（“no”）的实例数量随机减少到与少数类（“yes”）相同的数量，然后将欠采样后的多数类与少数类合并，形成一个平衡的数据集。原始数据集约 45K 实例，欠采样后平衡数据集约 10.5K 实例。\n\n### 策略 3：过采样 (Oversampling)\n这也是一种在模型训练前进行的数据预处理策略，主要通过 Pandas 实现。\n\n*   **原理：** 通过随机复制少数类实例（带替换采样）来增加其数量，使其与多数类数量相匹配。\n*   **优点：** 有助于缓解模型对多数类的偏向。\n*   **缺点：**\n    *   如果少数类本身不具代表性，或者重复实例可能引入噪声，则不适用。\n    *   可能导致过拟合，因为模型可能会过度学习重复的少数类实例。\n*   **适用场景：** 当少数类规模较小但具有代表性，且添加重复实例不太可能引入噪声或导致过拟合时。\n*   **示例：** 将少数类（“yes”）的实例数量随机复制增加到与多数类（“no”）相同的数量，然后将多数类与过采样后的少数类合并，形成一个平衡的数据集。\n\n## 总结\n本文探讨了数据集中的类不平衡问题，并介绍了使用 Pandas 和 Scikit-learn 库处理该问题的三种常用策略：训练平衡分类模型（逆频率依赖加权）、欠采样和过采样。值得注意的是，还有其他更高级的策略，如 Scikit-learn 的重采样工具和 SMOTE（合成少数类过采样技术）等。",
      "shortSummary": "不平衡数据集（如欺诈检测）会导致机器学习模型偏向多数类，影响性能。本文介绍了使用 Pandas 和 Scikit-learn 处理此类问题的三种策略：一是通过 Scikit-learn 的 `class_weight='balanced'` 参数进行逆频率依赖加权，使模型更关注少数类；二是通过 Pandas 进行欠采样，减少多数类实例；三是通过 Pandas 进行过采样，复制少数类实例。这些方法旨在平衡数据分布，提升模型在不平衡数据集上的表现。",
      "translated_title": "使用 Pandas 和 Scikit-learn 处理不平衡数据集",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-ipc-navigating-imbalanced-datasets.png",
          "alt": "Navigating Imbalanced Datasets with Pandas and Scikit-learn",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "Imbalanced datasets, where a majority of the data samples belong to one class and the remaining minority belong to others, are not that rare."
    },
    {
      "title": "使用FastAPI和Docker部署机器学习模型的逐步指南 (原标题: Step-by-Step Guide to Deploying Machine Learning Models with FastAPI and Docker)",
      "link": "https://machinelearningmastery.com/step-by-step-guide-to-deploying-machine-learning-models-with-fastapi-and-docker/",
      "pubDate": "Wed, 11 Jun 2025 16:17:27 +0000",
      "isoDate": "2025-06-11T16:17:27.000Z",
      "creator": "Bala Priya C",
      "summary": "## 使用FastAPI和Docker部署机器学习模型的逐步指南\n\n![使用FastAPI和Docker部署机器学习模型的逐步指南](https://machinelearningmastery.com/wp-content/uploads/2025/06/step-by-step-deploying-ml-models-docker.png)\n\n本文提供了一个将机器学习模型部署到生产环境的详细指南，通过构建一个糖尿病进展预测器并将其封装为可部署的API。该教程涵盖了从模型训练到使用FastAPI构建REST API，再到使用Docker进行容器化，并最终发布到Docker Hub的完整流程。\n\n### 1. 项目概述与目标\n\n*   **目标**：将训练好的机器学习模型部署为可供实际用户使用的生产级API。\n*   **项目**：基于scikit-learn的样本数据集构建一个糖尿病进展预测器。\n*   **最终成果**：\n    *   一个训练好的随机森林模型，用于预测糖尿病进展分数。\n    *   一个使用FastAPI构建的REST API，接受患者数据并返回预测结果。\n    *   一个完全容器化的应用程序，可随时部署到云端。\n\n### 2. 开发环境设置\n\n*   **先决条件**：Python 3.11+ (或 3.9+), Docker已安装并运行，对Python和API有基本了解。\n*   **项目结构**：\n    *   `diabetes-predictor/` (根目录)\n        *   `app/` (FastAPI应用)\n            *   `__init__.py`\n            *   `main.py`\n        *   `models/` (存放训练好的模型)\n            *   `diabetes_model.pkl`\n        *   `train_model.py` (模型训练脚本)\n        *   `requirements.txt` (Python依赖)\n        *   `Dockerfile` (容器配置)\n*   **安装依赖**：\n    1.  创建并激活虚拟环境：`python -m venv diabetes-env`，然后激活。\n    2.  安装所需库：`pip install scikit-learn pandas fastapi uvicorn`。\n\n### 3. 构建机器学习模型 (`train_model.py`)\n\n*   **模型选择**：使用`RandomForestRegressor`，因其鲁棒性、对不同特征尺度的良好处理能力以及提供特征重要性洞察。\n*   **数据集**：加载scikit-learn的糖尿病数据集，包含442条患者记录和10个生理特征，目标是衡量一年后的疾病进展。\n*   **数据准备**：将数据集按80/20比例划分为训练集和测试集 (`random_state=42`确保结果可复现)。\n*   **模型训练**：\n    *   初始化`RandomForestRegressor`，设置`n_estimators=100`, `random_state=42`, `max_depth=10`（防止过拟合）。\n    *   使用训练数据拟合模型。\n*   **模型评估**：\n    *   在测试集上进行预测。\n    *   计算均方误差 (MSE) 和 R² 分数。R² 分数高于0.4被认为是该数据集的良好表现。\n*   **模型保存**：将训练好的模型保存为`models/diabetes_model.pkl`。\n*   **运行脚本**：`python3 train_model.py`。\n\n### 4. 创建FastAPI应用程序 (`app/main.py`)\n\n*   **API结构**：\n    *   创建`app`目录和`__init__.py`。\n    *   导入`FastAPI`, `BaseModel` (Pydantic), `pickle`, `numpy`, `os`。\n*   **输入数据结构**：\n    *   使用Pydantic的`BaseModel`定义`PatientData`类，包含10个浮点型生理特征（如age, sex, bmi等）。\n    *   提供`schema_extra`示例，帮助API用户理解输入格式。\n*   **FastAPI初始化与模型加载**：\n    *   初始化`FastAPI`应用，设置标题、描述和版本。\n    *   加载之前保存的`diabetes_model.pkl`。\n*   **预测端点 (`/predict`)**：\n    *   定义POST请求端点，接受`PatientData`作为输入。\n    *   将输入转换为NumPy数组。\n    *   使用加载的模型进行预测。\n    *   返回包含`predicted_progression_score`（四舍五入到两位小数）和`interpretation`（通过`get_interpretation`函数提供）的JSON响应。\n*   **解释函数 (`get_interpretation`)**：根据预测分数提供人类可读的解释（低于平均、平均、高于平均进展）。\n*   **健康检查端点 (`/`)**：定义GET请求端点，返回API状态和模型信息。\n\n### 5. 本地测试API\n\n*   **运行命令**：在项目根目录运行`uvicorn app.main:app --reload --port 8000`。\n*   **访问**：在浏览器中打开`http://localhost:8000/`。\n*   **测试预测**：使用`curl`命令发送POST请求及示例数据进行测试，验证API功能。\n\n### 6. 使用Docker进行容器化\n\n*   **创建`requirements.txt`**：列出所有Python依赖及其精确版本，确保环境一致性。\n*   **创建`Dockerfile`**：\n    *   基于`python:3.11-slim`镜像，保持容器小巧。\n    *   设置工作目录为`/app`。\n    *   安装系统依赖（如果需要）。\n    *   复制`requirements.txt`并安装Python依赖（使用`--no-cache-dir`减少镜像大小）。\n    *   复制`app/`和`models/`目录到容器中。\n    *   暴露端口8000。\n    *   定义容器启动命令：`uvicorn app.main:app --host 0.0.0.0 --port 8000`。\n*   **构建Docker镜像**：`docker build -t diabetes-predictor .`。\n*   **运行Docker容器**：`docker run -d -p 8000:8000 diabetes-predictor`。\n\n### 7. 发布到Docker Hub\n\n*   **目的**：将容器化应用共享，便于云平台部署。\n*   **设置Docker Hub**：注册Docker Hub账户。\n*   **登录**：在终端运行`docker login`并输入凭据。\n*   **标记镜像**：\n    *   `docker tag diabetes-predictor your-username/diabetes-predictor:v1.0`\n    *   `docker tag diabetes-predictor your-username/diabetes-predictor:latest`\n*   **推送镜像**：\n    *   `docker push your-username/diabetes-predictor:v1.0`\n    *   `docker push your-username/diabetes-predictor:latest`\n*   **验证**：停止本地容器，然后从Docker Hub拉取并运行镜像，再次测试API以确保其正常工作。\n\n### 8. 总结与展望\n\n本文成功构建了一个完整的机器学习部署流程：训练了随机森林模型，使用FastAPI创建了REST API，并用Docker进行了容器化。该模型现在已准备好部署到AWS ECS、Fargate、Google Cloud或Azure等云平台。未来的改进方向包括添加认证和速率限制、模型监控和日志记录以及批量预测端点。",
      "shortSummary": "本文提供了一个使用FastAPI和Docker部署机器学习模型的逐步指南。教程从训练一个预测糖尿病进展的随机森林模型开始，接着使用FastAPI构建一个REST API，实现数据输入和预测输出。随后，详细介绍了如何使用Dockerfile将整个应用容器化，并将其发布到Docker Hub，以便于云端部署。最终，用户将获得一个功能完整的、可生产部署的机器学习API。",
      "translated_title": "使用FastAPI和Docker部署机器学习模型的逐步指南",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/step-by-step-deploying-ml-models-docker.png",
          "alt": "Step-by-Step Guide to Deploying Machine Learning Models with FastAPI and Docker",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "You've trained your machine learning model, and it's performing great on test data."
    },
    {
      "title": "从零开始实现向量搜索：一步步教程 (原标题: Implementing Vector Search from Scratch: A Step-by-Step Tutorial)",
      "link": "https://machinelearningmastery.com/implementing-vector-search-from-scratch-a-step-by-step-tutorial/",
      "pubDate": "Tue, 10 Jun 2025 14:49:47 +0000",
      "isoDate": "2025-06-10T14:49:47.000Z",
      "creator": "Kanwal Mehreen",
      "summary": "# 从零开始实现向量搜索：一步步教程\n\n## 搜索的演进与向量搜索的兴起\n\n文章指出，搜索是计算领域最基本的问题之一。传统的关键词搜索方法（如基于词频和稀有度）存在局限性，它们过于字面化，无法理解用户查询的上下文和真实意图。例如，搜索“汽车修理”可能无法匹配到“车辆维护”的文档。为了弥补这一语义鸿沟，向量搜索应运而生。\n\n向量搜索通过将查询和文档转换为数值向量（高维数组，捕捉文本的语义本质）来匹配含义，而非精确的关键词。它在向量空间中寻找与查询向量距离最近的文档向量，从而返回上下文相关的结果。本教程旨在从头开始构建一个向量搜索系统，帮助读者深入理解其工作原理。\n\n## 向量搜索的核心工作原理\n\n向量搜索主要包含三个核心步骤：\n\n1.  **向量表示（Vector Representation）**：\n    *   将数据（如文本、图像）转换为数值向量。\n    *   常用技术包括词嵌入（Word Embeddings）或神经网络。\n    *   每个向量在高维空间中代表数据的语义。\n2.  **相似度计算（Similarity Calculation）**：\n    *   衡量查询向量与数据集中其他向量的“接近”程度。\n    *   常用度量包括余弦相似度（Cosine Similarity）或欧几里得距离（Euclidean Distance）。\n    *   向量越接近，表示相似度越高。\n3.  **检索（Retrieval）**：\n    *   根据相似度得分返回最相似的 Top-k 项。\n    *   例如，查询“机器学习”会找到与“人工智能”或“深度学习”相关的文档。\n\n## 从零开始构建向量搜索系统（Python 实现）\n\n文章通过一个 Python 示例，逐步演示了如何构建一个简单的向量搜索系统：\n\n### 步骤 1：环境设置\n\n*   使用 NumPy 进行向量操作，Matplotlib 进行可视化。\n*   为保持“从零开始”的理念，避免使用 FAISS 或 spaCy 等外部高级库。\n*   词嵌入将通过一个预定义的字典模拟，而非使用 Word2Vec、GloVe 或 BERT 等预训练模型。\n*   导入必要的库：`numpy`、`matplotlib.pyplot`、`collections.defaultdict`、`re`。\n\n### 步骤 2：创建示例数据集和词嵌入\n\n*   **示例数据集**：包含关于技术的几句话，例如：“Machine learning is powerful”、“Artificial intelligence advances rapidly”等。\n*   **简化 2D 词嵌入**：创建一个字典，将每个词映射到一个 2D 向量。这些向量是任意的，但设计上使得相关词（如“machine”和“neural”）在向量空间中彼此接近，便于可视化。\n\n### 步骤 3：将句子转换为向量\n\n*   定义 `tokenize` 函数：将文本转换为小写并分词。\n*   定义 `sentence_to_vector` 函数：通过平均句子中所有词的词向量来将句子转换为单个向量。如果词不在嵌入字典中，则使用零向量。\n*   将所有文档转换为对应的向量表示 (`doc_vectors`)。\n\n### 步骤 4：实现余弦相似度\n\n*   余弦相似度是衡量向量间角度的常用指标，非常适合比较文本嵌入的语义相似性。\n*   计算公式为两个向量的点积除以它们各自范数的乘积。\n*   函数会处理零向量以避免除以零的错误。\n\n### 步骤 5：构建向量搜索函数\n\n*   实现核心的 `vector_search` 函数，该函数接受查询、文档、嵌入和 `top_k` 参数。\n*   将查询转换为向量。\n*   计算查询向量与每个文档向量之间的余弦相似度。\n*   使用 `np.argsort` 对相似度进行排序，并返回 `top_k` 个最相似的文档及其得分。\n*   **示例查询**：“Machine learning technology”的搜索结果展示了语义相关性，即使文档中不包含确切的短语。\n\n```\nQuery: Machine learning technology\nTop results:\nScore: 1.000, Document: Machine learning is powerful\nScore: 0.999, Document: Deep learning transforms technology\nScore: 0.997, Document: Artificial intelligence advances rapidly\n```\n\n### 步骤 6：向量可视化\n\n*   通过 `plot_vectors` 函数将文档向量（蓝色点）和查询向量（红色星号）在 2D 空间中可视化。\n*   可视化结果清晰地展示了相似项如何在向量空间中聚类，以及查询向量如何靠近语义相关的文档向量。\n\n![从零开始实现向量搜索](https://machinelearningmastery.com/wp-content/uploads/2025/06/Vector-Search-from-Scratch.png)\n*图：从零开始实现向量搜索* \n\n![向量搜索：文档和查询向量的可视化](https://machinelearningmastery.com/wp-content/uploads/2025/06/plot.png)\n*图：向量搜索：文档和查询向量的可视化*\n\n## 向量搜索对 RAG 的重要性\n\n文章强调，在检索增强生成（RAG）系统中，向量搜索是检索步骤的基石。它通过将文档和查询转换为向量，即使对于复杂的查询也能获取上下文相关的准确信息。本教程的简单实现模拟了这一过程：查询向量检索语义上接近的文档，这些文档随后可供语言模型用于生成响应。在实际应用中，虽然需要更高维的嵌入和优化的搜索算法（如 HNSW 或 IVF），但核心思想保持不变。\n\n## 结论\n\n本教程成功从零开始实现了向量搜索。读者可以基于此扩展，使用真实的词嵌入（如来自 Hugging Face Transformers）或通过近似最近邻（ANN）技术优化搜索，从而加深对向量搜索的理解。",
      "shortSummary": "向量搜索通过将查询和文档转换为数值向量并匹配其语义含义，克服了传统关键词搜索的局限性。文章详细介绍了向量搜索的核心原理：向量表示、相似度计算和检索。它通过一个Python教程，从零开始实现了向量搜索系统，包括环境设置、数据与嵌入创建、句子向量化、余弦相似度计算、搜索功能构建及向量可视化。该技术是RAG（检索增强生成）系统的关键组成部分，用于高效检索上下文相关信息。",
      "translated_title": "从零开始实现向量搜索：一步步教程",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/Vector-Search-from-Scratch.png",
          "alt": "Implementing Vector Search from Scratch",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/plot.png",
          "alt": "Output",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "There’s no doubt that search is one of the most fundamental problems in computing."
    },
    {
      "title": "如何优化部署语言模型的大小 (原标题: How to Optimize Language Model Size for Deployment)",
      "link": "https://machinelearningmastery.com/how-to-optimize-language-model-size-for-deployment/",
      "pubDate": "Mon, 09 Jun 2025 16:40:47 +0000",
      "isoDate": "2025-06-09T16:40:47.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "## 如何优化部署语言模型的大小\n\n大型语言模型（LLMs）已广泛应用于现代AI应用，从聊天机器人到企业自动化。然而，在部署这些模型时，其庞大的体积带来了挑战，需要在性能、可访问性、能耗和计算资源消耗之间取得平衡。本文探讨了优化模型大小的概念和实践策略。\n\n![如何优化部署语言模型的大小](https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-ipc-optimize-language-model-size-deployment.png)\n\n### 架构层面的LLM简化方法\n\n语言模型的规模呈指数级增长，例如GPT-2到GPT-4的参数量从15亿激增至超过1750亿。这种规模的扩大虽然带来了卓越的能力，但也导致了在设备端、云端和实时环境中高效部署的挑战，包括训练/微调成本、推理速度（延迟影响用户体验）以及量化带来的精度/鲁棒性权衡。模型大小可以通过多种架构策略进行优化：\n\n1.  **模型蒸馏（知识蒸馏）**\n    *   **原理**：采用“教师-学生”范式，训练一个较小的“学生”模型，使其通过观察“教师”模型生成的输出（如迭代的下一词预测结果和最可能词的概率分布）来学习。教师模型对每个输出的置信度是关键。\n    *   **目标**：在准确性和紧凑性之间取得平衡，通常通过损失函数来指导简化模型的训练过程。\n    *   **示例**：\n        ```python\n        output = teacher_model(input)\n        loss = distillation_loss(student_model(input), output)\n        ```\n\n2.  **模型剪枝**\n    *   **原理**：移除对模型输出贡献最小的权重（即值最低的权重），类似于决策树剪枝以降低复杂性。\n    *   **效果**：训练期间的动态稀疏技术允许模型学习保留或丢弃哪些层间连接。剪枝后得到的稀疏模型可以减少内存使用并可能提升计算速度。\n    *   **示例**：\n        ```python\n        import torch\n\n        def prune_small_weights(model, threshold=1e-3):\n            with torch.no_grad():\n                for name, param in model.named_parameters():\n                    if \"weight\" in name:\n                        mask = param.abs() > threshold\n                        param.mul_(mask) # zero out small weights\n        ```\n\n3.  **层级缩减**\n    *   **原理**：通过减少LLM底层Transformer架构中编码器和解码器的层数，使整体神经网络组件更浅。\n    *   **适用场景**：当语言任务不需要深层上下文推理，或延迟和资源限制优先于额外深度带来的边际效益时。\n    *   **应用**：可以在较高层面应用，移除部分重复的编码器或解码器层。\n    *   **示例**：\n        ```python\n        from transformers import BertModel\n\n        model = BertModel.from_pretrained(\"bert-base-uncased\")\n        # Keeping only the first 6 encoder layers (this Bert Model has 12)\n        model.encoder.layer = model.encoder.layer[:6]\n        ```\n\n4.  **模块化方法（如LoRA - Low-Rank Adaptation）**\n    *   **原理**：通过向冻结权重的预训练模型中注入轻量级、可训练的组件来简化模型适应。\n    *   **优势**：在资源受限和多任务环境中特别有效，减少了为每个任务微调或部署多个完整大小模型的需要。\n\n### 权重层面的优化\n\n与改变模型结构的架构方法不同，权重层面的优化不侧重于组件的硬性简化或权重消除，而是尝试压缩或数值近似权重，以生成更高效、可用于生产的模型。这些方法包括量化、权重共享和压缩编解码器，它们在通常对准确性影响最小的情况下，减少内存占用并提高推理速度。\n\n1.  **量化**\n    *   **原理**：降低权重的精度表示（例如，从32位降至8位）。\n    *   **优势**：在边缘和受限设备上，能显著加速模型的微调和推理。\n    *   **示例**：\n        ```python\n        import torch\n\n        model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n        quantized_model = torch.quantization.prepare(model, inplace=False)\n        quantized_model = torch.quantization.convert(quantized_model, inplace=False)\n        ```\n\n2.  **权重共享**\n    *   **原理**：利用张量分解将大型权重矩阵近似为较小的组件，从而减少冗余值。\n    *   **示例**：\n        ```python\n        import torch.nn as nn\n\n        original = nn.Linear(512, 512)\n        factorized = nn.Sequential(nn.Linear(512, 64), nn.Linear(64, 512))\n        ```\n\n3.  **压缩编解码器**\n    *   **原理**：一种算法方法，用于在模型的特定操作阶段（通常是模型存储和加载）压缩或解压缩权重。\n    *   **特点**：与模型量化不同，它们不会移除权重精度表示的一部分，并且可以在之后完全解压。\n    *   **示例**：\n        ```python\n        import torch\n        import zipfile\n\n        torch.save(model.state_dict(), \"model.pt\")\n        with zipfile.ZipFile(\"model.zip",
      "shortSummary": "本文探讨了优化大型语言模型（LLMs）部署大小的策略，以平衡性能、资源消耗和可访问性。主要方法分为两大类：架构层面的简化和权重层面的优化。架构方法包括知识蒸馏、模型剪枝、层级缩减和模块化方法（如LoRA），旨在改变模型结构。权重优化则通过量化、权重共享和压缩编解码器来压缩或近似权重，以减少内存占用并提高推理速度。这些技术对于LLM在实际生产环境中的高效运行至关重要。",
      "translated_title": "如何优化部署语言模型的大小",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-ipc-optimize-language-model-size-deployment.png",
          "alt": "How to Optimize Language Model Size for Deployment",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "The rise of language models, and more specifically large language models (LLMs), has been of such a magnitude that it has permeated every aspect of modern AI applications &mdash; from chatbots and search engines to enterprise automation and coding assistants."
    },
    {
      "title": "策略性处理缺失数据：Pandas和Scikit-learn中的高级插补技术 (原标题: Dealing with Missing Data Strategically: Advanced Imputation Techniques in Pandas and Scikit-learn)",
      "link": "https://machinelearningmastery.com/dealing-with-missing-data-strategically-advanced-imputation-techniques-in-pandas-and-scikit-learn/",
      "pubDate": "Fri, 06 Jun 2025 12:00:05 +0000",
      "isoDate": "2025-06-06T12:00:05.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "## 策略性处理缺失数据：Pandas和Scikit-learn中的高级插补技术\n\n![文章主图](https://machinelearningmastery.com/wp-content/uploads/2025/05/456XdBNIRKqg8jpZSt_wGA.jpeg)\n\n### 引言\n\n在许多真实世界的数据集中，缺失值普遍存在，原因多样，包括人为错误、数据损坏或不完整的数据收集过程（例如，来自带有可选字段的调查）。虽然存在处理缺失值的基本策略，如完全删除行或列，或用默认值（通常是属性的均值或中位数）进行插补，但这些策略有时不足以满足需求。本文介绍了通过结合使用Pandas和Scikit-learn库实现的一些高级缺失数据插补技术。\n\n### 使用合成员工数据集\n\n为了演示根据特定上下文和问题需求进行缺失值插补的高级策略，文章使用了一个合成创建的员工数据集。该数据集可以从指定的URL轻松加载，并用于后续的插补示例。\n\n### 高级插补技术\n\n文章详细介绍了三种高级插补方法：\n\n#### 1. 链式方程多重插补 (MICE)\n\n*   **原理**：MICE是一种迭代插补方法，它使用多种估计器（如随机森林、贝叶斯岭等）来插补缺失值。默认情况下，使用贝叶斯岭回归方法，该方法将缺失值视为待学习的参数。\n*   **实现**：通过Scikit-learn的`IterativeImputer`实现。文章展示了使用默认贝叶斯岭回归器和指定随机森林回归器进行插补的示例代码。\n*   **结果**：经过MICE处理后，数据集中的所有缺失值均被成功插补。\n\n![插补后数据集样本](https://machinelearningmastery.com/wp-content/uploads/2025/05/Captura-de-pantalla-2025-05-28-a-las-12.34.05.png)\n\n#### 2. K近邻 (KNN) 插补\n\n*   **原理**：与标准K-NN算法类似，此方法通过计算和利用样本间的相似性来估计给定实例中的缺失值。可以利用加权相似度和自定义度量。\n*   **实现**：通过Scikit-learn的`KNNImputer`实现。\n*   **示例**：\n    *   设置K=5，权重为'distance'（邻居对缺失值估计的贡献与它们和目标实例之间的距离成反比）。\n    *   设置K=10，权重为'uniform'（所有选定的邻居对缺失值的估计贡献相等）。\n\n#### 3. 多估计器（集成）插补\n\n*   **原理**：这种策略是构建多个不同类型的插补估计器，每个估计器都会生成一个带有插补值的完整数据集版本。然后，通过检查每个数据集并关注包含缺失值的最关键属性，可以根据哪个（或哪些）估计器为特定数据上下文提供最真实或最一致的插补结果来决定选择其中一个版本，甚至对两个或更多版本进行聚合。\n*   **实现**：文章展示了如何使用贝叶斯岭、Extra Trees和随机森林回归器作为估计器来创建多个插补数据集的示例。\n\n### 总结\n\n文章最后通过一个表格总结了所探讨的三种方法的特点，并提供了何时使用（或避免）每种方法的建议：\n\n![插补策略选择指南](https://machinelearningmastery.com/wp-content/uploads/2025/05/imputing_Table-scaled.png)\n\n*   **KNN插补**：非常适合小型数值数据集，但对于大型数据集计算成本高昂。\n*   **集成估计器**：通常提供最佳质量，但它们是最复杂且计算成本最高的方法。\n*   **MICE**：通常是一种平衡的方法，适用于各种场景。",
      "shortSummary": "本文探讨了使用Pandas和Scikit-learn处理缺失数据的高级插补技术，以克服传统方法的局限性。文章详细介绍了三种主要策略：链式方程多重插补（MICE）、K近邻（KNN）插补和多估计器（集成）插补。MICE是一种平衡的方法，KNN适用于小型数据集，而集成方法能提供最佳质量但计算成本最高。选择哪种方法取决于具体的数据特性和需求。",
      "translated_title": "策略性处理缺失数据：Pandas和Scikit-learn中的高级插补技术",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/05/456XdBNIRKqg8jpZSt_wGA.jpeg",
          "alt": "Dealing with Missing Data Strategically: Advanced Imputation Techniques in Pandas and Scikit-learn",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/05/Captura-de-pantalla-2025-05-28-a-las-12.34.05.png",
          "alt": "Dataset sample with imputed missing values",
          "title": "",
          "position": 2
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/05/imputing_Table-scaled.png",
          "alt": "When to use one imputation strategy or another.",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "完整文章",
      "content": "Missing values appear more often than not in many real-world datasets."
    },
    {
      "title": "损失函数详解：每种只需2分钟即可理解其数学原理 (原标题: Loss Functions Explained: Understand the Maths in Just 2 Minutes Each)",
      "link": "https://machinelearningmastery.com/loss-functions-explained-understand-the-maths-in-just-2-minutes-each/",
      "pubDate": "Thu, 05 Jun 2025 13:59:36 +0000",
      "isoDate": "2025-06-05T13:59:36.000Z",
      "creator": "Kanwal Mehreen",
      "summary": "# 损失函数详解：每种只需2分钟即可理解其数学原理\n\n![损失函数详解](https://machinelearningmastery.com/wp-content/uploads/2025/05/Loss-Functions-Explained.png)\n\n## 引言\n\n在机器学习领域，理解模型如何评估其预测的准确性至关重要。损失函数（Loss Function）正是实现这一目标的核心工具，它量化了模型预测值与真实值之间的差异。通过最小化损失函数，模型能够学习并优化其内部参数（权重和偏差），从而提高预测性能。\n\n## 损失函数与成本函数\n\n文章首先澄清了损失函数和成本函数（Cost Function）之间的常见混淆：\n*   **损失函数**：衡量单个数据点的预测误差。\n*   **成本函数**：衡量所有训练样本的平均损失。\n在模型训练过程中，通常是最小化成本函数，因为它反映了模型在整体数据上的表现。\n\n## 常见损失函数类型\n\n文章详细介绍了六种常用的损失函数，包括它们的数学原理、直观解释、适用场景及优缺点。\n\n### 1. 均方误差 (Mean Squared Error, MSE)\n\n*   **用途**：主要用于回归任务。\n*   **公式**：\n    $$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i – \\hat{y}_i)^2 $$\n    其中 \\( n \\) 是数据点数量，\\( y_i \\) 是真实值，\\( \\hat{y}_i \\) 是预测值。\n*   **直观解释**：计算真实值与预测值之差的平方的平均值。由于误差被平方，较大的误差会受到更严厉的惩罚。\n*   **优点**：\n    *   简单易懂，广泛使用。\n    *   对大错误有较强的惩罚作用，适用于对大偏差敏感的场景（如医疗剂量、金融预测）。\n*   **缺点**：\n    *   对异常值非常敏感，单个异常值可能显著影响损失，导致模型偏向拟合异常值。\n    *   损失的单位是原始单位的平方，直接解释性较差。\n\n### 2. 平均绝对误差 (Mean Absolute Error, MAE)\n\n*   **用途**：主要用于回归任务。\n*   **公式**：\n    $$ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i – \\hat{y}_i \\right| $$\n    其中 \\( n \\) 是数据点数量，\\( y_i \\) 是真实值，\\( \\hat{y}_i \\) 是预测值。\n*   **直观解释**：计算真实值与预测值之差的绝对值的平均值。所有误差都以线性方式贡献，无论大小。\n*   **优点**：\n    *   对异常值具有更好的鲁棒性，因为它不会平方误差。\n    *   损失的单位与原始数据单位一致，更具可解释性。\n*   **缺点**：\n    *   在误差为零时不可微，这可能使基于梯度的优化方法在训练过程中略显困难或缓慢。\n\n### 3. Huber 损失 (Huber Loss)\n\n*   **用途**：回归任务，结合了 MSE 和 MAE 的优点。\n*   **公式**：\n    $$ L_\\delta(y, \\hat{y}) = \\begin{cases} \\frac{1}{2}(y – \\hat{y})^2 & \\text{if } |y – \\hat{y}| \\leq \\delta \\\\ \\delta \\cdot \\left(|y – \\hat{y}| – \\frac{1}{2} \\delta\\right) & \\text{if } |y – \\hat{y}| > \\delta \\end{cases} $$\n    其中 \\( \\delta \\) 是一个阈值。\n*   **直观解释**：当误差较小时，行为类似于 MSE（平方误差）；当误差较大时，行为类似于 MAE（线性误差）。\n*   **优点**：\n    *   对异常值具有鲁棒性，同时在误差较小时保持可微性。\n    *   可以通过调整 \\( \\delta \\) 来控制对大误差的容忍度。\n*   **缺点**：\n    *   引入了一个新的超参数 \\( \\delta \\) 需要调优。\n    *   对于非常干净或非常嘈杂的数据集，可能不如纯粹的 MSE 或 MAE 表现好。\n\n### 4. 铰链损失 (Hinge Loss)\n\n*   **用途**：主要用于二分类问题，特别是支持向量机 (SVMs)，旨在实现“自信地正确”分类。\n*   **公式**：\n    $$ L(y, \\hat{y}) = \\max(0, 1 – y \\cdot \\hat{y}) $$\n    其中 \\( y \\) 是真实标签（期望为 -1 或 +1），\\( \\hat{y} \\) 是模型预测的原始分数。\n*   **直观解释**：当模型正确分类且具有足够大的决策边界时，损失为零。否则，损失为正，促使模型提高置信度。\n*   **优点**：\n    *   适用于 SVM 等旨在最大化决策边界的模型。\n    *   鼓励模型不仅正确分类，而且要“自信地”正确分类。\n*   **缺点**：\n    *   不适用于输出概率的模型。\n    *   在“铰链点”不可微，可能使优化复杂化。\n    *   要求真实标签为 -1 或 +1。\n\n### 5. 二元交叉熵 (Binary Cross-Entropy, BCE)\n\n*   **用途**：二分类问题，适用于模型输出概率的场景。\n*   **公式**：\n    $$ L = – \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 – y_i) \\log(1 – \\hat{y}_i) \\right] $$\n    其中 \\( y_i \\in \\{0, 1\\} \\) 是真实标签，\\( \\hat{y}_i \\in (0, 1) \\) 是预测概率。\n*   **直观解释**：当真实标签为 1 时，损失仅在预测概率接近 1 时才低；当真实标签为 0 时，损失仅在预测概率接近 0 时才低。对“自信但错误”的预测施加重罚。\n*   **优点**：\n    *   产生平滑的损失曲面，有利于基于梯度的优化算法。\n    *   非常适合模型输出概率且任务严格为二分类的情况。\n*   **缺点**：\n    *   可能过度惩罚噪声或错误标记的数据点。\n    *   在类别高度不平衡的数据集上可能表现不佳，除非采取额外处理措施。\n\n### 6. 分类交叉熵 (Categorical Cross-Entropy, CCE)\n\n*   **用途**：多分类问题，是二元交叉熵的扩展。\n*   **公式**：\n    $$ \\text{Loss} = – \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c}) $$\n    其中 \\( C \\) 是类别总数，\\( y_{i,c} \\in \\{0,1\\} \\) 是真实标签（独热编码），\\( \\hat{y}_{i,c} \\in (0,1) \\) 是预测概率。\n*   **直观解释**：本质上是提取真实类别的对数概率，并惩罚模型对真实类别预测概率较低的情况。\n*   **优点**：\n    *   适用于具有清晰、互斥类别的多分类问题。\n    *   对“自信但错误”的预测进行重罚，促使模型准确且谨慎。\n*   **缺点**：\n    *   不适用于多标签问题（一个样本可属于多个类别）。\n    *   在类别不平衡的数据集上可能需要额外的处理。",
      "shortSummary": "损失函数是机器学习中衡量模型预测误差的关键工具，通过最小化它来优化模型。损失函数衡量单个数据点误差，而成本函数衡量平均误差。文章介绍了六种常见损失函数：回归任务的均方误差（MSE）、平均绝对误差（MAE）和 Huber 损失；以及分类任务的铰链损失、二元交叉熵（BCE）和分类交叉熵（CCE）。每种损失函数都有其独特的数学原理、适用场景及优缺点，选择合适的损失函数对模型性能至关重要。",
      "translated_title": "损失函数详解：每种只需2分钟即可理解其数学原理",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/05/Loss-Functions-Explained.png",
          "alt": "Loss Functions Explained",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "I must say, with the ongoing hype around machine learning, a lot of people jump straight to the application side without really understanding how things work behind the scenes."
    },
    {
      "title": "机器学习从业者应了解的10种MLOps工具 (原标题: 10 MLOps Tools for Machine Learning Practitioners to Know)",
      "link": "https://machinelearningmastery.com/10-mlops-tools-for-machine-learning-practitioners-to-know/",
      "pubDate": "Thu, 05 Jun 2025 12:00:07 +0000",
      "isoDate": "2025-06-05T12:00:07.000Z",
      "creator": "Jayita Gulati",
      "summary": "## 机器学习从业者应了解的10种MLOps工具\n\n![MLOps封面图片](https://machinelearningmastery.com/wp-content/uploads/2025/05/mlops_cover_photo.jpeg)\n\n### MLOps简介\n\n机器学习不仅仅是构建模型，还包括模型的部署、管理和维护。机器学习运维（MLOps）将机器学习与DevOps实践相结合，旨在简化从训练到部署的整个模型生命周期。它确保了机器学习工作流的自动化、协作和可扩展性。为了支持这一目标，一系列工具应运而生。本文重点介绍了机器学习从业者应了解的10种基本MLOps工具，它们有助于构建可靠且可投入生产的机器学习系统。\n\n### 10种核心MLOps工具\n\n1.  **MLflow**\n    *   **功能：** 帮助跟踪机器学习实验，记录训练运行、版本化模型并管理部署阶段。它与多种流行的机器学习库兼容，可在任何环境中运行。\n    *   **关键特性：**\n        *   跟踪每次运行的指标、参数和工件。\n        *   保存和版本化模型以实现可复现性。\n        *   管理模型在不同生命周期阶段的状态。\n\n2.  **Weights & Biases (W&B)**\n    *   **功能：** 一个用于记录和可视化机器学习实验的平台。它帮助团队监控模型性能并随时间组织实验。W&B集成了TensorFlow、PyTorch和Keras等多种ML库。\n    *   **关键特性：**\n        *   实时记录训练性能。\n        *   比较多次运行和超参数。\n        *   跟踪数据集、代码和模型文件。\n\n3.  **Comet**\n    *   **功能：** 帮助端到端监控机器学习实验的工具。它跟踪指标、参数、代码和工件，使实验可复现且文档完善。\n    *   **关键特性：**\n        *   跟踪实验、超参数和结果。\n        *   使用可视化仪表板比较模型运行。\n        *   记录代码版本和数据集更改。\n        *   组织项目并与团队协作。\n\n4.  **Apache Airflow**\n    *   **功能：** 一个工作流自动化工具。它允许定义和调度机器学习任务，如数据预处理、训练、评估和部署。工作流以Python代码编写，Airflow负责执行顺序。\n    *   **关键特性：**\n        *   使用Python脚本定义机器学习工作流。\n        *   调度和自动化重复性任务。\n        *   通过Web界面监控任务进度。\n        *   处理重试、故障和依赖关系。\n\n5.  **Kubeflow**\n    *   **功能：** 一个基于Kubernetes的平台，用于构建和管理机器学习工作流。它允许在云端或本地Kubernetes集群上运行训练、超参数调优和模型服务。\n    *   **关键特性：**\n        *   完全控制地构建机器学习管道。\n        *   在Kubernetes集群上大规模运行作业。\n        *   提供用于调优、服务和跟踪模型的工具。\n\n6.  **DVC (Data Version Control)**\n    *   **功能：** 类似于Git，但用于数据和模型。它帮助版本化数据集、跟踪更改，并使所有内容在实验中保持同步。它与Git配合良好，并集成了S3或Google Drive等远程存储。\n    *   **关键特性：**\n        *   跟踪和版本化数据集和模型。\n        *   将大文件连接到Git而不直接存储它们。\n        *   使用一致的数据和代码复现实验。\n        *   通过远程存储集成共享项目。\n\n7.  **Metaflow**\n    *   **功能：** 帮助数据科学家和机器学习工程师使用简单的Python代码构建和管理工作流。它支持在本地和云端跟踪、调度和扩展机器学习管道。\n    *   **关键特性：**\n        *   在本地或云端运行管道。\n        *   自动跟踪运行和元数据。\n        *   从上一步恢复失败的运行。\n\n8.  **Pachyderm**\n    *   **功能：** 一个数据管道和版本控制系统。它帮助管理和跟踪数据更改，并构建可复现的管道，这些管道在数据更改时自动更新。\n    *   **关键特性：**\n        *   像Git一样对数据集进行版本控制。\n        *   构建在数据更新时自动运行的管道。\n        *   通过完整的数据和代码历史复现结果。\n        *   与Docker和任何机器学习语言兼容。\n\n9.  **Evidently AI**\n    *   **功能：** 一个用于机器学习模型的监控工具。它有助于在部署后检测数据漂移、性能下降或预测不一致等问题。\n    *   **关键特性：**\n        *   监控数据质量和模型性能。\n        *   检测数据漂移和随时间的变化。\n        *   生成清晰的可视化报告和仪表板。\n\n10. **TensorFlow Extended (TFX)**\n    *   **功能：** 谷歌的TensorFlow机器学习管道平台。它涵盖了从数据处理到模型训练、验证和在实际环境中部署的所有环节。\n    *   **关键特性：**\n        *   使用可重用组件构建完整的机器学习管道。\n        *   处理数据验证和模型评估。\n        *   使用可扩展的服务工具部署模型。\n        *   可与Apache Airflow或Kubeflow编排工具配合使用。\n\n### 总结\n\nMLOps是现代机器学习不可或缺的一部分，它帮助团队将模型从开发环境推向实际应用。没有MLOps，项目可能难以扩展或在生产中出现问题。选择合适的工具能使这一过程更简单、更可靠。MLflow和W&B等工具用于实验跟踪；Airflow和Kubeflow用于自动化和运行机器学习管道；DVC和Pachyderm负责数据和模型版本控制；Evidently AI支持模型性能监控；TFX则提供了一整套用于生产级机器学习系统的管道。最佳的设置取决于团队的规模、目标和基础设施。通过使用这些工具，可以节省时间、减少错误并提高模型质量。",
      "shortSummary": "MLOps（机器学习运维）对于现代机器学习至关重要，它简化了模型从训练到部署的整个生命周期。本文介绍了10种核心MLOps工具，涵盖实验跟踪（如MLflow、Weights & Biases）、工作流自动化（如Airflow、Kubeflow）、数据和模型版本控制（如DVC、Pachyderm）、模型监控（如Evidently AI）以及端到端管道构建（如TFX）。这些工具旨在帮助机器学习从业者构建可靠、可投入生产的系统，从而节省时间、减少错误并提升模型质量。",
      "translated_title": "机器学习从业者应了解的10种MLOps工具",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/05/mlops_cover_photo.jpeg",
          "alt": "10 MLOps Tools for Machine Learning Practitioners to Know",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "Machine learning is not just about building models."
    },
    {
      "title": "NumPy 忍术：掌握数组操作以实现高性能机器学习 (原标题: NumPy Ninjutsu: Mastering Array Operations for High-Performance Machine Learning)",
      "link": "https://machinelearningmastery.com/numpy-ninjutsu-mastering-array-operations-for-high-performance-machine-learning/",
      "pubDate": "Wed, 04 Jun 2025 12:00:20 +0000",
      "isoDate": "2025-06-04T12:00:20.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "# NumPy 忍术：掌握数组操作以实现高性能机器学习\n\n机器学习工作流涉及大量数值计算，数据通常以大型向量、矩阵或张量的形式存储。处理、训练和应用这些大型数据结构（如预测）会消耗大量时间和内存。因此，优化这些底层操作的效率至关重要。NumPy 库正是为此而生：NumPy 数组是为快速、内存高效的数值计算而设计的数据结构，它利用向量化和广播等敏捷计算过程，从而实现高性能的机器学习建模过程，如忍者般快速而无声。本文揭示了一些具有代表性的 NumPy 数组操作示例，它们在优化机器学习工作流性能方面特别有用。\n\n![NumPy Ninjutsu: Mastering Array Operations for High-Performance Machine Learning](https://machinelearningmastery.com/wp-content/uploads/2025/05/r2k5UU1ZRqyC91TTOXMyyQ.webp)\n\n## 核心 NumPy 数组操作\n\n### 1. 向量化操作\n\nNumPy 允许对整个数组应用算术操作或数学函数，从而实现元素级的操作，而无需使用循环。例如，给定一个包含 1000 个元素的数组 `arr`，`arr * 2` 会将数组中的每个元素乘以二。在使用函数方面，向量化在手动定义的神经网络中进行激活函数计算时非常方便。ReLU（修正线性单元）函数是一种非常常见的激活函数，已被证明在训练神经网络模型中有效。它通过将负值映射到 0 并保持正值不变来消除信息的线性。以下是使用 NumPy 数组向量化实现 ReLU 激活的示例：\n\n```python\nimport numpy as np\ninput_array = np.array([-2.0, 0.0, 1.5])\noutput = np.maximum(0, input_array)\nprint(output)\n```\n\n**输出:**\n```\n[0. 0. 1.5]\n```\n这个例子定义了一个包含三个元素的数组，以方便理解。当处理成千上万甚至数百万个元素时，效率优化才能真正发挥作用。\n\n### 2. 广播机制 (Broadcasting)\n\nNumPy 数组操作的另一个吸引人的特性是广播：它涉及调整参与数学运算的多个数组中至少一个的大小。数据矩阵中的值标准化就是一个很好的例子，这是各种机器学习建模技术中非常常见的处理过程，通常需要缩放数据以获得更有效的结果。\n\n```python\nimport numpy as np\nbatch = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\nnormalized = (batch - batch.mean(axis=0)) / batch.std(axis=0)\nprint(normalized)\n```\n\n**输出:**\n```\narray([[-1.22474487, -1.22474487],\n       [ 0.        ,  0.        ],\n       [ 1.22474487,  1.22474487]])\n```\n在上述示例中，`batch.mean(axis=0)` 和 `batch.std(axis=0)` 都是包含两个元素的 1D 数组：分别是按列计算的均值和标准差。因此，对于 2D 矩阵中的每个元素，标准化包括减去该元素所属列的均值并除以其标准差。\n\n### 3. 矩阵乘法 (Matrix Multiplication)\n\n矩阵乘法是许多机器学习模型（包括经典模型和基于神经网络的模型）应用线性变换的核心，例如将信息流与两个连续全连接神经元层之间的连接权重相乘。即使在像 Transformer 这样的大型模型中，这种操作也随处可见，它们是语言模型的基础。以下是 NumPy 中模拟两个各包含两个神经元的全连接层的工作方式：\n\n```python\nimport numpy as np\nweights = np.array([[0.2, 0.8], [0.5, 0.1]])\ninputs = np.array([1.0, 2.0])\nbias = np.array([0.1, -0.1])\noutput = np.dot(weights, inputs) + bias\nprint(np.round(output, 2))\n```\n\n**输出:**\n```\n[1.9 0.6]\n```\n\n### 4. 通过掩码进行高级行选择 (Advanced Row Selection by Masking)\n\n当需要根据某些外部条件选择数据集中的特定实例时，此功能非常有用，例如，通过布尔掩码（一个由“真”或“假”元素组成的 1D 数组）来决定过滤 2D 数据集中的哪些行。以下示例使用一个掩码来选择数据集矩阵中的第二个和第三个实例：\n\n```python\nimport numpy as np\ndata = np.array([[1, 2], [3, 4], [5, 6]])\nlabels = np.array([0, 1, 1])\nfiltered = data[labels == 1]\nprint(filtered)\n```\n\n**输出:**\n```\n[[3 4]\n [5 6]]\n```\n\n### 5. ArgMax 用于概率分类预测 (ArgMax for Probabilistic Class Prediction)\n\n几种分类模型使用名为 Softmax 的函数来计算实例属于某个类别的归一化概率（在多个互斥类别中）。在通过顺序应用下一个词预测问题来逐词生成文本响应的语言模型中，这种 Softmax 原理变得极其复杂，需要计算词汇表中每个词（通常是人类语言）作为下一个词的概率。多亏了 `np.argmax`，找到概率最高的词（或通常来说，类别）变得异常容易。此示例演示了此函数在两个实例中的应用，其中属于三个可能类别的概率存储在 `logits` 矩阵中：\n\n```python\nimport numpy as np\nlogits = np.array([[0.2, 0.8, 0.0], [0.5, 0.3, 0.2]])\npredictions = np.argmax(logits, axis=1)\nprint(predictions)\n```\n\n**输出:**\n```\n[1 0]\n```\n输出是为每个实例选择的类别（类别默认从 0 到 2 索引）。\n\n### 6. 使用 Einsum 进行自定义张量操作 (Custom Tensor Operations with Einsum)\n\nEinsum（爱因斯坦求和的缩写）是 NumPy 中一个有趣的函数。乍一看可能觉得微不足道，但这个函数通过特定的符号表示法来表达数组上的代数运算，如点积、外积，甚至 Transformer 注意力机制，使其易于理解。它还可以方便地在深度学习架构中构建自定义层。为了初步了解此函数，我们来看这个示例，它使用等效的“einsum”表达式来表示矩阵乘法的应用：`'ij,jk->ik'`。\n\n```python\nimport numpy as np\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\nresult = np.einsum('ij,jk->ik', A, B)\nprint(result)\n```\n\n**输出:**\n```\n[[19 22]\n [43 50]]\n```\n如果这些函数背后的机制不像真正的忍术，那还有什么像呢？有关此函数工作原理的更多信息，请查阅 NumPy 文档页面。\n\n## 结论\n\n本文揭示了 Python 的 NumPy 库提供的六种引人入胜的“忍术技巧”策略，用于高效执行数组操作，这对于扩展需要对数据、模型权重等进行密集计算的自定义机器学习工作流非常有用。",
      "shortSummary": "NumPy 是高性能机器学习的关键库，通过其高效的数组操作优化了数值计算。文章介绍了六种核心“忍术”技巧：向量化操作（如 ReLU）、广播机制（如数据标准化）、矩阵乘法、通过掩码进行高级行选择、使用 `np.argmax` 进行概率分类预测，以及利用 `np.einsum` 进行自定义张量操作。这些功能使 NumPy 能够显著提升机器学习工作流的计算效率和性能。",
      "translated_title": "NumPy 忍术：掌握数组操作以实现高性能机器学习",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/05/r2k5UU1ZRqyC91TTOXMyyQ.webp",
          "alt": "NumPy Ninjutsu: Mastering Array Operations for High-Performance Machine Learning",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "Machine learning workflows typically involve plenty of numerical computations in the form of mathematical and algebraic operations upon data stored as large vectors, matrices, or even tensors &mdash; matrix counterparts with three or more dimensions."
    },
    {
      "title": "10个简化特征工程的Python单行代码 (原标题: 10 Python One-Liners That Will Simplify Feature Engineering)",
      "link": "https://machinelearningmastery.com/10-python-one-liners-that-will-simplify-feature-engineering/",
      "pubDate": "Tue, 03 Jun 2025 12:00:59 +0000",
      "isoDate": "2025-06-03T12:00:59.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "## 10个简化特征工程的Python单行代码\n\n![10 Python One-Liners That Will Simplify Feature Engineering](https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-10-python-one-liners-feat-engineering.png)\n\n### 引言\n\n特征工程是数据分析工作流中的关键环节，尤其在构建机器学习模型时。它涉及基于现有原始数据特征创建新特征，以提取更深层次的分析洞察并提升模型性能。本文介绍了10个实用的Python单行代码，旨在简化和优化特征工程及数据准备流程，使其高效且简洁。\n\n### 准备工作\n\n在开始之前，需要导入一些关键的Python库和模块，包括`pandas`、`numpy`以及来自`sklearn.preprocessing`、`sklearn.feature_selection`和`sklearn.decomposition`的类。文章使用了Scikit-learn数据集模块中公开可用的葡萄酒数据集（`wine`）和波士顿住房数据集（`boston`），并将其加载到Pandas数据框`df_wine`和`df_boston`中。\n\n### 10个Python单行代码\n\n以下是简化特征工程的10个实用单行代码：\n\n1.  **数值特征标准化（Z-score缩放）**\n    *   **目的**：当数值特征的取值范围或量级差异较大，且可能存在适度异常值时，标准化是一种常用的缩放方法。它将数值转换为均值为0、标准差为1的标准正态分布。\n    *   **工具**：Scikit-learn的`StandardScaler`类。\n    *   **示例**：`df_wine_std = pd.DataFrame(StandardScaler().fit_transform(df_wine.drop('target', axis=1)), columns=df_wine.columns[:-1])`\n\n2.  **最小-最大缩放**\n    *   **目的**：当特征值在实例间均匀变化时，最小-最大缩放（Min-Max Scaling）是一种合适的缩放方式，它将特征值归一化到[0,1]区间。\n    *   **工具**：Scikit-learn的`MinMaxScaler`类。\n    *   **示例**：`df_boston_scaled = pd.DataFrame(MinMaxScaler().fit_transform(df_boston.drop('MEDV', axis=1)), columns=df_boston.columns[:-1])`（`MEDV`是目标变量，被排除在外）。\n\n3.  **添加多项式特征**\n    *   **目的**：当数据呈现非线性关系时，添加多项式特征非常有用。它通过将原始特征提升到一定幂次以及创建特征间的交互项来生成新特征。\n    *   **工具**：Scikit-learn的`PolynomialFeatures`类。\n    *   **示例**：`df_interactions = pd.DataFrame(PolynomialFeatures(degree=2, include_bias=False).fit_transform(df_wine[['alcohol', 'malic_acid']]))`（基于“alcohol”和“malic_acid”创建了“alcohol^2”、“malic_acid^2”和“alcohol * malic_acid”）。\n\n4.  **独热编码分类变量**\n    *   **目的**：将一个包含“m”个可能值的分类变量转换为“m”个二进制（0或1）特征，每个特征表示一个类别的出现或不出现。这对于机器学习模型处理纯分类特征至关重要。\n    *   **工具**：Pandas的`get_dummies`函数。\n    *   **示例**：`df_boston_ohe = pd.get_dummies(df_boston.astype({'CHAS': 'category'}), columns=['CHAS'])`\n\n5.  **离散化连续变量**\n    *   **目的**：将连续数值变量离散化为若干等宽子区间或箱，常用于可视化，使图表更易理解，同时仍能捕捉“大局”。\n    *   **工具**：Pandas的`pd.qcut`函数。\n    *   **示例**：`df_wine['alcohol_bin'] = pd.qcut(df_wine['alcohol'], q=4, labels=False)`（将“alcohol”属性离散化为四个箱）。\n\n6.  **偏斜特征的对数变换**\n    *   **目的**：如果数值特征呈右偏或正偏（即由于少数过大值导致长尾），对数变换有助于将其缩放到更适合进一步分析的形式。\n    *   **工具**：Numpy的`np.log1p`函数。\n    *   **示例**：`df_wine['log_malic'] = np.log1p(df_wine['malic_acid'])`\n\n7.  **创建两个特征之间的比率**\n    *   **目的**：在数据分析和预处理中，创建两个语义相关特征的比率作为新特征是一种直接而常见的特征工程步骤。\n    *   **工具**：Pandas直接进行除法运算。\n    *   **示例**：`df_wine['alcohol_malic_ratio'] = df_wine['alcohol'] / df_wine['malic_acid']`\n\n8.  **移除低方差特征**\n    *   **目的**：识别并移除方差非常小的特征，因为它们对分析或机器学习模型的贡献很小，甚至可能使结果变差。\n    *   **工具**：Scikit-learn的`VarianceThreshold`类。\n    *   **示例**：`df_boston_high_var = pd.DataFrame(VarianceThreshold(threshold=0.1).fit_transform(df_boston.drop('MEDV', axis=1)))`（`MEDV`作为目标变量被手动移除）。\n\n9.  **乘法交互**\n    *   **目的**：通过将两个现有特征相乘来创建新特征，通常用于将多个信息综合为一个单一的分数。\n    *   **工具**：Pandas直接进行乘法运算。\n    *   **示例**：`df_wine['wine_quality'] = df_wine['alcohol'] * df_wine['color_intensity']`\n\n10. **跟踪异常值**\n    *   **目的**：在某些情况下，与其移除异常值，不如创建一个新特征来指示数据实例是否为异常值。\n    *   **方法**：手动应用四分位距（IQR）方法来识别潜在异常值。\n    *   **示例**：`df_boston['tax_outlier'] = ((df_boston['TAX'] < df_boston['TAX'].quantile(0.25) - 1.5 * (df_boston['TAX'].quantile(0.75) - df_boston['TAX'].quantile(0.25))) | (df_boston['TAX'] > df_boston['TAX'].quantile(0.75) + 1.5 * (df_boston['TAX'].quantile(0.75) - df_boston['TAX'].quantile(0.25)))).astype(int)`\n\n### 结论\n\n本文介绍了10个有效的Python单行代码，它们能够高效地执行各种特征工程步骤，从而将数据转化为适合进一步分析或构建机器学习模型的良好形式。",
      "shortSummary": "本文介绍了10个实用的Python单行代码，旨在简化和加速特征工程流程。这些代码涵盖了多种关键任务，包括数值特征的标准化和最小-最大缩放、添加多项式特征、独热编码分类变量、离散化连续变量、对数变换偏斜特征、创建特征比率、移除低方差特征、乘法交互以及跟踪异常值。通过利用Pandas和Scikit-learn等库，这些单行代码能高效地准备数据，提升机器学习模型的性能。",
      "translated_title": "10个简化特征工程的Python单行代码",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-10-python-one-liners-feat-engineering.png",
          "alt": "10 Python One-Liners That Will Simplify Feature Engineering",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "Feature engineering is a key process in most data analysis workflows, especially when constructing machine learning models."
    },
    {
      "title": "语言模型中的词嵌入 (原标题: Word Embeddings in Language Models)",
      "link": "https://machinelearningmastery.com/word-embeddings-in-language-models/",
      "pubDate": "Mon, 02 Jun 2025 04:06:23 +0000",
      "isoDate": "2025-06-02T04:06:23.000Z",
      "creator": "Adrian Tam",
      "summary": "## 语言模型中的词嵌入：核心概念与应用\n\n自然语言处理（NLP）领域随着词嵌入（Word Embeddings）的引入发生了巨大变革。在此之前，NLP主要依赖将词汇视为离散符号的基于规则的方法。词嵌入的出现使计算机能够通过向量空间表示来理解语言。\n\n![图片 1](https://machinelearningmastery.com/wp-content/uploads/2025/06/pexels-satoshi-3322920-scaled.jpg)\n\n### 1. 理解词嵌入\n\n词嵌入将词汇表示为连续空间中的密集向量，其中语义相似的词汇彼此靠近。其核心原则是：出现在相似上下文中的词汇应具有相似的向量表示。这类模型通常通过无监督学习进行训练，以学习训练语料库中的词汇共现模式。\n\n*   **Word2Vec**：由“Efficient Estimation of Word Representations in Vector Space”论文首次提出，开创了这一方法。它使用神经网络根据局部上下文预测词汇，并有两种变体：\n    *   **连续词袋模型（CBOW）**：根据上下文预测目标词。速度更快，适用于大型数据集。\n    *   **Skip-gram**：根据目标词预测上下文词。在小型数据集和稀有词方面表现更好。\n    Word2Vec通过展示词嵌入向量可以满足“国王 – 男人 + 女人 ≈ 女王”等方程，证明了计算机能够理解词汇间的语义关系。\n\n*   **GloVe (Global Vectors for Word Representation)**：通过构建和分解词汇共现矩阵来获取嵌入。它结合了全局矩阵分解方法（如潜在语义分析）和局部上下文窗口方法（如Word2Vec）的优点，捕获词汇的语义和句法关系。\n\n*   **FastText**：在Word2Vec的基础上进行了改进，通过学习字符n-gram的向量而非整个词汇的向量。这种方法捕获了子词信息，解决了词汇表外（OOV）问题，并为形态丰富的语言提供了更好的性能。\n\n*   **ELMo (Embeddings from Language Models)**：使用深度双向LSTM生成上下文相关的词向量。与之前的模型不同，ELMo的词向量不是固定的，而是根据上下文变化的。尽管在大语言模型出现后使用较少，但ELMo的“词义应依赖于上下文”的核心思想构成了所有现代语言模型的基础。\n\n### 2. 使用预训练词嵌入\n\n可以轻松使用流行库中提供的预训练词嵌入。例如，使用`gensim`库加载GloVe嵌入，可以查询相似词或进行词语类比（如“国王 + 女人 – 男人”得到“女王”）。这需要下载相应的预训练模型文件。\n\n### 3. 训练自定义词嵌入\n\n*   **使用Gensim训练Word2Vec**：`gensim`提供了简单的接口来训练自定义的Word2Vec模型。训练一个有用的嵌入需要一个大型语料库。训练时可配置向量维度（`vector_size`）、上下文窗口大小（`window`）、最小词频（`min_count`）和模型类型（`sg`，0为CBOW，1为Skip-gram）。\n\n*   **使用PyTorch从零开始训练Word2Vec**：可以利用PyTorch实现一个基本的Skip-gram模型。模型包含一个`nn.Embedding`层（即词嵌入矩阵）和一个`nn.Linear`层。训练过程涉及将词汇转换为索引，然后使用这些索引获取嵌入向量。同样，为了获得高质量的模型，需要更大的语料库和更多的训练周期。词汇到索引的映射表（如`vocab_to_idx`）需要与模型一同保存。\n\n### 4. 嵌入在Transformer模型中的应用\n\n现代语言模型（如BERT）广泛使用学习到的嵌入。BERT模型包含`BertEmbeddings`层，其中又包含`word_embeddings`、`position_embeddings`和`token_type_embeddings`。`word_embeddings`层是一个数值矩阵，其行索引对应于分词器（Tokenizer）分配的词元ID。当输入文本被分词为词元ID序列后，嵌入层会将每个词元ID替换为嵌入矩阵中对应的行向量，从而将词元ID序列转换为嵌入向量序列。\n\n![Building Transformer Models with Attention](https://machinelearningmastery.com/wp-content/uploads/2022/11/BTMA-400.png)\n\n### 总结\n\n词嵌入将词汇表示为连续空间中的密集向量，使语义相似的词汇彼此靠近。预训练词嵌入可通过流行库直接使用，同时也可以使用Gensim或PyTorch训练自定义词嵌入。现代Transformer模型（如BERT）通过`nn.Embedding`层利用学习到的嵌入。嵌入对于捕捉词汇间的语义关系至关重要，是理解和处理人类语言的基础。",
      "shortSummary": "词嵌入通过将词汇转换为密集向量，彻底改变了自然语言处理。它们使计算机能够理解词汇间的语义关系，其中语义相似的词汇在向量空间中距离相近。Word2Vec、GloVe、FastText和ELMo是重要的词嵌入模型。现代语言模型（如Transformer）广泛使用这些学习到的嵌入，通常通过嵌入层将词元ID转换为向量。用户可以使用预训练嵌入，或利用Gensim和PyTorch训练自定义嵌入，这对于捕捉词汇语义至关重要。",
      "translated_title": "语言模型中的词嵌入",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/pexels-satoshi-3322920-scaled.jpg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2022/11/BTMA-400.png",
          "alt": "Building Transformer Models with Attention",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "This post is divided into three parts; they are: • Understanding Word Embeddings • Using Pretrained Word Embeddings • Training Word2Vec with Gensim • Training Word2Vec with PyTorch • Embeddings in Transformer Models Word embeddings represent words as dense vectors in a continuous space, where semantically similar words are positioned close to each other."
    }
  ],
  "lastUpdated": "2025-06-17T09:29:45.047Z"
}