{
  "sourceUrl": "https://machinelearningmastery.com/blog/feed/",
  "title": "MachineLearningMastery.com",
  "description": "Making developers awesome at machine learning",
  "link": "https://machinelearningmastery.com/blog/",
  "items": [
    {
      "title": "用于生成时间序列特征的10个Python单行代码 (原标题: 10 Python One-Liners for Generating Time Series Features)",
      "link": "https://machinelearningmastery.com/10-python-one-liners-for-generating-time-series-features/",
      "pubDate": "Mon, 27 Oct 2025 14:25:18 +0000",
      "isoDate": "2025-10-27T14:25:18.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "# 用于生成时间序列特征的10个Python单行代码\n\n## 引言\n时间序列数据需要深入理解才能构建有效且富有洞察力的预测模型。其中，**表示（Representation）**和**粒度（Granularity）**是时间序列预测的两个关键属性：\n*   **表示**：指采用有意义的方法将原始时间数据（如每日或每小时测量值）转换为信息丰富的模式。\n*   **粒度**：指分析这些模式捕捉时间变化信息的精确程度。\n这两者都通过特征工程实现。本文介绍了10个基于原始时间序列数据不同特性和属性的简单Python单行代码，用于生成时间序列特征。这些代码可以单独使用或组合使用，以创建更具信息量的数据集，揭示数据的时间行为（如何演变、波动以及随时间展现的趋势）。示例中使用了Pandas和NumPy库。\n\n![10 Python One-Liners for Generating Time Series Features](https://machinelearningmastery.com/wp-content/uploads/2025/10/mlm-10-python-one-liners-time-series-2.png)\n\n## 1. 滞后特征（自回归表示）\n*   **概念**：将前一个观测值作为当前观测值的新预测特征。\n*   **作用**：表示时间依赖性，例如当前时间点与之前时间点之间的关系。\n*   **示例代码**：`df['lag_1'] = df['value'].shift(1)`\n*   **说明**：`shift()`函数中的参数可以调整，以获取当前观测值之前n个时间点或观测值的值。例如，对于每日时间序列数据，`shift(7)`可捕获一周前的数值。\n\n## 2. 滚动均值（短期平滑）\n*   **概念**：计算当前观测值之前n个观测值的滚动平均值。\n*   **作用**：捕捉数据中的局部趋势或平滑短期波动，对原始时间序列值进行平滑处理。\n*   **示例代码**：`df['rolling_mean_3'] = df['value'].rolling(3).mean()`\n*   **说明**：此示例为每个观测值创建一个新特征，其中包含该特征最近3个值的滚动均值。\n\n![Smoothed time series feature with rolling mean](https://machinelearningmastery.com/wp-content/uploads/2025/10/smoothened.png)\n\n## 3. 滚动标准差（局部波动性）\n*   **概念**：基于滚动窗口计算标准差。\n*   **作用**：有效建模连续观测值的波动性。\n*   **示例代码**：`df['rolling_std_7'] = df['value'].rolling(7).std()`\n*   **说明**：此示例引入了一个特征，用于建模假设为每日观测数据中一周内最新值的变异性。\n\n## 4. 扩展均值（累积记忆）\n*   **概念**：计算时间序列中截至（并包括）当前观测值的所有数据点的平均值。\n*   **作用**：类似于窗口大小不断增加的滚动均值，用于分析时间序列属性值的平均值如何随时间演变，从而更可靠地捕捉长期上升或下降趋势。\n*   **示例代码**：`df['expanding_mean'] = df['value'].expanding().mean()`\n\n## 5. 差分（趋势移除）\n*   **概念**：计算目标属性的连续观测值（当前值与前一个值）之间的差值。\n*   **作用**：用于移除长期趋势，突出变化率，对于稳定非平稳时间序列非常重要。\n*   **示例代码**：`df['diff_1'] = df['value'].diff()`\n\n## 6. 基于时间的特征（时间分量提取）\n*   **概念**：从完整日期时间特征或时间序列的索引中分解并提取相关信息。\n*   **作用**：简单但非常实用，可提取月份、星期几、小时等信息。\n*   **示例代码**：\n    *   如果日期时间信息在常规属性中：`df['month'], df['dayofweek'] = df['Date'].dt.month, df['Date'].dt.dayofweek`\n    *   如果日期时间信息是数据结构的索引：`df['hour'], df['dayofweek'] = df.index.hour, df.index.dayofweek`\n\n## 7. 滚动相关性（时间关系）\n*   **概念**：在时间窗口内测量近期值与其滞后对应值之间的相关性。\n*   **作用**：帮助发现不断演变的自相关性，例如检测当滚动相关性开始减弱或反转时发生的“制度转变”（数据随时间发生的突然且持续的行为变化）。\n*   **示例代码**：`df['rolling_corr'] = df['value'].rolling(30).corr(df['value'].shift(1))`\n\n## 8. 傅里叶特征（季节性）\n*   **概念**：在原始时间序列属性中使用正弦（或余弦）傅里叶变换来捕捉周期性或季节性模式。\n*   **作用**：将日期时间特征中潜在的周期性信息（如一年中的第几天）转换为连续特征，有助于学习和建模年度模式。\n*   **示例代码**（使用两行代码以更好地捕捉周期性季节性模式）：\n    `df['fourier_sin'] = np.sin(2 * np.pi * df['Date'].dt.dayofyear / 365)`\n    `df['fourier_cos'] = np.cos(2 * np.pi * df['Date'].dt.dayofyear / 365)`\n\n## 9. 指数加权均值（自适应平滑）\n*   **概念**：应用指数衰减权重，赋予近期数据观测值更高的重要性，同时保留长期记忆。\n*   **作用**：一种更具适应性且“更智能”的方法，优先考虑近期观测值而非遥远的过去。\n*   **示例代码**：`df['ewm_mean'] = df['value'].ewm(span=5).mean()`\n\n## 10. 滚动熵（信息复杂度）\n*   **概念**：计算给定特征在时间窗口内值的随机性或分散程度。\n*   **作用**：揭示信息量和复杂性。较低的滚动熵值表示有序性和可预测性，而较高的值则表示“混乱和不确定性”。\n*   **示例代码**：`df['rolling_entropy'] = df['value'].rolling(10).apply(lambda x: -np.sum((p:=np.histogram(x, bins=5)[0]/len(x))*np.log(p+1e-9)))`\n\n## 总结\n本文探讨并演示了10种策略（每种策略仅用一行代码），用于从原始时间序列数据中提取各种模式和信息，从简单的趋势到更复杂的季节性和信息复杂度。",
      "shortSummary": "本文介绍了10个Python单行代码，用于时间序列特征工程。这些方法利用Pandas和NumPy，旨在从原始时间序列数据中提取有用的模式和信息，以构建更具洞察力的预测模型。涵盖的特征包括：滞后特征、滚动均值、滚动标准差、扩展均值、差分、基于时间的特征、滚动相关性、傅里叶特征、指数加权均值和滚动熵。这些简洁的代码片段能有效揭示数据的时间行为，显著提升时间序列分析和预测模型的性能。",
      "translated_title": "用于生成时间序列特征的10个Python单行代码",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/10/mlm-10-python-one-liners-time-series-2.png",
          "alt": "10 Python One-Liners for Generating Time Series Features",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/10/smoothened.png",
          "alt": "Smoothed time series feature with rolling mean",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "Time series data normally requires an in-depth understanding in order to build effective and insightful forecasting models."
    },
    {
      "title": "Python 开发者 Pydantic 完整指南 (原标题: The Complete Guide to Pydantic for Python Developers)",
      "link": "https://machinelearningmastery.com/the-complete-guide-to-pydantic-for-python-developers/",
      "pubDate": "Fri, 24 Oct 2025 13:59:01 +0000",
      "isoDate": "2025-10-24T13:59:01.000Z",
      "creator": "Bala Priya C",
      "summary": "# Pydantic Python 开发者完整指南\n\n本文旨在为 Python 开发者提供 Pydantic 的全面指南，教授如何使用类型提示来验证、解析和序列化结构化数据。Pydantic 解决了 Python 数据类型灵活性可能导致的运行时错误，尤其在处理 API、配置文件或用户输入时，通过自动数据验证和序列化确保应用程序的可靠性。\n\n![Pydantic Python 开发者完整指南](https://machinelearningmastery.com/wp-content/uploads/2025/10/mlm-bpc-pydantic-for-python-devs.jpg)\n\n## 核心概念与安装\n\n*   **Pydantic 的作用**：利用 Python 的类型提示系统，自动定义和强制执行数据结构规则，从而实现数据验证和序列化。\n*   **本文涵盖内容**：\n    *   使用类型强制转换和清晰验证错误定义核心模型。\n    *   有效使用可选字段、默认值和 `Field` 约束。\n    *   编写自定义验证器，处理嵌套结构，并导出 JSON。\n*   **安装**：在开始之前，需通过 `pip install pydantic` 进行安装。\n\n## Pydantic 基础模型\n\n*   **集成与优势**：Pydantic 与现有 Python 代码无缝集成，将类型提示转化为强大的验证逻辑，提供清晰、可操作的错误信息，而非隐晦的运行时异常，从而减少调试时间并提高代码可维护性。\n*   **模型定义**：Pydantic 模型继承自 `BaseModel`，并使用 Python 类型提示来定义预期的数据结构。\n    *   **示例**：定义一个 `User` 模型，包含 `name` (str)、`age` (int) 和 `email` (str) 字段。\n    *   **自动类型强制转换**：Pydantic 会自动将字符串（如“25”）转换为整数（25）。如果转换不可能，则会引发带有明确消息的验证错误。\n\n## 可选字段与默认值\n\n*   **处理缺失数据**：Pydantic 使用 `Optional` 类型和默认值来处理实际数据中可能缺失或可选的字段。\n    *   **示例**：`Product` 模型中的 `description: Optional[str] = None` 表示 `description` 可以是字符串或 `None`。\n    *   **默认值**：带有默认值的字段在创建实例时无需提供（如 `in_stock: bool = True`）。\n    *   **`Field()` 函数**：用于添加额外的验证约束，例如 `category: str = Field(default=\"general\", min_length=1)` 确保 `category` 至少有一个字符。\n\n## 自定义验证器\n\n*   **超越基本类型检查**：当需要更复杂的验证逻辑时，可以使用自定义验证器。\n*   **实现方式**：通过 `@field_validator` 装饰器定义验证函数。\n    *   **示例**：`Account` 模型为 `username`（长度、字母数字、转换为小写）、`email`（正则表达式格式）和 `password`（最小长度）定义了自定义验证器。\n*   **功能**：验证器可以在模型创建期间自动运行，用于转换数据或拒绝无效值，并提供描述性错误消息。\n\n## 嵌套模型与复杂结构\n\n*   **处理层次数据**：Pydantic 使嵌套验证变得简单，能够递归验证整个结构。\n    *   **示例**：`Company` 模型包含 `Address` 模型和 `Contact` 模型列表。`Address` 模型可以有自己的 `zip_code` 验证器。\n*   **自动解析**：`datetime` 字符串会自动解析为 `datetime` 对象。\n*   **详细错误**：如果嵌套结构中的任何部分无效，Pydantic 会提供详细的错误信息，精确指出问题所在。\n\n## 与 API 和 JSON 交互\n\n*   **理想场景**：Pydantic 非常适合处理 API 响应和 JSON 数据，这些数据通常格式不可预测。\n*   **高级验证**：\n    *   **`mode='before'`**：在验证器中使用此参数，使其在类型转换之前运行，从而能够处理原始字符串输入。\n    *   **`Union` 类型**：处理多种可能的输入类型（如 `created_at: Union[datetime, str]`）。\n    *   **`Field` 约束**：添加范围约束（如 `age: Optional[int] = Field(None, ge=0, le=150)`）。\n\n## 错误处理与验证\n\n*   **结构化错误信息**：当验证失败时，Pydantic 会提供 `ValidationError`，其中包含详细的结构化错误信息。\n*   **错误详情**：每个错误都包含字段位置 (`loc`)、错误类型 (`type`) 和人类可读的消息 (`msg`)。\n*   **获取错误**：可以通过 `e.errors()` 获取错误列表，或通过 `e.json()` 获取 JSON 格式的错误表示，便于调试或向用户提供反馈。\n\n## 序列化与导出\n\n*   **模型转换**：将 Pydantic 模型转换回字典或 JSON 字符串非常直接。\n*   **方法**：\n    *   `model_dump()`：将模型导出为字典。\n    *   `model_dump_json()`：将模型导出为 JSON 字符串。\n*   **灵活选项**：支持排除敏感字段 (`exclude`)、仅包含特定字段或自定义值的序列化方式，这在创建不同上下文的 API 响应时特别有用。\n\n## 总结\n\nPydantic 将数据验证从繁琐、易错的任务转变为自动化、声明式的过程。它利用 Python 的类型系统，为数据结构提供运行时保证，同时保持代码的整洁和可读性。Pydantic 帮助开发者及早发现错误，用更少的样板代码构建更可靠的应用程序。本文提供了 Pydantic 的良好基础，从基本模型到自定义验证器和嵌套结构，鼓励开发者在项目中继续探索其高级功能，使其成为 Python 开发工作流程中不可或缺的工具。",
      "shortSummary": "Pydantic 是一款 Python 库，通过类型提示实现自动数据验证、解析和序列化。它解决了 Python 数据类型灵活性导致的运行时错误，特别适用于处理 API、配置和用户输入。Pydantic 支持定义核心模型、处理可选字段、实现自定义验证器、管理嵌套结构以及方便地进行 JSON 导出。当数据不符合预期时，Pydantic 会提供清晰的错误信息，从而简化调试并提高代码可靠性与可维护性。",
      "translated_title": "Python 开发者 Pydantic 完整指南",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/10/mlm-bpc-pydantic-for-python-devs.jpg",
          "alt": "The Complete Guide to Pydantic for Python Developers",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "Python's flexibility with data types is convenient when coding, but it can lead to runtime errors when your code receives unexpected data formats."
    },
    {
      "title": "机器学习实践者的大语言模型微调指南 (原标题: The Machine Learning Practitioner’s Guide to Fine-Tuning Language Models)",
      "link": "https://machinelearningmastery.com/the-machine-learning-practitioners-guide-to-fine-tuning-language-models/",
      "pubDate": "Thu, 23 Oct 2025 14:17:27 +0000",
      "isoDate": "2025-10-23T14:17:27.000Z",
      "creator": "Vinod Chugani",
      "summary": "# 机器学习实践者的大语言模型微调指南\n\n本文为机器学习实践者提供了关于何时、如何以及使用哪些工具对大型语言模型（LLM）进行微调的实用指南，并强调了如何避免常见错误。\n\n![机器学习实践者的大语言模型微调指南](https://machinelearningmastery.com/wp-content/uploads/2025/10/mlm-chugani-machine-learning-practitioners-guide-fine-tuning-language-models-feature-1024x683.png)\n\n### 引言\n2024-2025年，大语言模型（LLM）的微调变得更加普及，参数高效方法使得70B+参数的模型也能在消费级GPU上运行。本指南旨在为实践者提供实用的指导，涵盖微调的适用场景、核心方法、工具选择以及常见陷阱的规避。微调不同于从头训练，它通过少量数据和计算资源，将预训练模型适应到特定任务，从而降低了先进自然语言处理（NLP）能力的门槛。\n\n### 何时进行微调而非选择替代方案\n微调应作为最后的手段。推荐的流程是：\n1.  **提示工程（Prompt Engineering）**：适用于基本的任务适应。\n2.  **检索增强生成（RAG）**：当需要引用来源、将响应基于特定文档或信息频繁变化时使用。\n3.  **微调**：仅当需要深度专业化时才考虑。\n\nMeta AI提出了微调真正有价值的五种场景：\n*   为特定受众定制语气和风格。\n*   维护敏感信息的数据隐私。\n*   支持低资源语言。\n*   通过蒸馏大型模型来降低推理成本。\n*   添加基础模型中不存在的全新功能。\n\n**数据可用性测试：**\n*   少于100个示例：坚持使用提示工程。\n*   100-1,000个示例且知识静态：考虑参数高效方法。\n*   1,000-100,000个示例且任务定义明确：尝试微调。\n\n最佳解决方案通常是结合微调（用于专业推理模式）和RAG（用于最新信息）。\n\n### 核心参数高效微调（PEFT）方法\n全量微调需要大量的计算和内存。PEFT通过仅更新约0.1%至3%的参数，在显著降低资源需求的同时，实现了与全量微调相当的性能。\n\n*   **LoRA (Low-Rank Adaptation)**：\n    *   主流技术，冻结预训练权重，并行注入可训练的低秩分解矩阵。\n    *   更新表示为低秩分解，通常秩为8就足够。\n    *   内存减少2-3倍，检查点大小减少1,000-10,000倍。\n    *   部署时，学习到的矩阵与冻结权重合并，不引入推理延迟。\n    *   训练速度可提升约25%。\n*   **QLoRA**：\n    *   LoRA的扩展，通过激进的量化（4位存储基础权重，16位bfloat16进行计算）保持准确性。\n    *   显著降低内存：65B模型可在48GB GPU上运行，13B模型可在16GB消费级硬件上运行，且性能与16位全量微调相当。\n*   **Spectrum (2024年创新)**：\n    *   通过信噪比分析识别信息最丰富的层，仅选择性微调约30%的顶层。\n    *   在数学推理任务上，以相当的资源实现了比QLoRA更高的准确性。\n\n**PEFT方法选择框架：**\n*   **LoRA**：需要零推理延迟和适度GPU资源（16-24 GB）。\n*   **QLoRA**：内存极端受限（消费级GPU，Google Colab）或超大型模型（30B+）。\n*   **Spectrum**：在分布式环境中处理大型模型。\n\n### 现代对齐与指令微调\n*   **指令微调**：将以补全为中心的基础模型转化为遵循指令的助手，建立基本能力。通过多样化的指令-响应对进行训练，质量远比数量重要（约1,000个高质量示例通常足够）。\n*   **DPO (Direct Preference Optimization)**：\n    *   通过简化强化学习（RLHF）迅速成为首选对齐方法。\n    *   核心思想：将奖励隐式地重新参数化到策略本身中，通过监督学习而非复杂的强化学习解决RLHF目标。\n    *   研究表明，DPO在单阶段训练、约50%更少的计算量和更高稳定性下，可实现与基于PPO的RLHF相当或更优的性能。\n    *   仅需要偏好数据（提示、选择的响应、拒绝的响应）、参考策略和标准监督学习基础设施。\n    *   已成为2024-2025年训练开源LLM的常用方法（如Zephyr-7B和Mistral系列模型）。\n*   **RLHF (Reinforcement Learning from Human Feedback)**：\n    *   基础对齐技术，但复杂性高（训练期间管理四个模型副本、实现困难、训练不稳定）。\n    *   大多数实践者应使用DPO，除非特定场景需要RLHF的灵活性。\n    *   建议路径：先进行指令微调（使用Alpaca或Dolly-15k等数据集），然后实施DPO进行对齐。\n\n### 数据准备最佳实践\n数据质量是微调成功的决定性因素。训练数据中的错误率线性增加，下游模型错误可能超线性增长，因此数据整理是最高杠杆的活动。\n\n*   **数据集大小要求**：\n    *   简单分类：约200-1,000个示例。\n    *   中等复杂任务（如问答）：约1,000-5,000个。\n    *   复杂生成或推理：可能需要5,000-10,000+个。\n*   **质量胜于数量**：1,000个高质量示例可能胜过100,000个平庸示例。\n*   **高质量数据特征**：领域相关性、场景多样性、代表性、标注准确性、时效性。\n*   **格式化**：使用结构化的问答对，保持数据集间格式一致。\n*   **数据划分**：约80%训练集，20%验证集（适用时采用分层抽样）。\n*   **预处理**：清理噪声、处理缺失值、使用模型特定分词器、删除重复项、标准化文本。\n*   **优先使用专有定制数据**，而非模型可能已在预训练中遇到的公共数据集。\n\n### 避免关键陷阱\n*   **过拟合（Overfitting）**：\n    *   模型记忆训练数据而非学习泛化模式。\n    *   迹象：训练损失下降而验证损失上升，训练准确率高但验证性能差。\n    *   预防：\n        *   **提前停止**：当验证性能趋于平稳时停止训练。\n        *   **正则化**：L2权重衰减、10%-30%的Dropout、权重惩罚。\n        *   **数据增强**：反向翻译、合成数据生成。\n        *   **K折交叉验证**：确保泛化。\n        *   **PEFT方法**：降低LoRA秩（r参数）和alpha值以减少可训练参数。\n        *   **学习率**：1e-4到2e-4。\n        *   持续监控训练和验证损失。\n        *   PEFT方法通过限制可训练参数（约0.1%-1%）自然减少过拟合。\n*   **灾难性遗忘（Catastrophic Forgetting）**：\n    *   在训练新任务时丢失先前学习的信息。\n    *   模型可能失去通用推理能力，对以前能回答的问题表现下降。\n    *   预防：\n        *   **弹性权重整合（EWC）**：识别并保护重要权重。\n        *   **“半微调”**：每轮冻结约一半参数。\n        *   **锐度感知最小化（SAM）**：平坦化损失景观。\n        *   最易实现的方法：将多样化的指令数据集与领域特定数据混合。\n\n### 实用工具与入门\n*   **Hugging Face生态系统**：Transformers（模型访问）、PEFT（参数高效方法）、TRL（强化学习和监督微调）、bitsandbytes（量化）。\n*   **Unsloth**：通过自定义Triton内核，训练速度提升约2倍，内存减少约80%，可在单个T4或消费级GPU上运行，免费在Colab和Kaggle上使用。\n*   **LlamaFactory**：统一解决方案，支持100+模型，基于配置的训练。\n*   **FSDP + QLoRA**：在双消费级GPU上训练70B模型。\n*   **推荐的2025年堆栈（约8B模型）**：QLoRA或Spectrum + FlashAttention-2 + Liger Kernels + 梯度检查点。\n    *   Llama-3.1-8B模型训练：单个强大GPU约2小时，8个GPU分布式训练不到半小时。\n*   **推荐初始配置**：\n    *   基础模型：Llama-3.1-8B或Phi-3-mini。\n    *   量化：QLoRA（4位）用于消费级GPU训练。\n    *   平台：Unsloth（免费）。\n    *   序列长度：512-1,024 token。\n    *   学习率：2e-4。\n    *   批处理大小：4-8，梯度累积2-4步。\n    *   效率：启用梯度检查点和序列打包。\n*   **实践数据集**：Alpaca（指令微调）、Dolly-15k（高质量人工示例）、OpenAssistant（对话数据）、Anthropic HH-RLHF（偏好学习）。\n\n### 你的学习路径\n对于微调新手，建议采用渐进式学习方法：\n1.  **从指令微调开始**：\n    *   在Alpaca数据集上微调基础T5或Llama-2。\n    *   理解指令-响应数据格式，使用Hugging Face TRL SFTTrainer结合LoRA进行高效训练。\n    *   建立数据准备、训练和评估的基础。\n2.  **进阶到DPO**：\n    *   在小型偏好数据集（如Anthropic HH-RLHF或UltraFeedback）上训练。\n    *   与监督微调基线进行性能比较。\n    *   理解隐式奖励和偏好学习。DPO的简洁性使其成为学习对齐概念的理想选择。\n3.  **实验生产系统**：\n    *   从小模型（1B到3B参数）开始快速迭代。\n    *   使用现有实现而非从头构建。\n    *   进行仔细的消融实验，隔离不同选择的影响。\n    *   在扩展到更大模型之前，使用多个指标进行严格评估。\n\n**入门清单**：\n*   定义清晰的任务和成功标准，包括目标指标。\n*   选择1-2个自定义评估指标和2-3个系统级指标（总计最多5个）。\n*   准备数据。",
      "shortSummary": "大语言模型微调在2024-2025年变得更易实现，即使是大型模型也能在消费级GPU上运行。微调应是提示工程和RAG之后的最后手段，适用于定制语气、隐私保护、低资源语言、成本优化或新增功能。核心方法包括参数高效微调（PEFT，如LoRA、QLoRA、Spectrum）和对齐技术（如DPO）。数据质量至关重要，需警惕过拟合和灾难性遗忘。Hugging Face生态系统、Unsloth等工具是关键。建议从指令微调开始，逐步学习DPO，并在小模型上进行迭代实践。",
      "translated_title": "机器学习实践者的大语言模型微调指南",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/10/mlm-chugani-machine-learning-practitioners-guide-fine-tuning-language-models-feature-1024x683.png",
          "alt": "Machine Learning Practitioners Guide Fine-Tuning Language Models",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "Fine-tuning has become much more accessible in 2024–2025, with parameter-efficient methods letting even 70B+ parameter models run on consumer GPUs."
    },
    {
      "title": "使用大型语言模型（LLM）进行表格数据高级特征工程的5种技术 (原标题: 5 Advanced Feature Engineering Techniques with LLMs for Tabular Data)",
      "link": "https://machinelearningmastery.com/5-advanced-feature-engineering-techniques-with-llms-for-tabular-data/",
      "pubDate": "Wed, 22 Oct 2025 14:41:10 +0000",
      "isoDate": "2025-10-22T14:41:10.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "# 使用大型语言模型（LLM）进行表格数据高级特征工程的5种技术\n\n![使用大型语言模型（LLM）进行表格数据高级特征工程的5种技术](https://machinelearningmastery.com/wp-content/uploads/2025/10/mlm-5-advanced-feature-engineering-techniques-with-llms-for-tabular-data.png)\n\n## 引言\n\n在大型语言模型（LLM）时代，特征工程这一经典的机器学习概念依然至关重要。它不仅能帮助预处理或结构化非结构化数据（如文本），还能增强LLM在结合表格（结构化）数据时提取、生成和转换信息的能力。将表格数据整合到LLM工作流中具有多重优势，包括丰富主文本输入下的特征空间、推动语义增强以及通过弥合结构化和非结构化数据之间的鸿沟来自动化模型管道。\n\n本文介绍了五种高级特征工程技术，通过这些技术，LLM可以将表格数据中的宝贵信息整合到其工作流中。\n\n## 5种高级特征工程技术\n\n### 1. 通过文本上下文生成语义特征\n\nLLM可以用于描述或总结表格数据集中的行、列或分类属性值，从而生成基于文本的嵌入。例如，LLM可以接收客户数据集中“邮政编码”属性的值，并输出上下文丰富的描述，如“该客户居住在农村邮政区域”。这些上下文感知的文本表示可以显著丰富原始数据集的信息。\n\n*   **实现方式**：结合Sentence Transformers模型（如Hugging Face上的模型），将LLM生成的文本转换为有意义的嵌入，然后与表格数据的其余部分无缝结合，为下游预测模型（如集成分类器和回归器）构建更具信息量的输入。\n*   **示例代码片段**：\n    ```python\n    from sentence_transformers import SentenceTransformer\n    import numpy as np\n\n    llm_description = \"A32 refers to a rural postal region in the northwest.\"\n    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n    embedding = model.encode(llm_description) # shape e.g. (384,)\n    numeric_features = np.array([0.42, 1.07])\n    hybrid_features = np.concatenate([numeric_features, embedding])\n    print(\"Hybrid feature vector shape:",
      "shortSummary": "本文介绍了5种利用大型语言模型（LLM）进行表格数据高级特征工程的技术。这些技术包括：通过文本上下文生成语义特征、智能缺失值填充、通过提示模板构建领域特定特征、融合结构化和非结构化数据的混合嵌入空间，以及通过LLM引导的推理进行特征选择和转换。LLM能显著增强表格数据的特征工程，提高模型性能和可解释性，尽管在集成到自动化管道时仍需优化。",
      "translated_title": "使用大型语言模型（LLM）进行表格数据高级特征工程的5种技术",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/10/mlm-5-advanced-feature-engineering-techniques-with-llms-for-tabular-data.png",
          "alt": "5 Advanced Feature Engineering Techniques with LLMs for Tabular Data",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "In the epoch of LLMs, it may seem like the most classical machine learning concepts, methods, and techniques like feature engineering are no longer in the spotlight."
    },
    {
      "title": "7个必知的智能体AI设计模式 (原标题: 7 Must-Know Agentic AI Design Patterns)",
      "link": "https://machinelearningmastery.com/7-must-know-agentic-ai-design-patterns/",
      "pubDate": "Tue, 21 Oct 2025 13:20:55 +0000",
      "isoDate": "2025-10-21T13:20:55.000Z",
      "creator": "Bala Priya C",
      "summary": "# 7个必知的智能体AI设计模式\n\n![7 Must-Know Agentic AI Design Patterns](https://machinelearningmastery.com/wp-content/uploads/2025/10/mlm-bala-7-must-know-agentic-ai-design-patterns.png)\n\n本文深入探讨了七种经过验证的智能体AI设计模式，旨在指导读者如何为生产工作负载选择和应用这些模式。构建在生产环境中运行的AI智能体，不仅需要强大的模型，更需要清晰的结构来指导它们如何推理、协调、自我纠正和使用工具来完成目标。设计模式提供了这种结构，它们是定义智能体行为的蓝图，有助于将强大的模型转化为可靠的系统。本文介绍的模式借鉴了Google、AWS等团队的最新研究和实践经验。\n\n## 核心设计模式\n\n### 1. ReAct模式：推理与行动\n\n![ReAct Pattern](https://www.kdnuggets.com/wp-content/uploads/react-pattern.png)\n\n*   **描述**：ReAct（Reason and Act）模式将智能体行为组织成明确的推理循环。智能体在推理（分析当前信息、识别差距）、行动（执行工具或查询）和观察（评估结果以确定下一步）之间交替进行，直到任务完成。这种模式通过外部化推理，使每个决策都可见，创建清晰的审计跟踪，防止过早下结论，并通过强制智能体将每一步都基于可观察的结果来减少幻觉。\n*   **适用场景**：需要自适应问题解决的复杂、不可预测任务，例如跨多个来源追踪证据的研究智能体、通过迭代假设测试诊断问题的调试助手、处理非标准请求的客户支持智能体。\n*   **局限性**：以速度换取思考，每个推理循环都需要额外的模型调用，增加延迟和成本。一个工具返回的错误数据可能传播。有效性取决于底层模型的推理能力。\n*   **建议**：可作为复杂、不可预测任务的默认起点，其透明性有助于快速调试和建立信任。\n\n### 2. 反思模式：自我批判的智能体\n\n![Reflection Pattern](https://www.kdnuggets.com/wp-content/uploads/bala-reflection-pattern.png)\n\n*   **描述**：反思模式在智能体输出中添加了一个自我评估层。智能体生成初始响应后，明确切换到批评模式，评估其自身工作，检查准确性、遵守约束，并识别逻辑漏洞或不一致。如果发现问题，智能体将修改输出并重复此过程，直到达到质量阈值。通过强制智能体退后一步进行评估，而不是辩护其第一个答案，减少了确认偏差。\n*   **适用场景**：输出质量远超速度考虑，且错误会带来严重后果的任务，例如需要安全审计或合规性检查的代码生成、发布前需要事实核查的内容创作、错误结论可能导致资本风险的金融分析。\n*   **局限性**：每个反思周期都会增加令牌消耗和延迟。如果没有明确的退出条件，智能体可能会不必要地循环。批评标准必须具体且可衡量，否则会产生不一致的结果。\n*   **建议**：当错误成本超过额外处理时间成本时适用，尤其适用于有明确质量标准的领域，但需要预先投入定义“足够好”的标准。\n\n### 3. 规划模式：先分解再构建\n\n![Planning Pattern](https://www.kdnuggets.com/wp-content/uploads/bala-planning-pattern.png)\n\n*   **描述**：规划智能体在执行前将复杂任务分解为结构化的路线图。它们首先分析需求、识别子任务之间的依赖关系，并按逻辑顺序安排操作。只有在创建详细计划后，智能体才开始实际工作，遵循其构建的路线图。这有助于处理隐藏复杂性的任务，防止在执行过程中发现错误方法。\n*   **适用场景**：涉及显著复杂性或协调性，并受益于明确结构的任务，例如需要特定序列以避免冲突的多系统集成、综合来自不同来源信息的研究项目、具有转换步骤之间依赖关系的数据迁移项目、协调设计、实施和测试的产品开发工作流。\n*   **局限性**：规划开销只适用于真正复杂的工作，简单任务不需要复杂的分解。挑战在于准确评估任务的初始复杂性。\n*   **建议**：用于防止复杂任务中昂贵的错误启动和返工，但对于简单任务而言是纯粹的开销。\n\n### 4. 工具使用模式：超越训练数据\n\n![Tool Use Pattern](https://www.kdnuggets.com/wp-content/uploads/bala-tool-use.png)\n\n*   **描述**：工具使用使智能体能够通过集成外部功能（如调用API、查询数据库、执行代码、抓取网站、与软件系统交互）来执行超出其训练数据的操作。模型根据任务需求协调这些功能，决定调用哪些工具，解释其输出，并链接工具调用以实现仅凭静态知识无法达成的目标。这使得智能体从知识库转变为能够与世界实时交互的活跃系统。\n*   **适用场景**：需要当前信息、外部计算或与系统交互的任务，例如查询订单数据库和库存系统的客户服务智能体、对实时数据集运行统计计算的数据分析智能体、访问超出训练截止日期当前信息的研究助手、在真实环境中执行和测试代码的开发助手。\n*   **局限性**：工具的可靠性成为智能体系统的可靠性。当API返回错误、达到速率限制或超时时，智能体也会继承这些故障。还需要承担每个集成工具的维护负担。\n*   **建议**：对于处理实际任务的生产智能体几乎是不可或缺的。挑战在于如何管理工具的可靠性、选择准确性以及随着工具库增大而增加的复杂性。\n\n### 5. 多智能体协作模式：专家协同工作\n\n![ Multi-Agent Collaboration Pattern](https://www.kdnuggets.com/wp-content/uploads/bala-multiagent-pattern.png)\n\n*   **描述**：多智能体系统将工作分配给专业智能体，而不是构建一个通才。每个智能体都具有专注的专业知识、特定工具和明确定义的系统角色。协调器智能体管理工作分配，将任务路由到适当的专家，并将其输出合成为统一的响应。每个智能体都可以针对其领域进行优化。\n*   **适用场景**：任务真正跨越多个领域，需要不同的专业知识和方法时，例如需要不同技能集（研究→分析→演示）的复杂工作流、受益于专业处理的任务路由应用、由专注智能体更好地解决的多元用例应用。\n*   **局限性**：比单智能体系统更难构建、调试和维护。协调增加了延迟和复杂性。智能体间通信引入新的故障模式。成本随智能体数量增加而倍增。\n*   **建议**：仅当单智能体方法确实无法有效处理多样化、复杂的需求，并且专业化能够带来可衡量的改进时才考虑，否则不建议增加系统复杂性。\n\n### 6. 顺序工作流：可预测的管道\n\n![Sequential Workflow](https://www.kdnuggets.com/wp-content/uploads/bala-seq-pattern.png)\n\n*   **描述**：顺序模式将智能体系统组织成固定顺序的管道。智能体A完成任务并将输出传递给智能体B，智能体B处理后传递给智能体C。每个专家处理预定序列中的一个步骤。编排不需要AI，只需预定义的逻辑来确定流程。这是一种流水线式的智能体系统方法。\n*   **优点**：可预测性、比动态路由系统更低的延迟、比基于协调器的方法更低的成本、可预测的行为简化了调试。\n*   **适用场景**：工作流遵循结构化、可重复模式且处理序列很少改变时，例如具有提取、转换和加载阶段的数据管道、文档处理流程（解析→分析→总结→存储）、内容审核（检测→分类→路由→行动）、订单处理、报告生成。\n*   **局限性**：无法适应工作流中途的变化条件。如果某些输入不需要第三步，它仍然会执行。当工作流需要条件逻辑或动态路由时，顺序模式会导致效率低下或完全失败。\n*   **建议**：适用于一致性和效率比灵活性更重要的生产管道，但对于需要根据中间结果进行调整的任务而言是错误的选择。\n\n### 7. 人在回路模式：高风险任务的安全保障\n\n![Human-in-the-Loop Pattern](https://www.kdnuggets.com/wp-content/uploads/bala-hil-pattern.png)\n\n*   **描述**：人在回路（Human-in-the-Loop）模式认识到某些决策不应完全自动化。在关键检查点，智能体暂停执行，并将信息呈现给人类审查员。人类专家评估工作、提供指导或批准，然后智能体继续。这不是缺乏自动化，而是智能系统设计，承认某些决策需要人类判断、问责或监督。\n*   **适用场景**：决策涉及重大后果、安全问题或需要人类问责的主观判断时，例如超出授权阈值的金融交易、需要细致判断的边缘内容审核、归档或签署前的法律文件批准、AI筛选但人类决定的招聘决策。\n*   **局限性**：增加了架构复杂性，需要暂停工作流、通知人类、管理交接和恢复执行的基础设施。需要明确的升级标准，否则会过度负担人类或自动化需要监督的决策。\n*   **建议**：对于错误可能造成重大损害或问责制需要人类决策的高风险应用是强制性的。它承认完全自动化并非总是目标，机器效率和人类判断的正确平衡往往能带来更好的结果。\n\n## 总结\n\n大多数模式决策归结为三个关键问题：\n\n1.  **工作流是否可预测？** 如果是，顺序模式在成本和速度上占优。如果否，则需要动态编排。\n2.  **质量是否比速度更重要？** 如果是，添加反思或人在回路模式。如果否，优化直接执行。\n3.  **任务是否真正复杂？** 如果是，考虑多智能体或规划模式。如果否，从单个智能体和工具使用开始。\n\n过早地采用复杂模式是代价高昂的错误，应从最简单的有效方案开始。",
      "shortSummary": "本文介绍了7种智能体AI设计模式，旨在构建可靠的生产级AI智能体。这些模式包括ReAct（推理与行动）、反思（自我批判）、规划（任务分解）、工具使用（扩展能力）、多智能体协作（专家协同）、顺序工作流（可预测管道）和人在回路（高风险安全保障）。每种模式都有其适用场景、优缺点和权衡（成本、延迟、可靠性）。选择合适的模式需考虑工作流的可预测性、质量与速度的优先级以及任务的实际复杂性，避免不必要的复杂化，建议从最简单的有效方案开始。",
      "translated_title": "7个必知的智能体AI设计模式",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/10/mlm-bala-7-must-know-agentic-ai-design-patterns.png",
          "alt": "7 Must-Know Agentic AI Design Patterns",
          "title": "",
          "position": 1
        },
        {
          "url": "https://www.kdnuggets.com/wp-content/uploads/react-pattern.png",
          "alt": "ReAct Pattern",
          "title": "",
          "position": 2
        },
        {
          "url": "https://www.kdnuggets.com/wp-content/uploads/bala-reflection-pattern.png",
          "alt": "Reflection Pattern",
          "title": "",
          "position": 3
        },
        {
          "url": "https://www.kdnuggets.com/wp-content/uploads/bala-planning-pattern.png",
          "alt": "Planning Pattern",
          "title": "",
          "position": 4
        },
        {
          "url": "https://www.kdnuggets.com/wp-content/uploads/bala-tool-use.png",
          "alt": "Tool Use Pattern",
          "title": "",
          "position": 5
        },
        {
          "url": "https://www.kdnuggets.com/wp-content/uploads/bala-multiagent-pattern.png",
          "alt": " Multi-Agent Collaboration Pattern",
          "title": "",
          "position": 6
        },
        {
          "url": "https://www.kdnuggets.com/wp-content/uploads/bala-seq-pattern.png",
          "alt": "Sequential Workflow",
          "title": "",
          "position": 7
        },
        {
          "url": "https://www.kdnuggets.com/wp-content/uploads/bala-hil-pattern.png",
          "alt": "Human-in-the-Loop Pattern",
          "title": "",
          "position": 8
        }
      ],
      "contentSource": "完整文章",
      "content": "Building AI agents that work in production requires more than powerful models."
    },
    {
      "title": "2026年：如何让你的AI工程职业生涯经久不衰 (原标题: Future-Proofing Your AI Engineering Career in 2026)",
      "link": "https://machinelearningmastery.com/future-proofing-your-ai-engineering-career-in-2026/",
      "pubDate": "Mon, 20 Oct 2025 11:00:38 +0000",
      "isoDate": "2025-10-20T11:00:38.000Z",
      "creator": "Nahla Davies",
      "summary": "# 2026年：如何让你的AI工程职业生涯经久不衰\n\n## 引言\n人工智能工程已从一个未来主义的利基领域转变为全球最受欢迎的技术职业之一。然而，行业创新步伐极快，自动化甚至开始影响其创造者。本文旨在探讨如何通过深化核心基础、拥抱系统级自动化以及与开源和不断演变的政策保持一致，来确保AI工程师的职业生涯面向未来。\n\n![2026年：如何让你的AI工程职业生涯经久不衰](https://machinelearningmastery.com/wp-content/uploads/2025/10/mlm-future-proof-ai-engineering-career-2026.png)\n\n## 核心策略\n\n1.  **掌握他人忽视的基础知识**\n    *   所有新兴AI趋势（如生成式智能体、多模态Transformer、合成数据管道）都建立在相同的基本原理之上。\n    *   优先学习数学基础（线性代数、优化、概率论、信息论），而非急于掌握框架（如PyTorch、TensorFlow）。\n    *   具备推导损失函数、理解收敛行为和推理数据分布的能力，是长期技术韧性的支柱。\n    *   对理论的深入理解能带来更强的适应性和通用性，例如诊断模型训练崩溃或梯度不稳定性。\n\n2.  **驾驭自动化而非对抗**\n    *   自动化工具（AutoML平台、代码生成模型、自动化数据标注）对AI工程师构成直接威胁。\n    *   关键在于管理和扩展自动化，而非与之对抗。\n    *   掌握微调自动化工具或将其集成到更大系统的能力。\n    *   理解人类直觉在哪些方面仍优于机器（例如，提示策略而非简单的提示工程）。\n    *   核心技能是“元工程”：构建基础设施，确保自动化安全、高效、合乎道德地运行。\n\n3.  **培养跨学科的流畅性**\n    *   未来的AI工程更注重系统集成而非孤立的模型性能。\n    *   雇主重视能将技术系统转化为商业、设计和伦理语境的工程师。\n    *   能够与数据隐私律师、用户体验研究员和DevOps工程师有效沟通。\n    *   弥合技术鸿沟，例如优化推理延迟并向非技术团队解释公平性指标。\n    *   从系统层面思考，关注系统如何交互、扩展和演变。\n\n4.  **学会利用开源生态系统**\n    *   开源是AI进步的核心驱动力，战略意义日益增强。\n    *   积极参与、贡献或领导开源项目能迅速建立信誉和知名度。\n    *   通过贡献代码、构建轻量级工具或以新颖方式实验预训练模型来保持创新前沿。\n    *   理解如何评估和组合开源组件（例如，将向量数据库与LLM API结合）。\n\n5.  **理解AI基础设施，而非仅仅模型**\n    *   数据摄取、GPU优化、分布式训练和模型服务等基础设施已成为生产级AI中最具挑战性的部分。\n    *   掌握端到端系统知识，能够管理整个工作流程。\n    *   熟悉云原生MLOps（Python）、容器化（Docker、Kubernetes）和框架（MLflow、Kubeflow）至关重要。\n    *   具备将模型从原型扩展到创收系统的能力。\n    *   未来的AI团队需要融合研究洞察和部署专业知识的混合型人才。\n\n6.  **适应伦理、法律和社会变革**\n    *   AI的未来将受政策影响（如欧盟AI法案、美国数据透明度框架）。\n    *   将合规性知识纳入AI工程师的工具包。\n    *   在模型中嵌入公平性、问责制和可解释性将成为强制要求。\n    *   伦理不仅是避免法律问题，更是改进系统的设计约束，建立信任。\n    *   能够将抽象原则转化为可衡量、可执行的保障措施。\n    *   预测自动化对社会产生的连锁反应，使工作更具防御性和吸引力。\n\n## 结论\n2026年的AI工程师不能仅凭技术技能生存。那些能够将扎实的基础知识与跨学科直觉、系统级理解和伦理远见相结合的人将脱颖而出。工具会变，API会消亡，新架构会主导，但适应能力永不过时。最大的优势不是掌握现有技术，而是为尚未出现的事物做好准备。",
      "shortSummary": "为确保2026年AI工程职业生涯经久不衰，工程师需深化数学与系统基础，通过元工程驾驭自动化，培养跨学科沟通能力，积极利用开源生态系统，并深入理解AI基础设施。同时，适应伦理、法律及社会政策变化至关重要。成功的关键在于持续适应、具备系统级思维，并将技术深度与广阔的背景知识相结合，为未来创新做好准备。",
      "translated_title": "2026年：如何让你的AI工程职业生涯经久不衰",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/10/mlm-future-proof-ai-engineering-career-2026.png",
          "alt": "Future-Proofing Your AI Engineering Career in 2026",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "AI engineering has shifted from a futuristic niche to one of the most in-demand tech careers on the planet."
    },
    {
      "title": "革新MLOps：增强型BigQuery ML UI实现无缝模型创建与管理 (原标题: Revolutionizing MLOps: Enhanced BigQuery ML UI for Seamless Model Creation and Management)",
      "link": "https://machinelearningmastery.com/revolutionizing-mlops-enhanced-bigquery-ml-ui-for-seamless-model-creation-and-management/",
      "pubDate": "Fri, 17 Oct 2025 18:00:44 +0000",
      "isoDate": "2025-10-17T18:00:44.000Z",
      "creator": "Nivedita Kumari",
      "summary": "# 革新MLOps：增强型BigQuery ML UI实现无缝模型创建与管理\n\n本文介绍了增强型BigQuery ML (BQML) 用户界面如何直接在BigQuery控制台中简化端到端的模型创建、管理和预测流程。\n\n## 主要改进与功能\n\n*   **简化模型创建流程**：通过引导式、可保存的SQL流程，极大地简化了模型创建。用户可以直接在模型创建流程中保存SQL查询（需指定区域）。\n*   **可重现的数据准备**：在SQL中准备可重现的训练、评估和预测数据集划分。\n*   **ML.PREDICT预测功能**：利用ML.PREDICT函数运行预测并理解其输出。\n\n## 实际操作示例：使用美国人口普查数据创建逻辑回归模型\n\n本文通过一个具体示例，演示了如何使用增强型BQML UI快速创建一个逻辑回归模型，以预测美国人口普查数据中的收入等级（$<=50K 或 $>50K）。\n\n### 1. 数据探索\n\n*   通过查询`bigquery-public-data.ml_datasets.census_adult_income`数据集，可以查看`age`、`workclass`、`marital_status`、`education_num`、`occupation`、`hours_per_week`、`income_bracket`和`functional_weight`等列。\n*   `income_bracket`列包含“<=50K”或“>50K”两个值。\n*   `functional_weight`列的值与`income_bracket`的值无关。\n\n![Revolutionizing MLOps: Enhanced BigQuery ML UI for Seamless Model Creation and Management](https://machinelearningmastery.com/wp-content/uploads/2025/10/mlm-nivedita-bigquery-ml-ui.png)\n*图：增强型BigQuery ML UI概览*\n\n### 2. 数据准备\n\n*   为了模型训练，需要准备一个样本数据集，并将其划分为训练、评估和预测集。\n*   通过从`functional_weight`列派生一个新列，将80%的数据用于训练，剩余的20%用于评估和预测。\n*   创建了一个名为`census.input_data`的视图，其中包含一个`dataframe`列，用于标识数据属于“training”、“evaluation”还是“prediction”集。\n\n![Streamlined Model Creation Flow](https://machinelearningmastery.com/wp-content/uploads/2025/10/nivedita-bigquery-ml-image-1.png)\n*图：数据探索结果示例*\n\n### 3. 在BigQuery UI中创建ML模型\n\n1.  **启动模型创建**：从BigQuery主屏幕点击“ML Model”开始。\n    ![Creating the ML Model in BigQuery UI](https://machinelearningmastery.com/wp-content/uploads/2025/10/nivedita-bigquery-ml-image-2.png)\n    *图：BigQuery UI中创建ML模型的入口*\n2.  **选择数据集和模型名称**：在“Create new ML model”页面，选择`census`数据集，并将模型命名为`logistic_reg`。\n    ![Creating the ML Model in BigQuery UI](https://machinelearningmastery.com/wp-content/uploads/2025/10/nivedita-bigquery-ml-image-3.png)\n    *图：选择数据集和命名模型*\n3.  **选择创建方法和建模目标**：在“Creation method and Modeling Objective”页面，选择“Train a model in BigQuery”，建模目标为“Classification”，模型类型选择“Logistic Regression”。\n    ![Creating the ML Model in BigQuery UI](https://machinelearningmastery.com/wp-content/uploads/2025/10/nivedita-bigquery-ml-image-4.png)\n    *图：选择建模目标和模型类型*\n4.  **设置训练数据**：选择`census`作为数据集，`input_data`作为表/视图。\n    ![Creating the ML Model in BigQuery UI](https://machinelearningmastery.com/wp-content/uploads/2025/10/nivedita-bigquery-ml-image-5.png)\n    *图：确认训练数据设置*\n5.  **指定输入标签并创建模型**：选择`income_bracket`作为`input_label_cols`，然后点击“Create model”。\n6.  **查看模型创建结果**：模型创建完成后，“Query results”部分将显示“Job information”、“Results”、“Execution details”和“Execution graph”四个标签页。其中，“Execution details”和“Execution graph”提供了关于模型创建作业的详细信息，包括损失和学习率等。\n    ![Creating the ML Model in BigQuery UI](https://machinelearningmastery.com/wp-content/uploads/2025/10/nivedita-bigquery-ml-image-6.png)\n    *图：模型创建后的查询结果界面*\n    ![Creating the ML Model in BigQuery UI](https://machinelearningmastery.com/wp-content/uploads/2025/10/nivedita-bigquery-ml-image-7.png)\n    *图：模型执行详情示例*\n\n### 4. 使用模型进行预测\n\n*   在BigQuery Studio查询编辑器中使用`ML.PREDICT`函数进行预测。\n*   该函数接受训练好的模型（例如`census.logistic_reg`）和`input_data`视图中`dataframe`列值为“prediction”的行作为输入。\n*   查询将生成`income_bracket`的预测结果，包括预测的收入等级、预测概率以及原始输入数据。\n\n![Creating the ML Model in BigQuery UI](https://machinelearningmastery.com/wp-content/uploads/2025/10/nivedita-bigquery-ml-image-8.png)\n*图：使用ML.PREDICT进行预测*\n\n## 总结\n\n这些UI增强功能为BigQuery ML用户提供了更直观、高效的体验，从模型创建到部署和监控，全面简化了MLOps工作流程。鼓励用户探索新的BigQuery ML UI，体验其带来的流畅MLOps工作流，并查阅BQML UI用户指南以了解更多功能。",
      "shortSummary": "增强型BigQuery ML UI革新了MLOps，通过直接在BigQuery控制台中简化端到端的模型创建、管理和预测流程。它提供引导式、可保存的SQL流来创建模型，简化了训练、评估和预测的数据准备，并通过ML.PREDICT实现轻松模型部署。这些改进显著提升了BigQuery ML工作流的效率和用户体验，使MLOps更加无缝和直观。",
      "translated_title": "革新MLOps：增强型BigQuery ML UI实现无缝模型创建与管理",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/10/mlm-nivedita-bigquery-ml-ui.png",
          "alt": "Revolutionizing MLOps: Enhanced BigQuery ML UI for Seamless Model Creation and Management",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/10/nivedita-bigquery-ml-image-1.png",
          "alt": "Streamlined Model Creation Flow",
          "title": "",
          "position": 2
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/10/nivedita-bigquery-ml-image-2.png",
          "alt": "Creating the ML Model in BigQuery UI",
          "title": "",
          "position": 3
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/10/nivedita-bigquery-ml-image-3.png",
          "alt": "Creating the ML Model in BigQuery UI",
          "title": "",
          "position": 4
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/10/nivedita-bigquery-ml-image-4.png",
          "alt": "Creating the ML Model in BigQuery UI",
          "title": "",
          "position": 5
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/10/nivedita-bigquery-ml-image-5.png",
          "alt": "Creating the ML Model in BigQuery UI",
          "title": "",
          "position": 6
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/10/nivedita-bigquery-ml-image-6.png",
          "alt": "Creating the ML Model in BigQuery UI",
          "title": "",
          "position": 7
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/10/nivedita-bigquery-ml-image-7.png",
          "alt": "Creating the ML Model in BigQuery UI",
          "title": "",
          "position": 8
        }
      ],
      "contentSource": "完整文章",
      "content": "Exciting news for BigQuery ML (BQML) users."
    },
    {
      "title": "机器学习向量数据库完整指南 (原标题: The Complete Guide to Vector Databases for Machine Learning)",
      "link": "https://machinelearningmastery.com/the-complete-guide-to-vector-databases-for-machine-learning/",
      "pubDate": "Fri, 17 Oct 2025 16:34:47 +0000",
      "isoDate": "2025-10-17T16:34:47.000Z",
      "creator": "Bala Priya C",
      "summary": "# 机器学习向量数据库完整指南\n\n![机器学习向量数据库完整指南](https://machinelearningmastery.com/wp-content/uploads/2025/10/mlm-bala-vector-db-guide.jpeg)\n\n## 引言\n\n向量数据库已成为现代AI应用的核心，尤其是在处理嵌入（embeddings）时，如语义搜索、推荐系统和RAG系统。传统数据库在处理数百万甚至数亿个高维向量（如1024或1536维）时面临性能瓶颈，因为每次查询都需要进行数十亿次浮点运算。向量数据库通过专门的算法避免了暴力距离计算，利用分层图和空间分区等技术，仅检查一小部分候选向量，从而实现近似最近邻（ANN）搜索，将速度提高千倍。\n\n本文旨在解释向量数据库在机器学习应用中的作用、工作原理以及何时需要使用它们，具体涵盖以下主题：\n\n*   传统数据库索引为何不适用于高维空间中的相似性搜索。\n*   驱动向量数据库的关键算法：HNSW、IVF和乘积量化（Product Quantization）。\n*   距离度量及其重要性。\n*   理解召回率与延迟的权衡，以及生产环境中的调优。\n*   向量数据库如何通过分片、压缩和混合索引处理规模化问题。\n*   何时真正需要向量数据库而非更简单的替代方案。\n*   主要选项概述：Pinecone、Weaviate、Chroma、Qdrant、Milvus等。\n\n## 传统数据库为何不适用于相似性搜索\n\n传统数据库擅长精确匹配（如按ID查找用户），依赖于B树索引。然而，机器学习处理的是高维向量（嵌入），它们代表语义含义。查找相似向量需要计算数百或数千维度的距离，暴力计算效率极低。传统索引无法处理这种高维空间中的“邻居”查找。\n\n## 向量数据库的独特之处\n\n向量数据库专为相似性搜索而设计。它们使用专门的数据结构来组织向量，实现近似最近邻（ANN）搜索，以牺牲完美精度换取显著的速度提升。其核心在于索引结构，它使用针对高维几何设计的算法，而非B树，从而避免了暴力距离计算，使实时语义搜索成为可能。\n\n## 向量数据库的核心概念\n\n向量数据库依赖于不同的算法方法，它们在搜索速度、精度和内存使用之间进行权衡。主要有以下三种关键向量索引方法：\n\n### 1. 分层可导航小世界（HNSW）\n\n*   **原理**：构建多层图结构，每层包含向量子集，通过边连接。顶层稀疏，底层包含所有向量。搜索从顶层开始，贪婪地导航到最近邻，然后逐层向下。\n*   **特点**：搜索复杂度为O(log N)，高效扩展到数百万向量。提供出色的召回率和速度，但需要将整个图保存在内存中，适用于延迟敏感型应用。\n\n![分层可导航小世界（HNSW）](https://www.kdnuggets.com/wp-content/uploads/vector-db-hnsw.png)\n\n### 2. 倒排文件索引（IVF）\n\n*   **原理**：使用K-means等聚类算法将向量空间划分为多个区域。索引时，每个向量分配给最近的聚类中心。搜索时，首先识别最相关的聚类，然后只在这些聚类中进行搜索。\n*   **特点**：通过搜索更多聚类提高精度，搜索更少聚类提高速度。比HNSW占用更少内存，因为搜索时只加载相关聚类，适用于内存无法容纳的超大数据集。缺点是在相同速度下召回率较低，但可与乘积量化结合改进。\n\n![IVF 倒排文件索引](https://www.kdnuggets.com/wp-content/uploads/bala-vector-db-ivf-scaled.png)\n\n### 3. 乘积量化（PQ）\n\n*   **原理**：通过将每个向量分割成子向量，然后独立聚类每个子空间来压缩向量，以减少内存使用并加速距离计算。向量被表示为聚类ID序列而非原始浮点数。\n*   **特点**：可将高维向量的内存占用降低数个数量级（例如，1536维float32向量从~6KB压缩到~8字节）。距离计算使用预计算的查找表，速度显著加快。代价是量化带来的精度损失。PQ通常与其他方法结合使用，如先用IVF进行初步过滤，再用PQ高效扫描候选向量。\n\n![乘积量化](https://www.kdnuggets.com/wp-content/uploads/bala-vector-pq-img.png)\n\n## 向量数据库如何处理规模化问题\n\n现代向量数据库结合多种技术来高效处理数十亿个向量：\n\n*   **分片（Sharding）**：将向量分布到多台机器上，每片独立运行ANN搜索，结果通过堆合并，实现索引和搜索的水平扩展。\n*   **过滤（Filtering）**：将元数据过滤与向量搜索集成。解决方案包括独立的元数据索引与向量结果交叉，或根据过滤值复制数据的分区索引。\n*   **混合搜索（Hybrid Search）**：结合向量相似性与传统全文搜索（如BM25），通过加权组合或倒数排名融合合并分数，处理需要语义理解和关键词精度的查询。\n*   **动态更新**：对于HNSW等图基索引，动态更新是挑战。大多数系统通过队列写入并定期重建索引，或使用支持增量更新的专用数据结构。\n\n## 关键相似性度量\n\n向量相似性依赖于量化向量之间距离的度量：\n\n*   **欧几里得距离（Euclidean distance）**：测量直线距离，对向量大小敏感。\n*   **余弦相似度（Cosine similarity）**：测量向量之间的角度，忽略大小，适用于方向编码含义但尺度不重要的嵌入。大多数语义搜索使用余弦相似度。\n*   **点积（Dot product）**：未归一化的余弦相似度。当所有向量都是单位长度时，等同于余弦相似度但计算更快。\n\n选择正确的度量很重要，应与嵌入模型训练时使用的度量一致。\n\n## 理解召回率与延迟的权衡\n\n向量数据库通过近似搜索牺牲完美精度以换取速度。理解这种权衡对生产系统至关重要：\n\n*   **召回率（Recall）**：衡量搜索返回的真实最近邻居的百分比。高召回率需要检查更多候选向量。\n*   **延迟（Latency）**：查询所需的时间。高召回率通常意味着高延迟。\n\n最佳点通常在90-95%的召回率。从95%提高到99%可能会使查询时间增加三倍，而语义搜索质量几乎没有改善。建议对特定用例进行基准测试，通常85%的召回率就能产生与99%无差别的结果，且成本更低。\n\n## 何时需要向量数据库\n\n并非所有使用嵌入的应用都需要专门的向量数据库。以下情况你**不需要**向量数据库：\n\n*   向量数量少于10万个：NumPy的暴力搜索可能足够快。\n*   向量频繁变化：索引开销可能超过搜索节省。\n*   需要完美精度：使用FAISS等优化库进行精确搜索。\n\n以下情况你**需要**向量数据库：\n\n*   拥有数百万向量并需要低延迟搜索。\n*   构建大规模语义搜索、RAG或推荐系统。\n*   需要通过元数据过滤向量同时保持搜索速度。\n*   需要处理分片、复制和更新的基础设施。\n\n许多团队从简单解决方案开始，随着规模扩大再迁移到向量数据库。\n\n## 生产级向量数据库选项\n\n向量数据库市场发展迅速，主要参与者包括：\n\n*   **Pinecone**：全托管云服务，专有算法，适合希望避免运维开销的团队。\n*   **Weaviate**：开源，可部署在任何地方，结合向量搜索和GraphQL模式，模块系统集成嵌入提供商，提供灵活性和控制。\n*   **Chroma**：专注于开发者体验，为AI应用设计，强调简洁性，可嵌入或作为服务器运行，适用于原型开发和中小型部署，底层使用HNSW。\n*   **Qdrant**：用Rust构建，性能高，通过负载索引高效支持过滤搜索，存储与搜索分离，支持磁盘操作，适合高性能需求。\n*   **Milvus**：处理大规模部署，计算与存储分离的解耦架构，支持多种索引类型（IVF、HNSW、DiskANN），配置丰富，操作复杂但扩展性强。\n*   **Postgres with pgvector**：为PostgreSQL添加向量搜索，适用于已使用Postgres的应用，性能适中，提供事务、连接和熟悉工具，支持精确搜索和IVF。\n*   **Elasticsearch and OpenSearch**：通过HNSW索引添加向量搜索，适用于已运行这些服务的用户，混合搜索能力强。\n\n## 超越简单的相似性搜索\n\n向量数据库正在超越简单的相似性搜索，发展出更先进的方法：\n\n*   **混合向量索引**：结合多个嵌入模型，同时搜索句子嵌入和关键词嵌入，捕捉不同方面的相似性。\n*   **多模态搜索**：在同一空间中索引不同模态（文本、图像、音频）的向量，实现跨模态搜索（如用文本查询图像）。\n*   **学习型索引**：使用机器学习优化索引结构以适应特定数据集，通过训练模型预测向量位置，适用于专业工作负载。\n*   **流式更新**：成为一流操作，而非批处理重建，新的索引结构支持增量更新。",
      "shortSummary": "向量数据库是现代AI应用中实现高维向量快速、可扩展相似性搜索的关键。它们通过HNSW、IVF和乘积量化等近似最近邻（ANN）算法，解决了传统数据库在高维嵌入搜索中的性能瓶颈。向量数据库通过分片、过滤和混合搜索处理规模化问题，并允许在召回率和延迟之间进行权衡。当处理数百万向量、需要低延迟搜索或元数据过滤时，向量数据库是必需的。主要选项包括Pinecone、Weaviate、Qdrant等，并且功能正向混合索引、多模态搜索和流式更新发展。",
      "translated_title": "机器学习向量数据库完整指南",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/10/mlm-bala-vector-db-guide.jpeg",
          "alt": "The Complete Guide to Vector Databases for Machine Learning",
          "title": "",
          "position": 1
        },
        {
          "url": "https://www.kdnuggets.com/wp-content/uploads/vector-db-hnsw.png",
          "alt": "Hierarchical Navigable Small World (HNSW)",
          "title": "",
          "position": 2
        },
        {
          "url": "https://www.kdnuggets.com/wp-content/uploads/bala-vector-db-ivf-scaled.png",
          "alt": "IVF Inverted File Index",
          "title": "",
          "position": 3
        },
        {
          "url": "https://www.kdnuggets.com/wp-content/uploads/bala-vector-pq-img.png",
          "alt": "Product Quantization",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "完整文章",
      "content": "Vector databases have become essential in most modern AI applications."
    },
    {
      "title": "无需更多GPU即可加速模型训练的3种方法 (原标题: 3 Ways to Speed Up Model Training Without More GPUs)",
      "link": "https://machinelearningmastery.com/3-ways-to-speed-up-model-training-without-more-gpus/",
      "pubDate": "Thu, 16 Oct 2025 17:14:34 +0000",
      "isoDate": "2025-10-16T17:14:34.000Z",
      "creator": "Shittu Olumide",
      "summary": "# 无需更多GPU即可加速模型训练的3种方法\n\n本文介绍了三种无需增加GPU即可显著加速模型训练的有效方法，主要通过优化精度、内存和数据流来实现。\n\n![无需更多GPU即可加速模型训练的3种方法](https://machinelearningmastery.com/wp-content/uploads/2025/10/mlm-3-ways-speed-model-training-without-gpu.png)\n\n## 引言\n\n训练大型模型通常耗时漫长，人们常本能地寻求更多GPU。然而，预算限制和云服务配额等问题使得额外硬件并非总是可行。好消息是，在不增加任何GPU的情况下，仍有多种方法可以显著加快训练速度。加速训练不仅仅是原始计算能力的问题，更关乎如何高效利用现有资源，减少内存交换、GPU空闲和未优化数据管道造成的浪费。\n\n## 方法一：混合精度与内存优化\n\n混合精度是无需新GPU即可加速训练的最简便方法之一。\n\n### 核心思想\n现代GPU能以比标准32位浮点数快得多的速度处理半精度（FP16）或bfloat16数学运算。通过使用更小的数据类型进行存储和计算，可以减少内存使用和带宽，使更多数据同时驻留在GPU上，从而加快操作完成速度。\n\n*   **主要操作**：使用较低精度（FP16或BF16）。\n*   **关键部分**：如损失缩放和少量累积，保持全精度（FP32）以维持稳定性。\n\n### 工作原理\n正确实施混合精度通常能使训练速度提高1.5至2倍，且对准确性几乎没有影响。PyTorch、TensorFlow和JAX都原生支持，大多数NVIDIA、AMD和Apple GPU也提供硬件加速。\n\n**PyTorch示例（自动混合精度AMP）:**\n使用`torch.cuda.amp.GradScaler`和`autocast()`上下文管理器。`autocast()`自动为操作选择FP16或FP32，而`GradScaler()`通过动态调整损失比例来防止下溢。GPU因处理和计算的字节数减少而执行更快。\n\n### 内存优化技巧\n与混合精度相辅相成，内存优化可以进一步提升效率：\n\n*   **梯度检查点（Gradient checkpointing）**：仅保存关键激活值，在反向传播时重新计算其他值，以计算换取内存。\n*   **激活卸载（Activation offloading）**：将不常用的张量临时移动到CPU内存。\n\n这些功能可通过`torch.utils.checkpoint`或DeepSpeed、Hugging Face Accelerate、bitsandbytes等库自动配置。\n\n### 适用场景与预期收益\n*   模型紧密占用GPU内存，或批次大小较小。\n*   使用较新的GPU（RTX 20系列或更新）。\n*   可容忍训练过程中轻微的数值变化。\n*   **预期收益**：训练速度提高30-100%，内存使用减少高达50%。\n\n## 方法二：梯度累积与有效批次大小技巧\n\n当GPU内存成为瓶颈，导致无法使用大批次训练时，梯度累积提供了一个优雅的解决方案。\n\n### 核心思想\n不一次性处理一个巨大的批次，而是将其拆分为更小的微批次。对每个微批次执行前向和反向传播，累积梯度，并在多次迭代后才更新模型权重。这允许在相同硬件上模拟大批次训练。\n\n### 工作原理\n*   损失值除以累积步数以保持梯度平衡。\n*   梯度在步骤之间存储在内存中，而不是被清除。\n*   在累积`accum_steps`个微批次后，优化器执行一次更新。\n\n这种方法可以将虚拟批次大小扩大四到八倍，提高稳定性并可能加速收敛，同时不超出GPU内存限制。\n\n### 重要性\n*   更大的有效批次减少了梯度更新中的噪声，改善了复杂模型的收敛性。\n*   可与混合精度结合使用以获得额外收益。\n*   当内存而非计算是限制因素时，尤其有效。\n\n### 适用场景\n*   使用大批次时遇到“内存不足”错误。\n*   希望获得大批次的好处，但不改变硬件。\n*   数据加载器或数据增强管道能够跟上每次更新的多个微步骤。\n\n## 方法三：智能卸载与分片训练 (ZeRO)\n\n随着模型规模的增长，GPU内存往往在计算能力之前成为主要瓶颈。智能卸载和分片训练（如ZeRO）旨在通过智能地分割和分配内存使用来解决此问题。\n\n### ZeRO工作原理\n在多GPU设置中，通常每个GPU都保存模型参数、梯度和优化器状态的完整副本，这对于大型模型来说是巨大的浪费。ZeRO（Zero Redundancy Optimizer）通过在设备间分片这些状态来打破这种重复：\n\n*   **ZeRO Stage 1**：分片优化器状态。\n*   **ZeRO Stage 2**：分片优化器状态和梯度。\n*   **ZeRO Stage 3**：分片所有内容，包括模型参数、梯度和优化器状态。\n\n这样，每个GPU仅持有总内存占用量的一小部分，但它们仍协同计算完整的更新。这使得训练模型的大小可以显著超过单个GPU的内存容量。\n\n### 适用场景\n*   训练大型模型（数亿或数十亿参数）。\n*   即使使用混合精度仍出现GPU内存不足。\n*   使用多个GPU或分布式节点。\n\n## 额外优化技巧\n\n除了上述三种主要方法，还有一些小而重要的优化可以带来显著差异：\n\n1.  **优化数据管道**：通过并行化和预取数据来提高GPU利用率。在PyTorch中，可调整`DataLoader`的`num_workers`、`pin_memory=True`和`prefetch_factor`。对于大型数据集，使用WebDataset、TFRecord或Parquet等优化格式。\n2.  **优化前先进行性能分析**：使用PyTorch Profiler、TensorBoard Profiler或NVIDIA Nsight Systems等工具找出训练循环中的实际瓶颈，可能不是GPU计算，而是数据增强、日志记录或损失计算。\n3.  **使用提前停止和课程学习**：提前停止可避免不必要的训练周期；课程学习从简单示例开始，逐步引入复杂示例，帮助模型更快收敛。\n4.  **监控内存和利用率**：定期检查GPU内存使用情况（如`torch.cuda.max_memory_allocated()`、`nvidia-smi`），确保GPU得到充分利用。\n5.  **智能组合技术**：将不同策略结合使用可获得最大收益，例如：\n    *   混合精度 + 梯度累积 = 更快、更稳定的训练。\n    *   ZeRO卸载 + 数据管道优化 = 更大模型且无内存错误。\n    *   提前停止 + 性能分析 = 减少浪费的训练周期。\n\n## 何时选择哪种方法\n\n| 方法                     | 最佳适用场景                               | 如何帮助                                                               | 典型速度增益 | 内存影响       | 复杂性   |\n| :----------------------- | :----------------------------------------- | :--------------------------------------------------------------------- | :----------- | :------------- | :------- |\n| 混合精度与内存优化       | 任何紧密占用GPU内存的模型                  | 使用较低精度和更轻量级张量，减少计算和传输开销                         | 1.5 – 2倍    | 减少30–50%     | 低       |\n| 梯度累积与有效批次大小   | 受GPU内存限制但需要大批次大小的模型        | 通过在小批次上累积梯度来模拟大批次训练                                 | 间接速度增益 | 适度额外内存   | 低 – 中等 |\n| 智能卸载与分片训练 (ZeRO) | 无法适应GPU内存的超大型模型                | 将优化器状态、梯度和参数分片到设备或CPU上                              | 10–30%吞吐量 | 释放大部分GPU内存 | 中等 – 高 |\n\n*   **即时效果**：从混合精度开始，它稳定、简单且内置于主流框架。\n*   **内存限制批次大小**：添加梯度累积，它轻量且易于集成。\n*   **模型仍不适配**：使用ZeRO或卸载来分片内存，在相同硬件上训练更大的模型。\n\n## 总结\n\n训练速度不仅取决于GPU数量，更取决于如何有效利用它们。本文介绍的三种方法是无需升级硬件即可加速训练最实用和广泛采用的方式。每种技术本身都能带来实际收益，但它们的真正优势在于组合使用。混合精度通常与梯度累积自然结合，ZeRO也与两者很好地集成。它们共同可以使有效速度翻倍，提高稳定性，并延长现有硬件设置的寿命。在应用这些方法之前，务必先对训练循环进行性能分析和基准测试。",
      "shortSummary": "本文介绍了无需增加GPU即可加速模型训练的三种核心策略：**混合精度与内存优化**利用FP16/BF16减少计算和内存开销，提升1.5-2倍速度；**梯度累积**通过分批处理模拟大批次训练，解决内存限制并提高稳定性；**智能卸载与分片训练 (ZeRO)**则将模型状态分片至多设备或CPU，支持超大型模型训练。此外，优化数据管道、性能分析、提前停止及技术组合也是提升效率的关键。",
      "translated_title": "无需更多GPU即可加速模型训练的3种方法",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/10/mlm-3-ways-speed-model-training-without-gpu.png",
          "alt": "3 Ways to Speed Up Model Training Without More GPUs",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "In this article, you will learn three proven ways to speed up model training by optimizing precision, memory, and data flow &mdash; without adding any..."
    },
    {
      "title": "文本数据特征工程的7个技巧 (原标题: 7 Feature Engineering Tricks for Text Data)",
      "link": "https://machinelearningmastery.com/7-feature-engineering-tricks-for-text-data/",
      "pubDate": "Thu, 16 Oct 2025 15:24:51 +0000",
      "isoDate": "2025-10-16T15:24:51.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "本文介绍了将原始文本数据转换为机器学习和深度学习模型可处理的数值特征的七个关键特征工程技巧。\n\n![文本数据特征工程的7个技巧](https://machinelearningmastery.com/wp-content/uploads/2025/10/mlm-ipc-7-feature-engineering-tricks-text-data.png)\n\n### 引言\n\n*   **背景**：越来越多的AI和机器学习系统依赖文本数据，但机器实际处理的是数字而非语言。\n*   **目的**：特征工程是将原始文本数据转化为有用的数值特征，供模型进行推断的关键步骤。\n*   **技巧分类**：\n    *   **经典机器学习**：技巧1至5通常用于决策树等传统模型。\n    *   **深度学习**：技巧6和7对循环神经网络和Transformer等深度学习模型不可或缺，技巧2（词干提取和词形还原）也可能增强其性能。\n\n### 7个特征工程技巧\n\n1.  **移除停用词 (Removing Stopwords)**\n    *   **目的**：减少数据维度，避免“维度诅咒”。\n    *   **方法**：移除文章、介词、助动词等常见且可能增加噪声的词，保留传达主要语义的词。\n    *   **实现**：通常使用NLTK等库。\n\n2.  **词干提取和词形还原 (Stemming and Lemmatization)**\n    *   **目的**：将词语还原为词根形式，合并不同变体（如动词的不同时态），统一特征表示。\n    *   **适用性**：在深度学习模型中较少需要，但在数据有限时可缓解稀疏性，使模型关注核心词义。\n    *   **实现**：NLTK的PorterStemmer是常用工具。\n\n3.  **基于计数的向量：词袋模型 (Count-based Vectors: Bag of Words)**\n    *   **原理**：经典机器学习中将文本转换为数值特征的最简单方法，通过词频编码为向量。\n    *   **优点**：捕获词在文档中的整体存在和相关性。\n    *   **缺点**：无法捕获词序、上下文或语义关系。\n    *   **适用场景**：对不太复杂的文本分类模型简单有效。\n    *   **实现**：scikit-learn的`CountVectorizer`。\n\n4.  **TF-IDF特征提取 (TF-IDF Feature Extraction)**\n    *   **原理**：自然语言处理的基石方法，超越词袋模型，不仅考虑词频，还考虑词在整个数据集中的逆文档频率（即词的独特性和重要性）。\n    *   **结果**：为每个词在每个文本中分配一个0到1之间的TF-IDF重要性权重。\n    *   **实现**：scikit-learn的`TfidfVectorizer`。\n\n5.  **基于句子的N-Gram (Sentence-based N-Grams)**\n    *   **目的**：捕获词之间的交互和短语级语义（如“new”和“york”）。\n    *   **方法**：使用scikit-learn的`CountVectorizer`，通过设置`ngram_range`参数来包含单词（unigrams）和连续词序列（如bigrams）。\n\n6.  **清洗和分词 (Cleaning and Tokenization)**\n    *   **目的**：移除标点、大小写和其他下游模型可能不理解的符号，生成干净、标准化的词单元（tokens）。\n    *   **基本流程**：将文本分割成词、小写化、移除标点符号或特殊字符。\n    *   **实现**：可使用Python的`re`库构建简单的分词器。\n\n7.  **密集特征：词嵌入 (Dense Features: Word Embeddings)**\n    *   **原理**：当前将文本转换为机器可读信息最强大的方法之一，通过将词映射到向量空间来捕获语义。\n    *   **优点**：意义相似的词（如“shogun”和“samurai”）被编码为数值相似的向量。\n    *   **实现**：使用Word2Vec或spaCy等预定义方法和模型。\n    *   **输出**：嵌入向量的维度由所选算法和模型决定。\n\n### 总结\n\n这些技巧对于机器学习和深度学习模型执行文本分类、摘要等自然语言处理任务时，理解和处理原始文本数据至关重要。",
      "shortSummary": "本文介绍了文本数据特征工程的七个核心技巧，旨在将原始文本转化为机器学习和深度学习模型可处理的数值特征。这些技巧包括：移除停用词、词干提取与词形还原、词袋模型、TF-IDF、N-Gram、文本清洗与分词，以及词嵌入。前五种适用于经典机器学习，后两种对深度学习模型至关重要，共同提升文本分类和摘要等NLP任务的性能。",
      "translated_title": "文本数据特征工程的7个技巧",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/10/mlm-ipc-7-feature-engineering-tricks-text-data.png",
          "alt": "7 Feature Engineering Tricks for Text Data",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "An increasing number of AI and machine learning-based systems feed on text data &mdash; language models are a notable example today."
    }
  ],
  "lastUpdated": "2025-10-30T09:28:56.549Z"
}