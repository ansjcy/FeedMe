{
  "sourceUrl": "https://machinelearningmastery.com/blog/feed/",
  "title": "MachineLearningMastery.com",
  "description": "Making developers awesome at machine learning",
  "link": "https://machinelearningmastery.com/blog/",
  "items": [
    {
      "title": "构建一个用于语言翻译的简单Seq2Seq模型 (原标题: Building a Plain Seq2Seq Model for Language Translation)",
      "link": "https://machinelearningmastery.com/building-a-plain-seq2seq-model-for-language-translation/",
      "pubDate": "Tue, 22 Jul 2025 02:27:18 +0000",
      "isoDate": "2025-07-22T02:27:18.000Z",
      "creator": "Adrian Tam",
      "summary": "## 构建一个用于语言翻译的简单Seq2Seq模型\n\n本文旨在指导读者如何使用LSTM构建和训练一个简单的序列到序列（Seq2Seq）模型，用于语言翻译任务。Seq2Seq模型是一种强大的架构，适用于将一个序列转换为另一个序列的任务，例如机器翻译。它采用编码器-解码器架构，其中编码器处理输入序列，解码器根据编码器的输出生成输出序列。理解Seq2Seq的工作原理有助于阐明注意力机制的原理。\n\n### 概述\n\n文章分为五个主要部分：\n\n*   准备训练数据集\n*   使用LSTM实现Seq2Seq模型\n*   训练Seq2Seq模型\n*   使用Seq2Seq模型\n*   改进Seq2Seq模型\n\n### 准备训练数据集\n\n为了训练Seq2Seq模型，需要一个句子对数据集。本文重用了之前用于Transformer模型的法英翻译数据集（Anki数据集），该数据集可从`manythings.org/anki/`或Google存储下载。数据集文件`fra.txt`每行包含一个英语句子、一个制表符和一个对应的法语句子。\n\n**数据归一化：**\n\n*   将Unicode字符归一化为NFKC形式，以帮助模型更好地理解句子。\n*   将字母转换为小写，以减小词汇量。\n\n**分词（Tokenization）：**\n\n*   模型不能直接处理单词序列，需要先将它们分词并编码为数值形式。\n*   使用Byte Pair Encoding (BPE) 技术，通过识别子词单元来更有效地处理未知词。\n*   为英语和法语分别创建单独的分词器，使用`tokenizers`库。\n*   词汇量大小设置为8000。\n*   添加了三个特殊标记：`[start]`（序列开始）、`[end]`（序列结束）和`[pad]`（填充）。\n*   预分词器（`ByteLevel`）配置为在句子开头添加空格，以帮助词汇重用。\n*   解码器（`ByteLevel`）配置为移除单词边界符号“Ġ”。\n*   训练后的分词器会保存为`en_tokenizer.json`和`fr_tokenizer.json`以供将来使用。\n\n![图片 1](https://machinelearningmastery.com/wp-content/uploads/2025/07/david-emrich-9a0S_8bU0lo-unsplash-scaled.jpg)\n\n### 使用LSTM的Seq2Seq架构\n\n处理任意长度序列通常需要循环神经网络（RNN）架构，其中LSTM是常用的模块之一。\n\n**1. 编码器（EncoderLSTM）：**\n\n*   实现为`nn.Module`的子类。\n*   包含`nn.Embedding`层将输入整数ID转换为嵌入向量。\n*   使用`nn.LSTM`模块处理嵌入后的序列，`batch_first=True`。\n*   默认`num_layers`为1。\n*   `forward`方法接收输入序列，返回LSTM的输出、最终隐藏状态和单元状态（`outputs, hidden, cell`）。\n\n**2. 解码器（DecoderLSTM）：**\n\n*   实现为`nn.Module`的子类，类似于编码器。\n*   除了`nn.Embedding`和`nn.LSTM`外，还包含一个`nn.Linear`层（`self.out`）将LSTM的输出转换为预测下一个标记的logit向量。\n*   `forward`方法接收部分目标序列以及编码器提供的隐藏状态和单元状态。\n*   LSTM模块使用编码器的隐藏状态和单元状态进行初始化。\n*   输出是经过线性层转换的预测序列。\n\n**3. 完整的Seq2Seq模型（Seq2SeqLSTM）：**\n\n*   将编码器和解码器模块连接起来。\n*   `forward`方法用于训练模型，接收输入序列（源语言）和目标序列（目标语言）。\n*   编码器将输入序列转换为“上下文向量”（即其最终隐藏状态和单元状态）。\n*   解码器从特殊标记`[start]`开始，迭代地生成下一个标记，直到达到目标序列的长度。\n*   每次解码器调用都会使用上一步的预测作为当前步的输入。\n*   返回所有时间步的预测结果。\n\n### 训练Seq2Seq模型\n\n为了训练上述模型，需要创建一个PyTorch `Dataset`对象，以便能够以批次和随机顺序迭代数据集。文章提到使用`TranslationDataset`类，但代码示例在关键部分被截断。",
      "shortSummary": "本文详细介绍了如何使用LSTM构建和训练一个简单的Seq2Seq模型进行语言翻译。内容涵盖了数据集的准备，包括数据归一化和使用BPE分词器处理法英句子对。模型架构基于PyTorch实现，包含一个`EncoderLSTM`和一个`DecoderLSTM`，它们通过`Seq2SeqLSTM`类连接。文章解释了编码器如何生成上下文向量，以及解码器如何迭代地生成目标序列。最后，简要提及了训练模型需要使用PyTorch的`Dataset`和`DataLoader`。",
      "translated_title": "构建一个用于语言翻译的简单Seq2Seq模型",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/david-emrich-9a0S_8bU0lo-unsplash-scaled.jpg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2022/11/BTMA-400.png",
          "alt": "Building Transformer Models with Attention",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "This post is divided into five parts; they are: • Preparing the Dataset for Training • Implementing the Seq2Seq Model with LSTM • Training the Seq2Seq Model • Using the Seq2Seq Model • Improving the Seq2Seq Model In <a href=\"https://machinelearningmastery."
    },
    {
      "title": "使用Faker生成合成数据集 (原标题: Synthetic Dataset Generation with Faker)",
      "link": "https://machinelearningmastery.com/synthetic-dataset-generation-with-faker/",
      "pubDate": "Mon, 21 Jul 2025 14:08:31 +0000",
      "isoDate": "2025-07-21T14:08:31.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "# 使用Faker生成合成数据集\n\n## 引言\n\n在许多场景中，获取足够的高质量数据以进行深入分析或构建有效的机器学习模型是一项挑战。因此，合成数据生成成为应对这一挑战的常用方法。Python的Faker库使得生成合成数据变得前所未有的简单，可用于引导现有数据集、测试或匿名化敏感信息。本文将介绍Faker库，并通过一个实践教程，探讨如何生成单条记录或数据实例、一次性生成完整数据集，并将其导出为不同格式。文章从两个角度进行代码演示：\n\n*   **学习：** 了解可生成的数据类型以及如何使用Pandas等库进行处理。\n*   **测试：** 利用生成的数据，演示如何在简化的ETL（提取、转换、加载）管道中测试数据问题。\n\n## 逐步数据生成\n\n### 1. 安装与导入Faker\n\n首次使用Faker需要通过`pip`安装：\n\n```bash\npip install Faker\n```\n\n然后导入必要的库和模块：\n\n```python\nfrom faker import Faker\nimport pandas as pd\nimport random\n```\n\n### 2. 创建Faker实例\n\n创建Faker类的实例，并设置一个固定的随机数生成器种子，以确保代码的可复现性：\n\n```python\nfake = Faker()\nFaker.seed(42)\n```\n\n### 3. 编写数据生成函数\n\n本例中，将编写一个函数`generate_user_for_learning()`来生成模拟真实世界的银行客户记录，包含基本的个人和社会人口属性。该函数返回一个Python字典，其中包含以下属性：\n\n*   **`id`**: 使用`uuid4()`函数生成的唯一用户标识符（UUID）。\n*   **`name`**: 使用`name()`函数随机生成的客户姓名。\n*   **`email`**: 随机生成的电子邮件地址，有10%的几率设为`None`，模拟数据缺失。\n*   **`phone`**: 使用`phone_number()`函数生成的电话号码。\n*   **`birthdate`**: 使用`date_of_birth()`函数生成的出生日期，可指定年龄范围（例如16到85岁）。\n*   **`country`**: 使用`random.choice()`从预定义列表中随机选择国家，包括`None`。\n*   **`income`**: 使用`pyfloat()`函数生成的浮点数收入，保留两位小数。有5%的几率设为`-1000.00`，模拟无效或缺失值。\n\n此函数展示了Faker在生成多种类型数据方面的灵活性，以及模拟真实世界数据缺陷（如缺失值、无效值）的能力。\n\n### 4. 调用函数创建数据\n\n可以通过迭代调用此函数来创建任意数量的客户实例，并将其存储在Pandas DataFrame对象中。例如，生成100个客户数据：\n\n```python\nusers_df = pd.DataFrame([generate_user_for_learning() for _ in range(100)])\nusers_df.head()\n```\n\n生成的部分数据示例如下：\n\n![合成生成的银行客户数据](https://machinelearningmastery.com/wp-content/uploads/2025/07/Captura-de-pantalla-2025-07-17-a-las-10.51.19.png)\n\n## 用例：ETL管道测试\n\n设想一个需要测试摄取银行交易数据的ETL管道的场景。以下代码生成了简化的客户实例和关联的银行交易数据集：\n\n*   **`generate_user_for_testing()`**: 生成简化的用户数据（`id`和`name`）。\n*   **`generate_transaction_for_user(user_id)`**: 生成交易数据，包含：\n    *   `transaction_id`: 2%的几率生成“DUPLICATE_ID”以模拟重复。\n    *   `user_id`: 关联到特定用户。\n    *   `amount`: 随机金额，可为负值（模拟收入和支出）。\n    *   `currency`: 随机选择货币代码。\n    *   `timestamp`: 当年日期时间。\n\n通过这些函数，可以创建用户和交易的DataFrame：\n\n```python\n# ... (代码省略，详见原文)\ndf_users = pd.DataFrame(users_test)\ndf_transactions = pd.DataFrame(transactions_test)\nprint(\"Sample Transactions:\")\ndf_transactions.head()\n```\n\n生成的部分交易数据示例如下：\n\n![合成生成的银行交易数据](https://machinelearningmastery.com/wp-content/uploads/2025/07/Captura-de-pantalla-2025-07-17-a-las-11.13.23.png)\n\n在这种ETL管道场景中，数据质量测试的重点领域可能包括：\n\n*   **孤立交易：** `user_id`未关联到任何现有用户ID的交易。\n*   **重复交易ID：** 具有相同`transaction_id`的交易。\n*   **无效货币代码：** 使用了非预期的货币代码。\n\n## 总结\n\n本文介绍了Python的Faker库及其生成合成数据集的能力。在数据稀缺的真实世界场景中，合成数据对于支持分析、测试和机器学习模型训练至关重要。Faker提供了一种灵活且强大的方式来满足这些需求，并能模拟真实世界的数据缺陷，从而提高测试的有效性。",
      "shortSummary": "Faker是一个Python库，用于生成合成数据集，以应对高质量数据获取的挑战。它能创建各种类型的假数据，包括单条记录和完整数据集，并支持导出。Faker的强大之处在于能够模拟真实世界数据缺陷，如缺失值、重复项和无效值，这使其成为测试ETL管道和机器学习模型训练的理想工具。文章通过银行客户和交易数据的生成示例，详细展示了Faker的安装、使用方法及其在数据质量测试中的应用。",
      "translated_title": "使用Faker生成合成数据集",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-faker-data-generation.png",
          "alt": "Synthetic Dataset Generation with Faker",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/Captura-de-pantalla-2025-07-17-a-las-10.51.19.png",
          "alt": "Synthetically generated bank customer data",
          "title": "",
          "position": 2
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/Captura-de-pantalla-2025-07-17-a-las-11.13.23.png",
          "alt": "Synthetically generated bank transaction data",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "完整文章",
      "content": "In this article, you will learn: • how to use the Faker library in Python to generate various types of synthetic data."
    },
    {
      "title": "从线性回归到XGBoost：性能并排比较 (原标题: From Linear Regression to XGBoost: A Side-by-Side Performance Comparison)",
      "link": "https://machinelearningmastery.com/from-linear-regression-to-xgboost-a-side-by-side-performance-comparison/",
      "pubDate": "Fri, 18 Jul 2025 12:00:24 +0000",
      "isoDate": "2025-07-18T12:00:24.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "## 从线性回归到XGBoost：性能并排比较\n\n![从线性回归到XGBoost：性能并排比较](https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-from-lr-to-xgboost-1.png)\n\n### 引言\n\n回归是机器学习模型能够解决的主流任务之一，旨在根据预测特征（predictors）对目标变量（label）进行数值预测或估计。本文重点比较了两种广泛使用的回归模型：线性回归和XGBoost，通过实际案例突出它们的特点、优缺点。\n\n### 线性回归\n\n*   **模型特性**\n    *   **参数化模型**：基于数学方程定义，形式为超平面。\n    *   **方程**：`y = β₀ + β₁x₁ + β₂x₂ + ... + βnxn + ε`，其中`β`是可学习参数（权重和偏置项），`ε`是误差项。\n    *   **可学习参数**：对于`n`个预测属性，模型有`n+1`个可学习参数（`n`个权重和一个偏置项`β₀`）。\n*   **实践案例**\n    *   **数据集**：使用简化版的加州房价数据集。\n    *   **数据准备**：\n        *   导入所需库：`pandas`, `numpy`, `sklearn` (用于模型、评估、数据分割和标准化), `xgboost`, `matplotlib`。\n        *   加载数据集并选择数值特征。\n        *   将数据分为特征`X`和目标`y`（`median_house_value`）。\n        *   使用`train_test_split`将数据分割为训练集和测试集（80%训练，20%测试）。\n        *   使用`StandardScaler`对特征进行标准化（推荐操作）。\n    *   **模型训练与预测**：\n        *   初始化并训练`LinearRegression`模型：`lr_model.fit(X_train_scaled, y_train)`。\n        *   在测试集上进行预测：`y_pred_lr = lr_model.predict(X_test_scaled)`。\n*   **模型评估**\n    *   **评估指标**：\n        *   **均方根误差 (RMSE)**：与目标变量单位相同，值越低越好。\n        *   **R² 分数 (决定系数)**：模型能解释目标变量方差的比例，值越高（越接近1）越好。\n    *   **线性回归结果**：\n        *   RMSE: 70025.94\n        *   R²: 0.6377\n    *   **结果解读**：考虑到房价在几十万美元的范围，RMSE误差中等，R²接近64%表示表现尚可，但有提升空间。\n*   **特征重要性**\n    *   通过访问`lr_model.coef_`（权重）和`lr_model.intercept_`（偏置项）获取。\n    *   **注意**：负权重表示对预测值的反向影响，而非重要性低；应关注绝对值大小。标准化有助于解释特征归因。\n*   **优缺点**\n    *   **优点**：可作为良好的基线模型，简单，参数数量可控，易于通过权重解释。\n    *   **缺点**：当数据存在非线性模式时，性能可能受限。\n\n### 从线性回归到XGBoost\n\n*   **模型特性**\n    *   **集成模型**：由多个单一模型组合而成，通常在预测精度上超越简单模型。\n    *   **优势**：在大多数场景下能显著提升性能。\n*   **实践案例**\n    *   **模型训练与预测**：\n        *   初始化并训练`XGBRegressor`模型：`xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=4, random_state=42)`。\n        *   在测试集上进行预测：`y_pred_xgb = xgb_model.predict(X_test_scaled)`。\n*   **模型评估**\n    *   **评估指标**：与线性回归使用相同的RMSE和R²分数。\n    *   **XGBoost结果**：\n        *   RMSE: 48493.29\n        *   R²: 0.8262\n    *   **结果解读**：预测精度显著提升，RMSE降低了30%，R²从0.64提高到近0.83。\n*   **特征重要性**\n    *   XGBoost提供直观的方式获取和可视化特征的相对重要性：`xgb.plot_importance(xgb_model)`。\n\n![XGBoost集成模型中的特征重要性](https://machinelearningmastery.com/wp-content/uploads/2025/07/featureimpXGBoost.png)\n\n*   **优缺点**\n    *   **优点**：在大多数场景下显著提升性能，能够建模复杂的非线性模式和特征间的交互（基于决策树）。\n\n### 比较与结论\n\n*   **性能对比**：XGBoost在加州房价数据集上的表现明显优于线性回归，RMSE更低，R²更高。\n*   **特征重要性**：两种模型在分配特征重要性方面表现相似，都认为房屋位置（经纬度）是最重要的属性。\n*   **总结**：\n    *   线性回归可作为机器学习实践者的良好起点和基线模型，在数据集足够简单时可能表现良好，且易于解释。\n    *   XGBoost作为更复杂、更灵活的模型，通常能带来更优异的结果，尤其擅长处理复杂和非线性数据模式。\n    *   在大多数情况下，选择XGBoost等更复杂的模型可能会获得更好的回报。",
      "shortSummary": "本文对比了线性回归和XGBoost两种机器学习回归模型的性能。线性回归作为简单、可解释的基线模型，在处理非线性数据时存在局限。XGBoost作为集成模型，在加州房价数据集上表现出显著优势，RMSE降低30%，R²提升至0.83，能有效处理复杂非线性模式。结论是，虽然线性回归是好的起点，但XGBoost通常能提供更优异的预测结果。",
      "translated_title": "从线性回归到XGBoost：性能并排比较",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-from-lr-to-xgboost-1.png",
          "alt": "From Linear Regression to XGBoost: A Side-by-Side Performance Comparison",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/featureimpXGBoost.png",
          "alt": "Feature importance in the XGBoost ensemble model",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "Regression is undoubtedly one of the most mainstream tasks machine learning models can address."
    },
    {
      "title": "使用LLM嵌入进行特征工程：增强Scikit-learn模型 (原标题: Feature Engineering with LLM Embeddings: Enhancing Scikit-learn Models)",
      "link": "https://machinelearningmastery.com/feature-engineering-with-llm-embeddings-enhancing-scikit-learn-models/",
      "pubDate": "Thu, 17 Jul 2025 12:00:17 +0000",
      "isoDate": "2025-07-17T12:00:17.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "# 使用LLM嵌入进行特征工程：增强Scikit-learn模型\n\n本文探讨了如何利用大型语言模型（LLM）嵌入来增强Scikit-learn模型的性能，特别是在需要深度文本上下文理解的任务中，如意图识别或情感分析。\n\n## 什么是LLM嵌入？\n\nLLM嵌入是LLM生成的整个文本序列的语义丰富的数值（向量）表示。它们与传统文本嵌入（如Word2Vec、FastText）有显著区别：\n\n*   **上下文感知性**：传统嵌入是单个单词的无上下文、固定向量表示；而LLM嵌入是整个序列的表示，其中单词的含义是上下文化的。\n*   **生成方式**：虽然LLM通常生成文本序列作为输出，但某些特定模型（如`all-miniLM`）专门设计用于生成上下文丰富的输出嵌入（即数值表示），而非文本。\n*   **语义信息**：这些输出嵌入具有更丰富的语义信息，使其成为下游模型的合适输入。\n\n![Feature Engineering with LLM Embeddings: Enhancing Scikit-learn Models](https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-feature-engineering-llm-embeddings.png)\n![LLM embeddings as LLM outputs](https://machinelearningmastery.com/wp-content/uploads/2025/07/LLMembeddings-scaled.png)\n\n## 在特征工程中使用LLM嵌入\n\n利用LLM嵌入进行特征工程的步骤如下：\n\n1.  **获取LLM嵌入**：\n    *   使用Hugging Face的`SentenceTransformers`库中合适的LLM模型，例如`all-MiniLM-L6-v2`。\n    *   文章以客户支持工单数据集为例，该数据集包含文本（查询和投诉）以及结构化（数值）客户行为特征。\n    *   通过以下Python代码将原始文本特征转换为LLM嵌入：\n        ```python\n        from sentence_transformers import SentenceTransformer\n        import pandas as pd\n\n        url = \"https://raw.githubusercontent.com/gakudo-ai/open-datasets/refs/heads/main/customer_support_dataset.csv\"\n        df = pd.read_csv(url)\n\n        # Extract columns\n        text_data = df[\"text\"].tolist()\n        structured_data = df[[\"prior_tickets\", \"account_age_days\"]].values\n        labels = df[\"label\"].tolist()\n\n        model = SentenceTransformer('all-MiniLM-L6-v2')\n        X_embeddings = model.encode(text_data)\n        ```\n\n2.  **准备结构化数据并合并特征**：\n    *   对数值特征`prior_tickets`和`account_age_days`进行标准化处理，因为它们的值范围差异较大。\n    *   使用`Numpy`的`hstack()`函数将标准化后的结构化特征与LLM嵌入合并。\n        ```python\n        from sklearn.preprocessing import StandardScaler\n        import numpy as np\n\n        scaler = StandardScaler()\n        structured_scaled = scaler.fit_transform(structured_data)\n        X_combined = np.hstack([structured_scaled, X_embeddings])\n        ```\n\n3.  **模型训练与评估**：\n    *   将合并后的数据集（包含LLM嵌入）分割为训练集和测试集（80%训练，20%测试）。\n    *   使用`stratify=labels`参数确保在小数据集中各类别在训练集和测试集中的代表性。\n    *   初始化并训练一个`RandomForestClassifier`模型进行客户工单分类（共五种类别）。\n    *   在测试集上评估模型性能。\n        ```python\n        from sklearn.model_selection import train_test_split\n        from sklearn.ensemble import RandomForestClassifier\n        from sklearn.metrics import classification_report\n\n        # Split\n        X_train, X_test, y_train, y_test = train_test_split(X_combined, labels, test_size=0.2, random_state=42, stratify=labels)\n\n        # Train\n        clf = RandomForestClassifier()\n        clf.fit(X_train, y_train)\n\n        # Evaluate\n        y_pred = clf.predict(X_test)\n        print(classification_report(y_test, y_pred))\n        ```\n\n## 评估结果与讨论\n\n模型评估结果显示：\n\n```\n              precision    recall  f1-score   support\n\n      billing       0.50      1.00      0.67         2\n          bug       1.00      1.00      1.00         2\n     delivery       1.00      0.50      0.67         2\n        login       1.00      1.00      1.00         2\n       refund       1.00      0.50      0.67         2\n\n     accuracy                           0.80        10\n    macro avg       0.90      0.80      0.80        10\n weighted avg       0.90      0.80      0.80        10\n```\n\n*   在五类别分类问题上，模型达到了80%的准确率和0.80的加权F1分数，即使在数据集非常小的情况下也表现出色。\n*   这主要归功于LLM嵌入，它们将原始文本转换为数值丰富的特征，捕捉了客户工单的语义意图，使得随机森林分类器能够区分细微的请求。\n*   **未来改进方向**：\n    *   与使用传统特征（如TF-IDF）的模型进行比较，以量化LLM嵌入的增值。\n    *   调整分类器的超参数。\n    *   将此技术应用于更大、更鲁棒的数据集以确认其有效性。\n\n## 总结\n\n本文强调了LLM嵌入在训练下游机器学习模型中的重要性，特别是对于包含文本的部分结构化数据。通过简单的步骤，文章展示了如何对原始文本特征进行特征工程，将其语义信息整合为LLM嵌入，从而有效提升Scikit-learn模型在文本密集型任务上的性能。",
      "shortSummary": "LLM嵌入通过提供语义丰富的上下文化文本向量表示，显著增强了Scikit-learn模型在文本相关任务中的性能。文章展示了如何将LLM嵌入与结构化数据结合进行特征工程，并在一个小型客户支持工单分类任务中，使用随机森林分类器实现了80%的准确率和0.80的加权F1分数。这证明了LLM嵌入在捕捉文本深层语义意图方面的强大能力，有效提升了模型的分类效果。",
      "translated_title": "使用LLM嵌入进行特征工程：增强Scikit-learn模型",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-feature-engineering-llm-embeddings.png",
          "alt": "Feature Engineering with LLM Embeddings: Enhancing Scikit-learn Models",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/LLMembeddings-scaled.png",
          "alt": "LLM embeddings as LLM outputs",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "Large language model embeddings, or LLM embeddings, are a powerful approach to capturing semantically rich information in text and utilizing it to leverage other machine learning models &mdash; like those trained using Scikit-learn &mdash; in tasks that require deep contextual understanding of text, such as intent recognition or sentiment analysis."
    },
    {
      "title": "重新审视 k-Means：使其表现更好的 3 种方法 (原标题: Revisiting k-Means: 3 Approaches to Make It Work Better)",
      "link": "https://machinelearningmastery.com/revisiting-k-means-3-approaches-to-make-it-work-better/",
      "pubDate": "Wed, 16 Jul 2025 14:32:02 +0000",
      "isoDate": "2025-07-16T14:32:02.000Z",
      "creator": "Matthew Mayo",
      "summary": "# 重新审视 k-Means：使其表现更好的 3 种方法\n\n## 引言\n\nk-means 算法是无监督机器学习的基石，以其简单性和在将数据划分为预定数量的簇方面的效率而闻名。它通过将数据点分配给最近的质心并更新质心来工作。然而，标准 k-means 在处理真实世界数据时面临挑战，包括对初始质心位置敏感、需要预先指定簇数量以及假设簇是球形且大小均匀。为了克服这些局限性，数据科学社区开发了多种改进方法。本文将探讨三种最有效的技术：\n\n*   使用 k-means++ 进行更智能的质心初始化\n*   利用轮廓系数（silhouette score）寻找最优簇数量\n*   应用核技巧（kernel trick）处理非球形数据\n\n![Revisiting k-Means: 3 Approaches to Make It Work Better](https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-making-kmeans-work-better.png)\n\n## 1. 使用 k-means++ 进行更智能的质心初始化\n\n标准 k-means 算法的一个主要弱点是其对随机质心初始化的依赖。糟糕的初始放置可能导致收敛到次优聚类解决方案并增加计算时间。k-means++ 算法旨在克服这一问题，它采用一种更智能、概率性的方法来选择初始质心。它首先随机选择第一个质心，然后以与该点到最近现有质心平方距离成比例的概率选择后续数据点作为质心。这种方法倾向于选择远离已选中心的数据点，从而实现更分散和战略性的初始放置。\n\n这种方法增加了找到更好最终聚类解决方案的可能性，并通常减少了收敛所需的迭代次数。在实践中，大多数现代机器学习库（包括 Scikit-learn）已将 k-means++ 集成作为默认初始化方法。\n\n**示例结果：**\n\n通过比较惯性（inertia，数据点到其质心距离的平方和，值越低越好），k-means++ 表现显著优于随机初始化：\n\n*   k-means++ 惯性：400582.24\n*   随机初始化惯性：664535.63\n\n## 2. 使用轮廓系数寻找最优簇数量\n\nk-means 的一个明显限制是需要预先指定簇的数量 k。在许多实际场景中，最优 k 值是未知的，不正确的选择可能导致数据过度或欠分割。“肘部法则”等方法可能含糊不清。\n\n轮廓系数是一种更健壮和定量的评估聚类解决方案质量的方法，它衡量簇之间的分离程度。对于每个数据点，轮廓系数基于两个值计算：\n\n*   **内聚度（cohesion）**：到同一簇中其他点的平均距离。\n*   **分离度（separation）**：到最近邻簇中点的平均距离。\n\n轮廓系数的范围是 -1 到 +1，高值表示该点与其自身簇匹配良好，而与相邻簇匹配不佳。为了找到最优的 k，可以对一系列不同的 k 值运行 k-means 算法，并计算每个 k 值的平均轮廓系数。产生最高平均分数的 k 值通常被认为是最佳选择。\n\n**示例结果：**\n\n文章展示了 k 从 2 到 10 的轮廓系数计算结果：\n\n*   k = 2, 轮廓系数: 0.4831\n*   k = 3, 轮廓系数: 0.4658\n*   k = 4, 轮廓系数: 0.5364\n*   k = 5, 轮廓系数: 0.5508\n*   k = 6, 轮廓系数: 0.4464\n*   k = 7, 轮廓系数: 0.3545\n*   k = 8, 轮廓系数: 0.2534\n*   k = 9, 轮廓系数: 0.1606\n*   k = 10, 轮廓系数: 0.0695\n\n最优簇数量 (k) 为：5，最高轮廓系数为：0.5508。\n\n![Figure 1: Silhouette scores for various numbers of clusters (for k values from 2 to 10)](https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-making-kmeans-work-better-figure-1.png)\n图 1: 不同簇数量的轮廓系数（k 值从 2 到 10）\n\n## 3. 使用核 k-Means 处理非球形簇\n\nk-means 最令人沮丧的限制之一是它假设簇是凸形和各向同性的（大致球形且大小相似）。当面对包含复杂、细长或非线性形状的真实世界数据时，标准 k-means 无法正确识别这些模式。\n\n为了解决这个问题，可以采用核技巧。核 k-means 通过将数据隐式投影到更高维空间中，在该空间中簇可能变得线性可分或更接近球形。这通过使用核函数（如径向基函数 RBF）来完成，该函数在不显式计算新坐标的情况下计算高维空间中数据点之间的相似性。通过在这个转换后的特征空间中操作，核 k-means 可以识别标准算法无法检测到的复杂、非球形形状的簇。\n\n虽然 Scikit-learn 没有直接的 KernelKMeans 实现，但其 SpectralClustering 算法提供了一个强大的替代方案，可以有效地实现类似的结果。谱聚类使用数据的连通性来形成簇，并且在寻找非凸簇方面特别有效，可被视为核 k-means 的一种形式。\n\n**示例结果：**\n\n文章通过“月亮形”非球形数据，比较了标准 k-means 和谱聚类的效果：\n\n![Figure 2: Sample non-spherical data](https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-making-kmeans-work-better-figure-2.png)\n图 2: 样本非球形数据\n\n![Figure 3: Comparison of clustering on non-spherical data](https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-making-kmeans-work-better-figure-3.png)\n图 3: 非球形数据上的聚类比较\n\n结果表明，谱聚类（作为核 k-means 的替代）在这种情况下明显优于标准 k-means。\n\n## 总结\n\n通过结合这些巧妙的方法，k-means 算法可以克服其最显著的局限性，并适应真实世界数据复杂、混乱的性质：\n\n*   **k-means++ 初始化**：提供更健壮的起点，带来更好、更一致的结果。\n*   **轮廓系数**：为确定最优簇数量提供定量方法，消除了算法关键参数的猜测。\n*   **核方法（通过谱聚类）**：使 k-means 摆脱球形簇的假设，识别数据中复杂的模式。\n\n应用这些实用技术，可以充分发挥 k-means 的潜力，并从数据中获得更深入、更准确的见解。",
      "shortSummary": "k-Means算法因其简单性而广受欢迎，但也存在局限性，如对初始质心敏感、需预设簇数量以及假设簇为球形。为克服这些问题，文章介绍了三种改进方法：使用k-means++进行更智能的质心初始化以提高收敛性和结果质量；利用轮廓系数定量确定最优簇数量，避免猜测；以及通过核技巧（如谱聚类）处理非球形数据，识别复杂模式。这些增强使k-Means在实际应用中更强大、更准确。",
      "translated_title": "重新审视 k-Means：使其表现更好的 3 种方法",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-making-kmeans-work-better.png",
          "alt": "Revisiting k-Means: 3 Approaches to Make It Work Better",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-making-kmeans-work-better-figure-1.png",
          "alt": "Figure 1: Silhouette scores for various numbers of clusters (for k values from 2 to 10)",
          "title": "",
          "position": 2
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-making-kmeans-work-better-figure-2.png",
          "alt": "Figure 2: Sample non-spherical data",
          "title": "",
          "position": 3
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-making-kmeans-work-better-figure-3.png",
          "alt": "Figure 3: Comparison of clustering on non-spherical data",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "完整文章",
      "content": "The k-means algorithm is a cornerstone of unsupervised machine learning, known for its simplicity and trusted for its efficiency in partitioning data into a predetermined number of clusters."
    },
    {
      "title": "探讨决策树：如何进行良好的分割？ (原标题: Discussing Decision Trees: What Makes a Good Split?)",
      "link": "https://machinelearningmastery.com/discussing-decision-trees-what-makes-a-good-split/",
      "pubDate": "Tue, 15 Jul 2025 12:00:57 +0000",
      "isoDate": "2025-07-15T12:00:57.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "## 探讨决策树：如何进行良好的分割？\n\n### 引言\n\n尽管当前先进的AI解决方案多基于Transformer、扩散模型等复杂深度学习架构，但决策树和随机森林等中等复杂度的模型在各种预测任务中，尤其是在商业领域（如金融欺诈检测、客户流失预测、供应链优化）中，仍是流行且有效的解决方案。本文深入探讨了决策树的内部工作机制，特别是如何通过数据驱动的分割来构建分支。\n\n### 决策树的构建方式\n\n构建决策树的核心原则是**同质性（Homogeneity）**。目标是将训练数据分割成尽可能同质的子集，即每个子集包含尽可能少的混合类别。\n\n![Discussing Decision Trees: What Makes a Good Split?](https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-discussing-dt-what-makes-good-split.png)\n\n**示例：** 想象一个装有不同颜色球的大盒子，需要将其分成两个小盒子。最佳分割方式是使每个小盒子内的球颜色尽可能单一。在提供的选项中，选项B能实现最高的同质性，因此是最佳选择。\n\n![Homogeneity is the guiding principle for building a good decision tree](https://machinelearningmastery.com/wp-content/uploads/2025/07/boxsplitting-scaled.png)\n\n**构建过程：**\n*   决策树的训练始于一个根节点，该节点包含所有训练数据。例如，在Iris数据集中，初始节点包含所有150个鸢尾花实例。\n*   **关键：** 类别同质性是分割根节点并逐步生长决策树的关键标准。数据实例应逐渐分离到尽可能同质的节点中。\n\n### 进行良好的分割\n\n节点分割并非随机发生，而是通过定义一个与数据集属性相关的**分割条件**和**阈值**来完成。例如，条件`[ petal length (cm) <= 2.45 ]`会将150个实例分成两组：花瓣长度小于等于2.45厘米的实例进入一个子节点（50个），大于2.45厘米的进入另一个子节点（100个）。\n\n![An example decision tree built on the iris dataset](https://machinelearningmastery.com/wp-content/uploads/2025/07/decisiontreeiris.png)\n\n**分割机制：**\n*   **算法：** 决策树构建算法（如CART，Classification And Regression Trees）会搜索并识别能够最大化类别同质性或最小化节点杂质的分割条件。\n*   **杂质度量：** 杂质是节点中实例异质性的度量，常用的度量包括**熵（Entropy）**和**基尼指数（Gini index）**。\n*   **目标：** 算法会评估大量可能的[属性-阈值]组合，并选择能够最大化**杂质减少量**的分割。这意味着从一个多样化的节点向两个更同质的节点转变。\n*   随着在树中向下遍历，每个节点中实例的类别分布会越来越集中于单一类别，这正是构建树的目标。\n\n### 实践考量：避免过拟合\n\n在实践中，过度追求树的每个部分的完全同质性通常不是一个好主意。过度生长树以实现完全同质性可能导致模型**过拟合**，即模型记忆了训练数据而非学习其潜在模式，从而无法很好地泛化到未来的未见数据。因此，在追求类别同质性的同时，必须谨慎应用此过程，以避免模型泛化能力不足。\n\n### 总结\n\n本文探讨了决策树模型中节点分割的关键方面，这是构建和生长决策树以进行分类和回归预测任务的核心。文章以通俗易懂的语言，揭示了构建能够基于动态定义的规则或条件进行准确预测的决策树的内部机制，强调了同质性作为指导原则，并提醒了避免过拟合的重要性。",
      "shortSummary": "决策树和随机森林是有效的预测模型，其核心在于通过数据驱动的分割来构建树结构。构建决策树的指导原则是最大化节点内数据的同质性，即最小化杂质。CART等算法通过评估属性和阈值条件，选择能最大程度减少杂质的分割点。然而，过度追求完全同质性可能导致模型过拟合，因此在实践中需谨慎平衡，以确保模型对未见数据的泛化能力。",
      "translated_title": "探讨决策树：如何进行良好的分割？",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-discussing-dt-what-makes-good-split.png",
          "alt": "Discussing Decision Trees: What Makes a Good Split?",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/boxsplitting-scaled.png",
          "alt": "Homogeneity is the guiding principle for building a good decision tree",
          "title": "",
          "position": 2
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/decisiontreeiris.png",
          "alt": "An example decision tree built on the iris dataset",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "完整文章",
      "content": "It’s no secret that most advanced artificial intelligence solutions today are predominantly based on impressively powerful and complex models like transformers, diffusion models, and other deep learning architectures."
    },
    {
      "title": "7个Pandas技巧，让你的数据准备时间减半 (原标题: 7 Pandas Tricks That Cut Your Data Prep Time in Half)",
      "link": "https://machinelearningmastery.com/7-pandas-tricks-that-cut-your-data-prep-time-in-half/",
      "pubDate": "Mon, 14 Jul 2025 13:45:04 +0000",
      "isoDate": "2025-07-14T13:45:04.000Z",
      "creator": "Jayita Gulati",
      "summary": "## 7个Pandas技巧，让你的数据准备时间减半\n\n![7 Pandas Tricks That Cut Your Data Prep Time in Half](https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-gulati-7-pandas-tricks-cut-data-prep-time-half.png)\n\n数据准备是数据科学或分析项目中耗时最长的部分之一。本文介绍了七个实用的Pandas技巧，可以显著加快数据准备过程，帮助用户将更多精力放在分析上，而不是数据清理。\n\n### 核心技巧\n\n1.  **使用 `assign()` 链式转换**\n    *   **目的**：创建新列或修改现有列时，通过链式调用方法，避免创建中间变量或将过程分解为多个步骤。\n    *   **示例**：\n        ```python\n        df = (df.assign(total_sales=lambda x: x['units_sold'] * x['unit_price'])\n              .assign(log_sales=lambda x: np.log1p(x['total_sales'])))\n        ```\n    *   **说明**：此方法允许在一个语句中连续创建或修改多个列，提高代码的简洁性和可读性。\n\n2.  **在 `fillna()` 中使用字典填充缺失值**\n    *   **目的**：同时填充多个列的缺失值，并为每个列指定不同的填充策略。\n    *   **示例**：\n        ```python\n        df.fillna({'price': 0, 'category': 'Unknown'}, inplace=True)\n        ```\n    *   **说明**：通过字典，可以灵活控制每个列的缺失值填充方式，例如将价格填充为0，类别填充为“Unknown”。\n\n3.  **使用 `explode()` 展平列表列**\n    *   **目的**：将包含列表数据的列（如JSON或嵌套CSV）展平，使列表中的每个元素成为单独的行，同时保持其他列不变。\n    *   **示例**：\n        ```python\n        df.explode('tags')\n        ```\n    *   **说明**：这对于处理一对多关系非常有用，例如一行数据关联多个标签时。\n\n4.  **使用 `query()` 进行可读性过滤**\n    *   **目的**：使用类似SQL的表达式进行数据过滤，提高复杂条件下的代码可读性，避免深层嵌套的括号。\n    *   **示例**：\n        ```python\n        df.query('region == \"West\" and sales > 5000')\n        ```\n    *   **说明**：使得过滤条件一目了然，即使是复杂的逻辑也能轻松理解。\n\n5.  **使用 `groupby().agg()` 进行命名聚合**\n    *   **目的**：在数据汇总时，为聚合指标分配自定义的、有意义的名称，而不是使用默认名称。\n    *   **示例**：\n        ```python\n        df.groupby('category').agg(\n            avg_sales=('sales', 'mean'),\n            max_discount=('discount', 'max'))\n        ```\n    *   **说明**：结果DataFrame的列名将更具描述性，如`avg_sales`和`max_discount`。\n\n6.  **使用 `pd.to_datetime()` 进行日期解析**\n    *   **目的**：将混乱的日期字符串转换为标准的日期时间对象，便于后续的日期操作。\n    *   **示例**：\n        ```python\n        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n        ```\n    *   **说明**：设置`errors='coerce'`可以将无法解析的日期字符串转换为`NaT`（非时间），避免解析错误。\n\n7.  **使用 `pipe()` 构建模块化工作流**\n    *   **目的**：通过链式调用自定义函数，构建模块化、可重用的数据转换管道，提高代码的可维护性和可读性。\n    *   **示例**：\n        ```python\n        def clean_prices(df):\n            return df.assign(price=lambda x: x['price'].fillna(0))\n\n        def add_sales_tax(df, rate=0.1):\n            return df.assign(final_price=lambda x: x['price'] * (1 + rate))\n\n        df = (\n            df\n            .pipe(clean_prices)\n            .pipe(add_sales_tax, rate=0.08)\n        )\n        ```\n    *   **说明**：每个函数都接收DataFrame作为第一个参数并返回修改后的DataFrame，使得数据流清晰可见。\n\n### 总结\n\n掌握这些Pandas技巧可以显著提高数据准备的速度和代码清晰度。它们有助于减少样板代码，编写更清晰的代码，并更快地准备好数据进行分析，从而将更多时间用于生成有价值的洞察。\n\n![With just a few Pandas tricks, you can drastically improve your data preparation speed and clarity. Here’s a quick recap.](https://machinelearningmastery.com/wp-content/uploads/2025/07/Screenshot-2025-07-14-at-9.41.33%E2%80%AFAM.png)",
      "shortSummary": "本文介绍了7个Pandas技巧，旨在将数据准备时间减半。这些技巧包括：使用`assign()`进行链式转换、`fillna()`字典填充缺失值、`explode()`展平列表列、`query()`可读性过滤、`groupby().agg()`命名聚合、`pd.to_datetime()`日期解析以及`pipe()`构建模块化工作流。掌握这些方法能帮助数据科学家更高效地清理和转换数据，从而将更多精力投入到数据分析和洞察生成中。",
      "translated_title": "7个Pandas技巧，让你的数据准备时间减半",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-gulati-7-pandas-tricks-cut-data-prep-time-half.png",
          "alt": "7 Pandas Tricks That Cut Your Data Prep Time in Half",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/Screenshot-2025-07-14-at-9.41.33%E2%80%AFAM.png",
          "alt": "With just a few Pandas tricks, you can drastically improve your data preparation speed and clarity. Here’s a quick recap.",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "Data preparation is one of the most time-consuming parts of any data science or analytics project, but it doesn't have to be."
    },
    {
      "title": "词嵌入在表格数据特征工程中的应用 (原标题: Word Embeddings for Tabular Data Feature Engineering)",
      "link": "https://machinelearningmastery.com/word-embeddings-for-tabular-data-feature-engineering/",
      "pubDate": "Fri, 11 Jul 2025 12:00:16 +0000",
      "isoDate": "2025-07-11T12:00:16.000Z",
      "creator": "Matthew Mayo",
      "summary": "### 词嵌入在表格数据特征工程中的应用\n\n**引言**\n\n词嵌入（Word Embeddings）作为词语的密集向量表示，通过量化捕捉词语间的语义关系，彻底改变了自然语言处理（NLP）领域。尽管其主要应用于传统语言处理任务，但本教程探讨了其在表格数据特征工程中的创新应用。传统表格数据中，分类特征通常通过独热编码或标签编码处理，但这两种方法无法捕捉类别间的语义相似性。例如，在产品类别中，“电子产品”和“小工具”可能比“电子产品”和“家具”更相似，而传统编码无法体现这一点。词嵌入则能有效表示这种语义关联，从而可能提升模型性能。\n\n本教程旨在指导读者如何利用预训练词嵌入为表格数据集生成新特征，特别关注表格数据中包含可映射到现有词嵌入的描述性文本的分类列。\n\n**核心概念**\n\n*   **词嵌入（Word Embeddings）**：词语在向量空间中的数值表示。语义相似的词语在该空间中距离更近。\n*   **Word2Vec**：由Google开发的一种流行词嵌入算法，主要架构包括连续词袋模型（CBOW）和Skip-gram。\n*   **GloVe（Global Vectors for Word Representation）**：另一种广泛使用的词嵌入模型，利用语料库中的全局词-词共现统计信息。\n*   **特征工程（Feature Engineering）**：将原始数据转换为能更好代表潜在问题的新特征，以提高机器学习模型性能的过程。\n\n本方法的核心是使用预训练的Word2Vec模型（例如在Google News上训练的模型），将分类文本条目转换为其对应的词向量。这些向量随后成为表格数据的新数值特征。当分类值具有可利用的内在文本含义时，此技术尤其有用。\n\n**实际应用：使用Word2Vec进行特征工程**\n\n教程以一个假设数据集为例，其中包含一个名为`ItemDescription`的列，用于描述商品。目标是使用预训练的Word2Vec模型将这些描述转换为数值特征。\n\n1.  **导入所需库**：`pandas`、`numpy`和`gensim.models.KeyedVectors`。\n2.  **模拟数据集**：创建一个包含`ItemID`、`Price`、`ItemDescription`（如'electronics', 'gadget', 'appliance'等）和`Sales`的Pandas DataFrame。\n3.  **加载预训练Word2Vec模型**：\n    *   尝试加载大型预训练模型（如`GoogleNews-vectors-negative300.bin`）。\n    *   如果文件不存在，则创建一个小型虚拟模型用于演示目的。\n4.  **创建获取词嵌入的函数**：\n    *   定义`get_word_embedding(description, model)`函数。\n    *   该函数查询模型以获取描述的嵌入向量。\n    *   如果词语未找到，则返回一个零向量。\n5.  **应用函数并生成新特征**：\n    *   确定嵌入维度（`model.vector_size`）。\n    *   为每个维度创建新的列名（例如`desc_embedding_0`, `desc_embedding_1`）。\n    *   将`get_word_embedding`函数应用于DataFrame的`ItemDescription`列。\n    *   将生成的嵌入向量扩展为独立的列，并与原始DataFrame（去除`ItemDescription`列）进行拼接，形成新的特征工程后的DataFrame。\n\n**总结**\n\n通过利用预训练词嵌入，教程成功地将一个分类文本特征转换为丰富的数值表示，该表示捕捉了语义信息。这组新特征可以输入到机器学习模型中，从而可能提高模型性能，尤其是在分类值之间的关系微妙且具有文本含义的任务中。\n\n需要注意的是，嵌入的质量很大程度上取决于所使用的预训练模型及其训练语料库。这项技术不仅限于产品描述，还可以应用于任何包含描述性文本的分类列，例如“职位”、“流派”或“客户反馈”（在适当的文本处理以提取关键词之后）。关键在于分类列中的文本应具有足够的意义，以便通过词嵌入进行有效表示。\n\n![Word Embeddings for Tabular Data Feature Engineering](https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-word-embeddings-tabular-data-feature-engineering.png)",
      "shortSummary": "本教程介绍了如何将词嵌入技术应用于表格数据进行特征工程。传统方法无法捕捉分类特征间的语义相似性，而词嵌入能将描述性文本（如产品类别）转换为捕捉语义关系的数值向量。通过使用预训练的Word2Vec模型，文章演示了如何将文本描述转换为新的数值特征，并将其整合到表格数据中。这种方法能为机器学习模型提供更丰富的语义信息，从而潜在地提升模型性能，适用于任何包含有意义描述性文本的分类列。",
      "translated_title": "词嵌入在表格数据特征工程中的应用",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-word-embeddings-tabular-data-feature-engineering.png",
          "alt": "Word Embeddings for Tabular Data Feature Engineering",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "It would be difficult to argue that word embeddings &mdash; dense vector representations of words &mdash; have not dramatically revolutionized the field of natural language processing (NLP) by quantitatively capturing semantic relationships between words."
    },
    {
      "title": "决策树不仅仅适用于表格数据 (原标题: Decision Trees Aren’t Just for Tabular Data)",
      "link": "https://machinelearningmastery.com/decision-trees-arent-just-for-tabular-data/",
      "pubDate": "Thu, 10 Jul 2025 09:57:51 +0000",
      "isoDate": "2025-07-10T09:57:51.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "# 决策树不仅仅适用于表格数据\n\n## 引言\n决策树作为一种多功能、可解释且高效的机器学习技术，数十年来一直是分类和回归任务中广泛使用的模型。它们既可以作为独立模型，也可以作为随机森林和梯度提升机等更强大集成方法的核心组件。除了这些优点，决策树的另一个吸引人之处在于它们能够处理多种数据格式，而不仅仅是完全结构化的表格数据。本文将从理论和实践相结合的角度探讨决策树的这一特性。\n\n## 决策树概述\n决策树是一种用于预测任务（分类和回归）的监督学习模型。它们通过一组带有已知预测输出的标记示例进行训练，例如，一组收集到的动物标本属性及其所属物种。树的构建是一个迭代和递归地将训练数据集划分为子集的过程，旨在使每个子集尽可能地具有类别（或数值标签）的同质性。一旦训练完成，模型就学习到了一组应用于数据属性的层次化决策规则，这些规则可以直观地表示为一棵树。\n\n![决策树不仅仅适用于表格数据](https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-decision-trees-tabular-data-1.png)\n\n对未知标签的示例进行推理预测时，模型会从上到下检查这些规则或条件，最终根据问题是分类还是回归，到达一个指向类别或值预测的“叶节点”。\n\n![决策树概述](https://machinelearningmastery.com/wp-content/uploads/2025/07/decisiontreeexample.png)\n\n## 决策树超越表格数据\n大多数经典机器学习模型（包括决策树）通常处理的是结构化或表格数据，这些数据以实例（行）的形式组织，并通过数值和分类属性（列）进行描述。然而，决策树也能够处理非严格表格的数据集或其部分。\n\n常见的非表格数据示例包括文本、图像和时间序列。通过应用合适的预处理技术，这些数据格式可以转换为更结构化的形式。例如，一段文本序列（如客户对产品的评论）可以通过特征提取或嵌入转换为结构化数据，然后作为决策树分类器的输入，用于分析客户评论背后的积极或消极情绪。\n\n另一种在包含部分非结构化数据的预测任务中利用决策树的策略是使用混合解决方案，将深度学习模型与决策树结合起来。例如，一个卷积神经网络（CNN）可以被训练用于从图像中提取结构化特征（推断出尺寸、形状、颜色等属性），然后这些基于图像的特征被传递给基于树的模型（如随机森林）进行预测，例如估计产品销售额。\n\n在研究领域，已经有一些努力直接调整基于决策树的模型来处理图数据和层次数据等非表格数据，尽管它们在主流应用中仍然罕见。\n\n## 实际示例\n为了增加一些实践性，我们将演示如何在一个结合了纯表格数据和文本数据的数据集上训练一个基于决策树的模型。\n\n该代码示例的核心逻辑如下：\n1.  使用一个包含三个预测属性（其中一个是文本）的客户支持工单数据集。\n2.  文本数据需要预处理才能输入到决策树模型中。\n3.  使用TF-IDF向量化器获取每个文本的向量表示。\n4.  将这个新的文本特征与其它数值特征合并，以训练决策树分类器，并在测试集上进行评估。\n\n您可以通过执行此代码来训练模型，但可能会对其性能感到失望（正确预测和不正确预测的数量大致相同）。这是预期的结果，因为我们使用的是一个只有100个实例的小数据集，而从文本表示中学习通常需要更多的实例。\n\n## 结论\n本文讨论了决策树和基于决策树的机器学习模型（如随机森林）处理非严格表格数据的能力。从文本到图像再到时间序列，机器学习模型和数据可以通过预处理或组合的方式来适应那些乍一看似乎无法处理的数据。",
      "shortSummary": "决策树不仅限于处理表格数据，还能有效应对文本、图像和时间序列等非结构化数据。这主要通过两种方式实现：一是将非结构化数据通过预处理（如TF-IDF）转换为结构化特征；二是采用混合方案，结合深度学习模型提取特征后，再由决策树进行预测。文章通过一个结合文本和表格数据的实际案例，展示了决策树在处理多源异构数据方面的强大能力。",
      "translated_title": "决策树不仅仅适用于表格数据",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-decision-trees-tabular-data-1.png",
          "alt": "Decision Trees Aren’t Just for Tabular Data",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/decisiontreeexample.png",
          "alt": "Outline of a decision tree",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "Versatile, interpretable, and effective for a variety of use cases, decision trees have been among the most well-established machine learning techniques for decades, widely used for classification and regression tasks."
    },
    {
      "title": "10 个 NumPy 单行代码简化特征工程 (原标题: 10 NumPy One-Liners to Simplify Feature Engineering)",
      "link": "https://machinelearningmastery.com/10-numpy-one-liners-to-simplify-feature-engineering/",
      "pubDate": "Tue, 08 Jul 2025 12:00:29 +0000",
      "isoDate": "2025-07-08T12:00:29.000Z",
      "creator": "Bala Priya C",
      "summary": "## 10 个 NumPy 单行代码简化特征工程\n\n![10 NumPy One-Liners to Simplify Feature Engineering](https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-bala-feature-eng-one-liners.jpeg)\n\n在构建机器学习模型时，特征工程是获得竞争优势的关键。虽然 Pandas 和 Scikit-learn 提供了强大的工具，但 NumPy 的向量化操作能使特征工程更快、更优雅。本文探讨了 10 种强大的 NumPy 单行代码，利用其广播、高级索引和数学函数来高效创建新特征。\n\n### 1. 使用中位数绝对偏差 (MAD) 进行鲁棒缩放\n\n标准缩放对正态分布数据有效，但容易受异常值影响。MAD 缩放提供了一种鲁棒的替代方案，能够处理大量异常值，尤其适用于金融或网络分析等领域。该方法通过将数据围绕中位数居中，然后除以 MAD（中位数绝对偏差的中位数）来实现，从而提供不受异常值影响的尺度度量。\n\n### 2. 使用分位数对连续变量进行分箱\n\n将连续变量转换为分类箱对于许多算法至关重要，并有助于捕获非线性关系。等宽分箱可能导致组不平衡，而基于分位数的分箱能确保每个箱中样本数量大致相同。这对于树模型或创建可解释特征特别有用。`np.percentile()` 计算分位数边界，`np.digitize()` 将值分配到相应的箱中。\n\n### 3. 无循环的多项式特征\n\n多项式特征有助于捕获变量间的非线性关系和交互效应。传统的循环方法效率低下，而 NumPy 的列表推导式结合 `np.column_stack()` 可以高效生成所有可能的二次多项式组合（包括平方项和交互项），为模型提供非线性关系信息。\n\n### 4. 时间序列的滞后特征\n\n时间序列分析常需要捕获时间依赖性。滞后特征允许模型访问历史值，这对于预测和异常检测至关重要。`np.roll()` 函数可以平移数组元素，结合列表推导式和 `np.column_stack()` 可以同时生成多个滞后版本，并通过切片处理边缘情况，确保训练数据干净。\n\n### 5. 无 Pandas 的独热编码\n\n独热编码是处理机器学习中分类变量的必要步骤。虽然 Pandas 提供了便利方法，但纯 NumPy 实现对于大型数据集而言更快、更节省内存。该技术利用广播将每个类别值与所有可能的类别进行比较，然后将布尔结果转换为整数，创建二进制矩阵。\n\n### 6. 基于坐标的距离特征\n\n地理空间特征通常需要计算与参考点的距离，这在位置模型中很常见。NumPy 的广播功能可以同时从所有位置减去参考点，然后通过平方差求和并取平方根来高效计算欧几里得距离。这对于处理数百万个数据点的大型数据集至关重要。\n\n### 7. 变量对之间的交互特征\n\n特征交互有助于理解单个特征可能遗漏的隐藏模式。手动创建所有成对交互既繁琐又容易出错。该向量化方法通过嵌套列表推导式系统地生成所有唯一的特征对乘积，并使用 `.T` 转置结果，确保每行代表一个样本，每列代表一个交互项。\n\n### 8. 滚动窗口统计\n\n滚动统计可以平滑噪声数据并捕获局部趋势，这对于时间序列分析和信号处理至关重要。基于卷积的方法既优雅又高效。`np.convolve()` 函数结合均匀平均核 (`np.ones(window_size)/window_size`) 和 `mode='valid'` 参数，可以自然地实现滚动窗口操作，轻松扩展到其他窗口函数。\n\n### 9. 异常值指示特征\n\n与其移除异常值，不如创建特征来标记它们的存在，这可以为模型提供有价值的信息，尤其是在欺诈检测或质量控制等领域。该方法使用数据的百分位数（例如 5% 和 95%）作为异常值阈值，将超出此范围的值标记为 1，从而创建二进制特征，指示异常观测值。\n\n### 10. 分类变量的频率编码\n\n频率编码用分类值的出现次数替换原始值，这可能比任意标签编码更具信息量，尤其当类别频率与目标变量相关时。该方法首先使用 `np.unique()` 找到所有唯一类别及其计数，然后为每个原始类别值查找对应的频率计数，生成一个数值特征，表示该类别在数据集中出现的频率。\n\n### 特征工程的最佳实践\n\n在创建新的代表性特征时，应牢记以下几点：\n*   **内存效率：** 处理大型数据集时，考虑特征工程对内存的影响。\n*   **特征选择：** 并非越多特征越好，应使用相关性分析或特征重要性等技术选择最相关的特征。\n*   **验证：** 始终在保留集上验证工程特征，确保它们能提高模型性能且不会导致过拟合。\n*   **领域知识：** 最好的工程特征通常源于对问题领域的深入理解。NumPy 技术是高效实现这些领域洞察的工具。\n\n### 结论\n\n这些 NumPy 单行代码是解决常见特征工程挑战的实用方案。无论处理时间序列、地理空间数据还是传统表格数据集，这些技术都能帮助构建更高效、更易维护的特征工程管道。关键在于了解何时使用每种方法以及如何组合它们以从数据中提取最大信号。最佳的特征工程技术是能帮助模型学习特定问题领域模式的技术。将这些单行代码作为构建块，并通过适当的交叉验证和领域专业知识来验证其有效性。",
      "shortSummary": "本文介绍了10个NumPy单行代码，旨在简化和加速机器学习中的特征工程。这些技巧利用NumPy的向量化操作、广播和高级索引，高效处理数据预处理任务，包括鲁棒缩放、分位数分箱、生成多项式和滞后特征、独热编码、计算距离、创建交互特征、滚动统计、标记异常值及频率编码。文章强调NumPy在处理大规模数据集时的效率优势，并提供了特征工程的最佳实践，帮助构建更高效、可维护的机器学习管道。",
      "translated_title": "10 个 NumPy 单行代码简化特征工程",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/07/mlm-bala-feature-eng-one-liners.jpeg",
          "alt": "10 NumPy One-Liners to Simplify Feature Engineering",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "When building machine learning models, most developers focus on model architectures and hyperparameter tuning."
    }
  ],
  "lastUpdated": "2025-07-29T09:34:35.016Z"
}