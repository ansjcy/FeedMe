{
  "sourceUrl": "https://machinelearningmastery.com/blog/feed/",
  "title": "MachineLearningMastery.com",
  "description": "Making developers awesome at machine learning",
  "link": "https://machinelearningmastery.com/blog/",
  "items": [
    {
      "title": "7 Agentic AI Trends to Watch in 2026",
      "link": "https://machinelearningmastery.com/7-agentic-ai-trends-to-watch-in-2026/",
      "pubDate": "Mon, 05 Jan 2026 11:00:16 +0000",
      "isoDate": "2026-01-05T11:00:16.000Z",
      "creator": "Vinod Chugani",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "7 Agentic AI Trends to Watch in 2026",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/12/mlm-chugani-agentic-ai-trends-watch-2026-feature-2.png",
          "alt": "Agentic AI Trends Watch 2026",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "RSS",
      "content": "The agentic AI field is moving from experimental prototypes to production-ready autonomous systems."
    },
    {
      "title": "Gradient Descent:The Engine of Machine Learning Optimization",
      "link": "https://machinelearningmastery.com/gradient-descentthe-engine-of-machine-learning-optimization/",
      "pubDate": "Fri, 02 Jan 2026 11:00:17 +0000",
      "isoDate": "2026-01-02T11:00:17.000Z",
      "creator": "Matthew Mayo",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Gradient Descent:The Engine of Machine Learning Optimization",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2026/01/mlm-visualizing-foundations-ml-gradient-descent-feature.png",
          "alt": "Gradient Descent: Visualizing the Foundations of Machine Learning",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2026/01/mlm-visualizing-foundations-ml-gradient-descent-infographic-scaled.png",
          "alt": "Gradient Descent: Visualizing the Foundations of Machine Learning [Infographic]",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "Editor's note: This article is a part of our series on visualizing the foundations of machine learning."
    },
    {
      "title": "Train Your Large Model on Multiple GPUs with Tensor Parallelism",
      "link": "https://machinelearningmastery.com/train-your-large-model-on-multiple-gpus-with-tensor-parallelism/",
      "pubDate": "Wed, 31 Dec 2025 21:22:39 +0000",
      "isoDate": "2025-12-31T21:22:39.000Z",
      "creator": "Adrian Tam",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Train Your Large Model on Multiple GPUs with Tensor Parallelism",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2026/01/seth-kane-XOEAHbE_vO8-unsplash-scaled.jpg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2026/01/ColwiseParallel.png",
          "alt": "",
          "title": "",
          "position": 2
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2026/01/RowwiseParallel.png",
          "alt": "",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "RSS",
      "content": "This article is divided into five parts; they are: • An Example of Tensor Parallelism • Setting Up Tensor Parallelism • Preparing Model for Tensor Parallelism • Train a Model with Tensor Parallelism • Combining Tensor Parallelism with FSDP Tensor parallelism originated from the Megatron-LM paper."
    },
    {
      "title": "Train Your Large Model on Multiple GPUs with Fully Sharded Data Parallelism",
      "link": "https://machinelearningmastery.com/train-your-large-model-on-multiple-gpus-with-fully-sharded-data-parallelism/",
      "pubDate": "Tue, 30 Dec 2025 22:12:18 +0000",
      "isoDate": "2025-12-30T22:12:18.000Z",
      "creator": "Adrian Tam",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Train Your Large Model on Multiple GPUs with Fully Sharded Data Parallelism",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/12/ferenc-horvath-9cYiqVDeXDc-unsplash-scaled.jpg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/12/FSDP-concept.png",
          "alt": "",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "This article is divided into five parts; they are: • Introduction to Fully Sharded Data Parallel • Preparing Model for FSDP Training • Training Loop with FSDP • Fine-Tuning FSDP Behavior • Checkpointing FSDP Models Sharding is a term originally used in database management systems, where it refers to dividing a database into smaller units, called shards, to improve performance."
    },
    {
      "title": "Beyond Short-term Memory: The 3 Types of Long-term Memory AI Agents Need",
      "link": "https://machinelearningmastery.com/beyond-short-term-memory-the-3-types-of-long-term-memory-ai-agents-need/",
      "pubDate": "Tue, 30 Dec 2025 11:00:59 +0000",
      "isoDate": "2025-12-30T11:00:59.000Z",
      "creator": "Vinod Chugani",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Beyond Short-term Memory: The 3 Types of Long-term Memory AI Agents Need",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/12/mlm-chugani-beyond-short-term-memory-3-types-long-term-memory-ai-agents-need-feature-b.png",
          "alt": "Beyond Short-term Memory: 3 Types Long-term Memory AI Agents Need",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/12/mlm-chugani-beyond-short-term-memory-3-types-long-term-memory-ai-agents-need-b.png",
          "alt": "beyond short-term memory 3 types long-term memory ai agents need",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "If you've built chatbots or worked with language models, you're already familiar with how AI systems handle memory within a single conversation."
    },
    {
      "title": "Train Your Large Model on Multiple GPUs with Pipeline Parallelism",
      "link": "https://machinelearningmastery.com/train-your-large-model-on-multiple-gpus-with-pipeline-parallelism/",
      "pubDate": "Mon, 29 Dec 2025 20:56:53 +0000",
      "isoDate": "2025-12-29T20:56:53.000Z",
      "creator": "Adrian Tam",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Train Your Large Model on Multiple GPUs with Pipeline Parallelism",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/12/ivan-ivankovic-M0uDTaOUZmw-unsplash-scaled.jpg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/12/pipeline-parallelism-bubbles.png",
          "alt": "",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "This article is divided into six parts; they are: • Pipeline Parallelism Overview • Model Preparation for Pipeline Parallelism • Stage and Pipeline Schedule • Training Loop • Distributed Checkpointing • Limitations of Pipeline Parallelism Pipeline parallelism means creating the model as a pipeline of stages."
    },
    {
      "title": "5 Python Libraries for Advanced Time Series Forecasting",
      "link": "https://machinelearningmastery.com/5-python-libraries-for-advanced-time-series-forecasting/",
      "pubDate": "Mon, 29 Dec 2025 11:00:23 +0000",
      "isoDate": "2025-12-29T11:00:23.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "5 Python Libraries for Advanced Time Series Forecasting",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/12/Untitled-design.png",
          "alt": "5 Python Libraries for Advanced Time Series Forecasting",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/12/mlm-5-python-libraries-advanced-ts-forecasting-2.jpeg",
          "alt": "5 Python Libraries for Advanced Time Series Forecasting: A Comparison",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "Predicting the future has always been the holy grail of analytics."
    },
    {
      "title": "Training a Model on Multiple GPUs with Data Parallelism",
      "link": "https://machinelearningmastery.com/training-a-model-on-multiple-gpus-with-data-parallelism/",
      "pubDate": "Fri, 26 Dec 2025 06:44:15 +0000",
      "isoDate": "2025-12-26T06:44:15.000Z",
      "creator": "Adrian Tam",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Training a Model on Multiple GPUs with Data Parallelism",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/12/ilse-orsel-hjmV0xG-KPk-unsplash-scaled.jpg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/12/data_parallelism.png",
          "alt": "",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "This article is divided into two parts; they are: • Data Parallelism • Distributed Data Parallelism If you have multiple GPUs, you can combine them to operate as a single GPU with greater memory capacity."
    },
    {
      "title": "Train a Model Faster with torch.compile and Gradient Accumulation",
      "link": "https://machinelearningmastery.com/train-a-model-faster-with-torch-compile-and-gradient-accumulation/",
      "pubDate": "Thu, 25 Dec 2025 16:44:48 +0000",
      "isoDate": "2025-12-25T16:44:48.000Z",
      "creator": "Adrian Tam",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Train a Model Faster with torch.compile and Gradient Accumulation",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/12/francois-genon-IvlV_Dlt9hg-unsplash-scaled.jpg",
          "alt": "",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "RSS",
      "content": "This article is divided into two parts; they are: • Using `torch."
    },
    {
      "title": "Training a Model with Limited Memory using Mixed Precision and Gradient Checkpointing",
      "link": "https://machinelearningmastery.com/training-a-model-with-limited-memory-using-mixed-precision-and-gradient-checkpointing/",
      "pubDate": "Wed, 24 Dec 2025 17:43:03 +0000",
      "isoDate": "2025-12-24T17:43:03.000Z",
      "creator": "Adrian Tam",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Training a Model with Limited Memory using Mixed Precision and Gradient Checkpointing",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/12/meduana-PdnseHuDFZU-unsplash-scaled.jpg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/12/Float_example.svg.png",
          "alt": "",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "RSS",
      "content": "This article is divided into three parts; they are: • Floating-point Numbers • Automatic Mixed Precision Training • Gradient Checkpointing Let's get started! The default data type in PyTorch is the IEEE 754 32-bit floating-point format, also known as single precision."
    }
  ],
  "lastUpdated": "2026-01-15T09:36:25.769Z"
}