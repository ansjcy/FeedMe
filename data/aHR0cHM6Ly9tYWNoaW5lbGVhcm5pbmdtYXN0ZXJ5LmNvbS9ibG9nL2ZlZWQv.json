{
  "sourceUrl": "https://machinelearningmastery.com/blog/feed/",
  "title": "MachineLearningMastery.com",
  "description": "Making developers awesome at machine learning",
  "link": "https://machinelearningmastery.com/blog/",
  "items": [
    {
      "title": "2025年MLOps必备的10个Python库 (原标题: 10 Must-Know Python Libraries for MLOps in 2025)",
      "link": "https://machinelearningmastery.com/10-must-know-python-libraries-for-mlops-in-2025/",
      "pubDate": "Thu, 19 Jun 2025 10:00:08 +0000",
      "isoDate": "2025-06-19T10:00:08.000Z",
      "creator": "Jayita Gulati",
      "summary": "# 2025年MLOps必备的10个Python库\n\nMLOps（机器学习运维）旨在管理机器学习模型从构建、训练、部署到维护的端到端流程。随着机器学习在实际应用中日益普及，拥有合适的工具变得前所未有的重要。2025年，Python依然是机器学习和MLOps领域最受欢迎的语言。本文将探讨2025年每位机器学习专业人士都应了解的10个Python库，它们能帮助数据科学家和机器学习工程师提高工作效率，减少错误，并构建更可靠的系统。\n\n![2025年MLOps必备的10个Python库](https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-10-python-libraries-mlops.png)\n\n以下是这些关键的Python库：\n\n1.  **MLflow**\n    *   **目的：** 帮助跟踪和管理机器学习实验及模型，便于比较结果并与团队共享模型。\n    *   **主要特点：**\n        *   **实验跟踪：** 跟踪和比较机器学习实验的多次运行。\n        *   **模型打包：** 使用MLproject文件以标准格式打包代码。\n        *   **模型注册表：** 集中存储，用于管理模型的生命周期阶段。\n\n2.  **Data Version Control (DVC)**\n    *   **目的：** 允许像管理代码一样对数据和机器学习模型进行版本控制，保持一切井然有序且可复现。\n    *   **主要特点：**\n        *   **数据版本控制：** 跟踪数据集和模型的不同版本。\n        *   **管道管理：** 创建易于重复和更新的机器学习管道。\n        *   **远程存储支持：** 支持将大型文件存储在云端或外部存储中，并与项目关联。\n        *   **Git集成：** 与Git协同工作，可在同一位置管理代码和数据。\n\n3.  **Kubeflow**\n    *   **目的：** 帮助在Kubernetes上运行和管理机器学习工作流，便于大规模构建、训练和部署模型。\n    *   **主要特点：**\n        *   **管道编排：** 使用Kubeflow Pipelines创建和管理机器学习工作流。\n        *   **模型训练：** 支持使用Kubernetes原生自定义资源进行分布式训练。\n        *   **超参数调优：** 自动化超参数调优引擎，支持网格搜索、随机搜索等。\n\n4.  **Apache Airflow**\n    *   **目的：** 允许使用工作流自动化和调度数据及机器学习任务，并提供仪表板来监控和管理这些工作流。\n    *   **主要特点：**\n        *   **DAGs (有向无环图)：** 将工作流定义为Python代码，其中每个节点是一个任务，边表示依赖关系。\n        *   **调度：** 使用类似cron的语法或内置预设，按特定间隔运行任务。\n        *   **监控与UI仪表板：** 附带基于Web的用户界面，用于查看DAGs和监控任务状态。\n        *   **可扩展性：** 具有可插拔架构，通过操作符和钩子支持AWS和Google Cloud等服务。\n\n5.  **BentoML**\n    *   **目的：** 帮助打包机器学习模型，以便将其作为API提供服务。它兼容多种流行的机器学习库。\n    *   **主要特点：**\n        *   **模型服务：** 通过REST API、gRPC或批量推理提供模型服务，设置简单。\n        *   **多框架支持：** 兼容TensorFlow、PyTorch、Scikit-learn、XGBoost、LightGBM等。\n        *   **模型打包：** 将来自多个框架的机器学习模型打包成标准化、版本化的容器。\n\n6.  **FastAPI**\n    *   **目的：** 一个现代、高性能的Python Web框架，用于构建API。它能自动生成交互式文档。\n    *   **主要特点：**\n        *   **高性能：** 基于ASGI构建，速度可与Node.js和Go媲美。\n        *   **API文档：** 使用Swagger UI和ReDoc自动生成交互式文档。\n        *   **Python类型提示：** 使用标准Python类型提示定义请求和响应模式。\n        *   **异步支持：** 内置async和await支持异步端点。\n\n7.  **Prefect**\n    *   **目的：** 帮助构建和运行具有内置错误处理功能的数据和ML管道，即使某些任务失败也能保持工作流运行。\n    *   **主要特点：**\n        *   **Pythonic工作流设计：** 使用Python定义工作流，任务清晰、模块化且可重用。\n        *   **动态调度：** 支持CRON、间隔或事件触发的灵活调度。\n        *   **容错与重试：** 自动重试失败任务，具有可自定义的重试策略和错误处理。\n        *   **可观测性与日志：** 提供管道执行的实时可见性，包括详细日志、警报和仪表板。\n\n8.  **Great Expectations**\n    *   **目的：** 在机器学习模型中使用数据之前检查其清洁度和正确性，并生成报告显示数据检查结果。\n    *   **主要特点：**\n        *   **数据文档：** 生成人类可读的HTML报告，显示应用了哪些检查以及哪些通过或失败。\n        *   **验证工作流与检查点：** 将数据验证作为机器学习或ETL管道的一部分运行，以保持可靠性。\n        *   **与数据生态系统集成：** 与Pandas、SQL数据库、Spark以及Airflow和Prefect等工具协同工作。\n\n9.  **Optuna**\n    *   **目的：** 自动为机器学习模型寻找最佳设置（超参数），通过提前停止表现不佳的测试来节省时间，并显示有用的调优图表。\n    *   **主要特点：**\n        *   **剪枝：** 支持提前停止表现不佳的试验，以节省计算资源。\n        *   **自动化超参数优化：** Optuna自动化寻找最佳超参数的过程，减少手动调优工作。\n        *   **可视化工具：** 提供内置的可视化功能，用于优化历史、参数重要性和中间值，以更好地理解调优过程。\n\n10. **Seldon Core**\n    *   **目的：** 帮助在Kubernetes上部署机器学习模型，以便它们能实时提供预测服务，并监控生产中的模型性能。\n    *   **主要特点：**\n        *   **Kubernetes原生部署：** 将机器学习模型无缝部署为Kubernetes集群上的微服务。\n        *   **多框架支持：** 兼容流行的机器学习框架，包括TensorFlow、PyTorch、XGBoost、Scikit-learn等。\n        *   **监控与日志：** 与Prometheus、Grafana及其他工具集成，提供实时指标、日志和跟踪。\n        *   **高级推理图：** 构建包含多个模型、转换器和路由器的复杂推理管道。\n\n**总结**\n\n在2025年，借助合适的Python库，管理机器学习项目将变得更加容易。这些工具帮助您跟踪实验、版本化数据、训练模型并将其投入生产。使用MLflow、DVC和Kubeflow等库可以节省时间并减少错误，同时使您的工作更有条理，更易于与团队共享。无论您是MLOps新手还是经验丰富，这些库都将帮助您构建更好、更快的机器学习系统。尝试使用它们来改进您的工作流程并获得更好的结果。",
      "shortSummary": "本文介绍了2025年MLOps（机器学习运维）领域必备的10个Python库。这些工具旨在优化机器学习模型的端到端生命周期管理，涵盖了实验跟踪（MLflow）、数据版本控制（DVC）、模型部署（BentoML, Seldon Core）、工作流编排（Kubeflow, Airflow, Prefect）、API构建（FastAPI）、数据质量（Great Expectations）和超参数优化（Optuna）等方面。它们能帮助专业人士提高效率、确保可复现性、减少错误，并加速机器学习系统的开发与部署。",
      "translated_title": "2025年MLOps必备的10个Python库",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-10-python-libraries-mlops.png",
          "alt": "10 Must-Know Python Libraries for MLOps in 2025",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "MLOps, or machine learning operations, is all about managing the end-to-end process of building, training, deploying, and maintaining machine learning models."
    },
    {
      "title": "释放性能：使用 Polars 加速 Pandas 操作 (原标题: Unlocking Performance: Accelerating Pandas Operations with Polars)",
      "link": "https://machinelearningmastery.com/unlocking-performance-accelerating-pandas-operations-with-polars/",
      "pubDate": "Wed, 18 Jun 2025 15:06:20 +0000",
      "isoDate": "2025-06-18T15:06:20.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "## Polars 简介\n\nPolars 是目前单机数据处理和操作领域最快的开源库之一。它采用 Rust 原生构建，旨在优化内存消耗和提高速度，同时提供直观且用户友好的 API。本文将介绍如何在 Python 中使用 Polars 库，并展示其如何像 Pandas 一样无缝地高效处理大型数据集。\n\n## 设置与数据加载\n\n文章使用加州住房数据集作为示例，这是一个中等规模的数据集，包含描述加州每个地区房屋和人口特征的数值和分类属性。\n\n*   **安装 Polars**: 如果是首次使用，需要安装 Polars 库：\n    ```bash\n    pip install polars\n    ```\n*   **导入与数据读取**: 导入 Polars 并使用 `pl.read_csv()` 函数读取数据集，其过程与 Pandas 类似：\n    ```python\n    import polars as pl\n    url = \"https://raw.githubusercontent.com/gakudo-ai/open-datasets/refs/heads/main/housing.csv\"\n    df = pl.read_csv(url)\n    ```\n*   **查看数据**: 查看前几行数据的方法 `df.head()` 也与 Pandas 类似。但与 Pandas 不同的是，Polars 提供了一个 `DataFrame` 属性 `df.schema` 来查看数据集的模式（即属性名称及其类型）。\n\n## 加速数据操作\n\nPolars 提供了两种执行模式：即时（eager）模式和惰性（lazy）模式。\n\n### 即时执行模式 (Eager Execution)\n\n在即时模式下，数据操作会立即执行。\n\n*   **缺失值填充**: 使用 `with_columns()` 方法填充 `total_bedrooms` 列中的缺失值，例如使用该列的中位数进行填充：\n    ```python\n    median_bedrooms = df.select(pl.col(\"total_bedrooms\").median()).item()\n    df = df.with_columns(pl.col(\"total_bedrooms\").fill_null(median_bedrooms))\n    ```\n*   **特征工程**: 基于现有特征创建新特征，例如计算每户房间数、每房间卧室数和每户人口数等比率：\n    ```python\n    df = df.with_columns([\n        (pl.col(\"total_rooms\") / pl.col(\"households\")).alias(\"rooms_per_household\"),\n        (pl.col(\"total_bedrooms\") / pl.col(\"total_rooms\")).alias(\"bedrooms_per_room\"),\n        (pl.col(\"population\") / pl.col(\"households\")).alias(\"population_per_household\")\n    ])\n    ```\n\n### 惰性执行模式 (Lazy Execution)\n\n惰性执行模式通过使用 `lazy()` 函数启用，它会在实际计算发生之前优化一系列操作。这种方法可以使复杂的数据处理工作流执行更高效。实际计算在调用 `collect()` 函数时才发生。\n\n*   **惰性模式下的操作**: 将缺失值填充和特征工程操作转换为惰性模式：\n    ```python\n    ldf = df.lazy()\n    ldf = ldf.with_columns(\n        pl.col(\"total_bedrooms\").fill_null(pl.col(\"total_bedrooms\").median())\n    )\n    ldf = ldf.with_columns([\n        (pl.col(\"total_rooms\") / pl.col(\"households\")).alias(\"rooms_per_household\"),\n        (pl.col(\"total_bedrooms\") / pl.col(\"total_rooms\")).alias(\"bedrooms_per_room\"),\n        (pl.col(\"population\") / pl.col(\"households\")).alias(\"population_per_household\")\n    ])\n    # 实际计算在调用 collect() 时应用\n    result_df = ldf.collect()\n    display(result_df.head())\n    ```\n*   **过滤数据**: 过滤中位数房价高于 $500K 的地区：\n    ```python\n    ldf_filtered = ldf.filter(pl.col(\"median_house_value\") > 500000)\n    ```\n*   **分组与聚合**: 按“海洋邻近度”类别对地区进行分组，并计算每组的平均房价。需要注意的是，Polars 惰性模式下的分组函数是 `group_by()`（带下划线），而非 `groupby()`。\n    ```python\n    avg_house_value = ldf.group_by(\"ocean_proximity\").agg(\n        pl.col(\"median_house_value\").mean().alias(\"avg_house_value\")\n    )\n    ```\n    如果未调用 `collect()`，尝试访问结果将显示一个阶段性管道的可视化图表：\n    \n    ![Polars 中的惰性数据操作](https://machinelearningmastery.com/wp-content/uploads/2025/06/Captura-de-pantalla-2025-06-06-a-las-13.27.01.png)\n    \n    因此，必须调用 `collect()` 来执行操作并获取结果：\n    ```python\n    avg_house_value_result = avg_house_value.collect()\n    display(avg_house_value_result)\n    ```\n\n## 总结\n\nPolars 是一个轻量级且高效的替代方案，用于管理基于 Pandas-like DataFrame 的复杂数据预处理和清洗工作流。本文通过多个示例展示了如何在 Python 中使用 Polars 库的即时和惰性执行模式，从而自定义数据处理管道的规划和执行方式。\n\n![释放性能：使用 Polars 加速 Pandas 操作](https://machinelearningmastery.com/wp-content/uploads/2025/06/1up8A_6kSdyCjJlb7ZFMHg.jpeg)",
      "shortSummary": "Polars 是一个用 Rust 原生构建的开源库，旨在高效、低内存地处理单机大型数据集，并提供直观的 API。它可作为 Pandas 的高性能替代品，支持即时和惰性两种执行模式。文章通过加州住房数据集示例，展示了 Polars 在数据加载、缺失值填充、特征工程、过滤和聚合等操作上的应用。惰性模式通过延迟计算并优化操作序列，进一步提升了复杂数据工作流的效率。",
      "translated_title": "释放性能：使用 Polars 加速 Pandas 操作",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/1up8A_6kSdyCjJlb7ZFMHg.jpeg",
          "alt": "Unlocking Performance: Accelerating Pandas Operations with Polars",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/Captura-de-pantalla-2025-06-06-a-las-13.27.01.png",
          "alt": "Lazy data operations in Polars",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "<a href=\"https://pola."
    },
    {
      "title": "大型语言模型背后的7个概念，7分钟解释清楚 (原标题: 7 Concepts Behind Large Language Models Explained in 7 Minutes)",
      "link": "https://machinelearningmastery.com/7-concepts-behind-large-language-models-explained-in-7-minutes/",
      "pubDate": "Tue, 17 Jun 2025 12:00:25 +0000",
      "isoDate": "2025-06-17T12:00:25.000Z",
      "creator": "Bala Priya C",
      "summary": "![大型语言模型背后的7个概念](https://machinelearningmastery.com/wp-content/uploads/2025/06/bala-llm-concepts.jpeg)\n\n大型语言模型（LLMs）如GPT-4和Claude能够编写代码、解释复杂主题，并提供上下文相关的响应，这背后有几个核心概念在支撑。本文旨在为开发者、产品经理和好奇者解释这些关键概念。\n\n### 1. 分词（Tokenization）\n\n在文本被神经网络处理之前，它必须被转换为数字表示。分词是这一转换过程，它比简单的空格或标点符号分割更复杂。分词器使用如字节对编码（BPE）、WordPiece或SentencePiece等算法来创建词汇表，以平衡效率和表示质量。\n\n![分词过程](https://www.kdnuggets.com/wp-content/uploads/tokenization.png)\n\n这些算法通过从单个字符开始，并逐步组合最常出现的字符对来构建子词词汇表。例如，“unhappiness”可能被分词为[“un”, “happy”, “ness”]。这种子词方法解决了许多关键问题，包括处理词汇表外（OOV）词汇、管理形态丰富的语言，并创建了模型可以处理的固定词汇量（通常为32K到100K个词元）。有效的分词可以缩短序列长度，从而降低处理需求。例如，GPT-4的8K上下文窗口允许处理8,000个词元，大约相当于6,000个单词。\n\n### 2. 嵌入（Embeddings）\n\n嵌入将离散的词元转换为向量表示，通常在数百或数千个维度中。嵌入是密集的向量表示，能够捕捉语义含义。它们将词语放置在一个多维空间中，使相似的概念聚集在一起。\n\n![词嵌入示例](https://www.kdnuggets.com/wp-content/uploads/word-embeddings1.png)\n\n例如，“king”和“queen”在嵌入空间中是近邻，而“king”和“bicycle”则相距遥远。在构建搜索功能或推荐系统时，嵌入是关键，因为即使两段文本不共享确切的词语，但如果它们的嵌入相似，则它们在语义上是相关的。\n\n### 3. Transformer架构\n\nTransformer架构通过引入“注意力机制”彻底改变了自然语言处理。与传统模型顺序处理文本不同，Transformer可以同时查看句子中的所有部分，并找出哪些词对于理解其他词最重要。例如，在处理“The cat sat on the mat because it was comfortable”时，注意力机制帮助模型理解“it”指的是“the mat”，而不是“the cat”。这使得现代LLM能够处理长距离依赖和文本中复杂的关联，从而保持多段对话的连贯性并理解跨文档的上下文。\n\n### 4. 训练阶段：预训练与微调\n\nLLM的开发分为不同的阶段：\n\n*   **预训练（Pre-training）**：模型从海量数据集中学习语言模式，这是一个昂贵且计算密集型的阶段，旨在让模型普遍理解和生成人类语言。\n*   **微调（Fine-tuning）**：在此阶段，预训练模型被专门化，以执行特定任务或适应特定领域。它不是从零开始学习语言，而是教导一个已具备能力的模型在特定应用（如代码生成、医疗诊断或客户支持）中表现出色。\n\n![预训练与微调](https://www.kdnuggets.com/wp-content/uploads/bala-pt-ft.png)\n\n这种方法效率高，因为它允许公司以相对适中的计算预算，通过使用自己的数据微调现有模型来构建强大的、领域特定的LLM。\n\n### 5. 上下文窗口（Context Windows）\n\n每个LLM都有一个上下文窗口，即它一次可以考虑的最大文本量，可以将其视为模型的“操作内存”。超出此窗口的任何内容对模型来说都是不存在的。这给开发者带来了挑战，例如如何构建一个能够记住跨多个会话对话的聊天机器人，或者如何处理比上下文窗口更长的文档。可能的解决方案包括维护对话摘要、在LLM系统中使用内存、检索增强生成（RAG）和滑动窗口技术。\n\n### 6. 温度与采样（Temperature and Sampling）\n\n*   **温度（Temperature）**：帮助平衡语言模型生成响应中的随机性与可预测性。温度为0时，模型总是选择最可能的下一个词元，产生一致但可能重复的结果。较高的温度引入随机性，使输出更具创造性但可预测性较低。\n*   **采样技术**：如Top-k采样和核采样（nucleus sampling）提供额外的文本生成控制机制。Top-k采样将选择限制在k个最高概率的词元，而核采样则通过累积概率阈值自适应地确定候选集。这些技术有助于平衡创造性和连贯性，为开发者提供对模型行为更精细的控制。\n\n### 7. 模型参数与规模（Model Parameters and Scale）\n\n模型参数是编码LLM所知一切的习得权重。大多数大型语言模型通常拥有数千亿个参数，而更大的模型则达到数万亿。这些参数捕捉语言中的模式，从基本语法到复杂的推理能力。通常，更多的参数意味着更好的性能，但这种关系并非线性。扩大模型规模需要指数级增长的计算资源、数据集和训练时间。对于实际开发而言，参数数量会影响推理成本、延迟和内存需求。例如，一个70亿参数的模型可能在消费级硬件上运行，而一个700亿参数的模型则需要企业级GPU。理解这种权衡有助于开发者根据其特定用例和基础设施限制选择合适的模型大小。\n\n**总结**\n\n这些概念构成了每个LLM系统的技术核心。鼓励读者通过构建应用、阅读经典论文（如“Attention Is All You Need”）、探索嵌入技术、实验不同的分词策略以及观察温度变化对输出的影响来进一步理解和实践。",
      "shortSummary": "本文解释了大型语言模型（LLMs）背后的七个核心概念。它们包括：将文本转换为数字表示的**分词**；捕捉语义含义的**嵌入**；通过注意力机制处理文本的**Transformer架构**；模型学习的**预训练与微调**阶段；模型处理文本量的限制——**上下文窗口**；控制生成文本随机性和创造性的**温度与采样**；以及影响性能和资源需求的**模型参数与规模**。理解这些概念对于开发和应用LLMs至关重要。",
      "translated_title": "大型语言模型背后的7个概念，7分钟解释清楚",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/bala-llm-concepts.jpeg",
          "alt": "7 Concepts Behind Large Language Models Explained in 7 Minutes",
          "title": "",
          "position": 1
        },
        {
          "url": "https://www.kdnuggets.com/wp-content/uploads/tokenization.png",
          "alt": "7 Concepts Behind Large Language Models Explained in 7 Minutes",
          "title": "",
          "position": 2
        },
        {
          "url": "https://www.kdnuggets.com/wp-content/uploads/word-embeddings1.png",
          "alt": "7 Concepts Behind Large Language Models Explained in 7 Minutes",
          "title": "",
          "position": 3
        },
        {
          "url": "https://www.kdnuggets.com/wp-content/uploads/bala-pt-ft.png",
          "alt": "7 Concepts Behind Large Language Models Explained in 7 Minutes",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "完整文章",
      "content": "If you've been using large language models like GPT-4 or Claude, you've probably wondered how they can write actually usable code, explain complex topics, or even help you debug your morning coffee routine (just kidding!)."
    },
    {
      "title": "位置编码中的插值与使用 YaRN 扩展上下文窗口 (原标题: Interpolation in Positional Encodings and Using YaRN for Larger Context Window)",
      "link": "https://machinelearningmastery.com/interpolation-in-positional-encodings-and-using-yarn-for-larger-context-window/",
      "pubDate": "Tue, 17 Jun 2025 05:00:10 +0000",
      "isoDate": "2025-06-17T05:00:10.000Z",
      "creator": "Adrian Tam",
      "summary": "# 位置编码中的插值与使用 YaRN 扩展上下文窗口\n\n本文探讨了 Transformer 模型在处理不同长度序列时，如何应对位置编码带来的挑战。由于 Transformer 模型通常在固定序列长度下训练，但在推理时可能需要处理更长的序列，这使得模型难以处理未在训练中遇到的位置编码。\n\n## 概述\n\n文章分为三个主要部分：\n1.  正弦编码和 RoPE 中的插值与外推\n2.  学习型编码中的插值\n3.  使用 YaRN 扩展上下文窗口\n\n## 正弦编码和 RoPE 中的插值与外推\n\n正弦编码（Sinusoidal encodings）由于使用连续函数，在**外推**方面表现出色。对于更长的序列，可以直接将位置 `p` 替换为更大的值来获取位置编码。\n\n此外，也可以使用**插值**。将 `p` 视为浮点数，通过 `p = (L/L')p'` 的方式，将新序列 `L'` 中的位置 `p'` 映射到原始训练长度 `L` 的范围内。\n\n这些技术同样适用于旋转位置编码（RoPE）。生成正弦位置编码或 RoPE 的函数无需修改即可处理任意长度的序列。然而，为了确保模型能够处理新的编码而性能不下降，可能需要进行微调。例如，Llama 模型使用 RoPE，最大训练序列长度为 16K，而 Code Llama 仅通过 1000 步微调就将其序列长度扩展到了 100K token。\n\n## 学习型编码中的插值\n\n学习型位置编码（Learned positional encodings）通过查找表获取位置编码向量，这意味着序列长度由查找表大小固定，因此无法进行**外推**。\n\n然而，学习型编码仍然可以通过**插值**来处理比训练长度更长的序列。对于长度为 `L'` (大于 `L`) 的序列，位置 `p'` 的编码向量可以通过线性插值计算，即在原始训练长度 `L` 范围内找到对应的浮点位置 `p`，然后插值其相邻的两个整数位置 `n` 和 `m` 的编码向量。\n\n尽管可以实现插值，但文章指出，这种基本插值实现并不能保证模型在不进行再训练的情况下，处理更长序列时性能不会下降。\n\n## 使用 YaRN 扩展上下文窗口\n\n旋转位置编码（RoPE）是当前大型语言模型中最广泛使用的位置编码方法。YaRN 是一种旨在扩展 RoPE 以处理更长序列的方法，并且比上述基本插值方法更有效。\n\nYaRN 的核心创新在于，当序列长度从 `L` 扩展到 `L'` 时，它会**不均匀地缩放 RoPE 正弦频率**。这种方法被称为“NTK-by-parts”插值。YaRN 通过引入一个混合了插值和外推效果的 `θ_i'` 来修改原始 RoPE 公式。此外，YaRN 还引入了一个额外的缩放因子 `sqrt(1/t)`，进一步提升了模型在更长上下文长度下的性能（通过降低困惑度衡量）。\n\n## 总结\n\n本文介绍了模型如何处理比训练时更长的输入序列：\n*   正弦编码和 RoPE 易于进行外推。\n*   学习型编码仅支持插值。\n*   YaRN 提供了一种先进的方法，用于将 RoPE 扩展到更长的序列长度。\n\n这些方法的共同目标是使模型能够在不进行再训练的情况下处理更长的输入序列。\n\n![Building Transformer Models with Attention](https://machinelearningmastery.com/wp-content/uploads/2022/11/BTMA-400.png)",
      "shortSummary": "本文探讨了Transformer模型如何处理不同长度序列的位置编码挑战。正弦编码和RoPE擅长外推，也可通过插值处理更长序列，但可能需微调。学习型编码仅支持插值。YaRN是一种先进方法，通过不均匀缩放RoPE频率并引入额外缩放因子，有效扩展RoPE的上下文窗口，提升模型在长序列下的性能。目标是使模型无需再训练即可处理更长输入。",
      "translated_title": "位置编码中的插值与使用 YaRN 扩展上下文窗口",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/enkuu-smile_-kbHvA6oXP8E-unsplash-scaled.jpg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2022/11/BTMA-400.png",
          "alt": "Building Transformer Models with Attention",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "This post is divided into three parts; they are: • Interpolation and Extrapolation in Sinusoidal Encodings and RoPE • Interpolation in Learned Encodings • YaRN for Larger Context Window Sinusoidal encodings excel at extrapolation due to their use of continuous functions: $$ \\begin{aligned} PE(p, 2i) &amp;= \\sin\\left(\\frac{p}{10000^{2i/d}}\\right) \\\\ PE(p, 2i+1) &amp;= \\cos\\left(\\frac{p}{10000^{2i/d}}\\right) \\end{aligned} $$ You can simply substitute $p$ with a larger value to obtain the positional encoding for a longer sequence."
    },
    {
      "title": "如何结合Scikit-learn、CatBoost和SHAP构建可解释的树模型 (原标题: How to Combine Scikit-learn, CatBoost, and SHAP for Explainable Tree Models)",
      "link": "https://machinelearningmastery.com/how-to-combine-scikit-learn-catboost-and-shap-for-explainable-tree-models/",
      "pubDate": "Mon, 16 Jun 2025 12:00:01 +0000",
      "isoDate": "2025-06-16T12:00:01.000Z",
      "creator": "Vinod Chugani",
      "summary": "## 如何结合Scikit-learn、CatBoost和SHAP构建可解释的树模型\n\n### 引言\n\n在机器学习工作流中，高性能模型与可解释性之间存在着微妙的平衡。CatBoost等高性能算法虽然能取得卓越成果，但其“黑箱”特性使得利益相关者难以理解模型决策背后的“为什么”。本文旨在通过结合Scikit-learn、CatBoost和SHAP这三个互补的库来解决这一挑战，从而实现模型的准确性和可解释性。\n\n### 核心整合库\n\n*   **Scikit-learn**：提供预处理生态系统和评估框架，是大多数机器学习工作流的基础。\n*   **CatBoost**：提供最先进的梯度提升性能，并原生支持分类特征处理。\n*   **SHAP (SHapley Additive exPlanations)**：将高性能预测转化为透明、可量化的解释。\n\n### 实际应用场景\n\n本教程以Ames Housing数据集为例，预测房屋价格。这是一个完美的用例，因为在房地产领域，专业人士不仅需要知道模型预测的价格，还需要确切了解哪些特征以及它们如何驱动这些预测。\n\n### 学习目标\n\n通过本教程，您将学会：\n*   创建从Scikit-learn预处理到CatBoost建模再到SHAP详细解释的无缝数据管道。\n*   比较不同的特征重要性方法。\n*   解释复杂的特征交互。\n*   量化邻里效应等分类变量的影响。\n*   构建既准确又可解释的树模型框架。\n\n### 先决条件\n\n在开始本教程之前，您需要：\n*   安装Python 3.7或更高版本。\n*   熟悉Python语法和编程概念。\n*   安装以下库：\n    *   Pandas (1.3.0或更高版本)\n    *   NumPy (1.20.0或更高版本)\n    *   scikit-learn (1.0.0或更高版本)\n    *   CatBoost (1.0.0或更高版本)\n    *   SHAP (0.40.0或更高版本)\n    *   Matplotlib (3.3.0或更高版本) 用于可视化。\n*   对机器学习概念（如回归、训练/测试分割、模型评估）有基本了解。\n\n### 构建CatBoost基础模型\n\n在进行解释之前，首先需要一个高性能模型。文章基于之前对CatBoost的探索，重新构建了一个针对Ames Housing数据集的优化回归模型，实现了0.9310的R²分数。该模型展示了CatBoost在不进行大量预处理的情况下，原生处理缺失值和分类数据的能力。这个高性能模型为后续的解释工作奠定了可靠的基础。\n\n### 整合点1：Scikit-learn → CatBoost 工作流\n\n本节展示了Scikit-learn的预处理和评估工具如何与CatBoost无缝协作。尽管CatBoost可以自动处理许多预处理任务，但结合Scikit-learn可以利用更广泛的数据科学工具生态系统，并建立可扩展到更复杂管道的模式。通过Scikit-learn进行数据分割（训练集2063个样本，测试集516个样本），并在训练数据上训练CatBoost模型，最终在测试集上获得了0.9335的R²分数，验证了模型的强大性能，为SHAP解释提供了可靠的基础。\n\n### 整合点2：CatBoost → SHAP 解释\n\nSHAP将高性能的CatBoost模型转化为可解释的系统。与传统的特征重要性（平均而言哪些变量最重要）不同，SHAP通过量化每个特征对每个个体预测的贡献来提供更深入的洞察。这不仅揭示了哪些特征重要，还揭示了它们在不同上下文和值范围内的行为方式。\n\n**特征重要性比较：CatBoost vs. SHAP**\n\n*   **SHAP TreeExplainer**：计算所有516个测试预测的精确解释，涵盖84个特征。\n*   **共同点**：SHAP和CatBoost的重要性排名都将“GrLivArea”（居住面积）和“OverallQual”（整体质量）列为最重要的特征。\n*   **差异**：在中间排名中存在显著差异，例如“Neighborhood”（社区）在SHAP中排名第三，而在CatBoost中排名第四，反之“TotalBsmtSF”（地下室总面积）则相反。这突出了一个关键区别：CatBoost的重要性反映了特征在树分裂中被使用的频率，而SHAP的重要性则衡量了特征对最终预测的实际影响大小。\n*   **量化影响**：SHAP条形图清晰地可视化了特征影响的程度，显示“GrLivArea”的平均影响几乎是其他任何特征的两倍。这意味着居住面积的变化对预测的影响大约是整体质量变化的两倍，是社区效应的近三倍。\n\n![SHAP Global Feature Importance Plot](https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-chugani-combine-sklearn-catboost-shap-explainable-tree-models-1.png)\n\n### 通过依赖图理解特征交互\n\n全局重要性排名无法揭示特征在不同值范围内的行为或它们之间的交互。SHAP依赖图通过展示特征值与其对个体预测影响之间的关系来解决这些限制。这使得分析从“OverallQual很重要”深入到“OverallQual显示阶梯式增长，其影响取决于其他房屋特征”。\n\n**交互作用示例：**\n\n*   **居住面积（GrLivArea）与地下室总面积（TotalBsmtSF）的交互**：\n    *   当居住面积超过2300平方英尺时，地下室面积越大（红色越深），房屋的SHAP值（即对价格的贡献）越高。这表明买家重视全面的空间，而不仅仅是地上建筑面积。\n*   **整体质量（OverallQual）与建造年份（YearBuilt）的交互**：\n    *   虽然质量始终驱动价值，但颜色模式表明质量评级可能根据房屋建造年份具有不同的含义或市场价值。这反映了随着时间推移，建筑标准和买家期望的变化。\n\n![SHAP Dependence Plots: Standalone vs Interactive Effects](https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-chugani-combine-sklearn-catboost-shap-explainable-tree-models-2-1024x813.png)\n\n这些依赖图揭示了CatBoost学习到的关于特征关系的复杂模式，并展示了SHAP依赖分析如何超越基本的特征重要性，提供更深层次的洞察。",
      "shortSummary": "本文介绍了如何结合Scikit-learn、CatBoost和SHAP库来构建高性能且可解释的树模型。Scikit-learn用于预处理和评估，CatBoost提供强大的梯度提升性能，而SHAP则负责将模型预测转化为透明的解释。通过Ames Housing数据集的案例，文章展示了如何利用SHAP量化特征对个体预测的影响，比较特征重要性，并深入理解特征间的复杂交互，从而揭示“黑箱”模型内部的决策机制，为实际应用提供更深层次的洞察。",
      "translated_title": "如何结合Scikit-learn、CatBoost和SHAP构建可解释的树模型",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-chugani-combine-sklearn-catboost-shap-explainable-tree-models-feature.png",
          "alt": "How to Combine Scikit-learn, CatBoost, and SHAP for Explainable Tree Models",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-chugani-combine-sklearn-catboost-shap-explainable-tree-models-1.png",
          "alt": "How to Combine Scikit-learn, CatBoost, and SHAP for Explainable Tree Models",
          "title": "",
          "position": 2
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-chugani-combine-sklearn-catboost-shap-explainable-tree-models-2-1024x813.png",
          "alt": "How to Combine Scikit-learn, CatBoost, and SHAP for Explainable Tree Models",
          "title": "",
          "position": 3
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-chugani-combine-sklearn-catboost-shap-explainable-tree-models-3-1024x910.png",
          "alt": "How to Combine Scikit-learn, CatBoost, and SHAP for Explainable Tree Models",
          "title": "",
          "position": 4
        }
      ],
      "contentSource": "完整文章",
      "content": "Machine learning workflows often involve a delicate balance: you want models that perform exceptionally well, but you also need to understand and explain their predictions."
    },
    {
      "title": "Transformer模型中的位置编码 (原标题: Positional Encodings in Transformer Models)",
      "link": "https://machinelearningmastery.com/positional-encodings-in-transformer-models/",
      "pubDate": "Sun, 15 Jun 2025 05:06:17 +0000",
      "isoDate": "2025-06-15T05:06:17.000Z",
      "creator": "Adrian Tam",
      "summary": "# Transformer模型中的位置编码\n\n本文深入探讨了Transformer模型中至关重要的位置编码，它们是捕获语言序列特性的关键创新。\n\n![图片 1](https://machinelearningmastery.com/wp-content/uploads/2025/06/svetlana-gumerova-6fAgnT3Dhl4-unsplash-scaled.jpg)\n*语言模型中的位置编码*\n\n## 理解位置编码的必要性\n\n*   **问题背景**：传统的循环神经网络（RNN）按顺序处理词语，自然地捕捉了词序信息。然而，Transformer模型并行处理所有词元，导致其无法区分词元相同但顺序不同的句子（例如：“狐狸跳过狗”和“狗跳过狐狸”）。\n*   **解决方案**：位置编码通过为序列中的每个词元提供位置信息来解决这一问题。\n*   **工作原理**：\n    *   每个词元首先通过模型的嵌入层转换为一个向量，其大小称为“隐藏维度”。\n    *   位置编码会生成一个与隐藏维度相同大小的向量。\n    *   这些位置编码向量被添加到输入词元嵌入中，通常在注意力模块中进行。\n    *   在点积操作期间，这些编码强调了相邻词元之间的关系，帮助模型理解上下文，从而区分词序不同的句子。\n\n## 常见位置编码类型\n\n文章详细介绍了以下五种主要的位置编码类型：\n\n1.  **正弦位置编码 (Sinusoidal Positional Encodings)**：\n    *   **特点**：在原始Transformer论文中引入，使用正弦和余弦函数构建的确定性向量。\n    *   **公式**：\n        $$ PE(p, 2i) = \\sin\\left(\\frac{p}{10000^{2i/d}}\\right) $$\n        $$ PE(p, 2i+1) = \\cos\\left(\\frac{p}{10000^{2i/d}}\\right) $$\n        其中，$p$ 是位置，$i$ 是维度索引，$d$ 是隐藏维度。常数10000应大于最大序列长度。\n    *   **实现原理**：通过计算 `div_term` ($1/N^{2i/d}$)，然后将 `position` 矩阵与 `div_term` 相乘的结果分别代入正弦和余弦函数，最后交错填充到输出矩阵中。\n    *   **优点**：确定性，能够外推到训练时未见的更长序列；利用正弦函数的性质，可以轻松计算词元间的相对位置。\n    *   **局限性**：不适应数据特性，对非常长的序列可能效果不佳。\n\n2.  **学习位置编码 (Learned Positional Encodings)**：\n    *   **特点**：在BERT和GPT等模型中使用，位置向量在训练过程中学习得到。\n    *   **实现原理**：通过 `nn.Embedding` 层实现，该层充当一个查找表，将整数位置索引映射到学习到的向量。在 `forward` 函数中，根据输入序列长度生成位置索引，然后通过嵌入层查找对应的位置编码，并将其添加到输入 `x` 中。\n    *   **优点**：能够适应数据特性，如果训练得当，可能提供更好的性能。\n    *   **局限性**：无法外推到更长的序列；可能存在过拟合风险；作为模型参数的一部分，会增加模型大小。\n\n3.  **旋转位置编码 (Rotary Positional Encodings, RoPE)**：\n    *   **特点**：Llama模型等大多数现代大型语言模型使用，通过旋转矩阵编码相对位置，每个位置代表一个几何级数的角度。\n    *   **公式**：\n        $$ \\hat{x}_m^{(i)} = x_m^{(i)} \\cos(m\\theta_i) + x_m^{(d/2+i)} \\sin(m\\theta_i) $$\n        $$ \\hat{x}_m^{(d/2+i)} = x_m^{(d/2+i)} \\cos(m\\theta_i) – x_m^{(i)} \\sin(m\\theta_i) $$\n        其中 $\\theta_i = 10000^{-2i/d}$，$m$ 是位置索引。\n        矩阵形式：$\\mathbf{\\hat{x}}_m = \\mathbf{R}_m\\mathbf{x}_m$。\n    *   **实现原理**：`rotate_half` 函数将向量的后半部分取负并与前半部分交换位置。`apply_rotary_pos_emb` 函数将输入 `x` 与预先计算好的 `cos` 和 `sin` 值进行旋转操作。`RotaryPositionalEncoding` 类预计算并缓存正弦和余弦值。\n    *   **优点**：\n        *   旋转矩阵 $\\mathbf{R}_m$ 几何地旋转2D输入向量。\n        *   转置 $\\mathbf{R}_m^\\top = \\mathbf{R}_m^{-1}$ 表示反向旋转，因此相对位置可以轻松计算为 $\\mathbf{R}_{m-n} = \\mathbf{R}_m\\mathbf{R}_n^\\top$。\n        *   由于角度的几何级数，可以外推到更长的序列。\n        *   保留向量范数，有助于训练稳定性。\n\n4.  **相对位置编码 (Relative Positional Encodings)**：\n    *   **特点**：T5和MPT等模型使用，关注词元之间的相对距离而非绝对位置。\n    *   **实现原理**：计算 `context_position` 和 `memory_position` 矩阵的差值得到 `relative_position` 矩阵，表示词元 $i$ 和 $j$ 之间的相对位置。通过 `relative_position_bucket` 将值转换为非负，然后从 `relative_attention_bias` 张量中查找对应的位置编码向量。\n    *   **优点**：自然处理可变长度序列，适用于翻译等任务。\n    *   **相关方法**：**线性偏置注意力 (Attention with Linear Bias, ALiBi)** 是一种相关方法，它不是修改输入序列，而是将一个偏置矩阵添加到注意力分数中，该偏置矩阵基于词元距离按比例缩放。缩放因子为 $m_h=1/2^{8h/H}$。\n\n## 总结\n\n本文详细介绍了Transformer模型中位置编码的重要性及其不同类型：\n\n*   位置编码对于Transformer模型至关重要，因为它们以并行方式处理词元，需要额外的位置信息来理解序列顺序。\n*   不同类型的位置编码各有优缺点：\n    *   **正弦编码**：确定性，可外推到更长序列。\n    *   **学习编码**：简单，但无法外推。\n    *   **RoPE**：在长序列上表现更好，保留向量范数。\n    *   **相对位置编码**：关注词元间距离，适用于可变长度序列。\n\n![Building Transformer Models with Attention](https://machinelearningmastery.com/wp-content/uploads/2022/11/BTMA-400.png)\n*构建带有注意力的Transformer模型*",
      "shortSummary": "Transformer模型并行处理词元，缺乏序列顺序信息，因此需要位置编码。位置编码通过向词元嵌入添加位置向量来解决此问题，帮助模型理解上下文和词序。主要类型包括：原始Transformer使用的确定性正弦编码、BERT和GPT中学习到的编码、Llama模型中通过旋转矩阵实现的RoPE，以及T5等模型中基于词元间相对距离的相对位置编码。每种编码都有其独特优势，如外推能力或适应数据特性。",
      "translated_title": "Transformer模型中的位置编码",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/svetlana-gumerova-6fAgnT3Dhl4-unsplash-scaled.jpg",
          "alt": "",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2022/11/BTMA-400.png",
          "alt": "Building Transformer Models with Attention",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "This post is divided into five parts; they are: • Understanding Positional Encodings • Sinusoidal Positional Encodings • Learned Positional Encodings • Rotary Positional Encodings (RoPE) • Relative Positional Encodings Consider these two sentences: \"The fox jumps over the dog\" and \"The dog jumps over the fox\"."
    },
    {
      "title": "使用Scikit-Learn管道、Pandas的ColumnTransformer和NumPy数组进行高级特征工程 (原标题: Advanced Feature Engineering Using Scikit-Learn Pipelines with Pandas’ ColumnTransformer and NumPy Arrays)",
      "link": "https://machinelearningmastery.com/advanced-feature-engineering-using-scikit-learn-pipelines-with-pandas-columntransformer-and-numpy-arrays/",
      "pubDate": "Fri, 13 Jun 2025 12:00:25 +0000",
      "isoDate": "2025-06-13T12:00:25.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "## 使用Scikit-Learn管道、Pandas的ColumnTransformer和NumPy数组进行高级特征工程\n\n![高级特征工程](https://machinelearningmastery.com/wp-content/uploads/2025/05/mlm-palomares-ollama-quantization-feature.png)\n\n本文介绍了如何利用Pandas、NumPy和Scikit-learn这三个强大的Python库，构建健壮、可扩展且模块化的高级特征工程工作流。文章通过一个实际示例，详细阐述了如何将这些工具结合起来，实现数据预处理和机器学习模型的无缝集成。\n\n### 核心组件\n\n*   **Scikit-learn 管道 (Pipelines)**：一种便捷的机制，用于定义一系列操作（通常是数据转换，也可包含机器学习模型），并将这些操作封装为一个单一的对象。这有助于简化工作流并提高代码的可读性。\n*   **Pandas ColumnTransformer**：一个专门的类，用于对数据集中特定列应用自定义的转换。它允许对不同类型的特征（如数值型和类别型）应用不同的预处理方法。\n*   **NumPy 数组 (Arrays)**：在目标问题中扮演关键角色，能够高效处理大量数据。Scikit-learn模型内部最终需要NumPy数组作为输入格式，而非Pandas DataFrame对象，因此NumPy数组是数据流转的关键。\n\n### 实践示例：构建特征工程管道\n\n文章通过一个自制数据集的例子，演示了如何构建一个完整的特征工程管道。\n\n#### 1. 导入必要的库\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\n```\n\n#### 2. 创建示例数据集\n\n为了演示，文章创建了一个包含20个实例的模拟公民数据DataFrame，包括数值型特征（`age`、`income`）和类别型特征（`gender`、`city`），以及一个二元目标标签（`label`）。\n\n```python\ndf = pd.DataFrame({\n    'age': np.random.randint(18, 70, size=20),\n    'income': np.random.randint(30000, 120000, size=20),\n    'gender': np.random.choice(['male', 'female'], size=20),\n    'city': np.random.choice(['NY', 'SF', 'LA'], size=20),\n    'label': np.random.choice([0, 1], size=20)\n})\n```\n\n#### 3. 分离特征与标签并定义特征列表\n\n将数据集分为特征 `X` 和标签 `y`，并分别定义数值型和类别型特征的名称列表，这对于后续 `ColumnTransformer` 的使用至关重要。\n\n```python\nX = df.drop('label', axis=1)\ny = df['label']\nnumeric_features = ['age', 'income']\ncategorical_features = ['gender', 'city']\n```\n\n![特征工程前的数据集特征](https://machinelearningmastery.com/wp-content/uploads/2025/05/Captura-de-pantalla-2025-05-29-a-las-9.59.41.png)\n\n#### 4. 定义预处理步骤\n\n文章定义了两个并行的管道，分别用于数值型和类别型特征的预处理，并通过 `ColumnTransformer` 将它们结合起来。\n\n*   **数值型转换器**：使用 `StandardScaler` 进行标准化。\n*   **类别型转换器**：使用 `OneHotEncoder` 进行独热编码，并设置 `handle_unknown='ignore'` 以处理未知类别。\n*   **预处理器 (preprocessor)**：一个 `ColumnTransformer` 对象，将上述两个管道并行应用于各自对应的特征列表。\n\n```python\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())\n])\ncategorical_transformer = Pipeline(steps=[\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ]\n)\n```\n\n#### 5. 应用预处理并查看结果\n\n通过 `preprocessor.fit_transform(X)` 应用定义的预处理管道，并将结果转换为DataFrame以便于可视化。\n\n```python\nX_preprocessed = preprocessor.fit_transform(X)\npreprocessed_feature_names = preprocessor.get_feature_names_out()\nX_preprocessed_df = pd.DataFrame(X_preprocessed, columns=preprocessed_feature_names)\nprint(\"Preprocessed Data:\")\nX_preprocessed_df.head()\n```\n\n![特征工程后的数据集特征](https://machinelearningmastery.com/wp-content/uploads/2025/05/Captura-de-pantalla-2025-05-29-a-las-9.59.48.png)\n\n#### 6. 将特征工程与机器学习模型结合\n\n最后，文章展示了如何将预处理管道 (`preprocessor`) 与一个机器学习模型（例如 `RandomForestClassifier`）组合成一个更高级别的端到端管道。这个管道包含两个顺序步骤：特征工程和模型训练。\n\n```python\nfeng_pipeline = Pipeline(steps=[\n    ('preprocessing', preprocessor),\n    ('classifier', RandomForestClassifier())\n])\n\nfeng_pipeline.fit(X, y)\npredictions = feng_pipeline.predict(X)\nprint(predictions)\n```\n\n这种集成方式使得数据从原始形式到模型训练的整个过程变得流畅，尤其得益于NumPy数组在预处理后作为数据结构传递给模型，无需额外的数据结构修改。\n\n### 总结\n\n本文详细阐述了如何利用Scikit-learn的 `Pipeline` 和 Pandas 的 `ColumnTransformer` 对象，结合 NumPy 数组，对包含不同类型特征的数据集执行高级且定制化的特征工程过程。这种方法不仅提高了工作流的效率和可维护性，也确保了数据在整个机器学习流程中的顺畅传递。",
      "shortSummary": "本文演示了如何使用Scikit-learn的Pipeline、Pandas的ColumnTransformer和NumPy数组进行高级特征工程。通过构建一个包含数据标准化、独热编码等预处理步骤的并行管道，并将其与机器学习模型（如随机森林）集成，文章展示了如何创建健壮、可扩展且模块化的端到端数据处理和建模工作流。NumPy数组在此过程中确保了数据在不同组件间的无缝传递。",
      "translated_title": "使用Scikit-Learn管道、Pandas的ColumnTransformer和NumPy数组进行高级特征工程",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/05/mlm-palomares-ollama-quantization-feature.png",
          "alt": "Advanced Feature Engineering Using Scikit-Learn Pipelines with Pandas’ ColumnTransformer and NumPy Arrays",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/05/Captura-de-pantalla-2025-05-29-a-las-9.59.41.png",
          "alt": "Dataset features before feature engineering",
          "title": "",
          "position": 2
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/05/Captura-de-pantalla-2025-05-29-a-las-9.59.48.png",
          "alt": "Dataset features after feature engineering",
          "title": "",
          "position": 3
        }
      ],
      "contentSource": "完整文章",
      "content": "Pandas , NumPy , and Scikit-learn ."
    },
    {
      "title": "使用 Pandas 和 Scikit-learn 处理不平衡数据集 (原标题: Navigating Imbalanced Datasets with Pandas and Scikit-learn)",
      "link": "https://machinelearningmastery.com/navigating-imbalanced-datasets-with-pandas-and-scikit-learn/",
      "pubDate": "Thu, 12 Jun 2025 12:00:56 +0000",
      "isoDate": "2025-06-12T12:00:56.000Z",
      "creator": "Iván Palomares Carrascosa",
      "summary": "# 使用 Pandas 和 Scikit-learn 处理不平衡数据集\n\n## 引言\n不平衡数据集在现实世界中并不少见，例如银行金融领域的欺诈检测（欺诈交易远少于合法交易）和医疗诊断（罕见疾病远少于常见健康状况）。这类数据集的主要问题在于，机器学习模型容易偏向多数类，导致模型性能下降，甚至可能退化为“虚拟分类器”。本文将介绍使用 Python 的 Pandas 和 Scikit-learn 库处理不平衡数据集的几种策略。\n\n![Navigating Imbalanced Datasets with Pandas and Scikit-learn](https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-ipc-navigating-imbalanced-datasets.png)\n*图：使用 Pandas 和 Scikit-learn 处理不平衡数据集*\n\n## 实践指南：银行营销数据集\n为了演示处理不平衡数据的实际方法，本文使用了公开可用的银行营销数据集。该数据集包含银行客户信息，并标记了客户是否在接到银行营销电话后订阅了定期存款（“是”或“否”）。\n\n**数据集不平衡性：**\n该数据集之所以不平衡，是因为只有约 11% 的客户订阅了定期存款，而约 89% 的客户拒绝了。具体来说：\n*   **未订阅（“no”）:** 39922 名客户 (88.3%)\n*   **已订阅（“yes”）:** 5289 名客户 (11.7%)\n这表明“yes”类是显著的少数类。\n\n## 处理不平衡数据集的策略\n\n### 策略 1：逆频率依赖加权 (Inverse Frequency-Dependent Weighting)\n这是 Scikit-learn 提供的一种策略，通过调整分类模型训练时的实例权重来处理不平衡数据。\n\n*   **原理：** 在 Scikit-learn 的某些分类器中（例如 `RandomForestClassifier`），设置 `class_weight='balanced'` 参数。这会使实例权重与类频率成反比，从而给予少数类更大的权重，以补偿类不平衡。\n*   **优点：** 模型在训练时会更加关注少数类，减少对多数类的偏向。\n*   **示例：** 使用 `RandomForestClassifier` 并设置 `class_weight='balanced'`，同时对分类特征进行独热编码（使用 Pandas 的 `pd.get_dummies()`）。\n\n### 策略 2：欠采样 (Undersampling)\n这是一种在模型训练前进行的数据预处理策略，主要通过 Pandas 实现。\n\n*   **原理：** 减少多数类实例的数量，使其与少数类实例的数量相匹配。\n*   **优点：** 减少模型对多数类的偏向。\n*   **缺点：**\n    *   可能导致信息丢失，特别是当多数类被大量欠采样时。\n    *   可能增加模型方差，有时甚至导致欠拟合。\n*   **适用场景：** 当数据集足够大，即使欠采样后仍能保留足够代表性和多样性的实例时。\n*   **示例：** 将多数类（“no”）的实例数量随机减少到与少数类（“yes”）相同的数量，然后将欠采样后的多数类与少数类合并，形成一个平衡的数据集。原始数据集约 45K 实例，欠采样后平衡数据集约 10.5K 实例。\n\n### 策略 3：过采样 (Oversampling)\n这也是一种在模型训练前进行的数据预处理策略，主要通过 Pandas 实现。\n\n*   **原理：** 通过随机复制少数类实例（带替换采样）来增加其数量，使其与多数类数量相匹配。\n*   **优点：** 有助于缓解模型对多数类的偏向。\n*   **缺点：**\n    *   如果少数类本身不具代表性，或者重复实例可能引入噪声，则不适用。\n    *   可能导致过拟合，因为模型可能会过度学习重复的少数类实例。\n*   **适用场景：** 当少数类规模较小但具有代表性，且添加重复实例不太可能引入噪声或导致过拟合时。\n*   **示例：** 将少数类（“yes”）的实例数量随机复制增加到与多数类（“no”）相同的数量，然后将多数类与过采样后的少数类合并，形成一个平衡的数据集。\n\n## 总结\n本文探讨了数据集中的类不平衡问题，并介绍了使用 Pandas 和 Scikit-learn 库处理该问题的三种常用策略：训练平衡分类模型（逆频率依赖加权）、欠采样和过采样。值得注意的是，还有其他更高级的策略，如 Scikit-learn 的重采样工具和 SMOTE（合成少数类过采样技术）等。",
      "shortSummary": "不平衡数据集（如欺诈检测）会导致机器学习模型偏向多数类，影响性能。本文介绍了使用 Pandas 和 Scikit-learn 处理此类问题的三种策略：一是通过 Scikit-learn 的 `class_weight='balanced'` 参数进行逆频率依赖加权，使模型更关注少数类；二是通过 Pandas 进行欠采样，减少多数类实例；三是通过 Pandas 进行过采样，复制少数类实例。这些方法旨在平衡数据分布，提升模型在不平衡数据集上的表现。",
      "translated_title": "使用 Pandas 和 Scikit-learn 处理不平衡数据集",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/mlm-ipc-navigating-imbalanced-datasets.png",
          "alt": "Navigating Imbalanced Datasets with Pandas and Scikit-learn",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "Imbalanced datasets, where a majority of the data samples belong to one class and the remaining minority belong to others, are not that rare."
    },
    {
      "title": "使用FastAPI和Docker部署机器学习模型的逐步指南 (原标题: Step-by-Step Guide to Deploying Machine Learning Models with FastAPI and Docker)",
      "link": "https://machinelearningmastery.com/step-by-step-guide-to-deploying-machine-learning-models-with-fastapi-and-docker/",
      "pubDate": "Wed, 11 Jun 2025 16:17:27 +0000",
      "isoDate": "2025-06-11T16:17:27.000Z",
      "creator": "Bala Priya C",
      "summary": "## 使用FastAPI和Docker部署机器学习模型的逐步指南\n\n![使用FastAPI和Docker部署机器学习模型的逐步指南](https://machinelearningmastery.com/wp-content/uploads/2025/06/step-by-step-deploying-ml-models-docker.png)\n\n本文提供了一个将机器学习模型部署到生产环境的详细指南，通过构建一个糖尿病进展预测器并将其封装为可部署的API。该教程涵盖了从模型训练到使用FastAPI构建REST API，再到使用Docker进行容器化，并最终发布到Docker Hub的完整流程。\n\n### 1. 项目概述与目标\n\n*   **目标**：将训练好的机器学习模型部署为可供实际用户使用的生产级API。\n*   **项目**：基于scikit-learn的样本数据集构建一个糖尿病进展预测器。\n*   **最终成果**：\n    *   一个训练好的随机森林模型，用于预测糖尿病进展分数。\n    *   一个使用FastAPI构建的REST API，接受患者数据并返回预测结果。\n    *   一个完全容器化的应用程序，可随时部署到云端。\n\n### 2. 开发环境设置\n\n*   **先决条件**：Python 3.11+ (或 3.9+), Docker已安装并运行，对Python和API有基本了解。\n*   **项目结构**：\n    *   `diabetes-predictor/` (根目录)\n        *   `app/` (FastAPI应用)\n            *   `__init__.py`\n            *   `main.py`\n        *   `models/` (存放训练好的模型)\n            *   `diabetes_model.pkl`\n        *   `train_model.py` (模型训练脚本)\n        *   `requirements.txt` (Python依赖)\n        *   `Dockerfile` (容器配置)\n*   **安装依赖**：\n    1.  创建并激活虚拟环境：`python -m venv diabetes-env`，然后激活。\n    2.  安装所需库：`pip install scikit-learn pandas fastapi uvicorn`。\n\n### 3. 构建机器学习模型 (`train_model.py`)\n\n*   **模型选择**：使用`RandomForestRegressor`，因其鲁棒性、对不同特征尺度的良好处理能力以及提供特征重要性洞察。\n*   **数据集**：加载scikit-learn的糖尿病数据集，包含442条患者记录和10个生理特征，目标是衡量一年后的疾病进展。\n*   **数据准备**：将数据集按80/20比例划分为训练集和测试集 (`random_state=42`确保结果可复现)。\n*   **模型训练**：\n    *   初始化`RandomForestRegressor`，设置`n_estimators=100`, `random_state=42`, `max_depth=10`（防止过拟合）。\n    *   使用训练数据拟合模型。\n*   **模型评估**：\n    *   在测试集上进行预测。\n    *   计算均方误差 (MSE) 和 R² 分数。R² 分数高于0.4被认为是该数据集的良好表现。\n*   **模型保存**：将训练好的模型保存为`models/diabetes_model.pkl`。\n*   **运行脚本**：`python3 train_model.py`。\n\n### 4. 创建FastAPI应用程序 (`app/main.py`)\n\n*   **API结构**：\n    *   创建`app`目录和`__init__.py`。\n    *   导入`FastAPI`, `BaseModel` (Pydantic), `pickle`, `numpy`, `os`。\n*   **输入数据结构**：\n    *   使用Pydantic的`BaseModel`定义`PatientData`类，包含10个浮点型生理特征（如age, sex, bmi等）。\n    *   提供`schema_extra`示例，帮助API用户理解输入格式。\n*   **FastAPI初始化与模型加载**：\n    *   初始化`FastAPI`应用，设置标题、描述和版本。\n    *   加载之前保存的`diabetes_model.pkl`。\n*   **预测端点 (`/predict`)**：\n    *   定义POST请求端点，接受`PatientData`作为输入。\n    *   将输入转换为NumPy数组。\n    *   使用加载的模型进行预测。\n    *   返回包含`predicted_progression_score`（四舍五入到两位小数）和`interpretation`（通过`get_interpretation`函数提供）的JSON响应。\n*   **解释函数 (`get_interpretation`)**：根据预测分数提供人类可读的解释（低于平均、平均、高于平均进展）。\n*   **健康检查端点 (`/`)**：定义GET请求端点，返回API状态和模型信息。\n\n### 5. 本地测试API\n\n*   **运行命令**：在项目根目录运行`uvicorn app.main:app --reload --port 8000`。\n*   **访问**：在浏览器中打开`http://localhost:8000/`。\n*   **测试预测**：使用`curl`命令发送POST请求及示例数据进行测试，验证API功能。\n\n### 6. 使用Docker进行容器化\n\n*   **创建`requirements.txt`**：列出所有Python依赖及其精确版本，确保环境一致性。\n*   **创建`Dockerfile`**：\n    *   基于`python:3.11-slim`镜像，保持容器小巧。\n    *   设置工作目录为`/app`。\n    *   安装系统依赖（如果需要）。\n    *   复制`requirements.txt`并安装Python依赖（使用`--no-cache-dir`减少镜像大小）。\n    *   复制`app/`和`models/`目录到容器中。\n    *   暴露端口8000。\n    *   定义容器启动命令：`uvicorn app.main:app --host 0.0.0.0 --port 8000`。\n*   **构建Docker镜像**：`docker build -t diabetes-predictor .`。\n*   **运行Docker容器**：`docker run -d -p 8000:8000 diabetes-predictor`。\n\n### 7. 发布到Docker Hub\n\n*   **目的**：将容器化应用共享，便于云平台部署。\n*   **设置Docker Hub**：注册Docker Hub账户。\n*   **登录**：在终端运行`docker login`并输入凭据。\n*   **标记镜像**：\n    *   `docker tag diabetes-predictor your-username/diabetes-predictor:v1.0`\n    *   `docker tag diabetes-predictor your-username/diabetes-predictor:latest`\n*   **推送镜像**：\n    *   `docker push your-username/diabetes-predictor:v1.0`\n    *   `docker push your-username/diabetes-predictor:latest`\n*   **验证**：停止本地容器，然后从Docker Hub拉取并运行镜像，再次测试API以确保其正常工作。\n\n### 8. 总结与展望\n\n本文成功构建了一个完整的机器学习部署流程：训练了随机森林模型，使用FastAPI创建了REST API，并用Docker进行了容器化。该模型现在已准备好部署到AWS ECS、Fargate、Google Cloud或Azure等云平台。未来的改进方向包括添加认证和速率限制、模型监控和日志记录以及批量预测端点。",
      "shortSummary": "本文提供了一个使用FastAPI和Docker部署机器学习模型的逐步指南。教程从训练一个预测糖尿病进展的随机森林模型开始，接着使用FastAPI构建一个REST API，实现数据输入和预测输出。随后，详细介绍了如何使用Dockerfile将整个应用容器化，并将其发布到Docker Hub，以便于云端部署。最终，用户将获得一个功能完整的、可生产部署的机器学习API。",
      "translated_title": "使用FastAPI和Docker部署机器学习模型的逐步指南",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/step-by-step-deploying-ml-models-docker.png",
          "alt": "Step-by-Step Guide to Deploying Machine Learning Models with FastAPI and Docker",
          "title": "",
          "position": 1
        }
      ],
      "contentSource": "完整文章",
      "content": "You've trained your machine learning model, and it's performing great on test data."
    },
    {
      "title": "从零开始实现向量搜索：一步步教程 (原标题: Implementing Vector Search from Scratch: A Step-by-Step Tutorial)",
      "link": "https://machinelearningmastery.com/implementing-vector-search-from-scratch-a-step-by-step-tutorial/",
      "pubDate": "Tue, 10 Jun 2025 14:49:47 +0000",
      "isoDate": "2025-06-10T14:49:47.000Z",
      "creator": "Kanwal Mehreen",
      "summary": "# 从零开始实现向量搜索：一步步教程\n\n## 搜索的演进与向量搜索的兴起\n\n文章指出，搜索是计算领域最基本的问题之一。传统的关键词搜索方法（如基于词频和稀有度）存在局限性，它们过于字面化，无法理解用户查询的上下文和真实意图。例如，搜索“汽车修理”可能无法匹配到“车辆维护”的文档。为了弥补这一语义鸿沟，向量搜索应运而生。\n\n向量搜索通过将查询和文档转换为数值向量（高维数组，捕捉文本的语义本质）来匹配含义，而非精确的关键词。它在向量空间中寻找与查询向量距离最近的文档向量，从而返回上下文相关的结果。本教程旨在从头开始构建一个向量搜索系统，帮助读者深入理解其工作原理。\n\n## 向量搜索的核心工作原理\n\n向量搜索主要包含三个核心步骤：\n\n1.  **向量表示（Vector Representation）**：\n    *   将数据（如文本、图像）转换为数值向量。\n    *   常用技术包括词嵌入（Word Embeddings）或神经网络。\n    *   每个向量在高维空间中代表数据的语义。\n2.  **相似度计算（Similarity Calculation）**：\n    *   衡量查询向量与数据集中其他向量的“接近”程度。\n    *   常用度量包括余弦相似度（Cosine Similarity）或欧几里得距离（Euclidean Distance）。\n    *   向量越接近，表示相似度越高。\n3.  **检索（Retrieval）**：\n    *   根据相似度得分返回最相似的 Top-k 项。\n    *   例如，查询“机器学习”会找到与“人工智能”或“深度学习”相关的文档。\n\n## 从零开始构建向量搜索系统（Python 实现）\n\n文章通过一个 Python 示例，逐步演示了如何构建一个简单的向量搜索系统：\n\n### 步骤 1：环境设置\n\n*   使用 NumPy 进行向量操作，Matplotlib 进行可视化。\n*   为保持“从零开始”的理念，避免使用 FAISS 或 spaCy 等外部高级库。\n*   词嵌入将通过一个预定义的字典模拟，而非使用 Word2Vec、GloVe 或 BERT 等预训练模型。\n*   导入必要的库：`numpy`、`matplotlib.pyplot`、`collections.defaultdict`、`re`。\n\n### 步骤 2：创建示例数据集和词嵌入\n\n*   **示例数据集**：包含关于技术的几句话，例如：“Machine learning is powerful”、“Artificial intelligence advances rapidly”等。\n*   **简化 2D 词嵌入**：创建一个字典，将每个词映射到一个 2D 向量。这些向量是任意的，但设计上使得相关词（如“machine”和“neural”）在向量空间中彼此接近，便于可视化。\n\n### 步骤 3：将句子转换为向量\n\n*   定义 `tokenize` 函数：将文本转换为小写并分词。\n*   定义 `sentence_to_vector` 函数：通过平均句子中所有词的词向量来将句子转换为单个向量。如果词不在嵌入字典中，则使用零向量。\n*   将所有文档转换为对应的向量表示 (`doc_vectors`)。\n\n### 步骤 4：实现余弦相似度\n\n*   余弦相似度是衡量向量间角度的常用指标，非常适合比较文本嵌入的语义相似性。\n*   计算公式为两个向量的点积除以它们各自范数的乘积。\n*   函数会处理零向量以避免除以零的错误。\n\n### 步骤 5：构建向量搜索函数\n\n*   实现核心的 `vector_search` 函数，该函数接受查询、文档、嵌入和 `top_k` 参数。\n*   将查询转换为向量。\n*   计算查询向量与每个文档向量之间的余弦相似度。\n*   使用 `np.argsort` 对相似度进行排序，并返回 `top_k` 个最相似的文档及其得分。\n*   **示例查询**：“Machine learning technology”的搜索结果展示了语义相关性，即使文档中不包含确切的短语。\n\n```\nQuery: Machine learning technology\nTop results:\nScore: 1.000, Document: Machine learning is powerful\nScore: 0.999, Document: Deep learning transforms technology\nScore: 0.997, Document: Artificial intelligence advances rapidly\n```\n\n### 步骤 6：向量可视化\n\n*   通过 `plot_vectors` 函数将文档向量（蓝色点）和查询向量（红色星号）在 2D 空间中可视化。\n*   可视化结果清晰地展示了相似项如何在向量空间中聚类，以及查询向量如何靠近语义相关的文档向量。\n\n![从零开始实现向量搜索](https://machinelearningmastery.com/wp-content/uploads/2025/06/Vector-Search-from-Scratch.png)\n*图：从零开始实现向量搜索* \n\n![向量搜索：文档和查询向量的可视化](https://machinelearningmastery.com/wp-content/uploads/2025/06/plot.png)\n*图：向量搜索：文档和查询向量的可视化*\n\n## 向量搜索对 RAG 的重要性\n\n文章强调，在检索增强生成（RAG）系统中，向量搜索是检索步骤的基石。它通过将文档和查询转换为向量，即使对于复杂的查询也能获取上下文相关的准确信息。本教程的简单实现模拟了这一过程：查询向量检索语义上接近的文档，这些文档随后可供语言模型用于生成响应。在实际应用中，虽然需要更高维的嵌入和优化的搜索算法（如 HNSW 或 IVF），但核心思想保持不变。\n\n## 结论\n\n本教程成功从零开始实现了向量搜索。读者可以基于此扩展，使用真实的词嵌入（如来自 Hugging Face Transformers）或通过近似最近邻（ANN）技术优化搜索，从而加深对向量搜索的理解。",
      "shortSummary": "向量搜索通过将查询和文档转换为数值向量并匹配其语义含义，克服了传统关键词搜索的局限性。文章详细介绍了向量搜索的核心原理：向量表示、相似度计算和检索。它通过一个Python教程，从零开始实现了向量搜索系统，包括环境设置、数据与嵌入创建、句子向量化、余弦相似度计算、搜索功能构建及向量可视化。该技术是RAG（检索增强生成）系统的关键组成部分，用于高效检索上下文相关信息。",
      "translated_title": "从零开始实现向量搜索：一步步教程",
      "images": [
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/Vector-Search-from-Scratch.png",
          "alt": "Implementing Vector Search from Scratch",
          "title": "",
          "position": 1
        },
        {
          "url": "https://machinelearningmastery.com/wp-content/uploads/2025/06/plot.png",
          "alt": "Output",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "There’s no doubt that search is one of the most fundamental problems in computing."
    }
  ],
  "lastUpdated": "2025-06-27T09:28:51.867Z"
}