{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "ScaleCUA：利用跨平台数据扩展开源计算机使用代理 (原标题: ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data)",
      "link": "https://arxiv.org/abs/2509.15221",
      "pubDate": "Thu, 18 Sep 2025 13:59:22 GMT",
      "isoDate": "2025-09-18T13:59:22.000Z",
      "creator": "Zhaoyang Liu, JingJing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, Shenglong Ye, Qingyun Li, Zeyue Tian, Gen Luo, Xiangyu Yue, Biqing Qi, Kai Chen, Bowen Zhou, Yu Qiao, Qifeng Chen, Wenhai Wang",
      "summary": "## ScaleCUA：利用跨平台数据扩展开源计算机使用代理\n\n### 引言\n\n视觉-语言模型（VLM）已使计算机使用代理（CUA）能够自主操作图形用户界面（GUI），展现出巨大潜力。然而，目前该领域的进展受限于缺乏大规模、开源的计算机使用数据和基础模型。\n\n### ScaleCUA的提出\n\n本文介绍了ScaleCUA，这是朝着扩展开源CUA迈出的重要一步。它旨在通过数据驱动的扩展方法，提升通用计算机使用代理的能力。\n\n### ScaleCUA数据集与构建\n\nScaleCUA提供了一个大规模数据集，其特点包括：\n\n*   **广泛的覆盖范围**：涵盖了6种不同的操作系统和3个主要任务领域。\n*   **构建方法**：该数据集通过一个创新的闭环管道构建，该管道有效结合了自动化代理和人类专家，确保了数据的质量、多样性和实用性。\n\n### 跨平台操作能力\n\n通过对这些大规模数据的训练，ScaleCUA能够实现跨平台的无缝操作，极大地增强了其通用性和适用性。\n\n### 性能表现\n\nScaleCUA在多项基准测试中取得了显著提升，并设置了新的最先进（SOTA）结果，具体表现如下：\n\n*   **相对于基线模型的提升：**\n    *   在WebArena-Lite-v2上，性能提升了26.6个百分点。\n    *   在ScreenSpot-Pro上，性能提升了10.7个百分点。\n*   **新的最先进结果：**\n    *   MMBench-GUI L1-Hard：达到94.4%的准确率。\n    *   OSWorld-G：达到60.6%的准确率。\n    *   WebArena-Lite-v2：达到47.4%的准确率。\n\n### 结论\n\n这些研究结果有力地强调了数据驱动的扩展策略对于开发和提升通用计算机使用代理的强大作用和重要性。\n\n### 未来展望\n\n作者计划发布相关的训练数据、模型和代码，以促进学术界和工业界在这一领域的进一步研究和发展。",
      "shortSummary": "ScaleCUA引入了一个大规模跨平台数据集，涵盖6种操作系统和3个任务领域，旨在扩展开源计算机使用代理（CUA）。通过结合自动化代理和人类专家的闭环管道构建，ScaleCUA训练后能无缝跨平台操作。它在WebArena-Lite-v2和ScreenSpot-Pro等基准测试中显著超越现有模型，并在MMBench-GUI L1-Hard、OSWorld-G和WebArena-Lite-v2上取得了新的最先进（SOTA）结果，证明了数据驱动扩展对通用CUA的强大作用。",
      "translated_title": "ScaleCUA：利用跨平台数据扩展开源计算机使用代理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research: https://github.com/OpenGVLab/ScaleCUA."
    },
    {
      "title": "RynnVLA-001：利用人类演示改进机器人操作 (原标题: RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation)",
      "link": "https://arxiv.org/abs/2509.15212",
      "pubDate": "Thu, 18 Sep 2025 13:58:02 GMT",
      "isoDate": "2025-09-18T13:58:02.000Z",
      "creator": "Yuming Jiang, Siteng Huang, Shengke Xue, Yaxi Zhao, Jun Cen, Sicong Leng, Kehan Li, Jiayan Guo, Kexiang Wang, Mingxiu Chen, Fan Wang, Deli Zhao, Xin Li",
      "summary": "### RynnVLA-001：利用人类演示改进机器人操作\n\n本文介绍了RynnVLA-001，一个基于大规模人类演示视频生成预训练的视觉-语言-动作（VLA）模型。该模型旨在通过有效利用人类演示数据，显著提升机器人的操作能力。\n\n#### 核心贡献\n\n*   **RynnVLA-001模型**：一个创新的VLA模型，其核心在于从人类演示中进行大规模视频生成预训练。\n*   **新颖的两阶段预训练方法**：\n    1.  **以自我为中心的视频生成预训练 (Ego-Centric Video Generative Pretraining)**：\n        *   在此阶段，模型被训练为一个图像到视频（Image-to-Video）模型。\n        *   训练数据包含1200万个以自我为中心的操作视频。\n        *   模型的目标是根据初始帧和语言指令预测未来的帧。\n    2.  **以人为中心的轨迹感知建模 (Human-Centric Trajectory-Aware Modeling)**：\n        *   此阶段在前一阶段的基础上进行扩展。\n        *   模型不仅预测未来的视觉帧，还联合预测未来的关键点轨迹。\n        *   这种方法有效地将视觉帧预测与动作预测连接起来，为机器人操作提供了更直接的指导。\n*   **ActionVAE**：\n    *   为了增强动作表示，研究人员提出了ActionVAE（变分自编码器）。\n    *   ActionVAE将一系列动作压缩成紧凑的潜在嵌入（latent embeddings）。\n    *   这显著降低了VLA模型输出空间的复杂性，使得动作学习和生成更加高效。\n\n#### 性能表现\n\n*   在相同的下游机器人数据集上进行微调时，RynnVLA-001取得了优于现有最先进基线的性能。\n*   这一结果证明了所提出的预训练策略为VLA模型提供了一个更有效的初始化方法，从而在机器人操作任务中实现了卓越的表现。",
      "shortSummary": "RynnVLA-001是一个利用人类演示改进机器人操作的视觉-语言-动作（VLA）模型。它采用新颖的两阶段预训练方法：首先进行以自我为中心的视频生成预训练，然后进行以人为中心的轨迹感知建模，将视觉预测与动作预测相结合。此外，ActionVAE通过压缩动作序列增强了动作表示。在下游机器人数据集上微调后，RynnVLA-001的性能超越了现有最先进模型，证明了其预训练策略的有效性。",
      "translated_title": "RynnVLA-001：利用人类演示改进机器人操作",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper presents RynnVLA-001, a vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose a novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric manipulation videos to predict future frames conditioned on an initial frame and a language instruction. The second stage, Human-Centric Trajectory-Aware Modeling, extends this by jointly predicting future keypoint trajectories, thereby effectively bridging visual frame prediction with action prediction. Furthermore, to enhance action representation, we propose ActionVAE, a variational autoencoder that compresses sequences of actions into compact latent embeddings, reducing the complexity of the VLA output space. When finetuned on the same downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, demonstrating that the proposed pretraining strategy provides a more effective initialization for VLA models."
    },
    {
      "title": "FlowRL：匹配LLM推理的奖励分布 (原标题: FlowRL: Matching Reward Distributions for LLM Reasoning)",
      "link": "https://arxiv.org/abs/2509.15207",
      "pubDate": "Thu, 18 Sep 2025 13:56:36 GMT",
      "isoDate": "2025-09-18T13:56:36.000Z",
      "creator": "Xuekai Zhu, Daixuan Cheng, Dinghuai Zhang, Hengli Li, Kaiyan Zhang, Che Jiang, Youbang Sun, Ermo Hua, Yuxin Zuo, Xingtai Lv, Qizheng Zhang, Lin Chen, Fanghao Shao, Bo Xue, Yunchong Song, Zhenjie Yang, Ganqu Cui, Ning Ding, Jianfeng Gao, Xiaodong Liu, Bowen Zhou, Hongyuan Mei, Zhouhan Lin",
      "summary": "## FlowRL：匹配LLM推理的奖励分布\n\n### 核心问题\n\n当前大型语言模型（LLM）强化学习（RL）中，主流的奖励最大化方法（如PPO和GRPO）存在以下局限性：\n\n*   **过度优化**：倾向于过度优化主导的奖励信号。\n*   **忽视多样性**：忽略了不那么频繁但同样有效的推理路径。\n*   **多样性降低**：导致模型生成的推理路径多样性不足。\n\n### FlowRL方法\n\nFlowRL提出了一种新的方法，旨在通过“流平衡”（flow balancing）来匹配完整的奖励分布，而非仅仅最大化奖励。其核心思想和机制包括：\n\n*   **转换奖励**：将标量奖励转换为一个标准化的目标分布，该分布通过一个可学习的配分函数（partition function）生成。\n*   **优化目标**：最小化策略（policy）与目标分布之间的反向KL散度（reverse KL divergence）。\n*   **流平衡优化**：将这一思想实现为一种流平衡优化方法，旨在促进更具多样性的探索和更具泛化能力的推理轨迹。\n\n### 实验结果\n\n研究团队在数学和代码推理任务上进行了实验，FlowRL展现出显著的性能提升：\n\n*   **数学基准**：在数学基准测试中，FlowRL相对于GRPO平均提升了10.0%，相对于PPO平均提升了5.1%。\n*   **代码推理**：在代码推理任务上，FlowRL也表现出持续的优异性能。\n\n### 结论与意义\n\n这些实验结果强调了奖励分布匹配在LLM强化学习中作为实现高效探索和多样化推理的关键一步。FlowRL为LLM的推理能力提供了一个新的优化方向，有助于模型生成更全面、更鲁棒的解决方案。",
      "shortSummary": "FlowRL提出了一种在大型语言模型（LLM）强化学习中匹配完整奖励分布的新方法，以解决传统奖励最大化方法（如PPO和GRPO）导致的过度优化和多样性不足问题。FlowRL通过流平衡优化，将标量奖励转换为目标分布并最小化KL散度，从而促进多样化探索和泛化推理。实验表明，FlowRL在数学和代码推理任务上均显著优于现有方法，证明了奖励分布匹配对于LLM高效探索和多样化推理的重要性。",
      "translated_title": "FlowRL：匹配LLM推理的奖励分布",
      "images": [],
      "contentSource": "完整文章",
      "content": "We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of 10.0% over GRPO and 5.1% over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning."
    },
    {
      "title": "无标签下语言模型的演进：多数决定选择，新颖促进多样性 (原标题: Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation)",
      "link": "https://arxiv.org/abs/2509.15194",
      "pubDate": "Thu, 18 Sep 2025 13:50:04 GMT",
      "isoDate": "2025-09-18T13:50:04.000Z",
      "creator": "Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kishan Panaganti, Linfeng Song, Dian Yu, Xiangliang Zhang, Haitao Mi, Dong Yu",
      "summary": "## 无标签下语言模型的演进：多数决定选择，新颖促进多样性\n\n### 背景与挑战\n\n大型语言模型（LLMs）的训练日益依赖于通过可验证奖励的强化学习（RLVR）。然而，在实际部署中，模型需要具备在没有外部标签或评判的情况下进行自我改进的能力。现有的无标签方法，如置信度最小化、自洽性或多数投票目标，虽然能够稳定学习过程，但却逐渐限制了模型的探索能力，导致“熵坍塌”现象。这意味着模型生成的响应会变得更短、多样性降低，并且更加脆弱。\n\n与Test-Time Reinforcement Learning (TTRL)等主要使模型适应当前无标签数据集的方法不同，本文的目标更为宏大：在不牺牲模型固有探索能力和泛化能力的前提下，实现模型的通用改进，即“演进”。\n\n### 提出的解决方案：EVOL-RL\n\n为了解决上述问题，本文提出了一种名为**EVOL-RL (EVolution-Oriented and Label-free Reinforcement Learning)** 的方法。EVOL-RL是一个简单的规则，它在无标签设置下巧妙地将学习的稳定性与生成内容的变异性结合起来。\n\n#### EVOL-RL的核心机制\n\nEVOL-RL的设计理念是“多数驱动选择，新颖促进变异”：\n\n1.  **选择（Selection）**：EVOL-RL将多数投票的答案作为稳定的锚点。这确保了学习过程的稳定性，避免模型偏离已验证的、多数认可的路径。\n2.  **变异（Variation）**：为了防止熵坍塌和促进探索，EVOL-RL引入了一个新颖性奖励。这个奖励机制鼓励模型生成那些推理过程与已产生内容在语义空间中存在显著差异的响应。通过这种方式，模型被激励去探索新的解决方案和思维路径，从而保持多样性。\n\n#### 实现细节\n\nEVOL-RL的实现基于GRPO（Generalized Reinforcement Policy Optimization）。此外，它还采用了以下技术来优化性能和稳定性：\n\n*   **非对称剪裁（Asymmetric Clipping）**：用于保留强大的信号，确保重要的学习信息不会被过度平滑或丢弃。\n*   **熵正则化器（Entropy Regularizer）**：用于维持模型的搜索能力和探索性，进一步防止多样性坍塌。\n\n### 实验结果与优势\n\nEVOL-RL的设计理念带来了显著的性能提升和多项优势：\n\n*   **防止熵坍塌**：有效避免了生成内容变短、多样性降低的问题。\n*   **保持思维链的长度和信息量**：模型能够生成更长、更具信息量的思维链（chains of thought）。\n*   **性能提升**：显著提高了pass@1和pass@n的性能。\n*   **超越基线**：EVOL-RL持续优于仅采用多数投票的TTRL基线。例如，在无标签AIME24数据集上训练Qwen3-4B-Base模型后，其在AIME25上的pass@1从TTRL的4.6%提升至16.4%，pass@16从18.5%提升至37.9%。\n*   **更强的泛化能力**：EVOL-RL不仅防止了多样性坍塌，还解锁了跨领域（如GPQA）的更强泛化能力。\n*   **广泛适用性**：研究还表明，EVOL-RL在RLVR设置中也能提升性能，这突显了其广泛的适用性。\n\n### 结论\n\nEVOL-RL通过结合多数驱动的选择和新颖性促进的变异，为无标签下语言模型的自我演进提供了一个有效且鲁棒的框架，成功解决了现有方法导致的熵坍塌问题，并显著提升了模型的性能和泛化能力。",
      "shortSummary": "本文提出EVOL-RL，一种无标签强化学习方法，旨在解决现有LLM无标签自我改进中出现的“熵坍塌”问题。EVOL-RL通过将多数投票答案作为稳定锚点（选择）并引入新颖性奖励（变异）来鼓励探索。实验表明，EVOL-RL有效防止了多样性坍塌，保持了更长的思维链，显著提高了pass@1和pass@n性能，并增强了模型在多个任务上的泛化能力，甚至在RLVR设置中也表现出色。",
      "translated_title": "无标签下语言模型的演进：多数决定选择，新颖促进多样性",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing label-free methods, confidence minimization, self-consistency, or majority-vote objectives, stabilize learning but steadily shrink exploration, causing an entropy collapse: generations become shorter, less diverse, and brittle. Unlike prior approaches such as Test-Time Reinforcement Learning (TTRL), which primarily adapt models to the immediate unlabeled dataset at hand, our goal is broader: to enable general improvements without sacrificing the model's inherent exploration capacity and generalization ability, i.e., evolving. We formalize this issue and propose EVolution-Oriented and Label-free Reinforcement Learning (EVOL-RL), a simple rule that couples stability with variation under a label-free setting. EVOL-RL keeps the majority-voted answer as a stable anchor (selection) while adding a novelty-aware reward that favors responses whose reasoning differs from what has already been produced (variation), measured in semantic space. Implemented with GRPO, EVOL-RL also uses asymmetric clipping to preserve strong signals and an entropy regularizer to sustain search. This majority-for-selection + novelty-for-variation design prevents collapse, maintains longer and more informative chains of thought, and improves both pass@1 and pass@n. EVOL-RL consistently outperforms the majority-only TTRL baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks stronger generalization across domains (e.g., GPQA). Furthermore, we demonstrate that EVOL-RL also boosts performance in the RLVR setting, highlighting its broad applicability."
    },
    {
      "title": "先理解再生成：自引导训练用于自回归图像生成 (原标题: Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation)",
      "link": "https://arxiv.org/abs/2509.15185",
      "pubDate": "Thu, 18 Sep 2025 13:47:40 GMT",
      "isoDate": "2025-09-18T13:47:40.000Z",
      "creator": "Xiaoyu Yue, Zidong Wang, Yuqing Wang, Wenlong Zhang, Xihui Liu, Wanli Ouyang, Lei Bai, Luping Zhou",
      "summary": "## 先理解再生成：自引导训练用于自回归图像生成\n\n### 背景与问题\n\n近期研究强调了高质量视觉表示在图像生成中的重要性，并指出了生成模型在图像理解方面的局限性。自回归模型最初是为自然语言处理设计的范式，在应用于视觉领域时也面临类似的挑战，尤其是在学习高级视觉语义方面。\n\n### 识别出的关键问题\n\n本研究首次系统地调查了将“下一词预测”范式应用于视觉领域时的机制，并识别出阻碍模型学习高级视觉语义的三个关键属性：\n\n*   **局部和条件依赖性 (Local and Conditional Dependence)：** 模型在生成过程中可能过度依赖局部信息和条件，导致对全局语义的理解不足。\n*   **步间语义不一致性 (Inter-Step Semantic Inconsistency)：** 在逐步生成图像的过程中，不同生成步骤之间可能出现语义上的不连贯或不一致。\n*   **空间不变性缺陷 (Spatial Invariance Deficiency)：** 模型可能缺乏对图像空间变换（如平移、旋转、缩放）的鲁棒性，难以识别相同对象在不同空间位置或姿态下的语义。\n\n### 提出的解决方案：ST-AR 框架\n\n为了解决上述问题，本研究提出了一种新颖的训练框架——**自引导训练用于自回归模型 (Self-guided Training for AutoRegressive models, ST-AR)**。该框架通过在训练过程中引入自监督目标，能够有效地解决这些问题。\n\n**ST-AR 的特点：**\n\n*   **无需预训练表示模型：** ST-AR 的优势在于它不依赖于任何预训练的表示模型，而是通过自监督学习来增强模型的理解能力。\n\n### 主要成果与优势\n\nST-AR 框架显著增强了自回归模型的图像理解能力，并带来了显著的生成质量提升：\n\n*   **图像理解能力提升：** 通过自监督目标，模型能够更好地捕捉和理解图像的高级语义信息。\n*   **生成质量显著改善：** 在保持相同采样策略的前提下，ST-AR 带来了显著的 FID (Fréchet Inception Distance) 改进：\n    *   LlamaGen-L 的 FID 提升约 **42%**。\n    *   LlamaGen-XL 的 FID 提升约 **49%**。\n\n### 接受情况\n\n该研究已被 NeurIPS 2025 接受。",
      "shortSummary": "本文提出ST-AR（自引导训练用于自回归模型）框架，旨在解决自回归图像生成模型在理解高级视觉语义方面的不足。研究识别出局部依赖、步间语义不一致和空间不变性缺陷等关键问题，并通过引入自监督目标来有效解决。ST-AR无需预训练模型，显著提升了模型的图像理解能力和生成质量，例如使LlamaGen-L和LlamaGen-XL的FID分别提升约42%和49%。",
      "translated_title": "先理解再生成：自引导训练用于自回归图像生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent studies have demonstrated the importance of high-quality visual representations in image generation and have highlighted the limitations of generative models in image understanding. As a generative paradigm originally designed for natural language, autoregressive models face similar challenges. In this work, we present the first systematic investigation into the mechanisms of applying the next-token prediction paradigm to the visual domain. We identify three key properties that hinder the learning of high-level visual semantics: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. We show that these issues can be effectively addressed by introducing self-supervised objectives during training, leading to a novel training framework, Self-guided Training for AutoRegressive models (ST-AR). Without relying on pre-trained representation models, ST-AR significantly enhances the image understanding ability of autoregressive models and leads to improved generation quality. Specifically, ST-AR brings approximately 42% FID improvement for LlamaGen-L and 49% FID improvement for LlamaGen-XL, while maintaining the same sampling strategy."
    },
    {
      "title": "释放多模态大型语言模型在零样本时空视频定位中的潜力 (原标题: Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding)",
      "link": "https://arxiv.org/abs/2509.15178",
      "pubDate": "Thu, 18 Sep 2025 13:35:50 GMT",
      "isoDate": "2025-09-18T13:35:50.000Z",
      "creator": "Zaiquan Yang, Yuhao Liu, Gerhard Hancke, Rynson W. H. Lau",
      "summary": "# 释放多模态大型语言模型在零样本时空视频定位中的潜力\n\n## 摘要\n\n本文探讨了利用多模态大型语言模型（MLLMs）解决零样本时空视频定位（STVG）问题。STVG 的目标是根据输入的文本查询，在视频中定位对应的时空区域。研究揭示了 MLLMs 在此任务中的关键洞察和挑战，并提出了一种创新的零样本框架来克服这些挑战。\n\n## MLLMs 的关键洞察\n\n研究人员对 MLLMs 在 STVG 任务中的行为进行了深入分析，发现了两个关键洞察：\n\n*   **定位标记的动态分配：** MLLMs 倾向于动态地分配特殊的标记（称为“定位标记”）来执行文本查询的定位任务。\n*   **文本线索整合不足：** MLLMs 常常因为无法充分整合文本查询中的所有线索（例如，属性、动作）进行推理，从而导致次优的定位性能。\n\n## 提出的零样本框架\n\n基于上述洞察，本文提出了一种基于 MLLM 的零样本 STVG 框架。该框架引入了两种新颖的策略：分解时空高亮（Decomposed Spatio-Temporal Highlighting, DSTH）和时间增强组装（Temporal-Augmented Assembling, TAS），旨在充分释放 MLLMs 的推理能力。\n\n### 1. 分解时空高亮 (DSTH) 策略\n\nDSTH 策略旨在更有效地利用文本查询中的信息，并引导模型关注相关的视觉区域：\n\n*   **查询解耦：** 首先将原始文本查询解耦为两个子查询：一个用于属性（attribute sub-query），另一个用于动作（action sub-query）。这两个子查询分别用于在空间和时间维度上探查目标的存在。\n*   **Logit 引导的重注意力 (LRA) 模块：** 引入了一个新颖的 Logit 引导的重注意力（LRA）模块。该模块通过规范每个子查询的标记预测，学习生成潜在变量，这些变量作为空间和时间提示。\n*   **提示作用：** 这些生成的提示分别突出属性和动作线索，有效地将模型的注意力引导至与查询相关的可靠空间和时间视觉区域。\n\n### 2. 时间增强组装 (TAS) 策略\n\n为了解决空间定位在时间上的一致性问题，TAS 策略被提出：\n\n*   **目的：** 确保由属性子查询进行的视频帧空间定位在时间维度上保持一致性。\n*   **方法：** 该策略通过将原始视频帧和经过时间增强处理的帧作为输入，来组装和整合预测结果，从而显著提高时间一致性。\n\n## 实验与结果\n\n研究人员在多种 MLLMs 上对所提出的方法进行了广泛评估。实验结果表明，该方法在三个常见的 STVG 基准测试中均超越了现有的最先进（SOTA）方法，验证了其有效性和优越性。",
      "shortSummary": "本文提出利用多模态大型语言模型（MLLMs）解决零样本时空视频定位（STVG）问题。研究发现 MLLMs 在定位标记和整合文本线索方面的不足。为此，提出了一种基于 MLLM 的零样本框架，包含分解时空高亮（DSTH）和时间增强组装（TAS）策略。DSTH 通过解耦查询和 Logit 引导的重注意力生成空间/时间提示。TAS 通过整合原始和增强帧提升时间一致性。实验证明，该方法在多个 STVG 基准上超越了现有最佳方法。",
      "translated_title": "释放多模态大型语言模型在零样本时空视频定位中的潜力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal tube of a video, as specified by the input text query. In this paper, we utilize multimodal large language models (MLLMs) to explore a zero-shot solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to dynamically assign special tokens, referred to as grounding tokens, for grounding the text query; and (2) MLLMs often suffer from suboptimal grounding due to the inability to fully integrate the cues in the text query (e.g., attributes, actions) for inference. Based on these insights, we propose a MLLM-based zero-shot framework for STVG, which includes novel decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH strategy first decouples the original query into attribute and action sub-queries for inquiring the existence of the target both spatially and temporally. It then uses a novel logit-guided re-attention (LRA) module to learn latent variables as spatial and temporal prompts, by regularizing token predictions for each sub-query. These prompts highlight attribute and action cues, respectively, directing the model's attention to reliable spatial and temporal related visual regions. In addition, as the spatial grounding by the attribute sub-query should be temporally consistent, we introduce the TAS strategy to assemble the predictions using the original video frames and the temporal-augmented frames as inputs to help improve temporal consistency. We evaluate our method on various MLLMs, and show that it outperforms SOTA methods on three common STVG benchmarks.   The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG."
    },
    {
      "title": "WorldForge：通过免训练指导解锁视频扩散模型中的涌现3D/4D生成 (原标题: WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance)",
      "link": "https://arxiv.org/abs/2509.15130",
      "pubDate": "Thu, 18 Sep 2025 12:40:47 GMT",
      "isoDate": "2025-09-18T12:40:47.000Z",
      "creator": "Chenxi Song, Yanming Yang, Tong Zhao, Ruibo Li, Chi Zhang",
      "summary": "### WorldForge：通过免训练指导解锁视频扩散模型中的涌现3D/4D生成\n\n#### 引言\n\n近期，视频扩散模型因其丰富的潜在世界先验，在空间智能任务中展现出巨大潜力。然而，其有限的控制性和几何不一致性阻碍了它们在3D/4D任务中的实际应用。目前，解决这些问题的方法通常依赖于重新训练或微调，这不仅可能损害预训练知识，还会带来高昂的计算成本。\n\n#### WorldForge 框架\n\n为应对上述挑战，研究人员提出了 **WorldForge**，这是一个免训练、推理时（inference-time）的框架，由三个紧密耦合的模块组成：\n\n1.  **步内递归细化 (Intra-Step Recursive Refinement)**\n    *   该模块在每个去噪步骤中引入递归细化机制。\n    *   它重复优化网络预测，以实现精确的轨迹注入。\n\n2.  **流门控潜在融合 (Flow-Gated Latent Fusion)**\n    *   利用光流相似性在潜在空间中解耦运动和外观。\n    *   选择性地将轨迹指导注入到与运动相关的通道中。\n\n3.  **双路径自校正指导 (Dual-Path Self-Corrective Guidance)**\n    *   通过比较有指导和无指导的去噪路径。\n    *   自适应地纠正由噪声或未对齐结构信号引起的轨迹漂移。\n\n#### 方法优势与成果\n\n这些组件协同工作，无需训练即可注入细粒度、轨迹对齐的指导，从而实现：\n\n*   **准确的运动控制**\n*   **逼真的内容生成**\n\n广泛的实验在多样化的基准测试中验证了 WorldForge 在真实感、轨迹一致性和视觉保真度方面的优越性。\n\n#### 贡献\n\n这项工作为可控视频合成引入了一种新颖的即插即用范式，为利用生成先验进行空间智能任务提供了新的视角。",
      "shortSummary": "WorldForge 提出了一种免训练、推理时的框架，旨在解决视频扩散模型在3D/4D生成中控制性差和几何不一致的问题。该框架包含步内递归细化、流门控潜在融合和双路径自校正指导三个模块，能够在不进行训练的情况下，精确注入轨迹指导，实现准确的运动控制和逼真的内容生成。实验证明其在真实感、轨迹一致性和视觉保真度方面表现优异，为可控视频合成提供了一种即插即用的新范式。",
      "translated_title": "WorldForge：通过免训练指导解锁视频扩散模型中的涌现3D/4D生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent video diffusion models demonstrate strong potential in spatial intelligence tasks due to their rich latent world priors. However, this potential is hindered by their limited controllability and geometric inconsistency, creating a gap between their strong priors and their practical use in 3D/4D tasks. As a result, current approaches often rely on retraining or fine-tuning, which risks degrading pretrained knowledge and incurs high computational costs. To address this, we propose WorldForge, a training-free, inference-time framework composed of three tightly coupled modules. Intra-Step Recursive Refinement introduces a recursive refinement mechanism during inference, which repeatedly optimizes network predictions within each denoising step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages optical flow similarity to decouple motion from appearance in the latent space and selectively inject trajectory guidance into motion-related channels. Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths to adaptively correct trajectory drift caused by noisy or misaligned structural signals. Together, these components inject fine-grained, trajectory-aligned guidance without training, achieving both accurate motion control and photorealistic content generation. Extensive experiments across diverse benchmarks validate our method's superiority in realism, trajectory consistency, and visual fidelity. This work introduces a novel plug-and-play paradigm for controllable video synthesis, offering a new perspective on leveraging generative priors for spatial intelligence."
    },
    {
      "title": "注意差距：深入探讨大型语言模型多项选择问答中的分词问题 (原标题: Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question Answering with LLMs)",
      "link": "https://arxiv.org/abs/2509.15020",
      "pubDate": "Thu, 18 Sep 2025 10:47:58 GMT",
      "isoDate": "2025-09-18T10:47:58.000Z",
      "creator": "Mario Sanz-Guerrero, Minh Duc Bui, Katharina von der Wense",
      "summary": "## 大型语言模型多项选择问答中的分词问题\n\n### 背景\n在使用大型语言模型（LLMs）进行多项选择问答（MCQA）评估时，通常会在提示符末尾添加“Answer:”字符串，以便通过下一个词元概率自动提取答案。然而，对于如何对冒号后的空格进行分词，目前没有统一的共识，这通常被视为一个微不足道的选择。\n\n### 核心发现\n本研究揭示了以下关键发现：\n*   这种看似无关紧要的分词差异会导致高达 **11%** 的准确率差异。\n*   它还会重新洗牌模型的排名，引发了对先前工作中LLM比较可靠性的担忧。\n\n### 推荐策略与益处\n*   研究出人意料地推荐了一种特定的策略：**将空格与答案字母一起分词**。\n*   这种策略带来了持续且具有统计学意义的 **性能提升**。\n*   此外，它还改善了 **模型校准**，增强了模型置信度估计的可靠性。\n\n### 研究意义\n本研究结果强调了以下重要性：\n*   精心设计评估的重要性。\n*   制定标准化、透明的评估协议的必要性，以确保结果的可靠性和可比性。\n\n### 其他信息\n*   该论文已被 **EMNLP 2025 主会议** 接受。\n*   主要研究领域：计算与语言 (cs.CL)。",
      "shortSummary": "本研究发现，在大型语言模型多项选择问答中，对“Answer:”后空格的分词方式，这一看似微小的选择，竟能导致高达11%的准确率差异并影响模型排名。研究推荐将空格与答案字母一起分词，此策略显著提升了模型性能和校准。这强调了标准化、透明评估协议对于确保LLM比较可靠性的重要性。",
      "translated_title": "注意差距：深入探讨大型语言模型多项选择问答中的分词问题",
      "images": [],
      "contentSource": "完整文章",
      "content": "When evaluating large language models (LLMs) with multiple-choice question answering (MCQA), it is common to end the prompt with the string \"Answer:\" to facilitate automated answer extraction via next-token probabilities. However, there is no consensus on how to tokenize the space following the colon, often overlooked as a trivial choice. In this paper, we uncover accuracy differences of up to 11% due to this (seemingly irrelevant) tokenization variation as well as reshuffled model rankings, raising concerns about the reliability of LLM comparisons in prior work. Surprisingly, we are able to recommend one specific strategy -- tokenizing the space together with the answer letter -- as we observe consistent and statistically significant performance improvements. Additionally, it improves model calibration, enhancing the reliability of the model's confidence estimates. Our findings underscore the importance of careful evaluation design and highlight the need for standardized, transparent evaluation protocols to ensure reliable and comparable results."
    },
    {
      "title": "EchoVLM：用于通用超声智能的动态专家混合视觉-语言模型 (原标题: EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence)",
      "link": "https://arxiv.org/abs/2509.14977",
      "pubDate": "Thu, 18 Sep 2025 10:07:53 GMT",
      "isoDate": "2025-09-18T10:07:53.000Z",
      "creator": "Chaoyin She, Ruifang Lu, Lida Chen, Wei Wang, Qinghua Huang",
      "summary": "# EchoVLM：用于通用超声智能的动态专家混合视觉-语言模型\n\n## 摘要\n\n本文介绍了EchoVLM，一个专门为超声医学影像设计的视觉-语言模型（VLM），旨在解决传统超声诊断中存在的挑战，并提升诊断效率和准确性。\n\n## 背景与挑战\n\n*   **超声影像的优势：** 超声成像因其无电离辐射、成本低和实时成像能力，已成为早期癌症筛查的首选影像模态。\n*   **传统诊断的局限性：** 传统的超声诊断高度依赖医生的专业知识，导致诊断主观性强、效率低下。\n*   **现有通用VLM的不足：** 现有的通用视觉-语言模型在超声医学任务中知识有限，在多器官病变识别中泛化能力差，且在多任务诊断中效率低下。\n\n## EchoVLM模型介绍\n\n*   **模型名称：** EchoVLM\n*   **设计目标：** 解决现有VLM在超声医学领域的局限性，提供一个通用超声智能解决方案。\n*   **核心架构：** 采用专家混合（Mixture of Experts, MoE）架构，使其能够动态地处理不同任务和数据。\n*   **训练数据：** 在涵盖七个解剖区域的数据集上进行训练，确保模型具有广泛的超声医学知识。\n*   **多任务能力：** 该模型能够执行多种超声医学任务，包括：\n    *   超声报告生成\n    *   诊断\n    *   视觉问答（VQA）\n\n## 实验结果与性能提升\n\n*   **对比模型：** EchoVLM与Qwen2-VL进行了对比实验。\n*   **任务：** 主要在超声报告生成任务上进行了评估。\n*   **显著改进：**\n    *   在BLEU-1分数上，EchoVLM相比Qwen2-VL提高了10.15点。\n    *   在ROUGE-1分数上，EchoVLM相比Qwen2-VL提高了4.77点。\n*   **结论：** 这些实验结果表明EchoVLM在提高超声影像诊断准确性方面具有巨大潜力。\n\n## 临床应用前景\n\nEchoVLM为未来的临床应用提供了一个可行的技术解决方案，有望显著提升超声诊断的效率和准确性，从而改善患者护理。\n\n## 资源可用性\n\n模型的源代码和权重可供获取。",
      "shortSummary": "EchoVLM是一个专为超声医学影像设计的动态专家混合视觉-语言模型，旨在解决传统超声诊断的主观性、低效率及通用VLM在超声领域知识不足的挑战。该模型采用专家混合架构，在七个解剖区域数据上训练，可执行超声报告生成、诊断和视觉问答。实验表明，EchoVLM在超声报告生成任务上显著优于Qwen2-VL，BLEU-1和ROUGE-1分数分别提升10.15和4.77点，展现出提升超声诊断准确性的巨大潜力。",
      "translated_title": "EchoVLM：用于通用超声智能的动态专家混合视觉-语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Ultrasound imaging has become the preferred imaging modality for early cancer screening due to its advantages of non-ionizing radiation, low cost, and real-time imaging capabilities. However, conventional ultrasound diagnosis heavily relies on physician expertise, presenting challenges of high subjectivity and low diagnostic efficiency. Vision-language models (VLMs) offer promising solutions for this issue, but existing general-purpose models demonstrate limited knowledge in ultrasound medical tasks, with poor generalization in multi-organ lesion recognition and low efficiency across multi-task diagnostics. To address these limitations, we propose EchoVLM, a vision-language model specifically designed for ultrasound medical imaging. The model employs a Mixture of Experts (MoE) architecture trained on data spanning seven anatomical regions. This design enables the model to perform multiple tasks, including ultrasound report generation, diagnosis and visual question-answering (VQA). The experimental results demonstrated that EchoVLM achieved significant improvements of 10.15 and 4.77 points in BLEU-1 scores and ROUGE-1 scores respectively compared to Qwen2-VL on the ultrasound report generation task. These findings suggest that EchoVLM has substantial potential to enhance diagnostic accuracy in ultrasound imaging, thereby providing a viable technical solution for future clinical applications. Source code and model weights are available at https://github.com/Asunatan/EchoVLM."
    },
    {
      "title": "跨越边界推理：通过测试时审议增强规范对齐 (原标题: Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Delibration)",
      "link": "https://arxiv.org/abs/2509.14760",
      "pubDate": "Thu, 18 Sep 2025 05:08:53 GMT",
      "isoDate": "2025-09-18T05:08:53.000Z",
      "creator": "Haoran Zhang, Yafu Li, Xuyang Hu, Dongrui Liu, Zhilin Wang, Bo Li, Yu Cheng",
      "summary": "### 跨越边界推理：通过测试时审议增强规范对齐\n\n**背景与挑战**\n\n*   大型语言模型（LLMs）在现实世界的应用日益广泛，每个应用场景都伴随着用户或组织定制的行为和安全规范（spec）。\n*   这些规范被分为安全规范和行为规范，它们因场景而异，并随着偏好和需求的改变而不断演进。\n*   本文将此挑战形式化为“规范对齐”，重点关注LLMs从行为和安全两个维度遵循动态、场景特定规范的能力。\n\n**提出的方法：Align3**\n\n*   为解决规范对齐的挑战，本文提出了一种名为Align3的轻量级方法。\n*   Align3利用“测试时审议”（Test-Time Deliberation, TTD）机制，结合分层反射和修订过程，以有效推理和遵循规范边界。\n\n**评估基准：SpecBench**\n\n*   为了系统地衡量规范对齐能力，本文还提出了一个统一的基准——SpecBench。\n*   SpecBench涵盖了5个不同的场景、103个具体的规范以及1,500个测试提示。\n\n**实验与主要发现**\n\n*   研究团队在15个推理模型和18个指令模型上进行了广泛实验，并对比了包括Self-Refine、TPO和MoreThink在内的多种TTD方法。\n*   实验结果揭示了三个关键发现：\n    1.  **测试时审议（TTD）显著增强了规范对齐能力。** 这表明在模型推理阶段进行反思和修订是有效的策略。\n    2.  **Align3以最小的额外开销，提升了安全-有用性权衡的前沿。** 这意味着Align3能够在不牺牲模型有用性的前提下，显著提高其安全性。\n    3.  **SpecBench能够有效地揭示LLMs在规范对齐方面的现有差距。** 证明了该基准在诊断模型弱点方面的实用性。\n\n**结论**\n\n*   这些实验结果共同强调了测试时审议作为一种有效策略，在推理和遵循现实世界复杂规范边界方面的巨大潜力。",
      "shortSummary": "本文提出了Align3方法，通过测试时审议（TTD）和分层反射修订，增强大型语言模型（LLMs）对动态、场景特定行为和安全规范的对齐能力。为评估此能力，作者构建了SpecBench基准。实验表明，TTD能有效提升规范对齐，Align3在安全-有用性权衡上表现出色，且SpecBench能揭示对齐差距。研究强调了TTD在处理现实世界规范边界方面的潜力。",
      "translated_title": "跨越边界推理：通过测试时审议增强规范对齐",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) are increasingly applied in diverse real-world scenarios, each governed by bespoke behavioral and safety specifications (spec) custom-tailored by users or organizations. These spec, categorized into safety-spec and behavioral-spec, vary across scenarios and evolve with changing preferences and requirements. We formalize this challenge as specification alignment, focusing on LLMs' ability to follow dynamic, scenario-specific spec from both behavioral and safety perspectives. To address this challenge, we propose Align3, a lightweight method that employs Test-Time Deliberation (TTD) with hierarchical reflection and revision to reason over the specification boundaries. We further present SpecBench, a unified benchmark for measuring specification alignment, covering 5 scenarios, 103 spec, and 1,500 prompts. Experiments on 15 reasoning and 18 instruct models with several TTD methods, including Self-Refine, TPO, and MoreThink, yield three key findings: (i) test-time deliberation enhances specification alignment; (ii) Align3 advances the safety-helpfulness trade-off frontier with minimal overhead; (iii) SpecBench effectively reveals alignment gaps. These results highlight the potential of test-time deliberation as an effective strategy for reasoning over the real-world specification boundaries."
    },
    {
      "title": "MultiEdit：在多样化和挑战性任务上推进基于指令的图像编辑 (原标题: MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks)",
      "link": "https://arxiv.org/abs/2509.14638",
      "pubDate": "Thu, 18 Sep 2025 01:33:38 GMT",
      "isoDate": "2025-09-18T01:33:38.000Z",
      "creator": "Mingsong Li, Lin Liu, Hongjun Wang, Haoxing Chen, Xijun Gu, Shizhan Liu, Dong Gong, Junbo Zhao, Zhenzhong Lan, Jianguo Li",
      "summary": "## MultiEdit：推进基于指令的图像编辑的新数据集\n\n### 背景与挑战\n当前基于指令的图像编辑（IBIE）方法在处理复杂编辑任务时面临显著挑战。主要问题包括：\n*   **数据集限制**：现有数据集的编辑类型和样本数量有限，难以覆盖广泛的编辑需求。\n*   **数据噪声**：传统数据集构建中常包含噪声图像-标题对，这可能引入模型偏差，并限制模型在复杂编辑场景中的能力。\n\n### MultiEdit 数据集介绍\n为解决上述局限性，研究人员引入了 **MultiEdit**，这是一个综合性的高质量图像编辑数据集，具有以下特点：\n*   **规模庞大**：包含超过107K个高质量图像编辑样本。\n*   **任务多样性**：涵盖6个具有挑战性的编辑任务。\n*   **操作丰富**：\n    *   包含18种非风格迁移编辑类型。\n    *   包含38种风格迁移操作。\n*   **覆盖范围广**：从复杂的风格迁移到更复杂的语义操作，如人物引用编辑和图像内文本编辑。\n\n### 数据集构建方法\nMultiEdit 采用了一种新颖的数据集构建流程，该流程利用了两个多模态大型语言模型（MLLMs）：\n1.  一个 MLLM 负责生成视觉自适应的编辑指令。\n2.  另一个 MLLM 负责生成高保真度的编辑图像。\n\n### 实验结果与贡献\n*   **性能提升**：广泛的实验表明，使用 MultiEdit-Train 集对开源基础模型进行微调，能够显著提升模型在 MultiEdit-Test 基准上处理复杂编辑任务的性能。\n*   **能力保持**：同时，这种微调有效保持了模型在标准编辑基准上的原有能力。\n*   **研究价值**：研究人员认为 MultiEdit 为推进 IBIE 领域在更多样化和挑战性能力方面的研究提供了宝贵的资源。\n\n### 数据可用性\nMultiEdit 数据集已公开可用。",
      "shortSummary": "MultiEdit是一个旨在解决当前基于指令的图像编辑（IBIE）方法在处理复杂任务时局限性的新数据集。它包含超过107K高质量样本，涵盖6种挑战性任务、18种非风格迁移和38种风格迁移操作，范围从语义编辑到风格转换。通过创新的MLLM驱动构建流程，MultiEdit显著提升了模型在复杂编辑任务上的性能，同时保持了其在标准基准上的能力，为IBIE研究提供了宝贵资源。",
      "translated_title": "MultiEdit：在多样化和挑战性任务上推进基于指令的图像编辑",
      "images": [],
      "contentSource": "完整文章",
      "content": "Current instruction-based image editing (IBIE) methods struggle with challenging editing tasks, as both editing types and sample counts of existing datasets are limited. Moreover, traditional dataset construction often contains noisy image-caption pairs, which may introduce biases and limit model capabilities in complex editing scenarios. To address these limitations, we introduce MultiEdit, a comprehensive dataset featuring over 107K high-quality image editing samples. It encompasses 6 challenging editing tasks through a diverse collection of 18 non-style-transfer editing types and 38 style transfer operations, covering a spectrum from sophisticated style transfer to complex semantic operations like person reference editing and in-image text editing. We employ a novel dataset construction pipeline that utilizes two multi-modal large language models (MLLMs) to generate visual-adaptive editing instructions and produce high-fidelity edited images, respectively. Extensive experiments demonstrate that fine-tuning foundational open-source models with our MultiEdit-Train set substantially improves models' performance on sophisticated editing tasks in our proposed MultiEdit-Test benchmark, while effectively preserving their capabilities on the standard editing benchmark. We believe MultiEdit provides a valuable resource for advancing research into more diverse and challenging IBIE capabilities. Our dataset is available at https://huggingface.co/datasets/inclusionAI/MultiEdit."
    },
    {
      "title": "AToken：一种统一的视觉分词器 (原标题: AToken: A Unified Tokenizer for Vision)",
      "link": "https://arxiv.org/abs/2509.14476",
      "pubDate": "Wed, 17 Sep 2025 19:11:18 GMT",
      "isoDate": "2025-09-17T19:11:18.000Z",
      "creator": "Jiasen Lu, Liangchen Song, Mingze Xu, Byeongjoo Ahn, Yanjun Wang, Chen Chen, Afshin Dehghan, Yinfei Yang",
      "summary": "## AToken：统一视觉分词器\n\nAToken是一种开创性的统一视觉分词器，旨在解决现有视觉分词器在多模态处理和任务统一方面的局限性。它首次实现了对图像、视频和3D资产的高保真重建和语义理解，并将这些能力整合到单一框架中。\n\n### 核心创新与目标\n\n*   **统一多模态与任务**：\n    *   现有分词器通常专注于单一模态（如图像或视频）的重建或理解任务。\n    *   AToken的目标是统一处理图像、视频和3D资产，并在一个框架内同时实现高保真重建和语义理解。\n*   **共享4D潜在空间**：\n    *   AToken将所有这些多样化的视觉输入编码到一个共享的4D潜在空间中，从而实现了模态和任务的统一。\n\n### 技术架构与训练策略\n\n*   **纯Transformer架构**：\n    *   AToken采用纯Transformer架构，结合4D旋转位置嵌入（4D rotary position embeddings），使其能够处理任意分辨率和时间长度的视觉输入。\n*   **无对抗训练目标**：\n    *   为了确保训练的稳定性并实现最先进的重建质量，AToken引入了一种无对抗训练目标。\n    *   该目标结合了感知损失（perceptual loss）和Gram矩阵损失（Gram matrix loss）。\n*   **渐进式训练课程**：\n    *   AToken采用渐进式训练课程，逐步从单一图像扩展到视频和3D资产。\n    *   它支持连续和离散的潜在token，提供了更大的灵活性。\n\n### 性能表现\n\nAToken在不同模态和任务上均取得了显著的性能：\n\n*   **图像**：\n    *   rFID：0.21\n    *   ImageNet准确率：82.2%\n*   **视频**：\n    *   rFVD：3.01\n    *   MSRVTT检索准确率：32.6%\n*   **3D**：\n    *   PSNR：28.19\n    *   分类准确率：90.9%\n\n### 下游应用\n\nAToken在多种下游应用中展现出强大的能力和竞争力：\n\n*   **视觉生成任务**：\n    *   图像生成（支持连续和离散token）\n    *   文本到视频生成\n    *   图像到3D合成\n*   **理解任务**：\n    *   多模态大型语言模型（LLMs）\n\n### 未来展望\n\n这些成果表明，AToken为构建基于统一视觉分词的下一代多模态AI系统提供了重要的基础和方向。",
      "shortSummary": "AToken是一种开创性的统一视觉分词器，能对图像、视频和3D资产进行高保真重建和语义理解。它通过纯Transformer架构将不同模态编码到共享的4D潜在空间，并采用无对抗训练和渐进式学习策略。AToken在生成和理解任务上均达到最先进的性能，例如图像的0.21 rFID和82.2% ImageNet准确率，以及3D的28.19 PSNR和90.9%分类准确率。它为构建下一代多模态AI系统提供了统一的视觉基础。",
      "translated_title": "AToken：一种统一的视觉分词器",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present AToken, the first unified visual tokenizer that achieves both high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets. Unlike existing tokenizers that specialize in either reconstruction or understanding for single modalities, AToken encodes these diverse visual inputs into a shared 4D latent space, unifying both tasks and modalities in a single framework. Specifically, we introduce a pure transformer architecture with 4D rotary position embeddings to process visual inputs of arbitrary resolutions and temporal durations. To ensure stable training, we introduce an adversarial-free training objective that combines perceptual and Gram matrix losses, achieving state-of-the-art reconstruction quality. By employing a progressive training curriculum, AToken gradually expands from single images, videos, and 3D, and supports both continuous and discrete latent tokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01 rFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9% classification accuracy for 3D. In downstream applications, AToken enables both visual generation tasks (e.g., image generation with continuous and discrete tokens, text-to-video generation, image-to-3D synthesis) and understanding tasks (e.g., multimodal LLMs), achieving competitive performance across all benchmarks. These results shed light on the next-generation multimodal AI systems built upon unified visual tokenization."
    },
    {
      "title": "Apertus：为全球语言环境普及开放且合规的LLM (原标题: Apertus: Democratizing Open and Compliant LLMs for Global Language Environments)",
      "link": "https://arxiv.org/abs/2509.14233",
      "pubDate": "Wed, 17 Sep 2025 13:59:21 GMT",
      "isoDate": "2025-09-17T13:59:21.000Z",
      "creator": "Alejandro Hernández-Cano, Alexander Hägele, Allen Hao Huang, Angelika Romanou, Antoni-Joan Solergibert, Barna Pasztor, Bettina Messmer, Dhia Garbaya, Eduard Frank Ďurech, Ido Hakimi, Juan García Giraldo, Mete Ismayilzada, Negar Foroutan, Skander Moalla, Tiancheng Chen, Vinko Sabolčec, Yixuan Xu, Michael Aerni, Badr AlKhamissi, Ines Altemir Marinas, Mohammad Hossein Amani, Matin Ansaripour, Ilia Badanin, Harold Benoit, Emanuela Boros, Nicholas Browning, Fabian Bösch, Maximilian Böther, Niklas Canova, Camille Challier, Clement Charmillot, Jonathan Coles, Jan Deriu, Arnout Devos, Lukas Drescher, Daniil Dzenhaliou, Maud Ehrmann, Dongyang Fan, Simin Fan, Silin Gao, Miguel Gila, María Grandury, Diba Hashemi, Alexander Hoyle, Jiaming Jiang, Mark Klein, Andrei Kucharavy, Anastasiia Kucherenko, Frederike Lübeck, Roman Machacek, Theofilos Manitaras, Andreas Marfurt, Kyle Matoba, Simon Matrenok, Henrique Mendoncça, Fawzi Roberto Mohamed, Syrielle Montariol, Luca Mouchel, Sven Najem-Meyer, Jingwei Ni, Gennaro Oliva, Matteo Pagliardini, Elia Palme, Andrei Panferov, Léo Paoletti, Marco Passerini, Ivan Pavlov, Auguste Poiroux, Kaustubh Ponkshe, Nathan Ranchin, Javi Rando, Mathieu Sauser, Jakhongir Saydaliev, Muhammad Ali Sayfiddinov, Marian Schneider, Stefano Schuppli, Marco Scialanga, Andrei Semenov, Kumar Shridhar, Raghav Singhal, Anna Sotnikova, Alexander Sternfeld, Ayush Kumar Tarun, Paul Teiletche, Jannis Vamvas, Xiaozhe Yao, Hao Zhao Alexander Ilic, Ana Klimovic, Andreas Krause, Caglar Gulcehre, David Rosenthal, Elliott Ash, Florian Tramèr, Joost VandeVondele, Livio Veraldi, Martin Rajman, Thomas Schulthess, Torsten Hoefler, Antoine Bosselut, Martin Jaggi, Imanol Schlag",
      "summary": "# Apertus：开放且合规的全球语言环境LLM套件\n\nApertus 是一个全面开放的大型语言模型（LLM）套件，旨在解决当前开放模型生态系统中存在的两个系统性缺陷：数据合规性问题和多语言表示不足。该项目致力于通过透明和负责任的方式，推动LLM技术的普及和发展。\n\n## 核心目标与创新\nApertus 的主要目标是：\n*   **确保数据合规性**：解决现有模型在数据来源和使用方面可能存在的版权、隐私及伦理问题。\n*   **增强多语言覆盖**：显著提升模型对全球多种语言的处理能力，减少对英语内容的过度依赖。\n\n## 数据合规性与伦理考量\nApertus 在数据处理方面采取了严格的措施，以确保合规性和负责任性：\n*   **专属开放数据预训练**：所有模型均仅使用公开可用的数据进行预训练。\n*   **尊重内容所有者权利**：追溯性地遵守 `robots.txt` 排除规则，并过滤掉非许可、有毒和包含个人身份信息（PII）的内容。\n*   **记忆化风险缓解**：在预训练阶段采用“Goldfish 目标”，该机制能有效抑制数据逐字逐句的记忆，同时保持下游任务的性能，从而降低模型泄露隐私或生成重复内容的风险。\n\n## 广泛的多语言支持\nApertus 在多语言能力方面取得了显著进展：\n*   **海量多语言数据**：模型在超过1800种语言的15万亿（15T）个token上进行训练。\n*   **非英语内容倾斜**：预训练数据中约有40%分配给非英语内容，这大大增强了模型在非英语环境下的表现和适用性。\n\n## 模型规模与性能\nApertus 模型以两种规模发布：\n*   **8B 参数**\n*   **70B 参数**\n这些模型在多语言基准测试中，其性能已接近或超越了完全开放模型中的最先进水平，并能与一些开放权重模型相媲美或超越。\n\n## 透明度与开放性\nApertus 项目秉持高度的透明和开放原则：\n*   **全面开放科学成果**：除了模型权重，项目还以宽松的许可协议发布了开发周期中的所有科学成果。\n*   **可审计和可扩展性**：这些成果包括数据准备脚本、检查点、评估套件和训练代码，使得社区能够对模型进行透明的审计、验证和进一步的扩展开发。\n\n## 总结\nApertus 代表了开放LLM领域的一个重要进步，它不仅提供了高性能的模型，更通过对数据合规性、多语言支持和开发透明度的承诺，为构建一个更公平、更负责任的全球语言AI生态系统奠定了基础。",
      "shortSummary": "Apertus是一个开放的大型语言模型（LLM）套件，旨在解决当前开放模型生态系统中的数据合规性和多语言表示不足问题。它仅使用公开可用数据进行预训练，严格过滤非许可内容，并采用“Goldfish目标”抑制记忆化。模型在超过1800种语言的15万亿token上训练，其中40%为非英语内容。Apertus以8B和70B规模发布，在多语言基准测试中表现出色，并开放所有开发成果，以促进透明审计和扩展。",
      "translated_title": "Apertus：为全球语言环境普及开放且合规的LLM",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Apertus, a fully open suite of large language models (LLMs) designed to address two systemic shortcomings in today's open model ecosystem: data compliance and multilingual representation. Unlike many prior models that release weights without reproducible data pipelines or regard for content-owner rights, Apertus models are pretrained exclusively on openly available data, retroactively respecting robots.txt exclusions and filtering for non-permissive, toxic, and personally identifiable content. To mitigate risks of memorization, we adopt the Goldfish objective during pretraining, strongly suppressing verbatim recall of data while retaining downstream task performance. The Apertus models also expand multilingual coverage, training on 15T tokens from over 1800 languages, with ~40% of pretraining data allocated to non-English content. Released at 8B and 70B scales, Apertus approaches state-of-the-art results among fully open models on multilingual benchmarks, rivalling or surpassing open-weight counterparts. Beyond model weights, we release all scientific artifacts from our development cycle with a permissive license, including data preparation scripts, checkpoints, evaluation suites, and training code, enabling transparent audit and extension."
    },
    {
      "title": "GenExam：一个多学科文本到图像考试 (原标题: GenExam: A Multidisciplinary Text-to-Image Exam)",
      "link": "https://arxiv.org/abs/2509.14232",
      "pubDate": "Wed, 17 Sep 2025 13:59:14 GMT",
      "isoDate": "2025-09-17T13:59:14.000Z",
      "creator": "Zhaokai Wang, Penghao Yin, Xiangyu Zhao, Changyao Tian, Yu Qiao, Wenhai Wang, Jifeng Dai, Gen Luo",
      "summary": "## GenExam：一个多学科文本到图像考试基准\n\n### 引言\n现有考试基准主要侧重于理解和推理任务，而当前的生成基准则强调世界知识和视觉概念的说明，却忽视了对严格绘图考试的评估。为了填补这一空白，研究人员引入了GenExam。\n\n### GenExam 介绍\n*   **定义**：GenExam 是首个用于多学科文本到图像考试的基准。\n*   **规模与范围**：\n    *   包含1,000个样本。\n    *   涵盖10个不同的学科。\n    *   考试式提示按照一个四级分类法进行组织。\n\n### 关键特性\n*   **真实图像**：每个问题都配备了真实图像（ground-truth images），作为参考标准。\n*   **细粒度评分**：提供细粒度评分点，能够精确评估生成图像的语义正确性和视觉合理性。\n\n### 实验结果与挑战\n*   **模型表现**：实验结果显示，即使是GPT-Image-1和Gemini-2.5-Flash-Image等最先进的模型，其严格得分也低于15%。\n*   **普遍情况**：大多数模型几乎获得0分。\n*   **挑战性**：这表明GenExam基准带来了巨大的挑战，远超当前模型的极限。\n\n### 意义与展望\n*   **评估能力**：通过将图像生成视为一项考试，GenExam 对模型整合知识、推理和生成的能力进行了严格评估。\n*   **AGI之路**：该基准为通向通用人工智能（AGI）的路径提供了深刻见解和新的研究方向。\n\n### 相关信息\n*   **学科领域**：计算机视觉与模式识别 (cs.CV)\n*   **引用**：arXiv:2509.14232 [cs.CV]",
      "shortSummary": "GenExam是一个开创性的多学科文本到图像考试基准，旨在评估AI模型整合知识、推理和生成的能力。它包含1000个样本，涵盖10个学科，并提供真实图像和细粒度评分点。实验显示，即使是顶尖模型也表现不佳，严格得分低于15%，凸显了该基准的巨大挑战性。GenExam为通用人工智能（AGI）的发展提供了新的评估视角和研究方向。",
      "translated_title": "GenExam：一个多学科文本到图像考试",
      "images": [],
      "contentSource": "完整文章",
      "content": "Exams are a fundamental test of expert-level intelligence and require integrated understanding, reasoning, and generation. Existing exam-style benchmarks mainly focus on understanding and reasoning tasks, and current generation benchmarks emphasize the illustration of world knowledge and visual concepts, neglecting the evaluation of rigorous drawing exams. We introduce GenExam, the first benchmark for multidisciplinary text-to-image exams, featuring 1,000 samples across 10 subjects with exam-style prompts organized under a four-level taxonomy. Each problem is equipped with ground-truth images and fine-grained scoring points to enable a precise evaluation of semantic correctness and visual plausibility. Experiments show that even state-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve less than 15% strict scores, and most models yield almost 0%, suggesting the great challenge of our benchmark. By framing image generation as an exam, GenExam offers a rigorous assessment of models' ability to integrate knowledge, reasoning, and generation, providing insights on the path to general AGI."
    },
    {
      "title": "MARS2 2025多模态推理挑战赛：数据集、方法、结果、讨论与展望 (原标题: MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook)",
      "link": "https://arxiv.org/abs/2509.14142",
      "pubDate": "Wed, 17 Sep 2025 12:21:34 GMT",
      "isoDate": "2025-09-17T12:21:34.000Z",
      "creator": "Peng Xu, Shengwu Xiong, Jiajun Zhang, Yaxiong Chen, Bowen Zhou, Chen Change Loy, David A. Clifton, Kyoung Mu Lee, Luc Van Gool, Ruiming He, Ruilin Yao, Xinwei Long, Jirui Huang, Kai Tian, Sa Yang, Yihua Shao, Jin Feng, Yue Zhong, Jiakai Zhou, Cheng Tang, Tianyu Zou, Yifang Zhang, Junming Liang, Guoyou Li, Zhaoxiang Wang, Qiang Zhou, Yichen Zhao, Shili Xiong, Hyeongjin Nam, Jaerin Lee, Jaeyoung Chung, JoonKyu Park, Junghun Oh, Kanggeon Lee, Wooseok Lee, Juneyoung Ro, Turghun Osman, Can Hu, Chaoyang Liao, Cheng Chen, Chengcheng Han, Chenhao Qiu, Chong Peng, Cong Xu, Dailin Li, Feiyu Wang, Feng Gao, Guibo Zhu, Guopeng Tang, Haibo Lu, Han Fang, Han Qi, Hanxiao Wu, Haobo Cheng, Hongbo Sun, Hongyao Chen, Huayong Hu, Hui Li, Jiaheng Ma, Jiang Yu, Jianing Wang, Jie Yang, Jing He, Jinglin Zhou, Jingxuan Li, Josef Kittler, Lihao Zheng, Linnan Zhao, Mengxi Jia, Muyang Yan, Nguyen Thanh Thien, Pu Luo, Qi Li, Shien Song, Shijie Dong, Shuai Shao, Shutao Li, Taofeng Xue, Tianyang Xu, Tianyi Gao, Tingting Li, Wei Zhang, Weiyang Su, Xiaodong Dong, Xiao-Jun Wu, Xiaopeng Zhou, Xin Chen, Xin Wei, Xinyi You, Xudong Kang, Xujie Zhou, Xusheng Liu, Yanan Wang, Yanbin Huang, Yang Liu, Yang Yang, Yanglin Deng, Yashu Kang, Ye Yuan, Yi Wen, Yicen Tian, Yilin Tao, Yin Tang, Yipeng Lin, Yiqing Wang, Yiting Xi, Yongkang Yu, Yumei Li, Yuxin Qin, Yuying Chen, Yuzhe Cen, Zhaofan Zou, Zhaohong Liu, Zhehao Shen, Zhenglin Du, Zhengyang Li, Zhenni Huang, Zhenwei Shao, Zhilong Song, Zhiyong Feng, Zhiyu Wang, Zhou Yu, Ziang Li, Zihan Zhai, Zijian Zhang, Ziyang Peng, Ziyun Xiao, Zongshu Li",
      "summary": "# MARS2 2025多模态推理挑战赛回顾\n\n本文对MARS2 2025多模态推理挑战赛进行了回顾，该挑战赛旨在推动多模态机器学习和大型语言模型（LLMs）领域的发展。\n\n## 挑战目标与背景\n\n*   **汇集前沿方法：** 挑战赛旨在通过一个大型基准测试，汇集多模态机器学习和LLMs领域的各种方法，以帮助研究人员了解该动态领域的最新进展。\n*   **聚焦真实世界与专业场景：** 鉴于通用大型语言模型（LLMs）测试平台日益增多，本届MARS2挑战赛特别关注真实世界和专业场景，旨在拓宽多模态大型语言模型（MLLMs）在多模态推理应用中的范围。\n\n## 发布数据集\n\n为支持挑战赛，组织团队发布了两个定制数据集作为测试集：\n\n*   **Lens：** 支持12种日常场景中的通用推理任务。\n*   **AdsQA：** 支持广告视频中的领域特定推理任务。\n\n## 评估与竞赛赛道\n\n*   **基线模型评估：** 挑战赛评估了40多个基线模型，其中包括通用型多模态大型语言模型（MLLMs）和任务特定模型。\n*   **三大竞赛赛道：** 挑战赛设立了三个竞争赛道，以考察不同方面的多模态推理能力：\n    *   **真实世界场景中的视觉定位 (Visual Grounding in Real-world Scenarios, VG-RS)**\n    *   **具有空间感知能力的视觉问答 (Visual Question Answering with Spatial Awareness, VQA-SA)**\n    *   **创意广告视频中的视觉推理 (Visual Reasoning in Creative Advertisement Videos, VR-Ads)**\n\n## 参与情况与结果\n\n*   **广泛参与：** 共有来自知名学术和工业机构的76支团队注册参与。\n*   **有效提交：** 在超过1200份提交中，有40多份有效提交被纳入最终排名列表。\n\n## 资源可用性与展望\n\n*   **公开资源：** 挑战赛的数据集、代码集（包含40多个基线模型和15个以上参与者的方法）以及排名结果已在MARS2研讨会网站和GitHub组织页面（`https://github.com/MARS2-Challenge`）上公开。\n*   **持续更新：** 该GitHub页面将持续提供更新和未来活动的公告。\n*   **研讨会背景：** 本文是ICCV 2025 MARS2研讨会和挑战赛“大模型时代的多模态推理和慢思考：迈向系统2及更远”的评论。",
      "shortSummary": "MARS2 2025多模态推理挑战赛旨在通过大型基准测试，推动多模态机器学习和LLMs在真实世界及专业场景中的应用。挑战赛发布了Lens（日常场景）和AdsQA（广告视频）两个定制数据集，并设立了视觉定位、空间感知视觉问答和创意广告视频视觉推理三大赛道。共有76支团队参与，40多份有效提交被评估。所有数据集、代码和排名结果已公开，以促进该领域的进一步研究和发展。",
      "translated_title": "MARS2 2025多模态推理挑战赛：数据集、方法、结果、讨论与展望",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim to bring together different approaches in multimodal machine learning and LLMs via a large benchmark. We hope it better allows researchers to follow the state-of-the-art in this very dynamic area. Meanwhile, a growing number of testbeds have boosted the evolution of general-purpose large language models. Thus, this year's MARS2 focuses on real-world and specialized scenarios to broaden the multimodal reasoning applications of MLLMs. Our organizing team released two tailored datasets Lens and AdsQA as test sets, which support general reasoning in 12 daily scenarios and domain-specific reasoning in advertisement videos, respectively. We evaluated 40+ baselines that include both generalist MLLMs and task-specific models, and opened up three competition tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and industrial institutions have registered and 40+ valid submissions (out of 1200+) have been included in our ranking lists. Our datasets, code sets (40+ baselines and 15+ participants' methods), and rankings are publicly available on the MARS2 workshop website and our GitHub organization page https://github.com/mars2workshop/, where our updates and announcements of upcoming events will be continuously provided."
    },
    {
      "title": "Wan-Animate：基于整体复制的统一角色动画与替换 (原标题: Wan-Animate: Unified Character Animation and Replacement with Holistic Replication)",
      "link": "https://arxiv.org/abs/2509.14055",
      "pubDate": "Wed, 17 Sep 2025 11:00:57 GMT",
      "isoDate": "2025-09-17T11:00:57.000Z",
      "creator": "Gang Cheng, Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Ju Li, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, Ke Sun, Linrui Tian, Feng Wang, Guangyuan Wang, Qi Wang, Zhongjian Wang, Jiayu Xiao, Sheng Xu, Bang Zhang, Peng Zhang, Xindi Zhang, Zhe Zhang, Jingren Zhou, Lian Zhuo",
      "summary": "Wan-Animate是一个创新的统一框架，旨在实现角色动画和替换任务。该框架能够处理多种场景，提供高保真度的输出和无缝的集成。\n\n### 核心功能\n\nWan-Animate主要提供两种核心功能：\n\n1.  **角色动画**：\n    *   用户提供一个角色图像和一个参考视频。\n    *   Wan-Animate能够精确复制参考视频中角色的表情和动作。\n    *   最终生成高保真度的角色视频，使静态图像中的角色栩栩如生。\n\n2.  **角色替换**：\n    *   将动画化的角色集成到参考视频中，替换视频中的原始角色。\n    *   该系统能够复制场景的光照和色调，确保动画角色与新环境的无缝融合，达到逼真的效果。\n\n### 技术基础与创新\n\nWan-Animate基于Wan模型构建，并引入了多项关键技术创新以适应角色动画任务：\n\n*   **修改的输入范式**：采用一种修改后的输入范式，以区分参考条件和生成区域，从而将多项任务统一到一个共同的符号表示中。\n*   **身体动作复制**：利用空间对齐的骨骼信号来精确复制身体动作，确保动画的自然流畅。\n*   **表情重现**：从源图像中提取隐式面部特征，用于重现角色的表情，从而生成具有高可控性和表现力的角色视频。\n*   **环境融合增强**：为了在角色替换过程中增强环境集成，Wan-Animate开发了一个辅助的Relighting LoRA模块。该模块能够在应用适当的环境光照和色调的同时，保持角色外观的一致性。\n\n### 性能与可用性\n\n实验结果表明，Wan-Animate实现了最先进的性能。作者团队承诺将开源模型权重和源代码，以促进社区的进一步研究和应用。",
      "shortSummary": "Wan-Animate是一个统一的角色动画与替换框架。它能根据参考视频，将给定角色图像动画化，精确复制表情和动作，生成高保真角色视频；或将动画角色无缝集成到参考视频中，替换原有角色并匹配场景光照和色调。该框架基于Wan模型，利用骨骼信号和隐式面部特征实现高可控性与表现力，并通过Relighting LoRA增强环境融合。Wan-Animate实现了最先进的性能，并将开源其模型和代码。",
      "translated_title": "Wan-Animate：基于整体复制的统一角色动画与替换",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Wan-Animate, a unified framework for character animation and replacement. Given a character image and a reference video, Wan-Animate can animate the character by precisely replicating the expressions and movements of the character in the video to generate high-fidelity character videos. Alternatively, it can integrate the animated character into the reference video to replace the original character, replicating the scene's lighting and color tone to achieve seamless environmental integration. Wan-Animate is built upon the Wan model. To adapt it for character animation tasks, we employ a modified input paradigm to differentiate between reference conditions and regions for generation. This design unifies multiple tasks into a common symbolic representation. We use spatially-aligned skeleton signals to replicate body motion and implicit facial features extracted from source images to reenact expressions, enabling the generation of character videos with high controllability and expressiveness. Furthermore, to enhance environmental integration during character replacement, we develop an auxiliary Relighting LoRA. This module preserves the character's appearance consistency while applying the appropriate environmental lighting and color tone. Experimental results demonstrate that Wan-Animate achieves state-of-the-art performance. We are committed to open-sourcing the model weights and its source code."
    },
    {
      "title": "SAIL-VL2 技术报告 (原标题: SAIL-VL2 Technical Report)",
      "link": "https://arxiv.org/abs/2509.14033",
      "pubDate": "Wed, 17 Sep 2025 10:34:02 GMT",
      "isoDate": "2025-09-17T10:34:02.000Z",
      "creator": "Weijie Yin, Yongjie Ye, Fangxun Shu, Yue Liao, Zijian Kang, Hongyuan Dong, Haiyang Yu, Dingkang Yang, Jiacong Wang, Han Wang, Wenzhuo Liu, Xiao Liang, Shuicheng Yan, Chao Feng",
      "summary": "SAIL-VL2 技术报告\n\nSAIL-VL2 是一个开放套件的视觉-语言基础模型（LVM），旨在实现全面的多模态理解和推理。作为其前身 SAIL-VL 的继任者，SAIL-VL2 在2B和8B参数规模上，在各种图像和视频基准测试中均取得了最先进的性能，展示了从细粒度感知到复杂推理的强大能力。\n\n**核心创新**\nSAIL-VL2 的有效性主要归功于以下三项核心创新：\n\n1.  **大规模数据整理管道**\n    *   采用评分和过滤策略，显著提升了字幕、OCR、问答和视频数据的数据质量和分布。\n    *   有效提高了训练效率。\n\n2.  **渐进式训练框架**\n    *   首先利用强大的预训练视觉编码器（SAIL-ViT）。\n    *   随后进行多模态预训练。\n    *   最终采用“思维融合”（thinking-fusion）的SFT-RL混合范式，系统性地增强了模型能力。\n\n3.  **架构改进**\n    *   超越了传统的密集型大型语言模型（LLMs）设计。\n    *   引入了高效的稀疏专家混合（MoE）架构。\n\n**性能表现**\n*   SAIL-VL2 在106个数据集上展现出具有竞争力的性能。\n*   在 MMMU 和 MathVista 等具有挑战性的推理基准测试中取得了最先进的结果。\n*   在 OpenCompass 排行榜上，SAIL-VL2-2B 在4B参数规模以下的官方发布开源模型中排名第一。\n\n**社区贡献**\nSAIL-VL2 为开源多模态社区提供了一个高效且可扩展的基础。",
      "shortSummary": "SAIL-VL2是一个开放套件的视觉-语言基础模型，旨在实现全面的多模态理解和推理。作为SAIL-VL的继任者，它在2B和8B参数规模上取得了最先进的性能，并在106个数据集和复杂推理基准测试（如MMMU、MathVista）上表现出色。其核心创新包括大规模数据整理、渐进式训练框架和高效的稀疏专家混合（MoE）架构。SAIL-VL2-2B在OpenCompass排行榜上4B参数规模以下开源模型中排名第一，为开源多模态社区提供了高效且可扩展的基础。",
      "translated_title": "SAIL-VL2 技术报告",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM) for comprehensive multimodal understanding and reasoning. As the successor to SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B parameter scales across diverse image and video benchmarks, demonstrating strong capabilities from fine-grained perception to complex reasoning. Three core innovations drive its effectiveness. First, a large-scale data curation pipeline with scoring and filtering strategies enhances both quality and distribution across captioning, OCR, QA, and video data, improving training efficiency. Second, a progressive training framework begins with a powerful pre-trained vision encoder (SAIL-ViT), advances through multimodal pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that systematically strengthens model capabilities. Third, architectural advances extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs. With these contributions, SAIL-VL2 demonstrates competitive performance across 106 datasets and achieves state-of-the-art results on challenging reasoning benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass leaderboard, SAIL-VL2-2B ranks first among officially released open-source models under the 4B parameter scale, while serving as an efficient and extensible foundation for the open-source multimodal community."
    },
    {
      "title": "Hala 技术报告：构建大规模以阿拉伯语为中心的指令和翻译模型 (原标题: Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale)",
      "link": "https://arxiv.org/abs/2509.14008",
      "pubDate": "Wed, 17 Sep 2025 10:19:28 GMT",
      "isoDate": "2025-09-17T10:19:28.000Z",
      "creator": "Hasan Abed Al Kader Hammoud, Mohammad Zbeeb, Bernard Ghanem",
      "summary": "# Hala 技术报告：构建大规模以阿拉伯语为中心的指令和翻译模型\n\n## 摘要\n\n本技术报告介绍了Hala，一个以阿拉伯语为中心的指令和翻译模型家族。这些模型通过独特的“翻译-调优”管道构建，并在阿拉伯语相关基准测试中取得了最先进的成果。Hala项目旨在加速阿拉伯语自然语言处理（NLP）领域的研究。\n\n## 关键方法与技术\n\nHala模型的开发遵循以下核心流程：\n\n*   **教师模型压缩与双语监督生成**\n    *   首先，将一个强大的阿拉伯语-英语（AR$\\leftrightarrow$EN）教师模型压缩到FP8精度。\n    *   这一压缩过程在不损失模型质量的前提下，将吞吐量提高了约2倍。\n    *   压缩后的教师模型用于生成高质量的双语监督数据。\n*   **轻量级语言模型微调与语料库构建**\n    *   一个轻量级语言模型LFM2-1.2B在此双语数据上进行微调。\n    *   随后，该模型被用于将高质量的英语指令集翻译成阿拉伯语。\n    *   这一过程生成了一个百万规模的语料库，专门用于指令遵循任务。\n*   **模型训练与平衡**\n    *   Hala模型在不同参数规模下进行训练，包括350M、700M、1.2B和9B参数。\n    *   采用slerp合并技术，以平衡模型的阿拉伯语专业化能力与基础模型的通用优势。\n\n## 性能表现\n\nHala模型在阿拉伯语相关基准测试中展现出卓越的性能：\n\n*   在“纳米”（$\\leq$2B参数）和“小型”（7-9B参数）两个类别中均取得了最先进（SOTA）的结果。\n*   其性能显著优于作为基础的模型。\n\n## 资源发布\n\n为了促进阿拉伯语自然语言处理（NLP）领域的研究，Hala项目团队发布了：\n\n*   训练好的模型\n*   相关数据集\n*   评估工具\n*   训练配方（recipes）\n\n## 提交信息\n\n*   **主题：** 计算与语言 (cs.CL); 人工智能 (cs.AI); 机器学习 (cs.LG)\n*   **引用方式：** arXiv:2509.14008 [cs.CL]\n*   **提交日期：** 2025年9月17日",
      "shortSummary": "Hala项目发布了一系列大规模以阿拉伯语为中心的指令和翻译模型。通过“翻译-调优”管道，该项目首先将强大的AR$\\leftrightarrow$EN教师模型压缩并生成高质量双语监督数据，然后用轻量级模型将英语指令集翻译成阿拉伯语。Hala模型（350M至9B参数）在阿拉伯语基准测试中取得了最先进（SOTA）结果，超越了基础模型。为加速研究，模型、数据和训练配方均已发布。",
      "translated_title": "Hala 技术报告：构建大规模以阿拉伯语为中心的指令和翻译模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Hala, a family of Arabic-centric instruction and translation models built with our translate-and-tune pipeline. We first compress a strong ARleftrightarrowEN teacher to FP8 (yielding sim2times higher throughput with no quality loss) and use it to create high-fidelity bilingual supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this data and used to translate high-quality English instruction sets into Arabic, producing a million-scale corpus tailored to instruction following. We train Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to balance Arabic specialization with base-model strengths. On Arabic-centric benchmarks, Hala achieves state-of-the-art results within both the \"nano\" (leq2B) and \"small\" (7-9B) categories, outperforming their bases. We release models, data, evaluation, and recipes to accelerate research in Arabic NLP."
    },
    {
      "title": "THOR：基于强化学习的工具集成层次优化，用于数学推理 (原标题: THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning)",
      "link": "https://arxiv.org/abs/2509.13761",
      "pubDate": "Wed, 17 Sep 2025 03:16:12 GMT",
      "isoDate": "2025-09-17T03:16:12.000Z",
      "creator": "Qikai Chang, Zhenrong Zhang, Pengfei Hu, Jiefeng Ma, Yicheng Pan, Jianshu Zhang, Jun Du, Quan Liu, Jianqing Gao",
      "summary": "## THOR：基于强化学习的工具集成层次优化，用于数学推理\n\n大型语言模型（LLMs）在数学推理方面取得了显著进展，但在高精度任务（如数值计算和形式符号操作）上仍面临挑战。将外部工具集成被视为弥补这一差距的有效方法。然而，现有方法在以下三个关键方面仍存在不足：\n\n*   构建工具集成推理数据\n*   执行细粒度优化\n*   增强推理能力\n\n为了克服这些限制，我们提出了 **THOR (Tool-Integrated Hierarchical Optimization via RL)** 框架，它通过以下核心组件实现目标：\n\n### 1. TIRGen：高质量工具集成推理数据生成\n\n*   **多智能体Actor-Critic流水线：** THOR引入了TIRGen，这是一个基于多智能体Actor-Critic的流水线，专门用于构建高质量的工具集成推理路径数据集。\n*   **策略对齐与泛化：** TIRGen旨在使生成的数据与策略对齐，并在各种不同的模型中实现良好的泛化能力。\n\n### 2. 分层强化学习优化策略\n\n*   **细粒度优化：** THOR提出了一种强化学习策略，用于执行细粒度的分层优化。\n*   **联合优化：** 该策略同时优化两个关键层面：\n    *   **轨迹级别的问题解决：** 关注整个解题路径的正确性。\n    *   **步骤级别的代码生成：** 优化每个中间步骤中工具调用的代码生成质量。\n*   **核心洞察：** 这一策略的动机源于一个关键洞察——中间工具调用的成功是最终答案正确性的强预测因子。\n\n### 3. 自我纠正机制\n\n*   **动态修正：** THOR在推理过程中整合了一个自我纠正机制。\n*   **即时工具反馈：** 该机制利用即时工具反馈，能够动态地修正错误的推理路径，从而提高推理的鲁棒性和准确性。\n\n### 成果与性能\n\nTHOR方法展现出卓越的性能和广泛的适用性：\n\n*   **强大的泛化能力：** 在不同类型的模型（包括推理模型和非推理模型）上都表现出强大的泛化能力。\n*   **最先进的性能：** 在多个数学基准测试中，对于同等规模的模型，THOR实现了最先进的性能。\n*   **代码基准改进：** 在代码基准测试中也带来了持续的改进。\n\n**代码可用性：** THOR的代码将公开发布。",
      "shortSummary": "THOR是一个基于强化学习的框架，旨在通过工具集成提升大型语言模型（LLMs）的数学推理能力。它通过TIRGen生成高质量工具集成数据，采用分层强化学习策略同时优化解题轨迹和代码生成，并利用自我纠正机制动态修正推理错误。THOR在不同模型上表现出强大的泛化能力，并在数学和代码基准测试中取得了最先进的性能和持续改进。",
      "translated_title": "THOR：基于强化学习的工具集成层次优化，用于数学推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) have made remarkable progress in mathematical reasoning, but still continue to struggle with high-precision tasks like numerical computation and formal symbolic manipulation. Integrating external tools has emerged as a promising approach to bridge this gap. Despite recent advances, existing methods struggle with three key challenges: constructing tool-integrated reasoning data, performing fine-grained optimization, and enhancing inference. To overcome these limitations, we propose THOR (Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen, a multi-agent actor-critic-based pipeline for constructing high-quality datasets of tool-integrated reasoning paths, aligning with the policy and generalizing well across diverse models. Second, to perform fine-grained hierarchical optimization, we introduce an RL strategy that jointly optimizes for both trajectory-level problem solving and step-level code generation. This is motivated by our key insight that the success of an intermediate tool call is a strong predictor of the final answer's correctness. Finally, THOR incorporates a self-correction mechanism that leverages immediate tool feedback to dynamically revise erroneous reasoning paths during inference. Our approach demonstrates strong generalization across diverse models, performing effectively in both reasoning and non-reasoning models. It further achieves state-of-the-art performance for models of a similar scale on multiple mathematical benchmarks, while also delivering consistent improvements on code benchmarks. Our code will be publicly available at https://github.com/JingMog/THOR."
    },
    {
      "title": "擦除它！通过机器遗忘技术擦除代码语言模型中的敏感记忆 (原标题: Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning)",
      "link": "https://arxiv.org/abs/2509.13755",
      "pubDate": "Wed, 17 Sep 2025 03:12:35 GMT",
      "isoDate": "2025-09-17T03:12:35.000Z",
      "creator": "Zhaoyang Chu, Yao Wan, Zhikun Zhang, Di Wang, Zhou Yang, Hongyu Zhang, Pan Zhou, Xuanhua Shi, Hai Jin, David Lo",
      "summary": "## 通过机器遗忘技术擦除代码语言模型中的敏感记忆\n\n### 引言\n\n代码语言模型（CLMs）在软件工程任务中表现出卓越性能，但近期研究揭示了一个关键的隐私漏洞：这些模型会无意中记忆敏感的训练数据，并在特定提示下逐字复现机密信息。为解决此问题，虽然已提出数据去重和差分隐私等方法，但它们需要对已部署的CLMs进行完全模型再训练，这会带来巨大的计算成本。\n\n### 研究问题\n\n本文旨在回答一个核心研究问题：能否有效且高效地擦除CLMs记忆的敏感信息？\n\n### 解决方案：机器遗忘\n\n本文开创性地通过机器遗忘（Machine Unlearning）来解决CLMs中的敏感记忆擦除问题。机器遗忘是一种事后修改方法，它无需完全再训练即可从已训练模型中移除特定信息。\n\n### 方法论\n\n1.  **风险量化与数据集构建**\n    *   首先量化CLMs训练数据集中敏感数据的记忆风险。\n    *   整理了一个包含50,000个敏感记忆样本的高风险数据集，作为遗忘目标。\n2.  **遗忘方法研究**\n    *   研究了两种广泛使用的基于梯度上升的遗忘方法：原始（vanilla）方法和基于约束（constraint-based）的方法。\n3.  **CodeEraser的引入**\n    *   本文引入了CodeEraser，这是一种先进的机器遗忘变体。\n    *   CodeEraser能够选择性地遗忘代码中敏感的记忆片段。\n    *   同时，它能保留周围代码的结构完整性和功能正确性。\n\n### 实验与结果\n\n*   **实验对象：** 在三类CLMs上进行了广泛实验，包括CodeParrot、CodeGen-Mono和Qwen2.5-Coder。\n*   **验证：** 实验结果验证了CodeEraser在擦除目标敏感记忆方面的有效性和效率。\n*   **模型效用：** 实验同时表明，CodeEraser在擦除敏感信息的同时，能保持模型的实用性。\n\n### 结论\n\nCodeEraser提供了一种有效且高效的解决方案，用于在不进行完全再训练的情况下，从代码语言模型中擦除敏感的记忆信息，同时保持模型的性能和实用性。",
      "shortSummary": "代码语言模型（CLMs）存在记忆敏感训练数据的隐私漏洞，现有解决方案成本高昂。本文提出CodeEraser，一种基于机器遗忘的事后修改方法，旨在有效且高效地擦除CLMs中的敏感记忆。CodeEraser通过选择性遗忘敏感代码片段，同时保持模型结构和功能。在CodeParrot、CodeGen-Mono和Qwen2.5-Coder上的实验验证了CodeEraser在擦除目标敏感记忆和保持模型效用方面的有效性。",
      "translated_title": "擦除它！通过机器遗忘技术擦除代码语言模型中的敏感记忆",
      "images": [],
      "contentSource": "完整文章",
      "content": "While Code Language Models (CLMs) have demonstrated superior performance in software engineering tasks such as code generation and summarization, recent empirical studies reveal a critical privacy vulnerability: these models exhibit unintended memorization of sensitive training data, enabling verbatim reproduction of confidential information when specifically prompted. To address this issue, several approaches, including training data de-duplication and differential privacy augmentation, have been proposed. However, these methods require full-model retraining for deployed CLMs, which incurs substantial computational costs. In this paper, we aim to answer the following research question: Can sensitive information memorized by CLMs be erased effectively and efficiently?   We conduct a pioneering investigation into erasing sensitive memorization in CLMs through machine unlearning - a post-hoc modification method that removes specific information from trained models without requiring full retraining. Specifically, we first quantify the memorization risks of sensitive data within CLM training datasets and curate a high-risk dataset of 50,000 sensitive memorized samples as unlearning targets. We study two widely used gradient ascent-based unlearning approaches: the vanilla and constraint-based methods, and introduce CodeEraser, an advanced variant that selectively unlearns sensitive memorized segments in code while preserving the structural integrity and functional correctness of the surrounding code. Extensive experiments on three families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder, validate the effectiveness and efficiency of CodeEraser in erasing targeted sensitive memorization while maintaining model utility."
    }
  ],
  "lastUpdated": "2025-09-21T09:25:39.264Z"
}