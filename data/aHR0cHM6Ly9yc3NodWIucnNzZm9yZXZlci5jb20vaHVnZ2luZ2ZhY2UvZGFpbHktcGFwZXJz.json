{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "VisionThink：通过强化学习实现的智能高效视觉语言模型 (原标题: VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2507.13348",
      "pubDate": "Thu, 17 Jul 2025 13:59:55 GMT",
      "isoDate": "2025-07-17T13:59:55.000Z",
      "creator": "Senqiao Yang, Junyi Li, Xin Lai, Bei Yu, Hengshuang Zhao, Jiaya Jia",
      "summary": "## VisionThink：智能高效视觉语言模型\n\n### 背景与挑战\n\n近期视觉语言模型（VLM）的性能提升通常依赖于增加视觉令牌的数量，这些令牌通常比文本令牌长得多。然而，研究发现，在大多数实际场景中，并不需要如此大量的视觉令牌。尽管在少数OCR（光学字符识别）相关任务中，减少视觉令牌会导致性能显著下降，但在大多数其他通用VQA（视觉问答）任务中，即使分辨率降至1/4，模型仍能保持准确性。\n\n### VisionThink 提出的新范式\n\n针对上述问题，本文提出了 **VisionThink**，一种用于视觉令牌压缩的新范式，旨在动态处理不同分辨率的样本：\n\n*   **动态分辨率处理**：VisionThink 从下采样图像开始处理。\n*   **智能决策**：模型会智能判断下采样图像是否足以解决当前问题。\n*   **按需请求高分辨率**：如果下采样图像不足，模型会输出一个特殊令牌，请求更高分辨率的图像。\n\n### 优势与创新\n\n与现有通过固定剪枝比例或阈值压缩令牌的高效VLM方法不同，VisionThink 能够根据具体情况自主决定是否压缩令牌，从而带来以下优势：\n\n*   **精细视觉理解**：在OCR相关任务上展现出强大的精细视觉理解能力。\n*   **资源节约**：在较简单的任务上显著节省视觉令牌。\n\n### 强化学习与实现\n\nVisionThink 采用了强化学习（RL）技术，并提出了 **LLM-as-Judge** 策略，成功将强化学习应用于通用VQA任务。此外，模型精心设计了奖励函数和惩罚机制，以实现稳定且合理的图像大小调整调用比例。\n\n### 实验结果与可用性\n\n广泛的实验证明了 VisionThink 方法的优越性、效率和有效性。项目的代码和模型已开源。",
      "shortSummary": "VisionThink 是一种智能高效的视觉语言模型，旨在解决现有VLM过度使用视觉令牌的问题。它通过强化学习，动态判断并按需调整图像分辨率，从下采样图像开始，并在必要时请求高分辨率。这种方法在OCR任务上表现出色，同时在简单任务上显著节省视觉令牌，实现了性能与效率的平衡。模型利用“LLM-as-Judge”策略和精心设计的奖励机制来优化图像分辨率调用。",
      "translated_title": "VisionThink：通过强化学习实现的智能高效视觉语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens. However, we observe that most real-world scenarios do not require such an extensive number of visual tokens. While the performance drops significantly in a small subset of OCR-related tasks, models still perform accurately in most other general VQA tasks with only 1/4 resolution. Therefore, we propose to dynamically process distinct samples with different resolutions, and present a new paradigm for visual token compression, namely, VisionThink. It starts with a downsampled image and smartly decides whether it is sufficient for problem solving. Otherwise, the model could output a special token to request the higher-resolution image. Compared to existing Efficient VLM methods that compress tokens using fixed pruning ratios or thresholds, VisionThink autonomously decides whether to compress tokens case by case. As a result, it demonstrates strong fine-grained visual understanding capability on OCR-related tasks, and meanwhile saves substantial visual tokens on simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge strategy to successfully apply RL to general VQA tasks. Moreover, we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio. Extensive experiments demonstrate the superiority, efficiency, and effectiveness of our method. Our code is available at https://github.com/dvlab-research/VisionThink."
    },
    {
      "title": "π^3: 可扩展的置换等变视觉几何学习 (原标题: π^3: Scalable Permutation-Equivariant Visual Geometry Learning)",
      "link": "https://arxiv.org/abs/2507.13347",
      "pubDate": "Thu, 17 Jul 2025 13:59:53 GMT",
      "isoDate": "2025-07-17T13:59:53.000Z",
      "creator": "Yifan Wang, Jianjun Zhou, Haoyi Zhu, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Jiangmiao Pang, Chunhua Shen, Tong He",
      "summary": "## π^3: 视觉几何重建的新范式\n\n### 引言\n\n传统的视觉几何重建方法通常依赖于一个固定的参考视图。这种归纳偏置可能导致模型在参考视图不理想时出现不稳定性和失败。\n\n### π^3 模型介绍\n\n我们引入了 **π^3**，一个前馈神经网络，它为视觉几何重建提供了一种新颖的方法，彻底打破了对传统固定参考视图的依赖。\n\n*   **核心设计：** π^3 采用完全置换等变（permutation-equivariant）的架构。\n*   **预测能力：**\n    *   预测仿射不变的相机姿态（affine-invariant camera poses）。\n    *   预测尺度不变的局部点图（scale-invariant local point maps）。\n*   **关键特点：** 无需任何参考帧，使其设计固有的无偏置。\n\n### 模型优势\n\nπ^3 的设计带来了显著的优势：\n\n*   **鲁棒性：** 对输入顺序具有固有的鲁棒性。\n*   **可扩展性：** 具有高度可扩展性。\n*   **简洁性：** 方法简单且无偏置。\n*   **性能：** 在广泛的任务中实现了最先进的性能。\n\n### 应用领域\n\nπ^3 在以下任务中表现出色：\n\n*   相机姿态估计（camera pose estimation）\n*   单目/视频深度估计（monocular/video depth estimation）\n*   密集点图重建（dense point map reconstruction）\n\n### 可用性\n\n模型的代码和预训练模型已公开提供。",
      "shortSummary": "π^3 是一种创新的前馈神经网络，通过采用完全置换等变架构，彻底改变了视觉几何重建。它摆脱了对固定参考视图的依赖，能够预测仿射不变的相机姿态和尺度不变的局部点图。π^3 对输入顺序具有固有鲁棒性，高度可扩展，并在相机姿态估计、深度估计和密集点图重建等任务中实现了最先进的性能。代码和模型已公开。",
      "translated_title": "π^3: 可扩展的置换等变视觉几何学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce pi^3, a feed-forward neural network that offers a novel approach to visual geometry reconstruction, breaking the reliance on a conventional fixed reference view. Previous methods often anchor their reconstructions to a designated viewpoint, an inductive bias that can lead to instability and failures if the reference is suboptimal. In contrast, pi^3 employs a fully permutation-equivariant architecture to predict affine-invariant camera poses and scale-invariant local point maps without any reference frames. This design makes our model inherently robust to input ordering and highly scalable. These advantages enable our simple and bias-free approach to achieve state-of-the-art performance on a wide range of tasks, including camera pose estimation, monocular/video depth estimation, and dense point map reconstruction. Code and models are publicly available."
    },
    {
      "title": "Diffuman4D: 基于时空扩散模型从稀疏视角视频合成4D一致性人体视图 (原标题: Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models)",
      "link": "https://arxiv.org/abs/2507.13344",
      "pubDate": "Thu, 17 Jul 2025 13:59:17 GMT",
      "isoDate": "2025-07-17T13:59:17.000Z",
      "creator": "Yudong Jin, Sida Peng, Xuan Wang, Tao Xie, Zhen Xu, Yifan Yang, Yujun Shen, Hujun Bao, Xiaowei Zhou",
      "summary": "### Diffuman4D：基于时空扩散模型从稀疏视角视频合成4D一致性人体视图\n\n本文旨在解决从稀疏视角视频输入进行高保真人体视图合成的挑战。现有方法通过利用4D扩散模型生成新视角的视频来解决观测不足的问题，但这些模型生成的视频通常缺乏时空一致性，从而降低了视图合成的质量。\n\n#### 提出的方法：滑动迭代去噪过程\n\n为了增强4D扩散模型的时空一致性，本文提出了一种新颖的滑动迭代去噪过程。其核心机制如下：\n\n1.  **潜在网格定义**：定义一个潜在网格（latent grid），其中每个潜在（latent）编码特定视角和时间戳的图像、相机姿态和人体姿态信息。\n2.  **交替去噪**：使用滑动窗口，沿着空间和时间维度交替地对潜在网格进行去噪。\n3.  **视频解码**：最后，从相应的去噪潜在中解码出目标视角的视频。\n\n#### 方法优势\n\n*   **增强4D一致性**：通过迭代滑动，信息在潜在网格中充分流动，使扩散模型获得大的感受野，从而显著增强了输出的4D一致性。\n*   **内存效率**：同时保持了GPU内存消耗在可承受范围内。\n\n#### 实验结果\n\n在DNA-Rendering和ActorsHQ数据集上的实验表明，本文提出的方法能够合成高质量且一致的新视角视频，并显著优于现有方法。\n\n更多交互式演示和视频结果可在项目页面查看。",
      "shortSummary": "Diffuman4D提出了一种新颖的滑动迭代去噪过程，以解决从稀疏视角视频合成人体视图时，现有4D扩散模型缺乏时空一致性的问题。该方法通过在潜在网格中交替进行空间和时间去噪，增强了4D一致性并优化了内存使用。实验证明，Diffuman4D在DNA-Rendering和ActorsHQ数据集上能合成高质量且一致的新视角视频，显著优于现有方法。",
      "translated_title": "Diffuman4D: 基于时空扩散模型从稀疏视角视频合成4D一致性人体视图",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper addresses the challenge of high-fidelity view synthesis of humans with sparse-view videos as input. Previous methods solve the issue of insufficient observation by leveraging 4D diffusion models to generate videos at novel viewpoints. However, the generated videos from these models often lack spatio-temporal consistency, thus degrading view synthesis quality. In this paper, we propose a novel sliding iterative denoising process to enhance the spatio-temporal consistency of the 4D diffusion model. Specifically, we define a latent grid in which each latent encodes the image, camera pose, and human pose for a certain viewpoint and timestamp, then alternately denoising the latent grid along spatial and temporal dimensions with a sliding window, and finally decode the videos at target viewpoints from the corresponding denoised latents. Through the iterative sliding, information flows sufficiently across the latent grid, allowing the diffusion model to obtain a large receptive field and thus enhance the 4D consistency of the output, while making the GPU memory consumption affordable. The experiments on the DNA-Rendering and ActorsHQ datasets demonstrate that our method is able to synthesize high-quality and consistent novel-view videos and significantly outperforms the existing approaches. See our project page for interactive demos and video results: https://diffuman4d.github.io/ ."
    },
    {
      "title": "大型语言模型上下文工程综述 (原标题: A Survey of Context Engineering for Large Language Models)",
      "link": "https://arxiv.org/abs/2507.13334",
      "pubDate": "Thu, 17 Jul 2025 13:50:36 GMT",
      "isoDate": "2025-07-17T13:50:36.000Z",
      "creator": "Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Baolong Bi, Yujun Cai, Jiazhi Liu, Mingyu Li, Zhong-Zhi Li, Duzhen Zhang, Chenlin Zhou, Jiayi Mao, Tianze Xia, Jiafeng Guo, Shenghua Liu",
      "summary": "# 大型语言模型上下文工程综述\n\n本综述深入探讨了“上下文工程”（Context Engineering）这一新兴学科，它超越了简单的提示设计，旨在系统性地优化提供给大型语言模型（LLMs）的输入信息负载，从而根本性地决定LLMs在推理时的性能。\n\n## 核心概念与分类\n\n文章提出了一个全面的分类体系，将上下文工程分解为基础组件和集成这些组件的复杂系统实现。\n\n### 基础组件\n\n1.  **上下文检索与生成（Context Retrieval and Generation）**：\n    *   涉及如何从大量信息中有效地获取相关上下文，以及如何根据需要生成新的上下文信息。\n2.  **上下文处理（Context Processing）**：\n    *   关注对检索或生成的上下文进行预处理、过滤、压缩、重排等操作，以使其更适合LLM的输入格式和理解能力。\n3.  **上下文管理（Context Management）**：\n    *   涵盖了对上下文生命周期、版本控制、多轮对话中的上下文维护等方面的管理策略。\n\n### 系统实现\n\n这些基础组件被巧妙地集成到以下复杂系统中，以提升LLMs的能力：\n\n1.  **检索增强生成（Retrieval-Augmented Generation, RAG）**：\n    *   通过将外部知识库的检索能力与LLM的生成能力相结合，显著提高了模型输出的准确性和信息量。\n2.  **记忆系统与工具集成推理（Memory Systems and Tool-Integrated Reasoning）**：\n    *   记忆系统使LLM能够存储和回顾长期或短期信息，而工具集成推理则允许LLM调用外部工具（如计算器、API等）来辅助解决复杂问题，扩展其能力边界。\n3.  **多智能体系统（Multi-Agent Systems）**：\n    *   构建由多个LLM或其他AI智能体组成的协作系统，每个智能体可能专注于特定任务或角色，通过交互和协作共同完成复杂目标。\n\n## 研究方法与发现\n\n本综述通过对超过1300篇研究论文进行系统性分析，不仅为该领域建立了技术路线图，还揭示了一个关键的研究空白：\n\n### 关键研究空白：能力不对称性\n\n尽管当前的LLMs在先进上下文工程的增强下，在理解复杂上下文方面表现出卓越的能力，但它们在生成同样复杂、长篇幅的输出方面却存在明显的局限性。这种理解能力与生成能力之间的显著不对称性是未来研究的当务之急。\n\n## 总结与展望\n\n本综述为推进上下文感知型人工智能的研究人员和工程师提供了一个统一的框架。解决LLM在生成复杂长篇输出方面的局限性，将是未来研究的重点。",
      "shortSummary": "本综述介绍了“上下文工程”，一个旨在系统优化大型语言模型（LLMs）输入信息的正式学科。它详细阐述了上下文检索、处理、管理等基础组件，以及检索增强生成（RAG）、记忆系统和多智能体系统等高级实现。通过分析1300多篇论文，综述揭示了LLM在理解复杂上下文方面表现出色，但在生成同样复杂的长篇输出方面存在显著局限性，并指出这是未来研究的关键方向。该综述为上下文感知型AI提供了统一框架。",
      "translated_title": "大型语言模型上下文工程综述",
      "images": [],
      "contentSource": "完整文章",
      "content": "The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1300 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI."
    },
    {
      "title": "模仿游戏：图灵机模仿器是长度泛化推理器 (原标题: The Imitation Game: Turing Machine Imitator is Length Generalizable Reasoner)",
      "link": "https://arxiv.org/abs/2507.13332",
      "pubDate": "Thu, 17 Jul 2025 13:50:07 GMT",
      "isoDate": "2025-07-17T13:50:07.000Z",
      "creator": "Zhouqi Hua, Wenwei Zhang, Chengqi Lyu, Yuzhe Gu, Songyang Gao, Kuikun Liu, Kai Chen",
      "summary": "## 图灵机模仿学习（TAIL）：提升大型语言模型长度泛化能力的新范式\n\n### 引言\n\n*   **核心挑战：** 基于Transformer的大型语言模型（LLM）在长度泛化方面面临严峻挑战，即它们难以解决比训练时所见序列更长的问题。\n*   **现有方法局限：** 当前的数据驱动方法主要集中于算术运算和符号操作，但这些方法通常是任务特定的，且整体性能有限，无法提供通用解决方案。\n\n### TAIL方法概述\n\n*   **研究视角：** 本文旨在寻求更通用的解决方案，将重点放在可计算的推理问题上，即那些可以通过算法解决，从而也能被图灵机解决的问题。\n*   **核心提案：** 提出了一种名为“图灵机模仿学习”（Turing MAchine Imitation Learning, TAIL）的新方法，旨在显著提升LLM的长度泛化能力。\n*   **数据合成机制：** TAIL通过计算机程序合成“思维链”（Chain-of-Thoughts, CoT）数据，这些数据精确模仿了图灵机的执行过程。具体机制包括：\n    *   **线性扩展推理步骤：** 将推理步骤线性地扩展为原子状态，这有助于缓解模型学习“捷径”（shortcut learning）的问题。\n    *   **显式内存获取机制：** 引入显式内存获取机制，以降低在基本操作中进行动态和长距离数据访问的难度。\n\n### 实验验证与结果\n\n*   **数据集构建：** 为验证TAIL的可靠性和普适性，研究团队构建了一个具有挑战性的合成数据集，该数据集涵盖了8类算法和18个具体任务。\n*   **性能表现：** 实验结果显示，TAIL在不依赖额外复杂技巧的情况下，仅使用合成数据就显著提升了Qwen2.5-7B模型在各种任务上的长度泛化能力和整体性能。\n*   **超越现有方法：** TAIL的表现超越了先前的多种方法以及DeepSeek-R1模型。\n\n### 关键发现\n\n*   实验揭示，对于TAIL实现长度泛化而言，图灵机中的**关键概念**（而非其思维风格）是不可或缺的。\n*   模型在注意力层中表现出与图灵机特性高度一致的读写行为，这进一步验证了TAIL方法的有效性。\n\n### 未来展望\n\n*   这项工作为未来从合成数据中学习LLM推理能力提供了一个充满前景的方向。\n\n### 相关信息\n\n*   **主题：** 计算与语言 (cs.CL)\n*   **引用：** arXiv:2507.13332 [cs.CL]",
      "shortSummary": "大型语言模型（LLM）在长度泛化方面面临挑战。本文提出“图灵机模仿学习”（TAIL），通过合成模仿图灵机执行过程的思维链（CoT）数据来解决此问题。TAIL通过将推理步骤原子化并引入显式内存机制，显著提升了Qwen2.5-7B等模型的长度泛化能力和性能，超越现有方法。研究表明，图灵机的核心概念对泛化至关重要。这项工作为LLM从合成数据中学习推理提供了新方向。",
      "translated_title": "模仿游戏：图灵机模仿器是长度泛化推理器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Length generalization, the ability to solve problems of longer sequences than those observed during training, poses a core challenge of Transformer-based large language models (LLM). Although existing studies have predominantly focused on data-driven approaches for arithmetic operations and symbolic manipulation tasks, these approaches tend to be task-specific with limited overall performance. To pursue a more general solution, this paper focuses on a broader case of reasoning problems that are computable, i.e., problems that algorithms can solve, thus can be solved by the Turing Machine. From this perspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to improve the length generalization ability of LLMs. TAIL synthesizes chain-of-thoughts (CoT) data that imitate the execution process of a Turing Machine by computer programs, which linearly expands the reasoning steps into atomic states to alleviate shortcut learning and explicit memory fetch mechanism to reduce the difficulties of dynamic and long-range data access in elementary operations. To validate the reliability and universality of TAIL, we construct a challenging synthetic dataset covering 8 classes of algorithms and 18 tasks. Without bells and whistles, TAIL significantly improves the length generalization ability as well as the performance of Qwen2.5-7B on various tasks using only synthetic data, surpassing previous methods and DeepSeek-R1. The experimental results reveal that the key concepts in the Turing Machine, instead of the thinking styles, are indispensable for TAIL for length generalization, through which the model exhibits read-and-write behaviors consistent with the properties of the Turing Machine in their attention layers. This work provides a promising direction for future research in the learning of LLM reasoning from synthetic data."
    },
    {
      "title": "AbGen：评估大型语言模型在科学研究中消融研究设计与评估的能力 (原标题: AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research)",
      "link": "https://arxiv.org/abs/2507.13300",
      "pubDate": "Thu, 17 Jul 2025 13:09:22 GMT",
      "isoDate": "2025-07-17T13:09:22.000Z",
      "creator": "Yilun Zhao, Weiyuan Chen, Zhijian Xu, Manasi Patwardhan, Yixin Liu, Chengye Wang, Lovekesh Vig, Arman Cohan",
      "summary": "### AbGen：评估大型语言模型在消融研究设计与评估中的能力\n\n本文介绍了 **AbGen**，这是首个旨在评估大型语言模型（LLMs）在科学研究中设计消融研究能力的基准。\n\n#### AbGen 基准概述\n*   **目的**：评估LLMs在为科学研究设计消融研究方面的能力。\n*   **构成**：包含1,500个由专家标注的示例，这些示例来源于807篇自然语言处理（NLP）领域的论文。\n*   **任务设定**：LLMs的任务是根据给定的研究背景，为一个指定的模块或过程生成详细的消融研究设计。\n\n#### LLM 性能评估\n*   **评估对象**：对DeepSeek-R1-0528和o4-mini等领先LLMs进行了评估。\n*   **发现**：这些模型在消融研究设计的重要性、忠实性和合理性方面，与人类专家之间存在显著的性能差距。\n\n#### 自动化评估方法的挑战\n*   **问题**：当前常用的自动化评估方法对于此任务并不可靠。\n*   **表现**：与人类评估相比，自动化方法显示出显著的不一致性。\n\n#### AbGen-Eval：元评估基准\n*   **开发目的**：为了更好地探究自动化评估的可靠性，研究人员开发了 **AbGen-Eval**。\n*   **功能**：AbGen-Eval是一个元评估基准，旨在评估常用自动化评估系统在衡量LLM在此任务上表现时的可靠性。\n*   **研究方向**：研究人员在AbGen-Eval上调查了各种“LLM即评判者”（LLM-as-Judge）系统。\n*   **未来展望**：这项工作为未来开发更有效、更可靠的、基于LLM的复杂科学任务评估系统提供了见解。\n\n#### 其他信息\n*   **会议**：ACL 2025\n*   **主题**：计算与语言 (cs.CL); 人工智能 (cs.AI)",
      "shortSummary": "AbGen是一个新基准，用于评估大型语言模型（LLMs）在科学研究中设计消融研究的能力，包含1,500个专家标注示例。评估显示，领先LLMs在设计消融研究方面与人类专家存在显著差距。此外，现有自动化评估方法被发现不可靠。为解决此问题，研究人员开发了AbGen-Eval元评估基准，旨在评估自动化评估系统的可靠性，为未来开发更有效的LLM评估系统提供指导。",
      "translated_title": "AbGen：评估大型语言模型在科学研究中消融研究设计与评估的能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce AbGen, the first benchmark designed to evaluate the capabilities of LLMs in designing ablation studies for scientific research. AbGen consists of 1,500 expert-annotated examples derived from 807 NLP papers. In this benchmark, LLMs are tasked with generating detailed ablation study designs for a specified module or process based on the given research context. Our evaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a significant performance gap between these models and human experts in terms of the importance, faithfulness, and soundness of the ablation study designs. Moreover, we demonstrate that current automated evaluation methods are not reliable for our task, as they show a significant discrepancy when compared to human assessment. To better investigate this, we develop AbGen-Eval, a meta-evaluation benchmark designed to assess the reliability of commonly used automated evaluation systems in measuring LLM performance on our task. We investigate various LLM-as-Judge systems on AbGen-Eval, providing insights for future research on developing more effective and reliable LLM-based evaluation systems for complex scientific tasks."
    },
    {
      "title": "通过残差学习使现有稀疏自编码器掌握新的领域知识 (原标题: Teach Old SAEs New Domain Tricks with Boosting)",
      "link": "https://arxiv.org/abs/2507.12990",
      "pubDate": "Thu, 17 Jul 2025 06:57:49 GMT",
      "isoDate": "2025-07-17T06:57:49.000Z",
      "creator": "Nikita Koriagin, Yaroslav Aksenov, Daniil Laptev, Gleb Gerasimov, Nikita Balagansky, Daniil Gavrilov",
      "summary": "# 论文摘要：通过残差学习使现有稀疏自编码器掌握新的领域知识\n\n## 核心问题\n*   稀疏自编码器（Sparse Autoencoders, SAEs）已被证明是解释大型语言模型（Large Language Models, LLMs）内部表示的强大工具。\n*   然而，它们通常难以捕获在其原始训练语料库中不普遍的领域特定特征，导致在特定领域上的解释能力受限。\n\n## 提出的方法：残差学习\n*   本文引入了一种创新的残差学习方法，旨在解决SAEs的“特征盲区”，且无需对整个模型进行耗时且资源密集型的完全重新训练。\n*   **核心思想：** 训练一个**辅助稀疏自编码器（secondary SAE）**。\n    *   该辅助SAE专门用于建模**预训练主稀疏自编码器（pretrained primary SAE）**在**领域特定文本**上的**重建误差**。\n    *   通过这种方式，辅助SAE能够有效地捕获并学习主模型在处理特定领域数据时所遗漏的特征。\n*   **推理阶段：** 在推理时，将预训练主SAE和新训练的辅助SAE的输出相加，以获得更全面的特征表示。\n\n## 实验结果与优势\n*   **性能提升：** 实验证明，该方法在多个专业领域显著改善了LLM的交叉熵（cross-entropy）和解释方差（explained variance）指标。\n*   **效率：** 这种方法能够高效地将新的领域知识融入现有SAEs，而无需进行代价高昂的完全重训练。\n*   **通用性保持：** 在融入新领域知识的同时，该方法能够有效保持SAEs在通用任务上的原有性能，避免了“灾难性遗忘”问题。\n\n## 意义与展望\n*   这种残差学习方法使研究人员能够选择性地增强SAEs对特定感兴趣领域的解释能力。\n*   它为LLMs的靶向机械可解释性（targeted mechanistic interpretability）开辟了新的可能性，有助于更深入地理解LLM在特定应用场景下的内部工作机制。",
      "shortSummary": "稀疏自编码器（SAEs）在解释大型语言模型时，常难以捕获领域特定特征。本文提出一种残差学习方法：训练一个辅助SAE来建模预训练SAE在领域文本上的重建误差。通过结合两者的输出，该方法显著提升了LLM在专业领域的性能，并能高效地将新领域知识融入现有SAEs，无需完全重训练，从而增强了LLM的靶向可解释性。",
      "translated_title": "通过残差学习使现有稀疏自编码器掌握新的领域知识",
      "images": [],
      "contentSource": "完整文章",
      "content": "Sparse Autoencoders have emerged as powerful tools for interpreting the internal representations of Large Language Models, yet they often fail to capture domain-specific features not prevalent in their training corpora. This paper introduces a residual learning approach that addresses this feature blindness without requiring complete retraining. We propose training a secondary SAE specifically to model the reconstruction error of a pretrained SAE on domain-specific texts, effectively capturing features missed by the primary model. By summing the outputs of both models during inference, we demonstrate significant improvements in both LLM cross-entropy and explained variance metrics across multiple specialized domains. Our experiments show that this method efficiently incorporates new domain knowledge into existing SAEs while maintaining their performance on general tasks. This approach enables researchers to selectively enhance SAE interpretability for specific domains of interest, opening new possibilities for targeted mechanistic interpretability of LLMs."
    },
    {
      "title": "FantasyPortrait：使用表情增强型扩散Transformer增强多角色肖像动画 (原标题: FantasyPortrait: Enhancing Multi-Character Portrait Animation with Expression-Augmented Diffusion Transformers)",
      "link": "https://arxiv.org/abs/2507.12956",
      "pubDate": "Thu, 17 Jul 2025 05:50:43 GMT",
      "isoDate": "2025-07-17T05:50:43.000Z",
      "creator": "Qiang Wang, Mengchao Wang, Fan Jiang, Yaqi Fan, Yonggang Qi, Mu Xu",
      "summary": "## FantasyPortrait：使用表情增强型扩散Transformer增强多角色肖像动画\n\n本文提出了一种名为 **FantasyPortrait** 的框架，旨在解决从静态图像生成富有表现力的面部动画的挑战，尤其是在多角色场景中。\n\n### 当前挑战：\n*   **面部动画生成困难：** 现有方法依赖于显式几何先验（如面部标志点或3DMM），常导致交叉重演中的伪影，并且难以捕捉细微情感。\n*   **缺乏多角色支持：** 不同个体驱动特征之间常相互干扰，使得多角色动画难以实现。\n\n### FantasyPortrait 的创新解决方案：\nFantasyPortrait 是一个基于扩散Transformer的框架，能够为单角色和多角色场景生成高保真且情感丰富的动画。其核心创新包括：\n\n*   **表情增强学习策略：**\n    *   利用隐式表示来捕捉与身份无关的面部动态。\n    *   增强模型渲染细粒度情感的能力。\n*   **掩码交叉注意力机制（针对多角色控制）：**\n    *   确保独立但协调的表情生成。\n    *   有效防止特征干扰。\n\n### 对研究领域的贡献：\n为了推动该领域的研究，作者提出了：\n*   **Multi-Expr 数据集：** 专门设计用于多角色肖像动画的训练。\n*   **ExprBench 基准：** 专门设计用于多角色肖像动画的评估。\n\n### 实验结果：\n广泛的实验表明，FantasyPortrait 在定量指标和定性评估方面均显著优于现有最先进的方法，尤其在具有挑战性的交叉重演和多角色情境中表现出色。",
      "shortSummary": "FantasyPortrait 是一种基于扩散Transformer的框架，旨在解决从静态图像生成富有表现力的单/多角色面部动画的挑战。它引入了表情增强学习策略，利用隐式表示捕捉细微情感，并通过掩码交叉注意力机制实现独立协调的多角色表情生成，有效避免特征干扰。该方法还提出了Multi-Expr数据集和ExprBench基准。实验证明，FantasyPortrait在交叉重演和多角色动画方面显著优于现有技术。",
      "translated_title": "FantasyPortrait：使用表情增强型扩散Transformer增强多角色肖像动画",
      "images": [],
      "contentSource": "完整文章",
      "content": "Producing expressive facial animations from static images is a challenging task. Prior methods relying on explicit geometric priors (e.g., facial landmarks or 3DMM) often suffer from artifacts in cross reenactment and struggle to capture subtle emotions. Furthermore, existing approaches lack support for multi-character animation, as driving features from different individuals frequently interfere with one another, complicating the task. To address these challenges, we propose FantasyPortrait, a diffusion transformer based framework capable of generating high-fidelity and emotion-rich animations for both single- and multi-character scenarios. Our method introduces an expression-augmented learning strategy that utilizes implicit representations to capture identity-agnostic facial dynamics, enhancing the model's ability to render fine-grained emotions. For multi-character control, we design a masked cross-attention mechanism that ensures independent yet coordinated expression generation, effectively preventing feature interference. To advance research in this area, we propose the Multi-Expr dataset and ExprBench, which are specifically designed datasets and benchmarks for training and evaluating multi-character portrait animations. Extensive experiments demonstrate that FantasyPortrait significantly outperforms state-of-the-art methods in both quantitative metrics and qualitative evaluations, excelling particularly in challenging cross reenactment and multi-character contexts. Our project page is https://fantasy-amap.github.io/fantasy-portrait/."
    },
    {
      "title": "AnyCap 项目：一个用于可控全模态图像描述的统一框架、数据集和基准 (原标题: AnyCap Project: A Unified Framework, Dataset, and Benchmark for Controllable Omni-modal Captioning)",
      "link": "https://arxiv.org/abs/2507.12841",
      "pubDate": "Thu, 17 Jul 2025 03:04:05 GMT",
      "isoDate": "2025-07-17T03:04:05.000Z",
      "creator": "Yiming Ren, Zhiqiang Lin, Yu Li, Gao Meng, Weiyun Wang, Junjie Wang, Zicheng Lin, Jifeng Dai, Yujiu Yang, Wenhai Wang, Ruihang Chu",
      "summary": "# AnyCap 项目：可控全模态图像描述的统一解决方案\n\n可控图像描述对于精确的多模态对齐和指令遵循至关重要。然而，现有模型通常缺乏细粒度控制和可靠的评估协议。为解决这一差距，研究人员提出了 **AnyCap 项目**，这是一个集模型、数据集和评估于一体的综合解决方案。\n\n## AnyCapModel (ACM)\n\nAnyCapModel (ACM) 是一种轻量级的即插即用框架，旨在增强现有基础模型在全模态图像描述方面的可控性，而无需重新训练基础模型。ACM 通过重用基础模型的原始描述，并结合用户指令和模态特征，生成改进的描述。\n\n## AnyCapDataset (ACD)\n\n为了弥补可控多模态图像描述领域的数据稀缺问题，AnyCap 项目构建了 **AnyCapDataset (ACD)**。该数据集涵盖了三种模态、28种用户指令类型，并包含30万条高质量数据条目。\n\n## AnyCapEval\n\nAnyCap 项目进一步提出了 **AnyCapEval**，这是一个新的基准测试，通过解耦内容准确性和风格保真度，为可控图像描述提供了更可靠的评估指标。\n\n## 关键成果\n\n*   AnyCapModel (ACM) 在 AnyCapEval 上显著提升了各种基础模型的描述质量。\n*   值得注意的是，ACM-8B 将 GPT-4o 的内容得分提高了45%，风格得分提高了12%。\n*   ACM 还在 MIA-Bench 和 VidCapBench 等广泛使用的基准测试中取得了显著的性能提升。",
      "shortSummary": "AnyCap 项目提出了一个用于可控全模态图像描述的统一解决方案。它包括：AnyCapModel (ACM)，一个无需重训即可增强基础模型可控性的轻量级框架；AnyCapDataset (ACD)，一个包含30万条高质量数据的多模态数据集；以及AnyCapEval，一个解耦内容和风格以提供可靠评估的新基准。ACM显著提升了描述质量，例如将GPT-4o的内容得分提高45%，风格得分提高12%。",
      "translated_title": "AnyCap 项目：一个用于可控全模态图像描述的统一框架、数据集和基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Controllable captioning is essential for precise multimodal alignment and instruction following, yet existing models often lack fine-grained control and reliable evaluation protocols. To address this gap, we present the AnyCap Project, an integrated solution spanning model, dataset, and evaluation. We introduce AnyCapModel (ACM), a lightweight plug-and-play framework that enhances the controllability of existing foundation models for omni-modal captioning without retraining the base model. ACM reuses the original captions from base models while incorporating user instructions and modality features to generate improved captions. To remedy the data scarcity in controllable multimodal captioning, we build AnyCapDataset (ACD), covering three modalities, 28 user-instruction types, and 300\\,k high-quality data entries. We further propose AnyCapEval, a new benchmark that provides more reliable evaluation metrics for controllable captioning by decoupling content accuracy and stylistic fidelity. ACM markedly improves caption quality across a diverse set of base models on AnyCapEval. Notably, ACM-8B raises GPT-4o\\'s content scores by 45\\% and style scores by 12\\%, and it also achieves substantial gains on widely used benchmarks such as MIA-Bench and VidCapBench."
    },
    {
      "title": "FLEXITOKENS：面向演化语言模型的灵活分词 (原标题: FLEXITOKENS: Flexible Tokenization for Evolving Language Models)",
      "link": "https://arxiv.org/abs/2507.12720",
      "pubDate": "Wed, 16 Jul 2025 21:55:41 GMT",
      "isoDate": "2025-07-16T21:55:41.000Z",
      "creator": "Abraham Toluase Owodunni, Orevaoghene Ahia, Sachin Kumar",
      "summary": "## FLEXITOKENS：面向演化语言模型的灵活分词\n\n### 摘要\n\n本文介绍了FLEXITOKENS，一种旨在解决现有语言模型（LMs）在适应新数据分布时面临的挑战的创新方法。这些挑战主要源于其子词分词器的僵化性，导致在处理域外数据、未见语言或脚本时出现低效的分词和过度碎片化。\n\n### 核心问题\n\n*   **分词器僵化性**：传统的语言模型在适应新数据分布时面临困难，因为它们的子词分词器在微调过程中通常保持不变。\n*   **过度碎片化**：这种僵化性导致在处理域外数据、未见语言或脚本时，分词效率低下，出现“过度碎片化”现象。\n\n### 现有方法的局限性\n\n*   现有的无分词器方法（如字节级语言模型）通过学习边界预测器来工作，但它们通常使用辅助损失来强制在整个训练语料库中保持固定的压缩率，这引入了另一种形式的僵化。\n\n### FLEXITOKENS 解决方案\n\n*   **可学习的分词器**：FLEXITOKENS 提出了一种带有可学习分词器的字节级语言模型。\n*   **边界预测子模块**：模型包含一个子模块，该子模块学习预测输入字节序列之间的边界，并将其编码成可变长度的片段。\n*   **简化的训练目标**：FLEXITOKENS 的核心创新在于其简化的训练目标，这在适应过程中提供了显著更大的灵活性，克服了现有方法的僵化性。\n\n### 评估与成果\n\n*   **广泛评估**：FLEXITOKENS 在多个多语言基准、形态多样的任务和不同领域进行了评估。\n*   **减少过度碎片化**：实验结果表明，FLEXITOKENS 持续有效地减少了词元（token）的过度碎片化。\n*   **性能提升**：与子词分词器和其他基于梯度的分词器相比，FLEXITOKENS 在下游任务性能上实现了高达10%的改进。",
      "shortSummary": "语言模型因分词器僵化而难以适应新数据，导致过度碎片化。FLEXITOKENS提出一种灵活的、可学习的字节级分词器，采用简化的训练目标。该方法显著减少了词元过度碎片化，并在多语言和形态任务中将下游性能提升高达10%，增强了语言模型的适应性。",
      "translated_title": "FLEXITOKENS：面向演化语言模型的灵活分词",
      "images": [],
      "contentSource": "完整文章",
      "content": "Language models (LMs) are challenging to adapt to new data distributions by simple finetuning. This is due to the rigidity of their subword tokenizers, which typically remain unchanged during adaptation. This inflexibility often leads to inefficient tokenization, causing overfragmentation of out-of-distribution domains, unseen languages, or scripts. In this work, we develop byte-level LMs with learnable tokenizers to make tokenization adaptive. Our models include a submodule that learns to predict boundaries between the input byte sequence, encoding it into variable-length segments. Existing tokenizer-free methods train this boundary predictor using an auxiliary loss that enforces a fixed compression rate across the training corpus, introducing a new kind of rigidity. We propose FLEXITOKENS, a simplified training objective that enables significantly greater flexibility during adaptation. Evaluating across multiple multilingual benchmarks, morphologically diverse tasks, and domains, we demonstrate that FLEXITOKENS consistently reduces token over-fragmentation and achieves up to 10\\% improvements on downstream task performance compared to subword and other gradient-based tokenizers. Code and data for our experiments will be released at https://github.com/owos/flexitokens"
    },
    {
      "title": "MindJourney：利用世界模型进行空间推理的测试时扩展 (原标题: MindJourney: Test-Time Scaling with World Models for Spatial Reasoning)",
      "link": "https://arxiv.org/abs/2507.12508",
      "pubDate": "Wed, 16 Jul 2025 13:59:36 GMT",
      "isoDate": "2025-07-16T13:59:36.000Z",
      "creator": "Yuncong Yang, Jiageng Liu, Zheyuan Zhang, Siyuan Zhou, Reuben Tan, Jianwei Yang, Yilun Du, Chuang Gan",
      "summary": "## MindJourney：利用世界模型进行空间推理的测试时扩展\n\n### 核心问题\n\n*   **现有视觉-语言模型（VLMs）的局限性**：当前最先进的VLMs在3D空间推理方面表现不佳，尤其是在预测以自我为中心的运动后场景将如何变化的任务上。它们主要感知2D图像，缺乏对3D动态的内部模型。\n\n### 解决方案：MindJourney框架\n\n*   **框架概述**：MindJourney是一个测试时扩展框架，旨在通过将VLM与一个基于视频扩散的可控世界模型耦合，来弥补VLM在3D动态建模方面的缺失能力。\n\n*   **工作原理**：\n    *   VLM迭代地勾勒出一条简洁的相机轨迹。\n    *   世界模型在每一步合成相应的视图。\n    *   VLM随后对在交互式探索过程中收集到的多视图证据进行推理。\n\n### 主要成果与优势\n\n*   **无需微调**：MindJourney无需任何微调即可运行。\n*   **性能提升**：在代表性的空间推理基准SAT上，MindJourney平均实现了超过8%的性能提升。\n*   **简单即插即用**：该方法提供了一种简单、即插即用的途径，以实现鲁棒的3D推理。\n*   **超越现有方法**：MindJourney在测试时推理方面也优于通过强化学习训练的VLM，这表明了其利用世界模型进行测试时扩展的巨大潜力。",
      "shortSummary": "MindJourney是一个创新的测试时扩展框架，旨在解决视觉-语言模型（VLMs）在3D空间推理上的不足。它通过将VLM与基于视频扩散的世界模型结合，使VLM能够迭代规划相机轨迹并从世界模型合成的视图中进行推理。该方法无需微调，在SAT空间推理基准上实现了平均8%以上的性能提升，提供了一种简单、即插即用的3D推理解决方案，并优于通过强化学习训练的VLM。",
      "translated_title": "MindJourney：利用世界模型进行空间推理的测试时扩展",
      "images": [],
      "contentSource": "完整文章",
      "content": "Spatial reasoning in 3D space is central to human cognition and indispensable for embodied tasks such as navigation and manipulation. However, state-of-the-art vision-language models (VLMs) struggle frequently with tasks as simple as anticipating how a scene will look after an egocentric motion: they perceive 2D images but lack an internal model of 3D dynamics. We therefore propose MindJourney, a test-time scaling framework that grants a VLM with this missing capability by coupling it to a controllable world model based on video diffusion. The VLM iteratively sketches a concise camera trajectory, while the world model synthesizes the corresponding view at each step. The VLM then reasons over this multi-view evidence gathered during the interactive exploration. Without any fine-tuning, our MindJourney achieves over an average 8% performance boost on the representative spatial reasoning benchmark SAT, showing that pairing VLMs with world models for test-time scaling offers a simple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also improves upon the test-time inference VLMs trained through reinforcement learning, which demonstrates the potential of our method that utilizes world models for test-time scaling."
    },
    {
      "title": "EXAONE 4.0：整合非推理和推理模式的统一大型语言模型 (原标题: EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes)",
      "link": "https://arxiv.org/abs/2507.11407",
      "pubDate": "Tue, 15 Jul 2025 11:24:51 GMT",
      "isoDate": "2025-07-15T11:24:51.000Z",
      "creator": "LG AI Research, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Yemuk Choi, Kyubeen Han, Seokhee Hong, Junwon Hwang, Taewan Hwang, Joonwon Jang, Hyojin Jeon, Kijeong Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Euisoon Kim, Hyosang Kim, Jihoon Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Gwangho Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Young Min Paik, Yongmin Park, Youngyong Park, Sanghyun Seo, Sihoon Yang, Heuiyeen Yeen, Sihyuk Yi, Hyeongu Yun",
      "summary": "# EXAONE 4.0：统一大型语言模型技术报告概述\n\n本技术报告介绍了由LG AI Research开发的EXAONE 4.0大型语言模型。EXAONE 4.0旨在通过整合“非推理模式”和“推理模式”，同时实现EXAONE 3.5卓越的可用性以及EXAONE Deep先进的推理能力。\n\n## 核心创新与目标\n*   **模式整合**：EXAONE 4.0的核心创新在于其统一架构，将非推理能力与强大的推理能力相结合，以提供更全面的AI解决方案。\n*   **代理AI时代准备**：为迎接代理AI时代的到来，EXAONE 4.0融入了关键的智能体工具使用（agentic tool use）功能。\n\n## 主要特性\n*   **多语言能力扩展**：除了原有的英语和韩语，EXAONE 4.0的多语言能力已扩展至支持西班牙语。\n*   **模型尺寸与应用**：\n    *   **中型模型 (32B)**：该版本经过优化，旨在提供高性能表现。\n    *   **小型模型 (1.2B)**：该版本专为设备端（on-device）应用设计，适用于资源受限的环境。\n\n## 性能表现\nEXAONE 4.0在性能上展现出显著优势：\n*   **超越同类开源模型**：与同级别的开源模型相比，EXAONE 4.0展现出卓越的性能。\n*   **与前沿模型竞争**：即使面对前沿级（frontier-class）模型，EXAONE 4.0也保持了强大的竞争力。\n\n## 可用性与研究\nEXAONE 4.0模型系列已向公众开放，供研究目的使用。研究人员可以通过提供的链接轻松下载这些模型。\n\n## 报告信息\n*   **作者**：LG AI Research团队（包括Kyunghoon Bae, Eunbi Choi等众多研究人员）。\n*   **类型**：技术报告，共30页。\n*   **主题**：计算与语言（cs.CL）；人工智能（cs.AI）。\n*   **引用方式**：arXiv:2507.11407。\n*   **提交日期**：2025年7月15日。",
      "shortSummary": "EXAONE 4.0是LG AI Research推出的大型语言模型，整合了非推理和推理模式。它具备智能体工具使用能力，并支持英语、韩语和西班牙语。该模型提供32B（高性能）和1.2B（设备端）两种尺寸，性能优于同类开源模型，并能与前沿模型竞争。EXAONE 4.0已公开供研究使用。",
      "translated_title": "EXAONE 4.0：整合非推理和推理模式的统一大型语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "This technical report introduces EXAONE 4.0, which integrates a Non-reasoning mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5 and the advanced reasoning abilities of EXAONE Deep. To pave the way for the agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool use, and its multilingual capabilities are extended to support Spanish in addition to English and Korean. The EXAONE 4.0 model series consists of two sizes: a mid-size 32B model optimized for high performance, and a small-size 1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates superior performance compared to open-weight models in its class and remains competitive even against frontier-class models. The models are publicly available for research purposes and can be easily downloaded via https://huggingface.co/LGAI-EXAONE."
    },
    {
      "title": "多模态基础模型能否理解示意图？一项关于科学论文信息检索问答的实证研究 (原标题: Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers)",
      "link": "https://arxiv.org/abs/2507.10787",
      "pubDate": "Mon, 14 Jul 2025 16:35:25 GMT",
      "isoDate": "2025-07-14T16:35:25.000Z",
      "creator": "Yilun Zhao, Chengye Wang, Chuhan Li, Arman Cohan",
      "summary": "### MISS-QA：评估多模态基础模型对科学示意图的理解能力\n\n本文介绍了一个名为MISS-QA的新基准，该基准专门用于评估模型解释科学文献中示意图的能力。\n\n**基准概述：**\n*   **设计目的：** 专门评估模型对科学文献中示意图的理解能力。\n*   **构成：** 包含来自465篇科学论文的1,500个由专家标注的示例。\n*   **任务设置：** 模型需要解释说明研究概览的示意图，并根据论文的更广泛上下文回答相应的信息检索问题。\n\n**模型评估与发现：**\n*   **评估对象：** 评估了18个前沿的多模态基础模型，包括o4-mini、Gemini-2.5-Flash和Qwen2.5-VL。\n*   **主要发现：** 在MISS-QA基准上，这些模型的表现与人类专家之间存在显著的性能差距。\n*   **深入分析：**\n    *   对模型在无法回答问题上的表现进行了分析。\n    *   进行了详细的错误分析。\n    *   这些分析突出了当前模型的优势和局限性。\n*   **研究意义：** 为增强模型理解多模态科学文献的能力提供了关键见解。\n\n**相关信息：**\n*   **会议：** ACL 2025 Findings\n*   **研究领域：** 计算与语言 (cs.CL); 计算机视觉与模式识别 (cs.CV)\n*   **引用信息：** arXiv:2507.10787 [cs.CL]",
      "shortSummary": "本文引入了MISS-QA，一个旨在评估多模态基础模型理解科学论文中示意图能力的基准。该研究测试了18个前沿模型，发现它们在解释示意图并回答相关信息检索问题方面与人类专家存在显著差距。详细的错误分析揭示了当前模型的优势与局限，为提升模型对多模态科学文献的理解提供了重要见解。",
      "translated_title": "多模态基础模型能否理解示意图？一项关于科学论文信息检索问答的实证研究",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper introduces MISS-QA, the first benchmark specifically designed to evaluate the ability of models to interpret schematic diagrams within scientific literature. MISS-QA comprises 1,500 expert-annotated examples over 465 scientific papers. In this benchmark, models are tasked with interpreting schematic diagrams that illustrate research overviews and answering corresponding information-seeking questions based on the broader context of the paper. We assess the performance of 18 frontier multimodal foundation models, including o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant performance gap between these models and human experts on MISS-QA. Our analysis of model performance on unanswerable questions and our detailed error analysis further highlight the strengths and limitations of current models, offering key insights to enhance models in comprehending multimodal scientific literature."
    },
    {
      "title": "EmbRACE-3K：复杂环境中的具身推理与行动 (原标题: EmbRACE-3K: Embodied Reasoning and Action in Complex Environments)",
      "link": "https://arxiv.org/abs/2507.10548",
      "pubDate": "Mon, 14 Jul 2025 13:59:46 GMT",
      "isoDate": "2025-07-14T13:59:46.000Z",
      "creator": "Mingxian Lin, Wei Huang, Yitang Li, Chengjie Jiang, Kui Wu, Fangwei Zhong, Shengju Qian, Xin Wang, Xiaojuan Qi",
      "summary": "## EmbRACE-3K：复杂环境中的具身推理与行动\n\n### 引言\n\n近期先进的视觉-语言模型（VLMs）在被动、离线的图像和视频理解任务中展现出强大的性能。然而，它们在具身设置中的有效性仍然有限，这类设置需要在线交互和主动的场景理解。在具身场景中，智能体从第一人称视角感知环境，每个动作都会动态地影响后续的观察。即使是GPT-4o、Claude 3.5 Sonnet和Gemini 2.5 Pro等最先进的模型，在开放环境交互中也表现出明显的局限性，尤其是在空间推理和长程规划方面。\n\n### 解决方案：引入EmRACE-3K数据集\n\n为了弥补这一差距，我们引入了EmRACE-3K数据集，它包含3000多个语言引导任务，这些任务设置在利用虚幻引擎（Unreal Engine）和UnrealCV-Zoo框架构建的各种逼真环境中。\n\n*   **任务范围广泛**：数据集中的任务涵盖了广泛的具身挑战，包括导航、物体操作和多阶段目标执行。\n*   **多步轨迹**：每个任务都以多步轨迹的形式展开，将第一人称视觉观察与高级指令、具身动作以及在每一步表达智能体意图的自然语言理由配对。\n\n### 基准评估\n\n我们利用EmRACE-3K建立了一个基准，以评估VLMs在三个关键维度上的具身推理能力：\n\n1.  **探索（Exploration）**\n2.  **动态空间-语义推理（Dynamic Spatial-Semantic Reasoning）**\n3.  **多阶段目标执行（Multi-stage Goal Execution）**\n\n在零样本（zero-shot）设置下，所有模型的成功率均低于20%，这突显了我们基准测试所带来的挑战以及当前VLMs在交互式环境中的局限性。\n\n### 数据集效用演示\n\n为了展示EmRACE-3K的实用性，我们进一步通过监督学习（supervised learning）和强化学习（reinforcement learning）对Qwen2.5-VL-7B进行了微调。这种方法在所有三个挑战类别中都取得了显著的改进，突显了该数据集在促进具身推理能力发展方面的有效性。",
      "shortSummary": "文章介绍了EmRACE-3K数据集，旨在解决现有视觉-语言模型（VLMs）在复杂具身环境中具身推理和行动能力不足的问题。该数据集包含3000多个语言引导任务，涵盖导航、物体操作和多阶段目标执行。基准测试显示，VLMs在零样本设置下成功率低于20%。通过EmRACE-3K对模型进行微调，能显著提升其在探索、动态空间-语义推理和多阶段目标执行方面的表现，证明了数据集在发展具身推理能力方面的有效性。",
      "translated_title": "EmbRACE-3K：复杂环境中的具身推理与行动",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advanced vision-language models(VLMs) have demonstrated strong performance on passive, offline image and video understanding tasks. However, their effectiveness in embodied settings, which require online interaction and active scene understanding remains limited. In such scenarios, an agent perceives the environment from a first-person perspective, with each action dynamically shaping subsequent observations. Even state-of-the-art models such as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment interactions, exhibiting clear limitations in spatial reasoning and long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset of over 3,000 language-guided tasks situated in diverse, photorealistic environments constructed using Unreal Engine and the UnrealCV-Zoo framework. The tasks encompass a wide range of embodied challenges, including navigation, object manipulation, and multi-stage goal execution. Each task unfolds as a multi-step trajectory, pairing first-person visual observations with high-level instructions, grounded actions, and natural language rationales that express the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to evaluate the embodied reasoning capabilities of VLMs across three key dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage Goal Execution. In zero-shot settings, all models achieve success rates below 20%, underscoring the challenge posed by our benchmark and the current limitations of VLMs in interactive environments. To demonstrate the utility of EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning followed by reinforcement learning. This approach yields substantial improvements across all three challenge categories, highlighting the dataset's effectiveness in enabling the development of embodied reasoning capabilities."
    },
    {
      "title": "REST：通过同时提出多个问题对大型推理模型进行压力测试 (原标题: REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once)",
      "link": "https://arxiv.org/abs/2507.10541",
      "pubDate": "Mon, 14 Jul 2025 13:58:47 GMT",
      "isoDate": "2025-07-14T13:58:47.000Z",
      "creator": "Zhuoshi Pan, Qizhi Pei, Yu Li, Qiyao Sun, Zinan Tang, H. Vicky Zhao, Conghui He, Lijun Wu",
      "summary": "## REST：大型推理模型的压力测试\n\n### 引言：现有评估方法的局限性\n\n当前对大型推理模型（LRMs）的评估主要局限于孤立的问题解决范式，即通过顺序测试评估模型对单个问题的推理能力。这种方法存在以下关键局限性：\n\n*   **数据污染和挑战性不足**：容易受到数据污染，且挑战性较低（例如，DeepSeek-R1在MATH500上达到97.0%），这导致需要投入大量人力和成本持续创建新问题。\n*   **缺乏多上下文压力评估**：未能评估模型在多上下文压力下的表现，而这在现实世界部署中至关重要。\n\n### REST 框架的提出\n\n为了弥补这一空白，研究人员提出了 **REST (Reasoning Evaluation through Simultaneous Testing)**，这是一个对LRMs进行压力测试的框架，它能够同时向模型提出多个问题。\n\n### REST 评估的能力\n\n除了基本的推理能力，REST 还特别评估了几种此前未充分测试的能力：\n\n*   **上下文优先级分配**：模型在多个问题中分配注意力的能力。\n*   **跨问题干扰抵抗**：模型在处理一个问题时，抵抗其他问题干扰的能力。\n*   **动态认知负荷管理**：模型在认知负荷变化时保持性能的能力。\n\n### 关键发现\n\n评估结果揭示了几个显著的发现：\n\n*   **性能显著下降**：即使是像DeepSeek-R1这样的最先进（SOTA）模型，在REST压力测试下也表现出显著的性能下降。\n*   **更强的区分能力**：REST比现有基准测试展现出更强的区分能力，能够揭示在单问题评估中表现相似、接近上限的模型之间明显的性能差异。\n\n### 机制洞察\n\n分析中得出了一些关键的机制洞察：\n\n*   **“过度思考陷阱”**：这是一个导致性能下降的关键因素。\n*   **“long2short”训练技术**：采用“long2short”技术训练的模型在REST测试中能更好地保持其单问题性能的准确性，优于标准训练的模型。\n\n### 结论：REST 的优势\n\n这些结果表明，REST 是一种：\n\n*   **成本效益高**：减少对持续人工标注的依赖。\n*   **面向未来**：能够更好地反映现实世界的推理需求。\n*   **更有效**：提供更全面、更具挑战性的评估范式。",
      "shortSummary": "REST（Reasoning Evaluation through Simultaneous Testing）是一个新颖的压力测试框架，旨在通过同时向大型推理模型（LRMs）提出多个问题，解决现有单问题评估的局限性。它评估模型在多上下文压力下的表现，包括上下文优先级分配和抗干扰能力。研究发现，即使是SOTA模型在REST测试下性能也会显著下降，且REST比现有基准具有更强的区分能力。此外，“过度思考陷阱”是性能下降的原因之一，而“long2short”训练技术有助于模型保持准确性。REST提供了一种更具成本效益且能反映真实世界需求的评估范式。",
      "translated_title": "REST：通过同时提出多个问题对大型推理模型进行压力测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent Large Reasoning Models (LRMs) have achieved remarkable progress on task-specific benchmarks, yet their evaluation methods remain constrained by isolated problem-solving paradigms. Existing benchmarks predominantly assess single-question reasoning through sequential testing, resulting critical limitations: (1) vulnerability to data contamination and less challenging (e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual creation of new questions with large human efforts, (2) failure to evaluate models under multi-context pressure, a key requirement for real-world deployment. To bridge this gap, we present REST (Reasoning Evaluation through Simultaneous Testing), a stress-testing framework that concurrently exposes LRMs to multiple problems simultaneously. Beyond basic reasoning, REST specifically evaluates several under-tested capabilities: contextual priority allocation, cross-problem interference resistance, and dynamic cognitive load management. Our evaluation reveals several striking findings: Even state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance degradation under stress testing. Crucially, REST demonstrates stronger discriminative power than existing benchmarks, revealing pronounced performance differences among models that exhibit similar, near-ceiling performance under single-question evaluations. Some key mechanistic insights emerge from our analysis: (1) the \"overthinking trap\" is a critical factor contributing to the performance degradation; (2) the models trained with \"long2short\" technique preserve more accuracy of their single-problem performance under REST, outperforming standard-trained counterparts. These results establish REST as a cost-efficient, future-proof evaluation paradigm that better reflects real-world reasoning demands while reducing reliance on continuous human annotation."
    },
    {
      "title": "推理还是记忆？强化学习因数据污染导致结果不可靠 (原标题: Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination)",
      "link": "https://arxiv.org/abs/2507.10532",
      "pubDate": "Mon, 14 Jul 2025 13:55:15 GMT",
      "isoDate": "2025-07-14T13:55:15.000Z",
      "creator": "Mingqi Wu, Zhihao Zhang, Qiaole Dong, Zhiheng Xi, Jun Zhao, Senjie Jin, Xiaoran Fan, Yuhao Zhou, Yanwei Fu, Qin Liu, Songyang Zhang, Qi Zhang",
      "summary": "## 强化学习中数据污染导致结果不可靠\n\n### 引言\n\n本文探讨了大型语言模型（LLMs）的推理能力，特别是强化学习（RL）在增强这些能力方面的应用，并指出当前研究中存在的潜在问题。\n\n### 问题与观察\n\n*   **声称的突破与局限性**：近期许多RL方法声称能显著提升LLM的推理能力，甚至在奖励信号随机或不正确的情况下也能实现。然而，这些突破主要集中在Qwen2.5模型家族上，并在MATH-500、AMC和AIME等知名基准上进行评估。令人担忧的是，在Llama等其他模型上未能观察到类似的性能提升，这促使了进一步的深入调查。\n*   **性能差异的疑问**：一些研究甚至提出随机或不正确的奖励信号也能增强推理性能，这与直觉相悖，并加剧了对结果可靠性的质疑。\n\n### 分析与发现\n\n*   **数据污染的风险**：研究分析表明，尽管Qwen2.5在数学推理方面表现出色，但其在大规模网络语料库上的预训练使其容易受到流行基准中数据污染的影响。这意味着模型可能不是真正地进行推理，而是在一定程度上“记忆”了训练数据中包含的基准问题或其解决方案。\n*   **结果的不可靠性**：由于数据污染的存在，从这些受污染基准得出的结果可能并不可靠，无法真实反映RL方法对LLM推理能力的提升效果。\n\n### 解决方案与贡献\n\n*   **生成合成数据集**：为解决数据污染问题，研究引入了一个生成器，能够生成任意长度和难度的完全合成算术问题。这种方法确保了生成的问题是全新的，未曾出现在任何预训练语料或现有基准中。\n*   **构建“RandomCalculation”数据集**：基于此生成器，作者构建了一个名为“RandomCalculation”的干净、无泄露（leakage-free）数据集。这个数据集为评估RL方法提供了一个公正、无偏的环境。\n\n### 关键实验结果\n\n*   **奖励信号的有效性**：使用这些无污染数据集进行评估，研究发现只有准确的奖励信号才能持续且一致地提升性能。这与之前声称随机或不正确奖励信号也能有效的说法形成鲜明对比。\n*   **嘈杂信号的无效性**：实验结果明确指出，嘈杂或不正确的奖励信号并不能带来性能提升，这进一步强调了奖励信号质量的重要性。\n\n### 研究建议\n\n*   **评估标准**：作者强烈倡导在无污染的基准上，并跨越不同的模型家族来评估强化学习方法。\n*   **确保结论可信**：这种严格的评估方法对于确保研究结论的可靠性和可信度至关重要，有助于推动LLM推理能力研究的健康发展。\n\n### 其他信息\n\n本文共26页，属于机器学习（cs.LG）、人工智能（cs.AI）和计算语言学（cs.CL）领域。",
      "shortSummary": "一项研究指出，强化学习（RL）提升大型语言模型（LLM）推理能力的结果可能因数据污染而不可靠。Qwen2.5在流行基准上的出色表现被发现可能源于预训练数据污染，而其他模型未见此提升。为解决此问题，作者创建了无污染的“RandomCalculation”数据集。实验表明，只有准确的奖励信号才能持续提升性能，嘈杂或不正确的信号无效。研究建议在无污染基准和多样模型上评估RL方法，以确保结论可信。",
      "translated_title": "推理还是记忆？强化学习因数据污染导致结果不可靠",
      "images": [],
      "contentSource": "完整文章",
      "content": "The reasoning capabilities of large language models (LLMs) have been a longstanding focus of research. Recent works have further enhanced these capabilities using reinforcement learning (RL), with many new methods claiming significant improvements with minimal or no external supervision. Surprisingly, some studies even suggest that random or incorrect reward signals can enhance reasoning performance. However, these breakthroughs are mostly reported on the Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500, AMC, and AIME, while failing to achieve similar gains on other models like Llama, which warrants further investigation. Our analysis shows that although Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on large-scale web corpora makes it vulnerable to data contamination in popular benchmarks. As a result, results derived from these benchmarks may be unreliable. To address this, we introduce a generator that produces fully synthetic arithmetic problems of arbitrary length and difficulty, yielding a clean dataset we call RandomCalculation. Using these leakage-free datasets, we show that only accurate reward signals consistently improve performance, while noisy or incorrect signals do not. We advocate for evaluating RL methods on uncontaminated benchmarks and across diverse model families to ensure trustworthy conclusions."
    },
    {
      "title": "递归混合体：学习动态递归深度以实现自适应令牌级计算 (原标题: Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation)",
      "link": "https://arxiv.org/abs/2507.10524",
      "pubDate": "Mon, 14 Jul 2025 13:49:00 GMT",
      "isoDate": "2025-07-14T13:49:00.000Z",
      "creator": "Sangmin Bae, Yujin Kim, Reza Bayat, Sungnyun Kim, Jiyoun Ha, Tal Schuster, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Aaron Courville, Se-Young Yun",
      "summary": "## 递归混合体 (MoR)：一种高效的语言模型框架\n\n### 核心问题\n\n大型语言模型虽然能力强大，但其训练和部署所需的计算与内存成本高昂。现有的效率提升方法通常侧重于参数共享或自适应计算，但未能同时实现两者。\n\n### 解决方案：递归混合体 (MoR)\n\nMoR 是一种统一的框架，它在一个单一的递归 Transformer 中结合了参数共享和自适应计算这两个效率维度。\n\n### MoR 的工作原理\n\n*   **参数效率**：MoR 通过在递归步骤中重用共享的层堆栈来实现参数效率。\n*   **自适应计算**：轻量级路由器能够动态地为单个令牌分配不同的递归深度，从而实现令牌级的自适应“思考”。\n*   **注意力计算优化**：MoR 仅将二次注意力计算集中在给定递归深度下仍然活跃的令牌上。\n*   **内存访问效率**：通过选择性地缓存仅活跃令牌的键值（KV）对，进一步提高了内存访问效率。\n\n### 额外机制：KV 共享变体\n\nMoR 还提出了一种 KV 共享变体，它重用第一次递归中的 KV 对，专门设计用于减少预填充延迟和内存占用。\n\n### 性能表现\n\nMoR 在 1.35 亿到 17 亿参数的模型规模范围内，形成了一个新的帕累托前沿：\n\n*   在相同的训练 FLOPs 和更小的模型尺寸下，MoR 显著降低了验证困惑度并提高了少样本准确性。\n*   与传统的和现有的递归基线相比，MoR 提供了更高的吞吐量。\n\n### 结论\n\n这些成果表明，MoR 是在不承担大型模型成本的情况下，实现大型模型质量的有效途径。",
      "shortSummary": "递归混合体（MoR）是一种新型框架，旨在解决大型语言模型的高成本问题。它在一个递归 Transformer 中结合了参数共享和自适应计算，通过重用层和动态分配递归深度来提高效率。MoR 优化了注意力计算和内存访问，并引入 KV 共享以减少延迟。实验表明，MoR 在更小的模型尺寸和相同计算量下，显著提升了性能（降低困惑度、提高准确性、增加吞吐量），实现了低成本下的高质量模型。",
      "translated_title": "递归混合体：学习动态递归深度以实现自适应令牌级计算",
      "images": [],
      "contentSource": "完整文章",
      "content": "Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost."
    },
    {
      "title": "MoVieS：一秒内运动感知4D动态视图合成 (原标题: MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second)",
      "link": "https://arxiv.org/abs/2507.10065",
      "pubDate": "Mon, 14 Jul 2025 04:49:57 GMT",
      "isoDate": "2025-07-14T04:49:57.000Z",
      "creator": "Chenguo Lin, Yuchen Lin, Panwang Pan, Yifan Yu, Honglei Yan, Katerina Fragkiadaki, Yadong Mu",
      "summary": "MoVieS 是一种新颖的前馈模型，它能够在一秒钟内从单目视频合成 4D 动态新视图。\n\n**核心技术与创新点：**\n\n*   **场景表示：** MoVieS 使用高斯基元（Gaussian primitives）的像素对齐网格来表示动态 3D 场景。\n*   **运动监督：** 模型明确地监督这些高斯基元的时变运动，这是其“运动感知”的关键。\n*   **统一建模：** MoVieS 首次实现了外观、几何和运动的统一建模，将这三个关键要素整合到一个框架中。\n\n**多功能性与应用：**\n\n*   **多任务支持：** 在一个单一的基于学习的框架内，MoVieS 能够同时进行视图合成、重建和 3D 点跟踪。\n*   **训练效率：** 通过将新视图合成与动态几何重建相结合，MoVieS 可以在多样化数据集上进行大规模训练，同时对特定任务的监督依赖性极小。\n*   **零样本能力：** 该模型自然支持广泛的零样本应用，例如场景流估计和运动物体分割，无需额外的特定任务训练。\n\n**性能表现：**\n\n*   大量实验验证了 MoVieS 在多项任务中的有效性和效率。\n*   它实现了具有竞争力的性能，同时提供了数量级的速度提升，显著优于现有方法。",
      "shortSummary": "MoVieS是一种创新的前馈模型，能在一秒内从单目视频合成4D动态新视图。它通过高斯基元统一建模外观、几何和运动，并明确监督时变运动。该模型在一个框架内支持视图合成、重建和3D点跟踪，并能进行零样本应用如场景流估计和运动物体分割。实验证明MoVieS高效且性能优异，实现了显著的速度提升。",
      "translated_title": "MoVieS：一秒内运动感知4D动态视图合成",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic novel views from monocular videos in one second. MoVieS represents dynamic 3D scenes using pixel-aligned grids of Gaussian primitives, explicitly supervising their time-varying motion. This allows, for the first time, the unified modeling of appearance, geometry and motion, and enables view synthesis, reconstruction and 3D point tracking within a single learning-based framework. By bridging novel view synthesis with dynamic geometry reconstruction, MoVieS enables large-scale training on diverse datasets with minimal dependence on task-specific supervision. As a result, it also naturally supports a wide range of zero-shot applications, such as scene flow estimation and moving object segmentation. Extensive experiments validate the effectiveness and efficiency of MoVieS across multiple tasks, achieving competitive performance while offering several orders of magnitude speedups."
    },
    {
      "title": "SpeakerVid-5M：一个用于音视频双向交互式数字人生成的大规模高质量数据集 (原标题: SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation)",
      "link": "https://arxiv.org/abs/2507.09862",
      "pubDate": "Sun, 13 Jul 2025 22:22:47 GMT",
      "isoDate": "2025-07-13T22:22:47.000Z",
      "creator": "Youliang Zhang, Zhaoyang Li, Duomin Wang, Jiahe Zhang, Deyu Zhou, Zixin Yin, Xili Dai, Gang Yu, Xiu Li",
      "summary": "## SpeakerVid-5M：音视频双向交互式数字人生成数据集\n\n### 背景与挑战\n\n随着大规模模型的快速发展，数字人领域取得了显著突破，尤其在虚拟形象驱动和渲染方面实现了高保真解决方案。学术界目前的焦点已转向下一个主要挑战：**音视频双向交互式虚拟人**的生成。\n\n### SpeakerVid-5M 数据集介绍\n\n为了促进这一新兴领域的研究，本文提出了 **SpeakerVid-5M** 数据集，这是首个专为音视频双向交互式虚拟人生成设计的大规模、高质量数据集。\n\n*   **规模宏大**：数据集总时长超过 **8,743 小时**，包含超过 **520 万个**人类肖像视频片段。\n*   **内容多样**：涵盖了多种尺度和交互类型，包括单人说话（monadic talking）、听众反应（listening）以及双向对话（dyadic conversations）。\n\n### 数据集结构与特点\n\nSpeakerVid-5M 数据集在两个关键维度上进行了结构化设计：**交互类型**和**数据质量**。\n\n1.  **基于交互场景的分类（交互类型）**：\n    *   **对话分支 (dialogue branch)**\n    *   **单人分支 (single branch)**\n    *   **听众分支 (listening branch)**\n    *   **多轮分支 (multi-turn branch)**\n\n2.  **基于数据质量的分层（数据质量）**：\n    *   一个**大规模预训练子集**，用于基础模型的训练。\n    *   一个经过精心策划的**高质量子集**，专为监督微调（Supervised Fine-Tuning, SFT）设计。\n\n这种双重结构能够支持广泛的2D虚拟人任务。\n\n### 附加贡献与基准\n\n除了数据集本身，研究团队还提供了以下资源：\n\n*   一个基于该数据训练的**自回归（AR）视频聊天基线模型**。\n*   一套专门的**评估指标和测试数据**，共同构成 **VidChatBench**，作为未来研究的基准。\n\n### 可用性\n\nSpeakerVid-5M 数据集及其相应的数据处理代码将**公开发布**，以促进社区研究。\n\n*   **项目页面**：提供更多详细信息（请注意，文章中提供的URL为占位符，此处不展示具体链接）。\n*   **研究领域**：计算机视觉与模式识别 (cs.CV)；音频与语音处理 (eess.AS)。",
      "shortSummary": "SpeakerVid-5M是首个大规模、高质量的音视频双向交互式数字人生成数据集。它包含超过8,743小时、520万个视频片段，涵盖单人、双向等多种交互类型。数据集按交互场景和数据质量分层，包括预训练和SFT子集，支持多样化的2D虚拟人任务。项目还提供了基于该数据的AR视频聊天基线模型和VidChatBench基准测试集。数据集及处理代码将公开发布，旨在推动相关领域研究。",
      "translated_title": "SpeakerVid-5M：一个用于音视频双向交互式数字人生成的大规模高质量数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "The rapid development of large-scale models has catalyzed significant breakthroughs in the digital human domain. These advanced methodologies offer high-fidelity solutions for avatar driving and rendering, leading academia to focus on the next major challenge: audio-visual dyadic interactive virtual human. To facilitate research in this emerging area, we present SpeakerVid-5M dataset, the first large-scale, high-quality dataset designed for audio-visual dyadic interactive virtual human generation. Totaling over 8,743 hours, SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It covers diverse scales and interaction types, including monadic talking, listening, and dyadic conversations. Crucially, the dataset is structured along two key dimensions: interaction type and data quality. First, it is categorized into four types (dialogue branch, single branch, listening branch and multi-turn branch) based on the interaction scenario. Second, it is stratified into a large-scale pre-training subset and a curated, high-quality subset for Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of 2D virtual human tasks. In addition, we provide an autoregressive (AR)-based video chat baseline trained on this data, accompanied by a dedicated set of metrics and test data to serve as a benchmark VidChatBench for future work. Both the dataset and the corresponding data processing code will be publicly released. Project page: https://dorniwang.github.io/SpeakerVid-5M/"
    },
    {
      "title": "LLMalMorph：关于使用大型语言模型生成变种恶意软件的可行性 (原标题: LLMalMorph: On The Feasibility of Generating Variant Malware using Large-Language-Models)",
      "link": "https://arxiv.org/abs/2507.09411",
      "pubDate": "Sat, 12 Jul 2025 18:11:10 GMT",
      "isoDate": "2025-07-12T18:11:10.000Z",
      "creator": "Md Ajwad Akil, Adrian Shuai Li, Imtiaz Karim, Arun Iyengar, Ashish Kundu, Vinny Parla, Elisa Bertino",
      "summary": "## LLMalMorph：使用大型语言模型生成变种恶意软件的可行性\n\n### 研究背景与动机\n\n*   大型语言模型（LLMs）已彻底改变了软件开发和自动化代码生成领域。\n*   受这些进展的启发，本文旨在探讨LLMs在修改恶意软件源代码以生成变种方面的可行性。\n\n### LLMalMorph框架介绍\n\n*   本文提出了 **LLMalMorph**，一个半自动化框架，它利用LLMs对代码的语义和语法理解能力来生成新的恶意软件变种。\n*   **工作原理：**\n    *   LLMalMorph从恶意软件源代码中提取函数级信息。\n    *   它结合了定制设计的提示（prompts）和策略性定义的代码转换。\n    *   这些机制共同引导LLM生成变种，而无需进行资源密集型的微调。\n\n### 实验评估\n\n*   **样本选择：** 为了评估LLMalMorph的有效性，研究人员收集了10个多样化的Windows恶意软件样本，这些样本在类型、复杂性和功能上各不相同。\n*   **变种生成：** 通过LLMalMorph，共生成了618个恶意软件变种。\n\n### 实验结果\n\n*   **规避传统检测：** 彻底的实验表明，这些恶意软件变种的杀毒引擎检测率可以在一定程度上降低，同时成功保留了恶意软件的原始功能。\n*   **对抗机器学习检测器：** 尽管LLMalMorph在设计时并未针对任何基于机器学习（ML）的恶意软件检测器进行优化，但一些生成的变种对基于ML的恶意软件分类器也取得了显著的攻击成功率。\n\n### 讨论与展望\n\n*   文章还讨论了当前LLM在从源代码生成恶意软件变种方面的能力局限性。\n*   评估了这项新兴技术在更广泛的恶意软件变种生成背景下的地位和潜力。",
      "shortSummary": "本文提出了LLMalMorph框架，利用大型语言模型（LLMs）从源代码生成恶意软件变种。通过对10个Windows恶意软件样本生成618个变种的实验表明，LLMalMorph能在一定程度上降低杀毒引擎的检测率，同时保持恶意软件功能。部分变种对基于机器学习的检测器也有效。研究还讨论了LLM在生成恶意软件变种方面的局限性。",
      "translated_title": "LLMalMorph：关于使用大型语言模型生成变种恶意软件的可行性",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) have transformed software development and automated code generation. Motivated by these advancements, this paper explores the feasibility of LLMs in modifying malware source code to generate variants. We introduce LLMalMorph, a semi-automated framework that leverages semantical and syntactical code comprehension by LLMs to generate new malware variants. LLMalMorph extracts function-level information from the malware source code and employs custom-engineered prompts coupled with strategically defined code transformations to guide the LLM in generating variants without resource-intensive fine-tuning. To evaluate LLMalMorph, we collected 10 diverse Windows malware samples of varying types, complexity and functionality and generated 618 variants. Our thorough experiments demonstrate that it is possible to reduce the detection rates of antivirus engines of these malware variants to some extent while preserving malware functionalities. In addition, despite not optimizing against any Machine Learning (ML)-based malware detectors, several variants also achieved notable attack success rates against an ML-based malware classifier. We also discuss the limitations of current LLM capabilities in generating malware variants from source code and assess where this emerging technology stands in the broader context of malware variant generation."
    }
  ],
  "lastUpdated": "2025-07-18T09:36:33.706Z"
}