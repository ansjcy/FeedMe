{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "LMEnt：一个用于分析语言模型中知识的套件，涵盖从预训练数据到表示形式 (原标题: LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations)",
      "link": "https://arxiv.org/abs/2509.03405",
      "pubDate": "Wed, 03 Sep 2025 11:31:18 GMT",
      "isoDate": "2025-09-03T11:31:18.000Z",
      "creator": "Daniela Gottesman, Alon Gilae-Dotan, Ido Cohen, Yoav Gur-Arieh, Marius Mosbach, Ori Yoran, Mor Geva",
      "summary": "## LMEnt：分析语言模型中知识的综合套件\n\n### 引言\n\n语言模型（LMs）在现实世界应用中日益依赖世界知识，但其将数据转化为关于世界知识和信念表示的内部过程仍知之甚少。深入了解这些过程对于开发具有更一致、鲁棒和完整知识表示的语言模型至关重要。\n\n### LMEnt 介绍\n\n为促进对这些问题的研究，本文提出了 **LMEnt**，一个用于分析语言模型在预训练期间知识获取的综合套件。LMEnt 引入了以下核心组成部分：\n\n1.  **知识丰富的预训练语料库：**\n    *   基于维基百科构建。\n    *   完全标注了实体提及。\n2.  **基于实体的预训练数据检索方法：**\n    *   该方法在性能上显著优于现有方法，提升高达 80.4%。\n3.  **预训练模型集合：**\n    *   包含 12 个预训练模型，参数量高达 10 亿（1B）。\n    *   提供 4000 个中间检查点。\n    *   在知识基准测试上的表现与流行的开源模型相当。\n\n### 资源用途\n\n这些资源共同提供了一个受控环境，用于：\n\n*   分析预训练中实体提及与下游性能之间的联系。\n*   研究预训练数据中因果干预的影响。\n\n### LMEnt 的实用性展示\n\n通过研究跨检查点的知识获取过程，LMEnt 展示了其实用性。研究发现：\n\n*   事实频率是知识获取的关键因素。\n*   但事实频率并不能完全解释学习趋势。\n\n### 发布目的\n\nLMEnt 已发布，旨在支持对语言模型中知识的深入研究，包括但不限于：\n\n*   知识表示\n*   可塑性\n*   编辑\n*   归因\n*   学习动态",
      "shortSummary": "LMEnt是一个用于分析语言模型预训练期间知识获取的综合套件。它包含一个基于维基百科、标注了实体的知识丰富预训练语料库、一个性能优越的实体检索方法，以及12个带有中间检查点的预训练模型。这些资源提供了一个受控环境，用于研究预训练数据与下游性能之间的关系，并分析知识获取动态。LMEnt的发布旨在支持对语言模型知识表示、可塑性、编辑和学习动态等方面的研究。",
      "translated_title": "LMEnt：一个用于分析语言模型中知识的套件，涵盖从预训练数据到表示形式",
      "images": [],
      "contentSource": "完整文章",
      "content": "Language models (LMs) increasingly drive real-world applications that require world knowledge. However, the internal processes through which models turn data into representations of knowledge and beliefs about the world, are poorly understood. Insights into these processes could pave the way for developing LMs with knowledge representations that are more consistent, robust, and complete. To facilitate studying these questions, we present LMEnt, a suite for analyzing knowledge acquisition in LMs during pretraining. LMEnt introduces: (1) a knowledge-rich pretraining corpus, fully annotated with entity mentions, based on Wikipedia, (2) an entity-based retrieval method over pretraining data that outperforms previous approaches by as much as 80.4%, and (3) 12 pretrained models with up to 1B parameters and 4K intermediate checkpoints, with comparable performance to popular open-sourced models on knowledge benchmarks. Together, these resources provide a controlled environment for analyzing connections between entity mentions in pretraining and downstream performance, and the effects of causal interventions in pretraining data. We show the utility of LMEnt by studying knowledge acquisition across checkpoints, finding that fact frequency is key, but does not fully explain learning trends. We release LMEnt to support studies of knowledge in LMs, including knowledge representations, plasticity, editing, attribution, and learning dynamics."
    },
    {
      "title": "面向大型语言模型的智能体强化学习全景：一项综述 (原标题: The Landscape of Agentic Reinforcement Learning for LLMs: A Survey)",
      "link": "https://arxiv.org/abs/2509.02547",
      "pubDate": "Tue, 02 Sep 2025 13:46:26 GMT",
      "isoDate": "2025-09-02T13:46:26.000Z",
      "creator": "Guibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhongzhi Li, Xiangyuan Xue, Yijiang Li, Yifan Zhou, Yang Chen, Chen Zhang, Yutao Fan, Zihu Wang, Songtao Huang, Yue Liao, Hongru Wang, Mengyue Yang, Heng Ji, Michael Littman, Jun Wang, Shuicheng Yan, Philip Torr, Lei Bai",
      "summary": "## 面向大型语言模型的智能体强化学习全景：一项综述\n\n### 引言\n\n本综述深入探讨了智能体强化学习（Agentic RL）这一新兴范式，它标志着大型语言模型（LLMs）应用领域的一项重大转变。传统上，LLMs被视为被动的序列生成器，而Agentic RL则将它们重塑为嵌入复杂动态世界中的自主决策智能体。这项工作旨在通过形式化概念、提出分类体系并整合现有资源，全面描绘这一快速发展的领域。\n\n### 核心概念与范式转变\n\n*   **从传统LLM RL到智能体RL的转变**：\n    *   **传统LLM RL**：通常将LLMs视为在退化的单步马尔可夫决策过程（MDPs）中运行，其主要任务是生成序列。LLMs在此情境下扮演的角色相对被动。\n    *   **智能体RL**：将LLMs置于时间上扩展、部分可观察的马尔可夫决策过程（POMDPs）中。这意味着LLMs需要进行多步决策，处理不确定性，并与环境进行持续交互，从而展现出更强的自主性和决策能力。\n*   **形式化对比**：综述通过对比这两种MDPs，明确了智能体RL在概念上的进步，强调了其在处理复杂、动态环境中的优势。\n\n### 综合分类体系\n\n本综述提出了一个全面的双重分类法，以组织和理解智能体强化学习的复杂性：\n\n1.  **围绕核心智能体能力**：\n    *   **规划（Planning）**：智能体制定多步行动序列以实现目标的能力。\n    *   **工具使用（Tool Use）**：智能体调用外部工具或API以扩展其功能和解决问题的能力。\n    *   **记忆（Memory）**：智能体存储、检索和利用过去经验以指导当前决策的能力。\n    *   **推理（Reasoning）**：智能体进行逻辑推断、问题解决和理解复杂情境的能力。\n    *   **自我改进（Self-improvement）**：智能体从经验中学习并优化自身行为策略的能力。\n    *   **感知（Perception）**：智能体从环境中获取和解释信息的能力。\n\n2.  **围绕多样化的任务领域**：\n    *   综述将智能体强化学习的应用划分为多个任务领域，展示了其在不同场景下的广泛潜力，例如游戏、机器人控制、科学发现、软件开发等。\n\n### 强化学习的关键作用\n\n本综述的核心论点是，强化学习（RL）是实现智能体行为转化的关键机制。它能够将上述核心能力从静态的、启发式的模块，转化为自适应的、鲁棒的智能体行为。通过RL，LLMs能够通过与环境的交互学习最优策略，从而在动态和不确定的环境中表现出更强的适应性和泛化能力。\n\n### 支持未来研究的资源整合\n\n为了支持和加速未来的研究，本综述整合了当前智能体强化学习领域的开放资源：\n\n*   **开源环境（Open-source Environments）**：提供模拟或真实世界环境，供智能体进行训练和评估。\n*   **基准（Benchmarks）**：提供标准化的任务和评估指标，用于比较不同智能体方法的性能。\n*   **框架（Frameworks）**：提供开发和实现智能体强化学习算法的工具和库。\n\n### 总结与展望\n\n通过综合分析超过五百篇近期研究工作，本综述描绘了智能体强化学习这一快速演进领域的轮廓。它不仅突出了当前面临的机遇，也指出了在开发可扩展、通用型AI智能体过程中将遇到的挑战。本综述旨在为研究人员提供一个全面的视角，以指导未来在这一前沿领域的研究和发展。",
      "shortSummary": "该综述深入探讨了面向大型语言模型（LLMs）的智能体强化学习（Agentic RL），将其定义为LLMs从被动生成器向复杂动态世界中自主决策智能体的范式转变。文章通过对比传统LLM-RL的单步MDPs与Agentic RL的扩展POMDPs，提出了一个围绕规划、工具使用、记忆等核心能力及其应用领域的双重分类法。综述强调强化学习是实现自适应智能体行为的关键，并整合了开源资源以加速未来研究，旨在推动可扩展、通用AI智能体的发展。",
      "translated_title": "面向大型语言模型的智能体强化学习全景：一项综述",
      "images": [],
      "contentSource": "完整文章",
      "content": "The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language models (LLM RL), reframing LLMs from passive sequence generators into autonomous, decision-making agents embedded in complex, dynamic worlds. This survey formalizes this conceptual shift by contrasting the degenerate single-step Markov Decision Processes (MDPs) of LLM-RL with the temporally extended, partially observable Markov decision processes (POMDPs) that define Agentic RL. Building on this foundation, we propose a comprehensive twofold taxonomy: one organized around core agentic capabilities, including planning, tool use, memory, reasoning, self-improvement, and perception, and the other around their applications across diverse task domains. Central to our thesis is that reinforcement learning serves as the critical mechanism for transforming these capabilities from static, heuristic modules into adaptive, robust agentic behavior. To support and accelerate future research, we consolidate the landscape of open-source environments, benchmarks, and frameworks into a practical compendium. By synthesizing over five hundred recent works, this survey charts the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose AI agents."
    },
    {
      "title": "UI-TARS-2 技术报告：通过多轮强化学习推进 GUI 智能体 (原标题: UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2509.02544",
      "pubDate": "Tue, 02 Sep 2025 13:44:45 GMT",
      "isoDate": "2025-09-02T13:44:45.000Z",
      "creator": "Haoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu, Qinyu Luo, Shihao Liang, Shijue Huang, Wanjun Zhong, Yining Ye, Yujia Qin, Yuwen Xiong, Yuxin Song, Zhiyong Wu, Bo Li, Chen Dun, Chong Liu, Fuxing Leng, Hanbin Wang, Hao Yu, Haobin Chen, Hongyi Guo, Jing Su, Jingjia Huang, Kai Shen, Kaiyu Shi, Lin Yan, Peiyao Zhao, Pengfei Liu, Qinghao Ye, Renjie Zheng, Wayne Xin Zhao, Wen Heng, Wenhao Huang, Wenqian Wang, Xiaobo Qin, Yi Lin, Youbin Wu, Zehui Chen, Zihao Wang, Baoquan Zhong, Xinchun Zhang, Xujing Li, Yuanfan Li, Zhongkai Zhao, Chengquan Jiang, Faming Wu, Haotian Zhou, Jinlin Pang, Li Han, Qianli Ma, Siyao Liu, Songhua Cai, Wenqi Fu, Xin Liu, Zhi Zhang, Bo Zhou, Guoliang Li, Jiajun Shi, Jiale Yang, Jie Tang, Li Li, Taoran Lu, Woyu Lin, Xiaokang Tong, Xinyao Li, Yichi Zhang, Yu Miao, Zhengxuan Jiang, Zili Li, Ziyuan Zhao, Chenxin Li, Dehua Ma, Feng Lin, Ge Zhang, Haihua Yang, Hangyu Guo, Hongda Zhu, Jiaheng Liu, Junda Du, Kai Cai, Kuanye Li, Lichen Yuan, Meilan Han, Minchao Wang, Shuyue Guo, Tianhao Cheng, Xiaobo Ma, Xiaojun Xiao, Xiaolong Huang, Xinjie Chen, Yidi Du, Yilin Chen, Yiwen Wang, Zhaojian Li, Zhenzhu Yang, Zhiyuan Zeng, Chaolin Jin, Chen Li, Hao Chen, Haoli Chen, Jian Chen, Qinghao Zhao, Guang Shi",
      "summary": "# UI-TARS-2 技术报告：通过多轮强化学习推进 GUI 智能体\n\n## 摘要\n\n本技术报告介绍了 UI-TARS-2，一个以图形用户界面（GUI）为中心的本地智能体模型。该模型旨在解决当前GUI智能体开发面临的主要挑战，包括数据可扩展性、多轮强化学习（RL）的复杂性、仅GUI操作的局限性以及环境稳定性问题。UI-TARS-2通过一套系统化的训练方法论，在多个方面取得了显著进展。\n\n## 核心方法论\n\nUI-TARS-2采用以下关键组件和策略来克服现有挑战：\n\n*   **数据飞轮（Data Flywheel）**：用于实现可扩展的数据生成，确保模型有充足且多样化的训练数据。\n*   **稳定多轮强化学习框架**：设计了一个稳定的框架，以有效处理多轮交互中的学习和决策过程，解决了传统多轮RL的稳定性问题。\n*   **混合GUI环境**：集成了文件系统和终端，构建了一个超越纯GUI操作的混合环境，使智能体能够处理更广泛的任务类型。\n*   **统一沙盒平台**：提供了一个统一的沙盒平台，支持大规模的智能体部署和测试，便于进行广泛的实验和评估。\n\n## 实验结果与性能\n\n经验评估表明，UI-TARS-2相较于其前身UI-TARS-1.5取得了显著的性能提升，并在多个基准测试中表现出色：\n\n### GUI基准测试\n\nUI-TARS-2在多个GUI基准测试中取得了优异成绩，超越了包括Claude和OpenAI智能体在内的强大基线模型：\n\n*   **Online-Mind2Web**：88.2\n*   **OSWorld**：47.5\n*   **WindowsAgentArena**：50.6\n*   **AndroidWorld**：73.3\n\n### 游戏环境\n\n在游戏环境中，UI-TARS-2也展现了强大的能力：\n\n*   在包含15款游戏的套件中，平均归一化得分达到59.8，约为人机水平的60%。\n*   在LMGame-Bench上，与OpenAI o3等前沿专有模型保持竞争力。\n\n### 泛化能力\n\n该模型还表现出强大的泛化能力，能够适应多样化的智能体任务：\n\n*   成功泛化到长周期信息检索任务。\n*   在软件工程基准测试中表现出色，突显了其在不同交互场景中的鲁棒性。\n\n## 深入分析与结论\n\n对训练动态的详细分析为在大规模智能体强化学习中实现稳定性和效率提供了宝贵见解。这些结果强调了UI-TARS-2在推进GUI智能体领域方面的巨大潜力，并展示了其在真实世界交互场景中的强大泛化能力。",
      "shortSummary": "UI-TARS-2是一个以GUI为中心的本地智能体模型，旨在解决GUI智能体开发中的数据可扩展性、多轮强化学习、GUI操作局限性及环境稳定性等挑战。它通过数据飞轮、稳定多轮RL框架、混合GUI环境和统一沙盒平台实现。实验表明，UI-TARS-2在GUI基准测试（如Online-Mind2Web 88.2）和游戏环境中均显著优于现有模型和前身，并展现出强大的泛化能力，有望推动GUI智能体在真实世界场景中的应用。",
      "translated_title": "UI-TARS-2 技术报告：通过多轮强化学习推进 GUI 智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "The development of autonomous agents for graphical user interfaces (GUIs) presents major challenges in artificial intelligence. While recent advances in native agent models have shown promise by unifying perception, reasoning, action, and memory through end-to-end learning, open problems remain in data scalability, multi-turn reinforcement learning (RL), the limitations of GUI-only operation, and environment stability. In this technical report, we present UI-TARS-2, a native GUI-centered agent model that addresses these challenges through a systematic training methodology: a data flywheel for scalable data generation, a stabilized multi-turn RL framework, a hybrid GUI environment that integrates file systems and terminals, and a unified sandbox platform for large-scale rollouts. Empirical evaluation demonstrates that UI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5. On GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on WindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines such as Claude and OpenAI agents. In game environments, it attains a mean normalized score of 59.8 across a 15-game suite-roughly 60% of human-level performance-and remains competitive with frontier proprietary models (e.g., OpenAI o3) on LMGame-Bench. Additionally, the model can generalize to long-horizon information-seeking tasks and software engineering benchmarks, highlighting its robustness across diverse agent tasks. Detailed analyses of training dynamics further provide insights into achieving stability and efficiency in large-scale agent RL. These results underscore UI-TARS-2's potential to advance the state of GUI agents and exhibit strong generalization to real-world interactive scenarios."
    },
    {
      "title": "在语言模型生成中联合增强多样性和质量 (原标题: Jointly Reinforcing Diversity and Quality in Language Model Generations)",
      "link": "https://arxiv.org/abs/2509.02534",
      "pubDate": "Tue, 02 Sep 2025 13:38:47 GMT",
      "isoDate": "2025-09-02T13:38:47.000Z",
      "creator": "Tianjian Li, Yiming Zhang, Ping Yu, Swarnadeep Saha, Daniel Khashabi, Jason Weston, Jack Lanchantin, Tianlu Wang",
      "summary": "## DARLING：联合增强语言模型生成的多样性和质量\n\n### 挑战：后训练语言模型中的多样性与质量权衡\n\n大型语言模型（LMs）的后训练过程通常优先考虑准确性和实用性，但这往往以牺牲生成内容的多样性为代价。这种做法虽然提高了响应质量，但也使得输出分布更加尖锐，减少了思想的范围，从而限制了LMs在头脑风暴、故事创作或问题解决等创意和探索性任务中的效用。\n\n### 解决方案：多样性感知强化学习（DARLING）框架\n\n为了解决这一挑战，研究人员提出了**多样性感知强化学习（DARLING）**框架。DARLING旨在联合优化响应质量和语义多样性，以生成既高质量又独特的输出。\n\n### DARLING 的核心机制\n\nDARLING的核心在于引入了一个**学习到的分区函数**，用于衡量超越表面词汇变化之外的语义多样性。这个多样性信号随后与质量奖励结合，在在线强化学习过程中共同优化模型，鼓励其生成高质量且独特的输出。\n\n### 实验与结果\n\nDARLING在多种模型家族和规模上进行了实验，并被证明适用于两种不同的任务类型：\n\n1.  **非可验证任务**：包括指令遵循和创意写作。\n    *   在五个基准测试中，DARLING持续优于仅优化质量的RL基线模型，生成的输出同时具有更高的质量和新颖性。\n\n2.  **可验证任务**：例如竞赛数学。\n    *   DARLING在这些任务中实现了更高的pass@1（解决方案质量）和pass@k（解决方案多样性）。\n\n### 关键发现：多样性促进探索与质量提升\n\n最引人注目的是，实验发现明确地优化多样性能够**促进在线强化学习中的探索**。这种探索的增加反过来表现为更高质量的响应，这表明多样性和质量之间存在协同效应，而非简单的权衡。",
      "shortSummary": "DARLING框架旨在解决大型语言模型后训练中质量与多样性之间的矛盾。该框架通过引入学习到的分区函数来衡量语义多样性，并将其与质量奖励结合，在在线强化学习中共同优化生成内容的质量和独特性。实验表明，DARLING在非可验证（如创意写作）和可验证（如数学竞赛）任务中均优于仅优化质量的基线模型，不仅提高了输出质量，还显著增强了多样性，甚至通过促进探索进一步提升了响应质量。",
      "translated_title": "在语言模型生成中联合增强多样性和质量",
      "images": [],
      "contentSource": "完整文章",
      "content": "Post-training of Large Language Models (LMs) often prioritizes accuracy and helpfulness at the expense of diversity. This creates a tension: while post-training improves response quality, it also sharpens output distributions and reduces the range of ideas, limiting the usefulness of LMs in creative and exploratory tasks such as brainstorming, storytelling, or problem solving. We address this challenge with Diversity-Aware Reinforcement Learning (DARLING), a framework that jointly optimizes for response quality and semantic diversity. At its core, DARLING introduces a learned partition function to measure diversity beyond surface-level lexical variations. This diversity signal is then combined with a quality reward during online reinforcement learning, encouraging models to generate outputs that are both high-quality and distinct. Experiments across multiple model families and sizes show that DARLING generalizes to two regimes: non-verifiable tasks (instruction following and creative writing) and verifiable tasks (competition math). On five benchmarks in the first setting, DARLING consistently outperforms quality-only RL baselines, producing outputs that are simultaneously of higher quality and novelty. In the second setting, DARLING achieves higher pass@1 (solution quality) and pass@k (solution variety). Most strikingly, explicitly optimizing for diversity catalyzes exploration in online RL, which manifests itself as higher-quality responses."
    },
    {
      "title": "通过监督学习框架实现RLVR的隐式Actor-Critic耦合 (原标题: Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR)",
      "link": "https://arxiv.org/abs/2509.02522",
      "pubDate": "Tue, 02 Sep 2025 13:22:46 GMT",
      "isoDate": "2025-09-02T13:22:46.000Z",
      "creator": "Jiaming Li, Longze Chen, Ze Gong, Yukun Chen, Lu Wang, Wanwei He, Run Luo, Min Yang",
      "summary": "## PACS：通过监督学习框架实现RLVR的隐式Actor-Critic耦合\n\n### 引言\n\n近年来，**可验证奖励强化学习 (RLVR)** 的进步显著提升了大型语言模型 (LLMs) 解决数学和编程等复杂推理任务的能力。RLVR通过利用可验证的结果奖励来指导策略优化，使LLMs能够以可靠且有依据的方式逐步提高输出质量。然而，RLVR范式面临重大挑战，现有方法，特别是基于强化学习 (RL) 的方法，常受限于稀疏的奖励信号和不稳定的策略梯度更新。\n\n### 提出的方法：PACS框架\n\n为了解决上述挑战，本文提出了一个名为 **PACS** 的新型RLVR框架。PACS代表了通过**S**upervised学习框架实现**P**licit **A**ctor **C**ritic耦合。其核心思想是将结果奖励视为可预测的标签，从而将RLVR问题重新构建为一个基于分数函数的监督学习任务。这个分数函数由策略模型参数化，并使用交叉熵损失进行优化。\n\n#### 机制优势\n\n详细的梯度分析表明，这种监督学习的公式固有地恢复了经典的策略梯度更新，同时实现了Actor和Critic角色的隐式耦合。这种耦合机制带来了更稳定和高效的训练过程。\n\n### 实验结果\n\n研究团队在具有挑战性的数学推理任务上对PACS进行了基准测试。结果显示，PACS的推理性能优于PPO和GRPO等强大的RLVR基线方法。例如：\n\n*   在AIME 2025的pass@256指标上，PACS达到了 **59.78%** 的成绩。\n*   这比PPO和GRPO分别提高了 **13.32** 和 **14.36** 个百分点。\n\n### 结论与展望\n\nPACS框架以其简洁而强大的特性，为LLMs通过可验证奖励进行后训练提供了一个有前景的途径。该研究的代码和数据已作为开源资源提供。",
      "shortSummary": "针对RLVR中稀疏奖励和策略梯度不稳定的问题，本文提出了PACS框架。PACS通过将RLVR重构为监督学习任务，并使用交叉熵损失优化分数函数，实现了Actor和Critic的隐式耦合。这种方法使得训练更稳定高效，并在数学推理任务上表现出色。例如，在AIME 2025上，PACS的pass@256达到59.78%，显著优于PPO和GRPO，为LLMs的后训练提供了更稳定高效的解决方案。",
      "translated_title": "通过监督学习框架实现RLVR的隐式Actor-Critic耦合",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have empowered large language models (LLMs) to tackle challenging reasoning tasks such as mathematics and programming. RLVR leverages verifiable outcome rewards to guide policy optimization, enabling LLMs to progressively improve output quality in a grounded and reliable manner. Despite its promise, the RLVR paradigm poses significant challenges, as existing methods often suffer from sparse reward signals and unstable policy gradient updates, particularly in RL-based approaches. To address the challenges, we propose PACS, a novel RLVR framework that achieves imPlicit Actor Critic coupling via a Supervised learning framework. By treating the outcome reward as a predictable label, we reformulate the RLVR problem into a supervised learning task over a score function parameterized by the policy model and optimized using cross-entropy loss. A detailed gradient analysis shows that this supervised formulation inherently recovers the classical policy gradient update while implicitly coupling actor and critic roles, yielding more stable and efficient training. Benchmarking on challenging mathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as PPO and GRPO, achieving superior reasoning performance. For instance, PACS achieves 59.78\\% at pass@256 on AIME 2025, representing improvements of 13.32 and 14.36 points over PPO and GRPO. This simple yet powerful framework offers a promising avenue for LLMs post-training with verifiable rewards. Our code and data are available as open source at https://github.com/ritzz-ai/PACS."
    },
    {
      "title": "SimpleTIR：用于多轮工具集成推理的端到端强化学习 (原标题: SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning)",
      "link": "https://arxiv.org/abs/2509.02479",
      "pubDate": "Tue, 02 Sep 2025 12:30:19 GMT",
      "isoDate": "2025-09-02T12:30:19.000Z",
      "creator": "Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Xiaosen Zheng, Zejun Ma, Bo An",
      "summary": "# SimpleTIR：用于多轮工具集成推理的端到端强化学习\n\n## 摘要\n\n大型语言模型（LLMs）通过与外部工具交互，可以显著提升其推理能力，这种范式被称为**工具集成推理（TIR）**。然而，将TIR扩展到多轮场景并结合强化学习（RL）时，经常会遇到训练不稳定和性能崩溃的问题。\n\n## 问题识别\n\n研究发现，这种不稳定性主要源于外部工具反馈导致的**分布漂移**，进而导致低概率令牌的生成。这个问题在连续的回合中会不断累积，引发灾难性的**梯度范数爆炸**，从而破坏训练过程。\n\n## SimpleTIR 解决方案\n\n为了解决这一挑战，本文引入了 **SimpleTIR**，一个即插即用的算法，旨在稳定多轮TIR的训练。\n\n*   **核心策略**：SimpleTIR 的核心策略是识别并过滤掉包含“空回合”（void turns）的轨迹。所谓“空回合”，是指那些既没有产生代码块也没有给出最终答案的回合。\n*   **工作原理**：通过从策略更新中移除这些有问题的轨迹，SimpleTIR 有效地阻止了有害的、高幅度的梯度，从而稳定了学习动态。\n\n## 实验结果与性能\n\n广泛的实验表明，SimpleTIR 在具有挑战性的数学推理基准测试中取得了最先进的性能：\n\n*   **AIME24 分数提升**：在使用 Qwen2.5-7B 基础模型时，SimpleTIR 将 AIME24 分数从纯文本基线的 22.1 显著提升至 50.5。\n*   **发现多样化推理模式**：通过避免监督微调的限制，SimpleTIR 鼓励模型发现多样化和复杂的推理模式，例如自我纠正和交叉验证。\n\n## 结论\n\nSimpleTIR 提供了一种有效的方法来解决多轮工具集成推理中强化学习训练不稳定的问题，显著提升了LLMs在复杂推理任务上的表现。",
      "shortSummary": "SimpleTIR提出了一种解决多轮工具集成推理（TIR）中强化学习训练不稳定性的方法。该不稳定性源于分布漂移和梯度爆炸。SimpleTIR通过识别并过滤掉无效回合（未生成代码或最终答案的回合），有效阻止了有害梯度，从而稳定了训练过程。实验证明，SimpleTIR在数学推理基准测试中达到了最先进的性能，例如将AIME24分数从22.1提升至50.5，并鼓励模型发现自我纠正等多样化推理模式。",
      "translated_title": "SimpleTIR：用于多轮工具集成推理的端到端强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) can significantly improve their reasoning capabilities by interacting with external tools, a paradigm known as Tool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios using Reinforcement Learning (RL) is often hindered by training instability and performance collapse. We identify that such instability is primarily caused by a distributional drift from external tool feedback, leading to the generation of low-probability tokens. This issue compounds over successive turns, causing catastrophic gradient norm explosions that derail the training process. To address this challenge, we introduce SimpleTIR , a plug-and-play algorithm that stabilizes multi-turn TIR training. Its core strategy is to identify and filter out trajectories containing void turns, i.e., turns that yield neither a code block nor a final answer. By removing these problematic trajectories from the policy update, SimpleTIR effectively blocks the harmful, high-magnitude gradients, thus stabilizing the learning dynamics. Extensive experiments show that SimpleTIR achieves state-of-the-art performance on challenging math reasoning benchmarks, notably elevating the AIME24 score from a text-only baseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model. Furthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR encourages the model to discover diverse and sophisticated reasoning patterns, such as self-correction and cross-validation."
    },
    {
      "title": "GenCompositor：基于扩散Transformer的生成式视频合成 (原标题: GenCompositor: Generative Video Compositing with Diffusion Transformer)",
      "link": "https://arxiv.org/abs/2509.02460",
      "pubDate": "Tue, 02 Sep 2025 12:10:13 GMT",
      "isoDate": "2025-09-02T12:10:13.000Z",
      "creator": "Shuzhou Yang, Xiaoyu Li, Xiaodong Cun, Guangzhi Wang, Lingen Li, Ying Shan, Jian Zhang",
      "summary": "# GenCompositor：基于扩散Transformer的生成式视频合成\n\n## 引言\n传统的视频合成流程面临诸多挑战，包括：\n*   **劳动密集型：** 需要大量人工投入。\n*   **专家协作：** 依赖多领域专家的紧密合作。\n*   **生产周期长：** 导致漫长的制作周期。\n*   **人力成本高：** 显著增加制作成本。\n\n为了解决这些问题，本文提出了一种名为“生成式视频合成”的新任务，旨在利用生成模型自动化这一过程。\n\n## 任务目标\n生成式视频合成旨在以交互方式，自适应地将前景视频的身份和运动信息注入到目标视频中。这允许用户高度定制最终视频中添加的动态元素，例如：\n*   调整其大小。\n*   设定其运动轨迹。\n*   修改其他相关属性。\n\n## 方法概述：基于扩散Transformer (DiT) 的GenCompositor\n研究团队设计了一个新颖的扩散Transformer (DiT) 管道，利用其内在特性来实现生成式视频合成。该方法包含以下关键组件：\n\n### 1. 轻量级DiT背景保持分支\n*   **目的：** 确保编辑前后目标视频的一致性。\n*   **实现：** 通过修订的轻量级DiT分支，结合掩码令牌注入（masked token injection）技术。\n\n### 2. DiT前景融合块\n*   **目的：** 从其他来源继承动态元素并进行融合。\n*   **实现：** 采用基于全自注意力（full self-attention）的DiT融合块。\n*   **训练辅助：** 配合一种简单而有效的前景增强（foreground augmentation）策略进行训练。\n\n### 3. 扩展旋转位置编码 (ERoPE)\n*   **目的：** 根据用户控制，融合具有不同布局的背景和前景视频。\n*   **实现：** 开发了一种新颖的位置编码方法，命名为扩展旋转位置编码 (ERoPE)。\n\n## 数据集\n为了支持这项新任务，研究团队精心策划并构建了一个名为 **VideoComp** 的数据集。该数据集包含：\n*   61K套视频。\n*   完整的动态元素。\n*   高质量的目标视频。\n\n## 实验结果\n实验证明，GenCompositor 方法能够有效地实现生成式视频合成。与现有解决方案相比，该方法在以下方面表现出显著优势：\n*   **保真度：** 生成视频的真实感更高。\n*   **一致性：** 视频内容在编辑前后保持更好的连贯性。",
      "shortSummary": "GenCompositor提出了一种基于扩散Transformer (DiT) 的生成式视频合成方法，旨在自动化传统视频合成中劳动密集且成本高的问题。该方法允许用户交互式地将前景视频的动态元素注入目标视频，并自定义其属性。GenCompositor包含一个DiT背景保持分支、一个DiT前景融合块（使用全自注意力）以及新颖的扩展旋转位置编码（ERoPE）以支持用户控制。研究团队还构建了VideoComp数据集。实验表明，该方法在保真度和一致性上优于现有方案。",
      "translated_title": "GenCompositor：基于扩散Transformer的生成式视频合成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Video compositing combines live-action footage to create video production, serving as a crucial technique in video creation and film production. Traditional pipelines require intensive labor efforts and expert collaboration, resulting in lengthy production cycles and high manpower costs. To address this issue, we automate this process with generative models, called generative video compositing. This new task strives to adaptively inject identity and motion information of foreground video to the target video in an interactive manner, allowing users to customize the size, motion trajectory, and other attributes of the dynamic elements added in final video. Specifically, we designed a novel Diffusion Transformer (DiT) pipeline based on its intrinsic properties. To maintain consistency of the target video before and after editing, we revised a light-weight DiT-based background preservation branch with masked token injection. As to inherit dynamic elements from other sources, a DiT fusion block is proposed using full self-attention, along with a simple yet effective foreground augmentation for training. Besides, for fusing background and foreground videos with different layouts based on user control, we developed a novel position embedding, named Extended Rotary Position Embedding (ERoPE). Finally, we curated a dataset comprising 61K sets of videos for our new task, called VideoComp. This data includes complete dynamic elements and high-quality target videos. Experiments demonstrate that our method effectively realizes generative video compositing, outperforming existing possible solutions in fidelity and consistency."
    },
    {
      "title": "MedDINOv3：如何将视觉基础模型应用于医学图像分割？ (原标题: MedDINOv3: How to adapt vision foundation models for medical image segmentation?)",
      "link": "https://arxiv.org/abs/2509.02379",
      "pubDate": "Tue, 02 Sep 2025 10:44:43 GMT",
      "isoDate": "2025-09-02T10:44:43.000Z",
      "creator": "Yuheng Li, Yizhou Wu, Yuxiang Lai, Mingzhe Hu, Xiaofeng Yang",
      "summary": "## MedDINOv3：将视觉基础模型应用于医学图像分割\n\n### 引言\n\n医学图像（如CT和MRI扫描）中器官和肿瘤的精确分割对于诊断、治疗规划和疾病监测至关重要。尽管深度学习在自动化分割方面取得了显著进展，但大多数现有模型仍是任务特定的，缺乏跨模态和跨机构的泛化能力。视觉基础模型（FMs）通过在数十亿张自然图像上进行预训练，提供了强大且可迁移的表示能力。然而，将这些模型应用于医学图像领域面临两大关键挑战：\n\n1.  **ViT骨干网络的性能限制：** 大多数基础模型的ViT（Vision Transformer）骨干网络在医学图像分割任务上的表现仍不如专门设计的CNN（卷积神经网络）。\n2.  **巨大的领域鸿沟：** 自然图像与医学图像之间存在显著的领域差异，这限制了预训练模型的可迁移性。\n\n### MedDINOv3 框架\n\n为了解决上述挑战，本文引入了 **MedDINOv3**，一个简单而有效的框架，旨在将DINOv3（一种自监督学习方法）适应于医学图像分割任务。MedDINOv3的核心设计包括以下几个方面：\n\n*   **ViT架构的重新审视与改进：** 研究人员首先重新审视了普通的ViT架构，并设计了一个简单而有效的、具有多尺度令牌聚合能力的架构。这一改进旨在提升ViT在处理医学图像时的特征提取能力。\n*   **领域自适应预训练：** MedDINOv3在 **CT-3M** 数据集上进行领域自适应预训练。CT-3M是一个精心策划的集合，包含387万张轴向CT切片，为模型提供了丰富的医学图像数据以缩小领域差距。\n*   **多阶段DINOv3预训练策略：** 采用多阶段DINOv3配方进行预训练，旨在学习鲁棒的密集特征。这种自监督学习方法有助于模型在没有大量标注数据的情况下，从医学图像中学习到高质量的表示。\n\n### 性能与结果\n\nMedDINOv3在四个不同的分割基准测试中，其性能达到或超越了最先进的水平。这一结果有力地证明了视觉基础模型作为医学图像分割统一骨干网络的巨大潜力。\n\n### 代码可用性\n\nMedDINOv3的相关代码已开源。",
      "shortSummary": "MedDINOv3是一个旨在将视觉基础模型应用于医学图像分割的框架，以解决现有模型泛化性差和领域鸿沟问题。它通过改进ViT架构，引入多尺度令牌聚合，并在包含387万张CT切片的CT-3M数据集上进行领域自适应预训练，采用多阶段DINOv3策略学习鲁棒特征。MedDINOv3在多个分割基准测试中表现出色，达到或超越了现有最佳水平，展示了视觉基础模型作为医学图像分割统一骨干网络的巨大潜力。",
      "translated_title": "MedDINOv3：如何将视觉基础模型应用于医学图像分割？",
      "images": [],
      "contentSource": "完整文章",
      "content": "Accurate segmentation of organs and tumors in CT and MRI scans is essential for diagnosis, treatment planning, and disease monitoring. While deep learning has advanced automated segmentation, most models remain task-specific, lacking generalizability across modalities and institutions. Vision foundation models (FMs) pretrained on billion-scale natural images offer powerful and transferable representations. However, adapting them to medical imaging faces two key challenges: (1) the ViT backbone of most foundation models still underperform specialized CNNs on medical image segmentation, and (2) the large domain gap between natural and medical images limits transferability. We introduce MedDINOv3, a simple and effective framework for adapting DINOv3 to medical segmentation. We first revisit plain ViTs and design a simple and effective architecture with multi-scale token aggregation. Then, we perform domain-adaptive pretraining on CT-3M, a curated collection of 3.87M axial CT slices, using a multi-stage DINOv3 recipe to learn robust dense features. MedDINOv3 matches or exceeds state-of-the-art performance across four segmentation benchmarks, demonstrating the potential of vision foundation models as unified backbones for medical image segmentation. The code is available at https://github.com/ricklisz/MedDINOv3."
    },
    {
      "title": "DCPO：动态裁剪策略优化 (原标题: DCPO: Dynamic Clipping Policy Optimization)",
      "link": "https://arxiv.org/abs/2509.02333",
      "pubDate": "Tue, 02 Sep 2025 10:01:07 GMT",
      "isoDate": "2025-09-02T10:01:07.000Z",
      "creator": "Shihui Yang, Chengfeng Dou, Peidong Guo, Kai Lu, Qiang Ju, Fei Deng, Rihui Xin",
      "summary": "# DCPO：动态裁剪策略优化\n\n## 1. 背景与问题\n强化学习与可验证奖励（RLVR）是提升大型语言模型（LLM）推理能力的一种有前景的框架。然而，现有方法如GRPO常面临零梯度问题。这主要是由于：\n*   **固定的token级概率比裁剪边界**：限制了token级别的探索。\n*   **相同奖励的标准化**：导致梯度更新效率低下，并对生成响应的利用不足。\n\n## 2. 提出的方法：DCPO\n本文提出了**动态裁剪策略优化（DCPO）**，通过引入两种核心技术来解决上述问题：\n\n### 2.1 动态裁剪策略\n*   **目标**：增强token级别的探索。\n*   **机制**：根据token特定的先验概率自适应地调整裁剪边界，而非使用固定边界。\n\n### 2.2 平滑优势标准化技术\n*   **目标**：提高生成响应在响应级别的有效利用。\n*   **机制**：在累积训练步骤中对奖励进行标准化。\n\n## 3. 实验结果与性能\nDCPO在四个基准测试上，基于四种不同的模型，均取得了最先进的性能：\n\n*   **AIME24基准测试 (基于Qwen2.5-Math-7B模型)**：\n    *   DCPO在贪婪解码下Avg@1达到46.7，在32次采样下Avg@32达到38.8。\n    *   显著超越DAPO (36.7/31.6) 和 GRPO (36.7/32.1)。\n\n*   **AIME25基准测试 (基于Qwen2.5-14B模型)**：\n    *   DCPO性能达到(23.3/19.0)。\n    *   超越GRPO (13.3/10.5) 和 DAPO (20.0/15.3)。\n\n*   **其他显著改进**：\n    *   在四种模型中，DCPO的非零优势（nonzero advantage）比GRPO平均提高了28%。\n    *   训练效率比DAPO提高了一倍。\n    *   与GRPO和DAPO相比，token裁剪率显著降低了一个数量级。\n\n## 4. 结论\n这些结果表明，DCPO在大型语言模型的强化学习中，能够更有效地利用生成数据，从而显著提升性能。",
      "shortSummary": "DCPO（动态裁剪策略优化）旨在解决RLVR中现有方法（如GRPO）因固定裁剪边界和奖励标准化导致的零梯度问题。DCPO引入了动态裁剪策略，根据token先验概率自适应调整裁剪边界以增强探索，并采用平滑优势标准化技术提高响应利用率。实验表明，DCPO在四个基准测试上均达到SOTA性能，显著超越DAPO和GRPO，平均非零优势提高28%，训练效率翻倍，并大幅降低了token裁剪率，有效提升了LLM强化学习中生成数据的利用效率。",
      "translated_title": "DCPO：动态裁剪策略优化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a promising framework for enhancing the reasoning capabilities of large language models. However, existing approaches such as GRPO often suffer from zero gradients. This problem arises primarily due to fixed clipping bounds for token-level probability ratios and the standardization of identical rewards, which can lead to ineffective gradient updates and underutilization of generated responses. In this work, we propose Dynamic Clipping Policy Optimization (DCPO), which introduces a dynamic clipping strategy that adaptively adjusts the clipping bounds based on token-specific prior probabilities to enhance token-level exploration, and a smooth advantage standardization technique that standardizes rewards across cumulative training steps to improve the response-level effective utilization of generated responses. DCPO achieved state-of-the-art performance on four benchmarks based on four different models. In particular, DCPO achieved an Avg@1 of 46.7 under greedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24 benchmark, surpassing both DAPO (36.7/31.6) and GRPO (36.7/32.1) on the Qwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO achieves a performance of (23.3/19.0), surpassing GRPO (13.3/10.5) and DAPO (20.0/15.3). Furthermore, DCPO achieved an average 28% improvement in the nonzero advantage over GRPO in four models, doubled the training efficiency over DAPO, and significantly reduced the token clipping ratio by an order of magnitude compared to both GRPO and DAPO, while achieving superior performance. These results highlight DCPO's effectiveness in leveraging generated data more efficiently for reinforcement learning in large language models."
    },
    {
      "title": "Baichuan-M2：通过大型验证系统提升医疗能力 (原标题: Baichuan-M2: Scaling Medical Capability with Large Verifier System)",
      "link": "https://arxiv.org/abs/2509.02208",
      "pubDate": "Tue, 02 Sep 2025 07:23:35 GMT",
      "isoDate": "2025-09-02T07:23:35.000Z",
      "creator": "Baichuan-M2 Team, Chengfeng Dou, Chong Liu, Fan Yang, Fei Li, Jiyuan Jia, Mingyang Chen, Qiang Ju, Shuai Wang, Shunya Dang, Tianpeng Li, Xiangrong Zeng, Yijie Zhou, Chenzheng Zhu, Da Pan, Fei Deng, Guangwei Ai, Guosheng Dong, Hongda Zhang, Jinyang Tai, Jixiang Hong, Kai Lu, Linzhuang Sun, Peidong Guo, Qian Ma, Rihui Xin, Shihui Yang, Shusen Zhang, Yichuan Mo, Zheng Liang, Zhishou Zhang, Hengfu Cui, Zuyi Zhu, Xiaochuan Wang",
      "summary": "## Baichuan-M2：通过大型验证系统提升医疗能力\n\n### 引言\n\n随着大型语言模型（LLMs）在对话和推理能力方面的显著进步，它们在医疗领域的实际应用已成为一个关键的研究焦点。然而，当前医疗LLMs在诸如美国执业医师资格考试（USMLE）等静态基准测试上的表现，与它们在真实世界临床决策中的实用性之间存在着明显的差距。这种差异主要源于传统考试无法捕捉医疗咨询所固有的动态和交互性质。\n\n### 解决方案：动态验证框架\n\n为了解决这一挑战，本研究引入了一种新颖的动态验证框架。该框架超越了传统的静态答案验证器，建立了一个大规模、高保真度的交互式强化学习系统，旨在更真实地模拟临床环境。\n\n#### 框架核心组件：\n\n1.  **患者模拟器（Patient Simulator）**：\n    *   利用去识别化的医疗记录，创建逼真的临床环境和患者案例。\n    *   这使得模型能够在接近真实世界的场景中进行交互和学习。\n\n2.  **临床评估标准生成器（Clinical Rubrics Generator）**：\n    *   动态生成多维度的评估指标。\n    *   这些指标能够更全面、细致地评估LLMs在复杂医疗情境中的表现，而不仅仅是基于单一的正确答案。\n\n### Baichuan-M2 模型\n\n基于上述动态验证框架，研究团队开发了 **Baichuan-M2**，一个拥有320亿参数的医疗增强推理模型。\n\n*   **训练策略**：Baichuan-M2 通过一种多阶段强化学习策略进行训练，并采用了改进的 **群组相对策略优化（GRPO）** 算法，以优化其在动态医疗环境中的决策能力。\n\n### 性能评估\n\nBaichuan-M2 在 **HealthBench** 基准测试上进行了严格评估，结果显示其性能卓越：\n\n*   **超越竞争对手**：Baichuan-M2 在HealthBench上超越了所有其他开源模型，并且表现优于大多数先进的闭源模型。\n*   **HealthBench Hard 挑战**：在极具挑战性的HealthBench Hard基准测试中，Baichuan-M2 的得分超过了32分。此前，只有GPT-5 曾达到过这一水平，这凸显了Baichuan-M2 在复杂医疗推理方面的强大能力。\n\n### 结论与意义\n\n本研究工作有力地证明，构建一个强大且动态的验证系统对于使大型语言模型的能力与实际临床应用需求保持一致至关重要。Baichuan-M2 的成功部署和卓越表现，为医疗AI在性能-参数权衡方面建立了新的帕累托前沿，预示着医疗AI在真实世界应用中将迎来更广阔的前景。",
      "shortSummary": "医疗LLM在静态基准测试与实际临床应用间存在差距。Baichuan-M2引入动态验证框架，包含患者模拟器和临床评估标准生成器，以解决此问题。该320亿参数模型通过多阶段强化学习（GRPO）训练，在HealthBench Hard上表现卓越，得分超过32分（此前仅GPT-5达到）。这证明动态验证系统对医疗LLM的实际应用至关重要，并为医疗AI树立了新标杆。",
      "translated_title": "Baichuan-M2：通过大型验证系统提升医疗能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "As large language models (LLMs) advance in conversational and reasoning capabilities, their practical application in healthcare has become a critical research focus. However, there is a notable gap between the performance of medical LLMs on static benchmarks such as USMLE and their utility in real-world clinical decision-making. This discrepancy arises because traditional exams fail to capture the dynamic, interactive nature of medical consultations. To address this challenge, we introduce a novel dynamic verification framework that moves beyond static answer verifier, establishing a large-scale, high-fidelity interactive reinforcement learning system. Our framework comprises two key components: a Patient Simulator that creates realistic clinical environments using de-identified medical records, and a Clinical Rubrics Generator that dynamically produces multi-dimensional evaluation metrics. Building on this foundation, we develop Baichuan-M2, a 32B-parameter medical augmented reasoning model trained through a multi-stage reinforcement learning strategy with an improved Group Relative Policy Optimization (GRPO) algorithm. Evaluated on HealthBench, Baichuan-M2 outperforms all other open-source models and most advanced closed-source counterparts, achieving a score above 32 on the challenging HealthBench Hard benchmark-previously exceeded only by GPT-5. Our work demonstrates that robust dynamic verifier system is essential for aligning LLM capabilities with practical clinical applications, establishing a new Pareto front in the performance-parameter trade-off for medical AI deployment."
    },
    {
      "title": "AMBEDKAR：一种通过知识增强的解码方法实现多级偏见消除，以实现语言模型的稳健宪法对齐 (原标题: AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models)",
      "link": "https://arxiv.org/abs/2509.02133",
      "pubDate": "Tue, 02 Sep 2025 05:33:30 GMT",
      "isoDate": "2025-09-02T05:33:30.000Z",
      "creator": "Snehasis Mukhopadhyay, Aryan Kasat, Shivam Dubey, Rahul Karthikeyan, Dhruv Sood, Vinija Jain, Aman Chadha, Amitava Das",
      "summary": "### AMBEDKAR：通过知识增强的解码方法消除语言模型中的多级偏见\n\n**1. 问题背景与挑战**\n*   **社会偏见反映**：大型语言模型（LLMs）在训练过程中无意中吸收了社会偏见，导致其输出可能带有有害或歧视性内容。\n*   **印度特定偏见**：在印度，对现有模型的评估显示，与种姓和宗教相关的偏见尤为突出和显著。\n*   **现有策略局限**：大多数现有的偏见缓解策略以西方为中心，未能有效解决印度语境下的这些本地化细微差别。\n\n**2. AMBEDKAR框架的提出**\n*   **灵感来源**：该框架名为AMBEDKAR，其灵感来源于印度宪法的主要设计者B. R. Ambedkar博士的平等主义愿景。\n*   **核心目标**：旨在引导LLM的输出符合印度宪法第14至17条所倡导的公平、中立和包容原则，实现语言模型的稳健宪法对齐。\n\n**3. 核心技术方法**\n*   **宪法感知解码层**：AMBEDKAR引入了一个“宪法感知解码层”，该层由“印度人工智能宪法”指导。\n    *   **应用时机**：此层仅在推理时应用，不涉及对基础模型参数的任何更新，从而避免了重新训练的计算和基础设施成本。\n*   **推测性解码算法**：框架整合了一种推测性解码算法，旨在在内容生成过程中主动减少种姓主义和社群偏见。\n    *   **操作机制**：该偏见缓解层直接在解码过程中发挥作用，避免了对模型内部结构的修改。\n\n**4. 推测性解码的重新诠释**\n*   **范式转变**：AMBEDKAR将推测性解码重新诠释为一种实现公平的机制，而非仅仅是提高效率的工具。\n*   **角色分工**：\n    *   **小型语言模型（SLM）**：充当潜在的偏见生成器。\n    *   **宪法指导的大型语言模型（LLM）**：作为验证器。\n*   **验证器作用**：LLM在此框架中的作用不是加速生成，而是强制SLM的输出遵循无偏见的轨迹，从而形成一种“通过推测实现公平”的新范式。\n\n**5. 实验结果与资源**\n*   **偏见减少**：与基线模型相比，AMBEDKAR方法实现了高达26.41%的绝对偏见减少。\n*   **资源可用性**：项目的源代码、数据集和实验结果已公开提供。",
      "shortSummary": "AMBEDKAR框架旨在通过知识增强的解码方法，消除大型语言模型（LLMs）中的多级偏见，特别是在印度背景下突出的种姓和宗教偏见。该框架受印度宪法启发，引入一个宪法感知解码层，并在推理时利用重新诠释的推测性解码算法。其中，一个小型语言模型生成内容，一个受宪法指导的大型语言模型进行验证，以确保输出符合公平、中立和包容的原则。该方法在不修改基础模型参数的情况下，实现了高达26.41%的偏见减少。",
      "translated_title": "AMBEDKAR：一种通过知识增强的解码方法实现多级偏见消除，以实现语言模型的稳健宪法对齐",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) can inadvertently reflect societal biases present in their training data, leading to harmful or prejudiced outputs. In the Indian context, our empirical evaluations across a suite of models reveal that biases around caste and religion are particularly salient. Yet, most existing mitigation strategies are Western-centric and fail to address these local nuances. We propose AMBEDKAR, a framework inspired by the egalitarian vision of Dr B. R. Ambedkar, architect of the Indian Constitution, to guide LLM outputs toward fairness, neutrality, and inclusion in line with Articles 14 to 17. Our approach introduces a Constitution-Aware Decoding Layer, guided by the AI Constitution of India and applied only at inference time, without any parameter updates to the base model. We incorporate a speculative decoding algorithm that proactively reduces casteist and communal bias during generation. This mitigation layer operates directly within the decoding process, avoiding changes to model internals and lowering the computational and infrastructural costs associated with retraining. We reinterpret speculative decoding not merely as an efficiency tool but as a mechanism for fairness. In this framework, a Small Language Model (SLM) acts as a potentially biased generator, while a constitutionally guided Large Language Model (LLM) serves as the verifier. Rather than accelerating generation, the LLM enforces bias-robust trajectories in the SLM outputs. This inversion of roles gives rise to a fairness-by-speculation paradigm. Our approach yields an absolute reduction of bias up to 26.41 percent compared to baseline. Our source code, datasets, and results are available at https://anonymous.4open.science/r/AMBEDKAR-983B/"
    },
    {
      "title": "出色的预训练优化器及其寻找方法 (原标题: Fantastic Pretraining Optimizers and Where to Find Them)",
      "link": "https://arxiv.org/abs/2509.02046",
      "pubDate": "Tue, 02 Sep 2025 03:43:22 GMT",
      "isoDate": "2025-09-02T03:43:22.000Z",
      "creator": "Kaiyue Wen, David Hall, Tengyu Ma, Percy Liang",
      "summary": "# 预训练优化器系统性研究：超越AdamW的性能评估\n\n## 引言\n\nAdamW长期以来一直是语言模型预训练领域的主导优化器。尽管有许多替代优化器声称能提供1.4到2倍的速度提升，但本文作者认为，这些声称往往是由于两种方法论上的缺陷所致：\n1.  **超参数调优不公平**：不同优化器之间未进行同等程度的超参数优化。\n2.  **评估设置有限或具有误导性**：评估环境和方法未能全面反映优化器的真实性能。\n\n## 研究方法与目的\n\n为了解决上述问题并进行公平、信息丰富的比较，研究人员对十种深度学习优化器进行了系统性研究。这项研究涵盖了：\n*   **四种模型规模**：从0.1亿参数到1.2亿参数。\n*   **数据与模型比例**：从Chinchilla最优值的1倍到8倍。\n\n## 主要发现\n\n通过这项彻底的调查，研究得出了以下关键结论：\n\n1.  **公平比较的关键要求**：\n    *   需要对超参数进行严格的调优。\n    *   需要在训练结束时，在不同模型规模和数据与模型比例下进行全面的评估。\n\n2.  **超参数调优的重要性**：\n    *   一个优化器的最佳超参数可能对另一个优化器来说是次优的。\n    *   盲目地转移超参数会导致不公平的比较结果。\n\n3.  **实际加速效果低于声称**：\n    *   许多被提出的优化器相对于经过良好调优的基线（如AdamW），其实际加速效果远低于声称。\n    *   这种加速效果会随着模型规模的增大而降低，对于1.2亿参数的模型，加速仅为1.1倍。\n\n4.  **中间检查点评估的误导性**：\n    *   在达到目标训练预算之前比较中间检查点可能会产生误导性结果。\n    *   由于学习率衰减等因素，两个优化器之间的排名在训练过程中可能会发生翻转。\n\n5.  **最快优化器的特性**：\n    *   所有最快的优化器，例如Muon和Soap，都使用矩阵作为预处理器。\n    *   这意味着它们通过矩阵而非逐元素标量来乘以梯度，从而实现更有效的更新。\n\n6.  **矩阵优化器加速效果与模型规模的关系**：\n    *   矩阵优化器的加速效果与模型规模成反比。\n    *   对于0.1亿参数的模型，相对于AdamW可实现1.4倍的加速。\n    *   然而，对于1.2亿参数的模型，加速效果显著下降，仅为1.1倍。\n\n## 结论\n\n尽管一些优化器在较小规模的模型上表现出显著的加速优势，但随着模型规模的增大，其相对于AdamW的优势逐渐减弱。进行公平且有意义的优化器比较，需要全面的超参数调优，并在训练结束时，在多种模型规模和数据与模型比例下进行评估。",
      "shortSummary": "本文系统研究了十种预训练优化器，旨在纠正AdamW替代者声称的加速效果中存在的超参数调优不公和评估误导问题。研究发现，公平比较需严格调优并在训练结束时评估。结果显示，许多优化器的实际加速低于声称，且随模型规模增大而减弱，1.2B模型仅1.1倍。最快的优化器（如Muon）使用矩阵预处理器，但其加速效果与模型规模成反比，在大模型上优势不明显。",
      "translated_title": "出色的预训练优化器及其寻找方法",
      "images": [],
      "contentSource": "完整文章",
      "content": "AdamW has long been the dominant optimizer in language model pretraining, despite numerous claims that alternative optimizers offer 1.4 to 2x speedup. We posit that two methodological shortcomings have obscured fair comparisons and hindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited or misleading evaluation setups. To address these two issues, we conduct a systematic study of ten deep learning optimizers across four model scales (0.1B-1.2B parameters) and data-to-model ratios (1-8x the Chinchilla optimum). We find that fair and informative comparisons require rigorous hyperparameter tuning and evaluations across a range of model scales and data-to-model ratios, performed at the end of training. First, optimal hyperparameters for one optimizer may be suboptimal for another, making blind hyperparameter transfer unfair. Second, the actual speedup of many proposed optimizers over well-tuned baselines is lower than claimed and decreases with model size to only 1.1x for 1.2B parameter models. Thirdly, comparing intermediate checkpoints before reaching the target training budgets can be misleading, as rankings between two optimizers can flip during training due to learning rate decay. Through our thorough investigation, we find that all the fastest optimizers such as Muon and Soap, use matrices as preconditioners -- multiplying gradients with matrices rather than entry-wise scalars. However, the speedup of matrix-based optimizers is inversely proportional to model scale, decreasing from 1.4x over AdamW for 0.1B parameter models to merely 1.1x for 1.2B parameter models."
    },
    {
      "title": "属性即文本基因：利用大型语言模型作为遗传算法模拟器进行条件合成数据生成 (原标题: Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm Simulators for Conditional Synthetic Data Generation)",
      "link": "https://arxiv.org/abs/2509.02040",
      "pubDate": "Tue, 02 Sep 2025 03:35:20 GMT",
      "isoDate": "2025-09-02T03:35:20.000Z",
      "creator": "Guangzeng Han, Weisi Liu, Xiaolei Huang",
      "summary": "# Genetic Prompt：一种基于LLM的遗传算法合成数据生成框架\n\n## 引言\n大型语言模型（LLMs）在生成合成数据方面展现出强大能力，但如何确保生成数据的质量和多样性仍是当前面临的关键挑战。\n\n## 核心方法：Genetic Prompt 框架\nGenetic Prompt 是一种新颖的框架，旨在通过结合遗传算法（GAs）与LLMs来增强合成数据的生成。\n\n*   **“文本基因”概念**：该方法将语义文本属性视为“基因序列”。\n*   **LLM的角色**：LLM被巧妙地用于模拟遗传算法中的“交叉”（crossover）和“突变”（mutation）操作。\n*   **目标与优势**：通过这种模拟遗传过程，Genetic Prompt 能够创建新颖的属性组合，从而显著提高生成数据的质量和多样性，使合成数据分布更接近真实世界数据。\n\n## 优化策略：主动学习\n为了进一步优化“亲本选择”（parent selection）过程，该框架还整合了一种主动学习（active learning）机制。\n\n*   **目的**：此机制旨在扩展“后代”（offspring）的搜索空间，从而进一步提升合成数据的生成效果和多样性。\n\n## 实验与关键发现\n研究团队在多个自然语言处理（NLP）任务上进行了广泛的实验，并取得了以下关键发现：\n\n*   **性能超越基线**：Genetic Prompt 在性能上显著优于现有的最先进基线方法。\n*   **鲁棒性表现**：该框架在不同规模和大小的生成器模型上均表现出稳健的性能。\n*   **下游任务提升**：将Genetic Prompt生成的合成数据与原始训练集融合后，显著提升了下游模型的性能，尤其是在类别不平衡（class-imbalanced）的场景中，效果更为明显。\n\n## 结论\n研究结果有力地验证了Genetic Prompt 是一种高效且有效的方法，能够为广泛的NLP应用生成高质量的合成数据。\n\n## 其他信息\n*   该研究已被EMNLP2025 Findings接受。\n*   研究领域：计算与语言 (cs.CL)。",
      "shortSummary": "Genetic Prompt 是一种结合遗传算法与大型语言模型（LLMs）的新型框架，旨在提升合成数据的质量和多样性。它将文本属性视为“基因”，利用LLM模拟交叉和突变操作，生成更接近真实数据分布的新颖属性组合。通过整合主动学习优化亲本选择，该框架在多项NLP任务中显著优于现有方法，并能有效提升下游模型性能，尤其在类别不平衡场景下表现突出。研究证实Genetic Prompt能为NLP应用生成高质量合成数据。",
      "translated_title": "属性即文本基因：利用大型语言模型作为遗传算法模拟器进行条件合成数据生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) excel at generating synthetic data, but ensuring its quality and diversity remains challenging. We propose Genetic Prompt, a novel framework that combines genetic algorithms with LLMs to augment synthetic data generation. Our approach treats semantic text attributes as gene sequences and leverages the LLM to simulate crossover and mutation operations. This genetic process enhances data quality and diversity by creating novel attribute combinations, yielding synthetic distributions closer to real-world data. To optimize parent selection, we also integrate an active learning scheme that expands the offspring search space. Our experiments on multiple NLP tasks reveal several key findings: Genetic Prompt not only significantly outperforms state-of-the-art baselines but also shows robust performance across various generator model sizes and scales. Moreover, we demonstrate that fusing our synthetic data with the original training set significantly boosts downstream model performance, particularly for class-imbalanced scenarios. Our findings validate that Genetic Prompt is an effective method for producing high-quality synthetic data for a wide range of NLP applications."
    },
    {
      "title": "用于下一尺度自回归文本图像编辑的离散噪声反演 (原标题: Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing)",
      "link": "https://arxiv.org/abs/2509.01984",
      "pubDate": "Tue, 02 Sep 2025 02:01:52 GMT",
      "isoDate": "2025-09-02T02:01:52.000Z",
      "creator": "Quan Dao, Xiaoxiao He, Ligong Han, Ngan Hoai Nguyen, Amin Heyrani Nobar, Faez Ahmed, Han Zhang, Viet Anh Nguyen, Dimitris Metaxas",
      "summary": "## VARIN：基于离散噪声反演的自回归文本图像编辑\n\n### 引言\n视觉自回归模型（VAR）作为一类新兴的生成模型，在文本到图像生成任务中已展现出与扩散模型相当的性能。然而，除了条件生成，无需额外训练的提示引导图像编辑能力同样至关重要，因为它支持众多实际应用。\n\n### 研究目的\n本文旨在深入探索VAR模型的文本到图像编辑能力，以满足实际应用中对灵活图像修改的需求。\n\n### 核心贡献：VARIN\n1.  **提出VARIN技术**：引入了“视觉自回归逆噪声”（Visual AutoRegressive Inverse Noise, VARIN），这是首个专为VAR模型设计的基于噪声反演的编辑技术。\n2.  **引入LAI函数**：VARIN利用了一种新颖的伪逆函数，名为“位置感知最大值反演”（Location-aware Argmax Inversion, LAI），用于argmax采样过程。\n3.  **生成逆Gumbel噪声**：LAI能够有效地生成逆Gumbel噪声，这是实现精确编辑的关键。\n4.  **实现精确可控编辑**：这些生成的逆噪声使得源图像能够被精确重建，并能够实现与文本提示对齐的、有针对性且高度可控的图像编辑。\n\n### 实验结果与验证\n*   **广泛实验**：通过广泛的实验，本文证明了VARIN方法的有效性。\n*   **有效修改**：VARIN能够根据指定的文本提示有效地修改源图像，实现预期的编辑效果。\n*   **细节保留**：在修改图像的同时，该方法显著保留了原始背景和结构细节，确保了编辑的自然性和一致性。\n*   **实用性验证**：这些积极的实验结果验证了VARIN作为一种实用且高效的图像编辑方法的有效性。\n\n### 研究意义\nVARIN为VAR模型提供了一种无需额外训练即可进行文本引导图像编辑的有效途径，极大地拓展了VAR模型在图像编辑领域的应用潜力和实用价值。",
      "shortSummary": "本文介绍了VARIN（Visual AutoRegressive Inverse Noise），一种专为视觉自回归模型（VAR）设计的文本图像编辑技术。VARIN利用新颖的“位置感知最大值反演”（LAI）函数生成逆Gumbel噪声，实现源图像的精确重建和基于文本提示的精确可控编辑。实验证明，VARIN能有效修改图像，同时显著保留原始背景和结构细节，验证了其作为实用编辑方法的有效性，无需额外训练。",
      "translated_title": "用于下一尺度自回归文本图像编辑的离散噪声反演",
      "images": [],
      "contentSource": "完整文章",
      "content": "Visual autoregressive models (VAR) have recently emerged as a promising class of generative models, achieving performance comparable to diffusion models in text-to-image generation tasks. While conditional generation has been widely explored, the ability to perform prompt-guided image editing without additional training is equally critical, as it supports numerous practical real-world applications. This paper investigates the text-to-image editing capabilities of VAR by introducing Visual AutoRegressive Inverse Noise (VARIN), the first noise inversion-based editing technique designed explicitly for VAR models. VARIN leverages a novel pseudo-inverse function for argmax sampling, named Location-aware Argmax Inversion (LAI), to generate inverse Gumbel noises. These inverse noises enable precise reconstruction of the source image and facilitate targeted, controllable edits aligned with textual prompts. Extensive experiments demonstrate that VARIN effectively modifies source images according to specified prompts while significantly preserving the original background and structural details, thus validating its efficacy as a practical editing approach."
    },
    {
      "title": "MOSAIC：通过对应感知对齐和解耦实现多主体个性化生成 (原标题: MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement)",
      "link": "https://arxiv.org/abs/2509.01977",
      "pubDate": "Tue, 02 Sep 2025 01:40:07 GMT",
      "isoDate": "2025-09-02T01:40:07.000Z",
      "creator": "Dong She, Siming Fu, Mushui Liu, Qiaoqiao Jin, Hualiang Wang, Mu Liu, Jidong Jiang",
      "summary": "## MOSAIC：通过对应感知对齐和解耦实现多主体个性化生成\n\n### 引言\n\n多主体个性化生成在合成基于多个参考主体的图像时，面临着维护身份保真度和语义一致性的独特挑战。现有方法由于未能充分建模不同主体在共享表示空间中的交互方式，常常导致身份融合和属性泄露。\n\n### MOSAIC框架概述\n\n本文提出了MOSAIC，一个以表示为中心的框架，通过显式语义对应和正交特征解耦，重新思考了多主体生成问题。其核心洞察在于，多主体生成需要在表示层面实现精确的语义对齐——即精确地知道生成图像中的哪些区域应该关注每个参考主体的哪些部分。\n\n### 主要贡献与技术细节\n\n1.  **SemAlign-MS数据集**：\n    *   为了实现精确的语义对齐，研究人员引入了SemAlign-MS，这是一个精心标注的新数据集。\n    *   该数据集提供了多参考主体与目标图像之间细粒度的语义对应关系，填补了该领域此前缺乏此类资源的空白。\n\n2.  **语义对应注意力损失 (Semantic Correspondence Attention Loss)**：\n    *   基于SemAlign-MS数据集，研究人员提出了语义对应注意力损失。\n    *   该损失函数旨在强制执行精确的点对点语义对齐，从而确保从每个参考主体到其指定区域的高度一致性。\n\n3.  **多参考解耦损失 (Multi-Reference Disentanglement Loss)**：\n    *   为了防止特征干扰并同时保留个体身份特征，MOSAIC开发了多参考解耦损失。\n    *   该损失将不同主体推入正交的注意力子空间，有效避免了不同主体特征之间的混淆。\n\n### 实验结果与优势\n\n广泛的实验证明，MOSAIC在多个基准测试中实现了最先进的性能。值得注意的是，现有方法在处理超过3个主体时性能通常会下降，而MOSAIC能够对4个或更多参考主体保持高保真度，这为复杂的、多主体合成应用开辟了新的可能性。",
      "shortSummary": "MOSAIC是一个用于多主体个性化生成的框架，旨在解决现有方法中身份融合和属性泄露的问题。它通过引入新数据集SemAlign-MS、语义对应注意力损失实现精确的点对点语义对齐，并利用多参考解耦损失将不同主体推入正交特征空间。实验表明，MOSAIC在多主体生成任务上达到了最先进的性能，尤其在处理4个以上参考主体时仍能保持高保真度，超越了现有方法的局限，为复杂的多主体合成应用提供了新可能。",
      "translated_title": "MOSAIC：通过对应感知对齐和解耦实现多主体个性化生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multi-subject personalized generation presents unique challenges in maintaining identity fidelity and semantic coherence when synthesizing images conditioned on multiple reference subjects. Existing methods often suffer from identity blending and attribute leakage due to inadequate modeling of how different subjects should interact within shared representation spaces. We present MOSAIC, a representation-centric framework that rethinks multi-subject generation through explicit semantic correspondence and orthogonal feature disentanglement. Our key insight is that multi-subject generation requires precise semantic alignment at the representation level - knowing exactly which regions in the generated image should attend to which parts of each reference. To enable this, we introduce SemAlign-MS, a meticulously annotated dataset providing fine-grained semantic correspondences between multiple reference subjects and target images, previously unavailable in this domain. Building on this foundation, we propose the semantic correspondence attention loss to enforce precise point-to-point semantic alignment, ensuring high consistency from each reference to its designated regions. Furthermore, we develop the multi-reference disentanglement loss to push different subjects into orthogonal attention subspaces, preventing feature interference while preserving individual identity characteristics. Extensive experiments demonstrate that MOSAIC achieves state-of-the-art performance on multiple benchmarks. Notably, while existing methods typically degrade beyond 3 subjects, MOSAIC maintains high fidelity with 4+ reference subjects, opening new possibilities for complex multi-subject synthesis applications."
    },
    {
      "title": "OpenVision 2：多模态学习的生成式预训练视觉编码器家族 (原标题: OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning)",
      "link": "https://arxiv.org/abs/2509.01644",
      "pubDate": "Mon, 01 Sep 2025 13:38:21 GMT",
      "isoDate": "2025-09-01T13:38:21.000Z",
      "creator": "Yanqing Liu, Xianhang Li, Letian Zhang, Zirui Wang, Zeyu Zheng, Yuyin Zhou, Cihang Xie",
      "summary": "## OpenVision 2：一种简化的多模态生成式视觉编码器\n\n本文介绍了OpenVision 2，这是对OpenVision架构和损失设计进行简化后的新版本，旨在显著提高训练效率。\n\n### 核心改进与设计理念\n\n*   **架构简化**：OpenVision 2移除了原始模型中的文本编码器和对比损失（contrastive loss）。\n*   **训练信号**：仅保留了标题生成损失（captioning loss）作为纯粹的生成式训练信号。\n*   **灵感来源**：这一简化设计借鉴了CapPa和AIMv2等先前的视觉-语言预训练工作，以及LLaVA等现代多模态模型的设计理念。\n\n### 性能与效率提升\n\n尽管进行了大幅简化，OpenVision 2展现出令人鼓舞的初步结果：\n\n*   **竞争力**：在广泛的多模态基准测试中，OpenVision 2的性能与原始模型保持了竞争力。\n*   **训练时间显著减少**：\n    *   以ViT-L/14为例，训练时间从83小时减少到57小时，缩短了约1.5倍。\n*   **内存消耗大幅降低**：\n    *   内存使用量从24.5GB减少到13.8GB，降低了约1.8倍。\n    *   这使得最大批处理大小（batch size）可以从2k显著增加到8k，从而提高了训练吞吐量。\n*   **可扩展性**：卓越的训练效率使得OpenVision 2能够超越OpenVision中使用的最大视觉编码器，扩展到超过10亿参数的模型规模。\n\n### 未来展望\n\n研究人员坚信，这种轻量级、纯生成式的范式对于未来多模态基础模型中视觉编码器的发展具有重要意义和吸引力。",
      "shortSummary": "OpenVision 2通过移除文本编码器和对比损失，仅保留生成式标题损失，简化了OpenVision的架构。这一改变显著提升了训练效率，将训练时间缩短约1.5倍，内存使用量降低约1.8倍，同时在多模态基准测试中保持了与原始模型相当的竞争力。其优越的训练效率也支持模型扩展至十亿级参数，预示着轻量级、纯生成式范式在未来多模态视觉编码器发展中的巨大潜力。",
      "translated_title": "OpenVision 2：多模态学习的生成式预训练视觉编码器家族",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper provides a simplification on OpenVision's architecture and loss design for enhancing its training efficiency. Following the prior vision-language pretraining works CapPa and AIMv2, as well as modern multimodal designs like LLaVA, our changes are straightforward: we remove the text encoder (and therefore the contrastive loss), retaining only the captioning loss as a purely generative training signal. We name this new version OpenVision 2. The initial results are promising: despite this simplification, OpenVision 2 competitively matches the original model's performance on a broad set of multimodal benchmarks while substantially cutting both training time and memory consumption. For example, with ViT-L/14, it reduces training time by about 1.5x (from 83h to 57h), and memory usage by about 1.8x (from 24.5GB to 13.8GB, equivalently allowing the maximum batch size to grow from 2k to 8k). This superior training efficiency also allows us to scale far beyond the largest vision encoder used in OpenVision, reaching more than 1 billion parameters. We hold a strong belief that this lightweight, generative-only paradigm is compelling for future vision encoder development in multimodal foundation models."
    },
    {
      "title": "通过向同行小组学习来改进大型视觉和语言模型 (原标题: Improving Large Vision and Language Models by Learning from a Panel of Peers)",
      "link": "https://arxiv.org/abs/2509.01610",
      "pubDate": "Mon, 01 Sep 2025 12:43:48 GMT",
      "isoDate": "2025-09-01T12:43:48.000Z",
      "creator": "Jefferson Hernandez, Jing Shi, Simon Jenni, Vicente Ordonez, Kushal Kafle",
      "summary": "## 通过向同行小组学习改进大型视觉和语言模型\n\n### 背景与挑战\n\n传统的**大型视觉和语言模型（LVLMs）**对齐方法主要依赖于人工标注的偏好数据。然而，这种方法存在显著局限性：\n\n*   **成本高昂**：人工生成偏好数据需要大量资源和时间。\n*   **质量受限**：机器生成的偏好数据在质量上往往不足。\n*   **幻觉问题**：自监督偏好数据在生成过程中常引入不准确或虚构的信息（幻觉）。\n\n### 提出的方法：同行小组学习框架 (Panel-of-Peers)\n\n为了克服上述限制，研究提出了一种新颖的**“同行小组学习”框架**，其灵感来源于人类之间的协作学习。\n\n*   **核心机制**：该方法利用一个由多个LVLMs组成的“小组”，每个模型通过迭代的自我改进过程，评估并学习其集体输出。\n*   **模拟环境**：它模拟了一个同行评审系统或课堂学习环境，模型根据一组精心策划的提示，生成、评估并不断完善其输出。\n\n### 主要优势与实验成果\n\n*   **无需大量人工数据**：该方法能够在不依赖大量人工标注数据集的情况下，有效提升模型性能。\n*   **性能显著提升**：实验结果表明，在多个基准测试中，该方法带来了显著的性能改进。\n*   **可扩展的替代方案**：同行评估被证明是自监督对齐的一种可扩展的替代方案，解决了传统方法的局限性。\n*   **具体数据**：在十五个基准测试中，采用“同行小组学习”框架后，模型的平均得分从48%提高到57%。\n\n### 其他信息\n\n*   该研究已被 **ICCV 2025** 接受。",
      "shortSummary": "针对LVLM对齐中人工数据成本高、机器数据质量低及自监督数据易产生幻觉的问题，研究提出“同行小组学习”框架。该框架通过模拟同行评审，让多个LVLM迭代评估并学习彼此输出。实验证明，此方法无需大量人工标注数据，显著提升了模型性能，在十五个基准测试中平均得分从48%增至57%，为LVLM对齐提供了一种可扩展的替代方案。该研究已被ICCV 2025接受。",
      "translated_title": "通过向同行小组学习来改进大型视觉和语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Traditional alignment methods for Large Vision and Language Models (LVLMs) primarily rely on human-curated preference data. Human-generated preference data is costly; machine-generated preference data is limited in quality; and self-supervised preference data often introduces hallucinations. To overcome these limitations, we propose a novel Panel-of-Peers learning framework inspired by collaborative learning among humans. This approach leverages a panel of LVLMs, each evaluating and learning from their collective outputs through an iterative self-improvement process. By simulating a peer review system, our models generate, assess, and refine outputs in response to a curated set of prompts, mimicking a classroom learning environment. We demonstrate that this methodology enhances model performance without requiring extensive human-labeled datasets. Our experiments show significant improvement across multiple benchmarks, demonstrating the potential of peer evaluations as a scalable alternative to self-supervised alignment. Notably, we show that Panel-of-Peers increases the average score on fifteen benchmarks from 48% to 57%"
    },
    {
      "title": "ViSTA-SLAM：基于对称双视图关联的视觉SLAM (原标题: ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association)",
      "link": "https://arxiv.org/abs/2509.01584",
      "pubDate": "Mon, 01 Sep 2025 12:12:23 GMT",
      "isoDate": "2025-09-01T12:12:23.000Z",
      "creator": "Ganlin Zhang, Shenhan Qian, Xi Wang, Daniel Cremers",
      "summary": "# ViSTA-SLAM：一种无需相机内参的实时单目视觉SLAM系统\n\nViSTA-SLAM是一个创新的实时单目视觉SLAM系统，其核心优势在于无需预先获取相机内参即可运行，这极大地扩展了其在不同相机设置下的适用性。\n\n## 核心组件与创新点\n\n### 1. 前端：轻量级对称双视图关联（STA）模型\n\n*   **功能：** 该模型能够仅从两张RGB图像中，同时估计相对相机姿态并回归局部点云图（local pointmaps）。\n*   **优势：**\n    *   **模型复杂度显著降低：** ViSTA-SLAM的前端模型大小仅为现有最先进方法的35%。\n    *   **约束质量提升：** 提高了在整个SLAM流程中使用的双视图约束的质量。\n\n### 2. 后端：专门设计的Sim(3)姿态图\n\n*   **功能：** 后端构建了一个专门设计的Sim(3)姿态图，并集成了回环检测（loop closures）机制。\n*   **目的：** 有效解决长期运行中累积的漂移问题，确保系统长时间运行的精度和鲁棒性。\n\n## 性能表现\n\n广泛的实验结果表明，ViSTA-SLAM在相机跟踪和稠密3D重建质量方面均展现出卓越的性能，优于当前主流方法。",
      "shortSummary": "ViSTA-SLAM是一种实时单目视觉SLAM系统，无需相机内参即可运行，适用性广。它采用轻量级对称双视图关联（STA）模型作为前端，从两张RGB图像同时估计相机姿态和局部点云图，显著降低了模型复杂度。后端构建专门的Sim(3)姿态图，通过回环检测解决漂移。该系统在相机跟踪和稠密3D重建方面表现出卓越性能。",
      "translated_title": "ViSTA-SLAM：基于对称双视图关联的视觉SLAM",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present ViSTA-SLAM as a real-time monocular visual SLAM system that operates without requiring camera intrinsics, making it broadly applicable across diverse camera setups. At its core, the system employs a lightweight symmetric two-view association (STA) model as the frontend, which simultaneously estimates relative camera poses and regresses local pointmaps from only two RGB images. This design reduces model complexity significantly, the size of our frontend is only 35\\% that of comparable state-of-the-art methods, while enhancing the quality of two-view constraints used in the pipeline. In the backend, we construct a specially designed Sim(3) pose graph that incorporates loop closures to address accumulated drift. Extensive experiments demonstrate that our approach achieves superior performance in both camera tracking and dense 3D reconstruction quality compared to current methods. Github repository: https://github.com/zhangganlin/vista-slam"
    },
    {
      "title": "快手 Keye-VL 1.5 技术报告 (原标题: Kwai Keye-VL 1.5 Technical Report)",
      "link": "https://arxiv.org/abs/2509.01563",
      "pubDate": "Mon, 01 Sep 2025 11:46:58 GMT",
      "isoDate": "2025-09-01T11:46:58.000Z",
      "creator": "Biao Yang, Bin Wen, Boyang Ding, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, Fan Yang, Guorui Zhou, Guowang Zhang, Han Shen, Hao Peng, Haojie Ding, Hao Wang, Hengrui Ju, Jiaming Huang, Jiangxia Cao, Jiankang Chen, Jingyun Hua, Kaibing Chen, Kaiyu Jiang, Kaiyu Tang, Kun Gai, Muhao Wei, Qiang Wang, Ruitao Wang, Sen Na, Shengnan Zhang, Siyang Mao, Sui Huang, Tianke Zhang, Tingting Gao, Wei Chen, Wei Yuan, Xiangyu Wu, Xiao Hu, Xingyu Lu, Yi-Fan Zhang, Yiping Yang, Yulong Chen, Zeyi Lu, Zhenhua Wu, Zhixin Ling, Zhuoran Yang, Ziming Li, Di Xu, Haixuan Gao, Hang Li, Jing Wang, Lejian Ren, Qigen Hu, Qianqian Wang, Shiyao Wang, Xinchen Luo, Yan Li, Yuhang Hu, Zixing Zhang",
      "summary": "## Keye-VL 1.5：面向视频理解的多模态大语言模型技术报告\n\n### 引言\n近年来，大型语言模型（LLMs）取得了显著进展，其能力通过多模态大型语言模型（MLLMs）扩展到了多模态任务。然而，由于视频的动态性和信息密集性，视频理解仍然是一个充满挑战的领域。现有模型在处理视频内容时，难以在空间分辨率和时间覆盖之间取得平衡。\n\n### Keye-VL 1.5 的三大核心创新\nKeye-VL 1.5 旨在通过以下三项关键创新，解决视频理解中的根本性挑战：\n\n1.  **慢速-快速视频编码策略**\n    *   引入了一种新颖的慢速-快速视频编码策略，能够根据帧间相似性动态分配计算资源。\n    *   对于具有显著视觉变化的关键帧，采用更高分辨率进行处理（慢速路径）。\n    *   对于相对静态的帧，则以较低分辨率但增加时间覆盖的方式进行处理（快速路径）。\n\n2.  **渐进式四阶段预训练方法**\n    *   实施了一种系统的渐进式四阶段预训练方法，将模型的上下文长度从8K tokens逐步扩展到128K tokens。\n    *   这一方法使得模型能够处理更长的视频和更复杂的视觉内容。\n\n3.  **全面的后训练流程**\n    *   开发了一个全面的后训练流程，重点关注推理能力增强和人类偏好对齐。\n    *   该流程包括：\n        *   一个五步思维链（chain-of-thought）数据构建过程。\n        *   基于GSPO的迭代强化学习，针对困难案例采用渐进式提示（progressive prompt hinting）。\n        *   以及对齐训练。\n\n### 性能评估\n通过在公共基准上的广泛评估和严格的内部人工评估，Keye-VL 1.5 展示了相对于现有模型的显著改进。该模型在视频理解任务中表现尤为出色，同时在通用多模态基准上保持了竞争力。",
      "shortSummary": "Keye-VL 1.5 是一个针对视频理解的多模态大语言模型，旨在解决视频处理中空间分辨率与时间覆盖的权衡问题。它通过三大创新实现：动态慢速-快速视频编码策略、将上下文长度扩展至128K tokens的渐进式预训练方法，以及增强推理和人类偏好对齐的全面后训练流程。Keye-VL 1.5 在视频理解任务上表现卓越，并在通用多模态基准上保持竞争力。",
      "translated_title": "快手 Keye-VL 1.5 技术报告",
      "images": [
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png",
          "alt": "BibSonomy logo",
          "title": "",
          "position": 1
        },
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png",
          "alt": "Reddit logo",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "In recent years, the development of Large Language Models (LLMs) has significantly advanced, extending their capabilities to multimodal tasks through Multimodal Large Language Models (MLLMs). However, video understanding remains a challenging area due to the dynamic and information-dense nature of videos. Existing models struggle with the trade-off between spatial resolution and temporal coverage when processing video content. We present Keye-VL-1.5, which addresses fundamental challenges in video comprehension through three key innovations. First, we introduce a novel Slow-Fast video encoding strategy that dynamically allocates computational resources based on inter-frame similarity, processing key frames with significant visual changes at higher resolution (Slow pathway) while handling relatively static frames with increased temporal coverage at lower resolution (Fast pathway). Second, we implement a progressive four-stage pre-training methodology that systematically extends the model's context length from 8K to 128K tokens, enabling processing of longer videos and more complex visual content. Third, we develop a comprehensive post-training pipeline focusing on reasoning enhancement and human preference alignment, incorporating a 5-step chain-of-thought data construction process, iterative GSPO-based reinforcement learning with progressive prompt hinting for difficult cases, and alignment training. Through extensive evaluation on public benchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstrates significant improvements over existing models, particularly excelling in video understanding tasks while maintaining competitive performance on general multimodal benchmarks."
    },
    {
      "title": "大型语言模型预训练优化器基准测试 (原标题: Benchmarking Optimizers for Large Language Model Pretraining)",
      "link": "https://arxiv.org/abs/2509.01440",
      "pubDate": "Mon, 01 Sep 2025 08:50:30 GMT",
      "isoDate": "2025-09-01T08:50:30.000Z",
      "creator": "Andrei Semenov, Matteo Pagliardini, Martin Jaggi",
      "summary": "### 大型语言模型预训练优化器基准测试\n\n#### 背景与挑战\n\n*   **LLM优化器多样性与比较难题：** 随着大型语言模型（LLMs）的迅速发展，涌现了大量旨在优化深度学习模型损失的新方法。这些方法声称能带来更快的收敛速度或减少对特定超参数的依赖。\n*   **实验协议不统一：** 验证这些优化器主张所使用的实验协议各不相同，这使得直接比较不同方法的效果变得非常困难。\n\n#### 研究目标与方法\n\n*   **全面评估：** 本研究旨在对近期涌现的优化技术进行全面评估。\n*   **标准化场景：** 评估在标准化的大型语言模型预训练场景下进行，以确保比较的公平性。\n*   **系统性变量：** 研究系统地改变了模型大小、批次大小和训练时长等关键参数。\n*   **仔细调优：** 对每种优化方法都进行了仔细的调优，以确保其性能得到充分发挥。\n\n#### 主要贡献与成果\n\n*   **对实践者的指导：** 为实践者提供了关于哪种优化器最适合特定预训练场景的实用指导。\n*   **对研究人员的启示：** 为未来的优化研究指明了有前景的方向。\n*   **促进可复现性：** 通过发布所有实验代码并确保其完全可复现，本研究旨在帮助未来优化方法的开发和严格的基准测试。",
      "shortSummary": "本研究对大型语言模型（LLMs）预训练中的优化器进行了全面基准测试。鉴于现有方法因实验协议多样而难以比较，研究人员在标准化场景下，通过系统性地改变模型大小、批次大小和训练时长，评估了多种优化技术。研究旨在为实践者提供优化器选择指导，为研究人员指出未来方向，并通过发布可复现代码促进该领域的进步。",
      "translated_title": "大型语言模型预训练优化器基准测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "The recent development of Large Language Models (LLMs) has been accompanied by an effervescence of novel ideas and methods to better optimize the loss of deep learning models. Claims from those methods are myriad: from faster convergence to removing reliance on certain hyperparameters. However, the diverse experimental protocols used to validate these claims make direct comparisons between methods challenging. This study presents a comprehensive evaluation of recent optimization techniques across standardized LLM pretraining scenarios, systematically varying model size, batch size, and training duration. Through careful tuning of each method, we provide guidance to practitioners on which optimizer is best suited for each scenario. For researchers, our work highlights promising directions for future optimization research. Finally, by releasing our code and making all experiments fully reproducible, we hope our efforts can help the development and rigorous benchmarking of future methods."
    }
  ],
  "lastUpdated": "2025-09-04T09:29:13.516Z"
}