{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "GUI-Reflection：赋予多模态GUI模型自反思能力 (原标题: GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior)",
      "link": "https://arxiv.org/abs/2506.08012",
      "pubDate": "Mon, 09 Jun 2025 13:59:57 GMT",
      "isoDate": "2025-06-09T13:59:57.000Z",
      "creator": "Penghao Wu, Shengnan Ma, Bo Wang, Jiaheng Yu, Lewei Lu, Ziwei Liu",
      "summary": "### GUI-Reflection：赋予多模态GUI模型自反思能力\n\n本文提出了一个名为GUI-Reflection的新框架，旨在解决现有多模态大语言模型（MLLMs）在图形用户界面（GUI）自动化中普遍存在的缺乏反思和错误恢复能力的问题。现有GUI模型主要依赖于从几乎无错误的离线轨迹中学习，因此在面对错误时表现不足。\n\n**核心问题与解决方案：**\n\n*   **问题：** 现有GUI模型缺乏自反思和错误恢复能力，因为它们主要从无错误的离线数据中学习。\n*   **解决方案：** GUI-Reflection框架通过专门的训练阶段，明确地将自反思和错误纠正能力整合到端到端的多模态GUI模型中。\n\n**GUI-Reflection框架的关键组成部分和创新点：**\n\n1.  **专用训练阶段：**\n    *   **GUI特定预训练：** 奠定基础。\n    *   **离线监督微调（SFT）：** 利用现有数据进行初步学习。\n    *   **在线反思调优：** 核心阶段，使模型能够在线学习反思和纠错。\n\n2.  **全自动化数据生成与学习：**\n    *   GUI-Reflection实现了完全自动化的数据生成和学习过程，无需任何人工标注，从而促进了自反思行为的出现。\n    *   **可扩展数据管道：** 提出可扩展的数据管道，能够从现有成功的轨迹中自动构建反思和错误纠正数据。\n\n3.  **GUI-Reflection任务套件：**\n    *   与现有GUI模型主要关注接地和UI理解能力不同，GUI-Reflection提出了一个专门的任务套件，用于明确学习和评估面向反思的能力。\n\n4.  **多样化高效的在线环境：**\n    *   构建了一个多样化且高效的环境，用于在移动设备上进行GUI模型的在线训练和数据收集。\n\n5.  **迭代在线反思调优算法：**\n    *   利用上述环境，提出了一种迭代的在线反思调优算法，使模型能够持续增强其反思和错误纠正能力。\n\n**预期成果与发布：**\n\n该框架旨在赋予GUI智能体自反思和纠正能力，为实现更鲁棒、适应性更强、更智能的GUI自动化铺平道路。所有相关数据、模型、环境和工具都将公开发布。",
      "shortSummary": "GUI-Reflection是一个新颖的框架，旨在通过整合自反思和错误纠正能力，革新多模态GUI模型。它通过GUI特定预训练、离线监督微调和在线反思调优等专用训练阶段实现这一目标。该框架采用全自动化数据生成和学习过程，无需人工标注，并引入了GUI-Reflection任务套件和迭代在线反思调优算法。最终目标是使GUI智能体更鲁棒、适应性更强，并实现更智能的GUI自动化。",
      "translated_title": "GUI-Reflection：赋予多模态GUI模型自反思能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal Large Language Models (MLLMs) have shown great potential in revolutionizing Graphical User Interface (GUI) automation. However, existing GUI models mostly rely on learning from nearly error-free offline trajectories, thus lacking reflection and error recovery capabilities. To bridge this gap, we propose GUI-Reflection, a novel framework that explicitly integrates self-reflection and error correction capabilities into end-to-end multimodal GUI models throughout dedicated training stages: GUI-specific pre-training, offline supervised fine-tuning (SFT), and online reflection tuning. GUI-reflection enables self-reflection behavior emergence with fully automated data generation and learning processes without requiring any human annotation. Specifically, 1) we first propose scalable data pipelines to automatically construct reflection and error correction data from existing successful trajectories. While existing GUI models mainly focus on grounding and UI understanding ability, we propose the GUI-Reflection Task Suite to learn and evaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a diverse and efficient environment for online training and data collection of GUI models on mobile devices. 3) We also present an iterative online reflection tuning algorithm leveraging the proposed environment, enabling the model to continuously enhance its reflection and error correction abilities. Our framework equips GUI agents with self-reflection and correction capabilities, paving the way for more robust, adaptable, and intelligent GUI automation, with all data, models, environments, and tools to be released publicly."
    },
    {
      "title": "视觉Transformer不需要训练过的寄存器 (原标题: Vision Transformers Don't Need Trained Registers)",
      "link": "https://arxiv.org/abs/2506.08010",
      "pubDate": "Mon, 09 Jun 2025 13:59:57 GMT",
      "isoDate": "2025-06-09T13:59:57.000Z",
      "creator": "Nick Jiang, Amil Dravid, Alexei Efros, Yossi Gandelsman",
      "summary": "本文研究了视觉Transformer中先前发现的一种现象的潜在机制——高范数token的出现，导致噪声注意力图。研究表明，在多个模型（例如，CLIP，DINOv2）中，一组稀疏的神经元负责将高范数激活集中在异常值token上，从而导致不规则的注意力模式并降低下游视觉处理的性能。现有的解决方案需要从头开始重新训练模型，并添加额外的学习到的寄存器token，但本文提出了一种无需训练的方法来缓解这些伪影。通过将高范数激活从发现的寄存器神经元转移到额外的未经训练的token中，可以模拟寄存器token对已经训练过的模型的影响。实验证明，该方法可以产生更清晰的注意力和特征图，提高多个下游视觉任务中优于基础模型的性能，并达到与使用寄存器token显式训练的模型相当的结果。此外，本文还将测试时寄存器扩展到现成的视觉语言模型，以提高其可解释性。结果表明，测试时寄存器有效地承担了测试时寄存器token的角色，为任何没有发布它们的预训练模型提供了一种无需训练的解决方案。",
      "shortSummary": "本文研究了视觉Transformer中高范数token导致噪声注意力图的现象，发现一组稀疏神经元负责将高范数激活集中在异常值token上。提出了一种无需训练的方法，通过将高范数激活转移到未经训练的token中，模拟寄存器token的效果，从而改善注意力和特征图，提高下游视觉任务的性能，并提高视觉语言模型的可解释性。该方法为没有寄存器token的预训练模型提供了一种无需训练的解决方案。",
      "translated_title": "视觉Transformer不需要训练过的寄存器",
      "images": [],
      "contentSource": "完整文章",
      "content": "We investigate the mechanism underlying a previously identified phenomenon in Vision Transformers -- the emergence of high-norm tokens that lead to noisy attention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a sparse set of neurons is responsible for concentrating high-norm activations on outlier tokens, leading to irregular attention patterns and degrading downstream visual processing. While the existing solution for removing these outliers involves retraining models from scratch with additional learned register tokens, we use our findings to create a training-free approach to mitigate these artifacts. By shifting the high-norm activations from our discovered register neurons into an additional untrained token, we can mimic the effect of register tokens on a model already trained without registers. We demonstrate that our method produces cleaner attention and feature maps, enhances performance over base models across multiple downstream visual tasks, and achieves results comparable to models explicitly trained with register tokens. We then extend test-time registers to off-the-shelf vision-language models to improve their interpretability. Our results suggest that test-time registers effectively take on the role of register tokens at test-time, offering a training-free solution for any pre-trained model released without them."
    },
    {
      "title": "强化预训练 (原标题: Reinforcement Pre-Training)",
      "link": "https://arxiv.org/abs/2506.08007",
      "pubDate": "Mon, 09 Jun 2025 13:59:53 GMT",
      "isoDate": "2025-06-09T13:59:53.000Z",
      "creator": "Qingxiu Dong, Li Dong, Yao Tang, Tianzhu Ye, Yutao Sun, Zhifang Sui, Furu Wei",
      "summary": "## 强化预训练 (RPT) 概述\n\n本文介绍了一种名为“强化预训练 (Reinforcement Pre-Training, RPT)”的新型扩展范式，旨在应用于大型语言模型 (LLM) 和强化学习 (RL) 领域。\n\n### RPT 的核心理念\n\n*   **任务重构**：RPT 将传统的“下一词元预测”任务重新定义为一项推理任务。\n*   **RL 训练**：该推理任务通过强化学习进行训练，模型因正确预测下一词元而获得可验证的奖励。\n\n### RPT 的优势与特点\n\n*   **数据利用效率**：RPT 提供了一种可扩展的方法，能够有效利用海量的文本数据进行通用目的的强化学习，从而避免了对特定领域标注答案的依赖。\n*   **能力提升**：通过激励模型进行下一词元推理的能力，RPT 显著提高了语言模型在预测下一词元方面的准确性。\n*   **基础奠定**：RPT 为后续的强化微调提供了强大的预训练基础。\n\n### 实验结果与展望\n\n*   **扩展性表现**：扩展曲线（scaling curves）表明，增加训练计算量能够持续提升下一词元预测的准确性。\n*   **前景**：这些结果表明，RPT 是一种有效且有前景的扩展范式，有望推动语言模型预训练的进一步发展。",
      "shortSummary": "本文提出“强化预训练 (RPT)”作为大型语言模型和强化学习的新范式。RPT 将下一词元预测重构为RL推理任务，通过可验证奖励进行训练。它能有效利用海量文本数据进行通用RL，显著提升语言模型准确性，并为后续微调奠定基础。实验显示，增加计算资源可持续提高预测准确率，表明RPT是推进语言模型预训练的有效方法。",
      "translated_title": "强化预训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling paradigm for large language models and reinforcement learning (RL). Specifically, we reframe next-token prediction as a reasoning task trained using RL, where it receives verifiable rewards for correctly predicting the next token for a given context. RPT offers a scalable method to leverage vast amounts of text data for general-purpose RL, rather than relying on domain-specific annotated answers. By incentivizing the capability of next-token reasoning, RPT significantly improves the language modeling accuracy of predicting the next tokens. Moreover, RPT provides a strong pre-trained foundation for further reinforcement fine-tuning. The scaling curves show that increased training compute consistently improves the next-token prediction accuracy. The results position RPT as an effective and promising scaling paradigm to advance language model pre-training."
    },
    {
      "title": "Dreamland：利用模拟器和生成模型实现可控世界创建 (原标题: Dreamland: Controllable World Creation with Simulator and Generative Models)",
      "link": "https://arxiv.org/abs/2506.08006",
      "pubDate": "Mon, 09 Jun 2025 13:59:52 GMT",
      "isoDate": "2025-06-09T13:59:52.000Z",
      "creator": "Sicheng Mo, Ziyang Leng, Leon Liu, Weizhen Wang, Honglin He, Bolei Zhou",
      "summary": "## Dreamland：可控世界创建的混合框架\n\n### 挑战\n大型视频生成模型虽然能合成多样化且逼真的动态世界内容，但它们通常缺乏对元素级别的精细控制能力。这限制了它们在场景编辑和训练具身AI智能体方面的应用。\n\n### 解决方案：Dreamland 框架\nDreamland 是一种混合世界生成框架，旨在结合物理模拟器的精细控制能力与大规模预训练生成模型的光照真实感内容输出。其核心设计包括：\n\n*   **分层世界抽象**：设计了一种中间表示，编码像素级和对象级的语义与几何信息，以桥接模拟器和生成模型。\n\n### 核心优势\nDreamland 方法带来了多重优势：\n\n*   **增强可控性**：通过结合模拟器，实现了对世界元素的更精细控制。\n*   **最小化适应成本**：通过早期与真实世界分布对齐，减少了模型适应的成本。\n*   **支持即插即用**：能够直接利用现有及未来的预训练生成模型。\n\n### 数据集\n为了促进混合生成管道的训练和评估，研究团队构建了 **D3Sim 数据集**。\n\n### 实验结果\n实验证明，Dreamland 在性能上超越了现有基线：\n\n*   **图像质量提升**：图像质量提高了50.8%。\n*   **可控性增强**：可控性增强了17.9%。\n*   **具身智能体训练潜力**：在增强具身智能体训练方面展现出巨大潜力。\n\n### 可用性\n项目代码和数据将对外开放。",
      "shortSummary": "Dreamland是一个混合世界生成框架，旨在解决现有大型视频生成模型缺乏精细控制的问题。它结合了物理模拟器的精确控制和生成模型的逼真内容输出，通过分层世界抽象作为中间表示。该框架显著提升了图像质量和可控性（分别提高50.8%和17.9%），并有望增强具身AI智能体的训练。研究团队还构建了D3Sim数据集，代码和数据将公开。",
      "translated_title": "Dreamland：利用模拟器和生成模型实现可控世界创建",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large-scale video generative models can synthesize diverse and realistic visual content for dynamic world creation, but they often lack element-wise controllability, hindering their use in editing scenes and training embodied AI agents. We propose Dreamland, a hybrid world generation framework combining the granular control of a physics-based simulator and the photorealistic content output of large-scale pretrained generative models. In particular, we design a layered world abstraction that encodes both pixel-level and object-level semantics and geometry as an intermediate representation to bridge the simulator and the generative model. This approach enhances controllability, minimizes adaptation cost through early alignment with real-world distributions, and supports off-the-shelf use of existing and future pretrained generative models. We further construct a D3Sim dataset to facilitate the training and evaluation of hybrid generation pipelines. Experiments demonstrate that Dreamland outperforms existing baselines with 50.8% improved image quality, 17.9% stronger controllability, and has great potential to enhance embodied agent training. Code and data will be made available."
    },
    {
      "title": "重新思考多模态扩散Transformer中的跨模态交互 (原标题: Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers)",
      "link": "https://arxiv.org/abs/2506.07986",
      "pubDate": "Mon, 09 Jun 2025 13:54:04 GMT",
      "isoDate": "2025-06-09T13:54:04.000Z",
      "creator": "Zhengyao Lv, Tianlin Pan, Chenyang Si, Zhaoxi Chen, Wangmeng Zuo, Ziwei Liu, Kwan-Yee K. Wong",
      "summary": "### 背景与问题\n\n*   多模态扩散Transformer (MM-DiTs) 在文本驱动的视觉生成方面取得了显著进展，但即使是FLUX等最先进的模型，在实现文本提示与生成内容之间的精确对齐方面仍面临挑战。\n*   研究发现MM-DiT的注意力机制存在两个关键问题，阻碍了对齐：\n    1.  由于视觉和文本模态之间的token不平衡，导致跨模态注意力受到抑制。\n    2.  缺乏时间步感知的注意力加权。\n\n### 提出的方法：温度调整跨模态注意力 (TACA)\n\n*   为解决上述问题，研究者提出了**温度调整跨模态注意力 (TACA)**。\n*   TACA是一种参数高效的方法，通过温度缩放和时间步依赖的调整，动态地重新平衡多模态交互。\n\n### 实验结果与优势\n\n*   当与LoRA微调结合时，TACA显著增强了T2I-CompBench基准上的文本-图像对齐，且计算开销极小。\n*   TACA已在FLUX和SD3.5等最先进模型上进行了测试，证明其能够改善图像-文本对齐，具体体现在：\n    *   物体外观\n    *   属性绑定\n    *   空间关系\n\n### 结论\n\n*   研究结果强调了平衡跨模态注意力在提高文本到图像扩散模型语义保真度方面的重要性。\n\n### 代码可用性\n\n*   相关代码已公开。",
      "shortSummary": "多模态扩散Transformer (MM-DiTs) 在文本-图像对齐方面存在挑战，主要源于注意力机制中的token不平衡和缺乏时间步感知加权。为解决此问题，研究提出了一种参数高效的**温度调整跨模态注意力 (TACA)** 方法。TACA通过动态调整跨模态交互，显著提升了T2I-CompBench基准上的文本-图像对齐精度，尤其在物体外观、属性绑定和空间关系方面。该方法在FLUX和SD3.5等模型上表现出良好效果，且计算开销极小，强调了平衡跨模态注意力对语义保真度的重要性。",
      "translated_title": "重新思考多模态扩散Transformer中的跨模态交互",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress in text-driven visual generation. However, even state-of-the-art MM-DiT models like FLUX struggle with achieving precise alignment between text prompts and generated content. We identify two key issues in the attention mechanism of MM-DiT, namely 1) the suppression of cross-modal attention due to token imbalance between visual and textual modalities and 2) the lack of timestep-aware attention weighting, which hinder the alignment. To address these issues, we propose Temperature-Adjusted Cross-modal Attention (TACA), a parameter-efficient method that dynamically rebalances multimodal interactions through temperature scaling and timestep-dependent adjustment. When combined with LoRA fine-tuning, TACA significantly enhances text-image alignment on the T2I-CompBench benchmark with minimal computational overhead. We tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating its ability to improve image-text alignment in terms of object appearance, attribute binding, and spatial relationships. Our findings highlight the importance of balancing cross-modal attention in improving semantic fidelity in text-to-image diffusion models. Our codes are publicly available at https://github.com/Vchitect/TACA"
    },
    {
      "title": "OneIG-Bench: 面向图像生成的全方位细致评估 (原标题: OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation)",
      "link": "https://arxiv.org/abs/2506.07977",
      "pubDate": "Mon, 09 Jun 2025 13:50:21 GMT",
      "isoDate": "2025-06-09T13:50:21.000Z",
      "creator": "Jingjing Chang, Yixiao Fang, Peng Xing, Shuhan Wu, Wei Cheng, Rui Wang, Xianfang Zeng, Gang Yu, Hai-Bao Chen",
      "summary": "本文介绍了一个名为OneIG-Bench的综合性基准框架，用于对文本到图像（T2I）模型进行细粒度的评估。该框架旨在解决现有基准测试的局限性，这些局限性包括缺乏对推理、文本渲染和风格等方面的全面评估。OneIG-Bench通过在多个维度上评估T2I模型，包括提示-图像对齐、文本渲染精度、推理生成内容、风格化和多样性，从而实现对模型性能的深入分析。该基准测试允许用户灵活地关注特定的评估子集，从而能够有针对性地进行评估。作者公开了代码库和数据集，以促进可重复的评估研究和T2I研究社区内的跨模型比较。",
      "shortSummary": "本文提出了OneIG-Bench，一个用于全面评估文本到图像(T2I)模型的基准框架。该框架旨在解决现有基准测试在推理、文本渲染和风格等方面评估不足的问题。OneIG-Bench通过评估提示-图像对齐、文本渲染精度、推理生成内容、风格化和多样性等维度，实现对模型性能的深入分析。代码库和数据集已公开，以促进可重复的评估研究。",
      "translated_title": "OneIG-Bench: 面向图像生成的全方位细致评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "Text-to-image (T2I) models have garnered significant attention for generating high-quality images aligned with text prompts. However, rapid T2I model advancements reveal limitations in early benchmarks, lacking comprehensive evaluations, for example, the evaluation on reasoning, text rendering and style. Notably, recent state-of-the-art models, with their rich knowledge modeling capabilities, show promising results on the image generation problems requiring strong reasoning ability, yet existing evaluation systems have not adequately addressed this frontier. To systematically address these gaps, we introduce OneIG-Bench, a meticulously designed comprehensive benchmark framework for fine-grained evaluation of T2I models across multiple dimensions, including prompt-image alignment, text rendering precision, reasoning-generated content, stylization, and diversity. By structuring the evaluation, this benchmark enables in-depth analysis of model performance, helping researchers and practitioners pinpoint strengths and bottlenecks in the full pipeline of image generation. Specifically, OneIG-Bench enables flexible evaluation by allowing users to focus on a particular evaluation subset. Instead of generating images for the entire set of prompts, users can generate images only for the prompts associated with the selected dimension and complete the corresponding evaluation accordingly. Our codebase and dataset are now publicly available to facilitate reproducible evaluation studies and cross-model comparisons within the T2I research community."
    },
    {
      "title": "MiniCPM4：面向终端设备的超高效大语言模型 (原标题: MiniCPM4: Ultra-Efficient LLMs on End Devices)",
      "link": "https://arxiv.org/abs/2506.07900",
      "pubDate": "Mon, 09 Jun 2025 12:16:50 GMT",
      "isoDate": "2025-06-09T12:16:50.000Z",
      "creator": "MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin Cong, Ganqu Cui, Ning Ding, Shengdan Fan, Yewei Fang, Zixuan Fu, Wenyu Guan, Yitong Guan, Junshao Guo, Yufeng Han, Bingxiang He, Yuxiang Huang, Cunliang Kong, Qiuzuo Li, Siyuan Li, Wenhao Li, Yanghao Li, Yishan Li, Zhen Li, Dan Liu, Biyuan Lin, Yankai Lin, Xiang Long, Quanyu Lu, Yaxi Lu, Peiyan Luo, Hongya Lyu, Litu Ou, Yinxu Pan, Zekai Qu, Qundong Shi, Zijun Song, Jiayuan Su, Zhou Su, Ao Sun, Xianghui Sun, Peijun Tang, Fangzheng Wang, Feng Wang, Shuo Wang, Yudong Wang, Yesai Wu, Zhenyu Xiao, Jie Xie, Zihao Xie, Yukun Yan, Jiarui Yuan, Kaihuo Zhang, Lei Zhang, Linyue Zhang, Xueren Zhang, Yudi Zhang, Hengyu Zhao, Weilin Zhao, Weilun Zhao, Yuanqian Zhao, Zhi Zheng, Ge Zhou, Jie Zhou, Wei Zhou, Zihan Zhou, Zixuan Zhou, Zhiyuan Liu, Guoyang Zeng, Chao Jia, Dahai Li, Maosong Sun",
      "summary": "# MiniCPM4：面向终端设备的超高效大语言模型\n\n本文介绍了MiniCPM4，这是一种专为终端设备设计的高度高效大语言模型（LLM）。MiniCPM4通过在四个关键维度进行系统性创新，实现了其卓越的效率：模型架构、训练数据、训练算法和推理系统。\n\n## 关键创新维度\n\n1.  **模型架构**\n    *   提出了 **InfLLM v2**，这是一种可训练的稀疏注意力机制。\n    *   该机制能够加速长上下文处理中的预填充（prefilling）和解码（decoding）阶段。\n\n2.  **训练数据**\n    *   提出了 **UltraClean**，一种高效且准确的预训练数据过滤和生成策略。\n    *   提出了 **UltraChat v2**，一个全面的监督微调数据集。\n    *   通过使用这些数据集，MiniCPM4仅需8万亿训练tokens即可达到令人满意的模型性能。\n\n3.  **训练算法**\n    *   提出了 **ModelTunnel v2**，用于高效的预训练策略搜索。\n    *   通过引入用于负载均衡强化学习的**分块式展开（chunk-wise rollout）**，改进了现有的后训练方法。\n    *   引入了数据高效的三元大语言模型 **BitCPM**。\n\n4.  **推理系统**\n    *   提出了一个集成了稀疏注意力、模型量化和推测采样（speculative sampling）的系统（原文中为“this http URL”，此处不具体指代）。\n    *   该系统旨在实现高效的预填充和解码。\n\n## 模型版本与性能评估\n\nMiniCPM4提供两个版本，分别具有0.5B和8B参数，以满足多样化的设备端需求。\n\n充分的评估结果表明，MiniCPM4在多个基准测试中均优于同等规模的开源模型，凸显了其效率和有效性。值得注意的是，MiniCPM4-8B在处理长序列时，相比Qwen3-8B展现出显著的速度提升。\n\n## 应用场景\n\n通过进一步的适配，MiniCPM4已成功应用于多种场景，包括：\n*   可信赖的调查问卷生成。\n*   结合模型上下文协议的工具使用。\n\n这清晰地展示了MiniCPM4广泛的可用性。",
      "shortSummary": "MiniCPM4是一款专为终端设备设计的超高效大语言模型。它通过在模型架构（InfLLM v2稀疏注意力）、训练数据（UltraClean、UltraChat v2）、训练算法（ModelTunnel v2、BitCPM）和推理系统等四个维度进行创新，实现了卓越性能。MiniCPM4提供0.5B和8B版本，在多个基准测试中表现优异，尤其MiniCPM4-8B在长序列处理上显著快于Qwen3-8B。它已成功应用于调查生成和工具使用等场景，展现了广泛的实用性。",
      "translated_title": "MiniCPM4：面向终端设备的超高效大语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Sufficient evaluation results show that MiniCPM4 outperforms open-source models of similar size across multiple benchmarks, highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B demonstrates significant speed improvements over Qwen3-8B when processing long sequences. Through further adaptation, MiniCPM4 successfully powers diverse applications, including trustworthy survey generation and tool use with model context protocol, clearly showcasing its broad usability."
    },
    {
      "title": "PolyVivid：具有跨模态交互和增强的生动多主体视频生成 (原标题: PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement)",
      "link": "https://arxiv.org/abs/2506.07848",
      "pubDate": "Mon, 09 Jun 2025 11:11:09 GMT",
      "isoDate": "2025-06-09T11:11:09.000Z",
      "creator": "Teng Hu, Zhentao Yu, Zhengguang Zhou, Jiangning Zhang, Yuan Zhou, Qinglin Lu, Ran Yi",
      "summary": "### PolyVivid：用于多主体视频生成的创新框架\n\n**研究背景与挑战**\n\n尽管视频生成技术取得了显著进展，但现有模型在细粒度控制方面仍存在不足，尤其是在涉及多主体定制时，难以保持一致的身份和主体间的自然交互。\n\n**PolyVivid 框架介绍**\n\n本文提出了一种名为 PolyVivid 的多主体视频定制框架。该框架旨在实现灵活且身份一致的视频生成，以解决当前模型面临的挑战。\n\n**核心模块与技术**\n\nPolyVivid 框架集成了多个创新模块，以确保高质量的多主体视频生成：\n\n*   **基于 VLLM 的文本-图像融合模块：**\n    *   **目的：** 建立主体图像与文本实体之间的精确对应关系。\n    *   **方法：** 将视觉身份信息嵌入到文本空间中，从而实现精确的身份接地（grounding）。\n\n*   **基于 3D-RoPE 的增强模块：**\n    *   **目的：** 进一步增强身份保留和主体间的交互。\n    *   **方法：** 实现文本和图像嵌入之间结构化的双向融合，促进信息流通。\n\n*   **注意力继承的身份注入模块：**\n    *   **目的：** 有效地将融合后的身份特征注入到视频生成过程中。\n    *   **作用：** 显著减轻了在视频生成过程中可能出现的身份漂移问题，确保主体身份的稳定性。\n\n*   **基于 MLLM 的数据管道：**\n    *   **目的：** 生成高质量的多主体数据，为视频生成提供坚实基础。\n    *   **方法：** 结合了基于 MLLM 的接地、分割以及基于团（clique-based）的主体整合策略。\n    *   **作用：** 有效增强了主体之间的区分度，并减少了下游视频生成中的歧义。\n\n**实验结果**\n\n广泛的实验验证表明，PolyVivid 在多个关键性能指标上均表现出色。它在身份保真度、视频真实感和主体对齐方面均取得了卓越的性能，并优于现有的开源和商业基线模型。\n\n**研究领域**\n\n该研究主要属于以下领域：\n\n*   计算机视觉与模式识别 (cs.CV)\n*   人工智能 (cs.AI)",
      "shortSummary": "PolyVivid 是一种创新的多主体视频生成框架，旨在解决现有模型在多主体身份一致性和交互控制方面的不足。它通过引入基于 VLLM 的文本-图像融合、基于 3D-RoPE 的增强、注意力继承的身份注入模块，以及基于 MLLM 的数据管道，实现了精确的身份接地、增强的交互和有效的身份保持。实验证明，PolyVivid 在身份保真度、视频真实感和主体对齐方面均优于现有基线。",
      "translated_title": "PolyVivid：具有跨模态交互和增强的生动多主体视频生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite recent advances in video generation, existing models still lack fine-grained controllability, especially for multi-subject customization with consistent identity and interaction. In this paper, we propose PolyVivid, a multi-subject video customization framework that enables flexible and identity-consistent generation. To establish accurate correspondences between subject images and textual entities, we design a VLLM-based text-image fusion module that embeds visual identities into the textual space for precise grounding. To further enhance identity preservation and subject interaction, we propose a 3D-RoPE-based enhancement module that enables structured bidirectional fusion between text and image embeddings. Moreover, we develop an attention-inherited identity injection module to effectively inject fused identity features into the video generation process, mitigating identity drift. Finally, we construct an MLLM-based data pipeline that combines MLLM-based grounding, segmentation, and a clique-based subject consolidation strategy to produce high-quality multi-subject data, effectively enhancing subject distinction and reducing ambiguity in downstream video generation. Extensive experiments demonstrate that PolyVivid achieves superior performance in identity fidelity, video realism, and subject alignment, outperforming existing open-source and commercial baselines."
    },
    {
      "title": "图像重建作为特征分析的工具 (原标题: Image Reconstruction as a Tool for Feature Analysis)",
      "link": "https://arxiv.org/abs/2506.07803",
      "pubDate": "Mon, 09 Jun 2025 10:32:18 GMT",
      "isoDate": "2025-06-09T10:32:18.000Z",
      "creator": "Eduard Allakhverdov, Dmitrii Tarasov, Elizaveta Goncharova, Andrey Kuznetsov",
      "summary": "本文提出了一种通过图像重建来解释视觉编码器内部特征表示的新方法。这项研究旨在揭示视觉编码器（包括纯视觉模型和多模态系统，如视觉-语言模型）如何在其内部表示特征，尽管它们在现代应用中取得了显著成功，但其内部机制仍不明确。\n\n**主要发现与应用：**\n\n*   **模型家族比较**：研究比较了SigLIP和SigLIP2两个模型家族，它们仅在训练目标上有所不同。结果显示，在基于图像的任务上预训练的编码器比在非图像任务（例如对比学习）上训练的编码器保留了显著更多的图像信息。\n*   **视觉编码器排名**：该方法被应用于一系列视觉编码器，并根据其特征表示的信息量对它们进行了排名。\n*   **特征空间操作**：研究表明，对特征空间进行操作会在重建图像中产生可预测的变化。具体而言，这揭示了正交旋转（而非空间变换）控制着颜色编码。\n\n**方法普适性与资源：**\n\n*   该方法具有普适性，可以应用于任何视觉编码器，从而有助于阐明其特征空间的内部结构。\n*   重现实验的代码和模型权重已在GitHub上提供。",
      "shortSummary": "本文提出一种通过图像重建解释视觉编码器特征的新方法。研究发现，在图像任务上训练的编码器比非图像任务保留更多图像信息，且通过操纵特征空间，揭示了正交旋转控制颜色编码。该方法普适于任何视觉编码器，有助于理解其内部特征结构。",
      "translated_title": "图像重建作为特征分析的工具",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision encoders are increasingly used in modern applications, from vision-only models to multimodal systems such as vision-language models. Despite their remarkable success, it remains unclear how these architectures represent features internally. Here, we propose a novel approach for interpreting vision features via image reconstruction. We compare two related model families, SigLIP and SigLIP2, which differ only in their training objective, and show that encoders pre-trained on image-based tasks retain significantly more image information than those trained on non-image tasks such as contrastive learning. We further apply our method to a range of vision encoders, ranking them by the informativeness of their feature representations. Finally, we demonstrate that manipulating the feature space yields predictable changes in reconstructed images, revealing that orthogonal rotations (rather than spatial transformations) control color encoding. Our approach can be applied to any vision encoder, shedding light on the inner structure of its feature space. The code and model weights to reproduce the experiments are available in GitHub."
    },
    {
      "title": "穿越峡谷：小语言模型有效长链思维训练的路径 (原标题: Through the Valley: Path to Effective Long CoT Training for Small Language Models)",
      "link": "https://arxiv.org/abs/2506.07712",
      "pubDate": "Mon, 09 Jun 2025 08:56:41 GMT",
      "isoDate": "2025-06-09T08:56:41.000Z",
      "creator": "Renjie Luo, Jiaxi Li, Chen Huang, Wei Lu",
      "summary": "## 穿越峡谷：小语言模型有效长链思维训练的路径\n\n本文研究了长链思维（CoT）监督在增强语言模型推理能力方面的应用。研究发现，对于小型语言模型（SLMs，参数≤3B），在有限的长CoT数据上训练会导致一种称为“长CoT退化”的现象，即性能显著下降。\n\n**主要发现：**\n\n*   **普遍性：** 在Qwen2.5、LLaMA3和Gemma3系列模型上的实验表明，这种退化现象在SLMs中普遍存在。\n*   **性能损失：** 在某些情况下，仅使用8k个长CoT示例训练的模型，其性能会损失高达75%。即使使用220k个长CoT示例训练，一些特别小的模型也无法恢复或超过其微调前的原始性能。\n*   **原因分析：** 这种效应归因于错误累积。虽然更长的响应增加了多步推理的能力，但也放大了累积错误的风险。\n*   **对下游强化学习的影响：** 长CoT退化可能会对下游强化学习（RL）产生负面影响，但可以通过充分扩展的监督微调（SFT）来缓解。\n\n**结论：**\n\n本文的研究结果挑战了关于长CoT训练对SLMs益处的常见假设，并为构建更有效的小型推理模型提供了实践指导。",
      "shortSummary": "本文研究了小型语言模型（SLMs）在长链思维（CoT）训练中出现的“长CoT退化”现象。研究发现，在有限的长CoT数据上训练SLMs会导致性能显著下降，即使使用大量CoT数据也难以恢复。这种退化归因于错误累积，并可能对下游强化学习产生负面影响。研究结果挑战了长CoT训练对SLMs益处的常见假设，并为构建更有效的小型推理模型提供了实践指导。",
      "translated_title": "穿越峡谷：小语言模型有效长链思维训练的路径",
      "images": [],
      "contentSource": "完整文章",
      "content": "Long chain-of-thought (CoT) supervision has become a common strategy to enhance reasoning in language models. While effective for large models, we identify a phenomenon we call Long CoT Degradation, in which small language models (SLMs; &lt;=3B parameters) trained on limited long CoT data experience significant performance deterioration. Through extensive experiments on the Qwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is widespread across SLMs. In some settings, models trained on only 8k long CoT examples lose up to 75% of their original performance before fine-tuning. Strikingly, we further observe that for some particularly small models, even training on 220k long CoT examples fails to recover or surpass their original performance prior to fine-tuning. Our analysis attributes this effect to error accumulation: while longer responses increase the capacity for multi-step reasoning, they also amplify the risk of compounding mistakes. Furthermore, we find that Long CoT Degradation may negatively impacts downstream reinforcement learning (RL), although this can be alleviated by sufficiently scaled supervised fine-tuning (SFT). Our findings challenge common assumptions about the benefits of long CoT training for SLMs and offer practical guidance for building more effective small-scale reasoning models."
    },
    {
      "title": "使用代理模型评估大型语言模型在低资源语言中的鲁棒性 (原标题: Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models)",
      "link": "https://arxiv.org/abs/2506.07645",
      "pubDate": "Mon, 09 Jun 2025 07:09:39 GMT",
      "isoDate": "2025-06-09T07:09:39.000Z",
      "creator": "Maciej Chrabąszcz, Katarzyna Lorenc, Karolina Seweryn",
      "summary": "# 使用代理模型评估大型语言模型在低资源语言中的鲁棒性\n\n## 摘要\n\n近年来，大型语言模型（LLMs）在各种自然语言处理（NLP）任务中展现出令人印象深刻的能力。然而，它们对“越狱”和扰动的敏感性，使得对其鲁棒性进行额外评估变得必要。\n\n## 问题背景\n\n*   **LLMs的脆弱性：** 尽管LLMs功能强大，但它们容易受到越狱攻击和各种扰动的影响。\n*   **语言资源不平衡：** 许多LLMs是多语言的，但其安全相关的训练数据主要包含高资源语言，例如英语。这使得LLMs在低资源语言（如波兰语）中可能更容易受到攻击。\n\n## 研究方法\n\n本研究展示了如何以低成本创建出乎意料的强大攻击：\n*   **少量字符修改：** 仅通过改变少量字符。\n*   **代理模型辅助：** 利用小型代理模型来计算词语的重要性。\n\n## 主要发现与结果\n\n*   **预测显著改变：** 研究发现，这些字符级和词语级的攻击能够显著改变不同LLMs的预测结果。\n*   **潜在安全漏洞：** 这表明LLMs可能存在潜在的漏洞，可以被用来规避其内部安全机制。\n*   **在低资源语言中的验证：** 研究团队在波兰语（一种低资源语言）上验证了其攻击构建方法，并成功发现了LLMs在该语言中的潜在漏洞。\n*   **方法可扩展性：** 此外，研究还表明该攻击方法可以扩展到其他语言。\n\n## 贡献\n\n*   为了促进进一步的研究，本研究发布了所创建的数据集和相关代码。",
      "shortSummary": "本研究评估了大型语言模型（LLMs）在低资源语言中的鲁棒性。发现LLMs因安全训练数据主要集中在高资源语言而易受攻击。研究提出一种低成本攻击方法，通过改变少量字符并利用代理模型，能显著改变LLMs预测，绕过其安全机制。该方法在波兰语中得到验证，并可扩展到其他语言。研究发布了数据集和代码以供进一步研究。",
      "translated_title": "使用代理模型评估大型语言模型在低资源语言中的鲁棒性",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) have demonstrated impressive capabilities across various natural language processing (NLP) tasks in recent years. However, their susceptibility to jailbreaks and perturbations necessitates additional evaluations. Many LLMs are multilingual, but safety-related training data contains mainly high-resource languages like English. This can leave them vulnerable to perturbations in low-resource languages such as Polish. We show how surprisingly strong attacks can be cheaply created by altering just a few characters and using a small proxy model for word importance calculation. We find that these character and word-level attacks drastically alter the predictions of different LLMs, suggesting a potential vulnerability that can be used to circumvent their internal safety mechanisms. We validate our attack construction methodology on Polish, a low-resource language, and find potential vulnerabilities of LLMs in this language. Additionally, we show how it can be extended to other languages. We release the created datasets and code for further research."
    },
    {
      "title": "GTR-CoT: 基于图遍历的视觉链式思考用于分子结构识别 (原标题: GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition)",
      "link": "https://arxiv.org/abs/2506.07553",
      "pubDate": "Mon, 09 Jun 2025 04:47:10 GMT",
      "isoDate": "2025-06-09T04:47:10.000Z",
      "creator": "Jingchao Wang, Haote Yang, Jiang Wu, Yifan He, Xingjian Wei, Yinfan Wang, Chengjin Liu, Lingli Ge, Lijun Wu, Bin Wang, Dahua Lin, Conghui He",
      "summary": "# GTR-CoT: 基于图遍历的视觉链式思考用于分子结构识别\n\n本文提出了一种名为GTR-Mol-VLM的新框架，用于解决光学化学结构识别（OCSR）中的挑战。该框架包含两个关键创新：\n\n*   **图遍历作为视觉链式思考（Graph Traversal as Visual Chain of Thought）机制**：通过顺序原子-键预测，逐步解析分子图，模拟人类推理过程。\n*   **忠实识别所见（Faithfully Recognize What You've Seen）的数据中心原则**：解决图像中缩写结构与其扩展注释之间的不匹配问题。\n\n为了支持模型开发，作者构建了一个大规模指令调优数据集GTR-CoT-1.3M，并引入了MolRec-Bench，这是第一个用于细粒度评估OCSR中图解析准确性的基准。实验结果表明，GTR-Mol-VLM在分子图像中涉及官能团缩写的情况下，优于其他模型，在基于SMILES和基于图的指标上，比第二好的基线高出约14个百分点。作者希望这项工作能够推动OCSR技术更有效地满足实际需求，从而推进化学信息学和人工智能科学领域的发展。GTR-CoT数据集将在https URL发布。",
      "shortSummary": "本文提出GTR-Mol-VLM框架，用于提升光学化学结构识别（OCSR）的准确性。该框架包含图遍历视觉链式思考机制，模拟人类推理，并采用“忠实识别所见”的数据原则，解决图像缩写结构与注释不匹配问题。作者构建了GTR-CoT-1.3M数据集和MolRec-Bench基准。实验表明，GTR-Mol-VLM在包含官能团缩写的分子图像识别中表现优异，显著优于其他模型。该研究旨在推动OCSR技术发展，服务于化学信息学和人工智能科学领域。",
      "translated_title": "GTR-CoT: 基于图遍历的视觉链式思考用于分子结构识别",
      "images": [],
      "contentSource": "完整文章",
      "content": "Optical Chemical Structure Recognition (OCSR) is crucial for digitizing chemical knowledge by converting molecular images into machine-readable formats. While recent vision-language models (VLMs) have shown potential in this task, their image-captioning approach often struggles with complex molecular structures and inconsistent annotations. To overcome these challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key innovations: (1) the Graph Traversal as Visual Chain of Thought mechanism that emulates human reasoning by incrementally parsing molecular graphs through sequential atom-bond predictions, and (2) the data-centric principle of Faithfully Recognize What You've Seen, which addresses the mismatch between abbreviated structures in images and their expanded annotations. To support model development, we constructed GTR-CoT-1.3M, a large-scale instruction-tuning dataset with meticulously corrected annotations, and introduced MolRec-Bench, the first benchmark designed for a fine-grained evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments demonstrate that GTR-Mol-VLM achieves superior results compared to specialist models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in scenarios involving molecular images with functional group abbreviations, GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage points, both in SMILES-based and graph-based metrics. We hope that this work will drive OCSR technology to more effectively meet real-world needs, thereby advancing the fields of cheminformatics and AI for Science. We will release GTR-CoT at https://github.com/opendatalab/GTR-CoT."
    },
    {
      "title": "BitVLA：用于机器人操作的1-bit视觉-语言-动作模型 (原标题: BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation)",
      "link": "https://arxiv.org/abs/2506.07530",
      "pubDate": "Mon, 09 Jun 2025 04:15:11 GMT",
      "isoDate": "2025-06-09T04:15:11.000Z",
      "creator": "Hongyu Wang, Chuyan Xiong, Ruiping Wang, Xilin Chen",
      "summary": "## BitVLA：用于机器人操作的1-bit视觉-语言-动作模型\n\n本文介绍了一种名为BitVLA的1-bit视觉-语言-动作（VLA）模型，专门用于机器人操作。该模型旨在解决VLA模型日益增长的尺寸所带来的挑战，尤其是在资源受限的机器人系统上的部署问题。\n\n**主要观点：**\n\n*   **1-bit量化：** BitVLA的关键创新在于其所有参数都是三元的，即{-1, 0, 1}，从而显著降低了内存占用。\n*   **蒸馏感知训练：** 为了进一步减少视觉编码器的内存占用，作者提出了一种蒸馏感知训练策略，将全精度编码器压缩到1.58-bit权重。在此过程中，全精度编码器充当教师模型，以更好地对齐潜在表示。\n*   **性能表现：** 尽管缺乏大规模的机器人预训练，BitVLA在LIBERO基准测试中实现了与最先进模型OpenVLA-OFT（采用4-bit后训练量化）相当的性能，同时仅消耗29.8%的内存。\n*   **适用性：** 实验结果表明BitVLA在内存受限的边缘设备上具有部署潜力。\n\n**研究意义：**\n\nBitVLA的提出为在资源受限的机器人平台上部署高性能VLA模型开辟了新的可能性。通过采用1-bit量化和蒸馏感知训练等技术，该模型在保持竞争力的同时，显著降低了内存需求，使其更适合在边缘设备上运行。\n\n**代码和模型权重：**\n\n代码和模型权重已在以下URL中发布：https://example.com",
      "shortSummary": "本文提出了一种名为BitVLA的1-bit视觉-语言-动作模型，用于机器人操作。该模型通过将所有参数量化为三元值{-1, 0, 1}，并采用蒸馏感知训练策略压缩视觉编码器，显著降低了内存占用。实验结果表明，BitVLA在LIBERO基准测试中实现了与最先进模型相当的性能，同时仅消耗29.8%的内存，使其更适合在资源受限的边缘设备上部署。代码和模型权重已发布。",
      "translated_title": "BitVLA：用于机器人操作的1-bit视觉-语言-动作模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-Language-Action (VLA) models have shown impressive capabilities across a wide range of robotics manipulation tasks. However, their growing model size poses significant challenges for deployment on resource-constrained robotic systems. While 1-bit pretraining has proven effective for enhancing the inference efficiency of large language models with minimal performance loss, its application to VLA models remains underexplored. In this work, we present BitVLA, the first 1-bit VLA model for robotics manipulation, in which every parameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint of the vision encoder, we propose the distillation-aware training strategy that compresses the full-precision encoder to 1.58-bit weights. During this process, a full-precision encoder serves as a teacher model to better align latent representations. Despite the lack of large-scale robotics pretraining, BitVLA achieves performance comparable to the state-of-the-art model OpenVLA-OFT with 4-bit post-training quantization on the LIBERO benchmark, while consuming only 29.8% of the memory. These results highlight BitVLA's promise for deployment on memory-constrained edge devices. We release the code and model weights in https://github.com/ustcwhy/BitVLA."
    },
    {
      "title": "强化学习无法学习的内容：针对最难题目的交错式在线微调 (原标题: Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions)",
      "link": "https://arxiv.org/abs/2506.07527",
      "pubDate": "Mon, 09 Jun 2025 04:11:20 GMT",
      "isoDate": "2025-06-09T04:11:20.000Z",
      "creator": "Lu Ma, Hao Liang, Meiyi Qiang, Lexiang Tang, Xiaochen Ma, Zhen Hao Wong, Junbo Niu, Chengyu Shen, Runming He, Bin Cui, Wentao Zhang",
      "summary": "### 强化学习的局限性与挑战\n\n大型语言模型（LLM）在推理方面取得了显著进展，强化学习（RL）在此过程中展现出规划和自我反思等复杂行为。然而，RL在当前形式下存在根本局限：它主要基于模型现有知识进行优化，难以帮助模型获取超出其基础模型限制的新能力或新信息。\n\n### 监督微调（SFT）的互补优势\n\n为了解决RL的这一局限性，研究引入了监督微调（SFT）。SFT通过利用高质量的示范数据，能够有效地融入新知识和推理模式。通过分析RL和SFT在LLM推理训练中的动态，研究发现两者具有互补优势：\n\n*   **RL的优势**：擅长维持和提升模型在其原有能力范围内的表现。\n*   **SFT的优势**：在使模型在超出当前范围的问题上取得进展方面更为有效。\n\n### ReLIFT：RL与在线微调的交错训练方法\n\n受RL和SFT互补优势的启发，本文提出了一种新颖的训练方法——**ReLIFT**（**Re**inforcement **L**earning **I**nterleaved with Online **F**ine-​**T**uning）。\n\n*   **工作原理**：ReLIFT主要采用RL进行模型训练。当模型遇到具有挑战性的问题时，系统会收集高质量的解决方案用于微调（SFT）。训练过程在RL和SFT之间交替进行，从而持续增强模型的推理能力。\n\n### 实验结果与显著提升\n\nReLIFT在多个基准测试中展现出卓越的性能：\n\n*   **性能提升**：与仅使用RL的模型相比，ReLIFT在五个竞赛级别基准测试和一个分布外基准测试中，平均提升了超过5.2分。\n*   **数据效率**：ReLIFT在仅使用13%的详细示范数据的情况下，其表现优于单独的RL和SFT。\n\n这些结果有力地证明了ReLIFT克服了RL的根本局限性，并突显了其在提升LLM推理能力方面的巨大潜力。",
      "shortSummary": "大型语言模型（LLM）的强化学习（RL）在获取新知识方面存在局限。研究发现，RL擅长优化现有能力，而监督微调（SFT）更有效于学习新知识。基于此，本文提出ReLIFT（强化学习与在线微调交错）方法。ReLIFT在RL训练中，对遇到的难题进行高质量解决方案的在线微调，实现RL与SFT的交替训练。实验表明，ReLIFT在多个基准测试中平均提升超过5.2分，且仅用13%的示范数据就超越了单独的RL和SFT，有效克服了RL的根本局限。",
      "translated_title": "强化学习无法学习的内容：针对最难题目的交错式在线微调",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in large language model (LLM) reasoning have shown that sophisticated behaviors such as planning and self-reflection can emerge through reinforcement learning (RL). However, despite these successes, RL in its current form remains insufficient to induce capabilities that exceed the limitations of the base model, as it is primarily optimized based on existing knowledge of the model rather than facilitating the acquisition of new information. To address this limitation, we employ supervised fine-tuning (SFT) to learn what RL cannot, which enables the incorporation of new knowledge and reasoning patterns by leveraging high-quality demonstration data. We analyze the training dynamics of RL and SFT for LLM reasoning and find that RL excels at maintaining and improving performance on questions within the model's original capabilities, while SFT is more effective at enabling progress on questions beyond the current scope of the model. Motivated by the complementary strengths of RL and SFT, we introduce a novel training approach, ReLIFT (Reinforcement Learning Interleaved with Online Fine-Tuning). In ReLIFT, the model is primarily trained using RL, but when it encounters challenging questions, high-quality solutions are collected for fine-tuning, and the training process alternates between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT achieves an average improvement of over +5.2 points across five competition-level benchmarks and one out-of-distribution benchmark compared to other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both RL and SFT while using only 13\\% of the detailed demonstration data, highlighting its scalability. These results provide compelling evidence that ReLIFT overcomes the fundamental limitations of RL and underscores the significant potential."
    },
    {
      "title": "SpatialLM：训练用于结构化室内建模的大型语言模型 (原标题: SpatialLM: Training Large Language Models for Structured Indoor Modeling)",
      "link": "https://arxiv.org/abs/2506.07491",
      "pubDate": "Mon, 09 Jun 2025 03:10:58 GMT",
      "isoDate": "2025-06-09T03:10:58.000Z",
      "creator": "Yongsen Mao, Junhao Zhong, Chuan Fang, Jia Zheng, Rui Tang, Hao Zhu, Ping Tan, Zihan Zhou",
      "summary": "### SpatialLM 概述\n\n*   **定义与功能**：SpatialLM 是一种大型语言模型（LLM），专门设计用于处理 3D 点云数据，并生成结构化的 3D 场景理解输出。\n*   **输出内容**：其输出包括各种建筑元素（如墙壁、门、窗户）以及带有语义类别的定向物体框。\n\n### 模型架构与训练\n\n*   **架构特点**：与以往依赖特定任务网络设计的方法不同，SpatialLM 遵循标准的**多模态 LLM 架构**，并直接从**开源 LLM** 进行微调。\n*   **训练数据**：为了训练 SpatialLM，研究团队构建了一个**大规模、高质量的合成数据集**。该数据集包含了 12,328 个室内场景（共 54,778 个房间）的点云数据，并附带了精确的真实 3D 注释。\n*   **训练过程**：在模型构建和训练决策方面，研究人员进行了细致深入的探索。\n\n### 性能表现\n\n*   在公共基准测试中，SpatialLM 在**布局估计**任务上取得了**最先进（state-of-the-art）**的性能。\n*   在 **3D 物体检测**方面也展现出**有竞争力**的结果。\n\n### 应用前景\n\n*   SpatialLM 的成功展示了增强现代 LLM 空间理解能力的可行路径。\n*   其潜在应用领域广泛，包括**增强现实（AR）**、**具身机器人（embodied robotics）**等。",
      "shortSummary": "SpatialLM是一种大型语言模型，旨在处理3D点云数据并生成结构化的3D室内场景理解，包括建筑元素和物体。它采用标准多模态LLM架构，并从开源LLM微调。该模型通过大规模合成数据集训练，在布局估计上达到最先进水平，在3D物体检测上表现出色。SpatialLM为增强LLM的空间理解能力提供了可行路径，适用于增强现实和具身机器人等领域。",
      "translated_title": "SpatialLM：训练用于结构化室内建模的大型语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "SpatialLM is a large language model designed to process 3D point cloud data and generate structured 3D scene understanding outputs. These outputs include architectural elements like walls, doors, windows, and oriented object boxes with their semantic categories. Unlike previous methods which exploit task-specific network designs, our model adheres to the standard multimodal LLM architecture and is fine-tuned directly from open-source LLMs.   To train SpatialLM, we collect a large-scale, high-quality synthetic dataset consisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with ground-truth 3D annotations, and conduct a careful study on various modeling and training decisions. On public benchmarks, our model gives state-of-the-art performance in layout estimation and competitive results in 3D object detection. With that, we show a feasible path for enhancing the spatial understanding capabilities of modern LLMs for applications in augmented reality, embodied robotics, and more."
    },
    {
      "title": "CCI4.0：一个用于增强大型语言模型推理能力的双语预训练数据集 (原标题: CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large Language Models)",
      "link": "https://arxiv.org/abs/2506.07463",
      "pubDate": "Mon, 09 Jun 2025 02:14:19 GMT",
      "isoDate": "2025-06-09T02:14:19.000Z",
      "creator": "Guang Liu, Liangdong Wang, Jijie Li, Yang Yu, Yao Xu, Jiabei Chen, Yu Bai, Feng Liao, Yonghua Lin",
      "summary": "## CCI4.0：一个用于增强大型语言模型推理能力的双语预训练数据集\n\n### 简介\n\n本文介绍了CCI4.0，一个大规模的双语预训练数据集，旨在提供卓越的数据质量和多样化的人类式推理轨迹。该数据集占据约35 TB的磁盘空间。\n\n### 数据集组成\n\nCCI4.0包含两个主要的子数据集：\n\n1.  **CCI4.0-M2-Base**\n    *   结合了5.2 TB精心策划的中文网络语料库。\n    *   包含来自Nemotron-CC的22.5 TB英文子集。\n    *   还整合了来自数学、维基百科、arXiv和代码等多样化来源的数据。\n\n2.  **CCI4.0-M2-CoT**\n    *   从CCI4.0-M2-Base中提取了45亿条CoT（思维链）模板。\n    *   与从大型模型中蒸馏CoT不同，CCI4.0采用分阶段的CoT提取方法，旨在展现多样化的推理模式，并显著降低幻觉的可能性。\n\n### 数据质量处理流程\n\n鉴于不同领域数据质量标准动态变化且需要大量人工专家经验，作者提出了一种新颖的数据质量处理流程，主要基于模型进行数据质量评估。该流程的关键步骤包括：\n\n*   **两阶段去重**（two-stage deduplication）。\n*   **多分类器质量评分**（multiclassifier quality scoring）。\n*   **领域感知流畅性过滤**（domain-aware fluency filtering）。\n\n### 实证评估与结果\n\n*   在CCI4.0上预训练的大型语言模型（LLMs）受益于更清晰、更可靠的训练信号。\n*   在下游任务中，尤其是在数学和代码反思任务中，LLMs表现出持续的改进。\n\n### 结论与意义\n\n研究结果强调了严格的数据管理和人类思维模板在提升LLM性能方面的关键作用，并为自动处理预训练语料库提供了新的思路。",
      "shortSummary": "CCI4.0是一个35TB的双语预训练数据集，旨在提升大型语言模型的推理能力。它包含CCI4.0-M2-Base（中文网络、英文Nemotron-CC、数学、代码等）和CCI4.0-M2-CoT（45亿条思维链模板）。该数据集采用模型驱动的新颖质量处理流程，包括去重、质量评分和流畅性过滤。实证评估表明，在CCI4.0上预训练的LLM在数学和代码等下游任务中表现出显著改进，凸显了高质量数据和思维链模板的重要性。",
      "translated_title": "CCI4.0：一个用于增强大型语言模型推理能力的双语预训练数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered for superior data quality and diverse human-like reasoning trajectory. CCI4.0 occupies roughly 35 TB of disk space and comprises two sub-datasets: CCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a 5.2 TB carefully curated Chinese web corpus, a 22.5 TB English subset from Nemotron-CC, and diverse sources from math, wiki, arxiv, and code. Although these data are mostly sourced from well-processed datasets, the quality standards of various domains are dynamic and require extensive expert experience and labor to process. So, we propose a novel pipeline justifying data quality mainly based on models through two-stage deduplication, multiclassifier quality scoring, and domain-aware fluency filtering. We extract 4.5 billion pieces of CoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the distillation of CoT from larger models, our proposed staged CoT extraction exemplifies diverse reasoning patterns and significantly decreases the possibility of hallucination. Empirical evaluations demonstrate that LLMs pre-trained in CCI4.0 benefit from cleaner, more reliable training signals, yielding consistent improvements in downstream tasks, especially in math and code reflection tasks. Our results underscore the critical role of rigorous data curation and human thinking templates in advancing LLM performance, shedding some light on automatically processing pretraining corpora."
    },
    {
      "title": "好的开始是成功的一半：通过弱到强解码实现的低资源偏好对齐 (原标题: Well Begun is Half Done: Low-resource Preference Alignment by Weak-to-Strong Decoding)",
      "link": "https://arxiv.org/abs/2506.07434",
      "pubDate": "Mon, 09 Jun 2025 01:21:22 GMT",
      "isoDate": "2025-06-09T01:21:22.000Z",
      "creator": "Feifan Song, Shaohang Wei, Wen Luo, Yuxuan Fan, Tianyu Liu, Guoyin Wang, Houfeng Wang",
      "summary": "# 好的开始是成功的一半：通过弱到强解码实现的低资源偏好对齐\n\n本文提出了一种名为“弱到强解码 (WSD)”的新框架，旨在通过小型对齐模型的指导，增强基础模型在低资源环境下的对齐能力。该方法的核心思想是，对齐响应的生成难度集中在解码的开始阶段。WSD框架首先由小型模型起草对齐良好的开头，然后由大型基础模型继续生成剩余部分，并通过精心设计的自动切换机制进行控制。\n\n**主要内容：**\n\n*   **问题：** 大型语言模型 (LLM) 需要与人类偏好对齐，以避免生成冒犯性、虚假或无意义的内容。在低资源环境下进行LLM对齐仍然面临挑战，难以获得高质量且对齐的内容。\n*   **方法：** 提出弱到强解码 (WSD) 框架，利用小型对齐模型指导大型基础模型，从而提升对齐能力。\n*   **机制：** 小型模型首先生成对齐良好的开头，然后大型模型继续生成剩余部分，通过自动切换机制控制。\n*   **数据集：** 收集了一个新的数据集 GenerAlign，用于微调小型 Pilot-3B 模型作为草稿模型。\n*   **结果：** 在WSD框架下，Pilot-3B 模型能够有效增强不同的基础模型，使其性能优于所有基线方法，同时避免了下游任务的性能下降（即“对齐税”）。\n*   **实验：** 进行了大量实验，以检验不同设置和时间效率的影响，并深入分析了WSD的内在机制。\n\n**其他信息：**\n\n*   本文已被 ACL 2025 Findings 接收。\n*   主题：计算与语言 (cs.CL)；人工智能 (cs.AI)\n*   Cite as: arXiv:2506.07434 [cs.CL]\n*   DOI: https://doi.org/10.48550/arXiv.2506.07434",
      "shortSummary": "本文提出了一种名为“弱到强解码 (WSD)”的新框架，旨在通过小型对齐模型的指导，增强基础模型在低资源环境下的对齐能力。WSD框架首先由小型模型起草对齐良好的开头，然后由大型基础模型继续生成剩余部分，并通过精心设计的自动切换机制进行控制。研究人员还收集了一个新的数据集 GenerAlign，用于微调小型 Pilot-3B 模型作为草稿模型。实验结果表明，WSD框架能够有效增强不同的基础模型，使其性能优于所有基线方法，同时避免了下游任务的性能下降。",
      "translated_title": "好的开始是成功的一半：通过弱到强解码实现的低资源偏好对齐",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) require alignment with human preferences to avoid generating offensive, false, or meaningless content. Recently, low-resource methods for LLM alignment have been popular, while still facing challenges in obtaining both high-quality and aligned content. Motivated by the observation that the difficulty of generating aligned responses is concentrated at the beginning of decoding, we propose a novel framework, Weak-to-Strong Decoding (WSD), to enhance the alignment ability of base models by the guidance of a small aligned model. The small model first drafts well-aligned beginnings, followed by the large base model to continue the rest, controlled by a well-designed auto-switch mechanism. We also collect a new dataset, GenerAlign, to fine-tune a small-sized Pilot-3B as the draft model, which effectively enhances different base models under the WSD framework to outperform all baseline methods, while avoiding degradation on downstream tasks, termed as the alignment tax. Extensive experiments are further conducted to examine the impact of different settings and time efficiency, as well as analyses on the intrinsic mechanisms of WSD in depth."
    },
    {
      "title": "ConfQA：只有在确信时才回答 (原标题: ConfQA: Answer Only If You Are Confident)",
      "link": "https://arxiv.org/abs/2506.07309",
      "pubDate": "Sun, 08 Jun 2025 18:51:46 GMT",
      "isoDate": "2025-06-08T18:51:46.000Z",
      "creator": "Yin Huang, Yifan Ethan Xu, Kai Sun, Vera Yan, Alicia Sun, Haidar Khan, Jimmy Nguyen, Mohammad Kachuee, Zhaojiang Lin, Yue Liu, Aaron Colak, Anuj Kumar, Wen-tau Yih, Xin Luna Dong",
      "summary": "## ConfQA：提升大型语言模型（LLM）的自信回答能力\n\n本文介绍了一种名为ConfQA的微调策略，旨在显著降低大型语言模型（LLM）的事实性幻觉。\n\n### 核心目标与成效\n*   **目标**：教会LLM在不确定时拒绝回答，从而减少事实性幻觉。\n*   **成效**：在多个事实性基准测试中，ConfQA能将LLM的幻觉率从20-40%降低到5%以下。\n\n### ConfQA的核心思想与关键因素\nConfQA的核心理念是：当LLM正确回答问题时，它被训练继续给出答案；否则，它被训练承认“我无法确定”（I am unsure）。实现这一目标的关键在于两个高效因素：\n\n1.  **引导性提示词**：引入了“只有在确信时才回答”（answer only if you are confident）这一明确的引导性提示词。如果没有这个提示，幻觉率仍会高达15%-25%。\n2.  **利用知识图谱校准置信度**：利用简单的事实性陈述（特别是来自知识图谱的属性值）来帮助LLM校准其置信度。这使得模型在不同领域和问题类型上都能实现稳健的泛化能力。\n\n### 双重神经知识框架（Dual Neural Knowledge framework）\n基于ConfQA的洞察，研究人员提出了“双重神经知识框架”：\n*   **功能**：该框架能够根据ConfQA的置信度，无缝地在内部参数化的神经知识和外部记录的符号知识之间进行选择。\n*   **优势**：\n    *   有望将准确率提升至95%以上。\n    *   减少超过30%的不必要的外部检索。",
      "shortSummary": "ConfQA是一种针对大型语言模型（LLM）的微调策略，旨在大幅减少事实性幻觉。通过训练LLM在确信时才回答，并在不确定时承认“我无法确定”，ConfQA能将幻觉率从20-40%降至5%以下。其核心在于引入“只有在确信时才回答”的提示词，并利用知识图谱校准置信度。该策略还支持“双重神经知识框架”，可提升准确率并减少外部检索。",
      "translated_title": "ConfQA：只有在确信时才回答",
      "images": [],
      "contentSource": "完整文章",
      "content": "Can we teach Large Language Models (LLMs) to refrain from hallucinating factual statements? In this paper we present a fine-tuning strategy that we call ConfQA, which can reduce hallucination rate from 20-40% to under 5% across multiple factuality benchmarks. The core idea is simple: when the LLM answers a question correctly, it is trained to continue with the answer; otherwise, it is trained to admit \"I am unsure\". But there are two key factors that make the training highly effective. First, we introduce a dampening prompt \"answer only if you are confident\" to explicitly guide the behavior, without which hallucination remains high as 15%-25%. Second, we leverage simple factual statements, specifically attribute values from knowledge graphs, to help LLMs calibrate the confidence, resulting in robust generalization across domains and question types. Building on this insight, we propose the Dual Neural Knowledge framework, which seamlessly select between internally parameterized neural knowledge and externally recorded symbolic knowledge based on ConfQA's confidence. The framework enables potential accuracy gains to beyond 95%, while reducing unnecessary external retrievals by over 30%."
    },
    {
      "title": "预训练大型语言模型在上下文中学习隐马尔可夫模型 (原标题: Pre-trained Large Language Models Learn Hidden Markov Models In-context)",
      "link": "https://arxiv.org/abs/2506.07298",
      "pubDate": "Sun, 08 Jun 2025 17:49:38 GMT",
      "isoDate": "2025-06-08T17:49:38.000Z",
      "creator": "Yijia Dai, Zhaolin Gao, Yahya Satter, Sarah Dean, Jennifer J. Sun",
      "summary": "### 预训练大型语言模型在上下文中学习隐马尔可夫模型\n\n本文探讨了预训练大型语言模型（LLMs）通过上下文学习（ICL）有效建模隐马尔可夫模型（HMMs）生成数据的能力。HMMs是建模具有潜在马尔可夫结构序列数据的基础工具，但将其拟合到真实世界数据在计算上具有挑战性。\n\n**核心发现与贡献：**\n\n*   **LLMs通过ICL建模HMMs：** 研究表明，预训练LLMs能够通过上下文学习（ICL）有效建模HMMs生成的数据。ICL是LLMs从提示中的示例推断模式的能力。\n*   **卓越的预测准确性：** 在多样化的合成HMMs数据集上，LLMs实现了接近理论最优的预测准确性。\n*   **新颖的缩放趋势：** 研究揭示了受HMM特性影响的新颖缩放趋势，并为这些经验观察提供了理论推测。\n*   **实际应用指导：** 文章为科学家提供了使用ICL作为复杂数据诊断工具的实用指南。\n*   **真实世界数据表现：** 在真实世界的动物决策任务中，ICL取得了与人类专家设计的模型相媲美的性能。\n*   **开创性工作：** 据作者所知，这是首次证明ICL能够学习和预测HMMs生成的序列。\n\n**研究意义：**\n\n*   加深了对LLMs中上下文学习的理解。\n*   确立了ICL作为揭示复杂科学数据中隐藏结构的强大工具的潜力。",
      "shortSummary": "本研究展示了预训练大型语言模型（LLMs）如何通过上下文学习（ICL）有效建模隐马尔可夫模型（HMMs）生成的数据。LLMs在合成HMMs上实现了接近最优的预测准确性，并在真实世界的动物决策任务中表现出色。这是首次证明ICL能学习和预测HMM序列，加深了对ICL的理解，并确立了其作为揭示复杂科学数据中隐藏结构的强大工具的潜力。",
      "translated_title": "预训练大型语言模型在上下文中学习隐马尔可夫模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Hidden Markov Models (HMMs) are foundational tools for modeling sequential data with latent Markovian structure, yet fitting them to real-world data remains computationally challenging. In this work, we show that pre-trained large language models (LLMs) can effectively model data generated by HMMs via in-context learning (ICL)x2013their ability to infer patterns from examples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve predictive accuracy approaching the theoretical optimum. We uncover novel scaling trends influenced by HMM properties, and offer theoretical conjectures for these empirical observations. We also provide practical guidelines for scientists on using ICL as a diagnostic tool for complex data. On real-world animal decision-making tasks, ICL achieves competitive performance with models designed by human experts. To our knowledge, this is the first demonstration that ICL can learn and predict HMM-generated sequencesx2013an advance that deepens our understanding of in-context learning in LLMs and establishes its potential as a powerful tool for uncovering hidden structure in complex scientific data."
    },
    {
      "title": "超频LLM推理：监控和控制LLM中的思维路径长度 (原标题: Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs)",
      "link": "https://arxiv.org/abs/2506.07240",
      "pubDate": "Sun, 08 Jun 2025 13:54:33 GMT",
      "isoDate": "2025-06-08T13:54:33.000Z",
      "creator": "Roy Eisenstadt, Itamar Zimerman, Lior Wolf",
      "summary": "本文探讨了大型语言模型（LLM）在显式思考过程中理解和调节推理长度的内在机制。研究发现，通过强制分离模型的内部“思考”过程和最终响应，显式结构化推理等技术最近展示了强大的测试时扩展行为。影响答案质量的一个关键因素是思考阶段的长度。推理太短，模型可能无法捕捉任务的复杂性；推理太长，模型可能过度思考，导致不必要的计算和性能下降。\n\n*   **LLM进度编码：** 研究表明，LLM能够编码其推理过程的进度。作者引入了一个交互式进度条可视化工具，用于揭示模型规划动态的见解。\n*   **内部进度操控：** 通过在推理过程中操控内部进度编码，可以减少不必要的步骤，并生成更简洁、更果断的思维链。\n*   **超频方法：** 实验结果表明，这种“超频”方法可以缓解过度思考，提高答案准确性，并减少推理延迟。\n\n该研究的代码已公开。",
      "shortSummary": "本文研究了大型语言模型（LLM）在显式思考过程中理解和调节推理长度的内在机制。研究发现LLM能够编码其推理过程的进度，并提出通过操控内部进度编码来减少不必要的步骤，从而缓解过度思考，提高答案准确性，并减少推理延迟。该研究的代码已公开。",
      "translated_title": "超频LLM推理：监控和控制LLM中的思维路径长度",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recently, techniques such as explicit structured reasoning have demonstrated strong test-time scaling behavior by enforcing a separation between the model's internal \"thinking\" process and the final response. A key factor influencing answer quality in this setting is the length of the thinking stage. When the reasoning is too short, the model may fail to capture the complexity of the task. Conversely, when it is too long, the model may overthink, leading to unnecessary computation and degraded performance. This paper explores and exploits the underlying mechanisms by which LLMs understand and regulate the length of their reasoning during explicit thought processes. First, we show that LLMs encode their progress through the reasoning process and introduce an interactive progress bar visualization, which is then used to reveal insights on the model's planning dynamics. Second, we manipulate the internal progress encoding during inference to reduce unnecessary steps and generate a more concise and decisive chain of thoughts. Our empirical results demonstrate that this \"overclocking\" method mitigates overthinking, improves answer accuracy, and reduces inference latency. Our code is publicly available."
    }
  ],
  "lastUpdated": "2025-06-10T09:34:33.654Z"
}