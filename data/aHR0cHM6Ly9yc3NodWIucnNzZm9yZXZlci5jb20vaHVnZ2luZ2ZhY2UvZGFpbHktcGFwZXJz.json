{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "MetaEmbed：通过灵活的后期交互在测试时扩展多模态检索 (原标题: MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction)",
      "link": "https://arxiv.org/abs/2509.18095",
      "pubDate": "Mon, 22 Sep 2025 13:59:42 GMT",
      "isoDate": "2025-09-22T13:59:42.000Z",
      "creator": "Zilin Xiao, Qi Ma, Mengting Gu, Chun-cheng Jason Chen, Xintao Chen, Vicente Ordonez, Vijai Mohan",
      "summary": "# MetaEmbed：通过灵活的后期交互在测试时扩展多模态检索\n\n本文介绍了一种名为 MetaEmbed 的新型多模态检索框架，旨在解决现有通用多模态嵌入模型在扩展性方面的挑战。\n\n## 现有方法的局限性\n*   **单向量表示：** 当前方法通常将查询和候选对象压缩成单个向量，这可能限制了对细粒度信息的表达能力。\n*   **多向量成本高昂：** 另一种方法生成过多的向量，导致多向量检索的计算成本过高，难以实际应用。\n\n## MetaEmbed 框架\nMetaEmbed 重新思考了多模态嵌入的构建和大规模交互方式。\n\n### 核心机制\n*   **训练阶段：** 在输入序列中附加固定数量的“可学习元令牌”（learnable Meta Tokens）。\n*   **测试阶段：** 这些元令牌的最后一层上下文表示被用作紧凑而富有表现力的多向量嵌入。\n\n### 训练方法\n*   **Matryoshka 多向量检索训练：** 通过这种训练方法，MetaEmbed 学习以不同粒度在多个向量中组织信息。\n\n## 主要优势与成果\n*   **测试时扩展性：** MetaEmbed 实现了多模态检索的测试时扩展。用户可以根据检索质量和效率需求，灵活选择用于索引和检索交互的令牌数量，从而在两者之间取得平衡。\n*   **最先进的性能：** 在大规模多模态嵌入基准 (MMEB) 和视觉文档检索基准 (ViDoRe) 上的广泛评估表明，MetaEmbed 取得了最先进的检索性能。\n*   **鲁棒的扩展能力：** 该框架能够稳定地扩展到具有 320 亿参数的模型。",
      "shortSummary": "MetaEmbed 是一种新的多模态检索框架，旨在解决现有单向量表达能力有限和多向量成本高昂的问题。它通过在训练时使用可学习的元令牌，并在测试时将其上下文表示作为紧凑的多向量嵌入。通过 Matryoshka 训练，MetaEmbed 实现了测试时扩展，允许用户平衡检索质量与效率。它在 MMEB 和 ViDoRe 上取得了最先进的性能，并能鲁棒地扩展到大型模型。",
      "translated_title": "MetaEmbed：通过灵活的后期交互在测试时扩展多模态检索",
      "images": [],
      "contentSource": "完整文章",
      "content": "Universal multimodal embedding models have achieved great success in capturing semantic relevance between queries and candidates. However, current methods either condense queries and candidates into a single vector, potentially limiting the expressiveness for fine-grained information, or produce too many vectors that are prohibitively expensive for multi-vector retrieval. In this work, we introduce MetaEmbed, a new framework for multimodal retrieval that rethinks how multimodal embeddings are constructed and interacted with at scale. During training, a fixed number of learnable Meta Tokens are appended to the input sequence. At test-time, their last-layer contextualized representations serve as compact yet expressive multi-vector embeddings. Through the proposed Matryoshka Multi-Vector Retrieval training, MetaEmbed learns to organize information by granularity across multiple vectors. As a result, we enable test-time scaling in multimodal retrieval, where users can balance retrieval quality against efficiency demands by selecting the number of tokens used for indexing and retrieval interactions. Extensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and the Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed achieves state-of-the-art retrieval performance while scaling robustly to models with 32B parameters."
    },
    {
      "title": "OnePiece：将上下文工程和推理引入工业级级联排序系统 (原标题: OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System)",
      "link": "https://arxiv.org/abs/2509.18091",
      "pubDate": "Mon, 22 Sep 2025 13:59:07 GMT",
      "isoDate": "2025-09-22T13:59:07.000Z",
      "creator": "Sunhao Dai, Jiakai Tang, Jiahua Wu, Kun Wang, Yuxuan Zhu, Bingjun Chen, Bangyang Hong, Yu Zhao, Cong Fu, Kangle Wu, Yabo Ni, Anxiang Zeng, Wenjie Wang, Xu Chen, Jun Xu, See-Kiong Ng",
      "summary": "# OnePiece：将上下文工程和推理引入工业级级联排序系统\n\n## 摘要\n\n尽管人们对在工业搜索和推荐系统中复制大型语言模型（LLMs）的成功越来越感兴趣，但目前大多数工业实践仅限于移植Transformer架构，这相对于强大的深度学习推荐模型（DLRMs）只带来了增量改进。本文提出了一种名为OnePiece的统一框架，旨在将LLM风格的上下文工程和推理机制无缝集成到工业级联管道的检索和排序模型中，以期实现实质性突破。\n\n## 背景与动机\n\nLLMs的突破不仅源于其架构，还源于两个互补的机制：\n\n*   **上下文工程（Context Engineering）**：通过上下文线索丰富原始输入查询，以更好地激发模型能力。\n*   **多步推理（Multi-step Reasoning）**：通过中间推理路径迭代地优化模型输出。\n\n然而，在工业排序系统中，这两个机制及其释放巨大改进潜力的可能性仍未得到充分探索。\n\n## OnePiece框架\n\nOnePiece是一个基于纯Transformer骨干的统一框架，它引入了三项关键创新：\n\n1.  **结构化上下文工程（Structured Context Engineering）**：\n    *   将交互历史与用户偏好和场景信号进行增强。\n    *   将这些信息统一为结构化的标记化输入序列，供检索和排序模型使用。\n\n2.  **块级潜在推理（Block-wise Latent Reasoning）**：\n    *   使模型具备多步表示细化的能力。\n    *   通过调整块大小来扩展推理带宽。\n\n3.  **渐进式多任务训练（Progressive Multi-task Training）**：\n    *   利用用户反馈链在训练过程中有效监督推理步骤。\n\n## 部署与成果\n\nOnePiece已成功部署在Shopee的主要个性化搜索场景中，并取得了显著的在线收益，具体包括：\n\n*   **GMV/UU（每用户商品交易总额）**：提升超过 +2%。\n*   **广告收入**：增长 +2.90%。\n\n这些成果表明OnePiece在实际工业应用中能够带来持续的业务指标提升。",
      "shortSummary": "OnePiece是一个统一框架，旨在将LLM风格的上下文工程和多步推理引入工业级联排序系统。它基于Transformer架构，通过结构化上下文工程、块级潜在推理和渐进式多任务训练三大创新，解决了现有系统仅移植Transformer架构带来的增量改进问题。OnePiece已在Shopee的个性化搜索中部署，并实现了超过+2%的GMV/UU和+2.90%的广告收入增长，证明了其在实际应用中的有效性。",
      "translated_title": "OnePiece：将上下文工程和推理引入工业级级联排序系统",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite the growing interest in replicating the scaled success of large language models (LLMs) in industrial search and recommender systems, most existing industrial efforts remain limited to transplanting Transformer architectures, which bring only incremental improvements over strong Deep Learning Recommendation Models (DLRMs). From a first principle perspective, the breakthroughs of LLMs stem not only from their architectures but also from two complementary mechanisms: context engineering, which enriches raw input queries with contextual cues to better elicit model capabilities, and multi-step reasoning, which iteratively refines model outputs through intermediate reasoning paths. However, these two mechanisms and their potential to unlock substantial improvements remain largely underexplored in industrial ranking systems.   In this paper, we propose OnePiece, a unified framework that seamlessly integrates LLM-style context engineering and reasoning into both retrieval and ranking models of industrial cascaded pipelines. OnePiece is built on a pure Transformer backbone and further introduces three key innovations: (1) structured context engineering, which augments interaction history with preference and scenario signals and unifies them into a structured tokenized input sequence for both retrieval and ranking; (2) block-wise latent reasoning, which equips the model with multi-step refinement of representations and scales reasoning bandwidth via block size; (3) progressive multi-task training, which leverages user feedback chains to effectively supervise reasoning steps during training. OnePiece has been deployed in the main personalized search scenario of Shopee and achieves consistent online gains across different key business metrics, including over +2% GMV/UU and a +2.90% increase in advertising revenue."
    },
    {
      "title": "ByteWrist：一种用于狭窄空间中实现灵活拟人化运动的并联机器人腕部 (原标题: ByteWrist: A Parallel Robotic Wrist Enabling Flexible and Anthropomorphic Motion for Confined Spaces)",
      "link": "https://arxiv.org/abs/2509.18084",
      "pubDate": "Mon, 22 Sep 2025 13:57:07 GMT",
      "isoDate": "2025-09-22T13:57:07.000Z",
      "creator": "Jiawen Tian, Liqun Huang, Zhongren Cui, Jingchao Qiao, Jiafeng Xu, Xiao Ma, Zeyu Ren",
      "summary": "# ByteWrist：一种用于狭窄空间中实现灵活拟人化运动的并联机器人腕部\n\n本文介绍了一种名为 **ByteWrist** 的新型高度灵活、拟人化的并联机器人腕部，旨在解决现有串联和并联腕部在狭窄空间操作中的关键局限性。\n\n## 核心设计与创新\n\nByteWrist 的核心在于其紧凑的三级并联驱动机构，该机构与弧形末端连杆巧妙集成，实现了卓越的紧凑性，并能提供精确的 RPY（滚动-俯仰-偏航）运动。这使其特别适用于家庭服务、医疗辅助和精密装配等复杂非结构化环境。\n\n其主要创新点包括：\n\n*   **嵌套式三级电机驱动连杆**：这种设计最大限度地减小了体积，同时能够实现独立的多自由度控制。\n*   **弧形末端连杆**：优化了力传输，并显著扩展了腕部的运动范围。\n*   **中央支撑球**：作为一个球形关节，它在不牺牲灵活性的前提下，有效增强了结构的刚度。\n\n## 运动学建模\n\n为了实现精确控制，研究人员为 ByteWrist 提供了全面的运动学建模，包括正向/逆向运动学和数值雅可比矩阵解。\n\n## 实验结果与性能\n\n通过实证观察，ByteWrist 在狭窄空间机动性和双臂协同操作任务中展现出强大的性能，并优于基于 Kinova 的系统。与传统设计相比，ByteWrist 在紧凑性、效率和刚度方面均取得了显著提升。\n\n## 结论\n\nByteWrist 的设计和性能使其成为受限环境中下一代机器人操作的一个有前景的解决方案。",
      "shortSummary": "ByteWrist 是一种新型并联机器人腕部，专为狭窄空间操作设计。它通过紧凑的三级并联驱动机构和弧形末端连杆，实现了灵活、拟人化的精确 RPY 运动。其创新包括嵌套式连杆、优化力传输的弧形连杆及增强刚度的中央支撑球。实验表明，ByteWrist 在狭窄空间机动性和双臂协作任务中表现出色，优于传统系统，在紧凑性、效率和刚度方面均有显著提升，是受限环境下机器人操作的理想方案。",
      "translated_title": "ByteWrist：一种用于狭窄空间中实现灵活拟人化运动的并联机器人腕部",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper introduces ByteWrist, a novel highly-flexible and anthropomorphic parallel wrist for robotic manipulation. ByteWrist addresses the critical limitations of existing serial and parallel wrists in narrow-space operations through a compact three-stage parallel drive mechanism integrated with arc-shaped end linkages. The design achieves precise RPY (Roll-Pitch-Yaw) motion while maintaining exceptional compactness, making it particularly suitable for complex unstructured environments such as home services, medical assistance, and precision assembly. The key innovations include: (1) a nested three-stage motor-driven linkages that minimize volume while enabling independent multi-DOF control, (2) arc-shaped end linkages that optimize force transmission and expand motion range, and (3) a central supporting ball functioning as a spherical joint that enhances structural stiffness without compromising flexibility. Meanwhile, we present comprehensive kinematic modeling including forward / inverse kinematics and a numerical Jacobian solution for precise control. Empirically, we observe ByteWrist demonstrates strong performance in narrow-space maneuverability and dual-arm cooperative manipulation tasks, outperforming Kinova-based systems. Results indicate significant improvements in compactness, efficiency, and stiffness compared to traditional designs, establishing ByteWrist as a promising solution for next-generation robotic manipulation in constrained environments."
    },
    {
      "title": "Reasoning Core：一个用于LLM符号推理的可扩展强化学习环境 (原标题: Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning)",
      "link": "https://arxiv.org/abs/2509.18083",
      "pubDate": "Mon, 22 Sep 2025 13:56:38 GMT",
      "isoDate": "2025-09-22T13:56:38.000Z",
      "creator": "Valentin Lacombe, Valentin Quesnel, Damien Sileo",
      "summary": "### Reasoning Core：LLM符号推理的新型可扩展强化学习环境\n\n**1. 项目介绍**\n*   **Reasoning Core** 是一个为**可验证奖励强化学习 (RLVR)** 设计的新型可扩展环境。\n*   其核心目标是推动**大型语言模型 (LLM)** 在基础**符号推理**方面的进步。\n\n**2. 与现有基准的区别**\n*   不同于现有专注于游戏或孤立谜题的基准。\n*   Reasoning Core 通过**程序化生成**方式，在多个核心形式领域创建问题。\n\n**3. 涵盖的核心形式领域**\n*   **PDDL 规划**\n*   **一阶逻辑**\n*   **上下文无关文法解析**\n*   **因果推理**\n*   **系统方程求解**\n\n**4. 关键设计原则**\n*   **高通用性问题分布**：确保生成的问题具有广泛的适用性和多样性。\n*   **通过外部工具进行验证**：利用外部工具对LLM的解决方案进行验证，确保正确性。\n*   **持续难度控制**：环境能够根据需要调整任务难度，提供渐进式的学习曲线。\n\n**5. 设计优势**\n*   这些设计原则共同提供了**几乎无限的新颖训练实例**，解决了传统基准数据量有限的问题。\n\n**6. 初步评估结果**\n*   对前沿LLM进行的**零样本评估**证实了Reasoning Core任务的难度，表明其具有挑战性。\n\n**7. 未来展望**\n*   Reasoning Core 被定位为一个**有前景的资源**，有望显著提升未来LLM的推理能力。",
      "shortSummary": "Reasoning Core是一个新型可扩展的强化学习环境，旨在通过可验证奖励（RLVR）提升大型语言模型（LLM）的符号推理能力。它通过程序化生成PDDL规划、一阶逻辑等核心形式领域的问题，而非依赖孤立谜题。该环境基于高通用性问题分布、外部工具验证和持续难度控制原则，提供无限训练实例。初步评估显示其任务难度高，有望成为提升未来LLM推理能力的关键资源。",
      "translated_title": "Reasoning Core：一个用于LLM符号推理的可扩展强化学习环境",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Reasoning Core, a new scalable environment for Reinforcement Learning with Verifiable Rewards (RLVR), designed to advance foundational symbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks that focus on games or isolated puzzles, Reasoning Core procedurally generates problems across core formal domains, including PDDL planning, first-order logic, context-free grammar parsing, causal reasoning, and system equation solving. The environment is built on key design principles of high-generality problem distributions, verification via external tools, and continuous difficulty control, which together provide a virtually infinite supply of novel training instances. Initial zero-shot evaluations with frontier LLMs confirm the difficulty of Reasoning Core's tasks, positioning it as a promising resource to improve the reasoning capabilities of future models."
    },
    {
      "title": "TempSamp-R1：基于强化微调的视频大语言模型有效时序采样 (原标题: TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs)",
      "link": "https://arxiv.org/abs/2509.18056",
      "pubDate": "Mon, 22 Sep 2025 13:30:15 GMT",
      "isoDate": "2025-09-22T13:30:15.000Z",
      "creator": "Yunheng Li, Jing Cheng, Shaoyong Jia, Hangyi Kuang, Shaohui Jiao, Qibin Hou, Ming-Ming Cheng",
      "summary": "### TempSamp-R1：用于视频大语言模型的有效时序采样与强化微调\n\n本文介绍了TempSamp-R1，一个旨在提升多模态大语言模型（MLLMs）在视频时序定位任务中适应性的新型强化微调框架。该框架解决了现有强化学习方法（如Group Relative Policy Optimization, GRPO）在处理具有大时序搜索空间任务时，因依赖on-policy采样而导致的效率低下和性能受限问题。\n\n#### 现有问题与挑战\n*   **on-policy采样的局限性：** 现有强化学习方法通常依赖on-policy采样进行策略更新。在视频时序定位这类具有巨大时序搜索空间的任务中，这种策略难以识别时序准确的解决方案，导致效率低下和性能瓶颈。\n\n#### TempSamp-R1 的核心创新与机制\nTempSamp-R1通过以下关键机制克服了上述限制：\n\n1.  **离线监督（Off-policy Supervision）：**\n    *   TempSamp-R1利用真实标注作为off-policy监督，提供时序精确的指导。\n    *   这有效弥补了on-policy解决方案中常见的稀疏性和错位问题，确保模型能够学习到更准确的时序信息。\n\n2.  **非线性软优势计算（Non-linear Soft Advantage Computation）：**\n    *   为了稳定训练过程并减少基于奖励更新的方差，TempSamp-R1引入了一种非线性软优势计算方法。\n    *   该方法通过非对称变换动态调整奖励反馈，从而优化学习效率和稳定性。\n\n3.  **混合思维链（CoT）训练范式：**\n    *   TempSamp-R1采用混合CoT训练范式，优化一个统一模型。\n    *   该模型能够同时支持CoT和非CoT两种推理模式，从而高效处理具有不同推理复杂度的查询，增强了模型的灵活性和适用性。\n\n#### 实验结果与性能提升\n实验结果表明，TempSamp-R1在多个基准数据集上显著超越了基于GRPO的基线方法，取得了新的最先进（SOTA）性能：\n\n*   **Charades-STA：** R1@0.7 达到 52.9%，相对提升 2.7%。\n*   **ActivityNet Captions：** R1@0.5 达到 56.0%，相对提升 5.3%。\n*   **QVHighlights：** mAP 达到 30.0%，相对提升 3.0%。\n\n此外，TempSamp-R1在有限数据条件下展现出强大的少样本泛化能力，证明了其在数据稀缺场景下的鲁棒性。\n\n#### 总结\nTempSamp-R1通过引入离线监督、非线性软优势计算和混合CoT训练范式，为视频大语言模型在时序定位任务中的强化微调提供了一个高效且性能卓越的解决方案，显著提升了模型在多个关键指标上的表现。",
      "shortSummary": "TempSamp-R1是一个为视频大语言模型（VLLMs）设计的强化微调框架，旨在解决现有方法在视频时序定位任务中on-policy采样效率低下的问题。它通过利用真实标注作为off-policy监督提供精确指导，并引入非线性软优势计算稳定训练。结合混合思维链（CoT）训练范式，TempSamp-R1在Charades-STA、ActivityNet Captions和QVHighlights等基准数据集上取得了新的最先进（SOTA）性能，并展现出强大的少样本泛化能力。",
      "translated_title": "TempSamp-R1：基于强化微调的视频大语言模型有效时序采样",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework designed to improve the effectiveness of adapting multimodal large language models (MLLMs) to video temporal grounding tasks. We reveal that existing reinforcement learning methods, such as Group Relative Policy Optimization (GRPO), rely on on-policy sampling for policy updates. However, in tasks with large temporal search spaces, this strategy becomes both inefficient and limited in performance, as it often fails to identify temporally accurate solutions. To address this limitation, TempSamp-R1 leverages ground-truth annotations as off-policy supervision to provide temporally precise guidance, effectively compensating for the sparsity and misalignment in on-policy solutions. To further stabilize training and reduce variance in reward-based updates, TempSamp-R1 provides a non-linear soft advantage computation method that dynamically reshapes the reward feedback via an asymmetric transformation. By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1 optimizes a single unified model to support both CoT and non-CoT inference modes, enabling efficient handling of queries with varying reasoning complexity. Experimental results demonstrate that TempSamp-R1 outperforms GRPO-based baselines, establishing new state-of-the-art performance on benchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions (R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover, TempSamp-R1 shows robust few-shot generalization capabilities under limited data. Code: https://github.com/HVision-NKU/TempSamp-R1"
    },
    {
      "title": "VideoFrom3D：通过互补图像和视频扩散模型生成3D场景视频 (原标题: VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video Diffusion Models)",
      "link": "https://arxiv.org/abs/2509.17985",
      "pubDate": "Mon, 22 Sep 2025 12:28:47 GMT",
      "isoDate": "2025-09-22T12:28:47.000Z",
      "creator": "Geonung Kim, Janghyeok Han, Sunghyun Cho",
      "summary": "# VideoFrom3D：通过互补图像和视频扩散模型生成3D场景视频\n\n本文提出了 **VideoFrom3D**，一个用于从粗糙几何结构、摄像机轨迹和参考图像合成高质量3D场景视频的新颖框架。该方法旨在简化3D图形设计工作流程，实现灵活的设计探索和快速交付成果。\n\n## 挑战与问题\n直接使用视频扩散模型从粗糙几何结构合成视频，往往难以在复杂场景中生成高保真结果。主要原因是现有模型难以同时建模视觉质量、运动和时间一致性。\n\n## 解决方案：VideoFrom3D 框架\nVideoFrom3D 框架通过结合图像和视频扩散模型的互补优势来解决上述挑战。它包含两个核心模块：\n\n### 1. 稀疏锚点视图生成 (Sparse Anchor-view Generation, SAG) 模块\n*   **功能**：生成高质量、跨视图一致的锚点视图。\n*   **技术**：利用图像扩散模型，并通过稀疏外观引导采样（Sparse Appearance-guided Sampling）进行辅助。\n\n### 2. 几何引导生成插值 (Geometry-guided Generative Inbetweening, GGI) 模块\n*   **功能**：基于SAG模块生成的锚点视图，忠实地插值生成中间帧。\n*   **技术**：利用视频扩散模型，并通过基于流的摄像机控制和结构引导进行增强。\n\n## 关键创新与优势\n*   **无配对数据集需求**：VideoFrom3D 的两个模块均无需依赖3D场景模型和自然图像的配对数据集。获取此类数据集极其困难，因此这一特性显著降低了方法实现的门槛。\n*   **高性能表现**：全面的实验结果表明，该方法能够在多样化且具有挑战性的场景下，生成高质量、风格一致的场景视频，并且性能优于简单及扩展的基线方法。\n\n## 输入与输出\n*   **输入**：粗糙几何结构、摄像机轨迹、参考图像。\n*   **输出**：高质量的3D场景视频。",
      "shortSummary": "VideoFrom3D是一个新颖框架，能从粗糙几何、摄像机轨迹和参考图像生成高质量3D场景视频。它通过结合图像和视频扩散模型的优势，解决了现有视频扩散模型在复杂场景中难以同时保证视觉质量、运动和时间一致性的问题。该框架包含稀疏锚点视图生成（SAG）和几何引导生成插值（GGI）两个模块，且无需配对的3D场景模型与自然图像数据集。实验证明其在各种场景下均能生成高质量、风格一致的视频。",
      "translated_title": "VideoFrom3D：通过互补图像和视频扩散模型生成3D场景视频",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this paper, we propose VideoFrom3D, a novel framework for synthesizing high-quality 3D scene videos from coarse geometry, a camera trajectory, and a reference image. Our approach streamlines the 3D graphic design workflow, enabling flexible design exploration and rapid production of deliverables. A straightforward approach to synthesizing a video from coarse geometry might condition a video diffusion model on geometric structure. However, existing video diffusion models struggle to generate high-fidelity results for complex scenes due to the difficulty of jointly modeling visual quality, motion, and temporal consistency. To address this, we propose a generative framework that leverages the complementary strengths of image and video diffusion models. Specifically, our framework consists of a Sparse Anchor-view Generation (SAG) and a Geometry-guided Generative Inbetweening (GGI) module. The SAG module generates high-quality, cross-view consistent anchor views using an image diffusion model, aided by Sparse Appearance-guided Sampling. Building on these anchor views, GGI module faithfully interpolates intermediate frames using a video diffusion model, enhanced by flow-based camera control and structural guidance. Notably, both modules operate without any paired dataset of 3D scene models and natural images, which is extremely difficult to obtain. Comprehensive experiments show that our method produces high-quality, style-consistent scene videos under diverse and challenging scenarios, outperforming simple and extended baselines."
    },
    {
      "title": "ContextFlow：通过自适应上下文丰富实现免训练视频对象编辑 (原标题: ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment)",
      "link": "https://arxiv.org/abs/2509.17818",
      "pubDate": "Mon, 22 Sep 2025 10:13:31 GMT",
      "isoDate": "2025-09-22T10:13:31.000Z",
      "creator": "Yiyang Chen, Xuanhua He, Xiujun Ma, Yue Ma",
      "summary": "# ContextFlow：通过自适应上下文丰富实现免训练视频对象编辑\n\n## 摘要\n\n本文介绍了 ContextFlow，一个新颖的免训练框架，专为基于 Diffusion Transformers (DiTs) 的视频对象编辑而设计。该框架旨在解决现有方法在对象级操作（如插入、交换、删除）中面临的保真度和时间一致性挑战，特别是针对不准确的反演和上下文冲突问题。\n\n## 现有挑战\n\n当前的免训练视频对象编辑方法，尤其是在 U-Net 架构中，存在以下主要局限性：\n\n*   **不准确的反演：** 主要由一阶求解器引起，导致编辑基础不稳固。\n*   **上下文冲突：** 粗糙的“硬”特征替换策略导致编辑区域与周围环境不协调。\n*   **DiT 架构的特殊挑战：** 在 Diffusion Transformers (DiTs) 中，由于先前层选择启发式方法的不适用性，上述问题变得更加严峻，有效引导难以实现。\n\n## ContextFlow 框架\n\nContextFlow 提出了一套创新的解决方案来克服这些挑战：\n\n### 1. 稳健的编辑基础：高阶整流流求解器\n\n*   **目的：** 建立一个强大的编辑基础。\n*   **方法：** 采用高阶整流流 (Rectified Flow) 求解器，有效解决了一阶求解器导致的不准确反演问题，为后续的精确编辑奠定基础。\n\n### 2. 解决上下文冲突：自适应上下文丰富 (Adaptive Context Enrichment, ACE)\n\n*   **目的：** 解决由粗糙特征替换引起的上下文冲突，明确“编辑什么”。\n*   **创新点：**\n    *   **替代硬替换：** ContextFlow 摒弃了传统的直接替换特征的方法。\n    *   **丰富自注意力上下文：** 它通过将来自并行重建路径和编辑路径的 Key-Value 对进行拼接，来丰富模型的自注意力上下文。\n    *   **动态信息融合：** 这种机制赋予模型动态融合信息的能力，使其能够更自然、更流畅地将编辑内容融入视频，避免了生硬的边界和不一致性。\n\n### 3. 精准引导定位：数据驱动的生命层分析\n\n*   **目的：** 确定“在哪里应用丰富”，实现有针对性的编辑。\n*   **方法：**\n    *   **系统性分析：** 提出了一种系统性的、数据驱动的分析方法，以识别 DiT 模型中对特定任务（例如，对象插入、交换）至关重要的层。\n    *   **引导响应度量：** 基于新颖的“引导响应度量 (Guidance Responsiveness Metric)”，该方法能够精确地找出最具影响力的 DiT 块，从而实现高度有效和精准的引导。\n\n## 实验结果\n\n广泛的实验表明，ContextFlow 在性能上取得了显著突破：\n\n*   **超越现有免训练方法：** ContextFlow 显著优于所有现有的免训练视频对象编辑方法。\n*   **媲美甚至超越基于训练的方法：** 甚至在多个方面超越了一些最先进的基于训练的方法。\n*   **高质量输出：** 最终结果展现出卓越的时间连贯性和高保真度。\n\n## 结论\n\nContextFlow 通过其创新的高阶求解器、自适应上下文丰富机制和数据驱动的层分析，为免训练视频对象编辑领域树立了新标杆，有效解决了该领域长期存在的挑战。",
      "shortSummary": "ContextFlow是一个免训练视频对象编辑框架，旨在解决现有方法在保真度和时间一致性方面的挑战。它通过引入高阶整流流求解器、自适应上下文丰富（通过拼接Key-Value对动态融合信息）以及基于引导响应度量的数据驱动层分析（识别关键DiT块）来克服不准确反演和上下文冲突问题。实验证明，ContextFlow在生成时间连贯、高保真结果方面显著优于现有免训练方法，并超越了部分基于训练的先进方法。",
      "translated_title": "ContextFlow：通过自适应上下文丰富实现免训练视频对象编辑",
      "images": [],
      "contentSource": "完整文章",
      "content": "Training-free video object editing aims to achieve precise object-level manipulation, including object insertion, swapping, and deletion. However, it faces significant challenges in maintaining fidelity and temporal consistency. Existing methods, often designed for U-Net architectures, suffer from two primary limitations: inaccurate inversion due to first-order solvers, and contextual conflicts caused by crude \"hard\" feature replacement. These issues are more challenging in Diffusion Transformers (DiTs), where the unsuitability of prior layer-selection heuristics makes effective guidance challenging. To address these limitations, we introduce ContextFlow, a novel training-free framework for DiT-based video object editing. In detail, we first employ a high-order Rectified Flow solver to establish a robust editing foundation. The core of our framework is Adaptive Context Enrichment (for specifying what to edit), a mechanism that addresses contextual conflicts. Instead of replacing features, it enriches the self-attention context by concatenating Key-Value pairs from parallel reconstruction and editing paths, empowering the model to dynamically fuse information. Additionally, to determine where to apply this enrichment (for specifying where to edit), we propose a systematic, data-driven analysis to identify task-specific vital layers. Based on a novel Guidance Responsiveness Metric, our method pinpoints the most influential DiT blocks for different tasks (e.g., insertion, swapping), enabling targeted and highly effective guidance. Extensive experiments show that ContextFlow significantly outperforms existing training-free methods and even surpasses several state-of-the-art training-based approaches, delivering temporally coherent, high-fidelity results."
    },
    {
      "title": "通义千问3-Omni技术报告 (原标题: Qwen3-Omni Technical Report)",
      "link": "https://arxiv.org/abs/2509.17765",
      "pubDate": "Mon, 22 Sep 2025 09:26:24 GMT",
      "isoDate": "2025-09-22T09:26:24.000Z",
      "creator": "Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, Junyang Lin",
      "summary": "通义千问3-Omni（Qwen3-Omni）是一款创新的多模态模型，首次在文本、图像、音频和视频等多种模态上实现了与单模态模型相当的顶尖性能，且未出现性能下降。该模型在音频任务上表现尤为突出。\n\n### 主要成就与性能\n*   **多模态SOTA**：Qwen3-Omni在保持与同尺寸单模态模型（Qwen系列内部）性能一致的同时，首次实现了跨文本、图像、音频和视频的顶尖性能。\n*   **音频卓越表现**：在36个音频及音视频基准测试中，Qwen3-Omni在32个基准测试中达到了开源SOTA水平，并在22个基准测试中实现了整体SOTA，超越了包括Gemini-2.5-Pro、Seed-ASR和GPT-4o-Transcribe在内的强大闭源模型。\n\n### 核心架构与功能\n*   **Thinker-Talker MoE架构**：该模型采用Thinker-Talker MoE（混合专家）架构，统一了文本、图像、音频和视频的感知与生成，能够生成流畅的文本和自然的实时语音。\n*   **多语言支持**：\n    *   支持119种语言的文本交互。\n    *   支持19种语言的语音理解。\n    *   支持10种语言的语音生成。\n\n### 语音生成优化\n*   **低延迟流式合成**：为减少流式合成中的首包延迟，Talker组件采用多码本方案自回归预测离散语音编解码器。\n*   **轻量级ConvNet**：利用码本的表示能力，模型用轻量级因果ConvNet取代了计算密集型的块状扩散模型，实现了从第一个编解码帧开始流式传输。\n*   **理论首包延迟**：在冷启动设置下，Qwen3-Omni的理论端到端首包延迟为234毫秒。\n\n### 多模态推理与应用扩展\n*   **Thinking模型**：引入了一个Thinking模型，旨在增强多模态推理能力，能够明确地对来自任何模态的输入进行推理。\n*   **音频字幕模型**：鉴于当前研究社区缺乏通用的音频字幕模型，研究人员对Qwen3-Omni-30B-A3B进行了微调，得到了Qwen3-Omni-30B-A3B-Captioner。该模型能为任意音频输入生成详细、低幻觉的字幕。\n\n### 开源发布\n*   Qwen3-Omni-30B-A3B\n*   Qwen3-Omni-30B-A3B-Thinking\n*   Qwen3-Omni-30B-A3B-Captioner\n\n以上模型均已在Apache 2.0许可下公开发布。\n\n### 页面图标\n*   BibSonomy图标：![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)\n*   Reddit图标：![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)",
      "shortSummary": "通义千问3-Omni（Qwen3-Omni）是一款突破性的多模态模型，首次在文本、图像、音频和视频上实现了与单模态模型相当的顶尖性能，且无性能损失。它在音频任务上表现卓越，在多项基准测试中达到SOTA，超越了包括GPT-4o在内的闭源模型。该模型采用Thinker-Talker MoE架构，支持多语言交互，并通过优化实现了234毫秒的低延迟语音生成。此外，还推出了增强多模态推理的Thinking模型和音频字幕模型，并已开源发布。",
      "translated_title": "通义千问3-Omni技术报告",
      "images": [
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png",
          "alt": "BibSonomy logo",
          "title": "",
          "position": 1
        },
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png",
          "alt": "Reddit logo",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "We present Qwen3-Omni, a single multimodal model that, for the first time, maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages, and speech generation in 10 languages. To reduce first-packet latency in streaming synthesis, Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms. To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license."
    },
    {
      "title": "Turk-LettuceDetect：土耳其语RAG应用的幻觉检测模型 (原标题: Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications)",
      "link": "https://arxiv.org/abs/2509.17671",
      "pubDate": "Mon, 22 Sep 2025 08:14:11 GMT",
      "isoDate": "2025-09-22T08:14:11.000Z",
      "creator": "Selva Taş, Mahmut El Huseyni, Özay Ezerceli, Reyhan Bayraktar, Fatma Betül Terzioğlu",
      "summary": "# Turk-LettuceDetect：土耳其语RAG应用的幻觉检测模型\n\n## 背景与问题\n\n大型语言模型（LLMs）的广泛应用受到其“幻觉”问题的阻碍，即生成看似合理但实际上不准确的信息。尽管检索增强生成（RAG）系统试图通过将响应基于外部知识来解决这一问题，但幻觉仍然是一个持续的挑战，尤其对于土耳其语这种形态复杂、资源稀缺的语言。\n\n## 解决方案：Turk-LettuceDetect\n\n本文介绍了Turk-LettuceDetect，这是首个专门为土耳其语RAG应用设计的幻觉检测模型套件。该框架基于LettuceDetect，将幻觉检测任务定义为令牌级分类。\n\n## 模型架构与训练\n\n1.  **编码器架构**：微调了三种不同的编码器架构：\n    *   土耳其语特有的ModernBERT\n    *   TurkEmbed4STS\n    *   多语言EuroBERT\n2.  **训练数据集**：模型在RAGTruth基准数据集的机器翻译版本上进行训练，该数据集包含17,790个实例，涵盖了问答、数据到文本生成和摘要任务。\n\n## 实验结果与性能\n\n*   **F1分数**：基于ModernBERT的模型在完整测试集上实现了0.7266的F1分数，在结构化任务上表现尤为出色。\n*   **计算效率与上下文支持**：模型保持了计算效率，并支持长达8,192个令牌的上下文，使其适用于实时部署。\n*   **与现有LLMs的比较**：\n    *   最先进的LLMs虽然召回率高，但由于过度生成幻觉内容，导致精确率较低。\n    *   这突显了专门的幻觉检测机制的必要性。\n\n## 贡献与意义\n\n*   **填补空白**：通过发布模型和翻译后的数据集，这项工作填补了多语言自然语言处理领域的关键空白。\n*   **奠定基础**：为开发更可靠、更值得信赖的土耳其语及其他语言的AI应用奠定了基础。",
      "shortSummary": "Turk-LettuceDetect是首个专为土耳其语RAG应用设计的幻觉检测模型套件。它将幻觉检测视为令牌级分类任务，并微调了ModernBERT、TurkEmbed4STS和EuroBERT等编码器。基于ModernBERT的模型在测试集上F1分数达到0.7266，且计算高效，支持长上下文。该工作旨在解决LLM在土耳其语RAG中幻觉问题，提升AI应用的可靠性，并发布了模型和数据集以促进研究。",
      "translated_title": "Turk-LettuceDetect：土耳其语RAG应用的幻觉检测模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "The widespread adoption of Large Language Models (LLMs) has been hindered by their tendency to hallucinate, generating plausible but factually incorrect information. While Retrieval-Augmented Generation (RAG) systems attempt to address this issue by grounding responses in external knowledge, hallucination remains a persistent challenge, particularly for morphologically complex, low-resource languages like Turkish. This paper introduces Turk-LettuceDetect, the first suite of hallucination detection models specifically designed for Turkish RAG applications. Building on the LettuceDetect framework, we formulate hallucination detection as a token-level classification task and fine-tune three distinct encoder architectures: a Turkish-specific ModernBERT, TurkEmbed4STS, and multilingual EuroBERT. These models were trained on a machine-translated version of the RAGTruth benchmark dataset containing 17,790 instances across question answering, data-to-text generation, and summarization tasks. Our experimental results show that the ModernBERT-based model achieves an F1-score of 0.7266 on the complete test set, with particularly strong performance on structured tasks. The models maintain computational efficiency while supporting long contexts up to 8,192 tokens, making them suitable for real-time deployment. Comparative analysis reveals that while state-of-the-art LLMs demonstrate high recall, they suffer from low precision due to over-generation of hallucinated content, underscoring the necessity of specialized detection mechanisms. By releasing our models and translated dataset, this work addresses a critical gap in multilingual NLP and establishes a foundation for developing more reliable and trustworthy AI applications for Turkish and other languages."
    },
    {
      "title": "AuditoryBench++: 语言模型能否在不听觉的情况下理解听觉知识？ (原标题: AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?)",
      "link": "https://arxiv.org/abs/2509.17641",
      "pubDate": "Mon, 22 Sep 2025 07:45:22 GMT",
      "isoDate": "2025-09-22T07:45:22.000Z",
      "creator": "Hyunjong Ok, Suho Yoo, Hyeonjun Kim, Jaeho Lee",
      "summary": "## AuditoryBench++: 语言模型中的听觉知识与推理\n\n### 核心问题\n\n尽管人类无需直接听觉输入即可轻松推理音高、响度或声源关联等听觉属性，但语言模型（LLMs）通常缺乏这种能力。这限制了它们在多模态交互中的有效性。\n\n### 解决方案的初步步骤\n\n为了弥补这一差距，研究人员提出了两个关键贡献：\n\n1.  **AuditoryBench++ 基准测试**：\n    *   **目的**：这是一个全面的基准，旨在评估文本环境中语言模型的听觉知识和推理能力。\n    *   **任务范围**：它涵盖了从基本的听觉比较到基于上下文的推理等一系列任务。\n    *   **分析能力**：该基准能够对模型如何处理和整合听觉概念进行细致的分析。\n\n2.  **AIR-CoT (Auditory Imagination Reasoning - Chain of Thought) 方法**：\n    *   **创新性**：这是一种新颖的听觉想象推理方法。\n    *   **工作原理**：它通过使用特殊标记进行跨度检测和知识注入，在推理过程中生成并整合听觉信息。\n\n### 实验结果\n\n*   对最新的大型语言模型（LLMs）和多模态大型语言模型（Multimodal LLMs）进行了广泛的实验。\n*   实验结果表明，AIR-CoT 方法普遍优于：\n    *   现成的（off-the-shelf）模型。\n    *   以及那些已通过听觉知识增强的模型。\n\n### 项目信息\n\n*   **评论**：预印本（Preprint）。\n*   **主题**：计算与语言 (cs.CL)、人工智能 (cs.AI)、机器学习 (cs.LG)、声音 (cs.SD)。\n*   **引用方式**：arXiv:2509.17641 [cs.CL]。\n*   **提交历史**：v1 版本于 2025 年 9 月 22 日提交。",
      "shortSummary": "语言模型（LLMs）在没有直接听觉输入的情况下，难以像人类一样进行听觉推理。为解决此问题，研究人员提出了 AuditoryBench++，这是一个评估文本环境中听觉知识和推理的基准。同时，他们引入了 AIR-CoT，一种新颖的听觉想象推理方法，通过生成和整合听觉信息来提高模型性能。实验表明，AIR-CoT 普遍优于现有模型和已增强听觉知识的模型。",
      "translated_title": "AuditoryBench++: 语言模型能否在不听觉的情况下理解听觉知识？",
      "images": [],
      "contentSource": "完整文章",
      "content": "Even without directly hearing sounds, humans can effortlessly reason about auditory properties, such as pitch, loudness, or sound-source associations, drawing on auditory commonsense. In contrast, language models often lack this capability, limiting their effectiveness in multimodal interactions. As an initial step to address this gap, we present AuditoryBench++, a comprehensive benchmark for evaluating auditory knowledge and reasoning in text-only settings. The benchmark encompasses tasks that range from basic auditory comparisons to contextually grounded reasoning, enabling fine-grained analysis of how models process and integrate auditory concepts. In addition, we introduce AIR-CoT, a novel auditory imagination reasoning method that generates and integrates auditory information during inference through span detection with special tokens and knowledge injection. Extensive experiments with recent LLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both the off-the-shelf models and those augmented with auditory knowledge. The project page is available at https://auditorybenchpp.github.io."
    },
    {
      "title": "OmniInsert：基于扩散Transformer模型的无掩码任意参考视频插入 (原标题: OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models)",
      "link": "https://arxiv.org/abs/2509.17627",
      "pubDate": "Mon, 22 Sep 2025 07:35:55 GMT",
      "isoDate": "2025-09-22T07:35:55.000Z",
      "creator": "Jinshu Chen, Xinghui Li, Xu Bai, Tianxiang Ma, Pengze Zhang, Zhuowei Chen, Gen Li, Lijie Liu, Songtao Zhao, Bingchuan Li, Qian He",
      "summary": "本文介绍了一个名为 **OmniInsert** 的创新框架，旨在解决无掩码视频插入领域中的关键挑战，特别是现有方法在主体一致性方面的不足以及对复杂控制信号的依赖。\n\n### 核心问题与挑战\n\n当前基于扩散模型的视频插入方法虽然取得了显著进展，但仍面临以下挑战：\n\n*   **主体一致性**：难以在插入过程中保持主体的一致性。\n*   **复杂控制信号**：现有方法通常依赖于复杂的控制信号，限制了其实用性。\n\nOmniInsert 专注于无掩码视频插入任务，并着力解决三个关键挑战：\n\n1.  **数据稀缺性 (Data Scarcity)**\n2.  **主体-场景平衡 (Subject-Scene Equilibrium)**\n3.  **插入和谐性 (Insertion Harmonization)**\n\n### 提出的解决方案\n\n为了克服上述挑战，作者提出了以下创新方法和组件：\n\n#### 1. 解决数据稀缺性：InsertPipe 数据管道\n\n*   **InsertPipe**：提出了一种新的数据管道，能够自动构建多样化的交叉配对数据，从而有效缓解了数据稀缺问题。\n\n#### 2. 解决主体-场景平衡：OmniInsert 框架与训练策略\n\n*   **OmniInsert**：基于 InsertPipe 数据管道，开发了一个新颖的统一框架，用于处理单主体和多主体参考的无掩码视频插入。\n*   **Condition-Specific Feature Injection (条件特定特征注入机制)**：引入了一种简单而有效的机制，用于明确地注入多源条件，以保持主体与场景之间的平衡。\n*   **Progressive Training (渐进式训练策略)**：提出了一种新颖的训练策略，使模型能够平衡来自主体和源视频的特征注入。\n*   **Subject-Focused Loss (主体聚焦损失)**：设计了该损失函数，以改善插入主体的细节外观。\n\n#### 3. 增强插入和谐性：优化与整合模块\n\n*   **Insertive Preference Optimization (插入偏好优化)**：提出了一种方法，通过模拟人类偏好来优化模型，从而进一步增强插入的和谐性。\n*   **Context-Aware Rephraser (上下文感知重构模块)**：在参考过程中整合此模块，以将主体无缝地融入原始场景。\n\n### 基准与评估\n\n*   **InsertBench**：为了弥补该领域缺乏统一基准的空白，作者引入了一个全面的基准，包含多样化的场景和精心选择的主体。\n*   **评估结果**：在 InsertBench 上的评估表明，OmniInsert 的性能优于最先进的闭源商业解决方案。\n\n### 展望\n\n*   **代码发布**：相关代码将公开发布。",
      "shortSummary": "OmniInsert是一个基于扩散Transformer模型的无掩码视频插入框架，旨在解决现有方法在主体一致性、数据稀缺、主体-场景平衡和插入和谐性方面的挑战。它通过`InsertPipe`数据管道解决数据稀缺，引入`Condition-Specific Feature Injection`和`Progressive Training`策略平衡主体与场景，并通过`Insertive Preference Optimization`和`Context-Aware Rephraser`增强插入和谐性。该方法在自建基准`InsertBench`上表现优于现有商业解决方案，代码即将发布。",
      "translated_title": "OmniInsert：基于扩散Transformer模型的无掩码任意参考视频插入",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in video insertion based on diffusion models are impressive. However, existing methods rely on complex control signals but struggle with subject consistency, limiting their practical applicability. In this paper, we focus on the task of Mask-free Video Insertion and aim to resolve three key challenges: data scarcity, subject-scene equilibrium, and insertion harmonization. To address the data scarcity, we propose a new data pipeline InsertPipe, constructing diverse cross-pair data automatically. Building upon our data pipeline, we develop OmniInsert, a novel unified framework for mask-free video insertion from both single and multiple subject references. Specifically, to maintain subject-scene equilibrium, we introduce a simple yet effective Condition-Specific Feature Injection mechanism to distinctly inject multi-source conditions and propose a novel Progressive Training strategy that enables the model to balance feature injection from subjects and source video. Meanwhile, we design the Subject-Focused Loss to improve the detailed appearance of the subjects. To further enhance insertion harmonization, we propose an Insertive Preference Optimization methodology to optimize the model by simulating human preferences, and incorporate a Context-Aware Rephraser module during reference to seamlessly integrate the subject into the original scenes. To address the lack of a benchmark for the field, we introduce InsertBench, a comprehensive benchmark comprising diverse scenes with meticulously selected subjects. Evaluation on InsertBench indicates OmniInsert outperforms state-of-the-art closed-source commercial solutions. The code will be released."
    },
    {
      "title": "LIMI：少即是多，赋能智能体能力 (原标题: LIMI: Less is More for Agency)",
      "link": "https://arxiv.org/abs/2509.17567",
      "pubDate": "Mon, 22 Sep 2025 06:59:32 GMT",
      "isoDate": "2025-09-22T06:59:32.000Z",
      "creator": "Yang Xiao, Mohan Jiang, Jie Sun, Keyu Li, Jifan Lin, Yumin Zhuang, Ji Zeng, Shijie Xia, Qishuo Hua, Xuefeng Li, Xiaojie Cai, Tongyu Wang, Yue Zhang, Liming Liu, Xia Wu, Jinlong Hou, Yuan Cheng, Wenjie Li, Xiang Wang, Dequan Wang, Pengfei Liu",
      "summary": "## LIMI：少即是多，赋能智能体能力\n\n### 引言：智能体能力 (Agency) 的定义与重要性\n\n文章定义了**智能体能力 (Agency)** 为AI系统的一种新兴能力，使其能够作为自主智能体，通过与环境和工具的自主交互，主动发现问题、制定假设并执行解决方案。这种基础能力标志着“AI智能体时代”的到来，其核心驱动力是行业对AI系统从“思考”到“工作”的迫切需求。当前AI在推理和生成响应方面表现出色，但工业界需要能够执行任务、操作工具并推动实际成果的自主智能体。智能体智能正成为区分认知系统和生产性工作者的决定性特征，因此，高效培养机器自主性变得至关重要。\n\n### 挑战传统范式：数据量并非唯一决定因素\n\n传统的AI开发方法，沿袭语言模型的扩展定律，普遍认为更多的数据会带来更好的智能体能力。然而，LIMI（Less Is More for Intelligent Agency，少即是多，赋能智能体能力）项目从根本上挑战了这一范式。研究表明，智能体能力的发展遵循着截然不同的原则。\n\n### LIMI 方法论：战略性策划，实现高效智能体能力\n\nLIMI通过战略性地关注协作软件开发和科学研究工作流，证明了复杂的智能体智能可以从**少量但经过战略性策划的自主行为演示**中涌现。其核心理念是“少即是多”，强调数据质量而非数量。\n\n### 实验结果与显著优势\n\nLIMI在实验中取得了令人瞩目的成果：\n\n*   **训练数据**：仅使用了78个精心设计的训练样本。\n*   **基准表现**：在综合智能体能力基准测试中达到了**73.5%**的成绩。\n*   **超越SOTA模型**：显著优于当前最先进的模型：\n    *   Kimi-K2-Instruct (24.1%)\n    *   DeepSeek-V3.1 (11.9%)\n    *   Qwen3-235B-A22B-Instruct (27.5%)\n    *   GLM-4.5 (45.1%)\n*   **效率提升**：最引人注目的是，LIMI比使用10,000个样本训练的模型性能提升了**53.7%**，这意味着它以**128倍更少的样本量**实现了卓越的智能体智能。\n\n### “智能体效率原则”的建立\n\n这些发现确立了**“智能体效率原则”**：机器自主性并非源于数据的丰富，而是源于对高质量智能体演示的战略性策划。这一原则为未来AI智能体能力的开发提供了新的方向和范式。",
      "shortSummary": "LIMI项目挑战了AI智能体能力发展中“数据越多越好”的传统范式。研究发现，通过战略性策划高质量的自主行为演示，仅用78个样本，LIMI在智能体能力基准测试中达到73.5%，显著优于使用10,000个样本训练的模型，性能提升53.7%。这表明机器自主性源于高质量演示的战略性策划，而非数据量的丰富，确立了“智能体效率原则”。",
      "translated_title": "LIMI：少即是多，赋能智能体能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "We define Agency as the emergent capacity of AI systems to function as autonomous agents actively discovering problems, formulating hypotheses, and executing solutions through self-directed engagement with environments and tools. This fundamental capability marks the dawn of the Age of AI Agency, driven by a critical industry shift: the urgent need for AI systems that don't just think, but work. While current AI excels at reasoning and generating responses, industries demand autonomous agents that can execute tasks, operate tools, and drive real-world outcomes. As agentic intelligence becomes the defining characteristic separating cognitive systems from productive workers, efficiently cultivating machine autonomy becomes paramount. Current approaches assume that more data yields better agency, following traditional scaling laws from language modeling. We fundamentally challenge this paradigm. LIMI (Less Is More for Intelligent Agency) demonstrates that agency follows radically different development principles. Through strategic focus on collaborative software development and scientific research workflows, we show that sophisticated agentic intelligence can emerge from minimal but strategically curated demonstrations of autonomous behavior. Using only 78 carefully designed training samples, LIMI achieves 73.5% on comprehensive agency benchmarks, dramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%), DeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%). Most strikingly, LIMI demonstrates 53.7% improvement over models trained on 10,000 samples-achieving superior agentic intelligence with 128 times fewer samples. Our findings establish the Agency Efficiency Principle: machine autonomy emerges not from data abundance but from strategic curation of high-quality agentic demonstrations."
    },
    {
      "title": "GeoPQA：弥合多模态大语言模型在几何推理中的视觉感知鸿沟 (原标题: GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning)",
      "link": "https://arxiv.org/abs/2509.17437",
      "pubDate": "Mon, 22 Sep 2025 03:28:09 GMT",
      "isoDate": "2025-09-22T03:28:09.000Z",
      "creator": "Guizhen Chen, Weiwen Xu, Hao Zhang, Hou Pong Chan, Deli Zhao, Anh Tuan Luu, Yu Rong",
      "summary": "### GeoPQA：弥合多模态大语言模型在几何推理中的视觉感知鸿沟\n\n#### 引言\n\n*   **背景：** 近期强化学习（RL）显著提升了大型语言模型（LLM）的推理能力。然而，这种提升对多模态大语言模型（MLLM）的影响有限，尤其是在几何推理等视觉密集型任务中。\n*   **问题：** MLLM在这些任务中频繁出现“幻觉”，导致推理不准确。作者将此归因于MLLM的“感知瓶颈”，认为其限制了推理训练所能带来的益处。\n\n#### GeoPQA基准\n\n*   **目的：** 为量化MLLM的感知瓶颈，研究团队设计了一个名为Geo-Perception Question-Answering (GeoPQA) 的基准。\n*   **内容：** GeoPQA基准专注于评估基本的几何概念和空间关系。\n*   **发现：** 在GeoPQA上进行的实验揭示了MLLM在视觉感知方面存在显著缺陷，这些缺陷反过来限制了RL奖励信号的有效性，从而阻碍了有效的模型训练。\n\n#### 两阶段RL训练框架\n\n*   **解决方案：** 为解决上述感知瓶颈，研究团队提出了一种两阶段的RL训练框架。\n*   **阶段一：** 首先致力于增强模型对几何结构的视觉感知能力。\n*   **阶段二：** 在感知能力提升的基础上，进一步培养模型的推理能力。\n\n#### 实验结果与泛化性\n\n*   **应用：** 该两阶段训练方法被应用于Qwen2.5-VL-3B-Instruct模型。\n*   **性能提升：** 与直接进行推理训练的方法相比，两阶段训练显著提升了模型的性能：\n    *   几何推理能力提高了9.7%。\n    *   几何问题解决能力提高了9.1%。\n*   **泛化性：** 此外，该方法还被证明可以泛化到其他视觉密集型领域，例如图形理解，这突显了感知基础在实现有效MLLM推理中的关键作用。\n\n#### 其他信息\n\n*   该研究已被EMNLP2025 Findings接受。",
      "shortSummary": "多模态大语言模型（MLLM）在几何推理中存在视觉感知瓶颈，导致推理不准确。为解决此问题，研究团队设计了GeoPQA基准来量化感知缺陷，并提出了一种两阶段强化学习训练框架。该框架首先增强几何结构的视觉感知，然后培养推理能力。实验表明，该方法显著提升了Qwen2.5-VL-3B-Instruct模型的几何推理（9.7%）和问题解决能力（9.1%），并具有良好的泛化性，强调了感知基础的重要性。",
      "translated_title": "GeoPQA：弥合多模态大语言模型在几何推理中的视觉感知鸿沟",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in reinforcement learning (RL) have enhanced the reasoning abilities of large language models (LLMs), yet the impact on multimodal LLMs (MLLMs) is limited. Particularly in vision-intensive tasks like geometric reasoning, MLLMs hallucinate frequently, leading to inaccurate reasoning. We attribute this to the perceptual bottleneck in MLLMs, which caps the benefits of reasoning training. To quantify this, we design a Geo-Perception Question-Answering (GeoPQA) benchmark, targeting basic geometric concepts and spatial relationships. Experiments on GeoPQA reveal significant shortcomings of MLLMs in visual perception, which constrain RL reward signals for effective training. To address this bottleneck, we propose a two-stage RL training framework by first enhancing the visual perception of geometric structures, then fostering reasoning capabilities. Applied to Qwen2.5-VL-3B-Instruct, our two-stage training improves geometric reasoning by 9.7% and geometric problem solving by 9.1%, compared to the direct reasoning training approach. Our method also generalizes to other vision-intensive domains like figure understanding, highlighting the importance of perceptual grounding in effective MLLM reasoning."
    },
    {
      "title": "QWHA：面向大型语言模型的量化感知Walsh-Hadamard自适应参数高效微调 (原标题: QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models)",
      "link": "https://arxiv.org/abs/2509.17428",
      "pubDate": "Mon, 22 Sep 2025 03:21:41 GMT",
      "isoDate": "2025-09-22T03:21:41.000Z",
      "creator": "Hyesung Jeon, Seojune Lee, Beomseok Kang, Yulhwa Kim, Jae-Joon Kim",
      "summary": "## QWHA：量化感知Walsh-Hadamard自适应参数高效微调\n\n### 背景与挑战\n\n大型语言模型（LLMs）的部署效率是当前研究的重点。为了降低推理成本，量化技术被广泛应用；为了减少训练开销，参数高效微调（PEFT）方法应运而生。因此，开发能够生成准确且高效量化模型的量化感知PEFT方法变得至关重要。然而，现有方法存在以下局限：\n\n*   **低秩自适应（Low-rank adaptation）**：这类方法在表示能力上存在局限性，难以有效捕捉复杂的模型信息。\n*   **傅里叶相关变换（FT）基适配器**：尽管它们提供了更强的表示能力，但将其直接集成到量化模型中，往往会导致误差减少不力，并显著增加计算开销。\n\n### QWHA方法提出\n\n为克服上述限制，本文提出了**QWHA**（Quantization-Aware Walsh-Hadamard Adaptation）方法，旨在将FT基适配器有效集成到量化模型中，同时解决误差和计算效率问题。\n\n#### QWHA的核心组件\n\n1.  **Walsh-Hadamard变换（WHT）作为变换核**：QWHA采用WHT作为其FT基适配器的核心变换核。WHT相比其他傅里叶相关变换，在某些场景下具有计算效率高和数值稳定性好的优势。\n2.  **新颖的适配器初始化方案**：QWHA引入了一种创新的初始化策略，该策略结合了：\n    *   **自适应参数选择**：根据模型特性和量化需求，智能地选择适配器参数。\n    *   **值细化**：对选定的参数进行精细化调整，以进一步优化其性能和减少量化误差。\n\n### QWHA的优势与实验结果\n\nQWHA的设计使其能够有效地解决量化感知PEFT中的关键问题，并带来了显著的性能提升：\n\n*   **有效缓解量化误差**：QWHA能够显著降低量化过程中产生的误差，从而提高量化模型的准确性。\n*   **促进微调过程**：通过优化适配器结构和初始化，QWHA使得微调过程更加高效和稳定。\n*   **显著降低计算成本**：QWHA的设计，特别是WHT的应用，大幅减少了计算开销，提升了整体效率。\n\n实验结果表明：\n\n*   **卓越的低比特量化精度**：QWHA在低比特量化精度方面持续优于现有基线方法。\n*   **显著的训练加速**：相对于现有的FT基适配器，QWHA实现了显著的训练速度提升。\n\n### 代码可用性\n\n相关代码已在GitHub上开源，方便研究人员和开发者进行复现和进一步研究。",
      "shortSummary": "QWHA是一种面向大型语言模型（LLMs）的量化感知参数高效微调（PEFT）方法。它旨在解决现有低秩自适应表示能力有限和傅里叶变换（FT）基适配器计算开销大的问题。QWHA采用Walsh-Hadamard变换（WHT）作为变换核，并结合自适应参数选择和值细化的新颖适配器初始化方案。实验证明，QWHA能有效缓解量化误差，提高低比特量化精度，并显著加速训练，同时降低计算成本。",
      "translated_title": "QWHA：面向大型语言模型的量化感知Walsh-Hadamard自适应参数高效微调",
      "images": [],
      "contentSource": "完整文章",
      "content": "The demand for efficient deployment of large language models (LLMs) has driven interest in quantization, which reduces inference cost, and parameter-efficient fine-tuning (PEFT), which lowers training overhead. This motivated the development of quantization-aware PEFT to produce accurate yet efficient quantized models. In this setting, reducing quantization error prior to fine-tuning is crucial for achieving high model accuracy. However, existing methods that rely on low-rank adaptation suffer from limited representational capacity. Recent Fourier-related transform (FT)-based adapters offer greater representational power than low-rank adapters, but their direct integration into quantized models often results in ineffective error reduction and increased computational overhead. To overcome these limitations, we propose QWHA, a method that integrates FT-based adapters into quantized models by employing the Walsh-Hadamard Transform (WHT) as the transform kernel, together with a novel adapter initialization scheme incorporating adaptive parameter selection and value refinement. We demonstrate that QWHA effectively mitigates quantization errors while facilitating fine-tuning, and that its design substantially reduces computational cost. Experimental results show that QWHA consistently outperforms baselines in low-bit quantization accuracy and achieves significant training speedups over existing FT-based adapters. The code is available at https://github.com/vantaa89/qwha."
    },
    {
      "title": "EpiCache: 用于长对话问答的片段式KV缓存管理 (原标题: EpiCache: Episodic KV Cache Management for Long Conversational Question Answering)",
      "link": "https://arxiv.org/abs/2509.17396",
      "pubDate": "Mon, 22 Sep 2025 02:56:35 GMT",
      "isoDate": "2025-09-22T02:56:35.000Z",
      "creator": "Minsoo Kim, Arnav Kundu, Han-Byul Kim, Richa Dixit, Minsik Cho",
      "summary": "# EpiCache: 用于长对话问答的片段式KV缓存管理\n\n## 背景与挑战\n大型语言模型（LLMs）的最新进展显著扩展了上下文长度，使得AI助手能够维持更长的对话历史，从而提供连贯且个性化的响应。然而，这一能力高度依赖于键值（KV）缓存。KV缓存的内存占用随对话长度线性增长，在严格的资源限制下，这很快成为一个主导性的瓶颈。\n\n当前旨在减少KV缓存开销的压缩方法面临两大主要局限性：\n*   **无限制的峰值内存：** 在完成全上下文预填充后才进行逐出操作，导致峰值内存占用可能不受限制。\n*   **多轮对话准确性下降：** 依赖于查询的逐出策略将缓存范围缩小到单个查询，这在多轮对话中会导致准确性显著下降。\n\n## EpiCache 框架介绍\nEpiCache 是一种无需训练的KV缓存管理框架，专为在固定内存预算下的长对话问答（LongConvQA）设计。它通过创新的方法解决了现有方法的局限性：\n\n### 核心机制\n*   **块级预填充（Block-wise Prefill）：** EpiCache 通过块级预填充来限制缓存的增长，从而有效控制内存占用。\n*   **片段式KV压缩（Episodic KV Compression）：** 该机制将对话历史聚类成连贯的“片段”（episodes），并对每个片段应用特定的KV缓存逐出策略，以保留与主题相关的上下文。\n*   **自适应分层预算分配（Adaptive Layer-wise Budget Allocation）：** EpiCache 设计了一种自适应的分层预算分配策略。该策略会衡量模型每个层对逐出操作的敏感度，并据此在不同层之间智能地分配内存预算，以优化整体性能。\n\n## 实验结果与优势\nEpiCache 在三个 LongConvQA 基准测试中展现出显著的性能提升和资源效率：\n*   **准确性提升：** 相较于近期基线方法，EpiCache 的准确性提高了高达40%。\n*   **高压缩率下的准确性：** 在4-6倍的压缩率下，EpiCache 仍能保持接近全KV缓存的准确性。\n*   **资源效率提升：** 该框架将延迟和内存分别降低了高达2.4倍和3.5倍。\n*   **高效多轮交互：** 最终，EpiCache 在严格的资源限制下，实现了高效且流畅的多轮交互。\n\n## 研究领域\n该研究属于计算与语言（cs.CL）领域。",
      "shortSummary": "EpiCache是一种用于长对话问答的KV缓存管理框架，旨在解决LLM中KV缓存内存线性增长和现有方法局限性。它通过块级预填充、片段式KV压缩（将对话历史聚类为片段并进行逐出）以及自适应分层预算分配来优化内存使用。EpiCache在固定内存预算下，显著提升了准确性（高达40%），并在4-6倍压缩率下保持高准确性，同时将延迟和内存分别降低了高达2.4倍和3.5倍，从而实现了高效的多轮交互。",
      "translated_title": "EpiCache: 用于长对话问答的片段式KV缓存管理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in large language models (LLMs) have extended context lengths, enabling assistants to sustain long histories for coherent, personalized responses. This ability, however, hinges on Key-Value (KV) caching, whose memory grows linearly with dialogue length and quickly dominates under strict resource constraints. An active line of research for reducing this overhead is KV cache compression, which seeks to limit cache size while preserving accuracy. Yet existing methods face two major limitations: (i) evicting entries after full-context prefill causes unbounded peak memory, and (ii) query-dependent eviction narrows the cache to a single query, leading to degraded accuracy in multi-turn conversations. We introduce EpiCache, a training-free KV cache management framework for long conversational question answering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth through block-wise prefill and preserves topic-relevant context via episodic KV compression, which clusters conversation history into coherent episodes and applies episode-specific KV cache eviction. We further design an adaptive layer-wise budget allocation strategy that measures each layer's sensitivity to eviction and distributes the memory budget across layers accordingly. Across three LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent baselines, sustains near-full KV accuracy under 4-6x compression, and reduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient multi-turn interaction under strict resource constraints."
    },
    {
      "title": "Mano 报告 (原标题: Mano Report)",
      "link": "https://arxiv.org/abs/2509.17336",
      "pubDate": "Sun, 21 Sep 2025 23:13:58 GMT",
      "isoDate": "2025-09-21T23:13:58.000Z",
      "creator": "Tianyu Fu, Anyang Su, Chenxu Zhao, Hanning Wang, Minghui Wu, Zhe Yu, Fei Hu, Mingjia Shi, Wei Dong, Jiayao Wang, Yuyang Chen, Ruiyang Yu, Siran Peng, Menglin Li, Nan Huang, Haitian Wei, Jiawei Yu, Yi Xin, Xilin Zhao, Kai Gu, Ping Jiang, Sifan Zhou, Shuo Wang",
      "summary": "## Mano：一个用于自动化图形用户界面（GUI）交互的鲁棒智能体\n\n### 背景与挑战\n图形用户界面（GUI）是人机交互的核心媒介，但自动化GUI交互仍然面临巨大挑战。这些挑战主要源于：\n*   **视觉元素的复杂性**：GUI界面包含多种动态变化的视觉元素。\n*   **动态环境**：GUI环境通常是动态且不可预测的。\n*   **多步骤推理需求**：完成复杂任务需要智能体进行多步骤的逻辑推理。\n\n现有基于视觉-语言模型（VLM）的方法在处理这些问题时，常受限于：\n*   **有限的分辨率**：难以处理高分辨率或细节丰富的界面。\n*   **领域不匹配**：预训练数据与GUI交互领域存在差异。\n*   **序列决策能力不足**：难以有效执行和规划多步骤的交互序列。\n\n### Mano 解决方案\n为解决上述问题，我们提出了 **Mano**，一个基于多模态基础模型构建的鲁棒GUI智能体。Mano 的核心优势在于其综合性的方法和强大的基础模型。\n\n### 核心方法与组件\nMano 的设计整合了多项创新技术：\n\n1.  **多模态基础模型**：Mano 建立在一个在大量网络和计算机系统数据上预训练的多模态基础模型之上，这为其提供了广泛的知识和理解能力。\n2.  **高保真数据生成**：集成了一个新颖的模拟环境，用于生成高质量、高保真度的GUI交互数据，有效弥补了真实数据获取的困难。\n3.  **三阶段训练流程**：Mano 采用了一个迭代且全面的训练范式，包括：\n    *   **监督微调（Supervised Fine-tuning）**：利用标注数据进行初步训练，使模型掌握基本交互技能。\n    *   **离线强化学习（Offline Reinforcement Learning）**：从预收集的数据中学习最优策略，提高决策效率和准确性。\n    *   **在线强化学习（Online Reinforcement Learning）**：通过与环境的实时交互进行持续学习和优化，以适应动态变化。\n4.  **验证模块**：包含一个专门的验证模块，用于检测和恢复错误，显著提升了智能体的鲁棒性和可靠性。\n\n### 性能表现\nMano 在多个主流GUI基准测试中展现了最先进（state-of-the-art）的性能，包括：\n*   **Mind2Web**\n*   **OSWorld**\n\n在这些基准测试中，Mano 在成功率和操作准确性方面均取得了显著提升。\n\n### 研究贡献与启示\n我们的工作为将强化学习与视觉-语言模型有效整合以部署实用GUI智能体提供了新见解。研究结果强调了以下几点的重要性：\n*   **领域特定数据**：针对特定领域的数据对于模型性能至关重要。\n*   **迭代训练**：多阶段、迭代的训练方法能够有效提升智能体的能力。\n*   **整体奖励设计**：设计全面的奖励机制对于引导智能体学习复杂行为至关重要。",
      "shortSummary": "Mano 是一种新型鲁棒GUI智能体，旨在解决自动化GUI交互的复杂挑战。它基于在海量网络和计算机系统数据上预训练的多模态基础模型，并采用三阶段训练流程（监督微调、离线和在线强化学习）及错误恢复模块。Mano 在Mind2Web和OSWorld等GUI基准测试中取得了最先进的性能，显著提升了成功率和操作准确性，为VLM与强化学习的整合提供了新思路，强调了领域数据、迭代训练和整体奖励设计的重要性。",
      "translated_title": "Mano 报告",
      "images": [],
      "contentSource": "完整文章",
      "content": "Graphical user interfaces (GUIs) are the primary medium for human-computer interaction, yet automating GUI interactions remains challenging due to the complexity of visual elements, dynamic environments, and the need for multi-step reasoning. Existing methods based on vision-language models (VLMs) often suffer from limited resolution, domain mismatch, and insufficient sequential decisionmaking capability. To address these issues, we propose Mano, a robust GUI agent built upon a multi-modal foundation model pre-trained on extensive web and computer system data. Our approach integrates a novel simulated environment for high-fidelity data generation, a three-stage training pipeline (supervised fine-tuning, offline reinforcement learning, and online reinforcement learning), and a verification module for error recovery. Mano demonstrates state-of-the-art performance on multiple GUI benchmarks, including Mind2Web and OSWorld, achieving significant improvements in success rate and operational accuracy. Our work provides new insights into the effective integration of reinforcement learning with VLMs for practical GUI agent deployment, highlighting the importance of domain-specific data, iterative training, and holistic reward design."
    },
    {
      "title": "VaseVQA：古希腊陶器多模态智能体与基准 (原标题: VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery)",
      "link": "https://arxiv.org/abs/2509.17191",
      "pubDate": "Sun, 21 Sep 2025 14:36:54 GMT",
      "isoDate": "2025-09-21T14:36:54.000Z",
      "creator": "Jinchao Ge, Tengfei Cheng, Biao Wu, Zeyu Zhang, Shiya Huang, Judith Bishop, Gillian Shepherd, Meng Fang, Ling Chen, Yang Zhao",
      "summary": "# VaseVQA：古希腊陶器多模态智能体与基准\n\n## 概述\n\n该研究旨在解决多模态大语言模型（MLLMs）在分析文化遗产文物（特别是古希腊陶器）时面临的挑战。现有模型缺乏领域专业知识，而监督微调（SFT）容易过拟合表面模式，导致在文物鉴定和历史归属方面的推理能力脆弱。\n\n## 核心问题\n\n如何使MLLMs具备对古希腊陶器进行稳健、专家级推理的能力？\n\n## 解决方案：VaseVL 系统\n\n研究提出了一种名为 **VaseVL** 的系统，它是一个结合了SFT和强化学习（RL）的方法，将评估过程转化为监督信号：\n\n*   **问题类型分类**：构建了一个问题类型分类体系。\n*   **性能差距诊断**：探测SFT模型以定位特定类型的问题性能差距。\n*   **奖励优化**：利用针对这些差距的、以类型为条件、以组合性为导向的奖励进行优化。\n\n## 基准：VaseVQA\n\n研究同时发布了一个名为 **VaseVQA** 的综合基准，旨在深入探究模型的理解能力：\n\n*   **数据集规模**：包含31,773张图像。\n*   **设计目的**：用于测试模型对古希腊陶器的深层理解。\n\n## 实验结果与贡献\n\n实验结果表明，VaseVL系统取得了显著的成果：\n\n*   **最先进性能**：在风格分类和历史归属任务上达到了最先进（state-of-the-art）的结果。\n*   **鲁棒性提升**：与仅使用SFT的基线模型相比，在组合鲁棒性方面有显著提升。\n*   **方法验证**：验证了诊断引导、分类条件奖励工程的有效性。\n*   **可复用资源**：为未来的研究提供了一个可复用的资源。\n\n## 可用性\n\n代码和数据集将在此处提供的链接中发布。",
      "shortSummary": "本研究提出了VaseVL系统和VaseVQA基准，旨在提升多模态大语言模型（MLLMs）对古希腊陶器的专家级分析能力。VaseVL是一个SFT-then-RL系统，通过诊断性能差距并利用类型条件奖励进行优化。VaseVQA是一个包含31,773张图像的综合基准。实验结果显示，该系统在风格分类和历史归属上达到了最先进水平，并显著提高了组合鲁棒性，为文化遗产分析提供了重要资源。",
      "translated_title": "VaseVQA：古希腊陶器多模态智能体与基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Analyzing cultural-heritage artifacts remains challenging for MLLMs: general models lack domain expertise, and SFT often overfits superficial patterns, yielding brittle reasoning for authentication and historical attribution. This raises the question of how to equip MLLMs with robust, expert-level reasoning for ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns evaluation into supervision: we construct a taxonomy of question types, probe the SFT model to localize type-specific performance gaps, and optimize with type-conditioned, compositionality-oriented rewards targeting those gaps. We also release VaseVQA, a comprehensive benchmark of 31,773 images designed to probe deep understanding. Experiments show state-of-the-art results on style classification and historical attribution with marked gains in compositional robustness over SFT-only baselines, validating diagnosis-guided, taxonomy-conditioned reward engineering and providing a reusable resource for future research. Code and dataset will be available at https://github.com/AIGeeksGroup/VaseVQA."
    },
    {
      "title": "ARE：扩展智能体环境与评估 (原标题: ARE: Scaling Up Agent Environments and Evaluations)",
      "link": "https://arxiv.org/abs/2509.17158",
      "pubDate": "Sun, 21 Sep 2025 12:59:45 GMT",
      "isoDate": "2025-09-21T12:59:45.000Z",
      "creator": "Pierre Andrews, Amine Benhalloum, Gerard Moreno-Torres Bertran, Matteo Bettini, Amar Budhiraja, Ricardo Silveira Cabral, Virginie Do, Romain Froger, Emilien Garreau, Jean-Baptiste Gaya, Hugo Laurençon, Maxime Lecanu, Kunal Malkan, Dheeraj Mekala, Pierre Ménard, Grégoire Mialon, Ulyana Piterbarg, Mikhail Plekhanov, Mathieu Rita, Andrey Rusakov, Thomas Scialom, Vladislav Vorotilov, Mengjue Wang, Ian Yu",
      "summary": "# ARE：扩展智能体环境与评估\n\n本文介绍了Meta智能体研究环境（Meta Agents Research Environments, ARE），这是一个旨在实现环境可扩展创建、合成或真实应用集成以及智能体编排执行的研究平台。ARE通过提供简单的抽象层来构建复杂多样的环境，每个环境都拥有自己的规则、工具、内容和验证器，从而帮助弥合模型开发与实际部署之间的鸿沟。\n\n## Gaia2基准测试\n\nARE平台中构建了Gaia2，这是一个专门用于衡量智能体通用能力的基准测试。Gaia2超越了传统的搜索和执行任务，要求智能体具备以下能力：\n\n*   处理模糊性和噪声。\n*   适应动态环境。\n*   与其他智能体协作。\n*   在时间约束下运行。\n\n与以往的基准测试不同，Gaia2采用异步运行模式，这能够揭示在静态设置中不可见的全新故障模式。\n\n## 实验发现与启示\n\n实验结果揭示了智能体能力发展中的关键洞察：\n\n*   **无单一系统主导**：在整个智能谱系中，没有一个系统能够完全占据主导地位。\n*   **推理与效率的权衡**：更强的推理能力往往伴随着效率的降低。\n*   **预算扩展曲线瓶颈**：预算扩展曲线出现平台期，这突出表明需要新的架构和自适应计算策略。\n\n## 对社区的赋能与未来展望\n\nARE的抽象层使得Gaia2能够持续扩展到其他环境，从而赋能社区快速创建针对其特定领域的新基准测试。文章强调，在人工智能发展的“下半场”，进步越来越依赖于定义有意义的任务和进行稳健的评估，以推动前沿能力的发展。",
      "shortSummary": "Meta Agents Research Environments (ARE) 是一个可扩展的智能体研究平台，旨在简化复杂环境创建、应用集成及智能体编排，以弥合模型开发与实际部署的差距。平台引入了Gaia2基准测试，该测试要求智能体处理模糊性、适应动态环境并进行协作，其异步运行模式揭示了新的故障模式。实验表明，没有单一系统能全面主导，且推理能力与效率存在权衡，凸显了新架构和自适应计算策略的必要性。ARE赋能社区创建定制化基准，推动AI前沿发展。",
      "translated_title": "ARE：扩展智能体环境与评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Meta Agents Research Environments (ARE), a research platform for scalable creation of environments, integration of synthetic or real applications, and execution of agentic orchestrations. ARE provides simple abstractions to build complex and diverse environments, each with their own rules, tools, content, and verifiers, helping to bridge the gap between model development and real-world deployment. We also propose Gaia2, a benchmark built in ARE and designed to measure general agent capabilities. Beyond search and execution, Gaia2 requires agents to handle ambiguities and noise, adapt to dynamic environments, collaborate with other agents, and operate under temporal constraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new failure modes that are invisible in static settings. Our experiments show that no system dominates across the intelligence spectrum: stronger reasoning often comes at the cost of efficiency, and budget scaling curves plateau, highlighting the need for new architectures and adaptive compute strategies. Perhaps more importantly, ARE abstractions enable continuous extension of Gaia2 to other environments, empowering the community to rapidly create new benchmarks tailored to their domains. In AI's second half, progress increasingly depends on defining meaningful tasks and robust evaluations to drive frontier capabilities forward."
    },
    {
      "title": "SWE-Bench Pro：AI智能体能否解决长周期软件工程任务？ (原标题: SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?)",
      "link": "https://arxiv.org/abs/2509.16941",
      "pubDate": "Sun, 21 Sep 2025 02:28:17 GMT",
      "isoDate": "2025-09-21T02:28:17.000Z",
      "creator": "Xiang Deng, Jeff Da, Edwin Pan, Yannis Yiming He, Charles Ide, Kanak Garg, Niklas Lauffer, Andrew Park, Nitin Pasari, Chetan Rane, Karmini Sampath, Maya Krishnan, Srivatsa Kundurthy, Sean Hendryx, Zifan Wang, Chen Bo Calvin Zhang, Noah Jacobson, Bing Liu, Brad Kenstler",
      "summary": "## SWE-Bench Pro：AI智能体在长周期软件工程任务中的表现\n\n### 1. 简介与背景\n\nSWE-Bench Pro是一个新推出的、更具挑战性的基准测试，旨在超越现有SWE-BENCH的范围，捕捉真实世界中复杂、企业级的软件工程问题。它基于SWE-BENCH的最佳实践构建，但专注于需要更长时间和更深层次理解的任务。\n\n### 2. 基准内容与来源\n\n*   **问题数量与来源**：SWE-Bench Pro包含1,865个问题，这些问题来源于41个活跃维护的软件仓库。这些仓库涵盖了广泛的领域，包括商业应用、B2B服务和开发者工具。\n*   **任务特点**：\n    *   **长周期任务**：这些任务的完成可能需要专业软件工程师数小时甚至数天的时间。\n    *   **复杂性**：通常涉及跨多个文件的补丁和大量的代码修改。\n    *   **质量保证**：所有任务都经过人工验证，并提供了足够的上下文信息以确保其可解决性。\n\n### 3. 基准划分\n\n为了平衡开放性与数据保护，SWE-Bench Pro被划分为三个部分：\n\n*   **公共集（Public Set）**：包含来自11个仓库的问题，可公开访问。\n*   **保留集（Held-out Set）**：包含来自12个仓库的问题，不公开访问。\n*   **商业集（Commercial Set）**：包含来自18个专有仓库的问题。这些问题来自与早期创业公司的正式合作协议，虽然问题本身不公开，但其评估结果会对外发布。\n\n### 4. 现有模型评估结果\n\n在统一的评估框架下，对当前广泛使用的编码模型进行了测试。结果显示：\n\n*   **整体表现**：AI智能体在SWE-Bench Pro上的表现仍然不佳，通过率（Pass@1）低于25%。\n*   **最佳表现**：截至目前，GPT-5取得了最高分，通过率为23.3%。\n\n### 5. 局限性分析与未来方向\n\n为了深入理解当前模型的局限性，研究人员对收集到的智能体轨迹中的失败模式进行了聚类分析，以更清晰地描绘现有模型的错误模式。\n\n### 6. 意义与展望\n\nSWE-Bench Pro提供了一个抗污染的测试平台，它更真实地反映了现实世界软件开发的复杂性和多样性。该基准的引入，旨在推动实现专业水平的真正自主软件工程智能体的研究与发展。",
      "shortSummary": "SWE-Bench Pro是一个新的、更具挑战性的基准测试，旨在评估AI智能体解决复杂、长周期企业级软件工程任务的能力。它包含1,865个问题，来源于41个活跃维护的仓库，任务通常需要专业工程师数小时到数天完成。评估结果显示，当前最先进的编码模型（如GPT-5）表现仍低于25%的通过率。该基准旨在揭示现有AI模型的局限性，并推动开发更自主、专业级的软件工程智能体。",
      "translated_title": "SWE-Bench Pro：AI智能体能否解决长周期软件工程任务？",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce SWE-Bench Pro, a substantially more challenging benchmark that builds upon the best practices of SWE-BENCH [25], but is explicitly designed to capture realistic, complex, enterprise-level problems beyond the scope of SWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of 41 actively maintained repositories spanning business applications, B2B services, and developer tools. The benchmark is partitioned into a public set with open access to problems sourced from 11 repositories, a held-out set of 12 repositories and a commercial set of 18 proprietary repositories where we have formal partnership agreements with early-stage startups. Problems in the held-out and the commercial set are not publicly accessible, but we release results on the commercial set. Our benchmark features long-horizon tasks that may require hours to days for a professional software engineer to complete, often involving patches across multiple files and substantial code modifications. All tasks are human-verified and augmented with sufficient context to ensure resolvability. In our evaluation of widely used coding models, under a unified scaffold, we observe that their performance on SWE-Bench PRO remains below 25% (Pass@1), with GPT-5 achieving the highest score to date at 23.3%. To better understand these limitations, we cluster the failure modes observed in the collected agent trajectories for a clearer characterization of the error patterns exhibited by current models. Overall, SWE-BENCH PRO provides a contamination-resistant testbed that more faithfully captures the complexity and diversity of real-world software development, advancing the pursuit of truly autonomous software engineering agents at a professional level."
    },
    {
      "title": "当大模型训练小模型时：基于小型视觉语言模型的无标签模型奇偶对齐，实现高效视觉问答 (原标题: When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs)",
      "link": "https://arxiv.org/abs/2509.16633",
      "pubDate": "Sat, 20 Sep 2025 07:12:23 GMT",
      "isoDate": "2025-09-20T07:12:23.000Z",
      "creator": "Abhirama Subramanyam Penamakuri, Navlika Singh, Piyush Arora, Anand Mishra",
      "summary": "## 大模型训练小模型：无标签模型奇偶对齐，实现高效视觉问答\n\n### 引言\n\n大型视觉语言模型（L-VLMs）在视觉问答（VQA）等多种视觉和语言任务中表现出色。然而，其高昂的计算成本使其在资源受限环境和推理密集型应用中不切实际。相比之下，小型视觉语言模型（S-VLMs）虽然效率更高，但与大型模型相比存在显著的性能差距。\n\n### 提出的方法：模型奇偶对齐器（MPA）\n\n本文引入了**模型奇偶对齐器（Model Parity Aligner, MPA）**，这是一个新颖的框架，旨在通过以下方式系统地改进S-VLMs：\n\n*   **利用无标签图像**：MPA利用未标记的图像进行训练。\n*   **有效的知识转移**：从L-VLMs中有效地转移知识到S-VLMs。\n*   **区别于传统知识蒸馏**：与依赖标记训练数据的传统知识蒸馏方法不同，MPA采用了一种战略性的基于奇偶对齐的方法。\n*   **精准识别和优化**：MPA精确识别S-VLMs和L-VLMs之间的知识差异，并通过仅针对这些差异进行优化训练。\n\n### 实验与结果\n\n研究人员在四个多样化的VQA基准测试上进行了广泛的实验，这些基准测试包括：\n\n*   **TextVQA**：需要文本识别能力。\n*   **ST-VQA**：需要文本识别能力。\n*   **ChartQA**：需要图表解释能力。\n*   **OKVQA**：需要常识和事实理解能力。\n\n实验结果表明：\n\n*   **性能提升**：MPA在所有基准测试中持续提升S-VLMs的性能。\n*   **缩小差距**：显著缩小了S-VLMs与L-VLMs之间的性能差距。\n*   **保持效率**：在提升性能的同时，保持了计算效率。\n\n### 代码可用性\n\n本研究的代码已公开发布。",
      "shortSummary": "大型视觉语言模型（L-VLMs）在视觉问答（VQA）中性能卓越但计算成本高昂，而小型视觉语言模型（S-VLMs）效率高但性能不足。本文提出Model Parity Aligner (MPA) 框架，通过利用无标签图像和L-VLM的知识转移，采用奇偶对齐方法识别并解决S-VLM与L-VLM之间的知识差异。MPA在TextVQA、ST-VQA、ChartQA和OKVQA等四个VQA基准测试中显著提升了S-VLM的性能，缩小了性能差距，同时保持了计算效率。",
      "translated_title": "当大模型训练小模型时：基于小型视觉语言模型的无标签模型奇偶对齐，实现高效视觉问答",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Vision-Language Models (L-VLMs) have demonstrated remarkable performance in various vision and language tasks, including visual question answering (VQA). However, their high computational cost makes them impractical for resource-constrained settings and inference-heavy applications. In contrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer from a significant performance gap compared to their larger counterparts. In this work, we introduce the Model Parity Aligner (MPA), a novel framework designed to systematically improve S-VLMs by leveraging unlabeled images and effective knowledge transfer from L-VLMs. Instead of traditional knowledge distillation methods that rely on labeled training data, MPA employs a strategic parity-based approach that precisely identifies the knowledge disparities between S-VLMs and L-VLMs, and optimizes training by targeting only these disparities. We conduct extensive experiments on four diverse VQA benchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires specialized reasoning capabilities such as text recognition, chart interpretation, and commonsense and factual understanding. Our results demonstrate that MPA consistently enhances the performance of S-VLMs on all benchmarks, reducing the performance gap while maintaining computational efficiency. We make our code publicly available."
    }
  ],
  "lastUpdated": "2025-09-23T09:34:47.660Z"
}