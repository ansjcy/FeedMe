{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "VMem：基于Surfel索引视图记忆的一致交互式视频场景生成 (原标题: VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory)",
      "link": "https://arxiv.org/abs/2506.18903",
      "pubDate": "Mon, 23 Jun 2025 13:59:56 GMT",
      "isoDate": "2025-06-23T13:59:56.000Z",
      "creator": "Runjia Li, Philip Torr, Andrea Vedaldi, Tomas Jakab",
      "summary": "## VMem：基于Surfel索引视图记忆的一致交互式视频场景生成\n\n### 摘要\n\n本文提出了一种名为“Surfel索引视图记忆”（VMem）的新型记忆机制，旨在构建能够交互式探索环境的视频生成器。现有方法在处理长期场景连贯性和错误累积方面存在显著局限性，VMem旨在解决这些问题。\n\n### 现有方法的局限性\n\n*   **2D视图外绘与3D几何重建：** 这种方法虽然能实现交互式探索，但会迅速累积误差，导致场景不一致。\n*   **短上下文窗口视频生成器：** 这类生成器难以在长期内保持场景的连贯性，随着时间的推移，场景细节容易出现偏差。\n\n### VMem的创新机制\n\n*   **核心思想：** VMem通过几何索引过去视图来“记忆”它们，这种索引基于这些视图所观察到的3D表面元素（surfels）。\n*   **高效检索：** 这种索引机制使得在生成新视图时，能够高效地检索出最相关的历史视图。\n*   **聚焦相关视图：** 通过仅关注这些相关的历史视图，VMem能够以远低于将所有历史视图作为上下文的计算成本，生成一致的想象环境探索。\n\n### 性能评估\n\n*   **基准测试：** 该方法在具有挑战性的长期场景合成基准测试中进行了评估。\n*   **卓越表现：** 与现有方法相比，VMem在保持场景连贯性和相机控制方面展现出卓越的性能。",
      "shortSummary": "VMem提出了一种基于Surfel索引视图记忆的新型机制，旨在解决交互式视频场景生成中长期场景连贯性差和错误累积的问题。通过几何索引过去视图并高效检索相关信息，VMem能以更低的计算成本生成一致的虚拟环境探索。该方法在长期场景合成基准测试中表现出优于现有方法的场景连贯性和相机控制能力。",
      "translated_title": "VMem：基于Surfel索引视图记忆的一致交互式视频场景生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "We propose a novel memory mechanism to build video generators that can explore environments interactively. Similar results have previously been achieved by out-painting 2D views of the scene while incrementally reconstructing its 3D geometry, which quickly accumulates errors, or by video generators with a short context window, which struggle to maintain scene coherence over the long term. To address these limitations, we introduce Surfel-Indexed View Memory (VMem), a mechanism that remembers past views by indexing them geometrically based on the 3D surface elements (surfels) they have observed. VMem enables the efficient retrieval of the most relevant past views when generating new ones. By focusing only on these relevant views, our method produces consistent explorations of imagined environments at a fraction of the computational cost of using all past views as context. We evaluate our approach on challenging long-term scene synthesis benchmarks and demonstrate superior performance compared to existing methods in maintaining scene coherence and camera control."
    },
    {
      "title": "视觉作为一种方言：通过文本对齐表示统一视觉理解与生成 (原标题: Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations)",
      "link": "https://arxiv.org/abs/2506.18898",
      "pubDate": "Mon, 23 Jun 2025 13:59:14 GMT",
      "isoDate": "2025-06-23T13:59:14.000Z",
      "creator": "Jiaming Han, Hao Chen, Yang Zhao, Hanyu Wang, Qi Zhao, Ziyan Yang, Hao He, Xiangyu Yue, Lu Jiang",
      "summary": "## 视觉作为一种方言：通过文本对齐表示统一视觉理解与生成\n\n本文提出了一种多模态框架，旨在通过共享的离散语义表示来统一视觉理解和生成。该框架的核心组件是**文本对齐分词器（Text-Aligned Tokenizer, TA-Tok）**。\n\n### 核心机制\n\n*   **TA-Tok 的功能**：将图像转换为离散的视觉标记（tokens）。\n*   **文本对齐码本**：TA-Tok 使用一个从大型语言模型（LLM）词汇表中投射出来的文本对齐码本，确保视觉标记与文本语义对齐。\n*   **统一表示空间**：通过这种方式，视觉和文本被整合到一个具有扩展词汇量的统一空间中。\n\n### 多模态 LLM (Tar)\n\n*   **跨模态输入输出**：该框架中的多模态 LLM 被命名为 Tar，它通过一个共享接口实现跨模态的输入和输出，无需针对特定模态进行单独设计。\n\n### 创新技术\n\n*   **尺度自适应编码与解码**：为平衡效率和视觉细节，论文提出了尺度自适应的编码和解码机制。\n*   **生成式去分词器**：为了生成高保真度的视觉输出，Tar 采用了生成式去分词器。\n*   **互补的去分词器**：为满足不同的解码需求，系统利用了两种互补的去分词器：一个快速自回归模型和一个基于扩散的模型。\n\n### 预训练与性能\n\n*   **模态融合增强**：研究人员探索了先进的预训练任务，以增强模态间的融合。\n*   **实验结果**：在多个基准测试中，Tar 的表现与现有多模态 LLM 方法持平或超越，并展现出更快的收敛速度和更高的训练效率。",
      "shortSummary": "本文提出了一个名为 Tar 的多模态框架，旨在通过共享的离散语义表示统一视觉理解和生成。其核心是文本对齐分词器（TA-Tok），它将图像转换为与大型语言模型词汇表对齐的离散标记，从而将视觉和文本整合到统一空间。Tar 作为多模态LLM，支持跨模态输入输出，并引入了尺度自适应编码/解码和生成式去分词器。实验表明，Tar 在性能和训练效率上均优于现有方法。",
      "translated_title": "视觉作为一种方言：通过文本对齐表示统一视觉理解与生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper presents a multimodal framework that attempts to unify visual understanding and generation within a shared discrete semantic representation. At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using a text-aligned codebook projected from a large language model's (LLM) vocabulary. By integrating vision and text into a unified space with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input and output through a shared interface, without the need for modality-specific designs. Additionally, we propose scale-adaptive encoding and decoding to balance efficiency and visual detail, along with a generative de-tokenizer to produce high-fidelity visual outputs. To address diverse decoding needs, we utilize two complementary de-tokenizers: a fast autoregressive model and a diffusion-based model. To enhance modality fusion, we investigate advanced pre-training tasks, demonstrating improvements in both visual understanding and generation. Experiments across benchmarks show that Tar matches or surpasses existing multimodal LLM methods, achieving faster convergence and greater training efficiency. Code, models, and data are available at https://tar.csuhan.com"
    },
    {
      "title": "ReasonFlux-PRM：用于LLM中长链式思维推理的轨迹感知PRM (原标题: ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs)",
      "link": "https://arxiv.org/abs/2506.18896",
      "pubDate": "Mon, 23 Jun 2025 13:59:02 GMT",
      "isoDate": "2025-06-23T13:59:02.000Z",
      "creator": "Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, Mengdi Wang",
      "summary": "### ReasonFlux-PRM：LLM中轨迹感知过程奖励模型\n\n**1. 引言与背景**\n*   **现有问题：** 过程奖励模型（PRM）是监督大型语言模型（LLM）中间推理步骤的强大框架。然而，先前的PRM主要基于模型的最终输出响应进行训练，难以鲁棒地评估中间思维轨迹，尤其是在Deepseek-R1等前沿推理模型生成的轨迹-响应输出的新兴场景中。\n\n**2. ReasonFlux-PRM的提出**\n*   **核心创新：** 本文引入了ReasonFlux-PRM，这是一种新颖的轨迹感知PRM，专门设计用于评估轨迹-响应类型的推理痕迹。\n*   **关键特性：** ReasonFlux-PRM整合了**步骤级**和**轨迹级**的监督，从而能够进行与结构化思维链数据对齐的细粒度奖励分配。\n\n**3. 应用场景**\nReasonFlux-PRM被设计用于支持离线和在线设置下的奖励监督，具体包括：\n*   **(i) 数据蒸馏：** 为下游较小模型的监督微调（SFT）选择高质量的模型蒸馏数据。\n*   **(ii) 强化学习：** 在强化学习（RL）过程中提供密集的进程级奖励，以优化策略。\n*   **(iii) 测试时扩展：** 实现奖励引导的Best-of-N测试时扩展。\n\n**4. 实验结果与性能**\n*   **基准测试：** 在AIME、MATH500和GPQA-Diamond等具有挑战性的下游基准测试中进行了实证评估。\n*   **数据质量：** 结果表明，ReasonFlux-PRM-7B选择的数据质量高于强大的PRM（例如Qwen2.5-Math-PRM-72B）和人工策划的基线。\n*   **性能提升：** 派生的ReasonFlux-PRM-7B带来了持续的性能改进，平均增益如下：\n    *   监督微调（SFT）：12.1%\n    *   强化学习（RL）：4.5%\n    *   测试时扩展：6.3%\n\n**5. 模型发布**\n*   为了支持资源受限的应用和边缘部署，研究团队还发布了高效的ReasonFlux-PRM-1.5B模型。",
      "shortSummary": "ReasonFlux-PRM是一种新型轨迹感知过程奖励模型（PRM），旨在解决现有PRM在评估LLM中间思维轨迹方面的不足。它通过整合步骤级和轨迹级监督，为轨迹-响应输出提供细粒度奖励。ReasonFlux-PRM支持数据蒸馏、强化学习和测试时扩展。在AIME、MATH500和GPQA-Diamond等基准测试中，ReasonFlux-PRM-7B在数据选择上优于现有PRM和人工基线，并在SFT、RL和测试时扩展中分别实现了12.1%、4.5%和6.3%的性能提升。团队还发布了轻量级ReasonFlux-PRM-1.5B模型。",
      "translated_title": "ReasonFlux-PRM：用于LLM中长链式思维推理的轨迹感知PRM",
      "images": [],
      "contentSource": "完整文章",
      "content": "Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory-response outputs generated by frontier reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling. We also release our efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Projects: https://github.com/Gen-Verse/ReasonFlux"
    },
    {
      "title": "法线之光：通用光度立体视觉的统一特征表示 (原标题: Light of Normals: Unified Feature Representation for Universal Photometric Stereo)",
      "link": "https://arxiv.org/abs/2506.18882",
      "pubDate": "Mon, 23 Jun 2025 13:53:11 GMT",
      "isoDate": "2025-06-23T13:53:11.000Z",
      "creator": "Hong Li, Houyuan Chen, Chongjie Ye, Zhaoxi Chen, Bohan Li, Shaocong Xu, Xianda Guo, Xuhui Liu, Yikai Wang, Baochang Zhang, Satoshi Ikehata, Boxin Shi, Anyi Rao, Hao Zhao",
      "summary": "### 通用光度立体视觉的挑战与进展\n\n本文探讨了通用光度立体视觉（Universal Photometric Stereo, PS）领域面临的核心挑战，该技术旨在在任意光照条件下，无需依赖特定的光照模型，即可恢复物体表面的高质量法线。\n\n尽管近期在通用光度立体视觉方面取得了一些进展，例如SDM-UniPS和Uni MS-PS等方法，但该领域仍存在两个根本性难题：\n\n1.  **光照与表面法线特征的深度耦合**：\n    *   在观察到的强度中存在模糊性，使得难以确定亮度变化是源于光照变化还是表面方向。这种深度耦合使得区分这两种因素变得复杂。\n\n2.  **复杂表面高频几何细节的保留**：\n    *   传统特征处理操作难以准确捕捉复杂几何形状中固有的高频细节。这些细节包括：\n        *   自阴影（self-shadowing）\n        *   相互反射（inter-reflections）\n        *   以及细微的法线变化\n    *   这些复杂性使得准确恢复表面法线变得更具挑战性。",
      "shortSummary": "本文讨论了通用光度立体视觉（PS）面临的挑战。该技术旨在从任意光照下的物体中恢复高质量表面法线，无需特定光照模型。尽管现有方法有所进展，但仍存在两大难题：一是光照与表面法线特征的深度耦合，导致亮度变化来源难以区分；二是难以保留复杂表面（如自阴影、相互反射）的高频几何细节，传统方法难以准确捕捉。",
      "translated_title": "法线之光：通用光度立体视觉的统一特征表示",
      "images": [],
      "contentSource": "完整文章",
      "content": "Universal photometric stereo (PS) aims to recover high-quality surface normals from objects under arbitrary lighting conditions without relying on specific illumination models. Despite recent advances such as SDM-UniPS and Uni MS-PS, two fundamental challenges persist: 1) the deep coupling between varying illumination and surface normal features, where ambiguity in observed intensity makes it difficult to determine whether brightness variations stem from lighting changes or surface orientation; and 2) the preservation of high-frequency geometric details in complex surfaces, where intricate geometries create self-shadowing, inter-reflections, and subtle normal variations that conventional feature processing operations struggle to capture accurately."
    },
    {
      "title": "CommVQ：KV缓存压缩的交换向量量化 (原标题: CommVQ: Commutative Vector Quantization for KV Cache Compression)",
      "link": "https://arxiv.org/abs/2506.18879",
      "pubDate": "Mon, 23 Jun 2025 13:50:11 GMT",
      "isoDate": "2025-06-23T13:50:11.000Z",
      "creator": "Junyan Li, Yang Zhang, Muhammad Yusuf Hassan, Talha Chafekar, Tianle Cai, Zhile Ren, Pengsheng Guo, Foroozan Karimzadeh, Colorado Reed, Chong Wang, Chuang Gan",
      "summary": "### CommVQ：KV缓存压缩的交换向量量化\n\n**引言**\n\n大型语言模型（LLMs）在需要长上下文的应用中越来越普及，但随着上下文长度的增长，键值（KV）缓存常常成为GPU上的内存瓶颈。为了解决这一问题，研究人员提出了Commutative Vector Quantization（CommVQ）方法，旨在显著减少长上下文LLM推理的内存使用。\n\n**CommVQ方法**\n\nCommVQ的核心在于其创新的量化和解码机制：\n\n1.  **加性量化（Additive Quantization）**\n    *   CommVQ首先引入了加性量化，利用轻量级编码器和码本对KV缓存进行压缩。\n    *   这种压缩后的数据可以通过简单的矩阵乘法进行解码，确保了高效的解码过程。\n\n2.  **RoPE交换码本设计（RoPE-Commutative Codebook Design）**\n    *   为了进一步降低解码过程中的计算成本，CommVQ设计了一种与旋转位置嵌入（Rotary Position Embedding, RoPE）具有交换性的码本。\n    *   该码本通过期望最大化（EM）算法进行训练，使其能够与RoPE操作兼容。\n    *   这种设计使得解码过程能够高效地集成到自注意力机制中，从而优化了整体计算效率。\n\n**优势与实验结果**\n\nCommVQ方法展现出显著的优势和卓越的性能：\n\n*   **高精度与低开销**：通过加性量化实现高精度，并通过RoPE交换码本确保低计算开销。\n*   **内存显著减少**：在长上下文基准测试和GSM8K上的实验表明，CommVQ使用2位量化可以将FP16 KV缓存大小减少87.5%。\n*   **超越现有技术**：该方法在KV缓存量化方面优于当前最先进的技术。\n*   **支持极低位量化**：CommVQ能够实现1位KV缓存量化，且精度损失极小。\n*   **实际应用能力**：例如，使用CommVQ，一个LLaMA-3.1 8B模型可以在单张RTX 4090 GPU上运行128K的上下文长度，这在之前是难以实现的。",
      "shortSummary": "CommVQ是一种针对大型语言模型（LLMs）KV缓存内存瓶颈的解决方案。它采用加性量化和与旋转位置嵌入（RoPE）具有交换性的码本设计，以显著压缩KV缓存并降低解码计算成本。实验证明，CommVQ能将KV缓存大小减少87.5%（2位量化），并支持1位量化，从而使LLaMA-3.1 8B模型在单张RTX 4090 GPU上运行128K上下文，性能优于现有方法。",
      "translated_title": "CommVQ：KV缓存压缩的交换向量量化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes a memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with a lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. The source code is available at: https://github.com/UMass-Embodied-AGI/CommVQ."
    },
    {
      "title": "OmniGen2：探索先进多模态生成 (原标题: OmniGen2: Exploration to Advanced Multimodal Generation)",
      "link": "https://arxiv.org/abs/2506.18871",
      "pubDate": "Mon, 23 Jun 2025 13:38:54 GMT",
      "isoDate": "2025-06-23T13:38:54.000Z",
      "creator": "Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, Zheng Liu",
      "summary": "# OmniGen2：探索先进多模态生成\n\n## 概述\nOmniGen2是一个多功能、开源的生成模型，旨在为多种生成任务提供统一的解决方案，包括文本到图像生成、图像编辑和上下文生成。它旨在提供一个统一的解决方案，以应对多样化的生成任务。\n\n## 核心特性与改进\n*   **双重解码路径：** 与OmniGen v1不同，OmniGen2为文本和图像模态设计了两个独立的解码路径，并使用不共享的参数。\n*   **解耦图像分词器：** 采用了独立的图像分词器。\n*   **兼容性与能力保留：** 这种设计使得OmniGen2能够基于现有的多模态理解模型进行构建，无需重新调整VAE输入，从而完整保留了原始的文本生成能力。\n\n## 数据构建与训练\n*   **全面的数据构建流程：** 为促进OmniGen2的训练，研究团队开发了全面的数据构建流程，涵盖了图像编辑和上下文生成数据。\n*   **反射机制与数据集：** 引入了一种专为图像生成任务设计的反射机制，并基于OmniGen2策划了一个专门的反射数据集。\n\n## 性能表现\n*   **参数规模与竞争力：** 尽管参数规模相对适中，OmniGen2在多个任务基准测试中（包括文本到图像和图像编辑）取得了有竞争力的结果。\n*   **OmniContext基准：** 为了进一步评估上下文生成（也称为主题驱动任务），研究团队引入了一个名为OmniContext的新基准。\n*   **领先表现：** 在OmniContext基准上，OmniGen2在开源模型中展现出最先进的一致性表现。\n\n## 资源发布\n研究团队计划发布OmniGen2的模型、训练代码、数据集以及数据构建流程，以支持该领域的未来研究。\n\n## 作者\nChenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, Zheng Liu",
      "shortSummary": "OmniGen2是一个多功能、开源的生成模型，旨在统一文本到图像、图像编辑和上下文生成任务。它采用独立的文本和图像解码路径及解耦的图像分词器，在保留文本生成能力的同时，能基于现有模型构建。OmniGen2在多项基准测试中表现出色，并在新引入的OmniContext基准上，在开源模型中实现了最先进的上下文生成一致性。相关模型、代码和数据集将公开发布。",
      "translated_title": "OmniGen2：探索先进多模态生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this work, we introduce OmniGen2, a versatile and open-source generative model designed to provide a unified solution for diverse generation tasks, including text-to-image, image editing, and in-context generation. Unlike OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image modalities, utilizing unshared parameters and a decoupled image tokenizer. This design enables OmniGen2 to build upon existing multimodal understanding models without the need to re-adapt VAE inputs, thereby preserving the original text generation capabilities. To facilitate the training of OmniGen2, we developed comprehensive data construction pipelines, encompassing image editing and in-context generation data. Additionally, we introduce a reflection mechanism tailored for image generation tasks and curate a dedicated reflection dataset based on OmniGen2. Despite its relatively modest parameter size, OmniGen2 achieves competitive results on multiple task benchmarks, including text-to-image and image editing. To further evaluate in-context generation, also referred to as subject-driven tasks, we introduce a new benchmark named OmniContext. OmniGen2 achieves state-of-the-art performance among open-source models in terms of consistency. We will release our models, training code, datasets, and data construction pipeline to support future research in this field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link: https://github.com/VectorSpaceLab/OmniGen2"
    },
    {
      "title": "Phantom-Data：迈向通用主体一致性视频生成数据集 (原标题: Phantom-Data : Towards a General Subject-Consistent Video Generation Dataset)",
      "link": "https://arxiv.org/abs/2506.18851",
      "pubDate": "Mon, 23 Jun 2025 13:11:56 GMT",
      "isoDate": "2025-06-23T13:11:56.000Z",
      "creator": "Zhuowei Chen, Bingchuan Li, Tianxiang Ma, Lijie Liu, Mingcong Liu, Yi Zhang, Gen Li, Xinghui Li, Siyu Zhou, Qian He, Xinglong Wu",
      "summary": "## Phantom-Data：解决主体一致性视频生成中的“复制粘贴”问题\n\n### 背景与挑战\n\n*   **主体到视频生成（Subject-to-video generation）**近年来取得了显著进展。\n*   然而，现有模型在忠实遵循文本指令方面仍面临重大挑战，这被称为**“复制粘贴问题”**。\n*   该问题源于广泛使用的**“成对训练范式”（in-pair training paradigm）**，即从与目标视频相同场景中采样参考图像。这种方法固有地将主体身份与背景和上下文属性纠缠在一起。\n\n### 解决方案：Phantom-Data 数据集\n\n*   为了解决上述问题，研究人员引入了 **Phantom-Data**，这是**首个通用跨对（cross-pair）主体到视频一致性数据集**。\n*   该数据集包含大约**一百万个身份一致的图像对**，涵盖了多样化的类别。\n\n### Phantom-Data 的构建流程\n\nPhantom-Data 的构建通过一个三阶段的流水线完成：\n\n1.  **通用且输入对齐的主体检测模块（General and input-aligned subject detection module）**：用于准确识别和定位图像中的主体。\n2.  **大规模跨上下文主体检索（Large-scale cross-context subject retrieval）**：从超过5300万个视频和30亿张图像中检索具有相同主体的不同上下文图像。\n3.  **先验引导的身份验证（Prior-guided identity verification）**：确保在上下文变化下视觉一致性得到保持。\n\n### 实验结果\n\n*   全面的实验表明，使用 Phantom-Data 进行训练显著改善了**提示对齐（prompt alignment）**和**视觉质量（visual quality）**。\n*   同时，它在**身份一致性（identity consistency）**方面与成对基线（in-pair baselines）保持了同等水平。",
      "shortSummary": "现有主体到视频生成模型面临“复制粘贴问题”，因成对训练将主体与背景混淆。为解决此，研究人员推出了Phantom-Data，首个通用跨对主体一致性视频生成数据集。该数据集包含约一百万个身份一致的图像对，通过主体检测、跨上下文检索和身份验证构建。实验证明，Phantom-Data显著提升了提示对齐和视觉质量，同时保持了身份一致性。",
      "translated_title": "Phantom-Data：迈向通用主体一致性视频生成数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "Subject-to-video generation has witnessed substantial progress in recent years. However, existing models still face significant challenges in faithfully following textual instructions. This limitation, commonly known as the copy-paste problem, arises from the widely used in-pair training paradigm. This approach inherently entangles subject identity with background and contextual attributes by sampling reference images from the same scene as the target video. To address this issue, we introduce Phantom-Data, the first general-purpose cross-pair subject-to-video consistency dataset, containing approximately one million identity-consistent pairs across diverse categories. Our dataset is constructed via a three-stage pipeline: (1) a general and input-aligned subject detection module, (2) large-scale cross-context subject retrieval from more than 53 million videos and 3 billion images, and (3) prior-guided identity verification to ensure visual consistency under contextual variation. Comprehensive experiments show that training with Phantom-Data significantly improves prompt alignment and visual quality while preserving identity consistency on par with in-pair baselines."
    },
    {
      "title": "LongWriter-Zero：通过强化学习掌握超长文本生成 (原标题: LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2506.18841",
      "pubDate": "Mon, 23 Jun 2025 12:59:02 GMT",
      "isoDate": "2025-06-23T12:59:02.000Z",
      "creator": "Yuhao Wu, Yushi Bai, Zhiqiang Hu, Roy Ka-Wei Lee, Juanzi Li",
      "summary": "## LongWriter-Zero：通过强化学习实现超长文本生成\n\n### 挑战与现有方法\n大型语言模型（LLMs）在生成超长文本时面临显著挑战，主要体现在最大生成长度限制以及随着序列长度增加而导致的整体质量下降。传统的解决方案，例如LongWriter，通常依赖于“教学”方法，即对合成的长文本输出进行监督微调（SFT）。然而，这种策略存在诸多局限性：\n*   **数据依赖性高**：严重依赖合成SFT数据，而此类数据构建起来既困难又昂贵。\n*   **质量问题**：合成数据往往缺乏连贯性和一致性。\n*   **人工痕迹重**：生成内容可能过于人工化，结构单调。\n\n### LongWriter-Zero 方法\n为了克服上述挑战，本文提出了一种名为LongWriter-Zero的激励式方法，该方法完全从零开始，不依赖任何标注或合成数据，而是利用强化学习（RL）来培养LLM生成超长、高质量文本的能力。其核心思想包括：\n*   **从基础模型开始**：RL训练从一个基础模型（类似于R1-Zero）开始。\n*   **引导推理过程**：引导模型在写作过程中进行推理，从而促进规划和细化。\n*   **专业奖励模型**：采用专门的奖励模型来指导LLM，以实现：\n    *   改进的长度控制。\n    *   提升写作质量。\n    *   优化结构格式。\n\n### 实验结果与性能\n实验评估表明，LongWriter-Zero模型（基于Qwen2.5-32B训练）在长文本写作任务上持续优于传统的SFT方法，取得了显著的成果：\n*   **SOTA表现**：在WritingBench和Arena-Write的所有指标上均达到了最先进（SOTA）的水平。\n*   **超越大型模型**：甚至超越了DeepSeek R1和Qwen3-235B等100B+参数的模型。\n\n### 开放资源\n研究团队已开源了相关数据和模型检查点。",
      "shortSummary": "LongWriter-Zero提出一种基于强化学习的新方法，旨在解决大型语言模型在超长文本生成中的质量和长度限制问题。与依赖合成数据的传统监督微调不同，LongWriter-Zero从零开始，通过强化学习和专业奖励模型，引导模型进行规划和细化，从而提升文本长度控制、写作质量和结构。实验证明，LongWriter-Zero在长文本写作任务上表现优异，超越了传统方法甚至更大的模型，达到了最先进水平。",
      "translated_title": "LongWriter-Zero：通过强化学习掌握超长文本生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on ''teaching'', which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. In this work, we propose an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. We perform RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. We open-source our data and model checkpoints under https://huggingface.co/THU-KEG/LongWriter-Zero-32B"
    },
    {
      "title": "ReDit: 奖励抖动用于改进LLM策略优化 (原标题: ReDit: Reward Dithering for Improved LLM Policy Optimization)",
      "link": "https://arxiv.org/abs/2506.18631",
      "pubDate": "Mon, 23 Jun 2025 09:36:24 GMT",
      "isoDate": "2025-06-23T09:36:24.000Z",
      "creator": "Chenxing Wei, Jiarui Yu, Ying Tiffany He, Hande Dong, Yao Shu, Fei Yu",
      "summary": "## ReDit: 改进LLM策略优化中的奖励抖动方法\n\n### 背景与问题\n\nDeepSeek-R1 通过其基于规则的奖励系统成功提升了大型语言模型（LLM）的推理能力。尽管这种奖励系统在有效缓解奖励作弊方面表现出色，但其奖励函数通常是离散的。研究观察表明，离散奖励可能导致梯度异常、优化不稳定以及收敛速度缓慢。\n\n### 提出的解决方案：ReDit\n\n为了解决上述问题，本文提出了 **ReDit (Reward Dithering)** 方法。ReDit 通过向离散奖励信号添加简单的随机噪声来对其进行抖动（dither）。\n\n#### ReDit 的优势：\n\n*   **平滑梯度更新**：通过扰动后的奖励，学习过程中可以持续提供探索性梯度，从而实现更平滑的梯度更新。\n*   **加速收敛**：平滑的梯度更新有助于模型更快地收敛。\n*   **鼓励探索**：注入的噪声在平坦的奖励区域引入了随机性，这鼓励模型探索新的策略并跳出局部最优。\n\n### 实验结果与验证\n\n*   **有效性与效率**：在各种任务上的实验证明了 ReDit 的有效性和效率。平均而言，ReDit 仅需约 10% 的训练步骤即可达到与传统 GRPO 相当的性能。\n*   **性能提升**：在相似的训练时长下，ReDit 仍比传统 GRPO 表现出 4% 的性能提升。\n*   **梯度问题缓解**：可视化结果证实 ReDit 显著缓解了梯度问题。\n*   **理论支持**：研究还提供了理论分析，进一步验证了这些优势。\n\n**注意：** 文章中提及包含15张图，但未提供有效的图片链接，因此摘要中未包含任何图片。",
      "shortSummary": "ReDit 是一种通过向离散奖励信号添加随机噪声来改进大型语言模型（LLM）策略优化的方法。针对离散奖励导致的梯度异常和收敛缓慢问题，ReDit 能提供更平滑的梯度更新，加速收敛，并鼓励模型探索新策略以跳出局部最优。实验表明，ReDit 能以更少的训练步骤达到与传统方法相当的性能，或在相同训练时长下实现性能提升，并有效缓解梯度问题。",
      "translated_title": "ReDit: 奖励抖动用于改进LLM策略优化",
      "images": [],
      "contentSource": "完整文章",
      "content": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While it's a ''perfect'' reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages."
    },
    {
      "title": "自回归生成多视角一致图像 (原标题: Auto-Regressively Generating Multi-View Consistent Images)",
      "link": "https://arxiv.org/abs/2506.18527",
      "pubDate": "Mon, 23 Jun 2025 07:28:37 GMT",
      "isoDate": "2025-06-23T07:28:37.000Z",
      "creator": "JiaKui Hu, Yuxiao Yang, Jialun Liu, Jinbo Wu, Chen Zhao, Yanye Lu",
      "summary": "## MV-AR：自回归生成多视角一致图像\n\n### 引言与挑战\n\n在3D内容创作领域，根据人类指令生成多视角图像至关重要。然而，这一过程面临两大主要挑战：\n\n*   **多视角一致性：** 如何在不同视角之间保持图像内容和风格的一致性。\n*   **形状与纹理合成：** 如何在多样化的条件下有效合成物体的形状和纹理。\n\n### MV-AR 方法概述\n\n本文提出了一种名为**多视角自回归（Multi-View Auto-Regressive, MV-AR）**的方法。MV-AR利用自回归模型，能够从任意提示词（prompts）逐步生成一致的多视角图像。\n\n### MV-AR 的关键特性与创新点\n\n1.  **渐进式多视角合成：**\n    *   自回归模型的“下一词预测”（next-token-prediction）能力显著增强了渐进式多视角合成的有效性。\n    *   在生成间隔较远的视角时，MV-AR能够利用所有之前的视角来提取有效的参考信息，从而确保一致性。\n\n2.  **统一模型设计与多模态条件处理：**\n    *   通过精巧的架构设计和训练策略，MV-AR提出了一个统一模型，以适应各种类型的提示词。\n    *   引入了**条件注入模块（condition injection modules）**，专门用于处理文本、相机姿态、图像和形状等多种条件。\n    *   为同时管理多模态条件，采用了**渐进式训练策略**：\n        *   该策略首先以文本到多视角（text-to-multi-view, t2mv）模型作为基线。\n        *   随后，通过随机丢弃和组合各种条件，逐步发展出一个全面的X到多视角（X-to-multi-view, X2mv）模型。\n\n3.  **“视角洗牌”数据增强技术：**\n    *   为缓解高质量数据有限可能导致的过拟合问题，MV-AR提出了一种名为**“视角洗牌”（\"Shuffle View\"）**的数据增强技术。\n    *   这项技术能够显著扩大训练数据量，达到数倍的增长，从而提升模型的泛化能力。\n\n### 实验结果\n\n实验结果充分证明了MV-AR方法的卓越性能和多功能性。该方法能够在各种条件下持续生成高度一致的多视角图像，并且其表现与当前领先的基于扩散（diffusion-based）的多视角图像生成模型不相上下。\n\n### 代码与模型\n\n相关代码和模型将在未来发布。",
      "shortSummary": "本文提出MV-AR（多视角自回归）方法，旨在解决3D内容创作中多视角图像生成的一致性与合成挑战。MV-AR利用自回归模型，能从任意提示词逐步生成一致的多视角图像。其创新点包括利用先前视角进行参考、统一模型设计、多模态条件注入及渐进式训练策略。为应对数据限制，还引入了“视角洗牌”数据增强技术。实验表明，MV-AR能稳定生成一致的多视角图像，性能与主流扩散模型相当。",
      "translated_title": "自回归生成多视角一致图像",
      "images": [],
      "contentSource": "完整文章",
      "content": "Generating multi-view images from human instructions is crucial for 3D content creation. The primary challenges involve maintaining consistency across multiple views and effectively synthesizing shapes and textures under diverse conditions. In this paper, we propose the Multi-View Auto-Regressive (MV-AR) method, which leverages an auto-regressive model to progressively generate consistent multi-view images from arbitrary prompts. Firstly, the next-token-prediction capability of the AR model significantly enhances its effectiveness in facilitating progressive multi-view synthesis. When generating widely-separated views, MV-AR can utilize all its preceding views to extract effective reference information. Subsequently, we propose a unified model that accommodates various prompts via architecture designing and training strategies. To address multiple conditions, we introduce condition injection modules for text, camera pose, image, and shape. To manage multi-modal conditions simultaneously, a progressive training strategy is employed. This strategy initially adopts the text-to-multi-view (t2mv) model as a baseline to enhance the development of a comprehensive X-to-multi-view (X2mv) model through the randomly dropping and combining conditions. Finally, to alleviate the overfitting problem caused by limited high-quality data, we propose the \"Shuffle View\" data augmentation technique, thus significantly expanding the training data by several magnitudes. Experiments demonstrate the performance and versatility of our MV-AR, which consistently generates consistent multi-view images across a range of conditions and performs on par with leading diffusion-based multi-view image generation models. Code and models will be released at https://github.com/MILab-PKU/MVAR."
    },
    {
      "title": "SlimMoE：通过专家精简和蒸馏对大型MoE模型进行结构化压缩 (原标题: SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation)",
      "link": "https://arxiv.org/abs/2506.18349",
      "pubDate": "Mon, 23 Jun 2025 03:15:59 GMT",
      "isoDate": "2025-06-23T03:15:59.000Z",
      "creator": "Zichong Li, Chen Liang, Zixuan Zhang, Ilgee Hong, Young Jin Kim, Weizhu Chen, Tuo Zhao",
      "summary": "# SlimMoE：大型MoE模型的结构化压缩框架\n\n## 摘要\n\nMoE（专家混合）架构已成为扩展大型语言模型（LLMs）同时保持推理效率的强大范式。然而，其巨大的内存需求使得在资源受限环境中进行微调或部署的成本极高。为解决这一挑战，本文引入了SlimMoE，一个多阶段压缩框架，旨在将大型MoE模型转换为更小、更高效的版本，同时避免从头训练的巨大成本。\n\n## SlimMoE 方法论\n\nSlimMoE采用系统性的方法来减少模型参数数量，其核心在于“专家精简”（expert slimming）和“通过中间阶段知识蒸馏”（knowledge transfer through intermediate stages）。这种分阶段的方法有效缓解了传统一次性剪枝方法中常见的性能下降问题。\n\n## 实验结果与性能\n\n研究团队使用SlimMoE框架对Phi 3.5-MoE模型（总参数41.9B，激活参数6.6B）进行了压缩，成功创建了两个更小的模型：\n\n*   **Phi-mini-MoE**：总参数7.6B，激活参数2.4B。\n*   **Phi-tiny-MoE**：总参数3.8B，激活参数1.1B。\n\n值得注意的是，整个压缩过程仅使用了4000亿个tokens，这不到原始模型训练数据量的10%。\n\n### 部署与可访问性\n\n这些压缩后的模型显著降低了硬件要求，使得它们更适合学术界和资源有限的环境：\n\n*   Phi-mini-MoE 可以在单个A100 GPU上进行微调。\n*   Phi-tiny-MoE 可以在单个A6000 GPU上进行微调。\n\n### 性能表现\n\n实验结果表明，SlimMoE压缩后的模型表现出色：\n\n*   它们超越了同等规模的其他模型。\n*   与更大的模型相比，它们仍保持竞争力。\n*   **Phi-mini-MoE 示例**：\n    *   与Phi-3-mini相比，在激活参数量仅为其2/3的情况下，实现了相似或更优的性能。\n    *   在MMLU（大规模多任务语言理解）得分上，与Llama 3.1 8B模型相当，但延迟显著更低。\n\n## 结论\n\n研究结果表明，结合结构化剪枝和分阶段蒸馏是一种创建高质量、紧凑MoE模型的有效途径。这为MoE架构的更广泛应用铺平了道路。\n\n## 模型可用性\n\n所有压缩后的模型已公开提供。",
      "shortSummary": "SlimMoE是一个多阶段压缩框架，旨在解决大型MoE模型内存需求高昂的问题。它通过专家精简和分阶段知识蒸馏，将大型MoE模型（如Phi 3.5-MoE）压缩成更小、更高效的版本（如Phi-mini-MoE和Phi-tiny-MoE），且仅使用少量训练数据。这些压缩模型能在单GPU上微调，性能优于同等规模模型，并与更大模型保持竞争力，显著降低了MoE模型的部署成本和门槛。",
      "translated_title": "SlimMoE：通过专家精简和蒸馏对大型MoE模型进行结构化压缩",
      "images": [],
      "contentSource": "完整文章",
      "content": "The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm for scaling large language models (LLMs) while maintaining inference efficiency. However, their enormous memory requirements make them prohibitively expensive to fine-tune or deploy in resource-constrained environments. To address this challenge, we introduce SlimMoE, a multi-stage compression framework for transforming large MoE models into much smaller, efficient variants without incurring the prohibitive costs of training from scratch. Our method systematically reduces parameter counts by slimming experts and transferring knowledge through intermediate stages, effectively mitigating the performance degradation common in one-shot pruning approaches. Using this framework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to create Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE (3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of the original model's training data. These compressed models can be fine-tuned on a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them highly suitable for academic and resource-limited settings. Our experiments demonstrate that these compressed models outperform others of similar size and remain competitive with larger models. For instance, Phi-mini-MoE achieves similar or better performance to Phi-3-mini using only 2/3 of the activated parameters and yields comparable MMLU scores to Llama 3.1 8B despite having significantly lower latency. Our findings demonstrate that structured pruning combined with staged distillation offers an effective path to creating high-quality, compact MoE models, paving the way for broader adoption of MoE architectures. We make our models publicly available at https://huggingface.co/microsoft/Phi-mini-MoE-instruct and https://huggingface.co/microsoft/Phi-tiny-MoE-instruct ."
    },
    {
      "title": "LettinGo：探索推荐系统中的用户画像生成 (原标题: LettinGo: Explore User Profile Generation for Recommendation System)",
      "link": "https://arxiv.org/abs/2506.18309",
      "pubDate": "Mon, 23 Jun 2025 01:51:52 GMT",
      "isoDate": "2025-06-23T01:51:52.000Z",
      "creator": "Lu Wang, Di Zhang, Fangkai Yang, Pu Zhao, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Qingwei Lin, Weiwei Deng, Dongmei Zhang, Feng Sun, Qi Zhang",
      "summary": "## LettinGo：探索推荐系统中的用户画像生成\n\n*   **背景与挑战**\n    *   用户画像对于推荐系统至关重要，它将原始用户交互数据转化为简洁且结构化的表示，从而驱动个性化推荐。\n    *   传统基于嵌入（embedding-based）的用户画像缺乏可解释性和适应性。\n    *   尽管大型语言模型（LLMs）的最新进展使得基于文本的画像语义更丰富、更透明，但现有方法通常遵循固定格式，限制了其捕捉用户行为多样性的能力。\n\n*   **LettinGo 框架介绍**\n    *   本文提出了一种名为 LettinGo 的新型框架，旨在生成多样化和自适应的用户画像。\n    *   该方法利用 LLMs 的强大表达能力，并结合来自下游推荐任务的直接反馈。\n    *   LettinGo 避免了监督微调（SFT）所施加的严格约束，转而采用直接偏好优化（Direct Preference Optimization, DPO）来使画像生成器与任务特定的性能对齐，从而确保画像保持自适应性和有效性。\n\n*   **LettinGo 的三阶段操作**\n    1.  **探索多样化的用户画像：** 通过使用多个 LLMs 来探索生成多样化的用户画像。\n    2.  **评估画像质量：** 根据画像在推荐系统中的影响来评估其质量。\n    3.  **对齐画像生成：** 通过从任务性能中导出的成对偏好数据来对齐画像的生成过程。\n\n*   **实验结果与意义**\n    *   实验结果表明，LettinGo 框架显著提升了推荐的准确性、灵活性和上下文感知能力。\n    *   这项工作将画像生成作为下一代推荐系统的关键创新加以增强。",
      "shortSummary": "LettinGo是一个为推荐系统生成多样化和自适应用户画像的新框架。它利用大型语言模型（LLMs）的强大能力，并通过直接偏好优化（DPO）将画像生成与下游推荐任务的性能对齐。该方法避免了传统监督微调的局限性，显著提升了推荐的准确性、灵活性和上下文感知能力，是下一代推荐系统的关键创新。",
      "translated_title": "LettinGo：探索推荐系统中的用户画像生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "User profiling is pivotal for recommendation systems, as it transforms raw user interaction data into concise and structured representations that drive personalized recommendations. While traditional embedding-based profiles lack interpretability and adaptability, recent advances with large language models (LLMs) enable text-based profiles that are semantically richer and more transparent. However, existing methods often adhere to fixed formats that limit their ability to capture the full diversity of user behaviors. In this paper, we introduce LettinGo, a novel framework for generating diverse and adaptive user profiles. By leveraging the expressive power of LLMs and incorporating direct feedback from downstream recommendation tasks, our approach avoids the rigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ Direct Preference Optimization (DPO) to align the profile generator with task-specific performance, ensuring that the profiles remain adaptive and effective. LettinGo operates in three stages: (1) exploring diverse user profiles via multiple LLMs, (2) evaluating profile quality based on their impact in recommendation systems, and (3) aligning the profile generation through pairwise preference data derived from task performance. Experimental results demonstrate that our framework significantly enhances recommendation accuracy, flexibility, and contextual awareness. This work enhances profile generation as a key innovation for next-generation recommendation systems."
    },
    {
      "title": "RLPR：将RLVR推广到没有验证器的通用领域 (原标题: RLPR: Extrapolating RLVR to General Domains without Verifiers)",
      "link": "https://arxiv.org/abs/2506.18254",
      "pubDate": "Sun, 22 Jun 2025 22:56:36 GMT",
      "isoDate": "2025-06-22T22:56:36.000Z",
      "creator": "Tianyu Yu, Bo Ji, Shouli Wang, Shu Yao, Zefan Wang, Ganqu Cui, Lifan Yuan, Ning Ding, Yuan Yao, Zhiyuan Liu, Maosong Sun, Tat-Seng Chua",
      "summary": "## RLPR：将RLVR推广到没有验证器的通用领域\n\n### 背景与挑战\n\n可验证奖励强化学习（RLVR）在提升大型语言模型（LLM）的推理能力方面展现出巨大潜力。然而，RLVR的成功主要局限于数学和代码领域。这一主要限制源于其对领域特定验证器的严重依赖，这导致了极高的复杂性和有限的可扩展性。\n\n### 核心洞察\n\n为了解决这一挑战，本文的关键观察是：LLM生成正确自由形式答案的内在概率直接表明了其对推理奖励的自我评估（即推理过程如何有效地导向正确答案）。\n\n### RLPR框架介绍\n\n基于这一洞察，本文提出了RLPR（Reinforcement Learning with Probability Rewards），一个简单、无需验证器的框架，旨在将RLVR推广到更广泛的通用领域。RLPR使用LLM自身对参考答案的token概率得分作为奖励信号，并在训练过程中最大化预期奖励。\n\n### 关键技术与方法\n\n作者发现，解决这种嘈杂概率奖励的高方差对于使其有效工作至关重要。为此，RLPR提出了“概率到奖励”（prob-to-reward）和“稳定化方法”（stabilizing methods），以确保从LLM内在概率中获得精确且稳定的奖励。\n\n### 实验结果与性能\n\n在四个通用领域基准测试和三个数学基准测试中进行了全面的实验，结果显示RLPR持续提升了基于Gemma、Llama和Qwen模型的推理能力，在通用领域和数学领域均表现出色。值得注意的是，RLPR在TheoremQA上比同期提出的VeriFree高出7.6分，在Minerva上高出7.5分。它甚至超越了依赖验证器的强大模型方法General-Reasoner，在七个基准测试中平均高出1.6分。",
      "shortSummary": "RLVR在LLM推理中受限于领域特定验证器。本文提出RLPR，一个无需验证器的框架，将RLVR推广到通用领域。RLPR利用LLM自身对参考答案的内在token概率作为奖励信号，并通过“概率到奖励”和稳定化方法解决奖励方差问题。实验表明，RLPR在通用和数学基准测试中持续提升了Gemma、Llama和Qwen模型的推理能力，并显著优于同期方法VeriFree和依赖验证器的方法General-Reasoner。",
      "translated_title": "RLPR：将RLVR推广到没有验证器的通用领域",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising potential in advancing the reasoning capabilities of LLMs. However, its success remains largely confined to mathematical and code domains. This primary limitation stems from the heavy reliance on domain-specific verifiers, which results in prohibitive complexity and limited scalability. To address the challenge, our key observation is that LLM's intrinsic probability of generating a correct free-form answer directly indicates its own evaluation of the reasoning reward (i.e., how well the reasoning process leads to the correct answer). Building on this insight, we propose RLPR, a simple verifier-free framework that extrapolates RLVR to broader general domains. RLPR uses the LLM's own token probability scores for reference answers as the reward signal and maximizes the expected reward during training. We find that addressing the high variance of this noisy probability reward is crucial to make it work, and propose prob-to-reward and stabilizing methods to ensure a precise and stable reward from LLM intrinsic probabilities. Comprehensive experiments in four general-domain benchmarks and three mathematical benchmarks show that RLPR consistently improves reasoning capabilities in both areas for Gemma, Llama, and Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6 points on TheoremQA and 7.5 points on Minerva, and even surpasses strong verifier-model-dependent approaches General-Reasoner by 1.6 average points across seven benchmarks."
    },
    {
      "title": "FaithfulSAE：在没有外部数据集依赖的情况下，使用稀疏自编码器捕获忠实特征 (原标题: FaithfulSAE: Towards Capturing Faithful Features with Sparse Autoencoders without External Dataset Dependencies)",
      "link": "https://arxiv.org/abs/2506.17673",
      "pubDate": "Sat, 21 Jun 2025 06:18:25 GMT",
      "isoDate": "2025-06-21T06:18:25.000Z",
      "creator": "Seonglae Cho, Harryn Oh, Donghyun Lee, Luis Eduardo Rodrigues Vieira, Andrew Bermingham, Ziad El Sayed",
      "summary": "## FaithfulSAE：一种捕获忠实特征的新方法\n\n### 引言：稀疏自编码器（SAE）的挑战\n\n稀疏自编码器（SAE）已被认为是分解大型语言模型（LLM）表示以获得可解释特征的一种有前景的解决方案。然而，现有研究指出了SAE的几个关键问题：\n\n*   **不稳定性：** Paulo和Belrose（2025）指出，SAE在不同初始化种子之间存在不稳定性。\n*   **特征捕获不足：** Heap et al.（2025）指出，SAE可能无法有效捕获模型内部的真实特征。\n\n这些问题很可能源于SAE的训练方式，即依赖外部数据集。这些数据集可能来自网络收集或由另一个模型生成，其中可能包含超出模型泛化能力的分布外（OOD）数据。这可能导致生成误导模型内部激活的“虚假特征”（Fake Features）。\n\n### FaithfulSAE：解决方案\n\n为了解决上述问题，本文提出了 **FaithfulSAE**，一种新颖的方法。FaithfulSAE的核心创新在于：\n\n*   **训练数据来源：** 它在模型自身的合成数据集上训练SAE。\n*   **目标：** 这种方法旨在消除对外部数据集的依赖，从而避免OOD数据带来的负面影响。\n\n### FaithfulSAE的优势与成果\n\n通过使用FaithfulSAE，研究人员展示了其在多个方面的显著优势：\n\n*   **稳定性提升：** 在OOD程度较低的指令数据集上训练SAE时，FaithfulSAE在不同初始化种子之间表现出更高的稳定性。\n*   **性能优越：** 在SAE探测任务中，FaithfulSAE的表现优于在传统网络数据集上训练的SAE。\n*   **虚假特征减少：** 在测试的7个模型中，FaithfulSAE在其中5个模型中展现出更低的“虚假特征比例”（Fake Feature Ratio）。\n\n### 结论\n\n总而言之，FaithfulSAE方法成功消除了对外部数据集的依赖，通过更好地捕获模型内部特征，显著提升了大型语言模型的可解释性。这项工作也强调了SAE训练数据集选择的重要性，这一方面在以往的研究中常被忽视。",
      "shortSummary": "稀疏自编码器（SAE）在解释大型语言模型时面临因外部数据集导致的稳定性差和“虚假特征”问题。FaithfulSAE提出在模型自身的合成数据集上训练SAE，从而消除外部依赖。实验表明，FaithfulSAE在稳定性、探测任务表现及减少虚假特征方面均优于传统方法，显著提升了模型可解释性，并强调了训练数据集的重要性。",
      "translated_title": "FaithfulSAE：在没有外部数据集依赖的情况下，使用稀疏自编码器捕获忠实特征",
      "images": [],
      "contentSource": "完整文章",
      "content": "Sparse Autoencoders (SAEs) have emerged as a promising solution for decomposing large language model representations into interpretable features. However, Paulo and Belrose (2025) have highlighted instability across different initialization seeds, and Heap et al. (2025) have pointed out that SAEs may not capture model-internal features. These problems likely stem from training SAEs on external datasets - either collected from the Web or generated by another model - which may contain out-of-distribution (OOD) data beyond the model's generalisation capabilities. This can result in hallucinated SAE features, which we term \"Fake Features\", that misrepresent the model's internal activations. To address these issues, we propose FaithfulSAE, a method that trains SAEs on the model's own synthetic dataset. Using FaithfulSAEs, we demonstrate that training SAEs on less-OOD instruction datasets results in SAEs being more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained on web-based datasets in the SAE probing task and exhibit a lower Fake Feature Ratio in 5 out of 7 models. Overall, our approach eliminates the dependency on external datasets, advancing interpretability by better capturing model-internal features while highlighting the often neglected importance of SAE training datasets."
    },
    {
      "title": "ConsumerBench: 在终端用户设备上对生成式人工智能应用进行基准测试 (原标题: ConsumerBench: Benchmarking Generative AI Applications on End-User Devices)",
      "link": "https://arxiv.org/abs/2506.17538",
      "pubDate": "Fri, 20 Jun 2025 21:32:22 GMT",
      "isoDate": "2025-06-20T21:32:22.000Z",
      "creator": "Yile Gu, Rohan Kadekodi, Hoang Nguyen, Keisuke Kamahori, Yiyu Liu, Baris Kasikci",
      "summary": "## ConsumerBench: 在终端用户设备上对生成式人工智能应用进行基准测试\n\n本文介绍了一个名为ConsumerBench的综合基准测试框架，旨在评估在终端用户设备上运行的生成式人工智能（GenAI）模型的系统效率和响应时间。\n\n*   **背景：** GenAI应用正从纯云环境转向终端用户设备，这给资源管理、系统效率和用户体验带来了新的挑战。\n*   **ConsumerBench的特点：**\n    *   模拟真实的**多应用场景**，这些场景在受限硬件上并发执行。\n    *   支持**可定制的工作流程**，模拟需要多个应用程序协调的复杂任务。\n    *   捕获**应用级别指标**（如延迟和服务级别目标（SLO）达成情况）和**系统级别指标**（如CPU/GPU利用率和内存带宽）。\n*   **实验结果：** ConsumerBench揭示了资源共享方面的效率低下、贪婪分配下的不公平调度以及静态模型服务器配置的性能缺陷。\n*   **实践见解：** 为模型开发者和系统设计者提供了实践见解，强调了为消费级GPU架构定制的自定义内核的优势，以及实施SLO感知调度策略的价值。\n*   **代码：** 代码可在以下URL获取：https://this URL",
      "shortSummary": "ConsumerBench是一个用于评估终端用户设备上生成式人工智能（GenAI）模型性能的基准测试框架。它模拟真实的多应用场景，捕获应用和系统级别的指标，揭示了资源共享的低效、不公平的调度以及静态模型服务器配置的缺陷。该框架还为模型开发者和系统设计者提供了实践见解，强调了定制内核和SLO感知调度策略的优势。",
      "translated_title": "ConsumerBench: 在终端用户设备上对生成式人工智能应用进行基准测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "The recent shift in Generative AI (GenAI) applications from cloud-only environments to end-user devices introduces new challenges in resource management, system efficiency, and user experience. This paper presents ConsumerBench, a comprehensive benchmarking framework designed to evaluate the system efficiency and response time of GenAI models running on end-user devices. Unlike existing benchmarks that assume exclusive model access on dedicated GPUs, ConsumerBench simulates realistic multi-application scenarios executing concurrently on constrained hardware. Furthermore, ConsumerBench supports customizable workflows that simulate complex tasks requiring coordination among multiple applications. ConsumerBench captures both application-level metrics, including latency and Service Level Objective (SLO) attainment, and system-level metrics like CPU/GPU utilization and memory bandwidth. Through extensive experiments, ConsumerBench reveals inefficiencies in resource sharing, unfair scheduling under greedy allocation, and performance pitfalls of static model server configurations. The paper also provides practical insights for model developers and system designers, highlighting the benefits of custom kernels tailored to consumer-grade GPU architectures and the value of implementing SLO-aware scheduling strategies."
    },
    {
      "title": "增强多模态大语言模型中逐步可验证的医学推理能力 (原标题: Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs)",
      "link": "https://arxiv.org/abs/2506.16962",
      "pubDate": "Fri, 20 Jun 2025 08:51:19 GMT",
      "isoDate": "2025-06-20T08:51:19.000Z",
      "creator": "Haoran Sun, Yankai Jiang, Wenjie Lou, Yujie Zhang, Wenjie Li, Lilong Wang, Mianxin Liu, Lei Liu, Xiaosong Wang",
      "summary": "### 增强多模态大语言模型中逐步可验证的医学推理能力\n\n**引言与背景**\n多模态大语言模型（MLLMs）在通用任务上已展现出强大的推理能力，但在医学领域的应用仍处于早期阶段。构建思维链（CoT）训练数据对于提升医学MLLMs的推理能力至关重要。然而，现有方法在提供一个全面的框架来搜索和评估针对关键诊断的有效推理路径方面存在不足。\n\n**提出的方法：导师-实习生协作搜索（MICS）**\n为了解决上述挑战，本文提出了一种新颖的推理路径搜索方案——导师-实习生协作搜索（MICS），旨在生成严谨有效的医学CoT数据。MICS的工作流程如下：\n\n*   **导师模型初始化推理：** MICS首先利用导师模型逐步初始化推理过程。\n*   **实习生模型延续思考：** 接着，每个实习生模型被提示沿着这些已初始化的路径继续思考。\n*   **选择最优推理路径：** 最后，根据多个实习生模型的整体推理表现来选择最优的推理路径。推理表现由一个名为MICS-Score的指标确定，该指标用于评估所生成推理路径的质量。\n\n**主要贡献与成果**\n通过MICS方法，研究人员取得了以下重要成果：\n\n*   **构建MMRP数据集：** 开发了一个名为MMRP的多任务医学推理数据集，该数据集具有分级难度。\n*   **开发Chiron-o1医学MLLM：** 设计并构建了一个新的医学MLLM，命名为Chiron-o1。Chiron-o1通过课程学习策略开发，具备强大的视觉问答能力和泛化推理能力。\n\n**实验结果**\n广泛的实验证明，Chiron-o1模型在利用MICS构建的CoT数据集进行训练后，在多项医学视觉问答和推理基准测试中取得了最先进（state-of-the-art）的性能。\n\n**代码可用性**\n相关代码已在GitHub上公开。",
      "shortSummary": "本文提出导师-实习生协作搜索（MICS）方案，旨在为医学多模态大语言模型（MLLMs）生成高质量、可验证的思维链（CoT）数据，以解决现有方法在医学推理路径搜索和评估方面的不足。MICS通过导师模型初始化、实习生模型延续思考并根据MICS-Score选择最优路径来工作。基于MICS构建的CoT数据集，研究人员开发了Chiron-o1医学MLLM和MMRP数据集。实验证明，Chiron-o1在医学视觉问答和推理任务上达到了最先进水平。",
      "translated_title": "增强多模态大语言模型中逐步可验证的医学推理能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the medical domain remains in its early stages. Constructing chain-of-thought (CoT) training data is essential for bolstering the reasoning abilities of medical MLLMs. However, existing approaches exhibit a deficiency in offering a comprehensive framework for searching and evaluating effective reasoning paths towards critical diagnosis. To address this challenge, we propose Mentor-Intern Collaborative Search (MICS), a novel reasoning-path searching scheme to generate rigorous and effective medical CoT data. MICS first leverages mentor models to initialize the reasoning, one step at a time, then prompts each intern model to continue the thinking along those initiated paths, and finally selects the optimal reasoning path according to the overall reasoning performance of multiple intern models. The reasoning performance is determined by an MICS-Score, which assesses the quality of generated reasoning paths. Eventually, we construct MMRP, a multi-task medical reasoning dataset with ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum learning strategy, with robust visual question-answering and generalizable reasoning capabilities. Extensive experiments demonstrate that Chiron-o1, trained on our CoT dataset constructed using MICS, achieves state-of-the-art performance across a list of medical visual question answering and reasoning benchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs"
    },
    {
      "title": "我知晓去年夏天是哪个LLM编写了你的代码：LLM生成代码的文体学归因 (原标题: I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution)",
      "link": "https://arxiv.org/abs/2506.17323",
      "pubDate": "Wed, 18 Jun 2025 15:49:41 GMT",
      "isoDate": "2025-06-18T15:49:41.000Z",
      "creator": "Tamas Bisztray, Bilel Cherif, Richard A. Dubniczky, Nils Gruschka, Bertalan Borsos, Mohamed Amine Ferrag, Attila Kovacs, Vasileios Mavroeidis, Norbert Tihanyi",
      "summary": "## LLM生成代码的作者归因研究\n\n### 引言与研究背景\n\n随着大型语言模型（LLM）生成的代码日益普及，检测AI生成内容（如代码、深度伪造）成为一个新兴的研究挑战。其中，识别特定LLM模型作为代码作者变得越来越重要。\n\n### 研究贡献与方法\n\n本文首次系统性地研究了C程序中LLM作者归因问题，并提出了以下关键贡献：\n\n1.  **CodeT5-Authorship模型**\n    *   **架构创新：** 该模型是一种新颖的架构，仅使用原始CodeT5编码器-解码器架构中的编码器层，舍弃了解码器，以专注于分类任务。\n    *   **分类机制：** 模型的编码器输出（第一个token）通过一个包含GELU激活函数和Dropout层的两层分类头，生成对可能作者的概率分布。\n\n2.  **LLM-AuthorBench基准数据集**\n    *   **数据集规模与来源：** 为了评估所提出的方法，研究引入了LLM-AuthorBench，这是一个包含32,000个可编译C程序的基准数据集。这些程序由八个最先进的LLM（包括Gemini 2.5 Flash、Claude 3.5 Haiku、GPT-4.1、Llama 3.3和DeepSeek-V3等）生成，涵盖了多种任务。\n\n### 实验与结果\n\n研究将CodeT5-Authorship模型与七种传统机器学习分类器以及八种微调的Transformer模型（包括BERT、RoBERTa、CodeBERT、ModernBERT、DistilBERT、DeBERTa-V3、Longformer和LoRA微调的Qwen2-1.5B）进行了比较。\n\n*   **二元分类表现：** 在区分由密切相关模型（如GPT-4.1和GPT-4o）生成的C程序时，CodeT5-Authorship模型实现了97.56%的准确率。\n*   **多类别归因表现：** 在对五个主要LLM（Gemini 2.5 Flash、Claude 3.5 Haiku、GPT-4.1、Llama 3.3和DeepSeek-V3）进行多类别归因时，模型达到了95.40%的准确率。\n\n### 开放科学实践\n\n为支持开放科学，研究团队已将CodeT5-Authorship架构、LLM-AuthorBench基准数据集以及所有相关的Google Colab脚本在GitHub上开源，链接为：[https://github.com/TamasBisztray/CodeT5-Authorship](https://github.com/TamasBisztray/CodeT5-Authorship)。\n\n### 研究领域\n\n本研究主要涉及机器学习（cs.LG）、人工智能（cs.AI）和软件工程（cs.SE）领域。",
      "shortSummary": "本研究首次系统性地探讨了C程序中LLM生成代码的作者归因问题。论文提出了CodeT5-Authorship模型和LLM-AuthorBench基准数据集。CodeT5-Authorship模型在区分密切相关的LLM（如GPT-4.1和GPT-4o）生成的代码时，二元分类准确率高达97.56%；在对五个主流LLM进行多类别归因时，准确率达到95.40%。所有研究资源均已开源，以促进开放科学。",
      "translated_title": "我知晓去年夏天是哪个LLM编写了你的代码：LLM生成代码的文体学归因",
      "images": [],
      "contentSource": "完整文章",
      "content": "Detecting AI-generated code, deepfakes, and other synthetic content is an emerging research challenge. As code generated by Large Language Models (LLMs) becomes more common, identifying the specific model behind each sample is increasingly important. This paper presents the first systematic study of LLM authorship attribution for C programs. We released CodeT5-Authorship, a novel model that uses only the encoder layers from the original CodeT5 encoder-decoder architecture, discarding the decoder to focus on classification. Our model's encoder output (first token) is passed through a two-layer classification head with GELU activation and dropout, producing a probability distribution over possible authors. To evaluate our approach, we introduce LLM-AuthorBench, a benchmark of 32,000 compilable C programs generated by eight state-of-the-art LLMs across diverse tasks. We compare our model to seven traditional ML classifiers and eight fine-tuned transformer models, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3, Longformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model achieves 97.56% accuracy in distinguishing C programs generated by closely related models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class attribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku, GPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the CodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant Google Colab scripts on GitHub: https://github.com/LLMauthorbench/."
    },
    {
      "title": "GenRecal：从大型到小型视觉-语言模型的重新校准后生成 (原标题: GenRecal: Generation after Recalibration from Large to Small Vision-Language Models)",
      "link": "https://arxiv.org/abs/2506.15681",
      "pubDate": "Wed, 18 Jun 2025 13:59:49 GMT",
      "isoDate": "2025-06-18T13:59:49.000Z",
      "creator": "Byung-Kwan Lee, Ryo Hachiuma, Yong Man Ro, Yu-Chiang Frank Wang, Yueh-Hua Wu",
      "summary": "## GenRecal：从大型到小型视觉-语言模型的重新校准后生成\n\n### 背景与挑战\n\n*   **大型视觉-语言模型（VLMs）的进步与部署难题：** 近期VLM取得了显著进展，性能可与GPT-4V等闭源系统媲美。然而，由于其巨大的计算需求，在资源受限设备上部署这些模型面临挑战。\n*   **知识蒸馏的需求：** 这促使研究人员探索将知识从大型VLM蒸馏到更小、更高效的模型。\n*   **核心挑战——VLM架构异构性：** 现有蒸馏方法面临的关键挑战是VLM架构的多样性。这些模型基于不同的LLMs构建，并采用不同的令牌类型（词汇量大小、令牌分割方式、令牌索引顺序），这限制了蒸馏方法对特定VLM类型的适用性。\n\n### GenRecal框架\n\n*   **提出GenRecal：** 为解决上述限制，本文提出了**GenRecal（Generation after Recalibration）**，一个新颖的、通用的VLM蒸馏框架。\n*   **核心机制——重新校准器（Recalibrator）：** GenRecal包含一个“重新校准器”，其作用是：\n    *   对齐并调整异构VLM之间的特征表示。\n    *   从而实现在不同类型VLM之间进行有效的知识迁移。\n\n### 实验结果\n\n*   **性能提升：** 通过在多个具有挑战性的基准测试上进行广泛实验，结果表明GenRecal显著提升了基线性能。\n*   **超越现有模型：** 最终，GenRecal甚至超越了大型开源和闭源VLM的性能。",
      "shortSummary": "GenRecal是一个通用的视觉-语言模型（VLM）蒸馏框架，旨在解决大型VLM部署的计算挑战。它通过引入“重新校准器”来对齐和适应异构VLM之间的特征表示，从而实现跨不同VLM类型的有效知识迁移。实验证明，GenRecal显著提升了性能，甚至超越了大型开源和闭源VLM，为在资源受限设备上部署高效VLM提供了解决方案。",
      "translated_title": "GenRecal：从大型到小型视觉-语言模型的重新校准后生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve performance on par with closed-source systems like GPT-4V. However, deploying these models in real-world scenarios, particularly on resource-constrained devices, remains challenging due to their substantial computational demands. This has spurred interest in distilling knowledge from large VLMs into smaller, more efficient counterparts. A key challenge arises here from the diversity of VLM architectures, which are built on different LLMs and employ varying token types-differing in vocabulary size, token splits, and token index ordering. To address this challenge of limitation to a specific VLM type, we present Generation after Recalibration (GenRecal), a novel, general-purpose distillation framework for VLMs. GenRecal incorporates a Recalibrator that aligns and adapts feature representations between heterogeneous VLMs, enabling effective knowledge transfer across different types of VLMs. Through extensive experiments on multiple challenging benchmarks, we demonstrate that GenRecal significantly improves baseline performances, eventually outperforming large-scale open- and closed-source VLMs."
    },
    {
      "title": "具身网络智能体：弥合物理-数字领域，实现集成智能 (原标题: Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence)",
      "link": "https://arxiv.org/abs/2506.15677",
      "pubDate": "Wed, 18 Jun 2025 13:58:17 GMT",
      "isoDate": "2025-06-18T13:58:17.000Z",
      "creator": "Yining Hong, Rui Sun, Bingxuan Li, Xingcheng Yao, Maxine Wu, Alexander Chien, Da Yin, Ying Nian Wu, Zhecan James Wang, Kai-Wei Chang",
      "summary": "# 具身网络智能体：弥合物理-数字领域，实现集成智能\n\n## 引言：当前AI智能体的局限性\n目前的AI智能体大多是孤立的：它们要么专注于检索和推理海量的在线数字信息和知识，要么通过具身感知、规划和行动与物理世界互动，但很少能同时做到这两点。这种分离限制了它们解决需要集成物理和数字智能的任务的能力，例如：\n*   根据在线食谱烹饪\n*   利用动态地图数据进行导航\n*   使用网络知识解释现实世界中的地标\n\n## 具身网络智能体：一种新范式\n为了解决上述限制，本文引入了“具身网络智能体”（Embodied Web Agents）这一新颖的AI智能体范式。该范式旨在流畅地连接具身（物理互动）与网络规模推理（数字信息），从而实现更全面的智能。\n\n## 概念的实现与评估\n为了将这一概念付诸实践，研究团队进行了以下工作：\n\n### 1. 具身网络智能体任务环境\n开发了一个统一的模拟平台，该平台紧密整合了：\n*   逼真的3D室内和室外环境\n*   功能性的网络界面\n\n这个平台为智能体在物理和数字领域进行交互提供了基础。\n\n### 2. 具身网络智能体基准测试\n基于上述平台，构建并发布了“具身网络智能体基准测试”（Embodied Web Agents Benchmark）。该基准测试包含一系列多样化的任务，旨在系统性地评估跨领域智能，这些任务包括：\n*   烹饪\n*   导航\n*   购物\n*   旅游\n*   地理定位\n\n所有这些任务都要求智能体在物理和数字领域之间进行协调推理。\n\n## 实验结果与未来展望\n实验结果揭示了当前最先进的AI系统与人类能力之间存在显著的性能差距。这表明在具身认知和网络规模知识访问的交叉领域，既存在巨大的挑战，也蕴含着广阔的机遇。\n\n## 资源可用性\n所有相关的数据集、代码和网站均已在其项目页面（this https URL）上公开提供。",
      "shortSummary": "“具身网络智能体”提出了一种新型AI范式，旨在弥合当前AI智能体在物理世界交互与网络知识推理之间的鸿沟。为实现此目标，研究团队开发了集成3D环境与网络界面的模拟平台，并构建了包含烹饪、导航等任务的基准测试。实验结果显示，现有AI系统与人类能力之间存在显著差距，揭示了具身认知与网络知识结合领域的挑战与机遇。所有资源均已公开。",
      "translated_title": "具身网络智能体：弥合物理-数字领域，实现集成智能",
      "images": [],
      "contentSource": "完整文章",
      "content": "AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/."
    },
    {
      "title": "Sekai：一个面向世界探索的视频数据集 (原标题: Sekai: A Video Dataset towards World Exploration)",
      "link": "https://arxiv.org/abs/2506.15675",
      "pubDate": "Wed, 18 Jun 2025 13:57:06 GMT",
      "isoDate": "2025-06-18T13:57:06.000Z",
      "creator": "Zhen Li, Chuanhao Li, Xiaofeng Mao, Shaoheng Lin, Ming Li, Shitian Zhao, Zhaopan Xu, Xinyue Li, Yukang Feng, Jianwen Sun, Zizhen Li, Fanrui Zhang, Jiaxin Ai, Zhixiang Wang, Yuwei Wu, Tong He, Jiangmiao Pang, Yu Qiao, Yunde Jia, Kaipeng Zhang",
      "summary": "# Sekai：一个面向世界探索的视频数据集\n\n## 摘要\n\n本文介绍了Sekai数据集，这是一个高质量的第一人称视角（FPV）全球视频数据集，旨在解决现有视频生成数据集在世界探索训练方面的局限性。\n\n## 现有问题\n\n*   **局限性：** 当前的视频生成技术在交互式世界探索方面潜力巨大，但现有数据集存在以下不足：\n    *   地点有限\n    *   持续时间短\n    *   场景静态\n    *   缺乏关于探索和世界本身的注释\n\n## Sekai数据集介绍\n\n*   **名称含义：** “Sekai”在日语中意为“世界”。\n*   **数据集类型：** 高质量的第一人称视角（FPV）全球视频数据集，包含丰富的世界探索注释。\n*   **内容规模：**\n    *   超过5,000小时的步行或无人机视角（FPV和UVA）视频。\n    *   视频来源覆盖100多个国家和地区，750个城市。\n*   **数据收集与处理：**\n    *   开发了一个高效且有效的工具箱，用于视频的收集、预处理和注释。\n*   **丰富注释：** 视频包含以下注释信息：\n    *   地点（Location）\n    *   场景（Scene）\n    *   天气（Weather）\n    *   人群密度（Crowd density）\n    *   字幕/描述（Captions）\n    *   摄像机轨迹（Camera trajectories）\n\n## 实验与应用\n\n*   **数据集质量：** 实验证明了Sekai数据集的质量。\n*   **模型训练：** 研究人员使用Sekai的一个子集训练了一个名为YUME（日语中意为“梦想”）的交互式视频世界探索模型。\n*   **预期影响：** 作者相信Sekai将：\n    *   推动视频生成和世界探索领域的发展。\n    *   激发有价值的应用。",
      "shortSummary": "Sekai是一个高质量的第一人称视角（FPV）全球视频数据集，旨在解决现有视频数据集在世界探索训练中的局限性。它包含来自100多个国家和750个城市的5000多小时视频，并提供地点、场景、天气、人群密度、字幕和摄像机轨迹等丰富注释。Sekai的推出有望推动视频生成和世界探索领域的发展，并已用于训练交互式模型YUME。",
      "translated_title": "Sekai：一个面向世界探索的视频数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "Video generation techniques have made remarkable progress, promising to be the foundation of interactive world exploration. However, existing video generation datasets are not well-suited for world exploration training as they suffer from some limitations: limited locations, short duration, static scenes, and a lack of annotations about exploration and the world. In this paper, we introduce Sekai (meaning ``world'' in Japanese), a high-quality first-person view worldwide video dataset with rich annotations for world exploration. It consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from over 100 countries and regions across 750 cities. We develop an efficient and effective toolbox to collect, pre-process and annotate videos with location, scene, weather, crowd density, captions, and camera trajectories. Experiments demonstrate the quality of the dataset. And, we use a subset to train an interactive video world exploration model, named YUME (meaning ``dream'' in Japanese). We believe Sekai will benefit the area of video generation and world exploration, and motivate valuable applications."
    }
  ],
  "lastUpdated": "2025-06-24T09:37:01.011Z"
}