{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "迈向指代音视频分割中的全模态表达与推理 (原标题: Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual Segmentation)",
      "link": "https://arxiv.org/abs/2507.22886",
      "pubDate": "Wed, 30 Jul 2025 13:59:31 GMT",
      "isoDate": "2025-07-30T13:59:31.000Z",
      "creator": "Kaining Ying, Henghui Ding, Guanquan Jie, Yu-Gang Jiang",
      "summary": "### 迈向指代音视频分割中的全模态表达与推理\n\n**背景与挑战**\n指代音视频分割（RAVS）领域近期取得了显著进展，但在整合多模态信息、深入理解和推理音视频内容方面仍面临挑战。\n\n**OmniAVS 数据集**\n为了拓展RAVS的边界并促进该领域的未来研究，本文提出了一个名为OmniAVS的新数据集。\n*   **规模与内容**：包含2,098个视频和59,458个多模态指代表达。\n*   **三大创新点**：\n    1.  **八种多模态表达类型**：灵活结合文本、语音、声音和视觉线索。\n    2.  **强调音频内容理解**：超越仅仅检测音频的存在，更注重其深层含义。\n    3.  **包含复杂推理和世界知识**：在表达中融入了需要复杂推理和世界知识才能理解的内容。\n\n**Omnimodal Instructed Segmentation Assistant (OISA) 模型**\n为应对OmniAVS数据集中多模态推理和音视频内容细粒度理解的挑战，本文引入了Omnimodal Instructed Segmentation Assistant (OISA) 模型。\n*   **核心机制**：OISA利用多模态大语言模型（MLLM）来理解复杂的线索并执行基于推理的分割任务。\n*   **性能表现**：广泛的实验表明，OISA在OmniAVS数据集上优于现有方法，并在其他相关任务中也取得了有竞争力的结果。\n\n**其他信息**\n*   本文已被ICCV 2025接收。",
      "shortSummary": "本文提出了OmniAVS数据集和OISA模型，旨在推动指代音视频分割（RAVS）领域的发展。OmniAVS包含2098个视频和近6万个多模态表达，其创新在于引入了8种灵活结合文本、语音、声音和视觉线索的表达类型，并强调对音频内容的深入理解及复杂推理。OISA是一个基于多模态大语言模型（MLLM）的助手，能够理解复杂线索并执行基于推理的分割。实验证明OISA在OmniAVS上表现优异，并超越现有方法。",
      "translated_title": "迈向指代音视频分割中的全模态表达与推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Referring audio-visual segmentation (RAVS) has recently seen significant advancements, yet challenges remain in integrating multimodal information and deeply understanding and reasoning about audiovisual content. To extend the boundaries of RAVS and facilitate future research in this field, we propose Omnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset containing 2,098 videos and 59,458 multimodal referring expressions. OmniAVS stands out with three key innovations: (1) 8 types of multimodal expressions that flexibly combine text, speech, sound, and visual cues; (2) an emphasis on understanding audio content beyond just detecting their presence; and (3) the inclusion of complex reasoning and world knowledge in expressions. Furthermore, we introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the challenges of multimodal reasoning and fine-grained understanding of audiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and perform reasoning-based segmentation. Extensive experiments show that OISA outperforms existing methods on OmniAVS and achieves competitive results on other related tasks."
    },
    {
      "title": "Repair-R1：修复前先进行更好的测试 (原标题: Repair-R1: Better Test Before Repair)",
      "link": "https://arxiv.org/abs/2507.22853",
      "pubDate": "Wed, 30 Jul 2025 13:24:05 GMT",
      "isoDate": "2025-07-30T13:24:05.000Z",
      "creator": "Haichuan Hu, Xiaochen Xie, Quanjun Zhang",
      "summary": "# Repair-R1：一种改进的自动化程序修复方法\n\n## 引言\n\n自动化程序修复（APR）旨在自动定位程序缺陷、生成补丁并验证修复。现有基于大型语言模型（LLM）的APR方法通常仅在推理阶段利用测试用例，采用迭代式方法：先执行修复，再通过测试验证。这种传统范式忽略了两个重要方面：\n\n*   测试用例在训练阶段的潜在贡献。\n*   在修复之前利用测试的可能性。\n\n## Repair-R1 方法\n\n为解决上述问题，本文提出了 **Repair-R1** 方法，其核心创新在于：\n\n*   将测试用例引入模型的训练阶段。\n*   将测试生成提前至修复之前。\n\n**Repair-R1 的工作流程：**\n\n1.  模型首先被要求生成能够区分缺陷行为的判别性测试用例。\n2.  然后，模型基于这些生成的测试用例执行修复。\n\n这种方法使模型能够更好地定位缺陷并理解缺陷的根本原因，从而显著提高修复效果。\n\n## 实现细节\n\n*   Repair-R1 使用了三种不同的骨干模型进行实现。\n*   采用强化学习（RL）来共同优化测试生成和错误修复过程。\n\n## 实验结果\n\n在四个广泛采用的基准测试上进行的实验结果表明了 Repair-R1 的优越性。与传统的（vanilla）模型相比，Repair-R1 取得了显著的性能提升：\n\n*   **修复成功率：** 提高 2.68% 至 48.29%。\n*   **测试生成成功率：** 提高 16.38% 至 53.28%。\n*   **测试覆盖率：** 提高 0.78% 至 53.96%。\n\n## 代码和权重\n\n该项目的代码和权重已公开发布。",
      "shortSummary": "Repair-R1 是一种改进的自动化程序修复（APR）方法，旨在解决现有基于大型语言模型（LLM）的 APR 方法中测试用例利用不足的问题。它将测试生成提前到修复之前，并把测试用例引入模型训练阶段。模型首先生成判别性测试用例，然后基于这些测试进行修复。实验结果表明，Repair-R1 在修复成功率、测试生成成功率和测试覆盖率方面均显著优于传统模型，有效提升了程序修复效果。",
      "translated_title": "Repair-R1：修复前先进行更好的测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "APR (Automated Program Repair) aims to automatically locate program defects, generate patches and validate the repairs. Existing techniques for APR are often combined with LLMs (Large Language Models), which leverages the code-related knowledge of LLMs to improve repair effectiveness. Current LLM-based APR methods typically utilize test cases only during the inference stage, adopting an iterative approach that performs repair first and validates it through test execution afterward. This conventional paradigm neglects two important aspects: the potential contribution of test cases in the training phase, and the possibility of leveraging testing prior to repair. To address this, we propose Repair-R1, which introduces test cases into the model's training phase and shifts test generation to precede repair. The model is required to first generate discriminative test cases that can distinguish defective behaviors, and then perform repair based on these tests. This enables the model to better locate defects and understand the underlying causes of defects, thereby improving repair effectiveness. We implement Repair-R1 with three different backbone models, using RL (reinforcement learning) to co-optimize test generation and bug repair. Experimental results on four widely adopted benchmarks demonstrate the superiority of Repair-R1. Specially, compared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to 48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage by 0.78\\% to 53.96\\%. We publish the code and weights at https://github.com/Tomsawyerhu/APR-RL and https://huggingface.co/tomhu/Qwen3-4B-RL-5000-step."
    },
    {
      "title": "ScreenCoder：通过模块化多模态代理推进前端自动化中的视觉到代码生成 (原标题: ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents)",
      "link": "https://arxiv.org/abs/2507.22827",
      "pubDate": "Wed, 30 Jul 2025 12:41:21 GMT",
      "isoDate": "2025-07-30T12:41:21.000Z",
      "creator": "Yilei Jiang, Yaozhi Zheng, Yuxuan Wan, Jiaming Han, Qunzhong Wang, Michael R. Lyu, Xiangyu Yue",
      "summary": "# ScreenCoder：通过模块化多模态代理推进前端自动化中的视觉到代码生成\n\n## 引言与背景\n将用户界面（UI）设计自动化转换为前端代码，对于加速软件开发和普及设计工作流程具有重要意义。尽管近期的大型语言模型（LLM）在文本到代码生成方面取得了进展，但许多现有方法仅依赖自然语言提示，这限制了它们在捕捉空间布局和视觉设计意图方面的有效性。然而，实际的UI开发本质上是多模态的，通常从视觉草图或模型开始。\n\n## ScreenCoder 框架：模块化多模态代理\n为解决现有方法的不足，本文引入了一个名为 **ScreenCoder** 的模块化多代理框架，该框架以三个可解释的阶段执行UI到代码的生成：\n\n*   **基础代理（Grounding Agent）**：利用视觉-语言模型（VLM）检测并标记UI组件。\n*   **规划代理（Planning Agent）**：利用前端工程先验知识构建分层布局。\n*   **生成代理（Generation Agent）**：通过自适应的基于提示的合成生成HTML/CSS代码。\n\n## 设计优势\n这种模块化设计相较于端到端的黑盒方法，显著提高了系统的鲁棒性、可解释性和保真度。\n\n## 可扩展数据引擎与模型训练\n*   **数据生成**：该框架被扩展为一个可扩展的数据引擎，能够自动生成大规模的图像-代码对（合成示例）。\n*   **模型微调与强化**：利用这些合成示例，研究人员对一个开源的视觉-语言模型（VLM）进行了微调和强化。\n\n## 实验结果与性能\n通过这种方法，模型在UI理解和代码质量方面取得了显著提升。广泛的实验表明，ScreenCoder 在布局准确性、结构连贯性和代码正确性方面均达到了最先进的性能。\n\n## 代码可用性\n相关代码已公开提供。",
      "shortSummary": "ScreenCoder 提出一个模块化多代理框架，用于将UI设计自动化转换为前端HTML/CSS代码。该框架分为基础、规划和生成三个阶段，利用视觉-语言模型理解UI，并结合前端工程知识构建布局。通过生成大规模合成数据来微调和强化模型，ScreenCoder 显著提升了UI理解和代码质量，并在布局准确性、结构连贯性和代码正确性方面达到了最先进水平，解决了现有文本到代码方法在视觉理解上的局限。",
      "translated_title": "ScreenCoder：通过模块化多模态代理推进前端自动化中的视觉到代码生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Automating the transformation of user interface (UI) designs into front-end code holds significant promise for accelerating software development and democratizing design workflows. While recent large language models (LLMs) have demonstrated progress in text-to-code generation, many existing approaches rely solely on natural language prompts, limiting their effectiveness in capturing spatial layout and visual design intent. In contrast, UI development in practice is inherently multimodal, often starting from visual sketches or mockups. To address this gap, we introduce a modular multi-agent framework that performs UI-to-code generation in three interpretable stages: grounding, planning, and generation. The grounding agent uses a vision-language model to detect and label UI components, the planning agent constructs a hierarchical layout using front-end engineering priors, and the generation agent produces HTML/CSS code via adaptive prompt-based synthesis. This design improves robustness, interpretability, and fidelity over end-to-end black-box methods. Furthermore, we extend the framework into a scalable data engine that automatically produces large-scale image-code pairs. Using these synthetic examples, we fine-tune and reinforce an open-source VLM, yielding notable gains in UI understanding and code quality. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in layout accuracy, structural coherence, and code correctness. Our code is made publicly available at https://github.com/leigest519/ScreenCoder."
    },
    {
      "title": "VL-Cogito：用于高级多模态推理的渐进式课程强化学习 (原标题: VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning)",
      "link": "https://arxiv.org/abs/2507.22607",
      "pubDate": "Wed, 30 Jul 2025 08:23:21 GMT",
      "isoDate": "2025-07-30T08:23:21.000Z",
      "creator": "Ruifeng Yuan, Chenghao Xiao, Sicong Leng, Jianyu Wang, Long Li, Weiwen Xu, Hou Pong Chan, Deli Zhao, Tingyang Xu, Zhongyu Wei, Hao Zhang, Yu Rong",
      "summary": "## VL-Cogito：用于高级多模态推理的渐进式课程强化学习\n\n### 研究背景与问题\n\n*   强化学习（RL）已被证实能有效增强大型语言模型（LLM）的推理能力。\n*   近期研究已将RL范式扩展到多模态推理任务中。\n*   然而，由于多模态任务固有的复杂性和多样性（尤其体现在语义内容和问题表述上），现有模型在不同领域和难度级别上的表现往往不稳定。\n\n### VL-Cogito 模型概述\n\n*   为解决上述局限性，研究人员提出了VL-Cogito，这是一种先进的多模态推理模型。\n*   该模型通过一种新颖的多阶段渐进式课程强化学习（PCuRL）框架进行训练。\n\n### 渐进式课程强化学习（PCuRL）框架\n\n*   PCuRL框架系统地引导模型逐步完成难度递增的任务，从而显著提升其在多样化多模态上下文中的推理能力。\n*   该框架引入了两项关键创新：\n    1.  **在线难度软加权机制：** 动态调整连续RL训练阶段的训练难度。\n    2.  **动态长度奖励机制：** 鼓励模型根据任务复杂性自适应地调节其推理路径长度，以平衡推理效率与正确性。\n\n### 实验结果与验证\n\n*   实验评估表明，VL-Cogito在涵盖数学、科学、逻辑和通用理解等主流多模态基准测试中，始终达到或超越了现有的面向推理的模型。\n*   这些结果充分验证了该方法的有效性。\n\n### 相关领域\n\n*   计算机视觉与模式识别 (cs.CV)\n*   人工智能 (cs.AI)\n*   计算与语言 (cs.CL)",
      "shortSummary": "VL-Cogito是一个先进的多模态推理模型，旨在解决现有模型在复杂多模态任务中表现不稳定的问题。它通过新颖的多阶段渐进式课程强化学习（PCuRL）框架进行训练。PCuRL引入了在线难度软加权和动态长度奖励机制，系统地提升模型推理能力。实验证明，VL-Cogito在数学、科学、逻辑和通用理解等多模态基准测试中，表现优于或媲美现有模型，验证了其有效性。",
      "translated_title": "VL-Cogito：用于高级多模态推理的渐进式课程强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement learning has proven its effectiveness in enhancing the reasoning capabilities of large language models. Recent research efforts have progressively extended this paradigm to multimodal reasoning tasks. Due to the inherent complexity and diversity of multimodal tasks, especially in semantic content and problem formulations, existing models often exhibit unstable performance across various domains and difficulty levels. To address these limitations, we propose VL-Cogito, an advanced multimodal reasoning model trained via a novel multi-stage Progressive Curriculum Reinforcement Learning (PCuRL) framework. PCuRL systematically guides the model through tasks of gradually increasing difficulty, substantially improving its reasoning abilities across diverse multimodal contexts. The framework introduces two key innovations: (1) an online difficulty soft weighting mechanism, dynamically adjusting training difficulty across successive RL training stages; and (2) a dynamic length reward mechanism, which encourages the model to adaptively regulate its reasoning path length according to task complexity, thus balancing reasoning efficiency with correctness. Experimental evaluations demonstrate that VL-Cogito consistently matches or surpasses existing reasoning-oriented models across mainstream multimodal benchmarks spanning mathematics, science, logic, and general understanding, validating the effectiveness of our approach."
    },
    {
      "title": "通过强化学习高效地对LLM进行差分隐私微调 (原标题: Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2507.22565",
      "pubDate": "Wed, 30 Jul 2025 06:46:53 GMT",
      "isoDate": "2025-07-30T06:46:53.000Z",
      "creator": "Afshin Khadangi, Amir Sartipi, Igor Tchappi, Ramin Bahmani, Gilbert Fridgen",
      "summary": "### RLDP：通过强化学习高效地对LLM进行差分隐私微调\n\n本文介绍了一种名为RLDP的新框架，旨在解决大型语言模型（LLM）在敏感数据上训练时数据隐私与模型效用之间的核心矛盾。\n\n#### 核心问题与现有挑战\n\n*   **隐私与效用冲突**：LLM在医疗等敏感语料库上训练时，数据隐私与模型效用之间的紧张关系是实际部署的主要瓶颈。\n*   **DP-SGD的局限性**：差分隐私随机梯度下降（DP-SGD）虽然能提供形式化的隐私保证，但代价高昂。它强制裁剪梯度并注入噪声，导致样本效率降低和最终准确性下降。\n*   **现有变体的不足**：尽管提出了许多DP-SGD变体来缓解这一权衡，但它们都存在一个共同的缺陷：其控制参数是硬编码、全局且对不断变化的优化过程不敏感。这迫使实践者要么为追求效用而过度消耗隐私预算，要么为满足隐私约束而接受平庸的模型。\n\n#### RLDP框架介绍\n\n*   **创新方法**：RLDP是首个将差分隐私优化本身视为一个闭环控制问题，并将其应用于现代深度强化学习（RL）的框架。\n*   **动态控制机制**：\n    *   RLDP能够持续感知学习动态的丰富统计信息。\n    *   它通过选择细粒度的**逐参数梯度裁剪阈值**以及注入高斯噪声的**幅度**来采取行动。\n*   **超策略训练**：一个软行动者-评论家（SAC）超策略在语言模型微调过程中在线训练。它从零开始学习如何以及何时将隐私预算分配到关键之处。\n\n#### 实验与成果\n\nRLDP在超过1,600次消融实验中，对多种LLM模型进行了评估，包括GPT2-small、Llama-1B、Llama-3B和Mistral-7B。\n\n*   **困惑度降低**：RLDP实现了1.3%至30.5%的困惑度降低（平均5.4%）。\n*   **下游效用提升**：平均下游效用增益为5.6%。\n*   **训练效率显著提升**：RLDP仅需13%至43%的梯度更新预算（平均加速71%）即可达到基线模型的最终效用。\n*   **隐私保障**：在实现上述性能提升的同时，RLDP严格遵守相同的（$\\epsilon$, $\\delta$）-DP契约。\n*   **抗攻击性**：RLDP对成员推断攻击和金丝雀提取攻击表现出相同或更低的敏感性。\n\n#### 结论\n\nRLDP通过将差分隐私优化转化为一个可学习的强化学习问题，显著提升了LLM在敏感数据上进行微调时的效用和效率，同时保持了强大的隐私保护。",
      "shortSummary": "RLDP是一种新颖的框架，通过将差分隐私（DP）优化视为强化学习（RL）的闭环控制问题，解决了LLM微调中隐私与效用之间的矛盾。它动态调整梯度裁剪和噪声注入，在线学习如何高效分配隐私预算。实验表明，RLDP在GPT2-small、Llama系列和Mistral-7B上显著降低了困惑度（平均5.4%），提升了下游效用（平均5.6%），并以更少的训练预算（平均加速71%）达到基线性能，同时保持了强大的DP保证和抗攻击性。",
      "translated_title": "通过强化学习高效地对LLM进行差分隐私微调",
      "images": [],
      "contentSource": "完整文章",
      "content": "The tension between data privacy and model utility has become the defining bottleneck for the practical deployment of large language models (LLMs) trained on sensitive corpora including healthcare. Differentially private stochastic gradient descent (DP-SGD) guarantees formal privacy, yet it does so at a pronounced cost: gradients are forcibly clipped and perturbed with noise, degrading sample efficiency and final accuracy. Numerous variants have been proposed to soften this trade-off, but they all share a handicap: their control knobs are hard-coded, global, and oblivious to the evolving optimization landscape. Consequently, practitioners are forced either to over-spend privacy budget in pursuit of utility, or to accept mediocre models in order to stay within privacy constraints. We present RLDP, the first framework to cast DP optimization itself as a closed-loop control problem amenable to modern deep reinforcement learning (RL). RLDP continuously senses rich statistics of the learning dynamics and acts by selecting fine-grained per parameter gradient-clipping thresholds as well as the magnitude of injected Gaussian noise. A soft actor-critic (SAC) hyper-policy is trained online during language model fine-tuning; it learns, from scratch, how to allocate the privacy budget where it matters and when it matters. Across more than 1,600 ablation experiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers perplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream utility gain. RLDP reaches each baseline's final utility after only 13-43% of the gradient-update budget (mean speed-up 71%), all while honoring the same (epsilon, delta)-DP contract and exhibiting equal or lower susceptibility to membership-inference and canary-extraction attacks."
    },
    {
      "title": "Falcon-H1：重新定义效率和性能的混合头语言模型系列 (原标题: Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance)",
      "link": "https://arxiv.org/abs/2507.22448",
      "pubDate": "Wed, 30 Jul 2025 03:55:33 GMT",
      "isoDate": "2025-07-30T03:55:33.000Z",
      "creator": "Jingwei Zuo, Maksim Velikanov, Ilyas Chahed, Younes Belkada, Dhia Eddine Rhayem, Guillaume Kunsch, Hakim Hacid, Hamza Yous, Brahim Farhat, Ibrahim Khadraoui, Mugariya Farooq, Giulia Campesan, Ruxandra Cojocaru, Yasser Djilali, Shi Hu, Iheb Chaabane, Puneesh Khanna, Mohamed El Amine Seddik, Ngoc Dung Huynh, Phuc Le Khac, Leen AlQadi, Billel Mokeddem, Mohamed Chami, Abdalgader Abubaker, Mikhail Lubinets, Kacper Piskorski, Slim Frikha",
      "summary": "## Falcon-H1：重新定义效率和性能的混合头语言模型系列\n\n### 简介\n\nFalcon-H1 是一个全新的大型语言模型（LLM）系列，其核心在于采用混合架构设计，旨在优化性能和效率，以适应各种不同的用例。与之前纯粹基于 Transformer 或 Mamba 架构的 Falcon 模型不同，Falcon-H1 引入了一种并行的混合方法，将基于 Transformer 的注意力机制与状态空间模型（SSM）相结合。SSM 以其卓越的长上下文记忆能力和计算效率而闻名。\n\n### 架构创新与开发策略\n\n*   **混合架构：** Falcon-H1 的核心创新在于其混合架构，它并行结合了 Transformer 的注意力机制和 SSM 的优势。这种设计旨在克服单一架构的局限性，同时提升模型的长上下文处理能力和计算效率。\n*   **系统性重访：** 团队系统性地重新审视了模型设计、数据策略和训练动态，挑战了该领域的传统实践，以实现性能和效率的突破。\n\n### 模型配置与可用性\n\nFalcon-H1 系列提供了多种配置，以满足不同需求：\n\n*   **参数规模：** 包括 0.5B、1.5B、1.5B-deep、3B、7B 和 34B 参数的基础模型和指令微调模型。\n*   **量化版本：** 还提供了量化后的指令微调模型。\n*   **可用性：** 总计超过 30 个检查点已在 Hugging Face Hub 上发布，方便研究人员和开发者访问和使用。\n\n### 卓越的性能与效率\n\nFalcon-H1 模型展现了最先进的性能，并在参数和训练效率方面表现出色：\n\n*   **旗舰模型 Falcon-H1-34B：** 性能与参数规模高达 70B 的模型（如 Qwen3-32B、Qwen2.5-72B 和 Llama3.3-70B）相当或超越，但使用的参数和数据量更少。\n*   **小型模型表现：** \n    *   Falcon-H1-1.5B-Deep 的性能可与当前领先的 7B-10B 模型相媲美。\n    *   Falcon-H1-0.5B 的性能与 2024 年典型的 7B 模型相当。\n*   **多领域能力：** 这些模型在推理、数学、多语言任务、指令遵循和科学知识方面表现卓越。\n\n### 应用范围与开放性\n\n*   **上下文支持：** Falcon-H1 支持高达 256K 的上下文令牌，使其能够处理极长的文本输入。\n*   **语言支持：** 支持 18 种语言，适用于广泛的全球应用。\n*   **开源许可：** 所有模型均在宽松的开源许可下发布，这体现了项目团队致力于推动人工智能研究的开放性和影响力。\n\n### 总结\n\nFalcon-H1 系列通过其创新的混合架构、卓越的性能和高效的资源利用，为大型语言模型领域树立了新的标杆。其广泛的配置和开放的可用性，使其成为各种应用场景的强大工具，并有望推动未来 AI 发展。",
      "shortSummary": "Falcon-H1 是一个创新的大型语言模型系列，采用混合架构（Transformer与SSM结合），旨在提升效率和性能。该系列模型参数范围从0.5B到34B，其中旗舰模型Falcon-H1-34B在参数和数据量更少的情况下，性能可媲美甚至超越70B级别的模型。Falcon-H1支持长达256K的上下文和18种语言，并在推理、数学、多语言任务等多个领域表现出色。所有模型均以开放源代码许可发布，致力于推动可访问和有影响力的AI研究。",
      "translated_title": "Falcon-H1：重新定义效率和性能的混合头语言模型系列",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this report, we introduce Falcon-H1, a new series of large language models (LLMs) featuring hybrid architecture designs optimized for both high performance and efficiency across diverse use cases. Unlike earlier Falcon models built solely on Transformer or Mamba architectures, Falcon-H1 adopts a parallel hybrid approach that combines Transformer-based attention with State Space Models (SSMs), known for superior long-context memory and computational efficiency. We systematically revisited model design, data strategy, and training dynamics, challenging conventional practices in the field. Falcon-H1 is released in multiple configurations, including base and instruction-tuned variants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized instruction-tuned models are also available, totaling over 30 checkpoints on Hugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and exceptional parameter and training efficiency. The flagship Falcon-H1-34B matches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B, and Llama3.3-70B, while using fewer parameters and less data. Smaller models show similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B models, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024. These models excel across reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge. With support for up to 256K context tokens and 18 languages, Falcon-H1 is suitable for a wide range of applications. All models are released under a permissive open-source license, underscoring our commitment to accessible and impactful AI research."
    },
    {
      "title": "BANG：通过生成式爆炸动力学分割3D资产 (原标题: BANG: Dividing 3D Assets via Generative Exploded Dynamics)",
      "link": "https://arxiv.org/abs/2507.21493",
      "pubDate": "Tue, 29 Jul 2025 00:21:21 GMT",
      "isoDate": "2025-07-29T00:21:21.000Z",
      "creator": "Longwen Zhang, Qixuan Zhang, Haoran Jiang, Yinuo Bai, Wei Yang, Lan Xu, Jingyi Yu",
      "summary": "## BANG：通过生成式爆炸动力学分割3D资产\n\n### 引言\n3D创作是人类独特的优势，源于我们通过眼睛、思维和双手对物体进行解构和重组的能力。然而，当前的3D设计工具难以复制这一自然过程，需要大量的艺术专业知识和手动劳动。\n\n### BANG方法概述\n本文介绍了一种名为BANG的新型生成式方法，它弥合了3D生成与推理之间的鸿沟，实现了3D对象的直观灵活的零件级分解。\n\n### 核心机制：“生成式爆炸动力学”\nBANG的核心是“生成式爆炸动力学”，它为输入几何体创建一系列平滑的爆炸状态序列。这一过程逐步分离零件，同时保持其几何和语义连贯性。\n\n### 技术实现\n*   **基础模型：** BANG利用一个预训练的大规模潜在扩散模型。\n*   **微调与控制：** 该模型通过一个轻量级的爆炸视图适配器进行微调，以实现对分解过程的精确控制。\n*   **时间一致性：** 它还包含一个时间注意力模块，以确保平滑的过渡和跨时间的一致性。\n\n### 增强控制与交互\n*   **空间提示：** BANG通过空间提示（如边界框和表面区域）增强了控制，允许用户指定要分解的零件以及分解方式。\n*   **多模态扩展：** 这种交互可以与GPT-4等多模态模型结合，实现2D到3D的操作，从而提供更直观和富有创意的设计工作流程。\n\n### 应用与能力\nBANG的能力延伸到多个领域：\n*   **零件级几何生成：** 能够生成详细的零件级几何结构。\n*   **功能描述关联：** 将零件与功能描述相关联。\n*   **组件感知工作流：** 促进组件感知的3D创建和制造工作流程。\n*   **3D打印：** 在3D打印中，BANG可以生成可分离的零件，便于打印和重新组装。\n\n### 结论\n本质上，BANG实现了从想象概念到详细3D资产的无缝转换，为3D创作提供了一个与人类直觉相符的新视角。",
      "shortSummary": "BANG是一种创新的生成式方法，通过“生成式爆炸动力学”实现3D对象的直观零件级分解。它利用微调的潜在扩散模型和时间注意力模块，创建平滑的零件分离序列，同时保持几何和语义连贯性。用户可通过空间提示和多模态模型进行精确控制。BANG在零件几何生成、功能描述关联、组件感知制造及3D打印等领域具有广泛应用，旨在简化3D创作流程，使其更符合人类直觉。",
      "translated_title": "BANG：通过生成式爆炸动力学分割3D资产",
      "images": [],
      "contentSource": "完整文章",
      "content": "3D creation has always been a unique human strength, driven by our ability to deconstruct and reassemble objects using our eyes, mind and hand. However, current 3D design tools struggle to replicate this natural process, requiring considerable artistic expertise and manual labor. This paper introduces BANG, a novel generative approach that bridges 3D generation and reasoning, allowing for intuitive and flexible part-level decomposition of 3D objects. At the heart of BANG is \"Generative Exploded Dynamics\", which creates a smooth sequence of exploded states for an input geometry, progressively separating parts while preserving their geometric and semantic coherence.   BANG utilizes a pre-trained large-scale latent diffusion model, fine-tuned for exploded dynamics with a lightweight exploded view adapter, allowing precise control over the decomposition process. It also incorporates a temporal attention module to ensure smooth transitions and consistency across time. BANG enhances control with spatial prompts, such as bounding boxes and surface regions, enabling users to specify which parts to decompose and how. This interaction can be extended with multimodal models like GPT-4, enabling 2D-to-3D manipulations for more intuitive and creative workflows.   The capabilities of BANG extend to generating detailed part-level geometry, associating parts with functional descriptions, and facilitating component-aware 3D creation and manufacturing workflows. Additionally, BANG offers applications in 3D printing, where separable parts are generated for easy printing and reassembly. In essence, BANG enables seamless transformation from imaginative concepts to detailed 3D assets, offering a new perspective on creation that resonates with human intuition."
    },
    {
      "title": "Rep-MTL：释放表示层任务显著性在多任务学习中的潜力 (原标题: Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning)",
      "link": "https://arxiv.org/abs/2507.21049",
      "pubDate": "Mon, 28 Jul 2025 13:59:28 GMT",
      "isoDate": "2025-07-28T13:59:28.000Z",
      "creator": "Zedong Wang, Siyuan Li, Dan Xu",
      "summary": "Rep-MTL：释放表示层任务显著性在多任务学习中的潜力\n\n*   **现有问题与局限**\n    *   尽管多任务学习（MTL）有望利用任务间的互补知识，但现有的多任务优化（MTO）技术主要通过以优化器为中心的损失缩放和梯度操纵策略来解决任务冲突。\n    *   这些方法未能提供持续的性能提升，并且很少探索共享表示空间在促进任务间互补性方面的潜力。\n\n*   **Rep-MTL的核心思想**\n    *   本文提出，共享表示空间是任务交互的自然发生地，蕴含丰富信息，可作为现有优化器的补充，尤其在促进任务间互补性方面。\n    *   基于此直觉，引入了Rep-MTL方法，该方法利用“表示层任务显著性”来量化任务特定优化与共享表示学习之间的交互。\n\n*   **Rep-MTL的工作机制与目标**\n    *   Rep-MTL通过基于熵的惩罚和样本级跨任务对齐来引导这些表示层任务显著性。\n    *   其目标是通过维持个体任务的有效训练（而非单纯解决冲突）来缓解负迁移，同时明确促进互补信息的共享。\n\n*   **实验验证与结果**\n    *   研究在四个具有挑战性的MTL基准测试上进行了实验，这些基准测试涵盖了任务转移和域转移场景。\n    *   结果显示，Rep-MTL即使与基本的等权重策略结合，也能实现具有竞争力的性能提升，并展现出良好的效率。\n    *   除了标准性能指标，幂律指数分析（Power Law exponent analysis）也进一步证明了Rep-MTL在平衡任务特定学习和跨任务共享方面的有效性。\n\n*   **项目与发表信息**\n    *   该研究已被ICCV 2025接受并被评为亮点论文。",
      "shortSummary": "Rep-MTL是一种新的多任务学习方法，旨在解决现有优化技术在任务冲突解决上的局限。它关注共享表示空间，通过量化和引导“表示层任务显著性”，促进任务间互补性并缓解负迁移。Rep-MTL通过基于熵的惩罚和样本级对齐，在保持个体任务有效训练的同时，明确促进信息共享。实验证明，Rep-MTL即使采用简单权重策略，也能实现有竞争力的性能和效率，有效平衡任务学习与跨任务共享。",
      "translated_title": "Rep-MTL：释放表示层任务显著性在多任务学习中的潜力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite the promise of Multi-Task Learning in leveraging complementary knowledge across tasks, existing multi-task optimization (MTO) techniques remain fixated on resolving conflicts via optimizer-centric loss scaling and gradient manipulation strategies, yet fail to deliver consistent gains. In this paper, we argue that the shared representation space, where task interactions naturally occur, offers rich information and potential for operations complementary to existing optimizers, especially for facilitating the inter-task complementarity, which is rarely explored in MTO. This intuition leads to Rep-MTL, which exploits the representation-level task saliency to quantify interactions between task-specific optimization and shared representation learning. By steering these saliencies through entropy-based penalization and sample-wise cross-task alignment, Rep-MTL aims to mitigate negative transfer by maintaining the effective training of individual tasks instead pure conflict-solving, while explicitly promoting complementary information sharing. Experiments are conducted on four challenging MTL benchmarks covering both task-shift and domain-shift scenarios. The results show that Rep-MTL, even paired with the basic equal weighting policy, achieves competitive performance gains with favorable efficiency. Beyond standard performance metrics, Power Law exponent analysis demonstrates Rep-MTL's efficacy in balancing task-specific learning and cross-task sharing. The project page is available at HERE."
    },
    {
      "title": "自进化智能体综述：通往人工超级智能之路 (原标题: A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence)",
      "link": "https://arxiv.org/abs/2507.21046",
      "pubDate": "Mon, 28 Jul 2025 13:59:05 GMT",
      "isoDate": "2025-07-28T13:59:05.000Z",
      "creator": "Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, Hongru Wang, Han Xiao, Yuhang Zhou, Shaokun Zhang, Jiayi Zhang, Jinyu Xiang, Yixiong Fang, Qiwen Zhao, Dongrui Liu, Qihan Ren, Cheng Qian, Zhenghailong Wang, Minda Hu, Huazheng Wang, Qingyun Wu, Heng Ji, Mengdi Wang",
      "summary": "### 自进化智能体综述：通往人工超级智能之路\n\n**1. 背景与问题**\n\n*   **大型语言模型（LLMs）的局限性**：尽管LLMs展现出强大的能力，但其本质是静态的。它们无法根据新任务、不断演变的知识领域或动态交互上下文来适应或调整其内部参数。\n*   **关键瓶颈**：随着LLMs越来越多地部署在开放、交互式的环境中，这种静态特性已成为一个关键瓶颈，亟需能够实时自适应推理、行动和进化的智能体。\n\n**2. 研究范式转变**\n\n*   研究重心正从扩展静态模型转向开发自进化智能体。\n*   这激发了对能够从数据、交互和经验中持续学习和适应的架构和方法日益增长的兴趣。\n\n**3. 本综述的结构与内容**\n\n本综述首次对自进化智能体进行了系统而全面的回顾，围绕三个基础维度进行组织：\n\n*   **进化什么 (What to Evolve)**：\n    *   探讨智能体组件的进化机制。\n    *   涵盖模型、记忆、工具和架构等方面的进化。\n*   **何时进化 (When to Evolve)**：\n    *   根据阶段对适应方法进行分类。\n    *   包括测试时内部（intra-test-time）和测试时之间（inter-test-time）的适应。\n*   **如何进化 (How to Evolve)**：\n    *   分析指导进化适应的算法和架构设计。\n    *   例如，使用标量奖励、文本反馈，以及在单智能体和多智能体系统中的应用。\n\n**4. 其他分析与展望**\n\n*   **评估与基准**：分析了为自进化智能体量身定制的评估指标和基准。\n*   **应用领域**：强调了自进化智能体在编码、教育和医疗保健等领域的潜在应用。\n*   **挑战与研究方向**：识别了安全、可扩展性和协同进化动力学方面的关键挑战和未来的研究方向。\n\n**5. 综述的意义**\n\n*   通过提供一个理解和设计自进化智能体的结构化框架，本综述为推进研究和实际部署中的自适应智能体系统建立了路线图。\n*   最终，它为实现人工超级智能（ASI）铺平了道路，即智能体能够自主进化，并在广泛的任务中达到或超越人类水平的智能。",
      "shortSummary": "本综述首次系统回顾了自进化智能体，旨在解决大型语言模型（LLMs）静态、无法适应的局限性。文章围绕“进化什么、何时进化、如何进化”三个核心维度，深入探讨了智能体组件的进化机制、适应方法和算法设计。同时，综述分析了评估指标、应用领域及面临的挑战。其目标是为自适应智能体系统提供结构化框架和发展路线图，最终推动人工超级智能（ASI）的实现。",
      "translated_title": "自进化智能体综述：通往人工超级智能之路",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) have demonstrated strong capabilities but remain fundamentally static, unable to adapt their internal parameters to novel tasks, evolving knowledge domains, or dynamic interaction contexts. As LLMs are increasingly deployed in open-ended, interactive environments, this static nature has become a critical bottleneck, necessitating agents that can adaptively reason, act, and evolve in real time. This paradigm shift -- from scaling static models to developing self-evolving agents -- has sparked growing interest in architectures and methods enabling continual learning and adaptation from data, interactions, and experiences. This survey provides the first systematic and comprehensive review of self-evolving agents, organized around three foundational dimensions -- what to evolve, when to evolve, and how to evolve. We examine evolutionary mechanisms across agent components (e.g., models, memory, tools, architecture), categorize adaptation methods by stages (e.g., intra-test-time, inter-test-time), and analyze the algorithmic and architectural designs that guide evolutionary adaptation (e.g., scalar rewards, textual feedback, single-agent and multi-agent systems). Additionally, we analyze evaluation metrics and benchmarks tailored for self-evolving agents, highlight applications in domains such as coding, education, and healthcare, and identify critical challenges and research directions in safety, scalability, and co-evolutionary dynamics. By providing a structured framework for understanding and designing self-evolving agents, this survey establishes a roadmap for advancing adaptive agentic systems in both research and real-world deployments, ultimately shedding lights to pave the way for the realization of Artificial Super Intelligence (ASI), where agents evolve autonomously, performing at or beyond human-level intelligence across a wide array of tasks."
    },
    {
      "title": "重建四维空间智能：一项综述 (原标题: Reconstructing 4D Spatial Intelligence: A Survey)",
      "link": "https://arxiv.org/abs/2507.21045",
      "pubDate": "Mon, 28 Jul 2025 13:59:02 GMT",
      "isoDate": "2025-07-28T13:59:02.000Z",
      "creator": "Yukang Cao, Jiahao Lu, Zhisheng Huang, Zhuowei Shen, Chengfeng Zhao, Fangzhou Hong, Zhaoxi Chen, Xin Li, Wenping Wang, Yuan Liu, Ziwei Liu",
      "summary": "### 重建四维空间智能：一项综述\n\n**引言与背景**\n\n从视觉观测中重建四维空间智能是计算机视觉领域的核心且极具挑战性的任务，具有广泛的实际应用。这些应用涵盖了从娱乐领域（如电影，侧重于重建基本视觉元素）到具身人工智能（强调交互建模和物理真实性）等多个方面。\n\n**研究动机**\n\n近年来，随着三维表示和深度学习架构的快速发展，该领域取得了显著进步，使得现有综述的范围已无法涵盖最新进展。此外，现有综述很少对四维场景重建的层次结构进行全面分析。为了弥补这一空白，本文提出了一种新的视角来组织现有方法。\n\n**四维空间智能的五个渐进层次**\n\n本综述将现有方法组织成五个渐进的四维空间智能层次，为理解和分类该领域的研究提供了清晰的框架：\n\n1.  **第一层：低级三维属性重建**\n    *   此层次关注重建基础的三维属性，例如深度图、姿态和点云图。这是构建更高级四维智能的基础。\n\n2.  **第二层：三维场景组件重建**\n    *   在这一层次，研究重点是重建三维场景中的独立组件，包括物体、人类和结构。这涉及到对场景中各个实体的识别和三维建模。\n\n3.  **第三层：四维动态场景重建**\n    *   此层次旨在重建动态的四维场景，即随着时间变化的三维场景。这包括捕捉运动、形变和时间演变。\n\n4.  **第四层：场景组件间交互建模**\n    *   在这一更高级的层次上，研究关注的是场景中各个组件之间的交互行为建模。这可能涉及理解物体之间的物理接触、人类与环境的互动等。\n\n5.  **第五层：物理定律与约束的融入**\n    *   最高层次的四维空间智能涉及将物理定律和约束融入重建过程。这有助于生成更真实、更符合物理规律的四维场景，并能预测未来的行为。\n\n**挑战与未来方向**\n\n本综述最后讨论了每个层次面临的关键挑战，并指出了推动四维空间智能迈向更丰富层次的有前景的研究方向。\n\n**项目页面**\n\n为了跟踪正在进行的进展，作者维护了一个最新的项目页面。",
      "shortSummary": "本综述提出了一种重建四维空间智能的新视角，将现有方法分为五个渐进层次：从低级三维属性、三维场景组件，到四维动态场景、组件间交互建模，直至融入物理定律。文章讨论了各层次的挑战并指出了未来方向，旨在应对计算机视觉领域中从视觉观测重建四维空间智能的复杂任务，并弥补现有综述的不足。",
      "translated_title": "重建四维空间智能：一项综述",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reconstructing 4D spatial intelligence from visual observations has long been a central yet challenging task in computer vision, with broad real-world applications. These range from entertainment domains like movies, where the focus is often on reconstructing fundamental visual elements, to embodied AI, which emphasizes interaction modeling and physical realism. Fueled by rapid advances in 3D representations and deep learning architectures, the field has evolved quickly, outpacing the scope of previous surveys. Additionally, existing surveys rarely offer a comprehensive analysis of the hierarchical structure of 4D scene reconstruction. To address this gap, we present a new perspective that organizes existing methods into five progressive levels of 4D spatial intelligence: (1) Level 1 -- reconstruction of low-level 3D attributes (e.g., depth, pose, and point maps); (2) Level 2 -- reconstruction of 3D scene components (e.g., objects, humans, structures); (3) Level 3 -- reconstruction of 4D dynamic scenes; (4) Level 4 -- modeling of interactions among scene components; and (5) Level 5 -- incorporation of physical laws and constraints. We conclude the survey by discussing the key challenges at each level and highlighting promising directions for advancing toward even richer levels of 4D spatial intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence."
    },
    {
      "title": "GenoMAS：一个通过代码驱动的基因表达分析实现科学发现的多智能体框架 (原标题: GenoMAS: A Multi-Agent Framework for Scientific Discovery via Code-Driven Gene Expression Analysis)",
      "link": "https://arxiv.org/abs/2507.21035",
      "pubDate": "Mon, 28 Jul 2025 13:55:08 GMT",
      "isoDate": "2025-07-28T13:55:08.000Z",
      "creator": "Haoyang Liu, Yijiang Li, Haohan Wang",
      "summary": "## GenoMAS：通过代码驱动的基因表达分析实现科学发现的多智能体框架\n\n### 1. 引言与背景\n\n基因表达分析是生物医学发现的关键，但从原始转录组数据中提取见解极具挑战性。这主要归因于：\n\n*   **数据复杂性**：涉及多个大型、半结构化的文件。\n*   **专业知识需求**：需要广泛的领域专业知识。\n\n当前的自动化方法存在局限性：\n\n*   **僵化工作流**：在边缘情况下容易崩溃。\n*   **完全自主智能体**：缺乏严谨科学探究所需的精确性。\n\n### 2. GenoMAS 框架概述\n\nGenoMAS 提出了一种不同的方法，它是一个基于大型语言模型（LLM）的科学家团队，旨在整合结构化工作流的可靠性与自主智能体的适应性。\n\n*   **核心理念**：结合了结构化工作流的可靠性和自主智能体的适应性。\n*   **智能体编排**：GenoMAS 协调六个专门的 LLM 智能体，通过类型化消息传递协议进行通信。\n*   **共享分析画布**：每个智能体都为共享的分析画布贡献互补的优势。\n\n### 3. 引导式规划框架\n\nGenoMAS 的核心是一个引导式规划框架，其工作机制如下：\n\n*   **任务分解**：编程智能体将高级任务指南分解为“行动单元”（Action Units）。\n*   **动态决策**：在每个关键节点，智能体可以选择：\n    *   **前进 (advance)**\n    *   **修订 (revise)**\n    *   **绕过 (bypass)**\n    *   **回溯 (backtrack)**\n*   **适应性与连贯性**：这种机制在保持逻辑连贯性的同时，能够优雅地适应基因组数据的特殊性。\n\n### 4. 性能与成果\n\nGenoMAS 在 GenoTEX 基准测试上进行了评估，并取得了显著成果：\n\n*   **数据预处理**：\n    *   复合相似性相关性（Composite Similarity Correlation）达到 **89.13%**。\n    *   超越现有最佳技术 **10.61%**。\n*   **基因识别**：\n    *   F1 分数达到 **60.48%**。\n    *   超越现有最佳技术 **16.85%**。\n\n除了量化指标，GenoMAS 还展现了其在生物学发现方面的能力：\n\n*   **生物学关联**：能够发现经文献证实的、生物学上合理的基因-表型关联。\n*   **混杂因素调整**：在发现这些关联时，能够调整潜在的混杂因素。\n\n### 5. 可用性\n\n相关代码已公开提供。",
      "shortSummary": "GenoMAS 是一个基于大型语言模型（LLM）的多智能体框架，旨在通过代码驱动的基因表达分析实现科学发现。它通过协调六个专业 LLM 智能体和引导式规划框架，克服了现有自动化方法的局限性。GenoMAS 在数据预处理和基因识别方面显著超越了现有技术，并在GenoTEX基准测试中表现出色，同时能发现生物学上合理的基因-表型关联。代码已公开。",
      "translated_title": "GenoMAS：一个通过代码驱动的基因表达分析实现科学发现的多智能体框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "Gene expression analysis holds the key to many biomedical discoveries, yet extracting insights from raw transcriptomic data remains formidable due to the complexity of multiple large, semi-structured files and the need for extensive domain expertise. Current automation approaches are often limited by either inflexible workflows that break down in edge cases or by fully autonomous agents that lack the necessary precision for rigorous scientific inquiry. GenoMAS charts a different course by presenting a team of LLM-based scientists that integrates the reliability of structured workflows with the adaptability of autonomous agents. GenoMAS orchestrates six specialized LLM agents through typed message-passing protocols, each contributing complementary strengths to a shared analytic canvas. At the heart of GenoMAS lies a guided-planning framework: programming agents unfold high-level task guidelines into Action Units and, at each juncture, elect to advance, revise, bypass, or backtrack, thereby maintaining logical coherence while bending gracefully to the idiosyncrasies of genomic data.   On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation of 89.13% for data preprocessing and an F_1 of 60.48% for gene identification, surpassing the best prior art by 10.61% and 16.85% respectively. Beyond metrics, GenoMAS surfaces biologically plausible gene-phenotype associations corroborated by the literature, all while adjusting for latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS."
    },
    {
      "title": "GPT-IMAGE-EDIT-1.5M：一个百万级、GPT生成的图像数据集 (原标题: GPT-IMAGE-EDIT-1.5M: A Million-Scale, GPT-Generated Image Dataset)",
      "link": "https://arxiv.org/abs/2507.21033",
      "pubDate": "Mon, 28 Jul 2025 13:54:04 GMT",
      "isoDate": "2025-07-28T13:54:04.000Z",
      "creator": "Yuhan Wang, Siwei Yang, Bingchen Zhao, Letian Zhang, Qing Liu, Yuyin Zhou, Cihang Xie",
      "summary": "## GPT-IMAGE-EDIT-1.5M：一个百万级、GPT生成的图像数据集\n\n### 摘要\n\n近期，大型多模态模型（如GPT-4o）在指令引导的图像编辑方面树立了新标准，但其专有性质和训练数据对开源研究构成了显著障碍。为弥合这一差距，研究人员引入了 **GPT-IMAGE-EDIT-1.5M**，这是一个公开可用的大规模图像编辑语料库。\n\n### 数据集详情\n\n*   **规模**：包含超过150万个高质量的三元组（指令、源图像、编辑后的图像）。\n*   **构建方法**：系统地利用GPT-4o的强大功能，统一并优化了三个流行的图像编辑数据集：OmniEdit、HQ-Edit和UltraEdit。\n    1.  **图像再生**：重新生成输出图像，以提高视觉质量和指令对齐性。\n    2.  **提示词重写**：选择性地重写提示词，以提高语义清晰度。\n\n### 验证与性能\n\n为了验证该数据集的有效性，研究人员在GPT-IMAGE-EDIT-1.5M上对先进的开源模型进行了微调。实证结果令人鼓舞：\n\n*   **模型表现**：经过微调的FluxKontext模型在综合基准测试中取得了极具竞争力的性能：\n    *   GEdit-EN：7.24\n    *   ImgEdit-Full：3.80\n    *   Complex-Edit：8.78\n*   **优势**：该模型展现出更强的指令遵循能力、更高的感知质量，同时保持了图像的身份。\n*   **对比**：这些分数显著超越了所有先前发布的开源方法，并大幅缩小了与领先专有模型之间的差距。\n\n### 展望\n\n研究人员希望GPT-IMAGE-EDIT-1.5M的全面发布能够促进指令引导图像编辑领域的进一步开源研究。",
      "shortSummary": "GPT-IMAGE-EDIT-1.5M是一个新发布的百万级图像编辑数据集，旨在弥补开源研究在指令引导图像编辑领域的空白。该数据集包含超过150万个高质量三元组，通过GPT-4o对现有数据集进行优化构建。在GPT-IMAGE-EDIT-1.5M上微调的开源模型（如FluxKontext）表现出卓越性能，超越了现有开源方法，并显著缩小了与专有模型的差距，有望推动开源图像编辑研究的发展。",
      "translated_title": "GPT-IMAGE-EDIT-1.5M：一个百万级、GPT生成的图像数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in large multimodal models like GPT-4o have set a new standard for high-fidelity, instruction-guided image editing. However, the proprietary nature of these models and their training data creates a significant barrier for open-source research. To bridge this gap, we introduce GPT-IMAGE-EDIT-1.5M, a publicly available, large-scale image-editing corpus containing more than 1.5 million high-quality triplets (instruction, source image, edited image). We systematically construct this dataset by leveraging the versatile capabilities of GPT-4o to unify and refine three popular image-editing datasets: OmniEdit, HQ-Edit, and UltraEdit. Specifically, our methodology involves 1) regenerating output images to enhance visual quality and instruction alignment, and 2) selectively rewriting prompts to improve semantic clarity. To validate the efficacy of our dataset, we fine-tune advanced open-source models on GPT-IMAGE-EDIT-1.5M. The empirical results are exciting, e.g., the fine-tuned FluxKontext achieves highly competitive performance across a comprehensive suite of benchmarks, including 7.24 on GEdit-EN, 3.80 on ImgEdit-Full, and 8.78 on Complex-Edit, showing stronger instruction following and higher perceptual quality while maintaining identity. These scores markedly exceed all previously published open-source methods and substantially narrow the gap to leading proprietary models. We hope the full release of GPT-IMAGE-EDIT-1.5M can help to catalyze further open research in instruction-guided image editing."
    },
    {
      "title": "SmallThinker：一族为本地部署原生训练的高效大型语言模型 (原标题: SmallThinker: A Family of Efficient Large Language Models Natively Trained for Local Deployment)",
      "link": "https://arxiv.org/abs/2507.20984",
      "pubDate": "Mon, 28 Jul 2025 12:45:14 GMT",
      "isoDate": "2025-07-28T12:45:14.000Z",
      "creator": "Yixin Song, Zhenliang Xue, Dongliang Wei, Feiyang Chen, Jianxiang Gao, Junchen Liu, Hangyu Liang, Guangshuo Qin, Chengrong Tian, Bo Wen, Longyu Zhao, Xinrui Zheng, Zeyu Mi, Haibo Chen",
      "summary": "SmallThinker：为本地部署而生的LLM家族\n\n本文介绍了一个名为SmallThinker的新型大型语言模型（LLM）家族，该家族旨在克服当前前沿LLM在本地设备上部署的挑战。\n\n**背景与挑战**\n当前领先的LLM虽然能力强大，但其部署主要依赖于GPU驱动的云基础设施。这是因为它们对计算能力、内存和存储有极高的要求。传统的本地部署方法通常是对现有云端模型进行压缩，但这并未从根本上解决本地设备（计算能力弱、内存有限、存储速度慢）的固有约束。\n\n**SmallThinker 的创新方法**\nSmallThinker与众不同之处在于，它并非简单地适应现有模型，而是从零开始为本地设备的原生部署而设计。其核心创新在于一种“部署感知型架构”，将这些约束转化为设计原则：\n\n*   **计算效率：两级稀疏结构**\n    为了大幅降低计算需求而不牺牲模型容量，SmallThinker引入了一种两级稀疏结构。该结构结合了细粒度的专家混合（Mixture-of-Experts, MoE）与稀疏前馈网络。\n*   **克服I/O瓶颈：预注意力路由器**\n    针对慢速存储带来的I/O瓶颈，SmallThinker设计了一个“预注意力路由器”。这个路由器与协同设计的推理引擎协同工作，能够在计算注意力时预取专家参数，从而有效隐藏了通常会严重影响设备上推理的存储延迟。\n*   **内存效率：NoPE-RoPE混合稀疏注意力**\n    为了提高内存效率，模型采用了NoPE-RoPE混合稀疏注意力机制，显著减少了键值（KV）缓存的需求。\n\n**模型发布与卓越性能**\n研究团队发布了SmallThinker家族的两个模型：SmallThinker-4B-A0.6B 和 SmallThinker-21B-A3B。这些模型取得了最先进的性能分数，甚至超越了某些更大的LLM。\n\n值得注意的是，SmallThinker的协同设计系统在很大程度上消除了对昂贵GPU硬件的需求。在Q4_0量化下，这两个模型都能在普通消费级CPU上实现每秒超过20个token的生成速度，同时分别仅消耗1GB和8GB的内存。\n\n**可用性**\nSmallThinker模型已公开发布。",
      "shortSummary": "SmallThinker是一族为本地部署原生设计的高效大型语言模型。它通过引入两级稀疏结构、预注意力路由器和NoPE-RoPE混合稀疏注意力机制，解决了本地设备计算、内存和存储的限制。SmallThinker模型在普通消费级CPU上实现了每秒超过20个token的生成速度，且内存占用极低（1GB/8GB），性能超越了许多大型LLM，无需昂贵GPU。该模型已公开发布。",
      "translated_title": "SmallThinker：一族为本地部署原生训练的高效大型语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "While frontier large language models (LLMs) continue to push capability boundaries, their deployment remains confined to GPU-powered cloud infrastructure. We challenge this paradigm with SmallThinker, a family of LLMs natively designed - not adapted - for the unique constraints of local devices: weak computational power, limited memory, and slow storage. Unlike traditional approaches that mainly compress existing models built for clouds, we architect SmallThinker from the ground up to thrive within these limitations. Our innovation lies in a deployment-aware architecture that transforms constraints into design principles. First, We introduce a two-level sparse structure combining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward networks, drastically reducing computational demands without sacrificing model capacity. Second, to conquer the I/O bottleneck of slow storage, we design a pre-attention router that enables our co-designed inference engine to prefetch expert parameters from storage while computing attention, effectively hiding storage latency that would otherwise cripple on-device inference. Third, for memory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to slash KV cache requirements. We release SmallThinker-4B-A0.6B and SmallThinker-21B-A3B, which achieve state-of-the-art performance scores and even outperform larger LLMs. Remarkably, our co-designed system mostly eliminates the need for expensive GPU hardware: with Q4_0 quantization, both models exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB and 8GB of memory respectively. SmallThinker is publicly available at hf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and hf.co/PowerInfer/SmallThinker-21BA3B-Instruct."
    },
    {
      "title": "弱监督下将航空图像车辆检测器适应到未知领域 (原标题: Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with Weak Supervision)",
      "link": "https://arxiv.org/abs/2507.20976",
      "pubDate": "Mon, 28 Jul 2025 12:38:06 GMT",
      "isoDate": "2025-07-28T12:38:06.000Z",
      "creator": "Xiao Fang, Minhyek Jeon, Zheyang Qin, Stanislav Panev, Celso de Melo, Shuowen Hu, Shayok Chakraborty, Fernando De la Torre",
      "summary": "## 航空图像车辆检测器在未知领域中的适应：弱监督方法\n\n### 引言\n\n航空图像中的车辆检测是一项关键任务，广泛应用于交通监控、城市规划和国防情报。尽管深度学习方法已在该领域取得了最先进（SOTA）的结果，但一个显著的挑战是，在某个地理区域数据上训练的模型难以有效地泛化到其他区域。这种泛化能力的不足源于领域漂移，其原因包括环境条件、城市布局、道路网络、车辆类型以及图像采集参数（如分辨率、光照和角度）等因素的差异，这些差异会显著降低模型性能。\n\n### 提出的方法\n\n本文提出了一种新颖的方法来解决上述挑战。该方法利用生成式AI技术合成高质量的航空图像及其对应的标签，从而通过数据增强的方式改进检测器的训练。\n\n**核心贡献：**\n\n*   开发了一个多阶段、多模态的知识迁移框架。\n*   该框架利用了经过微调的潜在扩散模型（LDMs），旨在有效弥合源环境和目标环境之间的分布差距。\n\n### 实验结果\n\n研究团队在多样化的航空图像领域进行了广泛的实验，结果显示所提出的方法在AP50（平均精度50%）指标上实现了持续的性能提升，具体表现如下：\n\n*   相对于在源域数据上进行监督学习的方法：性能提升4-23%。\n*   相对于弱监督适应方法：性能提升6-10%。\n*   相对于无监督域适应方法：性能提升7-40%。\n*   相对于开放集目标检测器：性能提升超过50%。\n\n### 新数据集\n\n为了进一步支持该领域的研究和发展，本文还引入了两个新标注的航空数据集，分别来源于新西兰和犹他州。\n\n### 其他信息\n\n*   该研究已提交至ICCV 2025。\n*   预印本可在arXiv上获取，编号为arXiv:2507.20976。",
      "shortSummary": "本文提出一种新颖方法，利用生成式AI和微调的潜在扩散模型（LDMs），通过合成高质量航空图像和标签，解决车辆检测器在不同地理区域泛化能力差的问题。该多阶段、多模态知识迁移框架显著弥合了源域与目标域的分布差距。实验结果显示，相对于多种现有方法，AP50性能提升了4-50%以上。此外，研究还发布了两个新标注的航空数据集，以促进该领域的研究。",
      "translated_title": "弱监督下将航空图像车辆检测器适应到未知领域",
      "images": [],
      "contentSource": "完整文章",
      "content": "Detecting vehicles in aerial imagery is a critical task with applications in traffic monitoring, urban planning, and defense intelligence. Deep learning methods have provided state-of-the-art (SOTA) results for this application. However, a significant challenge arises when models trained on data from one geographic region fail to generalize effectively to other areas. Variability in factors such as environmental conditions, urban layouts, road networks, vehicle types, and image acquisition parameters (e.g., resolution, lighting, and angle) leads to domain shifts that degrade model performance. This paper proposes a novel method that uses generative AI to synthesize high-quality aerial images and their labels, improving detector training through data augmentation. Our key contribution is the development of a multi-stage, multi-modal knowledge transfer framework utilizing fine-tuned latent diffusion models (LDMs) to mitigate the distribution gap between the source and target environments. Extensive experiments across diverse aerial imagery domains show consistent performance improvements in AP50 over supervised learning on source domain data, weakly supervised adaptation methods, unsupervised domain adaptation methods, and open-set object detectors by 4-23%, 6-10%, 7-40%, and more than 50%, respectively. Furthermore, we introduce two newly annotated aerial datasets from New Zealand and Utah to support further research in this field. Project page is available at: https://humansensinglab.github.io/AGenDA"
    },
    {
      "title": "ARC-Hunyuan-Video-7B：真实世界短视频的结构化理解 (原标题: ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World Shorts)",
      "link": "https://arxiv.org/abs/2507.20939",
      "pubDate": "Mon, 28 Jul 2025 11:52:36 GMT",
      "isoDate": "2025-07-28T11:52:36.000Z",
      "creator": "Yuying Ge, Yixiao Ge, Chen Li, Teng Wang, Junfu Pu, Yizhuo Li, Lu Qiu, Jin Ma, Lisheng Duan, Xinyu Zuo, Jinwen Luo, Weibo Gu, Zexuan Li, Xiaojing Zhang, Yangyu Tao, Han Hu, Di Wang, Ying Shan",
      "summary": "# ARC-Hunyuan-Video-7B：真实世界短视频的结构化理解\n\n## 1. 背景与挑战\n当前的大型多模态模型在理解真实世界用户生成短视频（如微信视频号、抖音等平台上的内容）方面存在局限性。它们缺乏必要的时序结构化、详细且深入的视频理解能力。然而，这种能力是高效视频搜索、推荐以及新兴视频应用的基础。理解真实世界短视频极具挑战性，因为它们通常包含复杂的视觉元素、高密度的视觉和音频信息，以及快节奏的叙事，侧重于情感表达和观点传递。这要求模型具备高级推理能力，以有效整合包括视觉、音频和文本在内的多模态信息。\n\n## 2. ARC-Hunyuan-Video 模型介绍\n本文引入了 **ARC-Hunyuan-Video**，一个多模态模型，能够端到端地处理原始视频输入中的视觉、音频和文本信号，以实现结构化理解。\n\n### 2.1 模型能力\nARC-Hunyuan-Video 模型具备以下多项能力：\n*   **多粒度带时间戳的视频字幕生成和摘要**：能够对视频内容进行详细描述和总结，并标注相应的时间点。\n*   **开放式视频问答**：回答关于视频内容的开放性问题。\n*   **时序视频定位**：在视频中准确识别和定位特定事件或对象。\n*   **视频推理**：对视频内容进行深层次的逻辑推理。\n\n### 2.2 模型架构与训练\n*   **参数规模**：该模型是一个紧凑的70亿参数模型（7B-parameter）。\n*   **数据来源**：利用高质量的自动化标注数据进行训练。\n*   **训练流程**：采用全面的训练方案，包括：\n    *   预训练（Pre-training）\n    *   指令微调（Instruction Fine-tuning）\n    *   冷启动（Cold Start）\n    *   强化学习后训练（Reinforcement Learning (RL) Post-training）\n    *   最终指令微调（Final Instruction Fine-tuning）\n\n## 3. 性能评估与实际应用\n*   **基准测试**：在本文引入的基准测试 **ShortVid-Bench** 上进行了定量评估。\n*   **性能表现**：通过定量评估和定性比较，模型在真实世界视频理解方面展现出强大的性能。\n*   **下游应用**：支持零样本（zero-shot）或少量样本微调（few-sample fine-tuning），适用于多样化的下游应用。\n*   **生产部署**：该模型已在实际生产环境中部署，并在用户参与度和满意度方面带来了显著且可衡量的提升。\n*   **效率**：模型效率显著，压力测试表明，在H20 GPU上处理一分钟视频的推理时间仅需10秒。",
      "shortSummary": "ARC-Hunyuan-Video-7B是一个针对真实世界短视频的70亿参数多模态理解模型。它旨在解决现有模型在短视频结构化、详细理解上的不足。该模型能端到端处理视频的视觉、音频和文本信息，实现多粒度字幕、问答、时序定位和推理。通过自动化高质量数据和多阶段训练，模型在自建基准ShortVid-Bench上表现出色，并已在生产环境中部署，显著提升用户体验，且推理效率高（1分钟视频仅需10秒）。",
      "translated_title": "ARC-Hunyuan-Video-7B：真实世界短视频的结构化理解",
      "images": [],
      "contentSource": "完整文章",
      "content": "Real-world user-generated short videos, especially those distributed on platforms such as WeChat Channel and TikTok, dominate the mobile internet. However, current large multimodal models lack essential temporally-structured, detailed, and in-depth video comprehension capabilities, which are the cornerstone of effective video search and recommendation, as well as emerging video applications. Understanding real-world shorts is actually challenging due to their complex visual elements, high information density in both visuals and audio, and fast pacing that focuses on emotional expression and viewpoint delivery. This requires advanced reasoning to effectively integrate multimodal information, including visual, audio, and text. In this work, we introduce ARC-Hunyuan-Video, a multimodal model that processes visual, audio, and textual signals from raw video inputs end-to-end for structured comprehension. The model is capable of multi-granularity timestamped video captioning and summarization, open-ended video question answering, temporal video grounding, and video reasoning. Leveraging high-quality data from an automated annotation pipeline, our compact 7B-parameter model is trained through a comprehensive regimen: pre-training, instruction fine-tuning, cold start, reinforcement learning (RL) post-training, and final instruction fine-tuning. Quantitative evaluations on our introduced benchmark ShortVid-Bench and qualitative comparisons demonstrate its strong performance in real-world video comprehension, and it supports zero-shot or fine-tuning with a few samples for diverse downstream applications. The real-world production deployment of our model has yielded tangible and measurable improvements in user engagement and satisfaction, a success supported by its remarkable efficiency, with stress tests indicating an inference time of just 10 seconds for a one-minute video on H20 GPU."
    },
    {
      "title": "音乐竞技场：文本到音乐的实时评估 (原标题: Music Arena: Live Evaluation for Text-to-Music)",
      "link": "https://arxiv.org/abs/2507.20900",
      "pubDate": "Mon, 28 Jul 2025 10:52:57 GMT",
      "isoDate": "2025-07-28T10:52:57.000Z",
      "creator": "Yonghyun Kim, Wayne Chi, Anastasios N. Angelopoulos, Wei-Lin Chiang, Koichi Saito, Shinji Watanabe, Yuki Mitsufuji, Chris Donahue",
      "summary": "## 音乐竞技场：文本到音乐模型的实时评估平台\n\n**核心问题与挑战：**\n\n*   **评估成本高昂且难以比较：** 文本到音乐（TTM）模型的评估通常依赖于耗时且昂贵的人类听力研究，但不同研究的协议差异导致结果难以相互比较。\n*   **缺乏开放且可再生的偏好数据源：** 尽管人类偏好对于TTM系统对齐和自动评估指标改进至关重要，但目前缺乏一个开放且持续更新的偏好数据来源。\n\n**解决方案：Music Arena 平台**\n\nMusic Arena 是一个开放平台，旨在通过**实时评估**来解决上述挑战，实现TTM模型人类偏好评估的可扩展性。\n\n**工作原理：**\n\n*   **用户驱动的评估：** 真实用户输入他们选择的文本提示，并比较两个TTM系统生成的输出。\n*   **偏好收集与排行榜：** 用户的偏好被收集并用于生成一个排行榜，直观展示不同模型的表现。\n\n**音乐领域的定制化特性：**\n\nMusic Arena 不仅遵循其他AI领域的最新评估趋势，还特别针对音乐领域设计了关键功能：\n\n*   **基于LLM的路由系统：** 用于处理TTM系统异构的类型签名（type signatures），确保不同系统间的兼容性和评估流程的顺畅。\n*   **详细偏好数据收集：** 除了简单的偏好选择外，平台还收集详细的偏好数据，包括听力数据和自然语言反馈，为研究提供更丰富的信息。\n\n**数据政策与透明度：**\n\n*   **滚动数据发布政策：** 平台采用滚动数据发布政策，提供持续更新的偏好数据源。\n*   **用户隐私保障：** 确保用户隐私得到充分保护。\n*   **增加平台透明度：** 通过透明的数据访问政策，提高平台的开放性和可信度。\n\n**平台意义与影响：**\n\nMusic Arena 通过其标准化的评估协议、透明的数据访问政策和针对音乐领域的独特功能，不仅解决了TTM生态系统中的关键挑战，还展示了如何将实时评估方法深思熟虑地应用于特定AI领域的独特特性。",
      "shortSummary": "Music Arena 是一个开放平台，旨在为文本到音乐（TTM）模型提供可扩展的实时人类偏好评估。它允许用户输入文本提示并比较两个TTM系统的输出，从而构建一个排行榜。该平台具有音乐领域特有的功能，如基于LLM的路由系统和详细偏好数据收集。通过透明的数据发布政策和隐私保障，Music Arena 提供了一个可再生的偏好数据源，解决了TTM评估中的高成本和可比性难题，并推动了该领域的发展。",
      "translated_title": "音乐竞技场：文本到音乐的实时评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Music Arena, an open platform for scalable human preference evaluation of text-to-music (TTM) models. Soliciting human preferences via listening studies is the gold standard for evaluation in TTM, but these studies are expensive to conduct and difficult to compare, as study protocols may differ across systems. Moreover, human preferences might help researchers align their TTM systems or improve automatic evaluation metrics, but an open and renewable source of preferences does not currently exist. We aim to fill these gaps by offering *live* evaluation for TTM. In Music Arena, real-world users input text prompts of their choosing and compare outputs from two TTM systems, and their preferences are used to compile a leaderboard. While Music Arena follows recent evaluation trends in other AI domains, we also design it with key features tailored to music: an LLM-based routing system to navigate the heterogeneous type signatures of TTM systems, and the collection of *detailed* preferences including listening data and natural language feedback. We also propose a rolling data release policy with user privacy guarantees, providing a renewable source of preference data and increasing platform transparency. Through its standardized evaluation protocol, transparent data access policies, and music-specific features, Music Arena not only addresses key challenges in the TTM ecosystem but also demonstrates how live evaluation can be thoughtfully adapted to unique characteristics of specific AI domains.   Music Arena is available at: https://music-arena.org"
    },
    {
      "title": "JAM：一个具有细粒度可控性和美学对齐的微型基于流的歌曲生成器 (原标题: JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability and Aesthetic Alignment)",
      "link": "https://arxiv.org/abs/2507.20880",
      "pubDate": "Mon, 28 Jul 2025 10:34:02 GMT",
      "isoDate": "2025-07-28T10:34:02.000Z",
      "creator": "Renhang Liu, Chia-Yu Hung, Navonil Majumder, Taylor Gautreaux, Amir Ali Bagherzadeh, Chuan Li, Dorien Herremans, Soujanya Poria",
      "summary": "## JAM：一个具有细粒度可控性和美学对齐的微型基于流的歌曲生成器\n\n### 引言\n\n近年来，扩散模型和流匹配模型在自动文本到音频生成领域取得了革命性进展，尤其在生成高质量且忠实的语音和声学事件方面表现出色。然而，在涉及音乐和歌曲等创意音频生成方面，仍有很大的改进空间。\n\n### 现有模型的局限性\n\n*   **缺乏细粒度控制：** 现有的开放式歌词到歌曲模型，如DiffRhythm、ACE-Step和LeVo，虽然为娱乐用途设定了可接受的自动歌曲生成标准，但它们缺乏音乐家在工作流程中经常需要的细粒度词级控制。\n\n### JAM的创新与特点\n\n*   **词级控制：** JAM是首个基于流匹配的模型，致力于在歌曲生成中实现词级时间控制和持续时间控制，从而提供细粒度的声乐控制。这是对现有模型的一大改进，满足了音乐创作者对精确控制的需求。\n*   **美学对齐：** 为了提升生成歌曲的质量，使其更好地符合人类偏好，JAM通过“直接偏好优化”（Direct Preference Optimization, DPO）实现了美学对齐。这项技术通过迭代地使用合成数据集来优化模型，从而消除了对手动数据标注的需求。\n\n### 标准化评估\n\n*   **JAME数据集：** 为了标准化此类歌词到歌曲模型的评估，研究团队推出了公共评估数据集JAME。\n\n### 性能表现\n\n*   研究结果表明，JAM在音乐特定属性方面优于现有模型。",
      "shortSummary": "JAM是一个基于流匹配的微型歌曲生成器，旨在解决现有模型缺乏细粒度控制和美学对齐的问题。JAM首次实现了歌曲生成的词级时间与持续时间控制，并通过直接偏好优化（DPO）提升美学质量，无需手动标注。此外，它引入了公共评估数据集JAME。实验证明，JAM在音乐特定属性上优于现有模型。",
      "translated_title": "JAM：一个具有细粒度可控性和美学对齐的微型基于流的歌曲生成器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Diffusion and flow-matching models have revolutionized automatic text-to-audio generation in recent times. These models are increasingly capable of generating high quality and faithful audio outputs capturing to speech and acoustic events. However, there is still much room for improvement in creative audio generation that primarily involves music and songs. Recent open lyrics-to-song models, such as, DiffRhythm, ACE-Step, and LeVo, have set an acceptable standard in automatic song generation for recreational use. However, these models lack fine-grained word-level controllability often desired by musicians in their workflows. To the best of our knowledge, our flow-matching-based JAM is the first effort toward endowing word-level timing and duration control in song generation, allowing fine-grained vocal control. To enhance the quality of generated songs to better align with human preferences, we implement aesthetic alignment through Direct Preference Optimization, which iteratively refines the model using a synthetic dataset, eliminating the need or manual data annotations. Furthermore, we aim to standardize the evaluation of such lyrics-to-song models through our public evaluation dataset JAME. We show that JAM outperforms the existing models in terms of the music-specific attributes."
    },
    {
      "title": "几何平均策略优化 (原标题: Geometric-Mean Policy Optimization)",
      "link": "https://arxiv.org/abs/2507.20673",
      "pubDate": "Mon, 28 Jul 2025 05:54:05 GMT",
      "isoDate": "2025-07-28T05:54:05.000Z",
      "creator": "Yuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao, Tengchao Lv, Shaohan Huang, Lei Cui, Qixiang Ye, Fang Wan, Furu Wei",
      "summary": "### 几何平均策略优化 (GMPO)\n\n本文提出了一种名为几何平均策略优化（GMPO）的新方法，旨在解决现有大型语言模型（LLM）策略优化方法（如群组相对策略优化，GRPO）在处理异常重要性加权奖励时遇到的不稳定性问题。\n\n#### GRPO 的局限性\n\n*   **优化目标**：GRPO 通过优化令牌级奖励的算术平均值来增强大型语言模型的推理能力。\n*   **不稳定性来源**：当处理具有异常重要性加权奖励的令牌时，GRPO 的策略更新会变得不稳定。这表现为训练过程中出现极端的“重要性采样比率”（即当前策略与旧策略分配给令牌的采样概率之比）。\n\n#### GMPO 的提出与优势\n\n*   **核心思想**：GMPO 是 GRPO 的一个稳定变体，它不是优化令牌级奖励的算术平均值，而是最大化其几何平均值。\n*   **稳定性提升**：\n    *   几何平均值本质上对异常值不那么敏感。\n    *   它能维持更稳定的重要性采样比率范围，从而提高策略更新的稳定性。\n*   **理论与实验支持**：作者提供了全面的理论和实验分析，以证明 GMPO 的设计合理性及其带来的稳定性益处。\n\n#### 性能表现\n\n*   **超越 GRPO**：GMPO-7B 在多个基准测试中表现优于 GRPO。\n    *   **数学基准**：平均性能提升 4.1%，包括 AIME24、AMC、MATH500 和 OlympiadBench。\n    *   **多模态推理基准**：平均性能提升 1.4%，包括 Minerva 和 Geometry3K。\n\n#### 代码可用性\n\n*   相关代码已公开提供。",
      "shortSummary": "本文提出了几何平均策略优化（GMPO），旨在解决群组相对策略优化（GRPO）在处理异常奖励时策略更新不稳定的问题。GMPO 通过最大化令牌级奖励的几何平均值，有效降低了对异常值的敏感度，并稳定了重要性采样比率。理论和实验均表明，GMPO 提高了稳定性，并且在数学和多模态推理基准测试中，GMPO-7B 的性能分别比 GRPO 平均提升了4.1%和1.4%。",
      "translated_title": "几何平均策略优化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements, such as Group Relative Policy Optimization (GRPO), have enhanced the reasoning capabilities of large language models by optimizing the arithmetic mean of token-level rewards. However, GRPO suffers from unstable policy updates when processing tokens with outlier importance-weighted rewards, which manifests as extreme importance sampling ratios during training, i.e., the ratio between the sampling probabilities assigned to a token by the current and old policies. In this work, we propose Geometric-Mean Policy Optimization (GMPO), a stabilized variant of GRPO. Instead of optimizing the arithmetic mean, GMPO maximizes the geometric mean of token-level rewards, which is inherently less sensitive to outliers and maintains a more stable range of importance sampling ratio. In addition, we provide comprehensive theoretical and experimental analysis to justify the design and stability benefits of GMPO. Beyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on multiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark, including AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is available at https://github.com/callsys/GMPO."
    },
    {
      "title": "基于区域的聚类判别用于视觉表示学习 (原标题: Region-based Cluster Discrimination for Visual Representation Learning)",
      "link": "https://arxiv.org/abs/2507.20025",
      "pubDate": "Sat, 26 Jul 2025 13:47:09 GMT",
      "isoDate": "2025-07-26T13:47:09.000Z",
      "creator": "Yin Xie, Kaicheng Yang, Xiang An, Kun Wu, Yongle Zhao, Weimo Deng, Zimin Ran, Yumeng Wang, Ziyong Feng, Roy Miles, Ismail Elezi, Jiankang Deng",
      "summary": "### Region-Aware Cluster Discrimination (RICE) 概述\n\n*   **研究背景与问题**\n    *   视觉表示学习是许多下游任务的基础。\n    *   近期视觉-语言对比模型（如CLIP和SigLIP）通过大规模视觉-语言对齐在零样本性能上表现出色。\n    *   然而，这些模型依赖于全局表示，限制了它们在密集预测任务（如接地（grounding）、光学字符识别（OCR）和分割）中的有效性。\n\n*   **RICE方法介绍**\n    *   为解决上述问题，本文提出了**Region-Aware Cluster Discrimination (RICE)**，一种旨在增强区域级视觉和OCR能力的新方法。\n    *   **核心组件和创新点**：\n        *   **大规模候选区域数据集构建**：首先构建了一个十亿规模的候选区域数据集。\n        *   **区域Transformer层**：提出了一种区域Transformer层，用于提取丰富的区域语义信息。\n        *   **统一区域聚类判别损失**：设计了一种统一的区域聚类判别损失，该损失在一个单一的分类框架内同时支持目标学习和OCR学习。\n        *   **高效可扩展训练**：该框架支持在大规模数据上进行高效且可扩展的分布式训练。\n\n*   **实验结果与性能**\n    *   广泛的实验表明，RICE在多项任务上持续优于现有方法，包括：\n        *   分割（Segmentation）\n        *   密集检测（Dense Detection）\n        *   多模态大型语言模型（MLLMs）的视觉感知（Visual Perception）\n\n*   **可用性与认可**\n    *   预训练模型已发布在[此链接](https://this.https.url)。\n    *   该论文已被ICCV 2025接收为亮点论文（highlight paper）。",
      "shortSummary": "本文提出了Region-Aware Cluster Discrimination (RICE)，旨在解决现有全局视觉表示模型在密集预测任务（如OCR、分割）上的局限性。RICE通过构建大规模区域数据集、引入区域Transformer层和设计统一的区域聚类判别损失，增强了区域级视觉和OCR能力。实验证明，RICE在分割、密集检测和MLLM视觉感知等任务上均优于现有方法。该研究成果已被ICCV 2025接收为亮点论文。",
      "translated_title": "基于区域的聚类判别用于视觉表示学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Learning visual representations is foundational for a broad spectrum of downstream tasks. Although recent vision-language contrastive models, such as CLIP and SigLIP, have achieved impressive zero-shot performance via large-scale vision-language alignment, their reliance on global representations constrains their effectiveness for dense prediction tasks, such as grounding, OCR, and segmentation. To address this gap, we introduce Region-Aware Cluster Discrimination (RICE), a novel method that enhances region-level visual and OCR capabilities. We first construct a billion-scale candidate region dataset and propose a Region Transformer layer to extract rich regional semantics. We further design a unified region cluster discrimination loss that jointly supports object and OCR learning within a single classification framework, enabling efficient and scalable distributed training on large-scale data. Extensive experiments show that RICE consistently outperforms previous methods on tasks, including segmentation, dense detection, and visual perception for Multimodal Large Language Models (MLLMs). The pre-trained models have been released at https://github.com/deepglint/MVT."
    },
    {
      "title": "代理强化策略优化 (原标题: Agentic Reinforced Policy Optimization)",
      "link": "https://arxiv.org/abs/2507.19849",
      "pubDate": "Sat, 26 Jul 2025 03:53:11 GMT",
      "isoDate": "2025-07-26T03:53:11.000Z",
      "creator": "Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, Zhicheng Dou",
      "summary": "### 代理强化策略优化 (ARPO)\n\n**背景与问题**\n\n*   大规模可验证奖励强化学习（RLVR）在利用大型语言模型（LLMs）处理单轮推理任务方面表现出有效性。\n*   在实际推理场景中，LLMs通常需要利用外部工具来辅助任务解决。\n*   然而，当前的强化学习（RL）算法未能充分平衡LLMs固有的长程推理能力与它们在多轮工具交互中的熟练度。\n*   初步实验观察到，LLMs在与外部工具交互后，其生成的token的熵分布会增加，表现出高度不确定性。\n\n**提出的方法：代理强化策略优化 (ARPO)**\n\n*   为弥补现有算法的不足，本文提出了ARPO，一种新颖的代理强化学习算法。\n*   ARPO专门为训练基于LLM的多轮代理而设计。\n\n**ARPO 的核心机制**\n\n1.  **基于熵的自适应回滚机制 (Entropy-based Adaptive Rollout Mechanism)**\n    *   受LLM在工具使用后不确定性增加的启发。\n    *   该机制动态平衡全局轨迹采样和步级采样。\n    *   它旨在促进在工具使用后具有高不确定性的步骤进行探索。\n2.  **优势归因估计 (Advantage Attribution Estimation)**\n    *   ARPO通过整合优势归因估计，使LLMs能够内化分步工具使用交互中的优势差异。\n\n**实验结果与优势**\n\n*   研究团队在计算推理、知识推理和深度搜索领域的13个具有挑战性的基准测试中进行了实验。\n*   实验结果表明，ARPO的性能优于现有的轨迹级RL算法。\n*   值得注意的是，ARPO仅使用现有方法所需工具使用预算的一半，就实现了性能提升。\n*   这为使基于LLM的代理与实时动态环境对齐提供了一种可扩展的解决方案。\n\n**资源可用性**\n\n*   ARPO的代码和数据集已发布。",
      "shortSummary": "代理强化策略优化（ARPO）是一种新型RL算法，旨在解决大型语言模型（LLMs）在多轮工具交互中长程推理与工具使用效率的平衡问题。它引入了基于熵的自适应回滚机制以促进探索，并利用优势归因估计帮助LLMs内化工具使用差异。实验证明，ARPO在多个推理基准测试中表现优异，且仅需一半的工具使用预算，为LLM代理在动态环境中提供了高效且可扩展的解决方案。",
      "translated_title": "代理强化策略优化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large-scale reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in harnessing the potential of large language models (LLMs) for single-turn reasoning tasks. In realistic reasoning scenarios, LLMs can often utilize external tools to assist in task-solving processes. However, current RL algorithms inadequately balance the models' intrinsic long-horizon reasoning capabilities and their proficiency in multi-turn tool interactions. To bridge this gap, we propose Agentic Reinforced Policy Optimization (ARPO), a novel agentic RL algorithm tailored for training multi-turn LLM-based agents. Through preliminary experiments, we observe that LLMs tend to exhibit highly uncertain behavior, characterized by an increase in the entropy distribution of generated tokens, immediately following interactions with external tools. Motivated by this observation, ARPO incorporates an entropy-based adaptive rollout mechanism, dynamically balancing global trajectory sampling and step-level sampling, thereby promoting exploration at steps with high uncertainty after tool usage. By integrating an advantage attribution estimation, ARPO enables LLMs to internalize advantage differences in stepwise tool-use interactions. Our experiments across 13 challenging benchmarks in computational reasoning, knowledge reasoning, and deep search domains demonstrate ARPO's superiority over trajectory-level RL algorithms. Remarkably, ARPO achieves improved performance using only half of the tool-use budget required by existing methods, offering a scalable solution for aligning LLM-based agents with real-time dynamic environments. Our code and datasets are released at https://github.com/dongguanting/ARPO"
    }
  ],
  "lastUpdated": "2025-07-31T09:38:05.195Z"
}