{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "UniWorld：用于统一视觉理解和生成的高分辨率语义编码器 (原标题: UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation)",
      "link": "https://arxiv.org/abs/2506.03147",
      "pubDate": "Tue, 03 Jun 2025 13:59:33 GMT",
      "isoDate": "2025-06-03T13:59:33.000Z",
      "creator": "Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, Yatian Pang, Li Yuan",
      "summary": "UniWorld：用于统一视觉理解和生成的高分辨率语义编码器\n\n*   **背景与挑战**\n    *   尽管现有统一模型在视觉-语言理解和文本到图像生成方面表现出色，但它们在图像感知和操作任务上存在局限性。\n    *   这些图像感知和操作能力是用户在广泛应用中迫切需要的。\n\n*   **灵感来源**\n    *   OpenAI 最近发布的 GPT-4o-Image 模型在全面的图像感知和操作方面展现出强大能力，引起了广泛关注。\n    *   通过对 GPT-4o-Image 的实验观察，研究人员推断该模型利用语义编码器提取特征，而非许多图像操作模型中常用的变分自编码器（VAEs）。\n\n*   **UniWorld 框架**\n    *   受上述启发，本文提出了一个名为 UniWorld 的统一生成框架。\n    *   UniWorld 基于强大的视觉-语言模型提供的语义特征和对比语义编码器构建。\n\n*   **主要成果与优势**\n    *   UniWorld 构建了一个强大的统一模型，在数据效率方面表现突出。\n    *   仅使用了 BAGEL 模型 1% 的数据量，UniWorld 便在图像编辑基准测试中持续超越 BAGEL。\n    *   该模型同时保持了有竞争力的图像理解和生成能力。\n    *   在多个图像感知任务中取得了优异表现。\n\n*   **开源信息**\n    *   研究团队已完全开源 UniWorld 模型，包括模型权重、训练和评估脚本以及数据集。\n\n*   **研究领域**\n    *   计算机视觉与模式识别 (cs.CV)\n    *   人工智能 (cs.AI)\n    *   计算与语言 (cs.CL)",
      "shortSummary": "UniWorld 是一个统一的生成框架，旨在解决现有模型在图像感知和操作上的局限性。受 GPT-4o-Image 启发，UniWorld 利用语义编码器而非 VAEs。该模型仅用 BAGEL 1% 的数据量，便在图像编辑基准测试中持续超越 BAGEL，并保持强大的图像理解和生成能力。UniWorld 在多个图像感知任务中表现出色，且已完全开源。",
      "translated_title": "UniWorld：用于统一视觉理解和生成的高分辨率语义编码器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Although existing unified models deliver strong performance on vision-language understanding and text-to-image generation, their models are limited in exploring image perception and manipulation tasks, which are urgently desired by users for wide applications. Recently, OpenAI released their powerful GPT-4o-Image model for comprehensive image perception and manipulation, achieving expressive capability and attracting community interests. By observing the performance of GPT-4o-Image in our carefully constructed experiments, we infer that GPT-4o-Image leverages features extracted by semantic encoders instead of VAE, while VAEs are considered essential components in many image manipulation models. Motivated by such inspiring observations, we present a unified generative framework named UniWorld based on semantic features provided by powerful visual-language models and contrastive semantic encoders. As a result, we build a strong unified model using only 1% amount of BAGEL's data, which consistently outperforms BAGEL on image editing benchmarks. UniWorld also maintains competitive image understanding and generation capabilities, achieving strong performance across multiple image perception tasks. We fully open-source our models, including model weights, training and evaluation scripts, and datasets."
    },
    {
      "title": "GUI-Actor：面向GUI智能体的无坐标视觉定位 (原标题: GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents)",
      "link": "https://arxiv.org/abs/2506.03143",
      "pubDate": "Tue, 03 Jun 2025 13:59:08 GMT",
      "isoDate": "2025-06-03T13:59:08.000Z",
      "creator": "Qianhui Wu, Kanzhi Cheng, Rui Yang, Chaoyun Zhang, Jianwei Yang, Huiqiang Jiang, Jian Mu, Baolin Peng, Bo Qiao, Reuben Tan, Si Qin, Lars Liden, Qingwei Lin, Huan Zhang, Tong Zhang, Jianbing Zhang, Dongmei Zhang, Jianfeng Gao",
      "summary": "# GUI-Actor：面向GUI智能体的无坐标视觉定位\n\n## 1. 引言与挑战\n\n在构建由视觉语言模型（VLM）驱动的图形用户界面（GUI）智能体时，**视觉定位**是一个核心挑战。视觉定位是指根据视觉内容和文本计划，在屏幕上定位出用于执行操作的合适区域。目前大多数方法将此任务表述为基于文本的坐标生成。然而，这些方法存在以下局限性：\n\n*   **空间语义对齐弱：** 难以准确地将文本描述与屏幕上的视觉区域关联起来。\n*   **难以处理模糊监督目标：** 当存在多个可能的目标区域时，难以做出准确判断。\n*   **特征粒度不匹配：** 屏幕坐标的密集性与Vision Transformers等模型提取的粗粒度、补丁级视觉特征之间存在不匹配。\n\n## 2. GUI-Actor 方法\n\n为了解决上述挑战，本文提出了一种名为 **GUI-Actor** 的VLM-based方法，用于实现**无坐标的GUI定位**。\n\n### 核心机制：注意力动作头\n\nGUI-Actor 的核心在于引入了一个基于注意力机制的**动作头（attention-based action head）**。该动作头学习将一个专用的 `<ACTOR>` token 与所有相关的视觉补丁（visual patch tokens）对齐。这使得模型能够在**单次前向传播**中提出一个或多个潜在的动作区域。\n\n### 辅助机制：定位验证器\n\n为了进一步提高定位的准确性，GUI-Actor 还设计了一个**定位验证器（grounding verifier）**。该验证器负责评估并从动作头提出的候选区域中选择最合理、最适合执行操作的区域。\n\n## 3. 优势与特点\n\nGUI-Actor 展现出多项显著优势：\n\n*   **无坐标定位：** 摆脱了传统方法对精确坐标生成的依赖，解决了坐标与视觉特征粒度不匹配的问题。\n*   **高效的动作区域提案：** 能够在单次前向传播中识别并提出多个潜在的动作区域。\n*   **增强的泛化能力：** 在未曾见过的屏幕分辨率和布局上表现出更好的泛化性能。\n*   **训练效率高：** 实验发现，仅微调新引入的动作头（对于7B模型约1亿参数），同时保持VLM主干模型冻结，即可达到与现有最先进模型相当的性能。这表明 GUI-Actor 能够在不损害VLM通用能力的前提下，赋予其有效的定位能力。\n\n## 4. 实验结果\n\n广泛的实验表明，GUI-Actor 在多个GUI动作定位基准测试中超越了现有的最先进方法。\n\n*   **ScreenSpot-Pro 基准测试：** 值得注意的是，GUI-Actor-7B 在 ScreenSpot-Pro 上甚至超越了 UI-TARS-72B (38.1分)，在使用 Qwen2-VL 作为主干时取得了 40.7 分，使用 Qwen2.5-VL 作为主干时更是达到了 44.6 分。",
      "shortSummary": "GUI-Actor 是一种面向GUI智能体的无坐标视觉定位方法，旨在解决现有VLM-powered GUI代理在视觉定位中面临的空间语义对齐弱、模糊监督和特征粒度不匹配等挑战。它通过引入注意力机制的动作头，使模型能在单次前向传播中提出动作区域，并辅以定位验证器进行选择。实验证明，GUI-Actor 在多个GUI动作定位基准测试中超越了现有SOTA，并展现出对未见屏幕分辨率和布局的更强泛化能力，同时保持了高效的训练。",
      "translated_title": "GUI-Actor：面向GUI智能体的无坐标视觉定位",
      "images": [],
      "contentSource": "完整文章",
      "content": "One of the principal challenges in building VLM-powered GUI agents is visual grounding, i.e., localizing the appropriate screen region for action execution based on both the visual content and the textual plans. Most existing work formulates this as a text-based coordinate generation task. However, these approaches suffer from several limitations: weak spatial-semantic alignment, inability to handle ambiguous supervision targets, and a mismatch between the dense nature of screen coordinates and the coarse, patch-level granularity of visual features extracted by models like Vision Transformers. In this paper, we propose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its core, GUI-Actor introduces an attention-based action head that learns to align a dedicated <actor> token with all relevant visual patch tokens, enabling the model to propose one or more action regions in a single forward pass. In line with this, we further design a grounding verifier to evaluate and select the most plausible action region from the candidates proposed for action execution. Extensive experiments show that GUI-Actor outperforms prior state-of-the-art methods on multiple GUI action grounding benchmarks, with improved generalization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B even surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7 with Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by incorporating the verifier, we find that fine-tuning only the newly introduced action head (~100M parameters for 7B model) while keeping the VLM backbone frozen is sufficient to achieve performance comparable to previous state-of-the-art models, highlighting that GUI-Actor can endow the underlying VLM with effective grounding capabilities without compromising its general-purpose strengths.</actor>"
    },
    {
      "title": "通过强化学习协同演化大型语言模型编码器和单元测试器 (原标题: Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2506.03136",
      "pubDate": "Tue, 03 Jun 2025 13:58:42 GMT",
      "isoDate": "2025-06-03T13:58:42.000Z",
      "creator": "Yinjie Wang, Ling Yang, Ye Tian, Ke Shen, Mengdi Wang",
      "summary": "### CURE框架：通过强化学习协同演化大型语言模型编码器和单元测试器\n\n本文提出了一种名为CURE（Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning）的新颖强化学习框架。该框架通过专门设计的奖励机制，基于编码器和单元测试器之间的交互结果，协同演化两者的能力。\n\n*   **核心创新与优势：**\n    *   CURE框架无需任何真实代码作为监督，这使得训练过程更加灵活和可扩展。\n    *   该方法允许单元测试器直接从编码器所犯的错误中学习，从而形成一个迭代优化的闭环，持续提升两者的性能。\n\n*   **ReasonFlux-Coder模型系列：**\n    *   研究团队在Qwen2.5-Instruct模型的基础上进行了优化，开发了ReasonFlux-Coder-7B和14B模型。\n    *   **性能提升：**\n        *   这些模型在代码生成准确率上提升了5.3%。\n        *   在Best-of-N准确率上提升了9.0%。\n        *   其性能超越了同等规模的Qwen-Coder、DeepSeek-Coder和Seed-Coder等现有模型。\n    *   **下游任务扩展性：**\n        *   ReasonFlux-Coder模型能够自然地扩展到多种下游任务，例如测试时缩放（test-time scaling）和代理式编码（agentic coding），相比基础模型实现了8.1%的性能提升。\n    *   **ReasonFlux-Coder-4B的特殊表现：**\n        *   针对长上下文链（long-CoT）模型，ReasonFlux-Coder-4B持续优于Qwen3-4B。\n        *   同时，在单元测试生成方面实现了64.8%的推理效率提升。\n\n*   **额外发现：**\n    *   研究还发现，所提出的模型可以作为基础模型进行强化学习的有效奖励模型。\n\n*   **项目信息：**\n    *   该研究项目提供了相关链接。",
      "shortSummary": "CURE是一个新颖的强化学习框架，通过协同演化大型语言模型编码器和单元测试器，无需真实代码监督，使单元测试器能从编码器错误中学习。优化后的ReasonFlux-Coder-7B和14B模型在代码生成准确率上提升5.3%，Best-of-N准确率提升9.0%，并超越了同类模型。该模型在测试时缩放和代理式编码等下游任务中表现出色，且可作为有效的奖励模型。",
      "translated_title": "通过强化学习协同演化大型语言模型编码器和单元测试器",
      "images": [],
      "contentSource": "完整文章",
      "content": "We propose CURE, a novel reinforcement learning framework with a dedicated reward design that co-evolves coding and unit test generation capabilities based on their interaction outcomes, without any ground-truth code as supervision. This approach enables flexible and scalable training and allows the unit tester to learn directly from the coder's mistakes. Our derived ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models, outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They naturally extend to downstream tasks such as test-time scaling and agentic coding-achieving a 8.1% improvement over the base model. For the long-CoT model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while achieving 64.8% inference efficiency in unit test generation. Notably, we also find that our model can serve as an effective reward model for reinforcement learning on base models. Project: https://github.com/Gen-Verse/CURE"
    },
    {
      "title": "OmniSpatial：构建面向视觉语言模型的综合空间推理基准 (原标题: OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models)",
      "link": "https://arxiv.org/abs/2506.03135",
      "pubDate": "Tue, 03 Jun 2025 13:58:29 GMT",
      "isoDate": "2025-06-03T13:58:29.000Z",
      "creator": "Mengdi Jia, Zekun Qi, Shaochen Zhang, Wenyao Zhang, Xinqiang Yu, Jiawei He, He Wang, Li Yi",
      "summary": "## OmniSpatial：面向视觉语言模型的综合空间推理基准\n\n### 背景与问题\n*   空间推理是认知心理学的核心组成部分，但对于当前的视觉语言模型（VLMs）而言，仍是一个主要瓶颈。\n*   尽管现有研究致力于评估或改进VLMs对基本空间关系（如区分左右、远近、物体计数）的理解，但这些任务仅代表了空间推理的最基础层面。\n\n### OmniSpatial基准的引入\n*   本文引入了OmniSpatial，一个基于认知心理学的综合且具有挑战性的空间推理基准。\n*   **覆盖范围**：OmniSpatial涵盖了四大主要类别：\n    *   动态推理（Dynamic Reasoning）\n    *   复杂空间逻辑（Complex Spatial Logic）\n    *   空间交互（Spatial Interaction）\n    *   视角采择（Perspective-Taking）\n*   **细粒度分类**：该基准包含50个细粒度的子类别。\n*   **数据构建**：通过互联网数据爬取和细致的人工标注，构建了超过1.5K个问答对。\n\n### 实验结果与发现\n*   广泛的实验表明，无论是开源还是闭源的VLMs，以及现有的推理和空间理解模型，在全面的空间理解方面都表现出显著的局限性。\n\n### 未来方向\n*   研究进一步分析了失败案例，并提出了未来研究的潜在方向。",
      "shortSummary": "OmniSpatial是一个新颖的、基于认知心理学的综合空间推理基准，旨在解决视觉语言模型（VLMs）在复杂空间理解方面的显著局限。该基准涵盖动态推理、复杂空间逻辑、空间交互和视角采择四大类，包含50个细粒度子类别和超过1.5K个问答对。实验结果显示，当前VLMs在全面的空间理解方面表现不佳，凸显了该领域仍需深入研究。",
      "translated_title": "OmniSpatial：构建面向视觉语言模型的综合空间推理基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Spatial reasoning is a key aspect of cognitive psychology and remains a major bottleneck for current vision-language models (VLMs). While extensive research has aimed to evaluate or improve VLMs' understanding of basic spatial relations, such as distinguishing left from right, near from far, and object counting, these tasks represent only the most fundamental level of spatial reasoning. In this work, we introduce OmniSpatial, a comprehensive and challenging benchmark for spatial reasoning, grounded in cognitive psychology. OmniSpatial covers four major categories: dynamic reasoning, complex spatial logic, spatial interaction, and perspective-taking, with 50 fine-grained subcategories. Through Internet data crawling and careful manual annotation, we construct over 1.5K question-answer pairs. Extensive experiments show that both open- and closed-source VLMs, as well as existing reasoning and spatial understanding models, exhibit significant limitations in comprehensive spatial understanding. We further analyze failure cases and propose potential directions for future research."
    },
    {
      "title": "原生分辨率图像合成 (原标题: Native-Resolution Image Synthesis)",
      "link": "https://arxiv.org/abs/2506.03131",
      "pubDate": "Tue, 03 Jun 2025 13:57:33 GMT",
      "isoDate": "2025-06-03T13:57:33.000Z",
      "creator": "Zidong Wang, Lei Bai, Xiangyu Yue, Wanli Ouyang, Yiyuan Zhang",
      "summary": "## 原生分辨率图像合成：一种新型生成模型范式\n\n本文介绍了一种名为“原生分辨率图像合成”的创新生成建模范式，旨在克服传统固定分辨率、方形图像生成方法的局限性。该方法通过原生处理可变长度的视觉令牌，解决了传统技术在处理不同图像尺寸和比例方面的核心挑战。\n\n### 核心创新与模型\n\n*   **原生处理可变长度视觉令牌**：与传统方法不同，该范式能够直接处理不同长度的视觉信息，从而支持任意分辨率和纵横比的图像合成。\n*   **原生分辨率扩散Transformer (NiT)**：为实现这一目标，研究人员引入了NiT架构。NiT在去噪过程中明确地建模了各种分辨率和纵横比，使其能够摆脱固定格式的约束。\n\n### NiT的性能与能力\n\n*   **学习内在视觉分布**：NiT能够从涵盖广泛分辨率和纵横比的图像中学习内在的视觉分布。\n*   **最先进的性能**：单个NiT模型在ImageNet-256x256和512x512基准测试上同时达到了最先进的性能。\n*   **卓越的零样本泛化能力**：令人惊讶的是，NiT仅在ImageNet上进行训练，却展现出类似于先进大型语言模型（LLM）的强大零样本泛化能力。它能够成功生成以前未见过的高分辨率（例如1536x1536）和多样纵横比（例如16:9、3:1、4:3）的高保真图像。\n\n### 潜在意义\n\n这些发现表明，原生分辨率建模在视觉生成建模和先进LLM方法之间架起了一座桥梁，具有显著的潜力。",
      "shortSummary": "本文提出了一种名为“原生分辨率图像合成”的新型生成建模范式，旨在克服传统固定分辨率方法的局限。核心是原生分辨率扩散Transformer (NiT) 模型，它能原生处理可变分辨率和纵横比的图像。NiT在ImageNet基准测试上表现出色，并展现了强大的零样本泛化能力，能生成高保真、多样分辨率和纵横比的图像。该研究为视觉生成建模与大型语言模型方法之间搭建了桥梁。",
      "translated_title": "原生分辨率图像合成",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce native-resolution image synthesis, a novel generative modeling paradigm that enables the synthesis of images at arbitrary resolutions and aspect ratios. This approach overcomes the limitations of conventional fixed-resolution, square-image methods by natively handling variable-length visual tokens, a core challenge for traditional techniques. To this end, we introduce the Native-resolution diffusion Transformer (NiT), an architecture designed to explicitly model varying resolutions and aspect ratios within its denoising process. Free from the constraints of fixed formats, NiT learns intrinsic visual distributions from images spanning a broad range of resolutions and aspect ratios. Notably, a single NiT model simultaneously achieves the state-of-the-art performance on both ImageNet-256x256 and 512x512 benchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in advanced large language models, NiT, trained solely on ImageNet, demonstrates excellent zero-shot generalization performance. It successfully generates high-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536) and diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These findings indicate the significant potential of native-resolution modeling as a bridge between visual generative modeling and advanced LLM methodologies."
    },
    {
      "title": "AnimeShooter：一个用于参考引导视频生成的多镜头动画数据集 (原标题: AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video Generation)",
      "link": "https://arxiv.org/abs/2506.03126",
      "pubDate": "Tue, 03 Jun 2025 13:55:18 GMT",
      "isoDate": "2025-06-03T13:55:18.000Z",
      "creator": "Lu Qiu, Yizhuo Li, Yuying Ge, Yixiao Ge, Ying Shan, Xihui Liu",
      "summary": "### AnimeShooter：参考引导多镜头动画数据集\n\n#### 1. 背景与问题\n\n*   **AI生成内容 (AIGC) 的进步**：显著加速了动画制作。\n*   **现有数据集的局限性**：\n    *   主要关注真实世界场景，缺乏用于一致角色指导的参考图像。\n    *   难以生成具有叙事脚本和角色参考的连贯多镜头视频片段。\n\n#### 2. AnimeShooter 数据集介绍\n\n*   **目的**：弥补现有数据集的不足，提供一个用于参考引导多镜头动画生成的数据集。\n*   **核心特点**：\n    *   通过自动化流程确保跨镜头的强视觉一致性。\n    *   包含全面的分层标注。\n\n#### 3. 分层标注详情\n\n*   **故事级别标注**：\n    *   提供叙事概览。\n    *   包含故事情节、关键场景。\n    *   包含带有**参考图像**的主要角色简介。\n*   **镜头级别标注**：\n    *   将故事分解为连续镜头。\n    *   每个镜头都标注了场景、角色。\n    *   包含叙事和描述性视觉字幕。\n\n#### 4. AnimeShooter-audio 子集\n\n*   **特点**：一个专门的子集，为每个镜头提供同步音轨。\n*   **附加信息**：包含音频描述和声源。\n\n#### 5. AnimeShooterGen：基线模型\n\n*   **目的**：\n    *   展示 AnimeShooter 数据集的有效性。\n    *   为参考引导的多镜头视频生成任务建立基线。\n*   **工作原理**：\n    *   利用多模态大型语言模型 (MLLM) 和视频扩散模型。\n    *   **处理流程**：参考图像和先前生成的镜头首先由 MLLM 处理，以生成同时感知参考和上下文的表示。\n    *   **条件生成**：这些表示随后作为扩散模型的条件，用于解码后续镜头。\n\n#### 6. 实验结果与价值\n\n*   **表现**：在 AnimeShooter 上训练的模型在跨镜头视觉一致性和对参考视觉指导的遵循方面表现出色。\n*   **结论**：这些结果凸显了 AnimeShooter 数据集对于生成连贯动画视频的价值。",
      "shortSummary": "AnimeShooter是一个新的多镜头动画数据集，旨在解决现有AI生成内容（AIGC）数据集在角色一致性指导方面的不足。它包含分层标注，如故事级别（含角色参考图）和镜头级别描述，并提供同步音轨子集。为展示其有效性，研究者提出了AnimeShooterGen基线模型，利用多模态大型语言模型和视频扩散模型生成连贯动画。实验证明，该数据集能有效提升跨镜头视觉一致性和对角色参考的遵循度，对动画视频生成具有重要价值。",
      "translated_title": "AnimeShooter：一个用于参考引导视频生成的多镜头动画数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in AI-generated content (AIGC) have significantly accelerated animation production. To produce engaging animations, it is essential to generate coherent multi-shot video clips with narrative scripts and character references. However, existing public datasets primarily focus on real-world scenarios with global descriptions, and lack reference images for consistent character guidance. To bridge this gap, we present AnimeShooter, a reference-guided multi-shot animation dataset. AnimeShooter features comprehensive hierarchical annotations and strong visual consistency across shots through an automated pipeline. Story-level annotations provide an overview of the narrative, including the storyline, key scenes, and main character profiles with reference images, while shot-level annotations decompose the story into consecutive shots, each annotated with scene, characters, and both narrative and descriptive visual captions. Additionally, a dedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each shot, along with audio descriptions and sound sources. To demonstrate the effectiveness of AnimeShooter and establish a baseline for the reference-guided multi-shot video generation task, we introduce AnimeShooterGen, which leverages Multimodal Large Language Models (MLLMs) and video diffusion models. The reference image and previously generated shots are first processed by MLLM to produce representations aware of both reference and context, which are then used as the condition for the diffusion model to decode the subsequent shot. Experimental results show that the model trained on AnimeShooter achieves superior cross-shot visual consistency and adherence to reference visual guidance, which highlight the value of our dataset for coherent animated video generation."
    },
    {
      "title": "ORV：以占据为中心的4D机器人视频生成 (原标题: ORV: 4D Occupancy-centric Robot Video Generation)",
      "link": "https://arxiv.org/abs/2506.03079",
      "pubDate": "Tue, 03 Jun 2025 13:00:32 GMT",
      "isoDate": "2025-06-03T13:00:32.000Z",
      "creator": "Xiuyu Yang, Bohan Li, Shaocong Xu, Nan Wang, Chongjie Ye, Zhaoxi Chen, Minghan Qin, Yikang Ding, Xin Jin, Hang Zhao, Hao Zhao",
      "summary": "## ORV：以占据为中心的4D机器人视频生成\n\n### 背景与问题\n\n*   **数据获取挑战**：通过远程操作获取真实世界机器人仿真数据耗时且劳动密集。\n*   **现有方法局限**：尽管动作驱动的生成模型在机器人学习和仿真中得到广泛应用，但其动作序列由于全局粗略对齐，导致控制精度有限且泛化能力差。\n\n### ORV框架介绍\n\n*   **解决方案**：为解决上述限制，本文提出了ORV（Occupancy-centric Robot Video generation framework），一个以占据为中心的机器人视频生成框架。\n*   **核心机制**：ORV利用4D语义占据序列作为细粒度表示，为视频生成提供更精确的语义和几何指导。\n\n### ORV的关键能力与优势\n\n*   **数据转换**：通过利用基于占据的表示，ORV能够将仿真数据无缝转换为逼真的机器人视频。\n*   **性能保障**：确保了高时间一致性和精确的可控性。\n*   **多视角生成**：支持同时生成机器人抓取操作的多视角视频，这对下游机器人学习任务而言是一项重要能力。\n\n### 实验结果\n\n*   广泛的实验结果表明，ORV在各种数据集和子任务上始终优于现有基线方法。",
      "shortSummary": "ORV是一个以占据为中心的4D机器人视频生成框架，旨在解决传统动作驱动模型控制精度低和泛化能力差的问题。它利用4D语义占据序列作为细粒度表示，为视频生成提供精确的语义和几何指导。ORV能将仿真数据转换为逼真的机器人视频，确保高时间一致性和精确可控性，并支持多视角抓取视频生成，对机器人学习任务有重要意义。实验证明ORV性能优于现有方法。",
      "translated_title": "ORV：以占据为中心的4D机器人视频生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Acquiring real-world robotic simulation data through teleoperation is notoriously time-consuming and labor-intensive. Recently, action-driven generative models have gained widespread adoption in robot learning and simulation, as they eliminate safety concerns and reduce maintenance efforts. However, the action sequences used in these methods often result in limited control precision and poor generalization due to their globally coarse alignment. To address these limitations, we propose ORV, an Occupancy-centric Robot Video generation framework, which utilizes 4D semantic occupancy sequences as a fine-grained representation to provide more accurate semantic and geometric guidance for video generation. By leveraging occupancy-based representations, ORV enables seamless translation of simulation data into photorealistic robot videos, while ensuring high temporal consistency and precise controllability. Furthermore, our framework supports the simultaneous generation of multi-view videos of robot gripping operations - an important capability for downstream robotic learning tasks. Extensive experimental results demonstrate that ORV consistently outperforms existing baseline methods across various datasets and sub-tasks. Demo, Code and Model: https://orangesodahub.github.io/ORV"
    },
    {
      "title": "Sparse-vDiT：释放稀疏注意力以加速视频扩散Transformer的潜力 (原标题: Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers)",
      "link": "https://arxiv.org/abs/2506.03065",
      "pubDate": "Tue, 03 Jun 2025 12:42:37 GMT",
      "isoDate": "2025-06-03T12:42:37.000Z",
      "creator": "Pengtao Chen, Xianfang Zeng, Maosen Zhao, Peng Ye, Mingzhu Shen, Wei Cheng, Gang Yu, Tao Chen",
      "summary": "# Sparse-vDiT：加速视频扩散Transformer的稀疏注意力框架\n\n## 引言\n视频扩散Transformer（DiTs）在视频生成领域取得了突破性进展。然而，由于注意力机制的二次复杂度，这项长序列生成任务仍然受到限制，导致显著的推理延迟。\n\n## 稀疏性模式的发现与分析\n通过对视频扩散Transformer（vDiT）中注意力图的详细分析，研究人员识别出三种重复出现的稀疏性模式：\n*   **对角线结构**\n*   **多对角线结构**\n*   **垂直条纹结构**\n此外，研究发现甚至有3-6%的注意力头可以被跳过。\n**关键发现：** 这些模式与层深度和头部位置表现出很强的相关性，但对输入内容的依赖性有限。\n\n## Sparse-vDiT框架\n基于上述发现，研究人员提出了Sparse-vDiT，一个用于vDiT的稀疏性加速框架，其主要组成部分包括：\n1.  **模式优化稀疏核：** 针对每种已识别的稀疏模式，用计算效率更高的实现来替代传统的密集注意力机制。\n2.  **离线稀疏扩散搜索算法：** 该算法通过硬件感知成本建模，为每个层和每个注意力头选择最优的稀疏计算策略。\n在确定了最优配置后，框架会融合同一层中共享相同注意力策略的注意力头，从而进一步提高推理效率。\n\n## 实验结果与性能\nSparse-vDiT被集成到当前最先进的vDiT模型中，包括CogVideoX1.5、HunyuanVideo和Wan2.1，并取得了显著的性能提升：\n*   **理论FLOPs减少量：**\n    *   CogVideoX1.5：2.09倍\n    *   HunyuanVideo：2.38倍\n    *   Wan2.1：1.67倍\n*   **实际推理加速比：**\n    *   CogVideoX1.5：1.76倍\n    *   HunyuanVideo：1.85倍\n    *   Wan2.1：1.58倍\n同时，Sparse-vDiT保持了高视觉保真度，PSNR值分别达到24.13、27.09和22.59。\n\n## 结论\n这项工作表明，vDiT中潜在的结构稀疏性可以被系统地利用，以实现高效的长视频合成。",
      "shortSummary": "视频扩散Transformer（vDiT）因注意力机制的二次复杂度而面临高推理延迟。通过分析vDiT注意力图，研究发现存在对角线、多对角线和垂直条纹等一致的稀疏模式，且这些模式与层深度和头部位置强相关。为此，研究提出了Sparse-vDiT框架，利用模式优化的稀疏核和离线搜索算法加速计算。在CogVideoX1.5、HunyuanVideo等模型上，Sparse-vDiT实现了1.58至1.85倍的实际推理加速和显著的FLOPs减少，同时保持了高视觉质量，证明了利用稀疏性进行长视频合成的潜力。",
      "translated_title": "Sparse-vDiT：释放稀疏注意力以加速视频扩散Transformer的潜力",
      "images": [],
      "contentSource": "完整文章",
      "content": "While Diffusion Transformers (DiTs) have achieved breakthroughs in video generation, this long sequence generation task remains constrained by the quadratic complexity of attention mechanisms, resulting in significant inference latency. Through detailed analysis of attention maps in Video Diffusion Transformer (vDiT), we identify three recurring sparsity patterns: diagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\% attention heads can be skipped. Crucially, these patterns exhibit strong layer-depth and head-position correlations but show limited dependence on the input content. Leveraging these findings, we propose Sparse-vDiT, a sparsity acceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels that replace dense attention with computationally efficient implementations for each identified sparsity pattern. 2) An offline sparse diffusion search algorithm that selects the optimal sparse computation strategy per layer and head via hardware-aware cost modeling. After determining the optimal configuration, we fuse heads within the same layer that share the same attention strategy, enhancing inference efficiency. Integrated into state-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1), Sparse-vDiT achieves 2.09times, 2.38times, and 1.67times theoretical FLOP reduction, and actual inference speedups of 1.76times, 1.85times, and 1.58times, respectively, while maintaining high visual fidelity, with PSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent structural sparsity in vDiTs can be systematically exploited for long video synthesis."
    },
    {
      "title": "RelationAdapter：使用扩散Transformer学习和迁移视觉关系 (原标题: RelationAdapter: Learning and Transferring Visual Relation with Diffusion Transformers)",
      "link": "https://arxiv.org/abs/2506.02528",
      "pubDate": "Tue, 03 Jun 2025 03:06:35 GMT",
      "isoDate": "2025-06-03T03:06:35.000Z",
      "creator": "Yan Gong, Yiren Song, Yicheng Li, Chenglin Li, Yin Zhang",
      "summary": "### RelationAdapter：使用扩散Transformer学习和迁移视觉关系\n\n本文提出了一种名为RelationAdapter的新方法，旨在解决现有图像编辑方法在处理非刚性变换方面的局限性，并提升视觉关系的学习与迁移能力。\n\n**背景与问题**\n\n*   受大型语言模型（LLMs）上下文学习机制的启发，基于视觉提示的通用图像编辑新范式正在兴起。\n*   现有的单参考图像编辑方法通常侧重于风格或外观调整，难以处理非刚性变换。\n\n**RelationAdapter方法**\n\n*   **核心思想**：利用源-目标图像对来提取和迁移内容感知的编辑意图到新的查询图像上。\n*   **模块介绍**：RelationAdapter是一个轻量级模块，专门设计用于使基于扩散Transformer（DiT）的模型能够有效地从少量示例中捕获和应用视觉变换。\n\n**Relation252K数据集**\n\n*   为了评估模型在视觉提示驱动场景下的泛化能力和适应性，研究者引入了一个名为Relation252K的综合数据集。\n*   该数据集包含218项多样化的编辑任务。\n\n**实验结果**\n\n*   在Relation252K数据集上的实验表明，RelationAdapter显著提高了模型理解和迁移编辑意图的能力。\n*   这带来了生成质量和整体编辑性能的显著提升。",
      "shortSummary": "本文提出了RelationAdapter，一个轻量级模块，旨在通过利用源-目标图像对，使扩散Transformer（DiT）模型能有效学习和迁移视觉变换。该方法解决了现有图像编辑在非刚性变换上的局限性。在包含218项编辑任务的Relation252K数据集上，实验证明RelationAdapter显著提升了模型理解和迁移编辑意图的能力，从而提高了生成质量和编辑性能。",
      "translated_title": "RelationAdapter：使用扩散Transformer学习和迁移视觉关系",
      "images": [],
      "contentSource": "完整文章",
      "content": "Inspired by the in-context learning mechanism of large language models (LLMs), a new paradigm of generalizable visual prompt-based image editing is emerging. Existing single-reference methods typically focus on style or appearance adjustments and struggle with non-rigid transformations. To address these limitations, we propose leveraging source-target image pairs to extract and transfer content-aware editing intent to novel query images. To this end, we introduce RelationAdapter, a lightweight module that enables Diffusion Transformer (DiT) based models to effectively capture and apply visual transformations from minimal examples. We also introduce Relation252K, a comprehensive dataset comprising 218 diverse editing tasks, to evaluate model generalization and adaptability in visual prompt-driven scenarios. Experiments on Relation252K show that RelationAdapter significantly improves the model's ability to understand and transfer editing intent, leading to notable gains in generation quality and overall editing performance."
    },
    {
      "title": "M^3FinMeeting：一个多语言、多行业、多任务的金融会议理解评估数据集 (原标题: M^3FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset)",
      "link": "https://arxiv.org/abs/2506.02510",
      "pubDate": "Tue, 03 Jun 2025 02:41:09 GMT",
      "isoDate": "2025-06-03T02:41:09.000Z",
      "creator": "Jie Zhu, Junhui Li, Yalong Wen, Xiandong Li, Lifan Guo, Feng Chen",
      "summary": "## M^3FinMeeting：一个多语言、多行业、多任务的金融会议理解评估数据集\n\n本文提出了一种名为 M^3FinMeeting 的新型基准数据集，旨在解决当前大型语言模型（LLMs）在金融领域评估中存在的不足。现有金融基准多依赖新闻文章、财报或公告，难以捕捉金融会议的真实世界动态。M^3FinMeeting 专为金融会议理解而设计，具有以下核心特点：\n\n*   **多语言支持：** M^3FinMeeting 支持英语、中文和日语，有助于增强对不同语言背景下金融讨论的理解能力。\n*   **多行业覆盖：** 该数据集涵盖了由全球行业分类标准（GICS）定义的多个行业领域，确保了基准能够跨越广泛的金融活动范围。\n*   **多任务评估：** M^3FinMeeting 包含三项任务，以实现更真实和全面的理解能力评估：\n    *   摘要生成\n    *   问答对提取\n    *   问答\n\n通过对七个流行 LLMs 进行的实验结果表明，即使是最先进的长上下文模型也存在显著的改进空间。这证明了 M^3FinMeeting 作为评估 LLMs 金融会议理解技能的有效性。该研究已被 ACL-2025 接受。",
      "shortSummary": "M^3FinMeeting 是一个新颖的多语言、多行业、多任务金融会议理解评估数据集。它旨在弥补现有金融LLM基准无法捕捉真实会议动态的不足。该数据集支持英、中、日三语，覆盖GICS定义的多个行业，并包含摘要、问答对提取和问答三项任务。实验表明，即使是先进的LLM在金融会议理解方面仍有提升空间，验证了M^3FinMeeting作为评估工具的有效性。",
      "translated_title": "M^3FinMeeting：一个多语言、多行业、多任务的金融会议理解评估数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent breakthroughs in large language models (LLMs) have led to the development of new benchmarks for evaluating their performance in the financial domain. However, current financial benchmarks often rely on news articles, earnings reports, or announcements, making it challenging to capture the real-world dynamics of financial meetings. To address this gap, we propose a novel benchmark called M^3FinMeeting, which is a multilingual, multi-sector, and multi-task dataset designed for financial meeting understanding. First, M^3FinMeeting supports English, Chinese, and Japanese, enhancing comprehension of financial discussions in diverse linguistic contexts. Second, it encompasses various industry sectors defined by the Global Industry Classification Standard (GICS), ensuring that the benchmark spans a broad range of financial activities. Finally, M^3FinMeeting includes three tasks: summarization, question-answer (QA) pair extraction, and question answering, facilitating a more realistic and comprehensive evaluation of understanding. Experimental results with seven popular LLMs reveal that even the most advanced long-context models have significant room for improvement, demonstrating the effectiveness of M^3FinMeeting as a benchmark for assessing LLMs' financial meeting comprehension skills."
    },
    {
      "title": "LumosFlow: 运动引导的长视频生成 (原标题: LumosFlow: Motion-Guided Long Video Generation)",
      "link": "https://arxiv.org/abs/2506.02497",
      "pubDate": "Tue, 03 Jun 2025 02:25:00 GMT",
      "isoDate": "2025-06-03T02:25:00.000Z",
      "creator": "Jiahao Chen, Hangjie Yuan, Yichen Qian, Jingyun Liang, Jiazheng Xing, Pengwei Liu, Weihua Chen, Fan Wang, Bing Su",
      "summary": "## LumosFlow: 运动引导的长视频生成\n\n### 引言\n\n长视频生成在娱乐和模拟等领域具有广泛应用，因此受到越来越多的关注。然而，合成时间连贯且视觉引人注目的长序列仍然是一个艰巨的挑战。传统方法通常通过顺序生成和连接短片段，或分层生成关键帧然后插值中间帧来合成长视频。但这两种方法都面临显著挑战，导致时间重复或不自然的过渡等问题。\n\n### LumosFlow 框架概述\n\n本文重新审视了分层长视频生成流程，并引入了 LumosFlow 框架，该框架明确引入了运动引导。LumosFlow 旨在解决现有方法中时间连贯性差和过渡不自然的问题。\n\n### 关键组件与流程\n\nLumosFlow 的生成流程分为以下主要步骤：\n\n*   **关键帧生成**\n    *   首先，LumosFlow 利用大型运动文本到视频扩散模型（Large Motion Text-to-Video Diffusion Model, LMTV-DM）来生成具有较大运动间隔的关键帧。这种方法确保了生成长视频的内容多样性。\n\n*   **中间帧插值与细化**\n    *   鉴于关键帧之间上下文过渡插值的复杂性，LumosFlow 将中间帧插值分解为运动生成和事后细化两个阶段。\n    *   **运动生成**：对于每对关键帧，潜在光流扩散模型（Latent Optical Flow Diffusion Model, LOF-DM）负责合成复杂且大运动的光流。\n    *   **事后细化**：随后，MotionControlNet 对扭曲结果进行细化，以提高质量并引导中间帧的生成。\n\n### 性能优势\n\n与传统的视频帧插值方法相比，LumosFlow 实现了 **15 倍**的插值能力，确保了相邻帧之间合理且连续的运动。\n\n### 实验结果与可用性\n\n实验结果表明，LumosFlow 方法能够生成具有一致运动和外观的长视频。该项目的代码和模型将在论文被接受后公开发布。",
      "shortSummary": "LumosFlow 是一个运动引导的长视频生成框架，旨在解决现有方法中时间连贯性差和过渡不自然的问题。它首先利用 LMTV-DM 生成具有多样内容的关键帧，然后通过 LOF-DM 合成复杂光流，并由 MotionControlNet 细化，实现中间帧的插值。该方法能达到 15 倍的插值效果，确保视频运动和外观的一致性与连续性。",
      "translated_title": "LumosFlow: 运动引导的长视频生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Long video generation has gained increasing attention due to its widespread applications in fields such as entertainment and simulation. Despite advances, synthesizing temporally coherent and visually compelling long sequences remains a formidable challenge. Conventional approaches often synthesize long videos by sequentially generating and concatenating short clips, or generating key frames and then interpolate the intermediate frames in a hierarchical manner. However, both of them still remain significant challenges, leading to issues such as temporal repetition or unnatural transitions. In this paper, we revisit the hierarchical long video generation pipeline and introduce LumosFlow, a framework introduce motion guidance explicitly. Specifically, we first employ the Large Motion Text-to-Video Diffusion Model (LMTV-DM) to generate key frames with larger motion intervals, thereby ensuring content diversity in the generated long videos. Given the complexity of interpolating contextual transitions between key frames, we further decompose the intermediate frame interpolation into motion generation and post-hoc refinement. For each pair of key frames, the Latent Optical Flow Diffusion Model (LOF-DM) synthesizes complex and large-motion optical flows, while MotionControlNet subsequently refines the warped results to enhance quality and guide intermediate frame generation. Compared with traditional video frame interpolation, we achieve 15x interpolation, ensuring reasonable and continuous motion between adjacent frames. Experiments show that our method can generate long videos with consistent motion and appearance. Code and models will be made publicly available upon acceptance. Our project page: https://jiahaochen1.github.io/LumosFlow/"
    },
    {
      "title": "多模态深度研究员：使用智能体框架从零开始生成文本-图表交错报告 (原标题: Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework)",
      "link": "https://arxiv.org/abs/2506.02454",
      "pubDate": "Tue, 03 Jun 2025 01:18:19 GMT",
      "isoDate": "2025-06-03T01:18:19.000Z",
      "creator": "Zhaorui Yang, Bo Pan, Han Wang, Yiyao Wang, Xingyu Liu, Minfeng Zhu, Bo Zhang, Wei Chen",
      "summary": "### 多模态深度研究员：生成文本-图表交错报告的创新框架\n\n**背景与挑战**\n\n*   **可视化重要性：** 视觉化在有效传达概念和信息方面发挥着关键作用。\n*   **LLM进展：** 大型语言模型（LLMs）在推理和检索增强生成方面的最新进展，使其能够进行深度研究并生成全面的报告。\n*   **现有局限：** 尽管LLM取得了进步，但现有的深度研究框架主要侧重于生成纯文本内容，而对文本和可视化交错内容的自动化生成探索不足。\n*   **核心挑战：** 这一新任务面临的主要挑战在于如何设计信息丰富的可视化图表，并有效地将其与文本报告整合。\n\n**提出的解决方案**\n\n为了解决上述挑战，研究人员提出了两项关键创新：\n\n1.  **可视化形式化描述（Formal Description of Visualization, FDV）：**\n    *   这是一种结构化的图表文本表示方法。\n    *   它使LLMs能够学习并生成多样化、高质量的可视化内容。\n\n2.  **多模态深度研究员（Multimodal DeepResearcher）：**\n    *   这是一个基于智能体（agentic）的框架。\n    *   它将文本-图表交错报告的生成任务分解为四个核心阶段：\n        1.  **研究（Researching）：** 收集和整理信息。\n        2.  **范例报告文本化（Exemplar report textualization）：** 将示例报告转化为文本形式，供模型学习。\n        3.  **规划（Planning）：** 制定报告结构和内容布局，包括文本与图表的整合策略。\n        4.  **多模态报告生成（Multimodal report generation）：** 实际生成包含文本和图表的最终报告。\n\n**评估与成果**\n\n*   **评估基准：** 为了评估生成的多模态报告，研究团队开发了 **MultimodalReportBench**。\n    *   该基准包含100个多样化的主题作为输入。\n    *   同时，它还配备了5个专门的评估指标。\n*   **实验结果：**\n    *   跨模型和评估方法的广泛实验证明了多模态深度研究员的有效性。\n    *   值得注意的是，在使用相同的Claude 3.7 Sonnet模型时，多模态深度研究员相对于基线方法取得了82%的总体胜率，显示出其卓越的性能。",
      "shortSummary": "《多模态深度研究员》提出了一种创新框架，旨在解决大型语言模型（LLMs）在生成文本与图表交错报告方面的不足。该框架引入了“可视化形式化描述（FDV）”来帮助LLMs理解和生成高质量图表，并通过一个四阶段的智能体系统（研究、文本化、规划、生成）来自动化报告创建过程。通过“MultimodalReportBench”基准测试，该方法在Claude 3.7 Sonnet模型上表现出色，相对于基线方法取得了82%的胜率，显著提升了多模态报告的生成能力。",
      "translated_title": "多模态深度研究员：使用智能体框架从零开始生成文本-图表交错报告",
      "images": [],
      "contentSource": "完整文章",
      "content": "Visualizations play a crucial part in effective communication of concepts and information. Recent advances in reasoning and retrieval augmented generation have enabled Large Language Models (LLMs) to perform deep research and generate comprehensive reports. Despite its progress, existing deep research frameworks primarily focus on generating text-only content, leaving the automated generation of interleaved texts and visualizations underexplored. This novel task poses key challenges in designing informative visualizations and effectively integrating them with text reports. To address these challenges, we propose Formal Description of Visualization (FDV), a structured textual representation of charts that enables LLMs to learn from and generate diverse, high-quality visualizations. Building on this representation, we introduce Multimodal DeepResearcher, an agentic framework that decomposes the task into four stages: (1) researching, (2) exemplar report textualization, (3) planning, and (4) multimodal report generation. For the evaluation of generated multimodal reports, we develop MultimodalReportBench, which contains 100 diverse topics served as inputs along with 5 dedicated metrics. Extensive experiments across models and evaluation methods demonstrate the effectiveness of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet model, Multimodal DeepResearcher achieves an 82\\% overall win rate over the baseline method."
    },
    {
      "title": "VS-Bench：评估多智能体环境中VLM的战略推理和决策能力 (原标题: VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in Multi-Agent Environments)",
      "link": "https://arxiv.org/abs/2506.02387",
      "pubDate": "Mon, 02 Jun 2025 22:57:38 GMT",
      "isoDate": "2025-06-02T22:57:38.000Z",
      "creator": "Zelai Xu, Zhexuan Xu, Xiangmin Yi, Huining Yuan, Xinlei Chen, Yi Wu, Chao Yu, Yu Wang",
      "summary": "## VS-Bench：评估多智能体环境中VLM的战略推理和决策能力\n\n### 背景与挑战\n\n*   **现有基准局限性**：尽管视觉语言模型（VLMs）在交互式智能体任务中取得了显著进展，但现有基准测试主要局限于单智能体或纯文本环境。\n*   **现实世界复杂性**：实际场景通常涉及多个智能体在丰富的视觉和语言上下文中进行交互，这带来了多模态观察和战略性互动的双重挑战。\n\n### VS-Bench介绍\n\n*   **目的**：为弥补上述空白，论文引入了**Visual Strategic Bench (VS-Bench)**，这是一个多模态基准测试，旨在评估VLM在多智能体环境中的战略推理和决策能力。\n*   **环境组成**：VS-Bench包含八个基于视觉的环境，涵盖了合作、竞争和混合动机的交互场景。\n*   **评估目标**：该基准旨在评估智能体预测其他智能体未来行动的能力，以及优化长期目标的能力。\n\n### 评估维度\n\nVS-Bench考虑了两个互补的评估维度：\n\n1.  **离线评估**：通过下一行动预测的准确率来评估战略推理能力。\n2.  **在线评估**：通过归一化回合回报来评估决策能力。\n\n### 实验与发现\n\n*   **模型测试**：研究对十四个领先的VLM进行了广泛实验。\n*   **性能差距**：实验结果揭示了当前模型与最优性能之间存在显著差距。\n*   **最佳表现**：表现最佳的模型在预测准确率上达到47.8%，在归一化回报上达到24.3%。\n\n### 深入分析与未来展望\n\n*   **分析内容**：研究进一步对多模态观察、测试时扩展性、社会行为以及VLM智能体的失败案例进行了深入分析。\n*   **研究基础**：通过标准化评估流程并突出现有模型的局限性，VS-Bench旨在为未来战略性多模态智能体的研究奠定坚实基础。\n*   **资源可用性**：相关的代码和数据已公开。",
      "shortSummary": "VS-Bench是一个新的多模态基准，旨在评估视觉语言模型（VLM）在多智能体环境中的战略推理和决策能力。它包含八个视觉驱动的合作、竞争及混合动机场景。通过离线（行动预测准确率）和在线（归一化回报）评估，研究发现当前领先的VLM与最优性能存在显著差距，最佳模型预测准确率47.8%，归一化回报24.3%。VS-Bench旨在推动战略性多模态智能体的研究。",
      "translated_title": "VS-Bench：评估多智能体环境中VLM的战略推理和决策能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in Vision Language Models (VLMs) have expanded their capabilities to interactive agent tasks, yet existing benchmarks remain limited to single-agent or text-only environments. In contrast, real-world scenarios often involve multiple agents interacting within rich visual and linguistic contexts, posing challenges with both multimodal observations and strategic interactions. To bridge this gap, we introduce Visual Strategic Bench (VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning and decision-making in multi-agent environments. VS-Bench comprises eight vision-grounded environments spanning cooperative, competitive, and mixed-motive interactions, designed to assess agents' ability to predict others' future moves and optimize for long-term objectives. We consider two complementary evaluation dimensions, including offline evaluation of strategic reasoning by next-action prediction accuracy and online evaluation of decision-making by normalized episode return. Extensive experiments of fourteen leading VLMs reveal a significant gap between current models and optimal performance, with the best models attaining 47.8% prediction accuracy and 24.3% normalized return. We further conduct in-depth analyses on multimodal observations, test-time scaling, social behaviors, and failure cases of VLM agents. By standardizing the evaluation and highlighting the limitations of existing models, we envision VS-Bench as a foundation for future research on strategic multimodal agents. Code and data are available at https://vs-bench.github.io."
    },
    {
      "title": "开源推理模型缺失的一环：一个用于缓解强化学习中短CoT LLM冷启动问题的数据集 (原标题: One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL)",
      "link": "https://arxiv.org/abs/2506.02338",
      "pubDate": "Mon, 02 Jun 2025 20:29:15 GMT",
      "isoDate": "2025-06-02T20:29:15.000Z",
      "creator": "Hyungjoo Chae, Dongjin Kang, Jihyuk Kim, Beong-woo Kwak, Sunghyun Park, Haeju Park, Jinyoung Yeo, Moontae Lee, Kyungjae Lee",
      "summary": "### 开源推理模型缺失的一环：一个用于缓解强化学习中短CoT LLM冷启动问题的数据集\n\n**背景与问题：**\n\n*   当前大型推理模型（LRM）的开发，如R1，通常依赖于在R1的长链式思维（CoT）推理结果上训练新的语言模型。\n*   这种对现有模型的持续依赖是推进该领域发展的关键限制，尤其是在独立LRM开发方面。\n\n**研究目标：**\n\n*   本文旨在探索使用未针对推理时间扩展训练的短CoT LLM构建长CoT数据集的可能性，以迈出独立LRM开发的第一步。\n\n**提出的解决方案与数据集：**\n\n*   **数据集名称：** “Long CoT Collection”\n*   **数据集内容：** 包含10万个CoT推理过程。\n*   **数据生成方式：** 使用现有短CoT LLM进行标注。\n\n**方法论：**\n\n*   开发了一个管道，将R1的新颖推理策略引入到短CoT LLM中。\n*   这使得短CoT LLM能够进行更长时间的思考。\n*   引入了对“思考预算”（thought budget）的控制，以更好地管理“过度思考”问题。\n\n**实验结果与验证：**\n\n*   **数据集质量：** 广泛分析验证了该数据集的质量与R1相当或略低于R1。\n*   **推理能力提升：** 实验表明，在该数据集上进行训练不仅能增强通用推理技能。\n*   **强化学习基础：** 还为强化学习（RL）提供了坚实的基础——使用该数据初始化的模型在RLVR中实现了2-3倍的更大收益。",
      "shortSummary": "针对开源推理模型对现有大型推理模型（如R1）的依赖问题，本文提出了“Long CoT Collection”数据集。该数据集包含10万个长链式思维（CoT）推理过程，通过一个管道将R1的推理策略引入到短CoT LLM中生成。实验证明，该数据集质量与R1相当，能增强通用推理能力，并为强化学习提供强大基础，使模型在RLVR中获得2-3倍的收益，是独立开发大型推理模型的重要一步。",
      "translated_title": "开源推理模型缺失的一环：一个用于缓解强化学习中短CoT LLM冷启动问题的数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "With the release of R1, a publicly available large reasoning model (LRM), researchers commonly train new LRMs by training language models on R1's long chain-of-thought (CoT) inferences. While prior works show that LRMs' capabilities can be reproduced through direct distillation, the continued reliance on the existing models (e.g., R1) remains a critical limitation in advancing the field. As a first step toward independent LRM development, this paper explores the possibility of constructing a long CoT dataset with LLMs that are not trained for inference-time scaling. To this end, we present the Long CoT Collection, a dataset of 100K CoT rationales annotated using existing short CoT LLMs. We develop a pipeline that induces o1's novel reasoning strategies into short CoT LLMs, enabling them to think longer and introducing controllability over the thought budget to better manage the overthinking problem. Our extensive analyses validate that our dataset achieves quality comparable to--or slightly below--R1. Furthermore, our experiments demonstrate that training on our dataset not only strengthens general reasoning skills, but also provides a strong foundation for reinforcement learning--models initialized on our data achieve 2-3x larger gains with RLVR."
    },
    {
      "title": "重新审视LRP：位置归因作为Transformer可解释性中缺失的关键要素 (原标题: Revisiting LRP: Positional Attribution as the Missing Ingredient for Transformer Explainability)",
      "link": "https://arxiv.org/abs/2506.02138",
      "pubDate": "Mon, 02 Jun 2025 14:07:55 GMT",
      "isoDate": "2025-06-02T14:07:55.000Z",
      "creator": "Yarden Bakish, Itamar Zimerman, Hila Chefer, Lior Wolf",
      "summary": "# 重新审视LRP：Transformer可解释性的新视角\n\nTransformer模型的可解释性工具开发是深度学习研究中的一个关键领域。层级相关性传播（LRP）是一种很有前景的方法，它通过预定义规则将相关性分数向后传播到输入空间。\n\n## 现有LRP方法的局限性\n\n然而，现有基于LRP的Transformer可解释性方法存在一个显著的局限性：\n*   它们完全忽略了Transformer架构中的一个关键组件：**位置编码（PE）**。\n*   这种忽略导致了**守恒性属性的违反**。\n*   同时，也**丢失了与结构和位置特征相关的重要且独特的关联性**。\n\n## 提出的解决方案：位置归因\n\n为了解决上述限制，本文提出了一种新的方法：\n*   **重新定义输入空间**：将Transformer可解释性的输入空间重新表述为一组**位置-token对**。\n*   **专门的LRP规则**：基于此，提出了专门的、有理论基础的LRP规则。\n*   **支持多种位置编码**：这些规则旨在跨各种位置编码方法传播归因，包括：\n    *   旋转式位置编码（Rotary PE）\n    *   可学习式位置编码（Learnable PE）\n    *   绝对位置编码（Absolute PE）\n\n## 实验与成果\n\n*   **广泛的实验验证**：研究人员对该方法进行了广泛的实验，包括在微调分类器和零样本基础模型（如LLaMA 3）上的测试。\n*   **显著优于现有技术**：实验结果表明，该方法在视觉和自然语言处理（NLP）的可解释性任务中，显著优于现有最先进的技术。\n\n## 代码可用性\n\n该方法的代码已公开可用。",
      "shortSummary": "本文重新审视了LRP在Transformer可解释性中的应用。现有LRP方法忽略了位置编码（PE），导致守恒性违反和关联性丢失。为解决此问题，研究提出将输入空间重新定义为位置-token对，并设计了专门的LRP规则来处理各种PE方法。实验证明，该方法在视觉和NLP可解释性任务中显著优于现有技术，提升了Transformer的可解释性。代码已公开。",
      "translated_title": "重新审视LRP：位置归因作为Transformer可解释性中缺失的关键要素",
      "images": [],
      "contentSource": "完整文章",
      "content": "The development of effective explainability tools for Transformers is a crucial pursuit in deep learning research. One of the most promising approaches in this domain is Layer-wise Relevance Propagation (LRP), which propagates relevance scores backward through the network to the input space by redistributing activation values based on predefined rules. However, existing LRP-based methods for Transformer explainability entirely overlook a critical component of the Transformer architecture: its positional encoding (PE), resulting in violation of the conservation property, and the loss of an important and unique type of relevance, which is also associated with structural and positional features. To address this limitation, we reformulate the input space for Transformer explainability as a set of position-token pairs. This allows us to propose specialized theoretically-grounded LRP rules designed to propagate attributions across various positional encoding methods, including Rotary, Learnable, and Absolute PE. Extensive experiments with both fine-tuned classifiers and zero-shot foundation models, such as LLaMA 3, demonstrate that our method significantly outperforms the state-of-the-art in both vision and NLP explainability tasks. Our code is publicly available."
    },
    {
      "title": "WebChoreArena：评估网络浏览代理在现实繁琐网络任务中的表现 (原标题: WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks)",
      "link": "https://arxiv.org/abs/2506.01952",
      "pubDate": "Mon, 02 Jun 2025 13:59:45 GMT",
      "isoDate": "2025-06-02T13:59:45.000Z",
      "creator": "Atsuyuki Miyai, Zaiying Zhao, Kazuki Egashira, Atsuki Sato, Tatsumi Sunada, Shota Onohara, Hiromasa Yamanishi, Mashiro Toyooka, Kunato Nishina, Ryoma Maeda, Kiyoharu Aizawa, Toshihiko Yamasaki",
      "summary": "## WebChoreArena：评估网络浏览代理在现实繁琐网络任务中的表现\n\n**引言**\n\n大型语言模型（LLM）驱动的网络浏览代理能够以类似人类的方式操作网络浏览器，为自动化日常任务提供了高度透明的途径。随着网络代理的能力不断增强，并在通用浏览任务中展现出熟练度，一个关键问题浮出水面：它们能否超越通用浏览，稳健地处理那些对人类而言繁琐、复杂或通常会避免的“家务”式任务？\n\n**WebChoreArena 介绍**\n\n本文引入了 **WebChoreArena**，这是一个全新的、完全可复现的基准测试。它包含 532 个精心策划的任务，旨在将 WebArena 的范围从通用浏览扩展到更耗时、更繁琐的任务。\n\n**WebChoreArena 集成的三大关键挑战**\n\nWebChoreArena 系统地整合了以下三个核心挑战，以更全面地评估代理的能力：\n\n1.  **海量记忆任务 (Massive Memory tasks)**：要求代理能够从观察中准确检索大量信息。\n2.  **计算任务 (Calculation tasks)**：要求代理具备精确的数学推理能力。\n3.  **长期记忆任务 (Long-Term Memory tasks)**：要求代理在多个网页之间保持长期记忆。\n\n**基准的构建与优势**\n\nWebChoreArena 建立在四个完全可复现且广泛采用的 WebArena 模拟环境之上，这确保了严格的可复现性，并能够与已建立的 WebArena 基准进行公平、直接的比较，从而为代理的进展提供关键洞察。\n\n**实验结果与发现**\n\n实验结果表明，随着 LLM 的发展（以 GPT-4o、Claude 3.7 Sonnet 和 Gemini 2.5 Pro 为代表），它们在 WebChoreArena 上的性能观察到显著提升。这些发现表明，WebChoreArena 非常适合更清晰地衡量最先进 LLM 的进步。然而，结果也指出，即使是 Gemini 2.5 Pro，与 WebArena 相比，仍有很大的改进空间，这凸显了 WebChoreArena 所带来的更高挑战。",
      "shortSummary": "WebChoreArena 是一个新颖的、可复现的基准测试，旨在评估网络浏览代理处理现实世界中繁琐复杂任务的能力。它包含 532 个任务，扩展了 WebArena 的范围，并集成了海量记忆、计算和长期记忆三大挑战。实验结果显示，GPT-4o、Claude 3.7 Sonnet 和 Gemini 2.5 Pro 等先进大型语言模型在 WebChoreArena 上表现出显著进步，但仍有较大提升空间，凸显了该基准的挑战性。",
      "translated_title": "WebChoreArena：评估网络浏览代理在现实繁琐网络任务中的表现",
      "images": [],
      "contentSource": "完整文章",
      "content": "Powered by a large language model (LLM), a web browsing agent operates web browsers in a human-like manner and offers a highly transparent path toward automating a wide range of everyday tasks. As web agents become increasingly capable and demonstrate proficiency in general browsing tasks, a critical question emerges: Can they go beyond general browsing to robustly handle tasks that are tedious and complex, or chores that humans often avoid doing themselves? In this paper, we introduce WebChoreArena, a new fully reproducible benchmark comprising 532 carefully curated tasks designed to extend the scope of WebArena beyond general browsing to more labor-intensive and tedious tasks. WebChoreArena systematically integrates three key challenges: (i) Massive Memory tasks requiring accurate retrieval of large amounts of information in the observations, (ii) Calculation tasks demanding precise mathematical reasoning, and (iii) Long-Term Memory tasks necessitating long-term memory across multiple webpages. Built on top of the fully reproducible and widely adopted four WebArena simulation environments, WebChoreArena ensures strict reproducibility and enables fair, direct comparisons with the established WebArena benchmark, offering key insights into agent progress. Our experimental results demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7 Sonnet, and Gemini 2.5 Pro, significant improvements in performance are observed on WebChoreArena. These findings suggest that WebChoreArena is well-suited to measure the advancement of state-of-the-art LLMs with greater clarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro, there remains substantial room for improvement compared to WebArena, highlighting the increased challenges posed by WebChoreArena."
    },
    {
      "title": "基于协作轨迹控制的机器人操作视频生成学习 (原标题: Learning Video Generation for Robotic Manipulation with Collaborative Trajectory Control)",
      "link": "https://arxiv.org/abs/2506.01943",
      "pubDate": "Mon, 02 Jun 2025 13:57:06 GMT",
      "isoDate": "2025-06-02T13:57:06.000Z",
      "creator": "Xiao Fu, Xintao Wang, Xian Liu, Jianhong Bai, Runsen Xu, Pengfei Wan, Di Zhang, Dahua Lin",
      "summary": "## RoboMaster：协作轨迹控制下的机器人操作视频生成\n\n### 引言与问题背景\n\n*   **视频扩散模型的潜力：** 近期视频扩散模型在生成机器人决策数据方面展现出巨大潜力，其中轨迹条件进一步实现了精细控制。\n*   **现有方法的局限性：** 然而，现有基于轨迹的方法主要关注单个物体的运动，难以捕捉复杂机器人操作中至关重要的多物体交互。\n*   **核心挑战：** 这种局限性源于重叠区域的多特征纠缠，导致视觉保真度下降。\n\n### RoboMaster 框架\n\n*   **创新点：** 本文提出了一个名为 RoboMaster 的新型框架，通过“协作轨迹公式”来建模物体间的动态。\n*   **交互过程分解：** 与以往分解物体的方法不同，RoboMaster 的核心在于将交互过程分解为三个子阶段：\n    *   **交互前 (pre-interaction)**\n    *   **交互中 (interaction)**\n    *   **交互后 (post-interaction)**\n*   **主导物体特征建模：** 每个阶段都使用“主导物体”的特征进行建模：\n    *   在交互前和交互后阶段，主导物体是机器人手臂。\n    *   在交互中阶段，主导物体是被操作的物体。\n    *   **优势：** 这种方法有效缓解了先前工作中在交互过程中出现的多物体特征融合的缺点。\n*   **语义一致性：** 为了进一步确保视频中主体语义的一致性，RoboMaster 引入了“外观感知和形状感知潜在表示”。\n\n### 实验与成果\n\n*   **评估数据集：** 在具有挑战性的 Bridge V2 数据集上进行了广泛的实验。\n*   **实际环境验证：** 还在实际环境中（in-the-wild）进行了评估。\n*   **性能表现：** 实验结果表明，RoboMaster 方法优于现有方法，在机器人操作的轨迹控制视频生成方面建立了新的最先进性能。",
      "shortSummary": "RoboMaster是一个新颖的机器人操作视频生成框架，旨在解决现有方法在多物体交互中视觉保真度下降的问题。它通过将交互过程分解为交互前、中、后三个阶段，并利用主导物体特征进行建模，有效避免了多特征纠缠。结合外观和形状感知潜在表示，RoboMaster在Bridge V2数据集和实际评估中表现出色，达到了轨迹控制视频生成领域的最新水平。",
      "translated_title": "基于协作轨迹控制的机器人操作视频生成学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in video diffusion models have demonstrated strong potential for generating robotic decision-making data, with trajectory conditions further enabling fine-grained control. However, existing trajectory-based methods primarily focus on individual object motion and struggle to capture multi-object interaction crucial in complex robotic manipulation. This limitation arises from multi-feature entanglement in overlapping regions, which leads to degraded visual fidelity. To address this, we present RoboMaster, a novel framework that models inter-object dynamics through a collaborative trajectory formulation. Unlike prior methods that decompose objects, our core is to decompose the interaction process into three sub-stages: pre-interaction, interaction, and post-interaction. Each stage is modeled using the feature of the dominant object, specifically the robotic arm in the pre- and post-interaction phases and the manipulated object during interaction, thereby mitigating the drawback of multi-object feature fusion present during interaction in prior work. To further ensure subject semantic consistency throughout the video, we incorporate appearance- and shape-aware latent representations for objects. Extensive experiments on the challenging Bridge V2 dataset, as well as in-the-wild evaluation, demonstrate that our method outperforms existing approaches, establishing new state-of-the-art performance in trajectory-controlled video generation for robotic manipulation."
    },
    {
      "title": "超越二八定律：高熵少数令牌驱动大语言模型推理的有效强化学习 (原标题: Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning)",
      "link": "https://arxiv.org/abs/2506.01939",
      "pubDate": "Mon, 02 Jun 2025 13:54:39 GMT",
      "isoDate": "2025-06-02T13:54:39.000Z",
      "creator": "Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, Junyang Lin",
      "summary": "### 1. 研究背景与目的\n\n*   **背景**：可验证奖励强化学习（RLVR）已成为增强大型语言模型（LLM）推理能力的强大方法，但其内在机制尚不明确。\n*   **目的**：本研究首次从令牌熵模式的视角深入探索RLVR，全面分析不同令牌如何影响推理性能。\n\n### 2. 关键发现\n\n*   **链式思考（CoT）推理中的令牌熵模式**：\n    *   观察到只有一小部分令牌表现出高熵。\n    *   这些高熵令牌充当关键的“分岔点”，引导模型走向不同的推理路径。\n*   **RLVR训练中熵模式的演变**：\n    *   RLVR在很大程度上遵循基础模型的熵模式。\n    *   RLVR主要调整的是高熵令牌的熵值。\n*   **高熵令牌的重要性**：\n    *   这些发现强调了高熵令牌（即分岔令牌）对RLVR的重要性。\n    *   RLVR的有效性主要源于优化决定推理方向的高熵令牌。\n\n### 3. 改进RLVR的方法与结果\n\n*   **改进方法**：通过将策略梯度更新限制在分岔令牌上，从而改进了RLVR。\n*   **实验结果**：\n    *   **超越二八定律的发现**：仅利用20%的令牌，即可实现与全梯度更新相当的性能。\n    *   **性能提升**：\n        *   在Qwen3-8B基础模型上，性能与全梯度更新相当。\n        *   在Qwen3-32B模型上，AIME'25得分显著提升11.04，AIME'24得分提升7.71。\n        *   在Qwen3-14B模型上，AIME'25得分提升4.79，AIME'24得分提升5.21。\n    *   **强大的扩展趋势**：这些结果凸显了该方法在更大模型上的强大扩展趋势。\n    *   **对比实验**：仅对80%最低熵的令牌进行训练会导致性能显著下降。\n\n### 4. 结论与展望\n\n*   **核心结论**：RLVR的有效性主要源于优化决定推理方向的高熵令牌。\n*   **研究意义**：本研究结果共同强调了通过令牌熵视角理解RLVR的潜力，并通过利用高熵少数令牌来优化RLVR，以进一步提升LLM的推理能力。",
      "shortSummary": "本研究从令牌熵模式视角探索了可验证奖励强化学习（RLVR）如何增强大语言模型（LLM）的推理能力。研究发现，高熵的少数令牌是链式思考（CoT）推理中的关键“分岔点”，RLVR主要通过调整这些高熵令牌来提升性能。通过将策略梯度更新限制在这些高熵令牌上，研究者实现了在Qwen3-8B上与全梯度更新相当的性能，并在Qwen3-32B和Qwen3-14B上显著超越全梯度更新，证明了利用少数高熵令牌优化RLVR的有效性和扩展潜力。",
      "translated_title": "超越二八定律：高熵少数令牌驱动大语言模型推理的有效强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning."
    },
    {
      "title": "SynthRL：通过可验证数据合成扩展视觉推理 (原标题: SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis)",
      "link": "https://arxiv.org/abs/2506.02096",
      "pubDate": "Mon, 02 Jun 2025 13:45:16 GMT",
      "isoDate": "2025-06-02T13:45:16.000Z",
      "creator": "Zijian Wu, Jinjie Ni, Xiangyan Liu, Zichen Liu, Hang Yan, Michael Qizhe Shieh",
      "summary": "### SynthRL：通过可验证数据合成扩展视觉推理\n\n本文介绍了 **SynthRL**，一个可扩展且有保障的自动化数据扩展管道，旨在改进面向推理的强化学习（RL）训练中的视觉语言模型（VLMs）。\n\n#### 研究背景与目标\n\n*   **背景**：通过可验证奖励进行强化学习（RLVR）训练的视觉语言模型（VLMs）在有效扩展测试时计算方面已取得显著进展。\n*   **目标**：本文旨在探讨合成的RL数据如何进一步提升RLVR的性能，特别是在视觉推理任务中。\n\n#### SynthRL 框架\n\nSynthRL 包含三个关键阶段，旨在生成高质量、高难度的训练数据：\n\n1.  **选择种子问题**：根据适当的分布选择初始的种子问题。\n2.  **增强挑战性变体**：将选定的种子问题增强为更具挑战性的变体，同时严格保留原始问题的正确答案。\n3.  **保证验证阶段**：一个关键的验证阶段，确保合成数据的近乎完美正确性，并有效提升其难度。\n\n#### 实验结果与有效性\n\n*   **数据合成规模**：在 MMK12 数据集上的实验表明，SynthRL 能够从大约 8K 个种子样本中合成超过 3.3K 个额外的、可验证且具有挑战性的问题。\n*   **模型性能提升**：\n    *   使用 SynthRL 合成数据训练的模型在五个域外视觉数学推理基准测试中均取得了持续的性能提升。\n    *   与仅使用种子数据训练的基线模型相比，性能有显著改善。\n*   **对复杂推理的促进**：详细分析显示，性能提升在最具挑战性的评估样本上更为显著，这突出表明 SynthRL 在激发更深层次、更复杂推理模式方面的有效性。\n\n#### 结论\n\nSynthRL 提供了一个有效且可扩展的方法，通过合成高质量、高难度的训练数据来增强视觉推理模型的性能，尤其在处理复杂推理任务时表现出卓越的能力。",
      "shortSummary": "SynthRL 是一种用于扩展视觉推理的强化学习数据合成管道。它通过选择、增强和验证种子问题，生成更多挑战性、可验证的训练数据。实验表明，SynthRL 从 MMK12 数据集合成了超过 3.3K 个额外问题。使用这些数据训练的模型在多个视觉数学推理基准测试中表现出显著提升，尤其在最困难的样本上效果更明显，证明了其在激发复杂推理模式方面的有效性。",
      "translated_title": "SynthRL：通过可验证数据合成扩展视觉推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-language models (VLMs) trained via reinforcement learning with verifiable reward (RLVR) have shown notable progress in scaling test-time compute effectively. In this work, we investigate how synthesized RL data can further improve RLVR. To this end, we propose SynthRL-a scalable and guaranteed pipeline for automatic data scaling in reasoning-oriented RL training. SynthRL comprises three key stages: (1) selecting seed questions with appropriate distribution, (2) augmenting them into more challenging variants while preserving the original answers, and (3) a guaranteed verification stage that ensures near-perfect correctness and difficulty enhancement. Our empirical experiments demonstrate SynthRL's scalability and effectiveness. When applied to the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable, challenging questions from approximately 8K seed samples. Models trained with our synthesized data achieve consistent gains across five out-of-domain visual math reasoning benchmarks, with a significant improvement over baseline models trained on seed data alone. Notably, detailed analysis reveals that the gains are more pronounced on the most challenging evaluation samples, highlighting SynthRL's effectiveness in eliciting deeper and more complex reasoning patterns."
    },
    {
      "title": "何时行动，何时等待：任务型对话中意图可触发性的结构轨迹建模 (原标题: WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent Triggerability in Task-Oriented Dialogue)",
      "link": "https://arxiv.org/abs/2506.01881",
      "pubDate": "Mon, 02 Jun 2025 13:11:10 GMT",
      "isoDate": "2025-06-02T13:11:10.000Z",
      "creator": "Yaoyao Qian, Jindan Huang, Yuanli Wang, Simon Yu, Kyrie Zhixuan Zhou, Jiayuan Mao, Mingfu Liang, Hanhan Zhou",
      "summary": "## 何时行动，何时等待：任务型对话中意图可触发性的结构轨迹建模\n\n### 引言与问题背景\n\n任务型对话系统在处理用户话语时常面临挑战。尽管用户表达在语义上可能完整，但往往缺乏触发系统采取适当行动所需的结构化信息。这主要是因为用户通常不完全理解自身需求，而系统则需要精确的意图定义才能有效响应。当前基于大型语言模型（LLM）的代理无法有效区分语言上完整但上下文不可触发的表达，且缺乏用于协作式意图形成的框架。\n\n### STORM框架介绍\n\n为解决上述问题，本文提出了 **STORM** 框架。STORM旨在建模对话中的非对称信息动态，其核心在于模拟UserLLM（拥有完全内部访问权限）和AgentLLM（仅能观察行为）之间的对话过程。通过这种方式，STORM能够生成带注释的语料库，捕捉表达轨迹和潜在的认知转换，从而实现对协作理解发展进行系统分析。\n\n### 主要贡献\n\nSTORM框架及其研究带来了以下关键贡献：\n\n1.  **非对称信息处理的正式化**：首次在对话系统中正式化了非对称信息处理的概念。\n2.  **意图形成建模**：构建了意图形成模型，能够追踪协作理解的演变过程。\n3.  **评估指标**：提出了新的评估指标，不仅衡量任务性能，还衡量内部认知改进。\n\n### 实验与发现\n\n研究团队在四种不同的语言模型上进行了实验。实验结果揭示了一些重要发现：\n\n*   在某些特定场景下，适度的不确定性（40-60%）表现可能优于完全透明的信息状态。\n*   观察到模型特定的模式，这表明在人机协作中，需要重新考虑最佳信息完整性的策略。\n\n### 结论与意义\n\n这些研究发现有助于深入理解非对称推理动态，并为设计能够有效处理不确定性的对话系统提供了重要指导。",
      "shortSummary": "任务型对话系统常因用户意图不明确而难以行动。本文提出STORM框架，通过模拟用户与代理LLM间的非对称信息对话，建模意图形成和协作理解的演变。STORM生成带注释语料，并提出新的评估指标。实验发现，适度不确定性（40-60%）在某些情况下优于完全透明，这为设计更适应不确定性的人机对话系统提供了新思路。",
      "translated_title": "何时行动，何时等待：任务型对话中意图可触发性的结构轨迹建模",
      "images": [],
      "contentSource": "完整文章",
      "content": "Task-oriented dialogue systems often face difficulties when user utterances seem semantically complete but lack necessary structural information for appropriate system action. This arises because users frequently do not fully understand their own needs, while systems require precise intent definitions. Current LLM-based agents cannot effectively distinguish between linguistically complete and contextually triggerable expressions, lacking frameworks for collaborative intent formation. We present STORM, a framework modeling asymmetric information dynamics through conversations between UserLLM (full internal access) and AgentLLM (observable behavior only). STORM produces annotated corpora capturing expression trajectories and latent cognitive transitions, enabling systematic analysis of collaborative understanding development. Our contributions include: (1) formalizing asymmetric information processing in dialogue systems; (2) modeling intent formation tracking collaborative understanding evolution; and (3) evaluation metrics measuring internal cognitive improvements alongside task performance. Experiments across four language models reveal that moderate uncertainty (40-60%) can outperform complete transparency in certain scenarios, with model-specific patterns suggesting reconsideration of optimal information completeness in human-AI collaboration. These findings contribute to understanding asymmetric reasoning dynamics and inform uncertainty-calibrated dialogue system design."
    }
  ],
  "lastUpdated": "2025-06-04T09:38:43.980Z"
}