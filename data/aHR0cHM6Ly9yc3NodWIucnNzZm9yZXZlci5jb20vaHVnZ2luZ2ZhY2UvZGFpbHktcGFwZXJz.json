{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "WebDancer：迈向自主信息搜索代理 (原标题: WebDancer: Towards Autonomous Information Seeking Agency)",
      "link": "https://arxiv.org/abs/2505.22648",
      "pubDate": "Wed, 28 May 2025 13:57:07 GMT",
      "isoDate": "2025-05-28T13:57:07.000Z",
      "creator": "Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou",
      "summary": "本文介绍了WebDancer，一个旨在实现自主信息搜索的代理系统，以解决复杂的现实世界问题，这些问题通常需要深入的信息搜索和多步骤推理。该研究提出了一个从数据中心和训练阶段角度构建端到端代理信息搜索代理的统一范式。\n\n**WebDancer的核心范式包括四个关键阶段：**\n\n1.  **浏览数据构建**：为代理的训练准备基础浏览数据。\n2.  **轨迹采样**：从构建的数据中采样轨迹，以供后续学习使用。\n3.  **监督微调（SFT）**：进行监督微调，以实现有效的“冷启动”，确保代理在初始阶段具备基本能力。\n4.  **强化学习（RL）**：通过强化学习进一步增强代理的泛化能力，使其能适应更广泛的任务和场景。\n\n**系统实例化与评估：**\n\n*   WebDancer作为一个基于ReAct框架的网页代理实现了这一框架。\n*   在具有挑战性的信息搜索基准测试（GAIA和WebWalkerQA）上进行了实证评估。\n*   评估结果表明WebDancer表现出色，取得了显著的成果，突显了所提出训练范式的有效性。\n\n**研究洞察与未来方向：**\n\n*   对代理训练的进一步分析提供了宝贵的见解。\n*   为开发更强大的代理模型提供了可操作的、系统性的途径。\n\n该项目的代码和演示将在此处发布：`this https URL`。",
      "shortSummary": "WebDancer是一个旨在实现自主信息搜索的网页代理，它提出了一种数据中心和训练阶段的统一范式。该范式包含四个关键阶段：浏览数据构建、轨迹采样、监督微调和强化学习。WebDancer在GAIA和WebWalkerQA等信息搜索基准测试中表现出色，验证了其训练范式的有效性，并为开发更强大的代理模型提供了系统性方法。",
      "translated_title": "WebDancer：迈向自主信息搜索代理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-to-end agentic information seeking agents from a data-centric and training-stage perspective. Our approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation. We instantiate this framework in a web agent based on the ReAct, WebDancer. Empirical evaluations on the challenging information seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. Further analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models. The codes and demo will be released in https://github.com/Alibaba-NLP/WebAgent."
    },
    {
      "title": "强化学习在推理语言模型中的熵机制 (原标题: The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models)",
      "link": "https://arxiv.org/abs/2505.22617",
      "pubDate": "Wed, 28 May 2025 13:38:45 GMT",
      "isoDate": "2025-05-28T13:38:45.000Z",
      "creator": "Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, Zhiyuan Liu, Hao Peng, Lei Bai, Wanli Ouyang, Yu Cheng, Bowen Zhou, Ning Ding",
      "summary": "本文旨在解决强化学习（RL）应用于大型语言模型（LLM）进行推理时面临的一个主要障碍：策略熵（policy entropy）的崩溃。\n\n*   **问题描述与现象：**\n    *   在没有熵干预的RL训练中，策略熵在训练早期阶段急剧下降，这种现象普遍存在。\n    *   熵的下降导致探索能力减弱，并伴随着策略性能的饱和。\n\n*   **经验规律与理论预测：**\n    *   研究建立了一个经验转换方程：`R = -a * e^H + b`，其中 `H` 代表熵，`R` 代表下游性能。\n    *   这一经验规律强烈表明，策略性能是以策略熵为代价获得的，并受其耗尽的限制。\n    *   当熵 `H` 趋近于0时，性能的上限是完全可预测的 `R = -a + b`。\n    *   这一发现强调了熵管理对于RL持续探索和扩展计算的重要性。\n\n*   **熵动态机制的理论与实证分析：**\n    *   **理论推导：** 策略熵的变化是由动作概率与logits变化之间的协方差驱动的。在使用策略梯度（Policy Gradient）类算法时，这种协方差与优势函数（advantage）成比例。\n    *   **实证研究：** 协方差项的值与熵差异精确匹配，支持了理论结论。\n    *   **单调下降解释：** 协方差项在整个训练过程中大部分时间保持正值，这进一步解释了策略熵为何会单调下降。\n\n*   **提出的解决方案：**\n    *   基于对熵动态机制的理解，论文提出了通过限制高协方差token的更新来控制熵的方法。\n    *   具体提出了两种简单而有效的技术：\n        *   **Clip-Cov：** 裁剪高协方差token的更新。\n        *   **KL-Cov：** 对高协方差token施加KL惩罚。\n\n*   **实验结果：**\n    *   实验表明，这些方法能够鼓励探索，从而帮助策略避免熵崩溃，并实现更好的下游性能。",
      "shortSummary": "本文探讨了强化学习（RL）应用于推理语言模型（LLM）时策略熵崩溃的问题。研究发现策略性能与熵存在权衡关系，熵的耗尽限制了性能上限。理论和实证分析揭示，熵的变化由动作概率与logits变化的协方差驱动，且该协方差通常为正，导致熵单调下降。为解决此问题，论文提出了Clip-Cov和KL-Cov两种方法，通过限制高协方差token的更新来管理熵。实验证明这些方法能促进探索，避免熵崩溃，并提升性能。",
      "translated_title": "强化学习在推理语言模型中的熵机制",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper aims to overcome a major obstacle in scaling RL for reasoning with LLMs, namely the collapse of policy entropy. Such phenomenon is consistently observed across vast RL runs without entropy intervention, where the policy entropy dropped sharply at the early training stage, this diminished exploratory ability is always accompanied with the saturation of policy performance. In practice, we establish a transformation equation R=-a*e^H+b between entropy H and downstream performance R. This empirical law strongly indicates that, the policy performance is traded from policy entropy, thus bottlenecked by its exhaustion, and the ceiling is fully predictable H=0, R=-a+b. Our finding necessitates entropy management for continuous exploration toward scaling compute for RL. To this end, we investigate entropy dynamics both theoretically and empirically. Our derivation highlights that, the change in policy entropy is driven by the covariance between action probability and the change in logits, which is proportional to its advantage when using Policy Gradient-like algorithms. Empirical study shows that, the values of covariance term and entropy differences matched exactly, supporting the theoretical conclusion. Moreover, the covariance term stays mostly positive throughout training, further explaining why policy entropy would decrease monotonically. Through understanding the mechanism behind entropy dynamics, we motivate to control entropy by restricting the update of high-covariance tokens. Specifically, we propose two simple yet effective techniques, namely Clip-Cov and KL-Cov, which clip and apply KL penalty to tokens with high covariances respectively. Experiments show that these methods encourage exploration, thus helping policy escape entropy collapse and achieve better downstream performance."
    },
    {
      "title": "PrismLayers：高质量多层透明图像生成模型的开放数据 (原标题: PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image Generative Models)",
      "link": "https://arxiv.org/abs/2505.22523",
      "pubDate": "Wed, 28 May 2025 12:09:33 GMT",
      "isoDate": "2025-05-28T12:09:33.000Z",
      "creator": "Junwen Chen, Heyang Jiang, Yanbin Wang, Keming Wu, Ji Li, Chao Zhang, Keiji Yanai, Dong Chen, Yuhui Yuan",
      "summary": "### PrismLayers：高质量多层透明图像生成模型的开放数据\n\n**核心问题**\n\n当前，从文本提示生成高质量、多层透明图像的能力受到限制，主要原因是缺乏大规模、高质量的多层透明数据语料库。这阻碍了多层生成模型的发展，使其落后于传统的文本到图像模型。\n\n**主要贡献与解决方案**\n\n为了解决这一根本性挑战，本文提出了以下关键贡献：\n\n1.  **发布开放数据集**：\n    *   首次发布了开放的、超高保真的PrismLayers (PrismLayersPro) 数据集。\n    *   该数据集包含20万（2万）张多层透明图像，并附带精确的Alpha蒙版。\n\n2.  **引入训练无关的合成流程**：\n    *   提出了一种无需训练的合成流程，能够利用现成的扩散模型按需生成多层透明数据。\n\n3.  **提供强大的开源生成模型**：\n    *   发布了一个强大的开源多层生成模型ART+。\n    *   ART+在美学上与现代文本到图像生成模型（如FLUX.1-[dev]）相匹配。\n\n**关键技术细节**\n\n*   **LayerFLUX**：该技术擅长生成高质量的单一透明层，并带有精确的Alpha蒙版。\n*   **MultiLayerFLUX**：该技术在人工标注的语义布局指导下，将多个LayerFLUX的输出组合成完整的图像。\n\n**质量控制与模型性能**\n\n*   为了确保更高质量的数据，研究团队应用了严格的过滤阶段，以去除伪影和语义不匹配，随后进行了人工筛选。\n*   通过在合成的PrismLayersPro数据集上对最先进的ART模型进行微调，得到了ART+模型。\n*   在面对面的用户研究比较中，ART+在60%的情况下优于原始ART模型。\n*   ART+甚至能够匹配FLUX.1-[dev]模型生成的图像的视觉质量。\n\n**预期影响**\n\n本文的工作有望为多层透明图像生成任务建立坚实的数据集基础，从而推动需要精确、可编辑和视觉上引人注目的分层图像的研究和应用。",
      "shortSummary": "为解决多层透明图像生成模型缺乏高质量数据的挑战，本文发布了首个开放的PrismLayersPro数据集（20万/2万张图像）。同时，引入了一种无需训练的合成流程，并推出了强大的开源多层生成模型ART+。ART+在用户研究中表现优异，甚至能匹配顶级模型的视觉质量。这些贡献为多层透明图像生成任务奠定了坚实基础，有望推动相关研究与应用。",
      "translated_title": "PrismLayers：高质量多层透明图像生成模型的开放数据",
      "images": [],
      "contentSource": "完整文章",
      "content": "Generating high-quality, multi-layer transparent images from text prompts can unlock a new level of creative control, allowing users to edit each layer as effortlessly as editing text outputs from LLMs. However, the development of multi-layer generative models lags behind that of conventional text-to-image models due to the absence of a large, high-quality corpus of multi-layer transparent data. In this paper, we address this fundamental challenge by: (i) releasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro) dataset of 200K (20K) multilayer transparent images with accurate alpha mattes, (ii) introducing a trainingfree synthesis pipeline that generates such data on demand using off-the-shelf diffusion models, and (iii) delivering a strong, open-source multi-layer generation model, ART+, which matches the aesthetics of modern text-to-image generation models. The key technical contributions include: LayerFLUX, which excels at generating high-quality single transparent layers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple LayerFLUX outputs into complete images, guided by human-annotated semantic layout. To ensure higher quality, we apply a rigorous filtering stage to remove artifacts and semantic mismatches, followed by human selection. Fine-tuning the state-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which outperforms the original ART in 60% of head-to-head user study comparisons and even matches the visual quality of images generated by the FLUX.1-[dev] model. We anticipate that our work will establish a solid dataset foundation for the multi-layer transparent image generation task, enabling research and applications that require precise, editable, and visually compelling layered imagery."
    },
    {
      "title": "通过GRPO对多模态大语言模型推理进行无监督后训练 (原标题: Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO)",
      "link": "https://arxiv.org/abs/2505.22453",
      "pubDate": "Wed, 28 May 2025 11:11:16 GMT",
      "isoDate": "2025-05-28T11:11:16.000Z",
      "creator": "Lai Wei, Yuting Li, Chen Wang, Yue Wang, Linghe Kong, Weiran Huang, Lichao Sun",
      "summary": "### MM-UPT：通过GRPO实现多模态大语言模型无监督后训练\n\n**1. 研究背景与问题**\n\n*   **现有方法局限性：** 提升多模态大语言模型（MLLMs）在后训练阶段通常依赖于监督微调（SFT）或强化学习（RL）。然而，这些监督方法需要昂贵且手动标注的多模态数据，这种资源是不可持续的。\n*   **无监督方法挑战：** 近期虽有探索无监督后训练的努力，但其方法复杂且难以迭代。\n\n**2. 提出的解决方案：MM-UPT框架**\n\n*   **核心思想：** 本文首次探索使用GRPO（一种稳定且可扩展的在线RL算法）来实现持续的自我改进，而无需任何外部监督。\n*   **MM-UPT的特点：** 提出了MM-UPT，一个简单而有效的MLLM无监督后训练框架。\n*   **机制创新：** MM-UPT基于GRPO，但用一种基于多数投票的自奖励机制取代了传统的奖励信号，该机制通过对多个采样响应进行投票来生成奖励。\n\n**3. 实验结果与性能**\n\n*   **显著提升：** 实验证明，MM-UPT显著提升了Qwen2.5-VL-7B的推理能力。\n    *   在MathVista数据集上，性能从66.3%提升至72.9%。\n    *   在We-Math数据集上，性能从62.9%提升至68.7%。\n*   **数据使用：** 这些改进是在标准数据集上实现的，且无需真实标签（ground truth labels）。\n*   **超越基线：** MM-UPT优于先前的无监督基线方法，甚至接近了监督式GRPO的结果。\n*   **可扩展性：** 研究还表明，结合完全由MLLM自身生成的合成问题，可以进一步提升性能，这突显了一种有前景的可扩展自我改进方法。\n\n**4. 结论与意义**\n\n*   **新范式：** 总体而言，MM-UPT为在缺乏外部监督的情况下，实现MLLM的持续、自主增强提供了一种新范式。\n*   **代码可用性：** 相关代码已在GitHub上开源。",
      "shortSummary": "本文提出了MM-UPT，一个基于GRPO的简单而有效的多模态大语言模型（MLLM）无监督后训练框架。针对传统监督方法对昂贵标注数据的依赖，MM-UPT采用多数投票的自奖励机制，无需外部监督即可实现模型自我改进。实验证明，MM-UPT显著提升了Qwen2.5-VL-7B在MathVista和We-Math等数据集上的推理能力，超越了现有无监督基线，并接近监督方法。该工作为MLLM的自主持续增强提供了新范式。",
      "translated_title": "通过GRPO对多模态大语言模型推理进行无监督后训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "Improving Multi-modal Large Language Models (MLLMs) in the post-training stage typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). However, these supervised methods require expensive and manually annotated multi-modal data--an ultimately unsustainable resource. While recent efforts have explored unsupervised post-training, their methods are complex and difficult to iterate. In this work, we are the first to investigate the use of GRPO, a stable and scalable online RL algorithm, for enabling continual self-improvement without any external supervision. We propose MM-UPT, a simple yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds upon GRPO, replacing traditional reward signals with a self-rewarding mechanism based on majority voting over multiple sampled responses. Our experiments demonstrate that MM-UPT significantly improves the reasoning ability of Qwen2.5-VL-7B (e.g., 66.3 %rightarrow72.9 % on MathVista, 62.9 %rightarrow68.7 % on We-Math), using standard dataset without ground truth labels. MM-UPT also outperforms prior unsupervised baselines and even approaches the results of supervised GRPO. Furthermore, we show that incorporating synthetic questions, generated solely by MLLM itself, can boost performance as well, highlighting a promising approach for scalable self-improvement. Overall, MM-UPT offers a new paradigm for continual, autonomous enhancement of MLLMs in the absence of external supervision. Our code is available at https://github.com/waltonfuture/MM-UPT."
    },
    {
      "title": "Text2Grad：从自然语言反馈中进行强化学习 (原标题: Text2Grad: Reinforcement Learning from Natural Language Feedback)",
      "link": "https://arxiv.org/abs/2505.22338",
      "pubDate": "Wed, 28 May 2025 09:23:49 GMT",
      "isoDate": "2025-05-28T09:23:49.000Z",
      "creator": "Hanyang Wang, Lu Wang, Chaoyun Zhang, Tianjun Mao, Si Qin, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang",
      "summary": "## Text2Grad：从自然语言反馈中进行强化学习\n\n### 摘要\n\n传统上，强化学习从人类反馈（RLHF）通过粗粒度的标量奖励来优化语言模型，但这掩盖了成功或失败背后的细粒度原因，导致学习过程缓慢且不透明。尽管近期工作通过提示或反思引入了文本批评来提高可解释性，但这些方法并未直接修改模型参数。\n\n### Text2Grad 方法介绍\n\nText2Grad 引入了一种新的强化学习范式，它能够将自由形式的文本反馈转化为跨度级别的梯度。其核心思想是：\n\n*   **反馈对齐与转化**：给定人类（或程序化）的批评，Text2Grad 将每个反馈短语与相关的 token 跨度对齐。\n*   **可微分奖励信号**：将这些对齐转化为可微分的奖励信号。\n*   **直接梯度更新**：执行梯度更新，直接精确地修正模型策略中“有问题”的部分，而非进行全局性的调整。\n\n这种方法实现了精确的、由反馈条件驱动的调整，而非笼统的全局性微调。\n\n### Text2Grad 的三大核心组件\n\n1.  **高质量反馈标注流程**：\n    *   该流程负责将批评性反馈与相应的 token 跨度进行配对，确保反馈的精确性和细粒度。\n\n2.  **细粒度奖励模型**：\n    *   此模型在生成解释性批评的同时，预测答案中跨度级别的奖励，从而为模型提供更具体的优化方向。\n\n3.  **跨度级别策略优化器**：\n    *   该优化器能够反向传播自然语言梯度，直接作用于模型策略的特定部分，实现精细化的参数调整。\n\n### 实验结果与优势\n\nText2Grad 在多项任务上进行了验证，包括：\n\n*   **摘要生成**\n*   **代码生成**\n*   **问答系统**\n\n实验结果表明，Text2Grad 始终优于传统的标量奖励强化学习基线和仅使用提示的基线。它不仅提供了更高的任务指标，还显著增强了模型的可解释性。\n\n### 结论\n\n研究结果有力地证明，当自然语言反馈被有效地转化为梯度时，它能成为一种强大的信号，用于实现细粒度的策略优化。",
      "shortSummary": "Text2Grad 提出了一种新的强化学习范式，通过将自由形式的自然语言反馈转化为跨度级别的梯度来优化语言模型。与传统粗粒度奖励不同，Text2Grad 精确地将反馈短语与相关 token 跨度对齐，并直接修正模型策略中“有问题”的部分。该方法包含反馈标注、细粒度奖励模型和跨度级策略优化器。在摘要、代码生成和问答等任务中，Text2Grad 表现优于传统方法，提供了更高的任务指标和更强的可解释性，证明了自然语言反馈在精细化策略优化中的强大潜力。",
      "translated_title": "Text2Grad：从自然语言反馈中进行强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Traditional RLHF optimizes language models with coarse, scalar rewards that mask the fine-grained reasons behind success or failure, leading to slow and opaque learning. Recent work augments RL with textual critiques through prompting or reflection, improving interpretability but leaving model parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm that turns free-form textual feedback into span-level gradients. Given human (or programmatic) critiques, Text2Grad aligns each feedback phrase with the relevant token spans, converts these alignments into differentiable reward signals, and performs gradient updates that directly refine the offending portions of the model's policy. This yields precise, feedback-conditioned adjustments instead of global nudges. Text2Grad is realized through three components: (1) a high-quality feedback-annotation pipeline that pairs critiques with token spans; (2) a fine-grained reward model that predicts span-level reward on answer while generating explanatory critiques; and (3) a span-level policy optimizer that back-propagates natural-language gradients. Across summarization, code generation, and question answering, Text2Grad consistently surpasses scalar-reward RL and prompt-only baselines, providing both higher task metrics and richer interpretability. Our results demonstrate that natural-language feedback, when converted to gradients, is a powerful signal for fine-grained policy optimization. The code for our method is available at https://github.com/microsoft/Text2Grad"
    },
    {
      "title": "通过冷启动强化学习推进多模态推理 (原标题: Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start)",
      "link": "https://arxiv.org/abs/2505.22334",
      "pubDate": "Wed, 28 May 2025 09:21:38 GMT",
      "isoDate": "2025-05-28T09:21:38.000Z",
      "creator": "Lai Wei, Yuting Li, Kaipeng Zheng, Chen Wang, Yue Wang, Linghe Kong, Lichao Sun, Weiran Huang",
      "summary": "### 通过冷启动强化学习推进多模态推理\n\n**引言与背景**\n\n*   大型语言模型（LLMs）在链式思维推理方面取得了显著进展，其中强化学习（RL）发挥了关键作用。\n*   “顿悟时刻”模式（即模型通过反思进行自我纠正的现象）常被认为是RL的涌现特性。\n*   然而，本文首先指出，这些模式在多模态LLMs（MLLMs）进行RL训练之前就已经存在，并且不一定与推理性能的提升相关联。\n\n**研究方法：两阶段方法**\n\n*   基于上述洞察，本文提出了一种综合性的两阶段方法来增强多模态推理能力：\n    1.  **第一阶段：监督微调（SFT）作为冷启动**\n        *   通过使用结构化的链式思维推理模式进行监督微调，为模型提供一个良好的初始状态。\n    2.  **第二阶段：通过GRPO进行强化学习**\n        *   在此基础上，进一步应用通过GRPO（Generalized Policy Optimization）实现的强化学习，以进一步完善和提升模型的推理能力。\n\n**实验结果与性能**\n\n*   广泛的实验表明，这种结合了SFT冷启动和RL的方法，在具有挑战性的多模态推理基准测试中，始终优于单独使用SFT或单独使用RL的方法。\n*   所产生的模型在开源MLLMs中，无论是在3B还是7B规模上，都达到了最先进（State-of-the-Art, SOTA）的性能。\n*   **7B模型表现显著提升：**\n    *   在MathVista数据集上，性能从66.3%提升至73.4%。\n    *   在We-Math数据集上，性能从62.9%提升至70.4%。\n*   **3B模型表现：**\n    *   其性能与多个7B模型具有竞争力。\n\n**结论与贡献**\n\n*   这项工作为构建先进的多模态推理模型提供了实用的指导。\n*   研究代码已开源。",
      "shortSummary": "本文提出一种两阶段方法，通过监督微调（SFT）冷启动结合强化学习（RL），显著提升多模态大语言模型（MLLMs）的推理能力。研究发现，“顿悟时刻”模式在RL训练前已存在。实验证明，该组合方法优于单独的SFT或RL，在3B和7B规模的开源MLLMs中达到最先进水平，并在MathVista和We-Math等基准测试上取得显著性能提升。该工作为构建高级多模态推理模型提供了实用指导。",
      "translated_title": "通过冷启动强化学习推进多模态推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in large language models (LLMs) have demonstrated impressive chain-of-thought reasoning capabilities, with reinforcement learning (RL) playing a crucial role in this progress. While \"aha moment\" patterns--where models exhibit self-correction through reflection--are often attributed to emergent properties from RL, we first demonstrate that these patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not necessarily correlate with improved reasoning performance. Building on these insights, we present a comprehensive study on enhancing multimodal reasoning through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start with structured chain-of-thought reasoning patterns, followed by (2) reinforcement learning via GRPO to further refine these capabilities. Our extensive experiments show that this combined approach consistently outperforms both SFT-only and RL-only methods across challenging multimodal reasoning benchmarks. The resulting models achieve state-of-the-art performance among open-source MLLMs at both 3B and 7B scales, with our 7B model showing substantial improvements over base models (e.g., 66.3 %rightarrow73.4 % on MathVista, 62.9 %rightarrow70.4 % on We-Math) and our 3B model achieving performance competitive with several 7B models. Overall, this work provides practical guidance for building advanced multimodal reasoning models. Our code is available at https://github.com/waltonfuture/RL-with-Cold-Start."
    },
    {
      "title": "Skywork Open Reasoner 1 技术报告 (原标题: Skywork Open Reasoner 1 Technical Report)",
      "link": "https://arxiv.org/abs/2505.22312",
      "pubDate": "Wed, 28 May 2025 08:56:04 GMT",
      "isoDate": "2025-05-28T08:56:04.000Z",
      "creator": "Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, Yahui Zhou",
      "summary": "## Skywork Open Reasoner 1 (Skywork-OR1) 技术报告\n\n本文介绍了 Skywork-OR1，这是一种针对长链式思考 (CoT) 模型的高效且可扩展的强化学习 (RL) 实现。该工作建立在 DeepSeek-R1 成功利用强化学习增强大型语言模型 (LLM) 推理能力的基础上。\n\n### 核心贡献与性能提升\n\n*   Skywork-OR1 基于 DeepSeek-R1-Distill 模型系列进行开发。\n*   通过强化学习方法，实现了显著的性能提升：\n    *   对于 32B 模型，在 AIME24、AIME25 和 LiveCodeBench 上的平均准确率从 57.8% 提高到 72.8%，提升了 15.0%。\n    *   对于 7B 模型，平均准确率从 43.6% 提高到 57.5%，提升了 13.9%。\n\n### 模型对比表现\n\n*   Skywork-OR1-32B 模型在 AIME24 和 AIME25 基准测试中超越了 DeepSeek-R1 和 Qwen3-32B。\n*   在 LiveCodeBench 上，Skywork-OR1-32B 取得了可媲美的结果。\n*   Skywork-OR1-7B 和 Skywork-OR1-Math-7B 模型在同等规模模型中展现出具有竞争力的推理能力。\n\n### 研究与分析\n\n*   对训练管道的核心组件进行了全面的消融研究，以验证其有效性。\n*   深入研究了“熵坍缩”现象，并确定了影响熵动态的关键因素。\n*   研究表明，缓解过早的熵坍缩对于提高测试性能至关重要。\n\n### 开放资源\n\n*   为了支持社区研究，Skywork-OR1 完全开源了其模型权重、训练代码和训练数据集。\n\n### 相关主题\n\n*   机器学习 (cs.LG)\n*   人工智能 (cs.AI)\n*   计算与语言 (cs.CL)",
      "shortSummary": "Skywork-OR1 是一项针对长链式思考 (CoT) 模型的高效可扩展强化学习 (RL) 实现。它基于 DeepSeek-R1-Distill 系列，显著提升了大型语言模型 (LLM) 的推理能力，例如 32B 模型准确率提升 15%。Skywork-OR1-32B 在 AIME24 和 AIME25 基准测试中超越了 DeepSeek-R1 和 Qwen3-32B。研究还深入探讨了熵坍缩现象。为支持社区研究，模型权重、训练代码和数据集已全面开源。",
      "translated_title": "Skywork Open Reasoner 1 技术报告",
      "images": [],
      "contentSource": "完整文章",
      "content": "The success of DeepSeek-R1 underscores the significant role of reinforcement learning (RL) in enhancing the reasoning capabilities of large language models (LLMs). In this work, we present Skywork-OR1, an effective and scalable RL implementation for long Chain-of-Thought (CoT) models. Building on the DeepSeek-R1-Distill model series, our RL approach achieves notable performance gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%) for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models demonstrate competitive reasoning capabilities among models of similar size. We perform comprehensive ablation studies on the core components of our training pipeline to validate their effectiveness. Additionally, we thoroughly investigate the phenomenon of entropy collapse, identify key factors affecting entropy dynamics, and demonstrate that mitigating premature entropy collapse is critical for improved test performance. To support community research, we fully open-source our model weights, training code, and training datasets."
    },
    {
      "title": "RenderFormer: 基于Transformer的具有全局光照的三角网格神经渲染 (原标题: RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination)",
      "link": "https://arxiv.org/abs/2505.21925",
      "pubDate": "Tue, 27 May 2025 23:20:46 GMT",
      "isoDate": "2025-05-27T23:20:46.000Z",
      "creator": "Chong Zeng, Yue Dong, Pieter Peers, Hongzhi Wu, Xin Tong",
      "summary": "### RenderFormer：一种新型神经渲染管线\n\nRenderFormer是一种创新的神经渲染管线，旨在直接从场景的三角网格表示中渲染图像，并完整模拟全局光照效果。其核心优势在于**无需针对每个场景进行单独训练或微调**。\n\n### 渲染范式转变：序列到序列转换\n\nRenderFormer放弃了传统的基于物理的渲染方法，将渲染过程重新定义为一种序列到序列的转换。具体而言：\n\n*   **输入：** 一个表示带有反射属性的三角形的令牌序列。\n*   **输出：** 一个表示小像素块的输出令牌序列。\n\n### 两阶段Transformer架构\n\nRenderFormer遵循一个两阶段的流水线，两个阶段均基于Transformer架构，并以最小的先验约束进行学习：\n\n1.  **第一阶段（视图无关）：** 建模三角形之间的光传输。\n2.  **第二阶段（视图相关）：** 将表示光线束的令牌转换为相应的像素值，该过程由第一阶段生成的三角形序列引导。\n\n### 性能评估与认可\n\nRenderFormer已在形状和光传输复杂性各异的场景中进行了演示和评估，展现了其有效性。该研究已被SIGGRAPH 2025接受。",
      "shortSummary": "RenderFormer是一种基于Transformer的神经渲染管线，能直接从三角网格渲染具有全局光照的图像，且无需逐场景训练。它将渲染视为序列到序列转换，通过两个Transformer阶段：一个处理三角形间光传输（视图无关），另一个将光线束转换为像素值（视图相关）。该方法已在复杂场景中得到验证，并被SIGGRAPH 2025接受。",
      "translated_title": "RenderFormer: 基于Transformer的具有全局光照的三角网格神经渲染",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present RenderFormer, a neural rendering pipeline that directly renders an image from a triangle-based representation of a scene with full global illumination effects and that does not require per-scene training or fine-tuning. Instead of taking a physics-centric approach to rendering, we formulate rendering as a sequence-to-sequence transformation where a sequence of tokens representing triangles with reflectance properties is converted to a sequence of output tokens representing small patches of pixels. RenderFormer follows a two stage pipeline: a view-independent stage that models triangle-to-triangle light transport, and a view-dependent stage that transforms a token representing a bundle of rays to the corresponding pixel values guided by the triangle-sequence from the view-independent stage. Both stages are based on the transformer architecture and are learned with minimal prior constraints. We demonstrate and evaluate RenderFormer on scenes with varying complexity in shape and light transport."
    },
    {
      "title": "R2R：通过大小模型令牌路由高效导航分歧推理路径 (原标题: R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing)",
      "link": "https://arxiv.org/abs/2505.21600",
      "pubDate": "Tue, 27 May 2025 12:57:20 GMT",
      "isoDate": "2025-05-27T12:57:20.000Z",
      "creator": "Tianyu Fu, Yi Ge, Yichen You, Enshu Liu, Zhihang Yuan, Guohao Dai, Shengen Yan, Huazhong Yang, Yu Wang",
      "summary": "## R2R：通过大小模型令牌路由高效导航分歧推理路径\n\n### 引言与背景\n大型语言模型（LLM）虽然展现出令人印象深刻的推理能力，但其高昂的推理开销带来了巨大的部署挑战。尽管蒸馏后的小型语言模型（SLM）显著提高了效率，但由于未能有效遵循LLM的推理路径，其性能受到影响。\n\n### 核心洞察\n研究揭示了一个关键发现：LLM和SLM之间只有一小部分令牌真正导致推理路径的分歧。大多数生成的令牌要么完全相同，要么仅表现出中性差异，例如缩写或表达方式上的微小变化。\n\n### R2R（Roads to Rome）方法\n\n*   **定义：** R2R是一种新颖的神经令牌路由方法。\n*   **工作原理：** 该方法利用上述洞察，选择性地仅对那些关键的、导致路径分歧的令牌使用LLM进行处理，而将绝大多数令牌的生成任务留给SLM。\n\n### 数据生成与训练\n\n*   为了训练轻量级路由器，R2R开发了一个自动数据生成流程。\n*   该流程能够识别分歧令牌，并生成令牌级别的路由标签。\n\n### 实验与结果\n\n*   **应用：** R2R被应用于组合DeepSeek家族的R1-1.5B和R1-32B模型。\n*   **评估基准：** 在具有挑战性的数学、编程和问答基准上进行了评估。\n*   **性能表现：**\n    *   **参数效率：** R2R的平均激活参数大小为5.6B。\n    *   **准确率提升：** R2R的平均准确率比R1-7B高1.6倍，甚至超越了R1-14B模型。\n    *   **速度提升：** 与R1-32B相比，R2R实现了2.8倍的实际运行速度提升，同时保持了可比的性能。\n*   **结论：** R2R显著推进了测试时扩展效率的帕累托前沿。",
      "shortSummary": "R2R（Roads to Rome）是一种创新的神经令牌路由方法，旨在优化大型语言模型（LLM）的推理效率。该方法基于LLM和小型语言模型（SLM）之间仅少数令牌导致推理路径分歧的发现。R2R选择性地让LLM处理这些关键的分歧令牌，而SLM负责其余大部分令牌生成。实验证明，R2R在平均激活参数为5.6B时，性能超越了R1-14B模型，并比R1-32B模型提速2.8倍，显著提升了测试时效率。",
      "translated_title": "R2R：通过大小模型令牌路由高效导航分歧推理路径",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs' reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce **Roads to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency. Our code is available at https://github.com/thu-nics/R2R."
    },
    {
      "title": "SageAttention2++：SageAttention2 的更高效实现 (原标题: SageAttention2++: A More Efficient Implementation of SageAttention2)",
      "link": "https://arxiv.org/abs/2505.21136",
      "pubDate": "Tue, 27 May 2025 08:50:36 GMT",
      "isoDate": "2025-05-27T08:50:36.000Z",
      "creator": "Jintao Zhang, Xiaoming Xu, Jia Wei, Haofeng Huang, Pengle Zhang, Chendong Xiang, Jun Zhu, Jianfei Chen",
      "summary": "# SageAttention2++：SageAttention2 的更高效实现\n\n## 摘要\n\n本文介绍了 SageAttention2++，一个旨在进一步提升注意力机制效率的新方法。\n\n## 背景与问题\n\n注意力机制的效率至关重要，因为其时间复杂度随序列长度呈平方增长，这在处理长序列时会成为性能瓶颈。\n\n## 现有解决方案（SageAttention2）\n\nSageAttention2 通过利用量化技术来加速注意力机制中的矩阵乘法（Matmul）操作，从而解决了部分效率问题。\n\n## 提出的新方法（SageAttention2++）\n\n为了进一步加速 SageAttention2，SageAttention2++ 提出利用一种更快的 FP8 矩阵乘法指令，该指令在 FP16 中累积。这种新的指令比 SageAttention2 中使用的 FP8 矩阵乘法快两倍。\n\n## 实验结果与性能\n\n*   **速度提升：** 实验结果表明，SageAttention2++ 比 FlashAttention 实现了 3.9 倍的加速。\n*   **精度保持：** SageAttention2++ 在加速的同时，保持了与 SageAttention2 相同的注意力精度。\n*   **应用范围：** 这意味着 SageAttention2++ 能够有效地加速各种模型，包括语言、图像和视频生成模型，且端到端指标损失可忽略不计。\n\n## 代码可用性\n\n相关代码将在未来提供。\n\n## 研究领域\n\n本文涉及机器学习、人工智能、硬件架构以及计算机视觉与模式识别等领域。",
      "shortSummary": "SageAttention2++ 提出了一种更高效的注意力机制实现。它通过利用在 FP16 中累积的更快 FP8 矩阵乘法指令，将 SageAttention2 的 Matmul 速度提升一倍。实验表明，SageAttention2++ 比 FlashAttention 快 3.9 倍，同时保持了相同的注意力精度。这使得它能有效加速语言、图像和视频生成等多种模型，且性能损失可忽略不计。",
      "translated_title": "SageAttention2++：SageAttention2 的更高效实现",
      "images": [],
      "contentSource": "完整文章",
      "content": "The efficiency of attention is critical because its time complexity grows quadratically with sequence length. SageAttention2 addresses this by utilizing quantization to accelerate matrix multiplications (Matmul) in attention. To further accelerate SageAttention2, we propose to utilize the faster instruction of FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8 Matmul used in SageAttention2. Our experiments show that SageAttention2++ achieves a 3.9x speedup over FlashAttention while maintaining the same attention accuracy as SageAttention2. This means SageAttention2++ effectively accelerates various models, including those for language, image, and video generation, with negligible end-to-end metrics loss. The code will be available at https://github.com/thu-ml/SageAttention."
    },
    {
      "title": "DeepResearchGym：一个免费、透明且可复现的深度研究评估沙盒 (原标题: DeepResearchGym: A Free, Transparent, and Reproducible Evaluation Sandbox for Deep Research)",
      "link": "https://arxiv.org/abs/2505.19253",
      "pubDate": "Sun, 25 May 2025 14:16:13 GMT",
      "isoDate": "2025-05-25T14:16:13.000Z",
      "creator": "João Coelho, Jingjie Ning, Jingyuan He, Kangrui Mao, Abhijay Paladugu, Pranav Setlur, Jiahe Jin, Jamie Callan, João Magalhães, Bruno Martins, Chenyan Xiong",
      "summary": "# DeepResearchGym：深度研究评估沙盒\n\nDeepResearchGym 是一个开源的评估沙盒，旨在解决当前深度研究系统在复现性、透明度和成本方面面临的挑战。现有的大多数深度研究系统（一种代理式信息检索方法，用于生成针对复杂查询的全面且有充分支持的报告）依赖于动态的商业搜索 API，这不仅成本高昂，还带来了复现性和透明度方面的难题。DeepResearchGym 通过结合可复现的搜索 API 和严格的评估协议，为深度研究系统提供了一个基准测试平台。\n\n## DeepResearchGym 的核心组件\n\n### 1. 可复现的搜索 API\n\n*   **数据索引：** 该 API 索引大规模的公共网络语料库，包括 ClueWeb22 和 FineWeb。\n*   **技术实现：** 采用先进的密集检索器（dense retriever）和通过 DiskANN 实现的近似最近邻搜索（approximate nearest neighbor search）。\n*   **性能优势：**\n    *   实现了比流行商业 API 更低的延迟。\n    *   确保了跨运行的文档排名稳定性。\n*   **可用性：** 免费供研究使用。\n\n### 2. 严格的评估协议\n\n*   **基准扩展：** 扩展了 Researchy Questions 基准测试。\n*   **自动化评估：** 通过“LLM-as-a-judge”（大语言模型作为评判者）的方法引入自动化指标。\n*   **评估维度：** 衡量以下方面：\n    *   与用户信息需求的对齐程度。\n    *   检索的忠实性（faithfulness）。\n    *   报告的质量。\n\n## 实验结果与验证\n\n*   **性能对比：** 实验结果表明，与 DeepResearchGym 集成的系统实现了与使用商业 API 的系统相当的性能。\n*   **排名一致性：** 性能排名在不同的评估指标之间保持一致。\n*   **人工评估验证：** 一项人工评估研究进一步证实，DeepResearchGym 的自动化评估协议与人类偏好高度一致。\n*   **框架价值：** 这些结果验证了 DeepResearchGym 框架能够支持对深度研究系统进行受控评估的能力。\n\n## 资源可用性\n\nDeepResearchGym 的代码和 API 文档已公开提供。",
      "shortSummary": "DeepResearchGym 是一个开源沙盒，旨在解决深度研究系统对商业搜索API的复现性、透明度和成本依赖问题。它提供一个可复现的搜索API，索引ClueWeb22和FineWeb等公共语料库，实现低延迟和稳定排名。同时，它包含一个基于“LLM-as-a-judge”的严格评估协议，衡量用户需求对齐、检索忠实性和报告质量。实验证明，其性能与商业API相当，且评估结果与人工偏好一致，为深度研究提供免费、透明且可复现的评估方案。",
      "translated_title": "DeepResearchGym：一个免费、透明且可复现的深度研究评估沙盒",
      "images": [],
      "contentSource": "完整文章",
      "content": "Deep research systems represent an emerging class of agentic information retrieval methods that generate comprehensive and well-supported reports to complex queries. However, most existing frameworks rely on dynamic commercial search APIs, which pose reproducibility and transparency challenges in addition to their cost. To address these limitations, we introduce DeepResearchGym, an open-source sandbox that combines a reproducible search API with a rigorous evaluation protocol for benchmarking deep research systems. The API indexes large-scale public web corpora, namely ClueWeb22 and FineWeb, using a state-of-the-art dense retriever and approximate nearest neighbor search via DiskANN. It achieves lower latency than popular commercial APIs while ensuring stable document rankings across runs, and is freely available for research use. To evaluate deep research systems' outputs, we extend the Researchy Questions benchmark with automatic metrics through LLM-as-a-judge assessments to measure alignment with users' information needs, retrieval faithfulness, and report quality. Experimental results show that systems integrated with DeepResearchGym achieve performance comparable to those using commercial APIs, with performance rankings remaining consistent across evaluation metrics. A human evaluation study further confirms that our automatic protocol aligns with human preferences, validating the framework's ability to help support controlled assessment of deep research systems. Our code and API documentation are available at https://www.deepresearchgym.ai."
    },
    {
      "title": "LIMOPro：用于高效有效推理时扩展的推理精炼 (原标题: LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling)",
      "link": "https://arxiv.org/abs/2505.19187",
      "pubDate": "Sun, 25 May 2025 11:17:57 GMT",
      "isoDate": "2025-05-25T11:17:57.000Z",
      "creator": "Yang Xiao, Jiashuo Wang, Ruifeng Yuan, Chunpu Xu, Kaishuai Xu, Wenjie Li, Pengfei Liu",
      "summary": "### LIMOPro：用于高效有效推理时扩展的推理精炼\n\n#### 引言：大型语言模型推理的挑战\n大型语言模型（LLM）通过测试时扩展方法，特别是当使用从更强大的大型推理模型（LRM）中提取的思维链（CoT）数据进行微调时，展现出卓越的推理能力。然而，这些推理链通常包含冗余元素，这些元素模仿了人类解决问题的过程，可分为两类：\n\n*   **渐进式推理 (Progressive Reasoning)**：这是解决方案开发的核心路径，对于问题解决至关重要。\n*   **功能性元素 (Functional Elements)**：包括验证过程、替代解决方案方法和错误修正等。尽管它们在人类思维中扮演一定角色，但在测试时推理过程中，这些元素显著增加了计算需求和资源消耗。\n\n#### LIMOPro 方法：PIR（基于困惑度的重要性精炼）\n为了解决CoT推理链的冗余问题，本文引入了PIR（Perplexity-based Importance Refinement，基于困惑度的重要性精炼）框架。PIR是一个原则性的框架，其核心目标是优化推理过程，使其在保持核心逻辑的同时，减少不必要的冗余。\n\n*   **核心思想**：PIR通过定量评估每个推理步骤的重要性来实现精炼，其评估依据是该步骤对最终答案预测置信度的影响。\n*   **工作原理**：\n    *   PIR系统性地识别并选择性地修剪那些重要性较低的功能性步骤。\n    *   同时，它确保保留了所有渐进式推理组件，从而维护了核心解决方案路径的完整性。\n    *   通过这种方式，PIR能够创建出经过优化的训练数据，这些数据在减少冗余的同时，依然能有效指导模型进行推理。\n\n#### 实验结果与优势\n在PIR优化数据上进行微调的模型展现出显著的性能提升和更优的测试时扩展特性：\n\n*   **推理链优化**：模型能够生成更简洁、更精炼的推理链，减少不必要的步骤。\n*   **准确性提高**：在AIME、AMC和GPQA Diamond等具有挑战性的推理基准测试中，模型的准确性提高了0.9%至6.6%。\n*   **资源消耗降低**：令牌使用量显著减少了3%至41%，这意味着更低的计算成本和更快的响应时间。\n*   **泛化能力**：该方法在不同模型尺寸、数据源和令牌预算下均表现出强大的泛化能力，证明了其广泛的适用性。\n\n#### 实际意义\nLIMOPro提出的PIR框架为在实际场景中部署具备推理能力的LLM提供了一个实用的解决方案。在那些对效率、响应时间和计算效率有严格要求的场景下，该方法尤其具有价值。",
      "shortSummary": "大型语言模型（LLM）的思维链（CoT）推理虽强大但冗长，增加了计算成本。LIMOPro提出PIR（基于困惑度的重要性精炼）框架，通过量化评估并修剪推理链中低重要性的功能性步骤，同时保留核心推理路径。在PIR优化数据上微调的模型，在保持或提高准确性（+0.9%至+6.6%）的同时，显著减少了令牌使用量（-3%至-41%），为高效部署推理型LLM提供了实用方案。",
      "translated_title": "LIMOPro：用于高效有效推理时扩展的推理精炼",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) have demonstrated remarkable reasoning capabilities through test-time scaling approaches, particularly when fine-tuned with chain-of-thought (CoT) data distilled from more powerful large reasoning models (LRMs). However, these reasoning chains often contain verbose elements that mirror human problem-solving, categorized as progressive reasoning (the essential solution development path) and functional elements (verification processes, alternative solution approaches, and error corrections). While progressive reasoning is crucial, the functional elements significantly increase computational demands during test-time inference. We introduce PIR (Perplexity-based Importance Refinement), a principled framework that quantitatively evaluates the importance of each reasoning step based on its impact on answer prediction confidence. PIR systematically identifies and selectively prunes only low-importance functional steps while preserving progressive reasoning components, creating optimized training data that maintains the integrity of the core solution path while reducing verbosity. Models fine-tuned on PIR-optimized data exhibit superior test-time scaling properties, generating more concise reasoning chains while achieving improved accuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to -41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond). Our approach demonstrates strong generalizability across different model sizes, data sources, and token budgets, offering a practical solution for deploying reasoning-capable LLMs in scenarios where efficient test-time scaling, response time, and computational efficiency are valuable constraints."
    },
    {
      "title": "GRE 套件：基于微调视觉-语言模型和增强推理链的地理定位推断 (原标题: GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains)",
      "link": "https://arxiv.org/abs/2505.18700",
      "pubDate": "Sat, 24 May 2025 09:48:57 GMT",
      "isoDate": "2025-05-24T09:48:57.000Z",
      "creator": "Chun Wang, Xiaoran Pan, Zihao Pan, Haofan Wang, Yiren Song",
      "summary": "## GRE 套件：基于微调视觉-语言模型和增强推理链的地理定位推断\n\n### 摘要\n\n近期视觉语言模型（VLMs）在视觉推理任务中展现出卓越性能。然而，地理定位（geo-localization）面临独特挑战，它要求从图像中提取多粒度视觉线索，并将其与外部世界知识相结合进行系统推理。当前地理定位任务的方法往往缺乏鲁棒的推理机制和可解释性，从而限制了其有效性。\n\n### 提出的解决方案：GRE 套件\n\n为了解决这些局限性，我们提出了 Geo Reason Enhancement (GRE) 套件，这是一个新颖的框架，旨在通过结构化推理链增强 VLMs，以实现准确且可解释的位置推断。GRE 套件在以下三个关键维度上系统开发：\n\n1.  **数据集**\n2.  **模型**\n3.  **基准**\n\n### GRE 套件的关键组成部分\n\n*   **1. 数据集：GRE30K**\n    *   我们引入了 GRE30K，一个高质量的地理定位推理数据集。\n    *   该数据集旨在促进细粒度的视觉和上下文分析。\n\n*   **2. 模型：GRE 模型**\n    *   我们提出了 GRE 模型，它采用多阶段推理策略。\n    *   该模型逐步推断场景属性、局部细节和语义特征。\n    *   通过这种方式，它能够以更高的精度缩小潜在的地理区域。\n\n*   **3. 基准：Geo Reason Evaluation Benchmark (GREval-Bench)**\n    *   我们构建了 Geo Reason Evaluation Benchmark (GREval-Bench)，这是一个全面的评估框架。\n    *   该基准评估 VLMs 在各种城市、自然和地标场景中的表现。\n    *   它衡量粗粒度（例如，国家、大陆）和细粒度（例如，城市、街道）的定位性能。\n\n### 实验结果\n\n实验结果表明，GRE 在所有粒度的地理定位任务中均显著优于现有方法。这强调了推理增强型 VLMs 在复杂地理推断中的有效性。\n\n### 代码和数据可用性\n\n代码和数据将发布。",
      "shortSummary": "GRE 套件是一个新颖的框架，旨在通过结构化推理链增强视觉语言模型（VLMs），以实现准确且可解释的地理定位推断。该套件包含高质量的GRE30K数据集、采用多阶段推理策略的GRE模型，以及全面的GREval-Bench评估基准。实验结果表明，GRE在粗粒度和细粒度地理定位任务中均显著优于现有方法，验证了推理增强型VLMs在复杂地理推断中的有效性。",
      "translated_title": "GRE 套件：基于微调视觉-语言模型和增强推理链的地理定位推断",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in Visual Language Models (VLMs) have demonstrated exceptional performance in visual reasoning tasks. However, geo-localization presents unique challenges, requiring the extraction of multigranular visual cues from images and their integration with external world knowledge for systematic reasoning. Current approaches to geo-localization tasks often lack robust reasoning mechanisms and explainability, limiting their effectiveness. To address these limitations, we propose the Geo Reason Enhancement (GRE) Suite, a novel framework that augments VLMs with structured reasoning chains for accurate and interpretable location inference. The GRE Suite is systematically developed across three key dimensions: dataset, model, and benchmark. First, we introduce GRE30K, a high-quality geo-localization reasoning dataset designed to facilitate fine-grained visual and contextual analysis. Next, we present the GRE model, which employs a multi-stage reasoning strategy to progressively infer scene attributes, local details, and semantic features, thereby narrowing down potential geographic regions with enhanced precision. Finally, we construct the Geo Reason Evaluation Benchmark (GREval-Bench), a comprehensive evaluation framework that assesses VLMs across diverse urban, natural, and landmark scenes to measure both coarse-grained (e.g., country, continent) and fine-grained (e.g., city, street) localization performance. Experimental results demonstrate that GRE significantly outperforms existing methods across all granularities of geo-localization tasks, underscoring the efficacy of reasoning-augmented VLMs in complex geographic inference. Code and data will be released at https://github.com/Thorin215/GRE."
    },
    {
      "title": "迈向动态心智理论：评估大型语言模型对人类状态时间演变的适应性 (原标题: Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal Evolution of Human States)",
      "link": "https://arxiv.org/abs/2505.17663",
      "pubDate": "Fri, 23 May 2025 05:27:40 GMT",
      "isoDate": "2025-05-23T05:27:40.000Z",
      "creator": "Yang Xiao, Jiashuo Wang, Qiancheng Xu, Changhe Song, Chunpu Xu, Yi Cheng, Wenjie Li, Pengfei Liu",
      "summary": "### 迈向动态心智理论：评估大型语言模型对人类状态时间演变的适应性\n\n随着大型语言模型（LLM）越来越多地参与人机交互，评估其心智理论（ToM）能力，特别是追踪动态心理状态的能力，变得至关重要。\n\n*   **现有基准的局限性**\n    *   当前的心智理论基准主要侧重于心理状态的静态快照。\n    *   它们忽视了现实世界社交互动中固有的时间演变特性。\n\n*   **提出 DynToM 基准**\n    *   **目的：** 为了解决上述局限性，研究提出了一种名为 \\textsc{DynToM} 的新型基准。\n    *   **设计理念：** \\textsc{DynToM} 专门用于评估 LLM 理解和追踪跨互联场景中心理状态时间演变的能力。\n    *   **数据生成：** 通过一个系统的四步框架生成数据。\n        *   包含 1,100 个社交语境。\n        *   涵盖 5,500 个具体场景。\n        *   总计 78,100 个问题。\n    *   **质量控制：** 所有生成的内容都经过了真实性和质量验证。\n\n*   **LLM 性能评估**\n    *   研究对十个最先进的 LLM 进行了全面评估。\n    *   **主要发现：**\n        *   LLM 的平均表现比人类低 44.7%。\n        *   当需要追踪和推理心理状态的转变时，LLM 的性能显著下降。\n    *   **结论：** 这一显著的性能差距揭示了当前 LLM 在建模人类心理状态动态性质方面的根本性局限。\n\n*   **会议接受**\n    *   该研究已被 ACL 2025 主会议接受。",
      "shortSummary": "该研究提出了 \\textsc{DynToM}，一个评估大型语言模型（LLM）动态心智理论（ToM）能力的新基准，旨在弥补现有基准只关注静态心理状态的不足。通过对十个先进 LLM 的评估发现，它们在追踪和推理人类心理状态的时间演变方面显著落后于人类（平均低 44.7%），凸显了当前 LLM 在理解动态人类状态方面的根本性局限。",
      "translated_title": "迈向动态心智理论：评估大型语言模型对人类状态时间演变的适应性",
      "images": [],
      "contentSource": "完整文章",
      "content": "As Large Language Models (LLMs) increasingly participate in human-AI interactions, evaluating their Theory of Mind (ToM) capabilities - particularly their ability to track dynamic mental states - becomes crucial. While existing benchmarks assess basic ToM abilities, they predominantly focus on static snapshots of mental states, overlooking the temporal evolution that characterizes real-world social interactions. We present DynToM, a novel benchmark specifically designed to evaluate LLMs' ability to understand and track the temporal progression of mental states across interconnected scenarios. Through a systematic four-step framework, we generate 1,100 social contexts encompassing 5,500 scenarios and 78,100 questions, each validated for realism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs reveals that their average performance underperforms humans by 44.7\\%, with performance degrading significantly when tracking and reasoning about the shift of mental states. This performance gap highlights fundamental limitations in current LLMs' ability to model the dynamic nature of human mental states."
    },
    {
      "title": "基于Hugging Face知识图谱的推荐、分类和追踪基准测试 (原标题: Benchmarking Recommendation, Classification, and Tracing Based on Hugging Face Knowledge Graph)",
      "link": "https://arxiv.org/abs/2505.17507",
      "pubDate": "Fri, 23 May 2025 02:00:20 GMT",
      "isoDate": "2025-05-23T02:00:20.000Z",
      "creator": "Qiaosheng Chen, Kaijia Huang, Xiao Zhou, Weiqing Luo, Yuanning Cui, Gong Cheng",
      "summary": "# 基于Hugging Face知识图谱的推荐、分类和追踪基准测试\n\n## 摘要\n\n### 背景与问题\n开源机器学习（ML）资源（如模型和数据集）的快速增长加速了信息检索（IR）研究。然而，现有平台如Hugging Face并未明确利用结构化表示，这限制了高级查询和分析，例如追踪模型演变和推荐相关数据集。\n\n### HuggingKG：大规模知识图谱的构建\n为了填补这一空白，研究构建了HuggingKG，这是首个从Hugging Face社区构建的大规模知识图谱，专为ML资源管理设计。\n*   **规模与内容**：\n    *   包含260万个节点和620万条边。\n    *   捕获了领域特定的关系和丰富的文本属性。\n*   **目的**：旨在实现更高效、更深入的ML资源管理。\n\n### HuggingBench：多任务基准测试\n基于HuggingKG，研究进一步提出了HuggingBench，这是一个多任务基准测试，包含三个新颖的测试集合，用于解决以下信息检索任务：\n*   资源推荐\n*   资源分类\n*   资源追踪\n\n### 实验与贡献\n*   实验揭示了HuggingKG及其衍生任务的独特特性。\n*   HuggingKG和HuggingBench均已公开发布，有望推动开源资源共享和管理领域的研究进展。\n\n### 论文信息\n*   该研究已被SIGIR 2025会议接受。\n*   主题：信息检索 (cs.IR)。",
      "shortSummary": "为解决Hugging Face平台缺乏结构化数据的问题，研究构建了HuggingKG，一个包含260万节点和620万边的机器学习资源知识图谱。基于此，开发了HuggingBench多任务基准，用于资源推荐、分类和追踪。HuggingKG和HuggingBench均已公开，旨在促进开源ML资源管理研究。",
      "translated_title": "基于Hugging Face知识图谱的推荐、分类和追踪基准测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "The rapid growth of open source machine learning (ML) resources, such as models and datasets, has accelerated IR research. However, existing platforms like Hugging Face do not explicitly utilize structured representations, limiting advanced queries and analyses such as tracing model evolution and recommending relevant datasets. To fill the gap, we construct HuggingKG, the first large-scale knowledge graph built from the Hugging Face community for ML resource management. With 2.6 million nodes and 6.2 million edges, HuggingKG captures domain-specific relations and rich textual attributes. It enables us to further present HuggingBench, a multi-task benchmark with three novel test collections for IR tasks including resource recommendation, classification, and tracing. Our experiments reveal unique characteristics of HuggingKG and the derived tasks. Both resources are publicly available, expected to advance research in open source resource sharing and management."
    },
    {
      "title": "Safe-Sora：通过图形水印实现安全的文本到视频生成 (原标题: Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking)",
      "link": "https://arxiv.org/abs/2505.12667",
      "pubDate": "Sun, 18 May 2025 23:31:31 GMT",
      "isoDate": "2025-05-18T23:31:31.000Z",
      "creator": "Zihan Su, Xuerui Qiu, Hongbin Xu, Tangyu Jiang, Junhao Zhuang, Chun Yuan, Ming Li, Shengfeng He, Fei Richard Yu",
      "summary": "# Safe-Sora：通过图形水印实现安全的文本到视频生成\n\n## 摘要\n\n随着生成式视频模型的爆炸性增长，对AI生成内容的可靠版权保护需求日益增加。尽管隐形生成水印在图像合成中已广受欢迎，但在视频生成领域仍未得到充分探索。为了弥补这一空白，本文提出了 **Safe-Sora**，这是首个将图形水印直接嵌入到视频生成过程中的框架。\n\n## 核心机制与创新\n\nSafe-Sora 的设计基于水印性能与水印和封面内容之间视觉相似性密切相关的观察，并引入了以下关键创新：\n\n*   **分层粗到细自适应匹配机制：**\n    *   水印图像被分割成多个补丁（patches）。\n    *   每个补丁被分配到视觉上最相似的视频帧。\n    *   进一步定位到最佳空间区域，以实现无缝嵌入。\n\n*   **3D小波变换增强的Mamba架构：**\n    *   开发了此架构以实现水印补丁在视频帧间的时空融合。\n    *   采用了一种新颖的时空局部扫描策略。\n    *   在水印嵌入和检索过程中有效建模长距离依赖关系。\n\n## 创新点\n\n据作者所知，这是首次尝试将状态空间模型（如Mamba）应用于水印技术，为高效和鲁棒的水印保护开辟了新途径。\n\n## 实验结果\n\n广泛的实验证明，Safe-Sora 在视频质量、水印保真度和鲁棒性方面均达到了最先进的性能，这主要归功于其提出的创新方法。\n\n## 代码发布\n\n相关代码将在论文发表后发布。",
      "shortSummary": "Safe-Sora是一个开创性的框架，旨在通过图形水印保护AI生成的视频内容版权。它首次将水印直接嵌入到视频生成过程中，解决了现有视频水印技术的不足。该框架引入了分层自适应匹配机制和增强型Mamba架构，实现了水印与视频内容的无缝融合及高效的时空依赖建模。实验证明，Safe-Sora在视频质量、水印保真度和鲁棒性方面均达到最先进水平，为AI视频版权保护提供了新途径。",
      "translated_title": "Safe-Sora：通过图形水印实现安全的文本到视频生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "The explosive growth of generative video models has amplified the demand for reliable copyright preservation of AI-generated content. Despite its popularity in image synthesis, invisible generative watermarking remains largely underexplored in video generation. To address this gap, we propose Safe-Sora, the first framework to embed graphical watermarks directly into the video generation process. Motivated by the observation that watermarking performance is closely tied to the visual similarity between the watermark and cover content, we introduce a hierarchical coarse-to-fine adaptive matching mechanism. Specifically, the watermark image is divided into patches, each assigned to the most visually similar video frame, and further localized to the optimal spatial region for seamless embedding. To enable spatiotemporal fusion of watermark patches across video frames, we develop a 3D wavelet transform-enhanced Mamba architecture with a novel spatiotemporal local scanning strategy, effectively modeling long-range dependencies during watermark embedding and retrieval. To the best of our knowledge, this is the first attempt to apply state space models to watermarking, opening new avenues for efficient and robust watermark protection. Extensive experiments demonstrate that Safe-Sora achieves state-of-the-art performance in terms of video quality, watermark fidelity, and robustness, which is largely attributed to our proposals. We will release our code upon publication."
    }
  ],
  "lastUpdated": "2025-05-29T03:34:25.246Z"
}