{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "Depth Anything 3: Recovering the Visual Space from Any Views",
      "link": "https://arxiv.org/abs/2511.10647",
      "pubDate": "Thu, 13 Nov 2025 13:59:53 GMT",
      "isoDate": "2025-11-13T13:59:53.000Z",
      "creator": "Haotong Lin, Sili Chen, Junhao Liew, Donny Y. Chen, Zhenyu Li, Guang Shi, Jiashi Feng, Bingyi Kang",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Depth Anything 3: Recovering the Visual Space from Any Views",
      "images": [],
      "contentSource": "RSS",
      "content": "We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets."
    },
    {
      "title": "WMPO: World Model-based Policy Optimization for Vision-Language-Action Models",
      "link": "https://arxiv.org/abs/2511.09515",
      "pubDate": "Wed, 12 Nov 2025 12:54:09 GMT",
      "isoDate": "2025-11-12T12:54:09.000Z",
      "creator": "Fangqi Zhu, Zhengyang Yan, Zicong Hong, Quanxin Shou, Xiao Ma, Song Guo",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "WMPO: World Model-based Policy Optimization for Vision-Language-Action Models",
      "images": [],
      "contentSource": "RSS",
      "content": "Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the \"imagined\" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities."
    },
    {
      "title": "LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls",
      "link": "https://arxiv.org/abs/2511.09148",
      "pubDate": "Wed, 12 Nov 2025 04:34:39 GMT",
      "isoDate": "2025-11-12T04:34:39.000Z",
      "creator": "Kangning Zhang, Wenxiang Jiao, Kounianhua Du, Yuan Lu, Weiwen Liu, Weinan Zhang, Lei Zhang, Yong Yu",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls",
      "images": [],
      "contentSource": "RSS",
      "content": "Augmenting Large Language Models (LLMs) with external tools enables them to execute complex, multi-step tasks. However, tool learning is hampered by the static synthetic data pipelines where data generation and model training are executed as two separate, non-interactive processes. This approach fails to adaptively focus on a model's specific weaknesses and allows noisy labels to persist, degrading training efficiency. We introduce LoopTool, a fully automated, model-aware data evolution framework that closes this loop by tightly integrating data synthesis and model training. LoopTool iteratively refines both the data and the model through three synergistic modules: (1) Greedy Capability Probing (GCP) diagnoses the model's mastered and failed capabilities; (2) Judgement-Guided Label Verification (JGLV) uses an open-source judge model to find and correct annotation errors, progressively purifying the dataset; and (3) Error-Driven Data Expansion (EDDE) generates new, challenging samples based on identified failures. This closed-loop process operates within a cost-effective, open-source ecosystem, eliminating dependence on expensive closed-source APIs. Experiments show that our 8B model trained with LoopTool significantly surpasses its 32B data generator and achieves new state-of-the-art results on the BFCL-v3 and ACEBench benchmarks for its scale. Our work demonstrates that closed-loop, self-refining data pipelines can dramatically enhance the tool-use capabilities of LLMs."
    },
    {
      "title": "TiDAR：扩散式思考，自回归生成 (原标题: TiDAR: Think in Diffusion, Talk in Autoregression)",
      "link": "https://arxiv.org/abs/2511.08923",
      "pubDate": "Tue, 11 Nov 2025 21:59:33 GMT",
      "isoDate": "2025-11-11T21:59:33.000Z",
      "creator": "Jingyu Liu, Xin Dong, Zhifan Ye, Rishabh Mehta, Yonggan Fu, Vartika Singh, Jan Kautz, Ce Zhang, Pavlo Molchanov",
      "summary": "# TiDAR：扩散式思考，自回归生成\n\n本文介绍了一种名为TiDAR（Think in Diffusion, Talk in Autoregression）的新型序列级混合架构，旨在结合扩散语言模型的高吞吐量并行生成能力与自回归（AR）模型卓越的生成质量。\n\n## 现有挑战\n\n*   **扩散语言模型**：具备快速并行生成的潜力，但在质量上通常不如自回归模型。\n*   **自回归模型**：因其因果结构与语言建模天然契合，通常在质量上表现出色，但生成过程是顺序的。\n*   **现有方法不足**：\n    *   **推测解码（Speculative Decoding）**：优先考虑自回归，使用较弱的模型进行顺序草稿，导致草稿效率低下。\n    *   **扩散模型的类自回归解码**：仍面临质量下降的问题，并丧失了其潜在的并行性。\n    *   这些方法未能有效平衡高吞吐量、GPU利用率和自回归级别的质量。\n\n## TiDAR 架构与设计\n\nTiDAR是一种创新的序列级混合架构，其核心设计理念是在单个前向传播中完成以下两个关键步骤：\n\n1.  **扩散式思考（Thinking in Diffusion）**：以扩散的方式并行地生成（起草）令牌。\n2.  **自回归表达（Talking in Autoregression）**：以自回归的方式采样最终输出。\n\n该架构通过**特殊设计的结构化注意力掩码**实现，充分利用了GPU的计算密度，从而在草稿生成和验证能力之间实现了强大的平衡。此外，TiDAR被设计为一个独立的模型，具有低开销，易于部署（serving-friendly）。\n\n## 性能评估与结果\n\n研究人员在1.5B和8B两种规模下，对TiDAR在生成和似然任务上进行了广泛评估，并将其与以下模型进行了比较：\n\n*   自回归模型\n*   推测解码（Speculative Decoding）\n*   扩散模型变体（如Dream和Llada）\n\n**主要发现和优势：**\n\n*   **吞吐量提升**：得益于并行草稿生成、并行采样以及对KV缓存的精确支持，TiDAR在实测吞吐量方面超越了推测解码。\n*   **效率与质量兼顾**：TiDAR在效率和质量两方面均超越了Dream和Llada等扩散模型。\n*   **弥合质量差距**：最值得注意的是，TiDAR是首个在保持与自回归模型相当的质量水平的同时，还能提供每秒4.71倍至5.91倍更多令牌的架构。这意味着它成功弥合了扩散模型与自回归模型之间的质量差距。\n\n## 结论\n\nTiDAR代表了语言模型生成领域的一个重大进步，它通过创新的混合架构，首次实现了高吞吐量、高GPU利用率与自回归级别质量的协同，为未来高效、高质量的语言生成提供了新的范式。",
      "shortSummary": "TiDAR是一种新型混合语言模型架构，它在单个前向传播中结合了扩散式并行令牌起草（Thinking）和自回归式最终输出采样（Talking）。该设计旨在平衡高吞吐量和自回归模型的生成质量。实验证明，TiDAR在效率和质量上均优于推测解码和现有扩散模型。最显著的是，TiDAR首次在保持与自回归模型相当质量的同时，实现了每秒4.71到5.91倍的令牌生成速度提升，为高效高质量语言生成开辟了新途径。",
      "translated_title": "TiDAR：扩散式思考，自回归生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Diffusion language models hold the promise of fast parallel generation, while autoregressive (AR) models typically excel in quality due to their causal structure aligning naturally with language modeling. This raises a fundamental question: can we achieve a synergy with high throughput, higher GPU utilization, and AR level quality? Existing methods fail to effectively balance these two aspects, either prioritizing AR using a weaker model for sequential drafting (speculative decoding), leading to lower drafting efficiency, or using some form of left-to-right (AR-like) decoding logic for diffusion, which still suffers from quality degradation and forfeits its potential parallelizability. We introduce TiDAR, a sequence-level hybrid architecture that drafts tokens (Thinking) in Diffusion and samples final outputs (Talking) AutoRegressively - all within a single forward pass using specially designed structured attention masks. This design exploits the free GPU compute density, achieving a strong balance between drafting and verification capacity. Moreover, TiDAR is designed to be serving-friendly (low overhead) as a standalone model. We extensively evaluate TiDAR against AR models, speculative decoding, and diffusion variants across generative and likelihood tasks at 1.5B and 8B scales. Thanks to the parallel drafting and sampling as well as exact KV cache support, TiDAR outperforms speculative decoding in measured throughput and surpasses diffusion models like Dream and Llada in both efficiency and quality. Most notably, TiDAR is the first architecture to close the quality gap with AR models while delivering 4.71x to 5.91x more tokens per second."
    },
    {
      "title": "Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds",
      "link": "https://arxiv.org/abs/2511.08892",
      "pubDate": "Tue, 11 Nov 2025 21:01:26 GMT",
      "isoDate": "2025-11-11T21:01:26.000Z",
      "creator": "Weihao Tan, Xiangyang Li, Yunhao Fang, Heyuan Yao, Shi Yan, Hao Luo, Tenglong Ao, Huihui Li, Hongbin Ren, Bairen Yi, Yujia Qin, Bo An, Libin Liu, Guang Shi",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds",
      "images": [],
      "contentSource": "RSS",
      "content": "We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments."
    },
    {
      "title": "未选择的路：RLVR 被证明偏离主方向学习 (原标题: The Path Not Taken: RLVR Provably Learns Off the Principals)",
      "link": "https://arxiv.org/abs/2511.08567",
      "pubDate": "Tue, 11 Nov 2025 13:49:45 GMT",
      "isoDate": "2025-11-11T13:49:45.000Z",
      "creator": "Hanqing Zhu, Zhenyu Zhang, Hanxian Huang, DiJia Su, Zechun Liu, Jiawei Zhao, Igor Fedorov, Hamed Pirsiavash, Zhizhou Sha, Jinwon Lee, David Z. Pan, Zhangyang Wang, Yuandong Tian, Kai Sheng Tai",
      "summary": "# RLVR学习机制的深入探究：偏离主方向的学习路径\n\n本文深入探讨了“可验证奖励强化学习”（RLVR）在提升大型语言模型（LLM）推理性能时所展现的“稀疏性悖论”——即RLVR在显著提高性能的同时，似乎只修改了模型参数的一小部分。作者们重新审视了这一现象，并揭示了稀疏性实际上是模型条件优化偏差的一种表面假象。\n\n## 核心发现：优化偏差与稀疏性\n\n研究表明，对于一个固定的预训练模型，RLVR的参数更新会持续地定位到特定的、偏好的参数区域。这种定位在不同运行、不同数据集和不同RL策略之间都高度一致，并且在很大程度上保持不变。\n\n## 三门理论：机制解释\n\n为了从机制上解释这些动态，作者提出了“三门理论”：\n\n*   **第一门（KL锚定）**：施加了KL散度约束的更新。\n*   **第二门（模型几何）**：将更新步骤引导偏离主方向，进入低曲率、谱保持的子空间。\n*   **第三门（精度）**：将非偏好区域的微小更新隐藏起来，使得这种偏离主方向的偏差表现为稀疏性。\n\n## RLVR学习动态的参数级表征\n\n作者们通过验证这一理论，首次提供了RLVR学习动态的参数级表征：\n\n*   **偏离主方向学习**：RLVR在权重空间中偏离主方向进行学习。\n*   **最小谱漂移**：通过最小化谱漂移来实现性能提升。\n*   **主子空间旋转减少**：减少了主子空间的旋转。\n*   **非主方向更新对齐**：实现了非主方向更新的对齐。\n\n## 与SFT的对比\n\n与RLVR形成鲜明对比的是，传统的监督微调（SFT）方法：\n\n*   **目标主权重**：SFT倾向于针对主权重进行更新。\n*   **扭曲频谱**：SFT会扭曲模型的频谱。\n*   **性能滞后**：在某些情况下，SFT甚至落后于RLVR。\n\n## 关键启示与未来方向\n\n这些结果首次从参数空间角度阐明了RLVR的训练动态，揭示了参数演化的清晰规律。至关重要的是，研究表明RL与SFT在优化机制上处于截然不同的状态。因此，直接改编SFT时代的参数高效微调（PEFT）方法可能存在缺陷，这一点通过对高级稀疏微调和LoRA变体的案例研究得到了证实。\n\n作者们希望这项工作能够为RLVR的“白盒”理解以及设计几何感知、RLVR原生学习算法指明方向，而不是简单地重复使用SFT时代的启发式方法。",
      "shortSummary": "本文揭示了RLVR在提升LLM性能时表现出的“稀疏性”并非真实稀疏，而是模型条件优化偏差的表面现象。作者提出了“三门理论”解释其机制：RLVR更新偏离主方向，进入低曲率子空间。与SFT不同，RLVR通过最小谱漂移和非主方向更新对齐实现增益。研究强调RL与SFT优化机制不同，直接套用SFT时代的PEFT方法可能存在缺陷，呼吁开发RLVR原生算法。",
      "translated_title": "未选择的路：RLVR 被证明偏离主方向学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement Learning with Verifiable Rewards (RLVR) reliably improves the reasoning performance of large language models, yet it appears to modify only a small fraction of parameters. We revisit this paradox and show that sparsity is a surface artifact of a model-conditioned optimization bias: for a fixed pretrained model, updates consistently localize to preferred parameter regions, highly consistent across runs and largely invariant to datasets and RL recipes. We mechanistically explain these dynamics with a Three-Gate Theory: Gate I (KL Anchor) imposes a KL-constrained update; Gate II (Model Geometry) steers the step off principal directions into low-curvature, spectrum-preserving subspaces; and Gate III (Precision) hides micro-updates in non-preferred regions, making the off-principal bias appear as sparsity. We then validate this theory and, for the first time, provide a parameter-level characterization of RLVR's learning dynamics: RLVR learns off principal directions in weight space, achieving gains via minimal spectral drift, reduced principal-subspace rotation, and off-principal update alignment. In contrast, SFT targets principal weights, distorts the spectrum, and even lags RLVR.   Together, these results provide the first parameter-space account of RLVR's training dynamics, revealing clear regularities in how parameters evolve. Crucially, we show that RL operates in a distinct optimization regime from SFT, so directly adapting SFT-era parameter-efficient fine-tuning (PEFT) methods can be flawed, as evidenced by our case studies on advanced sparse fine-tuning and LoRA variants. We hope this work charts a path toward a white-box understanding of RLVR and the design of geometry-aware, RLVR-native learning algorithms, rather than repurposed SFT-era heuristics."
    },
    {
      "title": "会话系统中的自适应多智能体响应精炼 (原标题: Adaptive Multi-Agent Response Refinement in Conversational Systems)",
      "link": "https://arxiv.org/abs/2511.08319",
      "pubDate": "Tue, 11 Nov 2025 09:48:34 GMT",
      "isoDate": "2025-11-11T09:48:34.000Z",
      "creator": "Soyeong Jeong, Aparna Elangovan, Emine Yilmaz, Oleg Rokhlenko",
      "summary": "### 会话系统中自适应多智能体响应精炼\n\n**1. 引言与背景**\n*   大型语言模型（LLMs）在生成类人会话响应方面表现出色，但仍存在局限性，尤其是在处理个性化信息或特定知识时。\n*   在实际应用中，期望用户自行发现并纠正LLM生成的错误是不切实际的。\n*   现有通过单个LLM进行响应精炼的方法，难以全面顾及会话所需的多个复杂方面。\n\n**2. 提出的解决方案：多智能体框架**\n*   本文提出了一种通过多智能体框架来精炼会话系统响应的方法。\n*   在该框架中，每个智能体被赋予一个特定角色，负责处理会话质量的一个特定方面。\n\n**3. 关键精炼方面**\n*   该研究聚焦于会话质量的三个关键方面：\n    *   **事实性 (Factuality)**：确保响应内容的准确性。\n    *   **个性化 (Personalization)**：使响应符合用户特点或偏好。\n    *   **连贯性 (Coherence)**：保证响应的逻辑性和流畅性。\n*   每个智能体负责审查和精炼其中一个方面，然后将其反馈合并以改进整体响应。\n\n**4. 动态通信策略**\n*   为了增强智能体之间的协作，本文引入了一种动态通信策略。\n*   与遵循固定智能体序列的方法不同，该策略能够根据每个查询的具体需求，自适应地选择和协调最相关的智能体。\n\n**5. 实验验证与结果**\n*   研究人员在具有挑战性的会话数据集上验证了所提出的框架。\n*   结果表明，该框架显著优于相关基线方法，尤其在涉及知识、用户画像（persona）或两者兼有的任务中表现出色。\n\n**6. 其他信息**\n*   本文将在2026年AAAI的LaCATODA研讨会上发表。\n*   相关主题包括：计算与语言 (cs.CL)、人工智能 (cs.AI)、多智能体系统 (cs.MA)。",
      "shortSummary": "本文提出一种会话系统中自适应多智能体响应精炼框架，以解决大型语言模型在个性化和知识处理上的不足。该框架为每个智能体分配特定角色，聚焦于事实性、个性化和连贯性三个关键方面。通过引入动态通信策略，系统能根据查询需求自适应选择和协调智能体。实验证明，该方法在涉及知识或用户画像的会话任务中显著优于现有基线，有效提升了响应质量。",
      "translated_title": "会话系统中的自适应多智能体响应精炼",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both."
    },
    {
      "title": "BiCA：基于引文感知的难负样本的有效生物医学密集检索 (原标题: BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives)",
      "link": "https://arxiv.org/abs/2511.08029",
      "pubDate": "Tue, 11 Nov 2025 04:31:37 GMT",
      "isoDate": "2025-11-11T04:31:37.000Z",
      "creator": "Aarush Sinha, Pavan Kumar S, Roshan Balaji, Nirav Pravinbhai Bhatt",
      "summary": "# BiCA：基于引文感知的难负样本的有效生物医学密集检索\n\n## 摘要\n\n本文提出了一种名为BiCA（Biomedical Dense Retrieval with Citation-Aware Hard Negatives）的方法，旨在通过利用引文链接来解决生物医学和科学领域中密集检索模型训练时难负样本挖掘的挑战。\n\n## 核心问题\n\n*   **难负样本挖掘的挑战**：在训练有效的检索模型时，难负样本至关重要。然而，在生物医学和科学领域，区分源文档和真正的难负样本非常困难，这使得难负样本挖掘变得具有挑战性。\n*   **传统方法的局限性**：传统的难负样本挖掘通常依赖于使用交叉编码器或基于相似性度量（如余弦距离）的静态嵌入模型对文档进行排序。\n\n## BiCA方法\n\n*   **创新点**：BiCA利用引文链接作为生成高质量难负样本的天然来源。被引用的文档与源文档在上下文上具有相关性，但并非重复，这使其成为理想的难负样本。\n*   **数据来源**：该方法利用了20,000篇PubMed文章中的引文链接进行难负样本挖掘。\n*   **模型微调**：BiCA使用这些引文感知的难负样本对GTE_small和GTE_Base模型进行了微调，以改进领域特定的小型密集检索器。\n\n## 主要发现与成果\n\n*   **性能提升**：BiCA在零样本密集检索任务中观察到持续的性能提升。\n*   **评估指标与数据集**：\n    *   在BEIR数据集上，对于域内和域外任务，使用nDCG@10指标取得了显著改进。\n    *   在LoTTE数据集的长尾主题上，使用Success@5指标超越了基线模型。\n*   **潜力与意义**：\n    *   研究结果强调了利用文档链接结构生成高度信息丰富的难负样本的巨大潜力。\n    *   BiCA能够以最少的微调实现最先进的性能。\n    *   该方法为实现高度数据高效的领域适应提供了一条途径。\n\n## 其他信息\n\n*   **发表情况**：该研究已被AAAI 2026接受进行口头报告。",
      "shortSummary": "BiCA提出了一种利用引文链接进行难负样本挖掘的生物医学密集检索方法。针对生物医学领域难负样本挖掘的挑战，BiCA通过分析PubMed文章的引文关系，为GTE_small和GTE_Base模型提供引文感知的难负样本进行微调。该方法在零样本密集检索任务中表现出一致的性能提升，超越了基线模型。BiCA证明了利用文档链接结构生成高质量难负样本的潜力，实现了高效的领域适应和卓越性能。",
      "translated_title": "BiCA：基于引文感知的难负样本的有效生物医学密集检索",
      "images": [],
      "contentSource": "完整文章",
      "content": "Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation."
    },
    {
      "title": "路由流形对齐改进了专家混合LLM的泛化能力 (原标题: Routing Manifold Alignment Improves Generalization of Mixture-of-Experts LLMs)",
      "link": "https://arxiv.org/abs/2511.07419",
      "pubDate": "Mon, 10 Nov 2025 13:59:53 GMT",
      "isoDate": "2025-11-10T13:59:53.000Z",
      "creator": "Zhongyang Li, Ziyue Li, Tianyi Zhou",
      "summary": "## 路由流形对齐改进专家混合LLM的泛化能力\n\n### 背景与问题\n*   稀疏专家混合（MoE）模型已被广泛应用于近期的大型语言模型（LLMs），以在不增加推理成本的情况下有效扩展模型能力。\n*   然而，在广泛的下游任务评估中发现，现有MoE LLMs中的路由器存在持续的次优性。\n*   这种次优性导致与最优路由之间存在严重的性能差距（例如，准确率相差10-20%）。\n\n### RoMA方法：路由流形对齐\n*   本文提出了一种名为“路由流形对齐（Routing Manifold Alignment, RoMA）”的方法，旨在解决上述问题。\n*   **核心思想：** 通过将路由权重的流形与任务嵌入的流形对齐，可以有效缩小性能差距并提高MoE LLMs的泛化性能。\n*   **实现机制：**\n    *   RoMA在后训练目标中引入了一个额外的流形正则化项。\n    *   该方法仅需要对路由器进行轻量级微调，而其他参数保持冻结。\n    *   具体而言，正则化项鼓励每个样本的路由权重在其任务嵌入空间中与其“成功邻居”（即路由权重导致正确答案的样本）的路由权重保持接近。\n    *   结果是，针对相似任务的样本将在不同层之间共享相似的专家选择。\n*   **理论基础：** 在不同样本之间建立任务与专家之间的这种绑定对于实现更好的泛化能力至关重要。\n*   **创新点：** RoMA展示了将任务理解（通过嵌入模型）与解决方案生成（通过MoE LLMs）相结合的优势。\n\n### 实验与结果\n*   研究人员使用RoMA对OLMoE、DeepSeekMoE和Qwen3-MoE中的路由器进行了微调。\n*   在多样化的基准测试中进行的评估以及与基线的广泛比较表明，RoMA带来了显著的性能提升。\n\n### 图片信息\n*   文章内容中未包含有效的实际图片链接，因此本摘要不包含任何图片。",
      "shortSummary": "MoE LLMs的路由器次优导致显著性能差距。本文提出“路由流形对齐（RoMA）”方法，通过在后训练中引入流形正则化项，轻量级微调路由器，将路由权重流形与任务嵌入流形对齐。RoMA促使相似任务的样本共享相似专家选择，从而显著提升了MoE LLMs的泛化能力，并在多个模型和基准测试中得到验证。",
      "translated_title": "路由流形对齐改进了专家混合LLM的泛化能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Sparse Mixture-of-Experts (MoE) have been widely adopted in recent large language models since it can efficiently scale up the model capability without increasing the inference cost. However, evaluations on broad downstream tasks reveal a consistent suboptimality of the routers in existing MoE LLMs, which results in a severe performance gap (e.g., 10-20% in accuracy) to the optimal routing. In this paper, we show that aligning the manifold of routing weights with that of task embedding can effectively reduce the gap and improve MoE LLMs' generalization performance. Our method, \"Routing Manifold Alignment (RoMA)\", introduces an additional manifold regularization term in the post-training objective and only requires lightweight finetuning of routers (with other parameters frozen). Specifically, the regularization encourages the routing weights of each sample to be close to those of its successful neighbors (whose routing weights lead to correct answers) in a task embedding space. Consequently, samples targeting similar tasks will share similar expert choices across layers. Building such bindings between tasks and experts over different samples is essential to achieve better generalization. Moreover, RoMA demonstrates the advantage of unifying the task understanding (by embedding models) with solution generation (by MoE LLMs). In experiments, we finetune routers in OLMoE, DeepSeekMoE, and Qwen3-MoE using RoMA. Evaluations on diverse benchmarks and extensive comparisons with baselines show the substantial improvement brought by RoMA."
    },
    {
      "title": "机器人从物理世界模型中学习 (原标题: Robot Learning from a Physical World Model)",
      "link": "https://arxiv.org/abs/2511.07416",
      "pubDate": "Mon, 10 Nov 2025 13:59:07 GMT",
      "isoDate": "2025-11-10T13:59:07.000Z",
      "creator": "Jiageng Mao, Sicheng He, Hao-Ning Wu, Yang You, Shuyang Sun, Zhicheng Wang, Yanan Bao, Huizhong Chen, Leonidas Guibas, Vitor Guizilini, Howard Zhou, Yue Wang",
      "summary": "## PhysWorld：通过物理世界建模实现机器人从视频生成中学习\n\n### 介绍\nPhysWorld 是一个创新框架，旨在通过物理世界建模，使机器人能够从视频生成中进行学习。该框架解决了现有视频生成模型在机器人领域应用时忽视物理规律，导致操作不准确的局限性。\n\n### 核心问题与解决方案\n\n*   **现有问题**：近期视频生成模型能够根据语言指令和图像合成逼真的视觉演示，为机器人提供了强大的训练信号。然而，直接将生成视频中的像素运动重新定位到机器人上，往往会忽略物理规律，导致操作不准确。\n*   **PhysWorld的解决方案**：PhysWorld 通过将视频生成与物理世界重建相结合来解决这一限制。它将隐式的视觉指导转化为物理上可执行的机器人轨迹。\n\n### PhysWorld 的工作原理\n\n1.  **输入**：给定一张单一图像和一个任务指令。\n2.  **视频生成**：系统生成与任务相关的视频。\n3.  **物理世界重建**：从生成的视频中重建底层的物理世界模型。\n4.  **动作接地**：通过结合物理世界模型，利用以物体为中心的残差强化学习，将生成的视频运动转化为物理上准确的动作。\n\n### 主要优势\n\n*   **消除真实机器人数据收集**：PhysWorld 框架无需收集真实的机器人数据。\n*   **零样本泛化操作**：它能够实现零样本（zero-shot）的泛化机器人操作。\n*   **提高操作精度**：实验表明，与以往方法相比，PhysWorld 大幅提高了操作精度。\n\n### 实验验证\n在各种真实世界任务上的实验证明，PhysWorld 显著提高了操作精度。",
      "shortSummary": "PhysWorld是一个创新框架，通过将视频生成与物理世界重建相结合，使机器人能够从生成的视频中学习。它解决了现有视频生成模型在机器人应用中忽视物理规律导致操作不准确的问题。PhysWorld根据图像和指令生成任务视频，重建物理世界，并通过强化学习将视频运动转化为物理准确的机器人动作。这消除了真实数据收集需求，实现了零样本泛化操作，并显著提高了机器人操作精度。",
      "translated_title": "机器人从物理世界模型中学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering a powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit https://pointscoder.github.io/PhysWorld_Web/{the project webpage} for details."
    },
    {
      "title": "DigiData：训练和评估通用移动控制代理 (原标题: DigiData: Training and Evaluating General-Purpose Mobile Control Agents)",
      "link": "https://arxiv.org/abs/2511.07413",
      "pubDate": "Mon, 10 Nov 2025 13:57:35 GMT",
      "isoDate": "2025-11-10T13:57:35.000Z",
      "creator": "Yuxuan Sun, Manchen Wang, Shengyi Qian, William R. Wong, Eric Gan, Pierluca D'Oro, Alejandro Castillejo Munoz, Sneha Silwal, Pedro Matias, Nitin Kamra, Satwik Kottur, Nick Raines, Xuanyi Zhao, Joy Chen, Joseph Greer, Andrea Madotto, Allen Bolourchi, James Valori, Kevin Carlberg, Karl Ridgeway, Joseph Tighe",
      "summary": "# DigiData：训练和评估通用移动控制代理\n\n## 摘要\n\n人工智能代理（AI agents）控制用户界面的能力有望彻底改变人类与数字设备的交互方式。为了加速这一转变，两个基本要素至关重要：\n\n1.  **高质量数据集**：使代理能够实现复杂且与人类相关的目标。\n2.  **强大的评估方法**：允许研究人员和从业者快速提升代理性能。\n\n本文介绍了 **DigiData** 及其配套的 **DigiData-Bench**，旨在解决上述挑战。\n\n## DigiData 数据集\n\n*   **目的与特点**：\n    *   DigiData 是一个大规模、高质量、多样化、多模态的数据集，专为训练移动控制代理而设计。\n    *   它旨在帮助代理实现复杂且与人类相关的目标。\n*   **创新之处**：\n    *   与现有数据集（通常从非结构化交互中获取目标）不同，DigiData 通过对应用程序功能的全面探索精心构建。\n    *   这种构建方式带来了更高的数据多样性和更复杂的目标设定。\n\n## DigiData-Bench 基准与评估方法\n\n*   **目的**：\n    *   DigiData-Bench 是一个用于评估移动控制代理在真实世界复杂任务上性能的基准。\n*   **评估方法的改进**：\n    *   研究表明，常用的“步长准确性”（step-accuracy）指标在可靠评估移动控制代理方面存在不足。\n    *   为了解决这一问题，本文提出了更严谨的替代方案：\n        *   **动态评估协议**\n        *   **AI驱动的评估**\n\n## 贡献与影响\n\n*   本文的贡献旨在显著推动移动控制代理的开发。\n*   最终目标是为更直观、更有效的人机设备交互铺平道路。",
      "shortSummary": "本文介绍了DigiData，一个大规模、高质量、多模态数据集，用于训练通用移动控制代理。该数据集通过全面探索应用功能构建，提供更高多样性和目标复杂性。同时，文章还提出了DigiData-Bench基准，用于评估代理在真实复杂任务上的表现，并提出动态评估协议和AI驱动评估来替代不足的传统指标。这些工作旨在加速移动控制代理的发展，提升人机交互体验。",
      "translated_title": "DigiData：训练和评估通用移动控制代理",
      "images": [],
      "contentSource": "完整文章",
      "content": "AI agents capable of controlling user interfaces have the potential to transform human interaction with digital devices. To accelerate this transformation, two fundamental building blocks are essential: high-quality datasets that enable agents to achieve complex and human-relevant goals, and robust evaluation methods that allow researchers and practitioners to rapidly enhance agent performance. In this paper, we introduce DigiData, a large-scale, high-quality, diverse, multi-modal dataset designed for training mobile control agents. Unlike existing datasets, which derive goals from unstructured interactions, DigiData is meticulously constructed through comprehensive exploration of app features, resulting in greater diversity and higher goal complexity. Additionally, we present DigiData-Bench, a benchmark for evaluating mobile control agents on real-world complex tasks. We demonstrate that the commonly used step-accuracy metric falls short in reliably assessing mobile control agents and, to address this, we propose dynamic evaluation protocols and AI-powered evaluations as rigorous alternatives for agent assessment. Our contributions aim to significantly advance the development of mobile control agents, paving the way for more intuitive and effective human-device interactions."
    },
    {
      "title": "DIMO：任意物体的多样化3D运动生成 (原标题: DIMO: Diverse 3D Motion Generation for Arbitrary Objects)",
      "link": "https://arxiv.org/abs/2511.07409",
      "pubDate": "Mon, 10 Nov 2025 13:56:49 GMT",
      "isoDate": "2025-11-10T13:56:49.000Z",
      "creator": "Linzhan Mou, Jiahui Lei, Chen Wang, Lingjie Liu, Kostas Daniilidis",
      "summary": "DIMO是一种创新的生成式方法，旨在从单张图像为任意物体生成多样化的3D运动。该方法的核心思想是利用预训练视频模型中丰富的先验知识来提取常见的运动模式，并将其嵌入到一个共享的低维潜在空间中。\n\n**方法论概述：**\n\n*   **多视频生成与运动嵌入：** 首先，系统会为同一物体生成多个具有多样化运动的视频。随后，每个运动都被编码成一个潜在向量。\n*   **共享运动解码器训练：** 训练一个共享的运动解码器，该解码器学习由结构化且紧凑的运动表示（即神经关键点轨迹）所代表的运动分布。\n*   **3D高斯模型驱动：** 规范的3D高斯模型由这些学习到的关键点轨迹驱动，并融合以精确建模物体的几何形状和外观。\n\n**推理与应用：**\n\n*   **即时多样化运动采样：** 在推理阶段，利用学习到的潜在空间，DIMO能够通过单次前向传播即时采样出多样化的3D运动。\n*   **广泛应用：** 该方法支持多种有趣的应用程序，包括3D运动插值和语言引导的运动生成。\n\n**项目信息：**\n\n*   该研究已在ICCV 2025上发表。\n*   项目页面可在提供的URL（this https URL）访问。",
      "shortSummary": "DIMO是一种创新的生成式方法，能够从单张图像为任意物体生成多样化的3D运动。它通过利用预训练视频模型提取运动模式，并将其嵌入到共享的低维潜在空间中。该方法训练一个运动解码器，使用神经关键点轨迹来表示运动分布，并驱动3D高斯模型以构建物体的几何和外观。DIMO支持单次前向传播即时采样多样化3D运动，并可应用于3D运动插值和语言引导的运动生成。",
      "translated_title": "DIMO：任意物体的多样化3D运动生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present DIMO, a generative approach capable of generating diverse 3D motions for arbitrary objects from a single image. The core idea of our work is to leverage the rich priors in well-trained video models to extract the common motion patterns and then embed them into a shared low-dimensional latent space. Specifically, we first generate multiple videos of the same object with diverse motions. We then embed each motion into a latent vector and train a shared motion decoder to learn the distribution of motions represented by a structured and compact motion representation, i.e., neural key point trajectories. The canonical 3D Gaussians are then driven by these key points and fused to model the geometry and appearance. During inference time with learned latent space, we can instantly sample diverse 3D motions in a single-forward pass and support several interesting applications including 3D motion interpolation and language-guided motion generation. Our project page is available at https://linzhanm.github.io/dimo."
    },
    {
      "title": "基于人类演示的计算机使用智能体基础构建 (原标题: Grounding Computer Use Agents on Human Demonstrations)",
      "link": "https://arxiv.org/abs/2511.07332",
      "pubDate": "Mon, 10 Nov 2025 12:35:21 GMT",
      "isoDate": "2025-11-10T12:35:21.000Z",
      "creator": "Aarash Feizi, Shravan Nayak, Xiangru Jian, Kevin Qinghong Lin, Kaixin Li, Rabiul Awal, Xing Han Lù, Johan Obando-Ceron, Juan A. Rodriguez, Nicolas Chapados, David Vazquez, Adriana Romero-Soriano, Reihaneh Rabbany, Perouz Taslakian, Christopher Pal, Spandana Gella, Sai Rajeswar",
      "summary": "## 基于人类演示的计算机使用智能体基础构建\n\n### 引言与背景\n构建可靠的计算机使用智能体需要“基础构建”（grounding），即准确地将自然语言指令与屏幕上的正确元素关联起来。尽管针对网络和移动交互存在大量数据集，但桌面环境的高质量资源却非常有限，这阻碍了通用计算机使用智能体的发展。\n\n### GroundCUA数据集：解决桌面环境数据稀缺问题\n为了弥补这一空白，研究人员引入了 **GroundCUA**，这是一个大规模的桌面基础构建数据集，其构建基于专家级的人类演示。该数据集旨在为模型训练提供高质量数据，以促进计算机使用智能体的开发。\n\n*   **覆盖范围**：GroundCUA涵盖了12个类别中的87个应用程序。\n*   **数据规模**：\n    *   包含5.6万张屏幕截图。\n    *   每个屏幕元素都经过精心注释，总计超过356万个人工验证的注释。\n*   **指令生成**：从这些人类演示中，研究人员生成了多样化的指令，这些指令能够捕捉广泛的真实世界任务，从而为模型训练提供了高质量的输入。\n\n### GroundNext模型家族：实现指令到UI元素的映射\n利用GroundCUA数据集，研究人员开发了 **GroundNext** 模型家族。这些模型旨在将自然语言指令映射到其目标用户界面（UI）元素。\n\n*   **模型规模**：GroundNext模型在3B（30亿参数）和7B（70亿参数）两种规模下进行了开发。\n*   **性能表现**：\n    *   **监督微调**：通过监督微调，GroundNext在五项基准测试中均取得了最先进（state-of-the-art）的结果。\n    *   **数据效率**：与现有工作相比，GroundNext所需的训练数据量不到其十分之一，显示出极高的数据效率。\n    *   **强化学习提升**：通过强化学习的后训练（post-training）进一步提升了模型的性能。\n    *   **智能体环境评估**：在OSWorld基准测试的智能体设置中，结合o3规划器，GroundNext取得了与使用大量更多数据训练的模型相当或更优异的结果。\n\n### 结论\n这些研究结果有力地证明了高质量、专家驱动的数据集在推动通用计算机使用智能体发展中的关键作用。",
      "shortSummary": "为解决桌面环境计算机使用智能体缺乏高质量基础构建数据集的问题，研究者引入了GroundCUA，一个基于专家人类演示的大规模桌面数据集，包含5.6万张截图和356万个注释。基于此数据集，他们开发了GroundNext模型家族，该模型在3B和7B规模下，通过监督微调在五项基准测试中取得了最先进结果，且所需训练数据量不到现有工作的十分之一。结合强化学习和智能体设置，GroundNext展现出与使用更多数据训练的模型相当或更优的性能，凸显了高质量数据集的重要性。",
      "translated_title": "基于人类演示的计算机使用智能体基础构建",
      "images": [],
      "contentSource": "完整文章",
      "content": "Building reliable computer-use agents requires grounding: accurately connecting natural language instructions to the correct on-screen elements. While large datasets exist for web and mobile interactions, high-quality resources for desktop environments are limited. To address this gap, we introduce GroundCUA, a large-scale desktop grounding dataset built from expert human demonstrations. It covers 87 applications across 12 categories and includes 56K screenshots, with every on-screen element carefully annotated for a total of over 3.56M human-verified annotations. From these demonstrations, we generate diverse instructions that capture a wide range of real-world tasks, providing high-quality data for model training. Using GroundCUA, we develop the GroundNext family of models that map instructions to their target UI elements. At both 3B and 7B scales, GroundNext achieves state-of-the-art results across five benchmarks using supervised fine-tuning, while requiring less than one-tenth the training data of prior work. Reinforcement learning post-training further improves performance, and when evaluated in an agentic setting on the OSWorld benchmark using o3 as planner, GroundNext attains comparable or superior results to models trained with substantially more data,. These results demonstrate the critical role of high-quality, expert-driven datasets in advancing general-purpose computer-use agents."
    },
    {
      "title": "IterResearch：通过马尔可夫状态重建重新思考长周期智能体 (原标题: IterResearch: Rethinking Long-Horizon Agents via Markovian State Reconstruction)",
      "link": "https://arxiv.org/abs/2511.07327",
      "pubDate": "Mon, 10 Nov 2025 12:30:08 GMT",
      "isoDate": "2025-11-10T12:30:08.000Z",
      "creator": "Guoxin Chen, Zile Qiao, Xuanzhong Chen, Donglei Yu, Haotian Xu, Wayne Xin Zhao, Ruihua Song, Wenbiao Yin, Huifeng Yin, Liwen Zhang, Kuan Li, Minpeng Liao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou",
      "summary": "IterResearch：通过马尔可夫状态重建重新思考长周期智能体\n\n### 现有问题\n\n*   **上下文饱和与噪声污染**：当前的深度研究智能体在处理长周期任务时，采用单一上下文范式，将所有信息累积在一个不断扩展的上下文窗口中。这导致上下文饱和和噪声污染，限制了它们在长周期任务中的有效性。\n\n### IterResearch 解决方案\n\n*   **新型迭代深度研究范式**：IterResearch 引入了一种新颖的迭代深度研究范式，将长周期研究重新定义为具有战略性工作空间重建的马尔可夫决策过程（Markov Decision Process, MDP）。\n*   **核心机制**：\n    *   **演进报告作为记忆**：通过维护一份不断演进的报告作为记忆。\n    *   **定期综合见解**：定期综合提炼见解。\n    *   **保持推理能力**：确保在任意探索深度下都能保持一致的推理能力。\n\n### 效率感知策略优化 (EAPO)\n\n*   **强化学习框架**：IterResearch 进一步开发了效率感知策略优化（Efficiency-Aware Policy Optimization, EAPO）框架。\n*   **目标**：激励高效探索。\n*   **关键特性**：\n    *   **几何奖励折扣**：通过几何奖励折扣实现。\n    *   **自适应下采样**：通过自适应下采样实现稳定的分布式训练。\n\n### 实验结果与影响\n\n*   **显著性能提升**：在六个基准测试中，IterResearch 相较于现有开源智能体平均提升了 +14.5 个百分点。\n*   **缩小差距**：显著缩小了与前沿专有系统之间的性能差距。\n*   **前所未有的交互扩展**：该范式展现出前所未有的交互扩展能力，可扩展至 2048 次交互，并带来显著的性能提升（从 3.5% 提升至 42.5%）。\n*   **有效提示策略**：作为一种有效的提示策略，在长周期任务中，它能将前沿模型的性能比 ReAct 提升高达 19.2 个百分点。\n\n### 结论\n\n*   **通用解决方案**：IterResearch 是一个用于长周期推理的通用解决方案。\n*   **双重效用**：它既可以作为训练有素的智能体，也可以作为前沿模型的提示范式发挥作用。",
      "shortSummary": "IterResearch 提出了一种迭代式深度研究范式，通过将长周期研究重构为带有战略性工作空间重建的马尔可夫决策过程，解决了现有智能体在长周期任务中面临的上下文饱和与噪声污染问题。它通过维护演进报告和定期综合见解来保持推理能力。结合效率感知策略优化（EAPO），IterResearch 在六个基准测试中平均提升14.5pp，并展现出前所未有的交互扩展能力，同时作为提示策略也能显著提升前沿模型性能，是长周期推理的通用解决方案。",
      "translated_title": "IterResearch：通过马尔可夫状态重建重新思考长周期智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in deep-research agents have shown promise for autonomous knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon tasks. We introduce IterResearch, a novel iterative deep-research paradigm that reformulates long-horizon research as a Markov Decision Process with strategic workspace reconstruction. By maintaining an evolving report as memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. We further develop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning framework that incentivizes efficient exploration through geometric reward discounting and enables stable distributed training via adaptive downsampling. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5\\% to 42.5\\%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct on long-horizon tasks. These findings position IterResearch as a versatile solution for long-horizon reasoning, effective both as a trained agent and as a prompting paradigm for frontier models."
    },
    {
      "title": "VADER：迈向基于关系感知大型语言模型的因果视频异常理解 (原标题: VADER: Towards Causal Video Anomaly Understanding with Relation-Aware Large Language Models)",
      "link": "https://arxiv.org/abs/2511.07299",
      "pubDate": "Mon, 10 Nov 2025 11:56:11 GMT",
      "isoDate": "2025-11-10T11:56:11.000Z",
      "creator": "Ying Cheng, Yu-Ho Lin, Min-Hung Chen, Fu-En Yang, Shang-Hong Lai",
      "summary": "## VADER：基于关系感知大型语言模型的因果视频异常理解\n\n### 摘要\n\n本文介绍了一种名为 VADER 的新型框架，旨在解决视频异常理解（VAU）领域中现有方法的局限性。传统的 VAU 方法主要侧重于异常事件的检测和定位，但往往忽视了对象之间深层次的因果关系和交互作用，而这些对于全面理解异常行为至关重要。\n\n### VADER 框架概述\n\nVADER 是一个由大型语言模型（LLM）驱动的框架，其核心目标是通过整合关键帧对象的关系特征与视觉线索，来增强对视频中异常事件的理解。该框架的关键组成部分和工作流程包括：\n\n*   **异常评分器（Anomaly Scorer）**：首先对视频中的每一帧分配异常分数，以识别潜在的异常时刻。\n*   **上下文感知采样（Context-AwarE Sampling, CAES）**：在异常评分的基础上，VADER 采用 CAES 策略来捕获每个异常事件的因果上下文，确保对异常发生前后的关键信息进行有效提取。\n*   **关系特征提取器（Relation Feature Extractor）与对比关系编码器（COntrastive Relation Encoder, CORE）**：这两个模块协同工作，对视频中对象的动态交互进行建模。它们能够生成紧凑的关系表示，为后续的推理提供基础。\n*   **LLM集成**：将提取到的视觉线索和关系线索与大型语言模型进行集成。通过这种集成，VADER 能够生成详细的、具有因果依据的异常描述，并支持对异常相关问题的鲁棒性回答。\n\n### 实验结果与贡献\n\n在多个真实世界的 VAU 基准测试中，VADER 展示了强大的性能。它在异常描述、解释和因果推理任务上均取得了显著成果，证明了其在理解复杂异常行为方面的有效性。VADER 的提出，标志着可解释视频异常分析领域的一个重要进展，为深入理解视频异常事件提供了新的视角和工具。",
      "shortSummary": "VADER是一个LLM驱动的框架，旨在实现因果视频异常理解（VAU）。它通过整合关键帧对象关系特征和视觉线索，克服了传统方法忽视对象间因果关系的问题。VADER包含异常评分器、上下文感知采样（CAES）以及关系特征提取器和对比关系编码器（CORE），用于建模动态对象交互。该框架能生成详细的因果描述并支持异常相关问答，在多个VAU基准测试中表现出色，显著推进了可解释视频异常分析领域。",
      "translated_title": "VADER：迈向基于关系感知大型语言模型的因果视频异常理解",
      "images": [],
      "contentSource": "完整文章",
      "content": "Video anomaly understanding (VAU) aims to provide detailed interpretation and semantic comprehension of anomalous events within videos, addressing limitations of traditional methods that focus solely on detecting and localizing anomalies. However, existing approaches often neglect the deeper causal relationships and interactions between objects, which are critical for understanding anomalous behaviors. In this paper, we propose VADER, an LLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe object Relation features with visual cues to enhance anomaly comprehension from video. Specifically, VADER first applies an Anomaly Scorer to assign per-frame anomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture the causal context of each anomalous event. A Relation Feature Extractor and a COntrastive Relation Encoder (CORE) jointly model dynamic object interactions, producing compact relational representations for downstream reasoning. These visual and relational cues are integrated with LLMs to generate detailed, causally grounded descriptions and support robust anomaly-related question answering. Experiments on multiple real-world VAU benchmarks demonstrate that VADER achieves strong results across anomaly description, explanation, and causal reasoning tasks, advancing the frontier of explainable video anomaly analysis."
    },
    {
      "title": "Omni-AVSR：迈向基于大型语言模型的统一多模态语音识别 (原标题: Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large Language Models)",
      "link": "https://arxiv.org/abs/2511.07253",
      "pubDate": "Mon, 10 Nov 2025 11:03:44 GMT",
      "isoDate": "2025-11-10T11:03:44.000Z",
      "creator": "Umberto Cappellazzo, Xubo Liu, Pingchuan Ma, Stavros Petridis, Maja Pantic",
      "summary": "## Omni-AVSR：基于大型语言模型的统一多模态语音识别\n\n### 摘要\n\n当前，大型语言模型（LLMs）在听觉语音识别（ASR）、视觉语音识别（VSR）和音视频语音识别（AVSR）等多种模态的语音识别任务中取得了显著进展。然而，现有基于LLM的方法通常独立处理每个任务，训练单独的模型，这不仅增加了计算和部署资源的消耗，还错失了潜在的跨任务协同效应。此外，这些方法依赖于固定速率的令牌压缩，限制了在平衡准确性与效率方面的灵活性。这些局限性凸显了对一个能够支持ASR、VSR和AVSR并实现弹性推理的统一框架的需求。\n\n### Omni-AVSR 解决方案\n\n为解决上述问题，我们提出了 **Omni-AVSR**，一个统一的音视频大型语言模型，它结合了高效的多粒度训练和参数高效的适应策略。\n\n#### 关键方法和创新点：\n\n*   **统一框架设计**：Omni-AVSR旨在提供一个单一模型，能够同时处理ASR、VSR和AVSR任务，从而减少资源消耗并提升跨模态协同。\n*   **高效多粒度训练**：\n    *   我们借鉴了“套娃式表示学习”（matryoshka representation learning）范式，以高效地在多个音频和视觉粒度上进行训练。\n    *   这种方法显著降低了固有的训练资源消耗，提高了训练效率。\n*   **参数高效适应**：\n    *   我们探索了三种基于LoRA（Low-Rank Adaptation）的策略，用于适应骨干LLM。\n    *   这些策略旨在平衡共享知识和任务特定专业化，确保模型在不同任务上的高性能。\n\n### 实验结果与性能\n\n我们在LRS2和LRS3数据集上进行了广泛的实验，结果表明：\n\n*   **卓越的准确性**：Omni-AVSR实现了与现有最先进（SOTA）基线模型相当或更优的准确性。\n*   **显著的资源节约**：在训练单个模型的同时，Omni-AVSR显著降低了训练和部署的资源消耗。\n*   **鲁棒性**：模型在声学噪声环境下仍能保持鲁棒性，展现了其在实际应用中的潜力。\n*   **可扩展性分析**：我们分析了LLM规模增加时的扩展行为，为性能与效率之间的权衡提供了深入见解。",
      "shortSummary": "Omni-AVSR是一个统一的音视频大型语言模型，旨在解决当前LLM在听觉、视觉和音视频语音识别任务中独立训练、资源消耗高的问题。它通过高效的多粒度训练（采用套娃式表示学习）和参数高效的LoRA适应策略，实现了单一模型对多种模态的支持。实验表明，Omni-AVSR在LRS2和LRS3数据集上达到了与SOTA相当或更优的准确性，同时显著降低了训练和部署资源，并在噪声环境下保持鲁棒性。",
      "translated_title": "Omni-AVSR：迈向基于大型语言模型的统一多模态语音识别",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) have recently achieved impressive results in speech recognition across multiple modalities, including Auditory Speech Recognition (ASR), Visual Speech Recognition (VSR), and Audio-Visual Speech Recognition (AVSR). Despite this progress, current LLM-based approaches typically address each task independently, training separate models that raise computational and deployment resource use while missing potential cross-task synergies. They also rely on fixed-rate token compression, which restricts flexibility in balancing accuracy with efficiency. These limitations highlight the need for a unified framework that can support ASR, VSR, and AVSR while enabling elastic inference. To this end, we present Omni-AVSR, a unified audio-visual LLM that combines efficient multi-granularity training with parameter-efficient adaptation. Specifically, we adapt the matryoshka representation learning paradigm to efficiently train across multiple audio and visual granularities, reducing its inherent training resource use. Furthermore, we explore three LoRA-based strategies for adapting the backbone LLM, balancing shared and task-specific specialization. Experiments on LRS2 and LRS3 show that Omni-AVSR achieves comparable or superior accuracy to state-of-the-art baselines while training a single model at substantially lower training and deployment resource use. The model also remains robust under acoustic noise, and we analyze its scaling behavior as LLM size increases, providing insights into the trade-off between performance and efficiency."
    },
    {
      "title": "MVU-Eval：迈向多视频理解评估的多模态大型语言模型 (原标题: MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs)",
      "link": "https://arxiv.org/abs/2511.07250",
      "pubDate": "Mon, 10 Nov 2025 11:02:33 GMT",
      "isoDate": "2025-11-10T11:02:33.000Z",
      "creator": "Tianhao Peng, Haochen Wang, Yuanxing Zhang, Zekun Wang, Zili Wang, Ge Zhang, Jian Yang, Shihao Li, Yanghai Wang, Xintao Wang, Houyi Li, Wei Ji, Pengfei Wan, Wenhao Huang, Zhaoxiang Zhang, Jiaheng Liu",
      "summary": "## MVU-Eval：多模态大型语言模型的多视频理解评估基准\n\n### 背景与动机\n\n*   **现有评估局限性**：尽管多模态大型语言模型（MLLMs）已将AI能力扩展到视觉模态，但现有的评估基准仍局限于单视频理解。\n*   **现实世界需求**：现实场景（如体育分析和自动驾驶）对多视频理解有关键需求，而现有评估未能满足这一需求。\n\n### MVU-Eval 介绍\n\n*   **首个综合基准**：MVU-Eval是首个旨在评估MLLMs多视频理解能力的综合基准。\n*   **评估范围**：\n    *   **核心能力**：主要评估八项核心能力。\n    *   **数据量**：包含1,824个精心策划的问题-答案对。\n    *   **视频数量**：涵盖来自不同领域的4,959个视频。\n    *   **任务类型**：涉及基础感知任务和高阶推理任务。\n*   **与现实应用对齐**：这些能力与多传感器合成（在自动驾驶系统中）和跨角度体育分析等现实世界应用紧密对齐。\n\n### 评估结果与发现\n\n*   **广泛评估**：对最先进的开源和闭源模型进行了广泛评估。\n*   **显著性能差异**：评估揭示了当前MLLMs在执行多视频理解能力方面存在显著的性能差异和局限性。\n\n### 可用性与未来展望\n\n*   **公开可用**：该基准将公开发布，以促进未来的研究。\n\n### 相关领域\n\n*   计算机视觉与模式识别 (cs.CV)\n*   人工智能 (cs.AI)",
      "shortSummary": "MVU-Eval是首个针对多模态大型语言模型（MLLMs）的多视频理解综合评估基准。它旨在弥补现有评估仅限于单视频理解的不足，通过1,824个问答对和4,959个视频，评估MLLMs的八项核心能力，涵盖感知与推理任务，并与自动驾驶、体育分析等现实应用对齐。评估结果显示，当前MLLMs在多视频理解方面存在显著局限性。该基准将公开发布以推动未来研究。",
      "translated_title": "MVU-Eval：迈向多视频理解评估的多模态大型语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "The advent of Multimodal Large Language Models (MLLMs) has expanded AI capabilities to visual modalities, yet existing evaluation benchmarks remain limited to single-video understanding, overlooking the critical need for multi-video understanding in real-world scenarios (e.g., sports analytics and autonomous driving). To address this significant gap, we introduce MVU-Eval, the first comprehensive benchmark for evaluating Multi-Video Understanding for MLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies through 1,824 meticulously curated question-answer pairs spanning 4,959 videos from diverse domains, addressing both fundamental perception tasks and high-order reasoning tasks. These capabilities are rigorously aligned with real-world applications such as multi-sensor synthesis in autonomous systems and cross-angle sports analytics. Through extensive evaluation of state-of-the-art open-source and closed-source models, we reveal significant performance discrepancies and limitations in current MLLMs' ability to perform understanding across multiple videos. The benchmark will be made publicly available to foster future research."
    },
    {
      "title": "Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance",
      "link": "https://arxiv.org/abs/2511.07499",
      "pubDate": "Mon, 10 Nov 2025 10:52:53 GMT",
      "isoDate": "2025-11-10T10:52:53.000Z",
      "creator": "Kwanyoung Kim",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance",
      "images": [],
      "contentSource": "RSS",
      "content": "Diffusion models have demonstrated strong generative performance when using guidance methods such as classifier-free guidance (CFG), which enhance output quality by modifying the sampling trajectory. These methods typically improve a target output by intentionally degrading another, often the unconditional output, using heuristic perturbation functions such as identity mixing or blurred conditions. However, these approaches lack a principled foundation and rely on manually designed distortions. In this work, we propose Adversarial Sinkhorn Attention Guidance (ASAG), a novel method that reinterprets attention scores in diffusion models through the lens of optimal transport and intentionally disrupt the transport cost via Sinkhorn algorithm. Instead of naively corrupting the attention mechanism, ASAG injects an adversarial cost within self-attention layers to reduce pixel-wise similarity between queries and keys. This deliberate degradation weakens misleading attention alignments and leads to improved conditional and unconditional sample quality. ASAG shows consistent improvements in text-to-image diffusion, and enhances controllability and fidelity in downstream applications such as IP-Adapter and ControlNet. The method is lightweight, plug-and-play, and improves reliability without requiring any model retraining."
    },
    {
      "title": "MPJudge：迈向音乐启发绘画的感知评估 (原标题: MPJudge: Towards Perceptual Assessment of Music-Induced Paintings)",
      "link": "https://arxiv.org/abs/2511.07137",
      "pubDate": "Mon, 10 Nov 2025 09:18:27 GMT",
      "isoDate": "2025-11-10T09:18:27.000Z",
      "creator": "Shiqi Jiang, Tianyi Liang, Changbo Wang, Chenhui Li",
      "summary": "## MPJudge：迈向音乐启发绘画的感知评估\n\n### 引言\n*   音乐启发绘画是一种独特的艺术实践，即在音乐影响下创作视觉艺术作品。\n*   评估绘画是否忠实反映其灵感音乐，是一项具有挑战性的感知评估任务。\n\n### 现有方法的局限性\n*   现有方法主要依赖情感识别模型来评估音乐与绘画的相似性。\n*   这些模型引入了相当大的噪声，并忽略了情感之外的更广泛的感知线索。\n\n### MPJudge 框架\n*   **目标：** 针对现有方法的局限性，提出一个新颖的框架，直接建模音乐与视觉艺术之间的感知一致性，以进行音乐启发绘画的评估。\n\n### MPD 数据集\n*   **首创：** 引入了MPD，这是第一个大规模的音乐-绘画对数据集。\n*   **标注：** 由领域专家根据感知一致性进行标注。\n*   **增强：** 为更好地处理模糊情况，进一步收集了成对偏好标注。\n\n### MPJudge 模型\n*   **构建基础：** 基于MPD数据集。\n*   **架构：** 将音乐特征通过基于调制的融合机制整合到视觉编码器中。\n*   **训练：** 采用直接偏好优化（Direct Preference Optimization, DPO）进行训练，以有效学习模糊情况。\n\n### 实验结果\n*   **性能提升：** 大量实验表明，MPJudge方法优于现有方法。\n*   **定性分析：** 质性结果进一步表明，MPJudge模型能更准确地识别绘画中与音乐相关的区域。\n\n### 研究领域与引用\n*   **研究领域：** 计算机视觉与模式识别 (cs.CV)\n*   **引用信息：** arXiv:2511.07137；期刊参考：AAAI 2026",
      "shortSummary": "该研究提出了MPJudge框架，旨在解决音乐启发绘画的感知评估挑战。现有方法依赖情感识别，但存在噪声且忽略了更广泛的感知线索。MPJudge通过直接建模音乐与视觉艺术间的感知一致性来改进。为此，研究构建了首个大规模音乐-绘画对数据集MPD，并引入MPJudge模型，该模型通过调制融合机制整合音乐特征，并利用直接偏好优化进行训练。实验证明，MPJudge优于现有方法，能更准确识别绘画中与音乐相关的区域。",
      "translated_title": "MPJudge：迈向音乐启发绘画的感知评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "Music induced painting is a unique artistic practice, where visual artworks are created under the influence of music. Evaluating whether a painting faithfully reflects the music that inspired it poses a challenging perceptual assessment task. Existing methods primarily rely on emotion recognition models to assess the similarity between music and painting, but such models introduce considerable noise and overlook broader perceptual cues beyond emotion. To address these limitations, we propose a novel framework for music induced painting assessment that directly models perceptual coherence between music and visual art. We introduce MPD, the first large scale dataset of music painting pairs annotated by domain experts based on perceptual coherence. To better handle ambiguous cases, we further collect pairwise preference annotations. Building on this dataset, we present MPJudge, a model that integrates music features into a visual encoder via a modulation based fusion mechanism. To effectively learn from ambiguous cases, we adopt Direct Preference Optimization for training. Extensive experiments demonstrate that our method outperforms existing approaches. Qualitative results further show that our model more accurately identifies music relevant regions in paintings."
    },
    {
      "title": "Wasm：一个构建结构化阿拉伯语交错多模态语料库的管道 (原标题: Wasm: A Pipeline for Constructing Structured Arabic Interleaved Multimodal Corpora)",
      "link": "https://arxiv.org/abs/2511.07080",
      "pubDate": "Mon, 10 Nov 2025 08:10:31 GMT",
      "isoDate": "2025-11-10T08:10:31.000Z",
      "creator": "Khalil Hennara, Ahmad Bastati, Muhammad Hreden, Mohamed Motasim Hamed, Zeina Aldallal, Sara Chrouf, Safwan AlModhayan",
      "summary": "# Wasm：构建结构化阿拉伯语交错多模态语料库的管道\n\n## 摘要\n\n大型语言模型（LLM）和大型多模态模型（LMM）的性能高度依赖于其预训练数据集的质量和规模。近期研究表明，在图像和文本交错的自然文档上训练的大型多模态模型，在广泛的基准测试中表现优于仅在图像-文本对上训练的模型。这些模型利用先进的预训练模型来强制执行语义对齐、图像序列一致性和文本连贯性。\n\n然而，对于阿拉伯语而言，由于缺乏高质量、保留文档结构的多模态数据集，其在该领域的进展受到了限制。\n\n## Wasm 管道介绍\n\n本文提出了 **Wasm** 管道，旨在解决阿拉伯语多模态数据集的不足。Wasm 管道用于处理 Common Crawl 数据集，以创建一个新的阿拉伯语多模态数据集，其独特之处在于提供 Markdown 输出。\n\n### Wasm 的主要特点和优势：\n\n*   **结构完整性保留**：与现有主要关注文本提取的阿拉伯语语料库不同，Wasm 方法保留了网络内容的结构完整性。\n*   **灵活性**：该管道在文本-only和多模态预训练场景中都保持了灵活性。\n*   **Markdown 输出**：数据集以 Markdown 格式输出，便于结构化信息的利用。\n\n## 比较分析与贡献\n\n研究团队对 Wasm 数据处理管道与现有主要数据集所使用的管道进行了全面的比较分析。分析结果突出了过滤策略的共通之处，并解释了 Wasm 特定设计选择的合理性。\n\n为了支持未来的研究，本文公开发布了一个具有代表性的数据集转储以及用于阿拉伯语的多模态处理管道。",
      "shortSummary": "本文提出了Wasm管道，旨在解决阿拉伯语高质量结构化多模态数据集的缺失。Wasm处理Common Crawl数据，创建了一个独特的阿拉伯语多模态数据集，其特点是提供Markdown输出并保留了网络内容的结构完整性。该管道旨在支持LLM和LMM的文本-only及多模态预训练，并已公开发布以促进未来研究。",
      "translated_title": "Wasm：一个构建结构化阿拉伯语交错多模态语料库的管道",
      "images": [],
      "contentSource": "完整文章",
      "content": "The performance of large language models (LLMs) and large multimodal models (LMMs) depends heavily on the quality and scale of their pre-training datasets. Recent research shows that large multimodal models trained on natural documents where images and text are interleaved outperform those trained only on image-text pairs across a wide range of benchmarks, leveraging advanced pre- trained models to enforce semantic alignment, image-sequence consistency, and textual coherence. For Arabic, however, the lack of high-quality multimodal datasets that preserve document structure has limited progress. In this paper, we present our pipeline Wasm for processing the Common Crawl dataset to create a new Arabic multimodal dataset that uniquely provides markdown output. Unlike existing Arabic corpora that focus solely on text extraction, our approach preserves the structural integrity of web content while maintaining flexibility for both text-only and multimodal pre-training scenarios. We provide a comprehensive comparative analysis of our data processing pipeline against those used for major existing datasets, highlighting the convergences in filtering strategies and justifying our specific design choices. To support future research, we publicly release a representative dataset dump along with the multimodal processing pipeline for Arabic."
    }
  ],
  "lastUpdated": "2025-11-14T09:32:21.555Z"
}