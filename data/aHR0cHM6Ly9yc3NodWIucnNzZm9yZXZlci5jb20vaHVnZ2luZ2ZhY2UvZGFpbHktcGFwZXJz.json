{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "通过注意力头选择实现细粒度扰动引导 (原标题: Fine-Grained Perturbation Guidance via Attention Head Selection)",
      "link": "https://arxiv.org/abs/2506.10978",
      "pubDate": "Thu, 12 Jun 2025 13:59:51 GMT",
      "isoDate": "2025-06-12T13:59:51.000Z",
      "creator": "Donghoon Ahn, Jiwon Kang, Sanghyun Lee, Minjae Kim, Jaewon Min, Wooseok Jang, Saungwu Lee, Sayak Paul, Susung Hong, Seungryong Kim",
      "summary": "本文深入探讨了扩散模型中注意力扰动引导的细粒度控制，并提出了名为“HeadHunter”的系统框架，以实现对生成质量和视觉属性的精细控制。\n\n*   **背景与问题：**\n    *   扩散模型中的引导方法通过扰动模型来构建隐式弱模型，从而引导生成。\n    *   注意力扰动在无条件生成场景（不适用无分类器引导）中表现出强大的经验性能。\n    *   然而，现有注意力扰动方法缺乏确定扰动应用位置的原则性方法，尤其是在Diffusion Transformer (DiT) 架构中，因为质量相关的计算分布在不同层中。\n\n*   **核心发现：**\n    *   研究发现，注意力扰动的粒度可以从层级细化到单个注意力头。\n    *   特定的注意力头控制着不同的视觉概念，例如结构、风格和纹理质量。\n\n*   **提出的方法：**\n    *   **HeadHunter框架：** 基于上述发现，本文提出了“HeadHunter”，这是一个系统框架，用于迭代选择与用户中心目标对齐的注意力头。这使得对生成质量和视觉属性的精细控制成为可能。\n    *   **SoftPAG：** 引入SoftPAG，它将每个选定注意力头的注意力图线性插值到单位矩阵。这提供了一个连续的旋钮来调整扰动强度并抑制伪影。\n\n*   **主要优势与贡献：**\n    *   **缓解过平滑问题：** 该方法不仅缓解了现有层级扰动导致的过平滑问题。\n    *   **实现目标化风格操作：** 通过组合式注意力头选择，能够实现对特定视觉风格的目标化操作。\n    *   **验证：** 在现代大型DiT文本到图像模型（包括Stable Diffusion 3和FLUX.1）上进行了验证，在通用质量增强和风格特定引导方面均表现出卓越性能。\n    *   **首次头级分析：** 本文首次对扩散模型中的注意力扰动进行了头级分析，揭示了注意力层内可解释的专业化分工，并为设计有效的扰动策略提供了实用指导。",
      "shortSummary": "本文提出“HeadHunter”框架，通过选择特定注意力头实现扩散模型中细粒度扰动引导。研究发现不同注意力头控制结构、风格等视觉概念。结合SoftPAG，该方法能有效缓解过平滑，并对生成质量和视觉风格进行精确控制。这是首次对扩散模型注意力扰动进行头级分析，并在Stable Diffusion 3和FLUX.1等模型上验证了其卓越性能。",
      "translated_title": "通过注意力头选择实现细粒度扰动引导",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent guidance methods in diffusion models steer reverse sampling by perturbing the model to construct an implicit weak model and guide generation away from it. Among these approaches, attention perturbation has demonstrated strong empirical performance in unconditional scenarios where classifier-free guidance is not applicable. However, existing attention perturbation methods lack principled approaches for determining where perturbations should be applied, particularly in Diffusion Transformer (DiT) architectures where quality-relevant computations are distributed across layers. In this paper, we investigate the granularity of attention perturbations, ranging from the layer level down to individual attention heads, and discover that specific heads govern distinct visual concepts such as structure, style, and texture quality. Building on this insight, we propose \"HeadHunter\", a systematic framework for iteratively selecting attention heads that align with user-centric objectives, enabling fine-grained control over generation quality and visual attributes. In addition, we introduce SoftPAG, which linearly interpolates each selected head's attention map toward an identity matrix, providing a continuous knob to tune perturbation strength and suppress artifacts. Our approach not only mitigates the oversmoothing issues of existing layer-level perturbation but also enables targeted manipulation of specific visual styles through compositional head selection. We validate our method on modern large-scale DiT-based text-to-image models including Stable Diffusion 3 and FLUX.1, demonstrating superior performance in both general quality enhancement and style-specific guidance. Our work provides the first head-level analysis of attention perturbation in diffusion models, uncovering interpretable specialization within attention layers and enabling practical design of effective perturbation strategies."
    },
    {
      "title": "AutoMind：用于自动化数据科学的自适应知识型智能体 (原标题: AutoMind: Adaptive Knowledgeable Agent for Automated Data Science)",
      "link": "https://arxiv.org/abs/2506.10974",
      "pubDate": "Thu, 12 Jun 2025 13:59:32 GMT",
      "isoDate": "2025-06-12T13:59:32.000Z",
      "creator": "Yixin Ou, Yujie Luo, Jingsheng Zheng, Lanning Wei, Shuofei Qiao, Jintian Zhang, Da Zheng, Huajun Chen, Ningyu Zhang",
      "summary": "# AutoMind：用于自动化数据科学的自适应知识型智能体\n\n## 摘要\n\n大型语言模型（LLM）智能体在解决现实世界数据科学问题方面展现出巨大潜力，有望自动化整个机器学习流程。然而，现有框架存在局限性，其依赖于僵化、预定义的工作流和不灵活的编码策略，导致它们仅在相对简单、经典的问题上表现出色，而无法捕捉人类实践者在处理复杂、创新任务时所具备的经验知识。\n\n## AutoMind 框架介绍\n\n本文引入了 **AutoMind**，一个自适应、知识型的 LLM 智能体框架，旨在克服上述缺陷。AutoMind 通过以下三个关键进展实现了其目标：\n\n1.  **精选专家知识库：**\n    *   该知识库将智能体建立在领域专家知识的基础上，为智能体提供坚实的基础，使其能够理解和应用专业的经验和最佳实践。\n\n2.  **智能体知识型树搜索算法：**\n    *   此算法能够策略性地探索可能的解决方案，通过结构化的搜索过程，智能体可以更有效地发现和评估不同的方法，从而找到最优解。\n\n3.  **自适应编码策略：**\n    *   这种策略能够根据任务的复杂性动态调整代码生成。这意味着智能体可以根据具体需求灵活地生成定制化的代码，而不是遵循一成不变的模式，从而提高解决复杂问题的能力。\n\n## 性能评估\n\nAutoMind 在两个自动化数据科学基准测试上进行了评估，结果表明其性能优于现有最先进的基线方法。\n\n## 额外分析与结论\n\n进一步的分析证实了 AutoMind 在以下方面的优势：\n\n*   **有效性：** 解决方案的质量和准确性。\n*   **效率：** 解决问题所需的时间和资源。\n*   **定性解决方案质量：** 生成代码和方法的实用性和优越性。\n\n这些结果突出表明，AutoMind 是迈向完全自动化数据科学的有效且稳健的一步。",
      "shortSummary": "AutoMind 是一个自适应、知识型的LLM智能体框架，旨在克服现有数据科学自动化工具的局限性。它通过整合精选专家知识库、智能体知识型树搜索算法和自适应编码策略，解决了传统框架僵化、缺乏经验知识的问题。在基准测试中，AutoMind 表现出优越的性能、效率和解决方案质量，是实现完全自动化数据科学的重要进展。",
      "translated_title": "AutoMind：用于自动化数据科学的自适应知识型智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) a curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) a self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AutoMind delivers superior performance versus state-of-the-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AutoMind as an efficient and robust step toward fully automated data science."
    },
    {
      "title": "ChineseHarm-Bench：一个中文有害内容检测基准 (原标题: ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark)",
      "link": "https://arxiv.org/abs/2506.10960",
      "pubDate": "Thu, 12 Jun 2025 13:57:05 GMT",
      "isoDate": "2025-06-12T13:57:05.000Z",
      "creator": "Kangwei Liu, Siyuan Cheng, Bozhong Tian, Xiaozhuan Liang, Yuyang Yin, Meng Han, Ningyu Zhang, Bryan Hooi, Xi Chen, Shumin Deng",
      "summary": "## ChineseHarm-Bench：一个中文有害内容检测基准\n\n### 引言\n\n大型语言模型（LLMs）在自动化有害内容检测任务中扮演着越来越重要的角色，它们能够协助内容审核员识别违规内容，从而提高内容审核的整体效率和准确性。然而，当前有害内容检测的现有资源主要集中在英文领域，中文数据集则相对稀缺且范围有限，这限制了LLMs在中文有害内容检测方面的应用和发展。\n\n### ChineseHarm-Bench：中文有害内容检测基准\n\n为了解决中文有害内容检测数据集稀缺的问题，本文提出了一个名为“ChineseHarm-Bench”的综合性、专业标注的中文内容有害性检测基准。该基准具有以下特点：\n\n*   **数据来源**：完全基于真实世界数据构建，确保了数据的实用性和代表性。\n*   **覆盖范围**：涵盖了六个具有代表性的有害内容类别，提供了广泛的检测范围。\n*   **标注质量**：经过专业标注，保证了数据集的准确性和可靠性。\n\n### 知识规则库\n\n在ChineseHarm-Bench的标注过程中，研究人员还额外产出了一个**知识规则库**。这个规则库提供了明确的专家知识，旨在辅助LLMs更有效地进行中文有害内容检测，弥补了模型在处理复杂中文语境时可能存在的知识空白。\n\n### 知识增强基线模型\n\n此外，文章还提出了一种**知识增强基线模型**。该模型创新性地整合了人工标注的知识规则和大型语言模型中蕴含的隐式知识。通过这种结合，即使是规模较小的模型也能够实现与当前最先进（state-of-the-art, SOTA）LLMs相媲美的性能，这为资源受限的环境下进行高效有害内容检测提供了新的可能性。\n\n### 资源可用性与项目状态\n\n*   **代码与数据**：相关代码和数据已公开可用，方便研究人员进行复现和进一步研究。\n*   **项目状态**：该工作目前仍在进行中（Work in progress）。\n*   **相关领域**：该研究涉及计算与语言（cs.CL）、人工智能（cs.AI）、密码学与安全（cs.CR）、信息检索（cs.IR）和机器学习（cs.LG）等多个学科领域。",
      "shortSummary": "针对中文有害内容检测数据集稀缺的问题，本文提出了“ChineseHarm-Bench”——一个综合性、专业标注的中文有害内容检测基准。该基准基于真实世界数据构建，涵盖六个代表性类别。研究还产出了一个知识规则库，并提出了一种知识增强基线模型，该模型结合人工规则和LLM隐式知识，使小型模型也能达到SOTA LLM的性能。代码和数据已公开。",
      "translated_title": "ChineseHarm-Bench：一个中文有害内容检测基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) have been increasingly applied to automated harmful content detection tasks, assisting moderators in identifying policy violations and improving the overall efficiency and accuracy of content review. However, existing resources for harmful content detection are predominantly focused on English, with Chinese datasets remaining scarce and often limited in scope. We present a comprehensive, professionally annotated benchmark for Chinese content harm detection, which covers six representative categories and is constructed entirely from real-world data. Our annotation process further yields a knowledge rule base that provides explicit expert knowledge to assist LLMs in Chinese harmful content detection. In addition, we propose a knowledge-augmented baseline that integrates both human-annotated knowledge rules and implicit knowledge from large language models, enabling smaller models to achieve performance comparable to state-of-the-art LLMs. Code and data are available at https://github.com/zjunlp/ChineseHarm-bench."
    },
    {
      "title": "SWE-Factory：您的自动化问题解决训练数据和评估基准工厂 (原标题: SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks)",
      "link": "https://arxiv.org/abs/2506.10954",
      "pubDate": "Thu, 12 Jun 2025 13:54:17 GMT",
      "isoDate": "2025-06-12T13:54:17.000Z",
      "creator": "Lianghong Guo, Yanlin Wang, Caihua Li, Pengyu Yang, Jiachi Chen, Wei Tao, Yingtian Zou, Duyu Tang, Zibin Zheng",
      "summary": "## SWE-Factory：自动化GitHub问题解决数据集构建\n\n### 引言\n\n为大型语言模型（LLMs）构建用于GitHub问题解决任务的大规模数据集，对于训练和评估其软件工程能力至关重要。然而，传统的基准创建过程面临巨大挑战且劳动密集，尤其是在设置评估环境、测试结果评分和验证任务实例等阶段。\n\n### SWE-Factory 解决方案\n\n本文提出 **SWE-Factory**，一个旨在解决这些挑战的自动化流程。该流程集成了三个核心自动化组件：\n\n1.  **SWE-Builder：** 一个多智能体系统，用于自动化评估环境的构建。它包含四个专业智能体，以协作、迭代循环的方式工作，并利用环境内存池来提高效率。\n2.  **标准化、基于退出码的评分方法：** 这种方法消除了手动编写自定义解析器的需要，简化了评分过程。\n3.  **自动化 fail2pass 验证：** 利用可靠的退出码信号，实现任务实例的自动化验证。\n\n### 实验与结果\n\n研究团队对四种编程语言的671个问题进行了实验，结果表明 SWE-Factory 流程能够有效地构建有效的任务实例：\n\n*   使用 GPT-4.1-mini 时，SWE-Builder 能够构建269个有效实例，每个实例的成本为0.045美元。\n*   使用 Gemini-2.5-flash 时，其性能与GPT-4.1-mini相当，但成本更低，每个实例仅为0.024美元。\n\n此外，实验还验证了 SWE-Factory 各组件的准确性：\n\n*   基于退出码的评分方法与人工检查相比，达到了100%的准确率。\n*   自动化 fail2pass 验证的精确度为0.92，召回率为1.00。\n\n### 结论与展望\n\nSWE-Factory 有望加速大规模、高质量GitHub问题解决数据集的收集，从而促进LLM在软件工程领域的训练和评估。相关代码和数据集已发布。",
      "shortSummary": "SWE-Factory是一个自动化流程，旨在解决为大型语言模型构建GitHub问题解决训练数据和评估基准的挑战。它通过SWE-Builder自动化评估环境构建，采用基于退出码的标准化评分方法，并实现自动化fail2pass验证。实验证明，SWE-Factory能有效构建高质量任务实例，显著降低成本，并实现高准确率，从而加速大规模数据集的收集。",
      "translated_title": "SWE-Factory：您的自动化问题解决训练数据和评估基准工厂",
      "images": [],
      "contentSource": "完整文章",
      "content": "Constructing large-scale datasets for the GitHub issue resolution task is crucial for both training and evaluating the software engineering capabilities of Large Language Models (LLMs). However, the traditional process for creating such benchmarks is notoriously challenging and labor-intensive, particularly in the stages of setting up evaluation environments, grading test outcomes, and validating task instances. In this paper, we propose SWE-Factory, an automated pipeline designed to address these challenges. To tackle these issues, our pipeline integrates three core automated components. First, we introduce SWE-Builder, a multi-agent system that automates evaluation environment construction, which employs four specialized agents that work in a collaborative, iterative loop and leverages an environment memory pool to enhance efficiency. Second, we introduce a standardized, exit-code-based grading method that eliminates the need for manually writing custom parsers. Finally, we automate the fail2pass validation process using these reliable exit code signals. Experiments on 671 issues across four programming languages show that our pipeline can effectively construct valid task instances; for example, with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at 0.045 per instance, while with Gemini-2.5-flash, it achieves comparable performance at the lowest cost of 0.024 per instance. We also demonstrate that our exit-code-based grading achieves 100% accuracy compared to manual inspection, and our automated fail2pass validation reaches a precision of 0.92 and a recall of 1.00. We hope our automated pipeline will accelerate the collection of large-scale, high-quality GitHub issue resolution datasets for both training and evaluation. Our code and datasets are released at https://github.com/DeepSoftwareAnalytics/swe-factory."
    },
    {
      "title": "为智能体构建网络，而非为网络构建智能体 (原标题: Build the web for agents, not agents for the web)",
      "link": "https://arxiv.org/abs/2506.10953",
      "pubDate": "Thu, 12 Jun 2025 13:53:58 GMT",
      "isoDate": "2025-06-12T13:53:58.000Z",
      "creator": "Xing Han Lù, Gaurav Kamath, Marius Mosbach, Siva Reddy",
      "summary": "### 核心问题：当前网络接口与AI智能体能力不匹配\n\n*   **背景**：大型语言模型（LLMs）及多模态模型的发展，激发了开发网络智能体（Web Agents）的巨大兴趣。这些AI系统旨在自主导航和完成网络环境中的任务。\n*   **挑战**：现有方法面临重大挑战，因为人类设计的网络接口与LLM的能力之间存在根本性不匹配。具体困难包括：\n    *   处理庞大的DOM树。\n    *   依赖截图并辅以额外信息。\n    *   通过API交互完全绕过用户界面。\n\n### 提出的解决方案：范式转变——为智能体设计网络接口\n\n*   **核心主张**：本文倡导网络智能体研究的范式转变。与其强迫网络智能体适应为人类设计的接口，不如开发一种专门为智能体能力优化的新型交互范式。\n*   **概念引入**：引入“智能体网络接口”（Agentic Web Interface, AWI）的概念。AWI是一种专门为智能体导航网站而设计的接口。\n\n### AWI设计原则与目标\n\n*   **六项指导原则**：为AWI设计建立了六项指导原则，强调：\n    *   安全性（Safety）\n    *   效率（Efficiency）\n    *   标准化（Standardization）\n*   **利益相关者**：这些原则旨在兼顾所有主要利益相关者的利益。\n*   **预期成果**：这种重新定位旨在克服现有接口的根本性局限，为设计更高效、可靠和透明的网络智能体铺平道路。\n\n### 协作努力\n\n*   **社区参与**：实现这一目标将是一项协作努力，需要更广泛的机器学习（ML）社区的参与。",
      "shortSummary": "当前网络接口与AI智能体能力不匹配，导致网络智能体开发面临挑战。本文提出范式转变，倡导为智能体设计专门的“智能体网络接口”（AWI）。AWI将遵循安全性、效率和标准化等原则，旨在克服现有接口局限，实现更高效、可靠、透明的网络智能体设计，这将是机器学习社区的协作成果。",
      "translated_title": "为智能体构建网络，而非为网络构建智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spurred significant interest in developing web agents -- AI systems capable of autonomously navigating and completing tasks within web environments. While holding tremendous promise for automating complex web interactions, current approaches face substantial challenges due to the fundamental mismatch between human-designed interfaces and LLM capabilities. Current methods struggle with the inherent complexity of web inputs, whether processing massive DOM trees, relying on screenshots augmented with additional information, or bypassing the user interface entirely through API interactions. This position paper advocates for a paradigm shift in web agent research: rather than forcing web agents to adapt to interfaces designed for humans, we should develop a new interaction paradigm specifically optimized for agentic capabilities. To this end, we introduce the concept of an Agentic Web Interface (AWI), an interface specifically designed for agents to navigate a website. We establish six guiding principles for AWI design, emphasizing safety, efficiency, and standardization, to account for the interests of all primary stakeholders. This reframing aims to overcome fundamental limitations of existing interfaces, paving the way for more efficient, reliable, and transparent web agent design, which will be a collaborative effort involving the broader ML community."
    },
    {
      "title": "Domain2Vec：向量化数据集以无需训练找到最优数据混合 (原标题: Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture without Training)",
      "link": "https://arxiv.org/abs/2506.10952",
      "pubDate": "Thu, 12 Jun 2025 13:53:51 GMT",
      "isoDate": "2025-06-12T13:53:51.000Z",
      "creator": "Mozhi Zhang, Howe Tissue, Lu Wang, Xipeng Qiu",
      "summary": "# Domain2Vec：无需训练寻找最优数据混合的向量化数据集方法\n\n## 核心概念与方法\n*   **Domain2Vec 介绍**：Domain2Vec 是一种新颖的方法，旨在将任何数据集分解为多个“元域”（meta-domains）的线性组合。元域是一个新概念，用于捕捉数据集的关键底层特征。\n*   **元域词汇表与分类器**：Domain2Vec 维护一个元域词汇表，并使用分类器将给定数据集分解为域向量。这些域向量对应于元域词汇表上的一个分布。\n\n## 应用与优势\n*   **优化语言模型预训练数据混合**：\n    *   通过域向量，Domain2Vec 能够在无需训练的情况下，识别出语言模型（LM）预训练的最佳数据混合。\n    *   这一过程基于“**分布对齐假设**”（DA²），即当训练集和验证集的数据分布对齐程度越高时，验证损失越低。\n*   **提升效率与可扩展性**：Domain2Vec 可以无缝集成到现有工作中，以建模域向量与 LM 性能之间的关系，从而显著提高现有方法的效率和可扩展性。\n*   **计算开销最小化**：该方法以最小的计算开销增强下游任务性能。\n\n## 实验结果\n*   **计算效率提升**：在 Pile-CC 数据集上，Domain2Vec 仅使用原始 The Pile 数据集混合训练所需计算量的 51.5%，就达到了相同的验证损失。\n*   **下游任务性能提升**：在同等计算预算下，Domain2Vec 平均将下游任务性能提高了 2.83%。\n\n## 其他信息\n*   **接受会议**：该研究已被 ICML2025 接受。\n*   **研究领域**：计算与语言 (cs.CL)、人工智能 (cs.AI)、机器学习 (cs.LG)。",
      "shortSummary": "Domain2Vec 是一种无需训练即可优化语言模型数据混合的新方法。它通过将数据集分解为“元域”并生成“域向量”，利用“分布对齐假设”来识别最佳数据组合。实验表明，Domain2Vec 能显著减少计算量（如在 Pile-CC 上节省 51.5% 计算），同时平均提升下游任务性能 2.83%，从而提高效率和可扩展性。",
      "translated_title": "Domain2Vec：向量化数据集以无需训练找到最优数据混合",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce~Domain2Vec, a novel approach that decomposes any dataset into a linear combination of several meta-domains, a new concept designed to capture the key underlying features of datasets. Domain2Vec maintains a vocabulary of meta-domains and uses a classifier to decompose any given dataset into a domain vector that corresponds to a distribution over this vocabulary. These domain vectors enable the identification of the optimal data mixture for language model (LM) pretraining in a training-free manner under the \\textbf{Distribution Alignment Assumption} (DA^{2}), which suggests that when the data distributions of the training set and the validation set are better aligned, a lower validation loss is achieved. Moreover, Domain2vec can be seamlessly integrated into previous works to model the relationship between domain vectors and LM performance, greatly enhancing the efficiency and scalability of previous methods. Extensive experiments demonstrate that Domain2Vec helps find the data mixture that enhances downstream task performance with minimal computational overhead. Specifically, Domain2Vec achieves the same validation loss on Pile-CC using only 51.5% of the computation required when training on the original mixture of The Pile dataset. Under equivalent compute budget, Domain2Vec improves downstream performance by an average of 2.83%."
    },
    {
      "title": "通过半非负矩阵分解将MLP激活分解为可解释特征 (原标题: Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization)",
      "link": "https://arxiv.org/abs/2506.10920",
      "pubDate": "Thu, 12 Jun 2025 13:33:29 GMT",
      "isoDate": "2025-06-12T13:33:29.000Z",
      "creator": "Or Shafran, Atticus Geiger, Mor Geva",
      "summary": "### 通过半非负矩阵分解（SNMF）实现MLP激活的可解释特征分解\n\n**1. 研究背景与问题**\n\n*   **目标：** 机械可解释性的核心目标是识别大型语言模型（LLMs）中能够因果解释其输出的分析单元。\n*   **现有挑战：** 早期工作侧重于单个神经元，但有证据表明神经元通常编码多个概念，这促使研究转向分析激活空间中的方向。关键问题是如何以无监督方式找到捕获可解释特征的方向。\n*   **当前方法局限性：** 现有方法依赖于稀疏自编码器（SAEs）进行字典学习，通常在残差流激活上训练以从头学习方向。然而，SAEs在因果评估中表现不佳，并且缺乏内在可解释性，因为它们的学习与模型的计算没有明确关联。\n\n**2. 提出的方法：半非负矩阵分解（SNMF）**\n\n*   **核心思想：** 本文通过直接使用半非负矩阵分解（SNMF）来分解多层感知器（MLP）的激活，以解决上述局限性。\n*   **SNMF特征的优势：**\n    *   **稀疏线性组合：** 学习到的特征是共同激活神经元的稀疏线性组合。\n    *   **直接可解释性：** 这些特征与其激活输入直接映射，使其具有直接可解释性。\n\n**3. 实验结果与发现**\n\n*   **模型与基准：** 在Llama 3.1、Gemma 2和GPT-2模型上进行了实验，并将SNMF与SAEs以及一个强大的监督基线（均值差异法）进行了比较。\n*   **性能表现：** SNMF导出的特征在因果引导（causal steering）方面优于SAEs和监督基线。\n*   **概念对齐：** SNMF特征与人类可解释的概念保持一致。\n*   **深层结构揭示：** 进一步分析表明，特定的神经元组合在语义相关的特征中被重复利用，这揭示了MLP激活空间中存在的层次结构。\n\n**4. 结论**\n\n*   这些结果共同表明，SNMF是一种简单且有效的工具，可用于识别可解释特征并剖析LLMs中的概念表示。",
      "shortSummary": "本研究提出通过半非负矩阵分解（SNMF）直接分解大型语言模型（LLMs）中多层感知器（MLP）的激活，以识别可解释特征。针对现有稀疏自编码器（SAEs）在因果评估和可解释性上的不足，SNMF将特征分解为神经元稀疏组合并映射到激活输入，实现直接可解释性。实验表明，SNMF在Llama 3.1、Gemma 2和GPT-2上，在因果引导方面优于SAEs和监督基线，并揭示了MLP激活空间的层次结构。SNMF被证明是识别LLM可解释特征的有效工具。",
      "translated_title": "通过半非负矩阵分解将MLP激活分解为可解释特征",
      "images": [],
      "contentSource": "完整文章",
      "content": "A central goal for mechanistic interpretability has been to identify the right units of analysis in large language models (LLMs) that causally explain their outputs. While early work focused on individual neurons, evidence that neurons often encode multiple concepts has motivated a shift toward analyzing directions in activation space. A key question is how to find directions that capture interpretable features in an unsupervised manner. Current methods rely on dictionary learning with sparse autoencoders (SAEs), commonly trained over residual stream activations to learn directions from scratch. However, SAEs often struggle in causal evaluations and lack intrinsic interpretability, as their learning is not explicitly tied to the computations of the model. Here, we tackle these limitations by directly decomposing MLP activations with semi-nonnegative matrix factorization (SNMF), such that the learned features are (a) sparse linear combinations of co-activated neurons, and (b) mapped to their activating inputs, making them directly interpretable. Experiments on Llama 3.1, Gemma 2 and GPT-2 show that SNMF derived features outperform SAEs and a strong supervised baseline (difference-in-means) on causal steering, while aligning with human-interpretable concepts. Further analysis reveals that specific neuron combinations are reused across semantically-related features, exposing a hierarchical structure in the MLP's activation space. Together, these results position SNMF as a simple and effective tool for identifying interpretable features and dissecting concept representations in LLMs."
    },
    {
      "title": "NoLoCo：大型模型无需All-reduce的低通信训练方法 (原标题: NoLoCo: No-all-reduce Low Communication Training Method for Large Models)",
      "link": "https://arxiv.org/abs/2506.10911",
      "pubDate": "Thu, 12 Jun 2025 13:23:23 GMT",
      "isoDate": "2025-06-12T13:23:23.000Z",
      "creator": "Jari Kolehmainen, Nikolay Blagoev, John Donaghy, Oğuzhan Ersoy, Christopher Nies",
      "summary": "# NoLoCo：大型模型无需All-reduce的低通信训练方法\n\n## 引言\n大型语言模型的训练通常在包含数万个加速器的集群上进行，这些集群通过高带宽互连进行通信。然而，扩展这些集群成本高昂且不切实际，从而限制了可训练模型的规模。尽管近期一些研究提出了通信密集度较低的训练方法，避免了对高度连接计算集群的需求，但这些最先进的低通信训练方法仍然需要对模型参数进行同步步骤。当在所有模型副本上执行此同步时，在低带宽网络上会变得非常昂贵。\n\n## NoLoCo 方法概述\n本文提出了一种新颖的优化方法——NoLoCo，它在训练过程中不显式同步所有模型参数，因此不需要任何集体通信（如all-reduce）。NoLoCo通过Nesterov动量优化器的一种新颖变体，隐式地同步模型权重。其核心机制是通过将模型权重与随机选择的另一个权重进行部分平均来实现隐式同步。\n\n## 性能与优势\n研究团队为NoLoCo优化器提供了理论收敛性分析，并展示了语言模型训练的实证结果。NoLoCo在广泛的加速器数量和模型大小（从1.25亿到68亿参数）上进行了基准测试。\n\n*   **通信效率显著提升**：与完全分片数据并行训练（Fully Sharded Data Parallel Training）甚至广泛使用的低通信训练方法DiLoCo相比，NoLoCo所需的通信开销显著减少。\n*   **同步速度大幅加快**：对于通过互联网训练的数百个加速器，NoLoCo的同步步骤估计比DiLoCo中使用的all-reduce快一个数量级。\n*   **减少加速器空闲时间**：NoLoCo不包含任何全局阻塞通信，从而有效减少了加速器的空闲时间。\n*   **更快的收敛速度**：与DiLoCo相比，NoLoCo在各种模型大小和加速器数量下，观察到高达4%的收敛速度提升。\n\n## 结论\nNoLoCo为大型语言模型提供了一种高效且通信开销极低的训练方法。通过避免显式全局同步和集体通信，NoLoCo克服了现有方法的通信瓶颈，有望在资源受限的环境下实现更大规模模型的训练。",
      "shortSummary": "NoLoCo是一种新型的大型模型训练方法，旨在解决现有方法中昂贵的全局同步（如all-reduce）带来的通信瓶颈。它通过Nesterov动量优化器的一种变体，实现模型权重的隐式同步，无需任何集体通信。与现有方法相比，NoLoCo显著降低了通信开销，同步速度快一个数量级，减少了加速器空闲时间，并能将收敛速度提升高达4%，从而实现更高效的大型模型训练。",
      "translated_title": "NoLoCo：大型模型无需All-reduce的低通信训练方法",
      "images": [],
      "contentSource": "完整文章",
      "content": "Training large language models is generally done via optimization methods on clusters containing tens of thousands of accelerators, communicating over a high-bandwidth interconnect. Scaling up these clusters is expensive and can become impractical, imposing limits on the size of models that can be trained. Several recent studies have proposed training methods that are less communication intensive, avoiding the need for a highly connected compute cluster. These state-of-the-art low communication training methods still employ a synchronization step for model parameters, which, when performed over all model replicas, can become costly on a low-bandwidth network.   In this work, we propose a novel optimization method, NoLoCo, that does not explicitly synchronize all model parameters during training and, as a result, does not require any collective communication. NoLoCo implicitly synchronizes model weights via a novel variant of the Nesterov momentum optimizer by partially averaging model weights with a randomly selected other one. We provide both a theoretical convergence analysis for our proposed optimizer as well as empirical results from language model training.   We benchmark NoLoCo on a wide range of accelerator counts and model sizes, between 125M to 6.8B parameters. Our method requires significantly less communication overhead than fully sharded data parallel training or even widely used low communication training method, DiLoCo. The synchronization step itself is estimated to be one magnitude faster than the all-reduce used in DiLoCo for few hundred accelerators training over the internet. We also do not have any global blocking communication that reduces accelerator idling time. Compared to DiLoCo, we also observe up to 4% faster convergence rate with wide range of model sizes and accelerator counts."
    },
    {
      "title": "Magistral",
      "link": "https://arxiv.org/abs/2506.10910",
      "pubDate": "Thu, 12 Jun 2025 13:22:37 GMT",
      "isoDate": "2025-06-12T13:22:37.000Z",
      "creator": "Mistral-AI, Abhinav Rastogi, Albert Q. Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, Léonard Blier, Lucile Saulnier, Matthieu Dinot, Maxime Darrin, Neha Gupta, Roman Soletskyi, Sagar Vaze, Teven Le Scao, Yihan Wang, Adam Yang, Alexander H. Liu, Alexandre Sablayrolles, Amélie Héliou, Amélie Martin, Andy Ehrenberg, Anmol Agarwal, Antoine Roux, Arthur Darcet, Arthur Mensch, Baptiste Bout, Baptiste Rozière, Baudouin De Monicault, Chris Bamford, Christian Wallenwein, Christophe Renaudin, Clémence Lanfranchi, Darius Dabert, Devon Mizelle, Diego de las Casas, Elliot Chane-Sane, Emilien Fugier, Emma Bou Hanna, Gauthier Delerce, Gauthier Guinet, Georgii Novikov, Guillaume Martin, Himanshu Jaju, Jan Ludziejewski, Jean-Hadrien Chabran, Jean-Malo Delignon, Joachim Studnia, Jonas Amar, Josselin Somerville Roberts, Julien Denize, Karan Saxena, Kush Jain, Lingxiao Zhao, Louis Martin, Luyu Gao, Lélio Renard Lavaud, Marie Pellat, Mathilde Guillaumin, Mathis Felardos, Maximilian Augustin, Mickaël Seznec, Nikhil Raghuraman, Olivier Duchenne, Patricia Wang, Patrick von Platen, Patryk Saffer, Paul Jacob, Paul Wambergue, Paula Kurylowicz, Pavankumar Reddy Muddireddy, Philomène Chagniot, Pierre Stock, Pravesh Agrawal, Romain Sauvestre, Rémi Delacourt, Sanchit Gandhi, Sandeep Subramanian, Shashwat Dalal, Siddharth Gandhi, Soham Ghosh, Srijan Mishra, Sumukh Aithal, Szymon Antoniak, Thibault Schueller, Thibaut Lavril, Thomas Robert, Thomas Wang, Timothée Lacroix, Valeriia Nemychnikova, Victor Paltz, Virgile Richard, Wen-Ding Li, William Marshall, Xuanyu Zhang, Yunhao Tang",
      "summary": "### Magistral：Mistral-AI 的首个推理模型与可扩展强化学习管线\n\nMistral-AI 团队隆重推出了 **Magistral**，这不仅是他们首个专注于推理能力的模型，更标志着他们自主研发的可扩展强化学习（RL）管线的诞生。\n\n#### 核心方法与创新\n\nMagistral 项目的核心在于其“从零开始”（ground-up）的独特方法。与以往依赖现有实现或从先前模型蒸馏 RL 轨迹不同，该项目完全基于 Mistral-AI 自己的模型和基础设施进行构建。这一创新方法使得团队能够：\n\n*   **探索纯 RL 训练的极限**：他们展示了一个能够深入探索大型语言模型（LLMs）纯粹强化学习训练潜力的技术栈。\n*   **强制推理语言**：提出了一种简单而有效的方法，能够引导模型使用特定的推理语言。\n\n#### 关键发现与成果\n\n研究结果令人鼓舞，表明仅在文本数据上进行强化学习训练，不仅能够保持初始检查点的大部分能力，甚至在以下关键领域有所提升：\n\n*   **多模态理解**：模型对多种数据形式的理解能力得到维持或增强。\n*   **指令遵循**：模型执行用户指令的准确性和效率得到保持或改善。\n*   **函数调用**：模型在调用外部函数方面的能力得到维持或提升。\n\n#### 模型发布\n\nMistral-AI 公布了两款 Magistral 系列模型：\n\n*   **Magistral Medium**：该模型在 Mistral Medium 3 的基础上，完全通过强化学习进行推理训练，展现了纯 RL 在复杂任务上的潜力。\n*   **Magistral Small**：该模型已根据 Apache 2.0 许可开源，并且进一步包含了来自 Magistral Medium 的冷启动数据，为社区提供了可访问的资源。\n\n#### 研究领域\n\n本研究属于计算与语言（cs.CL）领域。",
      "shortSummary": "Magistral 是 Mistral-AI 推出的首个推理模型，并引入了其自研的可扩展强化学习（RL）管线。该项目采用“从零开始”的方法，完全基于自有模型和基础设施。研究发现，仅通过文本数据上的 RL 训练，模型能保持并提升多模态理解、指令遵循和函数调用能力。Mistral-AI 发布了基于纯 RL 训练的 Magistral Medium，并开源了包含冷启动数据的 Magistral Small 模型。",
      "translated_title": "Magistral",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Magistral, Mistral's first reasoning model and our own scalable reinforcement learning (RL) pipeline. Instead of relying on existing implementations and RL traces distilled from prior models, we follow a ground up approach, relying solely on our own models and infrastructure. Notably, we demonstrate a stack that enabled us to explore the limits of pure RL training of LLMs, present a simple method to force the reasoning language of the model, and show that RL on text data alone maintains most of the initial checkpoint's capabilities. We find that RL on text maintains or improves multimodal understanding, instruction following and function calling. We present Magistral Medium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we open-source Magistral Small (Apache 2.0) which further includes cold-start data from Magistral Medium."
    },
    {
      "title": "CreatiPoster：迈向可编辑和可控的多层平面设计生成 (原标题: CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic Design Generation)",
      "link": "https://arxiv.org/abs/2506.10890",
      "pubDate": "Thu, 12 Jun 2025 12:54:39 GMT",
      "isoDate": "2025-06-12T12:54:39.000Z",
      "creator": "Zhao Zhang, Yutao Cheng, Dexiang Hong, Maoke Yang, Gonglei Shi, Lei Ma, Hui Zhang, Jie Shao, Xinglong Wu",
      "summary": "### CreatiPoster：可编辑和可控的多层平面设计生成框架\n\n**1. 引言与背景**\n\n*   **挑战：** 高质量、可编辑且美观的平面设计创作耗时耗力，尤其对初学者而言。现有AI工具虽能自动化部分流程，但在整合用户资产、保持可编辑性及实现专业视觉效果方面表现不佳。商业系统（如Canva Magic Design）依赖庞大的模板库，难以复制。\n\n**2. CreatiPoster 框架介绍**\n\n*   **核心目标：** 引入CreatiPoster，一个能够从自然语言指令或用户资产生成可编辑、多层设计作品的框架。\n*   **工作原理：** 采用双模型方法：\n    *   **协议模型（RGBA大型多模态模型）：** 首先生成一个JSON规范，详细定义每个图层（文本或资产）的精确布局、层级、内容和样式，并提供一个简洁的背景提示。\n    *   **条件背景模型：** 随后根据渲染出的前景图层合成一个连贯的背景。\n\n**3. 性能评估与贡献**\n\n*   **基准测试：** 构建了一个包含自动化指标的平面设计生成基准。\n*   **性能表现：** CreatiPoster 在该基准测试中超越了领先的开源方法和专有商业系统。\n*   **数据发布：** 为促进进一步研究，项目发布了一个包含100,000个多层设计的无版权语料库。\n\n**4. 多样化应用**\n\n*   CreatiPoster 支持多种应用场景，包括：\n    *   画布编辑\n    *   文本叠加\n    *   响应式尺寸调整\n    *   多语言适配\n    *   动态海报制作\n\n**5. 愿景**\n\n*   该框架旨在推动AI辅助平面设计的普及化和民主化。",
      "shortSummary": "CreatiPoster是一个创新的AI框架，旨在生成可编辑、多层的平面设计作品。它通过一个双模型系统，根据自然语言指令或用户资产生成详细的图层规范和背景。该框架在性能上超越了现有系统，并发布了一个包含10万个设计的无版权语料库。CreatiPoster支持多种应用，如画布编辑、文本叠加和动态海报，致力于推动AI辅助平面设计的普及。",
      "translated_title": "CreatiPoster：迈向可编辑和可控的多层平面设计生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Graphic design plays a crucial role in both commercial and personal contexts, yet creating high-quality, editable, and aesthetically pleasing graphic compositions remains a time-consuming and skill-intensive task, especially for beginners. Current AI tools automate parts of the workflow, but struggle to accurately incorporate user-supplied assets, maintain editability, and achieve professional visual appeal. Commercial systems, like Canva Magic Design, rely on vast template libraries, which are impractical for replicate. In this paper, we introduce CreatiPoster, a framework that generates editable, multi-layer compositions from optional natural-language instructions or assets. A protocol model, an RGBA large multimodal model, first produces a JSON specification detailing every layer (text or asset) with precise layout, hierarchy, content and style, plus a concise background prompt. A conditional background model then synthesizes a coherent background conditioned on this rendered foreground layers. We construct a benchmark with automated metrics for graphic-design generation and show that CreatiPoster surpasses leading open-source approaches and proprietary commercial systems. To catalyze further research, we release a copyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports diverse applications such as canvas editing, text overlay, responsive resizing, multilingual adaptation, and animated posters, advancing the democratization of AI-assisted graphic design. Project homepage: https://github.com/graphic-design-ai/creatiposter"
    },
    {
      "title": "VRBench：长篇叙事视频多步推理基准 (原标题: VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos)",
      "link": "https://arxiv.org/abs/2506.10857",
      "pubDate": "Thu, 12 Jun 2025 12:17:17 GMT",
      "isoDate": "2025-06-12T12:17:17.000Z",
      "creator": "Jiashuo Yu, Yue Wu, Meng Chu, Zhifei Ren, Zizheng Huang, Pei Chu, Ruijie Zhang, Yinan He, Qirui Li, Songze Li, Zhenxiang Li, Zhongying Tu, Conghui He, Yu Qiao, Yali Wang, Yi Wang, Limin Wang",
      "summary": "# VRBench：长篇叙事视频多步推理基准\n\n**引言**\n\nVRBench是首个专为评估大型模型在长篇叙事视频中多步推理能力而设计的基准。它旨在弥补现有评估在时间推理和程序有效性方面的不足。\n\n**基准构成与数据策展**\n\n*   **视频数据：** 包含1,010个长视频，平均时长1.6小时。\n*   **问答对：** 拥有9,468个人工标注的多步问答对。\n*   **推理步骤：** 包含30,292个带有时间戳的推理步骤。\n*   **数据策展流程：** 视频内容通过多阶段过滤过程进行精心策划，包括专家交叉评审，以确保情节的连贯性。\n\n**推理链生成框架**\n\n*   VRBench开发了一个人机协作框架，用于生成连贯的推理链。\n*   每个推理链都要求多个基于时间定位的步骤。\n*   推理类型涵盖七种，例如事件归因和隐式推理。\n\n**多阶段评估流程**\n\n*   该基准设计了一个多阶段评估流程，从结果和过程两个层面全面评估模型。\n*   **结果评估：** 采用多项选择题（MCQs）来评估最终结果。\n*   **过程评估：** 提出了一种由大型语言模型（LLM）引导的进度级别评分指标，用于多维度地综合评估推理链的质量。\n\n**实验与发现**\n\n*   研究人员在VRBench上对12个大型语言模型（LLMs）和16个视觉语言模型（VLMs）进行了广泛评估。\n*   通过深入分析，该研究为多步推理领域提供了宝贵的见解，推动了该领域的发展。\n\n**其他信息**\n\n*   该工作以技术报告形式发布。\n*   相关研究领域包括计算机视觉与模式识别（cs.CV）、人工智能（cs.AI）和多媒体（cs.MM）。",
      "shortSummary": "VRBench是首个用于评估大型模型在长篇叙事视频中多步推理能力的基准。它包含1,010个平均1.6小时的长视频，以及9,468个人工标注的多步问答对和30,292个带时间戳的推理步骤。该基准通过人机协作框架生成连贯的推理链，并设计了多阶段评估流程，从结果和过程层面全面评估模型。对12个LLM和16个VLM的广泛评估提供了多步推理领域的宝贵见解，填补了现有评估在时间推理和程序有效性方面的空白。",
      "translated_title": "VRBench：长篇叙事视频多步推理基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present VRBench, the first long narrative video benchmark crafted for evaluating large models' multi-step reasoning capabilities, addressing limitations in existing evaluations that overlook temporal reasoning and procedural validity. It comprises 1,010 long videos (with an average duration of 1.6 hours), along with 9,468 human-labeled multi-step question-answering pairs and 30,292 reasoning steps with timestamps. These videos are curated via a multi-stage filtering process including expert inter-rater reviewing to prioritize plot coherence. We develop a human-AI collaborative framework that generates coherent reasoning chains, each requiring multiple temporally grounded steps, spanning seven types (e.g., event attribution, implicit inference). VRBench designs a multi-phase evaluation pipeline that assesses models at both the outcome and process levels. Apart from the MCQs for the final results, we propose a progress-level LLM-guided scoring metric to evaluate the quality of the reasoning chain from multiple dimensions comprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on VRBench, we undertake a thorough analysis and provide valuable insights that advance the field of multi-step reasoning."
    },
    {
      "title": "VideoDeepResearch：使用智能体工具进行长视频理解 (原标题: VideoDeepResearch: Long Video Understanding With Agentic Tool Using)",
      "link": "https://arxiv.org/abs/2506.10821",
      "pubDate": "Thu, 12 Jun 2025 11:39:10 GMT",
      "isoDate": "2025-06-12T11:39:10.000Z",
      "creator": "Huaying Yuan, Zheng Liu, Junjie Zhou, Ji-Rong Wen, Zhicheng Dou",
      "summary": "VideoDeepResearch：使用智能体工具进行长视频理解\n\n**摘要**\n\n长视频理解（LVU）对当前的多模态大型语言模型（MLLM）构成了重大挑战，这主要是由于任务本身的复杂性以及上下文窗口的限制。传统观点认为，解决LVU任务需要具备扩展上下文窗口、强大视觉感知能力和专业领域知识的基础MLLM。\n\n**VideoDeepResearch 框架**\n\n*   **挑战传统观念**：本文提出VideoDeepResearch，一个新颖的智能体框架，旨在挑战上述传统观念。该框架表明，LVU任务不一定需要复杂的MLLM。\n*   **核心构成**：\n    *   **纯文本大型推理模型（LRM）**：VideoDeepResearch 仅依赖一个纯文本的LRM作为核心推理引擎。\n    *   **模块化多模态工具包**：LRM与一个模块化的多模态工具包结合使用，该工具包包含多模态检索器和视觉感知器。这些工具在实践中均可轻易获得。\n*   **工作原理**：\n    *   对于每个LVU任务，系统通过推理制定问题解决策略。\n    *   系统选择性地访问并利用必要的视频内容，通过工具使用来实现这一目标。\n\n**实验结果与性能**\n\n*   **实验基准**：研究团队在流行的LVU基准测试上进行了广泛实验，包括MLVU、Video-MME和LVBench。\n*   **显著提升**：VideoDeepResearch 在现有MLLM基线上取得了显著的性能提升：\n    *   在MLVU（测试集）上超越现有最佳水平9.6%。\n    *   在LVBench上超越现有最佳水平6.6%。\n    *   在LongVideoBench上超越现有最佳水平3.9%。\n\n**结论**\n\n这些发现强调了智能体系统在克服长视频理解问题中关键挑战方面的巨大潜力。",
      "shortSummary": "VideoDeepResearch 提出一种新颖的智能体框架，用于长视频理解（LVU）。该框架挑战了LVU必须依赖复杂多模态大型语言模型（MLLM）的传统观念，转而使用纯文本大型推理模型（LRM）结合模块化多模态工具包。系统通过推理制定策略并利用工具访问视频内容。实验结果显示，VideoDeepResearch 在多个LVU基准测试上显著超越现有MLLM基线，证明了智能体系统在解决LVU问题上的巨大潜力。",
      "translated_title": "VideoDeepResearch：使用智能体工具进行长视频理解",
      "images": [],
      "contentSource": "完整文章",
      "content": "Long video understanding (LVU) presents a significant challenge for current multi-modal large language models (MLLMs) due to the task's inherent complexity and context window constraint. It is widely assumed that addressing LVU tasks requires foundation MLLMs with extended context windows, strong visual perception capabilities, and proficient domain expertise. In this work, we challenge this common belief by introducing VideoDeepResearch, a novel agentic framework for long video understanding. Our approach relies solely on a text-only large reasoning model (LRM) combined with a modular multi-modal toolkit, including multimodal retrievers and visual perceivers, all of which are readily available in practice. For each LVU task, the system formulates a problem-solving strategy through reasoning, while selectively accessing and utilizing essential video content via tool using. We conduct extensive experiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench. Our results demonstrate that VideoDeepResearch achieves substantial improvements over existing MLLM baselines, surpassing the previous state-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and LongVideoBench, respectively. These findings highlight the promise of agentic systems in overcoming key challenges in LVU problems."
    },
    {
      "title": "PosterCraft：统一框架下高质量美学海报生成的再思考 (原标题: PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a Unified Framework)",
      "link": "https://arxiv.org/abs/2506.10741",
      "pubDate": "Thu, 12 Jun 2025 10:28:12 GMT",
      "isoDate": "2025-06-12T10:28:12.000Z",
      "creator": "SiXiang Chen, Jianyu Lai, Jialin Gao, Tian Ye, Haoyu Chen, Hengyu Shi, Shitong Shao, Yunlong Lin, Song Fei, Zhaohu Xing, Yeying Jin, Junfeng Luo, Xiaoming Wei, Lei Zhu",
      "summary": "## PosterCraft：统一框架下高质量美学海报生成\n\n### 引言\n生成美学海报比简单的图像设计更具挑战性，因为它不仅需要精确的文本渲染，还需要抽象艺术内容、引人注目的布局和整体风格和谐的无缝整合。\n\n### PosterCraft 框架概述\n为了应对这些挑战，研究人员提出了 PosterCraft，一个统一的框架。该框架摒弃了以往模块化的管道和僵硬、预定义的布局，允许模型自由探索连贯且视觉上引人注目的构图。\n\n### 核心工作流程\nPosterCraft 采用精心设计的级联工作流程来优化高美学海报的生成，具体包括以下四个阶段：\n\n1.  **大规模文本渲染优化**：在研究团队新引入的 Text-Render-2M 数据集上进行大规模文本渲染优化。\n2.  **区域感知监督微调**：在 HQ-Poster100K 数据集上进行区域感知监督微调。\n3.  **美学文本强化学习**：通过“最佳之N”偏好优化（best-of-n preference optimization）进行美学文本强化学习。\n4.  **视觉-语言联合反馈细化**：通过视觉和语言的联合反馈进行精炼。\n\n### 数据与训练\n每个阶段都由一个完全自动化的数据构建管道支持，该管道根据其特定需求量身定制，从而在不进行复杂架构修改的情况下实现稳健的训练。\n\n### 性能评估\nPosterCraft 在多项实验中进行了评估，结果显示其在渲染准确性、布局连贯性和整体视觉吸引力方面显著优于开源基线，其质量已接近最先进的商业系统。",
      "shortSummary": "PosterCraft 是一个统一的框架，旨在解决高质量美学海报生成中的复杂挑战。它通过放弃传统模块化设计，采用级联工作流，包括文本渲染优化、区域感知微调、美学文本强化学习和视觉-语言反馈细化。该框架利用自动化数据构建，显著提升了海报的渲染准确性、布局连贯性和视觉吸引力，性能超越开源基线并接近顶级商业系统。",
      "translated_title": "PosterCraft：统一框架下高质量美学海报生成的再思考",
      "images": [],
      "contentSource": "完整文章",
      "content": "Generating aesthetic posters is more challenging than simple design images: it requires not only precise text rendering but also the seamless integration of abstract artistic content, striking layouts, and overall stylistic harmony. To address this, we propose PosterCraft, a unified framework that abandons prior modular pipelines and rigid, predefined layouts, allowing the model to freely explore coherent, visually compelling compositions. PosterCraft employs a carefully designed, cascaded workflow to optimize the generation of high-aesthetic posters: (i) large-scale text-rendering optimization on our newly introduced Text-Render-2M dataset; (ii) region-aware supervised fine-tuning on HQ-Poster100K; (iii) aesthetic-text-reinforcement learning via best-of-n preference optimization; and (iv) joint vision-language feedback refinement. Each stage is supported by a fully automated data-construction pipeline tailored to its specific needs, enabling robust training without complex architectural modifications. Evaluated on multiple experiments, PosterCraft significantly outperforms open-source baselines in rendering accuracy, layout coherence, and overall visual appeal-approaching the quality of SOTA commercial systems. Our code, models, and datasets can be found in the Project page: https://ephemeral182.github.io/PosterCraft"
    },
    {
      "title": "TaxoAdapt: 将基于LLM的多维分类法构建与演进中的研究语料库对齐 (原标题: TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to Evolving Research Corpora)",
      "link": "https://arxiv.org/abs/2506.10737",
      "pubDate": "Thu, 12 Jun 2025 10:26:28 GMT",
      "isoDate": "2025-06-12T10:26:28.000Z",
      "creator": "Priyanka Kargupta, Nan Zhang, Yunyi Zhang, Rui Zhang, Prasenjit Mitra, Jiawei Han",
      "summary": "### TaxoAdapt：将基于LLM的多维分类法构建与演进中的研究语料库对齐\n\n**背景与挑战**\n\n*   科学领域的快速发展给科学文献的组织和检索带来了挑战。\n*   传统的专家策展分类法耗时且成本高昂。\n*   现有的自动化分类法构建方法存在以下局限：\n    *   过度依赖特定语料库，牺牲了通用性。\n    *   过度依赖大型语言模型（LLM）预训练数据中包含的通用知识，往往忽视了不断演进的科学领域的动态特性。\n    *   未能考虑到科学文献的多维性，即一篇研究论文可能涉及多个维度（例如，方法论、新任务、评估指标、基准等）。\n\n**TaxoAdapt 框架**\n\n*   **目的：** 为解决上述问题，本文提出了 TaxoAdapt 框架，旨在将LLM生成的分类法动态地适应给定的多维语料库。\n*   **工作原理：**\n    *   TaxoAdapt 执行迭代分层分类。\n    *   它根据语料库的主题分布，扩展分类法的广度和深度。\n\n**性能与成果**\n\n*   该框架在多年来多样化的计算机科学会议数据集上展示了最先进的性能，证明了其构建结构并捕捉科学领域演进的能力。\n*   作为一种多维方法，TaxoAdapt 生成的分类法在粒度保持性方面比最具竞争力的基线高出 26.51%，在连贯性方面高出 50.41%（由LLM评估）。\n\n**其他信息**\n\n*   该研究已被 ACL 2025 主会议接受。\n*   文章内容中未包含有效的实际图片链接，因此摘要中不包含图片。",
      "shortSummary": "TaxoAdapt是一个创新框架，旨在解决科学文献组织和检索的挑战。它通过动态调整LLM生成的分类法，使其适应不断演进的多维研究语料库。TaxoAdapt采用迭代分层分类，根据语料库主题分布扩展分类法的广度和深度。该方法在计算机科学会议数据集上表现出最先进的性能，生成的分类法在粒度保持性和连贯性方面显著优于现有基线，有效捕捉了科学领域的演进。",
      "translated_title": "TaxoAdapt: 将基于LLM的多维分类法构建与演进中的研究语料库对齐",
      "images": [],
      "contentSource": "完整文章",
      "content": "The rapid evolution of scientific fields introduces challenges in organizing and retrieving scientific literature. While expert-curated taxonomies have traditionally addressed this need, the process is time-consuming and expensive. Furthermore, recent automatic taxonomy construction methods either (1) over-rely on a specific corpus, sacrificing generalizability, or (2) depend heavily on the general knowledge of large language models (LLMs) contained within their pre-training datasets, often overlooking the dynamic nature of evolving scientific domains. Additionally, these approaches fail to account for the multi-faceted nature of scientific literature, where a single research paper may contribute to multiple dimensions (e.g., methodology, new tasks, evaluation metrics, benchmarks). To address these gaps, we propose TaxoAdapt, a framework that dynamically adapts an LLM-generated taxonomy to a given corpus across multiple dimensions. TaxoAdapt performs iterative hierarchical classification, expanding both the taxonomy width and depth based on corpus' topical distribution. We demonstrate its state-of-the-art performance across a diverse set of computer science conferences over the years to showcase its ability to structure and capture the evolution of scientific fields. As a multidimensional method, TaxoAdapt generates taxonomies that are 26.51% more granularity-preserving and 50.41% more coherent than the most competitive baselines judged by LLMs."
    },
    {
      "title": "超越真假：检索增强的细致主张分层分析 (原标题: Beyond True or False: Retrieval-Augmented Hierarchical Analysis of Nuanced Claims)",
      "link": "https://arxiv.org/abs/2506.10728",
      "pubDate": "Thu, 12 Jun 2025 10:17:45 GMT",
      "isoDate": "2025-06-12T10:17:45.000Z",
      "creator": "Priyanka Kargupta, Runchu Tian, Jiawei Han",
      "summary": "## 检索增强的细致主张分层分析：ClaimSpect 框架\n\n### 挑战：细致主张的复杂性\n\n在科学和政治领域，个人或实体提出的主张往往是细致入微的，无法简单地标记为完全“真”或“假”。例如，“疫苗A优于疫苗B”这样的主张，其判断涉及多个方面。\n\n### 解决方案：分层解构与ClaimSpect\n\n为了更全面、结构化地回应此类问题，并允许读者优先关注特定角度（如儿童安全性），本文提出了一种新颖的方法：\n\n*   **主张解构**：将一个主张分解为其组成方面和子方面（例如，疫苗的功效、安全性、分发）。这些独立的方面更容易验证。\n*   **ClaimSpect 框架**：\n    *   **类型**：基于检索增强生成（Retrieval-Augmented Generation, RAG）的框架。\n    *   **目标**：自动构建处理主张时通常考虑的方面层次结构，并利用语料库特定的视角进行丰富。\n    *   **工作原理**：\n        *   对输入语料库进行分层分区，以检索相关片段。\n        *   这些片段有助于发现新的子方面。\n        *   此外，它们还能揭示对主张某个方面的不同观点（支持、中立或反对）及其流行程度（例如，“有多少生物医学论文认为疫苗A比疫苗B更易于运输？”）。\n\n### 应用与验证\n\n*   **应用范围**：ClaimSpect 被应用于一个包含各种真实世界科学和政治主张的构建数据集。\n*   **成果**：该框架在解构细致主张和表示语料库内观点方面展现出强大的鲁棒性和准确性。\n*   **验证**：通过真实世界的案例研究和人工评估，ClaimSpect 的有效性得到了验证，并优于多个基线方法。\n\n### 附加信息\n\n*   **会议接受**：该工作已被 ACL 2025 主会议接受。\n*   **代码可用性**：相关代码已公开提供。",
      "shortSummary": "本文提出了 ClaimSpect，一个基于检索增强生成的框架，旨在分析细致入微的主张。针对科学和政治领域中难以简单判断真假的主张，ClaimSpect 能将其分解为可验证的方面和子方面，并构建分层结构。该框架通过检索语料库片段，发现新子方面及对各方面的不同观点及其流行程度。ClaimSpect 在真实世界案例中展现出强大的鲁棒性和准确性，并通过人类评估验证了其有效性。该研究已被 ACL 2025 主会议接受。",
      "translated_title": "超越真假：检索增强的细致主张分层分析",
      "images": [],
      "contentSource": "完整文章",
      "content": "Claims made by individuals or entities are oftentimes nuanced and cannot be clearly labeled as entirely \"true\" or \"false\" -- as is frequently the case with scientific and political claims. However, a claim (e.g., \"vaccine A is better than vaccine B\") can be dissected into its integral aspects and sub-aspects (e.g., efficacy, safety, distribution), which are individually easier to validate. This enables a more comprehensive, structured response that provides a well-rounded perspective on a given problem while also allowing the reader to prioritize specific angles of interest within the claim (e.g., safety towards children). Thus, we propose ClaimSpect, a retrieval-augmented generation-based framework for automatically constructing a hierarchy of aspects typically considered when addressing a claim and enriching them with corpus-specific perspectives. This structure hierarchically partitions an input corpus to retrieve relevant segments, which assist in discovering new sub-aspects. Moreover, these segments enable the discovery of varying perspectives towards an aspect of the claim (e.g., support, neutral, or oppose) and their respective prevalence (e.g., \"how many biomedical papers believe vaccine A is more transportable than B?\"). We apply ClaimSpect to a wide variety of real-world scientific and political claims featured in our constructed dataset, showcasing its robustness and accuracy in deconstructing a nuanced claim and representing perspectives within a corpus. Through real-world case studies and human evaluation, we validate its effectiveness over multiple baselines."
    },
    {
      "title": "TeleMath：电信数学问题解决中大型语言模型的基准测试 (原标题: TeleMath: A Benchmark for Large Language Models in Telecom Mathematical Problem Solving)",
      "link": "https://arxiv.org/abs/2506.10674",
      "pubDate": "Thu, 12 Jun 2025 09:04:18 GMT",
      "isoDate": "2025-06-12T09:04:18.000Z",
      "creator": "Vincenzo Colle, Mohamed Sana, Nicola Piovesan, Antonio De Domenico, Fadhel Ayed, Merouane Debbah",
      "summary": "## TeleMath：电信数学问题解决中大型语言模型的基准测试\n\n### 背景与动机\n\n*   随着人工智能在电信领域的日益普及，人们对大型语言模型（LLMs）解决领域特定、数学密集型任务的能力产生了浓厚兴趣。\n*   尽管LLMs在通用数学推理方面取得了显著进展，但它们在信号处理、网络优化和性能分析等专业领域中的有效性仍未得到充分探索。\n\n### TeleMath：新基准数据集的引入\n\n*   为了弥补这一研究空白，研究人员引入了 **TeleMath**，这是首个专门设计用于评估LLMs在电信领域解决数值数学问题的基准数据集。\n*   **数据集构成：** TeleMath包含500个问答（QnA）对。\n*   **主题覆盖：** 该数据集涵盖了电信领域的广泛主题，确保了评估的全面性。\n*   **QnA生成流程：** 论文详细概述了所提出的QnA生成流程，该流程始于由主题专家（SME）精心设计的初始问题集，确保了问题的专业性和准确性。\n\n### LLM评估结果\n\n*   研究团队对一系列开源LLMs进行了广泛评估，以测试它们在TeleMath上的表现。\n*   **表现最佳模型：** 评估结果显示，在TeleMath上表现最佳的是近期专门为数学或逻辑推理设计的模型。\n*   **通用模型表现：** 相比之下，即使是参数量很大的通用模型，也常常难以应对这些电信领域特有的数学挑战。\n\n### 数据与代码发布\n\n*   为了便于结果复现并支持未来的研究，研究团队已公开发布了该数据集和评估代码。\n\n### 其他信息\n\n*   论文共6页。\n*   主题分类：人工智能 (cs.AI)；计算与语言 (cs.CL)。\n*   引用信息：arXiv:2506.10674。",
      "shortSummary": "TeleMath是首个评估大型语言模型（LLMs）在电信领域解决数学问题能力的基准数据集。该数据集包含500个问答对，涵盖广泛的电信主题。研究发现，专门为数学或逻辑推理设计的LLMs表现最佳，而通用模型则难以应对。为促进研究和复现，数据集和评估代码已公开发布。",
      "translated_title": "TeleMath：电信数学问题解决中大型语言模型的基准测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "The increasing adoption of artificial intelligence in telecommunications has raised interest in the capability of Large Language Models (LLMs) to address domain-specific, mathematically intensive tasks. Although recent advancements have improved the performance of LLMs in general mathematical reasoning, their effectiveness within specialized domains, such as signal processing, network optimization, and performance analysis, remains largely unexplored. To address this gap, we introduce TeleMath, the first benchmark dataset specifically designed to evaluate LLM performance in solving mathematical problems with numerical solutions in the telecommunications domain. Comprising 500 question-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the telecommunications field. This paper outlines the proposed QnAs generation pipeline, starting from a selected seed of problems crafted by Subject Matter Experts. The evaluation of a wide range of open-source LLMs reveals that best performance on TeleMath is achieved by recent models explicitly designed for mathematical or logical reasoning. In contrast, general-purpose models, even those with a large number of parameters, often struggle with these challenges. We have released the dataset and the evaluation code to ease result reproducibility and support future research."
    },
    {
      "title": "EmbodiedGen：迈向具身智能的生成式3D世界引擎 (原标题: EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence)",
      "link": "https://arxiv.org/abs/2506.10600",
      "pubDate": "Thu, 12 Jun 2025 07:43:50 GMT",
      "isoDate": "2025-06-12T07:43:50.000Z",
      "creator": "Wang Xinjie, Liu Liu, Cao Yu, Wu Ruiqi, Qin Wenkang, Wang Dehui, Sui Wei, Su Zhizhong",
      "summary": "# EmbodiedGen：具身智能的生成式3D世界引擎\n\n## 背景与挑战\n\n当前具身智能任务的训练和评估，高度依赖手动创建和标注的传统3D计算机图形资产。然而，这些资产存在以下局限性：\n\n*   **生产成本高昂**：手动创建过程耗时且昂贵。\n*   **真实性有限**：难以达到物理真实和精确缩放的要求。\n*   **阻碍可扩展性**：限制了数据驱动方法在多样性、真实性、低成本可访问性方面的扩展。\n\n这些问题严重阻碍了具身AI实现泛化和可扩展性。\n\n## EmbodiedGen 平台介绍\n\nEmbodiedGen 是一个为交互式3D世界生成而设计的**基础平台**，旨在解决上述挑战。其核心特点和优势包括：\n\n*   **高质量、可控、逼真的3D资产生成**：能够大规模生成具有精确物理属性和真实世界尺度的3D资产。\n*   **低成本**：显著降低3D数据资产的生产成本。\n*   **标准格式输出**：生成的资产采用统一机器人描述格式（URDF）。\n*   **无缝集成**：这些资产可以直接导入各种物理模拟引擎，支持精细的物理控制，从而服务于下游的训练和评估任务。\n*   **易用且功能齐全**：EmbodiedGen 被设计为一个用户友好、功能全面的工具包。\n\n## 核心模块\n\nEmbodiedGen 工具包由六个关键模块组成：\n\n1.  **图像到3D (Image-to-3D)**：从图像生成3D模型。\n2.  **文本到3D (Text-to-3D)**：从文本描述生成3D模型。\n3.  **纹理生成 (Texture Generation)**：为3D模型生成纹理。\n4.  **关节对象生成 (Articulated Object Generation)**：生成具有可动关节的复杂对象。\n5.  **场景生成 (Scene Generation)**：创建完整的3D场景。\n6.  **布局生成 (Layout Generation)**：规划和组织场景中的对象布局。\n\n## 价值与意义\n\nEmbodiedGen 通过利用生成式AI，生成由生成式3D资产组成的多样化、交互式3D世界。这有助于解决具身智能相关研究在**泛化能力**和**评估需求**方面的挑战。\n\n## 代码可用性\n\n相关代码已公开。",
      "shortSummary": "EmbodiedGen是一个为具身智能设计的生成式3D世界引擎。它通过生成式AI，低成本、可扩展地创建高质量、物理精确且逼真的3D资产和交互式世界。该平台旨在解决传统3D资产生产成本高、真实性有限的挑战，支持将生成的URDF格式资产直接导入物理模拟引擎进行训练和评估。EmbodiedGen包含图像到3D、文本到3D等六大核心模块，显著提升具身AI研究的泛化能力和数据可扩展性。",
      "translated_title": "EmbodiedGen：迈向具身智能的生成式3D世界引擎",
      "images": [],
      "contentSource": "完整文章",
      "content": "Constructing a physically realistic and accurately scaled simulated 3D world is crucial for the training and evaluation of embodied intelligence tasks. The diversity, realism, low cost accessibility and affordability of 3D data assets are critical for achieving generalization and scalability in embodied AI. However, most current embodied intelligence tasks still rely heavily on traditional 3D computer graphics assets manually created and annotated, which suffer from high production costs and limited realism. These limitations significantly hinder the scalability of data driven approaches. We present EmbodiedGen, a foundational platform for interactive 3D world generation. It enables the scalable generation of high-quality, controllable and photorealistic 3D assets with accurate physical properties and real-world scale in the Unified Robotics Description Format (URDF) at low cost. These assets can be directly imported into various physics simulation engines for fine-grained physical control, supporting downstream tasks in training and evaluation. EmbodiedGen is an easy-to-use, full-featured toolkit composed of six key modules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object Generation, Scene Generation and Layout Generation. EmbodiedGen generates diverse and interactive 3D worlds composed of generative 3D assets, leveraging generative AI to address the challenges of generalization and evaluation to the needs of embodied intelligence related research. Code is available at https://horizonrobotics.github.io/robot_lab/embodied_gen/index.html."
    },
    {
      "title": "DreamActor-H1：通过运动设计的扩散Transformer生成高保真人机产品演示视频 (原标题: DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers)",
      "link": "https://arxiv.org/abs/2506.10568",
      "pubDate": "Thu, 12 Jun 2025 06:58:23 GMT",
      "isoDate": "2025-06-12T06:58:23.000Z",
      "creator": "Lizhen Wang, Zhurong Xia, Tianshu Hu, Pengrui Wang, Pengfei Wang, Zerong Zheng, Ming Zhou",
      "summary": "## DreamActor-H1：高保真人机产品演示视频生成框架\n\n### 引言\n\n在电子商务和数字营销领域，生成高保真的人机产品演示视频对于有效展示产品至关重要。然而，现有的大多数框架存在以下挑战：\n\n*   **身份保留不足：** 无法同时保留人物和产品的身份。\n*   **空间关系理解缺失：** 缺乏对人与产品之间空间关系的理解，导致生成的视频表现不真实，互动不自然。\n\n### DreamActor-H1 框架\n\n为了解决上述挑战，我们提出了一种基于**扩散Transformer (DiT)** 的框架——DreamActor-H1。该方法旨在同时保留人物身份和产品特定细节，并实现逼真自然的互动。\n\n#### 核心技术与创新点\n\n1.  **身份完整性保持：**\n    *   通过注入配对的人与产品参考信息，确保人物身份和产品细节（如标志、纹理）得到有效保留。\n    *   利用额外的**掩码交叉注意力机制**进一步增强身份的保持能力。\n\n2.  **精确运动指导：**\n    *   采用**3D人体网格模板**和**产品边界框**来提供精确的运动指导。\n    *   这使得手势能够与产品放置直观对齐，从而生成更自然的互动。\n\n3.  **3D一致性增强：**\n    *   使用**结构化文本编码**来整合类别级语义信息。\n    *   这有助于在帧间发生小幅度旋转变化时，增强视频的3D一致性。\n\n### 训练与性能\n\nDreamActor-H1 在一个混合数据集上进行训练，并采用了广泛的数据增强策略。实验结果表明，我们的方法在以下方面超越了现有最先进的技术：\n\n*   保持人物和产品身份的完整性。\n*   生成逼真自然的演示动作。\n\n### 应用领域\n\n本研究提出的框架在电子商务和数字营销领域具有重要的应用价值，能够帮助企业更有效地展示产品。",
      "shortSummary": "DreamActor-H1是一个基于扩散Transformer的框架，旨在生成高保真人机产品演示视频。它解决了现有方法在保留人物和产品身份以及理解空间关系方面的不足。通过注入参考信息、利用3D身体网格和产品边界框进行精确运动指导，并结合结构化文本编码，该框架能有效保持身份完整性并生成逼真动作。DreamActor-H1在电商和数字营销领域具有重要应用价值，其性能已超越现有最先进技术。",
      "translated_title": "DreamActor-H1：通过运动设计的扩散Transformer生成高保真人机产品演示视频",
      "images": [],
      "contentSource": "完整文章",
      "content": "In e-commerce and digital marketing, generating high-fidelity human-product demonstration videos is important for effective product presentation. However, most existing frameworks either fail to preserve the identities of both humans and products or lack an understanding of human-product spatial relationships, leading to unrealistic representations and unnatural interactions. To address these challenges, we propose a Diffusion Transformer (DiT)-based framework. Our method simultaneously preserves human identities and product-specific details, such as logos and textures, by injecting paired human-product reference information and utilizing an additional masked cross-attention mechanism. We employ a 3D body mesh template and product bounding boxes to provide precise motion guidance, enabling intuitive alignment of hand gestures with product placements. Additionally, structured text encoding is used to incorporate category-level semantics, enhancing 3D consistency during small rotational changes across frames. Trained on a hybrid dataset with extensive data augmentation strategies, our approach outperforms state-of-the-art techniques in maintaining the identity integrity of both humans and products and generating realistic demonstration motions. Project page: https://submit2025-dream.github.io/DreamActor-H1/."
    },
    {
      "title": "AniMaker：基于MCTS驱动片段生成的自动化多智能体动画故事创作 (原标题: AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation)",
      "link": "https://arxiv.org/abs/2506.10540",
      "pubDate": "Thu, 12 Jun 2025 06:06:21 GMT",
      "isoDate": "2025-06-12T06:06:21.000Z",
      "creator": "Haoyuan Shi, Yunxin Li, Xinyu Chen, Longyue Wang, Baotian Hu, Min Zhang",
      "summary": "# AniMaker：基于MCTS驱动片段生成的自动化多智能体动画故事创作\n\n## 引言\n\n尽管视频生成模型取得了快速进展，但生成跨越多个场景和角色的连贯故事视频仍然充满挑战。现有方法通常将预生成的关键帧僵硬地转换为固定长度的片段，导致叙事脱节和节奏问题。此外，视频生成模型固有的不稳定性意味着即使一个低质量的片段也可能显著降低整个输出动画的逻辑连贯性和视觉连续性。\n\n## AniMaker框架\n\n为了克服这些障碍，我们引入了AniMaker，一个多智能体框架，旨在实现高效的多候选片段生成和故事感知片段选择，从而仅从文本输入创建全局一致且故事连贯的动画。\n\n该框架围绕以下专业智能体构建：\n\n*   **导演智能体 (Director Agent)：** 负责故事板生成。\n*   **摄影智能体 (Photography Agent)：** 负责视频片段生成。\n*   **评审智能体 (Reviewer Agent)：** 负责评估。\n*   **后期制作智能体 (Post-Production Agent)：** 负责编辑和配音。\n\n## 关键技术组件\n\nAniMaker方法的核心是两个关键技术组件：\n\n1.  **MCTS-Gen (在摄影智能体中)：**\n    *   这是一种受蒙特卡洛树搜索 (MCTS) 启发的策略。\n    *   它能够智能地探索候选空间，生成高潜力的片段，同时优化资源使用。\n2.  **AniEval (在评审智能体中)：**\n    *   这是第一个专门为多镜头动画评估设计的框架。\n    *   它通过考虑每个片段与其前后片段的上下文关系，评估故事层面的连贯性、动作完成度以及动画特定特征。\n\n## 实验结果\n\n实验表明，AniMaker在VBench和我们提出的AniEval框架等流行指标上均取得了卓越的质量，同时显著提高了多候选生成的效率，推动了AI生成的动画故事创作更接近生产标准。\n\n## 研究领域\n\n*   多智能体系统 (cs.MA)\n*   计算机视觉与模式识别 (cs.CV)",
      "shortSummary": "AniMaker是一个多智能体框架，旨在通过文本输入自动化生成连贯的动画故事。它解决了现有视频生成中叙事脱节和质量不稳定的问题。框架包含导演、摄影、评审和后期制作智能体。核心技术包括MCTS-Gen（高效片段生成）和AniEval（多镜头动画评估）。实验证明，AniMaker在质量和效率上均优于现有方法，使AI动画更接近生产标准。",
      "translated_title": "AniMaker：基于MCTS驱动片段生成的自动化多智能体动画故事创作",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite rapid advancements in video generation models, generating coherent storytelling videos that span multiple scenes and characters remains challenging. Current methods often rigidly convert pre-generated keyframes into fixed-length clips, resulting in disjointed narratives and pacing issues. Furthermore, the inherent instability of video generation models means that even a single low-quality clip can significantly degrade the entire output animation's logical coherence and visual continuity. To overcome these obstacles, we introduce AniMaker, a multi-agent framework enabling efficient multi-candidate clip generation and storytelling-aware clip selection, thus creating globally consistent and story-coherent animation solely from text input. The framework is structured around specialized agents, including the Director Agent for storyboard generation, the Photography Agent for video clip generation, the Reviewer Agent for evaluation, and the Post-Production Agent for editing and voiceover. Central to AniMaker's approach are two key technical components: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search (MCTS)-inspired strategy that intelligently navigates the candidate space to generate high-potential clips while optimizing resource usage; and AniEval in Reviewer Agent, the first framework specifically designed for multi-shot animation evaluation, which assesses critical aspects such as story-level consistency, action completion, and animation-specific features by considering each clip in the context of its preceding and succeeding clips. Experiments demonstrate that AniMaker achieves superior quality as measured by popular metrics including VBench and our proposed AniEval framework, while significantly improving the efficiency of multi-candidate generation, pushing AI-generated storytelling animation closer to production standards."
    },
    {
      "title": "通过因果表示学习发现语言模型的层次化潜在能力 (原标题: Discovering Hierarchical Latent Capabilities of Language Models via Causal Representation Learning)",
      "link": "https://arxiv.org/abs/2506.10378",
      "pubDate": "Thu, 12 Jun 2025 02:07:42 GMT",
      "isoDate": "2025-06-12T02:07:42.000Z",
      "creator": "Jikai Jin, Vasilis Syrgkanis, Sham Kakade, Hanlin Zhang",
      "summary": "# 通过因果表示学习发现语言模型的层次化潜在能力\n\n## 引言\n对语言模型（LLM）能力的忠实评估对于获取可指导模型开发的实用见解至关重要。然而，在该领域进行严格的因果评估面临显著的方法学挑战，包括复杂的混杂效应和高昂的计算成本（如广泛的再训练）。\n\n## 提出的框架\n为了解决这些挑战，本文提出了一种**因果表示学习框架**。\n*   该框架将观察到的基准性能建模为少数潜在能力因素的线性变换。\n*   关键在于，这些潜在因素在适当控制基础模型作为共同混杂因素后，被识别为具有因果关联。\n\n## 应用与发现\n*   **数据集：** 该方法被应用于一个综合数据集，该数据集包含来自Open LLM Leaderboard的超过1500个模型，并在六个基准上进行了评估。\n*   **核心发现：** 识别出一个简洁的**三节点线性因果结构**，该结构能够可靠地解释观察到的性能变化。\n\n## 科学洞察\n*   对这一因果结构的进一步解释提供了超越简单数值排名的重要科学见解。\n*   具体而言，研究揭示了一个清晰的因果方向：\n    1.  从**通用问题解决能力**开始。\n    2.  通过**指令遵循熟练度**进一步发展。\n    3.  最终达到**数学推理能力**。\n\n## 结论\n研究结果强调了在评估过程中仔细控制基础模型变异的重要性，这是准确揭示模型潜在能力之间潜在因果关系的关键一步。",
      "shortSummary": "本文提出一种因果表示学习框架，旨在解决语言模型能力评估中存在的混杂效应和高计算成本问题。该框架将基准性能建模为潜在能力因素的线性变换，并在控制基础模型后识别其因果关系。研究对1500多个模型应用此方法，发现了一个三节点因果结构，揭示了从通用问题解决到指令遵循再到数学推理的清晰能力发展路径。结果强调了在评估中控制基础模型变异的重要性。",
      "translated_title": "通过因果表示学习发现语言模型的层次化潜在能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Faithful evaluation of language model capabilities is crucial for deriving actionable insights that can inform model development. However, rigorous causal evaluations in this domain face significant methodological challenges, including complex confounding effects and prohibitive computational costs associated with extensive retraining. To tackle these challenges, we propose a causal representation learning framework wherein observed benchmark performance is modeled as a linear transformation of a few latent capability factors. Crucially, these latent factors are identified as causally interrelated after appropriately controlling for the base model as a common confounder. Applying this approach to a comprehensive dataset encompassing over 1500 models evaluated across six benchmarks from the Open LLM Leaderboard, we identify a concise three-node linear causal structure that reliably explains the observed performance variations. Further interpretation of this causal structure provides substantial scientific insights beyond simple numerical rankings: specifically, we reveal a clear causal direction starting from general problem-solving capabilities, advancing through instruction-following proficiency, and culminating in mathematical reasoning ability. Our results underscore the essential role of carefully controlling base model variations during evaluation, a step critical to accurately uncovering the underlying causal relationships among latent model capabilities."
    }
  ],
  "lastUpdated": "2025-06-15T09:29:00.207Z"
}