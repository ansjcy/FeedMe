{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "VisionThink：通过强化学习实现的智能高效视觉语言模型 (原标题: VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2507.13348",
      "pubDate": "Thu, 17 Jul 2025 13:59:55 GMT",
      "isoDate": "2025-07-17T13:59:55.000Z",
      "creator": "Senqiao Yang, Junyi Li, Xin Lai, Bei Yu, Hengshuang Zhao, Jiaya Jia",
      "summary": "## VisionThink：智能高效视觉语言模型\n\n### 背景与挑战\n\n近期视觉语言模型（VLM）的性能提升通常依赖于增加视觉令牌的数量，这些令牌通常比文本令牌长得多。然而，研究发现，在大多数实际场景中，并不需要如此大量的视觉令牌。尽管在少数OCR（光学字符识别）相关任务中，减少视觉令牌会导致性能显著下降，但在大多数其他通用VQA（视觉问答）任务中，即使分辨率降至1/4，模型仍能保持准确性。\n\n### VisionThink 提出的新范式\n\n针对上述问题，本文提出了 **VisionThink**，一种用于视觉令牌压缩的新范式，旨在动态处理不同分辨率的样本：\n\n*   **动态分辨率处理**：VisionThink 从下采样图像开始处理。\n*   **智能决策**：模型会智能判断下采样图像是否足以解决当前问题。\n*   **按需请求高分辨率**：如果下采样图像不足，模型会输出一个特殊令牌，请求更高分辨率的图像。\n\n### 优势与创新\n\n与现有通过固定剪枝比例或阈值压缩令牌的高效VLM方法不同，VisionThink 能够根据具体情况自主决定是否压缩令牌，从而带来以下优势：\n\n*   **精细视觉理解**：在OCR相关任务上展现出强大的精细视觉理解能力。\n*   **资源节约**：在较简单的任务上显著节省视觉令牌。\n\n### 强化学习与实现\n\nVisionThink 采用了强化学习（RL）技术，并提出了 **LLM-as-Judge** 策略，成功将强化学习应用于通用VQA任务。此外，模型精心设计了奖励函数和惩罚机制，以实现稳定且合理的图像大小调整调用比例。\n\n### 实验结果与可用性\n\n广泛的实验证明了 VisionThink 方法的优越性、效率和有效性。项目的代码和模型已开源。",
      "shortSummary": "VisionThink 是一种智能高效的视觉语言模型，旨在解决现有VLM过度使用视觉令牌的问题。它通过强化学习，动态判断并按需调整图像分辨率，从下采样图像开始，并在必要时请求高分辨率。这种方法在OCR任务上表现出色，同时在简单任务上显著节省视觉令牌，实现了性能与效率的平衡。模型利用“LLM-as-Judge”策略和精心设计的奖励机制来优化图像分辨率调用。",
      "translated_title": "VisionThink：通过强化学习实现的智能高效视觉语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens. However, we observe that most real-world scenarios do not require such an extensive number of visual tokens. While the performance drops significantly in a small subset of OCR-related tasks, models still perform accurately in most other general VQA tasks with only 1/4 resolution. Therefore, we propose to dynamically process distinct samples with different resolutions, and present a new paradigm for visual token compression, namely, VisionThink. It starts with a downsampled image and smartly decides whether it is sufficient for problem solving. Otherwise, the model could output a special token to request the higher-resolution image. Compared to existing Efficient VLM methods that compress tokens using fixed pruning ratios or thresholds, VisionThink autonomously decides whether to compress tokens case by case. As a result, it demonstrates strong fine-grained visual understanding capability on OCR-related tasks, and meanwhile saves substantial visual tokens on simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge strategy to successfully apply RL to general VQA tasks. Moreover, we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio. Extensive experiments demonstrate the superiority, efficiency, and effectiveness of our method. Our code is available at https://github.com/dvlab-research/VisionThink."
    },
    {
      "title": "π^3: 可扩展的置换等变视觉几何学习 (原标题: π^3: Scalable Permutation-Equivariant Visual Geometry Learning)",
      "link": "https://arxiv.org/abs/2507.13347",
      "pubDate": "Thu, 17 Jul 2025 13:59:53 GMT",
      "isoDate": "2025-07-17T13:59:53.000Z",
      "creator": "Yifan Wang, Jianjun Zhou, Haoyi Zhu, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Jiangmiao Pang, Chunhua Shen, Tong He",
      "summary": "## π^3: 视觉几何重建的新范式\n\n### 引言\n\n传统的视觉几何重建方法通常依赖于一个固定的参考视图。这种归纳偏置可能导致模型在参考视图不理想时出现不稳定性和失败。\n\n### π^3 模型介绍\n\n我们引入了 **π^3**，一个前馈神经网络，它为视觉几何重建提供了一种新颖的方法，彻底打破了对传统固定参考视图的依赖。\n\n*   **核心设计：** π^3 采用完全置换等变（permutation-equivariant）的架构。\n*   **预测能力：**\n    *   预测仿射不变的相机姿态（affine-invariant camera poses）。\n    *   预测尺度不变的局部点图（scale-invariant local point maps）。\n*   **关键特点：** 无需任何参考帧，使其设计固有的无偏置。\n\n### 模型优势\n\nπ^3 的设计带来了显著的优势：\n\n*   **鲁棒性：** 对输入顺序具有固有的鲁棒性。\n*   **可扩展性：** 具有高度可扩展性。\n*   **简洁性：** 方法简单且无偏置。\n*   **性能：** 在广泛的任务中实现了最先进的性能。\n\n### 应用领域\n\nπ^3 在以下任务中表现出色：\n\n*   相机姿态估计（camera pose estimation）\n*   单目/视频深度估计（monocular/video depth estimation）\n*   密集点图重建（dense point map reconstruction）\n\n### 可用性\n\n模型的代码和预训练模型已公开提供。",
      "shortSummary": "π^3 是一种创新的前馈神经网络，通过采用完全置换等变架构，彻底改变了视觉几何重建。它摆脱了对固定参考视图的依赖，能够预测仿射不变的相机姿态和尺度不变的局部点图。π^3 对输入顺序具有固有鲁棒性，高度可扩展，并在相机姿态估计、深度估计和密集点图重建等任务中实现了最先进的性能。代码和模型已公开。",
      "translated_title": "π^3: 可扩展的置换等变视觉几何学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce pi^3, a feed-forward neural network that offers a novel approach to visual geometry reconstruction, breaking the reliance on a conventional fixed reference view. Previous methods often anchor their reconstructions to a designated viewpoint, an inductive bias that can lead to instability and failures if the reference is suboptimal. In contrast, pi^3 employs a fully permutation-equivariant architecture to predict affine-invariant camera poses and scale-invariant local point maps without any reference frames. This design makes our model inherently robust to input ordering and highly scalable. These advantages enable our simple and bias-free approach to achieve state-of-the-art performance on a wide range of tasks, including camera pose estimation, monocular/video depth estimation, and dense point map reconstruction. Code and models are publicly available."
    },
    {
      "title": "Diffuman4D: 基于时空扩散模型从稀疏视角视频合成4D一致性人体视图 (原标题: Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models)",
      "link": "https://arxiv.org/abs/2507.13344",
      "pubDate": "Thu, 17 Jul 2025 13:59:17 GMT",
      "isoDate": "2025-07-17T13:59:17.000Z",
      "creator": "Yudong Jin, Sida Peng, Xuan Wang, Tao Xie, Zhen Xu, Yifan Yang, Yujun Shen, Hujun Bao, Xiaowei Zhou",
      "summary": "### Diffuman4D：基于时空扩散模型从稀疏视角视频合成4D一致性人体视图\n\n本文旨在解决从稀疏视角视频输入进行高保真人体视图合成的挑战。现有方法通过利用4D扩散模型生成新视角的视频来解决观测不足的问题，但这些模型生成的视频通常缺乏时空一致性，从而降低了视图合成的质量。\n\n#### 提出的方法：滑动迭代去噪过程\n\n为了增强4D扩散模型的时空一致性，本文提出了一种新颖的滑动迭代去噪过程。其核心机制如下：\n\n1.  **潜在网格定义**：定义一个潜在网格（latent grid），其中每个潜在（latent）编码特定视角和时间戳的图像、相机姿态和人体姿态信息。\n2.  **交替去噪**：使用滑动窗口，沿着空间和时间维度交替地对潜在网格进行去噪。\n3.  **视频解码**：最后，从相应的去噪潜在中解码出目标视角的视频。\n\n#### 方法优势\n\n*   **增强4D一致性**：通过迭代滑动，信息在潜在网格中充分流动，使扩散模型获得大的感受野，从而显著增强了输出的4D一致性。\n*   **内存效率**：同时保持了GPU内存消耗在可承受范围内。\n\n#### 实验结果\n\n在DNA-Rendering和ActorsHQ数据集上的实验表明，本文提出的方法能够合成高质量且一致的新视角视频，并显著优于现有方法。\n\n更多交互式演示和视频结果可在项目页面查看。",
      "shortSummary": "Diffuman4D提出了一种新颖的滑动迭代去噪过程，以解决从稀疏视角视频合成人体视图时，现有4D扩散模型缺乏时空一致性的问题。该方法通过在潜在网格中交替进行空间和时间去噪，增强了4D一致性并优化了内存使用。实验证明，Diffuman4D在DNA-Rendering和ActorsHQ数据集上能合成高质量且一致的新视角视频，显著优于现有方法。",
      "translated_title": "Diffuman4D: 基于时空扩散模型从稀疏视角视频合成4D一致性人体视图",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper addresses the challenge of high-fidelity view synthesis of humans with sparse-view videos as input. Previous methods solve the issue of insufficient observation by leveraging 4D diffusion models to generate videos at novel viewpoints. However, the generated videos from these models often lack spatio-temporal consistency, thus degrading view synthesis quality. In this paper, we propose a novel sliding iterative denoising process to enhance the spatio-temporal consistency of the 4D diffusion model. Specifically, we define a latent grid in which each latent encodes the image, camera pose, and human pose for a certain viewpoint and timestamp, then alternately denoising the latent grid along spatial and temporal dimensions with a sliding window, and finally decode the videos at target viewpoints from the corresponding denoised latents. Through the iterative sliding, information flows sufficiently across the latent grid, allowing the diffusion model to obtain a large receptive field and thus enhance the 4D consistency of the output, while making the GPU memory consumption affordable. The experiments on the DNA-Rendering and ActorsHQ datasets demonstrate that our method is able to synthesize high-quality and consistent novel-view videos and significantly outperforms the existing approaches. See our project page for interactive demos and video results: https://diffuman4d.github.io/ ."
    },
    {
      "title": "大型语言模型上下文工程综述 (原标题: A Survey of Context Engineering for Large Language Models)",
      "link": "https://arxiv.org/abs/2507.13334",
      "pubDate": "Thu, 17 Jul 2025 13:50:36 GMT",
      "isoDate": "2025-07-17T13:50:36.000Z",
      "creator": "Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Baolong Bi, Yujun Cai, Jiazhi Liu, Mingyu Li, Zhong-Zhi Li, Duzhen Zhang, Chenlin Zhou, Jiayi Mao, Tianze Xia, Jiafeng Guo, Shenghua Liu",
      "summary": "# 大型语言模型上下文工程综述\n\n本综述深入探讨了“上下文工程”（Context Engineering）这一新兴学科，它超越了简单的提示设计，旨在系统性地优化提供给大型语言模型（LLMs）的输入信息负载，从而根本性地决定LLMs在推理时的性能。\n\n## 核心概念与分类\n\n文章提出了一个全面的分类体系，将上下文工程分解为基础组件和集成这些组件的复杂系统实现。\n\n### 基础组件\n\n1.  **上下文检索与生成（Context Retrieval and Generation）**：\n    *   涉及如何从大量信息中有效地获取相关上下文，以及如何根据需要生成新的上下文信息。\n2.  **上下文处理（Context Processing）**：\n    *   关注对检索或生成的上下文进行预处理、过滤、压缩、重排等操作，以使其更适合LLM的输入格式和理解能力。\n3.  **上下文管理（Context Management）**：\n    *   涵盖了对上下文生命周期、版本控制、多轮对话中的上下文维护等方面的管理策略。\n\n### 系统实现\n\n这些基础组件被巧妙地集成到以下复杂系统中，以提升LLMs的能力：\n\n1.  **检索增强生成（Retrieval-Augmented Generation, RAG）**：\n    *   通过将外部知识库的检索能力与LLM的生成能力相结合，显著提高了模型输出的准确性和信息量。\n2.  **记忆系统与工具集成推理（Memory Systems and Tool-Integrated Reasoning）**：\n    *   记忆系统使LLM能够存储和回顾长期或短期信息，而工具集成推理则允许LLM调用外部工具（如计算器、API等）来辅助解决复杂问题，扩展其能力边界。\n3.  **多智能体系统（Multi-Agent Systems）**：\n    *   构建由多个LLM或其他AI智能体组成的协作系统，每个智能体可能专注于特定任务或角色，通过交互和协作共同完成复杂目标。\n\n## 研究方法与发现\n\n本综述通过对超过1300篇研究论文进行系统性分析，不仅为该领域建立了技术路线图，还揭示了一个关键的研究空白：\n\n### 关键研究空白：能力不对称性\n\n尽管当前的LLMs在先进上下文工程的增强下，在理解复杂上下文方面表现出卓越的能力，但它们在生成同样复杂、长篇幅的输出方面却存在明显的局限性。这种理解能力与生成能力之间的显著不对称性是未来研究的当务之急。\n\n## 总结与展望\n\n本综述为推进上下文感知型人工智能的研究人员和工程师提供了一个统一的框架。解决LLM在生成复杂长篇输出方面的局限性，将是未来研究的重点。",
      "shortSummary": "本综述介绍了“上下文工程”，一个旨在系统优化大型语言模型（LLMs）输入信息的正式学科。它详细阐述了上下文检索、处理、管理等基础组件，以及检索增强生成（RAG）、记忆系统和多智能体系统等高级实现。通过分析1300多篇论文，综述揭示了LLM在理解复杂上下文方面表现出色，但在生成同样复杂的长篇输出方面存在显著局限性，并指出这是未来研究的关键方向。该综述为上下文感知型AI提供了统一框架。",
      "translated_title": "大型语言模型上下文工程综述",
      "images": [],
      "contentSource": "完整文章",
      "content": "The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1300 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI."
    },
    {
      "title": "模仿游戏：图灵机模仿器是长度泛化推理器 (原标题: The Imitation Game: Turing Machine Imitator is Length Generalizable Reasoner)",
      "link": "https://arxiv.org/abs/2507.13332",
      "pubDate": "Thu, 17 Jul 2025 13:50:07 GMT",
      "isoDate": "2025-07-17T13:50:07.000Z",
      "creator": "Zhouqi Hua, Wenwei Zhang, Chengqi Lyu, Yuzhe Gu, Songyang Gao, Kuikun Liu, Kai Chen",
      "summary": "## 图灵机模仿学习（TAIL）：提升大型语言模型长度泛化能力的新范式\n\n### 引言\n\n*   **核心挑战：** 基于Transformer的大型语言模型（LLM）在长度泛化方面面临严峻挑战，即它们难以解决比训练时所见序列更长的问题。\n*   **现有方法局限：** 当前的数据驱动方法主要集中于算术运算和符号操作，但这些方法通常是任务特定的，且整体性能有限，无法提供通用解决方案。\n\n### TAIL方法概述\n\n*   **研究视角：** 本文旨在寻求更通用的解决方案，将重点放在可计算的推理问题上，即那些可以通过算法解决，从而也能被图灵机解决的问题。\n*   **核心提案：** 提出了一种名为“图灵机模仿学习”（Turing MAchine Imitation Learning, TAIL）的新方法，旨在显著提升LLM的长度泛化能力。\n*   **数据合成机制：** TAIL通过计算机程序合成“思维链”（Chain-of-Thoughts, CoT）数据，这些数据精确模仿了图灵机的执行过程。具体机制包括：\n    *   **线性扩展推理步骤：** 将推理步骤线性地扩展为原子状态，这有助于缓解模型学习“捷径”（shortcut learning）的问题。\n    *   **显式内存获取机制：** 引入显式内存获取机制，以降低在基本操作中进行动态和长距离数据访问的难度。\n\n### 实验验证与结果\n\n*   **数据集构建：** 为验证TAIL的可靠性和普适性，研究团队构建了一个具有挑战性的合成数据集，该数据集涵盖了8类算法和18个具体任务。\n*   **性能表现：** 实验结果显示，TAIL在不依赖额外复杂技巧的情况下，仅使用合成数据就显著提升了Qwen2.5-7B模型在各种任务上的长度泛化能力和整体性能。\n*   **超越现有方法：** TAIL的表现超越了先前的多种方法以及DeepSeek-R1模型。\n\n### 关键发现\n\n*   实验揭示，对于TAIL实现长度泛化而言，图灵机中的**关键概念**（而非其思维风格）是不可或缺的。\n*   模型在注意力层中表现出与图灵机特性高度一致的读写行为，这进一步验证了TAIL方法的有效性。\n\n### 未来展望\n\n*   这项工作为未来从合成数据中学习LLM推理能力提供了一个充满前景的方向。\n\n### 相关信息\n\n*   **主题：** 计算与语言 (cs.CL)\n*   **引用：** arXiv:2507.13332 [cs.CL]",
      "shortSummary": "大型语言模型（LLM）在长度泛化方面面临挑战。本文提出“图灵机模仿学习”（TAIL），通过合成模仿图灵机执行过程的思维链（CoT）数据来解决此问题。TAIL通过将推理步骤原子化并引入显式内存机制，显著提升了Qwen2.5-7B等模型的长度泛化能力和性能，超越现有方法。研究表明，图灵机的核心概念对泛化至关重要。这项工作为LLM从合成数据中学习推理提供了新方向。",
      "translated_title": "模仿游戏：图灵机模仿器是长度泛化推理器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Length generalization, the ability to solve problems of longer sequences than those observed during training, poses a core challenge of Transformer-based large language models (LLM). Although existing studies have predominantly focused on data-driven approaches for arithmetic operations and symbolic manipulation tasks, these approaches tend to be task-specific with limited overall performance. To pursue a more general solution, this paper focuses on a broader case of reasoning problems that are computable, i.e., problems that algorithms can solve, thus can be solved by the Turing Machine. From this perspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to improve the length generalization ability of LLMs. TAIL synthesizes chain-of-thoughts (CoT) data that imitate the execution process of a Turing Machine by computer programs, which linearly expands the reasoning steps into atomic states to alleviate shortcut learning and explicit memory fetch mechanism to reduce the difficulties of dynamic and long-range data access in elementary operations. To validate the reliability and universality of TAIL, we construct a challenging synthetic dataset covering 8 classes of algorithms and 18 tasks. Without bells and whistles, TAIL significantly improves the length generalization ability as well as the performance of Qwen2.5-7B on various tasks using only synthetic data, surpassing previous methods and DeepSeek-R1. The experimental results reveal that the key concepts in the Turing Machine, instead of the thinking styles, are indispensable for TAIL for length generalization, through which the model exhibits read-and-write behaviors consistent with the properties of the Turing Machine in their attention layers. This work provides a promising direction for future research in the learning of LLM reasoning from synthetic data."
    },
    {
      "title": "AbGen：评估大型语言模型在科学研究中消融研究设计与评估的能力 (原标题: AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research)",
      "link": "https://arxiv.org/abs/2507.13300",
      "pubDate": "Thu, 17 Jul 2025 13:09:22 GMT",
      "isoDate": "2025-07-17T13:09:22.000Z",
      "creator": "Yilun Zhao, Weiyuan Chen, Zhijian Xu, Manasi Patwardhan, Yixin Liu, Chengye Wang, Lovekesh Vig, Arman Cohan",
      "summary": "### AbGen：评估大型语言模型在消融研究设计与评估中的能力\n\n本文介绍了 **AbGen**，这是首个旨在评估大型语言模型（LLMs）在科学研究中设计消融研究能力的基准。\n\n#### AbGen 基准概述\n*   **目的**：评估LLMs在为科学研究设计消融研究方面的能力。\n*   **构成**：包含1,500个由专家标注的示例，这些示例来源于807篇自然语言处理（NLP）领域的论文。\n*   **任务设定**：LLMs的任务是根据给定的研究背景，为一个指定的模块或过程生成详细的消融研究设计。\n\n#### LLM 性能评估\n*   **评估对象**：对DeepSeek-R1-0528和o4-mini等领先LLMs进行了评估。\n*   **发现**：这些模型在消融研究设计的重要性、忠实性和合理性方面，与人类专家之间存在显著的性能差距。\n\n#### 自动化评估方法的挑战\n*   **问题**：当前常用的自动化评估方法对于此任务并不可靠。\n*   **表现**：与人类评估相比，自动化方法显示出显著的不一致性。\n\n#### AbGen-Eval：元评估基准\n*   **开发目的**：为了更好地探究自动化评估的可靠性，研究人员开发了 **AbGen-Eval**。\n*   **功能**：AbGen-Eval是一个元评估基准，旨在评估常用自动化评估系统在衡量LLM在此任务上表现时的可靠性。\n*   **研究方向**：研究人员在AbGen-Eval上调查了各种“LLM即评判者”（LLM-as-Judge）系统。\n*   **未来展望**：这项工作为未来开发更有效、更可靠的、基于LLM的复杂科学任务评估系统提供了见解。\n\n#### 其他信息\n*   **会议**：ACL 2025\n*   **主题**：计算与语言 (cs.CL); 人工智能 (cs.AI)",
      "shortSummary": "AbGen是一个新基准，用于评估大型语言模型（LLMs）在科学研究中设计消融研究的能力，包含1,500个专家标注示例。评估显示，领先LLMs在设计消融研究方面与人类专家存在显著差距。此外，现有自动化评估方法被发现不可靠。为解决此问题，研究人员开发了AbGen-Eval元评估基准，旨在评估自动化评估系统的可靠性，为未来开发更有效的LLM评估系统提供指导。",
      "translated_title": "AbGen：评估大型语言模型在科学研究中消融研究设计与评估的能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce AbGen, the first benchmark designed to evaluate the capabilities of LLMs in designing ablation studies for scientific research. AbGen consists of 1,500 expert-annotated examples derived from 807 NLP papers. In this benchmark, LLMs are tasked with generating detailed ablation study designs for a specified module or process based on the given research context. Our evaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a significant performance gap between these models and human experts in terms of the importance, faithfulness, and soundness of the ablation study designs. Moreover, we demonstrate that current automated evaluation methods are not reliable for our task, as they show a significant discrepancy when compared to human assessment. To better investigate this, we develop AbGen-Eval, a meta-evaluation benchmark designed to assess the reliability of commonly used automated evaluation systems in measuring LLM performance on our task. We investigate various LLM-as-Judge systems on AbGen-Eval, providing insights for future research on developing more effective and reliable LLM-based evaluation systems for complex scientific tasks."
    },
    {
      "title": "Voxtral",
      "link": "https://arxiv.org/abs/2507.13264",
      "pubDate": "Thu, 17 Jul 2025 12:17:37 GMT",
      "isoDate": "2025-07-17T12:17:37.000Z",
      "creator": "Alexander H. Liu, Andy Ehrenberg, Andy Lo, Clément Denoix, Corentin Barreau, Guillaume Lample, Jean-Malo Delignon, Khyathi Raghavi Chandu, Patrick von Platen, Pavankumar Reddy Muddireddy, Sanchit Gandhi, Soham Ghosh, Srijan Mishra, Thomas Foubert, Abhinav Rastogi, Adam Yang, Albert Q. Jiang, Alexandre Sablayrolles, Amélie Héliou, Amélie Martin, Anmol Agarwal, Antoine Roux, Arthur Darcet, Arthur Mensch, Baptiste Bout, Baptiste Rozière, Baudouin De Monicault, Chris Bamford, Christian Wallenwein, Christophe Renaudin, Clémence Lanfranchi, Darius Dabert, Devendra Singh Chaplot, Devon Mizelle, Diego de las Casas, Elliot Chane-Sane, Emilien Fugier, Emma Bou Hanna, Gabrielle Berrada, Gauthier Delerce, Gauthier Guinet, Georgii Novikov, Guillaume Martin, Himanshu Jaju, Jan Ludziejewski, Jason Rute, Jean-Hadrien Chabran, Jessica Chudnovsky, Joachim Studnia, Joep Barmentlo, Jonas Amar, Josselin Somerville Roberts, Julien Denize, Karan Saxena, Karmesh Yadav, Kartik Khandelwal, Kush Jain, Lélio Renard Lavaud, Léonard Blier, Lingxiao Zhao, Louis Martin, Lucile Saulnier, Luyu Gao, Marie Pellat, Mathilde Guillaumin, Mathis Felardos, Matthieu Dinot, Maxime Darrin, Maximilian Augustin, Mickaël Seznec, Neha Gupta, Nikhil Raghuraman, Olivier Duchenne, Patricia Wang, Patryk Saffer, Paul Jacob, Paul Wambergue, Paula Kurylowicz, Philomène Chagniot, Pierre Stock, Pravesh Agrawal, Rémi Delacourt, Romain Sauvestre, Roman Soletskyi, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Shashwat Dalal, Siddharth Gandhi, Sumukh Aithal, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Robert, Thomas Wang, Timothée Lacroix, Tom Bewley, Valeriia Nemychnikova, Victor Paltz, Virgile Richard, Wen-Ding Li, William Marshall, Xuanyu Zhang, Yihan Wan, Yunhao Tang",
      "summary": "## Voxtral：多模态音频聊天模型\n\n本文介绍了 **Voxtral Mini** 和 **Voxtral Small**，两款创新的多模态音频聊天模型。这些模型旨在同时理解口语音频和文本文档，并在多项音频基准测试中取得了最先进的性能，同时保持了强大的文本处理能力。\n\n### 核心特性与优势\n\n*   **多模态理解**：Voxtral 模型能够同时处理和理解语音音频及文本信息，实现更全面的交互。\n*   **卓越的音频性能**：在各种音频基准测试中表现出色，达到当前领先水平。\n*   **强大的文本能力**：在增强音频理解的同时，模型依然保留了优秀的文本处理能力。\n*   **本地运行能力**：**Voxtral Small** 模型足够轻量，可以在本地设备上运行，同时其性能超越了许多闭源模型。\n*   **长上下文窗口**：模型具备 32K 的上下文窗口，使其能够处理长达 40 分钟的音频文件，并支持长时间的多轮对话。\n\n### 主要贡献\n\n*   **模型发布**：研究团队以 Apache 2.0 许可发布了 Voxtral Mini 和 Voxtral Small 两款模型，促进了社区的开放研究和应用。\n*   **新基准贡献**：本文还贡献了三个新的基准测试，用于评估语音理解模型在知识和常识问答方面的能力，为未来的研究提供了新的评估工具。\n\n### 其他信息\n\n*   **作者**：该研究由 Alexander H. Liu、Andy Ehrenberg 等众多研究人员共同完成。\n*   **页数**：文章共 17 页。\n*   **主题**：涉及声音（cs.SD）、人工智能（cs.AI）以及音频和语音处理（eess.AS）领域。",
      "shortSummary": "Voxtral Mini和Voxtral Small是多模态音频聊天模型，能理解语音和文本。它们在音频基准测试中表现卓越，同时保持强大的文本能力。Voxtral Small可本地运行，支持长达40分钟的音频和多轮对话。研究还贡献了三个语音理解基准。两款模型均以Apache 2.0许可发布。",
      "translated_title": "Voxtral",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Voxtral Mini and Voxtral Small, two multimodal audio chat models. Voxtral is trained to comprehend both spoken audio and text documents, achieving state-of-the-art performance across a diverse range of audio benchmarks, while preserving strong text capabilities. Voxtral Small outperforms a number of closed-source models, while being small enough to run locally. A 32K context window enables the model to handle audio files up to 40 minutes in duration and long multi-turn conversations. We also contribute three benchmarks for evaluating speech understanding models on knowledge and trivia. Both Voxtral models are released under Apache 2.0 license."
    },
    {
      "title": "多模态大语言模型安全自动引导 (原标题: Automating Steering for Safe Multimodal Large Language Models)",
      "link": "https://arxiv.org/abs/2507.13255",
      "pubDate": "Thu, 17 Jul 2025 12:04:55 GMT",
      "isoDate": "2025-07-17T12:04:55.000Z",
      "creator": "Lyucheng Wu, Mengru Wang, Ziwen Xu, Tri Cao, Nay Oo, Bryan Hooi, Shumin Deng",
      "summary": "# 多模态大语言模型安全自动引导技术：AutoSteer\n\n## 引言\n近期，多模态大语言模型（MLLMs）在跨模态推理能力方面取得了显著进展，但同时也引发了新的安全担忧，尤其是在面对对抗性多模态输入时。为了在推理阶段提升MLLMs的安全性，研究人员引入了一种名为AutoSteer的模块化、自适应的推理时干预技术。该技术的一大优势在于，它无需对底层模型进行任何微调。\n\n## AutoSteer核心组件\nAutoSteer技术由以下三个核心组件构成：\n\n1.  **安全意识分数（Safety Awareness Score, SAS）**：这是一种新颖的评分机制，能够自动识别模型内部层之间与安全性最相关的区别。\n2.  **自适应安全探测器（Adaptive Safety Prober）**：该探测器经过训练，能够根据模型的中间表示来估计产生有害输出的可能性。\n3.  **轻量级拒绝头（Lightweight Refusal Head）**：当检测到安全风险时，该组件会选择性地进行干预，以调节模型的生成行为。\n\nAutoSteer的独特之处在于它无需对现有MLLM进行任何微调，即可实现安全增强。\n\n## 实验与结果\n研究人员在LLaVA-OV和Chameleon模型上，针对多种安全关键基准进行了实验，以评估AutoSteer的性能。实验结果表明：\n\n*   AutoSteer显著降低了文本、视觉和跨模态威胁的攻击成功率（Attack Success Rate, ASR）。\n*   在降低ASR的同时，AutoSteer能够保持模型的通用能力。\n\n## 结论\n这些发现表明，AutoSteer是一个实用、可解释且有效的框架，可用于更安全地部署多模态AI系统。该技术目前仍在开发中。",
      "shortSummary": "AutoSteer是一种无需微调的推理时干预技术，旨在提升多模态大语言模型（MLLMs）的安全性。它通过引入安全意识分数、自适应安全探测器和轻量级拒绝头来识别并干预潜在的有害输出。实验证明，AutoSteer能显著降低文本、视觉及跨模态威胁的攻击成功率，同时保持模型的通用能力，为MLLMs的安全部署提供了实用且有效的解决方案。",
      "translated_title": "多模态大语言模型安全自动引导",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent progress in Multimodal Large Language Models (MLLMs) has unlocked powerful cross-modal reasoning abilities, but also raised new safety concerns, particularly when faced with adversarial multimodal inputs. To improve the safety of MLLMs during inference, we introduce a modular and adaptive inference-time intervention technology, AutoSteer, without requiring any fine-tuning of the underlying model. AutoSteer incorporates three core components: (1) a novel Safety Awareness Score (SAS) that automatically identifies the most safety-relevant distinctions among the model's internal layers; (2) an adaptive safety prober trained to estimate the likelihood of toxic outputs from intermediate representations; and (3) a lightweight Refusal Head that selectively intervenes to modulate generation when safety risks are detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical benchmarks demonstrate that AutoSteer significantly reduces the Attack Success Rate (ASR) for textual, visual, and cross-modal threats, while maintaining general abilities. These findings position AutoSteer as a practical, interpretable, and effective framework for safer deployment of multimodal AI systems."
    },
    {
      "title": "通过残差学习使现有稀疏自编码器掌握新的领域知识 (原标题: Teach Old SAEs New Domain Tricks with Boosting)",
      "link": "https://arxiv.org/abs/2507.12990",
      "pubDate": "Thu, 17 Jul 2025 06:57:49 GMT",
      "isoDate": "2025-07-17T06:57:49.000Z",
      "creator": "Nikita Koriagin, Yaroslav Aksenov, Daniil Laptev, Gleb Gerasimov, Nikita Balagansky, Daniil Gavrilov",
      "summary": "# 论文摘要：通过残差学习使现有稀疏自编码器掌握新的领域知识\n\n## 核心问题\n*   稀疏自编码器（Sparse Autoencoders, SAEs）已被证明是解释大型语言模型（Large Language Models, LLMs）内部表示的强大工具。\n*   然而，它们通常难以捕获在其原始训练语料库中不普遍的领域特定特征，导致在特定领域上的解释能力受限。\n\n## 提出的方法：残差学习\n*   本文引入了一种创新的残差学习方法，旨在解决SAEs的“特征盲区”，且无需对整个模型进行耗时且资源密集型的完全重新训练。\n*   **核心思想：** 训练一个**辅助稀疏自编码器（secondary SAE）**。\n    *   该辅助SAE专门用于建模**预训练主稀疏自编码器（pretrained primary SAE）**在**领域特定文本**上的**重建误差**。\n    *   通过这种方式，辅助SAE能够有效地捕获并学习主模型在处理特定领域数据时所遗漏的特征。\n*   **推理阶段：** 在推理时，将预训练主SAE和新训练的辅助SAE的输出相加，以获得更全面的特征表示。\n\n## 实验结果与优势\n*   **性能提升：** 实验证明，该方法在多个专业领域显著改善了LLM的交叉熵（cross-entropy）和解释方差（explained variance）指标。\n*   **效率：** 这种方法能够高效地将新的领域知识融入现有SAEs，而无需进行代价高昂的完全重训练。\n*   **通用性保持：** 在融入新领域知识的同时，该方法能够有效保持SAEs在通用任务上的原有性能，避免了“灾难性遗忘”问题。\n\n## 意义与展望\n*   这种残差学习方法使研究人员能够选择性地增强SAEs对特定感兴趣领域的解释能力。\n*   它为LLMs的靶向机械可解释性（targeted mechanistic interpretability）开辟了新的可能性，有助于更深入地理解LLM在特定应用场景下的内部工作机制。",
      "shortSummary": "稀疏自编码器（SAEs）在解释大型语言模型时，常难以捕获领域特定特征。本文提出一种残差学习方法：训练一个辅助SAE来建模预训练SAE在领域文本上的重建误差。通过结合两者的输出，该方法显著提升了LLM在专业领域的性能，并能高效地将新领域知识融入现有SAEs，无需完全重训练，从而增强了LLM的靶向可解释性。",
      "translated_title": "通过残差学习使现有稀疏自编码器掌握新的领域知识",
      "images": [],
      "contentSource": "完整文章",
      "content": "Sparse Autoencoders have emerged as powerful tools for interpreting the internal representations of Large Language Models, yet they often fail to capture domain-specific features not prevalent in their training corpora. This paper introduces a residual learning approach that addresses this feature blindness without requiring complete retraining. We propose training a secondary SAE specifically to model the reconstruction error of a pretrained SAE on domain-specific texts, effectively capturing features missed by the primary model. By summing the outputs of both models during inference, we demonstrate significant improvements in both LLM cross-entropy and explained variance metrics across multiple specialized domains. Our experiments show that this method efficiently incorporates new domain knowledge into existing SAEs while maintaining their performance on general tasks. This approach enables researchers to selectively enhance SAE interpretability for specific domains of interest, opening new possibilities for targeted mechanistic interpretability of LLMs."
    },
    {
      "title": "FantasyPortrait：使用表情增强型扩散Transformer增强多角色肖像动画 (原标题: FantasyPortrait: Enhancing Multi-Character Portrait Animation with Expression-Augmented Diffusion Transformers)",
      "link": "https://arxiv.org/abs/2507.12956",
      "pubDate": "Thu, 17 Jul 2025 05:50:43 GMT",
      "isoDate": "2025-07-17T05:50:43.000Z",
      "creator": "Qiang Wang, Mengchao Wang, Fan Jiang, Yaqi Fan, Yonggang Qi, Mu Xu",
      "summary": "## FantasyPortrait：使用表情增强型扩散Transformer增强多角色肖像动画\n\n本文提出了一种名为 **FantasyPortrait** 的框架，旨在解决从静态图像生成富有表现力的面部动画的挑战，尤其是在多角色场景中。\n\n### 当前挑战：\n*   **面部动画生成困难：** 现有方法依赖于显式几何先验（如面部标志点或3DMM），常导致交叉重演中的伪影，并且难以捕捉细微情感。\n*   **缺乏多角色支持：** 不同个体驱动特征之间常相互干扰，使得多角色动画难以实现。\n\n### FantasyPortrait 的创新解决方案：\nFantasyPortrait 是一个基于扩散Transformer的框架，能够为单角色和多角色场景生成高保真且情感丰富的动画。其核心创新包括：\n\n*   **表情增强学习策略：**\n    *   利用隐式表示来捕捉与身份无关的面部动态。\n    *   增强模型渲染细粒度情感的能力。\n*   **掩码交叉注意力机制（针对多角色控制）：**\n    *   确保独立但协调的表情生成。\n    *   有效防止特征干扰。\n\n### 对研究领域的贡献：\n为了推动该领域的研究，作者提出了：\n*   **Multi-Expr 数据集：** 专门设计用于多角色肖像动画的训练。\n*   **ExprBench 基准：** 专门设计用于多角色肖像动画的评估。\n\n### 实验结果：\n广泛的实验表明，FantasyPortrait 在定量指标和定性评估方面均显著优于现有最先进的方法，尤其在具有挑战性的交叉重演和多角色情境中表现出色。",
      "shortSummary": "FantasyPortrait 是一种基于扩散Transformer的框架，旨在解决从静态图像生成富有表现力的单/多角色面部动画的挑战。它引入了表情增强学习策略，利用隐式表示捕捉细微情感，并通过掩码交叉注意力机制实现独立协调的多角色表情生成，有效避免特征干扰。该方法还提出了Multi-Expr数据集和ExprBench基准。实验证明，FantasyPortrait在交叉重演和多角色动画方面显著优于现有技术。",
      "translated_title": "FantasyPortrait：使用表情增强型扩散Transformer增强多角色肖像动画",
      "images": [],
      "contentSource": "完整文章",
      "content": "Producing expressive facial animations from static images is a challenging task. Prior methods relying on explicit geometric priors (e.g., facial landmarks or 3DMM) often suffer from artifacts in cross reenactment and struggle to capture subtle emotions. Furthermore, existing approaches lack support for multi-character animation, as driving features from different individuals frequently interfere with one another, complicating the task. To address these challenges, we propose FantasyPortrait, a diffusion transformer based framework capable of generating high-fidelity and emotion-rich animations for both single- and multi-character scenarios. Our method introduces an expression-augmented learning strategy that utilizes implicit representations to capture identity-agnostic facial dynamics, enhancing the model's ability to render fine-grained emotions. For multi-character control, we design a masked cross-attention mechanism that ensures independent yet coordinated expression generation, effectively preventing feature interference. To advance research in this area, we propose the Multi-Expr dataset and ExprBench, which are specifically designed datasets and benchmarks for training and evaluating multi-character portrait animations. Extensive experiments demonstrate that FantasyPortrait significantly outperforms state-of-the-art methods in both quantitative metrics and qualitative evaluations, excelling particularly in challenging cross reenactment and multi-character contexts. Our project page is https://fantasy-amap.github.io/fantasy-portrait/."
    },
    {
      "title": "AnyCap 项目：一个用于可控全模态图像描述的统一框架、数据集和基准 (原标题: AnyCap Project: A Unified Framework, Dataset, and Benchmark for Controllable Omni-modal Captioning)",
      "link": "https://arxiv.org/abs/2507.12841",
      "pubDate": "Thu, 17 Jul 2025 03:04:05 GMT",
      "isoDate": "2025-07-17T03:04:05.000Z",
      "creator": "Yiming Ren, Zhiqiang Lin, Yu Li, Gao Meng, Weiyun Wang, Junjie Wang, Zicheng Lin, Jifeng Dai, Yujiu Yang, Wenhai Wang, Ruihang Chu",
      "summary": "# AnyCap 项目：可控全模态图像描述的统一解决方案\n\n可控图像描述对于精确的多模态对齐和指令遵循至关重要。然而，现有模型通常缺乏细粒度控制和可靠的评估协议。为解决这一差距，研究人员提出了 **AnyCap 项目**，这是一个集模型、数据集和评估于一体的综合解决方案。\n\n## AnyCapModel (ACM)\n\nAnyCapModel (ACM) 是一种轻量级的即插即用框架，旨在增强现有基础模型在全模态图像描述方面的可控性，而无需重新训练基础模型。ACM 通过重用基础模型的原始描述，并结合用户指令和模态特征，生成改进的描述。\n\n## AnyCapDataset (ACD)\n\n为了弥补可控多模态图像描述领域的数据稀缺问题，AnyCap 项目构建了 **AnyCapDataset (ACD)**。该数据集涵盖了三种模态、28种用户指令类型，并包含30万条高质量数据条目。\n\n## AnyCapEval\n\nAnyCap 项目进一步提出了 **AnyCapEval**，这是一个新的基准测试，通过解耦内容准确性和风格保真度，为可控图像描述提供了更可靠的评估指标。\n\n## 关键成果\n\n*   AnyCapModel (ACM) 在 AnyCapEval 上显著提升了各种基础模型的描述质量。\n*   值得注意的是，ACM-8B 将 GPT-4o 的内容得分提高了45%，风格得分提高了12%。\n*   ACM 还在 MIA-Bench 和 VidCapBench 等广泛使用的基准测试中取得了显著的性能提升。",
      "shortSummary": "AnyCap 项目提出了一个用于可控全模态图像描述的统一解决方案。它包括：AnyCapModel (ACM)，一个无需重训即可增强基础模型可控性的轻量级框架；AnyCapDataset (ACD)，一个包含30万条高质量数据的多模态数据集；以及AnyCapEval，一个解耦内容和风格以提供可靠评估的新基准。ACM显著提升了描述质量，例如将GPT-4o的内容得分提高45%，风格得分提高12%。",
      "translated_title": "AnyCap 项目：一个用于可控全模态图像描述的统一框架、数据集和基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Controllable captioning is essential for precise multimodal alignment and instruction following, yet existing models often lack fine-grained control and reliable evaluation protocols. To address this gap, we present the AnyCap Project, an integrated solution spanning model, dataset, and evaluation. We introduce AnyCapModel (ACM), a lightweight plug-and-play framework that enhances the controllability of existing foundation models for omni-modal captioning without retraining the base model. ACM reuses the original captions from base models while incorporating user instructions and modality features to generate improved captions. To remedy the data scarcity in controllable multimodal captioning, we build AnyCapDataset (ACD), covering three modalities, 28 user-instruction types, and 300\\,k high-quality data entries. We further propose AnyCapEval, a new benchmark that provides more reliable evaluation metrics for controllable captioning by decoupling content accuracy and stylistic fidelity. ACM markedly improves caption quality across a diverse set of base models on AnyCapEval. Notably, ACM-8B raises GPT-4o\\'s content scores by 45\\% and style scores by 12\\%, and it also achieves substantial gains on widely used benchmarks such as MIA-Bench and VidCapBench."
    },
    {
      "title": "FLEXITOKENS：面向演化语言模型的灵活分词 (原标题: FLEXITOKENS: Flexible Tokenization for Evolving Language Models)",
      "link": "https://arxiv.org/abs/2507.12720",
      "pubDate": "Wed, 16 Jul 2025 21:55:41 GMT",
      "isoDate": "2025-07-16T21:55:41.000Z",
      "creator": "Abraham Toluase Owodunni, Orevaoghene Ahia, Sachin Kumar",
      "summary": "## FLEXITOKENS：面向演化语言模型的灵活分词\n\n### 摘要\n\n本文介绍了FLEXITOKENS，一种旨在解决现有语言模型（LMs）在适应新数据分布时面临的挑战的创新方法。这些挑战主要源于其子词分词器的僵化性，导致在处理域外数据、未见语言或脚本时出现低效的分词和过度碎片化。\n\n### 核心问题\n\n*   **分词器僵化性**：传统的语言模型在适应新数据分布时面临困难，因为它们的子词分词器在微调过程中通常保持不变。\n*   **过度碎片化**：这种僵化性导致在处理域外数据、未见语言或脚本时，分词效率低下，出现“过度碎片化”现象。\n\n### 现有方法的局限性\n\n*   现有的无分词器方法（如字节级语言模型）通过学习边界预测器来工作，但它们通常使用辅助损失来强制在整个训练语料库中保持固定的压缩率，这引入了另一种形式的僵化。\n\n### FLEXITOKENS 解决方案\n\n*   **可学习的分词器**：FLEXITOKENS 提出了一种带有可学习分词器的字节级语言模型。\n*   **边界预测子模块**：模型包含一个子模块，该子模块学习预测输入字节序列之间的边界，并将其编码成可变长度的片段。\n*   **简化的训练目标**：FLEXITOKENS 的核心创新在于其简化的训练目标，这在适应过程中提供了显著更大的灵活性，克服了现有方法的僵化性。\n\n### 评估与成果\n\n*   **广泛评估**：FLEXITOKENS 在多个多语言基准、形态多样的任务和不同领域进行了评估。\n*   **减少过度碎片化**：实验结果表明，FLEXITOKENS 持续有效地减少了词元（token）的过度碎片化。\n*   **性能提升**：与子词分词器和其他基于梯度的分词器相比，FLEXITOKENS 在下游任务性能上实现了高达10%的改进。",
      "shortSummary": "语言模型因分词器僵化而难以适应新数据，导致过度碎片化。FLEXITOKENS提出一种灵活的、可学习的字节级分词器，采用简化的训练目标。该方法显著减少了词元过度碎片化，并在多语言和形态任务中将下游性能提升高达10%，增强了语言模型的适应性。",
      "translated_title": "FLEXITOKENS：面向演化语言模型的灵活分词",
      "images": [],
      "contentSource": "完整文章",
      "content": "Language models (LMs) are challenging to adapt to new data distributions by simple finetuning. This is due to the rigidity of their subword tokenizers, which typically remain unchanged during adaptation. This inflexibility often leads to inefficient tokenization, causing overfragmentation of out-of-distribution domains, unseen languages, or scripts. In this work, we develop byte-level LMs with learnable tokenizers to make tokenization adaptive. Our models include a submodule that learns to predict boundaries between the input byte sequence, encoding it into variable-length segments. Existing tokenizer-free methods train this boundary predictor using an auxiliary loss that enforces a fixed compression rate across the training corpus, introducing a new kind of rigidity. We propose FLEXITOKENS, a simplified training objective that enables significantly greater flexibility during adaptation. Evaluating across multiple multilingual benchmarks, morphologically diverse tasks, and domains, we demonstrate that FLEXITOKENS consistently reduces token over-fragmentation and achieves up to 10\\% improvements on downstream task performance compared to subword and other gradient-based tokenizers. Code and data for our experiments will be released at https://github.com/owos/flexitokens"
    },
    {
      "title": "MindJourney：利用世界模型进行空间推理的测试时扩展 (原标题: MindJourney: Test-Time Scaling with World Models for Spatial Reasoning)",
      "link": "https://arxiv.org/abs/2507.12508",
      "pubDate": "Wed, 16 Jul 2025 13:59:36 GMT",
      "isoDate": "2025-07-16T13:59:36.000Z",
      "creator": "Yuncong Yang, Jiageng Liu, Zheyuan Zhang, Siyuan Zhou, Reuben Tan, Jianwei Yang, Yilun Du, Chuang Gan",
      "summary": "## MindJourney：利用世界模型进行空间推理的测试时扩展\n\n### 核心问题\n\n*   **现有视觉-语言模型（VLMs）的局限性**：当前最先进的VLMs在3D空间推理方面表现不佳，尤其是在预测以自我为中心的运动后场景将如何变化的任务上。它们主要感知2D图像，缺乏对3D动态的内部模型。\n\n### 解决方案：MindJourney框架\n\n*   **框架概述**：MindJourney是一个测试时扩展框架，旨在通过将VLM与一个基于视频扩散的可控世界模型耦合，来弥补VLM在3D动态建模方面的缺失能力。\n\n*   **工作原理**：\n    *   VLM迭代地勾勒出一条简洁的相机轨迹。\n    *   世界模型在每一步合成相应的视图。\n    *   VLM随后对在交互式探索过程中收集到的多视图证据进行推理。\n\n### 主要成果与优势\n\n*   **无需微调**：MindJourney无需任何微调即可运行。\n*   **性能提升**：在代表性的空间推理基准SAT上，MindJourney平均实现了超过8%的性能提升。\n*   **简单即插即用**：该方法提供了一种简单、即插即用的途径，以实现鲁棒的3D推理。\n*   **超越现有方法**：MindJourney在测试时推理方面也优于通过强化学习训练的VLM，这表明了其利用世界模型进行测试时扩展的巨大潜力。",
      "shortSummary": "MindJourney是一个创新的测试时扩展框架，旨在解决视觉-语言模型（VLMs）在3D空间推理上的不足。它通过将VLM与基于视频扩散的世界模型结合，使VLM能够迭代规划相机轨迹并从世界模型合成的视图中进行推理。该方法无需微调，在SAT空间推理基准上实现了平均8%以上的性能提升，提供了一种简单、即插即用的3D推理解决方案，并优于通过强化学习训练的VLM。",
      "translated_title": "MindJourney：利用世界模型进行空间推理的测试时扩展",
      "images": [],
      "contentSource": "完整文章",
      "content": "Spatial reasoning in 3D space is central to human cognition and indispensable for embodied tasks such as navigation and manipulation. However, state-of-the-art vision-language models (VLMs) struggle frequently with tasks as simple as anticipating how a scene will look after an egocentric motion: they perceive 2D images but lack an internal model of 3D dynamics. We therefore propose MindJourney, a test-time scaling framework that grants a VLM with this missing capability by coupling it to a controllable world model based on video diffusion. The VLM iteratively sketches a concise camera trajectory, while the world model synthesizes the corresponding view at each step. The VLM then reasons over this multi-view evidence gathered during the interactive exploration. Without any fine-tuning, our MindJourney achieves over an average 8% performance boost on the representative spatial reasoning benchmark SAT, showing that pairing VLMs with world models for test-time scaling offers a simple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also improves upon the test-time inference VLMs trained through reinforcement learning, which demonstrates the potential of our method that utilizes world models for test-time scaling."
    },
    {
      "title": "RiemannLoRA：一个用于无歧义LoRA优化的统一黎曼框架 (原标题: RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization)",
      "link": "https://arxiv.org/abs/2507.12142",
      "pubDate": "Wed, 16 Jul 2025 07:17:12 GMT",
      "isoDate": "2025-07-16T07:17:12.000Z",
      "creator": "Vladimir Bogachev, Vladimir Aletov, Alexander Molozhavenko, Denis Bobkov, Vera Soboleva, Aibek Alanov, Maxim Rakhuba",
      "summary": "## RiemannLoRA：一个用于无歧义LoRA优化的统一黎曼框架\n\n### 背景与挑战\n\n*   **LoRA的广泛应用**：低秩适应（LoRA）已成为大型语言模型（LLM）参数高效微调的行业标准，显著降低了内存和计算需求。\n*   **现有问题**：尽管LoRA取得了成功，但仍面临挑战，主要包括：\n    *   寻找最优的初始化策略。\n    *   缓解低秩矩阵分解中可能出现的过参数化问题。\n\n### RiemannLoRA方法\n\n*   **核心思想**：本文提出了一种新颖的方法——RiemannLoRA，旨在在一个统一的框架内同时解决上述两个挑战。\n*   **流形视角**：该方法将一组固定秩的LoRA矩阵视为一个平滑流形。\n*   **解决过参数化**：通过将适配器（adapters）视为该流形上的元素，RiemannLoRA自然地消除了过参数化。\n*   **解决初始化**：通过确定沿流形损失下降最快的方向，该方法能够提供有效的初始化策略。\n\n### 实现细节\n\n*   **数值稳定性与计算效率**：在实现RiemannLoRA时，研究人员特别注重确保方法的数值稳定性和计算效率。\n*   **优化实践**：该实现采用了数值线性代数和黎曼优化领域的最佳实践。\n\n### 实验结果\n\n*   **性能提升**：在LLM和扩散模型架构上的实验结果表明，RiemannLoRA在以下方面均取得了显著改进：\n    *   **收敛速度**：比标准LoRA及其最先进的修改版本更快地收敛。\n    *   **最终性能**：在模型最终性能上也表现出一致的提升。\n\n### 相关主题领域\n\n*   机器学习 (cs.LG)\n*   计算与语言 (cs.CL)\n*   微分几何 (math.DG)\n*   数值分析 (math.NA)",
      "shortSummary": "RiemannLoRA提出一个统一的黎曼框架，旨在解决LoRA微调中的初始化和过参数化挑战。该方法将固定秩LoRA矩阵视为平滑流形，通过确定流形上损失下降最快的方向来优化适配器。实验证明，RiemannLoRA在LLM和扩散模型上显著提升了收敛速度和最终性能，优于现有LoRA方法，同时提高了数值稳定性和计算效率。",
      "translated_title": "RiemannLoRA：一个用于无歧义LoRA优化的统一黎曼框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "Low-Rank Adaptation (LoRA) has become a widely adopted standard for parameter-efficient fine-tuning of large language models (LLMs), significantly reducing memory and computational demands. However, challenges remain, including finding optimal initialization strategies or mitigating overparametrization in low-rank matrix factorization. In this work, we propose a novel approach that addresses both of the challenges simultaneously within a unified framework. Our method treats a set of fixed-rank LoRA matrices as a smooth manifold. Considering adapters as elements on this manifold removes overparametrization, while determining the direction of the fastest loss decrease along the manifold provides initialization. Special care is taken to obtain numerically stable and computationally efficient implementation of our method, using best practices from numerical linear algebra and Riemannian optimization. Experimental results on LLM and diffusion model architectures demonstrate that RiemannLoRA consistently improves both convergence speed and final performance over standard LoRA and its state-of-the-art modifications."
    },
    {
      "title": "EXAONE 4.0：整合非推理和推理模式的统一大型语言模型 (原标题: EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes)",
      "link": "https://arxiv.org/abs/2507.11407",
      "pubDate": "Tue, 15 Jul 2025 11:24:51 GMT",
      "isoDate": "2025-07-15T11:24:51.000Z",
      "creator": "LG AI Research, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Yemuk Choi, Kyubeen Han, Seokhee Hong, Junwon Hwang, Taewan Hwang, Joonwon Jang, Hyojin Jeon, Kijeong Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Euisoon Kim, Hyosang Kim, Jihoon Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Gwangho Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Young Min Paik, Yongmin Park, Youngyong Park, Sanghyun Seo, Sihoon Yang, Heuiyeen Yeen, Sihyuk Yi, Hyeongu Yun",
      "summary": "# EXAONE 4.0：统一大型语言模型技术报告概述\n\n本技术报告介绍了由LG AI Research开发的EXAONE 4.0大型语言模型。EXAONE 4.0旨在通过整合“非推理模式”和“推理模式”，同时实现EXAONE 3.5卓越的可用性以及EXAONE Deep先进的推理能力。\n\n## 核心创新与目标\n*   **模式整合**：EXAONE 4.0的核心创新在于其统一架构，将非推理能力与强大的推理能力相结合，以提供更全面的AI解决方案。\n*   **代理AI时代准备**：为迎接代理AI时代的到来，EXAONE 4.0融入了关键的智能体工具使用（agentic tool use）功能。\n\n## 主要特性\n*   **多语言能力扩展**：除了原有的英语和韩语，EXAONE 4.0的多语言能力已扩展至支持西班牙语。\n*   **模型尺寸与应用**：\n    *   **中型模型 (32B)**：该版本经过优化，旨在提供高性能表现。\n    *   **小型模型 (1.2B)**：该版本专为设备端（on-device）应用设计，适用于资源受限的环境。\n\n## 性能表现\nEXAONE 4.0在性能上展现出显著优势：\n*   **超越同类开源模型**：与同级别的开源模型相比，EXAONE 4.0展现出卓越的性能。\n*   **与前沿模型竞争**：即使面对前沿级（frontier-class）模型，EXAONE 4.0也保持了强大的竞争力。\n\n## 可用性与研究\nEXAONE 4.0模型系列已向公众开放，供研究目的使用。研究人员可以通过提供的链接轻松下载这些模型。\n\n## 报告信息\n*   **作者**：LG AI Research团队（包括Kyunghoon Bae, Eunbi Choi等众多研究人员）。\n*   **类型**：技术报告，共30页。\n*   **主题**：计算与语言（cs.CL）；人工智能（cs.AI）。\n*   **引用方式**：arXiv:2507.11407。\n*   **提交日期**：2025年7月15日。",
      "shortSummary": "EXAONE 4.0是LG AI Research推出的大型语言模型，整合了非推理和推理模式。它具备智能体工具使用能力，并支持英语、韩语和西班牙语。该模型提供32B（高性能）和1.2B（设备端）两种尺寸，性能优于同类开源模型，并能与前沿模型竞争。EXAONE 4.0已公开供研究使用。",
      "translated_title": "EXAONE 4.0：整合非推理和推理模式的统一大型语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "This technical report introduces EXAONE 4.0, which integrates a Non-reasoning mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5 and the advanced reasoning abilities of EXAONE Deep. To pave the way for the agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool use, and its multilingual capabilities are extended to support Spanish in addition to English and Korean. The EXAONE 4.0 model series consists of two sizes: a mid-size 32B model optimized for high performance, and a small-size 1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates superior performance compared to open-weight models in its class and remains competitive even against frontier-class models. The models are publicly available for research purposes and can be easily downloaded via https://huggingface.co/LGAI-EXAONE."
    },
    {
      "title": "爱因斯坦场：计算广义相对论的神经网络视角 (原标题: Einstein Fields: A Neural Perspective To Computational General Relativity)",
      "link": "https://arxiv.org/abs/2507.11589",
      "pubDate": "Tue, 15 Jul 2025 10:55:39 GMT",
      "isoDate": "2025-07-15T10:55:39.000Z",
      "creator": "Sandeep Suresh Cranganore, Andrei Bodnar, Arturs Berzins, Johannes Brandstetter",
      "summary": "### 爱因斯坦场：计算广义相对论的神经网络视角\n\n本研究引入了“爱因斯坦场”（Einstein Fields），这是一种新型的神经网络表示，旨在将计算密集型的四维数值相对论模拟压缩成紧凑的隐式神经网络权重。\n\n#### 核心概念与工作原理\n\n*   **建模“度规”**：爱因斯坦场通过建模“度规”（metric），即广义相对论的核心张量场，来编码时空几何。\n*   **自动微分推导物理量**：这种方法能够通过自动微分（automatic differentiation）方便地推导出各种物理量。\n\n#### 与传统神经场的区别\n\n*   **神经张量场**：与传统的神经场（如符号距离场、占用场或辐射场）不同，爱因斯坦场被定义为“神经张量场”（Neural Tensor Fields）。\n*   **动力学自然涌现**：其关键区别在于，当将广义相对论的时空几何编码到神经网络表示中时，动力学（dynamics）会自然而然地作为副产品出现。\n\n#### 显著潜力与优势\n\n爱因斯坦场展现出显著的潜力，包括但不限于：\n\n*   **四维时空连续体建模**：能够对四维时空进行连续体建模。\n*   **网格无关性**：不受传统计算网格的限制，提高了灵活性。\n*   **存储效率**：提供高效的数据存储，减少了模拟结果所需的空间。\n*   **导数精度**：确保高精度的导数计算，这对于物理量的推导至关重要。\n*   **易用性**：设计上考虑了使用的便捷性。\n\n#### 应用与未来展望\n\n*   研究团队在广义相对论的多个经典测试平台中验证了爱因斯坦场，并成功解决了相关挑战。\n*   他们发布了一个基于JAX的开源库，为数值相对论提供了更具可扩展性和表达力的方法，有望推动该领域的发展。\n\n#### 附加信息\n\n*   该研究论文共63页，包含22张图和10个表格。\n*   相关代码已在GitHub上开源。",
      "shortSummary": "爱因斯坦场是一种创新的神经网络表示，旨在高效压缩计算密集型的四维数值相对论模拟。它通过建模广义相对论的“度规”张量场，并利用自动微分推导物理量。作为一种独特的“神经张量场”，爱因斯坦场在编码时空几何时能自然产生动力学。该方法在四维时空建模、存储效率和导数精度等方面展现出巨大潜力，并已发布开源库，为数值相对论提供了更具可扩展性的新途径。",
      "translated_title": "爱因斯坦场：计算广义相对论的神经网络视角",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Einstein Fields, a neural representation that is designed to compress computationally intensive four-dimensional numerical relativity simulations into compact implicit neural network weights. By modeling the metric, which is the core tensor field of general relativity, Einstein Fields enable the derivation of physical quantities via automatic differentiation. However, unlike conventional neural fields (e.g., signed distance, occupancy, or radiance fields), Einstein Fields are Neural Tensor Fields with the key difference that when encoding the spacetime geometry of general relativity into neural field representations, dynamics emerge naturally as a byproduct. Einstein Fields show remarkable potential, including continuum modeling of 4D spacetime, mesh-agnosticity, storage efficiency, derivative accuracy, and ease of use. We address these challenges across several canonical test beds of general relativity and release an open source JAX-based library, paving the way for more scalable and expressive approaches to numerical relativity. Code is made available at https://github.com/AndreiB137/EinFields"
    },
    {
      "title": "多模态基础模型能否理解示意图？一项关于科学论文信息检索问答的实证研究 (原标题: Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers)",
      "link": "https://arxiv.org/abs/2507.10787",
      "pubDate": "Mon, 14 Jul 2025 16:35:25 GMT",
      "isoDate": "2025-07-14T16:35:25.000Z",
      "creator": "Yilun Zhao, Chengye Wang, Chuhan Li, Arman Cohan",
      "summary": "### MISS-QA：评估多模态基础模型对科学示意图的理解能力\n\n本文介绍了一个名为MISS-QA的新基准，该基准专门用于评估模型解释科学文献中示意图的能力。\n\n**基准概述：**\n*   **设计目的：** 专门评估模型对科学文献中示意图的理解能力。\n*   **构成：** 包含来自465篇科学论文的1,500个由专家标注的示例。\n*   **任务设置：** 模型需要解释说明研究概览的示意图，并根据论文的更广泛上下文回答相应的信息检索问题。\n\n**模型评估与发现：**\n*   **评估对象：** 评估了18个前沿的多模态基础模型，包括o4-mini、Gemini-2.5-Flash和Qwen2.5-VL。\n*   **主要发现：** 在MISS-QA基准上，这些模型的表现与人类专家之间存在显著的性能差距。\n*   **深入分析：**\n    *   对模型在无法回答问题上的表现进行了分析。\n    *   进行了详细的错误分析。\n    *   这些分析突出了当前模型的优势和局限性。\n*   **研究意义：** 为增强模型理解多模态科学文献的能力提供了关键见解。\n\n**相关信息：**\n*   **会议：** ACL 2025 Findings\n*   **研究领域：** 计算与语言 (cs.CL); 计算机视觉与模式识别 (cs.CV)\n*   **引用信息：** arXiv:2507.10787 [cs.CL]",
      "shortSummary": "本文引入了MISS-QA，一个旨在评估多模态基础模型理解科学论文中示意图能力的基准。该研究测试了18个前沿模型，发现它们在解释示意图并回答相关信息检索问题方面与人类专家存在显著差距。详细的错误分析揭示了当前模型的优势与局限，为提升模型对多模态科学文献的理解提供了重要见解。",
      "translated_title": "多模态基础模型能否理解示意图？一项关于科学论文信息检索问答的实证研究",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper introduces MISS-QA, the first benchmark specifically designed to evaluate the ability of models to interpret schematic diagrams within scientific literature. MISS-QA comprises 1,500 expert-annotated examples over 465 scientific papers. In this benchmark, models are tasked with interpreting schematic diagrams that illustrate research overviews and answering corresponding information-seeking questions based on the broader context of the paper. We assess the performance of 18 frontier multimodal foundation models, including o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant performance gap between these models and human experts on MISS-QA. Our analysis of model performance on unanswerable questions and our detailed error analysis further highlight the strengths and limitations of current models, offering key insights to enhance models in comprehending multimodal scientific literature."
    },
    {
      "title": "EmbRACE-3K：复杂环境中的具身推理与行动 (原标题: EmbRACE-3K: Embodied Reasoning and Action in Complex Environments)",
      "link": "https://arxiv.org/abs/2507.10548",
      "pubDate": "Mon, 14 Jul 2025 13:59:46 GMT",
      "isoDate": "2025-07-14T13:59:46.000Z",
      "creator": "Mingxian Lin, Wei Huang, Yitang Li, Chengjie Jiang, Kui Wu, Fangwei Zhong, Shengju Qian, Xin Wang, Xiaojuan Qi",
      "summary": "## EmbRACE-3K：复杂环境中的具身推理与行动\n\n### 引言\n\n近期先进的视觉-语言模型（VLMs）在被动、离线的图像和视频理解任务中展现出强大的性能。然而，它们在具身设置中的有效性仍然有限，这类设置需要在线交互和主动的场景理解。在具身场景中，智能体从第一人称视角感知环境，每个动作都会动态地影响后续的观察。即使是GPT-4o、Claude 3.5 Sonnet和Gemini 2.5 Pro等最先进的模型，在开放环境交互中也表现出明显的局限性，尤其是在空间推理和长程规划方面。\n\n### 解决方案：引入EmRACE-3K数据集\n\n为了弥补这一差距，我们引入了EmRACE-3K数据集，它包含3000多个语言引导任务，这些任务设置在利用虚幻引擎（Unreal Engine）和UnrealCV-Zoo框架构建的各种逼真环境中。\n\n*   **任务范围广泛**：数据集中的任务涵盖了广泛的具身挑战，包括导航、物体操作和多阶段目标执行。\n*   **多步轨迹**：每个任务都以多步轨迹的形式展开，将第一人称视觉观察与高级指令、具身动作以及在每一步表达智能体意图的自然语言理由配对。\n\n### 基准评估\n\n我们利用EmRACE-3K建立了一个基准，以评估VLMs在三个关键维度上的具身推理能力：\n\n1.  **探索（Exploration）**\n2.  **动态空间-语义推理（Dynamic Spatial-Semantic Reasoning）**\n3.  **多阶段目标执行（Multi-stage Goal Execution）**\n\n在零样本（zero-shot）设置下，所有模型的成功率均低于20%，这突显了我们基准测试所带来的挑战以及当前VLMs在交互式环境中的局限性。\n\n### 数据集效用演示\n\n为了展示EmRACE-3K的实用性，我们进一步通过监督学习（supervised learning）和强化学习（reinforcement learning）对Qwen2.5-VL-7B进行了微调。这种方法在所有三个挑战类别中都取得了显著的改进，突显了该数据集在促进具身推理能力发展方面的有效性。",
      "shortSummary": "文章介绍了EmRACE-3K数据集，旨在解决现有视觉-语言模型（VLMs）在复杂具身环境中具身推理和行动能力不足的问题。该数据集包含3000多个语言引导任务，涵盖导航、物体操作和多阶段目标执行。基准测试显示，VLMs在零样本设置下成功率低于20%。通过EmRACE-3K对模型进行微调，能显著提升其在探索、动态空间-语义推理和多阶段目标执行方面的表现，证明了数据集在发展具身推理能力方面的有效性。",
      "translated_title": "EmbRACE-3K：复杂环境中的具身推理与行动",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advanced vision-language models(VLMs) have demonstrated strong performance on passive, offline image and video understanding tasks. However, their effectiveness in embodied settings, which require online interaction and active scene understanding remains limited. In such scenarios, an agent perceives the environment from a first-person perspective, with each action dynamically shaping subsequent observations. Even state-of-the-art models such as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment interactions, exhibiting clear limitations in spatial reasoning and long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset of over 3,000 language-guided tasks situated in diverse, photorealistic environments constructed using Unreal Engine and the UnrealCV-Zoo framework. The tasks encompass a wide range of embodied challenges, including navigation, object manipulation, and multi-stage goal execution. Each task unfolds as a multi-step trajectory, pairing first-person visual observations with high-level instructions, grounded actions, and natural language rationales that express the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to evaluate the embodied reasoning capabilities of VLMs across three key dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage Goal Execution. In zero-shot settings, all models achieve success rates below 20%, underscoring the challenge posed by our benchmark and the current limitations of VLMs in interactive environments. To demonstrate the utility of EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning followed by reinforcement learning. This approach yields substantial improvements across all three challenge categories, highlighting the dataset's effectiveness in enabling the development of embodied reasoning capabilities."
    },
    {
      "title": "REST：通过同时提出多个问题对大型推理模型进行压力测试 (原标题: REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once)",
      "link": "https://arxiv.org/abs/2507.10541",
      "pubDate": "Mon, 14 Jul 2025 13:58:47 GMT",
      "isoDate": "2025-07-14T13:58:47.000Z",
      "creator": "Zhuoshi Pan, Qizhi Pei, Yu Li, Qiyao Sun, Zinan Tang, H. Vicky Zhao, Conghui He, Lijun Wu",
      "summary": "## REST：大型推理模型的压力测试\n\n### 引言：现有评估方法的局限性\n\n当前对大型推理模型（LRMs）的评估主要局限于孤立的问题解决范式，即通过顺序测试评估模型对单个问题的推理能力。这种方法存在以下关键局限性：\n\n*   **数据污染和挑战性不足**：容易受到数据污染，且挑战性较低（例如，DeepSeek-R1在MATH500上达到97.0%），这导致需要投入大量人力和成本持续创建新问题。\n*   **缺乏多上下文压力评估**：未能评估模型在多上下文压力下的表现，而这在现实世界部署中至关重要。\n\n### REST 框架的提出\n\n为了弥补这一空白，研究人员提出了 **REST (Reasoning Evaluation through Simultaneous Testing)**，这是一个对LRMs进行压力测试的框架，它能够同时向模型提出多个问题。\n\n### REST 评估的能力\n\n除了基本的推理能力，REST 还特别评估了几种此前未充分测试的能力：\n\n*   **上下文优先级分配**：模型在多个问题中分配注意力的能力。\n*   **跨问题干扰抵抗**：模型在处理一个问题时，抵抗其他问题干扰的能力。\n*   **动态认知负荷管理**：模型在认知负荷变化时保持性能的能力。\n\n### 关键发现\n\n评估结果揭示了几个显著的发现：\n\n*   **性能显著下降**：即使是像DeepSeek-R1这样的最先进（SOTA）模型，在REST压力测试下也表现出显著的性能下降。\n*   **更强的区分能力**：REST比现有基准测试展现出更强的区分能力，能够揭示在单问题评估中表现相似、接近上限的模型之间明显的性能差异。\n\n### 机制洞察\n\n分析中得出了一些关键的机制洞察：\n\n*   **“过度思考陷阱”**：这是一个导致性能下降的关键因素。\n*   **“long2short”训练技术**：采用“long2short”技术训练的模型在REST测试中能更好地保持其单问题性能的准确性，优于标准训练的模型。\n\n### 结论：REST 的优势\n\n这些结果表明，REST 是一种：\n\n*   **成本效益高**：减少对持续人工标注的依赖。\n*   **面向未来**：能够更好地反映现实世界的推理需求。\n*   **更有效**：提供更全面、更具挑战性的评估范式。",
      "shortSummary": "REST（Reasoning Evaluation through Simultaneous Testing）是一个新颖的压力测试框架，旨在通过同时向大型推理模型（LRMs）提出多个问题，解决现有单问题评估的局限性。它评估模型在多上下文压力下的表现，包括上下文优先级分配和抗干扰能力。研究发现，即使是SOTA模型在REST测试下性能也会显著下降，且REST比现有基准具有更强的区分能力。此外，“过度思考陷阱”是性能下降的原因之一，而“long2short”训练技术有助于模型保持准确性。REST提供了一种更具成本效益且能反映真实世界需求的评估范式。",
      "translated_title": "REST：通过同时提出多个问题对大型推理模型进行压力测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent Large Reasoning Models (LRMs) have achieved remarkable progress on task-specific benchmarks, yet their evaluation methods remain constrained by isolated problem-solving paradigms. Existing benchmarks predominantly assess single-question reasoning through sequential testing, resulting critical limitations: (1) vulnerability to data contamination and less challenging (e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual creation of new questions with large human efforts, (2) failure to evaluate models under multi-context pressure, a key requirement for real-world deployment. To bridge this gap, we present REST (Reasoning Evaluation through Simultaneous Testing), a stress-testing framework that concurrently exposes LRMs to multiple problems simultaneously. Beyond basic reasoning, REST specifically evaluates several under-tested capabilities: contextual priority allocation, cross-problem interference resistance, and dynamic cognitive load management. Our evaluation reveals several striking findings: Even state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance degradation under stress testing. Crucially, REST demonstrates stronger discriminative power than existing benchmarks, revealing pronounced performance differences among models that exhibit similar, near-ceiling performance under single-question evaluations. Some key mechanistic insights emerge from our analysis: (1) the \"overthinking trap\" is a critical factor contributing to the performance degradation; (2) the models trained with \"long2short\" technique preserve more accuracy of their single-problem performance under REST, outperforming standard-trained counterparts. These results establish REST as a cost-efficient, future-proof evaluation paradigm that better reflects real-world reasoning demands while reducing reliance on continuous human annotation."
    },
    {
      "title": "推理还是记忆？强化学习因数据污染导致结果不可靠 (原标题: Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination)",
      "link": "https://arxiv.org/abs/2507.10532",
      "pubDate": "Mon, 14 Jul 2025 13:55:15 GMT",
      "isoDate": "2025-07-14T13:55:15.000Z",
      "creator": "Mingqi Wu, Zhihao Zhang, Qiaole Dong, Zhiheng Xi, Jun Zhao, Senjie Jin, Xiaoran Fan, Yuhao Zhou, Yanwei Fu, Qin Liu, Songyang Zhang, Qi Zhang",
      "summary": "## 强化学习中数据污染导致结果不可靠\n\n### 引言\n\n本文探讨了大型语言模型（LLMs）的推理能力，特别是强化学习（RL）在增强这些能力方面的应用，并指出当前研究中存在的潜在问题。\n\n### 问题与观察\n\n*   **声称的突破与局限性**：近期许多RL方法声称能显著提升LLM的推理能力，甚至在奖励信号随机或不正确的情况下也能实现。然而，这些突破主要集中在Qwen2.5模型家族上，并在MATH-500、AMC和AIME等知名基准上进行评估。令人担忧的是，在Llama等其他模型上未能观察到类似的性能提升，这促使了进一步的深入调查。\n*   **性能差异的疑问**：一些研究甚至提出随机或不正确的奖励信号也能增强推理性能，这与直觉相悖，并加剧了对结果可靠性的质疑。\n\n### 分析与发现\n\n*   **数据污染的风险**：研究分析表明，尽管Qwen2.5在数学推理方面表现出色，但其在大规模网络语料库上的预训练使其容易受到流行基准中数据污染的影响。这意味着模型可能不是真正地进行推理，而是在一定程度上“记忆”了训练数据中包含的基准问题或其解决方案。\n*   **结果的不可靠性**：由于数据污染的存在，从这些受污染基准得出的结果可能并不可靠，无法真实反映RL方法对LLM推理能力的提升效果。\n\n### 解决方案与贡献\n\n*   **生成合成数据集**：为解决数据污染问题，研究引入了一个生成器，能够生成任意长度和难度的完全合成算术问题。这种方法确保了生成的问题是全新的，未曾出现在任何预训练语料或现有基准中。\n*   **构建“RandomCalculation”数据集**：基于此生成器，作者构建了一个名为“RandomCalculation”的干净、无泄露（leakage-free）数据集。这个数据集为评估RL方法提供了一个公正、无偏的环境。\n\n### 关键实验结果\n\n*   **奖励信号的有效性**：使用这些无污染数据集进行评估，研究发现只有准确的奖励信号才能持续且一致地提升性能。这与之前声称随机或不正确奖励信号也能有效的说法形成鲜明对比。\n*   **嘈杂信号的无效性**：实验结果明确指出，嘈杂或不正确的奖励信号并不能带来性能提升，这进一步强调了奖励信号质量的重要性。\n\n### 研究建议\n\n*   **评估标准**：作者强烈倡导在无污染的基准上，并跨越不同的模型家族来评估强化学习方法。\n*   **确保结论可信**：这种严格的评估方法对于确保研究结论的可靠性和可信度至关重要，有助于推动LLM推理能力研究的健康发展。\n\n### 其他信息\n\n本文共26页，属于机器学习（cs.LG）、人工智能（cs.AI）和计算语言学（cs.CL）领域。",
      "shortSummary": "一项研究指出，强化学习（RL）提升大型语言模型（LLM）推理能力的结果可能因数据污染而不可靠。Qwen2.5在流行基准上的出色表现被发现可能源于预训练数据污染，而其他模型未见此提升。为解决此问题，作者创建了无污染的“RandomCalculation”数据集。实验表明，只有准确的奖励信号才能持续提升性能，嘈杂或不正确的信号无效。研究建议在无污染基准和多样模型上评估RL方法，以确保结论可信。",
      "translated_title": "推理还是记忆？强化学习因数据污染导致结果不可靠",
      "images": [],
      "contentSource": "完整文章",
      "content": "The reasoning capabilities of large language models (LLMs) have been a longstanding focus of research. Recent works have further enhanced these capabilities using reinforcement learning (RL), with many new methods claiming significant improvements with minimal or no external supervision. Surprisingly, some studies even suggest that random or incorrect reward signals can enhance reasoning performance. However, these breakthroughs are mostly reported on the Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500, AMC, and AIME, while failing to achieve similar gains on other models like Llama, which warrants further investigation. Our analysis shows that although Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on large-scale web corpora makes it vulnerable to data contamination in popular benchmarks. As a result, results derived from these benchmarks may be unreliable. To address this, we introduce a generator that produces fully synthetic arithmetic problems of arbitrary length and difficulty, yielding a clean dataset we call RandomCalculation. Using these leakage-free datasets, we show that only accurate reward signals consistently improve performance, while noisy or incorrect signals do not. We advocate for evaluating RL methods on uncontaminated benchmarks and across diverse model families to ensure trustworthy conclusions."
    }
  ],
  "lastUpdated": "2025-07-19T09:30:39.468Z"
}