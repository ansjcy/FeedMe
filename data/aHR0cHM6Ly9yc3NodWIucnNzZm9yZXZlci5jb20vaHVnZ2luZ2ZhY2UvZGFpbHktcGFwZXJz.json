{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "Cambrian-S：迈向视频中的空间超感知 (原标题: Cambrian-S: Towards Spatial Supersensing in Video)",
      "link": "https://arxiv.org/abs/2511.04670",
      "pubDate": "Thu, 06 Nov 2025 13:55:17 GMT",
      "isoDate": "2025-11-06T13:55:17.000Z",
      "creator": "Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Muhan Wang, Daohan Lu, Rob Fergus, Yann LeCun, Li Fei-Fei, Saining Xie",
      "summary": "## Cambrian-S：迈向视频中的空间超感知\n\n本文作者提出，要实现真正的多模态智能，需要从被动、任务驱动的系统和暴力长上下文处理，转向更广泛的“超感知”范式，特别是“空间超感知”。\n\n### 空间超感知的四个阶段\n作者将空间超感知定义为超越纯语言理解的四个阶段：\n*   **语义感知**：识别所见事物并进行命名。\n*   **流式事件认知**：在连续的经验中维持记忆。\n*   **隐式3D空间认知**：从像素中推断出世界背后的三维结构。\n*   **预测性世界建模**：创建内部模型来过滤和组织信息。\n\n### 当前基准的局限性\n目前的基准测试主要集中在早期阶段，对空间认知的覆盖范围狭窄，并且很少以需要真正世界建模的方式来挑战模型。\n\n### VSI-SUPER基准\n为了推动空间超感知的发展，本文提出了VSI-SUPER，这是一个包含两部分的基准：\n*   **VSR (Long-horizon Visual Spatial Recall)**：长时程视觉空间回忆。\n*   **VSC (Continual Visual Spatial Counting)**：连续视觉空间计数。\n这些任务需要任意长的视频输入，但能有效抵抗暴力上下文扩展的方法。\n\n### 数据扩展与Cambrian-S模型\n作者通过整理VSI-590K数据集并训练Cambrian-S模型来测试数据扩展的极限。\n*   Cambrian-S在VSI-Bench上实现了30%的绝对性能提升，同时没有牺牲通用能力。\n*   然而，Cambrian-S在VSI-SUPER上的表现仍然有限，这表明仅靠规模不足以实现空间超感知。\n\n### 预测性感知：未来的方向\n本文提出“预测性感知”作为前进的道路，并提供了一个概念验证：\n*   一个自监督的下一潜在帧预测器利用“惊喜”（预测误差）来驱动记忆和事件分割。\n*   在VSI-SUPER上，这种方法显著优于领先的专有基线。\n\n### 结论\n空间超感知需要模型不仅能“看”，还能“预测”、“选择”和“组织”经验。",
      "shortSummary": "本文倡导多模态智能转向“空间超感知”范式，涵盖语义、事件、3D空间认知及预测性世界建模。为评估此能力，引入VSI-SUPER基准。研究发现，仅靠数据规模不足。利用预测误差的“预测性感知”方法，模型在VSI-SUPER上性能显著提升，表明模型需具备预测、选择和组织经验的能力。",
      "translated_title": "Cambrian-S：迈向视频中的空间超感知",
      "images": [],
      "contentSource": "完整文章",
      "content": "We argue that progress in true multimodal intelligence calls for a shift from reactive, task-driven systems and brute-force long context towards a broader paradigm of supersensing. We frame spatial supersensing as four stages beyond linguistic-only understanding: semantic perception (naming what is seen), streaming event cognition (maintaining memory across continuous experiences), implicit 3D spatial cognition (inferring the world behind pixels), and predictive world modeling (creating internal models that filter and organize information). Current benchmarks largely test only the early stages, offering narrow coverage of spatial cognition and rarely challenging models in ways that require true world modeling. To drive progress in spatial supersensing, we present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial recall) and VSC (continual visual spatial counting). These tasks require arbitrarily long video inputs yet are resistant to brute-force context expansion. We then test data scaling limits by curating VSI-590K and training Cambrian-S, achieving +30% absolute improvement on VSI-Bench without sacrificing general capabilities. Yet performance on VSI-SUPER remains limited, indicating that scale alone is insufficient for spatial supersensing. We propose predictive sensing as a path forward, presenting a proof-of-concept in which a self-supervised next-latent-frame predictor leverages surprise (prediction error) to drive memory and event segmentation. On VSI-SUPER, this approach substantially outperforms leading proprietary baselines, showing that spatial supersensing requires models that not only see but also anticipate, select, and organize experience."
    },
    {
      "title": "SIMS-V：模拟指令微调用于空间视频理解 (原标题: SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding)",
      "link": "https://arxiv.org/abs/2511.04668",
      "pubDate": "Thu, 06 Nov 2025 13:53:31 GMT",
      "isoDate": "2025-11-06T13:53:31.000Z",
      "creator": "Ellis Brown, Arijit Ray, Ranjay Krishna, Ross Girshick, Rob Fergus, Saining Xie",
      "summary": "## SIMS-V：模拟指令微调用于空间视频理解\n\n### 引言与问题背景\n尽管多模态语言模型在高级视频理解方面取得了令人瞩目的成就，但在跨越时间和空间的复杂空间推理方面仍面临挑战。当前用于空间训练的方法严重依赖真实世界的视频数据，然而，获取具有精确空间标注的多样化视频素材是一个主要的瓶颈。\n\n### SIMS-V 框架介绍\n为了缓解这一数据瓶颈，本文提出了 **SIMS-V**——一个系统性的数据生成框架。该框架利用 3D 模拟器所提供的特权信息，为多模态语言模型创建空间丰富的视频训练数据。\n\n### 研究方法与关键发现\n研究团队利用 SIMS-V 框架，通过系统性地消融问题类型、混合方式和规模，深入探究了模拟数据的哪些特性能够有效促进模型在真实世界任务上的迁移。他们识别出了一组最小但最有效的三类问题，这些问题在发展可迁移的空间智能方面表现最佳，甚至超越了全面覆盖所有问题类型的方法，尽管使用的问答类型更少：\n\n1.  **度量测量 (Metric Measurement)**：涉及对物体尺寸、距离等进行量化评估。\n2.  **依赖视角的推理 (Perspective-Dependent Reasoning)**：要求模型从不同视角理解空间关系和物体位置。\n3.  **时间跟踪 (Temporal Tracking)**：关注物体在时间维度上的运动轨迹和状态变化。\n\n### 训练效率与性能表现\n这些关键发现使得训练过程变得高度高效：\n\n*   一个仅在 2.5 万个模拟示例上进行微调的 7B 参数视频大型语言模型 (LLM)，其性能超越了更大的 72B 基线模型。\n*   在严格的真实世界空间推理基准测试中，该模型达到了与专有模型相当的竞争性能。\n\n### 泛化能力\nSIMS-V 方法展示了强大的泛化能力：\n\n*   在通用视频理解任务上保持了稳定的性能。\n*   在具身 (embodied) 和真实世界空间任务上显示出显著的性能改进。",
      "shortSummary": "SIMS-V 提出一个利用3D模拟器生成空间丰富视频训练数据的框架，旨在解决多模态语言模型在空间推理上的不足及真实世界数据稀缺问题。研究发现，通过度量测量、视角依赖推理和时间跟踪三类核心问题，可高效训练模型。一个7B参数模型在2.5万模拟示例上微调后，性能超越72B基线，并在真实世界空间推理基准上与专有模型媲美，同时保持通用视频理解能力并显著提升具身和真实世界空间任务表现。",
      "translated_title": "SIMS-V：模拟指令微调用于空间视频理解",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite impressive high-level video comprehension, multimodal language models struggle with spatial reasoning across time and space. While current spatial training approaches rely on real-world video data, obtaining diverse footage with precise spatial annotations remains a bottleneck. To alleviate this bottleneck, we present SIMS-V -- a systematic data-generation framework that leverages the privileged information of 3D simulators to create spatially-rich video training data for multimodal language models. Using this framework, we investigate which properties of simulated data drive effective real-world transfer through systematic ablations of question types, mixes, and scales. We identify a minimal set of three question categories (metric measurement, perspective-dependent reasoning, and temporal tracking) that prove most effective for developing transferable spatial intelligence, outperforming comprehensive coverage despite using fewer question types. These insights enable highly efficient training: our 7B-parameter video LLM fine-tuned on just 25K simulated examples outperforms the larger 72B baseline and achieves competitive performance with proprietary models on rigorous real-world spatial reasoning benchmarks. Our approach demonstrates robust generalization, maintaining performance on general video understanding while showing substantial improvements on embodied and real-world spatial tasks."
    },
    {
      "title": "基准设计者应“在测试集上训练”以揭示可利用的非视觉捷径 (原标题: Benchmark Designers Should \"Train on the Test Set\" to Expose Exploitable Non-Visual Shortcuts)",
      "link": "https://arxiv.org/abs/2511.04655",
      "pubDate": "Thu, 06 Nov 2025 13:43:21 GMT",
      "isoDate": "2025-11-06T13:43:21.000Z",
      "creator": "Ellis Brown, Jihan Yang, Shusheng Yang, Rob Fergus, Saining Xie",
      "summary": "### 基准设计者应“在测试集上训练”以揭示可利用的非视觉捷径\n\n本文探讨了多模态大型语言模型（MLLM）评估中基准测试的鲁棒性问题，并提出了一种新的设计原则和方法来解决现有基准中普遍存在的非视觉捷径利用问题。\n\n**核心问题**\n*   **MLLM的局限性**：研究发现，多模态大型语言模型（MLLM）在许多多模态基准测试中表现出色，并非完全依赖于强大的视觉理解能力。\n*   **利用非视觉捷径**：模型反而利用了偏见、语言先验和表面模式等非视觉捷径来“通过”测试。这对于旨在需要视觉输入的以视觉为中心的基准测试来说，是一个严重的问题。\n\n**诊断原则与解决方案**\n*   **“如果基准可以被利用，它就会被利用”**：作者提出一个诊断原则，即基准设计者应首先尝试“利用”自己的基准。\n*   **系统性识别和缓解**：通过诊断和去偏程序，系统地识别并缓解非视觉偏见。\n*   **“在测试集上训练”**：有效的诊断需要直接“在测试集上训练”，即探测已发布的测试集，以发现其固有的、可利用的模式。\n\n**操作化标准：两个核心组件**\n\n1.  **测试集压力测试（Test-set Stress-Test, TsT）方法诊断基准敏感性**\n    *   **主要诊断工具**：通过k折交叉验证，仅在测试集的非视觉文本输入上微调一个强大的大型语言模型。\n        *   目的：揭示捷径性能，并为每个样本分配一个偏见分数 `s(x)`。\n    *   **辅助诊断工具**：一个轻量级的基于随机森林的诊断工具，它在手工特征上运行，用于快速、可解释的审计。\n\n2.  **迭代偏见剪枝（Iterative Bias Pruning, IBP）程序去偏基准**\n    *   通过过滤掉高偏见样本来对基准进行去偏。\n\n**案例研究与发现**\n*   **应用范围**：将该框架应用于四个基准测试：VSI-Bench、CV-Bench、MMMU 和 VideoMME。\n*   **普遍存在的非视觉偏见**：研究揭示了这些基准中普遍存在的非视觉偏见。\n*   **VSI-Bench-Debiased**：作为一个案例研究，作者应用完整的框架创建了“VSI-Bench-Debiased”。\n    *   结果：与原始基准相比，去偏后的版本显著降低了非视觉可解性，并展现出更宽的视觉盲性能差距。\n\n**结论**\n本文强调了在基准设计中主动识别和消除非视觉捷径的重要性，并提供了一套实用的诊断和去偏方法，以确保多模态基准测试能够真正评估模型的视觉理解能力。",
      "shortSummary": "本文指出，多模态大型语言模型（MLLM）常通过利用非视觉捷径而非真正的视觉理解来“通过”基准测试。为解决此问题，作者提出基准设计者应“在测试集上训练”，主动诊断并缓解这些偏见。他们开发了“测试集压力测试”（TsT）来识别捷径，并使用“迭代偏见剪枝”（IBP）来去偏。应用于多个基准后，发现普遍存在的非视觉偏见，并成功创建了一个去偏的VSI-Bench版本，显著提升了基准的鲁棒性。",
      "translated_title": "基准设计者应“在测试集上训练”以揭示可利用的非视觉捷径",
      "images": [],
      "contentSource": "完整文章",
      "content": "Robust benchmarks are crucial for evaluating Multimodal Large Language Models (MLLMs). Yet we find that models can ace many multimodal benchmarks without strong visual understanding, instead exploiting biases, linguistic priors, and superficial patterns. This is especially problematic for vision-centric benchmarks that are meant to require visual inputs. We adopt a diagnostic principle for benchmark design: if a benchmark can be gamed, it will be. Designers should therefore try to ``game'' their own benchmarks first, using diagnostic and debiasing procedures to systematically identify and mitigate non-visual biases. Effective diagnosis requires directly ``training on the test set'' -- probing the released test set for its intrinsic, exploitable patterns.   We operationalize this standard with two components. First, we diagnose benchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology. Our primary diagnostic tool involves fine-tuning a powerful Large Language Model via k-fold cross-validation on exclusively the non-visual, textual inputs of the test set to reveal shortcut performance and assign each sample a bias score s(x). We complement this with a lightweight Random Forest-based diagnostic operating on hand-crafted features for fast, interpretable auditing. Second, we debias benchmarks by filtering high-bias samples using an ``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four benchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive non-visual biases. As a case study, we apply our full framework to create VSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider vision-blind performance gap than the original."
    },
    {
      "title": "视频思维：视频生成作为一种有前景的多模态推理范式 (原标题: Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm)",
      "link": "https://arxiv.org/abs/2511.04570",
      "pubDate": "Thu, 06 Nov 2025 12:25:23 GMT",
      "isoDate": "2025-11-06T12:25:23.000Z",
      "creator": "Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng Qiu",
      "summary": "文章介绍了一种名为“视频思维”（Thinking with Video）的新范式，旨在克服现有“文本思维”（Thinking with Text）和“图像思维”（Thinking with Images）范式在大型语言模型（LLMs）和视觉语言模型（VLMs）推理能力方面的固有局限性。\n\n**现有范式的局限性：**\n*   **图像局限性：** 图像只能捕捉单一瞬间，无法有效表示动态过程或连续变化。\n*   **模态分离：** 文本和视觉作为独立的模态，阻碍了统一的多模态理解和生成。\n\n**“视频思维”范式：**\n*   **核心理念：** 利用视频生成模型（如Sora-2）在一个统一的时间框架内桥接视觉和文本推理。\n*   **目标：** 实现统一的多模态理解和生成。\n\n**Video Thinking Benchmark (VideoThinkBench)：**\n*   为支持“视频思维”的探索，研究人员开发了VideoThinkBench基准。\n*   **任务类别：**\n    *   **以视觉为中心的任务：** 例如“目测谜题”（Eyeballing Puzzles）。\n    *   **以文本为中心的任务：** 例如GSM8K和MMMU的子集。\n\n**Sora-2的评估结果：**\n*   **推理能力：** 评估表明Sora-2是一个有能力的推理器。\n*   **视觉中心任务表现：**\n    *   通常与最先进的视觉语言模型（SOTA VLMs）相当。\n    *   在某些任务上甚至超越了VLMs，例如“目测游戏”（Eyeballing Games）。\n*   **文本中心任务表现：**\n    *   在MATH任务上取得了92%的准确率。\n    *   在MMMU任务上取得了75.53%的准确率。\n\n**能力来源分析与性能提升：**\n*   研究系统分析了Sora-2这些能力来源。\n*   发现自洽性（self-consistency）和上下文学习（in-context learning）可以进一步提升Sora-2的性能。\n\n**结论：**\n*   研究结果表明，视频生成模型具有成为统一多模态理解和生成模型的潜力。\n*   “视频思维”被定位为一种有前景的统一多模态推理范式。",
      "shortSummary": "该研究提出了“视频思维”新范式，利用Sora-2等视频生成模型克服了传统文本和图像思维在动态过程表示及模态统一方面的局限。通过VideoThinkBench基准测试，Sora-2在视觉中心任务上与SOTA VLMs相当甚至超越，在文本中心任务（如MATH和MMMU）上也表现出色。研究证实视频生成模型有潜力实现统一的多模态理解与推理，将“视频思维”确立为一种有前景的统一多模态推理范式。",
      "translated_title": "视频思维：视频生成作为一种有前景的多模态推理范式",
      "images": [],
      "contentSource": "完整文章",
      "content": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly improve the reasoning ability of large language models (LLMs) and Vision Language Models (VLMs). However, these paradigms have inherent limitations. (1) Images capture only single moments and fail to represent dynamic processes or continuous changes, and (2) The separation of text and vision as distinct modalities, hindering unified multimodal understanding and generation. To overcome these limitations, we introduce \"Thinking with Video\", a new paradigm that leverages video generation models, such as Sora-2, to bridge visual and textual reasoning in a unified temporal framework. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks, Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU. Furthermore, we systematically analyse the source of these abilities. We also find that self-consistency and in-context learning can improve Sora-2's performance. In summary, our findings demonstrate that the video generation model is the potential unified multimodal understanding and generation model, positions \"thinking with video\" as a unified multimodal reasoning paradigm."
    },
    {
      "title": "V-Thinker：图像交互式思维 (原标题: V-Thinker: Interactive Thinking with Images)",
      "link": "https://arxiv.org/abs/2511.04460",
      "pubDate": "Thu, 06 Nov 2025 10:32:29 GMT",
      "isoDate": "2025-11-06T10:32:29.000Z",
      "creator": "Runqi Qiao, Qiuna Tan, Minghan Yang, Guanting Dong, Peiqing Yang, Shiqiang Lang, Enhui Wan, Xiaowan Wang, Yida Xu, Lan Yang, Chong Sun, Chen Li, Honggang Zhang",
      "summary": "# V-Thinker：图像交互式思维\n\n## 挑战与背景\n大型多模态模型（LMMs）在将图像交互与长程推理能力深度融合方面，长期以来一直面临挑战。尽管近期“图像思维”（Thinking with Images）范式取得了进展，使模型能够关注细粒度图像区域，但其仍受限于有限的视觉工具空间和任务特定的工作流设计。\n\n## V-Thinker 解决方案\n为弥补这一差距，我们提出了 V-Thinker，一个通用的多模态推理助手。它通过端到端强化学习实现交互式、以视觉为中心的思维。\n\n### V-Thinker 的核心组件\n1.  **数据演化飞轮（Data Evolution Flywheel）**：\n    *   自动合成、演化和验证交互式推理数据集。\n    *   关注数据集的三个关键维度：多样性、质量和难度。\n2.  **视觉渐进式训练课程（Visual Progressive Training Curriculum）**：\n    *   首先通过点级监督对感知进行对齐。\n    *   然后通过两阶段强化学习框架整合交互式推理。\n\n## 评估基准\n我们还引入了 **VTBench**，这是一个经过专家验证的基准，专门针对以视觉为中心的交互式推理任务。\n\n## 实验结果\n广泛的实验表明，V-Thinker 在通用推理和交互式推理场景中，始终优于强大的基于 LMM 的基线模型。这为推动图像交互式推理应用提供了宝贵的见解。",
      "shortSummary": "V-Thinker是一个通用的多模态推理助手，旨在解决大型多模态模型（LMMs）在图像交互与长程推理融合方面的挑战。它包含“数据演化飞轮”自动生成和优化数据集，以及“视觉渐进式训练课程”通过两阶段强化学习整合交互式推理。V-Thinker在VTBench基准测试中表现出色，持续超越现有LMMs，为图像交互式推理应用提供了新思路。",
      "translated_title": "V-Thinker：图像交互式思维",
      "images": [],
      "contentSource": "完整文章",
      "content": "Empowering Large Multimodal Models (LMMs) to deeply integrate image interaction with long-horizon reasoning capabilities remains a long-standing challenge in this field. Recent advances in vision-centric reasoning explore a promising \"Thinking with Images\" paradigm for LMMs, marking a shift from image-assisted reasoning to image-interactive thinking. While this milestone enables models to focus on fine-grained image regions, progress remains constrained by limited visual tool spaces and task-specific workflow designs. To bridge this gap, we present V-Thinker, a general-purpose multimodal reasoning assistant that enables interactive, vision-centric thinking through end-to-end reinforcement learning. V-Thinker comprises two key components: (1) a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies interactive reasoning datasets across three dimensions-diversity, quality, and difficulty; and (2) a Visual Progressive Training Curriculum that first aligns perception via point-level supervision, then integrates interactive reasoning through a two-stage reinforcement learning framework. Furthermore, we introduce VTBench, an expert-verified benchmark targeting vision-centric interactive reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently outperforms strong LMM-based baselines in both general and interactive reasoning scenarios, providing valuable insights for advancing image-interactive reasoning applications."
    },
    {
      "title": "GUI-360：一个用于计算机使用代理的综合数据集和基准 (原标题: GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents)",
      "link": "https://arxiv.org/abs/2511.04307",
      "pubDate": "Thu, 06 Nov 2025 07:19:02 GMT",
      "isoDate": "2025-11-06T07:19:02.000Z",
      "creator": "Jian Mu, Chaoyun Zhang, Chiming Ni, Lu Wang, Bo Qiao, Kartik Mathur, Qianhui Wu, Yuhang Xie, Xiaojun Ma, Mengyu Zhou, Si Qin, Liqun Li, Yu Kang, Minghua Ma, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang",
      "summary": "# GUI-360：一个用于计算机使用代理的综合数据集和基准\n\n## 引言与背景\n计算机使用代理（CUAs）在实际应用中面临多重挑战，主要源于以下三个方面的持续性空白：\n*   **真实世界任务稀缺**：缺乏足够多的真实世界CUA任务来训练和评估代理。\n*   **数据收集与标注难题**：缺少针对多模态轨迹的自动化收集和标注流程。\n*   **统一基准缺失**：没有一个统一的基准能够全面评估GUI定位（GUI grounding）、屏幕解析（screen parsing）和动作预测（action prediction）等关键能力。\n\n## GUI-360$^\\circ$ 数据集与基准\nGUI-360$^\\circ$ 是一个大规模、综合性的数据集和基准套件，旨在解决上述问题，并推动计算机使用代理（CUAs）领域的发展。\n\n### 数据集构建流程\nGUI-360$^\\circ$ 的构建采用了一种由大型语言模型（LLM）增强的、高度自动化的流程，涵盖了从数据源头到质量控制的各个环节：\n*   **查询来源（Query Sourcing）**：自动化生成任务查询。\n*   **环境模板构建（Environment-Template Construction）**：创建标准化的任务执行环境模板。\n*   **任务实例化（Task Instantiation）**：根据模板生成具体的任务实例。\n*   **批量执行（Batched Execution）**：大规模并行执行任务，收集交互数据。\n*   **LLM驱动的质量过滤（LLM-driven Quality Filtering）**：利用LLM对收集到的数据进行质量检查和筛选。\n\n### 数据集内容\nGUI-360$^\\circ$ 语料库包含丰富的数据，主要特点如下：\n*   **动作步骤**：超过120万个已执行的动作步骤。\n*   **轨迹数量**：数千条完整的任务执行轨迹。\n*   **应用场景**：数据主要来源于流行的Windows办公应用程序。\n*   **数据类型**：\n    *   全分辨率屏幕截图。\n    *   可用的辅助功能元数据。\n    *   实例化的任务目标。\n    *   中间推理轨迹。\n    *   成功和失败的动作轨迹。\n\n### 支持的任务与动作空间\n该数据集支持对CUA能力进行评估的三种核心任务：\n1.  **GUI定位（GUI Grounding）**：代理识别和定位屏幕上特定用户界面元素的能力。\n2.  **屏幕解析（Screen Parsing）**：代理理解屏幕布局、元素关系和整体上下文的能力。\n3.  **动作预测（Action Prediction）**：代理根据当前屏幕状态和任务目标预测下一步操作的能力。\n\n此外，GUI-360$^\\circ$ 还支持一种混合的GUI+API动作空间，以更好地反映现代代理的设计和操作方式。\n\n## 基准测试结果\n对最先进的视觉-语言模型（VLMs）在GUI-360$^\\circ$ 上进行的基准测试揭示了以下关键发现：\n*   **开箱即用性能不足**：现有模型在GUI定位和动作预测方面表现出显著的局限性。\n*   **训练改进潜力**：通过监督微调（supervised fine-tuning）和强化学习（reinforcement learning）可以获得显著的性能提升。\n*   **与人类水平的差距**：尽管性能有所提升，但模型在可靠性方面仍未能弥合与人类水平之间的差距。\n\n## 发布与影响\nGUI-360$^\\circ$ 数据集及其配套代码已公开发布，旨在促进可复现的研究，并加速在开发鲁棒的桌面计算机使用代理方面的进展。完整数据集可在指定URL获取。",
      "shortSummary": "GUI-360$^\\circ$ 是一个大规模、综合性的数据集和基准，旨在推动计算机使用代理（CUAs）的发展。它通过LLM增强的自动化流程，收集了Windows办公应用中超过120万个动作步骤和数千条轨迹，解决了CUA任务稀缺、数据收集困难和统一基准缺失的问题。数据集支持GUI定位、屏幕解析和动作预测等任务。基准测试显示，当前最先进模型在定位和动作预测上存在不足，虽经微调和强化学习有所提升，但仍未达到人类水平。GUI-360$^\\circ$ 的发布将促进CUA领域的研究进展。",
      "translated_title": "GUI-360：一个用于计算机使用代理的综合数据集和基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce GUI-360^circ, a large-scale, comprehensive dataset and benchmark suite designed to advance computer-using agents (CUAs). CUAs present unique challenges and is constrained by three persistent gaps: a scarcity of real-world CUA tasks, the lack of automated collection-and-annotation pipelines for multi-modal trajectories, and the absence of a unified benchmark that jointly evaluates GUI grounding, screen parsing, and action prediction.   GUI-360^circ addresses these gaps with an LLM-augmented, largely automated pipeline for query sourcing, environment-template construction, task instantiation, batched execution, and LLM-driven quality filtering. The released corpus contains over 1.2M executed action steps across thousands of trajectories in popular Windows office applications, and includes full-resolution screenshots, accessibility metadata when available, instantiated goals, intermediate reasoning traces, and both successful and failed action trajectories. The dataset supports three canonical tasks, GUI grounding, screen parsing, and action prediction, and a hybrid GUI+API action space that reflects modern agent designs. Benchmarking state-of-the-art vision--language models on GUI-360^circ reveals substantial out-of-the-box shortcomings in grounding and action prediction; supervised fine-tuning and reinforcement learning yield significant gains but do not close the gap to human-level reliability. We release GUI-360^circ and accompanying code to facilitate reproducible research and accelerate progress on robust desktop CUAs.   The full dataset has been made public on https://huggingface.co/datasets/vyokky/GUI-360."
    },
    {
      "title": "多头注意力机制的强彩票假设 (原标题: The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms)",
      "link": "https://arxiv.org/abs/2511.04217",
      "pubDate": "Thu, 06 Nov 2025 04:29:58 GMT",
      "isoDate": "2025-11-06T04:29:58.000Z",
      "creator": "Hikari Otsuka, Daiki Chijiwa, Yasuyuki Okoshi, Daichi Fujiki, Susumu Takeuchi, Masato Motomura",
      "summary": "# 多头注意力机制的强彩票假设\n\n## 摘要\n\n本文探讨了多头注意力（MHA）机制中的强彩票假设（SLTH），该假设认为高性能子网络（称为强彩票，SLTs）隐藏在随机初始化的神经网络中。尽管SLTH已在多种神经网络架构中得到理论证实，但其在Transformer架构，特别是其核心组件MHA中的理论理解仍有待完善。\n\n## 研究目的与贡献\n\n为了填补这一理论空白，本文引入了对MHA中SLTs存在性的理论分析，并取得了以下主要成果：\n\n*   **MHA中的SLT存在性证明**：\n    *   研究证明，如果一个随机初始化的MHA，其拥有 $H$ 个头和输入维度 $d$，并且其键（key）和值（value）的隐藏维度为 $O(d\\log(Hd^{3/2}))$，那么它以高概率包含一个SLT。这个SLT能够近似任意具有相同输入维度的MHA。\n\n*   **SLTH向Transformer的扩展**：\n    *   基于MHA的理论，本文将SLTH扩展到了**不含归一化层**的Transformer架构。\n\n## 经验验证\n\n研究通过经验验证了其理论发现。结果表明，源模型（MHA和Transformer）中的SLT与近似目标对应物之间的近似误差，随着源模型隐藏维度的增加呈指数级下降。\n\n## 总结\n\n本文为MHA机制中的SLTH提供了重要的理论基础，并将其扩展到特定类型的Transformer模型，为理解和优化这些复杂模型提供了新的视角。",
      "shortSummary": "本文从理论上证明了多头注意力（MHA）机制中强彩票（SLTs）的存在性。研究发现，在特定隐藏维度条件下，随机初始化的MHA包含能近似任意MHA的SLT。基于此，该理论被扩展到不含归一化层的Transformer架构。经验验证进一步证实，近似误差随隐藏维度增加呈指数级下降，为理解和优化Transformer模型提供了新见解。",
      "translated_title": "多头注意力机制的强彩票假设",
      "images": [],
      "contentSource": "完整文章",
      "content": "The strong lottery ticket hypothesis (SLTH) conjectures that high-performing subnetworks, called strong lottery tickets (SLTs), are hidden in randomly initialized neural networks. Although recent theoretical studies have established the SLTH across various neural architectures, the SLTH for transformer architectures still lacks theoretical understanding. In particular, the current theory of the SLTH does not yet account for the multi-head attention (MHA) mechanism, a core component of transformers. To address this gap, we introduce a theoretical analysis of the existence of SLTs within MHAs. We prove that, if a randomly initialized MHA of H heads and input dimension d has the hidden dimension O(dlog(Hd^{3/2})) for the key and value, it contains an SLT that approximates an arbitrary MHA with the same input dimension with high probability. Furthermore, by leveraging this theory for MHAs, we extend the SLTH to transformers without normalization layers. We empirically validate our theoretical findings, demonstrating that the approximation error between the SLT within a source model (MHA and transformer) and an approximate target counterpart decreases exponentially by increasing the hidden dimension of the source model."
    },
    {
      "title": "人形机器人视觉驱动的反应式足球技能学习 (原标题: Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots)",
      "link": "https://arxiv.org/abs/2511.03996",
      "pubDate": "Wed, 05 Nov 2025 21:40:48 GMT",
      "isoDate": "2025-11-05T21:40:48.000Z",
      "creator": "Yushi Wang, Changsheng Luo, Penghui Chen, Jianran Liu, Weijian Sun, Tong Guo, Kechang Yang, Biao Hu, Yangang Zhang, Mingguo Zhao",
      "summary": "# 人形机器人视觉驱动的反应式足球技能学习\n\n## 摘要\n\n人形机器人足球对具身智能提出了严峻挑战，要求机器人在紧密耦合的感知-动作循环中运行。然而，现有系统通常依赖于解耦模块，导致在动态环境中响应延迟和行为不连贯，而现实世界的感知局限性进一步加剧了这些问题。\n\n## 核心方法：统一的强化学习控制器\n\n本文提出了一种统一的、基于强化学习（RL）的控制器，旨在解决上述挑战。该控制器通过直接整合视觉感知和运动控制，使人形机器人能够获得反应式足球技能。\n\n## 技术创新与架构\n\n1.  **扩展对抗性运动先验（AMP）**：\n    *   该方法将对抗性运动先验（Adversarial Motion Priors, AMP）扩展到现实世界动态环境中的感知设置。\n    *   这有效地连接了运动模仿和视觉驱动的动态控制。\n2.  **编码器-解码器架构与虚拟感知系统**：\n    *   引入了一种新颖的编码器-解码器架构。\n    *   结合了一个虚拟感知系统，该系统能够精确模拟现实世界的视觉特征。\n    *   这一设计使得策略能够从不完善的观测中恢复出“特权状态”（privileged states）。\n    *   在感知和动作之间建立了主动且高效的协调机制。\n\n## 成果与表现\n\n*   所开发的控制器展现出强大的反应能力。\n*   在各种场景中，包括真实的RoboCup比赛中，都能持续执行连贯且鲁棒的足球行为。\n*   这表明该方法能够有效应对动态环境下的复杂挑战。\n\n## 研究领域与项目信息\n\n*   该研究属于机器人学（cs.RO）领域。\n*   论文提交于2025年11月6日。",
      "shortSummary": "本文提出了一种统一的强化学习控制器，使人形机器人能够通过直接整合视觉感知和运动控制来学习反应式足球技能。该方法将对抗性运动先验扩展到现实世界感知环境，并引入编码器-解码器架构与虚拟感知系统，以从不完善观测中恢复特权状态，实现感知与动作的主动协调。该控制器在RoboCup比赛等多种场景中表现出强大的反应性和鲁棒的足球行为，有效解决了现有系统响应延迟和行为不连贯的问题。",
      "translated_title": "人形机器人视觉驱动的反应式足球技能学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Humanoid soccer poses a representative challenge for embodied intelligence, requiring robots to operate within a tightly coupled perception-action loop. However, existing systems typically rely on decoupled modules, resulting in delayed responses and incoherent behaviors in dynamic environments, while real-world perceptual limitations further exacerbate these issues. In this work, we present a unified reinforcement learning-based controller that enables humanoid robots to acquire reactive soccer skills through the direct integration of visual perception and motion control. Our approach extends Adversarial Motion Priors to perceptual settings in real-world dynamic environments, bridging motion imitation and visually grounded dynamic control. We introduce an encoder-decoder architecture combined with a virtual perception system that models real-world visual characteristics, allowing the policy to recover privileged states from imperfect observations and establish active coordination between perception and action. The resulting controller demonstrates strong reactivity, consistently executing coherent and robust soccer behaviors across various scenarios, including real RoboCup matches."
    },
    {
      "title": "NVIDIA Nemotron Nano V2 VL",
      "link": "https://arxiv.org/abs/2511.03929",
      "pubDate": "Wed, 05 Nov 2025 19:10:19 GMT",
      "isoDate": "2025-11-05T19:10:19.000Z",
      "creator": "NVIDIA, Amala Sanjay Deshmukh, Kateryna Chumachenko, Tuomas Rintamaki, Matthieu Le, Tyler Poon, Danial Mohseni Taheri, Ilia Karmanov, Guilin Liu, Jarno Seppanen, Guo Chen, Karan Sapra, Zhiding Yu, Adi Renduchintala, Charles Wang, Peter Jin, Arushi Goel, Mike Ranzinger, Lukas Voegtle, Philipp Fischer, Timo Roman, Wei Ping, Boxin Wang, Zhuolin Yang, Nayeon Lee, Shaokun Zhang, Fuxiao Liu, Zhiqi Li, Di Zhang, Greg Heinrich, Hongxu, Yin, Song Han, Pavlo Molchanov, Parth Mannan, Yao Xu, Jane Polak Scowcroft, Tom Balough, Subhashree Radhakrishnan, Paris Zhang, Sean Cha, Ratnesh Kumar, Zaid Pervaiz Bhat, Jian Zhang, Darragh Hanley, Pritam Biswas, Jesse Oliver, Kevin Vasques, Roger Waleffe, Duncan Riach, Oluwatobi Olabiyi, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Pritam Gundecha, Khanh Nguyen, Alexandre Milesi, Eugene Khvedchenia, Ran Zilberstein, Ofri Masad, Natan Bagrov, Nave Assaf, Tomer Asida, Daniel Afrimi, Amit Zuker, Netanel Haber, Zhiyu Cheng, Jingyu, Xin, Di, Wu, Nik Spirin, Maryam Moosaei, Roman Ageev, Vanshil Atul Shah, Yuting Wu, Daniel Korzekwa, Unnikrishnan Kizhakkemadam Sreekumar, Wanli Jiang, Padmavathy Subramanian, Alejandra Rico, Sandip Bhaskar, Saeid Motiian, Kedi Wu, Annie Surla, Chia-Chih Chen, Hayden Wolff, Matthew Feinberg, Melissa Corpuz, Marek Wawrzos, Eileen Long, Aastha Jhunjhunwala, Paul Hendricks, Farzan Memarian, Benika Hall, Xin-Yu Wang, David Mosallanezhad, Soumye Singhal, Luis Vega, Katherine Cheung, Krzysztof Pawelec, Michael Evans, Katherine Luna, Jie Lou, Erick Galinkin, Akshay Hazare, Kaustubh Purandare, Ann Guan, Anna Warno, Chen Cui, Yoshi Suhara, Shibani Likhite, Seph Mard, Meredith Price, Laya Sleiman, Saori Kaji, Udi Karpas, Kari Briski, Joey Conway, Michael Lightstone, Jan Kautz, Mohammad Shoeybi, Mostofa Patwary, Jonathen Cohen, Oleksii Kuchaiev, Andrew Tao, Bryan Catanzaro",
      "summary": "# NVIDIA Nemotron Nano V2 VL：新一代视觉语言模型\n\nNVIDIA 隆重推出 Nemotron Nano V2 VL，这是 Nemotron 视觉语言系列的最新模型。该模型专为实现强大的真实世界文档理解、长视频理解和复杂的推理任务而设计。\n\n## 主要特点与性能提升\n*   **显著改进**：Nemotron Nano V2 VL 在所有视觉和文本领域均比其前代模型 Llama-3.1-Nemotron-Nano-VL-8B 有显著提升。\n*   **核心增强**：这些性能上的飞跃主要归因于模型架构、数据集和训练方案的重大改进。\n*   **技术基础**：\n    *   该模型建立在 Nemotron Nano V2 的基础上，Nemotron Nano V2 是一种混合 Mamba-Transformer 大型语言模型（LLM）。\n    *   集成了创新的令牌（token）减少技术。\n*   **效率优势**：通过结合上述技术，Nemotron Nano V2 VL 在处理长文档和视频场景时，能够实现更高的推理吞吐量。\n\n## 发布内容与资源共享\nNVIDIA 正在积极发布：\n*   **模型检查点**：提供 BF16、FP8 和 FP4 等多种格式的模型检查点，以满足不同部署需求。\n*   **资源共享**：同时，NVIDIA 也将共享大部分用于训练模型的数据集、训练方案和训练代码，以促进研究和开发。\n\n## 作者与研究领域\n*   **主要作者**：该项目由 NVIDIA 的众多研究人员共同完成。\n*   **研究领域**：该研究主要涉及机器学习 (cs.LG)、人工智能 (cs.AI) 和计算机视觉与模式识别 (cs.CV) 等领域。",
      "shortSummary": "NVIDIA 发布了 Nemotron Nano V2 VL，这是其Nemotron视觉语言系列的最新模型。该模型在真实世界文档理解、长视频理解和推理任务上表现出色，相较于前代Llama-3.1-Nemotron-Nano-VL-8B有显著提升。它基于混合Mamba-Transformer LLM架构，并采用创新令牌减少技术，以实现更高的推理吞吐量。NVIDIA正发布BF16、FP8、FP4格式的模型检查点，并共享部分数据集、训练方案和代码。",
      "translated_title": "NVIDIA Nemotron Nano V2 VL",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Nemotron Nano V2 VL, the latest model of the Nemotron vision-language series designed for strong real-world document understanding, long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers significant improvements over our previous model, Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major enhancements in model architecture, datasets, and training recipes. Nemotron Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and innovative token reduction techniques to achieve higher inference throughput in long document and video scenarios. We are releasing model checkpoints in BF16, FP8, and FP4 formats and sharing large parts of our datasets, recipes and training code."
    },
    {
      "title": "使用多模态语义扰动的VLM污染检测 (原标题: Contamination Detection for VLMs using Multi-Modal Semantic Perturbation)",
      "link": "https://arxiv.org/abs/2511.03774",
      "pubDate": "Wed, 05 Nov 2025 13:59:52 GMT",
      "isoDate": "2025-11-05T13:59:52.000Z",
      "creator": "Jaden Park, Mu Cai, Feng Yao, Jingbo Shang, Soochahn Lee, Yong Jae Lee",
      "summary": "## VLM污染检测：基于多模态语义扰动的新方法\n\n### 背景与问题\n\n视觉-语言模型（VLMs）在多项基准任务上取得了最先进的性能。然而，由于使用了互联网规模的、通常是专有的预训练语料库，引发了一个关键问题：测试集泄露可能导致模型性能虚高。尽管先前的工作已经针对大型语言模型（LLMs）提出了缓解策略，例如预训练数据的去污和基准测试的重新设计，但开发针对受污染VLM的检测方法这一互补方向仍未得到充分探索。\n\n### 现有方法的局限性\n\n为了解决这一空白，研究人员故意污染了流行的基准测试上的开源VLM，并发现现有的检测方法要么完全失效，要么表现出不一致的行为，无法有效识别污染。\n\n### 提出的解决方案：多模态语义扰动\n\n本文提出了一种新颖、简单但有效的检测方法，其核心是基于多模态语义扰动。该方法的核心思想是：受污染的模型在受控的扰动下将无法有效泛化。\n\n### 方法验证与结果\n\n研究人员通过多种现实的污染策略对所提出的方法进行了验证，证实了其鲁棒性和有效性。这表明该方法能够可靠地识别出因测试集泄露而导致性能虚高的VLM。\n\n### 资源发布\n\n该研究的代码和经过扰动的数据集将公开发布，以促进后续研究和实际应用。",
      "shortSummary": "针对视觉-语言模型（VLMs）中因测试集泄露导致的性能虚高问题，现有污染检测方法效果不佳。本文提出一种新颖的多模态语义扰动检测方法。通过故意污染开源VLM并施加受控扰动，研究发现受污染模型难以泛化。该方法在多种污染策略下表现出鲁棒性和有效性，旨在解决VLM污染检测的空白，并计划公开发布代码和数据集。",
      "translated_title": "使用多模态语义扰动的VLM污染检测",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in Vision-Language Models (VLMs) have achieved state-of-the-art performance on numerous benchmark tasks. However, the use of internet-scale, often proprietary, pretraining corpora raises a critical concern for both practitioners and users: inflated performance due to test-set leakage. While prior works have proposed mitigation strategies such as decontamination of pretraining data and benchmark redesign for LLMs, the complementary direction of developing detection methods for contaminated VLMs remains underexplored. To address this gap, we deliberately contaminate open-source VLMs on popular benchmarks and show that existing detection approaches either fail outright or exhibit inconsistent behavior. We then propose a novel simple yet effective detection method based on multi-modal semantic perturbation, demonstrating that contaminated models fail to generalize under controlled perturbations. Finally, we validate our approach across multiple realistic contamination strategies, confirming its robustness and effectiveness. The code and perturbed dataset will be released publicly."
    },
    {
      "title": "通过经验合成扩展智能体学习 (原标题: Scaling Agent Learning via Experience Synthesis)",
      "link": "https://arxiv.org/abs/2511.03773",
      "pubDate": "Wed, 05 Nov 2025 13:58:48 GMT",
      "isoDate": "2025-11-05T13:58:48.000Z",
      "creator": "Zhaorun Chen, Zhuokai Zhao, Kai Zhang, Bo Liu, Qi Qi, Yifan Wu, Tarun Kalluri, Sara Cao, Yuanhao Xiong, Haibo Tong, Huaxiu Yao, Hengduo Li, Jiacheng Zhu, Xian Li, Dawn Song, Bo Li, Jason Weston, Dat Huynh",
      "summary": "## DreamGym：通过经验合成扩展智能体学习\n\n### 引言\n\n强化学习（RL）能够通过交互实现自我提升，从而增强大型语言模型（LLM）智能体的能力。然而，其在实际应用中面临诸多挑战，包括昂贵的实际环境交互（rollouts）、任务多样性有限、奖励信号不可靠以及基础设施复杂性，这些因素共同阻碍了可扩展经验数据的有效收集。\n\n### DreamGym框架概述\n\n为解决上述挑战，本文引入了**DreamGym**，这是首个统一框架，旨在以可扩展的方式合成多样化经验，从而实现自主智能体的有效在线RL训练。DreamGym的核心理念是，不依赖于昂贵的真实环境交互，而是将环境动态提炼成一个基于推理的经验模型。\n\n### DreamGym的关键机制\n\nDreamGym通过以下创新机制实现其目标：\n\n1.  **推理驱动的经验模型：**\n    *   该框架将环境动态蒸馏成一个基于推理的经验模型。\n    *   通过逐步推理，该模型能够推导出一致的状态转换和反馈信号。\n    *   这种方法使得智能体交互数据（rollout）的收集具有高度可扩展性，显著减少了对真实环境交互的依赖。\n\n2.  **经验回放缓冲区：**\n    *   为了提高转换的稳定性和质量，DreamGym利用一个经验回放缓冲区。\n    *   该缓冲区首先通过离线真实世界数据进行初始化。\n    *   随后，它通过新鲜的交互持续得到丰富，从而积极支持智能体训练。\n\n3.  **自适应任务生成：**\n    *   为了促进知识获取和提升学习效率，DreamGym能够自适应地生成新的任务。\n    *   这些新任务专门设计来挑战当前智能体策略的极限，从而实现更有效的在线课程学习。\n\n### 实验结果与优势\n\n在多样化的环境和智能体骨干网络上进行的实验表明，DreamGym显著改善了RL训练效果，无论是在完全合成的环境中还是在从模拟到真实的迁移场景中。\n\n*   **非RL就绪任务：** 在WebArena等非RL就绪任务上，DreamGym的表现超越了所有基线方法30%以上。\n*   **RL就绪但成本高昂的任务：** 在RL就绪但交互成本高昂的设置中，DreamGym仅使用合成交互就能达到GRPO和PPO的性能水平。\n*   **模拟到真实迁移（Sim-to-Real Transfer）：** 当将纯粹基于合成经验训练的策略迁移到真实环境的RL任务时，DreamGym在需要极少真实世界交互的情况下，仍能带来显著的额外性能提升，为通用RL提供了一种可扩展的“热启动”策略。\n\n### 结论\n\nDreamGym提供了一种创新且高效的解决方案，通过合成经验克服了传统RL训练中数据收集成本高昂、任务多样性不足等瓶颈，为智能体学习的规模化和实际应用铺平了道路。",
      "shortSummary": "DreamGym是一个统一框架，通过经验合成解决强化学习（RL）智能体训练中成本高昂、任务多样性不足和数据收集困难等挑战。它利用推理驱动的经验模型生成可扩展的交互数据，结合经验回放缓冲区和自适应任务生成，显著提升了RL训练效果。实验表明，DreamGym在多种任务中表现优异，尤其在模拟到真实迁移场景中，能以更少的真实交互实现显著性能提升，为通用RL提供可扩展的“热启动”策略。",
      "translated_title": "通过经验合成扩展智能体学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "While reinforcement learning (RL) can empower large language model (LLM) agents by enabling self-improvement through interaction, its practical adoption remains challenging due to costly rollouts, limited task diversity, unreliable reward signals, and infrastructure complexity, all of which obstruct the collection of scalable experience data. To address these challenges, we introduce DreamGym, the first unified framework designed to synthesize diverse experiences with scalability in mind to enable effective online RL training for autonomous agents. Rather than relying on expensive real-environment rollouts, DreamGym distills environment dynamics into a reasoning-based experience model that derives consistent state transitions and feedback signals through step-by-step reasoning, enabling scalable agent rollout collection for RL. To improve the stability and quality of transitions, DreamGym leverages an experience replay buffer initialized with offline real-world data and continuously enriched with fresh interactions to actively support agent training. To improve knowledge acquisition, DreamGym adaptively generates new tasks that challenge the current agent policy, enabling more effective online curriculum learning. Experiments across diverse environments and agent backbones demonstrate that DreamGym substantially improves RL training, both in fully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready tasks like WebArena, DreamGym outperforms all baselines by over 30%. And in RL-ready but costly settings, it matches GRPO and PPO performance using only synthetic interactions. When transferring a policy trained purely on synthetic experiences to real-environment RL, DreamGym yields significant additional performance gains while requiring far fewer real-world interactions, providing a scalable warm-start strategy for general-purpose RL."
    },
    {
      "title": "LiveTradeBench：利用大型语言模型寻求真实世界的超额收益 (原标题: LiveTradeBench: Seeking Real-World Alpha with Large Language Models)",
      "link": "https://arxiv.org/abs/2511.03628",
      "pubDate": "Wed, 05 Nov 2025 11:47:26 GMT",
      "isoDate": "2025-11-05T11:47:26.000Z",
      "creator": "Haofei Yu, Fenghai Li, Jiaxuan You",
      "summary": "LiveTradeBench：利用大型语言模型寻求真实世界的超额收益\n\n本文介绍了LiveTradeBench，一个用于在真实且不断变化的市场中评估大型语言模型（LLM）代理的实时交易环境。该研究旨在解决现有LLM基准测试的局限性，即它们通常在静态环境中进行，缺乏真实世界的动态性和不确定性，因此无法有效评估LLM在不确定性下的决策能力。\n\n**LiveTradeBench的设计原则**\n\nLiveTradeBench遵循以下三个核心设计原则，以提供更真实的评估环境：\n\n*   **实时数据流：**\n    *   系统直接流式传输市场价格和新闻数据，消除了对离线回测的依赖。\n    *   这有助于防止信息泄露，并能捕捉市场中的实时不确定性。\n*   **投资组合管理抽象：**\n    *   将控制从单一资产操作扩展到多资产配置。\n    *   整合了风险管理和跨资产推理能力。\n*   **多市场评估：**\n    *   在结构上不同的环境中进行评估，包括美国股票市场和Polymarket预测市场。\n    *   这些市场在波动性、流动性和信息流方面存在显著差异，提供了更全面的测试。\n\n**代理行为与评估过程**\n\n在LiveTradeBench的每一步，LLM代理会观察到最新的价格、新闻及其当前的投资组合状态。然后，代理需要输出百分比分配，以平衡风险和回报。\n\n研究人员使用LiveTradeBench对21个不同系列的LLM进行了为期50天的实时评估。\n\n**主要研究发现**\n\n评估结果揭示了以下关键发现：\n\n1.  **静态基准与实际表现的差异：** 在LMArena等静态基准测试中获得高分的模型，并不意味着它们在实际交易中也能取得优异表现。这表明静态评估与真实世界能力之间存在差距。\n2.  **模型独特的投资组合风格：** 不同的LLM模型展现出独特的投资组合风格，这反映了它们各自的风险偏好和推理动态。\n3.  **LLM利用实时信号的能力：** 一些LLM能够有效地利用实时市场信号来调整其决策，显示出一定的适应性。\n\n**结论与启示**\n\n这些发现强调了静态评估与真实世界能力之间的显著差距，并促使人们重新思考LLM的评估方式。研究呼吁开发新的基准测试，以在实时不确定性下测试LLM的顺序决策能力和一致性。",
      "shortSummary": "LiveTradeBench是一个评估大型语言模型（LLM）在真实、动态市场中决策能力的实时交易环境。它通过实时数据流、投资组合管理和多市场评估来模拟真实交易。研究发现，LLM在静态基准上的高分不代表实际交易成功，但部分模型能有效利用实时信号。这揭示了LLM静态评估与真实世界能力间的差距，呼吁更侧重不确定性下顺序决策的基准测试。",
      "translated_title": "LiveTradeBench：利用大型语言模型寻求真实世界的超额收益",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) achieve strong performance across benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but these tests occur in static settings, lacking real dynamics and uncertainty. Consequently, they evaluate isolated reasoning or problem-solving rather than decision-making under uncertainty. To address this, we introduce LiveTradeBench, a live trading environment for evaluating LLM agents in realistic and evolving markets. LiveTradeBench follows three design principles: (i) Live data streaming of market prices and news, eliminating dependence on offline backtesting and preventing information leakage while capturing real-time uncertainty; (ii) a portfolio-management abstraction that extends control from single-asset actions to multi-asset allocation, integrating risk management and cross-asset reasoning; and (iii) multi-market evaluation across structurally distinct environments--U.S. stocks and Polymarket prediction markets--differing in volatility, liquidity, and information flow. At each step, an agent observes prices, news, and its portfolio, then outputs percentage allocations that balance risk and return. Using LiveTradeBench, we run 50-day live evaluations of 21 LLMs across families. Results show that (1) high LMArena scores do not imply superior trading outcomes; (2) models display distinct portfolio styles reflecting risk appetite and reasoning dynamics; and (3) some LLMs effectively leverage live signals to adapt decisions. These findings expose a gap between static evaluation and real-world competence, motivating benchmarks that test sequential decision making and consistency under live uncertainty."
    },
    {
      "title": "UniAVGen：基于非对称跨模态交互的统一音视频生成 (原标题: UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions)",
      "link": "https://arxiv.org/abs/2511.03334",
      "pubDate": "Wed, 05 Nov 2025 05:06:51 GMT",
      "isoDate": "2025-11-05T05:06:51.000Z",
      "creator": "Guozhen Zhang, Zixiang Zhou, Teng Hu, Ziqiao Peng, Youliang Zhang, Yi Chen, Yuan Zhou, Qinglin Lu, Limin Wang",
      "summary": "UniAVGen 是一个为解决现有开源音视频生成方法中唇形同步不佳和语义一致性不足问题而提出的统一框架。这些问题通常源于缺乏有效的跨模态建模。\n\n**UniAVGen 框架概述：**\n*   **目标：** 实现联合音视频生成。\n*   **核心架构：** 基于双分支联合合成设计，包含两个并行的 Diffusion Transformers (DiTs)，旨在构建一个内聚的跨模态潜在空间。\n\n**关键机制与创新：**\n1.  **非对称跨模态交互 (Asymmetric Cross-Modal Interaction)：**\n    *   这是 UniAVGen 的核心机制。\n    *   它实现了双向、时间对齐的交叉注意力，从而确保了音视频之间精确的时空同步和语义一致性。\n2.  **面部感知调制模块 (Face-Aware Modulation)：**\n    *   该模块增强了跨模态交互过程。\n    *   它能够动态地优先处理交互中的显著区域，例如面部，以提高生成质量。\n3.  **模态感知无分类器指导 (Modality-Aware Classifier-Free Guidance)：**\n    *   为在推理阶段提升生成保真度而引入。\n    *   这是一种新颖的策略，能够明确地放大跨模态相关性信号。\n\n**UniAVGen 的能力与应用：**\n*   其强大的联合合成设计使得单一模型能够无缝统一多种关键的音视频任务，包括：\n    *   联合音视频生成与续写\n    *   视频到音频的配音\n    *   音频驱动的视频合成\n\n**实验验证：**\n*   全面的实验结果表明，UniAVGen 在使用远少于现有方法的训练样本（130万 vs. 3010万）的情况下，在音视频同步、音色一致性和情感一致性方面均展现出整体优势。",
      "shortSummary": "UniAVGen是一个统一的音视频生成框架，旨在解决现有方法中唇形同步差和语义不一致的问题。它采用双分支架构，核心是非对称跨模态交互机制，通过双向、时间对齐的交叉注意力确保精确的时空同步和语义一致性。此外，面部感知调制和模态感知无分类器指导进一步提升生成质量。UniAVGen能统一多种音视频任务，并在使用更少训练数据的情况下，在同步、音色和情感一致性方面表现出显著优势。",
      "translated_title": "UniAVGen：基于非对称跨模态交互的统一音视频生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Due to the lack of effective cross-modal modeling, existing open-source audio-video generation methods often exhibit compromised lip synchronization and insufficient semantic consistency. To mitigate these drawbacks, we propose UniAVGen, a unified framework for joint audio and video generation. UniAVGen is anchored in a dual-branch joint synthesis architecture, incorporating two parallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent space. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which enables bidirectional, temporally aligned cross-attention, thus ensuring precise spatiotemporal synchronization and semantic consistency. Furthermore, this cross-modal interaction is augmented by a Face-Aware Modulation module, which dynamically prioritizes salient regions in the interaction process. To enhance generative fidelity during inference, we additionally introduce Modality-Aware Classifier-Free Guidance, a novel strategy that explicitly amplifies cross-modal correlation signals. Notably, UniAVGen's robust joint synthesis design enables seamless unification of pivotal audio-video tasks within a single model, such as joint audio-video generation and continuation, video-to-audio dubbing, and audio-driven video synthesis. Comprehensive experiments validate that, with far fewer training samples (1.3M vs. 30.1M), UniAVGen delivers overall advantages in audio-video synchronization, timbre consistency, and emotion consistency."
    },
    {
      "title": "如何使用源感知神经机器翻译指标评估语音翻译 (原标题: How to Evaluate Speech Translation with Source-Aware Neural MT Metrics)",
      "link": "https://arxiv.org/abs/2511.03295",
      "pubDate": "Wed, 05 Nov 2025 03:49:22 GMT",
      "isoDate": "2025-11-05T03:49:22.000Z",
      "creator": "Mauro Cettolo, Marco Gaido, Matteo Negri, Sara Papi, Luisa Bentivogli",
      "summary": "## 如何使用源感知神经机器翻译指标评估语音翻译\n\n### 引言与背景\n\n当前，语音翻译（ST）系统的自动评估通常通过比较翻译假设与一个或多个参考翻译来完成。然而，这种方法继承了基于参考评估的局限性，即忽略了源输入中包含的宝贵信息。在机器翻译（MT）领域，最近的进展表明，结合了源文本的神经指标与人类判断的相关性更强。将这一思想扩展到ST并非易事，因为ST的源输入是音频而非文本，并且通常缺乏可靠的源文本转录或源与参考之间的对齐信息。\n\n### 研究目标与方法\n\n本研究首次系统地探讨了ST的源感知指标，特别关注在源文本转录不可用的实际操作条件。研究探索了两种互补的策略来生成输入音频的文本代理：\n\n*   **自动语音识别（ASR）转录**：将音频直接转换为文本。\n*   **参考翻译的逆向翻译（Back-translations）**：将参考翻译回译成源语言的文本。\n\n此外，为解决合成源与参考翻译之间的对齐不匹配问题，本研究引入了一种新颖的两步跨语言重分段算法。\n\n### 实验设计与结果\n\n实验在两个ST基准测试上进行，涵盖了79个语言对和六个具有不同架构和性能水平的ST系统。主要发现包括：\n\n*   当词错误率（WER）低于20%时，ASR转录比逆向翻译更能构成可靠的合成源。\n*   逆向翻译始终是一种计算成本更低但仍然有效的替代方案。\n*   本研究提出的跨语言重分段算法使得源感知MT指标能够稳健地应用于ST评估。\n\n### 结论与意义\n\n这项工作为语音翻译评估开辟了更准确和原则性的方法。通过利用源感知指标和创新的对齐技术，即使在缺乏原始源文本转录的情况下，也能显著提升ST系统评估的可靠性和有效性。\n\n### 相关领域\n\n*   计算与语言学 (cs.CL)\n*   人工智能 (cs.AI)",
      "shortSummary": "本研究首次系统探讨了在缺乏源文本转录的实际条件下，如何将源感知神经机器翻译（MT）指标应用于语音翻译（ST）评估。通过探索自动语音识别（ASR）转录和参考翻译的逆向翻译作为音频文本代理，并引入创新的跨语言重分段算法解决对齐问题。实验表明，ASR在词错误率低于20%时更可靠，而逆向翻译是经济有效的替代方案。该方法显著提升了语音翻译评估的准确性和原则性。",
      "translated_title": "如何使用源感知神经机器翻译指标评估语音翻译",
      "images": [],
      "contentSource": "完整文章",
      "content": "Automatic evaluation of speech-to-text translation (ST) systems is typically performed by comparing translation hypotheses with one or more reference translations. While effective to some extent, this approach inherits the limitation of reference-based evaluation that ignores valuable information from the source input. In machine translation (MT), recent progress has shown that neural metrics incorporating the source text achieve stronger correlation with human judgments. Extending this idea to ST, however, is not trivial because the source is audio rather than text, and reliable transcripts or alignments between source and references are often unavailable. In this work, we conduct the first systematic study of source-aware metrics for ST, with a particular focus on real-world operating conditions where source transcripts are not available. We explore two complementary strategies for generating textual proxies of the input audio, automatic speech recognition (ASR) transcripts, and back-translations of the reference translation, and introduce a novel two-step cross-lingual re-segmentation algorithm to address the alignment mismatch between synthetic sources and reference translations. Our experiments, carried out on two ST benchmarks covering 79 language pairs and six ST systems with diverse architectures and performance levels, show that ASR transcripts constitute a more reliable synthetic source than back-translations when word error rate is below 20%, while back-translations always represent a computationally cheaper but still effective alternative. Furthermore, our cross-lingual re-segmentation algorithm enables robust use of source-aware MT metrics in ST evaluation, paving the way toward more accurate and principled evaluation methodologies for speech translation."
    },
    {
      "title": "扩散语言模型是超级数据学习器 (原标题: Diffusion Language Models are Super Data Learners)",
      "link": "https://arxiv.org/abs/2511.03276",
      "pubDate": "Wed, 05 Nov 2025 03:17:42 GMT",
      "isoDate": "2025-11-05T03:17:42.000Z",
      "creator": "Jinjie Ni, Qian Liu, Longxu Dou, Chao Du, Zili Wang, Hang Yan, Tianyu Pang, Michael Qizhe Shieh",
      "summary": "扩散语言模型（DLMs）在严格控制的预训练设置下，当独特数据有限时，通过增加训练周期，始终超越自回归（AR）模型，展现出一种“交叉点”现象。这一交叉点会随着数据量增加或质量提高而推迟，随着模型增大而提前，并且在密集和稀疏架构中都持续存在。\n\n研究将DLM的性能提升归因于三个复合因素：\n\n*   **任意顺序建模（Any-order modeling）**：允许模型更灵活地处理数据。\n*   **迭代双向去噪带来的超密集计算（Super-dense compute from iterative bidirectional denoising）**：通过迭代过程更有效地利用计算资源。\n*   **内置蒙特卡洛增强（Built-in Monte Carlo augmentation）**：模型内部固有的数据增强机制。虽然输入或参数噪声也能在数据受限时改善AR模型，但无法弥补与DLM之间的差距。\n\n在规模化实验中，一个1.7B参数的DLM在10B独特Python tokens上，使用约1.5T-token的计算预算进行训练，其性能超越了在严格匹配设置下训练的AR编码器。此外，一个1B参数的DLM仅使用1B tokens（通过重复标准预训练数据，未采用任何特殊技巧），就在HellaSwag上取得了超过56%的准确率，在MMLU上取得了超过33%的准确率。\n\n研究还指出，在这种机制下，验证交叉熵的上升并不意味着下游性能的下降。",
      "shortSummary": "当独特训练数据有限时，扩散语言模型（DLMs）通过增加训练周期，持续超越自回归（AR）模型。DLMs的优势源于其任意顺序建模、迭代双向去噪带来的超密集计算以及内置的蒙特卡洛增强。实验表明，DLMs能从有限的重复数据中高效学习，在编码任务和HellaSwag、MMLU等基准测试上表现出色，证明了其作为“超级数据学习器”的能力。",
      "translated_title": "扩散语言模型是超级数据学习器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Under strictly controlled pre-training settings, we observe a Crossover: when unique data is limited, diffusion language models (DLMs) consistently surpass autoregressive (AR) models by training for more epochs. The crossover shifts later with more or higher-quality data, earlier with larger models, and persists across dense and sparse architectures. We attribute the gains to three compounding factors: (1) any-order modeling, (2) super-dense compute from iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation; input or parameter noise improves AR under data constraint but cannot close the gap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B unique Python tokens overtakes an AR coder trained with strictly matched settings. In addition, a 1B-parameter DLM achieves &gt; 56% accuracy on HellaSwag and &gt; 33% on MMLU using only 1B tokens, without any special tricks, just by repeating standard pre-training data. We also show that rising validation cross-entropy does not imply degraded downstream performance in this regime."
    },
    {
      "title": "MME-CC：一个具有挑战性的多模态认知能力评估基准 (原标题: MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity)",
      "link": "https://arxiv.org/abs/2511.03146",
      "pubDate": "Tue, 04 Nov 2025 22:09:16 GMT",
      "isoDate": "2025-11-04T22:09:16.000Z",
      "creator": "Kaiyuan Zhang, Chenghao Yang, Zhoufutu Wen, Sihang Yuan, Qiuyue Wang, Chaoyi Huang, Guosheng Zhu, He Wang, Huawenyu Lu, Jianing Wen, Jianpeng Jiao, Lishu Luo, Longxiang Liu, Sijin Wu, Xiaolei Zhu, Xuanliang Zhang, Ge Zhang, Yi Lin, Guang Shi, Chaoyou Fu, Wenhao Huang",
      "summary": "MME-CC：一个具有挑战性的多模态认知能力评估基准\n\n本文介绍了MME-CC（Multi-Modal Evaluation benchmark of Cognitive Capacity），一个旨在系统评估多模态大语言模型（MLLMs）视觉中心认知能力的新基准。\n\n**背景与问题**\n*   随着推理模型的快速发展，多模态在人类认知中的核心作用日益凸显，促使人们需要深入探究以视觉为中心的认知行为。\n*   然而，现有的多模态基准要么过分强调文本推理，要么未能系统地捕捉以视觉为中心的认知行为，导致对MLLMs认知能力的评估不足。\n\n**MME-CC基准介绍**\n*   **目的：** 解决现有基准的局限性，提供一个以视觉为基础的评估工具。\n*   **结构：** 将11个具有代表性的推理任务组织成三大类视觉信息：\n    *   **空间推理 (Spatial Reasoning)**\n    *   **几何推理 (Geometric Reasoning)**\n    *   **基于知识的推理 (Knowledge-based Reasoning)**\n*   **功能：** 提供对MLLMs在这些维度上认知能力的细粒度分析。\n\n**实验与发现**\n*   **实验对象：** 基于MME-CC，对16个具有代表性的MLLMs进行了广泛实验。\n*   **主要结果：**\n    *   **整体表现：** 闭源模型目前总体领先（例如，Gemini-2.5-Pro得分为42.66，而GLM-4.5V为30.45）。\n    *   **薄弱环节：** 空间推理和几何推理能力普遍较弱（得分低于或等于30%）。\n    *   **常见错误模式：**\n        *   方向错误 (orientation mistakes)\n        *   脆弱的跨视角身份持久性 (fragile cross-view identity persistence)\n        *   对反事实指令依从性差 (poor adherence to counterfactual instructions)\n    *   **思维链（Chain-of-Thought, CoT）分析：** CoT通常遵循一个三阶段过程（提取 -> 推理 -> 验证），并且严重依赖视觉提取。\n\n**展望**\n*   作者希望这项工作能够促使评估和模型设计将MLLMs的认知能力视为核心。",
      "shortSummary": "本文介绍了MME-CC，一个用于评估多模态大语言模型（MLLMs）视觉中心认知能力的新基准。MME-CC将11个推理任务分为空间、几何和知识三大类。实验发现，闭源模型总体表现领先，但MLLMs在空间和几何推理方面普遍较弱（≤30%），常出现方向错误和对反事实指令依从性差等问题。研究强调了将MLLMs认知能力作为评估和模型设计的核心。",
      "translated_title": "MME-CC：一个具有挑战性的多模态认知能力评估基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "As reasoning models scale rapidly, the essential role of multimodality in human cognition has come into sharp relief, driving a growing need to probe vision-centric cognitive behaviors. Yet, existing multimodal benchmarks either overemphasize textual reasoning or fall short of systematically capturing vision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs insufficiently assessed. To address this limitation, we introduce MME-CC (Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-grounded benchmark that organizes 11 representative reasoning tasks into three fundamental categories of visual information: spatial, geometric, and knowledge-based reasoning, and provides fine-grained analyses of MLLMs' cognitive capacity across these dimensions. Based on MME-CC, we conduct extensive experiments over 16 representative MLLMs. Our study reveals that closed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs. 30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak (less than or equal to 30%). We further identify common error patterns, including orientation mistakes, fragile cross-view identity persistence, and poor adherence to counterfactual instructions, and observe that Chain-of-Thought typically follows a three-stage process (extract -&gt; reason -&gt; verify) with heavy reliance on visual extraction. We hope this work catalyzes a shift toward treating the cognitive capacity of MLLMs as central to both evaluation and model design."
    },
    {
      "title": "LEGO-Eval：迈向通过工具增强实现3D具身环境合成的细粒度评估 (原标题: LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation)",
      "link": "https://arxiv.org/abs/2511.03001",
      "pubDate": "Tue, 04 Nov 2025 16:13:51 GMT",
      "isoDate": "2025-11-04T16:13:51.000Z",
      "creator": "Gyeom Hwangbo, Hyungjoo Chae, Minseok Kang, Hyeonjong Ju, Soohyun Oh, Jinyoung Yeo",
      "summary": "## LEGO-Eval：迈向通过工具增强实现3D具身环境合成的细粒度评估\n\n### 引言与背景\n\n尽管大型语言模型（LLM）在自动生成3D场景方面取得了进展，但生成的场景往往缺乏真实世界环境中应有的空间布局和对象属性。这一问题主要源于指令的粒度过粗，导致场景细节不足。因此，通过更详细、细粒度的指令来指导3D场景合成，使其能反映真实世界环境，变得至关重要。在不真实的场景中训练具身智能体，可能导致它们学习到与现实世界物理和语义显著偏离的先验知识，从而在实际部署时性能下降。因此，验证细粒度指令与生成场景之间的一致性对于有效学习至关重要。\n\n然而，当前的评估方法，如CLIPScore和视觉-语言模型（VLM），通常无法可靠地评估这种一致性。这主要是因为它们对3D场景的理解深度不足，常常导致场景组件的接地（grounding）不当。\n\n### LEGO-Eval 评估框架\n\n为了解决上述问题，本文引入了 **LEGO-Eval**，这是一个配备了多种工具的评估框架。这些工具旨在明确地对场景组件进行接地，从而实现更准确的场景-指令一致性评估。\n\n### LEGO-Bench 基准\n\n除了LEGO-Eval，本文还提出了 **LEGO-Bench**，这是一个包含详细指令的基准。这些指令具体指定了真实世界环境中复杂的布局和属性，为评估3D场景生成提供了标准。\n\n### 实验结果与发现\n\n实验结果表明：\n\n*   **LEGO-Eval 的优越性：** 在评估场景-指令一致性方面，LEGO-Eval 的F1分数比“VLM-as-a-judge”高出0.41，证明了其评估的准确性。\n*   **现有生成方法的局限性：** 使用LEGO-Bench进行基准测试揭示了当前生成方法存在的显著局限性。在所有评估的方法中，能够完全符合细粒度指令的场景生成成功率最高仅为10%。\n\n### 总结\n\n这项工作强调了细粒度评估对于提升3D具身环境合成质量的重要性，并提出了LEGO-Eval框架和LEGO-Bench基准来应对这一挑战。实验结果表明，当前3D场景生成技术在实现与细粒度指令完全一致的场景方面仍有很大的提升空间。\n\n### 其他信息\n\n*   **状态：** 工作进展中 (Work in Progress)\n*   **主题：** 计算与语言 (cs.CL)\n*   **arXiv ID：** arXiv:2511.03001",
      "shortSummary": "针对大型语言模型（LLM）生成的3D场景缺乏真实感和细粒度评估不足的问题，本文提出了LEGO-Eval框架和LEGO-Bench基准。LEGO-Eval利用工具明确地对场景组件进行接地，以实现更准确的场景-指令一致性评估，其F1分数比现有方法高0.41。LEGO-Bench提供包含复杂布局和属性的详细指令。实验表明，当前3D场景生成方法在完全符合细粒度指令方面成功率最高仅为10%，揭示了现有技术的显著局限性。",
      "translated_title": "LEGO-Eval：迈向通过工具增强实现3D具身环境合成的细粒度评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite recent progress in using Large Language Models (LLMs) for automatically generating 3D scenes, generated scenes often lack realistic spatial layouts and object attributes found in real-world environments. As this problem stems from insufficiently detailed, coarse-grained instructions, advancing 3D scene synthesis guided by more detailed, fine-grained instructions that reflect real-world environments becomes crucial. Without such realistic scenes, training embodied agents in unrealistic environments can lead them to learn priors that diverge significantly from real-world physics and semantics, degrading their performance when deployed. Thus, verifying the alignment between the fine-grained instruction and the generated scene is essential for effective learning. However, current evaluation methods, such as CLIPScore and vision-language models (VLMs), often fail to reliably assess such alignment. This shortcoming arises primarily from their shallow understanding of 3D scenes, which often leads to improperly grounded scene components. To address this, we introduce LEGO-Eval, an evaluation framework equipped with diverse tools designed to explicitly ground scene components, enabling more accurate alignment assessments. We also present LEGO-Bench, a benchmark of detailed instructions that specify complex layouts and attributes of real-world environments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge by 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with LEGO-Bench reveals significant limitations in current generation methods. Across all evaluated approaches, success rates reached at most 10% in generating scenes that fully align with fine-grained instructions."
    },
    {
      "title": "TWIST2：可扩展、便携、整体式人形机器人数据采集系统 (原标题: TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System)",
      "link": "https://arxiv.org/abs/2511.02832",
      "pubDate": "Tue, 04 Nov 2025 13:58:35 GMT",
      "isoDate": "2025-11-04T13:58:35.000Z",
      "creator": "Yanjie Ze, Siheng Zhao, Weizhuo Wang, Angjoo Kanazawa, Rocky Duan, Pieter Abbeel, Guanya Shi, Jiajun Wu, C. Karen Liu",
      "summary": "# TWIST2：可扩展、便携、整体式人形机器人数据采集系统\n\n## 1. 背景与挑战\n*   大规模数据已推动机器人技术取得突破，例如语言模型和双臂操作中的视觉-语言-动作模型。\n*   然而，人形机器人领域缺乏同样有效的数据采集框架。\n*   现有的人形机器人遥操作系统存在局限性：\n    *   采用解耦控制。\n    *   依赖昂贵的运动捕捉（mocap）设备。\n\n## 2. TWIST2 系统介绍\n*   **TWIST2** 是一个便携、无需运动捕捉的人形机器人遥操作和数据采集系统。\n*   它在提高可扩展性的同时，保留了完整的人形机器人全身控制能力。\n\n## 3. 核心技术与组件\n*   **全身运动获取**：利用 PICO4U VR 设备实时获取人类全身运动。\n*   **自我中心视觉**：配备定制的2自由度机器人颈部（成本约250美元），用于实现自我中心视觉。\n*   **整体控制**：实现从人类到人形机器人的整体控制。\n\n## 4. 系统性能与成果\n*   **技能演示**：成功展示了长周期、灵巧且移动的人形机器人技能。\n*   **数据采集效率**：\n    *   能够在15分钟内收集100个演示。\n    *   成功率接近100%。\n\n## 5. 分层视觉运动策略框架\n*   基于 TWIST2 采集的数据，研究人员提出了一个分层视觉运动策略框架。\n*   该框架能够基于自我中心视觉自主控制人形机器人的整个身体。\n*   **策略演示**：成功展示了全身灵巧操作和动态踢球任务。\n\n## 6. 开源与可复现性\n*   整个系统是完全可复现的，并已开源。\n*   系统代码库：[this https URL](this https URL)\n*   采集的数据集也已开源：[this https URL](this https URL)",
      "shortSummary": "TWIST2是一个创新的人形机器人数据采集系统，解决了现有系统昂贵或控制解耦的问题。它利用PICO4U VR和定制机器人颈部，实现便携、无需运动捕捉的全身遥操作，并能高效收集数据（15分钟内100个演示，成功率近100%）。基于此，TWIST2提出了一个分层视觉运动策略，使机器人能自主完成灵巧操作和动态踢球任务。系统和数据集均已开源，具有高度可复现性。",
      "translated_title": "TWIST2：可扩展、便携、整体式人形机器人数据采集系统",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large-scale data has driven breakthroughs in robotics, from language models to vision-language-action models in bimanual manipulation. However, humanoid robotics lacks equally effective data collection frameworks. Existing humanoid teleoperation systems either use decoupled control or depend on expensive motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid teleoperation and data collection system that preserves full whole-body control while advancing scalability. Our system leverages PICO4U VR for obtaining real-time whole-body human motions, with a custom 2-DoF robot neck (cost around $250) for egocentric vision, enabling holistic human-to-humanoid control. We demonstrate long-horizon dexterous and mobile humanoid skills and we can collect 100 demonstrations in 15 minutes with an almost 100% success rate. Building on this pipeline, we propose a hierarchical visuomotor policy framework that autonomously controls the full humanoid body based on egocentric vision. Our visuomotor policy successfully demonstrates whole-body dexterous manipulation and dynamic kicking tasks. The entire system is fully reproducible and open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also open-sourced at https://twist-data.github.io ."
    },
    {
      "title": "Orion-MSP：用于表格上下文学习的多尺度稀疏注意力 (原标题: Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning)",
      "link": "https://arxiv.org/abs/2511.02818",
      "pubDate": "Tue, 04 Nov 2025 13:43:44 GMT",
      "isoDate": "2025-11-04T13:43:44.000Z",
      "creator": "Mohamed Bouadi, Pratinav Seth, Aditya Tanna, Vinay Kumar Sankarapu",
      "summary": "# Orion-MSP：用于表格上下文学习的多尺度稀疏注意力\n\n## 引言与背景\n表格数据是现实世界应用中最主要的数据格式。然而，由于特征类型异构以及多尺度上复杂的交互，开发有效的表格数据神经网络模型仍然面临挑战。\n\n## 现有表格上下文学习（ICL）模型的局限性\n近期在表格上下文学习（ICL）方面的进展，如TabPFN和TabICL，已在无需任务特定微调的情况下，取得了与梯度提升树（GBTs）相当的最新（SOTA）性能。然而，当前架构存在以下关键局限：\n*   **单尺度特征处理：** 忽略了数据中固有的层次依赖性。\n*   **密集注意力机制：** 在表格宽度上呈现二次方缩放，导致在高维表格上效率低下。\n*   **严格顺序的组件处理：** 阻碍了迭代的表示细化和跨组件的信息通信。\n\n## Orion-MSP的创新之处\n为解决上述挑战，本文引入了Orion-MSP，一个全新的表格ICL架构，其核心包含三项关键创新：\n1.  **多尺度处理：** 旨在捕获表格数据中复杂的层次特征交互。\n2.  **块稀疏注意力：** 结合了窗口式、全局和随机注意力模式，以实现可扩展的效率和长程连接能力。\n3.  **Perceiver风格内存：** 实现了组件间安全的双向信息流，促进更丰富的表示学习。\n\n## 性能与结论\n在各种基准测试中，Orion-MSP的性能与最先进的模型相当或超越，同时能够有效扩展到高维表格。这为高效的表格上下文学习树立了新标准。\n\n## 模型可用性\nOrion-MSP模型已公开可用。",
      "shortSummary": "Orion-MSP是一种创新的表格上下文学习（ICL）架构，旨在解决现有模型在处理表格数据时的局限性。它通过引入多尺度处理、块稀疏注意力机制和Perceiver风格内存，有效捕获层次特征交互、提高可扩展性并实现双向信息流。Orion-MSP在多个基准测试中达到了或超越了最先进的性能，并能高效处理高维表格，为表格ICL树立了新标准。",
      "translated_title": "Orion-MSP：用于表格上下文学习的多尺度稀疏注意力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Tabular data remain the predominant format for real-world applications. Yet, developing effective neural models for tabular data remains challenging due to heterogeneous feature types and complex interactions occurring at multiple scales. Recent advances in tabular in-context learning (ICL), such as TabPFN and TabICL, have achieved state-of-the-art performance comparable to gradient-boosted trees (GBTs) without task-specific fine-tuning. However, current architectures exhibit key limitations: (1) single-scale feature processing that overlooks hierarchical dependencies, (2) dense attention with quadratic scaling in table width, and (3) strictly sequential component processing that prevents iterative representation refinement and cross-component communication. To address these challenges, we introduce Orion-MSP, a tabular ICL architecture featuring three key innovations: (1) multi-scale processing to capture hierarchical feature interactions; (2) block-sparse attention combining windowed, global, and random patterns for scalable efficiency and long-range connectivity; and (3) a Perceiver-style memory enabling safe bidirectional information flow across components. Across diverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance while scaling effectively to high-dimensional tables, establishing a new standard for efficient tabular in-context learning. The model is publicly available at https://github.com/Lexsi-Labs/Orion-MSP ."
    },
    {
      "title": "TabTune：一个用于表格基础模型推理和微调的统一库 (原标题: TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models)",
      "link": "https://arxiv.org/abs/2511.02802",
      "pubDate": "Tue, 04 Nov 2025 13:25:17 GMT",
      "isoDate": "2025-11-04T13:25:17.000Z",
      "creator": "Aditya Tanna, Pratinav Seth, Mohamed Bouadi, Utsav Avaiya, Vinay Kumar Sankarapu",
      "summary": "## TabTune：表格基础模型推理与微调的统一库\n\n### 引言：表格基础模型面临的挑战\n\n表格基础模型（Tabular Foundation Models, TFMs）代表了结构化数据学习中日益增长的范式，将大规模预训练的优势扩展到表格领域。然而，由于以下几个关键问题，TFMs的实际应用和普及受到了限制：\n\n*   **异构的预处理流程：** 缺乏统一的数据预处理标准和工具。\n*   **碎片化的API：** 不同模型和框架之间API不一致，增加了开发难度。\n*   **不一致的微调程序：** 缺乏标准化的微调策略和流程。\n*   **缺乏标准化评估：** 针对校准（calibration）和公平性（fairness）等部署导向指标，缺乏统一的评估方法。\n\n### TabTune：解决方案\n\n为了解决上述挑战，本文提出了 **TabTune**，一个统一的库。TabTune旨在通过一个单一的接口，标准化表格基础模型的完整工作流程，从而简化其采用和部署。\n\n### TabTune 的核心功能与特点\n\nTabTune 提供了一系列功能，以实现表格基础模型的标准化和高效使用：\n\n1.  **统一接口与模型访问：**\n    *   提供对七种最先进的表格基础模型的统一、一致性访问。\n    *   在内部管理不同模型架构的异构性，对用户透明。\n\n2.  **多样的适应策略支持：**\n    *   **零样本推理 (Zero-shot inference)：** 无需额外训练即可直接应用模型。\n    *   **元学习 (Meta-learning)：** 学习如何快速适应新任务。\n    *   **监督式微调 (Supervised Fine-Tuning, SFT)：** 使用带标签数据对预训练模型进行微调。\n    *   **参数高效微调 (Parameter-Efficient Fine-Tuning, PEFT)：** 在微调过程中只更新少量参数，提高效率。\n\n3.  **自动化与标准化：**\n    *   **自动化模型感知的预处理：** 根据所选模型的特点自动进行数据预处理。\n    *   **集成评估模块：** 内置了用于评估模型性能、校准和公平性的模块，确保评估的一致性和全面性。\n\n4.  **设计原则：**\n    *   **可扩展性 (Extensibility)：** 方便未来集成新的模型和适应策略。\n    *   **可复现性 (Reproducibility)：** 确保实验结果可以被一致地复现。\n\n### TabTune 的价值\n\nTabTune 的设计旨在促进表格基础模型的广泛采用，并通过提供一个标准化的框架，实现对不同适应策略的一致性基准测试。这有助于研究人员和开发者更有效地比较和选择最适合其特定任务的表格基础模型和微调方法。\n\n### 可用性\n\nTabTune 是一个开源库，可在指定的GitHub仓库获取。",
      "shortSummary": "TabTune是一个统一的开源库，旨在标准化表格基础模型（TFMs）的完整工作流程。它解决了TFMs在预处理、API、微调和评估方面的碎片化问题，通过单一接口提供对七种先进模型的访问，并支持零样本推理、元学习、监督式微调和参数高效微调等多种适应策略。TabTune自动化模型感知预处理，集成性能、校准和公平性评估，从而促进TFMs的广泛采用和一致性基准测试。",
      "translated_title": "TabTune：一个用于表格基础模型推理和微调的统一库",
      "images": [],
      "contentSource": "完整文章",
      "content": "Tabular foundation models represent a growing paradigm in structured data learning, extending the benefits of large-scale pretraining to tabular domains. However, their adoption remains limited due to heterogeneous preprocessing pipelines, fragmented APIs, inconsistent fine-tuning procedures, and the absence of standardized evaluation for deployment-oriented metrics such as calibration and fairness. We present TabTune, a unified library that standardizes the complete workflow for tabular foundation models through a single interface. TabTune provides consistent access to seven state-of-the-art models supporting multiple adaptation strategies, including zero-shot inference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient fine-tuning (PEFT). The framework automates model-aware preprocessing, manages architectural heterogeneity internally, and integrates evaluation modules for performance, calibration, and fairness. Designed for extensibility and reproducibility, TabTune enables consistent benchmarking of adaptation strategies of tabular foundation models. The library is open source and available at https://github.com/Lexsi-Labs/TabTune ."
    }
  ],
  "lastUpdated": "2025-11-08T09:29:00.109Z"
}