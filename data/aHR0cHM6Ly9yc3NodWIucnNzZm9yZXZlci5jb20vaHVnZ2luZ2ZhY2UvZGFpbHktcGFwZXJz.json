{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "olmOCR 2：文档OCR的单元测试奖励 (原标题: olmOCR 2: Unit Test Rewards for Document OCR)",
      "link": "https://arxiv.org/abs/2510.19817",
      "pubDate": "Wed, 22 Oct 2025 13:53:02 GMT",
      "isoDate": "2025-10-22T13:53:02.000Z",
      "creator": "Jake Poznanski, Luca Soldaini, Kyle Lo",
      "summary": "## olmOCR 2：文档OCR的单元测试奖励\n\nolmOCR 2 是我们系列中最新的强大OCR系统，旨在将数字化打印文档（如PDF）转换为清晰、自然排序的纯文本。\n\n### 核心技术与训练\n\n*   **驱动模型**：olmOCR 2 由 olmOCR-2-7B-1025 驱动，这是一个专门的7B视觉语言模型（VLM）。\n*   **训练方法**：该模型采用可验证奖励的强化学习（RLVR）进行训练。\n*   **奖励机制**：奖励基于一套多样化的二元单元测试。\n\n### 单元测试的创建\n\n*   为了实现单元测试的大规模创建，我们开发了一个流水线。\n*   该流水线能够生成具有多样化和挑战性布局的合成文档。\n*   这些合成文档包含已知的真实HTML源代码和提取的测试用例。\n\n### 性能提升\n\n*   在这些测试用例上进行的强化学习训练，使得 olmOCR 2 在我们的英语OCR基准测试 olmOCR-Bench 上取得了最先进的性能。\n*   与早期版本相比，在**数学公式转换**、**表格解析**和**多列布局**方面取得了最大的改进。\n\n### 可用性\n\n*   该模型、数据和代码均以开放许可发布。",
      "shortSummary": "olmOCR 2是一个先进的文档OCR系统，利用专门的7B视觉语言模型olmOCR-2-7B-1025。它通过可验证奖励的强化学习（RLVR）进行训练，奖励机制基于多样化的二元单元测试。这些测试通过生成具有复杂布局的合成文档创建。olmOCR 2在OCR基准测试中表现出色，尤其在数学公式、表格解析和多列布局方面显著提升。模型、数据和代码均已开源。",
      "translated_title": "olmOCR 2：文档OCR的单元测试奖励",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present olmOCR 2, the latest in our family of powerful OCR systems for converting digitized print documents, like PDFs, into clean, naturally ordered plain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B vision language model (VLM) trained using reinforcement learning with verifiable rewards (RLVR), where our rewards are a diverse set of binary unit tests. To scale unit test creation, we develop a pipeline for generating synthetic documents with diverse and challenging layouts, known ground-truth HTML source code, and extracted test cases. We show that RL training on these test cases results in state-of-the-art performance on olmOCR-Bench, our English-language OCR benchmark, with the largest improvements in math formula conversion, table parsing, and multi-column layouts compared to previous versions. We release our model, data and code under permissive open licenses."
    },
    {
      "title": "Pico-Banana-400K：一个用于文本引导图像编辑的大规模数据集 (原标题: Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing)",
      "link": "https://arxiv.org/abs/2510.19808",
      "pubDate": "Wed, 22 Oct 2025 13:43:15 GMT",
      "isoDate": "2025-10-22T13:43:15.000Z",
      "creator": "Yusu Qian, Eli Bocek-Rivele, Liangchen Song, Jialing Tong, Yinfei Yang, Jiasen Lu, Wenze Hu, Zhe Gan",
      "summary": "# Pico-Banana-400K：一个用于文本引导图像编辑的大规模数据集\n\n## 引言\n\n近年来，多模态模型在文本引导图像编辑方面取得了显著进展，例如GPT-4o和Nano-Banana等系统已树立了新的基准。然而，由于缺乏基于真实图像构建的大规模、高质量且开放获取的数据集，研究社区的进展受到了限制。\n\n## Pico-Banana-400K 数据集介绍\n\n为了解决这一挑战，研究人员引入了 **Pico-Banana-400K**，这是一个包含40万张图像的综合数据集，专为基于指令的图像编辑而设计。\n\n### 数据集构建与特点\n\n*   **数据来源：** 该数据集通过利用Nano-Banana模型，从OpenImages集合中的真实照片生成多样化的编辑对。\n*   **质量与多样性：** Pico-Banana-400K与以往的合成数据集不同，它采用系统化的方法来确保数据的质量和多样性。\n    *   **细粒度分类：** 采用细粒度的图像编辑分类法，以确保全面覆盖各种编辑类型。\n    *   **内容与指令忠实性：** 通过基于多模态大语言模型（MLLM）的质量评分和精心策划，严格保持了内容的完整性和指令的忠实性。\n\n### 支持复杂编辑场景的专业子集\n\n除了单轮编辑，Pico-Banana-400K还支持对复杂编辑场景的研究，为此包含了三个专门的子集：\n\n1.  **多轮编辑集合 (7.2万示例)：** 用于研究连续修改中的序列编辑、推理和规划能力。\n2.  **偏好子集 (5.6万示例)：** 专为对齐研究和奖励模型训练而设计。\n3.  **长短编辑指令对：** 用于开发指令重写和摘要能力。\n\n## 结论与展望\n\n通过提供这一大规模、高质量且任务丰富的资源，Pico-Banana-400K为训练和评估下一代文本引导图像编辑模型奠定了坚实的基础。\n\n## 相关领域\n\n*   计算机视觉与模式识别 (cs.CV)\n*   计算与语言 (cs.CL)\n*   机器学习 (cs.LG)",
      "shortSummary": "Pico-Banana-400K是一个大规模、高质量的文本引导图像编辑数据集，旨在解决现有研究中真实图像数据集稀缺的问题。该数据集包含40万张图像，通过Nano-Banana从OpenImages的真实照片生成，并采用系统方法确保编辑类型多样性、内容保留和指令忠实性。它还包含多轮编辑、偏好学习和指令重写等专业子集，为训练和评估下一代图像编辑模型提供了坚实基础。",
      "translated_title": "Pico-Banana-400K：一个用于文本引导图像编辑的大规模数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in multimodal models have demonstrated remarkable text-guided image editing capabilities, with systems like GPT-4o and Nano-Banana setting new benchmarks. However, the research community's progress remains constrained by the absence of large-scale, high-quality, and openly accessible datasets built from real images. We introduce Pico-Banana-400K, a comprehensive 400K-image dataset for instruction-based image editing. Our dataset is constructed by leveraging Nano-Banana to generate diverse edit pairs from real photographs in the OpenImages collection. What distinguishes Pico-Banana-400K from previous synthetic datasets is our systematic approach to quality and diversity. We employ a fine-grained image editing taxonomy to ensure comprehensive coverage of edit types while maintaining precise content preservation and instruction faithfulness through MLLM-based quality scoring and careful curation. Beyond single turn editing, Pico-Banana-400K enables research into complex editing scenarios. The dataset includes three specialized subsets: (1) a 72K-example multi-turn collection for studying sequential editing, reasoning, and planning across consecutive modifications; (2) a 56K-example preference subset for alignment research and reward model training; and (3) paired long-short editing instructions for developing instruction rewriting and summarization capabilities. By providing this large-scale, high-quality, and task-rich resource, Pico-Banana-400K establishes a robust foundation for training and benchmarking the next generation of text-guided image editing models."
    },
    {
      "title": "Transformer何时学习图连通性的启发式算法？ (原标题: When Do Transformers Learn Heuristics for Graph Connectivity?)",
      "link": "https://arxiv.org/abs/2510.19753",
      "pubDate": "Wed, 22 Oct 2025 12:43:32 GMT",
      "isoDate": "2025-10-22T12:43:32.000Z",
      "creator": "Qilin Ye, Deqing Fu, Robin Jia, Vatsal Sharan",
      "summary": "本文探讨了Transformer模型在学习可泛化算法时常遇到的挑战，即它们倾向于依赖脆弱的启发式算法而非通用算法。研究以图连通性作为测试平台，从理论和实证两方面解释了这一现象。\n\n### 核心问题\n\n*   Transformer模型在学习复杂算法时，往往未能掌握可泛化的算法，而是倾向于学习不稳定的启发式规则。\n\n### 研究方法与模型\n\n*   **测试平台**：图连通性问题。\n*   **简化模型**：采用了一种简化的Transformer架构，即“解耦Transformer”（disentangled Transformer）。\n\n### 理论发现\n\n*   **模型容量**：一个L层的解耦Transformer模型，其容量足以解决直径（diameter）最大为 $3^L$ 的图的连通性问题。\n*   **算法等效性**：该模型实现的算法等同于计算邻接矩阵的幂。\n\n### 训练动态分析\n\n*   **学习策略的关键**：模型学习到的策略，关键在于大多数训练实例是否在其容量范围之内。\n*   **容量内图**：当训练数据中的图的直径在模型容量之内（即直径 $\\leq 3^L$）时，模型会学习到正确的算法解决方案。\n*   **容量外图**：当训练数据中的图的直径超出模型容量时，模型则会学习到一种基于节点度（node degrees）的简单启发式算法。\n\n### 实验验证\n\n*   **数据限制效果**：研究通过实验证明，将训练数据限制在模型容量范围之内，能够促使标准Transformer和解耦Transformer都学习到精确的算法，而非基于度的启发式算法。\n\n### 结论\n\n*   Transformer模型学习算法或启发式算法，与其训练数据的特性（特别是图的直径与模型容量的关系）密切相关。通过合理控制训练数据的复杂度，可以引导Transformer学习到更精确、可泛化的算法。",
      "shortSummary": "本文探讨了Transformer模型在图连通性任务中学习算法或启发式算法的机制。研究发现，一个L层解耦Transformer的容量可解决直径达 $3^L$ 的图。当训练图的直径在模型容量内时，模型学习精确算法；超出容量时，则学习基于节点度的启发式算法。实验证明，限制训练数据在模型容量内，能促使Transformer学习到精确的算法。",
      "translated_title": "Transformer何时学习图连通性的启发式算法？",
      "images": [],
      "contentSource": "完整文章",
      "content": "Transformers often fail to learn generalizable algorithms, instead relying on brittle heuristics. Using graph connectivity as a testbed, we explain this phenomenon both theoretically and empirically. We consider a simplified Transformer architecture, the disentangled Transformer, and prove that an L-layer model has capacity to solve for graphs with diameters up to exactly 3^L, implementing an algorithm equivalent to computing powers of the adjacency matrix. We analyze the training-dynamics, and show that the learned strategy hinges on whether most training instances are within this model capacity. Within-capacity graphs (diameter leq 3^L) drive the learning of a correct algorithmic solution while beyond-capacity graphs drive the learning of a simple heuristic based on node degrees. Finally, we empirically demonstrate that restricting training data within a model's capacity leads to both standard and disentangled transformers learning the exact algorithm rather than the degree-based heuristic."
    },
    {
      "title": "机器文本检测器是成员推断攻击 (原标题: Machine Text Detectors are Membership Inference Attacks)",
      "link": "https://arxiv.org/abs/2510.19492",
      "pubDate": "Wed, 22 Oct 2025 07:39:01 GMT",
      "isoDate": "2025-10-22T07:39:01.000Z",
      "creator": "Ryuto Koike, Liam Dugan, Masahiro Kaneko, Chris Callison-Burch, Naoaki Okazaki",
      "summary": "本文深入探讨了机器生成文本检测与成员推断攻击（MIA）之间的内在联系，尽管它们的目标不同，但其方法论基础相似，都依赖于语言模型的概率分布信号。\n\n### 研究背景与问题\n*   **独立研究：** 机器生成文本检测和成员推断攻击（MIA）这两个领域长期以来一直独立研究。\n*   **潜在缺陷：** 这种独立性可能导致研究人员忽视了对方领域中更有效的方法和宝贵的见解。\n\n### 理论贡献\n*   **最优度量标准：** 作者证明了在机器生成文本检测和MIA这两个任务上，能够实现渐近最高性能的度量标准是相同的。\n*   **文献统一：** 现有大量文献被统一到这个最优度量标准的背景下，并提出假设：给定方法近似该度量标准的准确性与其在任务间的可迁移性直接相关。\n\n### 实证贡献\n*   **大规模实验：** 进行了大规模的实证实验，涵盖了：\n    *   7种最先进的MIA方法\n    *   5种最先进的机器文本检测器\n    *   13个不同的领域\n    *   10个不同的文本生成器\n*   **强相关性：** 实验结果显示，跨任务性能之间存在非常强的等级相关性（rho > 0.6）。\n*   **关键发现：** 最初为机器文本检测设计的Binoculars方法，在MIA基准测试中也取得了最先进的性能，有力地证明了这种可迁移性的实际影响。\n\n### 研究意义与展望\n*   **跨任务协作：** 强调了两个研究社区之间需要加强跨任务意识和协作。\n*   **MINT评估套件：** 为了促进跨任务开发和公平评估，本文引入了MINT，一个统一的MIA和机器生成文本检测评估套件，其中包含了来自两个任务的15种最新方法的实现。",
      "shortSummary": "本文研究发现，机器文本检测器与成员推断攻击（MIA）在方法论上高度相关，两者都利用语言模型的概率分布。研究理论证明了在两个任务上存在相同的最优性能度量标准，并通过大规模实验证实了强烈的跨任务可迁移性（例如，Binoculars在MIA任务中表现出色）。这强调了两个研究领域加强协作的必要性，并为此引入了MINT统一评估套件。",
      "translated_title": "机器文本检测器是成员推断攻击",
      "images": [],
      "contentSource": "完整文章",
      "content": "Although membership inference attacks (MIAs) and machine-generated text detection target different goals, identifying training samples and synthetic texts, their methods often exploit similar signals based on a language model's probability distribution. Despite this shared methodological foundation, the two tasks have been independently studied, which may lead to conclusions that overlook stronger methods and valuable insights developed in the other task. In this work, we theoretically and empirically investigate the transferability, i.e., how well a method originally developed for one task performs on the other, between MIAs and machine text detection. For our theoretical contribution, we prove that the metric that achieves the asymptotically highest performance on both tasks is the same. We unify a large proportion of the existing literature in the context of this optimal metric and hypothesize that the accuracy with which a given method approximates this metric is directly correlated with its transferability. Our large-scale empirical experiments, including 7 state-of-the-art MIA methods and 5 state-of-the-art machine text detectors across 13 domains and 10 generators, demonstrate very strong rank correlation (rho &gt; 0.6) in cross-task performance. We notably find that Binoculars, originally designed for machine text detection, achieves state-of-the-art performance on MIA benchmarks as well, demonstrating the practical impact of the transferability. Our findings highlight the need for greater cross-task awareness and collaboration between the two research communities. To facilitate cross-task developments and fair evaluations, we introduce MINT, a unified evaluation suite for MIAs and machine-generated text detection, with implementation of 15 recent methods from both tasks."
    },
    {
      "title": "VideoAgentTrek：从无标签视频中进行计算机使用预训练 (原标题: VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos)",
      "link": "https://arxiv.org/abs/2510.19488",
      "pubDate": "Wed, 22 Oct 2025 07:25:48 GMT",
      "isoDate": "2025-10-22T07:25:48.000Z",
      "creator": "Dunjie Lu, Yiheng Xu, Junli Wang, Haoyuan Wu, Xinyuan Wang, Zekun Wang, Junlin Yang, Hongjin Su, Jixuan Chen, Junda Chen, Yuchen Mao, Jingren Zhou, Junyang Lin, Binyuan Hui, Tao Yu",
      "summary": "## VideoAgentTrek：从无标签视频中进行计算机使用预训练\n\n### 引言\n训练能够操作计算机图形用户界面（GUI）的代理（computer-use agents）需要海量的GUI交互数据。然而，手动标注这些动作轨迹的成本极高，难以大规模实现。本文提出了 **VideoAgentTrek**，这是一个可扩展的流水线，旨在自动从网络上公开的屏幕录制视频中挖掘训练数据，从而彻底消除对人工标注的需求。\n\n### 核心挑战\n原始的屏幕录制视频虽然包含了隐式的操作演示，但缺乏明确的动作标签，这使得直接用于训练代理变得困难。\n\n### 解决方案：Video2Action (V2A) 模块\n为解决上述挑战，VideoAgentTrek 开发了 **Video2Action (V2A)** 模块，这是一个逆动力学模块（Inverse Dynamics Module, IDM），它包含两个关键组件：\n\n1.  **视频定位模型 (Video Grounding Model)**：\n    *   该模型负责检测和定位视频中的GUI动作。\n    *   它能够提供精确的动作时间边界和上下文信息。\n2.  **动作内容识别器 (Action-Content Recognizer)**：\n    *   此组件能够高精度地提取结构化参数，例如用户点击的精确坐标和输入的文本内容。\n\n### 数据生成与应用\n*   研究团队将 VideoAgentTrek 流水线应用于 39,000 个公开的 YouTube 教程视频。\n*   通过自动化处理，该流水线成功生成了 152 万个交互步骤数据。\n\n### 训练策略\n生成的数据被用于以下训练策略：\n*   **持续预训练 (continued pretraining)**：利用大规模自动生成的数据对模型进行预训练。\n*   **监督微调 (supervised fine-tuning)**：在预训练之后，进行监督微调以进一步优化模型性能。\n\n### 实验结果\nVideoAgentTrek 方法在多个基准测试中展现了显著的性能提升：\n\n*   **在 OSWorld-Verified 基准测试中**：\n    *   任务成功率从仅使用监督微调的基线模型的 9.3% 提高到 15.8%。\n    *   实现了 70% 的相对提升。\n*   **在 AgentNetBench 基准测试中**：\n    *   步骤准确率从 64.1% 提高到 69.3%。\n\n### 结论\n这些结果有力地证明，被动的互联网视频可以被有效地转化为高质量的监督信号，用于训练计算机使用代理。VideoAgentTrek 提供了一个可扩展且高效的替代方案，以取代传统上昂贵且耗时的人工标注过程。",
      "shortSummary": "VideoAgentTrek 提出了一种从无标签屏幕录制视频中自动生成计算机使用代理训练数据的方法。通过 Video2Action 模块，它能从 39,000 个 YouTube 视频中提取 152 万个交互步骤。将这些数据用于预训练和微调后，在 OSWorld-Verified 上任务成功率从 9.3% 提升至 15.8%（相对提升 70%），在 AgentNetBench 上步骤准确率从 64.1% 提升至 69.3%。该方法为计算机使用代理训练提供了可扩展的自动化数据生成方案，替代了昂贵的手动标注。",
      "translated_title": "VideoAgentTrek：从无标签视频中进行计算机使用预训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "Training computer-use agents requires massive amounts of GUI interaction data, but manually annotating action trajectories at scale is prohibitively expensive. We present VideoAgentTrek, a scalable pipeline that automatically mines training data from publicly available screen-recorded videos at web scale, eliminating the need for manual annotation. Our approach addresses a key challenge: raw videos contain implicit demonstrations but lack explicit action labels. To solve this, we develop Video2Action, an inverse dynamics module (IDM) with two components: (1) a video grounding model that detects and localizes GUI actions with precise temporal boundaries and context, and (2) an action-content recognizer that extracts structured parameters like click coordinates and typed text with high fidelity. Applied to 39,000 YouTube tutorial videos, our pipeline generates 1.52 million interaction steps automatically. We leverage this data through continued pretraining followed by supervised fine-tuning. On OSWorld-Verified, our approach improves task success rates from 9.3% (SFT-only baseline) to 15.8%, a 70% relative improvement. On AgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results demonstrate that passive internet videos can be transformed into high-quality supervision for computer-use agents, providing a scalable alternative to expensive manual annotation."
    },
    {
      "title": "MINED：探测和更新大型多模态模型中的多模态时效性知识 (原标题: MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for Large Multimodal Models)",
      "link": "https://arxiv.org/abs/2510.19457",
      "pubDate": "Wed, 22 Oct 2025 06:41:57 GMT",
      "isoDate": "2025-10-22T06:41:57.000Z",
      "creator": "Kailin Jiang, Ning Jiang, Yuchen Ren, Yuchen Li, Yifan Gao, Jinhe Bi, Yunpu Ma, Qingqing Liu, Xianhao Wang, Yifan Jia, Hongbo Jiang, Yaocong Hu, Bin Li, Lei Liu, Yuntao Du",
      "summary": "### MINED：大型多模态模型时效性知识探测与更新基准\n\n**引言**\n大型多模态模型（LMMs）通过跨模态预训练编码了丰富的知识，但其静态的知识表示难以准确理解和维护时效性知识。现有基准在评估LMMs理解时效性知识的能力方面存在局限，无法充分反映其处理时间敏感信息的真实水平。\n\n**MINED基准的提出**\n为解决LMMs在处理时效性知识方面的不足，并提供一个更全面的评估工具，研究人员提出了MINED。这是一个综合性的基准，旨在深入评估LMMs在时效性知识方面的能力。\n\n**MINED的特点与构成**\nMINED基准的设计旨在多维度、多任务地评估LMMs的时间感知能力：\n\n*   **评估维度**：MINED从6个关键维度评估LMMs的时间感知能力，以全面考察其对时效性知识的理解和处理：\n    *   **认知 (cognition)**：模型对时效性事实的识别和理解能力。\n    *   **意识 (awareness)**：模型对知识时间敏感性的感知能力。\n    *   **可信度 (trustworthiness)**：模型在处理时效性信息时保持准确性和可靠性的能力。\n    *   **理解 (understanding)**：模型对时效性知识内容的深层理解。\n    *   **推理 (reasoning)**：模型基于时效性知识进行逻辑推理的能力。\n    *   **鲁棒性 (robustness)**：模型在面对时效性知识变化时的稳定性和适应性。\n*   **任务数量**：基准包含11项具有挑战性的任务，这些任务旨在从不同角度测试LMMs处理时效性知识的各项能力。\n*   **数据来源与规模**：MINED由两名专业标注员从维基百科构建，确保了数据的质量和真实性。它包含2,104个时效性知识样本，这些样本覆盖了广泛的时间敏感信息。\n*   **知识类型**：数据集涵盖了六种不同的知识类型，以确保评估的全面性和多样性。\n\n**LMMs在MINED上的评估结果**\n研究人员在MINED上对15个广泛使用的LMMs进行了评估，揭示了当前LMMs在时效性知识处理方面的表现：\n\n*   **最佳表现**：Gemini-2.5-Pro取得了最高的平均CEM分数，达到63.07，表明其在处理时效性知识方面具有相对优势。\n*   **普遍表现**：大多数开源LMMs在时间理解能力方面仍然不足，这凸显了该领域仍有很大的改进空间。\n*   **知识类型表现差异**：\n    *   LMMs在“组织”类知识（如公司成立、机构变动等）上表现最佳，可能因为这类信息通常结构化且更新频率相对较低。\n    *   在“体育”类知识（如比赛结果、运动员表现等）上的表现最弱，这可能与体育事件的快速变化和大量非结构化信息有关。\n\n**时效性知识更新的探索**\n为应对LMMs在时效性知识方面的挑战，研究人员进一步探索了通过知识编辑方法更新LMMs中时效性知识的可行性：\n\n*   **研究目的**：旨在验证LMMs是否能够通过外部干预有效地吸收和更新最新的时效性信息。\n*   **研究发现**：在单一编辑场景下，LMMs能够通过知识编辑方法有效地更新知识。这一发现为未来LMMs的动态知识维护提供了有前景的方向，表明通过有针对性的编辑，可以提高模型对最新信息的响应能力。\n\n**结论**\nMINED基准的提出和评估结果揭示了当前LMMs在处理时效性知识方面的不足，并为未来的研究提供了明确的方向。初步探索表明，知识编辑是更新LMMs时效性知识的有效途径，为构建更具时效性和适应性的LMMs奠定了基础。",
      "shortSummary": "大型多模态模型（LMMs）难以处理时效性知识。为评估此能力，研究人员提出了MINED基准，涵盖6个维度和11项任务，包含2,104个时效性知识样本。对15个LMMs的评估显示，Gemini-2.5-Pro表现最佳（63.07分），而多数开源模型仍缺乏时间理解能力，且在“体育”类知识上表现最弱。研究还发现，LMMs可通过知识编辑有效更新时效性知识，为未来模型维护提供了方向。",
      "translated_title": "MINED：探测和更新大型多模态模型中的多模态时效性知识",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal pre-training, yet their static representations struggle to maintain an accurate understanding of time-sensitive factual knowledge. Existing benchmarks remain constrained by static designs, inadequately evaluating LMMs' ability to understand time-sensitive knowledge. To address this gap, we propose MINED, a comprehensive benchmark that evaluates temporal awareness along 6 key dimensions and 11 challenging tasks: cognition, awareness, trustworthiness, understanding, reasoning, and robustness. MINED is constructed from Wikipedia by two professional annotators, containing 2,104 time-sensitive knowledge samples spanning six knowledge types. Evaluating 15 widely used LMMs on MINED shows that Gemini-2.5-Pro achieves the highest average CEM score of 63.07, while most open-source LMMs still lack time understanding ability. Meanwhile, LMMs perform best on organization knowledge, whereas their performance is weakest on sport. To address these challenges, we investigate the feasibility of updating time-sensitive knowledge in LMMs through knowledge editing methods and observe that LMMs can effectively update knowledge via knowledge editing methods in single editing scenarios."
    },
    {
      "title": "GigaBrain-0：一个由世界模型驱动的视觉-语言-动作模型 (原标题: GigaBrain-0: A World Model-Powered Vision-Language-Action Model)",
      "link": "https://arxiv.org/abs/2510.19430",
      "pubDate": "Wed, 22 Oct 2025 05:57:13 GMT",
      "isoDate": "2025-10-22T05:57:13.000Z",
      "creator": "GigaBrain Team, Angen Ye, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Haoyun Li, Jie Li, Jiagang Zhu, Lv Feng, Peng Li, Qiuping Deng, Runqi Ouyang, Wenkang Qin, Xinze Chen, Xiaofeng Wang, Yang Wang, Yifan Li, Yilong Li, Yiran Ding, Yuan Xu, Yun Ye, Yukun Zhou, Zhehao Dong, Zhenan Wang, Zhichao Liu, Zheng Zhu",
      "summary": "### GigaBrain-0：一个由世界模型驱动的视觉-语言-动作模型\n\n本文介绍了GigaBrain-0，一个旨在解决通用机器人视觉-语言-动作（VLA）模型训练中数据收集挑战的新型基础模型。\n\n**核心问题：**\n*   训练通用VLA模型通常需要大规模的真实世界机器人数据。\n*   真实数据收集成本高昂且耗时，严重限制了当前VLA系统的可扩展性和泛化能力。\n\n**GigaBrain-0的解决方案：**\n*   **世界模型驱动的数据生成：** GigaBrain-0利用世界模型生成多样化且大规模的数据，包括：\n    *   视频生成\n    *   真实到真实（real2real）迁移\n    *   人类行为迁移\n    *   视角迁移\n    *   模拟到真实（sim2real）迁移\n*   **减少对真实数据的依赖：** 通过利用生成数据，GigaBrain-0显著减少了对昂贵真实机器人数据的需求。\n*   **提升泛化能力：** 该方法显著提高了模型在不同任务间的泛化能力。\n\n**关键技术和优势：**\n*   **RGBD输入建模：** 通过处理RGBD（彩色深度）输入，模型能够更好地理解空间几何。\n*   **具身思维链（CoT）监督：** 引入具身CoT监督，使模型能够在任务执行过程中推理：\n    *   空间几何\n    *   物体状态\n    *   长程依赖关系\n*   **策略鲁棒性：** 这些技术共同提升了策略的鲁棒性。\n\n**性能表现：**\n*   GigaBrain-0在灵巧操作、长程任务和移动操作等真实世界任务中取得了显著的性能提升。\n*   在外观（如纹理、颜色）、物体放置和摄像机视角变化方面，展现出卓越的泛化能力。\n\n**轻量级变体：**\n*   GigaBrain-0-Small是一个优化后的轻量级版本，专为在NVIDIA Jetson AGX Orin等边缘设备上高效运行而设计。",
      "shortSummary": "GigaBrain-0是一个由世界模型驱动的视觉-语言-动作（VLA）基础模型，旨在解决通用机器人训练中真实数据收集成本高昂的问题。它通过利用世界模型生成大规模多样化数据，显著减少了对真实机器人数据的依赖，并提升了跨任务泛化能力。结合RGBD输入建模和具身思维链监督，GigaBrain-0增强了策略鲁棒性，并在灵巧、长程和移动操作等真实世界任务中取得了卓越表现，同时还推出了适用于边缘设备的轻量级版本GigaBrain-0-Small。",
      "translated_title": "GigaBrain-0：一个由世界模型驱动的视觉-语言-动作模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Training Vision-Language-Action (VLA) models for generalist robots typically requires large-scale real-world robot data, which is expensive and time-consuming to collect. The inefficiency of physical data collection severely limits the scalability, and generalization capacity of current VLA systems. To address this challenge, we introduce GigaBrain-0, a novel VLA foundation model empowered by world model-generated data (e.g., video generation, real2real transfer, human transfer, view transfer, sim2real transfer data). By leveraging world models to generate diverse data at scale, GigaBrain-0 significantly reduces reliance on real robot data while improving cross-task generalization. Our approach further improves policy robustness through RGBD input modeling and embodied Chain-of-Thought (CoT) supervision, enabling the model to reason about spatial geometry, object states, and long-horizon dependencies during task execution. This leads to substantial gains in real-world performance on dexterous, long-horizon, and mobile manipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves superior generalization across variations in appearances (e.g., textures, colors), object placements, and camera viewpoints. Additionally, we present GigaBrain-0-Small, an optimized lightweight variant designed to run efficiently on devices such as the NVIDIA Jetson AGX Orin."
    },
    {
      "title": "ColorAgent：构建一个鲁棒、个性化和交互式的操作系统代理 (原标题: ColorAgent: Building A Robust, Personalized, and Interactive OS Agent)",
      "link": "https://arxiv.org/abs/2510.19386",
      "pubDate": "Wed, 22 Oct 2025 05:02:48 GMT",
      "isoDate": "2025-10-22T05:02:48.000Z",
      "creator": "Ning Li, Qiqiang Lin, Zheng Wu, Xiaoyun Mo, Weiming Zhang, Yin Zhao, Xiangmou Qu, Jiamu Zhou, Jun Wang, Congmin Zheng, Yuanyi Song, Hongjiang Chen, Heyuan Huang, Jihong Wang, Jiaxin Yin, Jingwei Yu, Junwei Liao, Qiuying Peng, Xingyu Lou, Jun Wang, Weiwen Liu, Zhuosheng Zhang, Weinan Zhang",
      "summary": "## ColorAgent：构建一个鲁棒、个性化和交互式的操作系统代理\n\n### 引言\n随着硬件、软件和大型语言模型技术的进步，人机与操作系统（OS）的交互方式已从传统的命令行界面发展到新兴的AI代理交互。构建一个能够准确执行用户指令并忠实遵循用户意愿的OS代理正逐渐成为现实。\n\n### ColorAgent 概述\n本文介绍了一个名为 ColorAgent 的OS代理。其设计目标是：\n*   与环境进行长期、鲁棒的交互。\n*   实现个性化和主动的用户交互。\n\n### 核心功能与方法\nColorAgent 通过以下关键策略实现其目标：\n\n1.  **实现长期环境交互：**\n    *   **模型能力增强：** 通过分步强化学习和自演化训练来提升模型能力。\n    *   **多代理框架：** 开发了一个定制的多代理框架，以确保系统的通用性、一致性和鲁棒性。\n\n2.  **优化用户交互：**\n    *   **个性化意图识别：** 探索并实现了对用户意图的个性化识别。\n    *   **主动参与：** 能够主动与用户进行互动。\n    *   **角色定位：** 将OS代理定位为“温暖、协作的伙伴”，而非仅仅是一个自动化工具。\n\n### 性能评估\nColorAgent 在以下基准测试中进行了评估，并取得了显著成果：\n*   **AndroidWorld：** 成功率达到 77.2%。\n*   **AndroidLab：** 成功率达到 50.7%。\n\n这些结果表明 ColorAgent 在相关领域建立了新的技术水平（State of the Art, SOTA）。\n\n### 局限性与未来工作\n尽管取得了优异的成绩，作者也指出：\n*   当前的基准测试不足以全面评估OS代理的复杂性。\n*   未来的研究方向将集中在：\n    *   改进评估范式。\n    *   增强代理间的协作能力。\n    *   提升系统的安全性。\n\n### 代码可用性\nColorAgent 的代码已公开。",
      "shortSummary": "ColorAgent 是一个鲁棒、个性化且交互式的操作系统代理。它通过分步强化学习、自演化训练和定制的多代理框架，实现了长期环境交互。在用户交互方面，ColorAgent 专注于个性化意图识别和主动参与，旨在成为用户的协作伙伴。它在 AndroidWorld 和 AndroidLab 基准测试中分别取得了 77.2% 和 50.7% 的成功率，达到了新的技术水平。作者指出当前基准测试仍有不足，并提出了未来在评估、协作和安全性方面的研究方向。",
      "translated_title": "ColorAgent：构建一个鲁棒、个性化和交互式的操作系统代理",
      "images": [],
      "contentSource": "完整文章",
      "content": "With the advancements in hardware, software, and large language model technologies, the interaction between humans and operating systems has evolved from the command-line interface to the rapidly emerging AI agent interactions. Building an operating system (OS) agent capable of executing user instructions and faithfully following user desires is becoming a reality. In this technical report, we present ColorAgent, an OS agent designed to engage in long-horizon, robust interactions with the environment while also enabling personalized and proactive user interaction. To enable long-horizon interactions with the environment, we enhance the model's capabilities through step-wise reinforcement learning and self-evolving training, while also developing a tailored multi-agent framework that ensures generality, consistency, and robustness. In terms of user interaction, we explore personalized user intent recognition and proactive engagement, positioning the OS agent not merely as an automation tool but as a warm, collaborative partner. We evaluate ColorAgent on the AndroidWorld and AndroidLab benchmarks, achieving success rates of 77.2% and 50.7%, respectively, establishing a new state of the art. Nonetheless, we note that current benchmarks are insufficient for a comprehensive evaluation of OS agents and propose further exploring directions in future work, particularly in the areas of evaluation paradigms, agent collaboration, and security. Our code is available at https://github.com/MadeAgents/mobile-use."
    },
    {
      "title": "LoongRL：基于强化学习的长上下文高级推理 (原标题: LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts)",
      "link": "https://arxiv.org/abs/2510.19363",
      "pubDate": "Wed, 22 Oct 2025 04:35:28 GMT",
      "isoDate": "2025-10-22T04:35:28.000Z",
      "creator": "Siyuan Wang, Gaokai Zhang, Li Lyna Zhang, Ning Shang, Fan Yang, Dongyao Chen, Mao Yang",
      "summary": "# LoongRL：基于强化学习的长上下文高级推理\n\n## 引言\n大型语言模型（LLMs）在处理长上下文推理方面面临挑战。尽管强化学习（RL）已通过在思维链中引入“顿悟”时刻来增强短上下文推理能力，但针对长上下文推理所需的更高级思维模式仍未被充分探索，且高难度RL数据稀缺。\n\n## LoongRL 方法\n本文介绍 LoongRL，一种数据驱动的强化学习方法，专为高级长上下文推理设计。\n\n### 核心组件：KeyChain\nLoongRL 的核心是 KeyChain，这是一种合成方法，它通过以下方式将短的多跳问答（multi-hop QA）任务转化为高难度的长上下文任务：\n*   在大量干扰文档中插入通用唯一标识符（UUID）链，以隐藏真实问题。\n*   解决这些 KeyChain 任务需要模型执行以下步骤：\n    1.  逐步追踪正确的链条。\n    2.  识别真实问题。\n    3.  检索相关事实。\n    4.  基于检索到的事实进行推理以正确回答。\n\n## LoongRL 的训练与效果\n*   在 KeyChain 数据上进行的RL训练，能够诱导模型产生一种新兴的“规划-检索-推理-复查”（plan-retrieve-reason-recheck）推理模式。\n*   这种推理模式具有强大的泛化能力，远超训练长度。\n*   模型在16K上下文长度上训练后，能够有效解决128K上下文长度的任务，且无需高昂的全长RL推理成本。\n\n## 实验结果与性能提升\nLoongRL 在多个方面展现了显著的性能提升：\n*   **多跳问答准确性**：在 Qwen2.5-7B 和 14B 模型上，LoongRL 显著提升了长上下文多跳问答的准确性：\n    *   Qwen2.5-7B 模型的绝对增益为 +23.5%。\n    *   Qwen2.5-14B 模型的绝对增益为 +21.1%。\n*   **模型竞争力**：LoongRL-14B 模型的得分达到 74.2，其性能表现与更大型的前沿模型（如 o3-mini 的 74.5 和 DeepSeek-R1 的 74.9）相当。\n*   **其他优势**：\n    *   改进了长上下文检索能力。\n    *   通过了所有 128K “大海捞针”（needle-in-a-haystack）压力测试。\n    *   保留了短上下文推理能力。\n\n## 领域与引用\n该研究属于计算与语言（cs.CL）领域，可引用为 arXiv:2510.19363 [cs.CL]。",
      "shortSummary": "LoongRL 是一种基于强化学习的长上下文高级推理方法。它通过 KeyChain 机制将短多跳问答转化为高难度长上下文任务，训练模型形成“规划-检索-推理-复查”模式。LoongRL 在16K上下文训练后能有效解决128K任务，显著提升了Qwen2.5-7B和14B模型在长上下文多跳问答上的准确性（分别提升23.5%和21.1%）。LoongRL-14B性能媲美大型前沿模型，并增强了长上下文检索和“大海捞针”测试表现，同时保留了短上下文推理能力。",
      "translated_title": "LoongRL：基于强化学习的长上下文高级推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reasoning over long contexts is essential for large language models. While reinforcement learning (RL) enhances short-context reasoning by inducing \"Aha\" moments in chain-of-thought, the advanced thinking patterns required for long-context reasoning remain largely unexplored, and high-difficulty RL data are scarce. In this paper, we introduce LoongRL, a data-driven RL method for advanced long-context reasoning. Central to LoongRL is KeyChain, a synthesis approach that transforms short multi-hop QA into high-difficulty long-context tasks by inserting UUID chains that hide the true question among large collections of distracting documents. Solving these tasks requires the model to trace the correct chain step-by-step, identify the true question, retrieve relevant facts and reason over them to answer correctly. RL training on KeyChain data induces an emergent plan-retrieve-reason-recheck reasoning pattern that generalizes far beyond training length. Models trained at 16K effectively solve 128K tasks without prohibitive full-length RL rollout costs. On Qwen2.5-7B and 14B, LoongRL substantially improves long-context multi-hop QA accuracy by +23.5% and +21.1% absolute gains. The resulting LoongRL-14B reaches a score of 74.2, rivaling much larger frontier models such as o3-mini (74.5) and DeepSeek-R1 (74.9). It also improves long-context retrieval, passes all 128K needle-in-a-haystack stress tests, and preserves short-context reasoning capabilities."
    },
    {
      "title": "每个注意力都重要：一种用于长上下文推理的高效混合架构 (原标题: Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning)",
      "link": "https://arxiv.org/abs/2510.19338",
      "pubDate": "Wed, 22 Oct 2025 03:59:38 GMT",
      "isoDate": "2025-10-22T03:59:38.000Z",
      "creator": "Ling Team, Bin Han, Caizhi Tang, Chen Liang, Donghao Zhang, Fan Yuan, Feng Zhu, Jie Gao, Jingyu Hu, Longfei Li, Meng Li, Mingyang Zhang, Peijie Jiang, Peng Jiao, Qian Zhao, Qingyuan Yang, Wenbo Shen, Xinxing Yang, Yalin Zhang, Yankun Ren, Yao Zhao, Yibo Cao, Yixuan Sun, Yue Zhang, Yuchen Fang, Zibin Lin, Zixuan Cheng, Jun Zhou",
      "summary": "## Ring-linear 模型系列：高效混合架构实现长上下文推理\n\n本技术报告介绍了 **Ring-linear 模型系列**，包括 **Ring-mini-linear-2.0** 和 **Ring-flash-linear-2.0**，旨在解决长上下文推理中的I/O和计算开销问题。\n\n### 模型概览\n\n*   **Ring-mini-linear-2.0**：包含160亿参数和9.57亿激活。\n*   **Ring-flash-linear-2.0**：包含1040亿参数和61亿激活。\n\n### 核心架构与优势\n\n1.  **混合注意力机制**：\n    *   模型采用了一种创新的混合架构，有效整合了线性注意力（linear attention）和softmax注意力（softmax attention）。\n    *   这种设计显著降低了在长上下文推理场景中的I/O和计算开销。\n\n2.  **成本效益显著**：\n    *   与一个320亿参数的密集模型相比，Ring-linear 系列的推理成本降低了十分之九。\n    *   与原始的Ring系列模型相比，成本也降低了超过50%。\n\n3.  **优化模型结构**：\n    *   通过系统性地探索混合架构中不同注意力机制的比例，研究人员确定了当前最优的模型结构。\n\n4.  **训练效率提升**：\n    *   利用团队自主开发的高性能FP8算子库——**linghe**，整体训练效率提高了50%。\n\n5.  **持续高性能表现**：\n    *   由于训练和推理引擎算子之间的高度对齐，模型在强化学习阶段能够进行长期、稳定且高效的优化。\n    *   这使得模型在多个具有挑战性的复杂推理基准测试中，持续保持最先进（SOTA）的性能。",
      "shortSummary": "本报告介绍了Ring-linear模型系列（Ring-mini-linear-2.0和Ring-flash-linear-2.0），该系列采用混合注意力架构，有效结合线性注意力和softmax注意力。与320亿参数密集模型相比，其推理成本降低至十分之一；与原始Ring系列相比，成本降低超50%。通过优化注意力机制比例和使用自研FP8算子库“linghe”，训练效率提升50%。模型在长上下文推理中展现出SOTA性能，显著降低了I/O和计算开销。",
      "translated_title": "每个注意力都重要：一种用于长上下文推理的高效混合架构",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this technical report, we present the Ring-linear model series, specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0. Ring-mini-linear-2.0 comprises 16B parameters and 957M activations, while Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both models adopt a hybrid architecture that effectively integrates linear attention and softmax attention, significantly reducing I/O and computational overhead in long-context inference scenarios. Compared to a 32 billion parameter dense model, this series reduces inference cost to 1/10, and compared to the original Ring series, the cost is also reduced by over 50%. Furthermore, through systematic exploration of the ratio between different attention mechanisms in the hybrid architecture, we have identified the currently optimal model structure. Additionally, by leveraging our self-developed high-performance FP8 operator library-linghe, overall training efficiency has been improved by 50%. Benefiting from the high alignment between the training and inference engine operators, the models can undergo long-term, stable, and highly efficient optimization during the reinforcement learning phase, consistently maintaining SOTA performance across multiple challenging complex reasoning benchmarks."
    },
    {
      "title": "DaMo：用于移动电话智能体的多模态大语言模型微调中的数据混合优化器 (原标题: DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents)",
      "link": "https://arxiv.org/abs/2510.19336",
      "pubDate": "Wed, 22 Oct 2025 03:57:59 GMT",
      "isoDate": "2025-10-22T03:57:59.000Z",
      "creator": "Kai Shi, Jun Yang, Ni Yang, Binqiang Pan, Qingsong Xie, Chao Zhang, Zhenyu Yang, Tianhuang Su, Haonan Lu",
      "summary": "## DaMo：用于移动电话智能体的多模态大语言模型微调中的数据混合优化器\n\n### 引言\n\n移动电话智能体（Mobile Phone Agents, MPAs）因其在各种场景中的广泛适用性，已成为一个极具前景的研究方向。多模态大语言模型（Multimodal Large Language Models, MLLMs）是MPAs的基础，然而，它们在同时处理多个移动电话任务时的有效性仍然有限。尽管多任务监督微调（Supervised Fine-Tuning, SFT）被广泛用于多任务学习，但现有方法难以确定最佳的训练数据组成以实现峰值性能。\n\n### DaMo：数据混合优化器\n\n为了解决上述挑战，我们提出了 **DaMo（Data Mixture Optimizer）**——一种新颖的解决方案，它采用一个可训练的网络。DaMo 的核心机制是通过预测给定数据集比例下的下游任务性能，从而确定最佳的数据混合比例。\n\n### PhoneAgentBench：新的评估基准\n\n为了支持全面的评估，我们引入了 **PhoneAgentBench**，这是第一个专门用于评估 MLLMs 在多模态移动电话任务上的基准。PhoneAgentBench 包含1235个问答对，涵盖了多样化的真实世界工业移动应用场景。\n\n### 实验结果与性能\n\nDaMo 在多项实验中展现出卓越的性能和泛化能力：\n\n*   **预测能力：** 在小规模初步实验中，DaMo 展现出强大的预测能力（R^2=0.81），能够高效地推断出最佳的数据混合配置。\n*   **PhoneAgentBench 表现：** 相较于其他替代方法，DaMo 在 PhoneAgentBench 上实现了 **3.38%** 的性能提升。\n*   **泛化能力：** 在包括 BFCL-v3、MME-Reasoning、MME-Perception 和 OCRBench 等现有基准上进行的广泛实验表明，DaMo 具有卓越的泛化能力，平均分数比其他方法高出 **2.57%**。\n*   **特定任务优化：** 当仅用于 BFCL-v3 任务的 MLLM 优化时，DaMo 将指标比其他方法提高了 **12.47%**。\n*   **可扩展性：** 值得注意的是，DaMo 保持了强大的可扩展性，在应用于其他模型架构时仍能保持其有效性。\n\n### 资源可用性\n\n本研究的代码和数据集已公开。",
      "shortSummary": "针对移动电话智能体中多模态大语言模型处理多任务时数据混合优化的挑战，本文提出了DaMo（数据混合优化器）。DaMo通过可训练网络预测最佳数据混合比例，并引入了首个专门评估MLLMs的基准PhoneAgentBench。实验表明，DaMo在PhoneAgentBench上实现了3.38%的性能提升，并在多个现有基准上展现出卓越的泛化能力，平均分数提升2.57%。DaMo还具有强大的可扩展性。",
      "translated_title": "DaMo：用于移动电话智能体的多模态大语言模型微调中的数据混合优化器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Mobile Phone Agents (MPAs) have emerged as a promising research direction due to their broad applicability across diverse scenarios. While Multimodal Large Language Models (MLLMs) serve as the foundation for MPAs, their effectiveness in handling multiple mobile phone tasks simultaneously remains limited. Although multitask supervised fine-tuning (SFT) is widely adopted for multitask learning, existing approaches struggle to determine optimal training data compositions for peak performance. To address this challenge, we propose DaMo (Data Mixture Optimizer) - a novel solution employing a trainable network that predicts optimal data mixtures by forecasting downstream task performance for any given dataset ratio. To support comprehensive evaluation, we introduce PhoneAgentBench, the first specialized benchmark to evaluate MLLMs on multimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse real-world industrial mobile application scenarios. Demonstrating strong predictive capability (R^2=0.81) in small-scale pilot experiments, DaMo efficiently extrapolates optimal data mixing configurations. Our results show DaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to alternative methods. Furthermore, extensive experiments across established benchmarks including BFCL-v3, MME-Reasoning, MME-Perception, and OCRBench reveal DaMo's superior generalization, outperforming other approaches by 2.57% in terms of average score. When used solely for MLLM optimization on the BFCL-v3 task, DaMo improves the metrics by 12.47% than other methods. Notably, DaMo maintains robust scalability, preserving its effectiveness when applied to other model architectures. The code and dataset are available at https://github.com/OPPO-Mente-Lab/DaMo.git"
    },
    {
      "title": "KORE：通过知识导向的增强和约束来增强大型多模态模型的知识注入 (原标题: KORE: Enhancing Knowledge Injection for Large Multimodal Models via Knowledge-Oriented Augmentations and Constraints)",
      "link": "https://arxiv.org/abs/2510.19316",
      "pubDate": "Wed, 22 Oct 2025 03:26:55 GMT",
      "isoDate": "2025-10-22T03:26:55.000Z",
      "creator": "Kailin Jiang, Hongbo Jiang, Ning Jiang, Zhi Gao, Jinhe Bi, Yuchen Ren, Bin Li, Yuntao Du, Lei Liu, Qing Li",
      "summary": "## KORE：大型多模态模型知识注入的新方法\n\n### 引言\n大型多模态模型（LMMs）在其预训练权重中包含了广泛的事实知识。然而，这些知识是静态且有限的，难以跟上现实世界的发展，这阻碍了模型持续获取新知识的能力。因此，有效的知识注入变得至关重要，它主要涉及两个目标：\n\n1.  **知识适应**：成功注入新的知识。\n2.  **知识保留**：有效保存模型已有的旧知识。\n\n现有方法在学习新知识方面常常面临困难，并且普遍存在“灾难性遗忘”问题，即在学习新知识时会遗忘旧知识。\n\n### KORE方法概述\n为了解决上述挑战，研究人员提出了一种名为 **KORE** 的协同方法。KORE代表“知识导向的增强和约束”（KnOwledge-oRientEd augmentations and constraints），旨在向大型多模态模型注入新知识的同时，有效保留旧知识。\n\n### KORE的关键机制\nKORE通过其独特的知识适应和知识保留机制来实现其目标：\n\n*   **知识适应（Knowledge Adaptation）**\n    *   与一般的文本或图像数据增强方法不同，KORE能够自动将单个知识项转换为结构化和全面的知识表示。\n    *   这种转换确保了模型能够准确、深入地学习新的知识，从而实现精确的知识适应。\n\n*   **知识保留（Knowledge Retention）**\n    *   KORE将模型先前的知识存储在LMM线性层激活的协方差矩阵中。\n    *   通过将原始权重投影到该协方差矩阵的零空间来初始化适配器（adapter）。\n    *   这种初始化方式定义了一个特定的微调方向，该方向最大限度地减少了对先前已学习知识的干扰，从而实现了强大的知识保留能力，有效缓解了灾难性遗忘。\n\n### 实验与结果\n研究人员在多种大型多模态模型上进行了广泛的实验，包括：\n\n*   LLaVA-v1.5-7B\n*   LLaVA-v1.5-13B\n*   Qwen2.5-VL-7B\n\n实验结果表明，KORE在新的知识注入性能方面表现出卓越的优势，并且能够有效地缓解灾难性遗忘问题，证明了其在知识注入任务中的有效性和优越性。\n\n### 结论\nKORE提供了一种新颖且高效的方法，能够使大型多模态模型在学习新知识的同时，避免遗忘已有的知识，从而克服了现有知识注入方法的局限性，为LMMs的持续学习和知识更新提供了新的途径。",
      "shortSummary": "KORE是一种旨在增强大型多模态模型（LMMs）知识注入效率的新方法。它通过“知识导向的增强和约束”来解决LMMs知识静态和灾难性遗忘的问题。KORE将知识项转换为结构化形式以促进准确的新知识学习，并通过将旧知识存储在协方差矩阵的零空间中来最小化对先前知识的干扰。实验证明，KORE能有效注入新知识并显著减轻灾难性遗忘，优于现有方法。",
      "translated_title": "KORE：通过知识导向的增强和约束来增强大型多模态模型的知识注入",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Multimodal Models encode extensive factual knowledge in their pre-trained weights. However, its knowledge remains static and limited, unable to keep pace with real-world developments, which hinders continuous knowledge acquisition. Effective knowledge injection thus becomes critical, involving two goals: knowledge adaptation (injecting new knowledge) and knowledge retention (preserving old knowledge). Existing methods often struggle to learn new knowledge and suffer from catastrophic forgetting. To address this, we propose KORE, a synergistic method of KnOwledge-oRientEd augmentations and constraints for injecting new knowledge into large multimodal models while preserving old knowledge. Unlike general text or image data augmentation, KORE automatically converts individual knowledge items into structured and comprehensive knowledge to ensure that the model accurately learns new knowledge, enabling accurate adaptation. Meanwhile, KORE stores previous knowledge in the covariance matrix of LMM's linear layer activations and initializes the adapter by projecting the original weights into the matrix's null space, defining a fine-tuning direction that minimizes interference with previous knowledge, enabling powerful retention. Extensive experiments on various LMMs, including LLaVA-v1.5-7B, LLaVA-v1.5-13B, and Qwen2.5-VL-7B, show that KORE achieves superior new knowledge injection performance and effectively mitigates catastrophic forgetting."
    },
    {
      "title": "面向视觉-语言模型的统一强化与模仿学习 (原标题: Unified Reinforcement and Imitation Learning for Vision-Language Models)",
      "link": "https://arxiv.org/abs/2510.19307",
      "pubDate": "Wed, 22 Oct 2025 03:12:14 GMT",
      "isoDate": "2025-10-22T03:12:14.000Z",
      "creator": "Byung-Kwan Lee, Ryo Hachiuma, Yong Man Ro, Yu-Chiang Frank Wang, Yueh-Hua Wu",
      "summary": "### 统一强化与模仿学习（RIL）用于轻量级视觉-语言模型\n\n**1. 背景与挑战**\n\n*   视觉-语言模型（VLMs）已取得显著进展，但在资源受限的环境中，其庞大的规模使其难以实际应用。\n\n**2. 提出的解决方案：统一强化与模仿学习（RIL）**\n\n*   本文介绍了一种新颖且高效的训练算法——统一强化与模仿学习（RIL），旨在创建强大而轻量级的VLM。\n*   RIL独特地结合了强化学习（Reinforcement Learning）和对抗性模仿学习（Adversarial Imitation Learning）的优势。\n\n**3. RIL的工作机制**\n\n*   **学生模型能力提升**：RIL使较小的学生VLM不仅能够模仿大型教师模型复杂的文本生成能力，还能通过强化信号系统地提升其生成能力。\n*   **模仿框架的关键组成**：\n    *   **基于LLM的判别器**：一个基于大型语言模型（LLM）的判别器，能够熟练地区分学生模型和教师模型的输出。\n    *   **多教师指导**：通过来自多个大型教师VLM的指导，确保学习过程的多样性。\n\n**4. 核心优势与性能**\n\n*   这种统一的学习策略，利用强化和模仿，赋予学生模型显著的性能提升，使其能够与领先的闭源VLM竞争。\n\n**5. 实验结果**\n\n*   在多样化的视觉-语言基准测试上进行了广泛的实验。\n*   结果表明，RIL显著缩小了与最先进的开源和闭源VLM的性能差距。\n*   在某些情况下，RIL甚至超越了这些先进模型。",
      "shortSummary": "本文提出统一强化与模仿学习（RIL），一种高效算法，旨在为资源受限环境创建强大、轻量级的视觉-语言模型（VLM）。RIL独特地结合了强化学习与对抗性模仿学习，使小型学生VLM能模仿大型教师模型并利用强化信号提升生成能力。通过基于LLM的判别器和多教师指导，RIL显著缩小了与现有先进VLM的性能差距，并在某些基准测试中超越它们。",
      "translated_title": "面向视觉-语言模型的统一强化与模仿学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-Language Models (VLMs) have achieved remarkable progress, yet their large scale often renders them impractical for resource-constrained environments. This paper introduces Unified Reinforcement and Imitation Learning (RIL), a novel and efficient training algorithm designed to create powerful, lightweight VLMs. RIL distinctively combines the strengths of reinforcement learning with adversarial imitation learning. This enables smaller student VLMs not only to mimic the sophisticated text generation of large teacher models but also to systematically improve their generative capabilities through reinforcement signals. Key to our imitation framework is an LLM-based discriminator that adeptly distinguishes between student and teacher outputs, complemented by guidance from multiple large teacher VLMs to ensure diverse learning. This unified learning strategy, leveraging both reinforcement and imitation, empowers student models to achieve significant performance gains, making them competitive with leading closed-source VLMs. Extensive experiments on diverse vision-language benchmarks demonstrate that RIL significantly narrows the performance gap with state-of-the-art open- and closed-source VLMs and, in several instances, surpasses them."
    },
    {
      "title": "TheMCPCompany：使用任务专用工具创建通用代理 (原标题: TheMCPCompany: Creating General-purpose Agents with Task-specific Tools)",
      "link": "https://arxiv.org/abs/2510.19286",
      "pubDate": "Wed, 22 Oct 2025 02:42:01 GMT",
      "isoDate": "2025-10-22T02:42:01.000Z",
      "creator": "Reza Esfandiarpoor, Vishwas Suryanarayanan, Stephen H. Bach, Vishal Chowdhary, Anthony Aue",
      "summary": "### TheMCPCompany：使用任务专用工具创建通用代理\n\n#### 引言与背景\n\n自模型上下文协议（MCP）引入以来，大型语言模型（LLM）可用的工具数量显著增加。这些任务专用工具为通用工具（如网络浏览器）提供了一种替代方案，并且比图形用户界面（GUI）更容易开发和维护。然而，当前的通用代理主要依赖网络浏览器与环境进行交互，这限制了它们在特定任务中的效率和灵活性。\n\n#### TheMCPCompany 基准的引入\n\n为了解决这一问题并推动工具调用代理的发展，本文引入了 **TheMCPCompany**，这是一个专门用于评估工具调用代理的基准。该基准的核心在于模拟真实世界的服务交互，其主要特点包括：\n\n*   **真实世界服务交互**：任务涉及与各种真实世界服务的交互，确保评估的实用性。\n*   **大规模工具集**：通过使用这些服务的 REST API，研究人员创建了 MCP 服务器，其中包含了超过 18,000 个工具，模拟了复杂的企业级环境。\n*   **真实工具标注**：为每项任务提供了手动标注的“真实工具”（ground-truth tools），为评估代理的潜在性能提供了基准。\n\n#### 实验与主要发现\n\n研究团队进行了两组主要实验来评估工具调用代理的性能和实用性：\n\n1.  **理想工具检索下的潜力**：\n    *   实验首先使用手动标注的真实工具，旨在展示在假设完美工具检索的情况下，工具调用代理在提高性能和降低成本方面的巨大潜力。\n\n2.  **实际工具检索下的代理性能**：\n    *   接下来，研究探讨了在实际工具检索情境下代理的性能，以评估其在现实世界中的实用性。主要发现包括：\n        *   **与浏览器代理的比较**：所有使用工具检索的模型表现均与基于浏览器的代理相似或更优，表明工具调用方法具有竞争力。\n        *   **模型规模的影响**：较小的模型难以通过检索充分利用所有可用工具，这表明工具检索的效率受模型规模和能力的限制。\n        *   **GPT-5 的表现**：GPT-5 在工具检索下的性能非常接近其使用真实工具时的性能，这突显了先进推理模型在工具发现和利用方面的强大能力。\n\n#### 当前挑战与未来方向\n\n尽管最先进的推理模型在更简单的环境中能有效发现工具，但 TheMCPCompany 基准揭示了它们在复杂企业环境中导航时面临的严重困难。具体挑战包括：\n\n*   **复杂环境导航**：在数万个工具中进行导航，并以非平凡的方式组合它们来解决复杂问题，对当前模型来说仍然是一项艰巨的任务。\n*   **推理与检索能力不足**：这表明当前模型需要更好的推理能力来理解任务需求和工具功能，以及更优秀的检索模型来高效地从庞大工具集中找到相关工具。\n\n#### 结论\n\nTheMCPCompany 为评估和推进工具调用代理提供了重要平台。研究结果强调了在复杂工具生态系统中，提升代理的推理和检索能力是未来研究的关键方向，以使通用代理能够更有效地利用任务专用工具。",
      "shortSummary": "TheMCPCompany 是一个新基准，用于评估使用任务专用工具的通用代理。它通过真实服务的 REST API 创建了包含超过18,000个工具的MCP服务器。研究发现，工具调用代理在理想情况下能显著提升性能并降低成本。在实际检索中，它们优于浏览器代理，但小型模型难以充分利用工具，而GPT-5表现接近理想。当前模型在导航和组合数万个复杂工具以解决企业级问题时仍面临挑战，亟需改进推理和检索能力。",
      "translated_title": "TheMCPCompany：使用任务专用工具创建通用代理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Since the introduction of the Model Context Protocol (MCP), the number of available tools for Large Language Models (LLMs) has increased significantly. These task-specific tool sets offer an alternative to general-purpose tools such as web browsers, while being easier to develop and maintain than GUIs. However, current general-purpose agents predominantly rely on web browsers for interacting with the environment. Here, we introduce TheMCPCompany, a benchmark for evaluating tool-calling agents on tasks that involve interacting with various real-world services. We use the REST APIs of these services to create MCP servers, which include over 18,000 tools. We also provide manually annotated ground-truth tools for each task. In our experiments, we use the ground truth tools to show the potential of tool-calling agents for both improving performance and reducing costs assuming perfect tool retrieval. Next, we explore agent performance using tool retrieval to study the real-world practicality of tool-based agents. While all models with tool retrieval perform similarly or better than browser-based agents, smaller models cannot take full advantage of the available tools through retrieval. On the other hand, GPT-5's performance with tool retrieval is very close to its performance with ground-truth tools. Overall, our work shows that the most advanced reasoning models are effective at discovering tools in simpler environments, but seriously struggle with navigating complex enterprise environments. TheMCPCompany reveals that navigating tens of thousands of tools and combining them in non-trivial ways to solve complex problems is still a challenging task for current models and requires both better reasoning and better retrieval models."
    },
    {
      "title": "他们是恋人还是朋友？评估大型语言模型在英语和韩语对话中的社交推理能力 (原标题: Are they lovers or friends? Evaluating LLMs' Social Reasoning in English and Korean Dialogues)",
      "link": "https://arxiv.org/abs/2510.19028",
      "pubDate": "Tue, 21 Oct 2025 15:12:47 GMT",
      "isoDate": "2025-10-21T15:12:47.000Z",
      "creator": "Eunsu Kim, Junyeong Park, Juhyun Oh, Kiwoong Park, Seyoung Song, A. Seza Dogruoz, Najoung Kim, Alice Oh",
      "summary": "# 评估大型语言模型（LLMs）的社交推理能力\n\n## 背景与重要性\n随着大型语言模型（LLMs）在人机交互中日益普及，它们在人际情境中的社交推理能力变得至关重要。本研究旨在评估LLMs在这方面的表现。\n\n## SCRIPTS数据集介绍\n*   **名称**：SCRIPTS\n*   **规模**：包含1000个对话\n*   **语言**：英语和韩语\n*   **来源**：电影剧本\n*   **任务**：评估模型推断对话者之间人际关系（例如，朋友、姐妹、恋人）的能力。\n*   **标注**：每个对话都由来自韩国和美国的母语（或同等水平）韩语和英语使用者进行概率关系标注，标签包括“极有可能”、“可能性较小”和“不可能”。\n\n## 关键研究发现\n*   **英语与韩语表现差异**：\n    *   当前专有LLMs在英语数据集上的表现约为75-80%。\n    *   但在韩语数据集上，其表现显著下降至58-69%。\n*   **“不可能”关系的误判**：模型在10-25%的响应中错误地选择了“不可能”的关系。\n*   **推理策略的局限性**：\n    *   “思考模型”（thinking models）和思维链（chain-of-thought）提示等对一般推理有效的策略，对社交推理的益处微乎其微。\n    *   这些策略有时甚至会放大社交偏见。\n\n## 结论与展望\n研究结果揭示了当前LLMs在社交推理能力方面存在显著局限性，强调了开发具有社交意识的语言模型的必要性。",
      "shortSummary": "本研究引入SCRIPTS数据集，包含1000个英语和韩语电影对话，用于评估大型语言模型（LLMs）的社交推理能力。结果显示，LLMs在英语对话中表现尚可（75-80%），但在韩语对话中表现显著下降（58-69%），且常误判“不可能”的关系。思维链等推理策略对社交推理帮助不大，甚至可能加剧偏见。这表明当前LLMs在社交推理方面存在严重不足，亟需开发更具社交意识的模型。",
      "translated_title": "他们是恋人还是朋友？评估大型语言模型在英语和韩语对话中的社交推理能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "As large language models (LLMs) are increasingly used in human-AI interactions, their social reasoning capabilities in interpersonal contexts are critical. We introduce SCRIPTS, a 1k-dialogue dataset in English and Korean, sourced from movie scripts. The task involves evaluating models' social reasoning capability to infer the interpersonal relationships (e.g., friends, sisters, lovers) between speakers in each dialogue. Each dialogue is annotated with probabilistic relational labels (Highly Likely, Less Likely, Unlikely) by native (or equivalent) Korean and English speakers from Korea and the U.S. Evaluating nine models on our task, current proprietary LLMs achieve around 75-80% on the English dataset, whereas their performance on Korean drops to 58-69%. More strikingly, models select Unlikely relationships in 10-25% of their responses. Furthermore, we find that thinking models and chain-of-thought prompting, effective for general reasoning, provide minimal benefits for social reasoning and occasionally amplify social biases. Our findings reveal significant limitations in current LLMs' social reasoning capabilities, highlighting the need for efforts to develop socially-aware language models."
    },
    {
      "title": "ProfBench：需要专业知识来回答和判断的多领域评估标准 (原标题: ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge)",
      "link": "https://arxiv.org/abs/2510.18941",
      "pubDate": "Tue, 21 Oct 2025 13:59:44 GMT",
      "isoDate": "2025-10-21T13:59:44.000Z",
      "creator": "Zhilin Wang, Jaehun Jung, Ximing Lu, Shizhe Diao, Ellie Evans, Jiaqi Zeng, Pavlo Molchanov, Yejin Choi, Jan Kautz, Yi Dong",
      "summary": "### ProfBench：评估LLM专业知识的新基准\n\n**1. 背景与挑战**\n\n*   **当前LLM评估的局限性**：目前大型语言模型（LLM）的评估往往受限于响应验证的难度，导致评估任务主要集中在数学、编程和简短问答等领域。\n*   **实际应用的需求**：然而，许多现实世界的应用要求LLM能够处理专业的文档、综合信息，并根据用户查询生成全面的报告，这超出了现有评估方法的范围。\n\n**2. ProfBench的引入**\n\n*   **基准内容**：ProfBench是一个新的评估基准，包含超过7000个响应-标准对。\n*   **专业领域**：这些标准涵盖了多个专业领域，包括物理学博士、化学博士、金融MBA和咨询MBA等，并由具备相应专业知识的人类专家进行评估。\n\n**3. 创新的评估方法：LLM-Judges**\n\n*   **开发目的**：为了有效且经济地评估ProfBench中的标准，研究团队开发了鲁棒且经济高效的LLM-Judges。\n*   **解决偏见与成本**：这些LLM-Judges旨在减轻LLM固有的“自我增强偏见”，并将评估成本降低2-3个数量级，从而使专业领域的LLM评估变得更加公平和可及。\n\n**4. 主要研究发现**\n\n*   **对SOTA LLM的挑战**：ProfBench对当前最先进的LLM构成了显著挑战。即使是表现最佳的模型，如GPT-5-high，其总体性能也仅达到65.9%。\n*   **模型性能差异**：研究揭示了专有模型和开源模型之间在性能上存在显著差异。\n*   **“扩展思维”的重要性**：研究还提供了关于“扩展思维”（extended thinking）在解决复杂专业领域任务中所扮演角色的深入见解。",
      "shortSummary": "ProfBench是一个新的基准，旨在评估大型语言模型（LLM）在需要专业知识的多领域任务中的表现。它包含7000多个由人类专家评估的响应-标准对，并引入了经济高效的LLM-Judges。研究发现，即使是GPT-5-high等最先进的LLM，在ProfBench上的表现也仅为65.9%，揭示了专有模型与开源模型之间的性能差距，并强调了扩展思维在复杂专业任务中的重要性。",
      "translated_title": "ProfBench：需要专业知识来回答和判断的多领域评估标准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Evaluating progress in large language models (LLMs) is often constrained by the challenge of verifying responses, limiting assessments to tasks like mathematics, programming, and short-form question-answering. However, many real-world applications require evaluating LLMs in processing professional documents, synthesizing information, and generating comprehensive reports in response to user queries. We introduce ProfBench: a set of over 7000 response-criterion pairs as evaluated by human-experts with professional knowledge across Physics PhD, Chemistry PhD, Finance MBA and Consulting MBA. We build robust and affordable LLM-Judges to evaluate ProfBench rubrics, by mitigating self-enhancement bias and reducing the cost of evaluation by 2-3 orders of magnitude, to make it fair and accessible to the broader community. Our findings reveal that ProfBench poses significant challenges even for state-of-the-art LLMs, with top-performing models like GPT-5-high achieving only 65.9\\% overall performance. Furthermore, we identify notable performance disparities between proprietary and open-weight models and provide insights into the role that extended thinking plays in addressing complex, professional-domain tasks. Data: https://huggingface.co/datasets/nvidia/ProfBench and Code: https://github.com/NVlabs/ProfBench"
    },
    {
      "title": "NeuroAda：激活每个神经元的潜力以实现参数高效微调 (原标题: NeuroAda: Activating Each Neuron's Potential for Parameter-Efficient Fine-Tuning)",
      "link": "https://arxiv.org/abs/2510.18940",
      "pubDate": "Tue, 21 Oct 2025 13:59:24 GMT",
      "isoDate": "2025-10-21T13:59:24.000Z",
      "creator": "Zhi Zhang, Yixian Shen, Congfeng Cao, Ekaterina Shutova",
      "summary": "## NeuroAda：激活每个神经元的潜力以实现参数高效微调\n\n### 背景：参数高效微调 (PEFT) 的挑战\n\n当前参数高效微调（PEFT）方法主要分为两大类，各自面临不同的挑战：\n\n*   **基于添加的方法（如LoRA）**：\n    *   通过引入额外的模块来使模型适应下游任务。\n    *   优点是内存效率高。\n    *   缺点是表示能力通常有限，不适用于需要细粒度适应的场景。\n\n*   **选择性原位适应方法**：\n    *   直接微调原始模型中经过精心选择的参数子集。\n    *   优点是能够实现更精确和有效的适应。\n    *   缺点是内存消耗显著增加。\n\n这两种方法之间存在一个核心权衡：如何在实现细粒度模型微调的同时，保持高内存效率。\n\n### NeuroAda 方法：弥合效率与适应性之间的鸿沟\n\n为了解决上述权衡，研究人员提出了 **NeuroAda**，一种新颖的PEFT方法，旨在在保持高内存效率的同时实现细粒度模型微调。\n\n**NeuroAda 的核心机制如下：**\n\n1.  **识别重要参数**：首先，NeuroAda 像选择性适应方法一样，识别网络中重要的参数（即连接）。\n2.  **引入旁路连接**：针对这些被选定的重要参数，NeuroAda 引入了旁路连接。\n3.  **冻结原始参数**：在微调过程中，只有这些新引入的旁路连接会被更新，而原始模型参数则保持冻结状态。\n\n通过这种方式，NeuroAda 能够专注于模型中最关键的连接进行适应，同时避免了对整个模型参数进行微调所带来的高内存开销。\n\n### 实验结果与优势\n\nNeuroAda 在广泛的任务上进行了实证评估，展示了其卓越的性能和效率：\n\n*   **广泛的任务覆盖**：在涵盖自然语言生成和理解的23+项任务上进行了测试。\n*   **最先进的性能**：NeuroAda 在这些任务上取得了最先进的性能。\n*   **极低的训练参数量**：仅使用 $\\leq \\textbf{0.02}\\%$ 的可训练参数，这显著低于现有方法。\n*   **显著的内存效率提升**：CUDA 内存使用量减少高达60%。\n\n这些结果表明，NeuroAda 成功地在细粒度模型适应和高内存效率之间取得了平衡，为参数高效微调领域提供了一个强大的新解决方案。\n\n### 代码发布\n\n研究团队已发布了 NeuroAda 的代码，以促进社区的进一步研究和应用。",
      "shortSummary": "NeuroAda是一种新颖的参数高效微调（PEFT）方法，旨在解决现有PEFT方法在表示能力和内存效率之间的权衡。它通过识别重要参数并引入旁路连接进行更新，同时冻结原始模型参数。实验证明，NeuroAda在23+项任务上实现了最先进的性能，可训练参数仅占 $\\leq \\textbf{0.02}\\%$，并能将CUDA内存使用量减少高达60%，从而在保持高内存效率的同时实现细粒度模型微调。",
      "translated_title": "NeuroAda：激活每个神经元的潜力以实现参数高效微调",
      "images": [],
      "contentSource": "完整文章",
      "content": "Existing parameter-efficient fine-tuning (PEFT) methods primarily fall into two categories: addition-based and selective in-situ adaptation. The former, such as LoRA, introduce additional modules to adapt the model to downstream tasks, offering strong memory efficiency. However, their representational capacity is often limited, making them less suitable for fine-grained adaptation. In contrast, the latter directly fine-tunes a carefully chosen subset of the original model parameters, allowing for more precise and effective adaptation, but at the cost of significantly increased memory consumption. To reconcile this trade-off, we propose NeuroAda, a novel PEFT method that enables fine-grained model finetuning while maintaining high memory efficiency. Our approach first identifies important parameters (i.e., connections within the network) as in selective adaptation, and then introduces bypass connections for these selected parameters. During finetuning, only the bypass connections are updated, leaving the original model parameters frozen. Empirical results on 23+ tasks spanning both natural language generation and understanding demonstrate that NeuroAda achieves state-of-the-art performance with as little as leq 0.02% trainable parameters, while reducing CUDA memory usage by up to 60%. We release our code here: https://github.com/FightingFighting/NeuroAda.git."
    },
    {
      "title": "看文本：从分词到视觉阅读 (原标题: See the Text: From Tokenization to Visual Reading)",
      "link": "https://arxiv.org/abs/2510.18840",
      "pubDate": "Tue, 21 Oct 2025 13:34:48 GMT",
      "isoDate": "2025-10-21T13:34:48.000Z",
      "creator": "Ling Xing, Alex Jinpeng Wang, Rui Yan, Hongyu Qu, Zechao Li, Jinhui Tang",
      "summary": "## SeeTok：从分词到视觉阅读的范式转变\n\n### 引言\n\n人类阅读文本的方式是将其识别为视觉对象，包括它们的形状、布局和模式，然后才将其与意义联系起来。这种能力使我们能够有效地处理错别字、扭曲字体和各种文字。\n\n然而，现代大型语言模型（LLMs）依赖于子词分词，将文本分解为固定词汇表中的片段。尽管这种方法对高资源语言有效，但它对低资源语言会过度分段，导致生成冗长、语言上无意义的序列，并显著增加计算量。\n\n### SeeTok 方法介绍\n\n本文挑战了这种根深蒂固的分词范式，并提出了一种以视觉为中心的替代方案——**SeeTok**。\n\nSeeTok 的核心工作原理如下：\n\n*   **文本渲染为图像**：将文本渲染成图像形式（即视觉文本）。\n*   **利用多模态LLMs**：利用预训练的多模态大型语言模型来解释这些视觉文本。\n*   **重用现有能力**：该方法重用了多模态LLMs从大规模多模态训练中学到的强大光学字符识别（OCR）和文本-视觉对齐能力。\n\n### 主要成果与优势\n\nSeeTok 在三项不同的语言任务中取得了显著成果：\n\n*   **性能表现**：其表现与传统的子词分词器持平或超越。\n*   **效率提升**：\n    *   所需的tokens数量减少了 **4.43倍**。\n    *   浮点运算（FLOPs）减少了 **70.5%**。\n*   **其他增益**：\n    *   增强了跨语言泛化能力。\n    *   提高了对排版噪声的鲁棒性。\n    *   更好地捕捉了语言的层次结构。\n\n### 结论与意义\n\nSeeTok 标志着从符号分词向类人视觉阅读的转变，并向构建更自然、更受认知启发的大型语言模型迈进了一步。",
      "shortSummary": "现代大型语言模型（LLMs）依赖子词分词，对低资源语言效率低下且计算成本高。本文提出SeeTok，一种视觉中心的方法，将文本渲染为图像并利用预训练的多模态LLMs进行解释，重用其OCR和文本-视觉对齐能力。SeeTok在语言任务中表现优异，所需tokens减少4.43倍，FLOPs降低70.5%，并增强了跨语言泛化和对排版噪声的鲁棒性。它标志着从符号分词向类人视觉阅读的转变，推动了更自然、认知启发式语言模型的发展。",
      "translated_title": "看文本：从分词到视觉阅读",
      "images": [],
      "contentSource": "完整文章",
      "content": "People see text. Humans read by recognizing words as visual objects, including their shapes, layouts, and patterns, before connecting them to meaning, which enables us to handle typos, distorted fonts, and various scripts effectively. Modern large language models (LLMs), however, rely on subword tokenization, fragmenting text into pieces from a fixed vocabulary. While effective for high-resource languages, this approach over-segments low-resource languages, yielding long, linguistically meaningless sequences and inflating computation. In this work, we challenge this entrenched paradigm and move toward a vision-centric alternative. Our method, SeeTok, renders text as images (visual-text) and leverages pretrained multimodal LLMs to interpret them, reusing strong OCR and text-vision alignment abilities learned from large-scale multimodal training. Across three different language tasks, SeeTok matches or surpasses subword tokenizers while requiring 4.43 times fewer tokens and reducing FLOPs by 70.5%, with additional gains in cross-lingual generalization, robustness to typographic noise, and linguistic hierarchy. SeeTok signals a shift from symbolic tokenization to human-like visual reading, and takes a step toward more natural and cognitively inspired language models."
    },
    {
      "title": "BAPO：通过自适应裁剪的平衡策略优化稳定LLM的离线强化学习 (原标题: BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping)",
      "link": "https://arxiv.org/abs/2510.18927",
      "pubDate": "Tue, 21 Oct 2025 08:55:04 GMT",
      "isoDate": "2025-10-21T08:55:04.000Z",
      "creator": "Zhiheng Xi, Xin Guo, Yang Nan, Enyu Zhou, Junrui Shen, Wenxiang Chen, Jiaqi Liu, Jixuan Huang, Zhihao Zhang, Honglin Guo, Xun Deng, Zhikai Lei, Miao Zheng, Guoteng Wang, Shuo Zhang, Peng Sun, Rui Zheng, Hang Yan, Tao Gui, Qi Zhang, Xuanjing Huang",
      "summary": "## BAPO：通过自适应裁剪的平衡策略优化稳定LLM的离线强化学习\n\n本文提出了一种名为BAlanced Policy Optimization with Adaptive Clipping (BAPO) 的方法，旨在解决大型语言模型（LLM）离线强化学习（RL）中策略熵急剧下降、优化不稳定甚至崩溃的问题。\n\n**核心观点：**\n\n*   **优化失衡：** 负优势样本在策略梯度中占据主导地位，抑制了有用的行为，并可能导致梯度爆炸。\n*   **熵裁剪规则：** 类似PPO的目标函数中固定的裁剪机制系统性地阻止了熵增加的更新，从而导致策略过度利用，牺牲了探索。\n\n**BAPO方法：**\n\n*   动态调整裁剪边界，自适应地重新平衡正负贡献。\n*   保持熵，稳定RL优化。\n\n**实验结果：**\n\n*   在多种离线场景（包括样本回放和部分rollout）中，BAPO实现了快速、稳定和数据高效的训练。\n*   在AIME 2024和AIME 2025基准测试中，7B BAPO模型超越了开源模型，如SkyWork-OR1-7B。\n*   32B BAPO模型不仅在同等规模的模型中取得了最先进的结果，而且优于领先的专有系统，如o3-mini和Gemini-2.5-Flash-Thinking。",
      "shortSummary": "本文提出了一种名为BAPO的方法，用于解决LLM离线强化学习中策略熵下降和优化不稳定的问题。BAPO通过动态调整裁剪边界来平衡正负贡献，保持熵并稳定优化。实验结果表明，BAPO在多种离线场景中实现了快速、稳定和数据高效的训练，并在AIME基准测试中超越了其他模型，甚至优于一些专有系统。",
      "translated_title": "BAPO：通过自适应裁剪的平衡策略优化稳定LLM的离线强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement learning (RL) has recently become the core paradigm for aligning and strengthening large language models (LLMs). Yet, applying RL in off-policy settings--where stale data from past policies are used for training--improves sample efficiency, but remains challenging: policy entropy declines sharply, optimization often becomes unstable and may even collapse. Through theoretical and empirical analysis, we identify two key insights: (i) an imbalance in optimization, where negative-advantage samples dominate the policy gradient, suppressing useful behaviors and risking gradient explosions; and (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping mechanism in PPO-like objectives systematically blocks entropy-increasing updates, thereby driving the policy toward over-exploitation at the expense of exploration. Building on these insights, we propose BAlanced Policy Optimization with Adaptive Clipping (BAPO), a simple yet effective method that dynamically adjusts clipping bounds to adaptively re-balance positive and negative contributions, preserve entropy, and stabilize RL optimization. Across diverse off-policy scenarios--including sample replay and partial rollout--BAPO achieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025 benchmarks, our 7B BAPO model surpasses open-source counterparts such as SkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art results among models of the same scale but also outperforms leading proprietary systems like o3-mini and Gemini-2.5-Flash-Thinking."
    },
    {
      "title": "AlphaOPT：利用自改进LLM经验库构建优化程序 (原标题: AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library)",
      "link": "https://arxiv.org/abs/2510.18428",
      "pubDate": "Tue, 21 Oct 2025 05:03:26 GMT",
      "isoDate": "2025-10-21T05:03:26.000Z",
      "creator": "Minwei Kong, Ao Qu, Xiaotong Guo, Wenbin Ouyang, Chonghe Jiang, Han Zheng, Yining Ma, Dingyi Zhuang, Yuhan Tang, Junyi Li, Hai Wang, Cathy Wu, Jinhua Zhao",
      "summary": "# AlphaOPT：利用自改进LLM经验库构建优化程序\n\n## 概述\n优化建模在各行业中对关键决策至关重要，但其自动化过程仍面临挑战：需要将非正式语言精确地映射到数学公式和可执行的求解器代码。现有的LLM方法要么依赖于脆弱的提示工程，要么需要昂贵且泛化能力有限的再训练。\n\n## AlphaOPT 方法\n本文提出了 AlphaOPT，一个自改进的经验库，它使大型语言模型（LLM）能够从有限的演示（甚至仅凭答案，无需黄金标准程序）和求解器反馈中学习，而无需带注释的推理轨迹或参数更新。\n\nAlphaOPT 采用持续的两阶段循环运行：\n\n### 1. 经验库学习阶段 (Library Learning)\n*   **目的：** 反思失败的尝试。\n*   **过程：** 提取经过求解器验证的结构化洞察，形式为 `{分类、条件、解释、示例}`。\n\n### 2. 经验库演进阶段 (Library Evolution)\n*   **目的：** 诊断检索失调并改进存储洞察的适用条件。\n*   **过程：** 提高知识在不同任务间的迁移能力。\n\n## 设计优势\nAlphaOPT 的设计带来了以下显著优势：\n*   **高效学习：** 能够从有限的演示中高效学习，无需精心策划的推理过程。\n*   **持续扩展：** 通过更新经验库而非模型权重，实现持续扩展，避免了昂贵的再训练成本。\n*   **知识可解释性：** 使知识变得明确和可解释，便于人工检查和干预。\n\n## 实验结果\n实验表明，AlphaOPT 表现出稳步提升和卓越的性能：\n*   **数据量影响：** 随着训练数据量的增加（从100个训练项到300个训练项，性能从65%提升至72%），AlphaOPT 的性能稳步提高。\n*   **超越基线：** 在仅使用答案进行训练的情况下，AlphaOPT 在分布外（out-of-distribution）的 OptiBench 数据集上，超越了最强的基线模型 7.7%。\n\n## 资源可用性\n代码和数据可供查阅。",
      "shortSummary": "AlphaOPT 提出一个自改进的LLM经验库，旨在自动化优化建模。它通过两阶段循环（经验库学习和演进）从有限演示和求解器反馈中高效学习，无需模型再训练或注释。该方法能持续扩展，并使知识可解释。实验显示，AlphaOPT 随数据量增加而稳步提升，并在OptiBench数据集上超越现有基线7.7%。代码和数据已公开。",
      "translated_title": "AlphaOPT：利用自改进LLM经验库构建优化程序",
      "images": [],
      "contentSource": "完整文章",
      "content": "Optimization modeling enables critical decisions across industries but remains difficult to automate: informal language must be mapped to precise mathematical formulations and executable solver code. Prior LLM approaches either rely on brittle prompting or costly retraining with limited generalization. We present AlphaOPT, a self-improving experience library that enables an LLM to learn from limited demonstrations (even answers alone, without gold-standard programs) and solver feedback - without annotated reasoning traces or parameter updates. AlphaOPT operates in a continual two-phase cycle: (i) a Library Learning phase that reflects on failed attempts, extracting solver-verified, structured insights as {taxonomy, condition, explanation, example}; and (ii) a Library Evolution phase that diagnoses retrieval misalignments and refines the applicability conditions of stored insights, improving transfer across tasks. This design (1) learns efficiently from limited demonstrations without curated rationales, (2) expands continually without costly retraining by updating the library rather than model weights, and (3) makes knowledge explicit and interpretable for human inspection and intervention. Experiments show that AlphaOPT steadily improves with more data (65% to 72% from 100 to 300 training items) and surpasses the strongest baseline by 7.7% on the out-of-distribution OptiBench dataset when trained only on answers. Code and data are available at: https://github.com/Minw913/AlphaOPT."
    }
  ],
  "lastUpdated": "2025-10-23T09:39:30.872Z"
}