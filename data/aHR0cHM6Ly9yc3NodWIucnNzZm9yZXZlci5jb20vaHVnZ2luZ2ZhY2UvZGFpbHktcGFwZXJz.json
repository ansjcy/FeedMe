{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "Phi-Ground 技术报告：提升 GUI 接地感知能力 (原标题: Phi-Ground Tech Report: Advancing Perception in GUI Grounding)",
      "link": "https://arxiv.org/abs/2507.23779",
      "pubDate": "Thu, 31 Jul 2025 13:59:09 GMT",
      "isoDate": "2025-07-31T13:59:09.000Z",
      "creator": "Miaosen Zhang, Ziqiang Xu, Jialiang Zhu, Qi Dai, Kai Qiu, Yifan Yang, Chong Luo, Tianyi Chen, Justin Wagle, Tim Franklin, Baining Guo",
      "summary": "## Phi-Ground 技术报告：提升 GUI 接地感知能力\n\n### 引言\n\n*   随着多模态推理模型的发展，计算机使用代理（CUAs），如《钢铁侠》中的贾维斯，正逐渐成为现实。\n*   GUI 接地（GUI grounding）是 CUAs 执行实际操作的核心组成部分，类似于机器人中的机械控制，直接决定系统的成败。它涉及确定点击、输入等操作及其相关参数（如点击坐标）。\n\n### 当前挑战\n\n*   目前的端到端接地模型在 ScreenSpot-pro 和 UI-Vision 等挑战性基准测试上的准确率仍低于 65%，远未达到部署要求。\n\n### 研究方法与成果\n\n*   本研究对接地模型的训练进行了实证研究，详细考察了从数据收集到模型训练的各个环节。\n*   最终开发出 **Phi-Ground 模型家族**。\n\n### Phi-Ground 模型性能\n\n*   在代理设置下，Phi-Ground 模型家族在所有五个接地基准测试中，对于参数量小于 100 亿的模型，均实现了最先进（SOTA）的性能。\n*   在端到端模型设置中，Phi-Ground 模型在 ScreenSpot-pro 上取得了 **43.2** 分，在 UI-Vision 上取得了 **27.2** 分，同样达到了 SOTA 结果。\n\n### 研究意义\n\n*   论文中讨论的各种细节，包括成功经验和失败教训，不仅阐明了接地模型的构建过程，也对其他感知任务有所裨益。",
      "shortSummary": "Phi-Ground 技术报告介绍了提升 GUI 接地感知能力的研究。GUI 接地是计算机使用代理（CUAs）执行操作的关键，但当前模型准确率低。本研究通过实证分析，开发了 Phi-Ground 模型家族。该模型在多个接地基准测试中，对于参数量小于 100 亿的模型，均实现了最先进（SOTA）的性能，包括在 ScreenSpot-pro 和 UI-Vision 上取得显著提升。研究成果对构建接地模型及其他感知任务具有重要意义。",
      "translated_title": "Phi-Ground 技术报告：提升 GUI 接地感知能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "With the development of multimodal reasoning models, Computer Use Agents (CUAs), akin to Jarvis from \"Iron Man\", are becoming a reality. GUI grounding is a core component for CUAs to execute actual actions, similar to mechanical control in robotics, and it directly leads to the success or failure of the system. It determines actions such as clicking and typing, as well as related parameters like the coordinates for clicks. Current end-to-end grounding models still achieve less than 65\\% accuracy on challenging benchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from being ready for deployment. % , as a single misclick can result in unacceptable consequences. In this work, we conduct an empirical study on the training of grounding models, examining details from data collection to model training. Ultimately, we developed the Phi-Ground model family, which achieves state-of-the-art performance across all five grounding benchmarks for models under 10B parameters in agent settings. In the end-to-end model setting, our model still achieves SOTA results with scores of \\textbf{43.2} on ScreenSpot-pro and \\textbf{27.2} on UI-Vision. We believe that the various details discussed in this paper, along with our successes and failures, not only clarify the construction of grounding models but also benefit other perception tasks. Project homepage: https://zhangmiaosen2000.github.io/Phi-Ground/{https://zhangmiaosen2000.github.io/Phi-Ground/}"
    },
    {
      "title": "Seed-Prover：用于自动化定理证明的深度与广度推理 (原标题: Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving)",
      "link": "https://arxiv.org/abs/2507.23726",
      "pubDate": "Thu, 31 Jul 2025 13:00:30 GMT",
      "isoDate": "2025-07-31T13:00:30.000Z",
      "creator": "Luoxin Chen, Jinming Gu, Liankai Huang, Wenhao Huang, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin, Chenggang Li, Kaijing Ma, Cheng Ren, Jiawei Shen, Wenlei Shi, Tong Sun, He Sun, Jiahui Wang, Siran Wang, Zhihong Wang, Chenrui Wei, Shufa Wei, Yonghui Wu, Yuchen Wu, Yihang Xia, Huajian Xin, Fan Yang, Huaiyuan Ying, Hongyi Yuan, Zheng Yuan, Tianyang Zhan, Chi Zhang, Yue Zhang, Ge Zhang, Tianyun Zhao, Jianqiu Zhao, Yichi Zhou, Thomas Hanwen Zhu",
      "summary": "大型语言模型（LLMs）在结合强化学习和长链式思考（chain-of-thought）时，已展现出强大的数学推理能力。然而，由于在仅使用自然语言时缺乏明确的监督信号，LLMs在定理证明方面仍面临挑战。相比之下，Lean等领域特定语言通过对证明进行形式化验证，提供了清晰的监督，从而能够进行有效的训练。\n\n本文提出了 **Seed-Prover**，一个引理风格的整体证明推理模型。Seed-Prover 能够基于 Lean 的反馈、已证明的引理以及自我总结，迭代地完善其证明。为了解决国际数学奥林匹克（IMO）级别的竞赛问题，研究团队设计了三种测试时推理策略，以实现深度（deep）和广度（broad）推理。\n\n为了弥补 Lean 在几何支持方面的不足，研究团队引入了一个名为 **Seed-Geometry** 的几何推理引擎。Seed-Geometry 在几何推理方面超越了之前形式化几何引擎的性能。\n\n**主要成果：**\n*   **IMO 问题：** Seed-Prover 成功证明了 78.1% 的形式化过的历届 IMO 问题。\n*   **MiniF2F：** 在 MiniF2F 基准测试中达到了饱和（saturates）。\n*   **PutnamBench：** 在 PutnamBench 上取得了超过 50% 的成绩。\n*   **超越SOTA：** 这些结果显著超越了之前的最先进（state-of-the-art, SOTA）水平。\n*   **IMO 2025 参与：** Seed-Prover 和 Seed-Geometry 这两个系统共同参与了 IMO 2025，并成功完全证明了 6 道问题中的 5 道。\n\n这项工作代表了自动化数学推理领域的一个重大进步，有力地证明了结合形式化验证与长链式思考推理的有效性。",
      "shortSummary": "Seed-Prover是一个引理风格的自动化定理证明模型，旨在解决LLM在定理证明中缺乏明确监督信号的问题。它利用Lean的反馈、已证明引理和自我总结迭代优化证明，并设计了深度与广度推理策略。为弥补Lean的几何不足，引入了Seed-Geometry。Seed-Prover在IMO、MiniF2F和PutnamBench上表现卓越，大幅超越现有技术，并在IMO 2025中成功证明5/6道题。这项工作展示了形式化验证与长链式思考在自动化数学推理中的强大潜力。",
      "translated_title": "Seed-Prover：用于自动化定理证明的深度与广度推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose Seed-Prover, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves 78.1% of formalized past IMO problems, saturates MiniF2F, and achieves over 50\\% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine Seed-Geometry, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning."
    },
    {
      "title": "可扩展的多任务强化学习，用于视觉运动智能体中可泛化的空间智能 (原标题: Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents)",
      "link": "https://arxiv.org/abs/2507.23698",
      "pubDate": "Thu, 31 Jul 2025 12:20:02 GMT",
      "isoDate": "2025-07-31T12:20:02.000Z",
      "creator": "Shaofei Cai, Zhancun Mu, Haiwen Xia, Bowei Zhang, Anji Liu, Yitao Liang",
      "summary": "### 背景与挑战\n\n*   强化学习（RL）在语言建模领域取得了显著成功，但其在视觉运动智能体中的应用尚未完全实现。\n*   RL模型面临的主要挑战是容易过拟合特定任务或环境，这阻碍了它们在多样化设置中获得可泛化的行为。\n\n### 本文贡献与研究目标\n\n*   本文通过在Minecraft中对RL微调的视觉运动智能体进行零样本泛化到未见过的世界，初步解决了上述挑战。\n*   研究旨在探索RL在增强3D世界中可泛化的空间推理和交互能力方面的潜力。\n\n### 提出的方法与技术\n\n*   **统一多任务目标空间**：为了解决多任务RL表示中的挑战，本文分析并建立了“跨视图目标规范”（cross-view goal specification）作为视觉运动策略的统一多任务目标空间。\n*   **自动化任务合成**：为了克服手动任务设计的显著瓶颈，本文提出在高度可定制的Minecraft环境中进行自动化任务合成，以支持大规模多任务RL训练。\n*   **高效分布式RL框架**：构建了一个高效的分布式RL框架来支持大规模多任务训练。\n\n### 实验结果\n\n*   实验结果表明，RL显著提升了交互成功率4倍。\n*   RL使得空间推理能够在多样化环境（包括真实世界设置）中实现零样本泛化。\n\n### 结论与意义\n\n*   研究结果强调了在3D模拟环境（特别是那些适合大规模任务生成的环境）中进行RL训练的巨大潜力。\n*   这种方法能够显著提升视觉运动智能体的空间推理能力。",
      "shortSummary": "本文解决了强化学习（RL）在视觉运动智能体中泛化能力不足的问题。研究通过在Minecraft中对RL微调的智能体进行大规模多任务训练，实现了对未见环境的零样本泛化。通过引入跨视图目标规范和自动化任务合成，RL将交互成功率提高了4倍，并使空间推理在多样化环境中实现零样本泛化。这突显了在可生成大规模任务的3D模拟环境中进行RL训练，对于提升视觉运动智能体空间智能的巨大潜力。",
      "translated_title": "可扩展的多任务强化学习，用于视觉运动智能体中可泛化的空间智能",
      "images": [],
      "contentSource": "完整文章",
      "content": "While Reinforcement Learning (RL) has achieved remarkable success in language modeling, its triumph hasn't yet fully translated to visuomotor agents. A primary challenge in RL models is their tendency to overfit specific tasks or environments, thereby hindering the acquisition of generalizable behaviors across diverse settings. This paper provides a preliminary answer to this challenge by demonstrating that RL-finetuned visuomotor agents in Minecraft can achieve zero-shot generalization to unseen worlds. Specifically, we explore RL's potential to enhance generalizable spatial reasoning and interaction capabilities in 3D worlds. To address challenges in multi-task RL representation, we analyze and establish cross-view goal specification as a unified multi-task goal space for visuomotor policies. Furthermore, to overcome the significant bottleneck of manual task design, we propose automated task synthesis within the highly customizable Minecraft environment for large-scale multi-task RL training, and we construct an efficient distributed RL framework to support this. Experimental results show RL significantly boosts interaction success rates by 4times and enables zero-shot generalization of spatial reasoning across diverse environments, including real-world settings. Our findings underscore the immense potential of RL training in 3D simulated environments, especially those amenable to large-scale task generation, for significantly advancing visuomotor agents' spatial reasoning."
    },
    {
      "title": "villa-X：增强视觉-语言-动作模型中的潜在动作建模 (原标题: villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models)",
      "link": "https://arxiv.org/abs/2507.23682",
      "pubDate": "Thu, 31 Jul 2025 11:57:46 GMT",
      "isoDate": "2025-07-31T11:57:46.000Z",
      "creator": "Xiaoyu Chen, Hangxing Wei, Pushi Zhang, Chuheng Zhang, Kaixin Wang, Yanjiang Guo, Rushuai Yang, Yucen Wang, Xinquan Xiao, Li Zhao, Jianyu Chen, Jiang Bian",
      "summary": "## villa-X：增强机器人操作的潜在动作建模\n\n### 引言\n\n视觉-语言-动作（VLA）模型已成为学习机器人操作策略的流行范式。这类模型能够理解语言指令并泛化到新场景。近期研究开始探索将“潜在动作”——一种表示两帧之间视觉变化的抽象表示——整合到VLA预训练中。\n\n### villa-X 框架\n\n本文引入了 **villa-X**，这是一个新颖的视觉-语言-潜在动作（ViLLA）框架，旨在推进潜在动作建模，以学习更具泛化能力的机器人操作策略。该方法在以下两个关键方面进行了改进：\n\n*   **潜在动作的学习方式**：优化了潜在动作的获取和表示方法。\n*   **潜在动作的整合方式**：改进了将潜在动作融入VLA预训练过程的机制。\n\n### 性能表现\n\n通过这些贡献，villa-X 在多个模拟环境（包括 SIMPLER 和 LIBERO）以及两个真实世界机器人设置（包括抓手和灵巧手操作）中均取得了卓越的性能。\n\n### 结论与展望\n\n研究人员认为，ViLLA 范式具有巨大的前景，而 villa-X 为未来的相关研究奠定了坚实的基础。",
      "shortSummary": "villa-X 是一种新颖的视觉-语言-潜在动作（ViLLA）框架，旨在增强机器人操作中的潜在动作建模。它改进了潜在动作的学习和整合方式，使机器人能够更好地遵循语言指令并泛化到新场景。该框架在模拟环境和真实世界机器人设置中均表现出色，为未来的机器人操作研究奠定了坚实基础。",
      "translated_title": "villa-X：增强视觉-语言-动作模型中的潜在动作建模",
      "images": [],
      "contentSource": "完整文章",
      "content": "Visual-Language-Action (VLA) models have emerged as a popular paradigm for learning robot manipulation policies that can follow language instructions and generalize to novel scenarios. Recent work has begun to explore the incorporation of latent actions, an abstract representation of visual change between two frames, into VLA pre-training. In this paper, we introduce villa-X, a novel Visual-Language-Latent-Action (ViLLA) framework that advances latent action modeling for learning generalizable robot manipulation policies. Our approach improves both how latent actions are learned and how they are incorporated into VLA pre-training. Together, these contributions enable villa-X to achieve superior performance across simulated environments including SIMPLER and LIBERO, as well as on two real-world robot setups including gripper and dexterous hand manipulation. We believe the ViLLA paradigm holds significant promise, and that our villa-X provides a strong foundation for future research."
    },
    {
      "title": "关于Softmax注意力机制的表达能力：一种循环神经网络视角 (原标题: On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective)",
      "link": "https://arxiv.org/abs/2507.23632",
      "pubDate": "Thu, 31 Jul 2025 11:10:03 GMT",
      "isoDate": "2025-07-31T11:10:03.000Z",
      "creator": "Gabriel Mongaras, Eric C. Larson",
      "summary": "# Softmax注意力机制的表达能力：一种循环神经网络视角\n\n## 引言\n\n自其引入以来，Softmax注意力机制已成为现代Transformer架构的核心支柱，这主要归因于其强大的表达能力以及在各种任务中的可扩展性。然而，Softmax注意力机制的主要缺点在于其内存需求和计算复杂度与序列长度呈二次方关系，这在处理长序列时构成了显著的瓶颈。\n\n## 线性注意力机制的挑战\n\n为了规避Softmax注意力的二次方瓶颈，研究人员引入了线性注意力及其类似方法，这些方法通过替换Softmax非线性函数来降低复杂度。尽管这些线性形式的注意力机制源自原始的Softmax公式，但它们在下游任务的准确性方面通常表现出滞后。虽然关于查询（query）和键（key）内积上的Softmax非线性函数具有优于其他非线性函数的强烈直观特性，但这种性能差异存在的原因仍然是一个悬而未决的问题。\n\n## 本文贡献与发现\n\n本研究旨在解决上述问题，并深入探讨Softmax注意力机制的内在表达能力。主要贡献和发现包括：\n\n*   **近似关系证明：** 本文通过推导出Softmax注意力的循环形式，明确证明了线性注意力是Softmax注意力的一种近似。这一关键发现为理解两种机制之间的关系提供了新的理论基础。\n*   **RNN视角：** 利用推导出的循环形式，Softmax注意力的每个组成部分都可以用循环神经网络（RNN）的语言进行描述。这种将Softmax注意力视为RNN的描述方式，为深入分析其内部机制提供了一个强大且直观的框架。\n*   **组件消融分析：** 将Softmax注意力描述为RNN，使得对Softmax注意力组件进行消融研究成为可能。通过这种方法，研究人员能够系统地理解每个部分的重要性以及它们如何相互作用，从而揭示Softmax注意力为何能够展现出更强的表达能力。\n*   **解释表达能力：** 最终，本研究的工作有助于解释为什么Softmax注意力比其替代品（如线性注意力）更具表达能力。这不仅加深了我们对现有注意力机制的理解，也为未来更高效、更强大的注意力机制的设计和优化提供了重要的理论指导。",
      "shortSummary": "Softmax注意力是Transformer核心，但存在二次方复杂度。线性注意力旨在解决此问题，但准确性较低，其原因不明。本文通过推导Softmax注意力的循环形式，证明线性注意力是其近似。研究将Softmax注意力描述为循环神经网络（RNN），这有助于理解其组件作用及其相互作用，并最终解释了Softmax注意力为何比其替代品更具表达能力。",
      "translated_title": "关于Softmax注意力机制的表达能力：一种循环神经网络视角",
      "images": [],
      "contentSource": "完整文章",
      "content": "Since its introduction, softmax attention has become the backbone of modern transformer architectures due to its expressiveness and scalability across a wide range of tasks. However, the main drawback of softmax attention is the quadratic memory requirement and computational complexity with respect to the sequence length. By replacing the softmax nonlinearity, linear attention and similar methods have been introduced to avoid the quadratic bottleneck of softmax attention. Despite these linear forms of attention being derived from the original softmax formulation, they typically lag in terms of downstream accuracy. While strong intuition of the softmax nonlinearity on the query and key inner product suggests that it has desirable properties compared to other nonlinearities, the question of why this discrepancy exists still remains unanswered. This work demonstrates that linear attention is an approximation of softmax attention by deriving the recurrent form of softmax attention. Using this form, each part of softmax attention can be described in the language of recurrent neural networks (RNNs). Describing softmax attention as an RNN allows for the ablation of the components of softmax attention to understand the importance of each part and how they interact. In this way, our work helps explain why softmax attention is more expressive than its counterparts."
    },
    {
      "title": "超越线性瓶颈：基于样条的知识蒸馏用于文化多元艺术风格分类 (原标题: Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for Culturally Diverse Art Style Classification)",
      "link": "https://arxiv.org/abs/2507.23436",
      "pubDate": "Thu, 31 Jul 2025 07:16:00 GMT",
      "isoDate": "2025-07-31T07:16:00.000Z",
      "creator": "Abdellah Zakaria Sellam, Salah Eddine Bekhouche, Cosimo Distante, Abdelmalik Taleb-Ahmed",
      "summary": "### 艺术风格分类的挑战与现有方法的局限\n\n艺术风格分类在计算美学领域仍是一个巨大挑战，主要原因有二：\n\n*   **数据集稀缺**：缺乏经过专家标注的艺术风格数据集。\n*   **复杂交互**：风格元素之间存在复杂且通常是非线性的相互作用。\n\n尽管近期出现的双教师自监督框架在一定程度上减少了对标注数据的依赖，但它们存在显著局限性：\n\n*   **线性投影层**：其固有的线性特性难以捕捉复杂的风格特征交互。\n*   **局部关注**：过于侧重局部信息，难以有效建模全局构图上下文。\n\n### 提出的解决方案：基于KAN的知识蒸馏\n\n为了克服上述局限，本文提出了一种增强型的双教师知识蒸馏框架。核心改进在于：\n\n*   **替换传统MLP**：将传统的MLP（多层感知机）投影和预测头替换为**Kolmogorov-Arnold Networks (KANs)**。\n\n### 方法原理与优势\n\n该方法通过以下机制实现性能提升：\n\n*   **保留互补指导**：\n    *   一个教师网络侧重于**局部纹理和笔触模式**。\n    *   另一个教师网络捕捉**更广泛的风格层次结构**。\n    *   这种双教师结构确保了对艺术风格多方面信息的全面学习。\n*   **KAN的非线性建模能力**：\n    *   利用KAN基于**样条的激活函数**，能够以数学精度建模复杂的非线性特征关联。\n    *   这使得模型能够更好地理解和解缠艺术风格中固有的复杂、非线性的特征关系。\n\n### 实验结果与发现\n\n*   **性能提升**：在WikiArt和Pandora18k数据集上的实验结果表明，本文提出的方法在Top-1准确率上优于基础的双教师架构。\n*   **KAN的重要性**：研究结果强调了KAN在**解缠复杂风格流形**方面的重要性，这直接导致了比传统MLP投影更好的线性探测准确率。\n\n### 结论\n\n本研究证明了将KAN引入知识蒸馏框架，能够有效解决艺术风格分类中线性瓶颈和复杂非线性交互建模的挑战，为文化多元艺术风格的精确分类提供了新的途径。",
      "shortSummary": "针对艺术风格分类中数据稀缺和复杂非线性交互的挑战，本文提出一种增强型双教师知识蒸馏框架。该框架用Kolmogorov-Arnold Networks (KANs) 替换传统MLP，利用KAN的样条基激活函数精确建模非线性特征关联，同时保留双教师的互补指导。实验证明，该方法在WikiArt和Pandora18k数据集上优于基线模型，显著提升了Top-1准确率和线性探测准确率，突显了KAN在解缠复杂风格流形中的关键作用。",
      "translated_title": "超越线性瓶颈：基于样条的知识蒸馏用于文化多元艺术风格分类",
      "images": [],
      "contentSource": "完整文章",
      "content": "Art style classification remains a formidable challenge in computational aesthetics due to the scarcity of expertly labeled datasets and the intricate, often nonlinear interplay of stylistic elements. While recent dual-teacher self-supervised frameworks reduce reliance on labeled data, their linear projection layers and localized focus struggle to model global compositional context and complex style-feature interactions. We enhance the dual-teacher knowledge distillation framework to address these limitations by replacing conventional MLP projection and prediction heads with Kolmogorov-Arnold Networks (KANs). Our approach retains complementary guidance from two teacher networks, one emphasizing localized texture and brushstroke patterns, the other capturing broader stylistic hierarchies while leveraging KANs' spline-based activations to model nonlinear feature correlations with mathematical precision. Experiments on WikiArt and Pandora18k demonstrate that our approach outperforms the base dual teacher architecture in Top-1 accuracy. Our findings highlight the importance of KANs in disentangling complex style manifolds, leading to better linear probe accuracy than MLP projections."
    },
    {
      "title": "基于注意力相关性评分的增强型阿拉伯语文本检索 (原标题: Enhanced Arabic Text Retrieval with Attentive Relevance Scoring)",
      "link": "https://arxiv.org/abs/2507.23404",
      "pubDate": "Thu, 31 Jul 2025 06:18:28 GMT",
      "isoDate": "2025-07-31T06:18:28.000Z",
      "creator": "Salah Eddine Bekhouche, Azeddine Benlamoudi, Yazid Bounab, Fadi Dornaika, Abdenour Hadid",
      "summary": "## 基于注意力相关性评分的增强型阿拉伯语文本检索\n\n### 挑战与背景\n\n*   **阿拉伯语的复杂性**：阿拉伯语在自然语言处理（NLP）和信息检索（IR）领域面临独特挑战，主要原因包括：\n    *   复杂的形态学。\n    *   可选的变音符号。\n    *   现代标准阿拉伯语（MSA）与各种方言并存。\n*   **研究不足**：尽管阿拉伯语在全球日益重要，但在NLP研究和基准资源方面仍处于代表性不足的状态。\n\n### 提出的解决方案\n\n*   **增强型密集段落检索（DPR）框架**：本文提出了一种专门为阿拉伯语开发的增强型DPR框架。\n*   **核心创新：注意力相关性评分（ARS）**：\n    *   ARS是该方法的核心，它取代了标准的交互机制。\n    *   ARS采用了一种自适应评分函数，能够更有效地建模问题和段落之间的语义相关性。\n*   **技术整合**：该方法集成了预训练的阿拉伯语语言模型和架构改进。\n\n### 成果与影响\n\n*   **性能提升**：显著提高了检索性能。\n*   **排名准确性**：在回答阿拉伯语问题时，显著提高了排名准确性。\n\n### 可用性\n\n*   **代码公开**：相关代码已在GitHub上公开提供。",
      "shortSummary": "本文提出了一种增强型密集段落检索（DPR）框架，旨在解决阿拉伯语在自然语言处理和信息检索中的挑战。该框架的核心是新颖的注意力相关性评分（ARS），它通过自适应评分函数更有效地建模问题与段落间的语义相关性。该方法整合了预训练的阿拉伯语语言模型，显著提升了阿拉伯语文本检索性能和排名准确性。代码已公开。",
      "translated_title": "基于注意力相关性评分的增强型阿拉伯语文本检索",
      "images": [],
      "contentSource": "完整文章",
      "content": "Arabic poses a particular challenge for natural language processing (NLP) and information retrieval (IR) due to its complex morphology, optional diacritics and the coexistence of Modern Standard Arabic (MSA) and various dialects. Despite the growing global significance of Arabic, it is still underrepresented in NLP research and benchmark resources. In this paper, we present an enhanced Dense Passage Retrieval (DPR) framework developed specifically for Arabic. At the core of our approach is a novel Attentive Relevance Scoring (ARS) that replaces standard interaction mechanisms with an adaptive scoring function that more effectively models the semantic relevance between questions and passages. Our method integrates pre-trained Arabic language models and architectural refinements to improve retrieval performance and significantly increase ranking accuracy when answering Arabic questions. The code is made publicly available at https://github.com/Bekhouche/APR{GitHub}."
    },
    {
      "title": "NeRF 是 3D Gaussian Splatting 的宝贵助手 (原标题: NeRF Is a Valuable Assistant for 3D Gaussian Splatting)",
      "link": "https://arxiv.org/abs/2507.23374",
      "pubDate": "Thu, 31 Jul 2025 05:43:31 GMT",
      "isoDate": "2025-07-31T05:43:31.000Z",
      "creator": "Shuangkang Fang, I-Chao Shen, Takeo Igarashi, Yufeng Wang, ZeSheng Wang, Yi Yang, Wenrui Ding, Shuchang Zhou",
      "summary": "### NeRF-GS：NeRF 与 3D Gaussian Splatting 的联合优化框架\n\n*   **引言与背景**\n    *   3D Gaussian Splatting (3DGS) 是一种高效的 3D 场景表示方法，但在实际应用中存在一些局限性。\n    *   这些局限性包括：对高斯初始化敏感、空间感知能力有限以及高斯间关联性较弱。\n    *   这些问题影响了 3DGS 的性能表现。\n\n*   **NeRF-GS 框架介绍**\n    *   本文提出了一种名为 NeRF-GS 的新型框架，旨在解决 3DGS 的上述局限性。\n    *   NeRF-GS 的核心思想是联合优化神经辐射场 (NeRF) 和 3D Gaussian Splatting (3DGS)。\n    *   该框架利用 NeRF 固有的连续空间表示能力来增强 3DGS。\n\n*   **核心方法与技术**\n    *   **空间特征对齐**：NeRF-GS 重新审视了 3DGS 的设计，并逐步将其空间特征与 NeRF 对齐。\n    *   **共享 3D 空间信息**：通过共享 3D 空间信息，NeRF 和 3DGS 两种表示可以在同一场景中进行联合优化。\n    *   **残差向量优化**：为了进一步解决两种方法之间的形式差异，NeRF-GS 优化了隐式特征和高斯位置的残差向量。这有助于增强 3DGS 的个性化能力。\n\n*   **实验结果与贡献**\n    *   在基准数据集上的实验结果表明，NeRF-GS 优于现有方法，并取得了最先进的性能。\n    *   这一结果证实了 NeRF 和 3DGS 之间是互补而非竞争的关系。\n    *   NeRF-GS 为结合 3DGS 和 NeRF 的混合方法提供了新的见解，以实现高效的 3D 场景表示。\n\n*   **其他信息**\n    *   该研究已被 ICCV 接受。",
      "shortSummary": "NeRF-GS 是一种新颖的框架，通过联合优化 NeRF 和 3D Gaussian Splatting (3DGS) 来提升 3DGS 的性能。它利用 NeRF 的连续空间表示，解决了 3DGS 对初始化敏感、空间感知弱和高斯间关联性差等问题。NeRF-GS 通过共享 3D 空间信息和优化残差向量，使两者在同一场景中协同优化。实验证明，NeRF-GS 取得了最先进的性能，表明 NeRF 和 3DGS 具有互补性，为高效 3D 场景表示提供了混合方法的新思路。",
      "translated_title": "NeRF 是 3D Gaussian Splatting 的宝贵助手",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce NeRF-GS, a novel framework that jointly optimizes Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework leverages the inherent continuous spatial representation of NeRF to mitigate several limitations of 3DGS, including sensitivity to Gaussian initialization, limited spatial awareness, and weak inter-Gaussian correlations, thereby enhancing its performance. In NeRF-GS, we revisit the design of 3DGS and progressively align its spatial features with NeRF, enabling both representations to be optimized within the same scene through shared 3D spatial information. We further address the formal distinctions between the two approaches by optimizing residual vectors for both implicit features and Gaussian positions to enhance the personalized capabilities of 3DGS. Experimental results on benchmark datasets show that NeRF-GS surpasses existing methods and achieves state-of-the-art performance. This outcome confirms that NeRF and 3DGS are complementary rather than competing, offering new insights into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene representation."
    },
    {
      "title": "迈向指代音视频分割中的全模态表达与推理 (原标题: Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual Segmentation)",
      "link": "https://arxiv.org/abs/2507.22886",
      "pubDate": "Wed, 30 Jul 2025 13:59:31 GMT",
      "isoDate": "2025-07-30T13:59:31.000Z",
      "creator": "Kaining Ying, Henghui Ding, Guanquan Jie, Yu-Gang Jiang",
      "summary": "### 迈向指代音视频分割中的全模态表达与推理\n\n**背景与挑战**\n指代音视频分割（RAVS）领域近期取得了显著进展，但在整合多模态信息、深入理解和推理音视频内容方面仍面临挑战。\n\n**OmniAVS 数据集**\n为了拓展RAVS的边界并促进该领域的未来研究，本文提出了一个名为OmniAVS的新数据集。\n*   **规模与内容**：包含2,098个视频和59,458个多模态指代表达。\n*   **三大创新点**：\n    1.  **八种多模态表达类型**：灵活结合文本、语音、声音和视觉线索。\n    2.  **强调音频内容理解**：超越仅仅检测音频的存在，更注重其深层含义。\n    3.  **包含复杂推理和世界知识**：在表达中融入了需要复杂推理和世界知识才能理解的内容。\n\n**Omnimodal Instructed Segmentation Assistant (OISA) 模型**\n为应对OmniAVS数据集中多模态推理和音视频内容细粒度理解的挑战，本文引入了Omnimodal Instructed Segmentation Assistant (OISA) 模型。\n*   **核心机制**：OISA利用多模态大语言模型（MLLM）来理解复杂的线索并执行基于推理的分割任务。\n*   **性能表现**：广泛的实验表明，OISA在OmniAVS数据集上优于现有方法，并在其他相关任务中也取得了有竞争力的结果。\n\n**其他信息**\n*   本文已被ICCV 2025接收。",
      "shortSummary": "本文提出了OmniAVS数据集和OISA模型，旨在推动指代音视频分割（RAVS）领域的发展。OmniAVS包含2098个视频和近6万个多模态表达，其创新在于引入了8种灵活结合文本、语音、声音和视觉线索的表达类型，并强调对音频内容的深入理解及复杂推理。OISA是一个基于多模态大语言模型（MLLM）的助手，能够理解复杂线索并执行基于推理的分割。实验证明OISA在OmniAVS上表现优异，并超越现有方法。",
      "translated_title": "迈向指代音视频分割中的全模态表达与推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Referring audio-visual segmentation (RAVS) has recently seen significant advancements, yet challenges remain in integrating multimodal information and deeply understanding and reasoning about audiovisual content. To extend the boundaries of RAVS and facilitate future research in this field, we propose Omnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset containing 2,098 videos and 59,458 multimodal referring expressions. OmniAVS stands out with three key innovations: (1) 8 types of multimodal expressions that flexibly combine text, speech, sound, and visual cues; (2) an emphasis on understanding audio content beyond just detecting their presence; and (3) the inclusion of complex reasoning and world knowledge in expressions. Furthermore, we introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the challenges of multimodal reasoning and fine-grained understanding of audiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and perform reasoning-based segmentation. Extensive experiments show that OISA outperforms existing methods on OmniAVS and achieves competitive results on other related tasks."
    },
    {
      "title": "C3：一个用于探索复杂对话中挑战的双语口语对话模型基准 (原标题: C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations)",
      "link": "https://arxiv.org/abs/2507.22968",
      "pubDate": "Wed, 30 Jul 2025 13:56:23 GMT",
      "isoDate": "2025-07-30T13:56:23.000Z",
      "creator": "Chengqian Ma, Wei Tao, Yiwen Guo",
      "summary": "### C3：探索复杂对话中挑战的双语口语对话模型基准\n\n**引言**\n\n口语对话模型（SDMs）因其直接生成语音响应的能力而备受关注。然而，与受益于广泛基准测试的文本大语言模型（LLMs）相比，SDMs在全面理解和模仿人类对话方面的实际效果研究存在空白。\n\n**口语交互的复杂性与挑战**\n\n人类语音交互本质上比文本更为复杂，主要体现在以下几个方面：\n\n*   **歧义性：**\n    *   **语义因素：** 如多义词（polysemy）。\n    *   **语音因素：** 如同形异义词（heterograph）、同音异形异义词（heteronyms）和重音模式（stress patterns）。\n*   **上下文依赖性：**\n    *   省略（omission）。\n    *   共指（coreference）。\n    *   多轮交互（multi-turn interaction）。\n\n**C3基准数据集的提出**\n\n为了揭示SDM的当前发展状况并应对上述挑战，本文提出了一个名为C3的基准数据集。\n\n*   **数据集构成：** 包含1,079个实例。\n*   **语言覆盖：** 涵盖英语和中文，是一个双语数据集。\n*   **评估方法：** 配备了一种基于大语言模型（LLM）的评估方法，该方法与人类判断高度一致。\n\n**目的**\n\n该数据集旨在促进对SDM在处理这些实际挑战时的性能进行全面探索。",
      "shortSummary": "本文提出了C3，一个双语基准数据集，旨在评估口语对话模型（SDMs）在复杂人类对话中的表现。与文本大语言模型（LLMs）相比，SDMs缺乏全面基准测试。语音对话的复杂性源于歧义性（语义和语音）和上下文依赖性。该数据集包含1,079个英语和中文实例，并采用与人类判断一致的LLM评估方法，以全面探索SDM应对这些挑战的能力。",
      "translated_title": "C3：一个用于探索复杂对话中挑战的双语口语对话模型基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Spoken Dialogue Models (SDMs) have recently attracted significant attention for their ability to generate voice responses directly to users' spoken queries. Despite their increasing popularity, there exists a gap in research focused on comprehensively understanding their practical effectiveness in comprehending and emulating human conversations. This is especially true compared to text-based Large Language Models (LLMs), which benefit from extensive benchmarking. Human voice interactions are inherently more complex than text due to characteristics unique to spoken dialogue. Ambiguity poses one challenge, stemming from semantic factors like polysemy, as well as phonological aspects such as heterograph, heteronyms, and stress patterns. Additionally, context-dependency, like omission, coreference, and multi-turn interaction, adds further complexity to human conversational dynamics. To illuminate the current state of SDM development and to address these challenges, we present a benchmark dataset in this paper, which comprises 1,079 instances in English and Chinese. Accompanied by an LLM-based evaluation method that closely aligns with human judgment, this dataset facilitates a comprehensive exploration of the performance of SDMs in tackling these practical challenges."
    },
    {
      "title": "RecGPT 技术报告 (原标题: RecGPT Technical Report)",
      "link": "https://arxiv.org/abs/2507.22879",
      "pubDate": "Wed, 30 Jul 2025 13:55:06 GMT",
      "isoDate": "2025-07-30T13:55:06.000Z",
      "creator": "Chao Yi, Dian Chen, Gaoyang Guo, Jiakai Tang, Jian Wu, Jing Yu, Sunhao Dai, Wen Chen, Wenjun Yang, Yuning Jiang, Zhujin Gao, Bo Zheng, Chi Li, Dimin Wang, Dixuan Wang, Fan Li, Fan Zhang, Haibin Chen, Haozhuang Liu, Jialin Zhu, Jiamang Wang, Jiawei Wu, Jin Cui, Ju Huang, Kai Zhang, Kan Liu, Lang Tian, Liang Rao, Longbin Li, Lulu Zhao, Mao Zhang, Na He, Peiyang Wang, Qiqi Huang, Tao Luo, Wenbo Su, Xiaoxiao He, Xin Tong, Xu Chen, Xunke Xi, Yang Li, Yaxuan Wu, Yeqiu Yang, Yi Hu, Yinnan Song, Yuchen Li, Yujie Luo, Yujin Yuan, Yuliang Yan, Zhengyang Wang, Zhibo Xiao, Zhixin Ma, Zile Zhou",
      "summary": "## RecGPT：以用户意图为中心的下一代推荐系统\n\n### 现有推荐系统的挑战\n当前的工业推荐系统主要依赖于历史共现模式和“日志拟合”目标，即优化用户过去的交互行为，但未能明确建模用户意图。这种方法导致：\n*   **过度拟合**：过度依赖狭窄的历史偏好。\n*   **未能捕捉兴趣**：无法捕捉用户不断演变和潜在的兴趣。\n*   **强化负面效应**：加剧“信息茧房”和“长尾效应”。\n*   **损害用户体验**：最终影响用户满意度并威胁整个推荐生态系统的可持续性。\n\n### RecGPT 框架的提出\n为解决上述挑战，研究人员重新思考了推荐系统的整体设计范式，并提出了 RecGPT——一个将用户意图置于推荐流程核心的下一代框架。\n\n### RecGPT 的核心机制\nRecGPT 通过将大型语言模型（LLMs）集成到推荐管道的关键阶段，将传统的“日志拟合”推荐转变为“意图中心”的流程。具体集成点包括：\n*   **用户兴趣挖掘**：更深入地理解用户潜在和演变的兴趣。\n*   **物品检索**：根据用户意图更精准地检索相关物品。\n*   **解释生成**：为推荐结果提供更具洞察力的解释。\n\n### LLM 的对齐与训练\n为了有效地将通用型 LLMs 应用于上述特定领域的推荐任务并实现规模化，RecGPT 采用了多阶段训练范式，该范式整合了：\n*   **推理增强的预对齐**：在训练早期阶段增强 LLM 的推理能力。\n*   **自训练演进**：通过自我学习和迭代优化模型性能。\n*   **人机协作判断系统**：由人类和 LLM 共同组成的判断系统对训练过程进行指导和评估。\n\n### 部署与在线实验成果\nRecGPT 已在淘宝 App 上全面部署。在线实验结果表明，RecGPT 在所有利益相关者之间均取得了显著且一致的性能提升：\n*   **用户受益**：内容多样性增加，满意度提高。\n*   **商家受益**：获得更大的曝光和更高的转化率。\n*   **平台受益**：整体生态系统更加健康和可持续。\n\n### 结论\n这些全面的改进结果验证了 LLM 驱动的、以意图为中心的设计能够促进一个更可持续、互利共赢的推荐生态系统。",
      "shortSummary": "RecGPT 是一种以用户意图为中心的新一代推荐系统，旨在解决传统系统过度依赖历史数据、忽视用户意图的问题。它通过将大型语言模型（LLMs）集成到用户兴趣挖掘、物品检索和解释生成等关键环节，将推荐过程从“日志拟合”转变为“意图驱动”。RecGPT 采用多阶段训练范式，并已在淘宝 App 全面部署。在线实验表明，RecGPT 显著提升了用户满意度、内容多样性，并增加了商家曝光和平台转化，验证了其可持续和互利共赢的潜力。",
      "translated_title": "RecGPT 技术报告",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recommender systems are among the most impactful applications of artificial intelligence, serving as critical infrastructure connecting users, merchants, and platforms. However, most current industrial systems remain heavily reliant on historical co-occurrence patterns and log-fitting objectives, i.e., optimizing for past user interactions without explicitly modeling user intent. This log-fitting approach often leads to overfitting to narrow historical preferences, failing to capture users' evolving and latent interests. As a result, it reinforces filter bubbles and long-tail phenomena, ultimately harming user experience and threatening the sustainability of the whole recommendation ecosystem.   To address these challenges, we rethink the overall design paradigm of recommender systems and propose RecGPT, a next-generation framework that places user intent at the center of the recommendation pipeline. By integrating large language models (LLMs) into key stages of user interest mining, item retrieval, and explanation generation, RecGPT transforms log-fitting recommendation into an intent-centric process. To effectively align general-purpose LLMs to the above domain-specific recommendation tasks at scale, RecGPT incorporates a multi-stage training paradigm, which integrates reasoning-enhanced pre-alignment and self-training evolution, guided by a Human-LLM cooperative judge system. Currently, RecGPT has been fully deployed on the Taobao App. Online experiments demonstrate that RecGPT achieves consistent performance gains across stakeholders: users benefit from increased content diversity and satisfaction, merchants and the platform gain greater exposure and conversions. These comprehensive improvement results across all stakeholders validates that LLM-driven, intent-centric design can foster a more sustainable and mutually beneficial recommendation ecosystem."
    },
    {
      "title": "Repair-R1：修复前先进行更好的测试 (原标题: Repair-R1: Better Test Before Repair)",
      "link": "https://arxiv.org/abs/2507.22853",
      "pubDate": "Wed, 30 Jul 2025 13:24:05 GMT",
      "isoDate": "2025-07-30T13:24:05.000Z",
      "creator": "Haichuan Hu, Xiaochen Xie, Quanjun Zhang",
      "summary": "# Repair-R1：一种改进的自动化程序修复方法\n\n## 引言\n\n自动化程序修复（APR）旨在自动定位程序缺陷、生成补丁并验证修复。现有基于大型语言模型（LLM）的APR方法通常仅在推理阶段利用测试用例，采用迭代式方法：先执行修复，再通过测试验证。这种传统范式忽略了两个重要方面：\n\n*   测试用例在训练阶段的潜在贡献。\n*   在修复之前利用测试的可能性。\n\n## Repair-R1 方法\n\n为解决上述问题，本文提出了 **Repair-R1** 方法，其核心创新在于：\n\n*   将测试用例引入模型的训练阶段。\n*   将测试生成提前至修复之前。\n\n**Repair-R1 的工作流程：**\n\n1.  模型首先被要求生成能够区分缺陷行为的判别性测试用例。\n2.  然后，模型基于这些生成的测试用例执行修复。\n\n这种方法使模型能够更好地定位缺陷并理解缺陷的根本原因，从而显著提高修复效果。\n\n## 实现细节\n\n*   Repair-R1 使用了三种不同的骨干模型进行实现。\n*   采用强化学习（RL）来共同优化测试生成和错误修复过程。\n\n## 实验结果\n\n在四个广泛采用的基准测试上进行的实验结果表明了 Repair-R1 的优越性。与传统的（vanilla）模型相比，Repair-R1 取得了显著的性能提升：\n\n*   **修复成功率：** 提高 2.68% 至 48.29%。\n*   **测试生成成功率：** 提高 16.38% 至 53.28%。\n*   **测试覆盖率：** 提高 0.78% 至 53.96%。\n\n## 代码和权重\n\n该项目的代码和权重已公开发布。",
      "shortSummary": "Repair-R1 是一种改进的自动化程序修复（APR）方法，旨在解决现有基于大型语言模型（LLM）的 APR 方法中测试用例利用不足的问题。它将测试生成提前到修复之前，并把测试用例引入模型训练阶段。模型首先生成判别性测试用例，然后基于这些测试进行修复。实验结果表明，Repair-R1 在修复成功率、测试生成成功率和测试覆盖率方面均显著优于传统模型，有效提升了程序修复效果。",
      "translated_title": "Repair-R1：修复前先进行更好的测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "APR (Automated Program Repair) aims to automatically locate program defects, generate patches and validate the repairs. Existing techniques for APR are often combined with LLMs (Large Language Models), which leverages the code-related knowledge of LLMs to improve repair effectiveness. Current LLM-based APR methods typically utilize test cases only during the inference stage, adopting an iterative approach that performs repair first and validates it through test execution afterward. This conventional paradigm neglects two important aspects: the potential contribution of test cases in the training phase, and the possibility of leveraging testing prior to repair. To address this, we propose Repair-R1, which introduces test cases into the model's training phase and shifts test generation to precede repair. The model is required to first generate discriminative test cases that can distinguish defective behaviors, and then perform repair based on these tests. This enables the model to better locate defects and understand the underlying causes of defects, thereby improving repair effectiveness. We implement Repair-R1 with three different backbone models, using RL (reinforcement learning) to co-optimize test generation and bug repair. Experimental results on four widely adopted benchmarks demonstrate the superiority of Repair-R1. Specially, compared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to 48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage by 0.78\\% to 53.96\\%. We publish the code and weights at https://github.com/Tomsawyerhu/APR-RL and https://huggingface.co/tomhu/Qwen3-4B-RL-5000-step."
    },
    {
      "title": "ScreenCoder：通过模块化多模态代理推进前端自动化中的视觉到代码生成 (原标题: ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents)",
      "link": "https://arxiv.org/abs/2507.22827",
      "pubDate": "Wed, 30 Jul 2025 12:41:21 GMT",
      "isoDate": "2025-07-30T12:41:21.000Z",
      "creator": "Yilei Jiang, Yaozhi Zheng, Yuxuan Wan, Jiaming Han, Qunzhong Wang, Michael R. Lyu, Xiangyu Yue",
      "summary": "# ScreenCoder：通过模块化多模态代理推进前端自动化中的视觉到代码生成\n\n## 引言与背景\n将用户界面（UI）设计自动化转换为前端代码，对于加速软件开发和普及设计工作流程具有重要意义。尽管近期的大型语言模型（LLM）在文本到代码生成方面取得了进展，但许多现有方法仅依赖自然语言提示，这限制了它们在捕捉空间布局和视觉设计意图方面的有效性。然而，实际的UI开发本质上是多模态的，通常从视觉草图或模型开始。\n\n## ScreenCoder 框架：模块化多模态代理\n为解决现有方法的不足，本文引入了一个名为 **ScreenCoder** 的模块化多代理框架，该框架以三个可解释的阶段执行UI到代码的生成：\n\n*   **基础代理（Grounding Agent）**：利用视觉-语言模型（VLM）检测并标记UI组件。\n*   **规划代理（Planning Agent）**：利用前端工程先验知识构建分层布局。\n*   **生成代理（Generation Agent）**：通过自适应的基于提示的合成生成HTML/CSS代码。\n\n## 设计优势\n这种模块化设计相较于端到端的黑盒方法，显著提高了系统的鲁棒性、可解释性和保真度。\n\n## 可扩展数据引擎与模型训练\n*   **数据生成**：该框架被扩展为一个可扩展的数据引擎，能够自动生成大规模的图像-代码对（合成示例）。\n*   **模型微调与强化**：利用这些合成示例，研究人员对一个开源的视觉-语言模型（VLM）进行了微调和强化。\n\n## 实验结果与性能\n通过这种方法，模型在UI理解和代码质量方面取得了显著提升。广泛的实验表明，ScreenCoder 在布局准确性、结构连贯性和代码正确性方面均达到了最先进的性能。\n\n## 代码可用性\n相关代码已公开提供。",
      "shortSummary": "ScreenCoder 提出一个模块化多代理框架，用于将UI设计自动化转换为前端HTML/CSS代码。该框架分为基础、规划和生成三个阶段，利用视觉-语言模型理解UI，并结合前端工程知识构建布局。通过生成大规模合成数据来微调和强化模型，ScreenCoder 显著提升了UI理解和代码质量，并在布局准确性、结构连贯性和代码正确性方面达到了最先进水平，解决了现有文本到代码方法在视觉理解上的局限。",
      "translated_title": "ScreenCoder：通过模块化多模态代理推进前端自动化中的视觉到代码生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Automating the transformation of user interface (UI) designs into front-end code holds significant promise for accelerating software development and democratizing design workflows. While recent large language models (LLMs) have demonstrated progress in text-to-code generation, many existing approaches rely solely on natural language prompts, limiting their effectiveness in capturing spatial layout and visual design intent. In contrast, UI development in practice is inherently multimodal, often starting from visual sketches or mockups. To address this gap, we introduce a modular multi-agent framework that performs UI-to-code generation in three interpretable stages: grounding, planning, and generation. The grounding agent uses a vision-language model to detect and label UI components, the planning agent constructs a hierarchical layout using front-end engineering priors, and the generation agent produces HTML/CSS code via adaptive prompt-based synthesis. This design improves robustness, interpretability, and fidelity over end-to-end black-box methods. Furthermore, we extend the framework into a scalable data engine that automatically produces large-scale image-code pairs. Using these synthetic examples, we fine-tune and reinforce an open-source VLM, yielding notable gains in UI understanding and code quality. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in layout accuracy, structural coherence, and code correctness. Our code is made publicly available at https://github.com/leigest519/ScreenCoder."
    },
    {
      "title": "VL-Cogito：用于高级多模态推理的渐进式课程强化学习 (原标题: VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning)",
      "link": "https://arxiv.org/abs/2507.22607",
      "pubDate": "Wed, 30 Jul 2025 08:23:21 GMT",
      "isoDate": "2025-07-30T08:23:21.000Z",
      "creator": "Ruifeng Yuan, Chenghao Xiao, Sicong Leng, Jianyu Wang, Long Li, Weiwen Xu, Hou Pong Chan, Deli Zhao, Tingyang Xu, Zhongyu Wei, Hao Zhang, Yu Rong",
      "summary": "## VL-Cogito：用于高级多模态推理的渐进式课程强化学习\n\n### 研究背景与问题\n\n*   强化学习（RL）已被证实能有效增强大型语言模型（LLM）的推理能力。\n*   近期研究已将RL范式扩展到多模态推理任务中。\n*   然而，由于多模态任务固有的复杂性和多样性（尤其体现在语义内容和问题表述上），现有模型在不同领域和难度级别上的表现往往不稳定。\n\n### VL-Cogito 模型概述\n\n*   为解决上述局限性，研究人员提出了VL-Cogito，这是一种先进的多模态推理模型。\n*   该模型通过一种新颖的多阶段渐进式课程强化学习（PCuRL）框架进行训练。\n\n### 渐进式课程强化学习（PCuRL）框架\n\n*   PCuRL框架系统地引导模型逐步完成难度递增的任务，从而显著提升其在多样化多模态上下文中的推理能力。\n*   该框架引入了两项关键创新：\n    1.  **在线难度软加权机制：** 动态调整连续RL训练阶段的训练难度。\n    2.  **动态长度奖励机制：** 鼓励模型根据任务复杂性自适应地调节其推理路径长度，以平衡推理效率与正确性。\n\n### 实验结果与验证\n\n*   实验评估表明，VL-Cogito在涵盖数学、科学、逻辑和通用理解等主流多模态基准测试中，始终达到或超越了现有的面向推理的模型。\n*   这些结果充分验证了该方法的有效性。\n\n### 相关领域\n\n*   计算机视觉与模式识别 (cs.CV)\n*   人工智能 (cs.AI)\n*   计算与语言 (cs.CL)",
      "shortSummary": "VL-Cogito是一个先进的多模态推理模型，旨在解决现有模型在复杂多模态任务中表现不稳定的问题。它通过新颖的多阶段渐进式课程强化学习（PCuRL）框架进行训练。PCuRL引入了在线难度软加权和动态长度奖励机制，系统地提升模型推理能力。实验证明，VL-Cogito在数学、科学、逻辑和通用理解等多模态基准测试中，表现优于或媲美现有模型，验证了其有效性。",
      "translated_title": "VL-Cogito：用于高级多模态推理的渐进式课程强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement learning has proven its effectiveness in enhancing the reasoning capabilities of large language models. Recent research efforts have progressively extended this paradigm to multimodal reasoning tasks. Due to the inherent complexity and diversity of multimodal tasks, especially in semantic content and problem formulations, existing models often exhibit unstable performance across various domains and difficulty levels. To address these limitations, we propose VL-Cogito, an advanced multimodal reasoning model trained via a novel multi-stage Progressive Curriculum Reinforcement Learning (PCuRL) framework. PCuRL systematically guides the model through tasks of gradually increasing difficulty, substantially improving its reasoning abilities across diverse multimodal contexts. The framework introduces two key innovations: (1) an online difficulty soft weighting mechanism, dynamically adjusting training difficulty across successive RL training stages; and (2) a dynamic length reward mechanism, which encourages the model to adaptively regulate its reasoning path length according to task complexity, thus balancing reasoning efficiency with correctness. Experimental evaluations demonstrate that VL-Cogito consistently matches or surpasses existing reasoning-oriented models across mainstream multimodal benchmarks spanning mathematics, science, logic, and general understanding, validating the effectiveness of our approach."
    },
    {
      "title": "通过强化学习高效地对LLM进行差分隐私微调 (原标题: Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2507.22565",
      "pubDate": "Wed, 30 Jul 2025 06:46:53 GMT",
      "isoDate": "2025-07-30T06:46:53.000Z",
      "creator": "Afshin Khadangi, Amir Sartipi, Igor Tchappi, Ramin Bahmani, Gilbert Fridgen",
      "summary": "### RLDP：通过强化学习高效地对LLM进行差分隐私微调\n\n本文介绍了一种名为RLDP的新框架，旨在解决大型语言模型（LLM）在敏感数据上训练时数据隐私与模型效用之间的核心矛盾。\n\n#### 核心问题与现有挑战\n\n*   **隐私与效用冲突**：LLM在医疗等敏感语料库上训练时，数据隐私与模型效用之间的紧张关系是实际部署的主要瓶颈。\n*   **DP-SGD的局限性**：差分隐私随机梯度下降（DP-SGD）虽然能提供形式化的隐私保证，但代价高昂。它强制裁剪梯度并注入噪声，导致样本效率降低和最终准确性下降。\n*   **现有变体的不足**：尽管提出了许多DP-SGD变体来缓解这一权衡，但它们都存在一个共同的缺陷：其控制参数是硬编码、全局且对不断变化的优化过程不敏感。这迫使实践者要么为追求效用而过度消耗隐私预算，要么为满足隐私约束而接受平庸的模型。\n\n#### RLDP框架介绍\n\n*   **创新方法**：RLDP是首个将差分隐私优化本身视为一个闭环控制问题，并将其应用于现代深度强化学习（RL）的框架。\n*   **动态控制机制**：\n    *   RLDP能够持续感知学习动态的丰富统计信息。\n    *   它通过选择细粒度的**逐参数梯度裁剪阈值**以及注入高斯噪声的**幅度**来采取行动。\n*   **超策略训练**：一个软行动者-评论家（SAC）超策略在语言模型微调过程中在线训练。它从零开始学习如何以及何时将隐私预算分配到关键之处。\n\n#### 实验与成果\n\nRLDP在超过1,600次消融实验中，对多种LLM模型进行了评估，包括GPT2-small、Llama-1B、Llama-3B和Mistral-7B。\n\n*   **困惑度降低**：RLDP实现了1.3%至30.5%的困惑度降低（平均5.4%）。\n*   **下游效用提升**：平均下游效用增益为5.6%。\n*   **训练效率显著提升**：RLDP仅需13%至43%的梯度更新预算（平均加速71%）即可达到基线模型的最终效用。\n*   **隐私保障**：在实现上述性能提升的同时，RLDP严格遵守相同的（$\\epsilon$, $\\delta$）-DP契约。\n*   **抗攻击性**：RLDP对成员推断攻击和金丝雀提取攻击表现出相同或更低的敏感性。\n\n#### 结论\n\nRLDP通过将差分隐私优化转化为一个可学习的强化学习问题，显著提升了LLM在敏感数据上进行微调时的效用和效率，同时保持了强大的隐私保护。",
      "shortSummary": "RLDP是一种新颖的框架，通过将差分隐私（DP）优化视为强化学习（RL）的闭环控制问题，解决了LLM微调中隐私与效用之间的矛盾。它动态调整梯度裁剪和噪声注入，在线学习如何高效分配隐私预算。实验表明，RLDP在GPT2-small、Llama系列和Mistral-7B上显著降低了困惑度（平均5.4%），提升了下游效用（平均5.6%），并以更少的训练预算（平均加速71%）达到基线性能，同时保持了强大的DP保证和抗攻击性。",
      "translated_title": "通过强化学习高效地对LLM进行差分隐私微调",
      "images": [],
      "contentSource": "完整文章",
      "content": "The tension between data privacy and model utility has become the defining bottleneck for the practical deployment of large language models (LLMs) trained on sensitive corpora including healthcare. Differentially private stochastic gradient descent (DP-SGD) guarantees formal privacy, yet it does so at a pronounced cost: gradients are forcibly clipped and perturbed with noise, degrading sample efficiency and final accuracy. Numerous variants have been proposed to soften this trade-off, but they all share a handicap: their control knobs are hard-coded, global, and oblivious to the evolving optimization landscape. Consequently, practitioners are forced either to over-spend privacy budget in pursuit of utility, or to accept mediocre models in order to stay within privacy constraints. We present RLDP, the first framework to cast DP optimization itself as a closed-loop control problem amenable to modern deep reinforcement learning (RL). RLDP continuously senses rich statistics of the learning dynamics and acts by selecting fine-grained per parameter gradient-clipping thresholds as well as the magnitude of injected Gaussian noise. A soft actor-critic (SAC) hyper-policy is trained online during language model fine-tuning; it learns, from scratch, how to allocate the privacy budget where it matters and when it matters. Across more than 1,600 ablation experiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers perplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream utility gain. RLDP reaches each baseline's final utility after only 13-43% of the gradient-update budget (mean speed-up 71%), all while honoring the same (epsilon, delta)-DP contract and exhibiting equal or lower susceptibility to membership-inference and canary-extraction attacks."
    },
    {
      "title": "Falcon-H1：重新定义效率和性能的混合头语言模型系列 (原标题: Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance)",
      "link": "https://arxiv.org/abs/2507.22448",
      "pubDate": "Wed, 30 Jul 2025 03:55:33 GMT",
      "isoDate": "2025-07-30T03:55:33.000Z",
      "creator": "Jingwei Zuo, Maksim Velikanov, Ilyas Chahed, Younes Belkada, Dhia Eddine Rhayem, Guillaume Kunsch, Hakim Hacid, Hamza Yous, Brahim Farhat, Ibrahim Khadraoui, Mugariya Farooq, Giulia Campesan, Ruxandra Cojocaru, Yasser Djilali, Shi Hu, Iheb Chaabane, Puneesh Khanna, Mohamed El Amine Seddik, Ngoc Dung Huynh, Phuc Le Khac, Leen AlQadi, Billel Mokeddem, Mohamed Chami, Abdalgader Abubaker, Mikhail Lubinets, Kacper Piskorski, Slim Frikha",
      "summary": "## Falcon-H1：重新定义效率和性能的混合头语言模型系列\n\n### 简介\n\nFalcon-H1 是一个全新的大型语言模型（LLM）系列，其核心在于采用混合架构设计，旨在优化性能和效率，以适应各种不同的用例。与之前纯粹基于 Transformer 或 Mamba 架构的 Falcon 模型不同，Falcon-H1 引入了一种并行的混合方法，将基于 Transformer 的注意力机制与状态空间模型（SSM）相结合。SSM 以其卓越的长上下文记忆能力和计算效率而闻名。\n\n### 架构创新与开发策略\n\n*   **混合架构：** Falcon-H1 的核心创新在于其混合架构，它并行结合了 Transformer 的注意力机制和 SSM 的优势。这种设计旨在克服单一架构的局限性，同时提升模型的长上下文处理能力和计算效率。\n*   **系统性重访：** 团队系统性地重新审视了模型设计、数据策略和训练动态，挑战了该领域的传统实践，以实现性能和效率的突破。\n\n### 模型配置与可用性\n\nFalcon-H1 系列提供了多种配置，以满足不同需求：\n\n*   **参数规模：** 包括 0.5B、1.5B、1.5B-deep、3B、7B 和 34B 参数的基础模型和指令微调模型。\n*   **量化版本：** 还提供了量化后的指令微调模型。\n*   **可用性：** 总计超过 30 个检查点已在 Hugging Face Hub 上发布，方便研究人员和开发者访问和使用。\n\n### 卓越的性能与效率\n\nFalcon-H1 模型展现了最先进的性能，并在参数和训练效率方面表现出色：\n\n*   **旗舰模型 Falcon-H1-34B：** 性能与参数规模高达 70B 的模型（如 Qwen3-32B、Qwen2.5-72B 和 Llama3.3-70B）相当或超越，但使用的参数和数据量更少。\n*   **小型模型表现：** \n    *   Falcon-H1-1.5B-Deep 的性能可与当前领先的 7B-10B 模型相媲美。\n    *   Falcon-H1-0.5B 的性能与 2024 年典型的 7B 模型相当。\n*   **多领域能力：** 这些模型在推理、数学、多语言任务、指令遵循和科学知识方面表现卓越。\n\n### 应用范围与开放性\n\n*   **上下文支持：** Falcon-H1 支持高达 256K 的上下文令牌，使其能够处理极长的文本输入。\n*   **语言支持：** 支持 18 种语言，适用于广泛的全球应用。\n*   **开源许可：** 所有模型均在宽松的开源许可下发布，这体现了项目团队致力于推动人工智能研究的开放性和影响力。\n\n### 总结\n\nFalcon-H1 系列通过其创新的混合架构、卓越的性能和高效的资源利用，为大型语言模型领域树立了新的标杆。其广泛的配置和开放的可用性，使其成为各种应用场景的强大工具，并有望推动未来 AI 发展。",
      "shortSummary": "Falcon-H1 是一个创新的大型语言模型系列，采用混合架构（Transformer与SSM结合），旨在提升效率和性能。该系列模型参数范围从0.5B到34B，其中旗舰模型Falcon-H1-34B在参数和数据量更少的情况下，性能可媲美甚至超越70B级别的模型。Falcon-H1支持长达256K的上下文和18种语言，并在推理、数学、多语言任务等多个领域表现出色。所有模型均以开放源代码许可发布，致力于推动可访问和有影响力的AI研究。",
      "translated_title": "Falcon-H1：重新定义效率和性能的混合头语言模型系列",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this report, we introduce Falcon-H1, a new series of large language models (LLMs) featuring hybrid architecture designs optimized for both high performance and efficiency across diverse use cases. Unlike earlier Falcon models built solely on Transformer or Mamba architectures, Falcon-H1 adopts a parallel hybrid approach that combines Transformer-based attention with State Space Models (SSMs), known for superior long-context memory and computational efficiency. We systematically revisited model design, data strategy, and training dynamics, challenging conventional practices in the field. Falcon-H1 is released in multiple configurations, including base and instruction-tuned variants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized instruction-tuned models are also available, totaling over 30 checkpoints on Hugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and exceptional parameter and training efficiency. The flagship Falcon-H1-34B matches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B, and Llama3.3-70B, while using fewer parameters and less data. Smaller models show similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B models, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024. These models excel across reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge. With support for up to 256K context tokens and 18 languages, Falcon-H1 is suitable for a wide range of applications. All models are released under a permissive open-source license, underscoring our commitment to accessible and impactful AI research."
    },
    {
      "title": "TARS：用于减少多模态大语言模型幻觉的最小-最大令牌自适应偏好策略 (原标题: TARS: MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs)",
      "link": "https://arxiv.org/abs/2507.21584",
      "pubDate": "Tue, 29 Jul 2025 04:39:19 GMT",
      "isoDate": "2025-07-29T04:39:19.000Z",
      "creator": "Kejia Zhang, Keda Tao, Zhiming Luo, Chang Liu, Jiasheng Tang, Huan Wang",
      "summary": "### TARS：用于减少多模态大语言模型幻觉的最小-最大令牌自适应偏好策略\n\n**引言与问题背景**\n\n*   多模态大语言模型（MLLMs）在视觉-语言推理方面展现出强大能力，但其输出常出现“幻觉”——即看似合理但事实不准确或视觉上无根据的内容，这严重损害了模型的可靠性。\n*   直接偏好优化（DPO）是纠正幻觉的常用策略，旨在通过将模型输出与人类偏好对齐来改善性能。\n*   **现有DPO方法的局限性：** 传统的DPO策略通常将与幻觉相关的偏好视为固定目标，在训练过程中依赖静态的监督信号。这种方法容易过度拟合偏好数据中表面的语言线索，导致模型在分布上僵化，并形成虚假关联，从而削弱了模型与因果相关视觉信息的接地能力。\n\n**TARS：令牌自适应偏好策略**\n\n*   为克服现有DPO策略的上述局限性，本文提出了一种名为TARS（令牌自适应偏好策略）的新方法。\n*   TARS将DPO问题重新构建为一个**最小-最大优化问题**，以更动态和鲁棒的方式处理偏好对齐。\n*   **核心机制：**\n    *   **最大化令牌级分布偏移：** 在语义约束下，TARS最大化令牌级别的分布偏移，以此来模拟对齐过程中的不确定性。\n    *   **最小化预期偏好损失：** 同时，TARS在这些受控的扰动下最小化预期的偏好损失。\n*   **目标：** 这种联合优化目标旨在在减轻模型对偏好模式过度拟合的同时，有效保持其与视觉信息的因果接地能力，从而显著减少多模态推理中产生的幻觉。\n\n**实验评估与结果**\n\n*   TARS在多个幻觉基准测试上进行了广泛评估，并展现出持续且强劲的性能。\n*   **数据效率：** TARS仅使用了4.8k个偏好样本，且无需任何专家反馈，显示出其高效性。\n*   **显著的幻觉减少：**\n    *   将幻觉率从26.4%显著降低至13.2%。\n    *   将认知价值（cognition value）从2.5降低至0.4，表明模型输出的准确性和可靠性大幅提升。\n*   **性能对比：** TARS的性能优于标准的DPO方法，并在多项关键指标上与先进的GPT-4o模型表现相当。",
      "shortSummary": "多模态大语言模型（MLLMs）常出现幻觉。针对现有直接偏好优化（DPO）易过拟合导致接地能力受损的问题，本文提出TARS，一种最小-最大令牌自适应偏好策略。TARS将DPO重构为最小-最大优化，通过模拟对齐不确定性并最小化偏好损失，有效保持因果接地并减少过拟合，从而降低幻觉。实验证明，TARS仅用4.8k样本就将幻觉率从26.4%降至13.2%，性能优于DPO并媲美GPT-4o。",
      "translated_title": "TARS：用于减少多模态大语言模型幻觉的最小-最大令牌自适应偏好策略",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal large language models (MLLMs) enable vision-language reasoning, yet often generate plausible outputs that are factually incorrect or visually ungrounded, thereby compromising their reliability. Direct preference optimization (DPO) is a common strategy for correcting hallucinations by aligning model outputs with human preferences. Existing DPO strategies typically treat hallucination-related preferences as fixed targets, relying on static supervision signals during training. This approach tends to overfit to superficial linguistic cues in preference data, leading to distributional rigidity and spurious correlations that impair grounding in causally relevant visual information. To overcome this limitation, we propose TARS, a token-adaptive preference strategy that reformulates DPO as a min-max optimization problem. TARS maximizes token-level distributional shifts under semantic constraints to simulate alignment uncertainty, and simultaneously minimizes the expected preference loss under these controlled perturbations. This joint objective preserves causal grounding while mitigating overfitting to preference patterns, thereby reducing hallucinations in multimodal reasoning. We evaluate TARS on multiple hallucination benchmarks and find consistently strong performance. Using only 4.8k preference samples and no expert feedback, TARS reduces hallucination rates from 26.4% to 13.2% and decreases cognition value from 2.5 to 0.4. It outperforms standard DPO and matches GPT-4o on several key metrics."
    },
    {
      "title": "人格向量：监控和控制语言模型中的性格特质 (原标题: Persona Vectors: Monitoring and Controlling Character Traits in Language Models)",
      "link": "https://arxiv.org/abs/2507.21509",
      "pubDate": "Tue, 29 Jul 2025 01:20:14 GMT",
      "isoDate": "2025-07-29T01:20:14.000Z",
      "creator": "Runjin Chen, Andy Arditi, Henry Sleight, Owain Evans, Jack Lindsey",
      "summary": "## 人格向量：监控和控制语言模型中的性格特质\n\n### 摘要\n\n大型语言模型（LLMs）通常通过模拟的“助手”角色与用户互动。尽管助手通常被训练成乐于助人、无害且诚实，但有时会偏离这些理想。\n\n### 核心概念：人格向量\n\n*   **定义**：本文识别了模型激活空间中的特定方向，称之为“人格向量”，这些方向与多种性格特质相关联，例如邪恶、谄媚和产生幻觉的倾向。\n*   **提取方法**：提取人格向量的方法是自动化的，并且只需对感兴趣的性格特质进行自然语言描述即可应用于任何特质。\n\n### 应用与发现\n\n1.  **部署时监控**：\n    *   人格向量可用于在模型部署时监控助手个性中的波动。\n\n2.  **训练期间的预测与控制**：\n    *   **预测**：人格向量可用于预测训练过程中发生的个性转变。\n    *   **相关性**：研究发现，微调后有意和无意的个性变化与相关人格向量的偏移密切相关。\n    *   **干预**：这些偏移可以通过事后干预来缓解，或者通过一种新的预防性引导方法从一开始就避免。\n\n3.  **训练数据筛选**：\n    *   人格向量可用于标记将产生不良个性变化的训练数据，无论是在数据集层面还是在单个样本层面。\n\n### 结论\n\n该研究提出了一种有效的方法——人格向量，用于理解、监控和控制大型语言模型中的性格特质，从而提高模型的可靠性和一致性。",
      "shortSummary": "该研究引入了“人格向量”，即语言模型激活空间中的特定方向，用于监控和控制模型（如“助手”角色）的性格特质。这些向量能识别邪恶、谄媚、幻觉等倾向，并可用于在部署时监控个性波动。在训练中，它们能预测并缓解微调导致的个性转变，甚至标记出可能导致不良变化的训练数据。该方法自动化且仅需自然语言描述，为提升LLM的可靠性提供了新途径。",
      "translated_title": "人格向量：监控和控制语言模型中的性格特质",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models interact with users through a simulated 'Assistant' persona. While the Assistant is typically trained to be helpful, harmless, and honest, it sometimes deviates from these ideals. In this paper, we identify directions in the model's activation space-persona vectors-underlying several traits, such as evil, sycophancy, and propensity to hallucinate. We confirm that these vectors can be used to monitor fluctuations in the Assistant's personality at deployment time. We then apply persona vectors to predict and control personality shifts that occur during training. We find that both intended and unintended personality changes after finetuning are strongly correlated with shifts along the relevant persona vectors. These shifts can be mitigated through post-hoc intervention, or avoided in the first place with a new preventative steering method. Moreover, persona vectors can be used to flag training data that will produce undesirable personality changes, both at the dataset level and the individual sample level. Our method for extracting persona vectors is automated and can be applied to any personality trait of interest, given only a natural-language description."
    },
    {
      "title": "BANG：通过生成式爆炸动力学分割3D资产 (原标题: BANG: Dividing 3D Assets via Generative Exploded Dynamics)",
      "link": "https://arxiv.org/abs/2507.21493",
      "pubDate": "Tue, 29 Jul 2025 00:21:21 GMT",
      "isoDate": "2025-07-29T00:21:21.000Z",
      "creator": "Longwen Zhang, Qixuan Zhang, Haoran Jiang, Yinuo Bai, Wei Yang, Lan Xu, Jingyi Yu",
      "summary": "## BANG：通过生成式爆炸动力学分割3D资产\n\n### 引言\n3D创作是人类独特的优势，源于我们通过眼睛、思维和双手对物体进行解构和重组的能力。然而，当前的3D设计工具难以复制这一自然过程，需要大量的艺术专业知识和手动劳动。\n\n### BANG方法概述\n本文介绍了一种名为BANG的新型生成式方法，它弥合了3D生成与推理之间的鸿沟，实现了3D对象的直观灵活的零件级分解。\n\n### 核心机制：“生成式爆炸动力学”\nBANG的核心是“生成式爆炸动力学”，它为输入几何体创建一系列平滑的爆炸状态序列。这一过程逐步分离零件，同时保持其几何和语义连贯性。\n\n### 技术实现\n*   **基础模型：** BANG利用一个预训练的大规模潜在扩散模型。\n*   **微调与控制：** 该模型通过一个轻量级的爆炸视图适配器进行微调，以实现对分解过程的精确控制。\n*   **时间一致性：** 它还包含一个时间注意力模块，以确保平滑的过渡和跨时间的一致性。\n\n### 增强控制与交互\n*   **空间提示：** BANG通过空间提示（如边界框和表面区域）增强了控制，允许用户指定要分解的零件以及分解方式。\n*   **多模态扩展：** 这种交互可以与GPT-4等多模态模型结合，实现2D到3D的操作，从而提供更直观和富有创意的设计工作流程。\n\n### 应用与能力\nBANG的能力延伸到多个领域：\n*   **零件级几何生成：** 能够生成详细的零件级几何结构。\n*   **功能描述关联：** 将零件与功能描述相关联。\n*   **组件感知工作流：** 促进组件感知的3D创建和制造工作流程。\n*   **3D打印：** 在3D打印中，BANG可以生成可分离的零件，便于打印和重新组装。\n\n### 结论\n本质上，BANG实现了从想象概念到详细3D资产的无缝转换，为3D创作提供了一个与人类直觉相符的新视角。",
      "shortSummary": "BANG是一种创新的生成式方法，通过“生成式爆炸动力学”实现3D对象的直观零件级分解。它利用微调的潜在扩散模型和时间注意力模块，创建平滑的零件分离序列，同时保持几何和语义连贯性。用户可通过空间提示和多模态模型进行精确控制。BANG在零件几何生成、功能描述关联、组件感知制造及3D打印等领域具有广泛应用，旨在简化3D创作流程，使其更符合人类直觉。",
      "translated_title": "BANG：通过生成式爆炸动力学分割3D资产",
      "images": [],
      "contentSource": "完整文章",
      "content": "3D creation has always been a unique human strength, driven by our ability to deconstruct and reassemble objects using our eyes, mind and hand. However, current 3D design tools struggle to replicate this natural process, requiring considerable artistic expertise and manual labor. This paper introduces BANG, a novel generative approach that bridges 3D generation and reasoning, allowing for intuitive and flexible part-level decomposition of 3D objects. At the heart of BANG is \"Generative Exploded Dynamics\", which creates a smooth sequence of exploded states for an input geometry, progressively separating parts while preserving their geometric and semantic coherence.   BANG utilizes a pre-trained large-scale latent diffusion model, fine-tuned for exploded dynamics with a lightweight exploded view adapter, allowing precise control over the decomposition process. It also incorporates a temporal attention module to ensure smooth transitions and consistency across time. BANG enhances control with spatial prompts, such as bounding boxes and surface regions, enabling users to specify which parts to decompose and how. This interaction can be extended with multimodal models like GPT-4, enabling 2D-to-3D manipulations for more intuitive and creative workflows.   The capabilities of BANG extend to generating detailed part-level geometry, associating parts with functional descriptions, and facilitating component-aware 3D creation and manufacturing workflows. Additionally, BANG offers applications in 3D printing, where separable parts are generated for easy printing and reassembly. In essence, BANG enables seamless transformation from imaginative concepts to detailed 3D assets, offering a new perspective on creation that resonates with human intuition."
    },
    {
      "title": "Rep-MTL：释放表示层任务显著性在多任务学习中的潜力 (原标题: Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning)",
      "link": "https://arxiv.org/abs/2507.21049",
      "pubDate": "Mon, 28 Jul 2025 13:59:28 GMT",
      "isoDate": "2025-07-28T13:59:28.000Z",
      "creator": "Zedong Wang, Siyuan Li, Dan Xu",
      "summary": "Rep-MTL：释放表示层任务显著性在多任务学习中的潜力\n\n*   **现有问题与局限**\n    *   尽管多任务学习（MTL）有望利用任务间的互补知识，但现有的多任务优化（MTO）技术主要通过以优化器为中心的损失缩放和梯度操纵策略来解决任务冲突。\n    *   这些方法未能提供持续的性能提升，并且很少探索共享表示空间在促进任务间互补性方面的潜力。\n\n*   **Rep-MTL的核心思想**\n    *   本文提出，共享表示空间是任务交互的自然发生地，蕴含丰富信息，可作为现有优化器的补充，尤其在促进任务间互补性方面。\n    *   基于此直觉，引入了Rep-MTL方法，该方法利用“表示层任务显著性”来量化任务特定优化与共享表示学习之间的交互。\n\n*   **Rep-MTL的工作机制与目标**\n    *   Rep-MTL通过基于熵的惩罚和样本级跨任务对齐来引导这些表示层任务显著性。\n    *   其目标是通过维持个体任务的有效训练（而非单纯解决冲突）来缓解负迁移，同时明确促进互补信息的共享。\n\n*   **实验验证与结果**\n    *   研究在四个具有挑战性的MTL基准测试上进行了实验，这些基准测试涵盖了任务转移和域转移场景。\n    *   结果显示，Rep-MTL即使与基本的等权重策略结合，也能实现具有竞争力的性能提升，并展现出良好的效率。\n    *   除了标准性能指标，幂律指数分析（Power Law exponent analysis）也进一步证明了Rep-MTL在平衡任务特定学习和跨任务共享方面的有效性。\n\n*   **项目与发表信息**\n    *   该研究已被ICCV 2025接受并被评为亮点论文。",
      "shortSummary": "Rep-MTL是一种新的多任务学习方法，旨在解决现有优化技术在任务冲突解决上的局限。它关注共享表示空间，通过量化和引导“表示层任务显著性”，促进任务间互补性并缓解负迁移。Rep-MTL通过基于熵的惩罚和样本级对齐，在保持个体任务有效训练的同时，明确促进信息共享。实验证明，Rep-MTL即使采用简单权重策略，也能实现有竞争力的性能和效率，有效平衡任务学习与跨任务共享。",
      "translated_title": "Rep-MTL：释放表示层任务显著性在多任务学习中的潜力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite the promise of Multi-Task Learning in leveraging complementary knowledge across tasks, existing multi-task optimization (MTO) techniques remain fixated on resolving conflicts via optimizer-centric loss scaling and gradient manipulation strategies, yet fail to deliver consistent gains. In this paper, we argue that the shared representation space, where task interactions naturally occur, offers rich information and potential for operations complementary to existing optimizers, especially for facilitating the inter-task complementarity, which is rarely explored in MTO. This intuition leads to Rep-MTL, which exploits the representation-level task saliency to quantify interactions between task-specific optimization and shared representation learning. By steering these saliencies through entropy-based penalization and sample-wise cross-task alignment, Rep-MTL aims to mitigate negative transfer by maintaining the effective training of individual tasks instead pure conflict-solving, while explicitly promoting complementary information sharing. Experiments are conducted on four challenging MTL benchmarks covering both task-shift and domain-shift scenarios. The results show that Rep-MTL, even paired with the basic equal weighting policy, achieves competitive performance gains with favorable efficiency. Beyond standard performance metrics, Power Law exponent analysis demonstrates Rep-MTL's efficacy in balancing task-specific learning and cross-task sharing. The project page is available at HERE."
    }
  ],
  "lastUpdated": "2025-08-01T09:38:40.570Z"
}