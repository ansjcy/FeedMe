{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "Open CaptchaWorld：一个用于测试和评估多模态大型语言模型代理的综合性网络平台 (原标题: Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents)",
      "link": "https://arxiv.org/abs/2505.24878",
      "pubDate": "Fri, 30 May 2025 13:59:55 GMT",
      "isoDate": "2025-05-30T13:59:55.000Z",
      "creator": "Yaxin Luo, Zhaoyi Li, Jiacheng Liu, Jiacheng Cui, Xiaohan Zhao, Zhiqiang Shen",
      "summary": "### Open CaptchaWorld：评估多模态大型语言模型代理的综合性网络平台\n\n**背景与挑战**\n\n*   **CAPTCHA的瓶颈作用：** 验证码（CAPTCHA）一直是部署网络代理在实际应用中的关键瓶颈，经常阻碍它们完成端到端自动化任务。\n*   **现有MLLM代理的局限性：** 尽管现代多模态大型语言模型（MLLM）代理在静态感知任务中表现出色，但它们处理像CAPTCHA这样需要交互式、多步骤推理的挑战的能力，在很大程度上尚未经过测试。\n\n**Open CaptchaWorld 平台介绍**\n\n*   **目的与定位：** 为解决上述空白，研究人员推出了Open CaptchaWorld，这是首个专门设计用于评估MLLM驱动代理的视觉推理和交互能力的网络基准测试平台。\n*   **评估方式：** 通过多样化和动态的CAPTCHA谜题进行评估。\n*   **基准范围：** 该基准涵盖了20种现代CAPTCHA类型，总计225个CAPTCHA。\n*   **新度量标准：** 引入了“CAPTCHA推理深度”（CAPTCHA Reasoning Depth）这一新度量标准，用于量化解决每个谜题所需的认知和运动步骤数量。\n\n**实验结果与发现**\n\n*   **人类表现：** 实验结果表明，人类在解决CAPTCHA时始终能达到近乎完美的得分，成功率高达93.3%。\n*   **SOTA MLLM代理表现：** 最先进的MLLM代理（如Browser-Use Openai-o3）表现显著不佳，成功率最高仅为40.0%，远低于人类水平。\n*   **平台的重要性：** 这突出表明Open CaptchaWorld是一个至关重要的基准，可用于诊断当前多模态代理的局限性，并指导更强大的多模态推理系统的开发。\n\n**资源可用性**\n\n*   代码和数据可在相关URL获取。",
      "shortSummary": "Open CaptchaWorld是一个综合性的网络平台，旨在测试和评估多模态大型语言模型（MLLM）代理处理交互式CAPTCHA的能力。该平台包含20种类型共225个CAPTCHA，并引入了“CAPTCHA推理深度”指标。实验表明，人类解决CAPTCHA的成功率接近完美（93.3%），而最先进的MLLM代理表现不佳，最高仅达40.0%。Open CaptchaWorld揭示了当前MLLM代理在多步推理方面的局限性，对未来多模态推理系统的发展具有重要指导意义。",
      "translated_title": "Open CaptchaWorld：一个用于测试和评估多模态大型语言模型代理的综合性网络平台",
      "images": [],
      "contentSource": "完整文章",
      "content": "CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL."
    },
    {
      "title": "时间盲区：视频-语言模型为何无法像人类一样感知？ (原标题: Time Blindness: Why Video-Language Models Can't See What Humans Can?)",
      "link": "https://arxiv.org/abs/2505.24867",
      "pubDate": "Fri, 30 May 2025 13:59:12 GMT",
      "isoDate": "2025-05-30T13:59:12.000Z",
      "creator": "Ujjwal Upadhyay, Mukul Ranjan, Zhiqiang Shen, Mohamed Elhoseiny",
      "summary": "### 时间盲区：视频-语言模型为何无法像人类一样感知？\n\n本文探讨了当前视频-语言模型（VLMs）在处理纯粹时间模式方面的局限性，尤其是在空间信息被遮蔽的情况下。\n\n**1. 问题背景**\n*   尽管视觉-语言模型（VLMs）在理解视频中的时空关系方面取得了显著进展，但当空间信息被遮蔽时，它们难以捕捉纯粹的时间模式。\n*   这些模型过度依赖帧级别的空间特征，导致无法从时间线索中提取意义。\n\n**2. 引入 SpookyBench 基准**\n*   为了系统性地评估这一局限性，研究人员引入了名为 **SpookyBench** 的新基准。\n*   在该基准中，信息仅通过类似噪声帧的时间序列进行编码，这模拟了从生物信号到隐蔽通信等自然现象。\n\n**3. 关键发现与性能差距**\n*   **人类表现：** 在这些时间序列中，人类能够以超过98%的准确率识别形状、文本和模式。\n*   **VLM 表现：** 然而，最先进的视频-语言模型却达到了0%的准确率。\n*   **差距分析：** 这一巨大的性能差距凸显了VLMs的一个关键局限性：它们过度依赖帧级别的空间特征，并且无法从纯粹的时间线索中提取意义。\n*   **退化现象：** 此外，当在空间信噪比（SNR）较低的数据集上进行训练时，模型的时序理解能力比人类感知退化得更快，尤其是在需要精细时间推理的任务中。\n*   **普遍性：** 系统分析表明，这个问题在不同模型规模和架构中普遍存在。\n\n**4. 展望与未来方向**\n*   克服这一局限性将需要新的架构或训练范式，以实现空间依赖与时间处理的解耦。\n*   研究人员发布了 SpookyBench 基准，旨在促进时间模式识别领域的研究，并弥合人类与机器视频理解之间的差距。\n\n**5. 资源可用性**\n*   SpookyBench 数据集和代码已在其项目网站上公开。",
      "shortSummary": "当前视频-语言模型（VLMs）在空间信息模糊时，难以理解纯粹的时间模式。研究引入了“SpookyBench”基准，发现人类能以98%以上准确率识别时间序列中的模式，而最先进的VLMs准确率为0%。这表明VLMs过度依赖空间特征，无法从时间线索中提取意义。为弥补这一“时间盲区”，需要开发新的模型架构或训练方法，以解耦空间和时间处理。",
      "translated_title": "时间盲区：视频-语言模型为何无法像人类一样感知？",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in vision-language models (VLMs) have made impressive strides in understanding spatio-temporal relationships in videos. However, when spatial information is obscured, these models struggle to capture purely temporal patterns. We introduce SpookyBench, a benchmark where information is encoded solely in temporal sequences of noise-like frames, mirroring natural phenomena from biological signaling to covert communication. Interestingly, while humans can recognize shapes, text, and patterns in these sequences with over 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance gap highlights a critical limitation: an over-reliance on frame-level spatial features and an inability to extract meaning from temporal cues. Furthermore, when trained in data sets with low spatial signal-to-noise ratios (SNR), temporal understanding of models degrades more rapidly than human perception, especially in tasks requiring fine-grained temporal reasoning. Overcoming this limitation will require novel architectures or training paradigms that decouple spatial dependencies from temporal processing. Our systematic analysis shows that this issue persists across model scales and architectures. We release SpookyBench to catalyze research in temporal pattern recognition and bridge the gap between human and machine video understanding. Dataset and code has been made available on our project website: https://timeblindness.github.io/."
    },
    {
      "title": "ProRL：长时间强化学习拓展大型语言模型的推理边界 (原标题: ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models)",
      "link": "https://arxiv.org/abs/2505.24864",
      "pubDate": "Fri, 30 May 2025 13:59:01 GMT",
      "isoDate": "2025-05-30T13:59:01.000Z",
      "creator": "Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, Yi Dong",
      "summary": "## ProRL：长时间强化学习拓展大型语言模型的推理边界\n\n### 摘要\n\n#### 研究背景与争议\n\n近期在以推理为中心的语言模型方面取得的进展，凸显了强化学习（RL）作为一种有前景的方法，能够使模型与可验证的奖励对齐。然而，RL是否真正拓展了模型的推理能力，或者仅仅放大了基础模型分布中已有的高奖励输出，以及持续增加RL计算量是否能可靠地提高推理性能，这些问题仍然存在争议。\n\n#### ProRL方法论\n\n本文通过展示长时间强化学习（ProRL）训练能够发现基础模型即使在大量采样下也无法触及的新颖推理策略，从而挑战了普遍的假设。我们引入了ProRL，这是一种新颖的训练方法，它结合了KL散度控制、参考策略重置和多样化的任务套件。\n\n#### 主要发现与实证分析\n\n我们的实证分析揭示了以下关键发现：\n\n*   经过RL训练的模型在广泛的`pass@k`评估中持续优于基础模型，包括基础模型无论尝试次数多少都完全失败的场景。\n*   推理边界的改进与基础模型的任务能力和训练时长密切相关。\n*   这表明RL能够随着时间的推移探索并填充新的解决方案空间区域。\n\n#### 研究意义\n\n这些发现为理解RL在何种条件下能够有意义地拓展语言模型的推理边界提供了新见解，并为未来关于长周期RL在推理方面的研究奠定了基础。",
      "shortSummary": "ProRL（长时间强化学习）是一种新颖的训练方法，旨在拓展大型语言模型的推理边界。研究表明，ProRL训练的模型在推理任务上持续优于基础模型，甚至能发现基础模型无法触及的新策略。其改进与基础模型能力和训练时长相关，表明RL能随时间探索新解决方案空间。这项工作为理解RL如何真正提升语言模型推理能力提供了新见解，并为未来研究奠定基础。",
      "translated_title": "ProRL：长时间强化学习拓展大型语言模型的推理边界",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a model's reasoning capabilities or merely amplifies high-reward outputs already latent in the base model's distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. In this work, we challenge prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. We introduce ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Our empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@k evaluations, including scenarios where base models fail entirely regardless of the number of attempts. We further show that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time. These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. We release model weights to support further research: https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B"
    },
    {
      "title": "AlphaOne：推理模型在测试时的慢思考与快思考 (原标题: AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time)",
      "link": "https://arxiv.org/abs/2505.24863",
      "pubDate": "Fri, 30 May 2025 13:58:36 GMT",
      "isoDate": "2025-05-30T13:58:36.000Z",
      "creator": "Junyu Zhang, Runpei Dong, Han Wang, Xuying Ning, Haoran Geng, Peihao Li, Xialin He, Yutong Bai, Jitendra Malik, Saurabh Gupta, Huan Zhang",
      "summary": "## AlphaOne：推理模型在测试时的慢思考与快思考\n\n### 引言\n\n本文介绍了一个名为 **AlphaOne ($\\alpha$1)** 的通用框架，旨在测试时调节大型推理模型（LRMs）的推理过程。\n\n### 核心概念：$\\alpha$ 时刻\n\n$\\alpha$1 引入了“$\\alpha$ 时刻”这一概念，它代表了一个通过通用参数 $\\alpha$ 进行缩放的思考阶段。\n\n### 工作机制\n\n$\\alpha$1 框架通过以下两个阶段实现慢思考到快思考的动态调制：\n\n*   **$\\alpha$ 时刻前（Scaled Pre-$\\alpha$ Moment Phase）**：\n    *   在此阶段，模型通过将推理转换标记的插入建模为伯努利随机过程，动态地调度慢思考的转换。\n    *   这意味着模型会根据需要，以一种概率性的方式插入“思考”步骤，从而允许更深入、更慢的推理。\n\n*   **$\\alpha$ 时刻后（Post-$\\alpha$ Moment）**：\n    *   在 $\\alpha$ 时刻之后，$\\alpha$1 会确定性地通过“思考结束”标记来终止慢思考。\n    *   这促进了快速推理和高效的答案生成，使模型能够迅速给出最终结果。\n\n### 优势与泛化能力\n\n*   **统一与泛化**：该方法统一并泛化了现有的单调缩放方法。\n*   **灵活调制**：它实现了从慢到快推理的灵活且密集的调制，允许模型根据任务需求调整其思考速度。\n\n### 实验结果\n\n在数学、编码和科学等多个具有挑战性的基准测试中进行的广泛实证研究表明，$\\alpha$1 框架在推理能力和效率方面均表现出卓越的性能。",
      "shortSummary": "AlphaOne ($\\alpha$1) 是一个通用框架，用于在测试时调节大型推理模型（LRMs）的思考过程。它引入了“$\\alpha$ 时刻”概念，在此之前动态调度慢思考，之后确定性地终止慢思考，从而促进快速、高效的答案生成。该方法统一并泛化了现有技术，实现了从慢到快推理的灵活调制。在数学、编码和科学等领域的广泛测试中，$\\alpha$1 展现了卓越的推理能力和效率。",
      "translated_title": "AlphaOne：推理模型在测试时的慢思考与快思考",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper presents AlphaOne (alpha1), a universal framework for modulating reasoning progress in large reasoning models (LRMs) at test time. alpha1 first introduces alpha moment, which represents the scaled thinking phase with a universal parameter alpha. Within this scaled pre-alpha moment phase, it dynamically schedules slow thinking transitions by modeling the insertion of reasoning transition tokens as a Bernoulli stochastic process. After the alpha moment, alpha1 deterministically terminates slow thinking with the end-of-thinking token, thereby fostering fast reasoning and efficient answer generation. This approach unifies and generalizes existing monotonic scaling methods by enabling flexible and dense slow-to-fast reasoning modulation. Extensive empirical studies on various challenging benchmarks across mathematical, coding, and scientific domains demonstrate alpha1's superior reasoning capability and efficiency. Project page: https://alphaone-project.github.io/"
    },
    {
      "title": "ViStoryBench：故事可视化综合基准测试套件 (原标题: ViStoryBench: Comprehensive Benchmark Suite for Story Visualization)",
      "link": "https://arxiv.org/abs/2505.24862",
      "pubDate": "Fri, 30 May 2025 13:58:21 GMT",
      "isoDate": "2025-05-30T13:58:21.000Z",
      "creator": "Cailin Zhuang, Ailin Huang, Wei Cheng, Jingwei Wu, Yaoqi Hu, Jiaqi Liao, Zhewei Huang, Hongyuan Wang, Xinyao Liao, Weiwei Cai, Hengyuan Xu, Xuanyang Zhang, Xianfang Zeng, Gang Yu, Chi Zhang",
      "summary": "# ViStoryBench：故事可视化综合基准测试套件\n\n## 概述\nViStoryBench 是一个新推出的综合性评估基准测试套件，旨在推动故事可视化（Story Visualization, SV）领域的发展。故事可视化是指根据给定的叙事文本和参考图像，生成一系列视觉上连贯的图像。随着生成模型技术的进步，故事可视化取得了显著进展，而 ViStoryBench 的目标是进一步提升现有框架在实际场景中的表现。\n\n## 核心特点与设计\nViStoryBench 的设计旨在全面评估故事可视化模型的各项能力：\n\n*   **多样化的数据集：**\n    *   收集了涵盖多种故事类型的数据，例如喜剧、恐怖等不同情节。\n    *   包含了多种艺术风格，如动漫、3D 渲染等，确保模型在不同视觉美学维度上得到评估。\n    *   这种多样性使得模型能够在多维度上进行测试。\n\n*   **精心策划的数据集内容：**\n    *   数据集在叙事结构和视觉元素之间取得了平衡。\n    *   包含单主角和多主角的故事，以测试模型维持角色一致性的能力。\n    *   引入了复杂的故事情节和精细的世界构建，对模型生成准确视觉内容的能力提出挑战。\n\n*   **全面的评估指标：**\n    *   集成了广泛的评估指标，用于衡量故事可视化模型的关键性能方面。\n    *   这种多方面的框架有助于研究人员全面识别不同模型的优势和劣势。\n\n## 目的与意义\nViStoryBench 旨在为研究人员提供一个结构化、多维度的评估工具，从而能够深入了解现有故事可视化模型的表现，促进有针对性的改进和创新。\n\n## 作者\n本文由 Cailin Zhuang 等人撰写。",
      "shortSummary": "ViStoryBench是一个全新的故事可视化综合基准测试套件。它旨在评估模型根据叙事和参考图像生成视觉连贯图像序列的能力。该基准包含多样化的数据集，涵盖不同故事类型、艺术风格、复杂情节和多主角场景，以测试模型在角色一致性和视觉准确性方面的表现。ViStoryBench通过全面的评估指标，帮助研究人员识别模型的优缺点，从而推动故事可视化技术的进一步发展。",
      "translated_title": "ViStoryBench：故事可视化综合基准测试套件",
      "images": [],
      "contentSource": "完整文章",
      "content": "Story visualization, which aims to generate a sequence of visually coherent images aligning with a given narrative and reference images, has seen significant progress with recent advancements in generative models. To further enhance the performance of story visualization frameworks in real-world scenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We collect a diverse dataset encompassing various story types and artistic styles, ensuring models are evaluated across multiple dimensions such as different plots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D renderings). ViStoryBench is carefully curated to balance narrative structures and visual elements, featuring stories with single and multiple protagonists to test models' ability to maintain character consistency. Additionally, it includes complex plots and intricate world-building to challenge models in generating accurate visuals. To ensure comprehensive comparisons, our benchmark incorporates a wide range of evaluation metrics assessing critical aspects. This structured and multifaceted framework enables researchers to thoroughly identify both the strengths and weaknesses of different models, fostering targeted improvements."
    },
    {
      "title": "MetaFaith：大型语言模型中忠实自然语言不确定性表达 (原标题: MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs)",
      "link": "https://arxiv.org/abs/2505.24858",
      "pubDate": "Fri, 30 May 2025 13:54:08 GMT",
      "isoDate": "2025-05-30T13:54:08.000Z",
      "creator": "Gabrielle Kaili-May Liu, Gal Yona, Avi Caciularu, Idan Szpektor, Tim G. J. Rudner, Arman Cohan",
      "summary": "### MetaFaith：大型语言模型中忠实自然语言不确定性表达研究\n\n本研究探讨了大型语言模型（LLMs）在自然语言中表达不确定性的可靠性问题，并提出了一种名为MetaFaith的新型校准方法。\n\n**核心问题与挑战：**\n*   **信任度受损：** LLMs在传达错误信息时常使用断言性语言，导致用户过度依赖并侵蚀信任。\n*   **缺乏忠实校准：** 缺乏对LLMs“忠实置信校准”的系统性研究，即模型使用语言表达的不确定性是否能忠实反映其内在不确定性。\n\n**研究发现：**\n*   **现有LLMs表现不佳：** 大多数LLMs在忠实表达不确定性方面表现不佳。\n*   **现有干预措施不足：**\n    *   标准提示方法仅带来微不足道的改进。\n    *   现有的基于事实性的校准技术甚至可能损害忠实校准。\n\n**MetaFaith方法介绍：**\n*   **创新方法：** 引入了MetaFaith，一种受人类元认知启发的、新颖的基于提示的校准方法。\n*   **目标：** 旨在解决LLMs在忠实表达不确定性方面的关键空白。\n\n**MetaFaith的有效性：**\n*   **鲁棒性提升：** MetaFaith在不同模型和任务领域中都能稳健地改善忠实校准。\n*   **显著效果：**\n    *   忠实性（faithfulness）提升高达61%。\n    *   在人类判断下，相对于原始生成内容，MetaFaith的胜率达到83%。\n\n**结论：**\nMetaFaith为提高LLMs的信任度和可靠性提供了一个有前景的解决方案，使其能够更忠实地通过自然语言表达自身的不确定性。",
      "shortSummary": "大型语言模型（LLMs）在表达不确定性时常使用断言性语言，导致信任度下降。本研究发现LLMs在此方面表现不佳，且现有方法无效甚至有害。为解决此问题，研究引入了MetaFaith，一种受人类元认知启发的提示校准方法。MetaFaith能显著提高LLMs的忠实不确定性表达能力，忠实性提升高达61%，并在人类评估中获得83%的胜率，有效提升了LLMs的可靠性。",
      "translated_title": "MetaFaith：大型语言模型中忠实自然语言不确定性表达",
      "images": [],
      "contentSource": "完整文章",
      "content": "A critical component in the trustworthiness of LLMs is reliable uncertainty communication, yet LLMs often use assertive language when conveying false claims, leading to over-reliance and eroded trust. We present the first systematic study of faithful confidence calibration of LLMs, benchmarking models' ability to use linguistic expressions of uncertainty that faithfully reflect their intrinsic uncertainty, across a comprehensive array of models, datasets, and prompting strategies. Our results demonstrate that LLMs largely fail at this task, and that existing interventions are insufficient: standard prompt approaches provide only marginal gains, and existing, factuality-based calibration techniques can even harm faithful calibration. To address this critical gap, we introduce MetaFaith, a novel prompt-based calibration approach inspired by human metacognition. We show that MetaFaith robustly improves faithful calibration across diverse models and task domains, enabling up to 61% improvement in faithfulness and achieving an 83% win rate over original generations as judged by humans."
    },
    {
      "title": "利用负面信号：从教师数据中进行强化蒸馏以实现LLM推理 (原标题: Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning)",
      "link": "https://arxiv.org/abs/2505.24850",
      "pubDate": "Fri, 30 May 2025 13:47:17 GMT",
      "isoDate": "2025-05-30T13:47:17.000Z",
      "creator": "Shuyao Xu, Cheng Peng, Jiangxuan Long, Weidi Xu, Wei Chu, Yuan Qi",
      "summary": "### 利用负面信号：从教师数据中进行强化蒸馏以实现LLM推理\n\n本文提出了一种名为“强化蒸馏”（Reinforcement Distillation, REDI）的两阶段框架，旨在解决大型语言模型（LLM）推理能力蒸馏中，标准方法通常会丢弃不正确推理示例（即“负面信号”）的问题。这些负面信号虽然有价值，但往往未被充分利用。\n\n#### 研究问题\n\n*   如何在离线设置中有效利用正面和负面蒸馏推理轨迹，以最大限度地提高LLM的推理性能？\n\n#### 提出的方法：强化蒸馏（REDI）\n\nREDI框架包含两个主要阶段：\n\n1.  **第一阶段：监督微调（SFT）**\n    *   模型通过监督微调从正面推理轨迹中学习。\n2.  **第二阶段：模型精炼**\n    *   模型利用所提出的REDI目标函数，进一步结合正面和负面推理轨迹进行精炼。\n\n#### REDI目标函数\n\n*   REDI目标函数是一种新颖的、简单的、无需参考的损失函数。\n*   在蒸馏语境下，该函数表现优于DPO和SimPO等现有方法。\n\n#### 实验评估与成果\n\n*   **性能优越性：** 经验评估表明，在数学推理任务上，REDI优于基线拒绝采样SFT或SFT结合DPO/SimPO的方法。\n*   **Qwen-REDI-1.5B模型：**\n    *   该模型仅使用来自开放Open-R1数据集的13.1万个正面和负面示例进行后训练。\n    *   在MATH-500 (pass@1) 测试中取得了83.1%的得分。\n    *   其性能与DeepSeek-R1-Distill-Qwen-1.5B（一个使用80万专有数据进行后训练的模型）在各种数学推理基准测试中持平或超越。\n*   **行业意义：** Qwen-REDI-1.5B模型为使用公开可用数据进行离线后训练的1.5B模型树立了新的最先进（state-of-the-art）水平。\n\n#### 总结\n\nREDI框架通过有效利用被传统方法忽视的负面推理信号，显著提升了LLM的推理能力，尤其是在数学推理领域，并证明了其在资源有限的离线训练场景下的高效性和优越性。",
      "shortSummary": "本文提出了“强化蒸馏”（REDI）框架，旨在利用大型语言模型（LLM）蒸馏过程中被忽视的负面推理信号。REDI通过两阶段训练，结合正面和负面数据，并引入了优于DPO和SimPO的新型目标函数。实验证明，REDI在数学推理任务上表现出色，其训练于13.1万公开数据的Qwen-REDI-1.5B模型，性能匹敌或超越了使用80万专有数据训练的模型，为1.5B模型离线训练树立了新标杆。",
      "translated_title": "利用负面信号：从教师数据中进行强化蒸馏以实现LLM推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in model distillation demonstrate that data from advanced reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer complex reasoning abilities to smaller, efficient student models. However, standard practices employ rejection sampling, discarding incorrect reasoning examples -- valuable, yet often underutilized data. This paper addresses the critical question: How can both positive and negative distilled reasoning traces be effectively leveraged to maximize LLM reasoning performance in an offline setting? To this end, We propose Reinforcement Distillation (REDI), a two-stage framework. Stage 1 learns from positive traces via Supervised Fine-Tuning (SFT). Stage 2 further refines the model using both positive and negative traces through our proposed REDI objective. This novel objective is a simple, reference-free loss function that outperforms established methods like DPO and SimPO in this distillation context. Our empirical evaluations demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT combined with DPO/SimPO on mathematical reasoning tasks. Notably, the Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1). Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a model post-trained on 800k proprietary data) across various mathematical reasoning benchmarks, establishing a new state-of-the-art for 1.5B models post-trained offline with openly available data."
    },
    {
      "title": "利用大型语言模型进行科学新颖性检测 (原标题: Harnessing Large Language Models for Scientific Novelty Detection)",
      "link": "https://arxiv.org/abs/2505.24615",
      "pubDate": "Fri, 30 May 2025 10:08:13 GMT",
      "isoDate": "2025-05-30T10:08:13.000Z",
      "creator": "Yan Liu, Zonglin Yang, Soujanya Poria, Thanh-Son Nguyen, Erik Cambria",
      "summary": "### 利用大型语言模型进行科学新颖性检测\n\n**引言**\n\n在科学研究呈指数级增长的时代，识别新颖的研究思想对于学术界至关重要，但也极具挑战性。尽管大型语言模型（LLMs）具有潜力，但缺乏合适的基准数据集阻碍了新颖性检测（ND）领域的研究。更重要的是，简单地采用现有自然语言处理（NLP）技术（例如，检索然后交叉检查）并非万能解决方案，因为文本相似性与思想概念之间存在差距。\n\n**核心贡献与方法**\n\n本文提出利用大型语言模型（LLMs）进行科学新颖性检测（ND），并为此构建了两个新的数据集，分别涵盖市场营销和NLP领域。\n\n1.  **数据集构建**：\n    *   为了构建用于ND的周全数据集，研究者提出基于论文之间的关系提取“闭包集”（closure sets）。\n    *   随后，利用LLMs总结这些闭包集中的主要思想。\n\n2.  **思想概念捕获与检索**：\n    *   为了捕获思想概念，研究者提出训练一个轻量级检索器。\n    *   该检索器通过从LLMs中提炼思想层面的知识，使其能够对齐概念相似的思想。\n    *   这使得LLM新颖性检测能够进行高效且准确的思想检索。\n\n**实验结果**\n\n实验结果表明，在所提出的基准数据集上，该方法在思想检索和ND任务上持续优于其他现有方法。\n\n**资源可用性**\n\n相关代码和数据可在此URL获取：`this https URL`。",
      "shortSummary": "本文提出利用大型语言模型（LLMs）进行科学新颖性检测（ND），以解决现有方法在识别新颖研究思想方面的局限性。研究者构建了两个新的领域数据集，并开发了一种通过从LLMs中提炼知识来训练轻量级检索器的方法，以实现高效准确的思想检索。实验证明，该方法在思想检索和新颖性检测任务上均优于其他方法，为科学发现提供了新的工具。",
      "translated_title": "利用大型语言模型进行科学新颖性检测",
      "images": [],
      "contentSource": "完整文章",
      "content": "In an era of exponential scientific growth, identifying novel research ideas is crucial and challenging in academia. Despite potential, the lack of an appropriate benchmark dataset hinders the research of novelty detection. More importantly, simply adopting existing NLP technologies, e.g., retrieving and then cross-checking, is not a one-size-fits-all solution due to the gap between textual similarity and idea conception. In this paper, we propose to harness large language models (LLMs) for scientific novelty detection (ND), associated with two new datasets in marketing and NLP domains. To construct the considerate datasets for ND, we propose to extract closure sets of papers based on their relationship, and then summarize their main ideas based on LLMs. To capture idea conception, we propose to train a lightweight retriever by distilling the idea-level knowledge from LLMs to align ideas with similar conception, enabling efficient and accurate idea retrieval for LLM novelty detection. Experiments show our method consistently outperforms others on the proposed benchmark datasets for idea retrieval and ND tasks. Codes and data are available at https://anonymous.4open.science/r/NoveltyDetection-10FB/."
    },
    {
      "title": "UniGeo：驯服视频扩散模型以实现统一一致的几何估计 (原标题: UniGeo: Taming Video Diffusion for Unified Consistent Geometry Estimation)",
      "link": "https://arxiv.org/abs/2505.24521",
      "pubDate": "Fri, 30 May 2025 08:31:59 GMT",
      "isoDate": "2025-05-30T08:31:59.000Z",
      "creator": "Yang-Tian Sun, Xin Yu, Zehuan Huang, Yi-Hua Huang, Yuan-Chen Guo, Ziyi Yang, Yan-Pei Cao, Xiaojuan Qi",
      "summary": "## UniGeo：驯服视频扩散模型以实现统一一致的几何估计\n\n### 引言与背景\n\n*   **研究背景：** 近期，利用扩散模型先验辅助单目几何估计（如深度和法线）的方法因其强大的泛化能力而受到广泛关注。\n*   **现有问题：** 大多数现有工作主要关注在单个视频帧的相机坐标系内估计几何属性，但却忽略了扩散模型在确定帧间对应关系方面的固有能力。\n\n### UniGeo方法概述\n\n*   本文提出UniGeo，旨在证明通过适当的设计和微调，可以有效利用视频生成模型固有的时间一致性来实现统一且一致的几何估计。\n\n### 核心贡献与方法\n\nUniGeo通过以下三个关键设计来解决现有问题并提升性能：\n\n1.  **预测目标选择：** 将全局坐标系中的几何属性作为预测目标，这些属性与视频帧共享相同的对应关系，从而确保跨帧的一致性。\n2.  **高效条件化方法：** 引入了一种新颖且高效的条件化方法，通过重用位置编码来实现，这有助于模型更好地理解和利用时空信息。\n3.  **性能增强策略：** 通过对共享相同对应关系的多个几何属性进行联合训练来提升整体性能，利用不同几何属性之间的内在联系。\n\n### 实验结果与泛化能力\n\n*   **卓越性能：** UniGeo在预测视频中的全局几何属性方面取得了卓越的性能。\n*   **应用潜力：** 该方法可以直接应用于三维重建任务，显示出其在实际应用中的强大潜力。\n*   **泛化能力：** 即使仅在静态视频数据上进行训练，UniGeo也展现出泛化到动态视频场景的潜力，表明其具有较强的鲁棒性和适应性。\n\n**（注：文章内容中不包含有效的实际图片链接，因此详细摘要中不包含图片。）**",
      "shortSummary": "UniGeo是一种利用视频扩散模型实现统一一致几何估计的新方法。它解决了现有方法忽略扩散模型帧间对应能力的问题。UniGeo通过在全局坐标系中选择预测目标、重用位置编码进行高效条件化以及联合训练多个几何属性，有效利用了视频生成模型的时间一致性。该方法在视频全局几何属性预测上表现出色，可应用于重建，并能泛化到动态场景。",
      "translated_title": "UniGeo：驯服视频扩散模型以实现统一一致的几何估计",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recently, methods leveraging diffusion model priors to assist monocular geometric estimation (e.g., depth and normal) have gained significant attention due to their strong generalization ability. However, most existing works focus on estimating geometric properties within the camera coordinate system of individual video frames, neglecting the inherent ability of diffusion models to determine inter-frame correspondence. In this work, we demonstrate that, through appropriate design and fine-tuning, the intrinsic consistency of video generation models can be effectively harnessed for consistent geometric estimation. Specifically, we 1) select geometric attributes in the global coordinate system that share the same correspondence with video frames as the prediction targets, 2) introduce a novel and efficient conditioning method by reusing positional encodings, and 3) enhance performance through joint training on multiple geometric attributes that share the same correspondence. Our results achieve superior performance in predicting global geometric attributes in videos and can be directly applied to reconstruction tasks. Even when trained solely on static video data, our approach exhibits the potential to generalize to dynamic video scenes."
    },
    {
      "title": "un^2CLIP: 通过反转 unCLIP 提升 CLIP 的视觉细节捕捉能力 (原标题: un^2CLIP: Improving CLIP's Visual Detail Capturing Ability via Inverting unCLIP)",
      "link": "https://arxiv.org/abs/2505.24517",
      "pubDate": "Fri, 30 May 2025 08:29:38 GMT",
      "isoDate": "2025-05-30T08:29:38.000Z",
      "creator": "Yinqi Li, Jiahe Zhao, Hong Chang, Ruibing Hou, Shiguang Shan, Xilin Chen",
      "summary": "### un^2CLIP: 提升 CLIP 视觉细节捕捉能力的新方法\n\n**1. 背景与问题**\n\n*   **CLIP (Contrastive Language-Image Pre-training)** 已成为基础模型，广泛应用于视觉和多模态任务。\n*   **现有挑战：** 近期研究表明，CLIP 在区分图像细节方面表现不足，在密集预测和以视觉为中心的多模态任务上性能不佳。\n*   **研究目标：** 旨在改进现有 CLIP 模型，使其能够尽可能多地捕捉图像中的视觉细节。\n\n**2. 核心思想与方法：反转 unCLIP (un^2CLIP)**\n\n*   **发现：** 一种名为 **unCLIP** 的生成模型为实现目标提供了合适的框架。\n*   **unCLIP 的特点：** unCLIP 训练一个以 CLIP 图像嵌入为条件的图像生成器，本质上是反转了 CLIP 图像编码器。\n*   **选择 unCLIP 的原因：**\n    *   **生成模型优势：** 与判别模型（如 CLIP）相比，生成模型通过学习图像数据分布，更擅长捕捉图像细节。\n    *   **空间对齐：** unCLIP 的条件输入空间与 CLIP 原始的图像-文本嵌入空间保持一致。\n*   **提出的方法 (un^2CLIP)：** 通过反转 unCLIP 来改进 CLIP 模型。\n*   **实现效果：** 改进后的图像编码器能够获得 unCLIP 的视觉细节捕捉能力，同时保持与原始文本编码器的对齐。\n\n**3. 实验评估与结果**\n\n*   **评估任务：**\n    *   具有挑战性的 MMVP-VLM 基准测试。\n    *   密集预测的开放词汇分割任务。\n    *   多模态大型语言模型任务。\n*   **实验结果：** un^2CLIP 显著优于原始 CLIP 模型以及之前提出的 CLIP 改进方法。\n\n**4. 代码与模型可用性**\n\n*   相关代码和模型将在此处提供（文章中未提供具体有效链接，故此处不展示）。",
      "shortSummary": "本研究提出 un^2CLIP，旨在提升 CLIP 模型捕捉视觉细节的能力。鉴于 CLIP 在细节区分和密集预测任务上的不足，研究者发现通过反转生成模型 unCLIP，可以有效改进 CLIP。unCLIP 作为生成模型，更擅长学习图像细节，且其条件输入空间与 CLIP 兼容。实验证明，un^2CLIP 在多项视觉和多模态任务上显著优于原始 CLIP 及现有改进方法，有效增强了 CLIP 的视觉细节捕捉能力。",
      "translated_title": "un^2CLIP: 通过反转 unCLIP 提升 CLIP 的视觉细节捕捉能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Contrastive Language-Image Pre-training (CLIP) has become a foundation model and has been applied to various vision and multimodal tasks. However, recent works indicate that CLIP falls short in distinguishing detailed differences in images and shows suboptimal performance on dense-prediction and vision-centric multimodal tasks. Therefore, this work focuses on improving existing CLIP models, aiming to capture as many visual details in images as possible. We find that a specific type of generative models, unCLIP, provides a suitable framework for achieving our goal. Specifically, unCLIP trains an image generator conditioned on the CLIP image embedding. In other words, it inverts the CLIP image encoder. Compared to discriminative models like CLIP, generative models are better at capturing image details because they are trained to learn the data distribution of images. Additionally, the conditional input space of unCLIP aligns with CLIP's original image-text embedding space. Therefore, we propose to invert unCLIP (dubbed un^2CLIP) to improve the CLIP model. In this way, the improved image encoder can gain unCLIP's visual detail capturing ability while preserving its alignment with the original text encoder simultaneously. We evaluate our improved CLIP across various tasks to which CLIP has been applied, including the challenging MMVP-VLM benchmark, the dense-prediction open-vocabulary segmentation task, and multimodal large language model tasks. Experiments show that un^2CLIP significantly improves the original CLIP and previous CLIP improvement methods. Code and models will be available at https://github.com/LiYinqi/un2CLIP."
    },
    {
      "title": "EasyText：可控扩散Transformer用于多语言文本渲染 (原标题: EasyText: Controllable Diffusion Transformer for Multilingual Text Rendering)",
      "link": "https://arxiv.org/abs/2505.24417",
      "pubDate": "Fri, 30 May 2025 05:55:39 GMT",
      "isoDate": "2025-05-30T05:55:39.000Z",
      "creator": "Runnan Lu, Yuxuan Zhang, Jailing Liu, Haifa Wang, Yiren Song",
      "summary": "# EasyText：可控扩散Transformer用于多语言文本渲染\n\n## 核心问题\n*   使用扩散模型生成准确的多语言文本一直是一个长期存在的挑战。\n*   尽管近期方法在单一语言文本渲染方面取得了进展，但任意语言的渲染仍是一个未充分探索的领域。\n\n## 提出的解决方案\n*   本文引入了 **EasyText**，一个基于DiT（Diffusion Transformer）的文本渲染框架。\n*   EasyText 的核心在于将去噪潜在空间与编码为字符token的多语言字符token连接起来。\n\n## 关键技术与创新\n*   **字符定位编码（Character Positioning Encoding）**：本文提出此技术以实现对文本渲染的可控性和精确性。\n*   **位置编码插值技术（Position Encoding Interpolation Techniques）**：结合字符定位编码，进一步提升了文本渲染的控制和精度。\n\n## 数据集构建\n*   构建了一个大规模的合成文本图像数据集，包含 **100万个多语言图像-文本标注**，用于模型的预训练。\n*   同时，构建了一个高质量数据集，包含 **2万个标注图像**，用于模型的精细调优。\n\n## 实验结果与贡献\n*   广泛的实验和评估表明，EasyText 方法在以下方面展现出显著的有效性和先进性：\n    *   多语言文本渲染能力\n    *   视觉质量\n    *   布局感知文本集成",
      "shortSummary": "EasyText 是一种基于扩散Transformer（DiT）的多语言文本渲染框架，旨在解决扩散模型生成准确多语言文本的挑战。它通过连接去噪潜在空间与多语言字符token，并引入字符定位编码和位置编码插值技术，实现了可控且精确的文本渲染。该研究构建了大规模合成数据集用于预训练，并使用高质量数据集进行微调。实验证明，EasyText 在多语言文本渲染、视觉质量和布局感知文本集成方面表现出色。",
      "translated_title": "EasyText：可控扩散Transformer用于多语言文本渲染",
      "images": [],
      "contentSource": "完整文章",
      "content": "Generating accurate multilingual text with diffusion models has long been desired but remains challenging. Recent methods have made progress in rendering text in a single language, but rendering arbitrary languages is still an unexplored area. This paper introduces EasyText, a text rendering framework based on DiT (Diffusion Transformer), which connects denoising latents with multilingual character tokens encoded as character tokens. We propose character positioning encoding and position encoding interpolation techniques to achieve controllable and precise text rendering. Additionally, we construct a large-scale synthetic text image dataset with 1 million multilingual image-text annotations as well as a high-quality dataset of 20K annotated images, which are used for pretraining and fine-tuning respectively. Extensive experiments and evaluations demonstrate the effectiveness and advancement of our approach in multilingual text rendering, visual quality, and layout-aware text integration."
    },
    {
      "title": "大型语言模型是局部线性映射 (原标题: Large Language Models are Locally Linear Mappings)",
      "link": "https://arxiv.org/abs/2505.24293",
      "pubDate": "Fri, 30 May 2025 03:08:33 GMT",
      "isoDate": "2025-05-30T03:08:33.000Z",
      "creator": "James R. Golden",
      "summary": "### 大型语言模型是局部线性映射\n\n本文探讨了大型语言模型（LLMs）的推理操作，并提出了一种创新方法，能够在不修改模型权重或改变输出预测的前提下，将LLMs的推理过程映射到一个精确等效的线性系统。\n\n**核心发现与方法：**\n\n*   **局部线性特性：** 论文指出，尽管现代LLMs具有强大的表达能力和全局非线性，但其推理操作可以被解释为近似精确的局部线性分解。\n*   **线性系统映射：** 研究人员借鉴了图像扩散模型中展现局部或分段线性的技术，策略性地调整了针对给定输入序列的梯度计算，以预测下一个token。这种方法使得模型的雅可比矩阵（Jacobian）能够通过一个线性系统几乎精确地再现前向预测。\n*   **低维子空间操作：** 通过对分离的雅可比矩阵进行奇异值分解（SVD），研究发现这些LLMs在极低维的子空间中运行。其中，许多最大的奇异向量解码为与最可能输出token相关的概念。\n*   **模型适用性：** 这种方法已在多种主流LLMs上得到验证，包括Llama 3、Gemma 3、Qwen 3、Phi 4、Mistral、OLMo 2，以及高达Llama 3.3 70B Q4等模型。\n\n**研究意义：**\n\n*   **内部机制洞察：** 这种局部线性分解方法为深入理解LLMs的内部表示提供了新的见解。\n*   **语义结构揭示：** 它揭示了在下一个token预测过程中可解释的语义结构。\n*   **逐层分析：** 该方法还允许将每个连续层（及其注意力机制和MLP组件）视为近似精确的线性系统进行检查，从而观察语义概念的出现。",
      "shortSummary": "本文提出大型语言模型（LLMs）的推理过程可映射为精确等效的局部线性系统。通过策略性地改变梯度计算，研究发现LLMs在极低维子空间中运行，其主要奇异向量解码出与预测token相关的语义概念。该方法适用于多种LLMs，为理解其内部表示和揭示可解释的语义结构提供了新视角。",
      "translated_title": "大型语言模型是局部线性映射",
      "images": [],
      "contentSource": "完整文章",
      "content": "We demonstrate that the inference operations of several open-weight large language models (LLMs) can be mapped to an exactly equivalent linear system for an input sequence without modifying the model weights or altering output predictions. Extending techniques from image diffusion models that exhibit local or piecewise linearity, we strategically alter the gradient computation with respect to a given input sequence for a next-token prediction such that the Jacobian of the model nearly exactly reproduces the forward prediction with a linear system. We demonstrate this approach across models (Llama 3, Gemma 3, Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show through the singular value decomposition of the detached Jacobian that these LLMs operate in extremely low-dimensional subspaces where many of the largest singular vectors decode to concepts related to the most-likely output token. This approach also allows us to examine the operation of each successive layer (and its attention and MLP components) as nearly-exact linear systems and observe the emergence of semantic concepts. Despite their expressive power and global nonlinearity, modern LLMs can be interpreted through nearly-exact locally linear decompositions that provide insights into their internal representations and reveal interpretable semantic structures in the next-token prediction process."
    },
    {
      "title": "CLaSp：自推测解码的上下文层跳过 (原标题: CLaSp: In-Context Layer Skip for Self-Speculative Decoding)",
      "link": "https://arxiv.org/abs/2505.24196",
      "pubDate": "Fri, 30 May 2025 00:15:06 GMT",
      "isoDate": "2025-05-30T00:15:06.000Z",
      "creator": "Longze Chen, Renke Shan, Huiming Wang, Lu Wang, Ziqiang Liu, Run Luo, Jiawei Wang, Hamid Alinejad-Rokny, Min Yang",
      "summary": "# CLaSp：自推测解码的上下文层跳过策略\n\n## 摘要\n\n本文介绍了一种名为 CLaSp 的创新方法，旨在显著加速大型语言模型（LLM）的解码过程。CLaSp 是一种用于自推测解码的上下文层跳过策略，其主要优势在于无需额外的训练或模块，即可实现高效且兼容的加速。\n\n## 背景：推测解码 (Speculative Decoding, SD)\n\n*   **加速 LLM 解码：** 推测解码（SD）是一种有前景的 LLM 解码加速方法。\n*   **效率瓶颈：** SD 的效率主要取决于草稿模型和验证模型之间的一致性。\n*   **现有方法局限：** 当前的草稿生成方法通常需要训练额外的模块，这在实际部署和确保与各种 LLM 的兼容性方面带来了挑战。\n\n## CLaSp 方法概述\n\n*   **核心创新：** CLaSp 提出了一种即插即用的机制，通过跳过验证模型的中间层来构建一个压缩的草稿模型。\n*   **无需额外训练：** 与以往方法不同，CLaSp 不需要额外的草稿模块或额外的训练，极大地简化了实施。\n*   **动态层跳过优化：**\n    *   **算法设计：** CLaSp 开发了一种动态规划算法，用于优化层跳过过程。\n    *   **优化目标：** 该算法利用上一个验证阶段的完整隐藏状态作为优化目标。\n    *   **自适应性：** 这种设计使得 CLaSp 能够在每个验证阶段后动态调整其层跳过策略，从而避免了对预先优化或固定跳过层集合的依赖。\n\n## 实验结果\n\n*   **测试模型：** CLaSp 在 LLaMA3 系列模型上进行了广泛的实验。\n*   **性能提升：** 在多种下游任务中，CLaSp 实现了 1.3 倍至 1.7 倍的解码速度提升。\n*   **文本质量保持：** 关键的是，CLaSp 在提供显著加速的同时，并未改变生成文本的原始分布，确保了生成内容的质量和准确性。",
      "shortSummary": "CLaSp 是一种无需额外训练的自推测解码方法，通过动态跳过验证模型的中间层来构建压缩的草稿模型。它利用动态规划算法优化层跳过，并能根据前一验证阶段的隐藏状态自适应调整策略。实验表明，CLaSp 在 LLaMA3 模型上实现了 1.3-1.7 倍的加速，同时保持了生成文本的质量，是加速 LLM 解码的有效即插即用方案。",
      "translated_title": "CLaSp：自推测解码的上下文层跳过",
      "images": [],
      "contentSource": "完整文章",
      "content": "Speculative decoding (SD) is a promising method for accelerating the decoding process of Large Language Models (LLMs). The efficiency of SD primarily hinges on the consistency between the draft model and the verify model. However, existing drafting approaches typically require additional modules to be trained, which can be challenging to implement and ensure compatibility across various LLMs. In this paper, we propose CLaSp, an in-context layer-skipping strategy for self-speculative decoding. Unlike prior methods, CLaSp does not require additional drafting modules or extra training. Instead, it employs a plug-and-play mechanism by skipping intermediate layers of the verify model to construct a compressed draft model. Specifically, we develop a dynamic programming algorithm that optimizes the layer-skipping process by leveraging the complete hidden states from the last verification stage as an objective. This enables CLaSp to dynamically adjust its layer-skipping strategy after each verification stage, without relying on pre-optimized sets of skipped layers. Experimental results across diverse downstream tasks demonstrate that CLaSp achieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the original distribution of the generated text."
    },
    {
      "title": "HardTests：为大型语言模型编程合成高质量测试用例 (原标题: HardTests: Synthesizing High-Quality Test Cases for LLM Coding)",
      "link": "https://arxiv.org/abs/2505.24098",
      "pubDate": "Thu, 29 May 2025 21:00:34 GMT",
      "isoDate": "2025-05-29T21:00:34.000Z",
      "creator": "Zhongmou He, Yee Man Choi, Kexun Zhang, Jiabao Ji, Junting Zhou, Dejia Xu, Ivan Bercovich, Aidan Zhang, Lei Li",
      "summary": "### 背景与挑战\n\n*   在大型语言模型（LLM）的推理过程中，验证器扮演着至关重要的角色，尤其对于强化学习等后训练技术而言。\n*   然而，对于复杂的编程问题，获取可靠的验证器面临挑战。\n*   主要原因在于，一个看似正确的错误解决方案可能伪装得很好，需要人工精心编写的边缘测试用例才能被检测出来，而这类测试用例很难自动合成。\n\n### 解决方案：HARDTESTGEN 流水线\n\n*   为解决上述问题，研究团队提出了 **HARDTESTGEN**，这是一个利用大型语言模型（LLM）合成高质量测试用例的流水线。\n\n### 数据集：HARDTESTS\n\n*   通过 HARDTESTGEN 流水线，研究团队整理并创建了一个全面的竞技编程数据集 **HARDTESTS**。\n*   该数据集包含 **47,000 个问题**，并附带了合成的高质量测试用例。\n\n### 评估结果与性能提升\n\n*   与现有测试用例相比，HARDTESTGEN 生成的测试在评估 LLM 生成的代码时展现出显著的性能提升：\n    *   **精确度** 提高了 **11.3 个百分点**。\n    *   **召回率** 提高了 **17.5 个百分点**。\n    *   对于更困难的编程问题，精确度的提升甚至可以高达 **40 个百分点**。\n\n### 对模型训练的益处\n\n*   HARDTESTS 数据集在模型训练方面也表现出更高的有效性，这一优势通过下游代码生成性能的提升得到了衡量。\n\n### 可用性\n\n*   该数据集和合成流水线将进行开源。",
      "shortSummary": "针对大型语言模型（LLM）编程中难以获取高质量验证器的问题，研究提出HARDTESTGEN流水线，用于合成高质量测试用例。该流水线构建了包含4.7万个问题的HARDTESTS数据集。实验表明，HARDTESTGEN测试在评估LLM生成代码时，精确度提高11.3%，召回率提高17.5%，对更难问题精确度提升高达40%。此外，HARDTESTS对模型训练也更有效。数据集和流水线将开源。",
      "translated_title": "HardTests：为大型语言模型编程合成高质量测试用例",
      "images": [],
      "contentSource": "完整文章",
      "content": "Verifiers play a crucial role in large language model (LLM) reasoning, needed by post-training techniques such as reinforcement learning. However, reliable verifiers are hard to get for difficult coding problems, because a well-disguised wrong solution may only be detected by carefully human-written edge cases that are difficult to synthesize. To address this issue, we propose HARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this pipeline, we curate a comprehensive competitive programming dataset HARDTESTS with 47k problems and synthetic high-quality tests. Compared with existing tests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points higher and recall that is 17.5 percentage points higher when evaluating LLM-generated code. For harder problems, the improvement in precision can be as large as 40 points. HARDTESTS also proves to be more effective for model training, measured by downstream code generation performance. We will open-source our dataset and synthesis pipeline at https://leililab.github.io/HardTests/."
    },
    {
      "title": "视觉语言模型存在偏见 (原标题: Vision Language Models are Biased)",
      "link": "https://arxiv.org/abs/2505.23941",
      "pubDate": "Thu, 29 May 2025 14:47:58 GMT",
      "isoDate": "2025-05-29T14:47:58.000Z",
      "creator": "An Vo, Khai-Nguyen Nguyen, Mohammad Reza Taesiri, Vy Tuong Dang, Anh Totti Nguyen, Daeyoung Kim",
      "summary": "# 视觉语言模型中的偏见研究\n\n## 摘要\n\n本研究探讨了大型语言模型（LLMs）从互联网中记忆的先验知识如何导致视觉语言模型（VLMs）在标准、客观的视觉任务（如计数和识别）中产生偏见并降低准确性。\n\n## 主要发现\n\n*   **显著偏见：** 最先进的视觉语言模型表现出强烈的偏见。例如，它们无法识别一个在原有三条杠的阿迪达斯标志上额外添加的第四条杠。\n*   **低计数准确率：** 在动物、标志、国际象棋、棋盘游戏、光学错觉和图案网格等7个不同领域中，VLMs在计数任务上的平均准确率仅为17.05%。\n*   **文本描述的影响：** 在反事实图像中插入描述主题名称的文本（例如“Adidas”）会进一步降低VLM的准确率。这表明文本提示可能会加剧模型的偏见。\n*   **缓解措施效果不佳：** 即使指示模型仔细核对结果或完全依赖图像细节进行回答，平均而言，计数准确率也仅能提高2个百分点，这表明偏见根深蒂固，难以通过简单指令消除。\n\n## 研究贡献\n\n本工作揭示了视觉语言模型中一个有趣的失败模式，并提出了一个用于测试VLM偏见的自动化框架。\n\n## 代码和数据\n\n研究中使用的代码和数据已公开提供。",
      "shortSummary": "本研究发现视觉语言模型（VLMs）因记忆的先验知识而存在严重偏见。在计数和识别等客观视觉任务中，最先进的VLMs平均准确率仅为17.05%，例如无法识别阿迪达斯标志上的额外条纹。插入文本描述会进一步降低准确率，而简单的指令也无法显著改善性能。该工作揭示了VLMs的一种失败模式，并提供了一个测试其偏见的自动化框架。",
      "translated_title": "视觉语言模型存在偏见",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) memorize a vast amount of prior knowledge from the Internet that help them on downstream tasks but also may notoriously sway their outputs towards wrong or biased answers. In this work, we test how the knowledge about popular subjects hurt the accuracy of vision language models (VLMs) on standard, objective visual tasks of counting and identification. We find that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a fourth stripe has been added to a 3-stripe Adidas logo) scoring an average of 17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo) across 7 diverse domains from animals, logos, chess, board games, optical illusions, to patterned grids. Insert text (e.g., \"Adidas\") describing the subject name into the counterfactual image further decreases VLM accuracy. The biases in VLMs are so strong that instructing them to double-check their results or rely exclusively on image details to answer improves counting accuracy by only +2 points, on average. Our work presents an interesting failure mode in VLMs and an automated framework for testing VLM biases. Code and data are available at: vlmsarebiased.github.io."
    },
    {
      "title": "Point-MoE：通过专家混合模型实现3D语义分割中的跨域泛化 (原标题: Point-MoE: Towards Cross-Domain Generalization in 3D Semantic Segmentation via Mixture-of-Experts)",
      "link": "https://arxiv.org/abs/2505.23926",
      "pubDate": "Thu, 29 May 2025 14:21:47 GMT",
      "isoDate": "2025-05-29T14:21:47.000Z",
      "creator": "Xuweiyi Chen, Wentao Zhou, Aruni RoyChowdhury, Zezhou Cheng",
      "summary": "## Point-MoE：通过专家混合模型实现3D语义分割中的跨域泛化\n\n### 摘要\n\n尽管规模化定律已彻底改变了自然语言处理和计算机视觉领域，但3D点云理解尚未达到同等阶段。这主要归因于两个方面：\n\n*   **数据集规模限制：** 3D数据集的规模相对较小。\n*   **数据来源异构性：** 点云数据由多种传感器（例如，深度相机、激光雷达）在不同领域（例如，室内、室外）捕获，每种来源都引入了独特的扫描模式、采样密度和语义偏差。这种领域异构性对大规模训练统一模型构成了主要障碍，尤其是在推理时通常无法获取领域标签的现实约束下。\n\n### 提出的解决方案：Point-MoE\n\n为解决上述挑战，本文提出了一种名为 **Point-MoE** 的专家混合（Mixture-of-Experts, MoE）架构，旨在实现3D感知中的大规模、跨域泛化。\n\n### 关键发现与机制\n\n*   研究表明，标准点云骨干网络在混合域数据上训练时，性能会显著下降。\n*   Point-MoE的优势在于，即使在没有领域标签的情况下，通过简单的top-k路由策略，也能自动地使专家模型进行专业化，从而适应不同领域的特性。\n\n### 实验结果\n\n*   实验证明，Point-MoE不仅在性能上超越了强大的多域基线模型。\n*   它还对未见过的领域表现出更好的泛化能力。\n\n### 结论与未来展望\n\n这项工作为3D理解指明了一条可扩展的道路：即让模型自行从多样化的3D数据中发现结构，而不是通过手动整理或领域监督来强加结构。",
      "shortSummary": "3D点云理解因数据异构性难以实现跨域泛化。本文提出Point-MoE，一种专家混合（MoE）架构，旨在解决此问题。Point-MoE通过简单的路由策略，即使在没有领域标签的情况下，也能自动使专家模型专业化。实验表明，Point-MoE不仅超越了现有的多域基线模型，还对未见领域展现出更强的泛化能力。这项工作为3D理解提供了一条可扩展的路径，即让模型自主发现多样化3D数据中的结构。",
      "translated_title": "Point-MoE：通过专家混合模型实现3D语义分割中的跨域泛化",
      "images": [],
      "contentSource": "完整文章",
      "content": "While scaling laws have transformed natural language processing and computer vision, 3D point cloud understanding has yet to reach that stage. This can be attributed to both the comparatively smaller scale of 3D datasets, as well as the disparate sources of the data itself. Point clouds are captured by diverse sensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor, outdoor), each introducing unique scanning patterns, sampling densities, and semantic biases. Such domain heterogeneity poses a major barrier towards training unified models at scale, especially under the realistic constraint that domain labels are typically inaccessible at inference time. In this work, we propose Point-MoE, a Mixture-of-Experts architecture designed to enable large-scale, cross-domain generalization in 3D perception. We show that standard point cloud backbones degrade significantly in performance when trained on mixed-domain data, whereas Point-MoE with a simple top-k routing strategy can automatically specialize experts, even without access to domain labels. Our experiments demonstrate that Point-MoE not only outperforms strong multi-domain baselines but also generalizes better to unseen domains. This work highlights a scalable path forward for 3D understanding: letting the model discover structure in diverse 3D data, rather than imposing it via manual curation or domain supervision."
    },
    {
      "title": "MMSI-Bench：多图像空间智能基准 (原标题: MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence)",
      "link": "https://arxiv.org/abs/2505.23764",
      "pubDate": "Thu, 29 May 2025 13:59:52 GMT",
      "isoDate": "2025-05-29T13:59:52.000Z",
      "creator": "Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, Dahua Lin, Tai Wang, Jiangmiao Pang",
      "summary": "# MMSI-Bench：多图像空间智能基准\n\n## 引言与背景\n空间智能对于在复杂物理世界中运行的多模态大型语言模型（MLLMs）至关重要。然而，现有基准仅探测单图像关系，无法评估真实世界部署所需的多图像空间推理能力。为了弥补这一空白，研究人员引入了MMSI-Bench，一个专为多图像空间智能设计的视觉问答（VQA）基准。\n\n## 基准的构建\nMMSI-Bench的构建过程严谨且耗时：\n*   **研究团队：** 由六位3D视觉研究人员共同完成。\n*   **投入时间：** 耗时超过300小时。\n*   **问题数量与来源：** 精心制作了1,000个具有挑战性、无歧义的多项选择题，这些问题来源于超过120,000张图像。\n*   **问题设计：** 每个问题都配有精心设计的干扰项和一步一步的推理过程注释。\n\n## 实验与评估结果\n研究团队对34个开源和专有MLLMs进行了广泛的实验和彻底评估，结果揭示了显著的性能差距：\n*   **模型表现：**\n    *   最强的开源模型准确率约为30%。\n    *   OpenAI的o3推理模型达到40%的准确率。\n    *   人类表现：高达97%的准确率。\n*   **挑战性：** 这些结果凸显了MMSI-Bench的巨大挑战性，以及未来研究的巨大提升空间。\n\n## 自动化错误分析\nMMSI-Bench利用其注释的推理过程，提供了一个自动化的错误分析流程，能够诊断四种主要的失败模式，为推进多图像空间智能提供了宝贵的见解：\n1.  **基础定位错误（Grounding errors）：** 模型未能正确识别或定位图像中的对象。\n2.  **重叠匹配和场景重建错误（Overlap-matching and scene-reconstruction errors）：** 模型在处理图像重叠或重建完整场景时出现问题。\n3.  **情境转换推理错误（Situation-transformation reasoning errors）：** 模型在理解情境变化或动态场景推理时遇到困难。\n4.  **空间逻辑错误（Spatial-logic errors）：** 模型在应用基本的空间逻辑规则时出错。\n\n## 总结\nMMSI-Bench是一个全面、完全由人工策划的多图像空间智能基准，带有推理注释，旨在推动MLLMs在该领域的发展。",
      "shortSummary": "MMSI-Bench是一个新颖的视觉问答（VQA）基准，专注于评估多模态大型语言模型（MLLMs）的多图像空间智能。它通过1,000个精心设计的问题，弥补了现有基准仅关注单图像关系的不足。对34个MLLMs的评估显示，模型表现（约30-40%）与人类（97%）存在巨大差距，表明该领域仍有广阔的研究空间。MMSI-Bench还提供了自动化错误分析，识别出四种主要失败模式，为未来研究提供了方向。",
      "translated_title": "MMSI-Bench：多图像空间智能基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Spatial intelligence is essential for multimodal large language models (MLLMs) operating in the complex physical world. Existing benchmarks, however, probe only single-image relations and thus fail to assess the multi-image spatial reasoning that real-world deployments demand. We introduce MMSI-Bench, a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision researchers spent more than 300 hours meticulously crafting 1,000 challenging, unambiguous multiple-choice questions from over 120,000 images, each paired with carefully designed distractors and a step-by-step reasoning process. We conduct extensive experiments and thoroughly evaluate 34 open-source and proprietary MLLMs, observing a wide gap: the strongest open-source model attains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while humans score 97%. These results underscore the challenging nature of MMSI-Bench and the substantial headroom for future research. Leveraging the annotated reasoning processes, we also provide an automated error analysis pipeline that diagnoses four dominant failure modes, including (1) grounding errors, (2) overlap-matching and scene-reconstruction errors, (3) situation-transformation reasoning errors, and (4) spatial-logic errors, offering valuable insights for advancing multi-image spatial intelligence. Project page: https://runsenxu.com/projects/MMSI_Bench ."
    },
    {
      "title": "ZeroGUI：以零人工成本自动化在线GUI学习 (原标题: ZeroGUI: Automating Online GUI Learning at Zero Human Cost)",
      "link": "https://arxiv.org/abs/2505.23762",
      "pubDate": "Thu, 29 May 2025 13:59:51 GMT",
      "isoDate": "2025-05-29T13:59:51.000Z",
      "creator": "Chenyu Yang, Shiqian Su, Shi Liu, Xuan Dong, Yue Yu, Weijie Su, Xuehui Wang, Zhaoyang Liu, Jinguo Zhu, Hao Li, Wenhai Wang, Yu Qiao, Xizhou Zhu, Jifeng Dai",
      "summary": "# ZeroGUI：以零人工成本自动化在线GUI学习\n\n## 引言\n随着大型视觉-语言模型（VLM）的快速发展，纯视觉GUI（图形用户界面）Agent在感知和操作GUI以自主完成用户指令方面取得了显著进展。然而，现有方法通常采用离线学习框架，这带来了两个核心局限性：\n\n1.  **高度依赖人工标注：** 需要大量高质量的人工标注来完成元素定位（element grounding）和动作监督。\n2.  **适应性有限：** 对动态和交互式环境的适应能力有限。\n\n## ZeroGUI框架\n为了解决上述局限性，本文提出了 **ZeroGUI**，一个可扩展的在线学习框架，旨在以零人工成本自动化GUI Agent的训练。ZeroGUI通过整合以下三个关键组件来实现这一目标：\n\n### 1. 基于VLM的自动任务生成\nZeroGUI能够从当前环境状态自动生成多样化的训练目标。这意味着系统不再需要预先定义或人工创建任务，而是可以根据实时的GUI界面情况，利用VLM的能力自动生成新的学习任务，从而大大增加了训练数据的多样性和覆盖范围。\n\n### 2. 基于VLM的自动奖励评估\n该框架利用VLM自动评估任务的成功与否，而无需手动设计复杂的评估函数。VLM可以直接理解任务目标和当前环境状态，并据此判断Agent的行动是否达到了预期效果，从而提供实时的奖励信号，指导Agent的学习过程。\n\n### 3. 两阶段在线强化学习\nZeroGUI采用两阶段的在线强化学习机制，使Agent能够持续与GUI环境进行交互并从中学习。这种在线学习范式使得Agent能够实时适应环境变化，并通过试错不断优化其操作策略，克服了离线学习在动态环境中的适应性问题。\n\n## 实验结果\n研究人员在两个先进的GUI Agent（UI-TARS和Aguvis）上对ZeroGUI进行了实验验证。实验结果表明，ZeroGUI在OSWorld和AndroidLab这两种GUI环境中显著提升了这些Agent的性能。这证明了ZeroGUI框架在提高GUI Agent训练效率和效果方面的有效性。\n\n## 结论\nZeroGUI提供了一种创新的、无需人工干预的在线学习方法，有效解决了当前GUI Agent训练中面临的标注成本高昂和环境适应性差的问题。通过自动化任务生成、奖励评估和在线强化学习，ZeroGUI为未来GUI Agent的自主学习和部署开辟了新的道路。",
      "shortSummary": "ZeroGUI是一个创新的在线学习框架，旨在以零人工成本自动化GUI Agent的训练。它解决了现有离线学习方法对人工标注的重度依赖和对动态环境适应性差的问题。ZeroGUI通过基于VLM的自动任务生成、自动奖励评估以及两阶段在线强化学习实现。实验证明，ZeroGUI显著提升了UI-TARS和Aguvis等GUI Agent在OSWorld和AndroidLab环境中的性能，为GUI Agent的自主学习提供了高效解决方案。",
      "translated_title": "ZeroGUI：以零人工成本自动化在线GUI学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "The rapid advancement of large Vision-Language Models (VLMs) has propelled the development of pure-vision-based GUI Agents, capable of perceiving and operating Graphical User Interfaces (GUI) to autonomously fulfill user instructions. However, existing approaches usually adopt an offline learning framework, which faces two core limitations: (1) heavy reliance on high-quality manual annotations for element grounding and action supervision, and (2) limited adaptability to dynamic and interactive environments. To address these limitations, we propose ZeroGUI, a scalable, online learning framework for automating GUI Agent training at Zero human cost. Specifically, ZeroGUI integrates (i) VLM-based automatic task generation to produce diverse training goals from the current environment state, (ii) VLM-based automatic reward estimation to assess task success without hand-crafted evaluation functions, and (iii) two-stage online reinforcement learning to continuously interact with and learn from GUI environments. Experiments on two advanced GUI Agents (UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance across OSWorld and AndroidLab environments. The code is available at https://github.com/OpenGVLab/ZeroGUI."
    },
    {
      "title": "差分信息：偏好优化中的信息论视角 (原标题: Differential Information: An Information-Theoretic Perspective on Preference Optimization)",
      "link": "https://arxiv.org/abs/2505.23761",
      "pubDate": "Thu, 29 May 2025 13:59:50 GMT",
      "isoDate": "2025-05-29T13:59:50.000Z",
      "creator": "Yunjae Won, Hyunji Lee, Hyeonbin Hwang, Minjoon Seo",
      "summary": "### 差分信息：偏好优化中的信息论视角\n\n本文旨在从信息论角度深入探讨直接偏好优化（DPO）的理论基础，特别是其对数比率奖励参数化的合理性。\n\n**核心问题与方法：**\n\n*   **问题：** 尽管直接偏好优化（DPO）在监督式对齐语言模型与人类偏好方面取得了经验上的成功，但其对数比率奖励参数化背后的理论依据尚不完整。\n*   **方法：** 本文通过引入**差分信息分布（Differential Information Distribution, DID）**来解决这一理论空白。DID是一种关于令牌序列的分布，它捕获了策略更新过程中获得的信息。\n\n**主要发现与贡献：**\n\n1.  **对数比率奖励的最优性：**\n    *   研究表明，当偏好标签编码了将参考策略转换为目标策略所需的差分信息时，DPO中的对数比率奖励形式是学习目标策略的唯一最优形式。\n    *   这一结果自然地导出了被拒绝响应的最优采样分布的闭式表达式。\n\n2.  **与隐含归纳偏置的关联：**\n    *   偏好编码差分信息的条件与对数边际有序策略（log-margin ordered policies）之间存在根本性联系。\n    *   对数边际有序策略是偏好优化中广泛使用但此前未被识别的一种隐含归纳偏置。\n\n3.  **差分信息熵的分析：**\n    *   通过分析DID的熵，本文揭示了不同熵值对策略行为的影响：\n        *   学习低熵差分信息会强化策略分布。\n        *   学习高熵差分信息会产生平滑效应，这解释了对数似然位移（log-likelihood displacement）现象。\n\n**实验验证与实际启示：**\n\n*   作者在合成实验中验证了理论发现，并将其扩展到真实的指令遵循数据集。\n*   研究结果表明：\n    *   学习高熵差分信息对于**通用指令遵循**至关重要。\n    *   学习低熵差分信息则有利于**知识密集型问答**。\n\n**总结：**\n\n本文通过差分信息的视角，为DPO目标、偏好数据结构以及由此产生的策略行为提供了一个统一的理解框架。",
      "shortSummary": "本文从信息论角度探讨直接偏好优化（DPO）的理论基础。通过引入差分信息分布（DID），研究发现当偏好标签编码了将参考策略转换为目标策略所需的差分信息时，DPO的对数比率奖励是学习目标策略的唯一最优形式，并揭示了DPO与对数边际有序策略的隐含关联。此外，分析DID的熵表明，低熵差分信息强化策略，高熵差分信息则产生平滑效应。实验验证了这些发现，并指出高熵差分信息对通用指令遵循至关重要，而低熵信息则有利于知识密集型问答。",
      "translated_title": "差分信息：偏好优化中的信息论视角",
      "images": [],
      "contentSource": "完整文章",
      "content": "Direct Preference Optimization (DPO) has become a standard technique for aligning language models with human preferences in a supervised manner. Despite its empirical success, the theoretical justification behind its log-ratio reward parameterization remains incomplete. In this work, we address this gap by utilizing the Differential Information Distribution (DID): a distribution over token sequences that captures the information gained during policy updates. First, we show that when preference labels encode the differential information required to transform a reference policy into a target policy, the log-ratio reward in DPO emerges as the uniquely optimal form for learning the target policy via preference optimization. This result naturally yields a closed-form expression for the optimal sampling distribution over rejected responses. Second, we find that the condition for preferences to encode differential information is fundamentally linked to an implicit assumption regarding log-margin ordered policies-an inductive bias widely used in preference optimization yet previously unrecognized. Finally, by analyzing the entropy of the DID, we characterize how learning low-entropy differential information reinforces the policy distribution, while high-entropy differential information induces a smoothing effect, which explains the log-likelihood displacement phenomenon. We validate our theoretical findings in synthetic experiments and extend them to real-world instruction-following datasets. Our results suggest that learning high-entropy differential information is crucial for general instruction-following, while learning low-entropy differential information benefits knowledge-intensive question answering. Overall, our work presents a unifying perspective on the DPO objective, the structure of preference data, and resulting policy behaviors through the lens of differential information."
    },
    {
      "title": "被谜题难倒：当视觉-语言模型无法领会暗示时 (原标题: Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint)",
      "link": "https://arxiv.org/abs/2505.23759",
      "pubDate": "Thu, 29 May 2025 13:59:47 GMT",
      "isoDate": "2025-05-29T13:59:47.000Z",
      "creator": "Heekyung Lee, Jiaxin Ge, Tsung-Han Wu, Minwoo Kang, Trevor Darrell, David M. Chan",
      "summary": "### 视觉-语言模型在字谜（Rebus Puzzles）面前的挑战\n\n**1. 引言与挑战**\n*   **字谜（Rebus Puzzles）定义：** 字谜是一种独特的视觉谜题，它通过图像、空间排列和符号替换来编码语言信息。\n*   **对现有模型构成挑战：** 这类谜题对当前的视觉-语言模型（VLMs）构成了独特的挑战。\n*   **与传统任务的区别：** 与传统的图像描述或问答任务不同，解决字谜需要：\n    *   多模态抽象能力\n    *   符号推理能力\n    *   对文化、语音和语言双关语的理解\n\n**2. 研究方法**\n*   **研究目的：** 本文旨在调查当代视觉-语言模型解释和解决字谜的能力。\n*   **基准数据集构建：** 研究人员构建了一个手工生成并标注的、多样化的英语字谜基准数据集。\n*   **谜题范围：** 数据集中的谜题难度各异，从简单的象形替换（如用图片代表单词）到依赖空间排列的线索（如“head”在“heels”上方表示“head over heels”）。\n\n**3. 研究发现**\n*   **模型表现分析：** 研究分析了不同视觉-语言模型的表现。\n*   **初步能力：** 研究结果显示，视觉-语言模型在解码简单的视觉线索方面展现出一些令人惊讶的能力。\n*   **主要局限：** 然而，它们在需要以下能力的任务上表现出显著的困难：\n    *   抽象推理\n    *   横向思维（跳出常规的思考方式）\n    *   理解视觉隐喻",
      "shortSummary": "当前视觉-语言模型（VLMs）在解决字谜（rebus puzzles）方面面临显著挑战。这类视觉谜题要求多模态抽象、符号推理及对文化和语言双关语的理解。研究人员构建了一个手工标注的英语字谜基准，发现VLMs虽能解码简单视觉线索，但在抽象推理、横向思维和理解视觉隐喻方面表现出明显不足。",
      "translated_title": "被谜题难倒：当视觉-语言模型无法领会暗示时",
      "images": [],
      "contentSource": "完整文章",
      "content": "Rebus puzzles, visual riddles that encode language through imagery, spatial arrangement, and symbolic substitution, pose a unique challenge to current vision-language models (VLMs). Unlike traditional image captioning or question answering tasks, rebus solving requires multi-modal abstraction, symbolic reasoning, and a grasp of cultural, phonetic and linguistic puns. In this paper, we investigate the capacity of contemporary VLMs to interpret and solve rebus puzzles by constructing a hand-generated and annotated benchmark of diverse English-language rebus puzzles, ranging from simple pictographic substitutions to spatially-dependent cues (\"head\" over \"heels\"). We analyze how different VLMs perform, and our findings reveal that while VLMs exhibit some surprising capabilities in decoding simple visual clues, they struggle significantly with tasks requiring abstract reasoning, lateral thinking, and understanding visual metaphors."
    }
  ],
  "lastUpdated": "2025-06-02T09:38:54.431Z"
}