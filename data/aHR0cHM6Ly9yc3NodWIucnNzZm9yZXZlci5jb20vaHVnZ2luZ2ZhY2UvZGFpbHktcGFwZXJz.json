{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation",
      "link": "https://arxiv.org/abs/2511.11434",
      "pubDate": "Fri, 14 Nov 2025 11:02:38 GMT",
      "isoDate": "2025-11-14T11:02:38.000Z",
      "creator": "Wei Chow, Jiachun Pan, Yongyuan Liang, Mingze Zhou, Xue Song, Liyu Jia, Saining Zhang, Siliang Tang, Juncheng Li, Fengda Zhang, Weijia Wu, Hanwang Zhang, Tat-Seng Chua",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation",
      "images": [],
      "contentSource": "RSS",
      "content": "Recent advances in unified multimodal models (UMMs) have enabled impressive progress in visual comprehension and generation. However, existing datasets and benchmarks focus primarily on single-turn interactions, failing to capture the multi-turn, context-dependent nature of real-world image creation and editing. To address this gap, we present WEAVE, the first suite for in-context interleaved cross-modality comprehension and generation. Our suite consists of two complementary parts. WEAVE-100k is a large-scale dataset of 100K interleaved samples spanning over 370K dialogue turns and 500K images, covering comprehension, editing, and generation tasks that require reasoning over historical context. WEAVEBench is a human-annotated benchmark with 100 tasks based on 480 images, featuring a hybrid VLM judger evaluation framework based on both the reference image and the combination of the original image with editing instructions that assesses models' abilities in multi-turn generation, visual memory, and world-knowledge reasoning across diverse domains. Experiments demonstrate that training on WEAVE-100k enables vision comprehension, image editing, and comprehension-generation collaboration capabilities. Furthermore, it facilitates UMMs to develop emergent visual-memory capabilities, while extensive evaluations on WEAVEBench expose the persistent limitations and challenges of current approaches in multi-turn, context-aware image generation and editing. We believe WEAVE provides a view and foundation for studying in-context interleaved comprehension and generation for multi-modal community."
    },
    {
      "title": "MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism",
      "link": "https://arxiv.org/abs/2511.11373",
      "pubDate": "Fri, 14 Nov 2025 09:52:34 GMT",
      "isoDate": "2025-11-14T09:52:34.000Z",
      "creator": "Shulin Liu, Dong Du, Tao Yang, Yang Li, Boyu Qiu",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism",
      "images": [],
      "contentSource": "RSS",
      "content": "Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks."
    },
    {
      "title": "AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery",
      "link": "https://arxiv.org/abs/2511.11257",
      "pubDate": "Fri, 14 Nov 2025 07:53:57 GMT",
      "isoDate": "2025-11-14T07:53:57.000Z",
      "creator": "Yuqi Yin, Yibo Fu, Siyuan Wang, Peng Sun, Hongyu Wang, Xiaohui Wang, Lei Zheng, Zhiyong Li, Zhirong Liu, Jianji Wang, Zhaoxi Sun",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery",
      "images": [],
      "contentSource": "RSS",
      "content": "The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery."
    },
    {
      "title": "Virtual Width Networks",
      "link": "https://arxiv.org/abs/2511.11238",
      "pubDate": "Fri, 14 Nov 2025 07:41:57 GMT",
      "isoDate": "2025-11-14T07:41:57.000Z",
      "creator": "Seed, Baisheng Li, Banggu Wu, Bole Ma, Bowen Xiao, Chaoyi Zhang, Cheng Li, Chengyi Wang, Chenyin Xu, Chi Zhang, Chong Hu, Daoguang Zan, Defa Zhu, Dongyu Xu, Du Li, Faming Wu, Fan Xia, Ge Zhang, Guang Shi, Haobin Chen, Hongyu Zhu, Hongzhi Huang, Huan Zhou, Huanzhang Dou, Jianhui Duan, Jianqiao Lu, Jianyu Jiang, Jiayi Xu, Jiecao Chen, Jin Chen, Jin Ma, Jing Su, Jingji Chen, Jun Wang, Jun Yuan, Juncai Liu, Jundong Zhou, Kai Hua, Kai Shen, Kai Xiang, Kaiyuan Chen, Kang Liu, Ke Shen, Liang Xiang, Lin Yan, Lishu Luo, Mengyao Zhang, Ming Ding, Mofan Zhang, Nianning Liang, Peng Li, Penghao Huang, Pengpeng Mu, Qi Huang, Qianli Ma, Qiyang Min, Qiying Yu, Renming Pang, Ru Zhang, Shen Yan, Shen Yan, Shixiong Zhao, Shuaishuai Cao, Shuang Wu, Siyan Chen, Siyu Li, Siyuan Qiao, Tao Sun, Tian Xin, Tiantian Fan, Ting Huang, Ting-Han Fan, Wei Jia, Wenqiang Zhang, Wenxuan Liu, Xiangzhong Wu, Xiaochen Zuo, Xiaoying Jia, Ximing Yang, Xin Liu, Xin Yu, Xingyan Bin, Xintong Hao, Xiongcai Luo, Xujing Li, Xun Zhou, Yanghua Peng, Yangrui Chen, Yi Lin, Yichong Leng, Yinghao Li, Yingshuan Song, Yiyuan Ma, Yong Shan, Yongan Xiang, Yonghui Wu, Yongtao Zhang, Yongzhen Yao, Yu Bao, Yuehang Yang, Yufeng Yuan, Yunshui Li, Yuqiao Xian, Yutao Zeng, Yuxuan Wang, Zehua Hong, Zehua Wang, Zengzhi Wang, Zeyu Yang, Zhengqiang Yin, Zhenyi Lu, Zhexi Zhang, Zhi Chen, Zhi Zhang, Zhiqi Lin, Zihao Huang, Zilin Xu, Ziyun Wei, Zuo Wang",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Virtual Width Networks",
      "images": [],
      "contentSource": "RSS",
      "content": "We introduce Virtual Width Networks (VWN), a framework that delivers the benefits of wider representations without incurring the quadratic cost of increasing the hidden size. VWN decouples representational width from backbone width, expanding the embedding space while keeping backbone compute nearly constant. In our large-scale experiment, an 8-times expansion accelerates optimization by over 2 times for next-token and 3 times for next-2-token prediction. The advantage amplifies over training as both the loss gap grows and the convergence-speedup ratio increases, showing that VWN is not only token-efficient but also increasingly effective with scale. Moreover, we identify an approximately log-linear scaling relation between virtual width and loss reduction, offering an initial empirical basis and motivation for exploring virtual-width scaling as a new dimension of large-model efficiency."
    },
    {
      "title": "CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios",
      "link": "https://arxiv.org/abs/2511.11168",
      "pubDate": "Fri, 14 Nov 2025 06:07:04 GMT",
      "isoDate": "2025-11-14T06:07:04.000Z",
      "creator": "Hangyu Li, Bofeng Cao, Zhaohui Liang, Wuzhen Li, Juyoung Oh, Yuxuan Chen, Shixiao Liang, Hang Zhou, Chengyuan Ma, Jiaxi Liu, Zheng Li, Peng Zhang, KeKe Long, Maolin Liu, Jackson Jiang, Chunlei Yu, Shengxiang Liu, Hongkai Yu, Xiaopeng Li",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios",
      "images": [],
      "contentSource": "RSS",
      "content": "Vehicle-to-Vehicle (V2V) cooperative perception has great potential to enhance autonomous driving performance by overcoming perception limitations in complex adverse traffic scenarios (CATS). Meanwhile, data serves as the fundamental infrastructure for modern autonomous driving AI. However, due to stringent data collection requirements, existing datasets focus primarily on ordinary traffic scenarios, constraining the benefits of cooperative perception. To address this challenge, we introduce CATS-V2V, the first-of-its-kind real-world dataset for V2V cooperative perception under complex adverse traffic scenarios. The dataset was collected by two hardware time-synchronized vehicles, covering 10 weather and lighting conditions across 10 diverse locations. The 100-clip dataset includes 60K frames of 10 Hz LiDAR point clouds and 1.26M multi-view 30 Hz camera images, along with 750K anonymized yet high-precision RTK-fixed GNSS and IMU records. Correspondingly, we provide time-consistent 3D bounding box annotations for objects, as well as static scenes to construct a 4D BEV representation. On this basis, we propose a target-based temporal alignment method, ensuring that all objects are precisely aligned across all sensor modalities. We hope that CATS-V2V, the largest-scale, most supportive, and highest-quality dataset of its kind to date, will benefit the autonomous driving community in related tasks."
    },
    {
      "title": "GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models",
      "link": "https://arxiv.org/abs/2511.11134",
      "pubDate": "Fri, 14 Nov 2025 05:07:53 GMT",
      "isoDate": "2025-11-14T05:07:53.000Z",
      "creator": "Jingxuan Wei, Caijun Jia, Xi Bai, Xinglong Xu, Siyuan Li, Linzhuang Sun, Bihui Yu, Conghui He, Lijun Wu, Cheng Tan",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models",
      "images": [],
      "contentSource": "RSS",
      "content": "The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/."
    },
    {
      "title": "LiteAttention: A Temporal Sparse Attention for Diffusion Transformers",
      "link": "https://arxiv.org/abs/2511.11062",
      "pubDate": "Fri, 14 Nov 2025 03:26:55 GMT",
      "isoDate": "2025-11-14T03:26:55.000Z",
      "creator": "Dor Shmilovich, Tony Wu, Aviad Dahan, Yuval Domb",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "LiteAttention: A Temporal Sparse Attention for Diffusion Transformers",
      "images": [],
      "contentSource": "RSS",
      "content": "Diffusion Transformers, particularly for video generation, achieve remarkable quality but suffer from quadratic attention complexity, leading to prohibitive latency. Existing acceleration methods face a fundamental trade-off: dynamically estimating sparse attention patterns at each denoising step incurs high computational overhead and estimation errors, while static sparsity patterns remain fixed and often suboptimal throughout denoising. We identify a key structural property of diffusion attention, namely, its sparsity patterns exhibit strong temporal coherence across denoising steps. Tiles deemed non-essential at step t typically remain so at step t+δ. Leveraging this observation, we introduce LiteAttention, a method that exploits temporal coherence to enable evolutionary computation skips across the denoising sequence. By marking non-essential tiles early and propagating skip decisions forward, LiteAttention eliminates redundant attention computations without repeated profiling overheads, combining the adaptivity of dynamic methods with the efficiency of static ones. We implement a highly optimized LiteAttention kernel on top of FlashAttention and demonstrate substantial speedups on production video diffusion models, with no degradation in quality. The code and implementation details will be publicly released."
    },
    {
      "title": "EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation",
      "link": "https://arxiv.org/abs/2511.11002",
      "pubDate": "Fri, 14 Nov 2025 01:44:21 GMT",
      "isoDate": "2025-11-14T01:44:21.000Z",
      "creator": "Zongyang Qiu, Bingyuan Wang, Xingbei Chen, Yingqing He, Zeyu Wang",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation",
      "images": [],
      "contentSource": "RSS",
      "content": "Emotion plays a pivotal role in video-based expression, but existing video generation systems predominantly focus on low-level visual metrics while neglecting affective dimensions. Although emotion analysis has made progress in the visual domain, the video community lacks dedicated resources to bridge emotion understanding with generative tasks, particularly for stylized and non-realistic contexts. To address this gap, we introduce EmoVid, the first multimodal, emotion-annotated video dataset specifically designed for creative media, which includes cartoon animations, movie clips, and animated stickers. Each video is annotated with emotion labels, visual attributes (brightness, colorfulness, hue), and text captions. Through systematic analysis, we uncover spatial and temporal patterns linking visual features to emotional perceptions across diverse video forms. Building on these insights, we develop an emotion-conditioned video generation technique by fine-tuning the Wan2.1 model. The results show a significant improvement in both quantitative metrics and the visual quality of generated videos for text-to-video and image-to-video tasks. EmoVid establishes a new benchmark for affective video computing. Our work not only offers valuable insights into visual emotion analysis in artistically styled videos, but also provides practical methods for enhancing emotional expression in video generation."
    },
    {
      "title": "DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains",
      "link": "https://arxiv.org/abs/2511.10984",
      "pubDate": "Fri, 14 Nov 2025 01:09:37 GMT",
      "isoDate": "2025-11-14T01:09:37.000Z",
      "creator": "Xiying Zhao, Zhoufutu Wen, Zhixuan Chen, Jingzhe Ding, Jianpeng Jiao, Shuai Li, Xi Li, Danni Liang, Shengda Long, Qianqian Liu, Xianbo Wu, Hongwan Gao, Xiang Gao, Liang Hu, Jiashuo Liu, Mengyun Liu, Weiran Shi, Chenghao Yang, Qianyu Yang, Xuanliang Zhang, Ge Zhang, Wenhao Huang",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains",
      "images": [],
      "contentSource": "RSS",
      "content": "The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation."
    },
    {
      "title": "Depth Anything 3: Recovering the Visual Space from Any Views",
      "link": "https://arxiv.org/abs/2511.10647",
      "pubDate": "Thu, 13 Nov 2025 13:59:53 GMT",
      "isoDate": "2025-11-13T13:59:53.000Z",
      "creator": "Haotong Lin, Sili Chen, Junhao Liew, Donny Y. Chen, Zhenyu Li, Guang Shi, Jiashi Feng, Bingyi Kang",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Depth Anything 3: Recovering the Visual Space from Any Views",
      "images": [],
      "contentSource": "RSS",
      "content": "We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets."
    },
    {
      "title": "大型语言模型的黑盒在线策略蒸馏 (原标题: Black-Box On-Policy Distillation of Large Language Models)",
      "link": "https://arxiv.org/abs/2511.10643",
      "pubDate": "Thu, 13 Nov 2025 13:58:37 GMT",
      "isoDate": "2025-11-13T13:58:37.000Z",
      "creator": "Tianzhu Ye, Li Dong, Zewen Chi, Xun Wu, Shaohan Huang, Furu Wei",
      "summary": "## 大型语言模型的黑盒在线策略蒸馏：生成对抗蒸馏 (GAD)\n\n### 引言\n黑盒蒸馏是一种创建学生大型语言模型（LLM）的方法，它仅通过学习专有教师模型的文本输出来实现，而无需访问教师模型的内部逻辑或参数。这种方法在保护教师模型知识产权的同时，能够训练出性能接近的轻量级学生模型。\n\n### 生成对抗蒸馏 (GAD) 方法\n本文提出了一种名为**生成对抗蒸馏（Generative Adversarial Distillation, GAD）**的新方法，旨在实现大型语言模型的在线策略和黑盒蒸馏。GAD 的核心思想是将蒸馏过程构建为一个最小最大（minimax）博弈：\n\n*   **学生LLM作为生成器**：学生LLM被视为一个生成器，其任务是生成与教师LLM输出相似的文本响应。\n*   **判别器**：训练一个判别器来区分学生LLM生成的响应和教师LLM生成的响应。\n*   **最小最大博弈**：学生LLM的目标是生成能够“欺骗”判别器的响应，使其无法区分；判别器的目标是准确地区分两者。\n\n### 判别器作为在线策略奖励模型\n在 GAD 框架中，判别器扮演着一个关键角色，它充当一个**在线策略奖励模型**。这意味着判别器会随着学生模型的训练而共同演化，并提供：\n\n*   **稳定反馈**：判别器能够提供持续且稳定的学习信号。\n*   **自适应反馈**：判别器会根据学生模型的当前表现动态调整其反馈，从而引导学生模型更有效地学习。\n\n### 实验结果与性能\n实验结果表明，GAD 在黑盒LLM蒸馏方面表现出卓越的性能：\n\n*   **超越传统方法**：GAD 始终优于常用的序列级知识蒸馏方法，这表明其在学习效率和效果上的优势。\n*   **性能媲美教师模型**：具体而言，使用 GAD 训练的 Qwen2.5-14B-Instruct（学生模型）在 LMSYS-Chat 自动评估中，其性能达到了与教师模型 GPT-5-Chat 相媲美的水平。\n\n### 结论\n这些实验结果有力地确立了 GAD 作为一种有前景且有效的黑盒LLM蒸馏范式。它为在不访问教师模型内部参数的情况下，高效训练出高性能学生模型提供了一条新途径。",
      "shortSummary": "本文提出生成对抗蒸馏（GAD），一种实现大型语言模型黑盒在线策略蒸馏的新方法。GAD将学生模型作为生成器，训练判别器区分其输出与教师模型，形成最小最大博弈。判别器充当在线策略奖励模型，提供稳定自适应反馈。实验证明，GAD优于传统序列级知识蒸馏，使学生模型（如Qwen2.5-14B-Instruct）在自动评估中性能可与教师模型（如GPT-5-Chat）媲美，为黑盒LLM蒸馏提供有效范式。",
      "translated_title": "大型语言模型的黑盒在线策略蒸馏",
      "images": [],
      "contentSource": "完整文章",
      "content": "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation."
    },
    {
      "title": "潜在空间一小步，像素生成一大步：扩散模型的快速潜在上采样适配器 (原标题: One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models)",
      "link": "https://arxiv.org/abs/2511.10629",
      "pubDate": "Thu, 13 Nov 2025 13:54:18 GMT",
      "isoDate": "2025-11-13T13:54:18.000Z",
      "creator": "Aleksandr Razin, Danil Kazantsev, Ilya Makarov",
      "summary": "# 潜在上采样适配器 (LUA)：扩散模型的高效高分辨率生成\n\n## 引言与问题\n\n扩散模型在生成超出其训练分辨率的图像时面临显著挑战：\n*   **高分辨率采样缓慢且成本高昂**：直接生成高分辨率图像需要大量的计算资源和时间。\n*   **后处理图像超分辨率 (ISR) 的局限性**：在图像解码后进行超分辨率处理会引入伪影，并增加额外的延迟。\n\n## LUA 解决方案\n\n文章提出了**潜在上采样适配器 (Latent Upscaler Adapter, LUA)**，这是一个旨在解决上述问题而设计的轻量级模块。\n\n### LUA 的工作原理\n*   **直接在潜在代码上执行超分辨率**：LUA 在最终的 VAE 解码步骤之前，直接对生成器的潜在代码进行超分辨率处理。\n*   **单次前向传播**：通过在潜在空间中进行一次前向传播，LUA 即可实现高分辨率合成。\n\n### LUA 的关键特性与优势\n*   **即插即用组件**：LUA 可以作为一个独立的模块集成到现有扩散模型中，无需对基础模型进行任何修改，也无需添加额外的扩散阶段。\n*   **共享的 Swin 风格骨干网络**：LUA 采用了一个共享的 Swin 风格骨干网络，并配备了尺度特定的像素混洗头（pixel-shuffle heads）。\n*   **支持多种放大因子**：该模块支持 2 倍和 4 倍的放大因子。\n*   **兼容性**：LUA 与图像空间超分辨率基线保持兼容。\n\n## 性能表现\n\nLUA 在效率和质量方面展现出卓越的性能：\n*   **感知质量**：LUA 能够达到与图像空间超分辨率基线相当的感知质量。\n*   **显著的效率提升**：\n    *   解码和上采样时间减少了近 3 倍。\n    *   例如，从 512 像素生成 1024 像素图像时，LUA 仅增加 +0.42 秒，而使用相同 SwinIR 架构的像素空间超分辨率方法则需要 1.87 秒。\n\n## 泛化能力\n\n*   **跨 VAE 潜在空间的强大泛化**：LUA 在不同 VAE 的潜在空间中表现出强大的泛化能力，这意味着它无需为每个新的解码器从头开始重新训练，从而简化了部署过程。\n\n## 结论\n\nLUA 提供了一条实用且高效的途径，以实现现代扩散管道中可扩展、高保真图像合成。它在保真度上与原生高分辨率生成非常接近，同时显著提高了效率和部署灵活性。",
      "shortSummary": "潜在上采样适配器 (LUA) 解决了扩散模型高分辨率生成效率低、成本高及后处理超分辨率引入伪影的问题。LUA 是一个轻量级、即插即用模块，在 VAE 解码前直接对潜在代码进行超分辨率处理。它显著提高了效率（解码和上采样时间减少近 3 倍），同时保持了与图像空间超分辨率相当的感知质量。LUA 具有强大的泛化能力，易于部署，为扩散模型提供了高效、可扩展的高保真图像合成方案。",
      "translated_title": "潜在空间一小步，像素生成一大步：扩散模型的快速潜在上采样适配器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines."
    },
    {
      "title": "通过属性条件人工评估对图像生成多样性进行基准测试 (原标题: Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation)",
      "link": "https://arxiv.org/abs/2511.10547",
      "pubDate": "Thu, 13 Nov 2025 12:48:38 GMT",
      "isoDate": "2025-11-13T12:48:38.000Z",
      "creator": "Isabela Albuquerque, Ira Ktena, Olivia Wiles, Ivana Kajić, Amal Rannen-Triki, Cristina Vasconcelos, Aida Nematzadeh",
      "summary": "## 图像生成多样性基准测试框架\n\n### 引言\n尽管文本到图像（T2I）模型的生成质量取得了显著进步，但当前模型常常缺乏多样性，倾向于生成同质化的输出。本文旨在解决T2I模型中对鲁棒多样性评估的需求，并为此引入了一个新的框架。\n\n### 框架概述\n该框架通过系统地评估个体概念及其相关的变异因素来评估多样性。例如，对于提示“一张苹果的图片”，其变异因素可以是“颜色”。\n\n### 主要贡献\n本研究的核心贡献包括：\n\n1.  **新颖的人工评估模板**：设计了一个创新的人工评估模板，用于对生成图像的多样性进行细致入微的评估。\n2.  **精选的提示集**：构建了一个精心策划的提示集，涵盖了多样化的概念及其已识别的变异因素。这有助于系统地探索模型在不同属性上的多样性表现。\n3.  **模型比较方法**：提出了一种通过二项式检验，根据人工标注来比较不同T2I模型多样性表现的方法。\n\n### 附加研究与成果\n*   **图像嵌入比较**：研究还严格比较了多种图像嵌入在多样性测量方面的表现。\n*   **模型排名与问题识别**：本文提出的原则性方法能够根据多样性对T2I模型进行排名，并识别出模型在哪些特定类别或属性上表现不足。\n\n### 结论与展望\n这项研究提供了一套稳健的方法和深刻的见解，为未来T2I模型多样性的改进以及相关评估指标的开发铺平了道路。",
      "shortSummary": "本文提出一个框架，旨在解决文本到图像（T2I）模型生成输出缺乏多样性的问题。该框架通过评估个体概念及其变异因素，系统地评估多样性。主要贡献包括一个新颖的人工评估模板、一个包含变异因素的提示集以及通过二项式检验比较模型的方法。该研究能够对T2I模型的多样性进行排名，并识别其不足之处，为T2I模型多样性的改进提供了稳健的方法和见解。",
      "translated_title": "通过属性条件人工评估对图像生成多样性进行基准测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite advances in generation quality, current text-to-image (T2I) models often lack diversity, generating homogeneous outputs. This work introduces a framework to address the need for robust diversity evaluation in T2I models. Our framework systematically assesses diversity by evaluating individual concepts and their relevant factors of variation. Key contributions include: (1) a novel human evaluation template for nuanced diversity assessment; (2) a curated prompt set covering diverse concepts with their identified factors of variation (e.g. prompt: An image of an apple, factor of variation: color); and (3) a methodology for comparing models in terms of human annotations via binomial tests.   Furthermore, we rigorously compare various image embeddings for diversity measurement. Notably, our principled approach enables ranking of T2I models by diversity, identifying categories where they particularly struggle. This research offers a robust methodology and insights, paving the way for improvements in T2I model diversity and metric development."
    },
    {
      "title": "基于评分标准的基准测试与强化学习，以提升大语言模型指令遵循能力 (原标题: Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following)",
      "link": "https://arxiv.org/abs/2511.10507",
      "pubDate": "Thu, 13 Nov 2025 12:14:01 GMT",
      "isoDate": "2025-11-13T12:14:01.000Z",
      "creator": "Yun He, Wenzhe Li, Hejia Zhang, Songlin Li, Karishma Mandyam, Sopan Khosla, Yuanhao Xiong, Nanshu Wang, Selina Peng, Beibin Li, Shengjie Bi, Shishir G. Patil, Qi Qi, Shengyu Feng, Julian Katz-Samuels, Richard Yuanzhe Pang, Sujan Gonugondla, Hunter Lang, Yue Yu, Yundi Qian, Maryam Fazel-Zarandi, Licheng Yu, Amine Benhalloum, Hany Awadalla, Manaal Faruqui",
      "summary": "本文介绍了在提升大语言模型（LLMs）指令遵循（IF）能力方面的最新研究，特别关注解决复杂、多轮和系统提示指令的挑战。研究人员指出，当前缺乏高质量、人工标注的基准和可靠、可解释的奖励信号，阻碍了对LLM高级指令遵循能力的严格评估和有效训练。\n\n**主要贡献与方法：**\n\n*   **AdvancedIF 基准测试：**\n    *   引入了一个全面的基准测试，包含超过1,600个提示。\n    *   该基准测试采用专家精心策划的评分标准（rubrics），旨在评估LLM遵循复杂、多轮和系统级指令的能力。\n    *   AdvancedIF 基准测试即将发布，将为研究社区提供宝贵的资源。\n\n*   **RIFL (Rubric-based Instruction-Following Learning) 管道：**\n    *   提出了一种新颖的后训练（post-training）流程，名为RIFL。\n    *   RIFL 利用评分标准生成、一个经过微调的评分标准验证器以及奖励塑形（reward shaping）技术，以实现指令遵循的有效强化学习。\n\n**实验结果与意义：**\n\n*   **显著性能提升：** 广泛的实验证明，RIFL 显著提升了LLM的指令遵循能力，在AdvancedIF 基准测试上实现了6.7%的绝对增益。\n*   **公共基准表现：** RIFL 在公共基准测试上也取得了优异的结果。\n*   **组件有效性：** 消融研究（ablation studies）证实了RIFL中每个组件的有效性。\n*   **未来展望：** 这项工作确立了评分标准作为训练和评估LLM高级指令遵循能力的强大工具，为开发更强大、更可靠的AI系统铺平了道路。",
      "shortSummary": "该研究针对大语言模型（LLMs）在复杂指令遵循方面的挑战，引入了AdvancedIF基准测试和RIFL（基于评分标准的指令遵循学习）后训练流程。AdvancedIF包含1600多个提示和专家评分标准。RIFL利用评分标准生成、验证器和奖励塑形进行强化学习。实验表明，RIFL显著提升了LLM的指令遵循能力，在AdvancedIF上实现了6.7%的绝对增益，并在公共基准上表现出色。该工作强调了评分标准在提升LLM指令遵循能力中的关键作用。",
      "translated_title": "基于评分标准的基准测试与强化学习，以提升大语言模型指令遵循能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent progress in large language models (LLMs) has led to impressive performance on a range of tasks, yet advanced instruction following (IF)-especially for complex, multi-turn, and system-prompted instructions-remains a significant challenge. Rigorous evaluation and effective training for such capabilities are hindered by the lack of high-quality, human-annotated benchmarks and reliable, interpretable reward signals. In this work, we introduce AdvancedIF (we will release this benchmark soon), a comprehensive benchmark featuring over 1,600 prompts and expert-curated rubrics that assess LLMs ability to follow complex, multi-turn, and system-level instructions. We further propose RIFL (Rubric-based Instruction-Following Learning), a novel post-training pipeline that leverages rubric generation, a finetuned rubric verifier, and reward shaping to enable effective reinforcement learning for instruction following. Extensive experiments demonstrate that RIFL substantially improves the instruction-following abilities of LLMs, achieving a 6.7% absolute gain on AdvancedIF and strong results on public benchmarks. Our ablation studies confirm the effectiveness of each component in RIFL. This work establishes rubrics as a powerful tool for both training and evaluating advanced IF in LLMs, paving the way for more capable and reliable AI systems."
    },
    {
      "title": "音乐火烈鸟：在音频语言模型中扩展音乐理解 (原标题: Music Flamingo: Scaling Music Understanding in Audio Language Models)",
      "link": "https://arxiv.org/abs/2511.10289",
      "pubDate": "Thu, 13 Nov 2025 08:21:09 GMT",
      "isoDate": "2025-11-13T08:21:09.000Z",
      "creator": "Sreyan Ghosh, Arushi Goel, Lasha Koroshinadze, Sang-gil Lee, Zhifeng Kong, Joao Felipe Santos, Ramani Duraiswami, Dinesh Manocha, Wei Ping, Mohammad Shoeybi, Bryan Catanzaro",
      "summary": "# 音乐火烈鸟：在音频语言模型中扩展音乐理解\n\n本文介绍了“音乐火烈鸟”（Music Flamingo），这是一种新型的大型音频语言模型，旨在提升基础音频模型对音乐（包括歌曲）的理解能力。\n\n## 背景与挑战\n尽管音频语言研究取得了快速进展，但音乐理解仍面临巨大挑战。这主要归因于音乐的动态性、层次性、信息密集性，以及高质量音乐数据和标注的稀缺性，这些因素限制了开放音频理解模型的扩展。现有模型通常只能生成简短、高层次的描述，回答表面问题，并且在不同音乐文化间的泛化能力有限。\n\n## 核心创新与方法\n为了解决上述挑战，Music Flamingo项目引入了多项创新：\n\n### 1. MF-Skills 大规模数据集\n*   **目的：** 应对高质量音乐数据和标注稀缺的问题。\n*   **构建：** 通过多阶段管道精心策划，生成了丰富详细的描述（captions）和问答对（Q&A pairs）。\n*   **内容覆盖：** 涵盖了和声、结构、音色、歌词和文化背景等多个方面，提供了深层次的音乐信息。\n\n### 2. 增强型 Audio Flamingo 3 主干模型\n*   Music Flamingo 基于增强的 Audio Flamingo 3 主干模型进行微调，并进一步强化了多项与音乐理解相关的技能。\n\n### 3. 提升推理能力的后训练方案\n为了增强模型的推理能力，研究团队引入了一种独特的后训练方案：\n*   **MF-Think 冷启动：** 首先使用 MF-Think 进行冷启动，这是一个新颖的思维链（chain-of-thought）数据集，其内容植根于音乐理论。\n*   **GRPO 强化学习：** 随后，通过基于 GRPO（Generalized Policy Optimization）的强化学习，并结合自定义奖励机制，进一步优化模型。\n\n## 成果与意义\n*   **卓越性能：** Music Flamingo 在超过10个音乐理解和推理基准测试中取得了最先进（state-of-the-art）的结果。\n*   **通用智能：** 它确立了自身作为一个通用且具有音乐智能的音频语言模型的地位。\n*   **新标准：** 该模型展示了如何将音乐理解从表面识别提升到对歌曲进行分层、类人感知的水平，为高级音乐理解设定了新标准。\n*   **社区贡献：** 这项工作为社区构建下一代能够像人类一样有意义地理解音乐的模型提供了基准和基础。",
      "shortSummary": "Music Flamingo 是一种新型大型音频语言模型，旨在解决现有模型在音乐理解方面的局限性。它通过构建大规模的MF-Skills数据集，并引入基于音乐理论的MF-Think思维链数据集和GRPO强化学习后训练方案，显著提升了模型对音乐和声、结构、音色、歌词及文化背景的深层理解与推理能力。Music Flamingo在多项基准测试中取得了最先进成果，实现了类人般的音乐感知，为未来音频语言模型发展奠定了基础。",
      "translated_title": "音乐火烈鸟：在音频语言模型中扩展音乐理解",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Music Flamingo, a novel large audio-language model designed to advance music (including song) understanding in foundational audio models. While audio-language research has progressed rapidly, music remains challenging due to its dynamic, layered, and information-dense nature. Progress has been further limited by the difficulty of scaling open audio understanding models, primarily because of the scarcity of high-quality music data and annotations. As a result, prior models are restricted to producing short, high-level captions, answering only surface-level questions, and showing limited generalization across diverse musical cultures. To address these challenges, we curate MF-Skills, a large-scale dataset labeled through a multi-stage pipeline that yields rich captions and question-answer pairs covering harmony, structure, timbre, lyrics, and cultural context. We fine-tune an enhanced Audio Flamingo 3 backbone on MF-Skills and further strengthen multiple skills relevant to music understanding. To improve the model's reasoning abilities, we introduce a post-training recipe: we first cold-start with MF-Think, a novel chain-of-thought dataset grounded in music theory, followed by GRPO-based reinforcement learning with custom rewards. Music Flamingo achieves state-of-the-art results across 10+ benchmarks for music understanding and reasoning, establishing itself as a generalist and musically intelligent audio-language model. Beyond strong empirical results, Music Flamingo sets a new standard for advanced music understanding by demonstrating how models can move from surface-level recognition toward layered, human-like perception of songs. We believe this work provides both a benchmark and a foundation for the community to build the next generation of models that engage with music as meaningfully as humans do."
    },
    {
      "title": "MuSc-V2：基于无标签样本互评分的零样本多模态工业异常分类与分割 (原标题: MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples)",
      "link": "https://arxiv.org/abs/2511.10047",
      "pubDate": "Thu, 13 Nov 2025 02:47:37 GMT",
      "isoDate": "2025-11-13T02:47:37.000Z",
      "creator": "Xurui Li, Feng Xue, Yu Zhou",
      "summary": "## MuSc-V2：零样本多模态工业异常分类与分割\n\n### 背景与挑战\n\n*   零样本异常分类（AC）和分割（AS）方法旨在无需任何标记样本的情况下识别和描绘缺陷。\n*   现有方法普遍忽略了一个关键特性：工业产品中的正常图像块在2D外观和3D形状上通常能找到许多其他相似的块，而异常则表现出多样性和孤立性。\n\n### MuSc-V2 框架概述\n\n*   为了明确利用上述判别性特性，本文提出了一个名为 **MuSc-V2（互评分框架）** 的零样本AC/AS框架。\n*   MuSc-V2 灵活支持单一2D/3D或多模态数据处理。\n\n### 核心组件与机制\n\n1.  **迭代点分组（Iterative Point Grouping, IPG）**\n    *   首先通过改进3D表示，有效减少由不连续表面引起的假阳性。\n2.  **多度相似性邻域聚合（Similarity Neighborhood Aggregation with Multi-Degrees, SNAMD）**\n    *   融合2D/3D邻域线索，生成更具判别性的多尺度图像块特征，为后续的互评分奠定基础。\n3.  **互评分机制（Mutual Scoring Mechanism, MSM）**\n    *   这是框架的核心，允许每种模态内的样本相互分配分数，以识别异常。\n4.  **跨模态异常增强（Cross-modal Anomaly Enhancement, CAE）**\n    *   融合2D和3D模态的分数，以恢复特定模态中可能缺失的异常信息，提高检测的完整性。\n5.  **约束邻域重评分（Re-scoring with Constrained Neighborhood, RsCon）**\n    *   基于与更具代表性样本的相似性，对分类结果进行重评分，从而有效抑制假分类（假阳性）。\n\n### 性能与优势\n\n*   该框架在完整数据集和较小的数据子集上均能灵活工作，并保持一致的鲁棒性能，确保了在不同产品线上的无缝适应性。\n*   MuSc-V2 取得了显著的性能提升：\n    *   在 **MVTec 3D-AD** 数据集上，AP（平均精度）提升了 **+23.7%**。\n    *   在 **Eyecandies** 数据集上，性能提升了 **+19.3%**。\n*   这些结果超越了先前的零样本基准，甚至优于大多数少样本方法。\n\n### 代码可用性\n\n*   相关代码将公开发布。",
      "shortSummary": "MuSc-V2提出了一种零样本多模态工业异常分类与分割框架，利用正常图像块相似而异常孤立的关键特性。该框架通过迭代点分组（IPG）改进3D表示，使用多度相似性邻域聚合（SNAMD）融合多尺度特征，并通过互评分机制（MSM）和跨模态异常增强（CAE）进行异常评分。最终，约束邻域重评分（RsCon）抑制假阳性。MuSc-V2在MVTec 3D-AD和Eyecandies数据集上分别实现了+23.7%和+19.3%的AP提升，超越了现有零样本甚至多数少样本方法。",
      "translated_title": "MuSc-V2：基于无标签样本互评分的零样本多模态工业异常分类与分割",
      "images": [],
      "contentSource": "完整文章",
      "content": "Zero-shot anomaly classification (AC) and segmentation (AS) methods aim to identify and outline defects without using any labeled samples. In this paper, we reveal a key property that is overlooked by existing methods: normal image patches across industrial products typically find many other similar patches, not only in 2D appearance but also in 3D shapes, while anomalies remain diverse and isolated. To explicitly leverage this discriminative property, we propose a Mutual Scoring framework (MuSc-V2) for zero-shot AC/AS, which flexibly supports single 2D/3D or multimodality. Specifically, our method begins by improving 3D representation through Iterative Point Grouping (IPG), which reduces false positives from discontinuous surfaces. Then we use Similarity Neighborhood Aggregation with Multi-Degrees (SNAMD) to fuse 2D/3D neighborhood cues into more discriminative multi-scale patch features for mutual scoring. The core comprises a Mutual Scoring Mechanism (MSM) that lets samples within each modality to assign score to each other, and Cross-modal Anomaly Enhancement (CAE) that fuses 2D and 3D scores to recover modality-specific missing anomalies. Finally, Re-scoring with Constrained Neighborhood (RsCon) suppresses false classification based on similarity to more representative samples. Our framework flexibly works on both the full dataset and smaller subsets with consistently robust performance, ensuring seamless adaptability across diverse product lines. In aid of the novel framework, MuSc-V2 achieves significant performance improvements: a +23.7% AP gain on the MVTec 3D-AD dataset and a +19.3% boost on the Eyecandies dataset, surpassing previous zero-shot benchmarks and even outperforming most few-shot methods. The code will be available at The code will be available at https://github.com/HUST-SLOW/MuSc-V2{https://github.com/HUST-SLOW/MuSc-V2}."
    },
    {
      "title": "AffordBot：通过多模态大型语言模型实现3D细粒度具身推理 (原标题: AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models)",
      "link": "https://arxiv.org/abs/2511.10017",
      "pubDate": "Thu, 13 Nov 2025 01:43:00 GMT",
      "isoDate": "2025-11-13T01:43:00.000Z",
      "creator": "Xinyi Wang, Xun Yang, Yanlong Xu, Yuchen Wu, Zhen Li, Na Zhao",
      "summary": "# AffordBot：通过多模态大型语言模型实现3D细粒度具身推理\n\n本文介绍了一种名为AffordBot的新型框架，旨在解决物理环境中人机协作中的细粒度3D具身推理问题。该框架利用多模态大型语言模型（MLLMs）和定制的思维链（CoT）推理范式，以实现对3D场景中可操作元素的深入理解和交互预测。\n\n## 1. 研究背景与问题\n\n*   **挑战：** 在物理环境中，有效的人机协作不仅需要理解“作用于什么”，还需要知道“可操作元素在哪里”以及“如何与它们互动”。这要求智能体具备对环境的细致理解和推理能力。\n*   **现有方法局限：** 当前的方法通常在对象层面操作，或将细粒度可供性（affordance）推理割裂处理。它们缺乏连贯的、指令驱动的接地（grounding）能力，也无法进行细粒度的推理，导致在复杂交互场景中表现不足。\n\n## 2. 新任务：细粒度3D具身推理\n\n*   **定义：** 本文提出了一项新任务——细粒度3D具身推理。该任务要求智能体根据给定的任务指令，为3D场景中每个被引用的可供性元素预测一个结构化三元组。\n*   **三元组构成：** 这个三元组包含以下三个关键信息：\n    *   **空间位置（spatial location）：** 元素在3D空间中的精确位置。\n    *   **运动类型（motion type）：** 与该元素相关的可能交互运动类型（例如，推、拉、旋转）。\n    *   **运动轴（motion axis）：** 如果存在运动，其发生的具体轴向。\n\n## 3. 解决方案：AffordBot 框架\n\n*   **核心：** AffordBot是一个新颖的框架，它通过将多模态大型语言模型（MLLMs）与量身定制的思维链（CoT）推理范式相结合来解决上述任务。\n*   **创新点：** 该框架旨在弥合3D感知与2D兼容的MLLMs之间的差距，并利用CoT的逐步推理能力来提高预测的准确性和物理合理性。\n\n## 4. AffordBot 的方法论\n\n*   **3D到2D的桥接：**\n    *   为了使2D兼容的MLLMs能够处理3D场景信息，AffordBot首先渲染场景的环绕视图图像。\n    *   然后，将3D场景中的元素候选对象投影到这些2D视图中，从而形成与场景几何结构对齐的丰富视觉表示。这种方法有效地将3D空间信息编码到MLLMs可以理解的2D图像格式中。\n*   **思维链（CoT）推理流程：** AffordBot的CoT管道包含两个主要阶段：\n    *   **主动感知阶段：** 在推理开始时，系统会提示MLLM根据任务指令选择最具信息量的视点。这有助于聚焦于与任务最相关的场景区域。\n    *   **逐步推理：** 选定视点后，MLLM会进行一步步的推理，首先定位可供性元素，然后推断出与这些元素相关的合理交互动作及其运动参数。\n\n## 5. 实验评估与成果\n\n*   **数据集：** AffordBot在SceneFun3D数据集上进行了评估。\n*   **性能：** 实验结果表明，AffordBot取得了最先进（state-of-the-art）的性能。\n*   **关键优势：**\n    *   展示了强大的泛化能力，能够适应不同的场景和任务。\n    *   实现了物理上接地（physically grounded）的推理，即预测的交互动作符合物理世界的规律。\n    *   值得注意的是，AffordBot仅使用3D点云输入和MLLMs就达到了这些成果，证明了其方法的有效性和效率。\n\n## 6. 其他信息\n\n*   **发布：** 本研究成果已被NeurIPS 2025接收。\n*   **预印本：** 论文可在arXiv上查阅，编号为arXiv:2511.10017 [cs.CV]。",
      "shortSummary": "AffordBot提出了一种利用多模态大型语言模型（MLLMs）和思维链（CoT）推理范式，解决3D细粒度具身推理的新框架。该框架旨在提升人机协作中对物理环境的理解，通过将3D场景渲染为2D图像并投影3D元素，使MLLMs能够预测可供性元素的空间位置、运动类型和运动轴。AffordBot在SceneFun3D数据集上取得了最先进的性能，展现出强大的泛化能力和物理接地推理，仅需3D点云输入和MLLMs。",
      "translated_title": "AffordBot：通过多模态大型语言模型实现3D细粒度具身推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Effective human-agent collaboration in physical environments requires understanding not only what to act upon, but also where the actionable elements are and how to interact with them. Existing approaches often operate at the object level or disjointedly handle fine-grained affordance reasoning, lacking coherent, instruction-driven grounding and reasoning. In this work, we introduce a new task: Fine-grained 3D Embodied Reasoning, which requires an agent to predict, for each referenced affordance element in a 3D scene, a structured triplet comprising its spatial location, motion type, and motion axis, based on a task instruction. To solve this task, we propose AffordBot, a novel framework that integrates Multimodal Large Language Models (MLLMs) with a tailored chain-of-thought (CoT) reasoning paradigm. To bridge the gap between 3D input and 2D-compatible MLLMs, we render surround-view images of the scene and project 3D element candidates into these views, forming a rich visual representation aligned with the scene geometry. Our CoT pipeline begins with an active perception stage, prompting the MLLM to select the most informative viewpoint based on the instruction, before proceeding with step-by-step reasoning to localize affordance elements and infer plausible interaction motions. Evaluated on the SceneFun3D dataset, AffordBot achieves state-of-the-art performance, demonstrating strong generalization and physically grounded reasoning with only 3D point cloud input and MLLMs."
    },
    {
      "title": "HI-TransPA: Hearing Impairments Translation Personal Assistant",
      "link": "https://arxiv.org/abs/2511.09915",
      "pubDate": "Wed, 12 Nov 2025 22:27:39 GMT",
      "isoDate": "2025-11-12T22:27:39.000Z",
      "creator": "Zhiming Ma, Shiyu Gan, Junhao Zhao, Xianming Li, Qingyun Pan, Peidong Wang, Mingjun Pan, Yuhao Mo, Jiajie Cheng, Chengxin Chen, Zhonglun Cao, Chonghan Liu, Shi Cheng",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "HI-TransPA: Hearing Impairments Translation Personal Assistant",
      "images": [],
      "contentSource": "RSS",
      "content": "To provide a unified and flexible solution for daily communication among hearing-impaired individuals, we introduce the Omni-Model paradigm into assistive technology and present HI-TransPA, an instruction-driven audio-visual personal assistant. The model fuses indistinct speech with high-frame-rate lip dynamics, enabling both translation and dialogue within a single multimodal framework. To tackle the challenges of noisy and heterogeneous raw data and the limited adaptability of existing Omni-Models to hearing-impaired speech, we construct a comprehensive preprocessing and curation pipeline that detects facial landmarks, isolates and stabilizes the lip region, and quantitatively assesses multimodal sample quality. These quality scores guide a curriculum learning strategy that first trains on clean, high-confidence samples and progressively incorporates harder cases to strengthen model robustness. We further adopt a SigLIP encoder combined with a Unified 3D-Resampler to efficiently encode high-frame-rate lip motion. Experiments on our purpose-built HI-Dialogue dataset show that HI-TransPA achieves state-of-the-art performance in both literal accuracy and semantic fidelity. This work establishes a foundation for applying Omni-Models to assistive communication technology, providing an end-to-end modeling framework and essential processing tools for future research."
    },
    {
      "title": "向窃贼致敬：探索去中心化GRPO中的攻击与防御 (原标题: Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO)",
      "link": "https://arxiv.org/abs/2511.09780",
      "pubDate": "Wed, 12 Nov 2025 17:29:07 GMT",
      "isoDate": "2025-11-12T17:29:07.000Z",
      "creator": "Nikolay Blagoev, Oğuzhan Ersoy, Lydia Yiyu Chen",
      "summary": "## 去中心化GRPO中的攻击与防御\n\n### 引言\n\n群组相对策略优化（Group Relative Policy Optimization, GRPO）在大型语言模型（LLM）的后训练中展现出巨大的应用潜力。在GRPO框架下，模型对提示（prompts）进行回答，并通过强化学习机制学习并采纳更受偏好的完成方式。由于其通信量小，GRPO天然适用于去中心化训练环境，允许多个节点并发地回答提示，并以字符串形式交换信息。\n\n### 首次对抗性攻击\n\n本研究首次提出了针对去中心化GRPO系统的对抗性攻击。研究表明，恶意方能够通过在良性模型中注入任意恶意令牌来毒害此类系统。这些攻击可以分为两种主要形式：\n\n*   **上下文外攻击（Out-of-context attacks）**：在与当前任务无关的上下文中注入恶意内容。\n*   **上下文内攻击（In-context attacks）**：在与当前任务相关的上下文中注入恶意内容。\n\n### 攻击效果与实证\n\n通过对数学和编码任务的实证测试，研究者展示了对抗性攻击能够轻易地毒害去中心化GRPO中的良性节点，从而污染其本地LLM的后训练过程。攻击效果显著，在短短50次迭代中，攻击成功率即可高达100%。\n\n### 防御机制\n\n为应对这些攻击，文章提出了两种防御策略。这些防御方法的选择取决于具体的训练场景，即所有用户是训练相同的模型还是不同的模型。研究结果表明，这些防御措施能够实现高达100%的阻止率，从而有效地使此类攻击变得不可能。",
      "shortSummary": "本文首次揭示了针对去中心化群组相对策略优化（GRPO）的对抗性攻击。GRPO在大型语言模型后训练中应用广泛，其去中心化特性使其易受恶意方通过注入恶意令牌进行毒害。研究通过数学和编码任务证实，攻击成功率可达100%。为应对此威胁，文章提出了两种防御策略，可有效阻止攻击，阻止率最高达100%，从而使攻击失效。",
      "translated_title": "向窃贼致敬：探索去中心化GRPO中的攻击与防御",
      "images": [],
      "contentSource": "完整文章",
      "content": "Group Relative Policy Optimization (GRPO) has demonstrated great utilization in post-training of Large Language Models (LLMs). In GRPO, prompts are answered by the model and, through reinforcement learning, preferred completions are learnt. Owing to the small communication volume, GRPO is inherently suitable for decentralised training as the prompts can be concurrently answered by multiple nodes and then exchanged in the forms of strings. In this work, we present the first adversarial attack in decentralised GRPO. We demonstrate that malicious parties can poison such systems by injecting arbitrary malicious tokens in benign models in both out-of-context and in-context attacks. Using empirical examples of math and coding tasks, we show that adversarial attacks can easily poison the benign nodes, polluting their local LLM post-training, achieving attack success rates up to 100% in as few as 50 iterations. We propose two ways to defend against these attacks, depending on whether all users train the same model or different models. We show that these defenses can achieve stop rates of up to 100%, making the attack impossible."
    },
    {
      "title": "SliderEdit：基于细粒度指令控制的连续图像编辑 (原标题: SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control)",
      "link": "https://arxiv.org/abs/2511.09715",
      "pubDate": "Wed, 12 Nov 2025 15:21:37 GMT",
      "isoDate": "2025-11-12T15:21:37.000Z",
      "creator": "Arman Zarei, Samyadeep Basu, Mobina Pournemat, Sayan Nag, Ryan Rossi, Soheil Feizi",
      "summary": "### SliderEdit：基于细粒度指令控制的连续图像编辑\n\n本文介绍了SliderEdit，一个用于实现细粒度、可解释指令控制的连续图像编辑框架。\n\n*   **现有问题：**\n    *   当前基于指令的图像编辑模型虽然在处理复杂编辑方面表现出色，但它们以固定强度应用提示中的每条指令。\n    *   这限制了用户精确且连续地控制单个编辑强度的能力。\n\n*   **SliderEdit框架介绍：**\n    *   SliderEdit旨在解决上述问题，提供一种连续图像编辑方法，具有细粒度、可解释的指令控制。\n    *   给定一个多部分编辑指令，SliderEdit能够解耦（disentangle）各个独立指令。\n    *   然后，它将每条指令作为一个全局训练的滑块（slider）暴露出来，允许用户平滑地调整其强度。\n\n*   **核心创新与优势：**\n    *   与以往在文本到图像生成中引入基于滑块的属性控制方法不同（这些方法通常需要为每个属性或概念进行单独训练或微调）。\n    *   SliderEdit学习了一组单一的低秩适应矩阵（low-rank adaptation matrices），这些矩阵能够泛化到各种不同的编辑、属性和组合指令。\n    *   这使得沿单个编辑维度进行连续插值成为可能，同时保持空间局部性和全局语义一致性。\n\n*   **应用与成果：**\n    *   SliderEdit已应用于最先进的图像编辑模型，包括FLUX-Kontext和Qwen-Image-Edit。\n    *   实验结果表明，在编辑可控性、视觉一致性和用户可引导性方面均有显著改进。\n\n*   **研究意义：**\n    *   据作者所知，SliderEdit是首个探索并提出在基于指令的图像编辑模型中实现连续、细粒度指令控制的框架。\n    *   其研究成果为实现具有连续和组合控制的交互式、指令驱动图像操作铺平了道路。",
      "shortSummary": "SliderEdit是一个创新的图像编辑框架，解决了现有指令编辑模型缺乏细粒度控制的问题。它能将多部分指令解耦为可调节强度的滑块，并学习一组泛化性强的低秩适应矩阵，实现对各种编辑的连续、精确控制。应用于FLUX-Kontext和Qwen-Image-Edit等模型后，显著提升了编辑可控性、视觉一致性和用户引导性，为交互式图像操作开辟了新途径。",
      "translated_title": "SliderEdit：基于细粒度指令控制的连续图像编辑",
      "images": [],
      "contentSource": "完整文章",
      "content": "Instruction-based image editing models have recently achieved impressive performance, enabling complex edits to an input image from a multi-instruction prompt. However, these models apply each instruction in the prompt with a fixed strength, limiting the user's ability to precisely and continuously control the intensity of individual edits. We introduce SliderEdit, a framework for continuous image editing with fine-grained, interpretable instruction control. Given a multi-part edit instruction, SliderEdit disentangles the individual instructions and exposes each as a globally trained slider, allowing smooth adjustment of its strength. Unlike prior works that introduced slider-based attribute controls in text-to-image generation, typically requiring separate training or fine-tuning for each attribute or concept, our method learns a single set of low-rank adaptation matrices that generalize across diverse edits, attributes, and compositional instructions. This enables continuous interpolation along individual edit dimensions while preserving both spatial locality and global semantic consistency. We apply SliderEdit to state-of-the-art image editing models, including FLUX-Kontext and Qwen-Image-Edit, and observe substantial improvements in edit controllability, visual consistency, and user steerability. To the best of our knowledge, we are the first to explore and propose a framework for continuous, fine-grained instruction control in instruction-based image editing models. Our results pave the way for interactive, instruction-driven image manipulation with continuous and compositional control."
    }
  ],
  "lastUpdated": "2025-11-17T09:35:19.392Z"
}