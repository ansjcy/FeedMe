{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "视觉扩散模型是几何求解器 (原标题: Visual Diffusion Models are Geometric Solvers)",
      "link": "https://arxiv.org/abs/2510.21697",
      "pubDate": "Fri, 24 Oct 2025 13:57:31 GMT",
      "isoDate": "2025-10-24T13:57:31.000Z",
      "creator": "Nir Goren, Shai Yehezkel, Omer Dahary, Andrey Voynov, Or Patashnik, Daniel Cohen-Or",
      "summary": "# 视觉扩散模型作为几何求解器\n\n本文提出了一种新颖的方法，表明视觉扩散模型能够作为有效的几何求解器，通过在像素空间中直接处理信息来解决复杂的几何问题。\n\n## 核心观点\n*   视觉扩散模型可以直接在像素空间中进行几何推理，从而有效地解决几何问题。\n\n## 应用案例\n该方法被成功应用于多个著名的、具有挑战性的几何问题：\n*   **内接正方形问题 (Inscribed Square Problem)**：这是一个长期存在的几何学难题，旨在确定每条Jordan曲线是否都包含形成正方形的四个点。\n*   **斯坦纳树问题 (Steiner Tree Problem)**：另一个广为人知的几何难题。\n*   **简单多边形问题 (Simple Polygon Problem)**：一个具有挑战性的几何问题。\n\n## 方法论\n*   **问题表示**：将每个几何问题实例视为一张图像。\n*   **模型训练**：训练一个标准的视觉扩散模型。\n*   **解决方案生成**：该模型能够将高斯噪声转换为代表有效近似解的图像，这些近似解与精确解高度匹配。\n*   **推理重塑**：通过这种方式，模型学习将嘈杂的几何结构转化为正确的配置，有效地将几何推理任务重塑为图像生成任务。\n\n## 创新与优势\n*   **简化架构**：与以往将扩散模型应用于参数化几何表示时需要专门架构和领域特定调整的方法不同，本文采用了一个标准的视觉扩散模型。\n*   **视觉操作**：该模型直接在问题的视觉表示上操作，而非参数化表示。\n*   **意外联系**：这种方法的简洁性揭示了生成建模与几何问题解决之间一个令人惊讶的、强大的联系。\n\n## 更广泛的意义\n*   **通用框架**：研究结果表明，在图像空间中操作提供了一个通用且实用的框架，用于近似解决那些臭名昭著的难题。\n*   **未来潜力**：这为解决更广泛的挑战性几何任务打开了大门，预示着一个更广阔的研究范式。",
      "shortSummary": "本文提出视觉扩散模型能作为几何求解器，通过在像素空间中操作解决几何问题。它将内接正方形、斯坦纳树和简单多边形等难题实例视为图像，训练标准扩散模型将高斯噪声转化为近似解。这种方法将几何推理重塑为图像生成，无需特殊架构，提供了一个通用且实用的框架来近似解决复杂几何任务，并为更广泛的挑战性几何任务开辟了道路。",
      "translated_title": "视觉扩散模型是几何求解器",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this paper we show that visual diffusion models can serve as effective geometric solvers: they can directly reason about geometric problems by working in pixel space. We first demonstrate this on the Inscribed Square Problem, a long-standing problem in geometry that asks whether every Jordan curve contains four points forming a square. We then extend the approach to two other well-known hard geometric problems: the Steiner Tree Problem and the Simple Polygon Problem.   Our method treats each problem instance as an image and trains a standard visual diffusion model that transforms Gaussian noise into an image representing a valid approximate solution that closely matches the exact one. The model learns to transform noisy geometric structures into correct configurations, effectively recasting geometric reasoning as image generation.   Unlike prior work that necessitates specialized architectures and domain-specific adaptations when applying diffusion to parametric geometric representations, we employ a standard visual diffusion model that operates on the visual representation of the problem. This simplicity highlights a surprising bridge between generative modeling and geometric problem solving. Beyond the specific problems studied here, our results point toward a broader paradigm: operating in image space provides a general and practical framework for approximating notoriously hard problems, and opens the door to tackling a far wider class of challenging geometric tasks."
    },
    {
      "title": "WorldGrow：生成无限3D世界 (原标题: WorldGrow: Generating Infinite 3D World)",
      "link": "https://arxiv.org/abs/2510.21682",
      "pubDate": "Fri, 24 Oct 2025 13:39:52 GMT",
      "isoDate": "2025-10-24T13:39:52.000Z",
      "creator": "Sikuang Li, Chen Yang, Jiemin Fang, Taoran Yi, Jia Lu, Jiazhong Cen, Lingxi Xie, Wei Shen, Qi Tian",
      "summary": "## WorldGrow：生成无限3D世界\n\n### 挑战与现有方法局限性\n\n文章指出，生成可无限扩展的3D世界是一个重大挑战，这种世界需要具备大型、连续的环境，以及连贯的几何形状和逼真的外观。现有方法面临以下关键局限：\n\n*   **2D提升方法**：在不同视角下存在几何和外观不一致的问题。\n*   **3D隐式表示**：难以扩展到大规模场景。\n*   **当前3D基础模型**：多以物体为中心，限制了它们在场景级生成中的应用。\n\n### WorldGrow的核心洞察与框架\n\nWorldGrow的核心洞察在于利用预训练3D模型中强大的生成先验知识进行结构化场景块生成。基于此，文章提出了WorldGrow，一个用于无界3D场景合成的分层框架。\n\n### WorldGrow的三个核心组件\n\nWorldGrow方法包含以下三个核心组件，以确保其在无限3D场景生成中的有效性：\n\n1.  **数据整理管道**：该组件负责提取高质量的场景块用于训练，使得3D结构化潜在表示能够适用于场景生成。\n2.  **3D块修复机制**：此机制能够实现上下文感知的场景扩展，确保新生成的区域与现有场景保持一致性。\n3.  **从粗到精的生成策略**：这种策略旨在确保全局布局的合理性，同时兼顾局部几何和纹理的保真度。\n\n### 评估与成果\n\nWorldGrow在大型3D-FRONT数据集上进行了评估，并取得了显著成果：\n\n*   在几何重建方面达到了SOTA（State-of-the-Art，即最先进）的性能。\n*   独特地支持无限场景生成，输出结果具有逼真度和结构一致性。\n\n### 潜在应用\n\n这些成果突显了WorldGrow在构建大规模虚拟环境方面的强大能力，并为其在未来世界模型建设中发挥作用提供了巨大潜力。",
      "shortSummary": "WorldGrow是一个分层框架，旨在解决生成无限可扩展3D世界的挑战。它通过利用预训练3D模型的生成先验知识，结合数据整理、3D块修复和从粗到精的生成策略，实现了上下文感知和结构一致的场景扩展。在3D-FRONT数据集上，WorldGrow在几何重建方面表现出色，并能生成逼真且结构连贯的无限3D场景，为构建大规模虚拟环境和未来世界模型提供了潜力。",
      "translated_title": "WorldGrow：生成无限3D世界",
      "images": [],
      "contentSource": "完整文章",
      "content": "We tackle the challenge of generating the infinitely extendable 3D world -- large, continuous environments with coherent geometry and realistic appearance. Existing methods face key challenges: 2D-lifting approaches suffer from geometric and appearance inconsistencies across views, 3D implicit representations are hard to scale up, and current 3D foundation models are mostly object-centric, limiting their applicability to scene-level generation. Our key insight is leveraging strong generation priors from pre-trained 3D models for structured scene block generation. To this end, we propose WorldGrow, a hierarchical framework for unbounded 3D scene synthesis. Our method features three core components: (1) a data curation pipeline that extracts high-quality scene blocks for training, making the 3D structured latent representations suitable for scene generation; (2) a 3D block inpainting mechanism that enables context-aware scene extension; and (3) a coarse-to-fine generation strategy that ensures both global layout plausibility and local geometric/textural fidelity. Evaluated on the large-scale 3D-FRONT dataset, WorldGrow achieves SOTA performance in geometry reconstruction, while uniquely supporting infinite scene generation with photorealistic and structurally consistent outputs. These results highlight its capability for constructing large-scale virtual environments and potential for building future world models."
    },
    {
      "title": "AstaBench：使用科学研究套件对AI智能体进行严格基准测试 (原标题: AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite)",
      "link": "https://arxiv.org/abs/2510.21652",
      "pubDate": "Fri, 24 Oct 2025 13:10:26 GMT",
      "isoDate": "2025-10-24T13:10:26.000Z",
      "creator": "Jonathan Bragg, Mike D'Arcy, Nishant Balepur, Dan Bareket, Bhavana Dalvi, Sergey Feldman, Dany Haddad, Jena D. Hwang, Peter Jansen, Varsha Kishore, Bodhisattwa Prasad Majumder, Aakanksha Naik, Sigal Rahamimov, Kyle Richardson, Amanpreet Singh, Harshit Surana, Aryeh Tiktinsky, Rosni Vasu, Guy Wiener, Chloe Anastasiades, Stefan Candra, Jason Dunkelberger, Dan Emery, Rob Evans, Malachi Hamada, Regan Huff, Rodney Kinney, Matt Latzke, Jaron Lochner, Ruben Lozano-Aguilera, Cecile Nguyen, Smita Rao, Amber Tanaka, Brooke Vlahos, Peter Clark, Doug Downey, Yoav Goldberg, Ashish Sabharwal, Daniel S. Weld",
      "summary": "## AstaBench：使用科学研究套件对AI智能体进行严格基准测试\n\n### 引言\nAI智能体在自动化文献综述、实验复现、数据分析乃至提出新研究方向方面，具有彻底改变科学生产力的巨大潜力。目前已存在多种此类智能体，从通用型“深度研究”系统到专门的科学智能体（如AI Scientist和AIGS）。对这些智能体进行严格评估对于其发展至关重要。\n\n### 现有基准的不足\n然而，现有基准在以下几个方面存在显著不足，无法有效评估AI智能体在科学研究中的实际应用能力：\n*   **缺乏整体性评估：** 未能提供针对科学研究等实际用例的整体性、以产品为导向的衡量标准。\n*   **缺乏可复现工具：** 缺少进行核心智能体能力受控比较所需的可复现智能体工具。\n*   **未考虑混杂变量：** 未能考虑模型成本和工具访问等可能影响评估结果的混杂变量。\n*   **缺乏标准化接口：** 未能提供用于快速智能体原型设计和评估的标准化接口，阻碍了开发效率。\n*   **缺乏全面的基线智能体：** 缺少识别真正技术进展所需的全面基线智能体，难以判断新方法的有效性。\n\n### AstaBench的提出\n为了解决现有基准的这些局限性，本文定义了更严格地对智能体进行基准测试的原则和工具，并在此基础上提出了AstaBench。AstaBench是一个综合性的套件，首次提供了衡量智能体执行科学研究能力的整体性指标。\n\n### AstaBench的主要特点\n*   **全面的问题集：** 包含2400多个问题，这些问题涵盖了整个科学发现过程和多个科学领域。其中许多问题灵感来源于Asta智能体实际用户的请求，确保了其现实意义。\n*   **生产级研究环境：** 提供了首个具有生产级搜索工具的科学研究环境。这使得评估能够受控且可复现，并能更好地考虑混杂变量的影响。\n*   **丰富的智能体和基线：** 提供了九类经过科学优化的Asta智能体以及众多基线，为研究人员提供了进行比较和分析的丰富资源。\n\n### 评估结果与发现\n研究人员对57个智能体（涵盖22个智能体类别）进行了广泛评估，揭示了一些有趣的发现。最重要的是，尽管AI在某些个体方面（如特定任务或子能力）取得了有意义的进展，但距离全面解决科学研究辅助的挑战仍有很长的路要走，表明该领域仍需大量深入研究和发展。",
      "shortSummary": "AstaBench是一个旨在严格评估AI智能体在科学研究中表现的新基准套件。它解决了现有基准在整体性、可复现性、混杂变量处理和标准化接口方面的不足。AstaBench包含2400多个问题和生产级搜索工具，并提供了多种优化的智能体和基线。对57个智能体的广泛评估显示，尽管AI在某些方面有所进步，但距离全面解决科学研究辅助挑战仍有显著差距。",
      "translated_title": "AstaBench：使用科学研究套件对AI智能体进行严格基准测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "AI agents hold the potential to revolutionize scientific productivity by automating literature reviews, replicating experiments, analyzing data, and even proposing new directions of inquiry; indeed, there are now many such agents, ranging from general-purpose \"deep research\" systems to specialized science-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of these agents is critical for progress. Yet existing benchmarks fall short on several fronts: they (1) fail to provide holistic, product-informed measures of real-world use cases such as science research; (2) lack reproducible agent tools necessary for a controlled comparison of core agentic capabilities; (3) do not account for confounding variables such as model cost and tool access; (4) do not provide standardized interfaces for quick agent prototyping and evaluation; and (5) lack comprehensive baseline agents necessary to identify true advances. In response, we define principles and tooling for more rigorously benchmarking agents. Using these, we present AstaBench, a suite that provides the first holistic measure of agentic ability to perform scientific research, comprising 2400+ problems spanning the entire scientific discovery process and multiple scientific domains, and including many problems inspired by actual user requests to deployed Asta agents. Our suite comes with the first scientific research environment with production-grade search tools that enable controlled, reproducible evaluation, better accounting for confounders. Alongside, we provide a comprehensive suite of nine science-optimized classes of Asta agents and numerous baselines. Our extensive evaluation of 57 agents across 22 agent classes reveals several interesting findings, most importantly that despite meaningful progress on certain individual aspects, AI remains far from solving the challenge of science research assistance."
    },
    {
      "title": "DeepAgent：一个具有可扩展工具集的通用推理智能体 (原标题: DeepAgent: A General Reasoning Agent with Scalable Toolsets)",
      "link": "https://arxiv.org/abs/2510.21618",
      "pubDate": "Fri, 24 Oct 2025 12:24:01 GMT",
      "isoDate": "2025-10-24T12:24:01.000Z",
      "creator": "Xiaoxi Li, Wenxiang Jiao, Jiarui Jin, Guanting Dong, Jiajie Jin, Yinuo Wang, Hao Wang, Yutao Zhu, Ji-Rong Wen, Yuan Lu, Zhicheng Dou",
      "summary": "# DeepAgent：一个具有可扩展工具集的通用推理智能体\n\n## 引言\n当前的大型推理模型在解决复杂问题方面表现出强大的能力。然而，在实际世界任务中，它们往往需要依赖外部工具进行操作，并涉及长时间的交互过程。现有的智能体框架通常遵循预定义的工作流程，这限制了智能体实现自主性和全局任务完成的能力。\n\n## DeepAgent 介绍\n本文提出了一种名为 **DeepAgent** 的端到端深度推理智能体。DeepAgent 的核心特点是能够在单一、连贯的推理过程中自主地进行思考、发现所需工具并执行相应动作，从而实现任务的自动化完成。\n\n## 解决长周期交互挑战\n在长周期交互中，智能体面临两大挑战：\n*   **上下文长度爆炸：** 随着多次工具调用和交互历史的积累，模型的上下文长度会急剧增加。\n*   **错误累积：** 冗长的交互历史可能导致错误信息的累积。\n\n为应对这些挑战，DeepAgent 引入了 **自主记忆折叠机制 (autonomous memory folding mechanism)**。该机制能够将过去的交互历史压缩成结构化的：\n*   **情景记忆 (episodic memory)**\n*   **工作记忆 (working memory)**\n*   **工具记忆 (tool memory)**\n\n通过这种方式，DeepAgent 既能有效减少错误累积，又能保留关键信息，确保推理过程的效率和准确性。\n\n## 高效稳定地教授通用工具使用\n为了高效且稳定地教授智能体通用工具的使用能力，DeepAgent 开发了一种端到端的强化学习策略，命名为 **ToolPO**。该策略的特点包括：\n*   **利用 LLM 模拟的 API：** 借助大型语言模型模拟的 API 来进行工具学习。\n*   **工具调用优势归因：** 应用细粒度的工具调用优势归因机制，为工具调用令牌分配精确的信用，从而优化学习过程。\n\n## 实验与结果\n研究团队在八个广泛使用的基准测试上对 DeepAgent 进行了全面的实验评估。这些基准测试涵盖了：\n*   **通用工具使用任务：** ToolBench、API-Bank、TMDB、Spotify、ToolHop。\n*   **下游应用：** ALFWorld、WebShop、GAIA、HLE。\n\n实验结果表明，无论是在标记工具检索场景还是开放集工具检索场景中，DeepAgent 的性能都持续优于现有基线方法。\n\n## 结论\nDeepAgent 的提出标志着在构建更通用、更强大、能够应对真实世界复杂应用的智能体方面迈出了重要一步。\n\n## 代码和演示\nDeepAgent 的代码和演示已公开可用。",
      "shortSummary": "DeepAgent是一个端到端的通用推理智能体，旨在解决大型模型在真实世界任务中对外部工具和长周期交互的需求。它通过自主思考、工具发现和行动执行实现任务完成。为应对上下文长度爆炸和错误累积，DeepAgent引入了自主记忆折叠机制。为高效教授工具使用，它开发了强化学习策略ToolPO。实验表明，DeepAgent在多个基准测试中表现优异，超越了现有基线，是迈向更强大通用智能体的重要进展。",
      "translated_title": "DeepAgent：一个具有可扩展工具集的通用推理智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To address the challenges of long-horizon interactions, particularly the context length explosion from multiple tool calls and the accumulation of interaction history, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. This work takes a step toward more general and capable agents for real-world applications. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent."
    },
    {
      "title": "逐步采样，分块优化：用于文本到图像生成的块级GRPO (原标题: Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image Generation)",
      "link": "https://arxiv.org/abs/2510.21583",
      "pubDate": "Fri, 24 Oct 2025 11:50:36 GMT",
      "isoDate": "2025-10-24T11:50:36.000Z",
      "creator": "Yifu Luo, Penghui Du, Bo Li, Sinan Du, Tiantian Zhang, Yongzhe Chang, Kai Wu, Kun Gai, Xueqian Wang",
      "summary": "# 块级GRPO：文本到图像生成的新范式\n\n## 引言\n\n*   **背景**：组相对策略优化（Group Relative Policy Optimization, GRPO）在基于流匹配的文本到图像（Text-to-Image, T2I）生成领域展现出强大潜力。\n*   **现有问题**：然而，GRPO面临两大关键限制：\n    1.  **优势归因不准确**：难以精确地归因策略改进的优势。\n    2.  **忽略时间动态**：未能充分考虑生成过程中的时间动态性。\n\n## 核心思想与Chunk-GRPO方法\n\n*   **优化范式转变**：本文提出将优化范式从传统的“步级”（step level）转移到创新的“块级”（chunk level），以有效缓解上述问题。\n*   **Chunk-GRPO的提出**：基于这一思想，作者提出了Chunk-GRPO，这是首个用于T2I生成的块级GRPO方法。\n*   **关键洞察**：\n    *   将连续的生成步骤分组为连贯的“块”（chunk）。\n    *   这些“块”旨在捕获流匹配固有的时间动态性。\n    *   策略优化在这些“块”的层面上进行，而非单个步骤。\n\n## 性能增强策略\n\n*   **加权采样**：Chunk-GRPO引入了一种可选的加权采样策略，以进一步提升模型的性能。\n\n## 实验结果与结论\n\n*   **卓越表现**：广泛的实验结果表明，Chunk-GRPO在偏好对齐（preference alignment）和图像质量（image quality）两方面均取得了卓越的成果。\n*   **未来前景**：这突出显示了块级优化对于基于GRPO的方法在T2I生成领域中的巨大潜力。",
      "shortSummary": "GRPO在文本到图像生成中存在优势归因不准确和忽略时间动态的问题。本文提出Chunk-GRPO，通过将优化从步级转移到块级来解决这些限制。Chunk-GRPO将连续步骤分组为捕获时间动态的“块”进行优化，并引入可选的加权采样。实验证明，Chunk-GRPO在偏好对齐和图像质量上均表现优异，展示了块级优化在GRPO方法中的潜力。",
      "translated_title": "逐步采样，分块优化：用于文本到图像生成的块级GRPO",
      "images": [],
      "contentSource": "完整文章",
      "content": "Group Relative Policy Optimization (GRPO) has shown strong potential for flow-matching-based text-to-image (T2I) generation, but it faces two key limitations: inaccurate advantage attribution, and the neglect of temporal dynamics of generation. In this work, we argue that shifting the optimization paradigm from the step level to the chunk level can effectively alleviate these issues. Building on this idea, we propose Chunk-GRPO, the first chunk-level GRPO-based approach for T2I generation. The insight is to group consecutive steps into coherent 'chunk's that capture the intrinsic temporal dynamics of flow matching, and to optimize policies at the chunk level. In addition, we introduce an optional weighted sampling strategy to further enhance performance. Extensive experiments show that ChunkGRPO achieves superior results in both preference alignment and image quality, highlighting the promise of chunk-level optimization for GRPO-based methods."
    },
    {
      "title": "使用范畴论进行文档理解、度量和操作 (原标题: Document Understanding, Measurement, and Manipulation Using Category Theory)",
      "link": "https://arxiv.org/abs/2510.21553",
      "pubDate": "Fri, 24 Oct 2025 11:12:08 GMT",
      "isoDate": "2025-10-24T11:12:08.000Z",
      "creator": "Jared Claypoole, Yunye Gong, Noson S. Yanofsky, Ajay Divakaran",
      "summary": "本文介绍了一种利用范畴论进行文档理解、度量和操作的创新方法。该研究旨在提取多模态文档结构，并在此基础上开发信息论度量、内容摘要、内容扩展以及大型预训练模型的自监督改进技术。\n\n**核心方法论：**\n\n*   **文档的数学表示：** 首先，研究将文档表示为“问答对”的范畴，为后续的信息处理奠定数学基础。\n*   **信息正交化：** 提出一种正交化程序，用于将一个或多个文档中包含的信息划分为互不重叠的独立部分，从而实现信息的清晰分离。\n*   **信息度量与枚举：** 基于前两步提取的结构，开发了测量和枚举文档中信息的方法，以量化文档内容的丰富性。\n*   **摘要与扩展技术：**\n    *   **新颖的摘要技术：** 开发了新的文档摘要方法，旨在更有效地捕捉核心信息。\n    *   **释义（Exegesis）：** 解决了一个新问题，即通过“释义”过程对原始文档进行扩展，生成更丰富、更深入的内容。\n*   **速率失真分析：** 问答对的方法使得对摘要技术进行新颖的速率失真分析成为可能，从而评估摘要质量与信息损失之间的权衡。\n*   **模型实现与多模态扩展：**\n    *   这些技术使用大型预训练模型进行实现。\n    *   提出了整体数学框架的多模态扩展，以适应处理不同类型数据（如文本、图像等）的需求。\n*   **自监督模型改进：** 开发了一种新颖的自监督方法，利用RLVR（Reinforcement Learning from Vague Rewards）和源自范畴论框架的一致性约束（例如可组合性和在某些操作下的闭包性）来改进大型预训练模型，提升其性能和鲁棒性。\n\n**研究领域：**\n\n*   计算与语言 (cs.CL)\n*   机器学习 (cs.LG)\n\n**重要提示：** 文章内容中未包含有效的实际图片链接，因此详细摘要中不包含任何图片。",
      "shortSummary": "本文提出使用范畴论进行文档理解、度量和操作。核心方法是将文档表示为问答对的范畴，并开发信息正交化、度量、摘要和扩展技术。通过新颖的速率失真分析，并利用大型预训练模型实现这些方法。此外，还引入了基于范畴论一致性约束的自监督学习方法，以改进预训练模型，并支持多模态扩展。",
      "translated_title": "使用范畴论进行文档理解、度量和操作",
      "images": [],
      "contentSource": "完整文章",
      "content": "We apply category theory to extract multimodal document structure which leads us to develop information theoretic measures, content summarization and extension, and self-supervised improvement of large pretrained models. We first develop a mathematical representation of a document as a category of question-answer pairs. Second, we develop an orthogonalization procedure to divide the information contained in one or more documents into non-overlapping pieces. The structures extracted in the first and second steps lead us to develop methods to measure and enumerate the information contained in a document. We also build on those steps to develop new summarization techniques, as well as to develop a solution to a new problem viz. exegesis resulting in an extension of the original document. Our question-answer pair methodology enables a novel rate distortion analysis of summarization techniques. We implement our techniques using large pretrained models, and we propose a multimodal extension of our overall mathematical framework. Finally, we develop a novel self-supervised method using RLVR to improve large pretrained models using consistency constraints such as composability and closure under certain operations that stem naturally from our category theoretic framework."
    },
    {
      "title": "PhysWorld：通过物理感知演示合成，从真实视频到可变形物体的世界模型 (原标题: PhysWorld: From Real Videos to World Models of Deformable Objects via Physics-Aware Demonstration Synthesis)",
      "link": "https://arxiv.org/abs/2510.21447",
      "pubDate": "Fri, 24 Oct 2025 09:25:39 GMT",
      "isoDate": "2025-10-24T09:25:39.000Z",
      "creator": "Yu Yang, Zhilu Zhang, Xiang Zhang, Yihan Zeng, Hui Li, Wangmeng Zuo",
      "summary": "## PhysWorld：通过物理感知演示合成，从真实视频到可变形物体的世界模型\n\n### 摘要\n\n本文介绍了PhysWorld，一个旨在解决从有限真实世界视频数据中学习可变形物体物理一致动力学模型挑战的新颖框架。该框架通过利用模拟器合成物理上合理且多样化的演示，以学习高效的世界模型。\n\n### 核心挑战\n\n*   **交互式世界模型的重要性：** 对于机器人、虚拟现实（VR）和增强现实（AR）领域至关重要。\n*   **学习物理一致动力学模型的困难：** 尤其是在以下情况下：\n    *   真实世界视频数据有限。\n    *   对象是具有空间变化物理属性的可变形物体。\n*   **数据稀缺性：** 是一个主要的障碍。\n\n### PhysWorld 框架方法\n\nPhysWorld 提出了一种多阶段方法来克服数据稀缺性并学习高效的世界模型：\n\n1.  **构建物理一致的数字孪生：**\n    *   在MPM（Material Point Method）模拟器中进行。\n    *   通过本构模型选择和物理属性的全局到局部优化来实现。\n\n2.  **合成广泛多样的演示：**\n    *   对数字孪生的物理属性施加部分感知的扰动（part-aware perturbations）。\n    *   为数字孪生生成各种运动模式，从而合成大量且多样化的演示数据。\n\n3.  **训练轻量级世界模型：**\n    *   利用这些合成的演示数据，训练一个轻量级的基于GNN（图神经网络）的世界模型。\n    *   该世界模型被嵌入了物理属性。\n\n4.  **真实视频的辅助作用：**\n    *   真实视频数据可以进一步用于完善和优化物理属性。\n\n### 实验结果与优势\n\n*   **准确和快速的未来预测：** PhysWorld 能够对各种可变形物体实现准确且快速的未来预测。\n*   **良好的泛化能力：** 该模型能够很好地泛化到新颖的交互场景。\n*   **竞争性性能：** 实验表明 PhysWorld 具有与现有技术相当甚至更优的性能。\n*   **显著的推理速度提升：** 相较于最近最先进的方法 PhysTwin，PhysWorld 的推理速度快了47倍。",
      "shortSummary": "PhysWorld是一个创新框架，旨在通过物理感知演示合成，从有限真实视频中为可变形物体构建世界模型。它首先在MPM模拟器中创建数字孪生，然后通过扰动物理属性合成多样化演示，并用这些数据训练一个嵌入物理属性的轻量级GNN世界模型。该方法实现了对各种可变形物体准确、快速的未来预测，并能很好地泛化到新颖交互，其推理速度比现有技术快47倍。",
      "translated_title": "PhysWorld：通过物理感知演示合成，从真实视频到可变形物体的世界模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Interactive world models that simulate object dynamics are crucial for robotics, VR, and AR. However, it remains a significant challenge to learn physics-consistent dynamics models from limited real-world video data, especially for deformable objects with spatially-varying physical properties. To overcome the challenge of data scarcity, we propose PhysWorld, a novel framework that utilizes a simulator to synthesize physically plausible and diverse demonstrations to learn efficient world models. Specifically, we first construct a physics-consistent digital twin within MPM simulator via constitutive model selection and global-to-local optimization of physical properties. Subsequently, we apply part-aware perturbations to the physical properties and generate various motion patterns for the digital twin, synthesizing extensive and diverse demonstrations. Finally, using these demonstrations, we train a lightweight GNN-based world model that is embedded with physical properties. The real video can be used to further refine the physical properties. PhysWorld achieves accurate and fast future predictions for various deformable objects, and also generalizes well to novel interactions. Experiments show that PhysWorld has competitive performance while enabling inference speeds 47 times faster than the recent state-of-the-art method, i.e., PhysTwin."
    },
    {
      "title": "通过令牌置换实现更稀疏的块稀疏注意力 (原标题: Sparser Block-Sparse Attention via Token Permutation)",
      "link": "https://arxiv.org/abs/2510.21270",
      "pubDate": "Fri, 24 Oct 2025 05:11:50 GMT",
      "isoDate": "2025-10-24T05:11:50.000Z",
      "creator": "Xinghao Wang, Pengyu Wang, Dong Zhang, Chenkun Tan, Shaojun Zhou, Zhaoxiang Liu, Shiguo Lian, Fangxu Liu, Kai Song, Xipeng Qiu",
      "summary": "# Permuted Block-Sparse Attention (PBS-Attn)：提升LLM长上下文计算效率\n\n## 背景与问题\n\n大型语言模型（LLMs）的上下文长度扩展带来了显著优势，但其计算成本高昂。这主要源于自注意力机制的$O(N^2)$复杂度，它在序列长度方面对内存和延迟构成了主要瓶颈。尽管注意力矩阵通常是稀疏的，特别是对于长序列，但现有的块稀疏注意力方法的效果高度依赖于底层的注意力模式，这可能导致次优的块级稀疏性。例如，单个块内查询的重要键令牌可能分散在许多其他块中，从而导致计算冗余。\n\n## 提出的解决方案：Permuted Block-Sparse Attention (PBS-Attn)\n\n本文提出了一种名为Permuted Block-Sparse Attention (PBS-Attn) 的即插即用方法。该方法利用注意力的置换特性来增加块级稀疏性，旨在增强LLM预填充的计算效率。\n\n## 主要贡献与实验结果\n\n*   **性能超越**：在具有挑战性的真实世界长上下文数据集上进行的综合实验表明，PBS-Attn在模型准确性方面始终优于现有的块稀疏注意力方法。\n*   **准确性匹配**：PBS-Attn的准确性与全注意力基线非常接近。\n*   **端到端加速**：得益于定制的置换FlashAttention内核，PBS-Attn在长上下文预填充中实现了高达2.75倍的端到端加速。\n*   **实用性验证**：实验结果充分证实了PBS-Attn的实际可行性。\n\n## 代码可用性\n\n相关代码已公开提供。",
      "shortSummary": "大型语言模型（LLMs）中自注意力机制的$O(N^2)$复杂度是长上下文扩展的瓶颈。现有块稀疏注意力因注意力模式分散而效率不足。本文提出Permuted Block-Sparse Attention (PBS-Attn)，通过令牌置换增加块级稀疏性，提升LLM预填充效率。实验表明，PBS-Attn在长上下文数据集上准确性优于现有方法并接近全注意力，同时实现高达2.75倍的端到端加速，证实了其在实际应用中的可行性。",
      "translated_title": "通过令牌置换实现更稀疏的块稀疏注意力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Scaling the context length of large language models (LLMs) offers significant benefits but is computationally expensive. This expense stems primarily from the self-attention mechanism, whose O(N^2) complexity with respect to sequence length presents a major bottleneck for both memory and latency. Fortunately, the attention matrix is often sparse, particularly for long sequences, suggesting an opportunity for optimization. Block-sparse attention has emerged as a promising solution that partitions sequences into blocks and skips computation for a subset of these blocks. However, the effectiveness of this method is highly dependent on the underlying attention patterns, which can lead to sub-optimal block-level sparsity. For instance, important key tokens for queries within a single block may be scattered across numerous other blocks, leading to computational redundancy. In this work, we propose Permuted Block-Sparse Attention (PBS-Attn), a plug-and-play method that leverages the permutation properties of attention to increase block-level sparsity and enhance the computational efficiency of LLM prefilling. We conduct comprehensive experiments on challenging real-world long-context datasets, demonstrating that PBS-Attn consistently outperforms existing block-sparse attention methods in model accuracy and closely matches the full attention baseline. Powered by our custom permuted-FlashAttention kernels, PBS-Attn achieves an end-to-end speedup of up to 2.75times in long-context prefilling, confirming its practical viability. Code available at https://github.com/xinghaow99/pbs-attn"
    },
    {
      "title": "使用功能性双锚的模型合并 (原标题: Model Merging with Functional Dual Anchors)",
      "link": "https://arxiv.org/abs/2510.21223",
      "pubDate": "Fri, 24 Oct 2025 03:54:06 GMT",
      "isoDate": "2025-10-24T03:54:06.000Z",
      "creator": "Kexuan Shi, Yandong Wen, Weiyang Liu",
      "summary": "## 使用功能性双锚的模型合并\n\n### 摘要\n\n*   **背景与挑战**\n    *   模型合并是一种高效的后训练策略，旨在整合来自共享基础模型多个微调检查点的知识。\n    *   当前方法主要在参数空间中操作，通过组合任务向量来缓解冲突，但其有效性受限于参数不一致性。\n\n*   **功能性双锚 (FDAs) 方法**\n    *   本文提出了一种名为功能性双锚（FDAs）的新框架，该框架转而对输入-表示空间进行建模。\n    *   FDAs是合成输入，其诱导梯度与任务向量对齐，能够捕获相对于预训练模型的任务特定功能性偏移。\n\n*   **FDAs 的优势与特性**\n    *   这种新的视角成功地弥合了联合多任务训练与事后合并之间的鸿沟。\n    *   该方法提供了更高的鲁棒性和灵活性。\n    *   文章还引入了一种原则性的初始化方案。\n    *   FDAs被证明与现有的参数空间模型合并方法具有互补性。\n\n*   **实验验证**\n    *   全面的实验结果表明，FDAs在模型合并任务中表现出显著的有效性。",
      "shortSummary": "本文提出功能性双锚（FDAs）框架，以解决现有模型合并方法在参数空间中的局限。FDAs通过在输入-表示空间建模，利用合成输入捕获任务特定功能性偏移，从而弥合多任务训练与事后合并。该方法提供鲁棒性和灵活性，并与参数空间方法互补。实验证明FDAs在模型合并中有效。",
      "translated_title": "使用功能性双锚的模型合并",
      "images": [],
      "contentSource": "完整文章",
      "content": "Model merging is an efficient post-training strategy for integrating knowledge from multiple finetuned checkpoints of a shared foundation model. Existing methods operate in the parameter space, combining task vectors to mitigate conflicts, but remain constrained by parameter inconsistencies. We propose Functional Dual Anchors (FDAs), a framework that instead models the input-representation space. FDAs are synthetic inputs whose induced gradients align with task vectors, capturing task-specific functional shifts relative to the pretrained model. This perspective bridges joint multi-task training and post-hoc merging, offering both robustness and flexibility. We further introduce a principled initialization scheme and show that FDAs are complementary to parameter-space model merging. Comprehensive experiments demonstrate the effectiveness of FDAs in model merging."
    },
    {
      "title": "PhysVLM-AVR：多模态大型语言模型在物理环境中的主动视觉推理 (原标题: PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments)",
      "link": "https://arxiv.org/abs/2510.21111",
      "pubDate": "Thu, 23 Oct 2025 22:59:00 GMT",
      "isoDate": "2025-10-23T22:59:00.000Z",
      "creator": "Weijie Zhou, Xuantang Xiong, Yi Peng, Manli Tao, Chaoyang Zhao, Honghui Dong, Ming Tang, Jinqiao Wang",
      "summary": "### PhysVLM-AVR：多模态大型语言模型在物理环境中的主动视觉推理\n\n**背景与挑战**\n\n*   **现有研究局限性**：多模态大型语言模型（MLLMs）的视觉推理研究主要集中在静态、完全可观察的设置中。这限制了它们在真实世界环境中的有效性，因为在真实世界中，信息常常因遮挡或有限视野而不完整。\n*   **人类能力对比**：与此不同，人类能够主动探索并与环境互动——通过移动、检查和操作物体——通过整合感知、推理和行动的闭环过程来收集信息。\n\n**引入主动视觉推理（AVR）任务**\n\n*   **任务定义**：受人类能力的启发，本文引入了主动视觉推理（Active Visual Reasoning, AVR）任务，将视觉推理扩展到部分可观察、交互式的环境中。\n*   **AVR对智能体的要求**：\n    1.  **主动信息获取**：通过顺序的物理动作主动获取信息。\n    2.  **多步信息整合**：整合多步骤的观察结果以进行连贯推理。\n    3.  **动态决策调整**：根据不断演变的视觉反馈动态调整决策。\n\n**评估基准与数据集**\n\n*   **CLEVR-AVR基准**：为了严格评估AVR，研究团队引入了CLEVR-AVR，这是一个模拟基准，具有多轮交互式环境，旨在评估推理的正确性和信息收集的效率。\n*   **AVR-152k大规模数据集**：提供了AVR-152k，一个大规模数据集。该数据集包含丰富的思维链（Chain-of-Thought, CoT）注释，详细说明了迭代推理过程，包括：\n    *   不确定性识别\n    *   行动条件下的信息增益预测\n    *   信息最大化行动选择\n    这些对于在更高阶马尔可夫决策过程中训练智能体至关重要。\n\n**PhysVLM-AVR模型与性能**\n\n*   **模型开发**：在此基础上，研究团队开发了PhysVLM-AVR，一个MLLM。\n*   **最先进性能**：PhysVLM-AVR在以下任务中取得了最先进的性能：\n    *   CLEVR-AVR\n    *   具身推理（OpenEQA, RoboVQA）\n    *   被动视觉推理（GeoMath, Geometry30K）\n\n**研究发现与未来方向**\n\n*   **当前MLLMs的局限**：分析揭示，当前的具身MLLMs尽管能够检测到信息不完整性，但难以通过交互主动获取和整合新信息。\n*   **核心差距**：这凸显了在主动推理能力方面存在的根本性差距。",
      "shortSummary": "多模态大型语言模型（MLLMs）在部分可观察的真实世界环境中进行视觉推理时面临挑战。本文提出了主动视觉推理（AVR）任务，要求智能体通过物理动作主动获取和整合信息。为评估AVR，引入了CLEVR-AVR基准和AVR-152k数据集。研究团队开发了PhysVLM-AVR模型，在多项任务上取得了最先进性能。分析表明，当前具身MLLMs在主动获取和整合新信息方面存在不足，存在根本性差距。",
      "translated_title": "PhysVLM-AVR：多模态大型语言模型在物理环境中的主动视觉推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Visual reasoning in multimodal large language models (MLLMs) has primarily been studied in static, fully observable settings, limiting their effectiveness in real-world environments where information is often incomplete due to occlusion or limited field of view. Humans, in contrast, actively explore and interact with their environment-moving, examining, and manipulating objects-to gather information through a closed-loop process integrating perception, reasoning, and action. Inspired by this human capability, we introduce the Active Visual Reasoning (AVR) task, extending visual reasoning to partially observable, interactive environments. AVR necessitates agents to: (1) actively acquire information via sequential physical actions, (2) integrate observations across multiple steps for coherent reasoning, and (3) dynamically adjust decisions based on evolving visual feedback. To rigorously evaluate AVR, we introduce CLEVR-AVR, a simulation benchmark featuring multi-round interactive environments designed to assess both reasoning correctness and information-gathering efficiency. We present AVR-152k, a large-scale dataset that offers rich Chain-of-Thought (CoT) annotations detailing iterative reasoning for uncertainty identification, action-conditioned information gain prediction, and information-maximizing action selection, crucial for training agents in a higher-order Markov Decision Process. Building on this, we develop PhysVLM-AVR, an MLLM achieving state-of-the-art performance on CLEVR-AVR, embodied reasoning (OpenEQA, RoboVQA), and passive visual reasoning (GeoMath, Geometry30K). Our analysis also reveals that current embodied MLLMs, despite detecting information incompleteness, struggle to actively acquire and integrate new information through interaction, highlighting a fundamental gap in active reasoning capabilities."
    },
    {
      "title": "HoloCine：电影级多镜头长视频叙事的整体生成 (原标题: HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives)",
      "link": "https://arxiv.org/abs/2510.20822",
      "pubDate": "Thu, 23 Oct 2025 13:59:59 GMT",
      "isoDate": "2025-10-23T13:59:59.000Z",
      "creator": "Yihao Meng, Hao Ouyang, Yue Yu, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Hanlin Wang, Yixuan Li, Cheng Chen, Yanhong Zeng, Yujun Shen, Huamin Qu",
      "summary": "## HoloCine：弥合叙事鸿沟，实现自动化电影制作\n\n### 核心问题\n\n当前最先进的文本到视频模型在生成独立的视频片段方面表现出色，但它们无法创建连贯、多镜头的叙事内容，而这正是故事讲述的精髓所在。这种能力上的缺失被称为“叙事鸿沟”。\n\n### HoloCine 的解决方案\n\nHoloCine 旨在弥合这一“叙事鸿沟”，它通过以下方式实现了电影级多镜头长视频叙事的整体生成：\n\n*   **整体场景生成**：HoloCine 能够整体生成整个场景，从而确保从第一个镜头到最后一个镜头都具有全局一致性。\n\n### 关键技术与架构\n\nHoloCine 的架构融合了创新机制，以实现精确控制和高效生成：\n\n*   **窗口交叉注意力（Window Cross-Attention）**：\n    *   该机制能够将文本提示精确地定位到特定的镜头，从而实现对导演意图的精确控制。\n*   **稀疏镜头间自注意力模式（Sparse Inter-Shot Self-Attention）**：\n    *   这种模式在镜头内部是密集的，但在镜头之间是稀疏的。它确保了在分钟级视频生成所需的效率。\n\n### 显著成就与新兴能力\n\nHoloCine 不仅在叙事连贯性方面树立了新的行业标杆，还展现出令人瞩目的新兴能力：\n\n*   **角色和场景的持久记忆**：模型能够记住角色和场景，并在整个叙事中保持它们的一致性。\n*   **对电影技术的直观理解**：HoloCine 展现出对电影拍摄技巧和叙事手法的直观掌握。\n\n### 意义与展望\n\nHoloCine 的工作标志着视频生成领域从单一片段合成向自动化电影制作的关键转变。它使端到端的电影级创作成为一个触手可及的未来。\n\n### 代码可用性\n\n项目的代码已公开发布。",
      "shortSummary": "HoloCine模型旨在弥合文本到视频生成中的“叙事鸿沟”，解决现有模型无法创建连贯多镜头叙事的问题。它通过整体生成场景，确保全局一致性。HoloCine采用窗口交叉注意力实现精确导演控制，并利用稀疏镜头间自注意力模式提高效率。该模型在叙事连贯性上达到新高度，并展现出对角色和场景的持久记忆以及对电影技术的直观理解，标志着向自动化电影制作迈出了关键一步。",
      "translated_title": "HoloCine：电影级多镜头长视频叙事的整体生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "State-of-the-art text-to-video models excel at generating isolated clips but fall short of creating the coherent, multi-shot narratives, which are the essence of storytelling. We bridge this \"narrative gap\" with HoloCine, a model that generates entire scenes holistically to ensure global consistency from the first shot to the last. Our architecture achieves precise directorial control through a Window Cross-Attention mechanism that localizes text prompts to specific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within shots but sparse between them) ensures the efficiency required for minute-scale generation. Beyond setting a new state-of-the-art in narrative coherence, HoloCine develops remarkable emergent abilities: a persistent memory for characters and scenes, and an intuitive grasp of cinematic techniques. Our work marks a pivotal shift from clip synthesis towards automated filmmaking, making end-to-end cinematic creation a tangible future. Our code is available at: https://holo-cine.github.io/."
    },
    {
      "title": "LayerComposer：通过空间感知分层画布实现交互式个性化文本到图像生成 (原标题: LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered Canvas)",
      "link": "https://arxiv.org/abs/2510.20820",
      "pubDate": "Thu, 23 Oct 2025 13:59:55 GMT",
      "isoDate": "2025-10-23T13:59:55.000Z",
      "creator": "Guocheng Gordon Qian, Ruihang Zhang, Tsai-Shien Chen, Yusuf Dalva, Anujraaj Argo Goyal, Willi Menapace, Ivan Skorokhodov, Meng Dong, Arpit Sahni, Daniil Ostashev, Ju Hu, Sergey Tulyakov, Kuan-Chieh Jackson Wang",
      "summary": "## LayerComposer：交互式个性化文本到图像生成框架\n\n### 引言\n\n现有个性化生成模型在空间构图的交互控制方面存在局限性，并且在处理多个主体时扩展性较差。为了解决这些挑战，研究人员提出了 LayerComposer，一个用于个性化、多主体文本到图像（T2I）生成的交互式框架。\n\n### 主要贡献\n\nLayerComposer 的方法引入了两项主要贡献，显著提升了多主体T2I生成的控制能力和保真度：\n\n1.  **分层画布（Layered Canvas）**\n    *   **新颖表示：** LayerComposer 采用了一种新颖的表示方法，其中每个主体都被放置在独立的层上。\n    *   **无遮挡构图：** 这种分层结构能够实现无遮挡的构图，确保每个主体都能清晰地呈现。\n    *   **直观操作：** 类似于专业的图像编辑软件，用户可以通过直观的层操作（如放置、调整大小或锁定输入主体）来灵活控制图像的布局。\n\n2.  **锁定机制（Locking Mechanism）**\n    *   **高保真保留：** 该机制能够高保真地保留用户选定的层，确保这些层中的主体身份和细节不被改变。\n    *   **灵活适应：** 同时，它允许其余未锁定的层灵活地适应周围的上下文，从而实现整体图像的和谐统一。\n    *   **技术实现：** 值得注意的是，这种多功能的锁定机制无需对模型架构进行任何更改，而是依赖于固有的位置嵌入和一种新的互补数据采样策略。\n\n### 实验结果\n\n广泛的实验表明，LayerComposer 在多主体个性化图像生成方面，相比现有最先进的方法，实现了卓越的空间控制和身份保留。\n\n### 技术领域与发布信息\n\n*   **主题：** 计算机视觉与模式识别 (cs.CV)\n*   **形式：** 9页预印本\n*   **引用：** arXiv:2510.20820 [cs.CV]",
      "shortSummary": "LayerComposer是一个交互式框架，旨在解决现有文本到图像生成模型在多主体空间构图和个性化控制方面的局限。它引入了“分层画布”将每个主体置于独立层以实现无遮挡构图，以及“锁定机制”可高保真保留选定层并允许其他层灵活适应。该方法通过直观的层操作提供卓越的空间控制和身份保留，无需架构更改，优于现有技术。",
      "translated_title": "LayerComposer：通过空间感知分层画布实现交互式个性化文本到图像生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite their impressive visual fidelity, existing personalized generative models lack interactive control over spatial composition and scale poorly to multiple subjects. To address these limitations, we present LayerComposer, an interactive framework for personalized, multi-subject text-to-image generation. Our approach introduces two main contributions: (1) a layered canvas, a novel representation in which each subject is placed on a distinct layer, enabling occlusion-free composition; and (2) a locking mechanism that preserves selected layers with high fidelity while allowing the remaining layers to adapt flexibly to the surrounding context. Similar to professional image-editing software, the proposed layered canvas allows users to place, resize, or lock input subjects through intuitive layer manipulation. Our versatile locking mechanism requires no architectural changes, relying instead on inherent positional embeddings combined with a new complementary data sampling strategy. Extensive experiments demonstrate that LayerComposer achieves superior spatial control and identity preservation compared to the state-of-the-art methods in multi-subject personalized image generation."
    },
    {
      "title": "Video-As-Prompt：视频生成中的统一语义控制 (原标题: Video-As-Prompt: Unified Semantic Control for Video Generation)",
      "link": "https://arxiv.org/abs/2510.20888",
      "pubDate": "Thu, 23 Oct 2025 13:59:52 GMT",
      "isoDate": "2025-10-23T13:59:52.000Z",
      "creator": "Yuxuan Bian, Xin Chen, Zenan Li, Tiancheng Zhi, Shen Sang, Linjie Luo, Qiang Xu",
      "summary": "### Video-As-Prompt (VAP)：视频生成中的统一语义控制\n\n**1. 核心挑战**\n\n*   在视频生成领域，实现统一、可泛化的语义控制仍然是一个关键的开放性难题。\n*   现有方法存在以下局限性：\n    *   通过强制施加不恰当的基于结构的像素级先验，容易引入伪影。\n    *   依赖于不可泛化、特定条件的微调或任务特定的架构。\n\n**2. 引入 Video-As-Prompt (VAP)**\n\n*   VAP 提出了一种新的范式，将视频生成中的语义控制问题重新定义为“上下文生成”（in-context generation）。\n*   **核心机制：**\n    *   利用参考视频作为直接的语义提示（semantic prompt）。\n    *   通过一个即插即用的 Transformer 混合专家（Mixture-of-Transformers, MoT）来引导一个冻结的视频扩散 Transformer (Video Diffusion Transformer, DiT)。\n*   **关键特性与优势：**\n    *   有效防止灾难性遗忘（catastrophic forgetting）。\n    *   通过时间偏置位置嵌入（temporally biased position embedding）进行引导，该嵌入能够消除虚假映射先验，从而实现鲁棒的上下文检索。\n\n**3. VAP-Data 数据集**\n\n*   为了支持 VAP 方法并促进未来的研究，研究团队构建了 VAP-Data。\n*   这是目前最大的语义控制视频生成数据集，包含超过 10 万对视频，涵盖 100 种语义条件。\n\n**4. 性能与成果**\n\n*   VAP 作为单一的统一模型，为开源方法树立了新的技术标准（state-of-the-art）。\n*   实现了 38.7% 的用户偏好率，这一表现足以与领先的特定条件商业模型相媲美。\n*   展现出强大的零样本泛化能力（zero-shot generalization）。\n*   支持各种下游应用。\n\n**5. 意义**\n\n*   VAP 的推出标志着在通用、可控视频生成方面取得了重大进展。",
      "shortSummary": "Video-As-Prompt (VAP) 提出了一种视频生成新范式，通过将参考视频作为语义提示，利用即插即用的Transformer混合专家引导冻结的视频扩散Transformer，实现统一的语义控制。VAP解决了现有方法的局限性，并构建了最大的语义控制视频生成数据集VAP-Data。作为单一模型，VAP在开源方法中达到了最先进水平，用户偏好率达38.7%，媲美商业模型，展现出强大的零样本泛化能力，是通用可控视频生成的重要进展。",
      "translated_title": "Video-As-Prompt：视频生成中的统一语义控制",
      "images": [],
      "contentSource": "完整文章",
      "content": "Unified, generalizable semantic control in video generation remains a critical open challenge. Existing methods either introduce artifacts by enforcing inappropriate pixel-wise priors from structure-based controls, or rely on non-generalizable, condition-specific finetuning or task-specific architectures. We introduce Video-As-Prompt (VAP), a new paradigm that reframes this problem as in-context generation. VAP leverages a reference video as a direct semantic prompt, guiding a frozen Video Diffusion Transformer (DiT) via a plug-and-play Mixture-of-Transformers (MoT) expert. This architecture prevents catastrophic forgetting and is guided by a temporally biased position embedding that eliminates spurious mapping priors for robust context retrieval. To power this approach and catalyze future research, we built VAP-Data, the largest dataset for semantic-controlled video generation with over 100K paired videos across 100 semantic conditions. As a single unified model, VAP sets a new state-of-the-art for open-source methods, achieving a 38.7% user preference rate that rivals leading condition-specific commercial models. VAP's strong zero-shot generalization and support for various downstream applications mark a significant advance toward general-purpose, controllable video generation."
    },
    {
      "title": "ARGenSeg: 基于自回归图像生成模型的图像分割 (原标题: ARGenSeg: Image Segmentation with Autoregressive Image Generation Model)",
      "link": "https://arxiv.org/abs/2510.20803",
      "pubDate": "Thu, 23 Oct 2025 13:58:26 GMT",
      "isoDate": "2025-10-23T13:58:26.000Z",
      "creator": "Xiaolong Wang, Lixiang Ru, Ziyuan Huang, Kaixiang Ji, Dandan Zheng, Jingdong Chen, Jun Zhou",
      "summary": "## ARGenSeg：基于自回归图像生成模型的图像分割新范式\n\n### 概述\nARGenSeg（AutoRegressive Generation-based paradigm for image Segmentation）提出了一种新颖的图像分割范式，旨在在一个统一的框架内实现多模态理解和像素级感知。该方法解决了现有将图像分割集成到多模态大型语言模型（MLLMs）中的局限性，即这些方法通常依赖于离散表示或语义提示，限制了MLLM捕获细粒度视觉细节的能力。\n\n### 现有方法的局限性\n*   **表示方式：** 传统的MLLM图像分割方法通常采用边界点表示或专用的分割头。\n*   **依赖性：** 这些方法依赖于离散表示或语义提示，并将其输入到特定任务的解码器中。\n*   **细节捕获限制：** 上述依赖性限制了MLLM捕获图像中细粒度视觉细节的能力。\n\n### ARGenSeg 的创新点与工作原理\n1.  **基于图像生成：** ARGenSeg引入了一种基于图像生成的MLLM分割框架，能够自然地为目标对象生成密集的掩码。\n2.  **像素级理解：** 该方法利用MLLM输出视觉token，然后使用通用的VQ-VAE（Vector Quantized Variational AutoEncoder）将这些视觉token反token化为图像。这使得分割过程完全依赖于MLLM的像素级理解能力。\n3.  **统一框架：** ARGenSeg在一个统一的框架内实现了多模态理解和像素级感知，避免了对特定任务解码器或离散表示的依赖。\n\n### 性能优化\n*   **推理延迟降低：** 为了显著降低推理延迟，ARGenSeg采用了一种“下一尺度预测”（next-scale-prediction）策略，能够并行生成所需的视觉token。\n\n### 实验结果与优势\n*   **超越SOTA：** 广泛的实验证明，ARGenSeg在多个分割数据集上超越了现有的最先进方法。\n*   **速度提升：** 该方法在保持强大理解能力的同时，显著提升了推理速度。\n*   **强大理解能力：** ARGenSeg能够保持并展现出强大的多模态理解能力。\n\n### 论文信息\n*   **接受情况：** 该研究已被NeurIPS 2025接收。\n*   **页数：** 18页。",
      "shortSummary": "ARGenSeg提出一种基于自回归图像生成的新型MLLM图像分割范式，旨在通过MLLM输出视觉token并使用VQ-VAE反token化为图像，实现像素级理解和密集掩码生成。该方法解决了传统MLLM分割方法在细粒度细节捕获上的局限性。ARGenSeg采用下一尺度预测策略减少推理延迟，并在多个分割数据集上超越现有最先进方法，显著提升了推理速度和理解能力。",
      "translated_title": "ARGenSeg: 基于自回归图像生成模型的图像分割",
      "images": [],
      "contentSource": "完整文章",
      "content": "We propose a novel AutoRegressive Generation-based paradigm for image Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level perception within a unified framework. Prior works integrating image segmentation into multimodal large language models (MLLMs) typically employ either boundary points representation or dedicated segmentation heads. These methods rely on discrete representations or semantic prompts fed into task-specific decoders, which limits the ability of the MLLM to capture fine-grained visual details. To address these challenges, we introduce a segmentation framework for MLLM based on image generation, which naturally produces dense masks for target objects. We leverage MLLM to output visual tokens and detokenize them into images using an universal VQ-VAE, making the segmentation fully dependent on the pixel-level understanding of the MLLM. To reduce inference latency, we employ a next-scale-prediction strategy to generate required visual tokens in parallel. Extensive experiments demonstrate that our method surpasses prior state-of-the-art approaches on multiple segmentation datasets with a remarkable boost in inference speed, while maintaining strong understanding capabilities."
    },
    {
      "title": "大型推理模型是优秀的翻译评估器吗？分析与性能提升 (原标题: Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost)",
      "link": "https://arxiv.org/abs/2510.20780",
      "pubDate": "Thu, 23 Oct 2025 13:48:36 GMT",
      "isoDate": "2025-10-23T13:48:36.000Z",
      "creator": "Runzhe Zhan, Zhihong Huang, Xinyi Yang, Lidia S. Chao, Min Yang, Derek F. Wong",
      "summary": "### 大型推理模型作为翻译评估器的分析与性能提升\n\n本文探讨了大型推理模型（LRMs）在机器翻译（MT）质量评估方面的潜力，并提出了改进其性能的方法。\n\n**背景与研究目的：**\n*   大型推理模型（LRMs）通过引入中间“思考”过程，提升了在复杂任务上的推理能力。\n*   然而，LRMs作为机器翻译质量评估器的潜力尚未得到充分探索。\n*   本文首次对LRM作为MT评估器进行了系统分析。\n\n**主要挑战与发现：**\n*   研究识别了LRMs在MT评估中的几个关键挑战：\n    *   **需要定制评估材料：** LRMs需要专门设计的评估材料才能有效工作。\n    *   **过度思考：** LRMs倾向于对更简单的实例进行“过度思考”，浪费计算资源。\n    *   **评分机制问题：** 现有的评分机制可能导致LRMs对翻译质量的过高估计。\n\n**提出的解决方案：**\n*   为了解决上述问题，本文提出了一种校准LRM思考过程的方法。\n*   该方法通过使用合成的、类似人类的思考轨迹来训练LRMs。\n\n**实验结果与性能提升：**\n*   在WMT24 Metrics基准测试上的实验表明，所提出的方法取得了显著成效：\n    *   **思考预算大幅减少：** 成功将LRMs的思考预算减少了约35倍。\n    *   **评估性能提升：** 在不同规模的LRMs（从7B到32B）上，评估性能均有所提高。\n    *   **具体案例：** 例如，R1-Distill-Qwen-7B模型的相关性分数提高了8.7个百分点。\n\n**结论与意义：**\n*   这些发现强调了经过高效校准的LRMs在推进细粒度自动化机器翻译评估方面的巨大潜力。",
      "shortSummary": "本文首次系统分析了大型推理模型（LRMs）作为机器翻译评估器的潜力。研究发现LRMs面临定制材料需求、过度思考和评分机制问题。为解决这些挑战，作者提出通过合成、类人思考轨迹训练LRMs进行校准。实验表明，此方法显著减少了思考预算约35倍，并提升了7B至32B规模LRMs的评估性能（如R1-Distill-Qwen-7B相关性提升8.7点），展现了校准LRMs在细粒度MT评估中的应用前景。",
      "translated_title": "大型推理模型是优秀的翻译评估器吗？分析与性能提升",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in large reasoning models (LRMs) have introduced an intermediate \"thinking\" process prior to generating final answers, improving their reasoning capabilities on complex downstream tasks. However, the potential of LRMs as evaluators for machine translation (MT) quality remains underexplored. We provides the first systematic analysis of LRM-as-a-judge in MT evaluation. We identify key challenges, revealing LRMs require tailored evaluation materials, tend to \"overthink\" simpler instances and have issues with scoring mechanisms leading to overestimation. To address these, we propose to calibrate LRM thinking by training them on synthetic, human-like thinking trajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that this approach largely reduces thinking budgets by ~35x while concurrently improving evaluation performance across different LRM scales from 7B to 32B (e.g., R1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). These findings highlight the potential of efficiently calibrated LRMs to advance fine-grained automatic MT evaluation."
    },
    {
      "title": "AlphaFlow：理解与改进MeanFlow模型 (原标题: AlphaFlow: Understanding and Improving MeanFlow Models)",
      "link": "https://arxiv.org/abs/2510.20771",
      "pubDate": "Thu, 23 Oct 2025 13:45:06 GMT",
      "isoDate": "2025-10-23T13:45:06.000Z",
      "creator": "Huijie Zhang, Aliaksandr Siarohin, Willi Menapace, Michael Vasilkovsky, Sergey Tulyakov, Qing Qu, Ivan Skorokhodov",
      "summary": "## AlphaFlow：理解与改进MeanFlow模型\n\n### MeanFlow的背景与挑战\n\nMeanFlow作为一种强大的少步生成建模框架，能够从头开始训练，但其成功的深层机制尚未被完全理解。\n\n### MeanFlow目标函数的分解与优化冲突\n\n本研究揭示了MeanFlow的目标函数自然分解为两个关键部分：\n\n*   **轨迹流匹配（trajectory flow matching）**\n*   **轨迹一致性（trajectory consistency）**\n\n通过深入的梯度分析，研究人员发现这两个项之间存在强烈的负相关性。这种负相关性导致了优化过程中的冲突，并减缓了模型的收敛速度。\n\n### 引入 $\\alpha$-Flow：统一与改进\n\n受上述洞察的启发，本研究引入了 **$\\alpha$-Flow**，这是一个广泛的目标函数家族。$\\alpha$-Flow 的核心在于它能够将以下三种重要的生成模型目标统一在一个公式之下：\n\n*   轨迹流匹配\n*   Shortcut Model\n*   MeanFlow\n\n### $\\alpha$-Flow的策略与优势\n\n$\\alpha$-Flow 采用了一种创新的 **课程学习策略（curriculum strategy）**。该策略通过从轨迹流匹配平滑地退火（anneals）到MeanFlow，有效地解耦了之前相互冲突的优化目标。这种方法带来了显著的优势：\n\n*   **解耦冲突目标**：消除了优化过程中的负面相互作用。\n*   **加速收敛**：实现了更快的模型收敛。\n\n### 实验结果与最先进性能\n\n研究团队在类条件 ImageNet-1K 256x256 数据集上，使用标准的 DiT 骨干网络（vanilla DiT backbones）从头开始训练并评估了 $\\alpha$-Flow。实验结果显示：\n\n*   **全面超越MeanFlow**：$\\alpha$-Flow 在各种规模和设置下都持续优于 MeanFlow。\n*   **新的最先进成果**：最大的 $\\alpha$-Flow-XL/2+ 模型，在沿用香草 DiT 骨干网络的情况下，取得了新的最先进（state-of-the-art）结果。\n    *   其 FID 分数在 1-NFE（Number of Function Evaluations）下达到 **2.58**。\n    *   在 2-NFE 下进一步优化至 **2.15**。\n\n### 研究领域\n\n本研究主要属于以下领域：\n\n*   计算机视觉与模式识别 (cs.CV)\n*   机器学习 (cs.LG)",
      "shortSummary": "本研究深入分析了MeanFlow模型，发现其目标函数中的轨迹流匹配和轨迹一致性存在优化冲突，导致收敛缓慢。为此，论文提出了$\\alpha$-Flow，一个统一了多种生成模型目标的新框架。$\\alpha$-Flow通过课程学习策略解耦了冲突目标，显著提升了收敛性。实验表明，在ImageNet-1K上，$\\alpha$-Flow在各种设置下均优于MeanFlow，其最大模型$\\alpha$-Flow-XL/2+取得了2.15（2-NFE）的最新FID分数。",
      "translated_title": "AlphaFlow：理解与改进MeanFlow模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "MeanFlow has recently emerged as a powerful framework for few-step generative modeling trained from scratch, but its success is not yet fully understood. In this work, we show that the MeanFlow objective naturally decomposes into two parts: trajectory flow matching and trajectory consistency. Through gradient analysis, we find that these terms are strongly negatively correlated, causing optimization conflict and slow convergence. Motivated by these insights, we introduce alpha-Flow, a broad family of objectives that unifies trajectory flow matching, Shortcut Model, and MeanFlow under one formulation. By adopting a curriculum strategy that smoothly anneals from trajectory flow matching to MeanFlow, alpha-Flow disentangles the conflicting objectives, and achieves better convergence. When trained from scratch on class-conditional ImageNet-1K 256x256 with vanilla DiT backbones, alpha-Flow consistently outperforms MeanFlow across scales and settings. Our largest alpha-Flow-XL/2+ model achieves new state-of-the-art results using vanilla DiT backbones, with FID scores of 2.58 (1-NFE) and 2.15 (2-NFE)."
    },
    {
      "title": "DyPE：用于超高分辨率扩散的动态位置外推 (原标题: DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion)",
      "link": "https://arxiv.org/abs/2510.20766",
      "pubDate": "Thu, 23 Oct 2025 13:42:14 GMT",
      "isoDate": "2025-10-23T13:42:14.000Z",
      "creator": "Noam Issachar, Guy Yariv, Sagie Benaim, Yossi Adi, Dani Lischinski, Raanan Fattal",
      "summary": "# DyPE：用于超高分辨率扩散的动态位置外推\n\n本文介绍了一种名为 **DyPE (Dynamic Position Extrapolation)** 的新颖、无需训练的方法，旨在解决扩散Transformer模型在生成超高分辨率图像时面临的巨大计算成本问题。\n\n## 核心问题\n\n*   **高昂的训练成本**：扩散Transformer模型虽然能够生成具有卓越保真度和细节的图像，但由于其自注意力机制的计算复杂度与图像token数量呈二次方增长，导致在超高分辨率下训练这些模型成本极高。\n\n## DyPE 方法介绍\n\n*   **创新性与免训练**：DyPE 是一种新颖的方法，它无需额外的训练，即可使预训练的扩散Transformer模型能够以远超其训练数据的分辨率合成图像，且不增加额外的采样成本。\n*   **利用扩散过程的频谱演进**：DyPE 的核心思想是利用扩散过程中固有的频谱演进特性：\n    *   低频结构（通常对应图像的大尺度特征）在扩散过程的早期阶段趋于收敛。\n    *   高频结构（对应图像的精细细节）需要更多步骤才能完全解析。\n*   **动态调整位置编码**：DyPE 在每个扩散步骤中动态调整模型的位置编码，使其频率频谱与生成过程的当前阶段相匹配。\n\n## 关键成果与优势\n\n*   **显著提升分辨率**：该方法能够生成分辨率远超训练分辨率的图像，例如，使用FLUX模型可以生成高达1600万像素的图像。\n*   **性能提升与最先进的保真度**：在多个基准测试中，DyPE 持续提升性能，并在超高分辨率图像生成方面实现了最先进的保真度。\n*   **分辨率越高，增益越明显**：DyPE 的性能提升在高分辨率下表现得更为显著。",
      "shortSummary": "DyPE（动态位置外推）是一种无需训练的新方法，旨在解决扩散Transformer模型生成超高分辨率图像的高成本问题。它利用扩散过程中的频谱演进特性，在每个扩散步骤中动态调整位置编码，使其频率频谱与生成阶段匹配。DyPE使预训练模型能够以远超训练数据的分辨率生成图像，且不增加采样成本，显著提升了超高分辨率图像生成的性能和保真度，尤其在高分辨率下效果更佳。",
      "translated_title": "DyPE：用于超高分辨率扩散的动态位置外推",
      "images": [],
      "contentSource": "完整文章",
      "content": "Diffusion Transformer models can generate images with remarkable fidelity and detail, yet training them at ultra-high resolutions remains extremely costly due to the self-attention mechanism's quadratic scaling with the number of image tokens. In this paper, we introduce Dynamic Position Extrapolation (DyPE), a novel, training-free method that enables pre-trained diffusion transformers to synthesize images at resolutions far beyond their training data, with no additional sampling cost. DyPE takes advantage of the spectral progression inherent to the diffusion process, where low-frequency structures converge early, while high-frequencies take more steps to resolve. Specifically, DyPE dynamically adjusts the model's positional encoding at each diffusion step, matching their frequency spectrum with the current stage of the generative process. This approach allows us to generate images at resolutions that exceed the training resolution dramatically, e.g., 16 million pixels using FLUX. On multiple benchmarks, DyPE consistently improves performance and achieves state-of-the-art fidelity in ultra-high-resolution image generation, with gains becoming even more pronounced at higher resolutions. Project page is available at https://noamissachar.github.io/DyPE/."
    },
    {
      "title": "多智能体协作中的思想通信 (原标题: Thought Communication in Multiagent Collaboration)",
      "link": "https://arxiv.org/abs/2510.20733",
      "pubDate": "Thu, 23 Oct 2025 12:48:02 GMT",
      "isoDate": "2025-10-23T12:48:02.000Z",
      "creator": "Yujia Zheng, Zhuokai Zhao, Zijian Li, Yaqi Xie, Mingze Gao, Lizhu Zhang, Kun Zhang",
      "summary": "## 多智能体协作中的思想通信\n\n### 摘要\n\n当前，自然语言在促进人类合作方面发挥着重要作用，但其固有的信息损耗、模糊性和间接性限制了集体智能的潜力。尽管机器不受这些限制，但大多数基于大型语言模型（LLM）的多智能体系统仍主要依赖自然语言，通过交换令牌或其嵌入进行通信。为了超越语言的局限，本研究引入了一种名为“思想通信”的新范式，使智能体能够直接进行“心智相通”的交互，类似于心灵感应。\n\n### 理论基础与发现\n\n1.  **形式化为潜在变量模型**：\n    *   研究将这一过程形式化为一个通用的潜在变量模型，其中智能体的状态由底层“思想”的未知函数生成。\n2.  **思想识别与共享结构恢复**：\n    *   在没有辅助信息的非参数设置下，理论证明可以识别任意一对智能体之间共享和私有的潜在思想。\n    *   此外，思想共享的全局结构，包括哪些智能体共享哪些思想以及这些关系如何构建，也可以通过理论保证得到恢复。\n\n### 实践框架\n\n*   在已建立的理论指导下，研究开发了一个框架，该框架在通信之前从所有智能体中提取潜在思想。\n*   然后，将相关的思想及其共享模式分配给每个智能体。\n\n### 适用性与实验验证\n\n*   这种范式自然地超越了LLM，扩展到所有模态，因为大多数观测数据都源于隐藏的生成过程。\n*   在合成和真实世界基准上的实验验证了该理论，并展示了思想通信在协作方面的优势。\n\n### 展望\n\n本研究希望能够阐明利用“隐藏世界”的潜力，因为许多挑战仅凭表面观察是无法解决的，无论计算能力或数据规模如何。",
      "shortSummary": "该研究引入了“思想通信”范式，旨在克服自然语言在多智能体协作中的局限性。通过将智能体交互形式化为潜在变量模型，该方法实现了智能体之间直接的“心智相通”，识别并共享潜在思想。这不仅提升了协作效率，还具有理论保证，并可推广至LLM之外的多种模态，为解决仅凭表面观察无法解决的问题提供了新途径。",
      "translated_title": "多智能体协作中的思想通信",
      "images": [],
      "contentSource": "完整文章",
      "content": "Natural language has long enabled human cooperation, but its lossy, ambiguous, and indirect nature limits the potential of collective intelligence. While machines are not subject to these constraints, most LLM-based multi-agent systems still rely solely on natural language, exchanging tokens or their embeddings. To go beyond language, we introduce a new paradigm, thought communication, which enables agents to interact directly mind-to-mind, akin to telepathy. To uncover these latent thoughts in a principled way, we formalize the process as a general latent variable model, where agent states are generated by an unknown function of underlying thoughts. We prove that, in a nonparametric setting without auxiliary information, both shared and private latent thoughts between any pair of agents can be identified. Moreover, the global structure of thought sharing, including which agents share which thoughts and how these relationships are structured, can also be recovered with theoretical guarantees. Guided by the established theory, we develop a framework that extracts latent thoughts from all agents prior to communication and assigns each agent the relevant thoughts, along with their sharing patterns. This paradigm naturally extends beyond LLMs to all modalities, as most observational data arise from hidden generative processes. Experiments on both synthetic and real-world benchmarks validate the theory and demonstrate the collaborative advantages of thought communication. We hope this work illuminates the potential of leveraging the hidden world, as many challenges remain unsolvable through surface-level observation alone, regardless of compute or data scale."
    },
    {
      "title": "从掩码到世界：世界模型搭便车指南 (原标题: From Masks to Worlds: A Hitchhiker's Guide to World Models)",
      "link": "https://arxiv.org/abs/2510.20668",
      "pubDate": "Thu, 23 Oct 2025 11:46:44 GMT",
      "isoDate": "2025-10-23T11:46:44.000Z",
      "creator": "Jinbin Bai, Yu Lei, Hecong Wu, Yuchen Zhu, Shufan Li, Yi Xin, Xiangtai Li, Molei Tao, Aditya Grover, Ming-Hsuan Yang",
      "summary": "### 文章目的与范围\n\n本文并非对世界模型的传统综述，而是一份旨在指导读者构建“世界”的指南。作者无意罗列所有提及“世界模型”的论文，而是遵循一条清晰的发展路径。\n\n### 世界模型的发展路径\n\n文章详细阐述了世界模型从早期阶段到高级阶段的演变，具体包括以下几个关键步骤：\n\n1.  **早期掩码模型：**\n    *   起源于用于跨模态统一表征学习的早期掩码模型。\n2.  **统一架构：**\n    *   随后进展到共享单一范式的统一架构。\n3.  **交互式生成模型：**\n    *   进一步发展到能够闭合行动-感知循环的交互式生成模型。\n4.  **记忆增强系统：**\n    *   最终达到能够长时间维持一致世界的记忆增强系统。\n\n### 核心关注点\n\n文章刻意避开与核心主题关联不紧密的旁支，专注于以下三大核心要素：\n\n*   **生成核心 (generative heart)**：负责生成和预测世界状态。\n*   **交互循环 (interactive loop)**：实现模型与环境之间的行动-感知闭环。\n*   **记忆系统 (memory system)**：确保世界模型能够长时间维持内部状态的一致性。\n\n### 未来展望\n\n作者认为，上述发展路径是通向真正世界模型最有前景的道路。",
      "shortSummary": "本文是一份构建世界模型的指南，而非传统综述。它描绘了一条清晰的发展路径：从用于跨模态表征学习的早期掩码模型，到统一架构，再到闭合行动-感知循环的交互式生成模型，最终是维持一致世界的记忆增强系统。文章专注于生成核心、交互循环和记忆系统这三大核心要素，并认为这是通向真正世界模型最有前景的道路。",
      "translated_title": "从掩码到世界：世界模型搭便车指南",
      "images": [],
      "contentSource": "完整文章",
      "content": "This is not a typical survey of world models; it is a guide for those who want to build worlds. We do not aim to catalog every paper that has ever mentioned a ``world model\". Instead, we follow one clear road: from early masked models that unified representation learning across modalities, to unified architectures that share a single paradigm, then to interactive generative models that close the action-perception loop, and finally to memory-augmented systems that sustain consistent worlds over time. We bypass loosely related branches to focus on the core: the generative heart, the interactive loop, and the memory system. We show that this is the most promising path towards true world models."
    },
    {
      "title": "Open-o3 Video：基于显式时空证据的视频推理 (原标题: Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence)",
      "link": "https://arxiv.org/abs/2510.20579",
      "pubDate": "Thu, 23 Oct 2025 10:05:56 GMT",
      "isoDate": "2025-10-23T10:05:56.000Z",
      "creator": "Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, Zhuochen Wang",
      "summary": "# Open-o3 Video：基于显式时空证据的视频推理\n\n本文介绍了Open-o3 Video，一个旨在解决当前视频推理模型局限性的非代理框架。现有的大多数视频推理模型仅生成文本推理轨迹，却未能明确指出关键证据出现的时间和地点。将OpenAI-o3等模型在图像领域取得的以证据为中心的推理能力扩展到视频领域更具挑战性，因为它需要跨动态场景进行联合的时间跟踪和空间定位。\n\n## 核心问题与解决方案\n\n*   **问题：** 视频推理模型缺乏明确的时空证据，导致推理结果难以与具体的视觉观察对应。\n*   **解决方案：** Open-o3 Video通过将显式时空证据整合到视频推理中，使得模型能够在其答案旁边突出显示关键时间戳、对象和边界框，从而将推理结果建立在具体的视觉观察之上。\n\n## 主要贡献与方法\n\n为了实现这一功能并应对上述挑战，Open-o3 Video项目采取了以下关键策略：\n\n### 1. 高质量数据集的构建与整理\n\n*   **背景：** 大多数现有数据集要么提供视频的时间跨度，要么提供图像上的空间框，缺乏统一的时空监督和推理轨迹。\n*   **创新：** Open-o3 Video精心策划并构建了两个高质量的数据集：\n    *   **STGR-CoT-30k：** 用于SFT（监督微调）。\n    *   **STGR-RL-36k：** 用于RL（强化学习）。\n    *   这两个数据集都包含精心构建的时间和空间标注，为模型提供了必要的统一时空监督。\n\n### 2. 创新的训练策略\n\n*   **方法：** 采用了冷启动强化学习策略。\n*   **奖励设计：** 该策略结合了多个专门设计的奖励机制，共同鼓励：\n    *   答案准确性\n    *   时间对齐\n    *   空间精度\n\n## 性能表现与影响\n\nOpen-o3 Video在多个视频理解基准测试中展现了卓越的性能：\n\n*   **V-STAR基准测试：** 在Qwen2.5-VL基线上，Open-o3 Video实现了最先进的性能，将mAM（平均精度）提高了14.4%，将mLGM（多语言生成度量）提高了24.2%。\n*   **广泛适用性：** 在VideoMME、WorldSense、VideoMMMU和TVGBench等一系列广泛的视频理解基准测试中也观察到了一致的改进。\n*   **额外价值：** 除了提高准确性，Open-o3 Video生成的推理轨迹还为测试时扩展提供了有价值的信号，从而实现了置信度感知的验证，并提高了答案的可靠性。",
      "shortSummary": "Open-o3 Video框架旨在解决现有视频推理模型缺乏显式时空证据的问题。它通过整合关键时间戳、对象和边界框来将推理结果与具体视觉观察相结合。为实现此目标，该项目构建了STGR-CoT-30k和STGR-RL-36k两个高质量数据集，并采用了结合答案准确性、时间对齐和空间精度的冷启动强化学习策略。Open-o3 Video在V-STAR等多个基准测试中取得了最先进的性能，显著提升了视频推理的准确性和可靠性。",
      "translated_title": "Open-o3 Video：基于显式时空证据的视频推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Most video reasoning models only generate textual reasoning traces without indicating when and where key evidence appears. Recent models such as OpenAI-o3 have sparked wide interest in evidence-centered reasoning for images, yet extending this ability to videos is more challenging, as it requires joint temporal tracking and spatial localization across dynamic scenes. We introduce Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal evidence into video reasoning, and carefully collect training data and design training strategies to address the aforementioned challenges. The model highlights key timestamps, objects, and bounding boxes alongside its answers, allowing reasoning to be grounded in concrete visual observations. To enable this functionality, we first curate and build two high-quality datasets, STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed temporal and spatial annotations, since most existing datasets offer either temporal spans for videos or spatial boxes on images, lacking unified spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start reinforcement learning strategy with multiple specially designed rewards that jointly encourage answer accuracy, temporal alignment, and spatial precision. On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance, raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent improvements are also observed on a broad range of video understanding benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond accuracy, the reasoning traces produced by Open-o3 Video also provide valuable signals for test-time scaling, enabling confidence-aware verification and improving answer reliability."
    }
  ],
  "lastUpdated": "2025-10-27T09:38:07.203Z"
}