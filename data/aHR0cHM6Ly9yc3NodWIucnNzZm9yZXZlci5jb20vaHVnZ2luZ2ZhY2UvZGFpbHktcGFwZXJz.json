{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "LEAML：多模态大型语言模型在分布外视觉任务中的标签高效适应 (原标题: LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models)",
      "link": "https://arxiv.org/abs/2510.03232",
      "pubDate": "Fri, 03 Oct 2025 13:59:56 GMT",
      "isoDate": "2025-10-03T13:59:56.000Z",
      "creator": "Ci-Siang Lin, Min-Hung Chen, Yu-Yang Sheng, Yu-Chiang Frank Wang",
      "summary": "# LEAML：多模态大型语言模型在分布外视觉任务中的标签高效适应框架\n\n## 核心问题\n多模态大型语言模型（MLLMs）在通用视觉基准测试中表现出色，但在专业领域（如医学影像）的分布外（OOD）任务中面临挑战。这些领域通常标签数据有限且昂贵。\n\n## 解决方案：LEAML框架\n本文引入了LEAML，一个标签高效的适应框架，旨在解决MLLMs在专业领域OOD任务中的数据稀缺问题。\n\n## LEAML方法论\nLEAML框架通过以下关键机制实现其目标：\n*   **数据利用**：同时利用稀缺的标注VQA（视觉问答）样本和丰富的未标注图像。\n*   **伪问答对生成**：为未标注数据生成领域相关的伪问答对。\n*   **QA生成器与蒸馏**：\n    *   使用一个QA生成器来创建这些伪问答对。\n    *   该生成器通过字幕蒸馏（caption distillation）进行正则化，以确保生成内容的质量和相关性。\n*   **选择性神经元更新**：\n    *   一个重要特点是，LEAML仅选择性地更新那些与问答最相关的神经元。\n    *   这种选择性更新使得QA生成器能够在蒸馏过程中高效地获取领域特定知识。\n\n## 实验结果\n在胃肠内窥镜检查和体育VQA数据集上进行的实验表明，在最小监督条件下，LEAML始终优于标准的微调方法。这突出显示了所提出的LEAML框架的有效性。",
      "shortSummary": "LEAML是一种标签高效的适应框架，旨在解决多模态大型语言模型（MLLMs）在医疗影像等专业领域中，因缺乏标签数据而难以处理分布外（OOD）视觉任务的问题。该框架利用少量标注VQA样本和大量未标注图像，通过一个由字幕蒸馏正则化的QA生成器，为未标注数据生成领域相关的伪问答对，并选择性更新与问答最相关的神经元。实验证明，LEAML在极少监督下显著优于标准微调方法。",
      "translated_title": "LEAML：多模态大型语言模型在分布外视觉任务中的标签高效适应",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal Large Language Models (MLLMs) have achieved strong performance on general visual benchmarks but struggle with out-of-distribution (OOD) tasks in specialized domains such as medical imaging, where labeled data is limited and expensive. We introduce LEAML, a label-efficient adaptation framework that leverages both scarce labeled VQA samples and abundant unlabeled images. Our approach generates domain-relevant pseudo question-answer pairs for unlabeled data using a QA generator regularized by caption distillation. Importantly, we selectively update only those neurons most relevant to question-answering, enabling the QA Generator to efficiently acquire domain-specific knowledge during distillation. Experiments on gastrointestinal endoscopy and sports VQA demonstrate that LEAML consistently outperforms standard fine-tuning under minimal supervision, highlighting the effectiveness of our proposed LEAML framework."
    },
    {
      "title": "通过显式位置到坐标映射改进GUI定位 (原标题: Improving GUI Grounding with Explicit Position-to-Coordinate Mapping)",
      "link": "https://arxiv.org/abs/2510.03230",
      "pubDate": "Fri, 03 Oct 2025 13:59:34 GMT",
      "isoDate": "2025-10-03T13:59:34.000Z",
      "creator": "Suyuchen Wang, Tianyu Zhang, Ahmed Masry, Christopher Pal, Spandana Gella, Bang Liu, Perouz Taslakian",
      "summary": "# 通过显式位置到坐标映射改进GUI定位\n\n## 核心问题\n\nGUI定位（GUI grounding）是将自然语言指令映射到像素坐标的任务，这对于自主代理至关重要。然而，对于当前的视觉语言模型（VLMs）来说，这项任务仍然面临巨大挑战。\n\n*   **主要瓶颈**：在于可靠的“块到像素”映射。当模型需要外推到训练时未见过的高分辨率显示器时，这种映射就会失效。\n*   **现有方法缺陷**：当前方法直接从视觉特征生成文本坐标，这迫使模型隐式地推断复杂的位置到像素映射。结果是，在新分辨率下的准确性会下降，并且失败率会增加。\n\n## 提出的创新解决方案\n\n为了解决上述问题，本文提出了两项互补的创新：\n\n1.  **RULER tokens（标尺标记）**：\n    *   这些标记作为显式的坐标指示器。\n    *   它们允许模型像地图上的网格线一样引用和调整位置，而不是从零开始生成坐标。\n\n2.  **Interleaved MRoPE (I-MRoPE)（交错式MRoPE）**：\n    *   该技术通过确保宽度和高度维度得到同等表示来改进空间编码。\n    *   它解决了标准位置编码方案中固有的不对称问题。\n\n## 实验结果与优势\n\n*   **性能提升**：在ScreenSpot、ScreenSpot-V2和ScreenSpot-Pro数据集上的实验表明，该方法在定位准确性方面取得了持续的提升。\n*   **高分辨率表现**：在高分辨率界面上，改进效果最为显著。\n*   **可靠性增强**：通过提供显式空间指导而非依赖隐式学习，本文提出的方法能够在不同分辨率和平台上实现更可靠的GUI自动化。",
      "shortSummary": "本文提出通过显式位置到坐标映射来改进GUI定位，以解决当前视觉语言模型在高分辨率显示器上定位准确性下降的问题。研究引入了RULER tokens作为显式坐标标记，并采用Interleaved MRoPE改进空间编码，确保宽度和高度维度得到同等表示。实验结果表明，该方法在多个数据集上显著提高了定位准确性，尤其在高分辨率界面上表现更佳，从而实现了更可靠的GUI自动化。",
      "translated_title": "通过显式位置到坐标映射改进GUI定位",
      "images": [],
      "contentSource": "完整文章",
      "content": "GUI grounding, the task of mapping natural-language instructions to pixel coordinates, is crucial for autonomous agents, yet remains difficult for current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which breaks when extrapolating to high-resolution displays unseen during training. Current approaches generate coordinates as text tokens directly from visual features, forcing the model to infer complex position-to-pixel mappings implicitly; as a result, accuracy degrades and failures proliferate on new resolutions. We address this with two complementary innovations. First, RULER tokens serve as explicit coordinate markers, letting the model reference positions similar to gridlines on a map and adjust rather than generate coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial encoding by ensuring that width and height dimensions are represented equally, addressing the asymmetry of standard positional schemes. Experiments on ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in grounding accuracy, with the largest improvements on high-resolution interfaces. By providing explicit spatial guidance rather than relying on implicit learning, our approach enables more reliable GUI automation across diverse resolutions and platforms."
    },
    {
      "title": "FocusAgent：修剪网络代理大型上下文的简单而有效方法 (原标题: FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents)",
      "link": "https://arxiv.org/abs/2510.03204",
      "pubDate": "Fri, 03 Oct 2025 13:41:30 GMT",
      "isoDate": "2025-10-03T13:41:30.000Z",
      "creator": "Imene Kerboua, Sahar Omidi Shayegan, Megh Thakkar, Xing Han Lù, Léo Boisvert, Massimo Caccia, Jérémy Espinas, Alexandre Aussem, Véronique Eglin, Alexandre Lacoste",
      "summary": "### FocusAgent：修剪网络代理大型上下文的简单而有效方法\n\n**1. 引言与问题背景**\n\n*   **挑战：** 由大型语言模型（LLMs）驱动的网络代理在完成用户目标时，需要处理冗长的网页观察（通常超过数万个token）。\n*   **后果：** 这导致上下文限制饱和、计算成本增加，并且使代理面临提示注入等安全风险。\n*   **现有策略的局限性：** 当前的修剪策略要么丢弃相关内容，要么保留不相关的上下文，导致行动预测不理想。\n\n**2. FocusAgent 方法介绍**\n\n*   **核心思想：** FocusAgent 是一种简单而有效的方法，它利用一个轻量级 LLM 检索器，根据任务目标从可访问性树（AxTree）观察中提取最相关的行。\n*   **工作原理：** 通过修剪嘈杂和不相关的内容，FocusAgent 能够实现高效推理。\n\n**3. FocusAgent 的优势**\n\n*   **效率提升：** 减少了需要处理的上下文大小。\n*   **安全性增强：** 降低了对注入攻击的脆弱性。\n\n**4. 实验结果与性能**\n\n*   **基准测试：** 在 WorkArena 和 WebArena 基准测试中进行了实验。\n*   **性能表现：** FocusAgent 的性能与强大的基线方法相匹配。\n*   **上下文大小缩减：** 观察大小减少了 50% 以上。\n*   **抗攻击能力：** FocusAgent 的一个变体显著降低了提示注入攻击（包括横幅和弹出攻击）的成功率，同时在无攻击设置下保持了任务成功性能。\n\n**5. 结论**\n\n*   **核心发现：** 针对性的基于 LLM 的检索是一种实用且稳健的策略，可用于构建高效、有效且安全的网络代理。",
      "shortSummary": "FocusAgent 提出了一种简单有效的方法，通过轻量级 LLM 检索器从网页可访问性树中提取最相关信息，以修剪网络代理的大型上下文。该方法解决了 LLM 代理处理冗长网页观察带来的上下文饱和、高成本和提示注入安全风险问题。实验表明，FocusAgent 在保持性能的同时，将观察大小减少了 50% 以上，并显著降低了提示注入攻击的成功率，为构建高效、安全且有效的网络代理提供了实用策略。",
      "translated_title": "FocusAgent：修剪网络代理大型上下文的简单而有效方法",
      "images": [],
      "contentSource": "完整文章",
      "content": "Web agents powered by large language models (LLMs) must process lengthy web page observations to complete user goals; these pages often exceed tens of thousands of tokens. This saturates context limits and increases computational cost processing; moreover, processing full pages exposes agents to security risks such as prompt injection. Existing pruning strategies either discard relevant content or retain irrelevant context, leading to suboptimal action prediction. We introduce FocusAgent, a simple yet effective approach that leverages a lightweight LLM retriever to extract the most relevant lines from accessibility tree (AxTree) observations, guided by task goals. By pruning noisy and irrelevant content, FocusAgent enables efficient reasoning while reducing vulnerability to injection attacks. Experiments on WorkArena and WebArena benchmarks show that FocusAgent matches the performance of strong baselines, while reducing observation size by over 50%. Furthermore, a variant of FocusAgent significantly reduces the success rate of prompt-injection attacks, including banner and pop-up attacks, while maintaining task success performance in attack-free settings. Our results highlight that targeted LLM-based retrieval is a practical and robust strategy for building web agents that are efficient, effective, and secure."
    },
    {
      "title": "CoDA：用于协作数据可视化的智能体系统 (原标题: CoDA: Agentic Systems for Collaborative Data Visualization)",
      "link": "https://arxiv.org/abs/2510.03194",
      "pubDate": "Fri, 03 Oct 2025 13:30:16 GMT",
      "isoDate": "2025-10-03T13:30:16.000Z",
      "creator": "Zichen Chen, Jiefeng Chen, Sercan Ö. Arik, Misha Sra, Tomas Pfister, Jinsung Yoon",
      "summary": "## CoDA：用于协作数据可视化的智能体系统\n\n### 引言\n深度研究已经彻底改变了数据分析，但数据科学家在手动创建可视化方面仍投入大量时间。现有自动化系统（基于自然语言查询）在处理包含多个文件的复杂数据集和迭代优化方面存在困难。目前的单智能体或多智能体方法往往过度简化任务，仅关注初始查询解析，而未能有效管理数据复杂性、代码错误或最终可视化质量。\n\n### CoDA的提出\n本文将这一挑战重新定义为一个协作式多智能体问题，并引入了CoDA系统。CoDA是一个多智能体系统，旨在通过专业化的LLM智能体实现数据可视化自动化。\n\n### CoDA的核心机制\nCoDA系统利用专门的大型语言模型（LLM）智能体来执行以下关键任务，从而形成一个形式化的管道：\n\n*   **元数据分析：** 深入理解数据集的结构和内容，这有助于绕过大型语言模型的token限制。\n*   **任务规划：** 制定生成所需可视化的策略和步骤。\n*   **代码生成：** 根据规划生成实现可视化的代码。\n*   **自我反思：** 评估生成的可视化质量和代码的正确性，并进行迭代改进，确保系统的鲁棒性和最终可视化的高质量。\n\n### 实验结果\nCoDA经过了广泛的评估，结果显示：\n\n*   在整体得分上取得了显著提升。\n*   性能比竞争基线高出多达41.5%。\n\n### 结论与展望\n这项工作表明，可视化自动化的未来不在于孤立的代码生成，而在于集成化的、协作式的智能体工作流。",
      "shortSummary": "CoDA是一个多智能体系统，旨在解决数据科学家手动创建可视化耗时且现有自动化系统效率低下的问题。它通过专业化的LLM智能体进行元数据分析、任务规划、代码生成和自我反思，将数据可视化重构为协作式智能体问题。CoDA的元数据驱动分析和质量驱动优化确保了鲁棒性，并在评估中表现出色，性能比基线高出41.5%。该研究强调了集成协作式智能体工作流在可视化自动化中的重要性。",
      "translated_title": "CoDA：用于协作数据可视化的智能体系统",
      "images": [],
      "contentSource": "完整文章",
      "content": "Deep research has revolutionized data analysis, yet data scientists still devote substantial time to manually crafting visualizations, highlighting the need for robust automation from natural language queries. However, current systems struggle with complex datasets containing multiple files and iterative refinement. Existing approaches, including simple single- or multi-agent systems, often oversimplify the task, focusing on initial query parsing while failing to robustly manage data complexity, code errors, or final visualization quality. In this paper, we reframe this challenge as a collaborative multi-agent problem. We introduce CoDA, a multi-agent system that employs specialized LLM agents for metadata analysis, task planning, code generation, and self-reflection. We formalize this pipeline, demonstrating how metadata-focused analysis bypasses token limits and quality-driven refinement ensures robustness. Extensive evaluations show CoDA achieves substantial gains in the overall score, outperforming competitive baselines by up to 41.5%. This work demonstrates that the future of visualization automation lies not in isolated code generation but in integrated, collaborative agentic workflows."
    },
    {
      "title": "SpineBench：一个由SpineMed-450k语料库驱动的、具有临床显著性且层级感知的基准 (原标题: SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus)",
      "link": "https://arxiv.org/abs/2510.03160",
      "pubDate": "Fri, 03 Oct 2025 12:32:02 GMT",
      "isoDate": "2025-10-03T12:32:02.000Z",
      "creator": "Ming Zhao, Wenhui Dong, Yang Zhang, Xiang Zheng, Zhonghao Zhang, Zian Zhou, Yunzhi Guan, Liukun Xu, Wei Peng, Zhaoyang Gong, Zhicheng Zhang, Dachuan Li, Xiaosheng Ma, Yuli Ma, Jianing Ni, Changjiang Jiang, Lixia Tian, Qixin Chen, Kaishun Xia, Pingping Liu, Tongshun Zhang, Zhiqiang Liu, Zhongan Bi, Chenyang Si, Tiansheng Sun, Caifeng Shan",
      "summary": "# SpineBench：一个由SpineMed-450k语料库驱动的、具有临床显著性且层级感知的基准\n\n## 引言\n\n脊柱疾病在全球范围内影响着6.19亿人，是导致残疾的主要原因之一。然而，AI辅助诊断在这一领域仍受到限制，主要原因在于缺乏层级感知（level-aware）的多模态数据集。脊柱疾病的临床决策需要对X射线、CT和MRI等多种影像模态在特定椎体层级进行复杂的推理。目前，由于缺乏可追溯、具有临床基础的指令数据以及标准化、针对脊柱的基准，该领域的研究进展受到了阻碍。\n\n## SpineMed生态系统\n\n为了解决上述挑战，研究团队推出了SpineMed生态系统，该系统是与执业脊柱外科医生共同设计的，包含以下核心组件：\n\n### 1. SpineMed-450k 数据集\n\n*   **规模与特点**：这是首个大规模数据集，明确设计用于跨影像模态的椎体层级推理，包含超过450,000个指令实例。\n*   **数据来源**：数据集的精心策划来源于多样化的资源，包括教科书、临床指南、开放数据集以及约1,000例去识别化的医院病例。\n*   **数据生成流程**：采用“临床医生参与循环”（clinician-in-the-loop）的管道，结合两阶段大型语言模型（LLM）生成方法（草稿和修订），以确保数据的高质量和可追溯性。\n*   **支持任务**：该数据集旨在支持问答、多轮会诊和报告生成等任务。\n\n### 2. SpineBench 评估框架\n\n*   **设计理念**：SpineBench是一个具有临床基础的评估框架。\n*   **评估维度**：它从临床显著性轴线评估模型，包括层级识别（level identification）、病理评估（pathology assessment）和手术规划（surgical planning）。\n\n## 实验结果与发现\n\n*   **现有模型表现**：对几个近期先进的大型视觉语言模型（LVLMs）在SpineBench上进行的全面评估显示，这些模型在细粒度、层级特定的推理方面存在系统性弱点。\n*   **作者模型表现**：与此形成对比的是，研究团队在SpineMed-450k上微调的模型在所有任务上都表现出持续且显著的改进。\n*   **临床医生评估**：临床医生的评估进一步证实了该模型输出的诊断清晰度和实际应用价值。\n\n## 研究领域\n\n本文的研究内容主要属于以下领域：\n\n*   计算机视觉与模式识别 (cs.CV)\n*   人工智能 (cs.AI)",
      "shortSummary": "针对脊柱疾病AI诊断中缺乏层级感知、多模态数据集及标准化基准的问题，本文推出了SpineMed生态系统。该系统包含SpineMed-450k数据集（首个大规模、专为椎体层级推理设计的数据集，含超45万指令实例）和SpineBench（一个临床导向的评估框架）。研究发现，现有大型视觉语言模型在细粒度推理上存在不足，而基于SpineMed-450k微调的模型在所有任务上均取得显著提升，并获得临床医生认可，证明了其诊断清晰度和实用性。",
      "translated_title": "SpineBench：一个由SpineMed-450k语料库驱动的、具有临床显著性且层级感知的基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Spine disorders affect 619 million people globally and are a leading cause of disability, yet AI-assisted diagnosis remains limited by the lack of level-aware, multimodal datasets. Clinical decision-making for spine disorders requires sophisticated reasoning across X-ray, CT, and MRI at specific vertebral levels. However, progress has been constrained by the absence of traceable, clinically-grounded instruction data and standardized, spine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem co-designed with practicing spine surgeons. It features SpineMed-450k, the first large-scale dataset explicitly designed for vertebral-level reasoning across imaging modalities with over 450,000 instruction instances, and SpineBench, a clinically-grounded evaluation framework. SpineMed-450k is curated from diverse sources, including textbooks, guidelines, open datasets, and ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline with a two-stage LLM generation method (draft and revision) to ensure high-quality, traceable data for question-answering, multi-turn consultations, and report generation. SpineBench evaluates models on clinically salient axes, including level identification, pathology assessment, and surgical planning. Our comprehensive evaluation of several recently advanced large vision-language models (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained, level-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k demonstrates consistent and significant improvements across all tasks. Clinician assessments confirm the diagnostic clarity and practical utility of our model's outputs."
    },
    {
      "title": "SurveyBench：LLM（-代理）撰写学术综述的能力如何？ (原标题: SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?)",
      "link": "https://arxiv.org/abs/2510.03120",
      "pubDate": "Fri, 03 Oct 2025 11:49:09 GMT",
      "isoDate": "2025-10-03T11:49:09.000Z",
      "creator": "Zhaojun Sun, Xuzhou Zhu, Xuanhe Zhou, Xin Tong, Shuo Wang, Jie Fu, Guoliang Li, Zhiyuan Liu, Fan Wu",
      "summary": "## SurveyBench：评估LLM撰写学术综述的能力\n\n### 引言\n学术综述写作是一项耗时且对智力要求极高的任务，需要将大量文献提炼成连贯且富有洞察力的叙述。尽管近期出现了如通用DeepResearch代理和专门的综述生成方法（统称为LLM4Survey），可以自动生成综述，但其产出往往未能达到人类标准。此外，目前缺乏一个严谨、以读者为导向的基准来彻底揭示这些方法的不足。\n\n### 提出SurveyBench\n为了填补这一空白，本文提出了一个名为 **SurveyBench** 的细粒度、测验驱动的评估框架，旨在全面评估大型语言模型（LLM）及其代理在撰写学术综述方面的能力。\n\n### SurveyBench 的主要特点\nSurveyBench 框架具有以下三个核心特点：\n\n1.  **典型综述主题来源**：\n    *   数据来源于近期发表的11,343篇arXiv论文。\n    *   同时参考了4,947篇高质量的综述文章。\n    *   这些主题确保了评估的现实性和相关性。\n\n2.  **多维度指标体系**：\n    *   **大纲质量评估**：考量综述的覆盖广度（coverage breadth）和逻辑连贯性（logical coherence）。\n    *   **内容质量评估**：关注综合粒度（synthesis granularity）和洞察清晰度（clarity of insights）。\n    *   **非文本丰富性**：评估综述中非文本元素的质量和有效性。\n\n3.  **双模式评估协议**：\n    *   **基于内容的评估（Content-based evaluation）**：直接评估综述内容的质量。\n    *   **基于测验的回答能力测试（Quiz-based answerability tests）**：通过设计测验问题来评估综述能否有效满足读者的信息需求。\n    *   这种双模式评估方法明确地与读者的信息需求保持一致。\n\n### 评估结果\n实验结果表明，SurveyBench 能够有效地挑战现有的LLM4Survey方法。例如，在基于内容的评估中，LLM4Survey方法的表现平均比人类低21%。这突显了当前LLM在学术综述写作方面与人类水平之间存在的显著差距。",
      "shortSummary": "为解决LLM生成学术综述质量不足且缺乏严谨评估基准的问题，本文提出了SurveyBench。这是一个细粒度、测验驱动的评估框架，其特点包括：来源于真实论文的综述主题、多维度指标体系（评估大纲、内容和非文本质量）以及与读者需求对齐的双模式评估协议。结果显示，SurveyBench有效揭示了现有LLM4Survey方法的不足，其在内容评估中平均比人类低21%。",
      "translated_title": "SurveyBench：LLM（-代理）撰写学术综述的能力如何？",
      "images": [],
      "contentSource": "完整文章",
      "content": "Academic survey writing, which distills vast literature into a coherent and insightful narrative, remains a labor-intensive and intellectually demanding task. While recent approaches, such as general DeepResearch agents and survey-specialized methods, can generate surveys automatically (a.k.a. LLM4Survey), their outputs often fall short of human standards and there lacks a rigorous, reader-aligned benchmark for thoroughly revealing their deficiencies. To fill the gap, we propose a fine-grained, quiz-driven evaluation framework SurveyBench, featuring (1) typical survey topics source from recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys; (2) a multifaceted metric hierarchy that assesses the outline quality (e.g., coverage breadth, logical coherence), content quality (e.g., synthesis granularity, clarity of insights), and non-textual richness; and (3) a dual-mode evaluation protocol that includes content-based and quiz-based answerability tests, explicitly aligned with readers' informational needs. Results show SurveyBench effectively challenges existing LLM4Survey approaches (e.g., on average 21% lower than human in content-based evaluation)."
    },
    {
      "title": "多模态大型语言模型的自我改进：一项综述 (原标题: Self-Improvement in Multimodal Large Language Models: A Survey)",
      "link": "https://arxiv.org/abs/2510.02665",
      "pubDate": "Thu, 02 Oct 2025 21:48:26 GMT",
      "isoDate": "2025-10-02T21:48:26.000Z",
      "creator": "Shijian Deng, Kai Wang, Tianyu Yang, Harsh Singh, Yapeng Tian",
      "summary": "## 多模态大型语言模型的自我改进：一项综述\n\n### 引言\n\n近期，大型语言模型（LLMs）的自我改进技术取得了显著进展，能够在不显著增加成本（特别是人力成本）的情况下，有效提升模型能力。尽管这一领域仍相对年轻，但将其扩展到多模态领域具有巨大的潜力，能够利用多样化的数据源并开发出更通用的自我改进模型。\n\n### 本综述的贡献\n\n本综述是首次对多模态大型语言模型（MLLMs）中的自我改进进行全面概述。它提供了一个结构化的文献综述，旨在促进MLLMs自我改进领域的进一步发展。\n\n### 研究视角与方法\n\n文章从以下三个主要视角讨论了自我改进方法：\n\n1.  **数据收集**：探讨了如何有效地收集多模态数据以支持模型的自我改进过程。\n2.  **数据组织**：研究了如何结构化和管理收集到的数据，以优化模型的学习和改进效率。\n3.  **模型优化**：分析了用于提升MLLMs性能的各种优化技术和策略。\n\n### 其他内容\n\n综述还包括了常用的评估方法和下游应用，以全面展示该领域的实践情况。\n\n### 开放挑战与未来方向\n\n最后，文章总结并概述了当前面临的开放挑战和未来的研究方向，为该领域的进一步探索提供了指导。",
      "shortSummary": "本综述首次全面审视了多模态大型语言模型（MLLMs）的自我改进技术。文章结构化地探讨了数据收集、数据组织和模型优化三个关键视角下的方法，旨在高效提升模型能力。此外，综述还涵盖了常用的评估方法、下游应用，并指出了开放挑战和未来的研究方向，为该领域的发展提供了清晰的路线图。",
      "translated_title": "多模态大型语言模型的自我改进：一项综述",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in self-improvement for Large Language Models (LLMs) have efficiently enhanced model capabilities without significantly increasing costs, particularly in terms of human effort. While this area is still relatively young, its extension to the multimodal domain holds immense potential for leveraging diverse data sources and developing more general self-improving models. This survey is the first to provide a comprehensive overview of self-improvement in Multimodal LLMs (MLLMs). We provide a structured overview of the current literature and discuss methods from three perspectives: 1) data collection, 2) data organization, and 3) model optimization, to facilitate the further development of self-improvement in MLLMs. We also include commonly used evaluations and downstream applications. Finally, we conclude by outlining open challenges and future research directions."
    },
    {
      "title": "视频模型有多自信？赋能视频模型表达其不确定性 (原标题: How Confident are Video Models? Empowering Video Models to Express their Uncertainty)",
      "link": "https://arxiv.org/abs/2510.02571",
      "pubDate": "Thu, 02 Oct 2025 17:20:41 GMT",
      "isoDate": "2025-10-02T17:20:41.000Z",
      "creator": "Zhiting Mei, Ola Shorinwa, Anirudha Majumdar",
      "summary": "### 视频模型不确定性量化框架：S-QUBED\n\n**背景与问题**\n\n*   **生成式视频模型的“幻觉”问题：** 尽管生成式视频模型在文本到视频生成方面展现出令人印象深刻的能力，并在实际应用中得到广泛采用，但它们与大型语言模型（LLMs）类似，容易产生“幻觉”。这意味着它们可能生成看似合理但事实错误的内容。\n*   **缺乏不确定性量化（UQ）：** 针对LLMs的不确定性量化已得到广泛研究，但目前尚无针对视频模型的UQ方法，这引发了严重的安全担忧。\n\n**本文贡献**\n\n*   **首次尝试量化视频模型不确定性：** 本文是首次致力于量化视频模型不确定性的工作。\n*   **提出不确定性量化框架：** 提出了一个用于生成式视频模型不确定性量化的综合框架。\n\n**框架组成**\n\n该框架包含三个关键组成部分：\n\n1.  **校准评估指标：**\n    *   基于鲁棒秩相关估计，无需严格的模型假设。\n    *   用于评估视频模型的校准程度。\n2.  **黑盒UQ方法（S-QUBED）：**\n    *   利用潜在空间建模，将预测不确定性严格分解为两个核心组成部分：\n        *   **偶然不确定性（Aleatoric Uncertainty）：** 源于数据固有的随机性或噪声。\n        *   **认知不确定性（Epistemic Uncertainty）：** 源于模型知识的不足或模型对数据的理解不充分。\n    *   通过在潜在空间中调节生成任务，S-QUBED能够区分由模糊任务规范引起的不确定性与因模型知识不足引起的不确定性。\n3.  **UQ数据集：**\n    *   专门用于促进视频模型校准的基准测试。\n\n**实验结果**\n\n*   通过在基准视频数据集上进行广泛实验，结果表明：\n    *   S-QUBED计算出的总不确定性估计是经过校准的。\n    *   这些不确定性估计与任务准确性呈负相关，即不确定性越高，准确性越低。\n    *   S-QUBED能够有效地计算并区分偶然不确定性成分和认知不确定性成分。",
      "shortSummary": "生成式视频模型存在“幻觉”问题且缺乏不确定性量化（UQ）方法，引发安全担忧。本文首次提出了一个针对生成式视频模型的UQ框架，包括一个校准评估指标、一个名为S-QUBED的黑盒UQ方法，以及一个UQ数据集。S-QUBED利用潜在空间建模将不确定性分解为偶然性和认知性成分。实验证明，S-QUBED能提供校准的总不确定性估计，并与任务准确性负相关，有效量化了不确定性来源。",
      "translated_title": "视频模型有多自信？赋能视频模型表达其不确定性",
      "images": [],
      "contentSource": "完整文章",
      "content": "Generative video models demonstrate impressive text-to-video capabilities, spurring widespread adoption in many real-world applications. However, like large language models (LLMs), video generation models tend to hallucinate, producing plausible videos even when they are factually wrong. Although uncertainty quantification (UQ) of LLMs has been extensively studied in prior work, no UQ method for video models exists, raising critical safety concerns. To our knowledge, this paper represents the first work towards quantifying the uncertainty of video models. We present a framework for uncertainty quantification of generative video models, consisting of: (i) a metric for evaluating the calibration of video models based on robust rank correlation estimation with no stringent modeling assumptions; (ii) a black-box UQ method for video models (termed S-QUBED), which leverages latent modeling to rigorously decompose predictive uncertainty into its aleatoric and epistemic components; and (iii) a UQ dataset to facilitate benchmarking calibration in video models. By conditioning the generation task in the latent space, we disentangle uncertainty arising due to vague task specifications from that arising from lack of knowledge. Through extensive experiments on benchmark video datasets, we demonstrate that S-QUBED computes calibrated total uncertainty estimates that are negatively correlated with the task accuracy and effectively computes the aleatoric and epistemic constituents."
    },
    {
      "title": "最优控制与流匹配的结合：实现多主体保真度的原理性途径 (原标题: Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity)",
      "link": "https://arxiv.org/abs/2510.02315",
      "pubDate": "Thu, 02 Oct 2025 13:59:58 GMT",
      "isoDate": "2025-10-02T13:59:58.000Z",
      "creator": "Eric Tillmann Bill, Enis Simsar, Thomas Hofmann",
      "summary": "# 最优控制与流匹配的结合：实现多主体保真度的原理性途径\n\n## 引言\n\n当前文本到图像（T2I）模型在处理单一实体提示时表现出色，但在处理包含多个主体的描述时面临显著挑战。这些挑战主要表现为属性泄露、身份纠缠以及主体遗漏等问题，导致生成图像的准确性和保真度下降。\n\n## 核心贡献与理论框架\n\n本文首次提出了一个具有原理性、可优化的理论框架，旨在引导T2I模型的采样动态，以实现更高的多主体保真度。该框架通过随机最优控制（Stochastic Optimal Control, SOC）的视角来审视流匹配（Flow Matching, FM）技术，并将主体解耦问题重新表述为对已训练的FM采样器进行有效控制。\n\n## 提出的两种架构无关算法\n\n基于上述理论框架，本文开发了两种与具体模型架构无关的算法，以解决多主体生成问题：\n\n1.  **无需训练的测试时控制器（Training-free Test-Time Controller）**\n    *   该控制器在测试阶段无需额外训练，通过单次更新即可扰动基础速度场。\n    *   其设计旨在高效运行，能够在商用GPU上实现快速处理。\n\n2.  **伴随匹配（Adjoint Matching）**\n    *   这是一种轻量级的微调规则，通过将一个控制网络回归到反向伴随信号来工作。\n    *   该方法在提升多主体保真度的同时，能够有效保留基础模型的原有能力和风格。\n\n## 通用性与扩展性\n\n本文提出的公式具有广泛的通用性：\n\n*   它能够统一并解释先前用于多主体生成的注意力启发式方法。\n*   通过流-扩散对应关系，该框架可以无缝扩展到扩散模型。\n*   它提供了首个明确为解决多主体保真度问题而设计的微调途径。\n\n## 实验结果与性能\n\n研究团队在多个主流文本到图像模型上进行了实证评估，包括Stable Diffusion 3.5、FLUX和Stable Diffusion XL。实验结果显示：\n\n*   两种算法均持续且显著地改进了多主体对齐效果，同时成功保持了基础模型的原始风格。\n*   无需训练的测试时控制器在商用GPU上表现出高效率。\n*   通过有限提示进行微调的控制器能够有效地泛化到未见的、新的提示上。\n*   文章特别强调了名为FOCUS（Flow Optimal Control for Unentangled Subjects）的方法，该方法在不同模型上均实现了最先进（state-of-the-art）的多主体保真度。\n\n## 结论\n\n本文为解决文本到图像模型在多主体生成方面的挑战提供了一个原理性、可优化的新方法。通过将最优控制与流匹配相结合，该研究显著提升了多主体图像生成的质量和准确性，为未来的T2I模型发展奠定了坚实基础。",
      "shortSummary": "当前文本到图像模型在多主体描述上存在属性泄露和身份纠缠问题。本文提出一个基于随机最优控制和流匹配的理论框架，将主体解耦视为对采样器的控制。研究引入了两种架构无关的算法：一种无需训练的测试时控制器和一种轻量级微调规则（伴随匹配）。实验证明，这些算法在Stable Diffusion等模型上显著提升了多主体对齐效果，同时保持了模型风格，并实现了最先进的多主体保真度。",
      "translated_title": "最优控制与流匹配的结合：实现多主体保真度的原理性途径",
      "images": [],
      "contentSource": "完整文章",
      "content": "Text-to-image (T2I) models excel on single-entity prompts but struggle with multi-subject descriptions, often showing attribute leakage, identity entanglement, and subject omissions. We introduce the first theoretical framework with a principled, optimizable objective for steering sampling dynamics toward multi-subject fidelity. Viewing flow matching (FM) through stochastic optimal control (SOC), we formulate subject disentanglement as control over a trained FM sampler. This yields two architecture-agnostic algorithms: (i) a training-free test-time controller that perturbs the base velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight fine-tuning rule that regresses a control network to a backward adjoint signal while preserving base-model capabilities. The same formulation unifies prior attention heuristics, extends to diffusion models via a flow-diffusion correspondence, and provides the first fine-tuning route explicitly designed for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and Stable Diffusion XL, both algorithms consistently improve multi-subject alignment while maintaining base-model style. Test-time control runs efficiently on commodity GPUs, and fine-tuned controllers trained on limited prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal Control for Unentangled Subjects), which achieves state-of-the-art multi-subject fidelity across models."
    },
    {
      "title": "StealthAttack：通过密度引导幻觉实现鲁棒3D高斯泼溅中毒攻击 (原标题: StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions)",
      "link": "https://arxiv.org/abs/2510.02314",
      "pubDate": "Thu, 02 Oct 2025 13:59:57 GMT",
      "isoDate": "2025-10-02T13:59:57.000Z",
      "creator": "Bo-Hsu Ke, You-Zhe Xie, Yu-Lun Liu, Wei-Chen Chiu",
      "summary": "## StealthAttack：通过密度引导幻觉实现鲁棒3D高斯泼溅中毒攻击\n\n### 背景与问题\n\n*   神经辐射场（NeRF）和3D高斯泼溅（3DGS）等3D场景表示方法在新型视图合成方面取得了显著进展。\n*   随着这些方法的日益普及，解决其潜在漏洞变得至关重要。\n*   本文分析了3DGS在图像级中毒攻击下的鲁棒性。\n\n### 提出的方法：StealthAttack\n\n本文提出了一种新颖的**密度引导中毒方法**，旨在对3DGS进行鲁棒的攻击。\n\n1.  **密度引导的高斯点注入：**\n    *   该方法通过核密度估计（KDE）识别场景中的低密度区域。\n    *   策略性地将高斯点注入这些低密度区域。\n\n2.  **视点依赖的幻觉对象：**\n    *   注入的高斯点能够嵌入**视点依赖的幻觉对象**。\n    *   这些幻觉对象在中毒视图中清晰可见。\n    *   同时，对“无害”视图的影响极小，从而增强了攻击的隐蔽性。\n\n3.  **自适应噪声策略：**\n    *   引入了一种**自适应噪声策略**，以进一步破坏多视图一致性。\n    *   该策略有效增强了攻击的有效性。\n\n### 评估协议\n\n*   提出了一种**基于KDE的评估协议**，用于系统地评估攻击难度。\n*   该协议旨在为未来的研究提供客观的基准。\n\n### 实验结果\n\n*   广泛的实验证明，与现有最先进的技术相比，所提出的方法表现出**卓越的性能**。\n\n### 总结与展望\n\n*   该研究揭示了3DGS等新兴3D表示方法的安全风险。\n*   为未来的防御机制提供了重要的见解。\n*   该论文已被ICCV 2025接收。",
      "shortSummary": "本文提出“StealthAttack”，一种针对3D高斯泼溅（3DGS）的鲁棒中毒攻击方法。该方法利用核密度估计（KDE）在低密度区域策略性注入高斯点，创建视点依赖的幻觉对象，使其在中毒视图中清晰可见，同时对无害视图影响极小。通过引入自适应噪声策略，进一步增强攻击效果。研究还提出了KDE评估协议，并实验证明其优于现有技术，揭示了3DGS的安全漏洞。",
      "translated_title": "StealthAttack：通过密度引导幻觉实现鲁棒3D高斯泼溅中毒攻击",
      "images": [],
      "contentSource": "完整文章",
      "content": "3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As these methods become prevalent, addressing their vulnerabilities becomes critical. We analyze 3DGS robustness against image-level poisoning attacks and propose a novel density-guided poisoning method. Our method strategically injects Gaussian points into low-density regions identified via Kernel Density Estimation (KDE), embedding viewpoint-dependent illusory objects clearly visible from poisoned views while minimally affecting innocent views. Additionally, we introduce an adaptive noise strategy to disrupt multi-view consistency, further enhancing attack effectiveness. We propose a KDE-based evaluation protocol to assess attack difficulty systematically, enabling objective benchmarking for future research. Extensive experiments demonstrate our method's superior performance compared to state-of-the-art techniques. Project page: https://hentci.github.io/stealthattack/"
    },
    {
      "title": "交互式训练：反馈驱动的神经网络优化 (原标题: Interactive Training: Feedback-Driven Neural Network Optimization)",
      "link": "https://arxiv.org/abs/2510.02297",
      "pubDate": "Thu, 02 Oct 2025 13:59:00 GMT",
      "isoDate": "2025-10-02T13:59:00.000Z",
      "creator": "Wentao Zhang, Yang Young Lu, Yuntian Deng",
      "summary": "## 交互式训练：反馈驱动的神经网络优化\n\n### 引言\n传统的神经网络训练通常依赖于固定、预定义的优化方案，这导致其在面对训练过程中的不稳定或新出现的问题时，缺乏动态响应和调整的灵活性。\n\n### 核心概念：交互式训练框架\n本文介绍了一个名为“交互式训练”（Interactive Training）的开源框架。该框架旨在通过人类专家或自动化AI代理的实时、反馈驱动的干预，来优化神经网络的训练过程。\n\n### 工作机制：控制服务器\n交互式训练的核心是一个控制服务器，它在用户（无论是人类专家还是AI代理）与正在进行的神经网络训练过程之间建立通信桥梁。通过这个控制服务器，用户可以动态地调整以下关键参数和元素：\n\n*   **优化器超参数：** 例如学习率、动量等，可以根据训练进展实时修改。\n*   **训练数据：** 允许动态调整或选择训练数据子集，以应对特定问题或改进模型表现。\n*   **模型检查点：** 能够保存、加载或回滚到不同的模型状态，从而更好地管理训练过程。\n\n### 案例研究与优势\n通过三个具体的案例研究，作者展示了交互式训练带来的显著优势：\n\n1.  **卓越的训练稳定性：** 框架能够有效应对训练过程中的波动和不稳定性，确保更平滑、更可靠的收敛。\n2.  **降低对初始超参数的敏感性：** 减少了训练结果对初始超参数选择的依赖，使得模型对超参数的设置不那么挑剔，提高了训练的鲁棒性。\n3.  **提高对不断变化用户需求的适应性：** 允许根据实时反馈和 evolving 的用户需求调整训练策略，使模型能够更好地适应实际应用场景。\n\n### 未来展望\n交互式训练为未来的神经网络训练范式奠定了基础。在这一未来范式中，AI代理将能够自主监控训练日志，主动识别并解决不稳定性，并持续优化训练动态，从而实现更高效、更智能的训练过程。\n\n### 其他信息\n该论文被提交为EMNLP 2025 Demo，并涉及机器学习（cs.LG）、人工智能（cs.AI）和计算与语言（cs.CL）等领域。\n\n*文章内容中未包含任何有效的图片链接，因此详细摘要中不包含图片。*",
      "shortSummary": "“交互式训练”是一个开源框架，旨在通过人类专家或AI代理的实时、反馈驱动干预来优化神经网络训练。它利用控制服务器动态调整超参数、训练数据和模型检查点。通过案例研究，该框架展示了卓越的训练稳定性、降低的初始超参数敏感性以及增强的适应性，预示着AI代理自主监控和优化训练的未来范式。",
      "translated_title": "交互式训练：反馈驱动的神经网络优化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Traditional neural network training typically follows fixed, predefined optimization recipes, lacking the flexibility to dynamically respond to instabilities or emerging training issues. In this paper, we introduce Interactive Training, an open-source framework that enables real-time, feedback-driven intervention during neural network training by human experts or automated AI agents. At its core, Interactive Training uses a control server to mediate communication between users or agents and the ongoing training process, allowing users to dynamically adjust optimizer hyperparameters, training data, and model checkpoints. Through three case studies, we demonstrate that Interactive Training achieves superior training stability, reduced sensitivity to initial hyperparameters, and improved adaptability to evolving user needs, paving the way toward a future training paradigm where AI agents autonomously monitor training logs, proactively resolve instabilities, and optimize training dynamics."
    },
    {
      "title": "VideoNSA：原生稀疏注意力扩展视频理解 (原标题: VideoNSA: Native Sparse Attention Scales Video Understanding)",
      "link": "https://arxiv.org/abs/2510.02295",
      "pubDate": "Thu, 02 Oct 2025 13:58:54 GMT",
      "isoDate": "2025-10-02T13:58:54.000Z",
      "creator": "Enxin Song, Wenhao Chai, Shusheng Yang, Ethan Armand, Xiaojun Shan, Haiyang Xu, Jianwen Xie, Zhuowen Tu",
      "summary": "# VideoNSA：原生稀疏注意力扩展视频理解\n\n## 摘要\n\n当前多模态语言模型在视频理解方面面临上下文长度的限制，这导致模型难以捕捉关键过渡帧，并在长时间尺度上保持连贯性。为了解决这一挑战，本文提出了一种名为 VideoNSA 的方法，该方法将原生稀疏注意力（Native Sparse Attention, NSA）应用于视频-语言模型。\n\n## 方法\n\n*   **模型适应**：VideoNSA 通过在包含21.6万个视频指令的数据集上进行端到端训练，对 Qwen2.5-VL 模型进行了适应性改造。\n*   **混合注意力机制**：该方法采用了一种硬件感知的混合注意力方法：\n    *   对文本部分使用密集注意力（dense attention）。\n    *   对视频部分则采用原生稀疏注意力（NSA）。\n\n## 性能提升\n\n与基于 token 压缩和免训练的稀疏基线方法相比，VideoNSA 在以下方面取得了显著改进：\n\n*   长视频理解\n*   时间推理\n*   空间基准测试\n\n## 消融分析的关键发现\n\n进一步的消融分析揭示了四个关键发现：\n\n1.  **可靠的扩展性**：模型能够可靠地扩展到128K个 token。\n2.  **最优的注意力分配**：在固定预算下，实现了全局-局部注意力的最佳分配。\n3.  **任务依赖的分支使用模式**：不同的任务表现出不同的注意力分支使用模式。\n4.  **动态注意力汇聚**：可学习的组合稀疏注意力有助于诱导动态注意力汇聚（attention sinks）。",
      "shortSummary": "多模态语言模型在长视频理解中受限于上下文长度。VideoNSA通过将原生稀疏注意力（NSA）应用于Qwen2.5-VL模型，解决了这一问题。它采用混合注意力机制，对文本使用密集注意力，对视频使用NSA，并在21.6万视频指令数据集上进行端到端训练。VideoNSA显著提升了长视频理解、时间推理和空间基准测试的性能，并能可靠地扩展到128K token。",
      "translated_title": "VideoNSA：原生稀疏注意力扩展视频理解",
      "images": [],
      "contentSource": "完整文章",
      "content": "Video understanding in multimodal language models remains limited by context length: models often miss key transition frames and struggle to maintain coherence across long time scales. To address this, we adapt Native Sparse Attention (NSA) to video-language models. Our method, VideoNSA, adapts Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We employ a hardware-aware hybrid approach to attention, preserving dense attention for text, while employing NSA for video. Compared to token-compression and training-free sparse baselines, VideoNSA achieves improved performance on long-video understanding, temporal reasoning, and spatial benchmarks. Further ablation analysis reveals four key findings: (1) reliable scaling to 128K tokens; (2) an optimal global-local attention allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4) the learnable combined sparse attention help induce dynamic attention sinks."
    },
    {
      "title": "F2LLM 技术报告：利用 600 万开源数据匹敌 SOTA 嵌入性能 (原标题: F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data)",
      "link": "https://arxiv.org/abs/2510.02294",
      "pubDate": "Thu, 02 Oct 2025 13:58:49 GMT",
      "isoDate": "2025-10-02T13:58:49.000Z",
      "creator": "Ziyin Zhang, Zihan Liao, Hang Yu, Peng Di, Rui Wang",
      "summary": "## F2LLM 技术报告：利用 600 万开源数据匹敌 SOTA 嵌入性能\n\n### 介绍\n本文介绍了 F2LLM (Foundation to Feature Large Language Models) 系列模型，这是一套包含 0.6B、1.7B 和 4B 三种尺寸的先进嵌入模型。\n\n### 核心创新与优势\nF2LLM 的独特之处在于其训练方法和数据来源，使其在训练成本、模型大小和嵌入性能之间取得了卓越的平衡：\n\n*   **简化训练流程**：与以往需要大规模对比预训练、复杂训练管道和昂贵合成数据的顶级嵌入模型不同，F2LLM 直接从基础模型进行微调。\n*   **开源非合成数据**：F2LLM 使用了从开源、非合成数据集中精心整理的 600 万个查询-文档-负样本三元组进行训练。\n*   **成本效益高**：这种方法显著降低了训练成本，同时保持了最先进的嵌入性能。\n\n### 性能表现\nF2LLM 模型在 MTEB 英文排行榜上展现了出色的性能：\n\n*   **F2LLM-4B**：在参数量约为 4B 的模型中排名第 2，在所有模型中总体排名第 7。\n*   **F2LLM-1.7B**：在 1B-2B 参数范围的模型中排名第 1。\n\n### 开放资源与未来展望\n为了促进该领域的未来研究，F2LLM 团队采取了开放策略：\n\n*   **资源发布**：公开发布了 F2LLM 模型、训练数据集和相关代码。\n*   **基线定位**：F2LLM 被定位为一个强大、可复现且经济高效的基线，旨在为未来的研究工作提供便利和参考。\n\n### 相关领域\n该研究主要涉及计算与语言 (cs.CL) 和人工智能 (cs.AI) 领域。",
      "shortSummary": "F2LLM 引入了一系列最先进的嵌入模型（0.6B、1.7B、4B），通过使用 600 万个开源、非合成数据直接对基础模型进行微调，实现了与 SOTA 模型匹敌的性能。与传统方法不同，F2LLM 避免了大规模对比预训练和昂贵的合成数据。在 MTEB 英文排行榜上，F2LLM-4B 总体排名第 7，F2LLM-1.7B 在其尺寸范围内排名第 1。该项目发布了模型、数据集和代码，旨在提供一个可复现且经济高效的基线。",
      "translated_title": "F2LLM 技术报告：利用 600 万开源数据匹敌 SOTA 嵌入性能",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce F2LLM - Foundation to Feature Large Language Models, a suite of state-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike previous top-ranking embedding models that require massive contrastive pretraining, sophisticated training pipelines, and costly synthetic training data, F2LLM is directly finetuned from foundation models on 6 million query-document-negative tuples curated from open-source, non-synthetic datasets, striking a strong balance between training cost, model size, and embedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd among models with approximately 4B parameters and 7th overall, while F2LLM-1.7B ranks 1st among models in the 1B-2B size range. To facilitate future research in the field, we release the models, training dataset, and code, positioning F2LLM as a strong, reproducible, and budget-friendly baseline for future works."
    },
    {
      "title": "基于树的对话强化策略优化用于红队攻击 (原标题: Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks)",
      "link": "https://arxiv.org/abs/2510.02286",
      "pubDate": "Thu, 02 Oct 2025 13:57:05 GMT",
      "isoDate": "2025-10-02T13:57:05.000Z",
      "creator": "Ruohao Guo, Afshin Oroojlooy, Roshan Sridhar, Miguel Ballesteros, Alan Ritter, Dan Roth",
      "summary": "### 基于树的对话强化策略优化用于红队攻击 (DialTree-RPO)\n\n本文介绍了一种名为 DialTree-RPO 的新方法，旨在解决大型语言模型（LLMs）在多轮交互中面临的对抗性攻击问题。\n\n#### 现有挑战\n\n*   **LLMs的脆弱性：** 尽管AI安全取得了快速进展，但当前的大型语言模型在多轮交互设置中仍然容易受到对抗性攻击。攻击者可以策略性地调整其提示，这构成了更关键且现实的挑战。\n*   **现有方法的局限性：**\n    *   **人工红队：** 依赖人类专家进行手动红队攻击。\n    *   **自动化方法：** 使用预定义模板和人工策划的攻击数据，且大多数方法侧重于单轮攻击。\n    *   **探索空间不足：** 这些方法未能探索多轮攻击的广阔空间，无法发现从复杂对话动态和策略性对话规划中出现的新颖攻击轨迹。\n*   **多轮攻击的严重性：** 最近的研究发现，LLMs对多轮攻击的脆弱性显著高于单轮攻击，这使得上述局限性尤为关键。\n\n#### 提出的解决方案：DialTree-RPO\n\n*   **核心思想：** DialTree-RPO 是一个基于策略（on-policy）的强化学习框架，它集成了树搜索功能。\n*   **工作原理：**\n    *   **对话建模：** 将对话视为一个序列决策问题。\n    *   **自主发现：** 能够自主发现多样化的多轮攻击策略。\n    *   **系统探索：** 无需手动策划数据即可实现系统性的探索。\n    *   **优化目标：** 通过学习最优对话策略，最大化多轮攻击的成功率。\n\n#### 实验结果\n\n*   **攻击成功率（ASR）提升：** 相比于现有最先进的方法，DialTree-RPO 在10个目标模型上的攻击成功率（ASR）提高了超过25.9%。\n*   **新攻击策略的发现：** 该方法有效地揭示了新的攻击策略，证明了其在复杂对话场景中学习最优攻击策略的能力。\n\n#### 相关领域\n\n*   机器学习 (cs.LG)\n*   人工智能 (cs.AI)\n*   计算与语言 (cs.CL)",
      "shortSummary": "本文提出了DialTree-RPO，一个结合树搜索的基于策略强化学习框架，用于自主发现多轮红队攻击策略。针对大型语言模型在多轮交互中易受攻击且现有方法不足的挑战，DialTree-RPO将对话视为序列决策问题，无需人工数据即可系统探索攻击空间。实验表明，该方法在10个目标模型上将攻击成功率提高了25.9%以上，并能有效揭示新的攻击策略。",
      "translated_title": "基于树的对话强化策略优化用于红队攻击",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite recent rapid progress in AI safety, current large language models remain vulnerable to adversarial attacks in multi-turn interaction settings, where attackers strategically adapt their prompts across conversation turns and pose a more critical yet realistic challenge. Existing approaches that discover safety vulnerabilities either rely on manual red-teaming with human experts or employ automated methods using pre-defined templates and human-curated attack data, with most focusing on single-turn attacks. However, these methods did not explore the vast space of possible multi-turn attacks, failing to consider novel attack trajectories that emerge from complex dialogue dynamics and strategic conversation planning. This gap is particularly critical given recent findings that LLMs exhibit significantly higher vulnerability to multi-turn attacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy reinforcement learning framework integrated with tree search that autonomously discovers diverse multi-turn attack strategies by treating the dialogue as a sequential decision-making problem, enabling systematic exploration without manually curated data. Through extensive experiments, our approach not only achieves more than 25.9% higher ASR across 10 target models compared to previous state-of-the-art approaches, but also effectively uncovers new attack strategies by learning optimal dialogue policies that maximize attack success across multiple turns."
    },
    {
      "title": "Self-Forcing++: 迈向分钟级高质量视频生成 (原标题: Self-Forcing++: Towards Minute-Scale High-Quality Video Generation)",
      "link": "https://arxiv.org/abs/2510.02283",
      "pubDate": "Thu, 02 Oct 2025 13:55:42 GMT",
      "isoDate": "2025-10-02T13:55:42.000Z",
      "creator": "Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, Cho-Jui Hsieh",
      "summary": "## Self-Forcing++: 迈向分钟级高质量视频生成\n\n### 引言\n\n扩散模型在图像和视频生成领域取得了突破性进展，实现了前所未有的视觉质量。然而，它们对Transformer架构的依赖导致在生成长视频时计算成本过高。尽管现有研究探索了长视频生成的自回归方法，通常通过短时程双向教师模型进行蒸馏，但由于教师模型无法合成长视频，学生模型在超出其训练范围外推时，常因连续潜在空间中的错误累积而导致质量显著下降。\n\n### 提出的方法：Self-Forcing++\n\n本文提出了一种简单而有效的方法——Self-Forcing++，旨在缓解长时程视频生成中的质量下降问题，且无需长视频教师模型的监督或在长视频数据集上进行重新训练。该方法的核心思想在于：\n\n*   **利用教师知识：** 充分利用教师模型丰富的知识。\n*   **指导学生模型：** 通过从自生成的长视频中采样的片段，为学生模型提供指导。\n\n### 主要优势与成果\n\nSelf-Forcing++方法展现了多项显著优势和成果：\n\n*   **时间一致性：** 能够有效保持视频的时间一致性。\n*   **视频长度扩展：** 将视频长度扩展到教师模型能力的20倍以上。\n*   **问题规避：** 避免了常见的过曝和错误累积问题，且无需像以往方法那样重新计算重叠帧。\n*   **生成能力：** 在扩展计算能力后，该方法能够生成长达4分15秒的视频，这相当于基础模型位置嵌入支持的最大跨度的99.9%，比基线模型长50倍以上。\n*   **性能表现：** 在标准基准测试和本文提出的改进基准测试中，Self-Forcing++在保真度（fidelity）和一致性（consistency）方面均显著优于基线方法。",
      "shortSummary": "扩散模型在长视频生成中面临高计算成本和质量下降问题。Self-Forcing++提出一种新方法，无需长视频教师或重新训练，通过利用教师模型知识指导学生模型，从自生成的长视频中采样片段。该方法显著提升了长视频生成质量和时间一致性，将视频长度扩展至教师模型能力的20倍以上，并能生成长达4分15秒的视频，有效避免了错误累积，在保真度和一致性上均优于基线方法。",
      "translated_title": "Self-Forcing++: 迈向分钟级高质量视频生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Diffusion models have revolutionized image and video generation, achieving unprecedented visual quality. However, their reliance on transformer architectures incurs prohibitively high computational costs, particularly when extending generation to long videos. Recent work has explored autoregressive formulations for long video generation, typically by distilling from short-horizon bidirectional teachers. Nevertheless, given that teacher models cannot synthesize long videos, the extrapolation of student models beyond their training horizon often leads to pronounced quality degradation, arising from the compounding of errors within the continuous latent space. In this paper, we propose a simple yet effective approach to mitigate quality degradation in long-horizon video generation without requiring supervision from long-video teachers or retraining on long video datasets. Our approach centers on exploiting the rich knowledge of teacher models to provide guidance for the student model through sampled segments drawn from self-generated long videos. Our method maintains temporal consistency while scaling video length by up to 20x beyond teacher's capability, avoiding common issues such as over-exposure and error-accumulation without recomputing overlapping frames like previous methods. When scaling up the computation, our method shows the capability of generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the maximum span supported by our base model's position embedding and more than 50x longer than that of our baseline model. Experiments on standard benchmarks and our proposed improved benchmark demonstrate that our approach substantially outperforms baseline methods in both fidelity and consistency. Our long-horizon videos demo can be found at https://self-forcing-plus-plus.github.io/"
    },
    {
      "title": "并行缩放定律：通过跨语言视角揭示推理泛化能力 (原标题: Parallel Scaling Law: Unveiling Reasoning Generalization through A Cross-Linguistic Perspective)",
      "link": "https://arxiv.org/abs/2510.02272",
      "pubDate": "Thu, 02 Oct 2025 13:49:49 GMT",
      "isoDate": "2025-10-02T13:49:49.000Z",
      "creator": "Wen Yang, Junhong Wu, Chong Li, Chengqing Zong, Jiajun Zhang",
      "summary": "## 跨语言视角下的推理泛化能力研究\n\n### 摘要\n\n近期强化后训练（RPT）的进展显著提升了大型推理模型（LRMs）的能力，激发了对基于强化学习的推理泛化能力的兴趣。现有研究主要集中于探究其在任务或模态间的泛化，而本研究则提出了一种新颖的跨语言视角来调查推理泛化能力，核心问题是：从英语RPT获得的推理能力能否有效迁移到其他语言？\n\n### 研究方法\n\n1.  **系统评估**：在多语言推理基准上系统评估以英语为中心的LRMs。\n2.  **引入新指标**：提出一个新指标来量化跨语言可迁移性。\n3.  **干预研究**：通过干预研究，探究模型初始英语能力对跨语言泛化的影响。\n4.  **并行训练研究**：进行深入的并行训练研究，以理解其对跨语言推理迁移的作用。\n\n### 主要发现\n\n本研究通过实验结果揭示了以下关键发现：\n\n*   **跨语言可迁移性差异显著**：跨语言可迁移性因初始模型、目标语言和训练范式而异。\n*   **英语能力过强导致泛化减弱**：研究发现，初始英语能力越强的模型，越倾向于过度依赖英语特有的模式，从而导致其跨语言泛化能力下降。\n*   **“首次并行飞跃”（First-Parallel Leap）**：从单语言训练过渡到仅使用一种并行语言进行训练时，模型的性能会实现显著提升。\n*   **“并行缩放定律”（Parallel Scaling Law）**：跨语言推理迁移遵循幂律关系，其性能与并行训练语言的数量呈幂律关系。\n*   **“单语言泛化差距”（Monolingual Generalization Gap）**：实际单语言性能与幂律预测之间存在差异，这表明以英语为中心的LRMs未能完全实现跨语言泛化。\n\n### 研究意义\n\n本研究挑战了LRM推理能力反映人类认知的假设，并为开发更具语言无关性的LRMs提供了关键见解。",
      "shortSummary": "本研究从跨语言视角探究大型推理模型（LRMs）的推理泛化能力。发现以英语为中心的LRMs的推理能力向其他语言迁移存在显著差异，且初始英语能力过强反而会削弱跨语言泛化。研究揭示了“首次并行飞跃”现象和“并行缩放定律”，即跨语言推理迁移遵循与并行训练语言数量相关的幂律。同时，指出了“单语言泛化差距”，表明现有LRMs未能完全实现跨语言泛化，为开发更语言无关的LRMs提供了重要见解。",
      "translated_title": "并行缩放定律：通过跨语言视角揭示推理泛化能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in Reinforcement Post-Training (RPT) have significantly enhanced the capabilities of Large Reasoning Models (LRMs), sparking increased interest in the generalization of RL-based reasoning. While existing work has primarily focused on investigating its generalization across tasks or modalities, this study proposes a novel cross-linguistic perspective to investigate reasoning generalization. This raises a crucial question: Does the reasoning capability achieved from English RPT effectively transfer to other languages? We address this by systematically evaluating English-centric LRMs on multilingual reasoning benchmarks and introducing a metric to quantify cross-lingual transferability. Our findings reveal that cross-lingual transferability varies significantly across initial model, target language, and training paradigm. Through interventional studies, we find that models with stronger initial English capabilities tend to over-rely on English-specific patterns, leading to diminished cross-lingual generalization. To address this, we conduct a thorough parallel training study. Experimental results yield three key findings: First-Parallel Leap, a substantial leap in performance when transitioning from monolingual to just a single parallel language, and a predictable Parallel Scaling Law, revealing that cross-lingual reasoning transfer follows a power-law with the number of training parallel languages. Moreover, we identify the discrepancy between actual monolingual performance and the power-law prediction as Monolingual Generalization Gap, indicating that English-centric LRMs fail to fully generalize across languages. Our study challenges the assumption that LRM reasoning mirrors human cognition, providing critical insights for the development of more language-agnostic LRMs."
    },
    {
      "title": "RLAD：训练大型语言模型发现抽象以解决推理问题 (原标题: RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems)",
      "link": "https://arxiv.org/abs/2510.02263",
      "pubDate": "Thu, 02 Oct 2025 13:44:23 GMT",
      "isoDate": "2025-10-02T13:44:23.000Z",
      "creator": "Yuxiao Qu, Anikait Singh, Yoonho Lee, Amrith Setlur, Ruslan Salakhutdinov, Chelsea Finn, Aviral Kumar",
      "summary": "## RLAD：通过抽象提升大型语言模型（LLM）的推理能力\n\n### 核心问题\n\n当前的大型语言模型（LLMs）在解决推理问题时，尽管经过强化学习（RL）的后训练，但往往难以持续地捕捉和重用“算法程序”。它们倾向于进行冗长且退化的探索，而非有效地识别和利用相关的基本要素、中间结果或共享过程来推导复杂问题的答案。这限制了LLMs超越简单的模式匹配或解决方案记忆的能力。\n\n### 解决方案：推理抽象\n\n为了解决这一问题，研究引入了“推理抽象”（reasoning abstractions）。这些抽象是程序性和事实性知识的简洁自然语言描述，旨在为模型提供清晰的指导，使其能够学习并采纳成功的推理路径。\n\n### RLAD训练范式\n\nRLAD（Reinforcement Learning for Abstraction Discovery）是一种创新的双玩家强化学习训练范式，它共同训练两个关键组件：\n\n1.  **抽象生成器（Abstraction Generator）：** 负责根据给定的问题提出多个潜在的推理抽象。\n2.  **解决方案生成器（Solution Generator）：** 负责利用这些抽象所提供的信息来构建问题的解决方案。\n\n这种联合训练机制带来了多项显著优势：\n\n*   **结构化探索：** 抽象的引入为模型的探索过程提供了明确的结构和方向，避免了无目的的漫游。\n*   **解耦学习信号：** 它将抽象的提出与解决方案的生成这两个过程的学习信号解耦，使得每个组件都能更有效地学习其特定任务。\n*   **提高泛化能力：** 这种方法显著提高了模型对更难问题的泛化能力，使其能够更好地应对未见过的复杂推理场景。\n\n### 关键发现\n\n研究还发现，在测试阶段，将更多的计算资源分配给生成高质量的抽象，比简单地生成更多的解决方案更能有效地提升模型的整体性能。这有力地证明了抽象在引导有意义探索和提高推理效率方面所扮演的关键角色。",
      "shortSummary": "RLAD提出了一种新的双玩家强化学习范式，旨在训练大型语言模型（LLMs）发现“推理抽象”。这些抽象是简洁的自然语言描述，用于指导模型学习有效的推理过程。通过联合训练抽象生成器和解决方案生成器，RLAD实现了结构化探索，解耦了学习信号，并显著提高了模型解决复杂推理问题的泛化能力和性能。研究表明，分配更多计算资源用于生成抽象能更有效提升性能。",
      "translated_title": "RLAD：训练大型语言模型发现抽象以解决推理问题",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reasoning requires going beyond pattern matching or memorization of solutions to identify and implement \"algorithmic procedures\" that can be used to deduce answers to hard problems. Doing so requires realizing the most relevant primitives, intermediate results, or shared procedures, and building upon them. While RL post-training on long chains of thought ultimately aims to uncover this kind of algorithmic behavior, most reasoning traces learned by large models fail to consistently capture or reuse procedures, instead drifting into verbose and degenerate exploration. To address more effective reasoning, we introduce reasoning abstractions: concise natural language descriptions of procedural and factual knowledge that guide the model toward learning successful reasoning. We train models to be capable of proposing multiple abstractions given a problem, followed by RL that incentivizes building a solution while using the information provided by these abstractions. This results in a two-player RL training paradigm, abbreviated as RLAD, that jointly trains an abstraction generator and a solution generator. This setup effectively enables structured exploration, decouples learning signals of abstraction proposal and solution generation, and improves generalization to harder problems. We also show that allocating more test-time compute to generating abstractions is more beneficial for performance than generating more solutions at large test budgets, illustrating the role of abstractions in guiding meaningful exploration."
    },
    {
      "title": "Transformer在没有图先验的情况下发现分子结构 (原标题: Transformers Discover Molecular Structure Without Graph Priors)",
      "link": "https://arxiv.org/abs/2510.02259",
      "pubDate": "Thu, 02 Oct 2025 13:42:10 GMT",
      "isoDate": "2025-10-02T13:42:10.000Z",
      "creator": "Tobias Kreiman, Yutong Bai, Fadi Atieh, Elizabeth Weaver, Eric Qu, Aditi S. Krishnapriyan",
      "summary": "# Transformer在没有图先验的情况下发现分子结构\n\n## 引言\n\n图神经网络（GNNs）是目前分子机器学习领域的主流架构，尤其在分子性质预测和机器学习原子间势（MLIPs）方面表现突出。GNNs通过在预定义图上进行消息传递来工作，这些图通常由固定半径截止或k近邻方案生成。尽管这种设计与许多分子任务中存在的局部性相符，但硬编码的图可能因其固定的感受野而限制模型的表达能力，并且稀疏图操作会减慢推理速度。\n\n## 研究目的\n\n本研究旨在探究纯粹的、未经修改的Transformer模型，在直接使用笛卡尔坐标进行训练的情况下（不依赖预定义的图或任何物理先验），是否能够有效地近似分子的能量和力。\n\n## 主要发现与结果\n\n*   **性能表现**：研究人员展示了如何训练一个Transformer模型，使其在与最先进的等变GNN模型在OMol25数据集上进行匹配计算预算的训练后，能够达到具有竞争力的能量和力平均绝对误差（MAE）。\n*   **物理一致性学习**：研究发现，Transformer模型能够学习到物理上一致的模式，例如注意力权重与原子间距离呈反比衰减的规律。\n*   **适应性与灵活性**：由于模型中没有硬编码的偏置，Transformer能够灵活地将这些学习到的模式适应于不同的分子环境。\n*   **可扩展性**：使用标准Transformer模型还带来了可预测的性能提升，这与在其他领域观察到的经验性缩放定律保持一致。\n\n## 结论\n\n研究结果表明，GNN的许多有利特性可以在Transformer模型中自适应地涌现。这挑战了硬编码图归纳偏置在分子建模中的必要性，并为该领域指明了标准化、可扩展的架构方向。",
      "shortSummary": "本文研究表明，纯粹的Transformer模型在直接使用笛卡尔坐标训练时，无需预定义图或物理先验，也能有效近似分子能量和力。在OMol25数据集上，Transformer达到了与先进GNN相当的性能，并能自适应地学习到物理一致的模式（如注意力权重与原子间距离的反比关系）。这挑战了硬编码图归纳偏置的必要性，为分子建模提供了标准化、可扩展的新架构方向。",
      "translated_title": "Transformer在没有图先验的情况下发现分子结构",
      "images": [],
      "contentSource": "完整文章",
      "content": "Graph Neural Networks (GNNs) are the dominant architecture for molecular machine learning, particularly for molecular property prediction and machine learning interatomic potentials (MLIPs). GNNs perform message passing on predefined graphs often induced by a fixed radius cutoff or k-nearest neighbor scheme. While this design aligns with the locality present in many molecular tasks, a hard-coded graph can limit expressivity due to the fixed receptive field and slows down inference with sparse graph operations. In this work, we investigate whether pure, unmodified Transformers trained directly on Cartesian coordinatesx2013without predefined graphs or physical priorsx2013can approximate molecular energies and forces. As a starting point for our analysis, we demonstrate how to train a Transformer to competitive energy and force mean absolute errors under a matched training compute budget, relative to a state-of-the-art equivariant GNN on the OMol25 dataset. We discover that the Transformer learns physically consistent patternsx2013such as attention weights that decay inversely with interatomic distancex2013and flexibly adapts them across different molecular environments due to the absence of hard-coded biases. The use of a standard Transformer also unlocks predictable improvements with respect to scaling training resources, consistent with empirical scaling laws observed in other domains. Our results demonstrate that many favorable properties of GNNs can emerge adaptively in Transformers, challenging the necessity of hard-coded graph inductive biases and pointing toward standardized, scalable architectures for molecular modeling."
    },
    {
      "title": "DragFlow：利用基于区域的监督释放DiT先验进行拖拽编辑 (原标题: DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing)",
      "link": "https://arxiv.org/abs/2510.02253",
      "pubDate": "Thu, 02 Oct 2025 13:39:13 GMT",
      "isoDate": "2025-10-02T13:39:13.000Z",
      "creator": "Zihan Zhou, Shilin Lu, Shuli Leng, Shaocong Zhang, Zhuming Lian, Xinlei Yu, Adams Wai-Kin Kong",
      "summary": "### DragFlow：利用基于区域的监督释放DiT先验进行拖拽编辑\n\n本文介绍了DragFlow，一个旨在解决拖拽式图像编辑中长期存在的失真问题的框架。传统上，由于早期基础模型（如Stable Diffusion）的先验知识不足以将优化后的潜在表示投影回自然图像流形，导致目标区域出现失真。\n\n#### 背景与问题\n\n*   **生成模型演进**：图像生成模型已从基于UNet的DDPMs转向更具可扩展性的DiT（Diffusion Transformers）与流匹配技术（如SD3.5, FLUX），这显著增强了生成先验能力，推动了各种编辑任务的进步。\n*   **拖拽编辑的滞后**：尽管DiT带来了更强的先验，但拖拽式编辑尚未从中受益。\n*   **直接应用DiT的挑战**：研究发现，直接将基于点的拖拽编辑应用于DiT模型效果不佳。与UNet高度压缩的特征不同，DiT的特征结构不足以提供可靠的逐点运动监督。\n\n#### DragFlow的创新方法\n\n为了克服上述限制，DragFlow提出了以下关键创新点：\n\n1.  **区域基编辑范式**：引入了基于区域的编辑范式，利用仿射变换实现更丰富、更一致的特征监督，解决了DiT特征结构不足的问题。\n2.  **个性化适配器集成**：集成了预训练的开放域个性化适配器（例如IP-Adapter），以增强主体一致性。\n3.  **背景保真度**：通过基于梯度掩码的硬约束来保持背景的保真度。\n4.  **多模态大语言模型（MLLMs）**：进一步利用MLLMs来解决任务中的歧义。\n\n#### 评估与成果\n\n*   **新型基准**：为了进行评估，研究团队策划了一个新的“基于区域的拖拽基准”（ReD Bench），该基准包含区域级别的拖拽指令。\n*   **卓越性能**：在DragBench-DR和ReD Bench上的大量实验表明，DragFlow超越了基于点和基于区域的基线方法，在拖拽式图像编辑领域树立了新的技术标杆（state-of-the-art）。\n\n#### 可用性\n\n代码和数据集将在论文发表后公开发布。",
      "shortSummary": "DragFlow是一个利用DiT（Diffusion Transformers）强大先验进行拖拽式图像编辑的新框架。它解决了传统拖拽编辑中因模型先验不足导致的失真问题，并克服了直接将点基编辑应用于DiT的局限性。DragFlow引入了区域基编辑范式，结合仿射变换、个性化适配器和MLLMs，实现了更丰富、一致的特征监督。在DragBench-DR和新策的ReD Bench上，DragFlow表现优于现有基线，达到了拖拽式图像编辑的最新技术水平。",
      "translated_title": "DragFlow：利用基于区域的监督释放DiT先验进行拖拽编辑",
      "images": [],
      "contentSource": "完整文章",
      "content": "Drag-based image editing has long suffered from distortions in the target region, largely because the priors of earlier base models, Stable Diffusion, are insufficient to project optimized latents back onto the natural image manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow matching (e.g., SD3.5, FLUX), generative priors have become significantly stronger, enabling advances across diverse editing tasks. However, drag-based editing has yet to benefit from these stronger priors. This work proposes the first framework to effectively harness FLUX's rich prior for drag-based editing, dubbed DragFlow, achieving substantial gains over baselines. We first show that directly applying point-based drag editing to DiTs performs poorly: unlike the highly compressed features of UNets, DiT features are insufficiently structured to provide reliable guidance for point-wise motion supervision. To overcome this limitation, DragFlow introduces a region-based editing paradigm, where affine transformations enable richer and more consistent feature supervision. Additionally, we integrate pretrained open-domain personalization adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving background fidelity through gradient mask-based hard constraints. Multimodal large language models (MLLMs) are further employed to resolve task ambiguities. For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench) featuring region-level dragging instructions. Extensive experiments on DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and region-based baselines, setting a new state-of-the-art in drag-based image editing. Code and datasets will be publicly available upon publication."
    },
    {
      "title": "规模化代理在计算机使用中的不合理有效性 (原标题: The Unreasonable Effectiveness of Scaling Agents for Computer Use)",
      "link": "https://arxiv.org/abs/2510.02250",
      "pubDate": "Thu, 02 Oct 2025 13:37:08 GMT",
      "isoDate": "2025-10-02T13:37:08.000Z",
      "creator": "Gonzalo Gonzalez-Pumariega, Vincent Tu, Chih-Lun Lee, Jiachen Yang, Ang Li, Xin Eric Wang",
      "summary": "# 规模化代理在计算机使用中的不合理有效性：Behavior Best-of-N (bBoN) 方法\n\n## 背景与挑战\n*   **计算机使用代理（CUAs）的潜力与局限**：\n    *   CUAs在自动化日常数字任务方面展现出巨大潜力。\n    *   然而，其固有的不可靠性和高变异性严重阻碍了它们在需要长期规划和复杂操作的任务中的应用。\n\n## 解决方案：Behavior Best-of-N (bBoN)\n*   **方法介绍**：\n    *   我们引入了一种名为Behavior Best-of-N (bBoN) 的新方法，旨在通过对多个代理进行规模化来解决CUAs的可靠性问题。\n    *   bBoN的核心机制是生成多个任务执行轨迹（rollouts），并利用描述这些代理行为的“行为叙事”（behavior narratives）从中进行选择。\n*   **bBoN的优势**：\n    *   **广泛探索与原则性选择**：该方法既能实现广泛的任务空间探索，又能进行有原则的轨迹选择。\n    *   **显著提升性能**：通过这种机制，bBoN能够大幅提高CUAs的鲁棒性（robustness）和任务成功率（success rates）。\n\n## 实验结果与性能\n*   **OSWorld基准测试**：\n    *   在OSWorld基准测试中，bBoN方法取得了69.9%的最新技术水平（SoTA），显著优于现有方法。\n    *   其性能已接近人类水平（72%）。\n    *   全面的消融实验（ablations）验证了关键设计选择的有效性。\n*   **泛化能力**：\n    *   bBoN在WindowsAgentArena和AndroidWorld等不同操作系统上展示了强大的泛化能力，证明了其方法的通用性。\n\n## 核心洞察\n*   **规模化CUAs的有效性**：研究结果突出强调了在正确实施时，规模化CUAs所展现出的“不合理有效性”。\n*   **有效规模化的关键**：成功的规模化策略需要结构化的轨迹理解和选择机制，而bBoN提供了一个实现这一目标的实用框架。",
      "shortSummary": "计算机使用代理（CUAs）因不可靠性难以处理复杂任务。本研究提出Behavior Best-of-N (bBoN) 方法，通过生成多条执行轨迹并基于行为叙事进行选择，显著提升了CUAs的鲁棒性和成功率。bBoN在OSWorld上取得了69.9%的最新技术水平，接近人类表现，并展现出强大的泛化能力。这表明，正确的规模化策略（如bBoN）能极大地提高CUAs的有效性。",
      "translated_title": "规模化代理在计算机使用中的不合理有效性",
      "images": [],
      "contentSource": "完整文章",
      "content": "Computer-use agents (CUAs) hold promise for automating everyday digital tasks, but their unreliability and high variance hinder their application to long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method that scales over agents by generating multiple rollouts and selecting among them using behavior narratives that describe the agents' rollouts. It enables both wide exploration and principled trajectory selection, substantially improving robustness and success rates. On OSWorld, our bBoN scaling method establishes a new state of the art (SoTA) at 69.9%, significantly outperforming prior methods and approaching human-level performance at 72%, with comprehensive ablations validating key design choices. We further demonstrate strong generalization results to different operating systems on WindowsAgentArena and AndroidWorld. Crucially, our results highlight the unreasonable effectiveness of scaling CUAs, when you do it right: effective scaling requires structured trajectory understanding and selection, and bBoN provides a practical framework to achieve this."
    }
  ],
  "lastUpdated": "2025-10-06T09:33:31.925Z"
}