{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "路由流形对齐改进了专家混合LLM的泛化能力 (原标题: Routing Manifold Alignment Improves Generalization of Mixture-of-Experts LLMs)",
      "link": "https://arxiv.org/abs/2511.07419",
      "pubDate": "Mon, 10 Nov 2025 13:59:53 GMT",
      "isoDate": "2025-11-10T13:59:53.000Z",
      "creator": "Zhongyang Li, Ziyue Li, Tianyi Zhou",
      "summary": "## 路由流形对齐改进专家混合LLM的泛化能力\n\n### 背景与问题\n*   稀疏专家混合（MoE）模型已被广泛应用于近期的大型语言模型（LLMs），以在不增加推理成本的情况下有效扩展模型能力。\n*   然而，在广泛的下游任务评估中发现，现有MoE LLMs中的路由器存在持续的次优性。\n*   这种次优性导致与最优路由之间存在严重的性能差距（例如，准确率相差10-20%）。\n\n### RoMA方法：路由流形对齐\n*   本文提出了一种名为“路由流形对齐（Routing Manifold Alignment, RoMA）”的方法，旨在解决上述问题。\n*   **核心思想：** 通过将路由权重的流形与任务嵌入的流形对齐，可以有效缩小性能差距并提高MoE LLMs的泛化性能。\n*   **实现机制：**\n    *   RoMA在后训练目标中引入了一个额外的流形正则化项。\n    *   该方法仅需要对路由器进行轻量级微调，而其他参数保持冻结。\n    *   具体而言，正则化项鼓励每个样本的路由权重在其任务嵌入空间中与其“成功邻居”（即路由权重导致正确答案的样本）的路由权重保持接近。\n    *   结果是，针对相似任务的样本将在不同层之间共享相似的专家选择。\n*   **理论基础：** 在不同样本之间建立任务与专家之间的这种绑定对于实现更好的泛化能力至关重要。\n*   **创新点：** RoMA展示了将任务理解（通过嵌入模型）与解决方案生成（通过MoE LLMs）相结合的优势。\n\n### 实验与结果\n*   研究人员使用RoMA对OLMoE、DeepSeekMoE和Qwen3-MoE中的路由器进行了微调。\n*   在多样化的基准测试中进行的评估以及与基线的广泛比较表明，RoMA带来了显著的性能提升。\n\n### 图片信息\n*   文章内容中未包含有效的实际图片链接，因此本摘要不包含任何图片。",
      "shortSummary": "MoE LLMs的路由器次优导致显著性能差距。本文提出“路由流形对齐（RoMA）”方法，通过在后训练中引入流形正则化项，轻量级微调路由器，将路由权重流形与任务嵌入流形对齐。RoMA促使相似任务的样本共享相似专家选择，从而显著提升了MoE LLMs的泛化能力，并在多个模型和基准测试中得到验证。",
      "translated_title": "路由流形对齐改进了专家混合LLM的泛化能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Sparse Mixture-of-Experts (MoE) have been widely adopted in recent large language models since it can efficiently scale up the model capability without increasing the inference cost. However, evaluations on broad downstream tasks reveal a consistent suboptimality of the routers in existing MoE LLMs, which results in a severe performance gap (e.g., 10-20% in accuracy) to the optimal routing. In this paper, we show that aligning the manifold of routing weights with that of task embedding can effectively reduce the gap and improve MoE LLMs' generalization performance. Our method, \"Routing Manifold Alignment (RoMA)\", introduces an additional manifold regularization term in the post-training objective and only requires lightweight finetuning of routers (with other parameters frozen). Specifically, the regularization encourages the routing weights of each sample to be close to those of its successful neighbors (whose routing weights lead to correct answers) in a task embedding space. Consequently, samples targeting similar tasks will share similar expert choices across layers. Building such bindings between tasks and experts over different samples is essential to achieve better generalization. Moreover, RoMA demonstrates the advantage of unifying the task understanding (by embedding models) with solution generation (by MoE LLMs). In experiments, we finetune routers in OLMoE, DeepSeekMoE, and Qwen3-MoE using RoMA. Evaluations on diverse benchmarks and extensive comparisons with baselines show the substantial improvement brought by RoMA."
    },
    {
      "title": "机器人从物理世界模型中学习 (原标题: Robot Learning from a Physical World Model)",
      "link": "https://arxiv.org/abs/2511.07416",
      "pubDate": "Mon, 10 Nov 2025 13:59:07 GMT",
      "isoDate": "2025-11-10T13:59:07.000Z",
      "creator": "Jiageng Mao, Sicheng He, Hao-Ning Wu, Yang You, Shuyang Sun, Zhicheng Wang, Yanan Bao, Huizhong Chen, Leonidas Guibas, Vitor Guizilini, Howard Zhou, Yue Wang",
      "summary": "## PhysWorld：通过物理世界建模实现机器人从视频生成中学习\n\n### 介绍\nPhysWorld 是一个创新框架，旨在通过物理世界建模，使机器人能够从视频生成中进行学习。该框架解决了现有视频生成模型在机器人领域应用时忽视物理规律，导致操作不准确的局限性。\n\n### 核心问题与解决方案\n\n*   **现有问题**：近期视频生成模型能够根据语言指令和图像合成逼真的视觉演示，为机器人提供了强大的训练信号。然而，直接将生成视频中的像素运动重新定位到机器人上，往往会忽略物理规律，导致操作不准确。\n*   **PhysWorld的解决方案**：PhysWorld 通过将视频生成与物理世界重建相结合来解决这一限制。它将隐式的视觉指导转化为物理上可执行的机器人轨迹。\n\n### PhysWorld 的工作原理\n\n1.  **输入**：给定一张单一图像和一个任务指令。\n2.  **视频生成**：系统生成与任务相关的视频。\n3.  **物理世界重建**：从生成的视频中重建底层的物理世界模型。\n4.  **动作接地**：通过结合物理世界模型，利用以物体为中心的残差强化学习，将生成的视频运动转化为物理上准确的动作。\n\n### 主要优势\n\n*   **消除真实机器人数据收集**：PhysWorld 框架无需收集真实的机器人数据。\n*   **零样本泛化操作**：它能够实现零样本（zero-shot）的泛化机器人操作。\n*   **提高操作精度**：实验表明，与以往方法相比，PhysWorld 大幅提高了操作精度。\n\n### 实验验证\n在各种真实世界任务上的实验证明，PhysWorld 显著提高了操作精度。",
      "shortSummary": "PhysWorld是一个创新框架，通过将视频生成与物理世界重建相结合，使机器人能够从生成的视频中学习。它解决了现有视频生成模型在机器人应用中忽视物理规律导致操作不准确的问题。PhysWorld根据图像和指令生成任务视频，重建物理世界，并通过强化学习将视频运动转化为物理准确的机器人动作。这消除了真实数据收集需求，实现了零样本泛化操作，并显著提高了机器人操作精度。",
      "translated_title": "机器人从物理世界模型中学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering a powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit https://pointscoder.github.io/PhysWorld_Web/{the project webpage} for details."
    },
    {
      "title": "DigiData：训练和评估通用移动控制代理 (原标题: DigiData: Training and Evaluating General-Purpose Mobile Control Agents)",
      "link": "https://arxiv.org/abs/2511.07413",
      "pubDate": "Mon, 10 Nov 2025 13:57:35 GMT",
      "isoDate": "2025-11-10T13:57:35.000Z",
      "creator": "Yuxuan Sun, Manchen Wang, Shengyi Qian, William R. Wong, Eric Gan, Pierluca D'Oro, Alejandro Castillejo Munoz, Sneha Silwal, Pedro Matias, Nitin Kamra, Satwik Kottur, Nick Raines, Xuanyi Zhao, Joy Chen, Joseph Greer, Andrea Madotto, Allen Bolourchi, James Valori, Kevin Carlberg, Karl Ridgeway, Joseph Tighe",
      "summary": "# DigiData：训练和评估通用移动控制代理\n\n## 摘要\n\n人工智能代理（AI agents）控制用户界面的能力有望彻底改变人类与数字设备的交互方式。为了加速这一转变，两个基本要素至关重要：\n\n1.  **高质量数据集**：使代理能够实现复杂且与人类相关的目标。\n2.  **强大的评估方法**：允许研究人员和从业者快速提升代理性能。\n\n本文介绍了 **DigiData** 及其配套的 **DigiData-Bench**，旨在解决上述挑战。\n\n## DigiData 数据集\n\n*   **目的与特点**：\n    *   DigiData 是一个大规模、高质量、多样化、多模态的数据集，专为训练移动控制代理而设计。\n    *   它旨在帮助代理实现复杂且与人类相关的目标。\n*   **创新之处**：\n    *   与现有数据集（通常从非结构化交互中获取目标）不同，DigiData 通过对应用程序功能的全面探索精心构建。\n    *   这种构建方式带来了更高的数据多样性和更复杂的目标设定。\n\n## DigiData-Bench 基准与评估方法\n\n*   **目的**：\n    *   DigiData-Bench 是一个用于评估移动控制代理在真实世界复杂任务上性能的基准。\n*   **评估方法的改进**：\n    *   研究表明，常用的“步长准确性”（step-accuracy）指标在可靠评估移动控制代理方面存在不足。\n    *   为了解决这一问题，本文提出了更严谨的替代方案：\n        *   **动态评估协议**\n        *   **AI驱动的评估**\n\n## 贡献与影响\n\n*   本文的贡献旨在显著推动移动控制代理的开发。\n*   最终目标是为更直观、更有效的人机设备交互铺平道路。",
      "shortSummary": "本文介绍了DigiData，一个大规模、高质量、多模态数据集，用于训练通用移动控制代理。该数据集通过全面探索应用功能构建，提供更高多样性和目标复杂性。同时，文章还提出了DigiData-Bench基准，用于评估代理在真实复杂任务上的表现，并提出动态评估协议和AI驱动评估来替代不足的传统指标。这些工作旨在加速移动控制代理的发展，提升人机交互体验。",
      "translated_title": "DigiData：训练和评估通用移动控制代理",
      "images": [],
      "contentSource": "完整文章",
      "content": "AI agents capable of controlling user interfaces have the potential to transform human interaction with digital devices. To accelerate this transformation, two fundamental building blocks are essential: high-quality datasets that enable agents to achieve complex and human-relevant goals, and robust evaluation methods that allow researchers and practitioners to rapidly enhance agent performance. In this paper, we introduce DigiData, a large-scale, high-quality, diverse, multi-modal dataset designed for training mobile control agents. Unlike existing datasets, which derive goals from unstructured interactions, DigiData is meticulously constructed through comprehensive exploration of app features, resulting in greater diversity and higher goal complexity. Additionally, we present DigiData-Bench, a benchmark for evaluating mobile control agents on real-world complex tasks. We demonstrate that the commonly used step-accuracy metric falls short in reliably assessing mobile control agents and, to address this, we propose dynamic evaluation protocols and AI-powered evaluations as rigorous alternatives for agent assessment. Our contributions aim to significantly advance the development of mobile control agents, paving the way for more intuitive and effective human-device interactions."
    },
    {
      "title": "DIMO：任意物体的多样化3D运动生成 (原标题: DIMO: Diverse 3D Motion Generation for Arbitrary Objects)",
      "link": "https://arxiv.org/abs/2511.07409",
      "pubDate": "Mon, 10 Nov 2025 13:56:49 GMT",
      "isoDate": "2025-11-10T13:56:49.000Z",
      "creator": "Linzhan Mou, Jiahui Lei, Chen Wang, Lingjie Liu, Kostas Daniilidis",
      "summary": "DIMO是一种创新的生成式方法，旨在从单张图像为任意物体生成多样化的3D运动。该方法的核心思想是利用预训练视频模型中丰富的先验知识来提取常见的运动模式，并将其嵌入到一个共享的低维潜在空间中。\n\n**方法论概述：**\n\n*   **多视频生成与运动嵌入：** 首先，系统会为同一物体生成多个具有多样化运动的视频。随后，每个运动都被编码成一个潜在向量。\n*   **共享运动解码器训练：** 训练一个共享的运动解码器，该解码器学习由结构化且紧凑的运动表示（即神经关键点轨迹）所代表的运动分布。\n*   **3D高斯模型驱动：** 规范的3D高斯模型由这些学习到的关键点轨迹驱动，并融合以精确建模物体的几何形状和外观。\n\n**推理与应用：**\n\n*   **即时多样化运动采样：** 在推理阶段，利用学习到的潜在空间，DIMO能够通过单次前向传播即时采样出多样化的3D运动。\n*   **广泛应用：** 该方法支持多种有趣的应用程序，包括3D运动插值和语言引导的运动生成。\n\n**项目信息：**\n\n*   该研究已在ICCV 2025上发表。\n*   项目页面可在提供的URL（this https URL）访问。",
      "shortSummary": "DIMO是一种创新的生成式方法，能够从单张图像为任意物体生成多样化的3D运动。它通过利用预训练视频模型提取运动模式，并将其嵌入到共享的低维潜在空间中。该方法训练一个运动解码器，使用神经关键点轨迹来表示运动分布，并驱动3D高斯模型以构建物体的几何和外观。DIMO支持单次前向传播即时采样多样化3D运动，并可应用于3D运动插值和语言引导的运动生成。",
      "translated_title": "DIMO：任意物体的多样化3D运动生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present DIMO, a generative approach capable of generating diverse 3D motions for arbitrary objects from a single image. The core idea of our work is to leverage the rich priors in well-trained video models to extract the common motion patterns and then embed them into a shared low-dimensional latent space. Specifically, we first generate multiple videos of the same object with diverse motions. We then embed each motion into a latent vector and train a shared motion decoder to learn the distribution of motions represented by a structured and compact motion representation, i.e., neural key point trajectories. The canonical 3D Gaussians are then driven by these key points and fused to model the geometry and appearance. During inference time with learned latent space, we can instantly sample diverse 3D motions in a single-forward pass and support several interesting applications including 3D motion interpolation and language-guided motion generation. Our project page is available at https://linzhanm.github.io/dimo."
    },
    {
      "title": "IterResearch：通过马尔可夫状态重建重新思考长周期智能体 (原标题: IterResearch: Rethinking Long-Horizon Agents via Markovian State Reconstruction)",
      "link": "https://arxiv.org/abs/2511.07327",
      "pubDate": "Mon, 10 Nov 2025 12:30:08 GMT",
      "isoDate": "2025-11-10T12:30:08.000Z",
      "creator": "Guoxin Chen, Zile Qiao, Xuanzhong Chen, Donglei Yu, Haotian Xu, Wayne Xin Zhao, Ruihua Song, Wenbiao Yin, Huifeng Yin, Liwen Zhang, Kuan Li, Minpeng Liao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou",
      "summary": "IterResearch：通过马尔可夫状态重建重新思考长周期智能体\n\n### 现有问题\n\n*   **上下文饱和与噪声污染**：当前的深度研究智能体在处理长周期任务时，采用单一上下文范式，将所有信息累积在一个不断扩展的上下文窗口中。这导致上下文饱和和噪声污染，限制了它们在长周期任务中的有效性。\n\n### IterResearch 解决方案\n\n*   **新型迭代深度研究范式**：IterResearch 引入了一种新颖的迭代深度研究范式，将长周期研究重新定义为具有战略性工作空间重建的马尔可夫决策过程（Markov Decision Process, MDP）。\n*   **核心机制**：\n    *   **演进报告作为记忆**：通过维护一份不断演进的报告作为记忆。\n    *   **定期综合见解**：定期综合提炼见解。\n    *   **保持推理能力**：确保在任意探索深度下都能保持一致的推理能力。\n\n### 效率感知策略优化 (EAPO)\n\n*   **强化学习框架**：IterResearch 进一步开发了效率感知策略优化（Efficiency-Aware Policy Optimization, EAPO）框架。\n*   **目标**：激励高效探索。\n*   **关键特性**：\n    *   **几何奖励折扣**：通过几何奖励折扣实现。\n    *   **自适应下采样**：通过自适应下采样实现稳定的分布式训练。\n\n### 实验结果与影响\n\n*   **显著性能提升**：在六个基准测试中，IterResearch 相较于现有开源智能体平均提升了 +14.5 个百分点。\n*   **缩小差距**：显著缩小了与前沿专有系统之间的性能差距。\n*   **前所未有的交互扩展**：该范式展现出前所未有的交互扩展能力，可扩展至 2048 次交互，并带来显著的性能提升（从 3.5% 提升至 42.5%）。\n*   **有效提示策略**：作为一种有效的提示策略，在长周期任务中，它能将前沿模型的性能比 ReAct 提升高达 19.2 个百分点。\n\n### 结论\n\n*   **通用解决方案**：IterResearch 是一个用于长周期推理的通用解决方案。\n*   **双重效用**：它既可以作为训练有素的智能体，也可以作为前沿模型的提示范式发挥作用。",
      "shortSummary": "IterResearch 提出了一种迭代式深度研究范式，通过将长周期研究重构为带有战略性工作空间重建的马尔可夫决策过程，解决了现有智能体在长周期任务中面临的上下文饱和与噪声污染问题。它通过维护演进报告和定期综合见解来保持推理能力。结合效率感知策略优化（EAPO），IterResearch 在六个基准测试中平均提升14.5pp，并展现出前所未有的交互扩展能力，同时作为提示策略也能显著提升前沿模型性能，是长周期推理的通用解决方案。",
      "translated_title": "IterResearch：通过马尔可夫状态重建重新思考长周期智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in deep-research agents have shown promise for autonomous knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon tasks. We introduce IterResearch, a novel iterative deep-research paradigm that reformulates long-horizon research as a Markov Decision Process with strategic workspace reconstruction. By maintaining an evolving report as memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. We further develop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning framework that incentivizes efficient exploration through geometric reward discounting and enables stable distributed training via adaptive downsampling. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5\\% to 42.5\\%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct on long-horizon tasks. These findings position IterResearch as a versatile solution for long-horizon reasoning, effective both as a trained agent and as a prompting paradigm for frontier models."
    },
    {
      "title": "VADER：迈向基于关系感知大型语言模型的因果视频异常理解 (原标题: VADER: Towards Causal Video Anomaly Understanding with Relation-Aware Large Language Models)",
      "link": "https://arxiv.org/abs/2511.07299",
      "pubDate": "Mon, 10 Nov 2025 11:56:11 GMT",
      "isoDate": "2025-11-10T11:56:11.000Z",
      "creator": "Ying Cheng, Yu-Ho Lin, Min-Hung Chen, Fu-En Yang, Shang-Hong Lai",
      "summary": "## VADER：基于关系感知大型语言模型的因果视频异常理解\n\n### 摘要\n\n本文介绍了一种名为 VADER 的新型框架，旨在解决视频异常理解（VAU）领域中现有方法的局限性。传统的 VAU 方法主要侧重于异常事件的检测和定位，但往往忽视了对象之间深层次的因果关系和交互作用，而这些对于全面理解异常行为至关重要。\n\n### VADER 框架概述\n\nVADER 是一个由大型语言模型（LLM）驱动的框架，其核心目标是通过整合关键帧对象的关系特征与视觉线索，来增强对视频中异常事件的理解。该框架的关键组成部分和工作流程包括：\n\n*   **异常评分器（Anomaly Scorer）**：首先对视频中的每一帧分配异常分数，以识别潜在的异常时刻。\n*   **上下文感知采样（Context-AwarE Sampling, CAES）**：在异常评分的基础上，VADER 采用 CAES 策略来捕获每个异常事件的因果上下文，确保对异常发生前后的关键信息进行有效提取。\n*   **关系特征提取器（Relation Feature Extractor）与对比关系编码器（COntrastive Relation Encoder, CORE）**：这两个模块协同工作，对视频中对象的动态交互进行建模。它们能够生成紧凑的关系表示，为后续的推理提供基础。\n*   **LLM集成**：将提取到的视觉线索和关系线索与大型语言模型进行集成。通过这种集成，VADER 能够生成详细的、具有因果依据的异常描述，并支持对异常相关问题的鲁棒性回答。\n\n### 实验结果与贡献\n\n在多个真实世界的 VAU 基准测试中，VADER 展示了强大的性能。它在异常描述、解释和因果推理任务上均取得了显著成果，证明了其在理解复杂异常行为方面的有效性。VADER 的提出，标志着可解释视频异常分析领域的一个重要进展，为深入理解视频异常事件提供了新的视角和工具。",
      "shortSummary": "VADER是一个LLM驱动的框架，旨在实现因果视频异常理解（VAU）。它通过整合关键帧对象关系特征和视觉线索，克服了传统方法忽视对象间因果关系的问题。VADER包含异常评分器、上下文感知采样（CAES）以及关系特征提取器和对比关系编码器（CORE），用于建模动态对象交互。该框架能生成详细的因果描述并支持异常相关问答，在多个VAU基准测试中表现出色，显著推进了可解释视频异常分析领域。",
      "translated_title": "VADER：迈向基于关系感知大型语言模型的因果视频异常理解",
      "images": [],
      "contentSource": "完整文章",
      "content": "Video anomaly understanding (VAU) aims to provide detailed interpretation and semantic comprehension of anomalous events within videos, addressing limitations of traditional methods that focus solely on detecting and localizing anomalies. However, existing approaches often neglect the deeper causal relationships and interactions between objects, which are critical for understanding anomalous behaviors. In this paper, we propose VADER, an LLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe object Relation features with visual cues to enhance anomaly comprehension from video. Specifically, VADER first applies an Anomaly Scorer to assign per-frame anomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture the causal context of each anomalous event. A Relation Feature Extractor and a COntrastive Relation Encoder (CORE) jointly model dynamic object interactions, producing compact relational representations for downstream reasoning. These visual and relational cues are integrated with LLMs to generate detailed, causally grounded descriptions and support robust anomaly-related question answering. Experiments on multiple real-world VAU benchmarks demonstrate that VADER achieves strong results across anomaly description, explanation, and causal reasoning tasks, advancing the frontier of explainable video anomaly analysis."
    },
    {
      "title": "Omni-AVSR：迈向基于大型语言模型的统一多模态语音识别 (原标题: Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large Language Models)",
      "link": "https://arxiv.org/abs/2511.07253",
      "pubDate": "Mon, 10 Nov 2025 11:03:44 GMT",
      "isoDate": "2025-11-10T11:03:44.000Z",
      "creator": "Umberto Cappellazzo, Xubo Liu, Pingchuan Ma, Stavros Petridis, Maja Pantic",
      "summary": "## Omni-AVSR：基于大型语言模型的统一多模态语音识别\n\n### 摘要\n\n当前，大型语言模型（LLMs）在听觉语音识别（ASR）、视觉语音识别（VSR）和音视频语音识别（AVSR）等多种模态的语音识别任务中取得了显著进展。然而，现有基于LLM的方法通常独立处理每个任务，训练单独的模型，这不仅增加了计算和部署资源的消耗，还错失了潜在的跨任务协同效应。此外，这些方法依赖于固定速率的令牌压缩，限制了在平衡准确性与效率方面的灵活性。这些局限性凸显了对一个能够支持ASR、VSR和AVSR并实现弹性推理的统一框架的需求。\n\n### Omni-AVSR 解决方案\n\n为解决上述问题，我们提出了 **Omni-AVSR**，一个统一的音视频大型语言模型，它结合了高效的多粒度训练和参数高效的适应策略。\n\n#### 关键方法和创新点：\n\n*   **统一框架设计**：Omni-AVSR旨在提供一个单一模型，能够同时处理ASR、VSR和AVSR任务，从而减少资源消耗并提升跨模态协同。\n*   **高效多粒度训练**：\n    *   我们借鉴了“套娃式表示学习”（matryoshka representation learning）范式，以高效地在多个音频和视觉粒度上进行训练。\n    *   这种方法显著降低了固有的训练资源消耗，提高了训练效率。\n*   **参数高效适应**：\n    *   我们探索了三种基于LoRA（Low-Rank Adaptation）的策略，用于适应骨干LLM。\n    *   这些策略旨在平衡共享知识和任务特定专业化，确保模型在不同任务上的高性能。\n\n### 实验结果与性能\n\n我们在LRS2和LRS3数据集上进行了广泛的实验，结果表明：\n\n*   **卓越的准确性**：Omni-AVSR实现了与现有最先进（SOTA）基线模型相当或更优的准确性。\n*   **显著的资源节约**：在训练单个模型的同时，Omni-AVSR显著降低了训练和部署的资源消耗。\n*   **鲁棒性**：模型在声学噪声环境下仍能保持鲁棒性，展现了其在实际应用中的潜力。\n*   **可扩展性分析**：我们分析了LLM规模增加时的扩展行为，为性能与效率之间的权衡提供了深入见解。",
      "shortSummary": "Omni-AVSR是一个统一的音视频大型语言模型，旨在解决当前LLM在听觉、视觉和音视频语音识别任务中独立训练、资源消耗高的问题。它通过高效的多粒度训练（采用套娃式表示学习）和参数高效的LoRA适应策略，实现了单一模型对多种模态的支持。实验表明，Omni-AVSR在LRS2和LRS3数据集上达到了与SOTA相当或更优的准确性，同时显著降低了训练和部署资源，并在噪声环境下保持鲁棒性。",
      "translated_title": "Omni-AVSR：迈向基于大型语言模型的统一多模态语音识别",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) have recently achieved impressive results in speech recognition across multiple modalities, including Auditory Speech Recognition (ASR), Visual Speech Recognition (VSR), and Audio-Visual Speech Recognition (AVSR). Despite this progress, current LLM-based approaches typically address each task independently, training separate models that raise computational and deployment resource use while missing potential cross-task synergies. They also rely on fixed-rate token compression, which restricts flexibility in balancing accuracy with efficiency. These limitations highlight the need for a unified framework that can support ASR, VSR, and AVSR while enabling elastic inference. To this end, we present Omni-AVSR, a unified audio-visual LLM that combines efficient multi-granularity training with parameter-efficient adaptation. Specifically, we adapt the matryoshka representation learning paradigm to efficiently train across multiple audio and visual granularities, reducing its inherent training resource use. Furthermore, we explore three LoRA-based strategies for adapting the backbone LLM, balancing shared and task-specific specialization. Experiments on LRS2 and LRS3 show that Omni-AVSR achieves comparable or superior accuracy to state-of-the-art baselines while training a single model at substantially lower training and deployment resource use. The model also remains robust under acoustic noise, and we analyze its scaling behavior as LLM size increases, providing insights into the trade-off between performance and efficiency."
    },
    {
      "title": "MVU-Eval：迈向多视频理解评估的多模态大型语言模型 (原标题: MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs)",
      "link": "https://arxiv.org/abs/2511.07250",
      "pubDate": "Mon, 10 Nov 2025 11:02:33 GMT",
      "isoDate": "2025-11-10T11:02:33.000Z",
      "creator": "Tianhao Peng, Haochen Wang, Yuanxing Zhang, Zekun Wang, Zili Wang, Ge Zhang, Jian Yang, Shihao Li, Yanghai Wang, Xintao Wang, Houyi Li, Wei Ji, Pengfei Wan, Wenhao Huang, Zhaoxiang Zhang, Jiaheng Liu",
      "summary": "## MVU-Eval：多模态大型语言模型的多视频理解评估基准\n\n### 背景与动机\n\n*   **现有评估局限性**：尽管多模态大型语言模型（MLLMs）已将AI能力扩展到视觉模态，但现有的评估基准仍局限于单视频理解。\n*   **现实世界需求**：现实场景（如体育分析和自动驾驶）对多视频理解有关键需求，而现有评估未能满足这一需求。\n\n### MVU-Eval 介绍\n\n*   **首个综合基准**：MVU-Eval是首个旨在评估MLLMs多视频理解能力的综合基准。\n*   **评估范围**：\n    *   **核心能力**：主要评估八项核心能力。\n    *   **数据量**：包含1,824个精心策划的问题-答案对。\n    *   **视频数量**：涵盖来自不同领域的4,959个视频。\n    *   **任务类型**：涉及基础感知任务和高阶推理任务。\n*   **与现实应用对齐**：这些能力与多传感器合成（在自动驾驶系统中）和跨角度体育分析等现实世界应用紧密对齐。\n\n### 评估结果与发现\n\n*   **广泛评估**：对最先进的开源和闭源模型进行了广泛评估。\n*   **显著性能差异**：评估揭示了当前MLLMs在执行多视频理解能力方面存在显著的性能差异和局限性。\n\n### 可用性与未来展望\n\n*   **公开可用**：该基准将公开发布，以促进未来的研究。\n\n### 相关领域\n\n*   计算机视觉与模式识别 (cs.CV)\n*   人工智能 (cs.AI)",
      "shortSummary": "MVU-Eval是首个针对多模态大型语言模型（MLLMs）的多视频理解综合评估基准。它旨在弥补现有评估仅限于单视频理解的不足，通过1,824个问答对和4,959个视频，评估MLLMs的八项核心能力，涵盖感知与推理任务，并与自动驾驶、体育分析等现实应用对齐。评估结果显示，当前MLLMs在多视频理解方面存在显著局限性。该基准将公开发布以推动未来研究。",
      "translated_title": "MVU-Eval：迈向多视频理解评估的多模态大型语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "The advent of Multimodal Large Language Models (MLLMs) has expanded AI capabilities to visual modalities, yet existing evaluation benchmarks remain limited to single-video understanding, overlooking the critical need for multi-video understanding in real-world scenarios (e.g., sports analytics and autonomous driving). To address this significant gap, we introduce MVU-Eval, the first comprehensive benchmark for evaluating Multi-Video Understanding for MLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies through 1,824 meticulously curated question-answer pairs spanning 4,959 videos from diverse domains, addressing both fundamental perception tasks and high-order reasoning tasks. These capabilities are rigorously aligned with real-world applications such as multi-sensor synthesis in autonomous systems and cross-angle sports analytics. Through extensive evaluation of state-of-the-art open-source and closed-source models, we reveal significant performance discrepancies and limitations in current MLLMs' ability to perform understanding across multiple videos. The benchmark will be made publicly available to foster future research."
    },
    {
      "title": "MPJudge：迈向音乐启发绘画的感知评估 (原标题: MPJudge: Towards Perceptual Assessment of Music-Induced Paintings)",
      "link": "https://arxiv.org/abs/2511.07137",
      "pubDate": "Mon, 10 Nov 2025 09:18:27 GMT",
      "isoDate": "2025-11-10T09:18:27.000Z",
      "creator": "Shiqi Jiang, Tianyi Liang, Changbo Wang, Chenhui Li",
      "summary": "## MPJudge：迈向音乐启发绘画的感知评估\n\n### 引言\n*   音乐启发绘画是一种独特的艺术实践，即在音乐影响下创作视觉艺术作品。\n*   评估绘画是否忠实反映其灵感音乐，是一项具有挑战性的感知评估任务。\n\n### 现有方法的局限性\n*   现有方法主要依赖情感识别模型来评估音乐与绘画的相似性。\n*   这些模型引入了相当大的噪声，并忽略了情感之外的更广泛的感知线索。\n\n### MPJudge 框架\n*   **目标：** 针对现有方法的局限性，提出一个新颖的框架，直接建模音乐与视觉艺术之间的感知一致性，以进行音乐启发绘画的评估。\n\n### MPD 数据集\n*   **首创：** 引入了MPD，这是第一个大规模的音乐-绘画对数据集。\n*   **标注：** 由领域专家根据感知一致性进行标注。\n*   **增强：** 为更好地处理模糊情况，进一步收集了成对偏好标注。\n\n### MPJudge 模型\n*   **构建基础：** 基于MPD数据集。\n*   **架构：** 将音乐特征通过基于调制的融合机制整合到视觉编码器中。\n*   **训练：** 采用直接偏好优化（Direct Preference Optimization, DPO）进行训练，以有效学习模糊情况。\n\n### 实验结果\n*   **性能提升：** 大量实验表明，MPJudge方法优于现有方法。\n*   **定性分析：** 质性结果进一步表明，MPJudge模型能更准确地识别绘画中与音乐相关的区域。\n\n### 研究领域与引用\n*   **研究领域：** 计算机视觉与模式识别 (cs.CV)\n*   **引用信息：** arXiv:2511.07137；期刊参考：AAAI 2026",
      "shortSummary": "该研究提出了MPJudge框架，旨在解决音乐启发绘画的感知评估挑战。现有方法依赖情感识别，但存在噪声且忽略了更广泛的感知线索。MPJudge通过直接建模音乐与视觉艺术间的感知一致性来改进。为此，研究构建了首个大规模音乐-绘画对数据集MPD，并引入MPJudge模型，该模型通过调制融合机制整合音乐特征，并利用直接偏好优化进行训练。实验证明，MPJudge优于现有方法，能更准确识别绘画中与音乐相关的区域。",
      "translated_title": "MPJudge：迈向音乐启发绘画的感知评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "Music induced painting is a unique artistic practice, where visual artworks are created under the influence of music. Evaluating whether a painting faithfully reflects the music that inspired it poses a challenging perceptual assessment task. Existing methods primarily rely on emotion recognition models to assess the similarity between music and painting, but such models introduce considerable noise and overlook broader perceptual cues beyond emotion. To address these limitations, we propose a novel framework for music induced painting assessment that directly models perceptual coherence between music and visual art. We introduce MPD, the first large scale dataset of music painting pairs annotated by domain experts based on perceptual coherence. To better handle ambiguous cases, we further collect pairwise preference annotations. Building on this dataset, we present MPJudge, a model that integrates music features into a visual encoder via a modulation based fusion mechanism. To effectively learn from ambiguous cases, we adopt Direct Preference Optimization for training. Extensive experiments demonstrate that our method outperforms existing approaches. Qualitative results further show that our model more accurately identifies music relevant regions in paintings."
    },
    {
      "title": "RedOne 2.0：重新思考社交网络服务中领域特定LLM的后训练 (原标题: RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social Networking Services)",
      "link": "https://arxiv.org/abs/2511.07070",
      "pubDate": "Mon, 10 Nov 2025 08:04:34 GMT",
      "isoDate": "2025-11-10T08:04:34.000Z",
      "creator": "Fei Zhao, Chonggang Lu, Haofu Qian, Fangcheng Shi, Zijie Meng, Jianzhao Huang, Xu Tang, Zheyong Xie, Zheyu Ye, Zhe Xu, Yao Hu, Shaosheng Cao",
      "summary": "### RedOne 2.0：社交网络服务（SNS）中领域特定大型语言模型（LLM）的后训练新范式\n\n**1. 背景与挑战**\n\n*   **SNS的独特挑战**：社交网络服务作为人类互动和信息交换的关键媒介，对LLM提出了独特的挑战，包括：\n    *   异构工作负载。\n    *   快速变化的规范和俚语。\n    *   多语言、文化多元的语料库，导致严重的分布偏移。\n*   **SFT的局限性**：传统的监督微调（SFT）虽然能使模型专业化，但常在领域内（in-distribution）收益与领域外（out-of-distribution）鲁棒性之间引发“跷跷板效应”，尤其对于小型模型而言。\n\n**2. RedOne 2.0 解决方案**\n\n*   **引入RedOne 2.0**：为应对上述挑战，研究人员提出了RedOne 2.0，这是一种面向SNS的LLM，采用渐进式、RL（强化学习）优先的后训练范式，旨在实现快速且稳定的适应。\n\n**3. RedOne 2.0 的三阶段训练流程**\n\nRedOne 2.0 的训练流程包含以下三个阶段：\n\n*   **阶段1：探索性学习（Exploratory Learning）**\n    *   在精心策划的SNS语料库上进行学习，以建立初始对齐。\n    *   识别系统性弱点。\n*   **阶段2：目标性微调（Targeted Fine-Tuning）**\n    *   有选择地对诊断出的差距应用SFT。\n    *   混合少量通用数据，以减轻遗忘效应。\n*   **阶段3：精炼学习（Refinement Learning）**\n    *   重新应用RL，并结合以SNS为中心的信号。\n    *   巩固改进，并在不同任务之间协调权衡。\n\n**4. 实验结果与性能**\n\n*   **显著性能提升**：在涵盖三类任务的各项测试中，RedOne 2.0 的4B规模模型相对于7B的次优基线，平均性能提升约2.41。\n*   **卓越的数据效率与稳定性**：RedOne 2.0 从基础模型获得了约8.74的平均性能提升，而所需数据量不到以SFT为中心的RedOne方法的一半，这证明了其在紧凑规模下卓越的数据效率和稳定性。\n\n**5. 结论**\n\n*   RedOne 2.0 为SNS场景中的领域特定LLM建立了一个具有竞争力且成本效益的基线，在提升能力的同时不牺牲鲁棒性。",
      "shortSummary": "RedOne 2.0 提出了一种渐进式、RL优先的后训练范式，旨在解决社交网络服务（SNS）中大型语言模型（LLM）面临的异构工作负载、快速变化的语料和分布偏移等挑战。该方法通过探索性学习、目标性微调和精炼学习三个阶段，实现了快速稳定的适应。实验表明，RedOne 2.0 的4B模型在性能上显著优于7B基线，且数据效率和稳定性更高，为SNS领域特定LLM提供了一个成本效益高且鲁棒性强的解决方案。",
      "translated_title": "RedOne 2.0：重新思考社交网络服务中领域特定LLM的后训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "As a key medium for human interaction and information exchange, social networking services (SNS) pose unique challenges for large language models (LLMs): heterogeneous workloads, fast-shifting norms and slang, and multilingual, culturally diverse corpora that induce sharp distribution shift. Supervised fine-tuning (SFT) can specialize models but often triggers a ``seesaw'' between in-distribution gains and out-of-distribution robustness, especially for smaller models. To address these challenges, we introduce RedOne 2.0, an SNS-oriented LLM trained with a progressive, RL-prioritized post-training paradigm designed for rapid and stable adaptation. The pipeline consist in three stages: (1) Exploratory Learning on curated SNS corpora to establish initial alignment and identify systematic weaknesses; (2) Targeted Fine-Tuning that selectively applies SFT to the diagnosed gaps while mixing a small fraction of general data to mitigate forgetting; and (3) Refinement Learning that re-applies RL with SNS-centric signals to consolidate improvements and harmonize trade-offs across tasks. Across various tasks spanning three categories, our 4B scale model delivers an average improvements about 2.41 over the 7B sub-optimal baseline. Additionally, RedOne 2.0 achieves average performance lift about 8.74 from the base model with less than half the data required by SFT-centric method RedOne, evidencing superior data efficiency and stability at compact scales. Overall, RedOne 2.0 establishes a competitive, cost-effective baseline for domain-specific LLMs in SNS scenario, advancing capability without sacrificing robustness."
    },
    {
      "title": "大型语言模型有情感吗？通过提示、检索和课程学习教授情感识别 (原标题: Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and Curriculum Learning)",
      "link": "https://arxiv.org/abs/2511.07061",
      "pubDate": "Mon, 10 Nov 2025 07:52:11 GMT",
      "isoDate": "2025-11-10T07:52:11.000Z",
      "creator": "Xinran Li, Xiujuan Xu, Jiaqi Qiao, Yu Liu",
      "summary": "## 大型语言模型在对话情感识别（ERC）中的应用：PRC-Emo框架\n\n### 研究背景与挑战\n\n*   对话情感识别（ERC）是理解人类情感和实现自然人机交互的关键任务。\n*   尽管大型语言模型（LLMs）在此领域展现出巨大潜力，但它们在捕捉显式和隐式情感之间内在联系的能力上仍存在局限。\n\n### PRC-Emo框架介绍\n\n*   本文提出了一种名为PRC-Emo的新型ERC训练框架，旨在探索LLMs是否能有效感知对话情境中的情感。\n*   PRC-Emo集成了以下三个核心组件：\n    *   **提示工程（Prompt engineering）**\n    *   **示例检索（demonstration Retrieval）**\n    *   **课程学习（Curriculum learning）**\n\n### PRC-Emo核心组件详解\n\n*   **情感敏感的提示模板**\n    *   设计了基于显式和隐式情感线索的提示模板。\n    *   这些模板旨在更好地引导模型理解说话者的心理状态。\n*   **专用示例检索库**\n    *   构建了首个专用于ERC的示例检索库。\n    *   该库包含来自广泛使用数据集的训练样本。\n    *   还包括由LLMs生成并经过人工验证的高质量对话示例。\n*   **课程学习策略**\n    *   将课程学习策略引入LoRA微调过程。\n    *   通过计算同说话者和不同说话者话语之间的加权情感转变，为对话样本分配难度级别。\n    *   然后，按照从易到难的顺序组织训练序列。\n\n### 实验结果与贡献\n\n*   在两个基准数据集（IEMOCAP和MELD）上的实验结果表明，PRC-Emo方法达到了新的最先进（SOTA）性能。\n*   这证明了该方法在提升基于LLM的情感理解方面的有效性和泛化能力。\n*   该研究已被AAAI 2026接受。",
      "shortSummary": "本文提出PRC-Emo框架，旨在提升大型语言模型（LLMs）在对话情感识别（ERC）中的表现。该框架整合了情感敏感的提示工程、首个专用的ERC示例检索库以及基于情感转变的课程学习策略。通过LoRA微调，PRC-Emo能够有效捕捉显式和隐式情感联系。在IEMOCAP和MELD数据集上的实验结果表明，该方法达到了最先进（SOTA）性能，验证了其在改善LLM情感理解方面的有效性和泛化能力。",
      "translated_title": "大型语言模型有情感吗？通过提示、检索和课程学习教授情感识别",
      "images": [],
      "contentSource": "完整文章",
      "content": "Emotion Recognition in Conversation (ERC) is a crucial task for understanding human emotions and enabling natural human-computer interaction. Although Large Language Models (LLMs) have recently shown great potential in this field, their ability to capture the intrinsic connections between explicit and implicit emotions remains limited. We propose a novel ERC training framework, PRC-Emo, which integrates Prompt engineering, demonstration Retrieval, and Curriculum learning, with the goal of exploring whether LLMs can effectively perceive emotions in conversational contexts. Specifically, we design emotion-sensitive prompt templates based on both explicit and implicit emotional cues to better guide the model in understanding the speaker's psychological states. We construct the first dedicated demonstration retrieval repository for ERC, which includes training samples from widely used datasets, as well as high-quality dialogue examples generated by LLMs and manually verified. Moreover, we introduce a curriculum learning strategy into the LoRA fine-tuning process, incorporating weighted emotional shifts between same-speaker and different-speaker utterances to assign difficulty levels to dialogue samples, which are then organized in an easy-to-hard training sequence. Experimental results on two benchmark datasets-- IEMOCAP and MELD --show that our method achieves new state-of-the-art (SOTA) performance, demonstrating the effectiveness and generalizability of our approach in improving LLM-based emotional understanding."
    },
    {
      "title": "SofT-GRPO：通过Gumbel重参数化软思维策略优化超越离散令牌LLM强化学习 (原标题: SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization)",
      "link": "https://arxiv.org/abs/2511.06411",
      "pubDate": "Sun, 09 Nov 2025 09:55:50 GMT",
      "isoDate": "2025-11-09T09:55:50.000Z",
      "creator": "Zhi Zheng, Wee Sun Lee",
      "summary": "## SofT-GRPO：通过Gumbel重参数化软思维策略优化超越离散令牌LLM强化学习\n\n### 引言与背景\n\n*   大型语言模型（LLM）的软思维推理范式在某些场景下表现优于传统的离散令牌思维链（CoT）推理，这凸显了其重要的研究和应用价值。\n*   然而，使用强化学习（RL）来强化软思维模式面临显著挑战。这些挑战主要源于：\n    *   向软思维令牌注入随机性的复杂性。\n    *   相应地更新软思维策略的难度。\n*   因此，此前将软思维与组相对策略优化（GRPO）结合的尝试，通常表现不如其离散令牌GRPO对应方法。\n\n### SofT-GRPO算法\n\n*   为了充分释放软思维的潜力，本文提出了一种新颖的策略优化算法——SofT-GRPO。\n*   该算法专门设计用于在软思维推理模式下强化LLMs。\n\n### 核心技术与方法\n\nSofT-GRPO通过以下关键技术克服了现有挑战：\n\n1.  **注入Gumbel噪声：** 将Gumbel噪声注入到逻辑值（logits）中，以引入必要的随机性。\n2.  **Gumbel-Softmax技术：** 采用Gumbel-Softmax技术，有效避免了软思维令牌生成超出预训练嵌入空间的情况。\n3.  **重参数化技巧：** 在策略梯度计算中巧妙地利用了重参数化技巧，以实现更稳定的策略更新。\n\n### 实验与结果\n\n*   **实验范围：** 研究人员在参数量从1.5B到7B的基础LLMs上进行了广泛的实验。\n*   **性能提升：** 实验结果显著表明SofT-GRPO的有效性：\n    *   在Pass@1指标上，SofT-GRPO使软思维LLMs略微超越了离散令牌GRPO，平均准确率提升了0.13%。\n    *   在Pass@32指标上，SofT-GRPO展现出更为显著的提升，平均准确率提升了2.19%。\n\n### 结论\n\nSofT-GRPO成功地解决了强化软思维LLM的挑战，并在多项关键指标上超越了传统的离散令牌GRPO方法，尤其在处理复杂任务（Pass@32）时取得了显著进步，充分展示了软思维与强化学习结合的巨大潜力。相关代码和权重已公开发布。",
      "shortSummary": "本文提出SofT-GRPO算法，旨在通过强化学习优化大型语言模型（LLMs）的软思维推理模式。传统方法在强化软思维时面临随机性注入和策略更新的挑战，导致性能不如离散令牌GRPO。SofT-GRPO通过注入Gumbel噪声、Gumbel-Softmax技术和重参数化技巧，成功克服了这些难题。实验结果表明，SofT-GRPO使软思维LLMs在Pass@1上略优于离散令牌GRPO（+0.13%），并在Pass@32上实现了显著提升（+2.19%），展现了超越传统方法的潜力。",
      "translated_title": "SofT-GRPO：通过Gumbel重参数化软思维策略优化超越离散令牌LLM强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "The soft-thinking paradigm for Large Language Model (LLM) reasoning can outperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in some scenarios, underscoring its research and application value. However, while the discrete-token CoT reasoning pattern can be reinforced through policy optimization algorithms such as group relative policy optimization (GRPO), extending the soft-thinking pattern with Reinforcement Learning (RL) remains challenging. This difficulty stems from the complexities of injecting stochasticity into soft-thinking tokens and updating soft-thinking policies accordingly. As a result, previous attempts to combine soft-thinking with GRPO typically underperform their discrete-token GRPO counterparts. To fully unlock the potential of soft-thinking, this paper presents a novel policy optimization algorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning pattern. SofT-GRPO injects the Gumbel noise into logits, employs the Gumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained embedding space, and leverages the reparameterization trick in policy gradient. We conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and results demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly outperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while exhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes and weights are available on https://github.com/zz1358m/SofT-GRPO-master"
    },
    {
      "title": "Station：一个由AI驱动的开放世界发现环境 (原标题: The Station: An Open-World Environment for AI-Driven Discovery)",
      "link": "https://arxiv.org/abs/2511.06309",
      "pubDate": "Sun, 09 Nov 2025 05:13:00 GMT",
      "isoDate": "2025-11-09T05:13:00.000Z",
      "creator": "Stephen Chung, Wenyu Du",
      "summary": "# Station：一个由AI驱动的开放世界发现环境\n\n“Station”是一个新颖的开放世界多智能体环境，旨在模拟一个微型科学生态系统。该环境的核心目标是推动由AI驱动的自主科学发现。\n\n## 核心概念与功能\n\n*   **开放世界多智能体环境**：Station是一个没有中心化协调系统的环境。智能体可以自由选择自己的行动，并发展自己的研究叙事。\n*   **长程科学探索**：利用其扩展的上下文窗口，Station中的AI智能体能够进行长时间的科学探索，包括：\n    *   阅读同行论文\n    *   制定假设\n    *   提交代码\n    *   执行分析\n    *   发布研究成果\n*   **自主行为**：智能体在Station中是自主的，它们独立地追求研究、与同行互动，并在此过程中建立起累积的历史。\n\n## 实验成果与突破\n\n*   **卓越性能**：实验证明，Station中的AI智能体在广泛的基准测试中取得了新的最先进性能，涵盖了数学、计算生物学和机器学习等领域。\n*   **超越现有技术**：值得注意的是，在圆堆积（circle packing）问题上，Station的智能体显著超越了AlphaEvolve。\n*   **涌现的叙事与方法**：\n    *   智能体独立研究、相互作用和累积历史，形成了一个丰富的叙事图景。\n    *   从这些涌现的叙事中，新的方法有机地产生，例如一种用于单细胞RNA测序（scRNA-seq）批次整合的密度自适应算法。\n\n## 意义与未来展望\n\n*   **自主科学发现的里程碑**：Station代表了迈向由开放世界环境中涌现行为驱动的自主科学发现的第一步。\n*   **新范式**：它标志着一种超越僵化优化的新范式，强调通过智能体间的自由互动和探索来推动科学进步。",
      "shortSummary": "“Station”是一个开放世界多智能体环境，模拟微型科学生态系统，旨在实现AI驱动的自主科学发现。AI智能体在此环境中自主进行科学探索，包括阅读论文、提出假设、执行分析和发布结果，且无中心化协调。实验表明，Station智能体在数学、计算生物学和机器学习等多个基准测试中取得了最先进的性能，并能有机地产生新方法。这代表了通过涌现行为实现自主科学发现的新范式。",
      "translated_title": "Station：一个由AI驱动的开放世界发现环境",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce the STATION, an open-world multi-agent environment that models a miniature scientific ecosystem. Leveraging their extended context windows, agents in the Station can engage in long scientific journeys that include reading papers from peers, formulating hypotheses, submitting code, performing analyses, and publishing results. Importantly, there is no centralized system coordinating their activities - agents are free to choose their own actions and develop their own narratives within the Station. Experiments demonstrate that AI agents in the Station achieve new state-of-the-art performance on a wide range of benchmarks, spanning from mathematics to computational biology to machine learning, notably surpassing AlphaEvolve in circle packing. A rich tapestry of narratives emerges as agents pursue independent research, interact with peers, and build upon a cumulative history. From these emergent narratives, novel methods arise organically, such as a new density-adaptive algorithm for scRNA-seq batch integration. The Station marks a first step towards autonomous scientific discovery driven by emergent behavior in an open-world environment, representing a new paradigm that moves beyond rigid optimization."
    },
    {
      "title": "DRIVE：竞争性代码生成中可验证奖励强化学习的数据管理最佳实践 (原标题: DRIVE: Data Curation Best Practices for Reinforcement Learning with Verifiable Reward in Competitive Code Generation)",
      "link": "https://arxiv.org/abs/2511.06307",
      "pubDate": "Sun, 09 Nov 2025 05:11:28 GMT",
      "isoDate": "2025-11-09T05:11:28.000Z",
      "creator": "Speed Zhu, Jianwei Cai, Guang Chen, Lulu Wu, Saiyong Yang, Wiggin Zhou",
      "summary": "## DRIVE：竞争性代码生成中可验证奖励强化学习的数据管理最佳实践\n\n### 摘要\n\n近期以推理为主的模型（如OpenAI o1、DeepSeek R1）重新激发了对可验证奖励强化学习（RLVR）的兴趣。然而，这一领域的进展主要集中在数学领域（如AIME），而竞争性编程代码生成领域尚未得到充分探索，数据管理受到的关注也少于RL算法设计。本文旨在研究如何构建RLVR数据集（即RL提示），并提出实用的训练技术，以在竞争性编程代码生成中取得强大性能。\n\n### 方法论与管道\n\n我们的管道从以下步骤开始：\n\n1.  **监督微调（SFT）**：\n    *   从强大的开源模型中提取SFT。\n    *   通过通用和推理密集型数据进行增强。\n\n2.  **两阶段强化学习（RL）**：\n    *   RL阶段采用可执行的、测试用例驱动的奖励。\n    *   **第一阶段：基于GRPO的训练**\n        *   在大量、均匀分布的竞争性编程问题集上进行训练。\n        *   使用**组相对策略优化（GRPO）**算法。\n        *   每个提示进行8次rollout。\n        *   相对较短的响应生成窗口（例如，SFT期间为32k，此阶段为24k）。\n        *   目标：扩展熵，缓解重复和截断问题。\n    *   **第二阶段：Pre-GRPO**\n        *   在一个小规模、高质量的挑战性问题集上进行更新。\n        *   采用较大的rollout预算：每个提示64次rollout。\n        *   实施**硬聚焦课程（hard-focus curriculum）**：在整个训练过程中持续保留最困难的实例。\n\n### 实施与评估\n\n*   我们将所提出的方法在Qwen2.5-32B模型上实现。\n*   在LeetCode和Codeforces周赛上进行评估，以避免数据泄露。\n\n### 结果与观察\n\n*   **性能表现**：\n    *   在同等规模模型中取得了最先进（SOTA）的性能。\n    *   与DeepSeek v3.1和Doubao-1.5-Thinking等领先系统表现相当。\n*   **扩展趋势**：\n    *   在内部大规模MoE模型上观察到强大的RL扩展能力。\n\n### 贡献与最佳实践\n\n本研究为竞争性编程代码生成中的RLVR提炼出以下简洁的最佳实践：\n\n*   数据管理。\n*   熵扩展。\n*   课程设计。",
      "shortSummary": "本文探讨了竞争性代码生成中可验证奖励强化学习（RLVR）的数据管理和训练技术。研究提出了一种两阶段RL管道：首先进行监督微调，随后是基于可执行奖励的两阶段RL过程，包括使用GRPO进行熵扩展训练，以及采用硬聚焦课程的Pre-GRPO阶段。该方法在Qwen2.5-32B上实现了最先进性能，与领先系统相当，并提炼出RLVR中数据管理、熵扩展和课程设计的最佳实践。",
      "translated_title": "DRIVE：竞争性代码生成中可验证奖励强化学习的数据管理最佳实践",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent reasoning-first models (e.g., OpenAI o1, DeepSeek R1) have spurred a resurgence of interest in RLVR. Nevertheless, advances are dominated by mathematics (e.g., AIME), with competitive-programming code generation underexplored and data curation receiving less attention than RL algorithm design. We investigate how to construct RLVR datasets (i.e., RL prompts) and present practical training techniques that yield strong performance on competitive-programming code generation. Our pipeline begins with supervised fine-tuning (SFT) distilled from strong open-source models, augmented with general-purpose and reasoning-intensive data. RL then follows a two-stage process with executable, testcase-driven rewards: first, training on a large, uniformly distributed set of competitive-programming problems using Group Relative Policy Optimization (GRPO) with 8 rollouts per prompt and a relatively short response-generation window (e.g., 32k during SFT and 24k in this stage) to expand entropy and mitigate repetition and truncation; second, we perform Pre-GRPO: updating on a small, high-quality set of challenging problems with a large rollout budget (64 rollouts per prompt) under a hard-focus curriculum that continuously retains the most difficult instances throughout training. We implement our method on Qwen2.5-32B and evaluate on LeetCode and Codeforces weekly contests to avoid data leakage. The resulting model achieves state-of-the-art performance among models of similar scale and is comparable to leading systems such as DeepSeek v3.1 and Doubao-1.5-Thinking. We also examine scaling trends and observe strong RL scaling on an internal large-scale MoE model. Our study distills concise best practices for data curation, entropy expansion, and curriculum design in RLVR for competitive-programming code generation."
    },
    {
      "title": "置信推理：通过不确定性头部高效验证LLM推理步骤 (原标题: Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps via Uncertainty Heads)",
      "link": "https://arxiv.org/abs/2511.06209",
      "pubDate": "Sat, 08 Nov 2025 22:38:29 GMT",
      "isoDate": "2025-11-08T22:38:29.000Z",
      "creator": "Jingwei Ni, Ekaterina Fadeeva, Tianyi Wu, Mubashara Akhtar, Jiaheng Zhang, Elliott Ash, Markus Leippold, Timothy Baldwin, See-Kiong Ng, Artem Shelmanov, Mrinmaya Sachan",
      "summary": "## 置信推理：通过不确定性头部高效验证LLM推理步骤\n\n### 摘要\n\n本文提出了一种名为“不确定性量化头部”（UHeads）的轻量级方法，用于高效验证大型语言模型（LLM）在解决复杂任务时生成的长多步推理链。该方法旨在解决现有验证方案（如过程奖励模型PRMs）计算成本高昂、领域受限或需要大量人工/模型生成标注的问题。\n\n### 核心问题\n\n*   LLM解决复杂任务通常需要生成多步推理链。\n*   验证这些推理步骤的正确性可以提高LLM的性能、效率和解决方案的可解释性。\n*   现有验证方法（如PRMs）存在以下缺点：\n    *   计算成本高昂。\n    *   局限于特定领域。\n    *   需要大规模的人工或模型生成标注。\n\n### 提出的解决方案：不确定性量化头部（UHeads）\n\n*   **方法概述**：基于数据驱动的不确定性分数，提出了一种轻量级的步骤级推理验证替代方案。\n*   **工作原理**：\n    *   训练基于Transformer的不确定性量化头部（UHeads）。\n    *   UHeads利用冻结LLM的内部状态，在生成过程中估计其推理步骤的不确定性。\n    *   该方法是全自动的：目标标签可以由另一个更大的LLM（例如DeepSeek R1）生成，也可以由原始模型以自监督方式生成。\n*   **关键特性与优势**：\n    *   **高效且轻量级**：UHeads参数量少于1000万，远小于PRMs（PRMs的规模可达UHeads的810倍）。\n    *   **性能卓越**：在多个领域（包括数学、规划和通用知识问答）中，UHeads的性能与PRMs相当甚至超越PRMs。\n    *   **通用性强**：适用于多种任务和领域。\n    *   **全自动化**：无需人工干预即可生成验证标签。\n\n### 研究发现与意义\n\n*   LLM的内部状态编码了其自身的不确定性，并且可以作为推理验证的可靠信号。\n*   这项研究为开发可扩展和通用化的自省式LLM提供了一个有前景的方向。",
      "shortSummary": "本文提出UHeads，一种轻量级、数据驱动的方法，通过不确定性分数高效验证LLM的多步推理。UHeads利用冻结LLM的内部状态，自动估计推理步骤的不确定性，参数量少于1000万。它在数学、规划和通用知识问答等多个领域，性能与大810倍的PRMs相当或超越。研究表明LLM内部状态能可靠地指示不确定性，为开发可扩展的自省式LLM提供了新方向。",
      "translated_title": "置信推理：通过不确定性头部高效验证LLM推理步骤",
      "images": [],
      "contentSource": "完整文章",
      "content": "Solving complex tasks usually requires LLMs to generate long multi-step reasoning chains. Previous work has shown that verifying the correctness of individual reasoning steps can further improve the performance and efficiency of LLMs on such tasks and enhance solution interpretability. However, existing verification approaches, such as Process Reward Models (PRMs), are either computationally expensive, limited to specific domains, or require large-scale human or model-generated annotations. Thus, we propose a lightweight alternative for step-level reasoning verification based on data-driven uncertainty scores. We train transformer-based uncertainty quantification heads (UHeads) that use the internal states of a frozen LLM to estimate the uncertainty of its reasoning steps during generation. The approach is fully automatic: target labels are generated either by another larger LLM (e.g., DeepSeek R1) or in a self-supervised manner by the original model itself. UHeads are both effective and lightweight, containing less than 10M parameters. Across multiple domains, including mathematics, planning, and general knowledge question answering, they match or even surpass the performance of PRMs that are up to 810x larger. Our findings suggest that the internal states of LLMs encode their uncertainty and can serve as reliable signals for reasoning verification, offering a promising direction toward scalable and generalizable introspective LLMs."
    },
    {
      "title": "LUT-LLM：基于FPGA存储器计算的高效大语言模型推理 (原标题: LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs)",
      "link": "https://arxiv.org/abs/2511.06174",
      "pubDate": "Sat, 08 Nov 2025 20:17:08 GMT",
      "isoDate": "2025-11-08T20:17:08.000Z",
      "creator": "Zifan He, Shengyu Ye, Rui Ma, Yang Wang, Jason Cong",
      "summary": "## LUT-LLM：基于FPGA存储器计算的高效大语言模型推理\n\n### 摘要\n\n本文介绍了LUT-LLM，一个创新的FPGA加速器，旨在通过将大语言模型（LLM）推理从传统的算术计算转向基于存储器的查表计算，从而显著提高推理效率。\n\n### 背景与挑战\n\n*   **LLM的快速发展**：大语言模型在众多应用中取得了显著进展。\n*   **单批次推理的重要性**：高效的单批次推理对于实现设备端智能至关重要。\n*   **FPGA的优势与局限**：\n    *   FPGA在数据精细控制和高能效方面具有固有优势。\n    *   然而，近期GPU的优化，尤其是在算术计算方面，缩小了FPGA的优势。\n\n### LUT-LLM解决方案\n\n为克服上述挑战，LUT-LLM利用FPGA丰富的片上存储器资源，提出了一种新的推理范式：\n\n*   **计算范式转变**：将LLM推理从依赖算术运算转变为通过查表实现的存储器计算。\n*   **向量量化存储器操作**：LUT-LLM是首个通过向量量化存储器操作实现10亿+参数LLM推理的FPGA加速器。\n\n### 关键技术与设计\n\nLUT-LLM的设计基于以下核心策略和技术：\n\n*   **激活-权重协同量化**：分析表明，激活-权重协同量化（activation-weight co-quantization）是最有效的量化方案。\n*   **支持技术**：该方案由以下三个关键技术支撑：\n    1.  **带宽感知并行质心搜索**（bandwidth-aware parallel centroid search）：优化数据访问效率。\n    2.  **高效的二维查表**（efficient 2D table lookups）：加速存储器查找过程。\n    3.  **时空混合设计**（spatial-temporal hybrid design）：通过最小化数据缓存来提高整体效率。\n\n### 性能评估与结果\n\nLUT-LLM在实际硬件上进行了实现和评估，展示了卓越的性能：\n\n*   **实现平台**：在AMD V80 FPGA上为定制的Qwen 3 1.7B模型进行了实现。\n*   **性能指标**：\n    *   **延迟**：与AMD MI210相比，延迟降低1.66倍。\n    *   **能效**：与NVIDIA A100相比，能效提高1.72倍。\n*   **可扩展性**：该方法可扩展到32B模型，与NVIDIA A100相比，能效增益达2.16倍。\n\n### 结论\n\nLUT-LLM通过创新的存储器计算方法，为FPGA上的高效LLM推理提供了新的途径，显著提升了性能和能效，并展现了良好的可扩展性。",
      "shortSummary": "LUT-LLM提出一种FPGA加速器，通过利用FPGA片上存储器，将大语言模型（LLM）推理从算术计算转变为基于查表的存储器计算，以提高效率。该方案采用激活-权重协同量化，并结合带宽感知搜索、高效2D查表和时空混合设计。在AMD V80 FPGA上，LUT-LLM对1.7B模型实现比AMD MI210低1.66倍的延迟和比NVIDIA A100高1.72倍的能效，并可扩展至32B模型，能效增益达2.16倍，为设备端LLM推理提供了高效解决方案。",
      "translated_title": "LUT-LLM：基于FPGA存储器计算的高效大语言模型推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "The rapid progress of large language models (LLMs) has advanced numerous applications, yet efficient single-batch inference remains vital for on-device intelligence. While FPGAs offer fine-grained data control and high energy efficiency, recent GPU optimizations have narrowed their advantage, especially under arithmetic-based computation. To overcome this, we leverage FPGAs' abundant on-chip memory to shift LLM inference from arithmetic- to memory-based computation through table lookups. We present LUT-LLM, the first FPGA accelerator enabling 1B+ LLM inference via vector-quantized memory operations. Our analysis identifies activation-weight co-quantization as the most effective scheme, supported by (1) bandwidth-aware parallel centroid search, (2) efficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing data caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B model, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher energy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency gain over A100."
    },
    {
      "title": "SWE-fficiency：语言模型能否在真实工作负载下优化真实世界的代码库？ (原标题: SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?)",
      "link": "https://arxiv.org/abs/2511.06090",
      "pubDate": "Sat, 08 Nov 2025 12:55:09 GMT",
      "isoDate": "2025-11-08T12:55:09.000Z",
      "creator": "Jeffrey Jian Ma, Milad Hashemi, Amir Yazdanbakhsh, Kevin Swersky, Ofir Press, Enhui Li, Vijay Janapa Reddi, Parthasarathy Ranganathan",
      "summary": "## SWE-fficiency：评估语言模型优化真实世界代码库性能的新基准\n\n### 1. 背景与挑战\n\n*   优化大规模软件代码库的性能，需要深入的代码推理能力和专业的软件工程（SWE）知识，以在保持程序正确性的前提下减少运行时长。\n*   然而，当前大多数基准测试侧重于识别“需要修复什么”的问题，而非指导“如何修复”代码以实现性能提升。\n\n### 2. 引入 SWE-fficiency 基准\n\n*   **目标：** 我们推出了 \\textsc{SWE-fficiency}，这是一个专门用于评估语言模型在真实工作负载下进行代码库级别性能优化的新基准。\n*   **规模与范围：** 该套件包含498个任务，涵盖了九个广泛使用的数据科学、机器学习和高性能计算（HPC）代码库，例如 numpy、pandas 和 scipy。\n*   **代理任务：** 给定一个完整的代码库和一个运行缓慢的工作负载，代理必须完成以下任务：\n    *   调查代码语义。\n    *   定位性能瓶颈和相关的测试。\n    *   生成一个补丁，该补丁的加速效果需匹配或超越专家水平，同时通过所有相同的单元测试。\n\n### 3. 自动化数据管道\n\n*   为了实现这种“如何修复”的评估，我们开发了一个自动化管道。\n*   **数据来源：** 该管道从 GitHub 拉取请求中抓取了性能改进的编辑。\n*   **验证机制：** 它结合了关键词过滤、静态分析、覆盖率工具和执行验证，以：\n    *   确认专家加速的基线。\n    *   识别相关的代码库单元测试。\n\n### 4. 实验结果\n\n*   对当前最先进的代理进行了实证评估，结果显示它们表现出显著的不足。\n*   **性能差距：** 代理平均只能达到专家加速效果的不到0.15倍。\n*   **主要挑战：** 代理在以下方面面临困难：\n    *   定位优化机会。\n    *   跨函数执行推理。\n    *   在提出的修改中保持代码的正确性。\n\n### 5. 意义与展望\n\n*   我们发布了此基准及其配套的数据管道，旨在促进自动化性能工程和长周期软件推理领域的研究。",
      "shortSummary": "SWE-fficiency是一个新基准，旨在评估语言模型在真实工作负载下优化真实世界代码库性能的能力。该基准包含498个任务，要求代理定位性能瓶颈、生成能匹配或超越专家加速效果且保持代码正确性的补丁。初步评估显示，当前最先进的代理表现不佳，平均加速效果不到专家水平的0.15倍，主要挑战在于瓶颈定位、跨函数推理和正确性维护。该基准的发布旨在推动自动化性能工程研究。",
      "translated_title": "SWE-fficiency：语言模型能否在真实工作负载下优化真实世界的代码库？",
      "images": [],
      "contentSource": "完整文章",
      "content": "Optimizing the performance of large-scale software repositories demands expertise in code reasoning and software engineering (SWE) to reduce runtime while preserving program correctness. However, most benchmarks emphasize what to fix rather than how to fix code. We introduce SWE-fficiency, a benchmark for evaluating repository-level performance optimization on real workloads. Our suite contains 498 tasks across nine widely used data-science, machine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a complete codebase and a slow workload, an agent must investigate code semantics, localize bottlenecks and relevant tests, and produce a patch that matches or exceeds expert speedup while passing the same unit tests. To enable this how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests for performance-improving edits, combining keyword filtering, static analysis, coverage tooling, and execution validation to both confirm expert speedup baselines and identify relevant repository unit tests. Empirical evaluation of state-of-the-art agents reveals significant underperformance. On average, agents achieve less than 0.15x the expert speedup: agents struggle in localizing optimization opportunities, reasoning about execution across functions, and maintaining correctness in proposed edits. We release the benchmark and accompanying data pipeline to facilitate research on automated performance engineering and long-horizon software reasoning."
    },
    {
      "title": "10个开放性挑战引领视觉-语言-动作模型的未来 (原标题: 10 Open Challenges Steering the Future of Vision-Language-Action Models)",
      "link": "https://arxiv.org/abs/2511.05936",
      "pubDate": "Sat, 08 Nov 2025 04:02:13 GMT",
      "isoDate": "2025-11-08T04:02:13.000Z",
      "creator": "Soujanya Poria, Navonil Majumder, Chia-Yu Hung, Amir Ali Bagherzadeh, Chuan Li, Kenneth Kwok, Ziwei Wang, Cheston Tan, Jiajun Wu, David Hsu",
      "summary": "## 视觉-语言-动作（VLA）模型的未来挑战与趋势\n\n### 引言\n视觉-语言-动作（VLA）模型因其遵循自然语言指令的能力，在具身AI领域日益普及。它们是大型语言模型（LLMs）和视觉-语言模型（VLMs）成功后的自然演进。\n\n### 10个主要里程碑/挑战\n本文深入探讨了VLA模型持续发展中的10个主要里程碑，这些挑战是推动VLA技术走向成熟的关键：\n\n*   **多模态 (Multimodality)**：如何有效整合和处理来自不同模态的信息（视觉、语言、动作）。\n*   **推理 (Reasoning)**：提升VLA模型进行复杂逻辑推理和常识推理的能力。\n*   **数据 (Data)**：获取、标注和利用高质量、多样化的数据来训练和改进VLA模型。\n*   **评估 (Evaluation)**：开发全面、可靠的评估方法来衡量VLA模型的性能和泛化能力。\n*   **跨机器人动作泛化 (Cross-robot action generalization)**：使模型能够将其学到的技能和动作泛化到不同类型或配置的机器人上。\n*   **效率 (Efficiency)**：提高VLA模型的计算效率和资源利用率，使其能在实际应用中部署。\n*   **全身协调 (Whole-body coordination)**：实现机器人全身各部分的协调运动，以完成复杂任务。\n*   **安全 (Safety)**：确保VLA模型在与物理世界交互时的安全性和鲁棒性，避免潜在的危险。\n*   **智能体 (Agents)**：构建能够自主学习、适应环境并与人类有效协作的智能体。\n*   **与人类协调 (Coordination with humans)**：增强VLA模型理解人类意图、预测人类行为并与人类进行流畅交互的能力。\n\n### 新兴趋势\n为了应对上述挑战并实现这些里程碑，文章还讨论了以下几个新兴趋势：\n\n*   **空间理解 (Spatial understanding)**：增强模型对三维空间和物体关系的理解。\n*   **世界动态建模 (Modeling world dynamics)**：让模型能够预测物理世界的变化和交互结果。\n*   **后训练 (Post training)**：在模型预训练之后，通过特定任务的微调或持续学习来提升性能。\n*   **数据合成 (Data synthesis)**：利用合成数据来扩充训练集，解决真实数据获取困难的问题。\n\n### 目标\n通过对这些挑战和新兴趋势的讨论，作者旨在引起研究界对这些关键研究方向的关注，以期加速VLA模型的开发进程，并最终使其在更广泛的领域中获得接受和应用。",
      "shortSummary": "本文探讨了视觉-语言-动作（VLA）模型在具身AI领域的未来发展。文章识别了10个关键挑战，包括多模态、推理、数据、评估、跨机器人泛化、效率、全身协调、安全、智能体和人机协调。同时，文章还讨论了空间理解、世界动态建模、后训练和数据合成等新兴趋势，旨在加速VLA模型的进步和广泛应用，以期获得更广泛的接受。",
      "translated_title": "10个开放性挑战引领视觉-语言-动作模型的未来",
      "images": [],
      "contentSource": "完整文章",
      "content": "Due to their ability of follow natural language instructions, vision-language-action (VLA) models are increasingly prevalent in the embodied AI arena, following the widespread success of their precursors -- LLMs and VLMs. In this paper, we discuss 10 principal milestones in the ongoing development of VLA models -- multimodality, reasoning, data, evaluation, cross-robot action generalization, efficiency, whole-body coordination, safety, agents, and coordination with humans. Furthermore, we discuss the emerging trends of using spatial understanding, modeling world dynamics, post training, and data synthesis -- all aiming to reach these milestones. Through these discussions, we hope to bring attention to the research avenues that may accelerate the development of VLA models into wider acceptability."
    },
    {
      "title": "视觉空间调优 (原标题: Visual Spatial Tuning)",
      "link": "https://arxiv.org/abs/2511.05491",
      "pubDate": "Fri, 07 Nov 2025 13:59:16 GMT",
      "isoDate": "2025-11-07T13:59:16.000Z",
      "creator": "Rui Yang, Ziyu Zhu, Yanwei Li, Jingjia Huang, Shen Yan, Siyuan Zhou, Zhe Liu, Xiangtai Li, Shuangye Li, Wenqian Wang, Yi Lin, Hengshuang Zhao",
      "summary": "## 视觉空间调优 (VST)：增强视觉-语言模型的空间能力\n\n### 核心问题\n\n捕捉视觉输入中的空间关系是实现类人通用智能的关键。现有的视觉-语言模型 (VLM) 通常通过添加额外的专家编码器来增强空间感知，但这会带来额外的开销，并可能损害模型的通用能力。\n\n### 解决方案：视觉空间调优 (VST) 框架\n\n本文提出了一种名为“视觉空间调优 (VST)”的综合框架，旨在培养 VLM 具备类人的视觉空间能力，涵盖从空间感知到空间推理的各个方面，同时不影响其通用能力。\n\n### VST 的主要组成部分和方法\n\n1.  **增强空间感知：VST-P 数据集**\n    *   为了提升 VLM 的空间感知能力，研究团队构建了一个大规模数据集 VST-P。\n    *   该数据集包含 **410 万个样本**，涵盖了跨单视图、多图像和视频的 **19 种不同技能**。\n\n2.  **提升空间推理：VST-R 数据集**\n    *   为了指导模型进行空间推理，研究团队提出了一个精选数据集 VST-R。\n    *   该数据集包含 **13.5 万个样本**，专门用于训练模型的空间推理能力。\n\n3.  **渐进式训练流程**\n    *   VST 采用了一种渐进式的训练流程：\n        *   **监督微调 (Supervised Fine-tuning)**：首先进行监督微调，以建立模型的基础空间知识。\n        *   **强化学习 (Reinforcement Learning)**：随后通过强化学习进一步提升模型的空间推理能力。\n\n### 实验结果与影响\n\n*   **卓越的性能**：所提出的 VST 框架在多个空间基准测试中持续取得了最先进 (SOTA) 的结果。\n    *   在 MMSI-Bench 上达到 **34.8%** 的性能。\n    *   在 VSIBench 上达到 **61.2%** 的性能。\n*   **无副作用**：VST 在增强空间能力的同时，未对模型的通用能力产生负面影响。\n*   **未来展望**：研究表明，通过这种空间调优范式，视觉-语言-动作模型可以得到显著增强，为开发更具物理基础的 AI 铺平了道路。",
      "shortSummary": "本文提出了“视觉空间调优 (VST)”框架，旨在提升视觉-语言模型 (VLM) 的类人视觉空间能力。VST 通过构建大规模的 VST-P (空间感知) 和 VST-R (空间推理) 数据集，并采用监督微调结合强化学习的渐进式训练流程。实验结果显示，VST 在多个空间基准测试中取得了最先进的性能，且未损害模型的通用能力，为构建更具物理基础的 AI 奠定了基础。",
      "translated_title": "视觉空间调优",
      "images": [],
      "contentSource": "完整文章",
      "content": "Capturing spatial relationships from visual inputs is a cornerstone of human-like general intelligence. Several previous studies have tried to enhance the spatial awareness of Vision-Language Models (VLMs) by adding extra expert encoders, which brings extra overhead and usually harms general capabilities. To enhance the spatial ability in general architectures, we introduce Visual Spatial Tuning (VST), a comprehensive framework to cultivate VLMs with human-like visuospatial abilities, from spatial perception to reasoning. We first attempt to enhance spatial perception in VLMs by constructing a large-scale dataset termed VST-P, which comprises 4.1 million samples spanning 19 skills across single views, multiple images, and videos. Then, we present VST-R, a curated dataset with 135K samples that instruct models to reason in space. In particular, we adopt a progressive training pipeline: supervised fine-tuning to build foundational spatial knowledge, followed by reinforcement learning to further improve spatial reasoning abilities. Without the side-effect to general capabilities, the proposed VST consistently achieves state-of-the-art results on several spatial benchmarks, including 34.8% on MMSI-Bench and 61.2% on VSIBench. It turns out that the Vision-Language-Action models can be significantly enhanced with the proposed spatial tuning paradigm, paving the way for more physically grounded AI."
    },
    {
      "title": "密集运动字幕生成 (原标题: Dense Motion Captioning)",
      "link": "https://arxiv.org/abs/2511.05369",
      "pubDate": "Fri, 07 Nov 2025 10:55:10 GMT",
      "isoDate": "2025-11-07T10:55:10.000Z",
      "creator": "Shiyao Xu, Benedetta Liberatori, Gül Varol, Paolo Rota",
      "summary": "# 密集运动字幕生成：3D人体运动理解的新范式\n\n## 摘要\n当前3D人体运动与语言整合的研究主要集中在文本到运动的生成，而运动理解任务相对未被充分探索。本文引入了一项名为“密集运动字幕生成”的新任务，旨在对3D人体运动序列中的动作进行时间定位和字幕描述。\n\n## 核心问题\n*   现有研究偏重文本到运动生成，运动理解领域探索不足。\n*   现有数据集在提供详细时间注释方面不足，且多由包含少量动作的短序列组成。\n\n## 解决方案与贡献\n\n### 1. Complex Motion Dataset (CompMo)\n为了克服现有数据集的局限性，本文提出了CompMo，这是首个大规模、具有丰富注释和精确时间边界的复杂运动数据集。\n*   **规模与构成：** 包含60,000个运动序列。\n*   **复杂性：** 每个序列由至少2到10个动作组成。\n*   **注释质量：** 动作及其时间范围均经过精确标注。\n*   **生成方式：** 通过精心设计的数据生成流程构建。\n\n### 2. DEMO 模型\n本文还提出了DEMO模型，用于生成密集的、基于时间的字幕。\n*   **架构：** 该模型将一个大型语言模型与一个简单的运动适配器相结合。\n*   **功能：** 经过训练，能够生成密集且具有时间依据的字幕。\n\n## 实验结果与意义\n*   **性能：** 实验证明，DEMO模型在CompMo数据集以及其他经过调整的基准测试中，显著优于现有方法。\n*   **未来展望：** DEMO模型为3D运动理解和字幕生成领域的未来研究奠定了坚实的基础。",
      "shortSummary": "本文引入了“密集运动字幕生成”这一新任务，旨在对3D人体运动序列中的动作进行时间定位和描述。为解决现有数据集不足的问题，研究者构建了首个大规模、高注释质量的复杂运动数据集CompMo。同时，提出了DEMO模型，该模型结合大型语言模型与运动适配器，能生成密集的、基于时间的字幕。实验表明，DEMO在CompMo及其他基准测试中表现出色，为3D运动理解和字幕生成奠定了基础。",
      "translated_title": "密集运动字幕生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in 3D human motion and language integration have primarily focused on text-to-motion generation, leaving the task of motion understanding relatively unexplored. We introduce Dense Motion Captioning, a novel task that aims to temporally localize and caption actions within 3D human motion sequences. Current datasets fall short in providing detailed temporal annotations and predominantly consist of short sequences featuring few actions. To overcome these limitations, we present the Complex Motion Dataset (CompMo), the first large-scale dataset featuring richly annotated, complex motion sequences with precise temporal boundaries. Built through a carefully designed data generation pipeline, CompMo includes 60,000 motion sequences, each composed of multiple actions ranging from at least two to ten, accurately annotated with their temporal extents. We further present DEMO, a model that integrates a large language model with a simple motion adapter, trained to generate dense, temporally grounded captions. Our experiments show that DEMO substantially outperforms existing methods on CompMo as well as on adapted benchmarks, establishing a robust baseline for future research in 3D motion understanding and captioning."
    }
  ],
  "lastUpdated": "2025-11-11T09:36:55.104Z"
}