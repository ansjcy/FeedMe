{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "VideoMathQA：通过视频中的多模态理解来衡量数学推理能力 (原标题: VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal Understanding in Videos)",
      "link": "https://arxiv.org/abs/2506.05349",
      "pubDate": "Thu, 05 Jun 2025 13:59:58 GMT",
      "isoDate": "2025-06-05T13:59:58.000Z",
      "creator": "Hanoona Rasheed, Abdelrahman Shaker, Anqi Tang, Muhammad Maaz, Ming-Hsuan Yang, Salman Khan, Fahad Khan",
      "summary": "## VideoMathQA：视频中的多模态数学推理基准\n\n### 引言\n在真实世界的视频环境中进行数学推理，与在静态图像或文本中进行推理相比，面临着根本性的不同挑战。这要求模型能够解释细粒度的视觉信息、准确读取手写或数字文本，并整合通常非线性分散在时间上的口头提示。在这种多模态背景下，模型的成功不仅取决于感知能力，更在于从丰富而嘈杂的内容流中选择性地识别和整合正确的上下文细节。\n\n### VideoMathQA 基准介绍\n为了解决上述挑战，本文引入了 **VideoMathQA**，这是一个专门设计的基准，旨在评估模型是否能够执行这种时间上扩展的跨模态推理任务。\n\n### 基准范围与特点\n*   **领域多样性：** VideoMathQA 涵盖了10个不同的数学领域。\n*   **视频长度：** 基准中的视频时长从10秒到1小时以上不等，模拟了真实世界的多样性。\n*   **模态整合要求：** 它要求模型能够：\n    *   解释结构化的视觉内容。\n    *   理解教学叙述。\n    *   联合地在视觉、音频和文本模态中建立概念关联。\n\n### 质量保证\n为了确保基准的高质量和可靠性，研究团队聘请了研究生级别的专家进行标注工作，总计投入了超过920个人工小时。\n\n### 问题设计：三大核心推理挑战\n为了反映真实世界的场景，VideoMathQA 中的问题围绕以下三个核心推理挑战进行设计：\n1.  **直接问题解决：** 答案直接来源于视频中呈现的问题。\n2.  **概念迁移：** 要求模型将学到的方法和知识应用于新的、未曾直接出现过的问题。\n3.  **深度教学理解：** 涉及对扩展的解释和部分已解决的方案进行多步骤推理。\n\n### 诊断能力\n每个问题都包含详细的多步骤推理标注，这使得研究人员能够对模型的能力进行细粒度的诊断和分析。\n\n### 研究意义与影响\n通过引入 VideoMathQA 基准，本文旨在：\n*   揭示现有方法在处理复杂多模态数学推理任务时的局限性。\n*   建立一个系统的评估框架，用于评估那些必须在时间上扩展且模态丰富的数学问题设置中进行“推理”（而不仅仅是“感知”）的模型。\n\n### 可用性\nVideoMathQA 基准及其评估代码已对外提供。",
      "shortSummary": "VideoMathQA是一个新颖的基准，旨在评估模型在视频中进行多模态数学推理的能力。它涵盖10个数学领域，要求模型整合视觉、音频和文本信息，以解决从直接问题到深度教学理解的各类挑战。该基准由专家高质量标注，旨在揭示现有方法的局限性，并为在复杂视频环境中进行数学推理的模型提供一个系统评估框架。",
      "translated_title": "VideoMathQA：通过视频中的多模态理解来衡量数学推理能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Mathematical reasoning in real-world video settings presents a fundamentally different challenge than in static images or text. It requires interpreting fine-grained visual information, accurately reading handwritten or digital text, and integrating spoken cues, often dispersed non-linearly over time. In such multimodal contexts, success hinges not just on perception, but on selectively identifying and integrating the right contextual details from a rich and noisy stream of content. To this end, we introduce VideoMathQA, a benchmark designed to evaluate whether models can perform such temporally extended cross-modal reasoning on videos. The benchmark spans 10 diverse mathematical domains, covering videos ranging from 10 seconds to over 1 hour. It requires models to interpret structured visual content, understand instructional narratives, and jointly ground concepts across visual, audio, and textual modalities. We employ graduate-level experts to ensure high quality, totaling over 920 man-hours of annotation. To reflect real-world scenarios, questions are designed around three core reasoning challenges: direct problem solving, where answers are grounded in the presented question; conceptual transfer, which requires applying learned methods to new problems; and deep instructional comprehension, involving multi-step reasoning over extended explanations and partially worked-out solutions. Each question includes multi-step reasoning annotations, enabling fine-grained diagnosis of model capabilities. Through this benchmark, we highlight the limitations of existing approaches and establish a systematic evaluation framework for models that must reason, rather than merely perceive, across temporally extended and modality-rich mathematical problem settings. Our benchmark and evaluation code are available at: https://mbzuai-oryx.github.io/VideoMathQA"
    },
    {
      "title": "FreeTimeGS：随时随地自由高斯用于动态场景重建 (原标题: FreeTimeGS: Free Gaussians at Anytime and Anywhere for Dynamic Scene Reconstruction)",
      "link": "https://arxiv.org/abs/2506.05348",
      "pubDate": "Thu, 05 Jun 2025 13:59:57 GMT",
      "isoDate": "2025-06-05T13:59:57.000Z",
      "creator": "Yifan Wang, Peishan Yang, Zhen Xu, Jiaming Sun, Zhanhua Zhang, Yong Chen, Hujun Bao, Sida Peng, Xiaowei Zhou",
      "summary": "# FreeTimeGS：一种用于动态场景重建的新型4D表示\n\n## 引言与背景\n本文旨在解决**具有复杂运动的动态3D场景重建**的挑战。\n*   **现有方法：** 近期一些工作通过在规范空间定义3D高斯基元，并利用形变场将其映射到观察空间，实现了实时动态视图合成。\n*   **现有问题：** 然而，由于形变场优化困难，这些方法往往难以有效处理具有复杂运动的场景。\n\n## FreeTimeGS 方法概述\n为克服上述问题，本文提出了一种名为 **FreeTimeGS** 的新型4D表示。\n*   **核心创新：** FreeTimeGS 允许高斯基元在**任意时间点和任意位置**出现。\n*   **主要优势：**\n    *   与传统的规范高斯基元相比，FreeTimeGS 具有**更强的灵活性**，显著提升了对动态3D场景的建模能力。\n    *   为每个高斯基元赋予了一个**运动函数**，使其能够随时间移动到相邻区域，从而有效**减少了时间冗余**。\n\n## 实验结果\n*   在多个数据集上进行的实验结果表明，FreeTimeGS 的渲染质量**显著优于**最新的现有方法。\n\n## 其他信息\n*   该论文已被 **CVPR 2025** 接收。\n*   所属领域：计算机视觉与模式识别 (cs.CV)。\n*   预印本引用号：arXiv:2506.05348。",
      "shortSummary": "FreeTimeGS是一种新型4D表示，旨在解决复杂动态3D场景的重建问题。它允许高斯基元在任意时间地点自由出现，并赋予其运动函数以减少时间冗余。与现有方法相比，FreeTimeGS具有更强的灵活性和建模能力。实验结果表明，其渲染质量显著优于最新方法。该工作已被CVPR 2025接收。",
      "translated_title": "FreeTimeGS：随时随地自由高斯用于动态场景重建",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper addresses the challenge of reconstructing dynamic 3D scenes with complex motions. Some recent works define 3D Gaussian primitives in the canonical space and use deformation fields to map canonical primitives to observation spaces, achieving real-time dynamic view synthesis. However, these methods often struggle to handle scenes with complex motions due to the difficulty of optimizing deformation fields. To overcome this problem, we propose FreeTimeGS, a novel 4D representation that allows Gaussian primitives to appear at arbitrary time and locations. In contrast to canonical Gaussian primitives, our representation possesses the strong flexibility, thus improving the ability to model dynamic 3D scenes. In addition, we endow each Gaussian primitive with an motion function, allowing it to move to neighboring regions over time, which reduces the temporal redundancy. Experiments results on several datasets show that the rendering quality of our method outperforms recent methods by a large margin."
    },
    {
      "title": "推理时超扩展与KV缓存压缩 (原标题: Inference-Time Hyper-Scaling with KV Cache Compression)",
      "link": "https://arxiv.org/abs/2506.05345",
      "pubDate": "Thu, 05 Jun 2025 13:59:55 GMT",
      "isoDate": "2025-06-05T13:59:55.000Z",
      "creator": "Adrian Łańcucki, Konrad Staniszewski, Piotr Nawrot, Edoardo M. Ponti",
      "summary": "### 推理时超扩展与KV缓存压缩\n\n本文探讨了在大型语言模型（LLMs）中，通过压缩键值（KV）缓存来实现推理时超扩展的方法，以提高推理效率和准确性。\n\n**背景与问题：**\n*   推理时扩展（Inference-time scaling）通过生成更长或更并行的序列来提高推理准确性，但会牺牲效率。\n*   在Transformer架构的LLMs中，生成成本的主要瓶颈在于KV缓存的大小，而非生成的token数量。\n\n**核心思想：推理时超扩展**\n*   通过压缩KV缓存，可以在相同的计算预算内生成更多的token。\n*   这进一步提升了扩展推理的准确性。\n\n**挑战：**\n*   压缩方法必须在高压缩比下仍能保持准确性。\n\n**解决方案：动态内存稀疏化（Dynamic Memory Sparsification, DMS）**\n*   DMS是一种新颖的KV缓存稀疏化方法，旨在使超扩展变得实用。\n*   **训练效率：** 仅需1K训练步骤即可实现8倍压缩。\n*   **准确性优势：** 相比于无需训练的稀疏注意力方法，DMS能保持更好的准确性。\n*   **工作原理：** DMS通过延迟token的逐出，而非过早丢弃，从而隐式地合并表示并保留关键信息。\n\n**效果验证：**\n*   研究在多种LLM家族上验证了DMS在推理时超扩展中的有效性。\n*   结果表明，在可比的推理运行时和内存负载下，DMS显著提升了准确性。\n*   **具体案例：** 在Qwen-R1 32B模型上，DMS在不同计算预算下，平均提升了AIME 24基准9.1分，GPQA基准7.6分，以及LiveCodeBench基准9.6分。\n\n**结论：**\n*   DMS为LLMs的推理时超扩展提供了一种实用且高效的KV缓存压缩方案，有效缓解了KV缓存瓶颈，并在保持计算效率的同时显著提升了模型性能。",
      "shortSummary": "本文提出“推理时超扩展”概念，通过压缩大型语言模型（LLMs）的键值（KV）缓存来提升推理效率和准确性。针对KV缓存大小瓶颈，研究引入了动态内存稀疏化（DMS）方法。DMS仅需1K训练步骤即可实现8倍压缩，并能保持高准确性。实验证明，DMS在多种LLMs上有效提升了推理准确性，例如在Qwen-R1 32B模型上显著提高了多个基准测试的得分，同时保持了可比的运行时和内存负载。",
      "translated_title": "推理时超扩展与KV缓存压缩",
      "images": [],
      "contentSource": "完整文章",
      "content": "Inference-time scaling trades efficiency for increased reasoning accuracy by generating longer or more parallel sequences. However, in Transformer LLMs, generation cost is bottlenecked by the size of the key-value (KV) cache, rather than the number of generated tokens. Hence, we explore inference-time hyper-scaling: by compressing the KV cache, we can generate more tokens within the same compute budget and further improve the accuracy of scaled inference. The success of this approach, however, hinges on the ability of compression methods to preserve accuracy even at high compression ratios. To make hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a novel method for sparsifying KV caches that only requires 1K training steps to achieve 8times compression, while maintaining better accuracy than training-free sparse attention. Instead of prematurely discarding cached tokens, DMS delays token eviction, implicitly merging representations and preserving critical information. We demonstrate the effectiveness of inference-time hyper-scaling with DMS on multiple families of LLMs, showing that it boosts accuracy for comparable inference runtime and memory load. For instance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on GPQA, and 9.6 on LiveCodeBench across compute budgets."
    },
    {
      "title": "SparseMM：多模态大语言模型中视觉概念响应引发的注意力头稀疏性 (原标题: SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs)",
      "link": "https://arxiv.org/abs/2506.05344",
      "pubDate": "Thu, 05 Jun 2025 13:59:55 GMT",
      "isoDate": "2025-06-05T13:59:55.000Z",
      "creator": "Jiahui Wang, Zuyan Liu, Yongming Rao, Jiwen Lu",
      "summary": "### SparseMM：多模态大语言模型中视觉概念响应引发的注意力头稀疏性\n\n本文深入探讨了多模态大语言模型（MLLMs）如何处理视觉输入，特别是通过分析其注意力机制。\n\n#### 核心发现：注意力头稀疏性\n\n*   研究揭示了一个令人惊讶的稀疏现象：在LLMs中，只有一小部分（约低于5%）的注意力头积极地参与视觉理解，这些被称为“视觉头”。\n\n#### 识别视觉头的方法\n\n*   为高效识别这些视觉头，本文设计了一个无需训练的框架。\n*   该框架通过目标响应分析来量化注意力头层面的视觉相关性。\n\n#### SparseMM：KV-Cache优化策略\n\n*   基于上述发现，本文引入了SparseMM，这是一种创新的KV-Cache优化策略。\n*   **工作原理：** SparseMM根据注意力头的视觉得分，为其分配不对称的计算预算，从而利用视觉头的稀疏性来加速MLLMs的推理过程。\n*   **独特优势：** 与以往忽略视觉特殊性的KV-Cache加速方法不同，SparseMM在解码过程中优先强调并保留视觉语义。\n\n#### 实验结果与性能\n\n*   在主流多模态基准测试中进行了广泛评估，结果表明SparseMM实现了卓越的准确性-效率权衡。\n*   **具体性能提升：**\n    *   在生成过程中实现了1.38倍的实时加速。\n    *   内存占用减少了52%。\n    *   在效率测试中保持了性能一致性。",
      "shortSummary": "本文研究发现多模态大语言模型（MLLMs）中仅少数（<5%）注意力头（视觉头）负责视觉理解。基于此，提出SparseMM，一种无需训练的KV-Cache优化策略。SparseMM通过量化视觉相关性识别视觉头，并为其分配不对称计算预算，从而利用视觉稀疏性加速MLLM推理，同时保留视觉语义。实验证明，SparseMM在保持性能的同时，实现了1.38倍实时加速和52%内存减少，展现了优异的准确性-效率权衡。",
      "translated_title": "SparseMM：多模态大语言模型中视觉概念响应引发的注意力头稀疏性",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal Large Language Models (MLLMs) are commonly derived by extending pre-trained Large Language Models (LLMs) with visual capabilities. In this work, we investigate how MLLMs process visual inputs by analyzing their attention mechanisms. We reveal a surprising sparsity phenomenon: only a small subset (approximately less than 5%) of attention heads in LLMs actively contribute to visual understanding, termed visual heads. To identify these heads efficiently, we design a training-free framework that quantifies head-level visual relevance through targeted response analysis. Building on this discovery, we introduce SparseMM, a KV-Cache optimization strategy that allocates asymmetric computation budgets to heads in LLMs based on their visual scores, leveraging the sparity of visual heads for accelerating the inference of MLLMs. Compared with prior KV-Cache acceleration methods that ignore the particularity of visual, SparseMM prioritizes stress and retaining visual semantics during decoding. Extensive evaluations across mainstream multimodal benchmarks demonstrate that SparseMM achieves superior accuracy-efficiency trade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52% memory reduction during generation while maintaining performance parity on efficiency test. Our project is open sourced at https://github.com/CR400AF-A/SparseMM."
    },
    {
      "title": "搜索竞技场：分析搜索增强型大型语言模型 (原标题: Search Arena: Analyzing Search-Augmented LLMs)",
      "link": "https://arxiv.org/abs/2506.05334",
      "pubDate": "Thu, 05 Jun 2025 13:59:26 GMT",
      "isoDate": "2025-06-05T13:59:26.000Z",
      "creator": "Mihran Miroyan, Tsung-Han Wu, Logan King, Tianle Li, Jiayi Pan, Xinyan Hu, Wei-Lin Chiang, Anastasios N. Angelopoulos, Trevor Darrell, Narges Norouzi, Joseph E. Gonzalez",
      "summary": "### 搜索竞技场：分析搜索增强型大型语言模型\n\n**背景与挑战**\n\n*   搜索增强型大型语言模型（Search-augmented LLMs）通过结合网络搜索与大型语言模型（LLMs），旨在提高响应的扎实性和时效性。\n*   然而，分析这些系统面临挑战：现有数据集规模有限，范围狭窄，通常局限于静态、单轮的事实核查问题，难以全面评估。\n\n**引入 Search Arena 数据集**\n\n*   为解决上述挑战，研究人员引入了 **Search Arena**，这是一个大规模、众包、人类偏好数据集。\n*   **数据集特点**：\n    *   包含超过 **24,000 对**多轮用户与搜索增强型LLM的交互。\n    *   涵盖了多样化的用户意图和多种语言。\n    *   包含完整的系统追踪信息。\n    *   收集了约 **12,000 个人类偏好投票**。\n\n**关键分析发现**\n\n*   **引用数量对用户偏好的影响**：\n    *   用户偏好受引用数量的影响，即使被引用的内容并未直接支持所归属的主张。\n    *   这揭示了用户感知到的可信度与实际可信度之间存在差距。\n*   **引用来源的偏好差异**：\n    *   用户偏好因引用的来源而异。\n    *   社区驱动的平台（如论坛、社交媒体）通常更受用户青睐。\n    *   静态的百科全书式来源并非总是合适和可靠的。\n\n**跨竞技场性能评估**\n\n*   研究人员进行了跨竞技场分析，在通用聊天环境和搜索密集型设置中测试了搜索增强型LLMs和传统LLMs。\n*   **网络搜索的益处**：\n    *   网络搜索在非搜索设置中不会降低模型性能，甚至可能有所提升。\n*   **仅依赖参数化知识的局限性**：\n    *   在搜索设置中，如果模型仅依赖其自身的参数化知识（即不进行外部搜索），其响应质量会显著受影响。\n\n**开源与未来研究**\n\n*   Search Arena 数据集和相关代码已开源，旨在支持未来在该方向上的研究。\n*   数据集和代码可在提供的链接中获取。",
      "shortSummary": "Search Arena是一个大规模众包数据集，旨在分析搜索增强型大型语言模型（LLMs）。该数据集包含24,000多对多轮用户交互和12,000个人类偏好投票。分析发现，用户偏好受引用数量和来源类型（偏好社区驱动平台）影响，揭示了感知可信度与实际可信度之间的差距。研究还表明，网络搜索在非搜索场景中不会降低性能，但在搜索场景中，仅依赖模型参数化知识会显著影响质量。数据集和代码已开源。",
      "translated_title": "搜索竞技场：分析搜索增强型大型语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Search-augmented language models combine web search with Large Language Models (LLMs) to improve response groundedness and freshness. However, analyzing these systems remains challenging: existing datasets are limited in scale and narrow in scope, often constrained to static, single-turn, fact-checking questions. In this work, we introduce Search Arena, a crowd-sourced, large-scale, human-preference dataset of over 24,000 paired multi-turn user interactions with search-augmented LLMs. The dataset spans diverse intents and languages, and contains full system traces with around 12,000 human preference votes. Our analysis reveals that user preferences are influenced by the number of citations, even when the cited content does not directly support the attributed claims, uncovering a gap between perceived and actual credibility. Furthermore, user preferences vary across cited sources, revealing that community-driven platforms are generally preferred and static encyclopedic sources are not always appropriate and reliable. To assess performance across different settings, we conduct cross-arena analyses by testing search-augmented LLMs in a general-purpose chat environment and conventional LLMs in search-intensive settings. We find that web search does not degrade and may even improve performance in non-search settings; however, the quality in search settings is significantly affected if solely relying on the model's parametric knowledge. We open-sourced the dataset to support future research in this direction. Our dataset and code are available at: https://github.com/lmarena/search-arena."
    },
    {
      "title": "Kinetics：重新思考测试时缩放定律 (原标题: Kinetics: Rethinking Test-Time Scaling Laws)",
      "link": "https://arxiv.org/abs/2506.05333",
      "pubDate": "Thu, 05 Jun 2025 13:59:24 GMT",
      "isoDate": "2025-06-05T13:59:24.000Z",
      "creator": "Ranajoy Sadhukhan, Zhuoming Chen, Haizhong Zheng, Yang Zhou, Emma Strubell, Beidi Chen",
      "summary": "# Kinetics：重新思考测试时缩放定律\n\n本文重新审视了测试时缩放定律，并从实际效率的角度揭示了现有研究中对小型模型有效性的高估。\n\n## 现有问题与分析范围\n*   **问题所在：** 以计算最优性为基础的现有工作（例如，Best-of-N、长CoTs等推理时策略）忽视了推理过程中引入的关键内存访问瓶颈，从而高估了小型模型的有效性。\n*   **分析范围：** 本研究对参数量从0.6B到32B的模型进行了全面分析。\n\n## 新的“Kinetics缩放定律”\n*   **提出：** 本文提出了一个新的“Kinetics缩放定律”，该定律通过整合计算成本和内存访问成本，能更好地指导资源分配。\n*   **核心发现：** “Kinetics缩放定律”表明，在测试时，将计算资源投入到超过一定阈值的大型模型上，比投入到小型模型上更为有效。\n*   **主导成本因素：** 在测试时缩放（TTS）中，注意力机制而非参数数量，成为主导成本因素。\n\n## 稀疏注意力范式\n*   **动机：** 鉴于注意力机制是测试时的主导成本，本文提出了一种以稀疏注意力为中心的新型缩放范式。\n*   **优势：**\n    *   降低了每token的成本。\n    *   在相同的资源预算内，支持更长的生成序列。\n    *   允许更多的并行样本。\n\n## 实验结果与意义\n*   **性能提升：** 实验结果表明，稀疏注意力模型始终优于其密集注意力对应模型。\n    *   在低成本场景下，解决AIME问题（数学竞赛问题）的准确率提升超过60个百分点。\n    *   在高成本场景下，准确率提升超过5个百分点。\n    *   评估涵盖了最先进的MoE（Mixture-of-Experts）模型。\n*   **重要意义：** 这些结果表明，稀疏注意力对于充分发挥测试时缩放的潜力至关重要。与训练阶段参数缩放会达到饱和不同，测试时精度可以通过增加生成量持续提高。\n\n## 代码可用性\n*   相关代码已公开。",
      "shortSummary": "本文重新思考了测试时缩放定律，指出现有工作因忽视内存瓶颈而高估了小型模型。研究提出了新的“Kinetics缩放定律”，强调测试时注意力是主导成本。为解决此问题，作者提出稀疏注意力范式，能显著降低每token成本并支持更长生成。实验证明，稀疏注意力模型在问题解决准确率上表现优异（AIME上提升超60点），对充分发挥测试时缩放潜力至关重要。",
      "translated_title": "Kinetics：重新思考测试时缩放定律",
      "images": [],
      "contentSource": "完整文章",
      "content": "We rethink test-time scaling laws from a practical efficiency perspective, revealing that the effectiveness of smaller models is significantly overestimated. Prior work, grounded in compute-optimality, overlooks critical memory access bottlenecks introduced by inference-time strategies (e.g., Best-of-N, long CoTs). Our holistic analysis, spanning models from 0.6B to 32B parameters, reveals a new Kinetics Scaling Law that better guides resource allocation by incorporating both computation and memory access costs. Kinetics Scaling Law suggests that test-time compute is more effective when used on models above a threshold than smaller ones. A key reason is that in TTS, attention, rather than parameter count, emerges as the dominant cost factor. Motivated by this, we propose a new scaling paradigm centered on sparse attention, which lowers per-token cost and enables longer generations and more parallel samples within the same resource budget. Empirically, we show that sparse attention models consistently outperform dense counterparts, achieving over 60 points gains in low-cost regimes and over 5 points gains in high-cost regimes for problem-solving accuracy on AIME, encompassing evaluations on state-of-the-art MoEs. These results suggest that sparse attention is essential for realizing the full potential of test-time scaling because, unlike training, where parameter scaling saturates, test-time accuracy continues to improve through increased generation. The code is available at https://github.com/Infini-AI-Lab/Kinetics."
    },
    {
      "title": "MINT-CoT：在数学思维链推理中实现交错视觉令牌 (原标题: MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical Chain-of-Thought Reasoning)",
      "link": "https://arxiv.org/abs/2506.05331",
      "pubDate": "Thu, 05 Jun 2025 13:59:02 GMT",
      "isoDate": "2025-06-05T13:59:02.000Z",
      "creator": "Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, Hongsheng Li",
      "summary": "MINT-CoT：在数学思维链推理中实现交错视觉令牌\n\n*   **背景与挑战**\n    *   思维链（CoT）显著增强了大型语言模型（LLM）的数学推理能力。\n    *   然而，将其扩展到多模态领域（涉及图像）仍面临挑战。\n    *   现有方法存在三个主要局限性：\n        1.  依赖粗粒度的、盒状的图像区域。\n        2.  视觉编码器对数学内容的感知能力有限。\n        3.  依赖外部能力进行视觉修改。\n\n*   **MINT-CoT 提案**\n    *   **核心思想：** 提出 MINT-CoT（Mathematical INterleaved Tokens for Chain-of-Thought），旨在实现数学思维链的视觉推理。\n    *   **工作机制：** 通过一个“交错令牌”（Interleave Token），MINT-CoT 能够自适应地将相关的视觉令牌交错到文本推理步骤中。\n    *   **关键创新：** 能够动态选择数学图形中任意形状的视觉区域，而非局限于传统的盒状区域。\n\n*   **MINT-CoT 数据集**\n    *   **规模：** 构建了包含5.4万个数学问题的 MINT-CoT 数据集。\n    *   **特点：** 数据集中的每个推理步骤都与令牌级别的视觉区域精确对齐。\n    *   **生成：** 采用严格的数据生成流程来确保数据质量和对齐的准确性。\n\n*   **MINT-CoT 训练策略**\n    *   **三阶段方法：** 提出了一种三阶段的 MINT-CoT 训练策略，逐步结合以下阶段：\n        1.  纯文本 CoT 监督微调（SFT）。\n        2.  交错 CoT 监督微调（SFT）。\n        3.  交错 CoT 强化学习（RL）。\n    *   **模型：** 通过此策略训练并得到了 MINT-CoT-7B 模型。\n\n*   **实验结果**\n    *   **有效性：** 广泛的实验证明了 MINT-CoT 方法在数学领域进行有效视觉交错推理的能力。\n    *   **性能提升：** MINT-CoT-7B 模型在多个数学视觉推理基准测试中显著超越了基线模型：\n        *   在 MathVista 上提升 +34.08%。\n        *   在 GeoQA 上提升 +28.78%。\n        *   在 MMStar 上提升 +23.2%。\n\n*   **代码与数据：** 相关代码和数据已公开发布。",
      "shortSummary": "MINT-CoT 提出了一种在数学思维链推理中交错视觉令牌的新方法。针对现有方法在多模态数学推理中依赖粗粒度区域和视觉感知有限的局限，MINT-CoT通过“交错令牌”动态选择任意形状的视觉区域。研究构建了包含5.4万个问题的MINT-CoT数据集，并采用三阶段训练策略。实验表明，MINT-CoT-7B模型在MathVista、GeoQA和MMStar等数学视觉推理基准上性能显著提升，验证了其有效性。",
      "translated_title": "MINT-CoT：在数学思维链推理中实现交错视觉令牌",
      "images": [],
      "contentSource": "完整文章",
      "content": "Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large Language Models (LLMs), but it still remains challenging for extending it to multimodal domains. Existing works either adopt a similar textual reasoning for image input, or seek to interleave visual signals into mathematical CoT. However, they face three key limitations for math problem-solving: reliance on coarse-grained box-shaped image regions, limited perception of vision encoders on math content, and dependence on external capabilities for visual modification. In this paper, we propose MINT-CoT, introducing Mathematical INterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively interleaves relevant visual tokens into textual reasoning steps via an Interleave Token, which dynamically selects visual regions of any shapes within math figures. To empower this capability, we construct the MINT-CoT dataset, containing 54K mathematical problems aligning each reasoning step with visual regions at the token level, accompanied by a rigorous data generation pipeline. We further present a three-stage MINT-CoT training strategy, progressively combining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which derives our MINT-CoT-7B model. Extensive experiments demonstrate the effectiveness of our method for effective visual interleaved reasoning in mathematical domains, where MINT-CoT-7B outperforms the baseline model by +34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our code and data are available at https://github.com/xinyan-cxy/MINT-CoT"
    },
    {
      "title": "AV-Reasoner：改进和评估多模态大语言模型（MLLMs）的线索-接地音视频计数能力 (原标题: AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual Counting for MLLMs)",
      "link": "https://arxiv.org/abs/2506.05328",
      "pubDate": "Thu, 05 Jun 2025 13:58:33 GMT",
      "isoDate": "2025-06-05T13:58:33.000Z",
      "creator": "Lidong Lu, Guo Chen, Zhiqi Li, Yicheng Liu, Tong Lu",
      "summary": "### AV-Reasoner：改进和评估多模态大语言模型（MLLMs）的线索-接地音视频计数能力\n\n**1. 背景与问题**\n\n*   尽管视频理解领域取得了显著进展，但当前的多模态大语言模型（MLLMs）在执行精确的计数任务时仍面临挑战。\n*   现有用于评估计数能力的基准测试存在多重局限性：\n    *   视频长度通常较短。\n    *   查询方式多为封闭式（close-set）。\n    *   缺乏关键的线索标注。\n    *   多模态信息覆盖不足。\n\n**2. 新基准：CG-AV-Counting**\n\n为解决上述问题，本论文引入了CG-AV-Counting，这是一个经过手动标注的线索-接地计数基准，旨在提供更全面和真实的评估环境。\n\n*   **规模与内容：**\n    *   包含1,027个多模态问题。\n    *   拥有5,845个详细标注的线索。\n    *   基于497个长视频构建。\n*   **评估支持：**\n    *   支持黑盒（black-box）评估，即对模型整体性能进行测试。\n    *   支持白盒（white-box）评估，允许深入分析模型的推理过程。\n*   **用途：** 作为一个综合性的测试平台，它适用于评估端到端（end-to-end）计数模型以及基于推理（reasoning-based）的计数方法。\n\n**3. 新模型：AV-Reasoner**\n\n为了探索和提升模型的计数能力，研究团队提出了AV-Reasoner模型。\n\n*   **训练方法：**\n    *   AV-Reasoner采用GRPO（Generalized Reinforcement Policy Optimization）和课程学习（curriculum learning）相结合的方式进行训练。\n    *   这种训练策略旨在使模型能够从相关任务中泛化其计数能力。\n*   **性能表现：**\n    *   AV-Reasoner在多个现有基准测试中取得了最先进（state-of-the-art）的结果。\n    *   这充分证明了强化学习在提升计数能力方面的有效性。\n\n**4. 局限性**\n\n*   尽管AV-Reasoner表现出色，但实验结果显示，在域外（out-of-domain）基准测试中，模型在语言空间进行的推理未能带来显著的性能提升。\n\n**5. 资源发布**\n\n*   本研究的代码和CG-AV-Counting基准已公开发布。",
      "shortSummary": "本文介绍了AV-Reasoner，一个旨在改进多模态大语言模型（MLLMs）音视频计数能力的新模型。为解决现有基准的局限性，作者提出了CG-AV-Counting，一个包含长视频、多模态问题和详细线索标注的综合性计数基准。AV-Reasoner通过强化学习和课程学习训练，在多个基准上实现了最先进的性能。然而，研究发现其在域外基准上的语言空间推理效果不佳。代码和基准已发布。",
      "translated_title": "AV-Reasoner：改进和评估多模态大语言模型（MLLMs）的线索-接地音视频计数能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite progress in video understanding, current MLLMs struggle with counting tasks. Existing benchmarks are limited by short videos, close-set queries, lack of clue annotations, and weak multimodal coverage. In this paper, we introduce CG-AV-Counting, a manually-annotated clue-grounded counting benchmark with 1,027 multimodal questions and 5,845 annotated clues over 497 long videos. It supports both black-box and white-box evaluation, serving as a comprehensive testbed for both end-to-end and reasoning-based counting. To explore ways to improve model's counting capability, we propose AV-Reasoner, a model trained with GRPO and curriculum learning to generalize counting ability from related tasks. AV-Reasoner achieves state-of-the-art results across multiple benchmarks, demonstrating the effectiveness of reinforcement learning. However, experiments show that on out-of-domain benchmarks, reasoning in the language space fails to bring performance gains. The code and benchmark have been realeased on https://av-reasoner.github.io."
    },
    {
      "title": "重新审视前向3D高斯泼溅中的深度表示 (原标题: Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting)",
      "link": "https://arxiv.org/abs/2506.05327",
      "pubDate": "Thu, 05 Jun 2025 13:58:23 GMT",
      "isoDate": "2025-06-05T13:58:23.000Z",
      "creator": "Duochao Shi, Weijie Wang, Donny Y. Chen, Zeyu Zhang, Jia-Wang Bian, Bohan Zhuang, Chunhua Shen",
      "summary": "### 重新审视前向3D高斯泼溅中的深度表示\n\n**引言与背景**\n\n*   深度图在前向3D高斯泼溅（3DGS）管线中被广泛应用，通过将它们反投影到3D点云中以实现新视角合成。\n*   这种方法具有多项优势，包括高效的训练、利用已知的相机姿态以及精确的几何估计。\n\n**核心问题**\n\n*   然而，深度图在物体边界处常常出现不连续性，这导致生成的点云碎片化或稀疏。\n*   这种碎片化和稀疏性会降低渲染质量，是基于深度表示的一个众所周知的局限。\n\n**提出的解决方案：PM-Loss**\n\n*   为解决上述问题，本文引入了一种名为PM-Loss的新型正则化损失。\n*   PM-Loss基于由预训练Transformer预测的“点图”（pointmap）。\n\n**PM-Loss的工作原理与优势**\n\n*   尽管点图本身可能不如深度图精确，但它能有效地强制执行几何平滑性，尤其是在物体边界周围。\n*   通过PM-Loss改进后的深度图，本文提出的方法显著提升了跨多种架构和场景的前向3DGS性能。\n*   实验结果表明，该方法能够持续提供更好的渲染效果。",
      "shortSummary": "前向3D高斯泼溅（3DGS）中的深度图在物体边界处存在不连续性，导致渲染质量下降。为解决此问题，本文引入了PM-Loss，这是一种基于预训练Transformer预测的点图的新型正则化损失。尽管点图本身精度不高，但它能有效强制几何平滑性，尤其是在物体边界。实验表明，该方法显著改善了前向3DGS的渲染效果，提供了更优的视觉质量。",
      "translated_title": "重新审视前向3D高斯泼溅中的深度表示",
      "images": [],
      "contentSource": "完整文章",
      "content": "Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS) pipelines by unprojecting them into 3D point clouds for novel view synthesis. This approach offers advantages such as efficient training, the use of known camera poses, and accurate geometry estimation. However, depth discontinuities at object boundaries often lead to fragmented or sparse point clouds, degrading rendering quality -- a well-known limitation of depth-based representations. To tackle this issue, we introduce PM-Loss, a novel regularization loss based on a pointmap predicted by a pre-trained transformer. Although the pointmap itself may be less accurate than the depth map, it effectively enforces geometric smoothness, especially around object boundaries. With the improved depth map, our method significantly improves the feed-forward 3DGS across various architectures and scenes, delivering consistently better rendering results. Our project page: https://aim-uofa.github.io/PMLoss"
    },
    {
      "title": "MARBLE：CLIP空间中的材质重组与混合 (原标题: MARBLE: Material Recomposition and Blending in CLIP-Space)",
      "link": "https://arxiv.org/abs/2506.05313",
      "pubDate": "Thu, 05 Jun 2025 13:55:16 GMT",
      "isoDate": "2025-06-05T13:55:16.000Z",
      "creator": "Ta-Ying Cheng, Prafull Sharma, Mark Boss, Varun Jampani",
      "summary": "### 引言与背景\n\n基于示例图像编辑图像中物体的材质是计算机视觉和图形学领域的一个活跃研究方向。\n\n### MARBLE方法概述\n\n本文提出了一种名为MARBLE（Material Recomposition and Blending in CLIP-Space）的方法。MARBLE旨在通过在CLIP空间中寻找材质嵌入，并利用这些嵌入来控制预训练的文本到图像模型，从而实现材质的混合与细粒度材质属性的重组。\n\n### 核心创新与机制\n\n*   **改进示例材质编辑：** MARBLE通过识别去噪UNet中负责材质归因的特定模块，改进了基于示例的材质编辑。\n*   **材质混合：** 给定两个材质示例图像，MARBLE能够在CLIP空间中找到用于混合这些材质的方向。\n*   **参数化控制：** 该方法利用一个浅层网络预测所需材质属性（如粗糙度、金属度、透明度和发光度）变化的CLIP方向，从而实现对这些细粒度属性的参数化控制。\n\n### 方法能力与评估\n\n*   通过定性和定量分析证明了所提出方法的有效性。\n*   MARBLE还能够在一个前向传播中执行多次编辑。\n*   该方法也适用于绘画应用。",
      "shortSummary": "MARBLE是一种在CLIP空间中进行图像材质编辑和重组的新方法。它通过在CLIP空间中寻找材质嵌入并控制预训练的文本到图像模型，实现了基于示例的材质混合和对粗糙度、金属度、透明度、发光度等细粒度属性的参数化控制。该方法识别了去噪UNet中负责材质的模块，并能在一个前向传播中执行多项编辑，适用于图像材质编辑和绘画。",
      "translated_title": "MARBLE：CLIP空间中的材质重组与混合",
      "images": [],
      "contentSource": "完整文章",
      "content": "Editing materials of objects in images based on exemplar images is an active area of research in computer vision and graphics. We propose MARBLE, a method for performing material blending and recomposing fine-grained material properties by finding material embeddings in CLIP-space and using that to control pre-trained text-to-image models. We improve exemplar-based material editing by finding a block in the denoising UNet responsible for material attribution. Given two material exemplar-images, we find directions in the CLIP-space for blending the materials. Further, we can achieve parametric control over fine-grained material attributes such as roughness, metallic, transparency, and glow using a shallow network to predict the direction for the desired material attribute change. We perform qualitative and quantitative analysis to demonstrate the efficacy of our proposed method. We also present the ability of our method to perform multiple edits in a single forward pass and applicability to painting.   Project Page: https://marblecontrol.github.io/"
    },
    {
      "title": "SeedVR2：通过扩散对抗后训练实现一步视频恢复 (原标题: SeedVR2: One-Step Video Restoration via Diffusion Adversarial Post-Training)",
      "link": "https://arxiv.org/abs/2506.05301",
      "pubDate": "Thu, 05 Jun 2025 13:51:05 GMT",
      "isoDate": "2025-06-05T13:51:05.000Z",
      "creator": "Jianyi Wang, Shanchuan Lin, Zhijie Lin, Yuxi Ren, Meng Wei, Zongsheng Yue, Shangchen Zhou, Hao Chen, Yang Zhao, Ceyuan Yang, Xuefeng Xiao, Chen Change Loy, Lu Jiang",
      "summary": "## SeedVR2：一步视频恢复模型\n\n### 挑战与背景\n\n当前基于扩散的视频恢复（VR）方法在视觉质量上取得了显著提升，但在推理过程中计算成本高昂。尽管一些基于蒸馏的方法已展现出一步图像恢复的潜力，但将其扩展到视频恢复，尤其是在处理高分辨率视频的实际场景中，仍然具有挑战性且有待深入探索。\n\n### SeedVR2 模型介绍\n\n本文提出了一种名为 **SeedVR2** 的一步扩散基视频恢复模型。该模型通过对真实数据进行对抗性视频恢复训练，旨在解决现有方法的计算效率问题。\n\n### 关键技术增强\n\n为了在单一步骤内处理具有挑战性的高分辨率视频恢复任务，SeedVR2 在模型架构和训练流程上引入了多项增强：\n\n*   **自适应窗口注意力机制（Adaptive Window Attention Mechanism）**：\n    *   该机制能够动态调整窗口大小，以适应不同的输出分辨率。\n    *   有效避免了在使用预定义窗口大小的窗口注意力机制时，在高分辨率视频恢复中出现的窗口不一致问题。\n\n*   **对抗性后训练的损失函数优化**：\n    *   研究验证了一系列损失函数在稳定和改进视频恢复对抗性后训练方面的有效性。\n    *   其中包括一种新提出的特征匹配损失，该损失在不显著牺牲训练效率的前提下，进一步提升了模型性能。\n\n### 实验结果\n\n广泛的实验结果表明，SeedVR2 能够在单一步骤内实现与现有视频恢复方法相当甚至更优的性能。",
      "shortSummary": "SeedVR2 是一种新颖的一步扩散基视频恢复（VR）模型，旨在解决现有扩散VR方法计算成本高的问题。它通过对真实数据进行对抗性训练，并引入了自适应窗口注意力机制和优化的损失函数（包括特征匹配损失），以高效处理高分辨率视频。实验证明，SeedVR2 能在单一步骤内达到与现有VR方法相当或更优的性能。",
      "translated_title": "SeedVR2：通过扩散对抗后训练实现一步视频恢复",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in diffusion-based video restoration (VR) demonstrate significant improvement in visual quality, yet yield a prohibitive computational cost during inference. While several distillation-based approaches have exhibited the potential of one-step image restoration, extending existing approaches to VR remains challenging and underexplored, particularly when dealing with high-resolution video in real-world settings. In this work, we propose a one-step diffusion-based VR model, termed as SeedVR2, which performs adversarial VR training against real data. To handle the challenging high-resolution VR within a single step, we introduce several enhancements to both model architecture and training procedures. Specifically, an adaptive window attention mechanism is proposed, where the window size is dynamically adjusted to fit the output resolutions, avoiding window inconsistency observed under high-resolution VR using window attention with a predefined window size. To stabilize and improve the adversarial post-training towards VR, we further verify the effectiveness of a series of losses, including a proposed feature matching loss without significantly sacrificing training efficiency. Extensive experiments show that SeedVR2 can achieve comparable or even better performance compared with existing VR approaches in a single step."
    },
    {
      "title": "EOC-Bench：多模态大语言模型能否在第一人称视角世界中识别、回忆和预测物体？ (原标题: EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an Egocentric World?)",
      "link": "https://arxiv.org/abs/2506.05287",
      "pubDate": "Thu, 05 Jun 2025 13:44:12 GMT",
      "isoDate": "2025-06-05T13:44:12.000Z",
      "creator": "Yuqian Yuan, Ronghao Dang, Long Li, Wentong Li, Dian Jiao, Xin Li, Deli Zhao, Fan Wang, Wenqiao Zhang, Jun Xiao, Yueting Zhuang",
      "summary": "## EOC-Bench：评估多模态大语言模型（MLLMs）在第一人称视角下的物体认知能力\n\n### 引言\n多模态大语言模型（MLLMs）的出现推动了第一人称视角（egocentric vision）应用领域的重大突破。这类应用要求模型能够对物体进行持续的、上下文感知的理解，因为用户在动态且杂乱的环境中与工具进行交互。然而，现有的大多数具身（embodied）基准测试主要侧重于静态场景探索，强调物体的外观和空间属性，却忽视了评估由用户交互引起的动态变化。\n\n### EOC-Bench：创新性基准测试\n为了弥补这一关键空白，研究人员引入了EOC-Bench。这是一个创新的基准测试，专门设计用于系统地评估多模态大语言模型在动态第一人称视角场景中的以物体为中心的具身认知能力。\n\n### EOC-Bench 的主要特点\nEOC-Bench数据集和评估框架具有以下显著特征：\n*   **数据集规模与构成**：包含3,277个经过精心标注的问答对（QA pairs）。\n*   **时间维度分类**：这些问答对被细致地归类为三个关键时间类别，以评估模型对时间变化的理解：\n    *   过去（Past）\n    *   现在（Present）\n    *   未来（Future）\n*   **评估维度**：涵盖11个细粒度的评估维度，确保对模型能力的全面考量。\n*   **视觉物体引用类型**：包含3种不同的视觉物体引用类型，以测试模型在不同引用方式下的理解能力。\n\n### 评估方法与指标\n为了确保评估的彻底性和有效性，EOC-Bench采用了以下先进的方法和指标：\n*   **混合格式的人机协作标注框架**：设计了四种不同类型的问题，结合人工标注，以提高数据质量和多样性。\n*   **新型多尺度时间准确性指标**：引入了一种新颖的多尺度时间准确性指标，专门用于开放式的时间评估，能够更精确地衡量模型对时间动态的把握。\n\n### 实验与重要意义\n基于EOC-Bench，研究人员对各种专有、开源以及物体级别的多模态大语言模型进行了全面的评估。EOC-Bench被视为提升多模态大语言模型具身物体认知能力的关键工具，它为开发可靠的具身系统核心模型奠定了坚实的基础，有助于推动该领域的进一步发展。",
      "shortSummary": "EOC-Bench是一个新的基准测试，旨在评估多模态大语言模型（MLLMs）在动态第一人称视角世界中对物体的识别、回忆和预测能力。它通过3,277个标注的问答对，涵盖过去、现在、未来三个时间维度、11个评估维度和3种物体引用类型，弥补了现有具身基准忽视动态交互的不足。EOC-Bench采用人机协作标注和新型时间准确性指标，对多种MLLMs进行了评估，为发展可靠的具身系统核心模型提供了重要工具。",
      "translated_title": "EOC-Bench：多模态大语言模型能否在第一人称视角世界中识别、回忆和预测物体？",
      "images": [],
      "contentSource": "完整文章",
      "content": "The emergence of multimodal large language models (MLLMs) has driven breakthroughs in egocentric vision applications. These applications necessitate persistent, context-aware understanding of objects, as users interact with tools in dynamic and cluttered environments. However, existing embodied benchmarks primarily focus on static scene exploration, emphasizing object's appearance and spatial attributes while neglecting the assessment of dynamic changes arising from users' interactions. To address this gap, we introduce EOC-Bench, an innovative benchmark designed to systematically evaluate object-centric embodied cognition in dynamic egocentric scenarios. Specially, EOC-Bench features 3,277 meticulously annotated QA pairs categorized into three temporal categories: Past, Present, and Future, covering 11 fine-grained evaluation dimensions and 3 visual object referencing types. To ensure thorough assessment, we develop a mixed-format human-in-the-loop annotation framework with four types of questions and design a novel multi-scale temporal accuracy metric for open-ended temporal evaluation. Based on EOC-Bench, we conduct comprehensive evaluations of various proprietary, open-source, and object-level MLLMs. EOC-Bench serves as a crucial tool for advancing the embodied object cognitive capabilities of MLLMs, establishing a robust foundation for developing reliable core models for embodied systems."
    },
    {
      "title": "具有长期空间记忆的视频世界模型 (原标题: Video World Models with Long-term Spatial Memory)",
      "link": "https://arxiv.org/abs/2506.05284",
      "pubDate": "Thu, 05 Jun 2025 13:42:34 GMT",
      "isoDate": "2025-06-05T13:42:34.000Z",
      "creator": "Tong Wu, Shuai Yang, Ryan Po, Yinghao Xu, Ziwei Liu, Dahua Lin, Gordon Wetzstein",
      "summary": "## 具有长期空间记忆的视频世界模型\n\n### 引言\n\n新兴的视频世界模型能够根据动作（如摄像机移动和文本提示等控制信号）自回归地生成视频帧。然而，由于时间上下文窗口大小的限制，这些模型在场景重访时往往难以保持一致性，导致对先前生成环境的严重“遗忘”。\n\n### 研究背景与问题\n\n*   **当前挑战**：现有的视频世界模型在生成视频时，由于其有限的时间上下文窗口，难以在场景重访时保持一致性。\n*   **具体表现**：模型容易“遗忘”之前生成的环境，导致场景在后续生成中出现不连贯或不一致的问题。\n\n### 解决方案：引入长期空间记忆\n\n*   **灵感来源**：研究团队从人类记忆机制中获得启发。\n*   **提出的框架**：引入了一种新颖的框架，通过“基于几何的长期空间记忆”来增强视频世界模型的长期一致性。\n*   **核心机制**：该框架包含用于从长期空间记忆中存储和检索信息的机制。\n\n### 实验与评估\n\n*   **数据集**：研究人员策划了定制数据集，专门用于训练和评估具有明确存储3D记忆机制的世界模型。\n*   **评估结果**：与相关基线模型相比，该方法在以下方面显示出显著改进：\n    *   生成质量\n    *   场景一致性\n    *   上下文长度\n\n### 结论与展望\n\n这项工作为实现长期一致的世界生成铺平了道路，有望解决当前视频世界模型在保持场景连贯性方面的核心难题。",
      "shortSummary": "当前视频世界模型因时间上下文限制，在场景重访时存在“遗忘”和一致性问题。受人类记忆启发，本文提出一种新颖框架，通过“基于几何的长期空间记忆”来增强模型一致性。该框架包含信息存储与检索机制，并使用定制数据集进行训练和评估。实验结果表明，与基线模型相比，新方法显著提升了生成质量、一致性和上下文长度，为实现长期一致的世界生成奠定了基础。",
      "translated_title": "具有长期空间记忆的视频世界模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Emerging world models autoregressively generate video frames in response to actions, such as camera movements and text prompts, among other control signals. Due to limited temporal context window sizes, these models often struggle to maintain scene consistency during revisits, leading to severe forgetting of previously generated environments. Inspired by the mechanisms of human memory, we introduce a novel framework to enhancing long-term consistency of video world models through a geometry-grounded long-term spatial memory. Our framework includes mechanisms to store and retrieve information from the long-term spatial memory and we curate custom datasets to train and evaluate world models with explicitly stored 3D memory mechanisms. Our evaluations show improved quality, consistency, and context length compared to relevant baselines, paving the way towards long-term consistent world generation."
    },
    {
      "title": "矫正点流：通用点云位姿估计 (原标题: Rectified Point Flow: Generic Point Cloud Pose Estimation)",
      "link": "https://arxiv.org/abs/2506.05282",
      "pubDate": "Thu, 05 Jun 2025 13:36:03 GMT",
      "isoDate": "2025-06-05T13:36:03.000Z",
      "creator": "Tao Sun, Liyuan Zhu, Shengyu Huang, Shuran Song, Iro Armeni",
      "summary": "# 矫正点流：通用点云位姿估计\n\n本文介绍了一种名为“Rectified Point Flow (RPF)”的新型统一参数化方法。该方法将成对点云配准（pairwise point cloud registration）和多部件形状组装（multi-part shape assembly）统一为一个单一的条件生成问题。\n\n## 方法概述\n\n*   **核心思想：** RPF方法在给定未定位点云的情况下，学习一个连续的点级速度场（point-wise velocity field）。\n*   **位姿恢复：** 这个速度场能够将带有噪声的点传输到它们的预期目标位置，从而恢复出部件的位姿。\n\n## 创新与优势\n\n*   **对称性处理：** 与以往通过临时（ad-hoc）对称性处理来回归部件位姿的方法不同，RPF能够内在地学习组装对称性，而无需预先提供对称性标签。\n*   **自监督学习：** 该方法结合了一个专注于重叠点的自监督编码器。\n\n## 性能表现\n\n*   **最先进水平：** RPF在六个涵盖点云配准和形状组装任务的基准测试中，均取得了新的最先进性能。\n*   **统一框架的益处：** 值得注意的是，其统一的公式化方法使得对不同数据集进行有效的联合训练成为可能，这促进了共享几何先验知识的学习，并因此显著提升了准确性。",
      "shortSummary": "Rectified Point Flow (RPF) 是一种新颖的统一方法，将点云配准和多部件组装建模为条件生成问题。它通过学习点级速度场，内在地恢复部件位姿并处理对称性，无需对称标签。RPF在六个基准测试中实现了最先进性能，其统一框架通过联合训练提升了准确性，促进了共享几何先验的学习。",
      "translated_title": "矫正点流：通用点云位姿估计",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Rectified Point Flow, a unified parameterization that formulates pairwise point cloud registration and multi-part shape assembly as a single conditional generative problem. Given unposed point clouds, our method learns a continuous point-wise velocity field that transports noisy points toward their target positions, from which part poses are recovered. In contrast to prior work that regresses part-wise poses with ad-hoc symmetry handling, our method intrinsically learns assembly symmetries without symmetry labels. Together with a self-supervised encoder focused on overlapping points, our method achieves a new state-of-the-art performance on six benchmarks spanning pairwise registration and shape assembly. Notably, our unified formulation enables effective joint training on diverse datasets, facilitating the learning of shared geometric priors and consequently boosting accuracy. Project page: https://rectified-pointflow.github.io/."
    },
    {
      "title": "Micro-Act：通过可操作的自推理缓解问答中的知识冲突 (原标题: Micro-Act: Mitigate Knowledge Conflict in Question Answering via Actionable Self-Reasoning)",
      "link": "https://arxiv.org/abs/2506.05278",
      "pubDate": "Thu, 05 Jun 2025 13:33:02 GMT",
      "isoDate": "2025-06-05T13:33:02.000Z",
      "creator": "Nan Huo, Jinyang Li, Bowen Qin, Ge Qu, Xiaolong Li, Xiaodong Li, Chenhao Ma, Reynold Cheng",
      "summary": "## Micro-Act：通过可操作的自推理缓解问答中的知识冲突\n\n### 摘要\n\n本文介绍了 Micro-Act 框架，旨在解决检索增强生成（RAG）系统中普遍存在的“知识冲突”问题。知识冲突指的是检索到的外部知识与大型语言模型（LLM）固有的参数知识相矛盾，这严重影响了问答（QA）等下游任务的性能。\n\n### 现有方法的局限性\n\n*   **直接比较的不足：** 现有方法通常尝试通过并排直接比较两种知识来源来缓解冲突。\n*   **上下文过载：** 这种方法可能导致 LLM 被无关或冗长的上下文信息所淹没，从而阻碍其识别和缓解不一致性的能力。\n\n### Micro-Act 框架\n\n为了解决上述问题，研究人员提出了 Micro-Act 框架，其核心特点包括：\n\n*   **分层行动空间：** Micro-Act 采用一个分层的行动空间，能够自动感知上下文的复杂性。\n*   **自适应分解：** 它能自适应地将每个知识来源分解为一系列细粒度的比较。\n*   **可操作步骤：** 这些比较被表示为可操作的步骤，使得模型能够进行超越表面上下文的更深层次推理。\n\n### 实验结果与性能\n\n*   **广泛评估：** Micro-Act 在五个基准数据集上进行了广泛的实验。\n*   **显著的准确性提升：** 在所有五个数据集和三种冲突类型上，Micro-Act 都比最先进的基线方法显著提高了问答准确性。\n*   **特定冲突类型优势：** 在时间（temporal）和语义（semantic）冲突类型中，Micro-Act 的表现尤为突出，而所有基线方法在这些类型上都表现不佳。\n*   **鲁棒性：** 更重要的是，Micro-Act 在处理非冲突问题时也表现出强大的鲁棒性，这突显了其在实际 RAG 应用中的实用价值。\n\n### 结论\n\nMicro-Act 通过其创新的分层行动空间和细粒度、可操作的自推理方法，有效地缓解了 RAG 系统中的知识冲突，并在多项基准测试中展现出卓越的性能和实用性。该研究已被 ACL 2025 主会议接受。",
      "shortSummary": "RAG系统常面临知识冲突，即外部知识与LLM固有知识矛盾。现有方法因上下文冗长而效率低下。Micro-Act提出一个分层行动空间，通过将知识源分解为细粒度、可操作的自推理步骤来缓解冲突。实验表明，Micro-Act在五个基准数据集上显著提高了问答准确性，尤其在时间与语义冲突类型上表现出色，并对非冲突问题保持鲁棒性，具有重要的实际应用价值。",
      "translated_title": "Micro-Act：通过可操作的自推理缓解问答中的知识冲突",
      "images": [],
      "contentSource": "完整文章",
      "content": "Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge Conflicts, where retrieved external knowledge contradicts the inherent, parametric knowledge of large language models (LLMs). It adversely affects performance on downstream tasks such as question answering (QA). Existing approaches often attempt to mitigate conflicts by directly comparing two knowledge sources in a side-by-side manner, but this can overwhelm LLMs with extraneous or lengthy contexts, ultimately hindering their ability to identify and mitigate inconsistencies. To address this issue, we propose Micro-Act a framework with a hierarchical action space that automatically perceives context complexity and adaptively decomposes each knowledge source into a sequence of fine-grained comparisons. These comparisons are represented as actionable steps, enabling reasoning beyond the superficial context. Through extensive experiments on five benchmark datasets, Micro-Act consistently achieves significant increase in QA accuracy over state-of-the-art baselines across all 5 datasets and 3 conflict types, especially in temporal and semantic types where all baselines fail significantly. More importantly, Micro-Act exhibits robust performance on non-conflict questions simultaneously, highlighting its practical value in real-world RAG applications."
    },
    {
      "title": "使用流先验对齐潜在空间 (原标题: Aligning Latent Spaces with Flow Priors)",
      "link": "https://arxiv.org/abs/2506.05240",
      "pubDate": "Thu, 05 Jun 2025 12:59:53 GMT",
      "isoDate": "2025-06-05T12:59:53.000Z",
      "creator": "Yizhuo Li, Yuying Ge, Yixiao Ge, Ying Shan, Ping Luo",
      "summary": "## 潜在空间对齐新框架\n\n### 摘要\n\n本文提出了一种新颖的框架，旨在利用基于流的生成模型作为先验，将可学习的潜在空间与任意目标分布进行对齐。该方法通过创新的机制，解决了传统潜在空间对齐中存在的计算效率和复杂性问题。\n\n### 方法论\n\n1.  **流模型预训练**：首先，在目标特征上预训练一个流模型。这个预训练的目的是精确捕捉目标特征的底层分布，使其能够作为后续潜在空间对齐的强大先验。\n2.  **对齐损失定义**：预训练好的流模型被固定下来，并用于正则化潜在空间。通过重新构造流匹配目标，论文引入了一种“对齐损失”，其中潜在变量被视为优化目标。这种损失函数的设计使得潜在空间能够逐步向目标分布靠拢。\n\n### 理论基础与优势\n\n*   **数学证明**：论文形式化证明，最小化所提出的对齐损失，实际上是最大化潜在变量在目标分布下的对数似然变分下界的一个计算上可处理的替代目标。这为方法的有效性提供了坚实的理论支撑。\n*   **计算效率**：该框架的一个显著优势在于其计算效率。在优化过程中，它消除了对计算成本高昂的似然评估的需求，并且避免了求解常微分方程（ODE），从而大大降低了计算负担。\n\n### 实验验证\n\n*   **概念验证**：在受控设置下，实验证明对齐损失的损失函数曲面与目标分布的负对数似然函数曲面非常接近。这表明对齐损失能够有效地引导潜在空间向目标分布的形状靠拢。\n*   **大规模应用**：为了进一步验证方法的有效性，研究人员在ImageNet数据集上进行了大规模图像生成实验，并使用了多种不同的目标分布。实验结果充分展示了该方法在复杂真实世界数据上的强大性能。\n*   **深入分析**：论文还提供了详细的讨论和全面的消融研究，以深入分析框架的各个组成部分及其对整体性能的贡献。\n\n### 结论\n\n该框架通过严谨的理论推导和充分的经验验证，为潜在空间对齐领域开辟了一条全新的、高效且可行的途径。它在机器学习和计算机视觉领域具有广泛的应用潜力。",
      "shortSummary": "该论文提出一种新颖的框架，利用基于流的生成模型作为先验，实现可学习潜在空间与任意目标分布的对齐。核心方法是预训练流模型捕获目标分布，然后通过重新定义的“对齐损失”来正则化潜在空间。该方法在优化过程中避免了昂贵的似然评估和ODE求解，具有计算效率。理论上，最小化对齐损失等同于最大化对数似然的变分下界。实验在ImageNet上验证了其有效性，为潜在空间对齐提供了新途径。",
      "translated_title": "使用流先验对齐潜在空间",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper presents a novel framework for aligning learnable latent spaces to arbitrary target distributions by leveraging flow-based generative models as priors. Our method first pretrains a flow model on the target features to capture the underlying distribution. This fixed flow model subsequently regularizes the latent space via an alignment loss, which reformulates the flow matching objective to treat the latents as optimization targets. We formally prove that minimizing this alignment loss establishes a computationally tractable surrogate objective for maximizing a variational lower bound on the log-likelihood of latents under the target distribution. Notably, the proposed method eliminates computationally expensive likelihood evaluations and avoids ODE solving during optimization. As a proof of concept, we demonstrate in a controlled setting that the alignment loss landscape closely approximates the negative log-likelihood of the target distribution. We further validate the effectiveness of our approach through large-scale image generation experiments on ImageNet with diverse target distributions, accompanied by detailed discussions and ablation studies. With both theoretical and empirical validation, our framework paves a new way for latent space alignment."
    },
    {
      "title": "对角线批处理解锁循环记忆Transformer在长上下文中的并行性 (原标题: Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers for Long Contexts)",
      "link": "https://arxiv.org/abs/2506.05229",
      "pubDate": "Thu, 05 Jun 2025 12:43:48 GMT",
      "isoDate": "2025-06-05T12:43:48.000Z",
      "creator": "Danil Sivtsov, Ivan Rodkin, Gleb Kuzmin, Yuri Kuratov, Ivan Oseledets",
      "summary": "本文介绍了“对角线批处理”技术，旨在解决Transformer模型在长上下文推理中面临的性能瓶颈，特别是针对循环记忆Transformer (RMTs) 的顺序执行限制。\n\n**问题背景：**\n*   传统Transformer模型在处理长上下文时，由于其二次方的时间复杂度和线性的内存复杂度，导致推理效率低下。\n*   循环记忆Transformer (RMTs) 旨在通过将渐近成本降低到线性时间复杂度和常数内存使用来解决这一问题。\n*   然而，RMTs的内存更新机制本质上是顺序执行的，这成为了其性能的瓶颈，限制了并行性。\n\n**提出的解决方案：对角线批处理 (Diagonal Batching)**\n*   **核心思想：** 对角线批处理是一种创新的调度方案，它通过重新排序计算，在RMTs中实现了跨段的并行性，同时精确地保留了递归性。\n*   **优势：**\n    *   它消除了RMTs固有的顺序执行约束，使得即使是单个长上下文输入也能在GPU上实现高效推理。\n    *   该技术无需复杂的批处理和流水线技术。\n    *   由于它纯粹是运行时计算的重新排序，现有的RMT模型无需重新训练即可直接采用。\n\n**实验结果与性能提升：**\n*   研究人员将对角线批处理应用于一个LLaMA-1B ARMT模型。\n*   在处理131,072个token的序列时，该技术展现出显著的性能提升：\n    *   相比标准的完整注意力LLaMA-1B模型，实现了3.3倍的加速。\n    *   相比顺序执行的RMT实现，实现了1.8倍的加速。\n\n**结论与影响：**\n*   通过成功消除顺序瓶颈，对角线批处理显著降低了RMT模型的推理成本和延迟。\n*   这进一步增强了RMTs作为处理真实世界长上下文应用的实用解决方案的地位。",
      "shortSummary": "Transformer模型在长上下文推理中面临二次方复杂度问题。循环记忆Transformer (RMTs) 虽降低了渐近成本，但其顺序内存更新机制造成性能瓶颈。本文提出“对角线批处理”调度方案，通过重新排序计算，解锁RMTs跨段并行性，消除顺序限制。该技术无需重新训练，使GPU高效推理长上下文成为可能。实验显示，在131,072个token序列上，LLaMA-1B ARMT模型采用此方案比标准LLaMA-1B快3.3倍，比顺序RMT快1.8倍，显著降低了推理成本和延迟。",
      "translated_title": "对角线批处理解锁循环记忆Transformer在长上下文中的并行性",
      "images": [],
      "contentSource": "完整文章",
      "content": "Transformer models struggle with long-context inference due to their quadratic time and linear memory complexity. Recurrent Memory Transformers (RMTs) offer a solution by reducing the asymptotic cost to linear time and constant memory usage. However, their memory update mechanism leads to sequential execution, causing a performance bottleneck.   We introduce Diagonal Batching, a scheduling scheme that unlocks parallelism across segments in RMTs while preserving exact recurrence. This approach eliminates the sequential constraint, enabling efficient GPU inference even for single long-context inputs without complex batching and pipelining techniques. Because the technique is purely a run-time computation reordering, existing RMT models adopt it with no retraining.   Applied to a LLaMA-1B ARMT model, Diagonal Batching yields a 3.3x speedup over standard full-attention LLaMA-1B and a 1.8x speedup over the sequential RMT implementation on 131,072-token sequences. By removing sequential bottleneck, Diagonal Batching reduces inference cost and latency, thereby strengthening RMTs as a practical solution for real-world, long-context applications."
    },
    {
      "title": "Common Pile v0.1：一个8TB的公共领域和开放许可文本数据集 (原标题: The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text)",
      "link": "https://arxiv.org/abs/2506.05209",
      "pubDate": "Thu, 05 Jun 2025 12:21:30 GMT",
      "isoDate": "2025-06-05T12:21:30.000Z",
      "creator": "Nikhil Kandpal, Brian Lester, Colin Raffel, Sebastian Majstorovic, Stella Biderman, Baber Abbasi, Luca Soldaini, Enrico Shippole, A. Feder Cooper, Aviya Skowron, John Kirchenbauer, Shayne Longpre, Lintang Sutawika, Alon Albalak, Zhenlin Xu, Guilherme Penedo, Loubna Ben Allal, Elie Bakouch, John David Pressman, Honglu Fan, Dashiell Stander, Guangyu Song, Aaron Gokaslan, Tom Goldstein, Brian R. Bartoldson, Bhavya Kailkhura, Tyler Murray",
      "summary": "# Common Pile v0.1：开放许可文本的8TB数据集\n\n## 引言\n大型语言模型（LLM）的训练通常依赖于大量未经许可的文本数据，这引发了知识产权侵权和伦理方面的担忧。尽管使用开放许可文本进行LLM训练是解决这些问题的第一步，但此前的数据收集工作所产生的开放许可数据集规模过小或质量不高，不足以训练出高性能的LLM。\n\n## Common Pile v0.1 数据集\n为了弥补这一空白，研究人员收集、整理并发布了Common Pile v0.1，这是一个专为LLM预训练设计的8TB开放许可文本集合。\n\n*   **规模与许可：** Common Pile v0.1是一个8TB的数据集，其内容全部来自公共领域和开放许可的文本。\n*   **内容来源：** 该数据集整合了来自30个不同来源的内容，涵盖了广泛的领域，包括：\n    *   研究论文\n    *   代码\n    *   书籍\n    *   百科全书\n    *   教育材料\n    *   音频转录\n    *   以及更多其他类型的数据。\n\n## 模型验证与性能\n为了验证Common Pile v0.1数据集的有效性，研究人员基于该数据集训练了两个70亿参数的LLM：\n\n*   **Comma v0.1-1T：** 使用1万亿个token进行训练。\n*   **Comma v0.1-2T：** 使用2万亿个token进行训练。\n\n这两款模型在性能上均达到了与使用非许可文本训练的LLM（如Llama 1和Llama 2 7B）相当的竞争力，且计算预算相似。\n\n## 发布内容\n除了Common Pile v0.1数据集本身，研究人员还公开发布了：\n\n*   用于创建Common Pile v0.1数据集的代码。\n*   Comma v0.1模型的训练混合物和检查点。",
      "shortSummary": "Common Pile v0.1是一个8TB的公共领域和开放许可文本数据集，旨在解决大型语言模型（LLM）训练中知识产权和伦理问题。它包含来自30个多样化来源的内容。研究人员利用该数据集训练了两个70亿参数的LLM（Comma v0.1-1T和Comma v0.1-2T），其性能与使用非许可文本训练的LLM相当。Common Pile v0.1数据集、其创建代码以及训练模型均已公开发布。",
      "translated_title": "Common Pile v0.1：一个8TB的公共领域和开放许可文本数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) are typically trained on enormous quantities of unlicensed text, a practice that has led to scrutiny due to possible intellectual property infringement and ethical concerns. Training LLMs on openly licensed text presents a first step towards addressing these issues, but prior data collection efforts have yielded datasets too small or low-quality to produce performant LLMs. To address this gap, we collect, curate, and release the Common Pile v0.1, an eight terabyte collection of openly licensed text designed for LLM pretraining. The Common Pile comprises content from 30 sources that span diverse domains including research papers, code, books, encyclopedias, educational materials, audio transcripts, and more. Crucially, we validate our efforts by training two 7 billion parameter LLMs on text from the Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion tokens respectively. Both models attain competitive performance to LLMs trained on unlicensed text with similar computational budgets, such as Llama 1 and 2 7B. In addition to releasing the Common Pile v0.1 itself, we also release the code used in its creation as well as the training mixture and checkpoints for the Comma v0.1 models."
    },
    {
      "title": "Qwen3 Embedding：通过基础模型推进文本嵌入和重排序 (原标题: Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models)",
      "link": "https://arxiv.org/abs/2506.05176",
      "pubDate": "Thu, 05 Jun 2025 11:49:48 GMT",
      "isoDate": "2025-06-05T11:49:48.000Z",
      "creator": "Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, Jingren Zhou",
      "summary": "## Qwen3 Embedding 系列介绍\n\n### 概述\nQwen3 Embedding 系列是文本嵌入和重排序领域的一项重大进展，它显著超越了其前身 GTE-Qwen 系列。该系列基于 Qwen3 基础模型构建，旨在提供强大的文本理解和表示能力。\n\n### 核心技术与训练流程\n*   **利用 Qwen3 LLM 能力**：Qwen3 Embedding 系列充分利用了 Qwen3 大型语言模型（LLM）在多语言文本理解和生成方面的强大能力。\n*   **创新多阶段训练流程**：\n    *   结合了大规模无监督预训练。\n    *   在高质量数据集上进行有监督微调。\n    *   采用有效的模型合并策略，以确保模型的鲁棒性和适应性。\n*   **Qwen3 LLM 在训练中的关键作用**：\n    *   不仅作为骨干模型。\n    *   还在训练过程中合成高质量、丰富多样且跨领域、跨语言的训练数据，从而显著增强了训练流程。\n\n### 模型规格与应用\n*   **多尺寸模型**：Qwen3 Embedding 系列提供多种模型尺寸，包括 0.6B、4B 和 8B，适用于文本嵌入和重排序任务。\n*   **灵活部署**：这些不同尺寸的模型能够满足多样化的部署场景需求，用户可以根据具体情况优化效率或效果。\n\n### 性能表现\n*   **最先进水平（SOTA）**：经验评估表明，Qwen3 Embedding 系列在多项基准测试中均取得了最先进的成果。\n*   **多语言评估**：在文本嵌入的多语言评估基准 MTEB 上表现尤为出色。\n*   **检索任务**：在多种检索任务中也表现卓越，包括：\n    *   代码检索\n    *   跨语言检索\n    *   多语言检索\n\n### 可用性\n*   **公开可用**：Qwen3 Embedding 模型已根据 Apache 2.0 许可协议公开发布。\n*   **促进研究**：此举旨在促进研究的可复现性，并推动社区驱动的研发工作。",
      "shortSummary": "Qwen3 Embedding系列是基于Qwen3基础模型构建的先进文本嵌入和重排序技术，显著超越前代。它利用Qwen3 LLM强大的多语言能力，通过创新的多阶段训练和LLM合成数据来提升性能。该系列提供0.6B、4B、8B等多种模型尺寸，在MTEB多语言评估基准和代码、跨语言、多语言检索等任务中均达到最先进水平。Qwen3 Embedding模型已根据Apache 2.0许可协议公开发布，旨在促进社区研究与开发。",
      "translated_title": "Qwen3 Embedding：通过基础模型推进文本嵌入和重排序",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this work, we introduce the Qwen3 Embedding series, a significant advancement over its predecessor, the GTE-Qwen series, in text embedding and reranking capabilities, built upon the Qwen3 foundation models. Leveraging the Qwen3 LLMs' robust capabilities in multilingual text understanding and generation, our innovative multi-stage training pipeline combines large-scale unsupervised pre-training with supervised fine-tuning on high-quality datasets. Effective model merging strategies further ensure the robustness and adaptability of the Qwen3 Embedding series. During the training process, the Qwen3 LLMs serve not only as backbone models but also play a crucial role in synthesizing high-quality, rich, and diverse training data across multiple domains and languages, thus enhancing the training pipeline. The Qwen3 Embedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both embedding and reranking tasks, addressing diverse deployment scenarios where users can optimize for either efficiency or effectiveness. Empirical evaluations demonstrate that the Qwen3 Embedding series achieves state-of-the-art results across diverse benchmarks. Notably, it excels on the multilingual evaluation benchmark MTEB for text embedding, as well as in various retrieval tasks, including code retrieval, cross-lingual retrieval and multilingual retrieval. To facilitate reproducibility and promote community-driven research and development, the Qwen3 Embedding models are publicly available under the Apache 2.0 license."
    },
    {
      "title": "FlowDirector：用于精确文本到视频编辑的免训练流引导 (原标题: FlowDirector: Training-Free Flow Steering for Precise Text-to-Video Editing)",
      "link": "https://arxiv.org/abs/2506.05046",
      "pubDate": "Thu, 05 Jun 2025 09:54:40 GMT",
      "isoDate": "2025-06-05T09:54:40.000Z",
      "creator": "Guangzhao Li, Yanming Yang, Chenxi Song, Chi Zhang",
      "summary": "# FlowDirector：用于精确文本到视频编辑的免训练流引导\n\nFlowDirector 是一种新颖的免训练视频编辑框架，旨在通过自然语言指令修改视频内容。\n\n## 现有挑战\n当前文本驱动的视频编辑方法，特别是那些利用预训练扩散模型的免训练方法，通常依赖于基于反演（inversion-based）的技术。这些技术将输入视频映射到潜在空间，但往往导致：\n\n*   时间不一致性（temporal inconsistencies）\n*   结构保真度下降（degraded structural fidelity）\n\n## FlowDirector 的核心创新\nFlowDirector 提出了一种**免反演（inversion-free）**的视频编辑范式，通过以下机制解决上述问题：\n\n### 1. 数据空间中的直接演化\n\n*   **建模方式**：将编辑过程建模为数据空间中的直接演化。\n*   **引导机制**：通过常微分方程（ODE）引导视频，使其沿着固有的时空流形平滑过渡。\n*   **优势**：这种方法能够有效保留视频的时间连贯性（temporal coherence）和结构细节。\n\n### 2. 注意力引导的遮罩机制\n\n*   **目的**：实现局部化和可控的编辑。\n*   **作用**：引入注意力引导的遮罩机制来调节 ODE 的速度场，从而在空间和时间上保留非目标区域，确保背景或未编辑区域的完整性。\n\n### 3. 引导增强的编辑策略\n\n*   **灵感来源**：受无分类器引导（Classifier-Free Guidance）的启发。\n*   **目的**：解决不完整编辑问题，并增强编辑指令的语义对齐。\n*   **实现方式**：利用多个候选流之间的微分信号来引导编辑轨迹，使其更强地与语义对齐，同时不损害结构一致性。\n\n## 实验结果与意义\n\n*   **性能表现**：在多个基准测试中，FlowDirector 在指令遵循、时间一致性和背景保留方面均达到了最先进的性能。\n*   **范式建立**：它为高效、连贯且无需反演的视频编辑建立了一个新范式。",
      "shortSummary": "FlowDirector 是一种创新的免训练框架，用于精确的文本到视频编辑。它通过引入免反演方法，将编辑过程建模为数据空间中的常微分方程（ODE）引导，有效解决了现有方法的时间不一致性和结构保真度问题。FlowDirector 还采用注意力引导遮罩和引导增强策略，实现局部化编辑并提升语义对齐。实验证明，该框架在指令遵循、时间一致性和背景保留方面均达到最先进水平，为视频编辑开辟了新途径。",
      "translated_title": "FlowDirector：用于精确文本到视频编辑的免训练流引导",
      "images": [],
      "contentSource": "完整文章",
      "content": "Text-driven video editing aims to modify video content according to natural language instructions. While recent training-free approaches have made progress by leveraging pre-trained diffusion models, they typically rely on inversion-based techniques that map input videos into the latent space, which often leads to temporal inconsistencies and degraded structural fidelity. To address this, we propose FlowDirector, a novel inversion-free video editing framework. Our framework models the editing process as a direct evolution in data space, guiding the video via an Ordinary Differential Equation (ODE) to smoothly transition along its inherent spatiotemporal manifold, thereby preserving temporal coherence and structural details. To achieve localized and controllable edits, we introduce an attention-guided masking mechanism that modulates the ODE velocity field, preserving non-target regions both spatially and temporally. Furthermore, to address incomplete edits and enhance semantic alignment with editing instructions, we present a guidance-enhanced editing strategy inspired by Classifier-Free Guidance, which leverages differential signals between multiple candidate flows to steer the editing trajectory toward stronger semantic alignment without compromising structural consistency. Extensive experiments across benchmarks demonstrate that FlowDirector achieves state-of-the-art performance in instruction adherence, temporal consistency, and background preservation, establishing a new paradigm for efficient and coherent video editing without inversion."
    }
  ],
  "lastUpdated": "2025-06-08T09:32:47.475Z"
}