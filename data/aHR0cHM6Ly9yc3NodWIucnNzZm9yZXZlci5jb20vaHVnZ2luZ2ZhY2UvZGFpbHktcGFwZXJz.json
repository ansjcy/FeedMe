{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "Parallel-R1：通过强化学习实现并行思维 (原标题: Parallel-R1: Towards Parallel Thinking via Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2509.07980",
      "pubDate": "Tue, 09 Sep 2025 13:59:35 GMT",
      "isoDate": "2025-09-09T13:59:35.000Z",
      "creator": "Tong Zheng, Hongming Zhang, Wenhao Yu, Xiaoyang Wang, Xinyu Yang, Runpeng Dai, Rui Liu, Huiwen Bao, Chengsong Huang, Heng Huang, Dong Yu",
      "summary": "## Parallel-R1：通过强化学习实现并行思维\n\n本文介绍了**Parallel-R1**，这是首个利用强化学习（RL）框架来增强大型语言模型（LLMs）并行思维能力的新方法，旨在解决现有方法在训练中面临的挑战。\n\n### 背景与挑战\n*   **并行思维**：作为一种新颖的方法，它通过同时探索多个推理路径来提升LLM的推理能力。\n*   **训练难题**：通过训练激活LLM的并行思维能力仍然具有挑战性。\n*   **现有方法局限**：当前方法主要依赖于对合成数据进行监督微调（SFT），这种方式鼓励了教师强制模仿，而非模型自主的探索和泛化能力。\n\n### Parallel-R1 框架\n*   **RL驱动**：Parallel-R1是第一个专门为复杂真实世界推理任务实现并行思维行为的强化学习框架。\n*   **渐进式课程**：该框架采用了一种渐进式课程，旨在明确解决RL训练并行思维中的“冷启动”问题。具体步骤如下：\n    1.  **SFT阶段**：首先，在来自较简单任务的提示生成轨迹上使用监督微调（SFT），以初步灌输并行思维能力。\n    2.  **RL阶段**：随后，模型过渡到强化学习（RL），在更难的问题上进一步探索和泛化这项技能。\n\n### 实验结果与发现\n*   **性能显著提升**：在MATH、AMC23和AIME等多种数学基准测试中，Parallel-R1成功地灌输了并行思维，与直接在挑战性任务上使用RL训练的顺序思维模型相比，准确率提高了8.4%。\n*   **思维行为转变**：进一步的分析揭示了模型思维行为的明显转变：\n    *   **早期阶段**：模型将并行思维主要用作一种探索策略。\n    *   **后期阶段**：模型将相同的并行思维能力用于多视角验证，以确保推理的准确性。\n*   **“训练中期探索支架”**：最重要的是，研究验证了并行思维作为一种“训练中期探索支架”（mid-training exploration scaffold）的作用。这种临时的探索阶段在RL之后解锁了更高的性能上限，在AIME25基准测试上比基线模型提高了42.9%。\n\n### 开源信息\n*   Parallel-R1的模型、数据和代码将进行开源。",
      "shortSummary": "Parallel-R1提出首个强化学习（RL）框架，通过渐进式课程实现大型语言模型（LLMs）的并行思维。它首先利用监督微调（SFT）在简单任务上奠定基础，随后通过RL在复杂任务上进行探索和泛化。实验表明，Parallel-R1显著提升了LLMs在数学基准上的推理准确率，并作为“训练中期探索支架”解锁了更高的性能上限，比基线模型在AIME25上提高了42.9%。",
      "translated_title": "Parallel-R1：通过强化学习实现并行思维",
      "images": [],
      "contentSource": "完整文章",
      "content": "Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose Parallel-R1, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the model's thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a mid-training exploration scaffold, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at https://github.com/zhengkid/Parallel-R1."
    },
    {
      "title": "多模态大型语言模型的视觉表征对齐 (原标题: Visual Representation Alignment for Multimodal Large Language Models)",
      "link": "https://arxiv.org/abs/2509.07979",
      "pubDate": "Tue, 09 Sep 2025 13:59:14 GMT",
      "isoDate": "2025-09-09T13:59:14.000Z",
      "creator": "Heeji Yoon, Jaewoo Jung, Junwan Kim, Hyungyu Choi, Heeseong Shin, Sangbeom Lim, Honggyu An, Chaehyun Kim, Jisang Han, Donghyun Kim, Chanho Eom, Sunghwan Hong, Seungryong Kim",
      "summary": "## 多模态大型语言模型的视觉表征对齐 (VIRAL)\n\n### 摘要\n\n多模态大型语言模型 (MLLMs) 尽管通过视觉指令微调在各种任务中取得了强大性能，但在诸如物体计数或空间推理等以视觉为中心的任务中仍存在局限性。本文将这一差距归因于当前普遍存在的仅文本监督范式，该范式仅为视觉通路提供间接指导，并常常导致 MLLMs 在训练过程中丢弃细粒度的视觉细节。\n\n### 提出的方法：VIRAL\n\n本文提出了一种名为 **VIsual Representation ALignment (VIRAL)** 的简单而有效的正则化策略。VIRAL 的核心思想是将 MLLMs 的内部视觉表征与预训练视觉基础模型 (VFMs) 的表征进行对齐。\n\n### VIRAL 的工作原理与优势\n\n通过明确地强制执行这种对齐，VIRAL 能够使模型实现以下关键优势：\n\n*   **保留关键视觉细节**：模型能够从输入视觉编码器中保留重要的视觉细节，避免在训练过程中丢失。\n*   **补充视觉知识**：模型能够从预训练的 VFMs 中获取并补充额外的视觉知识。\n*   **增强复杂视觉输入推理能力**：通过结合上述两点，VIRAL 显著增强了 MLLMs 对复杂视觉输入进行推理的能力。\n\n### 实验结果与验证\n\n*   **一致性提升**：实验结果表明，VIRAL 在广泛采用的多模态基准测试中的所有任务上都取得了持续的性能提升。\n*   **消融研究**：研究人员进行了全面的消融研究，以验证其框架背后关键设计选择的有效性。\n\n### 结论与未来方向\n\n作者认为，这一简单的发现为在训练 MLLMs 中有效整合视觉信息开辟了一个重要的方向。",
      "shortSummary": "多模态大型语言模型（MLLMs）在视觉中心任务上表现受限，主要因文本监督导致视觉细节丢失。本文提出VIRAL，一种通过将MLLM内部视觉表征与预训练视觉基础模型对齐的正则化策略。VIRAL能保留关键视觉细节并补充视觉知识，从而增强MLLM对复杂视觉输入的推理能力。实验证明，VIRAL在多模态基准测试中持续提升性能，为MLLM中视觉信息的有效整合提供了重要方向。",
      "translated_title": "多模态大型语言模型的视觉表征对齐",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal large language models (MLLMs) trained with visual instruction tuning have achieved strong performance across diverse tasks, yet they remain limited in vision-centric tasks such as object counting or spatial reasoning. We attribute this gap to the prevailing text-only supervision paradigm, which provides only indirect guidance for the visual pathway and often leads MLLMs to discard fine-grained visual details during training. In this paper, we present VIsual Representation ALignment (VIRAL), a simple yet effective regularization strategy that aligns the internal visual representations of MLLMs with those of pre-trained vision foundation models (VFMs). By explicitly enforcing this alignment, VIRAL enables the model not only to retain critical visual details from the input vision encoder but also to complement additional visual knowledge from VFMs, thereby enhancing its ability to reason over complex visual inputs. Our experiments demonstrate consistent improvements across all tasks on widely adopted multimodal benchmarks. Furthermore, we conduct comprehensive ablation studies to validate the key design choices underlying our framework. We believe this simple finding opens up an important direction for the effective integration of visual information in training MLLMs."
    },
    {
      "title": "Mini-o3：扩展视觉搜索中的推理模式和交互回合 (原标题: Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search)",
      "link": "https://arxiv.org/abs/2509.07969",
      "pubDate": "Tue, 09 Sep 2025 13:54:21 GMT",
      "isoDate": "2025-09-09T13:54:21.000Z",
      "creator": "Xin Lai, Junyi Li, Wei Li, Tao Liu, Tianjian Li, Hengshuang Zhao",
      "summary": "本文介绍了Mini-o3系统，旨在解决现有大型多模态模型（LMMs）在视觉问题上推理模式单一、交互回合受限的问题，使其无法有效处理需要试错探索的复杂任务。Mini-o3通过扩展基于工具的交互，实现了深度、多回合（可达数十步）的推理，并在挑战性视觉搜索任务上取得了最先进的性能。\n\nMini-o3重现OpenAI o3风格行为的关键组成部分包括：\n\n*   **构建视觉探测数据集（Visual Probe Dataset）**\n    *   该数据集包含数千个专为探索性推理设计的挑战性视觉搜索问题。\n*   **开发迭代数据收集流程**\n    *   该流程用于获取“冷启动”轨迹，这些轨迹展现了多样化的推理模式，包括深度优先搜索、试错法和目标维护。\n*   **提出“超回合掩蔽”策略（Over-turn Masking Strategy）**\n    *   此策略在强化学习训练期间，避免对超出最大回合数的响应进行惩罚，从而平衡了训练效率和测试时的可扩展性。\n\n**主要成果与特点：**\n\n*   尽管模型在训练时仅设定了最多六个交互回合的上限，但在推理时，Mini-o3能够自然地生成扩展到数十个回合的轨迹。\n*   随着交互回合数的增加，模型的准确性也随之提高。\n*   广泛的实验证明，Mini-o3能够产生丰富的推理模式和深入的思维路径，有效解决了挑战性的视觉搜索问题。\n\n**资源可用性：**\n代码、数据集和模型均可在项目页面获取。",
      "shortSummary": "Mini-o3是一个旨在解决现有大型多模态模型在视觉搜索中推理模式单一和交互回合受限问题的系统。它通过扩展基于工具的交互，实现了深度、多回合（可达数十步）的推理，并在挑战性视觉搜索任务上取得了最先进的性能。Mini-o3引入了视觉探测数据集、迭代数据收集流程和超回合掩蔽策略，使其能够生成丰富的推理模式和深入的思维路径，即使在有限训练回合下也能在推理时自然扩展到更多回合，并随回合数增加提高准确性。",
      "translated_title": "Mini-o3：扩展视觉搜索中的推理模式和交互回合",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems."
    },
    {
      "title": "SimpleQA Verified：一个衡量参数化知识的可靠事实性基准 (原标题: SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge)",
      "link": "https://arxiv.org/abs/2509.07968",
      "pubDate": "Tue, 09 Sep 2025 13:53:58 GMT",
      "isoDate": "2025-09-09T13:53:58.000Z",
      "creator": "Lukas Haas, Gal Yona, Giovanni D'Antonio, Sasha Goldshtein, Dipanjan Das",
      "summary": "### SimpleQA Verified：衡量参数化知识的可靠事实性基准\n\n#### 引言\n本文介绍了 **SimpleQA Verified**，这是一个包含1,000个提示的基准测试，专门用于评估大型语言模型（LLM）的短形式事实性。该基准基于OpenAI的SimpleQA，旨在解决其原始版本中的关键局限性。\n\n#### 解决的问题\nSimpleQA Verified旨在纠正OpenAI原始SimpleQA基准中存在的以下问题：\n*   **嘈杂和不正确的标签**：原始基准中包含大量错误或不准确的标签。\n*   **主题偏见**：评估问题在主题分布上存在不平衡，导致评估结果可能不具代表性。\n*   **问题冗余**：存在重复或高度相似的问题，降低了评估效率和挑战性。\n\n#### 创建过程\nSimpleQA Verified的创建经历了一个严格的多阶段过滤过程，以确保其高质量和可靠性：\n*   **去重（De-duplication）**：移除了重复的问题，确保每个评估点都是独特的。\n*   **主题平衡（Topic Balancing）**：对问题的主题分布进行了调整，以覆盖更广泛的知识领域，减少偏见。\n*   **来源协调（Source Reconciliation）**：对事实来源进行了仔细核对和统一，提高了标签的准确性。\n*   **自动评分器提示改进**：除了数据集的改进，用于自动评估模型响应的提示词也得到了优化，以提高评估的准确性和一致性。\n\n通过这些改进，SimpleQA Verified成为了一个更可靠、更具挑战性的评估集。\n\n#### 模型性能表现\n在新基准测试上，**Gemini 2.5 Pro** 取得了显著的性能，其F1分数达到了 **55.6**，创下了最先进的（state-of-the-art）记录。这一成绩表明Gemini 2.5 Pro在事实性方面表现优异，超越了包括GPT-5在内的其他前沿模型。\n\n#### 意义与可用性\n这项工作为研究社区提供了一个更高保真度的工具，具有以下重要意义：\n*   **跟踪真实进展**：能够更准确地跟踪参数化模型在事实性方面的真正进步。\n*   **缓解幻觉**：有助于识别和减轻LLM生成虚假信息（即“幻觉”）的问题。\n\n基准数据集、评估代码和排行榜均已公开，可在提供的链接（this https URL）获取，方便研究人员使用和贡献。",
      "shortSummary": "本文介绍了SimpleQA Verified，一个基于OpenAI SimpleQA的1,000个提示的LLM事实性基准。它通过严格的多阶段过滤过程，解决了原始基准中标签错误、主题偏见和问题冗余等关键限制，创建了一个更可靠、更具挑战性的评估集。在新基准上，Gemini 2.5 Pro以55.6的F1分数达到最先进水平，超越了GPT-5。SimpleQA Verified为跟踪参数化模型事实性进展和缓解幻觉提供了高保真度工具。数据集和代码已公开。",
      "translated_title": "SimpleQA Verified：一个衡量参数化知识的可靠事实性基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large Language Model (LLM) short-form factuality based on OpenAI's SimpleQA. It addresses critical limitations in OpenAI's benchmark, including noisy and incorrect labels, topical biases, and question redundancy. SimpleQA Verified was created through a rigorous multi-stage filtering process involving de-duplication, topic balancing, and source reconciliation to produce a more reliable and challenging evaluation set, alongside improvements in the autorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a state-of-the-art F1-score of 55.6, outperforming other frontier models, including GPT-5. This work provides the research community with a higher-fidelity tool to track genuine progress in parametric model factuality and to mitigate hallucinations. The benchmark dataset, evaluation code, and leaderboard are available at: https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified."
    },
    {
      "title": "ΔL 归一化：重新思考RLVR中的损失聚合 (原标题: ΔL Normalization: Rethink Loss Aggregation in RLVR)",
      "link": "https://arxiv.org/abs/2509.07558",
      "pubDate": "Tue, 09 Sep 2025 05:52:34 GMT",
      "isoDate": "2025-09-09T05:52:34.000Z",
      "creator": "Zhiyuan He, Xufang Luo, Yike Zhang, Yuqing Yang, Lili Qiu",
      "summary": "## ΔL 归一化：重新思考RLVR中的损失聚合\n\n### 摘要\n\n本文提出了一种名为 **ΔL 归一化**的损失聚合方法，专门针对强化学习与可验证奖励（RLVR）中动态生成长度的特性。该方法旨在解决RLVR训练过程中响应长度高度可变性所导致的梯度方差高和优化不稳定的问题。\n\n### 背景与问题\n\n*   **RLVR的潜力**：强化学习与可验证奖励（RLVR）在提升大型语言模型（LLMs）的推理能力方面展现出巨大潜力。\n*   **主要挑战**：在RLVR训练过程中，响应长度的巨大变异性是一个核心挑战。这种变异性导致梯度方差高，进而使得优化过程不稳定。\n\n### 现有方法及其局限性\n\n*   **现有方法**：GRPO、DAPO和Dr. GRPO等方法曾引入不同的损失归一化项来尝试解决这一问题。\n*   **局限性**：这些方法存在以下问题：\n    *   要么产生有偏估计。\n    *   要么仍然面临高梯度方差的问题。\n\n### 本文贡献与提出的方法\n\n*   **问题重构**：通过理论和经验分析不同生成长度对策略损失的影响，本文将问题重新表述为寻找一个**最小方差无偏估计器**。\n*   **ΔL 归一化**：基于此，本文提出了 **ΔL 归一化**方法。\n\n### ΔL 归一化的优势\n\n*   **无偏估计**：ΔL 归一化不仅提供了真实策略损失的无偏估计。\n*   **最小化梯度方差**：理论上，该方法能够最小化梯度方差。\n\n### 实验结果与可用性\n\n*   **实验验证**：广泛的实验表明，ΔL 归一化在不同模型大小、最大生成长度和任务上持续取得优越结果。\n*   **代码公开**：本文的代码将公开发布。",
      "shortSummary": "本文提出ΔL归一化，一种针对强化学习与可验证奖励（RLVR）中动态生成长度问题的损失聚合方法。RLVR训练中响应长度变异性导致梯度方差高和优化不稳定，现有方法存在偏倚或高方差。ΔL归一化通过理论分析，提供策略损失的无偏估计并最小化梯度方差。实验证明，该方法在多种设置下均表现优异，有效解决了RLVR中的损失聚合挑战。",
      "translated_title": "ΔL 归一化：重新思考RLVR中的损失聚合",
      "images": [],
      "contentSource": "完整文章",
      "content": "We propose Delta L Normalization, a simple yet effective loss aggregation method tailored to the characteristic of dynamic generation lengths in Reinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has demonstrated strong potential in improving the reasoning capabilities of large language models (LLMs), but a major challenge lies in the large variability of response lengths during training, which leads to high gradient variance and unstable optimization. Although previous methods such as GRPO, DAPO, and Dr. GRPO introduce different loss normalization terms to address this issue, they either produce biased estimates or still suffer from high gradient variance. By analyzing the effect of varying lengths on policy loss both theoretically and empirically, we reformulate the problem as finding a minimum-variance unbiased estimator. Our proposed Delta L Normalization not only provides an unbiased estimate of the true policy loss but also minimizes gradient variance in theory. Extensive experiments show that it consistently achieves superior results across different model sizes, maximum lengths, and tasks. Our code will be made public at https://github.com/zerolllin/Delta-L-Normalization."
    },
    {
      "title": "用于无数据训练的语言自博弈 (原标题: Language Self-Play For Data-Free Training)",
      "link": "https://arxiv.org/abs/2509.07414",
      "pubDate": "Tue, 09 Sep 2025 01:51:34 GMT",
      "isoDate": "2025-09-09T01:51:34.000Z",
      "creator": "Jakub Grudzien Kuba, Mengting Gu, Qi Ma, Yuandong Tian, Vijai Mohan",
      "summary": "## 用于无数据训练的语言自博弈 (Language Self-Play For Data-Free Training)\n\n### 背景与挑战\n\n近年来，大型语言模型（LLMs）在规模、丰富的优质训练数据以及强化学习的推动下取得了飞速发展。然而，这种进步面临一个根本性的瓶颈：模型需要不断增加的数据才能持续学习和改进。\n\n### 提出的方法：语言自博弈（LSP）\n\n为了解决对额外数据的依赖问题，本文提出了一种基于强化学习的方法，使模型无需额外数据即可自我提升。该方法的核心是“语言自博弈”（Language Self-Play, LSP），其主要特点如下：\n\n*   **博弈论框架**：LSP 利用了自博弈的博弈论框架。在这个框架中，模型的各项能力被视为在竞争性博弈中的表现。\n*   **自我对弈机制**：通过让模型与自身进行对弈，可以逐步产生更强大的策略。这一过程使得模型能够在没有新外部数据输入的情况下，通过内部迭代和优化来增强其性能。\n\n### 实验与结果\n\n研究人员使用 Llama-3.2-3B-Instruct 模型在指令遵循基准任务上进行了实验。实验结果表明：\n\n*   **性能提升**：预训练模型不仅能够通过单独的自博弈显著提升其在具有挑战性任务上的表现。\n*   **超越基线**：LSP 方法的提升效果甚至比数据驱动的基线方法更为有效。\n\n### 核心贡献\n\nLSP 提供了一种新颖的范式，使大型语言模型能够摆脱对不断增长的训练数据的依赖，通过自我博弈实现持续的能力增强，从而有望突破当前LLM发展面临的数据瓶颈。",
      "shortSummary": "本文提出“语言自博弈”（LSP）方法，通过强化学习使大型语言模型（LLMs）无需额外数据即可自我提升。LSP利用博弈论框架，让模型与自身对弈以生成更强策略。实验表明，Llama-3.2-3B-Instruct模型通过LSP在指令遵循任务上显著提升性能，甚至优于数据驱动的基线方法，有效解决了LLM对数据日益增长的依赖瓶颈。",
      "translated_title": "用于无数据训练的语言自博弈",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) have advanced rapidly in recent years, driven by scale, abundant high-quality training data, and reinforcement learning. Yet this progress faces a fundamental bottleneck: the need for ever more data from which models can continue to learn. In this work, we propose a reinforcement learning approach that removes this dependency by enabling models to improve without additional data. Our method leverages a game-theoretic framework of self-play, where a model's capabilities are cast as performance in a competitive game and stronger policies emerge by having the model play against itself - a process we call Language Self-Play (LSP). Experiments with Llama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained models can not only enhance their performance on challenging tasks through self-play alone, but can also do so more effectively than data-driven baselines."
    },
    {
      "title": "带前瞻键的因果注意力 (原标题: Causal Attention with Lookahead Keys)",
      "link": "https://arxiv.org/abs/2509.07301",
      "pubDate": "Mon, 08 Sep 2025 20:15:23 GMT",
      "isoDate": "2025-09-08T20:15:23.000Z",
      "creator": "Zhuoqing Song, Peng Sun, Huizhuo Yuan, Quanquan Gu",
      "summary": "## 带前瞻键的因果注意力 (CASTLE)\n\n### 引言：标准因果注意力的局限性\n\n在标准的因果注意力机制中，每个token的查询（Query）、键（Key）和值（Value, QKV）是静态的，并且仅编码其自身之前（preceding context）的信息。这限制了模型在处理序列数据时对未来上下文的潜在利用。\n\n### CASTLE机制的引入\n\n文章提出了一种名为“带前瞻键的因果注意力”（CAuSal aTtention with Lookahead kEys, 简称CASTLE）的新型注意力机制。CASTLE旨在克服标准因果注意力的上述局限性。\n\n### 前瞻键（Lookahead Keys）的定义与特性\n\n*   **核心思想：** CASTLE机制的核心在于，它会随着上下文的展开，持续更新每个token的键（keys）。\n*   **“前瞻键”的含义：** 这些更新后的键被称为“前瞻键”，因为它们虽然属于较早的位置，但却集成了相对于这些位置而言后续出现的token的信息。\n*   **自回归属性的保留：** 尽管前瞻键整合了后续信息，CASTLE机制严格保留了自回归（autoregressive）属性。这意味着在生成当前token时，模型仍然只依赖于之前的token信息，但键的更新过程巧妙地考虑了未来的上下文，从而增强了表示能力。\n\n### 效率与并行训练\n\n*   **表面上的顺序性：** 尽管CASTLE机制在概念上可能显得是顺序执行的，因为它涉及键的持续更新。\n*   **数学等价形式：** 作者推导出了一个数学等价形式，该形式避免了在每个位置显式地实例化前瞻键。\n*   **高效并行训练：** 这一数学等价形式使得CASTLE能够实现高效的并行训练，从而解决了潜在的计算效率问题，使其在大规模模型训练中具有实用性。\n\n### 实验结果与性能提升\n\n*   **语言建模基准测试：** 在语言建模基准测试中，CASTLE在不同模型规模下始终优于标准的因果注意力机制。\n*   **具体性能指标：**\n    *   显著降低了验证困惑度（validation perplexity）。\n    *   提高了一系列下游任务的性能，表明其具有更强的泛化能力和表示学习能力。\n\n### 研究领域\n\n*   计算与语言（cs.CL）\n*   机器学习（cs.LG）",
      "shortSummary": "文章介绍了CASTLE（带前瞻键的因果注意力）机制，旨在解决标准因果注意力中QKV静态且仅编码前文信息的局限。CASTLE通过在上下文展开时持续更新每个token的键（称为“前瞻键”），使其能整合后续信息，同时严格保持自回归特性。尽管机制看似顺序，但通过数学等价实现了高效并行训练。实验表明，CASTLE在语言建模任务中持续优于标准因果注意力，降低了困惑度并提升了下游任务性能。",
      "translated_title": "带前瞻键的因果注意力",
      "images": [],
      "contentSource": "完整文章",
      "content": "In standard causal attention, each token's query, key, and value (QKV) are static and encode only preceding context. We introduce CAuSal aTtention with Lookahead kEys (CASTLE), an attention mechanism that continually updates each token's keys as the context unfolds. We term these updated keys lookahead keys because they belong to earlier positions yet integrate information from tokens that appear later relative to those positions, while strictly preserving the autoregressive property. Although the mechanism appears sequential, we derive a mathematical equivalence that avoids explicitly materializing lookahead keys at each position and enables efficient parallel training. On language modeling benchmarks, CASTLE consistently outperforms standard causal attention across model scales, reducing validation perplexity and improving performance on a range of downstream tasks."
    },
    {
      "title": "重建对齐改进统一多模态模型 (原标题: Reconstruction Alignment Improves Unified Multimodal Models)",
      "link": "https://arxiv.org/abs/2509.07295",
      "pubDate": "Mon, 08 Sep 2025 19:59:32 GMT",
      "isoDate": "2025-09-08T19:59:32.000Z",
      "creator": "Ji Xie, Trevor Darrell, Luke Zettlemoyer, XuDong Wang",
      "summary": "### 重建对齐改进统一多模态模型\n\n本文介绍了一种名为“重建对齐”（Reconstruction Alignment, RecA）的资源高效型后训练方法，旨在解决统一多模态模型（UMMs）在训练过程中面临的挑战。\n\n**背景问题：**\n*   统一多模态模型（UMMs）旨在将视觉理解和生成整合到单一架构中。\n*   传统的UMM训练依赖于图像-文本对，但这些文本描述通常稀疏，即使使用大量词语也难以捕捉图像中细粒度的视觉细节。\n\n**RecA方法介绍：**\n*   **核心思想：** RecA利用视觉理解编码器嵌入作为密集的“文本提示”，提供丰富的监督信息，而无需依赖人工标注的文本描述。\n*   **工作原理：**\n    1.  RecA使UMM以其自身的视觉理解嵌入为条件。\n    2.  通过自监督重建损失，优化UMM以重建输入图像。\n    3.  这一过程有效地重新对齐了模型的理解能力和生成能力。\n*   **特点：**\n    *   资源高效的后训练方法。\n    *   无需依赖稀疏的图像-文本描述。\n    *   广泛适用于各种UMM架构，包括自回归、掩码自回归和基于扩散的UMM。\n\n**实验结果与性能提升：**\n*   **训练效率：** 仅需27个GPU小时进行后训练。\n*   **图像生成性能显著提升：**\n    *   在GenEval基准测试中，性能从0.73提升至0.90。\n    *   在DPGBench基准测试中，性能从80.93提升至88.15。\n*   **图像编辑基准表现优异：**\n    *   在ImgEdit基准测试中，性能从3.38提升至3.75。\n    *   在GEdit基准测试中，性能从6.94提升至7.25。\n\n**结论：**\nRecA方法尽管简单，但其有效性使其能够超越许多更大的开源模型。它作为一种高效且通用的UMM后训练对齐策略，在不同UMM架构中均表现出广泛的适用性，显著提高了生成和编辑的保真度。",
      "shortSummary": "重建对齐（RecA）是一种资源高效的后训练方法，旨在改进统一多模态模型（UMMs）。它通过利用视觉理解编码器嵌入作为密集提示，并优化UMM以自监督方式重建输入图像，从而解决传统训练中稀疏文本描述的问题，重新对齐模型的理解与生成能力。RecA适用于多种UMM架构，仅用27个GPU小时，便显著提升了图像生成（如GenEval从0.73升至0.90）和编辑（如ImgEdit从3.38升至3.75）的性能，超越了许多大型开源模型，是一种高效通用的对齐策略。",
      "translated_title": "重建对齐改进统一多模态模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense \"text prompts,\" providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73rightarrow0.90) and DPGBench (80.93rightarrow88.15), while also boosting editing benchmarks (ImgEdit 3.38rightarrow3.75, GEdit 6.94rightarrow7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs"
    },
    {
      "title": "F1：一种连接理解与生成到行动的视觉-语言-行动模型 (原标题: F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions)",
      "link": "https://arxiv.org/abs/2509.06951",
      "pubDate": "Mon, 08 Sep 2025 13:58:30 GMT",
      "isoDate": "2025-09-08T13:58:30.000Z",
      "creator": "Qi Lv, Weijie Kong, Hao Li, Jia Zeng, Zherui Qiu, Delin Qu, Haoming Song, Qizhi Chen, Xiang Deng, Jiangmiao Pang",
      "summary": "# F1：一种连接理解与生成到行动的视觉-语言-行动模型\n\n## 引言\n在具身AI领域，于动态视觉环境中执行语言条件任务仍是一个核心挑战。现有视觉-语言-行动（VLA）模型主要采用反应式的状态-行动映射，这往往导致在动态场景中出现短视行为和较差的鲁棒性。\n\n## F1模型概述\n本文介绍F1，一个预训练的VLA框架，它将视觉预见生成整合到决策流程中，旨在弥合理解、生成与行动之间的鸿沟。\n\n### 核心架构与机制\n*   **架构：** F1采用Transformer混合架构，包含专门的模块用于感知、预见生成和控制。\n*   **核心机制：** F1运用“下一尺度预测”机制来合成目标条件下的视觉预见，将其作为明确的规划目标。\n*   **行动生成：** 通过预测合理的未来视觉状态，F1将行动生成重新定义为“预见引导的逆动力学问题”，从而能够生成隐式实现视觉目标的行动。\n\n## 训练方法与能力\n*   **训练方案：** 为赋予F1鲁棒和泛化能力，研究者提出了一种三阶段训练方法。该方法在一个包含超过33万条轨迹和136项多样化任务的广泛数据集上进行。\n*   **能力提升：** 这种训练方案增强了模型的模块化推理能力，并使其具备可迁移的视觉预见能力，这对于在复杂和动态环境中执行任务至关重要。\n\n## 实验结果\n在真实世界任务和模拟基准测试中，F1持续优于现有方法，在任务成功率和泛化能力方面均取得了显著提升。",
      "shortSummary": "F1是一种创新的预训练视觉-语言-行动（VLA）模型，旨在解决具身AI在动态视觉环境中执行语言条件任务的挑战。它通过将视觉预见生成整合到决策流程中，克服了现有VLA模型的短视和鲁棒性问题。F1采用Transformer混合架构，利用目标条件下的视觉预见作为规划目标，将行动生成重构为预见引导的逆动力学问题。经过大规模数据集的三阶段训练，F1在真实世界和模拟任务中均显著超越现有方法，大幅提升了任务成功率和泛化能力。",
      "translated_title": "F1：一种连接理解与生成到行动的视觉-语言-行动模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Executing language-conditioned tasks in dynamic visual environments remains a central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to short-sighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, a pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts a Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as a foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose a three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability."
    },
    {
      "title": "保持在最佳区域：通过能力自适应提示支架实现响应式推理演化 (原标题: Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding)",
      "link": "https://arxiv.org/abs/2509.06923",
      "pubDate": "Mon, 08 Sep 2025 13:36:21 GMT",
      "isoDate": "2025-09-08T13:36:21.000Z",
      "creator": "Ziheng Li, Zexu Sun, Jinman Zhao, Erxue Min, Yongcheng Zeng, Hui Wu, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Xu Chen, Zhi-Hong Deng",
      "summary": "### SEELE：通过能力自适应提示支架优化大型语言模型推理\n\n**1. 背景与问题**\n*   可验证奖励强化学习（RLVR）在提升大型语言模型（LLM）的推理能力方面取得了显著成功。\n*   然而，现有RLVR方法面临探索效率低下的挑战，主要原因在于训练数据难度与模型自身能力之间存在不匹配。\n*   具体表现为：当问题难度过高时，LLM难以发现有效的推理路径；而当问题过于简单时，模型学习到的新能力有限，导致学习效率不高。\n\n**2. 理论基础**\n*   本文通过量化损失下降速度与rollout准确性之间的关系，形式化地阐明了问题难度对学习过程的影响。\n\n**3. SEELE框架介绍**\n*   **SEELE**是一个新颖的、监督辅助的RLVR框架，其核心目标是动态调整问题难度，以确保模型始终处于高效学习的“最佳区域”。\n*   **核心机制：** SEELE通过在原始问题之后附加一个“提示”（即完整解决方案的一部分）来增强每个训练样本。\n*   **创新之处：** 与以往基于提示的方法不同，SEELE的关键在于它能有意识地、自适应地调整每个问题的提示长度，从而达到一个最优的难度水平。\n\n**4. 提示长度确定机制**\n*   SEELE采用多轮rollout采样策略来确定每个问题的最佳提示长度。\n*   在每一轮采样中，它会利用前几轮收集到的“准确性-提示对”数据，拟合一个项目反应理论（IRT）模型，进而预测下一轮训练所需的最佳提示长度。\n*   这种实例级别、实时的问题难度调整方法，确保了问题难度与模型不断演进的能力保持同步，从而显著提高了探索效率。\n\n**5. 实验结果**\n*   实验结果表明，SEELE在六个数学推理基准测试中表现出色：\n    *   平均优于Group Relative Policy Optimization (GRPO) 11.8个百分点。\n    *   平均优于Supervised Fine-tuning (SFT) 10.5个百分点。\n    *   平均超越之前最佳的监督辅助方法3.6个百分点。",
      "shortSummary": "SEELE是一种新颖的监督辅助强化学习框架，旨在解决大型语言模型（LLM）推理训练中因问题难度与模型能力不匹配导致的探索效率低下问题。该框架通过动态调整每个训练样本的提示长度来优化问题难度，使模型始终处于高效学习区域。SEELE利用多轮rollout采样和项目反应理论模型预测最佳提示长度。实验表明，SEELE在多个数学推理基准测试中显著优于现有方法，有效提升了LLM的推理能力和学习效率。",
      "translated_title": "保持在最佳区域：通过能力自适应提示支架实现响应式推理演化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable success in enhancing the reasoning capabilities of large language models (LLMs). However, existing RLVR methods often suffer from exploration inefficiency due to mismatches between the training data's difficulty and the model's capability. LLMs fail to discover viable reasoning paths when problems are overly difficult, while learning little new capability when problems are too simple. In this work, we formalize the impact of problem difficulty by quantifying the relationship between loss descent speed and rollout accuracy. Building on this analysis, we propose SEELE, a novel supervision-aided RLVR framework that dynamically adjusts problem difficulty to stay within the high-efficiency region. SEELE augments each training sample by appending a hint (part of a full solution) after the original problem. Unlike previous hint-based approaches, SEELE deliberately and adaptively adjusts the hint length for each problem to achieve an optimal difficulty. To determine the optimal hint length, SEELE employs a multi-round rollout sampling strategy. In each round, it fits an item response theory model to the accuracy-hint pairs collected in preceding rounds to predict the required hint length for the next round. This instance-level, real-time difficulty adjustment aligns problem difficulty with the evolving model capability, thereby improving exploration efficiency. Experimental results show that SEELE outperforms Group Relative Policy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5 points, respectively, and surpasses the best previous supervision-aided approach by +3.6 points on average across six math reasoning benchmarks."
    },
    {
      "title": "Curia：一种用于放射学的多模态基础模型 (原标题: Curia: A Multi-Modal Foundation Model for Radiology)",
      "link": "https://arxiv.org/abs/2509.06830",
      "pubDate": "Mon, 08 Sep 2025 12:04:12 GMT",
      "isoDate": "2025-09-08T12:04:12.000Z",
      "creator": "Corentin Dancette, Julien Khlaut, Antoine Saporta, Helene Philippe, Elodie Ferreres, Baptiste Callard, Théo Danielou, Léo Alberge, Léo Machado, Daniel Tordjman, Julie Dupuis, Korentin Le Floch, Jean Du Terrail, Mariam Moshiri, Laurent Dercle, Tom Boeken, Jules Gregory, Maxime Ronot, François Legou, Pascal Roux, Marc Sapoval, Pierre Manceron, Paul Hérent",
      "summary": "# Curia：一种用于放射学的多模态基础模型\n\n## 1. 引言与背景\n\n*   **当前挑战：** 现有的放射学AI辅助解释主要依赖于狭窄的、单任务模型。这种方法对于覆盖广泛的成像模态、疾病和放射学发现来说是不切实际的。\n*   **基础模型的潜力：** 基础模型（FMs）有望在不同模态和低数据环境下实现广泛泛化，但在放射学领域尚未充分实现这一潜力。\n\n## 2. Curia模型介绍\n\n*   **模型名称：** Curia\n*   **模型类型：** 一种多模态基础模型，专为放射学设计。\n*   **训练数据：**\n    *   训练于一家大型医院多年来的全部横断面成像输出。\n    *   据作者称，这是迄今为止最大的真实世界数据语料库。\n    *   **数据规模：** 包含150,000次检查，总计130 TB的数据。\n\n## 3. 性能与评估\n\n*   **评估基准：** 在一个新策划的19项任务外部验证基准上进行测试。\n*   **核心能力：**\n    *   准确识别器官。\n    *   检测多种病症，例如脑出血和心肌梗死。\n    *   预测肿瘤分期中的结果。\n*   **性能表现：**\n    *   达到或超越了放射科医生和近期其他基础模型的表现。\n    *   在跨模态和低数据条件下展现出具有临床意义的涌现特性。\n\n## 4. 可用性\n\n*   为了加速研究进展，Curia的基础模型权重已被发布。\n\n## 5. 作者与分类\n\n*   **作者：** Corentin Dancette 等多位研究人员。\n*   **研究领域：** 计算机视觉与模式识别 (cs.CV)；机器学习 (cs.LG)。\n*   **引用信息：** arXiv:2509.06830。",
      "shortSummary": "Curia是一种用于放射学的多模态基础模型，旨在克服现有单任务AI的局限。它利用一家大型医院15万次检查（130 TB）的真实世界数据进行训练。Curia在一个19项任务的基准测试中表现出色，能准确识别器官、检测脑出血和心肌梗死等病症，并预测肿瘤分期结果。其性能超越了放射科医生及其他基础模型，并在跨模态和低数据环境下展现出重要的涌现特性。模型权重已发布，以加速研究进展。",
      "translated_title": "Curia：一种用于放射学的多模态基础模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "AI-assisted radiological interpretation is based on predominantly narrow, single-task models. This approach is impractical for covering the vast spectrum of imaging modalities, diseases, and radiological findings. Foundation models (FMs) hold the promise of broad generalization across modalities and in low-data settings. However, this potential has remained largely unrealized in radiology. We introduce Curia, a foundation model trained on the entire cross-sectional imaging output of a major hospital over several years, which to our knowledge is the largest such corpus of real-world data-encompassing 150,000 exams (130 TB). On a newly curated 19-task external validation benchmark, Curia accurately identifies organs, detects conditions like brain hemorrhages and myocardial infarctions, and predicts outcomes in tumor staging. Curia meets or surpasses the performance of radiologists and recent foundation models, and exhibits clinically significant emergent properties in cross-modality, and low-data regimes. To accelerate progress, we release our base model's weights at https://huggingface.co/raidium/curia."
    },
    {
      "title": "UMO：通过匹配奖励扩展图像定制中的多身份一致性 (原标题: UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward)",
      "link": "https://arxiv.org/abs/2509.06818",
      "pubDate": "Mon, 08 Sep 2025 11:54:55 GMT",
      "isoDate": "2025-09-08T11:54:55.000Z",
      "creator": "Yufeng Cheng, Wenxu Wu, Shaojin Wu, Mengqi Huang, Fei Ding, Qian He",
      "summary": "## UMO：通过匹配奖励扩展图像定制中的多身份一致性\n\n### 摘要\n\n近期图像定制技术在能力上取得了显著进步，展现出广泛的应用前景。然而，由于人类对人脸的敏感性，在处理多参考图像时，如何保持一致的身份并避免身份混淆，仍然是一个重大挑战，这限制了定制模型在身份可扩展性方面的表现。\n\n### UMO 框架：统一的多身份优化\n\n为了解决上述问题，研究人员提出了 UMO（Unified Multi-identity Optimization）框架。该框架旨在实现高保真度的身份保留，并有效缓解多身份场景下的身份混淆问题，同时提升模型的可扩展性。\n\n#### 核心方法：多对多匹配范式\n\nUMO 采用了一种“多对多匹配”（multi-to-multi matching）范式，将多身份生成问题重新定义为一个全局分配优化问题。通过在扩散模型上应用强化学习，UMO 能够普遍地增强现有图像定制方法的多身份一致性。\n\n### 辅助开发与评估\n\n1.  **可扩展定制数据集：** 为了促进 UMO 的训练，研究团队开发了一个包含多参考图像的可扩展定制数据集，该数据集结合了合成数据和真实数据。\n2.  **新度量标准：** 此外，研究人员还提出了一种新的度量标准，专门用于量化和衡量身份混淆的程度。\n\n### 实验结果与性能\n\n广泛的实验结果表明，UMO 框架不仅显著提高了身份一致性，而且在多种图像定制方法上有效降低了身份混淆。在身份保留维度上，UMO 在开源方法中达到了新的最先进水平。",
      "shortSummary": "UMO是一个统一的多身份优化框架，旨在解决图像定制中多参考图像的身份一致性保持和身份混淆问题。它通过“多对多匹配”范式，将多身份生成重构为全局分配优化问题，并利用扩散模型上的强化学习来提升现有方法的身份一致性。UMO还引入了新的数据集和评估指标。实验证明，UMO显著提高了身份一致性，减少了身份混淆，并在身份保留方面达到了开源方法的最新水平。",
      "translated_title": "UMO：通过匹配奖励扩展图像定制中的多身份一致性",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in image customization exhibit a wide range of application prospects due to stronger customization capabilities. However, since we humans are more sensitive to faces, a significant challenge remains in preserving consistent identity while avoiding identity confusion with multi-reference images, limiting the identity scalability of customization models. To address this, we present UMO, a Unified Multi-identity Optimization framework, designed to maintain high-fidelity identity preservation and alleviate identity confusion with scalability. With \"multi-to-multi matching\" paradigm, UMO reformulates multi-identity generation as a global assignment optimization problem and unleashes multi-identity consistency for existing image customization methods generally through reinforcement learning on diffusion models. To facilitate the training of UMO, we develop a scalable customization dataset with multi-reference images, consisting of both synthesised and real parts. Additionally, we propose a new metric to measure identity confusion. Extensive experiments demonstrate that UMO not only improves identity consistency significantly, but also reduces identity confusion on several image customization methods, setting a new state-of-the-art among open-source methods along the dimension of identity preserving. Code and model: https://github.com/bytedance/UMO"
    },
    {
      "title": "WinT3R：基于窗口的流式重建与相机令牌池 (原标题: WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool)",
      "link": "https://arxiv.org/abs/2509.05296",
      "pubDate": "Fri, 05 Sep 2025 13:59:47 GMT",
      "isoDate": "2025-09-05T13:59:47.000Z",
      "creator": "Zizun Li, Jianjun Zhou, Yifan Wang, Haoyu Guo, Wenzheng Chang, Yang Zhou, Haoyi Zhu, Junyi Chen, Chunhua Shen, Tong He",
      "summary": "# WinT3R：基于窗口的流式重建与相机令牌池\n\n## 摘要\n本文介绍了WinT3R，一个前馈重建模型，能够在线预测精确的相机姿态和高质量的点云图。\n\n## 现有问题\n*   以往的方法在重建质量和实时性能之间存在权衡，难以同时达到高水平。\n\n## WinT3R 的核心创新与解决方案\nWinT3R模型旨在解决上述问题，其主要设计包括：\n\n1.  **滑动窗口机制：**\n    *   **目的：** 确保窗口内帧之间有足够的信息交换，从而在不增加大量计算的情况下提高几何预测的质量。\n    *   **优势：** 改善重建质量，同时保持计算效率。\n2.  **紧凑的相机表示与全局相机令牌池：**\n    *   **目的：** 增强相机姿态估计的可靠性。\n    *   **优势：** 在不牺牲效率的前提下，提高相机姿态估计的准确性。\n\n## 性能与验证\n*   **成果：** 这些设计使WinT3R在在线重建质量、相机姿态估计和重建速度方面达到了最先进的性能。\n*   **验证：** 通过在各种数据集上进行的大量实验验证了其卓越性能。\n\n## 可用性\n*   WinT3R的代码和模型已公开提供。",
      "shortSummary": "WinT3R是一个前馈模型，用于在线预测精确的相机姿态和高质量点云图。它通过引入滑动窗口机制来增强帧间信息交换，并在不牺牲效率的情况下，利用紧凑的相机表示和全局相机令牌池提高相机姿态估计的可靠性。WinT3R在在线重建质量、相机姿态估计和重建速度方面均达到了最先进的性能。",
      "translated_title": "WinT3R：基于窗口的流式重建与相机令牌池",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present WinT3R, a feed-forward reconstruction model capable of online prediction of precise camera poses and high-quality point maps. Previous methods suffer from a trade-off between reconstruction quality and real-time performance. To address this, we first introduce a sliding window mechanism that ensures sufficient information exchange among frames within the window, thereby improving the quality of geometric predictions without large computation. In addition, we leverage a compact representation of cameras and maintain a global camera token pool, which enhances the reliability of camera pose estimation without sacrificing efficiency. These designs enable WinT3R to achieve state-of-the-art performance in terms of online reconstruction quality, camera pose estimation, and reconstruction speed, as validated by extensive experiments on diverse datasets. Code and model are publicly available at https://github.com/LiZizun/WinT3R."
    },
    {
      "title": "LatticeWorld: 一个由多模态大型语言模型驱动的交互式复杂世界生成框架 (原标题: LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation)",
      "link": "https://arxiv.org/abs/2509.05263",
      "pubDate": "Fri, 05 Sep 2025 13:22:33 GMT",
      "isoDate": "2025-09-05T13:22:33.000Z",
      "creator": "Yinglin Duan, Zhengxia Zou, Tongwei Gu, Wei Jia, Zhan Zhao, Luyi Xu, Xinzhu Liu, Hao Jiang, Kang Chen, Shuang Qiu",
      "summary": "### LatticeWorld: 多模态大型语言模型赋能的交互式复杂世界生成框架\n\n#### 背景与挑战\n*   **研究焦点**: 近期研究日益关注开发模拟复杂现实世界场景的3D世界模型。\n*   **应用领域**: 世界模型在具身AI、自动驾驶、娱乐等多个领域有广泛应用。\n*   **目标**: 通过更真实的模拟和精确的物理效果，有效缩小“模拟到现实”的差距，并方便地获取关于现实世界的丰富信息。\n*   **传统方法局限**: 传统手动建模虽然能创建虚拟3D场景，但效率不高。\n*   **现代方法**: 现代方法利用先进的机器学习算法进行3D世界生成，最新进展集中于根据用户指令创建虚拟世界的生成式方法。\n\n#### LatticeWorld框架介绍\n*   **核心理念**: 本文提出了LatticeWorld，一个简单而有效的3D世界生成框架，旨在简化3D环境的工业生产流程。\n*   **技术构成**: \n    *   利用轻量级大型语言模型（LLM，如LLaMA-2-7B）。\n    *   结合工业级渲染引擎（如Unreal Engine 5）。\n    *   共同生成动态环境。\n*   **多模态输入**: LatticeWorld接受文本描述和视觉指令作为多模态输入。\n*   **生成内容**: 创建大规模的3D交互式世界，其中包含动态智能体。\n*   **关键特性**: \n    *   具有竞争力的多智能体交互。\n    *   高保真物理模拟。\n    *   实时渲染。\n\n#### 实验与成果\n*   **评估**: 对LatticeWorld进行了全面的实验评估。\n*   **准确性与视觉保真度**: 结果表明，LatticeWorld在场景布局生成和视觉保真度方面表现出色。\n*   **生产效率提升**: 相较于传统手动生产方法，LatticeWorld在保持高创造性质量的同时，将工业生产效率提高了90倍以上。",
      "shortSummary": "LatticeWorld是一个由多模态大型语言模型（如LLaMA-2-7B）和工业级渲染引擎（如Unreal Engine 5）驱动的3D世界生成框架。它接受文本和视觉指令，能高效生成包含动态智能体、高保真物理模拟和实时渲染的大规模交互式3D世界。实验证明，LatticeWorld在场景布局准确性和视觉保真度上表现优异，并能将工业生产效率提升90倍以上，同时保持高创造性质量。",
      "translated_title": "LatticeWorld: 一个由多模态大型语言模型驱动的交互式复杂世界生成框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a 90times increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18"
    },
    {
      "title": "大型语言模型下的符号图形编程 (原标题: Symbolic Graphics Programming with Large Language Models)",
      "link": "https://arxiv.org/abs/2509.05208",
      "pubDate": "Fri, 05 Sep 2025 12:10:53 GMT",
      "isoDate": "2025-09-05T12:10:53.000Z",
      "creator": "Yamei Chen, Haoquan Zhang, Yangyi Huang, Zeju Qiu, Kaipeng Zhang, Yandong Wen, Weiyang Liu",
      "summary": "## 大型语言模型下的符号图形编程研究\n\n### 引言与研究背景\n\n大型语言模型（LLMs）在程序合成方面表现出色，但其生成能够渲染出精确视觉内容的符号图形程序（SGPs）的能力尚未得到充分探索。本研究旨在探讨符号图形编程，即从自然语言描述生成SGP。这项任务也为我们提供了一个视角，以观察LLMs如何通过生成由SGP渲染的图像来理解视觉世界。在各种SGP中，本文专注于可伸缩矢量图形（SVGs）。\n\n### SGP-GenBench基准测试\n\n为了评估LLMs生成SGP的程度，我们引入了SGP-GenBench，这是一个全面的基准测试，涵盖了以下方面：\n\n*   **对象保真度**：生成对象的准确性。\n*   **场景保真度**：整个场景的准确性。\n*   **组合性**：包括属性绑定、空间关系和数值能力。\n\n在SGP-GenBench上的测试发现，领先的专有模型显著优于开源模型，并且性能与通用编码能力高度相关。\n\n### 提出的方法：基于可验证奖励的强化学习\n\n为了弥补LLMs在SGP生成方面的差距，我们提出了一种基于可验证奖励的强化学习（RL）方法。该方法包含以下关键机制：\n\n*   **格式有效性门**：确保生成的SVG是可渲染的。\n*   **跨模态奖励**：通过强大的视觉编码器（例如，SigLIP用于文本-图像对齐，DINO用于图像-图像对齐）来对齐文本描述和渲染图像，从而提供奖励信号。\n\n### 实验结果与分析\n\n我们将所提出的方法应用于Qwen-2.5-7B模型，结果显示：\n\n*   显著提升了SVG的生成质量和语义。\n*   实现了与领先系统相当的性能。\n\n进一步的训练动态分析表明，强化学习促使：\n\n*   将对象更精细地分解为可控的基元。\n*   引入上下文细节以提高场景的连贯性。\n\n### 结论\n\n我们的研究结果表明，符号图形编程为理解跨模态接地提供了一个精确且可解释的视角。",
      "shortSummary": "本研究探讨大型语言模型（LLMs）生成精确符号图形程序（SGPs），特别是可伸缩矢量图形（SVGs）的能力。我们引入SGP-GenBench基准测试，发现专有模型优于开源模型。为提升LLMs性能，我们提出一种基于可验证奖励的强化学习（RL）方法，通过格式有效性门和跨模态奖励（使用SigLIP和DINO）对齐文本与渲染图像。应用于Qwen-2.5-7B，该方法显著提高了SVG生成质量，达到领先水平，并揭示了RL如何促进对象分解和场景连贯性，为跨模态接地提供了精确视角。",
      "translated_title": "大型语言模型下的符号图形编程",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) excel at program synthesis, yet their ability to produce symbolic graphics programs (SGPs) that render into precise visual content remains underexplored. We study symbolic graphics programming, where the goal is to generate an SGP from a natural-language description. This task also serves as a lens into how LLMs understand the visual world by prompting them to generate images rendered from SGPs. Among various SGPs, our paper sticks to scalable vector graphics (SVGs). We begin by examining the extent to which LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a comprehensive benchmark covering object fidelity, scene fidelity, and compositionality (attribute binding, spatial relations, numeracy). On SGP-GenBench, we discover that frontier proprietary models substantially outperform open-source models, and performance correlates well with general coding capabilities. Motivated by this gap, we aim to improve LLMs' ability to generate SGPs. We propose a reinforcement learning (RL) with verifiable rewards approach, where a format-validity gate ensures renderable SVG, and a cross-modal reward aligns text and the rendered image via strong vision encoders (e.g., SigLIP for text-image and DINO for image-image). Applied to Qwen-2.5-7B, our method substantially improves SVG generation quality and semantics, achieving performance on par with frontier systems. We further analyze training dynamics, showing that RL induces (i) finer decomposition of objects into controllable primitives and (ii) contextual details that improve scene coherence. Our results demonstrate that symbolic graphics programming offers a precise and interpretable lens on cross-modal grounding."
    },
    {
      "title": "WildScore：真实世界符号音乐推理中多模态大语言模型的基准测试 (原标题: WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning)",
      "link": "https://arxiv.org/abs/2509.04744",
      "pubDate": "Thu, 04 Sep 2025 21:54:50 GMT",
      "isoDate": "2025-09-04T21:54:50.000Z",
      "creator": "Gagan Mundada, Yash Vishe, Amit Namburi, Xin Xu, Zachary Novack, Julian McAuley, Junda Wu",
      "summary": "## WildScore：多模态大语言模型在符号音乐推理中的基准测试\n\n### 引言与背景\n\n*   **MLLMs的进展与局限：** 近期多模态大语言模型（MLLMs）在各类视觉-语言任务中展现出令人印象深刻的能力。然而，它们在多模态符号音乐领域的推理能力仍未被充分探索。\n\n### WildScore基准的引入\n\n*   **首个真实世界基准：** 本文介绍了WildScore，这是首个“真实世界”（in-the-wild）多模态符号音乐推理与分析基准。\n*   **设计目标：** WildScore旨在评估MLLMs解释真实世界乐谱并回答复杂音乐学查询的能力。\n\n### 数据来源与特点\n\n*   **真实性：** WildScore中的每个实例均来源于真实的音乐作品。\n*   **用户生成内容：** 配有真实的用户生成问题和讨论，捕捉了实际音乐分析的复杂性。\n\n### 评估方法\n\n*   **系统分类法：** 为促进系统性评估，研究者提出了一个系统分类法，包含高层和细粒度音乐学本体。\n*   **问题框架：** 将复杂的音乐推理问题构建为多项选择题形式，从而实现对MLLMs符号音乐理解的受控和可扩展评估。\n\n### 实证结果与发现\n\n*   **MLLMs表现：** 对最先进的MLLMs在WildScore上进行的实证基准测试揭示了其视觉-符号推理中引人入胜的模式。\n*   **前景与挑战：** 研究结果揭示了MLLMs在符号音乐推理与分析方面有前景的方向和持续存在的挑战。\n\n### 资源发布与相关主题\n\n*   **可用资源：** 数据集和代码已发布。\n*   **相关研究领域：** 声音（cs.SD）、计算与语言（cs.CL）、音频与语音处理（eess.AS）。",
      "shortSummary": "本文介绍了WildScore，这是首个用于评估多模态大语言模型（MLLMs）在真实世界符号音乐推理与分析能力的基准。该基准利用真实的音乐作品和用户生成的问题，旨在测试MLLMs解释乐谱和回答复杂音乐学查询的能力。通过将推理问题框架为多项选择题，并采用系统分类法，WildScore揭示了现有MLLMs在该领域的潜力和挑战。数据集和代码已公开发布。",
      "translated_title": "WildScore：真实世界符号音乐推理中多模态大语言模型的基准测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across various vision-language tasks. However, their reasoning abilities in the multimodal symbolic music domain remain largely unexplored. We introduce WildScore, the first in-the-wild multimodal symbolic music reasoning and analysis benchmark, designed to evaluate MLLMs' capacity to interpret real-world music scores and answer complex musicological queries. Each instance in WildScore is sourced from genuine musical compositions and accompanied by authentic user-generated questions and discussions, capturing the intricacies of practical music analysis. To facilitate systematic evaluation, we propose a systematic taxonomy, comprising both high-level and fine-grained musicological ontologies. Furthermore, we frame complex music reasoning as multiple-choice question answering, enabling controlled and scalable assessment of MLLMs' symbolic music understanding. Empirical benchmarking of state-of-the-art MLLMs on WildScore reveals intriguing patterns in their visual-symbolic reasoning, uncovering both promising directions and persistent challenges for MLLMs in symbolic music reasoning and analysis. We release the dataset and code."
    },
    {
      "title": "语言模型为何会产生幻觉 (原标题: Why Language Models Hallucinate)",
      "link": "https://arxiv.org/abs/2509.04664",
      "pubDate": "Thu, 04 Sep 2025 17:26:31 GMT",
      "isoDate": "2025-09-04T17:26:31.000Z",
      "creator": "Adam Tauman Kalai, Ofir Nachum, Santosh S. Vempala, Edwin Zhang",
      "summary": "## 语言模型幻觉的成因与解决方案\n\n### 引言\n大型语言模型（LLM）在面对不确定性时，有时会像学生面对难题一样进行“猜测”，从而产生看似合理但实际上不正确的陈述，而非承认其不确定性。这种现象被称为“幻觉”，即使在最先进的系统中也普遍存在，并严重损害了用户对这些系统的信任。\n\n### 幻觉的根本原因\n文章指出，语言模型产生幻觉的根本原因在于当前的训练和评估程序奖励模型进行“猜测”，而不是承认不确定性。\n\n从统计学角度来看，幻觉并非神秘现象，它们本质上是二元分类中的错误。如果在预训练阶段，模型无法有效地区分错误的陈述与事实，那么在自然统计压力的作用下，幻觉就会在预训练的语言模型中产生。\n\n### 幻觉持续存在的原因\n幻觉之所以持续存在，是因为大多数评估基准的评分方式。语言模型被优化为“善于考试者”，而当模型不确定时进行猜测，往往能提高其在这些测试中的表现。这种“惩罚不确定性回应”的现象导致了幻觉的持续存在。\n\n### 提出的解决方案\n为了解决这种“幻觉流行病”，文章提出了一种社会技术层面的缓解措施，而非仅仅引入更多的幻觉评估。核心建议是：\n*   **修改现有基准测试的评分方式**：针对那些目前存在偏差但主导着排行榜的基准测试，对其评分机制进行调整。\n*   **避免引入额外评估**：不应简单地增加更多的幻觉评估，因为这可能无法从根本上解决问题。\n\n### 目标\n通过上述改变，该领域有望被引导向开发出更值得信赖的AI系统。",
      "shortSummary": "大型语言模型（LLM）产生“幻觉”是因为训练和评估机制奖励猜测而非承认不确定性。从统计学角度看，幻觉是二元分类错误，源于预训练中无法区分事实与错误陈述。幻觉持续存在是因为模型被优化为善于考试，猜测能提高表现。解决之道在于修改现有基准测试的评分方式，而非增加新的幻觉评估，以促进更值得信赖的AI系统发展。",
      "translated_title": "语言模型为何会产生幻觉",
      "images": [],
      "contentSource": "完整文章",
      "content": "Like students facing hard exam questions, large language models sometimes guess when uncertain, producing plausible yet incorrect statements instead of admitting uncertainty. Such \"hallucinations\" persist even in state-of-the-art systems and undermine trust. We argue that language models hallucinate because the training and evaluation procedures reward guessing over acknowledging uncertainty, and we analyze the statistical causes of hallucinations in the modern training pipeline. Hallucinations need not be mysterious -- they originate simply as errors in binary classification. If incorrect statements cannot be distinguished from facts, then hallucinations in pretrained language models will arise through natural statistical pressures. We then argue that hallucinations persist due to the way most evaluations are graded -- language models are optimized to be good test-takers, and guessing when uncertain improves test performance. This \"epidemic\" of penalizing uncertain responses can only be addressed through a socio-technical mitigation: modifying the scoring of existing benchmarks that are misaligned but dominate leaderboards, rather than introducing additional hallucination evaluations. This change may steer the field toward more trustworthy AI systems."
    },
    {
      "title": "Delta激活：一种微调大型语言模型的表示方法 (原标题: Delta Activations: A Representation for Finetuned Large Language Models)",
      "link": "https://arxiv.org/abs/2509.04442",
      "pubDate": "Thu, 04 Sep 2025 13:59:06 GMT",
      "isoDate": "2025-09-04T13:59:06.000Z",
      "creator": "Zhiqiu Xu, Amish Sethi, Mayur Naik, Ser-Nam Lim",
      "summary": "## Delta激活：一种微调大型语言模型的表示方法\n\n### 背景与挑战\n*   强大的开源大型语言模型（LLMs）的成功，使得社区能够创建大量针对特定任务和领域进行微调的模型。\n*   然而，由于元数据不一致和存储库结构化不足，理解和管理这些模型仍然具有挑战性。\n\n### Delta激活方法介绍\n*   本文提出了一种名为“Delta激活”（Delta Activations）的新方法。\n*   该方法通过测量微调模型相对于基础模型的内部激活变化，将微调模型表示为向量嵌入。\n\n### Delta激活的特性与优势\n*   **有效聚类：** 这种表示方法能够有效地按领域和任务对模型进行聚类，从而揭示模型格局中的结构。\n*   **鲁棒性：** Delta激活在不同的微调设置下表现出鲁棒性。\n*   **可加性：** 当微调数据集混合时，Delta激活展现出可加性。\n*   **任务嵌入：** Delta激活还可以通过少样本微调来嵌入任务。\n*   **应用潜力：** 进一步探索了其在模型选择和模型合并方面的应用。\n\n### 目标与资源\n*   作者希望Delta激活能够促进对现有公开模型的重用。\n*   相关代码已公开。\n\n### 研究领域\n*   机器学习 (cs.LG)\n*   人工智能 (cs.AI)\n*   计算与语言 (cs.CL)\n*   信息检索 (cs.IR)",
      "shortSummary": "“Delta激活”是一种新方法，通过测量微调模型相对于基础模型的内部激活变化，将其表示为向量嵌入。这解决了微调LLM难以管理和理解的问题。该方法能有效按领域和任务聚类模型，揭示模型结构，并具有鲁棒性和可加性。它还可用于任务嵌入、模型选择和合并，旨在促进公开模型的重用。",
      "translated_title": "Delta激活：一种微调大型语言模型的表示方法",
      "images": [],
      "contentSource": "完整文章",
      "content": "The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collection of post-trained models adapted to specific tasks and domains. However, navigating and understanding these models remains challenging due to inconsistent metadata and unstructured repositories. We introduce Delta Activations, a method to represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model. This representation allows for effective clustering by domain and task, revealing structure in the model landscape. Delta Activations also demonstrate desirable properties: it is robust across finetuning settings and exhibits an additive property when finetuning datasets are mixed. In addition, we show that Delta Activations can embed tasks via few-shot finetuning, and further explore its use for model selection and merging. We hope Delta Activations can facilitate the practice of reusing publicly available models. Code is available at https://github.com/OscarXZQ/delta_activations."
    },
    {
      "title": "榴莲：双参考引导的人像动画与属性迁移 (原标题: Durian: Dual Reference-guided Portrait Animation with Attribute Transfer)",
      "link": "https://arxiv.org/abs/2509.04434",
      "pubDate": "Thu, 04 Sep 2025 13:53:03 GMT",
      "isoDate": "2025-09-04T13:53:03.000Z",
      "creator": "Hyunsoo Cha, Byungjun Kim, Hanbyul Joo",
      "summary": "## Durian：双参考引导的人像动画与属性迁移\n\n### 概述\n\nDurian 是一种开创性的方法，首次实现了从给定参考图像向目标人像进行零样本（zero-shot）面部属性迁移，并生成高质量的人像动画视频。\n\n### 核心技术与方法\n\n*   **双参考网络（Dual Reference Networks）**\n    *   为确保跨帧属性迁移的高保真度和空间一致性，Durian 引入了双参考网络。\n    *   这些网络能够将来自人像图像和属性图像的空间特征注入到扩散模型的去噪过程中。\n\n*   **自重建训练范式（Self-reconstruction Formulation）**\n    *   模型采用独特的自重建方式进行训练。\n    *   具体而言，从同一人像视频中采样两帧：一帧被视为属性参考，另一帧作为目标人像。\n    *   模型随后根据这些输入及其对应的掩码重建视频中的剩余帧。\n\n*   **掩码扩展策略（Mask Expansion Strategy）**\n    *   为了有效支持具有不同空间范围的属性迁移，Durian 提出了一种基于关键点条件图像生成（keypoint-conditioned image generation）的掩码扩展策略，应用于训练阶段。\n\n*   **鲁棒性增强（Robustness Augmentation）**\n    *   为了提高模型对属性图像和人像图像之间可能存在的姿态或位置不对齐的鲁棒性，Durian 进一步通过空间和外观级别的变换来增强这些图像。\n\n### 主要成果与优势\n\n*   **强大的泛化能力**\n    *   尽管在训练时没有明确的三元组监督，上述策略使得 Durian 模型能够有效地泛化到多样化的属性和“野外”（in-the-wild）参考组合。\n\n*   **最先进的性能**\n    *   Durian 在带属性迁移的人像动画领域取得了最先进的性能。\n\n*   **多属性组合能力**\n    *   值得注意的是，其独特的双参考设计使得在单次生成过程中即可实现多属性的组合，而无需进行额外的训练或模型调整。",
      "shortSummary": "Durian 是一种创新方法，首次实现了从参考图像向目标人像进行零样本面部属性迁移，并生成人像动画。它通过引入双参考网络，将人像和属性特征注入扩散模型，并采用自重建和掩码扩展策略进行训练。Durian 在属性迁移人像动画方面取得了最先进的性能，并能实现多属性组合，无需额外训练，展现了强大的泛化能力。",
      "translated_title": "榴莲：双参考引导的人像动画与属性迁移",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Durian, the first method for generating portrait animation videos with facial attribute transfer from a given reference image to a target portrait in a zero-shot manner. To enable high-fidelity and spatially consistent attribute transfer across frames, we introduce dual reference networks that inject spatial features from both the portrait and attribute images into the denoising process of a diffusion model. We train the model using a self-reconstruction formulation, where two frames are sampled from the same portrait video: one is treated as the attribute reference and the other as the target portrait, and the remaining frames are reconstructed conditioned on these inputs and their corresponding masks. To support the transfer of attributes with varying spatial extent, we propose a mask expansion strategy using keypoint-conditioned image generation for training. In addition, we further augment the attribute and portrait images with spatial and appearance-level transformations to improve robustness to positional misalignment between them. These strategies allow the model to effectively generalize across diverse attributes and in-the-wild reference combinations, despite being trained without explicit triplet supervision. Durian achieves state-of-the-art performance on portrait animation with attribute transfer, and notably, its dual reference design enables multi-attribute composition in a single generation pass without additional training."
    },
    {
      "title": "迈向大语言模型后训练的统一视角 (原标题: Towards a Unified View of Large Language Model Post-Training)",
      "link": "https://arxiv.org/abs/2509.04419",
      "pubDate": "Thu, 04 Sep 2025 13:40:33 GMT",
      "isoDate": "2025-09-04T13:40:33.000Z",
      "creator": "Xingtai Lv, Yuxin Zuo, Youbang Sun, Hongyi Liu, Yuntian Wei, Zhekai Chen, Lixuan He, Xuekai Zhu, Kaiyan Zhang, Bingning Wang, Ning Ding, Bowen Zhou",
      "summary": "## 迈向大语言模型后训练的统一视角：核心发现与HPT算法\n\n本文深入探讨了现代大语言模型（LLM）后训练的两种主要方法——强化学习（RL）和监督微调（SFT），并提出了一个统一的理论框架和相应的算法。\n\n### 1. 后训练数据源与现有方法\n\n*   **数据源**：LLM的后训练主要依赖两种类型的训练数据：\n    *   **在线数据**：通常指模型自身生成的“回滚数据”（rollouts）。\n    *   **离线数据**：通常指人类或其它模型提供的“演示数据”（demonstrations）。\n*   **典型方法**：\n    *   **强化学习（RL）**：常用于处理在线数据。\n    *   **监督微调（SFT）**：常用于处理离线数据。\n\n### 2. 核心理论贡献：统一策略梯度估计器\n\n*   **统一视角**：本文的核心观点是，RL和SFT这两种看似不同的方法并非相互矛盾，而是单一优化过程的不同实例。\n*   **理论推导**：作者推导出了一个“统一策略梯度估计器”（Unified Policy Gradient Estimator）。\n    *   该估计器能够将广泛的后训练方法计算表示为在不同数据分布假设和各种偏差-方差权衡下，一个共同目标函数的梯度。\n*   **估计器构成**：该梯度估计器由四个可互换的部分构成：\n    *   **稳定化掩码**（stabilization mask）\n    *   **参考策略分母**（reference policy denominator）\n    *   **优势估计**（advantage estimate）\n    *   **似然梯度**（likelihood gradient）\n\n### 3. 提出的算法：混合后训练（Hybrid Post-Training, HPT）\n\n*   **算法动机**：受上述理论发现的启发，本文提出了一种名为“混合后训练”（HPT）的新算法。\n*   **设计目标**：\n    *   HPT旨在动态选择不同的训练信号，以实现最佳效果。\n    *   它被设计为能够有效利用演示数据（exploitation）。\n    *   同时，HPT也能实现稳定的探索（exploration）。\n    *   在上述过程中，HPT致力于不牺牲模型已学习到的推理模式。\n\n### 4. 实验验证与成果\n\n*   **广泛实验**：通过大量的实验和消融研究，本文验证了其统一理论框架和HPT算法的有效性。\n*   **卓越性能**：在六个数学推理基准测试和两个分布外（out-of-distribution）套件上，HPT算法持续超越了不同规模和家族模型的强大基线，展现出其优越性。",
      "shortSummary": "本文提出大语言模型后训练中的强化学习（RL）和监督微调（SFT）并非矛盾，而是单一优化过程的实例。作者推导了统一策略梯度估计器，将多种后训练方法统一在一个共同目标函数下。受此启发，提出混合后训练（HPT）算法，旨在动态选择训练信号，有效利用演示数据并实现稳定探索，同时保留推理模式。实验证明，HPT在多个数学推理基准上持续超越现有基线。",
      "translated_title": "迈向大语言模型后训练的统一视角",
      "images": [],
      "contentSource": "完整文章",
      "content": "Two major sources of training data exist for post-training modern language models: online (model-generated rollouts) data, and offline (human or other-model demonstrations) data. These two types of data are typically used by approaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT), respectively. In this paper, we show that these approaches are not in contradiction, but are instances of a single optimization process. We derive a Unified Policy Gradient Estimator, and present the calculations of a wide spectrum of post-training approaches as the gradient of a common objective under different data distribution assumptions and various bias-variance tradeoffs. The gradient estimator is constructed with four interchangeable parts: stabilization mask, reference policy denominator, advantage estimate, and likelihood gradient. Motivated by our theoretical findings, we propose Hybrid Post-Training (HPT), an algorithm that dynamically selects different training signals. HPT is designed to yield both effective exploitation of demonstration and stable exploration without sacrificing learned reasoning patterns. We provide extensive experiments and ablation studies to verify the effectiveness of our unified theoretical framework and HPT. Across six mathematical reasoning benchmarks and two out-of-distribution suites, HPT consistently surpasses strong baselines across models of varying scales and families."
    }
  ],
  "lastUpdated": "2025-09-10T09:32:19.881Z"
}