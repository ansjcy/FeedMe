{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "Qwen-Image 技术报告 (原标题: Qwen-Image Technical Report)",
      "link": "https://arxiv.org/abs/2508.02324",
      "pubDate": "Mon, 04 Aug 2025 07:49:20 GMT",
      "isoDate": "2025-08-04T07:49:20.000Z",
      "creator": "Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, Zenan Liu",
      "summary": "# Qwen-Image 技术报告概述\n\nQwen-Image 是 Qwen 系列中的一个图像生成基础模型，在复杂文本渲染和精确图像编辑方面取得了显著进展。\n\n## 核心能力与创新点\n\n### 1. 复杂文本渲染\n\n*   **挑战应对：** 针对复杂文本渲染的挑战，Qwen-Image 采用了多项创新策略：\n    *   **数据管线设计：** 构建了一个全面的数据管线，包括大规模数据收集、过滤、标注、合成和平衡。\n    *   **渐进式训练策略：** 采用课程学习（curriculum learning）方法，逐步提升模型的文本渲染能力：\n        *   从非文本到文本渲染开始。\n        *   逐步从简单到复杂的文本输入进行训练。\n        *   最终扩展到段落级别的描述。\n*   **效果提升：** 这种训练方法显著增强了模型原生的文本渲染能力。\n    *   在英文等字母语言上表现出色。\n    *   在中文等更具挑战性的表意文字语言上也取得了显著进展。\n\n### 2. 精确图像编辑\n\n*   **一致性增强：** 为提升图像编辑的一致性，Qwen-Image 引入了改进的多任务训练范式：\n    *   **任务整合：** 不仅包含传统的文本到图像（T2I）和文本-图像到图像（TI2I）任务，还加入了图像到图像（I2I）重建任务。\n    *   **潜在表示对齐：** 有效地对齐了 Qwen2.5-VL 和 MMDiT 之间的潜在表示。\n    *   **双编码机制：** 将原始图像分别输入到 Qwen2.5-VL 和 VAE 编码器：\n        *   Qwen2.5-VL 用于获取语义表示。\n        *   VAE 编码器用于获取重建表示。\n    *   **平衡语义与视觉：** 这种双编码机制使编辑模块能够在保持语义一致性和维护视觉保真度之间取得平衡。\n\n## 性能表现\n\nQwen-Image 在多个基准测试中均取得了最先进（state-of-the-art）的性能，充分展示了其在图像生成和编辑方面的强大能力。",
      "shortSummary": "Qwen-Image 是 Qwen 系列的图像生成模型，在复杂文本渲染和精确图像编辑方面实现重大突破。它通过设计全面的数据管线和渐进式训练策略，显著提升了多语言（包括中文）文本渲染能力。同时，通过改进的多任务训练范式和双编码机制，增强了图像编辑的一致性，平衡了语义与视觉保真度。模型在图像生成和编辑领域均达到最先进水平。",
      "translated_title": "Qwen-Image 技术报告",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Qwen-Image, an image generation foundation model in the Qwen series that achieves significant advances in complex text rendering and precise image editing. To address the challenges of complex text rendering, we design a comprehensive data pipeline that includes large-scale data collection, filtering, annotation, synthesis, and balancing. Moreover, we adopt a progressive training strategy that starts with non-text-to-text rendering, evolves from simple to complex textual inputs, and gradually scales up to paragraph-level descriptions. This curriculum learning approach substantially enhances the model's native text rendering capabilities. As a result, Qwen-Image not only performs exceptionally well in alphabetic languages such as English, but also achieves remarkable progress on more challenging logographic languages like Chinese. To enhance image editing consistency, we introduce an improved multi-task training paradigm that incorporates not only traditional text-to-image (T2I) and text-image-to-image (TI2I) tasks but also image-to-image (I2I) reconstruction, effectively aligning the latent representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and reconstructive representations, respectively. This dual-encoding mechanism enables the editing module to strike a balance between preserving semantic consistency and maintaining visual fidelity. Qwen-Image achieves state-of-the-art performance, demonstrating its strong capabilities in both image generation and editing across multiple benchmarks."
    },
    {
      "title": "VeOmni：使用以模型为中心的分布式配方库扩展任意模态模型训练 (原标题: VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo)",
      "link": "https://arxiv.org/abs/2508.02317",
      "pubDate": "Mon, 04 Aug 2025 07:33:04 GMT",
      "isoDate": "2025-08-04T07:33:04.000Z",
      "creator": "Qianli Ma, Yaowei Zheng, Zhelun Shi, Zhongkai Zhao, Bin Jia, Ziyue Huang, Zhiqi Lin, Youjie Li, Jiacheng Yang, Yanghua Peng, Zhi Zhang, Xin Liu",
      "summary": "# VeOmni：一种高效的全模态大语言模型训练框架\n\n## 挑战与背景\n\n近期大型语言模型（LLMs）的进展推动了全模态理解和生成能力的显著提升。然而，训练全模态LLMs面临着重大挑战，主要原因包括：\n\n*   **异构模型架构：** 处理多样化模态需要复杂的、异构的模型架构。\n*   **系统设计复杂性：** 实现高效的大规模训练需要精密的系统设计。\n*   **现有框架局限：** 当前框架通常将模型定义与并行逻辑紧密耦合，这导致可扩展性受限，并为端到端全模态训练带来了巨大的工程开销。\n\n## VeOmni 框架特点\n\n为应对上述挑战，研究人员提出了 **VeOmni**，一个模块化且高效的训练框架，旨在加速全模态LLMs的开发。VeOmni 的核心创新和特点包括：\n\n*   **以模型为中心的分布式配方：** VeOmni 引入了“以模型为中心的分布式配方”概念，实现了通信与计算的解耦。\n*   **高效的3D并行化：** 通过这种解耦，VeOmni 能够在全模态LLMs上实现高效的3D并行化。\n*   **灵活的配置接口：** 框架提供了一个灵活的配置接口，支持以最少的代码更改无缝集成新的模态。\n\n## 性能与成果\n\nVeOmni 在实际应用中展现了卓越的效率和可扩展性：\n\n*   **模型规模：** 使用 VeOmni 成功训练了一个参数量达300亿的全模态混合专家（MoE）模型。\n*   **训练吞吐量：** 该模型训练吞吐量超过2,800 tokens/秒/GPU。\n*   **上下文长度与并行规模：** 通过3D并行化，该模型在128个GPU上可扩展到160K的上下文长度。\n*   **总结：** 这些结果充分展示了 VeOmni 在训练大型全模态LLMs方面的卓越效率和可扩展性。",
      "shortSummary": "VeOmni是一个高效的全模态大语言模型（LLMs）训练框架。它通过引入以模型为中心的分布式配方，解耦通信与计算，实现了高效的3D并行化，并支持新模态的灵活集成。VeOmni显著提升了训练效率和可扩展性，例如，一个300亿参数的全模态MoE模型在128个GPU上能达到2,800 tokens/秒/GPU的吞吐量，并支持160K上下文长度。",
      "translated_title": "VeOmni：使用以模型为中心的分布式配方库扩展任意模态模型训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in large language models (LLMs) have driven impressive progress in omni-modal understanding and generation. However, training omni-modal LLMs remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training. Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training. % We present \\veomni, a modular and efficient training framework to accelerate the development of omni-modal LLMs. \\veomni introduces model-centric distributed recipes that decouples communication from computation, enabling efficient 3D parallelism on omni-modal LLMs. \\veomni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change. % Using \\veomni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal LLMs."
    },
    {
      "title": "CellForge：虚拟细胞模型的智能体设计 (原标题: CellForge: Agentic Design of Virtual Cell Models)",
      "link": "https://arxiv.org/abs/2508.02276",
      "pubDate": "Mon, 04 Aug 2025 06:43:31 GMT",
      "isoDate": "2025-08-04T06:43:31.000Z",
      "creator": "Xiangru Tang, Zhuoyun Yu, Jiapeng Chen, Yan Cui, Daniel Shao, Weixu Wang, Fang Wu, Yuchen Zhuang, Wenqi Shi, Zhi Huang, Arman Cohan, Xihong Lin, Fabian Theis, Smita Krishnaswamy, Mark Gerstein",
      "summary": "## CellForge：虚拟细胞模型的智能体设计\n\n### 引言与背景\n\n虚拟细胞建模是人工智能与生物学交叉领域的新兴前沿，旨在定量预测细胞对各种扰动的响应。然而，由于生物系统的复杂性、数据模态的异质性以及跨多个学科对领域特定专业知识的需求，自主构建虚拟细胞的计算模型极具挑战性。\n\n### CellForge 系统概述\n\nCellForge 是一个创新的智能体系统，它利用多智能体框架，将提供的生物数据集和研究目标直接转化为优化的虚拟细胞计算模型。该系统仅以原始单细胞多组学数据和任务描述作为输入，即可输出优化的模型架构和用于训练虚拟细胞模型及进行推断的可执行代码。\n\n### 核心模块\n\nCellForge 框架集成了三个核心模块：\n\n*   **任务分析 (Task Analysis)**：负责对提供的生物数据集进行特征化，并检索相关的文献信息。\n*   **方法设计 (Method Design)**：在此模块中，专业的智能体协同开发优化的建模策略。设计模块中的智能体分为具有不同视角的专家和一个中央协调者，它们必须协作交换解决方案，直到达成合理的共识。\n*   **实验执行 (Experiment Execution)**：负责自动化生成用于模型训练和推断的代码。\n\n### 能力展示与性能\n\nCellForge 在单细胞扰动预测任务中展示了其强大的能力。研究使用了六个多样化的数据集，涵盖了基因敲除、药物处理和细胞因子刺激等多种模态。CellForge 在这些任务中始终优于特定任务的最新方法。\n\n### 核心洞察与结论\n\nCellForge 的成功表明，具有不同视角的 LLM（大型语言模型）智能体之间的迭代交互，能够比直接解决建模挑战提供更好的解决方案。这突出了多智能体协作在复杂科学问题解决中的潜力。\n\n### 代码可用性\n\nCellForge 的代码已公开可用。",
      "shortSummary": "CellForge是一个多智能体系统，旨在自动化虚拟细胞模型的构建。它接收单细胞多组学数据和任务描述，输出优化的模型架构和可执行代码。系统包含任务分析、方法设计（智能体协作）和实验执行模块。CellForge在单细胞扰动预测等任务中表现出色，超越现有方法，证明了多视角LLM智能体迭代交互能提供更优解决方案。",
      "translated_title": "CellForge：虚拟细胞模型的智能体设计",
      "images": [],
      "contentSource": "完整文章",
      "content": "Virtual cell modeling represents an emerging frontier at the intersection of artificial intelligence and biology, aiming to predict quantities such as responses to diverse perturbations quantitatively. However, autonomously building computational models for virtual cells is challenging due to the complexity of biological systems, the heterogeneity of data modalities, and the need for domain-specific expertise across multiple disciplines. Here, we introduce CellForge, an agentic system that leverages a multi-agent framework that transforms presented biological datasets and research objectives directly into optimized computational models for virtual cells. More specifically, given only raw single-cell multi-omics data and task descriptions as input, CellForge outputs both an optimized model architecture and executable code for training virtual cell models and inference. The framework integrates three core modules: Task Analysis for presented dataset characterization and relevant literature retrieval, Method Design, where specialized agents collaboratively develop optimized modeling strategies, and Experiment Execution for automated generation of code. The agents in the Design module are separated into experts with differing perspectives and a central moderator, and have to collaboratively exchange solutions until they achieve a reasonable consensus. We demonstrate CellForge's capabilities in single-cell perturbation prediction, using six diverse datasets that encompass gene knockouts, drug treatments, and cytokine stimulations across multiple modalities. CellForge consistently outperforms task-specific state-of-the-art methods. Overall, CellForge demonstrates how iterative interaction between LLM agents with differing perspectives provides better solutions than directly addressing a modeling challenge. Our code is publicly available at https://github.com/gersteinlab/CellForge."
    },
    {
      "title": "超越权衡：面向推理模型指令遵循的自监督强化学习 (原标题: Beyond the Trade-off: Self-Supervised Reinforcement Learning for Reasoning Models' Instruction Following)",
      "link": "https://arxiv.org/abs/2508.02150",
      "pubDate": "Mon, 04 Aug 2025 03:48:59 GMT",
      "isoDate": "2025-08-04T03:48:59.000Z",
      "creator": "Qingyu Ren, Qianyu He, Bowei Zhang, Jie Zeng, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, Fei Yu",
      "summary": "# 超越权衡：面向推理模型指令遵循的自监督强化学习\n\n## 摘要\n\n本文提出了一种自监督强化学习（RL）框架，旨在解决推理模型在复杂问题解决能力和指令遵循能力之间存在的权衡问题。\n\n## 核心问题\n\n*   推理模型在复杂问题解决方面表现出色。\n*   然而，这些模型在推理能力和指令遵循能力之间存在一个令人担忧的权衡。\n\n## 现有方法的局限性\n\n*   当前提高指令遵循能力的方法依赖于更强的外部模型。\n*   这导致了方法论上的瓶颈和实际限制，包括成本增加和可访问性受限。\n\n## 提出的解决方案：自监督强化学习框架\n\n*   **方法名称**：自监督强化学习（RL）框架。\n*   **核心思想**：利用推理模型自身的内部信号来改进指令遵循能力。\n*   **关键特点**：无需外部监督。\n\n## 实验结果与优势\n\n*   **效果显著**：广泛的实验证明，该框架显著提高了指令遵循能力。\n*   **性能保持**：在提高指令遵循能力的同时，保持了原有的推理性能。\n*   **实用价值**：提供了一种可扩展且成本效益高的方法，以增强推理模型的指令遵循能力。\n\n## 数据与代码可用性\n\n*   相关数据和代码已公开可用。",
      "shortSummary": "推理模型在推理能力和指令遵循之间存在权衡。现有方法依赖外部模型，成本高昂且受限。本文提出一种自监督强化学习框架，利用模型自身内部信号，无需外部监督，显著提升指令遵循能力，同时保持推理性能。该方法具有可扩展性和成本效益，数据和代码已公开可用。",
      "translated_title": "超越权衡：面向推理模型指令遵循的自监督强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reasoning models excel in complex problem solving but exhibit a concerning trade off between reasoning capabilities and instruction following abilities. Existing approaches for improving instruction following rely on stronger external models, creating methodological bottlenecks and practical limitations including increased costs and accessibility constraints. We propose a self-supervised RL framework that leverages reasoning models' own internal signals to improve instruction following capabilities without external supervision. Extensive experiments demonstrate that our framework significantly improves instruction following capabilities while maintaining reasoning performance, offering a scalable and cost-effective approach to enhance instruction following in reasoning models. The data and code are publicly available at https://github.com/Rainier-rq/verl-if."
    },
    {
      "title": "SitEmb-v1.5：改进的上下文感知密集检索，用于语义关联和长篇故事理解 (原标题: SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension)",
      "link": "https://arxiv.org/abs/2508.01959",
      "pubDate": "Sun, 03 Aug 2025 19:59:31 GMT",
      "isoDate": "2025-08-03T19:59:31.000Z",
      "creator": "Junjie Wu, Jiangnan Li, Yuqing Li, Lemao Liu, Liyan Xu, Jiwei Li, Dit-Yan Yeung, Jie Zhou, Mo Yu",
      "summary": "# SitEmb-v1.5：改进的上下文感知密集检索，用于语义关联和长篇故事理解\n\n## 核心问题与挑战\n在对长文档进行检索增强生成（RAG）时，通常会将文本分割成较小的块进行检索。然而，由于原始文档内部的依赖关系，准确解释每个块往往需要上下文信息。尽管现有工作尝试通过编码更长的上下文窗口来生成更长块的嵌入，但检索和下游任务的性能提升仍然有限。这主要有以下两个原因：\n\n*   **模型容量限制**：更长的文本块增加了嵌入模型需要编码的信息量，从而对其容量造成压力。\n*   **实际应用需求**：许多实际应用场景仍需返回局部证据，以适应模型或人类带宽的限制。\n\n## SitEmb 方法提出\n为了解决上述挑战，本文提出了一种新颖的方法：通过将短文本块的表示与更广泛的上下文窗口进行条件关联，从而增强检索性能。这种方法旨在将文本块的意义“情境化”到其上下文中。\n\n研究发现，现有嵌入模型未能有效编码这种情境化上下文。因此，本文引入了一种新的训练范式，并开发了情境化嵌入模型（SitEmb）。\n\n## 评估与性能\n为了评估SitEmb方法的能力，研究团队专门策划了一个“图书情节检索”数据集，旨在评估情境化检索能力。\n\n*   **SitEmb-v1**：基于BGE-M3模型，参数量仅为10亿，但在该基准测试中显著优于最先进的嵌入模型，包括一些参数量高达70-80亿的模型。\n*   **SitEmb-v1.5**：参数量为80亿，在此基础上进一步将性能提升了10%以上，并在不同语言和多个下游应用中展现出强大的效果。\n\n## 模型可用性\n训练好的模型可从提供的链接下载。",
      "shortSummary": "长文档的检索增强生成（RAG）因文本分块导致上下文丢失而面临挑战。为解决此问题，本文提出了SitEmb模型，通过将短文本块的表示与更广泛的上下文关联，从而增强检索性能。SitEmb-v1（10亿参数）在图书情节检索数据集上显著超越了现有最先进模型（70-80亿参数）。其升级版SitEmb-v1.5（80亿参数）性能进一步提升超10%，并在多语言及多种下游应用中表现出色。",
      "translated_title": "SitEmb-v1.5：改进的上下文感知密集检索，用于语义关联和长篇故事理解",
      "images": [],
      "contentSource": "完整文章",
      "content": "Retrieval-augmented generation (RAG) over long documents typically involves splitting the text into smaller chunks, which serve as the basic units for retrieval. However, due to dependencies across the original document, contextual information is often essential for accurately interpreting each chunk. To address this, prior work has explored encoding longer context windows to produce embeddings for longer chunks. Despite these efforts, gains in retrieval and downstream tasks remain limited. This is because (1) longer chunks strain the capacity of embedding models due to the increased amount of information they must encode, and (2) many real-world applications still require returning localized evidence due to constraints on model or human bandwidth.   We propose an alternative approach to this challenge by representing short chunks in a way that is conditioned on a broader context window to enhance retrieval performance -- i.e., situating a chunk's meaning within its context. We further show that existing embedding models are not well-equipped to encode such situated context effectively, and thus introduce a new training paradigm and develop the situated embedding models (SitEmb). To evaluate our method, we curate a book-plot retrieval dataset specifically designed to assess situated retrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3 substantially outperforms state-of-the-art embedding models, including several with up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model further improves performance by over 10% and shows strong results across different languages and several downstream applications."
    },
    {
      "title": "压缩一瞥：大型视觉-语言模型中的动态视觉令牌剪枝 (原标题: A Glimpse to Compress: Dynamic Visual Token Pruning for Large Vision-Language Models)",
      "link": "https://arxiv.org/abs/2508.01548",
      "pubDate": "Sat, 02 Aug 2025 22:15:43 GMT",
      "isoDate": "2025-08-02T22:15:43.000Z",
      "creator": "Quan-Sheng Zeng, Yunheng Li, Qilong Wang, Peng-Tao Jiang, Zuxuan Wu, Ming-Ming Cheng, Qibin Hou",
      "summary": "### 压缩一瞥：大型视觉-语言模型中的动态视觉令牌剪枝\n\n本文介绍了一种名为 GlimpsePrune 的动态剪枝框架，旨在解决大型视觉-语言模型（LVLMs）在处理高分辨率输入时视觉令牌压缩的效率问题。\n\n**现有方法的局限性：**\n*   当前的视觉令牌压缩方法通常采用固定的压缩比。\n*   这种固定比率无法适应场景复杂度的变化，常常导致不精确的剪枝。\n*   不精确的剪枝可能丢弃信息丰富的视觉令牌，从而降低模型性能。\n\n**GlimpsePrune 框架：**\n*   **灵感来源：** 受人类认知过程启发。\n*   **核心机制：** 在答案生成之前，通过一次前向传播，进行数据驱动的“一瞥”，并剪除不相关的视觉令牌。\n*   **主要优势：**\n    *   **高效性：** 平均剪除了 92.6% 的视觉令牌。\n    *   **性能保持：** 在自由形式的视觉问答（VQA）任务中，平均完全保留了基线模型的性能。\n    *   **促进微调：** 计算成本的降低使得模型能够进行更有效的微调。\n*   **增强版 GlimpsePrune+：**\n    *   在保持相似高剪枝率的同时，实现了基线性能的 110%。\n\n**研究意义：**\n*   这项工作为构建更强大、更高效的 LVLMs 开辟了新途径。",
      "shortSummary": "本文提出了 GlimpsePrune，一个受人类认知启发的动态视觉令牌剪枝框架，旨在解决大型视觉-语言模型（LVLMs）中固定压缩比导致性能下降的问题。GlimpsePrune 在答案生成前动态剪除不相关令牌，实现了 92.6% 的高剪枝率，同时保持了基线性能。其增强版 GlimpsePrune+ 甚至能将性能提升至基线的 110%。这项工作为构建更高效、更强大的 LVLMs 提供了新方法。",
      "translated_title": "压缩一瞥：大型视觉-语言模型中的动态视觉令牌剪枝",
      "images": [],
      "contentSource": "完整文章",
      "content": "Visual token compression is critical for Large Vision-Language Models (LVLMs) to efficiently process high-resolution inputs. Existing methods that typically adopt fixed compression ratios cannot adapt to scenes of varying complexity, often causing imprecise pruning that discards informative visual tokens and results in degraded model performance. To address this issue, we introduce a dynamic pruning framework, GlimpsePrune, inspired by human cognition. It takes a data-driven ''glimpse'' and prunes irrelevant visual tokens in a single forward pass before answer generation. This approach prunes 92.6% of visual tokens while on average fully retaining the baseline performance on free-form VQA tasks. The reduced computational cost also enables more effective fine-tuning: an enhanced GlimpsePrune+ achieves 110% of the baseline performance while maintaining a similarly high pruning rate. Our work paves a new way for building more powerful and efficient LVLMs."
    },
    {
      "title": "RoboMemory：一种受大脑启发的物理具身系统终身学习多记忆智能体框架 (原标题: RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong Learning in Physical Embodied Systems)",
      "link": "https://arxiv.org/abs/2508.01415",
      "pubDate": "Sat, 02 Aug 2025 11:39:42 GMT",
      "isoDate": "2025-08-02T11:39:42.000Z",
      "creator": "Mingcong Lei, Honghao Cai, Zezhou Cui, Liangchen Tan, Junkun Hong, Gehan Hu, Shuangyu Zhu, Yimou Wu, Shaohan Jiang, Ge Wang, Zhen Li, Shuguang Cui, Yiming Zhao, Yatong Han",
      "summary": "## RoboMemory：受大脑启发的终身学习框架\n\nRoboMemory是一个受认知神经科学启发的、用于物理具身系统终身学习的多记忆智能体框架。它旨在解决现实世界环境中面临的关键挑战，包括持续学习、多模块记忆延迟、任务关联捕获以及闭环规划中的无限循环缓解。\n\n### 核心模块与大脑类比\nRoboMemory集成了四个核心模块，每个模块都与大脑的特定功能相对应，以实现长期规划和累积学习：\n\n*   **信息预处理器 (Information Preprocessor)**：类似于**丘脑**，负责初步处理输入信息。\n*   **终身具身记忆系统 (Lifelong Embodied Memory System)**：类似于**海马体**，是框架的核心，负责记忆的存储和检索。\n*   **闭环规划模块 (Closed-Loop Planning Module)**：类似于**前额叶**，负责高级规划和决策。\n*   **低级执行器 (Low-Level Executer)**：类似于**小脑**，负责具体的动作执行。\n\n### 终身具身记忆系统详解\n作为框架的核心，终身具身记忆系统旨在通过以下方式缓解复杂记忆框架中的推理速度问题：\n\n*   **并行化更新与检索**：实现记忆的高效处理。\n*   **多子模块**：包含空间 (Spatial)、时间 (Temporal)、情景 (Episodic) 和语义 (Semantic) 四个子模块，以处理不同类型的记忆。\n*   **动态知识图谱 (Knowledge Graph, KG)**：整合动态知识图谱以增强记忆的一致性和可扩展性。\n*   **一致的架构设计**：确保系统内部的统一性和效率。\n\n### 性能评估与成果\nRoboMemory在EmbodiedBench基准测试上进行了严格评估，并取得了显著成果：\n\n*   **超越开源基线**：在平均成功率方面，RoboMemory比开源基线Qwen2.5-VL-72B-Ins高出25%。\n*   **超越闭源SOTA**：它甚至超越了闭源的最新技术（State-of-the-Art, SOTA）Claude3.5-Sonnet 5%，确立了新的SOTA地位。\n*   **消融研究**：通过消融研究验证了关键组件（如批评器、空间记忆和长期记忆）的有效性。\n*   **真实世界部署**：在实际部署中，RoboMemory在重复任务中的成功率显著提高，证实了其终身学习能力。\n\n### 结论与意义\nRoboMemory成功缓解了高延迟挑战，并展现出良好的可扩展性。它为在物理机器人中集成多模态记忆系统提供了基础性参考，推动了具身智能领域的发展。",
      "shortSummary": "RoboMemory是一种受大脑启发的、用于物理具身系统终身学习的多记忆智能体框架。它通过整合信息预处理器、终身具身记忆系统（核心）、闭环规划模块和低级执行器，解决了持续学习、记忆延迟和规划等挑战。该框架在EmbodiedBench上表现出色，超越了现有基线和SOTA模型，并在真实世界部署中验证了其终身学习能力，为物理机器人中的多模态记忆系统提供了重要参考。",
      "translated_title": "RoboMemory：一种受大脑启发的物理具身系统终身学习多记忆智能体框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present RoboMemory, a brain-inspired multi-memory framework for lifelong learning in physical embodied systems, addressing critical challenges in real-world environments: continuous learning, multi-module memory latency, task correlation capture, and infinite-loop mitigation in closed-loop planning. Grounded in cognitive neuroscience, it integrates four core modules: the Information Preprocessor (thalamus-like), the Lifelong Embodied Memory System (hippocampus-like), the Closed-Loop Planning Module (prefrontal lobe-like), and the Low-Level Executer (cerebellum-like) to enable long-term planning and cumulative learning. The Lifelong Embodied Memory System, central to the framework, alleviates inference speed issues in complex memory frameworks via parallelized updates/retrieval across Spatial, Temporal, Episodic, and Semantic submodules. It incorporates a dynamic Knowledge Graph (KG) and consistent architectural design to enhance memory consistency and scalability. Evaluations on EmbodiedBench show RoboMemory outperforms the open-source baseline (Qwen2.5-VL-72B-Ins) by 25% in average success rate and surpasses the closed-source State-of-the-Art (SOTA) (Claude3.5-Sonnet) by 5%, establishing new SOTA. Ablation studies validate key components (critic, spatial memory, long-term memory), while real-world deployment confirms its lifelong learning capability with significantly improved success rates across repeated tasks. RoboMemory alleviates high latency challenges with scalability, serving as a foundational reference for integrating multi-modal memory systems in physical robots."
    },
    {
      "title": "探索所需，唯有利用 (原标题: Exploitation Is All You Need... for Exploration)",
      "link": "https://arxiv.org/abs/2508.01287",
      "pubDate": "Sat, 02 Aug 2025 05:42:59 GMT",
      "isoDate": "2025-08-02T05:42:59.000Z",
      "creator": "Micah Rentschler, Jesse Roberts",
      "summary": "## 探索所需，唯有利用：从利用中涌现探索\n\n### 核心挑战与传统方法\n\n在训练元强化学习（meta-RL）代理以解决新颖环境时，确保足够的探索是一个核心挑战。传统的探索-利用困境解决方案通常通过注入显式激励来鼓励探索，例如：\n\n*   随机化\n*   不确定性奖励（uncertainty bonuses）\n*   内在奖励（intrinsic rewards）\n\n### 本文假设\n\n本研究提出一个新颖的假设：一个仅为最大化贪婪（即仅利用）目标而训练的代理，在满足特定条件的情况下，也能表现出**涌现的探索行为**。这些关键条件包括：\n\n1.  **重复的环境结构（Recurring Environmental Structure）：** 环境中存在可重复的规律性，使得代理能够利用过去的经验来指导未来的选择。\n2.  **代理记忆（Agent Memory）：** 代理具备保留和利用历史交互数据的能力。\n3.  **长时程信用分配（Long-Horizon Credit Assignment）：** 学习过程能够在足够长的时间范围内传播回报，从而使探索带来的延迟收益能够有效地指导当前的决策。\n\n### 实验验证与主要发现\n\n研究人员通过在两种不同的实验环境中验证了这一假设：\n\n*   **随机多臂老虎机（stochastic multi-armed bandits）**\n*   **时间扩展网格世界（temporally extended gridworlds）**\n\n实验观察到以下关键结果：\n\n*   **涌现探索的条件：** 当环境结构和代理记忆这两个条件同时存在时，一个严格基于贪婪目标训练的策略确实表现出了信息寻求（information-seeking）的探索行为。\n*   **条件缺失的影响：** 通过受控消融实验（controlled ablations）进一步证明，如果缺少环境结构或代理记忆（即条件1或条件2），涌现的探索行为便会消失。\n*   **长时程信用分配的意外发现：** 令人惊讶的是，移除长时程信用分配（条件3）并不总是阻止涌现的探索。研究人员将这一结果归因于“伪汤普森采样效应”（pseudo-Thompson Sampling effect）。\n\n### 结论与启示\n\n这些发现表明，在适当的先决条件下，探索和利用不必被视为相互独立或正交的目标。相反，它们可以从一个统一的奖励最大化过程中自然涌现出来。",
      "shortSummary": "本文提出，元强化学习代理的探索行为可以从纯粹的贪婪（利用）目标中涌现，无需显式探索激励。这需要满足三个条件：重复的环境结构、代理记忆和长时程信用分配。实验表明，当前两个条件存在时，贪婪策略会展现信息寻求的探索行为。研究结果挑战了探索与利用是正交目标的传统观念，认为它们可从统一的奖励最大化过程中自然产生。",
      "translated_title": "探索所需，唯有利用",
      "images": [],
      "contentSource": "完整文章",
      "content": "Ensuring sufficient exploration is a central challenge when training meta-reinforcement learning (meta-RL) agents to solve novel environments. Conventional solutions to the exploration-exploitation dilemma inject explicit incentives such as randomization, uncertainty bonuses, or intrinsic rewards to encourage exploration. In this work, we hypothesize that an agent trained solely to maximize a greedy (exploitation-only) objective can nonetheless exhibit emergent exploratory behavior, provided three conditions are met: (1) Recurring Environmental Structure, where the environment features repeatable regularities that allow past experience to inform future choices; (2) Agent Memory, enabling the agent to retain and utilize historical interaction data; and (3) Long-Horizon Credit Assignment, where learning propagates returns over a time frame sufficient for the delayed benefits of exploration to inform current decisions. Through experiments in stochastic multi-armed bandits and temporally extended gridworlds, we observe that, when both structure and memory are present, a policy trained on a strictly greedy objective exhibits information-seeking exploratory behavior. We further demonstrate, through controlled ablations, that emergent exploration vanishes if either environmental structure or agent memory is absent (Conditions 1 &amp; 2). Surprisingly, removing long-horizon credit assignment (Condition 3) does not always prevent emergent exploration-a result we attribute to the pseudo-Thompson Sampling effect. These findings suggest that, under the right prerequisites, exploration and exploitation need not be treated as orthogonal objectives but can emerge from a unified reward-maximization process."
    },
    {
      "title": "文本到图像扩散模型的个性化安全对齐 (原标题: Personalized Safety Alignment for Text-to-Image Diffusion Models)",
      "link": "https://arxiv.org/abs/2508.01151",
      "pubDate": "Fri, 01 Aug 2025 22:23:20 GMT",
      "isoDate": "2025-08-01T22:23:20.000Z",
      "creator": "Yu Lei, Jinbin Bai, Qingyu Shi, Aosong Feng, Kaidong Yu",
      "summary": "## 文本到图像扩散模型的个性化安全对齐 (PSA)\n\n### 背景与问题\n\n*   **现有模型的局限性：** 文本到图像扩散模型在视觉内容生成方面取得了革命性进展，但其当前的安全机制采用统一标准，未能充分考虑个体用户的多样化偏好。\n*   **忽视个性化需求：** 这些模型忽略了由年龄、心理健康状况和个人信仰等因素塑造的、因人而异的安全边界。\n\n### 提出的解决方案：个性化安全对齐 (PSA)\n\n*   **核心目标：** PSA 是一个旨在解决上述问题的框架，它允许用户对生成模型的安全行为进行个性化控制。\n*   **工作原理：**\n    *   PSA 将个性化的用户配置文件整合到扩散过程中。\n    *   通过调整模型的行为来匹配用户的个体安全偏好，同时确保图像质量不受影响。\n    *   具体实现上，用户配置文件通过一种交叉注意力机制（cross-attention mechanism）被整合到模型中。\n\n### 新数据集：Sage\n\n*   为了支持PSA框架，研究引入了一个名为 **Sage** 的新数据集。\n*   该数据集专门用于捕获和表示用户特定的安全偏好。\n\n### 实验结果与优势\n\n*   **性能超越：** 实验结果表明，PSA 在有害内容抑制方面表现优于现有方法。\n*   **更好的用户对齐：** PSA 能够更好地使生成内容与用户设定的约束对齐，并在评估中取得了更高的胜率（Win Rate）和通过率（Pass Rate）分数。\n\n### 可用性\n\n*   该研究的代码、数据和模型已公开提供。",
      "shortSummary": "文本到图像扩散模型现有的统一安全标准忽视了用户个性化偏好。为解决此问题，研究提出了“个性化安全对齐”（PSA）框架。PSA通过将用户配置文件整合到扩散过程中，使模型行为与个体安全偏好匹配，同时保持图像质量。引入新数据集Sage捕获用户偏好。实验证明，PSA在抑制有害内容和对齐用户约束方面优于现有方法，提升了安全性和用户满意度。",
      "translated_title": "文本到图像扩散模型的个性化安全对齐",
      "images": [],
      "contentSource": "完整文章",
      "content": "Text-to-image diffusion models have revolutionized visual content generation, but current safety mechanisms apply uniform standards that often fail to account for individual user preferences. These models overlook the diverse safety boundaries shaped by factors like age, mental health, and personal beliefs. To address this, we propose Personalized Safety Alignment (PSA), a framework that allows user-specific control over safety behaviors in generative models. PSA integrates personalized user profiles into the diffusion process, adjusting the model's behavior to match individual safety preferences while preserving image quality. We introduce a new dataset, Sage, which captures user-specific safety preferences and incorporates these profiles through a cross-attention mechanism. Experiments show that PSA outperforms existing methods in harmful content suppression and aligns generated content better with user constraints, achieving higher Win Rate and Pass Rate scores. Our code, data, and models are publicly available at https://torpedo2648.github.io/PSAlign/."
    },
    {
      "title": "Llama-3.1-基础AI安全LLM-8B-Instruct 技术报告 (原标题: Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report)",
      "link": "https://arxiv.org/abs/2508.01059",
      "pubDate": "Fri, 01 Aug 2025 16:25:57 GMT",
      "isoDate": "2025-08-01T16:25:57.000Z",
      "creator": "Sajana Weerawardhena, Paul Kassianik, Blaine Nelson, Baturay Saglam, Anu Vellore, Aman Priyanshu, Supriti Vijay, Massimo Aufiero, Arthur Goldblatt, Fraser Burch, Ed Li, Jianliang He, Dhruv Kedia, Kojin Oshiba, Zhouran Yang, Yaron Singer, Amin Karbasi",
      "summary": "### Foundation-Sec-8B-Instruct 技术报告\n\n#### 引言与背景\n大型语言模型（LLM）在多个领域取得了显著成功，但在网络安全应用中的整合仍面临挑战。这主要是由于缺乏通用的网络安全数据、表征复杂性以及安全和监管方面的担忧。为解决这一差距，研究人员此前推出了Foundation-Sec-8B，这是一个专注于网络安全的LLM，适用于下游任务的微调。然而，该模型并非为聊天式交互或指令遵循而设计。\n\n#### Foundation-Sec-8B-Instruct 模型介绍\n本报告发布了Foundation-Sec-8B-Instruct，这是一个专门为通用网络安全对话训练的模型。它基于Foundation-Sec-8B构建，将领域特定知识与指令遵循、对话能力以及与人类偏好的对齐相结合，旨在生成高质量、相关的响应。\n\n#### 性能评估\n全面的评估结果显示，Foundation-Sec-8B-Instruct在多项网络安全任务上表现优于Llama 3.1-8B-Instruct，同时在指令遵循性能上与其持平。此外，该模型在网络威胁情报和指令遵循任务上与GPT-4o-mini也具有竞争力。\n\n#### 愿景与发布\n研究人员设想Foundation-Sec-8B-Instruct将成为网络安全专业人员日常工作流程中不可或缺的助手。该模型已公开发布。",
      "shortSummary": "Foundation-Sec-8B-Instruct是一个专为网络安全对话设计的新型大语言模型。它基于Foundation-Sec-8B，融合了领域知识、指令遵循和对话能力。评估显示，该模型在网络安全任务上超越Llama 3.1-8B-Instruct，并在网络威胁情报和指令遵循方面与GPT-4o-mini相当。该模型旨在成为网络安全专业人员的得力助手，并已公开发布。",
      "translated_title": "Llama-3.1-基础AI安全LLM-8B-Instruct 技术报告",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) have shown remarkable success across many domains, yet their integration into cybersecurity applications remains limited due to a lack of general-purpose cybersecurity data, representational complexity, and safety and regulatory concerns. To address this gap, we previously introduced Foundation-Sec-8B, a cybersecurity-focused LLM suitable for fine-tuning on downstream tasks. That model, however, was not designed for chat-style interactions or instruction-following. In this report, we release Foundation-Sec-8B-Instruct: a model specifically trained for general-purpose cybersecurity dialogue. Built on Foundation-Sec-8B, it combines domain-specific knowledge with instruction-following, conversational capabilities, and alignment with human preferences to produce high-quality, relevant responses. Comprehensive evaluations show that Foundation-Sec-8B-Instruct outperforms Llama 3.1-8B-Instruct on a range of cybersecurity tasks while matching its instruction-following performance. It is also competitive with GPT-4o-mini on cyber threat intelligence and instruction-following tasks. We envision Foundation-Sec-8B-Instruct becoming an indispensable assistant in the daily workflows of cybersecurity professionals. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct."
    },
    {
      "title": "Phi-Ground 技术报告：提升 GUI 接地感知能力 (原标题: Phi-Ground Tech Report: Advancing Perception in GUI Grounding)",
      "link": "https://arxiv.org/abs/2507.23779",
      "pubDate": "Thu, 31 Jul 2025 13:59:09 GMT",
      "isoDate": "2025-07-31T13:59:09.000Z",
      "creator": "Miaosen Zhang, Ziqiang Xu, Jialiang Zhu, Qi Dai, Kai Qiu, Yifan Yang, Chong Luo, Tianyi Chen, Justin Wagle, Tim Franklin, Baining Guo",
      "summary": "## Phi-Ground 技术报告：提升 GUI 接地感知能力\n\n### 引言\n\n*   随着多模态推理模型的发展，计算机使用代理（CUAs），如《钢铁侠》中的贾维斯，正逐渐成为现实。\n*   GUI 接地（GUI grounding）是 CUAs 执行实际操作的核心组成部分，类似于机器人中的机械控制，直接决定系统的成败。它涉及确定点击、输入等操作及其相关参数（如点击坐标）。\n\n### 当前挑战\n\n*   目前的端到端接地模型在 ScreenSpot-pro 和 UI-Vision 等挑战性基准测试上的准确率仍低于 65%，远未达到部署要求。\n\n### 研究方法与成果\n\n*   本研究对接地模型的训练进行了实证研究，详细考察了从数据收集到模型训练的各个环节。\n*   最终开发出 **Phi-Ground 模型家族**。\n\n### Phi-Ground 模型性能\n\n*   在代理设置下，Phi-Ground 模型家族在所有五个接地基准测试中，对于参数量小于 100 亿的模型，均实现了最先进（SOTA）的性能。\n*   在端到端模型设置中，Phi-Ground 模型在 ScreenSpot-pro 上取得了 **43.2** 分，在 UI-Vision 上取得了 **27.2** 分，同样达到了 SOTA 结果。\n\n### 研究意义\n\n*   论文中讨论的各种细节，包括成功经验和失败教训，不仅阐明了接地模型的构建过程，也对其他感知任务有所裨益。",
      "shortSummary": "Phi-Ground 技术报告介绍了提升 GUI 接地感知能力的研究。GUI 接地是计算机使用代理（CUAs）执行操作的关键，但当前模型准确率低。本研究通过实证分析，开发了 Phi-Ground 模型家族。该模型在多个接地基准测试中，对于参数量小于 100 亿的模型，均实现了最先进（SOTA）的性能，包括在 ScreenSpot-pro 和 UI-Vision 上取得显著提升。研究成果对构建接地模型及其他感知任务具有重要意义。",
      "translated_title": "Phi-Ground 技术报告：提升 GUI 接地感知能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "With the development of multimodal reasoning models, Computer Use Agents (CUAs), akin to Jarvis from \"Iron Man\", are becoming a reality. GUI grounding is a core component for CUAs to execute actual actions, similar to mechanical control in robotics, and it directly leads to the success or failure of the system. It determines actions such as clicking and typing, as well as related parameters like the coordinates for clicks. Current end-to-end grounding models still achieve less than 65\\% accuracy on challenging benchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from being ready for deployment. % , as a single misclick can result in unacceptable consequences. In this work, we conduct an empirical study on the training of grounding models, examining details from data collection to model training. Ultimately, we developed the Phi-Ground model family, which achieves state-of-the-art performance across all five grounding benchmarks for models under 10B parameters in agent settings. In the end-to-end model setting, our model still achieves SOTA results with scores of \\textbf{43.2} on ScreenSpot-pro and \\textbf{27.2} on UI-Vision. We believe that the various details discussed in this paper, along with our successes and failures, not only clarify the construction of grounding models but also benefit other perception tasks. Project homepage: https://zhangmiaosen2000.github.io/Phi-Ground/{https://zhangmiaosen2000.github.io/Phi-Ground/}"
    },
    {
      "title": "Seed-Prover：用于自动化定理证明的深度与广度推理 (原标题: Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving)",
      "link": "https://arxiv.org/abs/2507.23726",
      "pubDate": "Thu, 31 Jul 2025 13:00:30 GMT",
      "isoDate": "2025-07-31T13:00:30.000Z",
      "creator": "Luoxin Chen, Jinming Gu, Liankai Huang, Wenhao Huang, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin, Chenggang Li, Kaijing Ma, Cheng Ren, Jiawei Shen, Wenlei Shi, Tong Sun, He Sun, Jiahui Wang, Siran Wang, Zhihong Wang, Chenrui Wei, Shufa Wei, Yonghui Wu, Yuchen Wu, Yihang Xia, Huajian Xin, Fan Yang, Huaiyuan Ying, Hongyi Yuan, Zheng Yuan, Tianyang Zhan, Chi Zhang, Yue Zhang, Ge Zhang, Tianyun Zhao, Jianqiu Zhao, Yichi Zhou, Thomas Hanwen Zhu",
      "summary": "大型语言模型（LLMs）在结合强化学习和长链式思考（chain-of-thought）时，已展现出强大的数学推理能力。然而，由于在仅使用自然语言时缺乏明确的监督信号，LLMs在定理证明方面仍面临挑战。相比之下，Lean等领域特定语言通过对证明进行形式化验证，提供了清晰的监督，从而能够进行有效的训练。\n\n本文提出了 **Seed-Prover**，一个引理风格的整体证明推理模型。Seed-Prover 能够基于 Lean 的反馈、已证明的引理以及自我总结，迭代地完善其证明。为了解决国际数学奥林匹克（IMO）级别的竞赛问题，研究团队设计了三种测试时推理策略，以实现深度（deep）和广度（broad）推理。\n\n为了弥补 Lean 在几何支持方面的不足，研究团队引入了一个名为 **Seed-Geometry** 的几何推理引擎。Seed-Geometry 在几何推理方面超越了之前形式化几何引擎的性能。\n\n**主要成果：**\n*   **IMO 问题：** Seed-Prover 成功证明了 78.1% 的形式化过的历届 IMO 问题。\n*   **MiniF2F：** 在 MiniF2F 基准测试中达到了饱和（saturates）。\n*   **PutnamBench：** 在 PutnamBench 上取得了超过 50% 的成绩。\n*   **超越SOTA：** 这些结果显著超越了之前的最先进（state-of-the-art, SOTA）水平。\n*   **IMO 2025 参与：** Seed-Prover 和 Seed-Geometry 这两个系统共同参与了 IMO 2025，并成功完全证明了 6 道问题中的 5 道。\n\n这项工作代表了自动化数学推理领域的一个重大进步，有力地证明了结合形式化验证与长链式思考推理的有效性。",
      "shortSummary": "Seed-Prover是一个引理风格的自动化定理证明模型，旨在解决LLM在定理证明中缺乏明确监督信号的问题。它利用Lean的反馈、已证明引理和自我总结迭代优化证明，并设计了深度与广度推理策略。为弥补Lean的几何不足，引入了Seed-Geometry。Seed-Prover在IMO、MiniF2F和PutnamBench上表现卓越，大幅超越现有技术，并在IMO 2025中成功证明5/6道题。这项工作展示了形式化验证与长链式思考在自动化数学推理中的强大潜力。",
      "translated_title": "Seed-Prover：用于自动化定理证明的深度与广度推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose Seed-Prover, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves 78.1% of formalized past IMO problems, saturates MiniF2F, and achieves over 50\\% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine Seed-Geometry, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning."
    },
    {
      "title": "可扩展的多任务强化学习，用于视觉运动智能体中可泛化的空间智能 (原标题: Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents)",
      "link": "https://arxiv.org/abs/2507.23698",
      "pubDate": "Thu, 31 Jul 2025 12:20:02 GMT",
      "isoDate": "2025-07-31T12:20:02.000Z",
      "creator": "Shaofei Cai, Zhancun Mu, Haiwen Xia, Bowei Zhang, Anji Liu, Yitao Liang",
      "summary": "### 背景与挑战\n\n*   强化学习（RL）在语言建模领域取得了显著成功，但其在视觉运动智能体中的应用尚未完全实现。\n*   RL模型面临的主要挑战是容易过拟合特定任务或环境，这阻碍了它们在多样化设置中获得可泛化的行为。\n\n### 本文贡献与研究目标\n\n*   本文通过在Minecraft中对RL微调的视觉运动智能体进行零样本泛化到未见过的世界，初步解决了上述挑战。\n*   研究旨在探索RL在增强3D世界中可泛化的空间推理和交互能力方面的潜力。\n\n### 提出的方法与技术\n\n*   **统一多任务目标空间**：为了解决多任务RL表示中的挑战，本文分析并建立了“跨视图目标规范”（cross-view goal specification）作为视觉运动策略的统一多任务目标空间。\n*   **自动化任务合成**：为了克服手动任务设计的显著瓶颈，本文提出在高度可定制的Minecraft环境中进行自动化任务合成，以支持大规模多任务RL训练。\n*   **高效分布式RL框架**：构建了一个高效的分布式RL框架来支持大规模多任务训练。\n\n### 实验结果\n\n*   实验结果表明，RL显著提升了交互成功率4倍。\n*   RL使得空间推理能够在多样化环境（包括真实世界设置）中实现零样本泛化。\n\n### 结论与意义\n\n*   研究结果强调了在3D模拟环境（特别是那些适合大规模任务生成的环境）中进行RL训练的巨大潜力。\n*   这种方法能够显著提升视觉运动智能体的空间推理能力。",
      "shortSummary": "本文解决了强化学习（RL）在视觉运动智能体中泛化能力不足的问题。研究通过在Minecraft中对RL微调的智能体进行大规模多任务训练，实现了对未见环境的零样本泛化。通过引入跨视图目标规范和自动化任务合成，RL将交互成功率提高了4倍，并使空间推理在多样化环境中实现零样本泛化。这突显了在可生成大规模任务的3D模拟环境中进行RL训练，对于提升视觉运动智能体空间智能的巨大潜力。",
      "translated_title": "可扩展的多任务强化学习，用于视觉运动智能体中可泛化的空间智能",
      "images": [],
      "contentSource": "完整文章",
      "content": "While Reinforcement Learning (RL) has achieved remarkable success in language modeling, its triumph hasn't yet fully translated to visuomotor agents. A primary challenge in RL models is their tendency to overfit specific tasks or environments, thereby hindering the acquisition of generalizable behaviors across diverse settings. This paper provides a preliminary answer to this challenge by demonstrating that RL-finetuned visuomotor agents in Minecraft can achieve zero-shot generalization to unseen worlds. Specifically, we explore RL's potential to enhance generalizable spatial reasoning and interaction capabilities in 3D worlds. To address challenges in multi-task RL representation, we analyze and establish cross-view goal specification as a unified multi-task goal space for visuomotor policies. Furthermore, to overcome the significant bottleneck of manual task design, we propose automated task synthesis within the highly customizable Minecraft environment for large-scale multi-task RL training, and we construct an efficient distributed RL framework to support this. Experimental results show RL significantly boosts interaction success rates by 4times and enables zero-shot generalization of spatial reasoning across diverse environments, including real-world settings. Our findings underscore the immense potential of RL training in 3D simulated environments, especially those amenable to large-scale task generation, for significantly advancing visuomotor agents' spatial reasoning."
    },
    {
      "title": "villa-X：增强视觉-语言-动作模型中的潜在动作建模 (原标题: villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models)",
      "link": "https://arxiv.org/abs/2507.23682",
      "pubDate": "Thu, 31 Jul 2025 11:57:46 GMT",
      "isoDate": "2025-07-31T11:57:46.000Z",
      "creator": "Xiaoyu Chen, Hangxing Wei, Pushi Zhang, Chuheng Zhang, Kaixin Wang, Yanjiang Guo, Rushuai Yang, Yucen Wang, Xinquan Xiao, Li Zhao, Jianyu Chen, Jiang Bian",
      "summary": "## villa-X：增强机器人操作的潜在动作建模\n\n### 引言\n\n视觉-语言-动作（VLA）模型已成为学习机器人操作策略的流行范式。这类模型能够理解语言指令并泛化到新场景。近期研究开始探索将“潜在动作”——一种表示两帧之间视觉变化的抽象表示——整合到VLA预训练中。\n\n### villa-X 框架\n\n本文引入了 **villa-X**，这是一个新颖的视觉-语言-潜在动作（ViLLA）框架，旨在推进潜在动作建模，以学习更具泛化能力的机器人操作策略。该方法在以下两个关键方面进行了改进：\n\n*   **潜在动作的学习方式**：优化了潜在动作的获取和表示方法。\n*   **潜在动作的整合方式**：改进了将潜在动作融入VLA预训练过程的机制。\n\n### 性能表现\n\n通过这些贡献，villa-X 在多个模拟环境（包括 SIMPLER 和 LIBERO）以及两个真实世界机器人设置（包括抓手和灵巧手操作）中均取得了卓越的性能。\n\n### 结论与展望\n\n研究人员认为，ViLLA 范式具有巨大的前景，而 villa-X 为未来的相关研究奠定了坚实的基础。",
      "shortSummary": "villa-X 是一种新颖的视觉-语言-潜在动作（ViLLA）框架，旨在增强机器人操作中的潜在动作建模。它改进了潜在动作的学习和整合方式，使机器人能够更好地遵循语言指令并泛化到新场景。该框架在模拟环境和真实世界机器人设置中均表现出色，为未来的机器人操作研究奠定了坚实基础。",
      "translated_title": "villa-X：增强视觉-语言-动作模型中的潜在动作建模",
      "images": [],
      "contentSource": "完整文章",
      "content": "Visual-Language-Action (VLA) models have emerged as a popular paradigm for learning robot manipulation policies that can follow language instructions and generalize to novel scenarios. Recent work has begun to explore the incorporation of latent actions, an abstract representation of visual change between two frames, into VLA pre-training. In this paper, we introduce villa-X, a novel Visual-Language-Latent-Action (ViLLA) framework that advances latent action modeling for learning generalizable robot manipulation policies. Our approach improves both how latent actions are learned and how they are incorporated into VLA pre-training. Together, these contributions enable villa-X to achieve superior performance across simulated environments including SIMPLER and LIBERO, as well as on two real-world robot setups including gripper and dexterous hand manipulation. We believe the ViLLA paradigm holds significant promise, and that our villa-X provides a strong foundation for future research."
    },
    {
      "title": "关于Softmax注意力机制的表达能力：一种循环神经网络视角 (原标题: On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective)",
      "link": "https://arxiv.org/abs/2507.23632",
      "pubDate": "Thu, 31 Jul 2025 11:10:03 GMT",
      "isoDate": "2025-07-31T11:10:03.000Z",
      "creator": "Gabriel Mongaras, Eric C. Larson",
      "summary": "# Softmax注意力机制的表达能力：一种循环神经网络视角\n\n## 引言\n\n自其引入以来，Softmax注意力机制已成为现代Transformer架构的核心支柱，这主要归因于其强大的表达能力以及在各种任务中的可扩展性。然而，Softmax注意力机制的主要缺点在于其内存需求和计算复杂度与序列长度呈二次方关系，这在处理长序列时构成了显著的瓶颈。\n\n## 线性注意力机制的挑战\n\n为了规避Softmax注意力的二次方瓶颈，研究人员引入了线性注意力及其类似方法，这些方法通过替换Softmax非线性函数来降低复杂度。尽管这些线性形式的注意力机制源自原始的Softmax公式，但它们在下游任务的准确性方面通常表现出滞后。虽然关于查询（query）和键（key）内积上的Softmax非线性函数具有优于其他非线性函数的强烈直观特性，但这种性能差异存在的原因仍然是一个悬而未决的问题。\n\n## 本文贡献与发现\n\n本研究旨在解决上述问题，并深入探讨Softmax注意力机制的内在表达能力。主要贡献和发现包括：\n\n*   **近似关系证明：** 本文通过推导出Softmax注意力的循环形式，明确证明了线性注意力是Softmax注意力的一种近似。这一关键发现为理解两种机制之间的关系提供了新的理论基础。\n*   **RNN视角：** 利用推导出的循环形式，Softmax注意力的每个组成部分都可以用循环神经网络（RNN）的语言进行描述。这种将Softmax注意力视为RNN的描述方式，为深入分析其内部机制提供了一个强大且直观的框架。\n*   **组件消融分析：** 将Softmax注意力描述为RNN，使得对Softmax注意力组件进行消融研究成为可能。通过这种方法，研究人员能够系统地理解每个部分的重要性以及它们如何相互作用，从而揭示Softmax注意力为何能够展现出更强的表达能力。\n*   **解释表达能力：** 最终，本研究的工作有助于解释为什么Softmax注意力比其替代品（如线性注意力）更具表达能力。这不仅加深了我们对现有注意力机制的理解，也为未来更高效、更强大的注意力机制的设计和优化提供了重要的理论指导。",
      "shortSummary": "Softmax注意力是Transformer核心，但存在二次方复杂度。线性注意力旨在解决此问题，但准确性较低，其原因不明。本文通过推导Softmax注意力的循环形式，证明线性注意力是其近似。研究将Softmax注意力描述为循环神经网络（RNN），这有助于理解其组件作用及其相互作用，并最终解释了Softmax注意力为何比其替代品更具表达能力。",
      "translated_title": "关于Softmax注意力机制的表达能力：一种循环神经网络视角",
      "images": [],
      "contentSource": "完整文章",
      "content": "Since its introduction, softmax attention has become the backbone of modern transformer architectures due to its expressiveness and scalability across a wide range of tasks. However, the main drawback of softmax attention is the quadratic memory requirement and computational complexity with respect to the sequence length. By replacing the softmax nonlinearity, linear attention and similar methods have been introduced to avoid the quadratic bottleneck of softmax attention. Despite these linear forms of attention being derived from the original softmax formulation, they typically lag in terms of downstream accuracy. While strong intuition of the softmax nonlinearity on the query and key inner product suggests that it has desirable properties compared to other nonlinearities, the question of why this discrepancy exists still remains unanswered. This work demonstrates that linear attention is an approximation of softmax attention by deriving the recurrent form of softmax attention. Using this form, each part of softmax attention can be described in the language of recurrent neural networks (RNNs). Describing softmax attention as an RNN allows for the ablation of the components of softmax attention to understand the importance of each part and how they interact. In this way, our work helps explain why softmax attention is more expressive than its counterparts."
    },
    {
      "title": "超越线性瓶颈：基于样条的知识蒸馏用于文化多元艺术风格分类 (原标题: Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for Culturally Diverse Art Style Classification)",
      "link": "https://arxiv.org/abs/2507.23436",
      "pubDate": "Thu, 31 Jul 2025 07:16:00 GMT",
      "isoDate": "2025-07-31T07:16:00.000Z",
      "creator": "Abdellah Zakaria Sellam, Salah Eddine Bekhouche, Cosimo Distante, Abdelmalik Taleb-Ahmed",
      "summary": "### 艺术风格分类的挑战与现有方法的局限\n\n艺术风格分类在计算美学领域仍是一个巨大挑战，主要原因有二：\n\n*   **数据集稀缺**：缺乏经过专家标注的艺术风格数据集。\n*   **复杂交互**：风格元素之间存在复杂且通常是非线性的相互作用。\n\n尽管近期出现的双教师自监督框架在一定程度上减少了对标注数据的依赖，但它们存在显著局限性：\n\n*   **线性投影层**：其固有的线性特性难以捕捉复杂的风格特征交互。\n*   **局部关注**：过于侧重局部信息，难以有效建模全局构图上下文。\n\n### 提出的解决方案：基于KAN的知识蒸馏\n\n为了克服上述局限，本文提出了一种增强型的双教师知识蒸馏框架。核心改进在于：\n\n*   **替换传统MLP**：将传统的MLP（多层感知机）投影和预测头替换为**Kolmogorov-Arnold Networks (KANs)**。\n\n### 方法原理与优势\n\n该方法通过以下机制实现性能提升：\n\n*   **保留互补指导**：\n    *   一个教师网络侧重于**局部纹理和笔触模式**。\n    *   另一个教师网络捕捉**更广泛的风格层次结构**。\n    *   这种双教师结构确保了对艺术风格多方面信息的全面学习。\n*   **KAN的非线性建模能力**：\n    *   利用KAN基于**样条的激活函数**，能够以数学精度建模复杂的非线性特征关联。\n    *   这使得模型能够更好地理解和解缠艺术风格中固有的复杂、非线性的特征关系。\n\n### 实验结果与发现\n\n*   **性能提升**：在WikiArt和Pandora18k数据集上的实验结果表明，本文提出的方法在Top-1准确率上优于基础的双教师架构。\n*   **KAN的重要性**：研究结果强调了KAN在**解缠复杂风格流形**方面的重要性，这直接导致了比传统MLP投影更好的线性探测准确率。\n\n### 结论\n\n本研究证明了将KAN引入知识蒸馏框架，能够有效解决艺术风格分类中线性瓶颈和复杂非线性交互建模的挑战，为文化多元艺术风格的精确分类提供了新的途径。",
      "shortSummary": "针对艺术风格分类中数据稀缺和复杂非线性交互的挑战，本文提出一种增强型双教师知识蒸馏框架。该框架用Kolmogorov-Arnold Networks (KANs) 替换传统MLP，利用KAN的样条基激活函数精确建模非线性特征关联，同时保留双教师的互补指导。实验证明，该方法在WikiArt和Pandora18k数据集上优于基线模型，显著提升了Top-1准确率和线性探测准确率，突显了KAN在解缠复杂风格流形中的关键作用。",
      "translated_title": "超越线性瓶颈：基于样条的知识蒸馏用于文化多元艺术风格分类",
      "images": [],
      "contentSource": "完整文章",
      "content": "Art style classification remains a formidable challenge in computational aesthetics due to the scarcity of expertly labeled datasets and the intricate, often nonlinear interplay of stylistic elements. While recent dual-teacher self-supervised frameworks reduce reliance on labeled data, their linear projection layers and localized focus struggle to model global compositional context and complex style-feature interactions. We enhance the dual-teacher knowledge distillation framework to address these limitations by replacing conventional MLP projection and prediction heads with Kolmogorov-Arnold Networks (KANs). Our approach retains complementary guidance from two teacher networks, one emphasizing localized texture and brushstroke patterns, the other capturing broader stylistic hierarchies while leveraging KANs' spline-based activations to model nonlinear feature correlations with mathematical precision. Experiments on WikiArt and Pandora18k demonstrate that our approach outperforms the base dual teacher architecture in Top-1 accuracy. Our findings highlight the importance of KANs in disentangling complex style manifolds, leading to better linear probe accuracy than MLP projections."
    },
    {
      "title": "基于注意力相关性评分的增强型阿拉伯语文本检索 (原标题: Enhanced Arabic Text Retrieval with Attentive Relevance Scoring)",
      "link": "https://arxiv.org/abs/2507.23404",
      "pubDate": "Thu, 31 Jul 2025 06:18:28 GMT",
      "isoDate": "2025-07-31T06:18:28.000Z",
      "creator": "Salah Eddine Bekhouche, Azeddine Benlamoudi, Yazid Bounab, Fadi Dornaika, Abdenour Hadid",
      "summary": "## 基于注意力相关性评分的增强型阿拉伯语文本检索\n\n### 挑战与背景\n\n*   **阿拉伯语的复杂性**：阿拉伯语在自然语言处理（NLP）和信息检索（IR）领域面临独特挑战，主要原因包括：\n    *   复杂的形态学。\n    *   可选的变音符号。\n    *   现代标准阿拉伯语（MSA）与各种方言并存。\n*   **研究不足**：尽管阿拉伯语在全球日益重要，但在NLP研究和基准资源方面仍处于代表性不足的状态。\n\n### 提出的解决方案\n\n*   **增强型密集段落检索（DPR）框架**：本文提出了一种专门为阿拉伯语开发的增强型DPR框架。\n*   **核心创新：注意力相关性评分（ARS）**：\n    *   ARS是该方法的核心，它取代了标准的交互机制。\n    *   ARS采用了一种自适应评分函数，能够更有效地建模问题和段落之间的语义相关性。\n*   **技术整合**：该方法集成了预训练的阿拉伯语语言模型和架构改进。\n\n### 成果与影响\n\n*   **性能提升**：显著提高了检索性能。\n*   **排名准确性**：在回答阿拉伯语问题时，显著提高了排名准确性。\n\n### 可用性\n\n*   **代码公开**：相关代码已在GitHub上公开提供。",
      "shortSummary": "本文提出了一种增强型密集段落检索（DPR）框架，旨在解决阿拉伯语在自然语言处理和信息检索中的挑战。该框架的核心是新颖的注意力相关性评分（ARS），它通过自适应评分函数更有效地建模问题与段落间的语义相关性。该方法整合了预训练的阿拉伯语语言模型，显著提升了阿拉伯语文本检索性能和排名准确性。代码已公开。",
      "translated_title": "基于注意力相关性评分的增强型阿拉伯语文本检索",
      "images": [],
      "contentSource": "完整文章",
      "content": "Arabic poses a particular challenge for natural language processing (NLP) and information retrieval (IR) due to its complex morphology, optional diacritics and the coexistence of Modern Standard Arabic (MSA) and various dialects. Despite the growing global significance of Arabic, it is still underrepresented in NLP research and benchmark resources. In this paper, we present an enhanced Dense Passage Retrieval (DPR) framework developed specifically for Arabic. At the core of our approach is a novel Attentive Relevance Scoring (ARS) that replaces standard interaction mechanisms with an adaptive scoring function that more effectively models the semantic relevance between questions and passages. Our method integrates pre-trained Arabic language models and architectural refinements to improve retrieval performance and significantly increase ranking accuracy when answering Arabic questions. The code is made publicly available at https://github.com/Bekhouche/APR{GitHub}."
    },
    {
      "title": "NeRF 是 3D Gaussian Splatting 的宝贵助手 (原标题: NeRF Is a Valuable Assistant for 3D Gaussian Splatting)",
      "link": "https://arxiv.org/abs/2507.23374",
      "pubDate": "Thu, 31 Jul 2025 05:43:31 GMT",
      "isoDate": "2025-07-31T05:43:31.000Z",
      "creator": "Shuangkang Fang, I-Chao Shen, Takeo Igarashi, Yufeng Wang, ZeSheng Wang, Yi Yang, Wenrui Ding, Shuchang Zhou",
      "summary": "### NeRF-GS：NeRF 与 3D Gaussian Splatting 的联合优化框架\n\n*   **引言与背景**\n    *   3D Gaussian Splatting (3DGS) 是一种高效的 3D 场景表示方法，但在实际应用中存在一些局限性。\n    *   这些局限性包括：对高斯初始化敏感、空间感知能力有限以及高斯间关联性较弱。\n    *   这些问题影响了 3DGS 的性能表现。\n\n*   **NeRF-GS 框架介绍**\n    *   本文提出了一种名为 NeRF-GS 的新型框架，旨在解决 3DGS 的上述局限性。\n    *   NeRF-GS 的核心思想是联合优化神经辐射场 (NeRF) 和 3D Gaussian Splatting (3DGS)。\n    *   该框架利用 NeRF 固有的连续空间表示能力来增强 3DGS。\n\n*   **核心方法与技术**\n    *   **空间特征对齐**：NeRF-GS 重新审视了 3DGS 的设计，并逐步将其空间特征与 NeRF 对齐。\n    *   **共享 3D 空间信息**：通过共享 3D 空间信息，NeRF 和 3DGS 两种表示可以在同一场景中进行联合优化。\n    *   **残差向量优化**：为了进一步解决两种方法之间的形式差异，NeRF-GS 优化了隐式特征和高斯位置的残差向量。这有助于增强 3DGS 的个性化能力。\n\n*   **实验结果与贡献**\n    *   在基准数据集上的实验结果表明，NeRF-GS 优于现有方法，并取得了最先进的性能。\n    *   这一结果证实了 NeRF 和 3DGS 之间是互补而非竞争的关系。\n    *   NeRF-GS 为结合 3DGS 和 NeRF 的混合方法提供了新的见解，以实现高效的 3D 场景表示。\n\n*   **其他信息**\n    *   该研究已被 ICCV 接受。",
      "shortSummary": "NeRF-GS 是一种新颖的框架，通过联合优化 NeRF 和 3D Gaussian Splatting (3DGS) 来提升 3DGS 的性能。它利用 NeRF 的连续空间表示，解决了 3DGS 对初始化敏感、空间感知弱和高斯间关联性差等问题。NeRF-GS 通过共享 3D 空间信息和优化残差向量，使两者在同一场景中协同优化。实验证明，NeRF-GS 取得了最先进的性能，表明 NeRF 和 3DGS 具有互补性，为高效 3D 场景表示提供了混合方法的新思路。",
      "translated_title": "NeRF 是 3D Gaussian Splatting 的宝贵助手",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce NeRF-GS, a novel framework that jointly optimizes Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework leverages the inherent continuous spatial representation of NeRF to mitigate several limitations of 3DGS, including sensitivity to Gaussian initialization, limited spatial awareness, and weak inter-Gaussian correlations, thereby enhancing its performance. In NeRF-GS, we revisit the design of 3DGS and progressively align its spatial features with NeRF, enabling both representations to be optimized within the same scene through shared 3D spatial information. We further address the formal distinctions between the two approaches by optimizing residual vectors for both implicit features and Gaussian positions to enhance the personalized capabilities of 3DGS. Experimental results on benchmark datasets show that NeRF-GS surpasses existing methods and achieves state-of-the-art performance. This outcome confirms that NeRF and 3DGS are complementary rather than competing, offering new insights into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene representation."
    },
    {
      "title": "iLRM：一种迭代式大型3D重建模型 (原标题: iLRM: An Iterative Large 3D Reconstruction Model)",
      "link": "https://arxiv.org/abs/2507.23277",
      "pubDate": "Thu, 31 Jul 2025 02:33:07 GMT",
      "isoDate": "2025-07-31T02:33:07.000Z",
      "creator": "Gyeongjin Kang, Seungtae Nam, Xiangyu Sun, Sameh Khamis, Abdelrahman Mohamed, Eunbyung Park",
      "summary": "## iLRM：一种迭代式大型3D重建模型\n\n### 摘要\n\n前馈式3D建模，特别是直接生成3D高斯飞溅等显式3D表示的方法，因其快速高质量的渲染及广泛应用而备受关注。然而，许多基于Transformer架构的最新方法存在严重的扩展性问题。它们依赖于对多个输入视图的图像令牌进行全注意力操作，导致随着视图数量或图像分辨率的增加，计算成本呈指数级增长。\n\n### iLRM：解决方案\n\n为了实现可扩展且高效的前馈式3D重建，研究人员引入了一种迭代式大型3D重建模型（iLRM）。该模型通过迭代细化机制生成3D高斯表示，并遵循以下三个核心原则：\n\n1.  **解耦场景表示与输入视图图像**：这使得能够实现紧凑的3D表示，提高了效率。\n2.  **分解多视图交互**：将全注意力多视图交互分解为两阶段注意力机制，显著降低了计算成本。\n3.  **注入高分辨率信息**：在模型的每一层注入高分辨率信息，以实现高保真度的重建。\n\n### 实验结果与优势\n\n在RE10K和DL3DV等广泛使用的数据集上进行的实验结果表明，iLRM在重建质量和速度方面均优于现有方法。值得注意的是，iLRM展现出卓越的扩展性，通过有效利用更多的输入视图，在相似的计算成本下提供了显著更高的重建质量。",
      "shortSummary": "iLRM是一种新型的迭代式大型3D重建模型，旨在解决现有前馈式3D重建方法（如基于Transformer的模型）在处理多视图和高分辨率图像时面临的扩展性及高计算成本问题。iLRM通过迭代细化机制生成3D高斯表示，其核心在于解耦场景表示、采用两阶段注意力机制以及在每层注入高分辨率信息。实验证明，iLRM在重建质量、速度和扩展性方面均优于现有方法，能更高效地利用大量输入视图实现高质量重建。",
      "translated_title": "iLRM：一种迭代式大型3D重建模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Feed-forward 3D modeling has emerged as a promising approach for rapid and high-quality 3D reconstruction. In particular, directly generating explicit 3D representations, such as 3D Gaussian splatting, has attracted significant attention due to its fast and high-quality rendering, as well as numerous applications. However, many state-of-the-art methods, primarily based on transformer architectures, suffer from severe scalability issues because they rely on full attention across image tokens from multiple input views, resulting in prohibitive computational costs as the number of views or image resolution increases. Toward a scalable and efficient feed-forward 3D reconstruction, we introduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D Gaussian representations through an iterative refinement mechanism, guided by three core principles: (1) decoupling the scene representation from input-view images to enable compact 3D representations; (2) decomposing fully-attentional multi-view interactions into a two-stage attention scheme to reduce computational costs; and (3) injecting high-resolution information at every layer to achieve high-fidelity reconstruction. Experimental results on widely used datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms existing methods in both reconstruction quality and speed. Notably, iLRM exhibits superior scalability, delivering significantly higher reconstruction quality under comparable computational cost by efficiently leveraging a larger number of input views."
    },
    {
      "title": "通过影响力近似实现高效机器遗忘 (原标题: Efficient Machine Unlearning via Influence Approximation)",
      "link": "https://arxiv.org/abs/2507.23257",
      "pubDate": "Thu, 31 Jul 2025 01:34:27 GMT",
      "isoDate": "2025-07-31T01:34:27.000Z",
      "creator": "Jiawei Liu, Chenwang Wu, Defu Lian, Enhong Chen",
      "summary": "## 高效机器遗忘：通过影响力近似实现\n\n### 背景与挑战\n\n随着隐私问题的日益突出，机器遗忘（Machine Unlearning）——旨在使机器学习模型“遗忘”特定训练数据——受到了越来越多的关注。在现有方法中，基于影响力（Influence-based）的遗忘方法因其无需重新训练即可估计单个训练样本对模型参数影响的能力而脱颖而出。然而，这种方法存在巨大的计算开销，需要计算所有训练样本和参数的Hessian矩阵及其逆矩阵，这使得它对于大规模模型和频繁数据删除请求的场景来说不切实际，凸显了“遗忘”的难度。\n\n### 理论联系与创新方法\n\n*   **认知科学启发：** 本文受到认知科学中“记忆比遗忘更容易”这一观点的启发，在“记忆”（增量学习）和“遗忘”（机器遗忘）之间建立了理论联系。\n*   **视角转换：** 这种联系使得机器遗忘问题可以从增量学习（Incremental Learning）的角度来解决。与遗忘（unlearning）中耗时的Hessian计算不同，增量学习（memorizing）通常依赖于更高效的梯度优化，这支持了上述认知理论。\n\n### 影响力近似遗忘（IAU）算法\n\n基于上述理论联系，本文提出了**影响力近似遗忘（Influence Approximation Unlearning, IAU）算法**，旨在从增量学习的角度实现高效的机器遗忘。\n\n### 实验结果与优势\n\n广泛的实证评估表明，IAU算法在以下几个方面取得了卓越的平衡：\n\n*   **移除保证（Removal Guarantee）：** 确保数据被有效遗忘。\n*   **遗忘效率（Unlearning Efficiency）：** 提高遗忘过程的速度。\n*   **模型效用（Model Utility）：** 保持模型在遗忘后的性能。\n\nIAU算法在多样化的数据集和模型架构上均优于现有最先进的方法。\n\n### 代码可用性\n\n本文的实现代码已公开。",
      "shortSummary": "针对现有机器遗忘方法（特别是基于影响力的方法）计算成本高昂的问题，本文提出了一种高效的解决方案。受认知科学中“记忆比遗忘更容易”的启发，研究建立了机器遗忘与增量学习之间的理论联系。基于此，论文引入了影响力近似遗忘（IAU）算法，该算法利用增量学习中更高效的梯度优化。实验证明，IAU在遗忘保证、效率和模型效用之间取得了优越平衡，并超越了现有先进方法。",
      "translated_title": "通过影响力近似实现高效机器遗忘",
      "images": [],
      "contentSource": "完整文章",
      "content": "Due to growing privacy concerns, machine unlearning, which aims at enabling machine learning models to ``forget\" specific training data, has received increasing attention. Among existing methods, influence-based unlearning has emerged as a prominent approach due to its ability to estimate the impact of individual training samples on model parameters without retraining. However, this approach suffers from prohibitive computational overhead arising from the necessity to compute the Hessian matrix and its inverse across all training samples and parameters, rendering it impractical for large-scale models and scenarios involving frequent data deletion requests. This highlights the difficulty of forgetting. Inspired by cognitive science, which suggests that memorizing is easier than forgetting, this paper establishes a theoretical link between memorizing (incremental learning) and forgetting (unlearning). This connection allows machine unlearning to be addressed from the perspective of incremental learning. Unlike the time-consuming Hessian computations in unlearning (forgetting), incremental learning (memorizing) typically relies on more efficient gradient optimization, which supports the aforementioned cognitive theory. Based on this connection, we introduce the Influence Approximation Unlearning (IAU) algorithm for efficient machine unlearning from the incremental perspective. Extensive empirical evaluations demonstrate that IAU achieves a superior balance among removal guarantee, unlearning efficiency, and comparable model utility, while outperforming state-of-the-art methods across diverse datasets and model architectures. Our code is available at https://github.com/Lolo1222/IAU."
    }
  ],
  "lastUpdated": "2025-08-05T09:44:09.172Z"
}