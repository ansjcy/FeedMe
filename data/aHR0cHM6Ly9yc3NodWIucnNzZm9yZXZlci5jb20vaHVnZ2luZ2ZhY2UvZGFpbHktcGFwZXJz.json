{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "CodePlot-CoT：通过代码驱动图像进行数学视觉推理 (原标题: CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images)",
      "link": "https://arxiv.org/abs/2510.11718",
      "pubDate": "Mon, 13 Oct 2025 13:59:55 GMT",
      "isoDate": "2025-10-13T13:59:55.000Z",
      "creator": "Chengqi Duan, Kaiyue Sun, Rongyao Fang, Manyuan Zhang, Yan Feng, Ying Luo, Yufang Liu, Ke Wang, Peng Pei, Xunliang Cai, Hongsheng Li, Yi Ma, Xihui Liu",
      "summary": "## CodePlot-CoT：通过代码驱动图像进行数学视觉推理\n\n### 引言\n\n大型语言模型（LLMs）和视觉语言模型（VLMs）在数学推理方面取得了显著进展。然而，它们在处理需要视觉辅助（例如，绘制辅助线或函数图）的数学问题时，仍面临关键瓶颈。大多数LLMs和VLMs局限于纯文本推理链，而能生成交错文本和图像的多模态统一模型，在这些任务中又缺乏必要的精度和可控性。\n\n### CodePlot-CoT 方法\n\n为解决这一挑战，研究人员提出了 **CodePlot-CoT**，这是一种代码驱动的思维链（Chain-of-Thought）范式，旨在实现数学领域的“通过图像思考”。该方法的核心机制如下：\n\n*   **VLM生成**：利用视觉语言模型（VLM）生成文本推理过程。\n*   **代码生成**：同时，VLM还生成可执行的绘图代码。\n*   **视觉思维**：这些绘图代码随后被渲染成图像，作为“视觉思维”（visual thought），以辅助解决复杂的数学问题。\n\n### 核心贡献与实现\n\n为了实现CodePlot-CoT并验证其有效性，研究团队完成了以下关键工作：\n\n1.  **构建Math-VR数据集**：\n    *   首次构建了大规模、双语的数学视觉推理（Mathematics problems with Visual Reasoning, Math-VR）数据集和基准。\n    *   该数据集包含17.8万个样本，为需要视觉辅助的数学问题提供了丰富的资源。\n2.  **开发图像到代码转换器**：\n    *   开发了一个最先进的图像到代码转换器，专门用于将复杂的数学图形解析成可执行代码。\n    *   这对于生成高质量的训练数据至关重要。\n3.  **训练CodePlot-CoT模型**：\n    *   利用上述构建的Math-VR数据集和通过图像到代码转换器生成的高质量训练数据，训练了CodePlot-CoT模型来解决数学问题。\n\n### 实验结果与影响\n\n*   **显著性能提升**：实验结果表明，CodePlot-CoT模型在新基准测试上比基础模型性能提升高达21%。这充分验证了所提出的代码驱动推理范式的有效性。\n*   **开辟新方向**：这项工作为多模态数学推理开辟了新方向，展示了通过结合代码生成和视觉渲染来增强模型推理能力的可行性。\n*   **资源公开**：为促进未来的研究，研究团队已将数据集、代码和预训练模型公开，可在 [this https URL](this%20https%20URL) 获取。\n\nCodePlot-CoT为解决需要视觉辅助的数学问题提供了一个强大的新方法，并为社区提供了首个大规模数据集、全面的基准和有效的解决方案。",
      "shortSummary": "CodePlot-CoT提出了一种代码驱动的思维链范式，通过生成可执行绘图代码并将其渲染为“视觉思维”图像，解决了LLMs和VLMs在需要视觉辅助的数学推理中的瓶颈。研究团队构建了首个大规模双语数学视觉推理数据集Math-VR（17.8万样本），并开发了图像到代码转换器。实验证明，CodePlot-CoT模型在新基准上比基础模型性能提升高达21%，验证了其有效性，并为多模态数学推理开辟了新方向。",
      "translated_title": "CodePlot-CoT：通过代码驱动图像进行数学视觉推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in Large Language Models (LLMs) and Vision Language Models (VLMs) have shown significant progress in mathematical reasoning, yet they still face a critical bottleneck with problems requiring visual assistance, such as drawing auxiliary lines or plotting functions to solve the problems. Most LLMs and VLMs are constrained to text-only reasoning chains, while multimodal unified models that can generate interleaved text and images lack the necessary precision and controllability for such tasks. To address this, we propose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking with images\" in mathematics. Our approach leverages the VLM to generate text reasoning as well as executable plotting code, which is then rendered into images as \"visual thought\", to solve mathematical problems. To achieve this, we first construct Math-VR, the first large-scale, bilingual dataset and benchmark for Mathematics problems with Visual Reasoning, comprising 178K samples. Second, to create high-quality training data, we develop a state-of-the-art image-to-code converter specialized for parsing complex mathematical figures into codes. Finally, using these training data, we train the CodePlot-CoT model for solving mathematical problems. Experimental results show that our model achieves up to 21% increase over base model on our new benchmark, fully validating the efficacy of our proposed code-driven reasoning paradigm. Our work opens a new direction for multimodal mathematical reasoning and provides the community with the first large-scale dataset, comprehensive benchmark, and strong approach for such problems. To facilitate future research, we make our datasets, code, and pretrained models publicly available at https://github.com/HKU-MMLab/Math-VR-CodePlot-CoT."
    },
    {
      "title": "大型推理模型是否可中断？ (原标题: Are Large Reasoning Models Interruptible?)",
      "link": "https://arxiv.org/abs/2510.11713",
      "pubDate": "Mon, 13 Oct 2025 13:59:35 GMT",
      "isoDate": "2025-10-13T13:59:35.000Z",
      "creator": "Tsung-Han Wu, Mihran Miroyan, David M. Chan, Trevor Darrell, Narges Norouzi, Joseph E. Gonzalez",
      "summary": "# 大型推理模型在动态环境下的鲁棒性评估\n\n## 引言\n大型推理模型（LRMs）在处理复杂推理任务方面表现卓越。然而，它们传统上是在“静态、冻结世界”的假设下进行评估的，即模型响应被认为是瞬时的，并且请求的上下文在响应期间保持不变。虽然这种假设对于短期任务通常成立，但在现代推理任务（如辅助编程）中，它便不再适用。在这些任务中，模型可能需要数小时进行思考，并且从模型开始思考到最终输出，代码或上下文可能会发生显著变化。\n\n## 研究目的与方法\n本研究旨在挑战“冻结世界”的假设，并在两种现实的动态场景下评估LRM的鲁棒性：\n*   **中断：** 评估模型在有限预算下其部分输出的质量。\n*   **动态上下文：** 评估模型对进行中上下文变化的适应能力。\n\n研究使用了需要长篇推理的数学和编程基准来测试模型。\n\n## 主要发现\n*   **鲁棒性被高估：** 静态评估始终高估了LRM的鲁棒性。即使是最先进的LRMs，在静态设置中能达到高准确率，但在被中断或暴露于变化的上下文时，其表现会变得不可预测。\n*   **性能显著下降：** 当更新在推理过程后期引入时，模型的性能下降幅度可高达60%。\n\n## 新颖的故障模式\n本研究进一步揭示了几种新颖的故障模式：\n*   **推理泄露 (Reasoning leakage)：** 当模型被中断时，它会将部分推理过程融入到最终答案中。\n*   **恐慌 (Panic)：** 在时间压力下，模型会完全放弃推理，并返回不正确的答案。\n*   **自我怀疑 (Self-doubt)：** 在整合更新信息时，模型的性能反而会下降。\n\n## 结论\n这项研究表明，大型推理模型在动态、非静态环境中的表现远不如在传统静态评估中那样稳定。它强调了在更贴近实际应用场景的动态条件下评估和改进LRM的必要性，以确保它们在现实世界任务中的可靠性和有效性。",
      "shortSummary": "大型推理模型（LRMs）传统上在静态环境中评估，但本研究挑战了这一“冻结世界”假设。通过在中断和动态上下文的现实场景中评估LRMs，发现静态评估严重高估了模型的鲁棒性。即使是先进模型，在动态变化下性能也会下降高达60%，尤其当更新在推理后期引入时。研究还揭示了推理泄露、恐慌和自我怀疑等新故障模式，强调了在动态环境中重新评估和改进LRMs的重要性。",
      "translated_title": "大型推理模型是否可中断？",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Reasoning Models (LRMs) excel at complex reasoning but are traditionally evaluated in static, \"frozen world\" settings: model responses are assumed to be instantaneous, and the context of a request is presumed to be immutable over the duration of the response. While generally true for short-term tasks, the \"frozen world\" assumption breaks down in modern reasoning tasks such as assistive programming, where models may take hours to think through problems and code may change dramatically from the time the model starts thinking to the model's final output. In this work, we challenge the frozen world assumption and evaluate LRM robustness under two realistic dynamic scenarios: interruptions, which test the quality of the model's partial outputs on a limited budget, and dynamic context, which tests model adaptation to in-flight changes. Across mathematics and programming benchmarks that require long-form reasoning, static evaluations consistently overestimate robustness: even state-of-the-art LRMs, which achieve high accuracy in static settings, can fail unpredictably when interrupted or exposed to changing context, with performance dropping by up to 60% when updates are introduced late in the reasoning process. Our analysis further reveals several novel failure modes, including reasoning leakage, where models fold the reasoning into their final answer when interrupted; panic, where under time pressure models abandon reasoning entirely and return incorrect answers; and self-doubt, where performance degrades while incorporating updated information."
    },
    {
      "title": "DiT360：通过混合训练实现高保真全景图像生成 (原标题: DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training)",
      "link": "https://arxiv.org/abs/2510.11712",
      "pubDate": "Mon, 13 Oct 2025 13:59:15 GMT",
      "isoDate": "2025-10-13T13:59:15.000Z",
      "creator": "Haoran Feng, Dizhe Zhang, Xiangtai Li, Bo Du, Lu Qi",
      "summary": "# DiT360：通过混合训练实现高保真全景图像生成\n\n## 1. 概述\n*   DiT360是一个基于DiT的框架，旨在通过对透视和全景数据进行混合训练来生成高保真全景图像。\n*   该研究将生成质量中几何保真度和照片真实感不足的主要原因归结为缺乏大规模、高质量的真实世界全景数据，这是一种以数据为中心的方法，与以往专注于模型设计的方法不同。\n\n## 2. DiT360的关键模块与方法\nDiT360包含多个关键模块，用于域间转换（inter-domain transformation）和域内增强（intra-domain augmentation），这些模块在图像（pre-VAE）和token（post-VAE）两个层面应用。\n\n### 2.1 图像层面（Pre-VAE Image Level）\n*   **跨域知识整合**：通过透视图像引导（perspective image guidance）和全景精炼（panoramic refinement）来融入跨域知识。\n*   **效果**：这些方法增强了感知质量，同时规范了多样性和照片真实感。\n\n### 2.2 Token层面（Post-VAE Token Level）\n*   **混合监督**：在多个模块中应用混合监督。\n*   **具体模块**：\n    *   **循环填充（Circular Padding）**：用于确保边界连续性。\n    *   **偏航损失（Yaw Loss）**：用于提高旋转鲁棒性。\n    *   **立方体损失（Cube Loss）**：用于增强对失真的感知。\n\n## 3. 实验结果\n*   DiT360在文本到全景图生成（text-to-panorama）、图像修复（inpainting）和图像外绘（outpainting）任务上进行了广泛实验。\n*   实验结果表明，该方法在边界一致性和图像保真度方面表现更优。\n*   性能通过十一个定量指标进行评估。\n\n## 4. 代码可用性\n*   该方法的代码已公开。",
      "shortSummary": "DiT360是一个基于DiT的框架，通过对透视和全景数据进行混合训练，旨在生成高保真全景图像。它通过解决大规模高质量全景数据缺乏的问题，提升了几何保真度和照片真实感。DiT360在图像和token层面引入了透视引导、全景精炼、循环填充、偏航损失和立方体损失等关键模块。实验表明，该方法在文本到全景、修复和外绘任务中，在边界一致性和图像保真度方面均优于现有方法。代码已公开。",
      "translated_title": "DiT360：通过混合训练实现高保真全景图像生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this work, we propose DiT360, a DiT-based framework that performs hybrid training on perspective and panoramic data for panoramic image generation. For the issues of maintaining geometric fidelity and photorealism in generation quality, we attribute the main reason to the lack of large-scale, high-quality, real-world panoramic data, where such a data-centric view differs from prior methods that focus on model design. Basically, DiT360 has several key modules for inter-domain transformation and intra-domain augmentation, applied at both the pre-VAE image level and the post-VAE token level. At the image level, we incorporate cross-domain knowledge through perspective image guidance and panoramic refinement, which enhance perceptual quality while regularizing diversity and photorealism. At the token level, hybrid supervision is applied across multiple modules, which include circular padding for boundary continuity, yaw loss for rotational robustness, and cube loss for distortion awareness. Extensive experiments on text-to-panorama, inpainting, and outpainting tasks demonstrate that our method achieves better boundary consistency and image fidelity across eleven quantitative metrics. Our code is available at https://github.com/Insta360-Research-Team/DiT360."
    },
    {
      "title": "揭秘智能体推理中的强化学习 (原标题: Demystifying Reinforcement Learning in Agentic Reasoning)",
      "link": "https://arxiv.org/abs/2510.11701",
      "pubDate": "Mon, 13 Oct 2025 13:57:15 GMT",
      "isoDate": "2025-10-13T13:57:15.000Z",
      "creator": "Zhaochen Yu, Ling Yang, Jiaru Zou, Shuicheng Yan, Mengdi Wang",
      "summary": "### 揭秘智能体推理中的强化学习\n\n本文对大型语言模型（LLMs）在智能体推理中应用强化学习（RL）的关键设计原则和优化实践进行了全面而系统的研究。尽管智能体RL已显示出有效提升LLMs推理能力，但其核心机制仍不明确。本研究从数据、算法和推理模式三个关键视角深入探讨，旨在揭示智能体推理中强化学习的奥秘。\n\n#### 核心洞察与发现\n\n研究总结了以下关键洞察，这些洞察共同提升了智能体推理能力和训练效率：\n\n1.  **数据视角：**\n    *   **高质量SFT初始化：** 用真实的端到端工具使用轨迹替换拼接合成轨迹，能够产生更强大的监督微调（SFT）初始化。\n    *   **多样性和模型感知数据集：** 具有高多样性且模型感知的数据集有助于维持探索，并显著提高RL性能。\n\n2.  **算法视角：**\n    *   **探索友好型技术至关重要：** 对于智能体RL而言，采用有利于探索的技术是成功的关键。\n    *   **具体实践：** 例如，“clip higher”奖励裁剪、超长奖励塑形（overlong reward shaping）以及保持足够的策略熵（policy entropy），这些方法都能有效提高训练效率。\n\n3.  **推理模式视角：**\n    *   **审慎策略的优势：** 采用更少工具调用的审慎策略，其表现优于频繁工具调用或冗长的自我推理模式。\n    *   **效率与准确性提升：** 这种策略不仅提高了工具使用效率，也提升了最终的准确性。\n\n#### 实践效果与贡献\n\n*   **性能显著提升：** 这些简单而有效的实践方法持续增强了智能体推理能力和训练效率，使得较小的模型也能在具有挑战性的基准测试中取得优异成绩。\n*   **建立实用基线：** 本研究为未来的智能体RL研究建立了一个实用的基线。\n*   **高质量数据集贡献：** 除了经验性洞察，本文还贡献了一个高质量的真实端到端智能体SFT数据集，以及一个高质量的RL数据集。\n*   **跨基准测试的有效性：** 研究证明了这些洞察在提升LLMs智能体推理能力方面的有效性，并在四个具有挑战性的基准测试中得到了验证，包括AIME2024/AIME2025、GPQA-Diamond和LiveCodeBench-v6。\n*   **小模型超越大模型：** 值得注意的是，通过采用这些方法，4B大小的模型也能够实现优于32B大小模型的智能体推理性能。\n\n#### 结论\n\n本文通过对数据、算法和推理模式的深入分析，揭示了智能体推理中强化学习的关键设计原则。这些实践不仅提升了模型的性能和训练效率，还为智能体RL领域提供了宝贵的资源和研究方向。",
      "shortSummary": "本文系统性地研究了智能体推理中的强化学习，旨在揭示其关键设计原则。研究从数据、算法和推理模式三个角度提出了核心洞察：使用真实的端到端工具使用轨迹进行SFT初始化；采用有利于探索的算法技术；以及选择更少工具调用的审慎推理策略。这些实践显著提升了智能体推理能力和训练效率，使4B模型在挑战性基准测试中超越32B模型，并为未来研究提供了实用基线和高质量数据集。",
      "translated_title": "揭秘智能体推理中的强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recently, the emergence of agentic RL has showcased that RL could also effectively improve the agentic reasoning ability of LLMs, yet the key design principles and optimal practices remain unclear. In this work, we conduct a comprehensive and systematic investigation to demystify reinforcement learning in agentic reasoning from three key perspectives: data, algorithm, and reasoning mode. We highlight our key insights: (i) Replacing stitched synthetic trajectories with real end-to-end tool-use trajectories yields a far stronger SFT initialization; high-diversity, model-aware datasets sustain exploration and markedly improve RL performance. (ii) Exploration-friendly techniques are crucial for agentic RL, such as clip higher, overlong reward shaping, and maintaining adequate policy entropy could improve the training efficiency. (iii) A deliberative strategy with fewer tool calls outperforms frequent tool calls or verbose self-reasoning, improving tool efficiency and final accuracy. Together, these simple practices consistently enhance agentic reasoning and training efficiency, achieving strong results on challenging benchmarks with smaller models, and establishing a practical baseline for future agentic RL research. Beyond these empirical insights, we further contribute a high-quality, real end-to-end agentic SFT dataset along with a high-quality RL dataset, and demonstrate the effectiveness of our insights in boosting the agentic reasoning ability of LLMs across four challenging benchmarks, including AIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes, 4B-sized models could also achieve superior agentic reasoning performance compared to 32B-sized models. Code and models: https://github.com/Gen-Verse/Open-AgentRL"
    },
    {
      "title": "QeRL：超越效率——面向大型语言模型的量化增强强化学习 (原标题: QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs)",
      "link": "https://arxiv.org/abs/2510.11696",
      "pubDate": "Mon, 13 Oct 2025 13:55:09 GMT",
      "isoDate": "2025-10-13T13:55:09.000Z",
      "creator": "Wei Huang, Yi Ge, Shuai Yang, Yicheng Xiao, Huizi Mao, Yujun Lin, Hanrong Ye, Sifei Liu, Ka Chun Cheung, Hongxu Yin, Yao Lu, Xiaojuan Qi, Song Han, Yukang Chen",
      "summary": "## QeRL：面向大型语言模型的量化增强强化学习\n\n### 概述\n\nQeRL（Quantization-enhanced Reinforcement Learning）是一个为大型语言模型（LLMs）设计的强化学习（RL）框架。该框架旨在解决LLM强化学习过程中资源消耗巨大（如GPU内存和rollout时长）的问题，并在此基础上进一步提升探索能力。\n\n### 核心机制\n\nQeRL通过以下两种关键技术结合，实现效率和性能的提升：\n\n*   **NVFP4 量化与低秩适应（LoRA）的结合**：\n    *   QeRL将NVFP4量化技术与LoRA相结合，显著加速了RL的rollout阶段，并大幅降低了内存开销。\n\n*   **量化噪声增强探索**：\n    *   研究发现，量化噪声不仅有助于效率提升，还能增加策略熵，从而增强模型的探索能力，使其在RL训练过程中发现更好的策略。\n\n*   **自适应量化噪声（AQN）机制**：\n    *   为了进一步优化探索过程，QeRL引入了自适应量化噪声（AQN）机制。该机制能够根据训练动态调整噪声水平，从而更有效地引导模型进行探索。\n\n### 实验结果与性能\n\n实验证明，QeRL在多个方面展现出卓越的性能：\n\n*   **效率提升**：\n    *   在rollout阶段，QeRL实现了超过1.5倍的速度提升。\n    *   它是首个能够在单个H100 80GB GPU上实现32B LLM的RL训练的框架，并为RL训练带来了整体加速。\n\n*   **性能超越**：\n    *   QeRL比传统的16位LoRA和QLoRA方法实现了更快的奖励增长和更高的最终准确性。\n    *   在7B模型上，QeRL在数学基准测试（如GSM8K达到90.8%，MATH 500达到77.4%）中，其性能与全参数微调相当。\n\n### 结论\n\nQeRL被确立为一种高效且有效的LLM强化学习训练框架，它不仅解决了资源密集型问题，还通过量化噪声的巧妙利用提升了模型的探索能力和最终性能。",
      "shortSummary": "QeRL是一个为大型语言模型（LLMs）设计的量化增强强化学习框架。它通过结合NVFP4量化和LoRA，显著加速了RL的rollout阶段并减少内存消耗。该框架发现量化噪声能增强策略探索，并通过自适应量化噪声（AQN）机制进一步优化。实验表明，QeRL在rollout阶段提速1.5倍以上，首次在单H100 GPU上实现32B LLM的RL训练，并超越现有方法，在数学基准上达到与全参数微调相当的性能，证明了其高效性和有效性。",
      "translated_title": "QeRL：超越效率——面向大型语言模型的量化增强强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for large language models (LLMs). While RL is essential for LLMs' reasoning capabilities, it is resource-intensive, requiring substantial GPU memory and long rollout durations. QeRL addresses these issues by combining NVFP4 quantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL while reducing memory overhead. Beyond efficiency, our findings show that quantization noise increases policy entropy, enhancing exploration, and enabling the discovery of better strategies during RL. To further optimize exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism, which dynamically adjusts noise during training. Experiments demonstrate that QeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is the first framework to enable RL training of a 32B LLM on a single H100 80GB GPU, while delivering overall speedups for RL training. It also achieves faster reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while matching the performance of full-parameter fine-tuning on mathematical benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These results establish QeRL as an efficient and effective framework for RL training in LLMs."
    },
    {
      "title": "带有表示自编码器的扩散Transformer (原标题: Diffusion Transformers with Representation Autoencoders)",
      "link": "https://arxiv.org/abs/2510.11690",
      "pubDate": "Mon, 13 Oct 2025 13:51:39 GMT",
      "isoDate": "2025-10-13T13:51:39.000Z",
      "creator": "Boyang Zheng, Nanye Ma, Shengbang Tong, Saining Xie",
      "summary": "### 带有表示自编码器的扩散Transformer\n\n本文探讨了扩散Transformer (DiT) 中潜在生成建模的关键组件——自编码器——的改进。\n\n**当前问题与局限性：**\n*   目前，大多数DiT模型仍依赖于原始的变分自编码器 (VAE) 编码器。\n*   这导致了以下限制：\n    *   **过时的骨干网络**：影响架构的简洁性。\n    *   **低维潜在空间**：限制了信息容量。\n    *   **弱表示**：纯粹基于重建的训练导致表示能力不足，最终限制了生成质量。\n\n**提出的解决方案：表示自编码器 (RAEs)**\n*   作者提出用预训练的表示编码器（例如DINO、SigLIP、MAE）与训练过的解码器相结合，形成“表示自编码器”(RAEs)，来替代传统的VAE。\n*   **RAE的优势**：\n    *   提供高质量的重建。\n    *   生成语义丰富的潜在空间。\n    *   允许可扩展的基于Transformer的架构。\n\n**关键挑战与解决方案：**\n*   **挑战**：RAE生成的潜在空间通常是高维的，这使得扩散Transformer难以在其内部有效操作。\n*   **解决方案**：\n    *   分析了造成这一困难的根源。\n    *   提出了理论上合理且经过实证验证的解决方案。\n    *   实现了更快的收敛，且无需辅助表示对齐损失。\n\n**实验结果与性能：**\n*   使用配备了轻量级、宽DDT头的DiT变体，在ImageNet上取得了强大的图像生成结果：\n    *   **无引导**：在256x256分辨率下，FID达到1.51。\n    *   **有引导**：在256x256和512x512分辨率下，FID均达到1.13。\n\n**结论与建议：**\n*   RAE提供了明显的优势，应成为扩散Transformer训练的新默认选择。",
      "shortSummary": "本文提出用表示自编码器（RAEs）替代扩散Transformer（DiT）中过时的VAE，以解决其架构、潜在空间和表示能力不足的问题。RAEs利用预训练的表示编码器，提供高质量重建和语义丰富的潜在空间。通过解决高维潜在空间操作的挑战，该方法实现了更快的收敛，并在ImageNet上取得了显著的图像生成性能，例如256x256分辨率下无引导FID为1.51，有引导FID为1.13。RAE被认为是DiT训练的新默认选择。",
      "translated_title": "带有表示自编码器的扩散Transformer",
      "images": [],
      "contentSource": "完整文章",
      "content": "Latent generative modeling, where a pretrained autoencoder maps pixels into a latent space for the diffusion process, has become the standard strategy for Diffusion Transformers (DiT); however, the autoencoder component has barely evolved. Most DiTs continue to rely on the original VAE encoder, which introduces several limitations: outdated backbones that compromise architectural simplicity, low-dimensional latent spaces that restrict information capacity, and weak representations that result from purely reconstruction-based training and ultimately limit generative quality. In this work, we explore replacing the VAE with pretrained representation encoders (e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term Representation Autoencoders (RAEs). These models provide both high-quality reconstructions and semantically rich latent spaces, while allowing for a scalable transformer-based architecture. Since these latent spaces are typically high-dimensional, a key challenge is enabling diffusion transformers to operate effectively within them. We analyze the sources of this difficulty, propose theoretically motivated solutions, and validate them empirically. Our approach achieves faster convergence without auxiliary representation alignment losses. Using a DiT variant equipped with a lightweight, wide DDT head, we achieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no guidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers clear advantages and should be the new default for diffusion transformer training."
    },
    {
      "title": "ACADREASON：用学术研究问题探索推理模型的极限 (原标题: ACADREASON: Exploring the Limits of Reasoning Models with Academic Research Problems)",
      "link": "https://arxiv.org/abs/2510.11652",
      "pubDate": "Mon, 13 Oct 2025 13:30:36 GMT",
      "isoDate": "2025-10-13T13:30:36.000Z",
      "creator": "Xin Gui, King Zhu, JinCheng Ren, Qianben Chen, Zekun Moore Wang, Yizhi LI, Xinpeng Liu, Xiaowan Li, Wenli Ren, Linyu Miao, Tianrui Qin, Ziqi Shu, He Zhu, Xiangru Tang, Dingfeng Shi, Jiaheng Liu, Yuchen Eleanor Jiang, Minghao Liu, Ge Zhang, Wangchunshu Zhou",
      "summary": "### ACADREASON：用学术研究问题探索推理模型的极限\n\n**背景与问题**\n近年来，大型语言模型（LLMs）和智能体（agents）的研究重点已从展示新颖能力转向复杂的推理和应对挑战性任务。然而，现有评估主要集中在数学/代码竞赛或通用任务上，而现有的多领域学术基准则缺乏足够的推理深度，导致该领域缺乏一个用于高级推理的严格基准。\n\n**Acadreason 基准的引入**\n为了填补这一空白，研究人员引入了 Acadreason 基准。该基准旨在评估 LLMs 和智能体获取和推理学术知识的能力。\n\n**Acadreason 的构成**\n*   **问题数量与来源**：Acadreason 包含 50 个由专家标注的学术问题。所有问题均来源于近年来顶级出版物。\n*   **领域覆盖**：这些问题涵盖了五个高推理领域，包括：\n    *   计算机科学\n    *   经济学\n    *   法律\n    *   数学\n    *   哲学\n*   **质量控制**：所有问题都经过严格的标注和质量控制，以确保它们既具有挑战性又可回答。\n\n**系统评估与结果**\n研究人员对超过 10 个主流 LLMs 和智能体进行了系统评估。\n*   **LLMs 的表现**：\n    *   大多数 LLMs 的得分低于 20 分。\n    *   即使是尖端的 GPT-5 也仅获得 16 分。\n*   **智能体的表现**：\n    *   智能体取得了更高的分数。\n    *   但没有一个智能体的得分超过 40 分。\n\n**结论**\n评估结果表明，当前 LLMs 和智能体在超智能学术研究任务中存在能力差距，并凸显了 Acadreason 基准的挑战性。",
      "shortSummary": "Acadreason 基准被引入，旨在评估大型语言模型（LLMs）和智能体在学术研究问题上的高级推理能力。该基准包含来自计算机科学、经济学、法律、数学和哲学等五个高推理领域的50个专家标注问题。对主流模型进行评估后发现，大多数LLMs得分低于20分（GPT-5为16分），智能体也未超过40分。这揭示了当前模型在处理复杂学术推理任务时存在的显著能力差距。",
      "translated_title": "ACADREASON：用学术研究问题探索推理模型的极限",
      "images": [],
      "contentSource": "完整文章",
      "content": "In recent years, the research focus of large language models (LLMs) and agents has shifted increasingly from demonstrating novel capabilities to complex reasoning and tackling challenging tasks. However, existing evaluations focus mainly on math/code contests or general tasks, while existing multi-domain academic benchmarks lack sufficient reasoning depth, leaving the field without a rigorous benchmark for high-level reasoning. To fill this gap, we introduce the Acadreason benchmark, designed to evaluate the ability of LLMs and agents to acquire and reason over academic knowledge. It consists of 50 expert-annotated academic problems across five high-reasoning domains, including computer science, economics, law, mathematics, and philosophy. All questions are sourced from top-tier publications in recent years and undergo rigorous annotation and quality control to ensure they are both challenging and answerable. We conduct systematic evaluations of over 10 mainstream LLMs and agents. The results show that most LLMs scored below 20 points, with even the cutting-edge GPT-5 achieving only 16 points. While agents achieved higher scores, none exceeded 40 points. This demonstrates the current capability gap between LLMs and agents in super-intelligent academic research tasks and highlights the challenges of Acadreason."
    },
    {
      "title": "InfiniHuman：无限3D人体精确控制生成 (原标题: InfiniHuman: Infinite 3D Human Creation with Precise Control)",
      "link": "https://arxiv.org/abs/2510.11650",
      "pubDate": "Mon, 13 Oct 2025 13:29:55 GMT",
      "isoDate": "2025-10-13T13:29:55.000Z",
      "creator": "Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, Gerard Pons-Moll",
      "summary": "# InfiniHuman：无限3D人体精确控制生成\n\n本文介绍了 **InfiniHuman**，一个旨在解决生成逼真且可控的3D人体虚拟形象这一长期挑战的框架。当前，生成涵盖广泛属性（如种族、年龄、服装风格和详细身体形状）的3D人体面临巨大困难，主要原因是捕获和标注大规模人体数据集的成本过高，且规模和多样性受限。本文的核心问题是：能否利用现有基础模型，生成理论上无限且标注丰富的三维人体数据？\n\n## InfiniHuman 框架概述\nInfiniHuman 框架通过协同蒸馏现有基础模型，以极低的成本和理论上无限的可扩展性生成标注丰富的人体数据。该框架包含两个主要组成部分：InfiniHumanData 和 InfiniHumanGen。\n\n## InfiniHumanData：大规模多模态数据集\nInfiniHumanData 是一个全自动的数据生成流水线，它利用视觉-语言模型和图像生成模型来创建大规模多模态数据集。\n*   **规模与多样性：** 该数据集包含111K个身份，展现了前所未有的多样性。\n*   **丰富标注：** 每个身份都附带多粒度文本描述、多视角RGB图像、详细的服装图像以及SMPL身体形状参数。\n*   **质量验证：** 用户研究表明，InfiniHuman 自动生成的身份与扫描渲染结果几乎无法区分。\n\n## InfiniHumanGen：扩散模型生成流水线\nInfiniHumanGen 是一个基于扩散的生成流水线，它以文本、身体形状和服装资产为条件。\n*   **能力：** 该流水线能够实现快速、逼真且精确可控的虚拟形象生成。\n*   **控制粒度：** 提供了对生成过程的细粒度控制。\n\n## 实验成果与优势\n广泛的实验证明，InfiniHuman 在视觉质量、生成速度和可控性方面显著优于现有最先进的方法。\n*   **核心优势：** 通过一个实用且经济的解决方案，实现了高质量、细粒度控制的虚拟形象生成，且规模可有效扩展至无限。\n\n## 公开资源与展望\n该研究已被 ACM SIGGRAPH Asia 2025 接受。作者计划公开：\n*   自动数据生成流水线。\n*   全面的 InfiniHumanData 数据集。\n*   InfiniHumanGen 模型。",
      "shortSummary": "InfiniHuman 框架通过蒸馏基础模型，解决了3D人体虚拟形象生成中数据稀缺和多样性不足的挑战。它包括 InfiniHumanData（自动生成111K个多样化、标注丰富的3D人体数据集）和 InfiniHumanGen（基于扩散的生成模型，支持文本、身体形状和服装的精确控制）。该方法显著提升了生成质量、速度和可控性，以低成本实现了无限规模的高质量3D人体生成。",
      "translated_title": "InfiniHuman：无限3D人体精确控制生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Generating realistic and controllable 3D human avatars is a long-standing challenge, particularly when covering broad attribute ranges such as ethnicity, age, clothing styles, and detailed body shapes. Capturing and annotating large-scale human datasets for training generative models is prohibitively expensive and limited in scale and diversity. The central question we address in this paper is: Can existing foundation models be distilled to generate theoretically unbounded, richly annotated 3D human data? We introduce InfiniHuman, a framework that synergistically distills these models to produce richly annotated human data at minimal cost and with theoretically unlimited scalability. We propose InfiniHumanData, a fully automatic pipeline that leverages vision-language and image generation models to create a large-scale multi-modal dataset. User study shows our automatically generated identities are undistinguishable from scan renderings. InfiniHumanData contains 111K identities spanning unprecedented diversity. Each identity is annotated with multi-granularity text descriptions, multi-view RGB images, detailed clothing images, and SMPL body-shape parameters. Building on this dataset, we propose InfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body shape, and clothing assets. InfiniHumanGen enables fast, realistic, and precisely controllable avatar generation. Extensive experiments demonstrate significant improvements over state-of-the-art methods in visual quality, generation speed, and controllability. Our approach enables high-quality avatar generation with fine-grained control at effectively unbounded scale through a practical and affordable solution. We will publicly release the automatic data generation pipeline, the comprehensive InfiniHumanData dataset, and the InfiniHumanGen models at https://yuxuan-xue.com/infini-human."
    },
    {
      "title": "IVEBench：指令引导视频编辑评估的现代基准套件 (原标题: IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing Assessment)",
      "link": "https://arxiv.org/abs/2510.11647",
      "pubDate": "Mon, 13 Oct 2025 13:27:08 GMT",
      "isoDate": "2025-10-13T13:27:08.000Z",
      "creator": "Yinan Chen, Jiangning Zhang, Teng Hu, Yuxiang Zeng, Zhucun Xue, Qingdong He, Chengjie Wang, Yong Liu, Xiaobin Hu, Shuicheng Yan",
      "summary": "## IVEBench：指令引导视频编辑评估的现代基准套件\n\n### 引言\n指令引导视频编辑作为一项快速发展的研究方向，为直观的内容转换带来了新的机遇，但同时也对系统性评估提出了严峻挑战。现有的视频编辑基准在评估指令引导视频编辑方面存在不足，主要体现在来源多样性有限、任务覆盖范围狭窄以及评估指标不完整。\n\n### IVEBench的提出\n为解决上述局限性，研究人员引入了 **IVEBench**，这是一个专门为指令引导视频编辑评估设计的现代基准套件。\n\n### IVEBench的核心组成与特点\nIVEBench通过以下几个方面提升了指令引导视频编辑的评估能力：\n\n1.  **多样化的数据库**\n    *   包含600个高质量的源视频。\n    *   视频内容涵盖七个语义维度，确保了广泛的场景和主题。\n    *   视频长度范围从32帧到1024帧，覆盖了不同时长的编辑需求。\n\n2.  **丰富的编辑任务**\n    *   设计了8个主要类别的编辑任务，并进一步细分为35个子类别。\n    *   任务提示（prompts）的生成和优化结合了大型语言模型（LLMs）的强大能力和专家评审的严谨性，确保了任务的清晰性和有效性。\n\n3.  **三维评估协议**\n    *   IVEBench建立了一个全面的三维评估协议，以多角度衡量编辑效果。\n    *   **视频质量 (Video Quality)**：评估编辑后视频的视觉效果和技术质量。\n    *   **指令依从性 (Instruction Compliance)**：衡量编辑结果与用户指令的匹配程度。\n    *   **视频保真度 (Video Fidelity)**：评估编辑过程中对原始视频内容和风格的保持程度。\n    *   该协议整合了传统的评估指标和基于多模态大型语言模型（MLLMs）的先进评估方法，提供了更全面和智能的评估视角。\n\n### 实验结果\n广泛的实验证明了IVEBench在基准测试最先进的指令引导视频编辑方法方面的有效性。它能够提供全面且与人类判断高度一致的评估结果，从而推动该领域的研究进展。\n\n### 其他信息\n*   本文的前两位作者贡献相同。\n*   该研究属于计算机视觉与模式识别（cs.CV）领域。\n*   相关项目页面、代码和数据集链接在原文中以占位符形式给出。",
      "shortSummary": "IVEBench是一个为指令引导视频编辑评估设计的现代基准套件，旨在解决现有基准在多样性、任务覆盖和评估指标方面的不足。它包含600个高质量视频、8类35个子任务，并采用三维评估协议，涵盖视频质量、指令依从性和视频保真度，结合传统指标与多模态大模型评估。实验证明IVEBench能有效评估最先进方法，提供全面且与人类判断一致的结果。",
      "translated_title": "IVEBench：指令引导视频编辑评估的现代基准套件",
      "images": [],
      "contentSource": "完整文章",
      "content": "Instruction-guided video editing has emerged as a rapidly advancing research direction, offering new opportunities for intuitive content transformation while also posing significant challenges for systematic evaluation. Existing video editing benchmarks fail to support the evaluation of instruction-guided video editing adequately and further suffer from limited source diversity, narrow task coverage and incomplete evaluation metrics. To address the above limitations, we introduce IVEBench, a modern benchmark suite specifically designed for instruction-guided video editing assessment. IVEBench comprises a diverse database of 600 high-quality source videos, spanning seven semantic dimensions, and covering video lengths ranging from 32 to 1,024 frames. It further includes 8 categories of editing tasks with 35 subcategories, whose prompts are generated and refined through large language models and expert review. Crucially, IVEBench establishes a three-dimensional evaluation protocol encompassing video quality, instruction compliance and video fidelity, integrating both traditional metrics and multimodal large language model-based assessments. Extensive experiments demonstrate the effectiveness of IVEBench in benchmarking state-of-the-art instruction-guided video editing methods, showing its ability to provide comprehensive and human-aligned evaluation outcomes."
    },
    {
      "title": "LikePhys：通过似然偏好评估视频扩散模型中的直观物理理解 (原标题: LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference)",
      "link": "https://arxiv.org/abs/2510.11512",
      "pubDate": "Mon, 13 Oct 2025 11:19:07 GMT",
      "isoDate": "2025-10-13T11:19:07.000Z",
      "creator": "Jianhao Yuan, Fabio Pizzati, Francesco Pinto, Lars Kunze, Ivan Laptev, Paul Newman, Philip Torr, Daniele De Martini",
      "summary": "### LikePhys：通过似然偏好评估视频扩散模型中的直观物理理解\n\n#### 引言\n在构建通用且物理上合理的“世界模拟器”时，视频扩散模型对直观物理的理解至关重要。然而，准确评估这种能力一直是一个挑战，因为很难将生成内容中的物理正确性与视觉外观区分开来。\n\n#### LikePhys 方法\n为解决上述评估难题，本文引入了 **LikePhys**，这是一种无需训练的评估方法。LikePhys 的核心思想是通过区分物理上有效和不可能的视频来评估视频扩散模型中的直观物理理解。它利用去噪目标作为基于ELBO（证据下界）的似然替代，在一个精心策划的有效-无效视频对数据集上进行评估。\n\n#### 评估基准与指标\n研究人员构建了一个包含12个场景、涵盖4个物理领域的基准测试，用于LikePhys的评估。在此基础上，他们提出了一个新的评估指标——**合理性偏好错误（Plausibility Preference Error, PPE）**。实验结果表明，PPE 指标与人类偏好表现出高度一致性，并且在评估性能上优于现有的最先进评估基线。\n\n#### 对当前视频扩散模型的系统基准测试与分析\n本文进一步系统地基准测试了当前视频扩散模型在直观物理理解方面的表现。研究分析了模型设计和推理设置如何影响这种理解能力，并强调了不同物理定律之间模型能力的领域特异性变化。\n\n**实证结果显示：**\n*   当前模型在处理复杂和混沌动力学方面仍然面临挑战。\n*   然而，随着模型容量和推理设置的扩展，视频扩散模型的物理理解能力呈现出明显的改进趋势。\n\n#### 结论\nLikePhys 提供了一种有效且与人类偏好高度一致的直观物理评估方法，为视频扩散模型的发展提供了重要工具。尽管当前模型在处理复杂物理场景时仍有局限，但研究揭示了随着模型规模和推理能力的提升，其物理理解能力将持续增强的积极趋势。",
      "shortSummary": "本文提出LikePhys，一种无需训练的方法，通过区分物理上有效和不可能的视频来评估视频扩散模型中的直观物理理解。该方法利用去噪目标作为似然替代，并在包含12个场景、4个物理领域的基准上进行测试。提出的合理性偏好错误（PPE）指标与人类偏好高度一致，并优于现有评估器。研究发现，尽管当前模型在复杂动力学上仍有不足，但随着模型规模和推理设置的提升，物理理解能力呈现明显改善趋势。",
      "translated_title": "LikePhys：通过似然偏好评估视频扩散模型中的直观物理理解",
      "images": [],
      "contentSource": "完整文章",
      "content": "Intuitive physics understanding in video diffusion models plays an essential role in building general-purpose physically plausible world simulators, yet accurately evaluating such capacity remains a challenging task due to the difficulty in disentangling physics correctness from visual appearance in generation. To the end, we introduce LikePhys, a training-free method that evaluates intuitive physics in video diffusion models by distinguishing physically valid and impossible videos using the denoising objective as an ELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By testing on our constructed benchmark of twelve scenarios spanning over four physics domains, we show that our evaluation metric, Plausibility Preference Error (PPE), demonstrates strong alignment with human preference, outperforming state-of-the-art evaluator baselines. We then systematically benchmark intuitive physics understanding in current video diffusion models. Our study further analyses how model design and inference settings affect intuitive physics understanding and highlights domain-specific capacity variations across physical laws. Empirical results show that, despite current models struggling with complex and chaotic dynamics, there is a clear trend of improvement in physics understanding as model capacity and inference settings scale."
    },
    {
      "title": "ReLook：基于视觉的强化学习与多模态LLM评论器用于智能体式网页编码 (原标题: ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding)",
      "link": "https://arxiv.org/abs/2510.11498",
      "pubDate": "Mon, 13 Oct 2025 11:05:50 GMT",
      "isoDate": "2025-10-13T11:05:50.000Z",
      "creator": "Yuhang Li, Chenchen Zhang, Ruilin Lv, Ao Liu, Ken Deng, Yuanxing Zhang, Jiaheng Liu, Wiggin Zhou, Bo Zhou",
      "summary": "## ReLook：基于视觉的强化学习与多模态LLM评论器用于智能体式网页编码\n\n### 摘要\n\n大型语言模型（LLMs）在算法代码生成方面表现出色，但在前端开发中面临挑战，因为前端代码的正确性需要通过渲染的像素和交互来判断。为了解决这一问题，本文提出了ReLook，一个智能体式（agentic）、基于视觉的强化学习框架。\n\n### ReLook 框架的核心机制\n\nReLook 赋予智能体一个强大的“生成-诊断-改进”循环，其核心是调用一个多模态大型语言模型（MLLM）作为工具。\n\n#### 1. 训练阶段：\n\n*   **MLLM 作为视觉评论器：** 在训练过程中，智能体将 MLLM 整合到循环中，使其能够通过分析代码生成的屏幕截图来评估代码。MLLM 充当一个视觉判别器，对代码的视觉表现进行评分。\n*   **提供视觉反馈：** MLLM 不仅评分，还提供可操作的、基于视觉的反馈，指导智能体进行改进，从而形成一个闭环的优化过程。\n*   **严格的零奖励规则：** 对于生成无效渲染的代码，ReLook 实施严格的零奖励规则。这确保了代码的可渲染性，并有效防止了模型通过生成不符合实际渲染要求的代码来“欺骗”奖励机制（reward hacking）。\n*   **强制优化（Forced Optimization）：** 为了避免训练过程中可能出现的行为崩溃，ReLook 引入了强制优化机制。这是一个严格的接受规则，只接受能够带来改进的修订版本，从而产生单调递增的优化轨迹，确保模型持续向更好的方向发展。\n\n#### 2. 推理阶段：\n\n*   **评论器解耦：** 在推理时，ReLook 将 MLLM 评论器从循环中解耦。这意味着在实际应用中，无需实时调用昂贵的 MLLM 进行评估。\n*   **轻量级自编辑循环：** 智能体运行一个轻量级、无评论器的自编辑循环，依靠自身学习到的能力进行代码修正和优化。\n*   **效率与性能：** 这种解耦策略使得推理延迟与基础解码（base decoding）相当，保持了高效性，同时保留了大部分训练阶段获得的性能提升。\n\n### 实验结果与优势\n\nReLook 在三个广泛使用的基准测试中，在基于视觉的前端代码生成方面持续优于强大的基线模型。这突出并验证了以下关键优势：\n\n*   **智能体感知（Agentic Perception）：** 智能体能够有效地感知和理解视觉信息，将其融入到决策和代码生成中。\n*   **视觉奖励（Visual Rewards）：** 利用视觉信息作为奖励信号，能够更准确地引导模型学习符合视觉要求的前端代码。\n*   **训练-推理解耦（Training-Inference Decoupling）：** 实现了训练和推理阶段的有效分离，兼顾了性能和效率，使得系统在实际部署中更具可行性。\n\n**图片说明：**\n文章内容中未包含任何实际图片链接，因此本摘要不包含图片。",
      "shortSummary": "ReLook是一个基于视觉的强化学习框架，旨在解决LLMs在前端开发中因视觉正确性判断而面临的挑战。它利用多模态LLM作为视觉评论器，在训练中提供基于屏幕截图的反馈和严格的零奖励规则，并通过“强制优化”确保改进。推理时，ReLook解耦评论器，采用轻量级自编辑循环，保持低延迟并保留性能。在多个基准测试中，ReLook在视觉基础的前端代码生成方面表现优异，凸显了智能体感知、视觉奖励和训练-推理解耦的优势。",
      "translated_title": "ReLook：基于视觉的强化学习与多模态LLM评论器用于智能体式网页编码",
      "images": [],
      "contentSource": "完整文章",
      "content": "While Large Language Models (LLMs) excel at algorithmic code generation, they struggle with front-end development, where correctness is judged on rendered pixels and interaction. We present ReLook, an agentic, vision-grounded reinforcement learning framework that empowers an agent to close a robust generate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool. During training, the agent uses the MLLM-in-the-loop both as a visual critic--scoring code with screenshots--and as a source of actionable, vision-grounded feedback; a strict zero-reward rule for invalid renders anchors renderability and prevents reward hacking. To prevent behavioral collapse, we introduce Forced Optimization, a strict acceptance rule that admits only improving revisions, yielding monotonically better trajectories. At inference, we decouple the critic and run a lightweight, critic-free self-edit cycle, keeping latency comparable to base decoding while retaining most of the gains. Across three widely used benchmarks, ReLook consistently outperforms strong baselines in vision-grounded front-end code generation, highlighting the benefits of agentic perception, visual rewards, and training-inference decoupling."
    },
    {
      "title": "DocReward：一种用于结构化和风格化的文档奖励模型 (原标题: DocReward: A Document Reward Model for Structuring and Stylizing)",
      "link": "https://arxiv.org/abs/2510.11391",
      "pubDate": "Mon, 13 Oct 2025 09:36:32 GMT",
      "isoDate": "2025-10-13T09:36:32.000Z",
      "creator": "Junpeng Liu, Yuzhong Zhao, Bowen Cao, Jiayu Ding, Yilin Jia, Tengchao Lv, Yupan Huang, Shaohan Huang, Nan Yang, Li Dong, Lei Cui, Tao Ge, Xun Wang, Huitian Jiao, Sun Mao, FNU Kartik, Si-Qing Chen, Wai Lam, Furu Wei",
      "summary": "# DocReward：一种用于结构化和风格化的文档奖励模型\n\n## 引言与问题背景\n当前的代理工作流在自动化专业文档生成方面取得了进展，但其主要关注点在于文本质量，往往忽视了文档的视觉结构和风格。然而，这些视觉元素对于文档的可读性和用户参与度至关重要。这种不足主要源于缺乏合适的奖励模型来指导代理工作流生成具有更强结构和风格质量的文档。\n\n## DocReward 模型提案\n为解决上述问题，研究团队提出了 **DocReward**，这是一种专门用于评估文档结构和风格的文档奖励模型。\n\n## DocPair 数据集构建\n为了训练和评估DocReward，研究人员构建了一个名为 **DocPair** 的多领域数据集：\n*   **规模与范围**：包含117,000对文档，覆盖32个不同领域和267种文档类型。\n*   **内容特点**：每对文档都包含一个高专业度版本和一个低专业度版本。这两个版本的内容完全相同，但其结构和风格存在显著差异。\n*   **设计目的**：这种独特的设计使得模型能够以与文本质量无关的方式，全面评估文档的专业度。\n\n## 模型训练方法\nDocReward 模型采用 **Bradley-Terry 损失** 进行训练。该训练方法通过惩罚与标注排名相矛盾的预测，来对文档进行评分。\n\n## 性能评估与结果\n为了评估奖励模型的性能，研究团队创建了一个包含由受过良好教育的人类评估者排名的文档集合的测试数据集。\n*   **准确性表现**：\n    *   DocReward 在准确性方面显著优于现有基线模型。\n    *   与 GPT-4o 相比，DocReward 的准确性高出 30.6 个百分点。\n    *   与 GPT-5 相比，DocReward 的准确性高出 19.4 个百分点。\n*   **外部评估（文档生成任务）**：\n    *   在文档生成任务的外部评估中，DocReward 取得了更高的胜率，达到 60.8%。\n    *   相比之下，GPT-5 的胜率为 37.7%。\n    *   这表明 DocReward 在指导生成代理生成人类偏好的文档方面具有显著的实用性。\n\n## 研究领域\n本研究涉及计算机视觉与模式识别 (cs.CV)、人工智能 (cs.AI) 和计算与语言 (cs.CL) 等领域。",
      "shortSummary": "DocReward是一种新型文档奖励模型，旨在解决现有代理工作流在文档生成中忽视视觉结构和风格的问题。它通过评估文档的结构和风格来指导生成。研究团队构建了包含11.7万对文档的DocPair数据集进行训练。DocReward在准确性上显著优于GPT-4o和GPT-5，并在文档生成任务中取得了60.8%的更高胜率，证明其能有效引导生成代理产出人类偏好的文档。",
      "translated_title": "DocReward：一种用于结构化和风格化的文档奖励模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in agentic workflows have enabled the automation of tasks such as professional document generation. However, they primarily focus on textual quality, neglecting visual structure and style, which are crucial for readability and engagement. This gap arises mainly from the absence of suitable reward models to guide agentic workflows toward producing documents with stronger structural and stylistic quality. To address this, we propose DocReward, a document reward model that evaluates documents based on their structure and style. We construct a multi-domain dataset DocPair of 117K paired documents, covering 32 domains and 267 document types, each including a high- and low-professionalism document with identical content but different structure and style. This enables the model to evaluate professionalism comprehensively, and in a textual-quality-agnostic way. DocReward is trained using the Bradley-Terry loss to score documents, penalizing predictions that contradict the annotated ranking. To assess the performance of reward models, we create a test dataset containing document bundles ranked by well-educated human evaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6 and 19.4 percentage points, respectively, demonstrating its superiority over baselines. In an extrinsic evaluation of document generation, DocReward achieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7% win rate, demonstrating its utility in guiding generation agents toward producing human-preferred documents."
    },
    {
      "title": "InternSVG：利用多模态大型语言模型实现统一的SVG任务 (原标题: InternSVG: Towards Unified SVG Tasks with Multimodal Large Language Models)",
      "link": "https://arxiv.org/abs/2510.11341",
      "pubDate": "Mon, 13 Oct 2025 08:38:04 GMT",
      "isoDate": "2025-10-13T08:38:04.000Z",
      "creator": "Haomin Wang, Jinhui Yin, Qi Wei, Wenguang Zeng, Lixin Gu, Shenglong Ye, Zhangwei Gao, Yaohui Wang, Yanting Zhang, Yuanqi Li, Yanwen Guo, Wenhai Wang, Kai Chen, Yu Qiao, Hongjie Zhang",
      "summary": "### InternSVG：利用多模态大型语言模型实现统一的SVG任务\n\n**1. 研究背景与挑战**\n\n*   **SVG建模的挑战**：通用的SVG建模面临多重困难，包括：\n    *   数据集碎片化。\n    *   方法在不同任务间的可迁移性有限。\n    *   处理SVG结构复杂性方面的难题。\n\n**2. 核心思想与解决方案**\n\n*   **利用MLLM**：为应对上述挑战，研究团队利用多模态大型语言模型（MLLMs）强大的迁移和泛化能力，旨在实现SVG理解、编辑和生成的统一建模。\n\n**3. InternSVG家族：集成的数据-基准-模型套件**\n\n该研究提出了一个名为“InternSVG家族”的集成套件，包含以下核心组件：\n\n*   **SAgoge：综合性多模态SVG数据集**\n    *   **规模与范围**：目前最大、最全面的SVG任务多模态数据集。\n    *   **内容覆盖**：涵盖静态图形和动态动画，包括图标、长序列插图、科学图表和动态动画。\n    *   **任务多样性**：支持不同难度级别的任务。\n    *   **丰富性**：与现有数据集相比，提供了更深层次的结构和更丰富的属性。\n\n*   **SArena：配套基准测试平台**\n    *   **定义与评估**：提供全面的任务定义和标准化的评估方法。\n    *   **对齐性**：与SAgoge数据集所涵盖的领域和难度范围保持一致。\n\n*   **InternSVG：统一的MLLM模型**\n    *   **模型目标**：一个用于SVG理解、编辑和生成的统一MLLM。\n    *   **关键特性**：\n        *   **SVG专用特殊标记（special tokens）**：用于处理SVG特有的结构和元素。\n        *   **基于子词的嵌入初始化（subword-based embedding initialization）**：优化模型对SVG数据的理解。\n        *   **两阶段训练策略**：\n            *   第一阶段：从短的静态SVG开始训练。\n            *   第二阶段：逐步扩展到长序列插图和复杂的动画。\n\n**4. 成果与优势**\n\n*   **积极迁移与性能提升**：这种统一的建模方式能够促进积极的知识迁移，从而全面提升整体性能。\n*   **实验验证**：在SArena基准测试和先前的基准测试中，InternSVG取得了显著的性能提升，并持续优于领先的开源和专有同类模型。",
      "shortSummary": "InternSVG项目旨在通过多模态大型语言模型（MLLMs）解决SVG建模中的数据集碎片化、方法迁移性差和结构复杂性等挑战，实现SVG理解、编辑和生成的统一任务。该项目推出了“InternSVG家族”，包括：最大的多模态SVG数据集SAgoge、配套的基准测试平台SArena，以及核心模型InternSVG。InternSVG是一个统一的MLLM，采用SVG专用标记和两阶段训练策略。实验证明，InternSVG在各项任务中表现出色，显著优于现有模型，实现了积极的知识迁移和整体性能提升。",
      "translated_title": "InternSVG：利用多模态大型语言模型实现统一的SVG任务",
      "images": [],
      "contentSource": "完整文章",
      "content": "General SVG modeling remains challenging due to fragmented datasets, limited transferability of methods across tasks, and the difficulty of handling structural complexity. In response, we leverage the strong transfer and generalization capabilities of multimodal large language models (MLLMs) to achieve unified modeling for SVG understanding, editing, and generation. We present the InternSVG family, an integrated data-benchmark-model suite. At its core is SAgoge, the largest and most comprehensive multimodal dataset for SVG tasks, encompassing both static graphics and dynamic animations. It covers icons, long-sequence illustrations, scientific diagrams, and dynamic animations, supporting tasks of varied difficulty levels and providing deeper hierarchies with richer attributes compared to previous datasets. Based on this resource, we introduce SArena, a companion benchmark with comprehensive task definitions and standardized evaluation that aligns with the domains and difficulty spectrum covered by SAgoge. Building on these foundations, we propose InternSVG, a unified MLLM for SVG understanding, editing, and generation with SVG-specific special tokens, subword-based embedding initialization, and a two-stage training strategy that progresses from short static SVGs to long-sequence illustrations and complex animations. This unified formulation induces positive transfer and improves overall performance. Experiments on SArena and prior benchmark confirm that InternSVG achieves substantial gains and consistently outperforms leading open and proprietary counterparts."
    },
    {
      "title": "Vlaser：具有协同具身推理能力的视觉-语言-动作模型 (原标题: Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning)",
      "link": "https://arxiv.org/abs/2510.11027",
      "pubDate": "Mon, 13 Oct 2025 01:51:22 GMT",
      "isoDate": "2025-10-13T01:51:22.000Z",
      "creator": "Ganlin Yang, Tianyi Zhang, Haoran Hao, Weiyun Wang, Yibin Liu, Dehui Wang, Guanzhou Chen, Zijian Cai, Junting Chen, Weijie Su, Wengang Zhou, Yu Qiao, Jifeng Dai, Jiangmiao Pang, Gen Luo, Wenhai Wang, Yao Mu, Zhi Hou",
      "summary": "## Vlaser：弥合具身推理与VLA策略学习的鸿沟\n\n### 核心问题\n\n当前研究在开发基于视觉-语言模型（VLM）的具身推理能力或将先进VLM集成到视觉-语言-动作（VLA）模型中以实现端到端机器人控制方面取得了显著进展。然而，很少有研究直接解决上游VLM推理与下游VLA策略学习之间的关键差距。\n\n### Vlaser模型介绍\n\n本文迈出了弥合具身推理与VLA策略学习之间鸿沟的第一步，引入了 **Vlaser**——一个具有协同具身推理能力的视觉-语言-动作模型。Vlaser是一个基础性的视觉-语言模型，旨在将高级推理与具身智能体的低级控制相结合。\n\n### 关键特性与数据集\n\n*   **协同具身推理能力**：Vlaser的核心在于其协同具身推理能力，能够有效整合视觉、语言和动作信息。\n*   **高级推理与低级控制集成**：模型设计旨在无缝连接高层次的语义理解和决策与低层次的物理动作执行。\n*   **Vlaser-6M数据集**：Vlaser的构建基于高质量的Vlaser-6M数据集，这为其卓越性能奠定了基础。\n\n### 性能表现\n\nVlaser在多个具身推理基准上取得了最先进的性能，包括：\n\n*   **空间推理**\n*   **具身接地**\n*   **具身问答（QA）**\n*   **任务规划**\n\n### 领域偏移缓解与机器人控制\n\n研究系统地考察了不同VLM初始化对监督式VLA微调的影响，并提供了关于如何缓解互联网规模预训练数据与具身特定策略学习数据之间领域偏移的新颖见解。基于这些见解，Vlaser方法在以下机器人控制基准上取得了显著成果：\n\n*   在 **WidowX 基准**上取得了最先进的结果。\n*   在 **Google Robot 基准**上取得了有竞争力的表现。",
      "shortSummary": "Vlaser是一个视觉-语言-动作模型，旨在弥合基于VLM的具身推理与VLA策略学习之间的关键鸿沟。该模型利用Vlaser-6M数据集，集成了高级推理与低级控制，在空间推理、具身接地、具身问答和任务规划等具身推理基准上取得了最先进的性能。研究还提供了关于缓解预训练数据与具身策略学习数据之间领域偏移的新见解，并在WidowX和Google Robot基准上实现了领先或有竞争力的表现。",
      "translated_title": "Vlaser：具有协同具身推理能力的视觉-语言-动作模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark."
    },
    {
      "title": "GIR-Bench：用于生成具有推理能力的图像的多功能基准 (原标题: GIR-Bench: Versatile Benchmark for Generating Images with Reasoning)",
      "link": "https://arxiv.org/abs/2510.11026",
      "pubDate": "Mon, 13 Oct 2025 01:50:44 GMT",
      "isoDate": "2025-10-13T01:50:44.000Z",
      "creator": "Hongxiang Li, Yaowei Li, Bin Lin, Yuwei Niu, Yuhang Yang, Xiaoshuang Huang, Jiayin Cai, Xiaolong Jiang, Yao Hu, Long Chen",
      "summary": "# GIR-Bench：用于生成具有推理能力的图像的多功能基准\n\n## 引言\n统一多模态模型通过整合大型语言模型的推理能力与图像理解和生成，在高级多模态智能方面展现出巨大潜力。然而，当前社区缺乏一个以推理为中心的严格基准，以系统地评估理解与生成之间的一致性，以及它们在复杂视觉任务中的泛化能力。\n\n## GIR-Bench 介绍\n为此，研究人员引入了 **GIR-Bench**，这是一个全面的基准，旨在从三个互补的角度评估统一模型：\n\n### 1. 理解-生成一致性 (GIR-Bench-UGC)\n*   **目标：** 调查模型在理解和生成任务中是否能一致地利用相同的知识。\n*   **评估内容：** 衡量模型在不同模态任务中知识应用的一致性。\n\n### 2. 以推理为中心的文本到图像生成 (GIR-Bench-T2I)\n*   **目标：** 评估模型是否能够执行需要应用逻辑约束和隐式知识的文本到图像生成，以生成忠实的视觉内容。\n*   **评估内容：** 考察模型将复杂文本描述中的推理要求转化为视觉图像的能力。\n\n### 3. 编辑中的多步推理 (GIR-Bench-Edit)\n*   **目标：** 评估模型处理图像编辑中多步推理任务的能力。\n*   **评估内容：** 考察模型在复杂编辑场景中执行连续推理步骤的能力。\n\n## 评估方法\n对于每个子集，研究人员精心设计了针对特定任务的评估流程。这使得能够进行细粒度且可解释的评估，同时减轻了当前流行的“MLLM-as-a-Judge”（多模态大语言模型作为评判者）范式带来的偏差。\n\n## 主要发现\n对各种统一模型和仅生成系统进行的广泛消融实验表明：\n*   尽管统一模型在推理驱动的视觉任务中表现出更强的能力，但它们在理解和生成之间仍然存在持续的差距。\n\n## 数据与代码\nGIR-Bench 的数据和代码可在 [this https URL](this https URL) 获取。",
      "shortSummary": "GIR-Bench是一个新的多功能基准，旨在解决当前统一多模态模型在复杂视觉任务中缺乏推理能力评估的问题。它从理解-生成一致性、以推理为中心的文本到图像生成以及编辑中的多步推理三个方面，系统评估模型的理解与生成对齐及其泛化潜力。研究发现，尽管统一模型在推理驱动任务中表现更优，但理解与生成之间仍存在显著差距。数据和代码已公开。",
      "translated_title": "GIR-Bench：用于生成具有推理能力的图像的多功能基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Unified multimodal models integrate the reasoning capacity of large language models with both image understanding and generation, showing great promise for advanced multimodal intelligence. However, the community still lacks a rigorous reasoning-centric benchmark to systematically evaluate the alignment between understanding and generation, and their generalization potential in complex visual tasks. To this end, we introduce GIR-Bench, a comprehensive benchmark that evaluates unified models across three complementary perspectives. Firstly, we investigate understanding-generation consistency (GIR-Bench-UGC), asking whether models can consistently leverage the same knowledge in both understanding and generation tasks. Secondly, we investigate whether models can perform reasoning-centric text-to-image generation that requires applying logical constraints and implicit knowledge to generate faithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models can handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset, we carefully design different task-specific evaluation pipelines tailored for each task. This enables fine-grained and interpretable evaluation while mitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive ablations over various unified models and generation-only systems have shown that: Although unified models are more capable of reasoning-driven visual tasks, they still exhibit a persistent gap between understanding and generation. The data and code for GIR-Bench are available at https://hkust-longgroup.github.io/GIR-Bench{https://hkust-longgroup.github.io/GIR-Bench}."
    },
    {
      "title": "FastHMR：通过令牌和层合并与扩散解码加速人体网格恢复 (原标题: FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging with Diffusion Decoding)",
      "link": "https://arxiv.org/abs/2510.10868",
      "pubDate": "Sun, 12 Oct 2025 20:23:17 GMT",
      "isoDate": "2025-10-12T20:23:17.000Z",
      "creator": "Soroush Mehraban, Andrea Iaboni, Babak Taati",
      "summary": "## FastHMR：通过令牌和层合并与扩散解码加速人体网格恢复\n\n本文介绍了一种名为FastHMR的新方法，旨在解决当前基于Transformer的3D人体网格恢复（HMR）模型所面临的计算成本高昂和复杂性问题。这些问题主要源于深度Transformer架构和冗余令牌。\n\n### 核心贡献\n\nFastHMR通过引入两种HMR特定的合并策略和一个扩散解码器来优化性能：\n\n*   **HMR特定合并策略**\n    *   **误差约束层合并 (Error-Constrained Layer Merging, ECLM)**：\n        *   该策略选择性地合并Transformer层。\n        *   合并的依据是这些层对平均关节位置误差（Mean Per Joint Position Error, MPJPE）的影响最小。\n        *   目标是减少模型的深度和计算量，同时尽量保持预测精度。\n    *   **掩码引导令牌合并 (Mask-guided Token Merging, Mask-ToMe)**：\n        *   该策略专注于合并对最终预测贡献较小的背景令牌。\n        *   通过识别并整合这些冗余令牌，可以有效降低Transformer处理的序列长度，从而加速计算。\n\n*   **扩散解码器**\n    *   为了进一步解决合并策略可能导致的性能下降问题，FastHMR提出了一种基于扩散的解码器。\n    *   该解码器能够整合时间上下文信息。\n    *   它还利用从大规模运动捕捉数据集中学习到的姿态先验知识。\n    *   通过这些机制，扩散解码器有助于弥补合并操作可能带来的信息损失，并提升模型的鲁棒性和准确性。\n\n### 实验结果\n\n在多个基准测试中进行的实验表明，FastHMR方法取得了显著的性能提升：\n\n*   **速度提升**：相比基线模型，FastHMR实现了高达2.3倍的加速。\n*   **性能提升**：在加速的同时，该方法还略微改善了模型的性能。\n\n### 结论\n\nFastHMR通过创新的令牌和层合并策略，结合强大的扩散解码器，成功地在显著加速3D人体网格恢复过程的同时，保持并略微提升了模型的预测精度，为该领域的高效应用提供了新的途径。",
      "shortSummary": "FastHMR提出通过误差约束层合并（ECLM）和掩码引导令牌合并（Mask-ToMe）策略，结合扩散解码器，加速3D人体网格恢复（HMR）。该方法选择性合并Transformer层和背景令牌，并利用扩散解码器整合时间上下文和姿态先验，以解决现有HMR模型计算成本高的问题。实验结果显示，FastHMR实现了高达2.3倍的加速，同时略微提升了性能。",
      "translated_title": "FastHMR：通过令牌和层合并与扩散解码加速人体网格恢复",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent transformer-based models for 3D Human Mesh Recovery (HMR) have achieved strong performance but often suffer from high computational cost and complexity due to deep transformer architectures and redundant tokens. In this paper, we introduce two HMR-specific merging strategies: Error-Constrained Layer Merging (ECLM) and Mask-guided Token Merging (Mask-ToMe). ECLM selectively merges transformer layers that have minimal impact on the Mean Per Joint Position Error (MPJPE), while Mask-ToMe focuses on merging background tokens that contribute little to the final prediction. To further address the potential performance drop caused by merging, we propose a diffusion-based decoder that incorporates temporal context and leverages pose priors learned from large-scale motion capture datasets. Experiments across multiple benchmarks demonstrate that our method achieves up to 2.3x speed-up while slightly improving performance over the baseline."
    },
    {
      "title": "OmniVideoBench：面向全方位多模态大语言模型（MLLMs）的音视频理解评估 (原标题: OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs)",
      "link": "https://arxiv.org/abs/2510.10689",
      "pubDate": "Sun, 12 Oct 2025 12:34:00 GMT",
      "isoDate": "2025-10-12T12:34:00.000Z",
      "creator": "Caorui Li, Yu Chen, Yiyan Ji, Jin Xu, Zhenyu Cui, Shihao Li, Yuanxing Zhang, Jiafu Tang, Zhenghao Song, Dingling Zhang, Ying He, Haoxiang Liu, Yuxuan Wang, Qiufeng Wang, Zhenhe Wu, Jiehui Luo, Zhiyu Pan, Weihao Xie, Chenchen Zhang, Zhaohui Wang, Jiayi Tian, Yanghai Wang, Zhe Cao, Minxin Dai, Ke Wang, Runzhe Wen, Yinghao Ma, Yaning Pan, Sungkyun Chang, Termeh Taheri, Haiwen Xia, Christos Plachouras, Emmanouil Benetos, Yizhi Li, Ge Zhang, Jian Yang, Tianhao Peng, Zili Wang, Minghao Liu, Junran Peng, Zhaoxiang Zhang, Jiaheng Liu",
      "summary": "## OmniVideoBench：音视频理解评估基准\n\n### 引言\n\n多模态大语言模型（MLLMs）在视频理解方面展现出巨大潜力。然而，现有基准在评估音视频模态协同推理能力方面存在不足，常常忽视其中一种模态，或以逻辑不一致的方式整合它们，导致无法全面评估模型真正的音视频理解能力。\n\n### OmniVideoBench 的提出\n\n为了弥补这一空白，研究人员引入了 **OmniVideoBench**，这是一个大规模且设计严谨的基准，专门用于评估协同音视频理解能力，并强调模态互补性和逻辑一致性。\n\n### 基准特点\n\n*   **高质量数据集**：包含1000对高质量的问答（QA）对，每对都附有分步推理轨迹。\n*   **多样化视频来源**：数据来源于628个多样化的视频，时长从几秒到30分钟不等。\n*   **严格验证**：所有数据都经过人工验证，以确保其完全正确性和独特性。\n*   **全面的问题类型**：涵盖13种精心设计的问题类型，包括：\n    *   时间推理\n    *   空间定位\n    *   计数\n    *   因果推断\n    *   摘要生成\n    *   以及其他捕捉视频理解核心挑战的类型。\n\n### 评估结果与发现\n\n对多个 MLLMs 在 OmniVideoBench 上进行的评估揭示了模型性能与人类推理能力之间存在显著差距。其中，开源模型的表现明显落后于闭源模型，这突显了真正音视频推理固有的难度。\n\n### 展望\n\n研究团队将发布 OmniVideoBench，以期促进具有更强、更通用推理能力的 MLLMs 的发展。",
      "shortSummary": "OmniVideoBench是一个旨在评估多模态大语言模型（MLLMs）协同音视频理解能力的新型基准。它解决了现有评估方法中模态整合不足和逻辑不一致的问题。该基准包含1000对高质量问答对，来源于628个多样化视频，并涵盖13种问题类型。评估结果显示，模型性能与人类推理存在显著差距，开源模型表现落后，突显了音视频推理的挑战性。OmniVideoBench的发布旨在推动MLLMs的发展。",
      "translated_title": "OmniVideoBench：面向全方位多模态大语言模型（MLLMs）的音视频理解评估",
      "images": [
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png",
          "alt": "BibSonomy logo",
          "title": "",
          "position": 1
        },
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png",
          "alt": "Reddit logo",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "Recent advances in multimodal large language models (MLLMs) have demonstrated substantial potential in video understanding. However, existing benchmarks fail to comprehensively evaluate synergistic reasoning capabilities across audio and visual modalities, often neglecting either one of the modalities or integrating them in a logically inconsistent manner. To bridge this gap, we introduce OmniVideoBench, a large-scale and rigorously designed benchmark dedicated to assessing synergistic audio-visual understanding, with a strong emphasis on modality complementarity and logical consistency. Specifically, OmniVideoBench comprises 1000 high-quality question-answer(QA) pairs, each annotated with step-by-step reasoning traces, derived from 628 diverse videos ranging from several seconds to 30 minutes, and manually verified to guarantee complete correctness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully designed question types, covering temporal reasoning, spatial localization, counting, causal inference, summarization, and beyond, thereby capturing the essential challenges of video understanding. Evaluation of multiple MLLMs on OmniVideoBench reveals a pronounced gap between model performance and human reasoning, with open-source models lagging significantly behind their closed-source counterparts, underscoring the inherent difficulty of genuine audio-visual reasoning. We will release OmniVideoBench to foster the development of MLLMs with stronger and more generalizable reasoning capabilities."
    },
    {
      "title": "RePro：训练语言模型以忠实地回收网络进行预训练 (原标题: RePro: Training Language Models to Faithfully Recycle the Web for Pretraining)",
      "link": "https://arxiv.org/abs/2510.10681",
      "pubDate": "Sun, 12 Oct 2025 12:08:38 GMT",
      "isoDate": "2025-10-12T12:08:38.000Z",
      "creator": "Zichun Yu, Chenyan Xiong",
      "summary": "# RePro：忠实回收网络数据以进行语言模型预训练\n\n## 引言\n大型语言模型（LLM）的预训练数据质量至关重要，但高质量数据储备日益稀缺。本文介绍了一种名为 RePro 的新型网络数据回收方法，旨在有效解决这一问题。\n\n## RePro 方法概述\nRePro 通过训练一个相对较小的语言模型（LM）来生成预训练数据的有效且忠实的复述。该训练过程采用强化学习，并设计了以下奖励机制：\n\n*   **一个质量奖励**：确保生成内容的整体质量。\n*   **三个忠实度奖励**：确保复述内容在核心语义和结构上与原始数据保持一致。\n\n通过这些奖励，RePro 优化了 LM 复述器，使其能够将原始数据转换为高质量的复述，同时保留其核心含义和结构。\n\n## 实验设置与结果\n研究人员进行了以下实验来验证 RePro 的有效性：\n\n### 实验训练\n*   训练了一个 **4B 参数的复述器**。\n*   从 DCLM-RefinedWeb 中回收了 **720 亿个 token**。\n*   使用回收数据对 **400M 和 1.4B 参数的模型**进行了预训练。\n\n### 性能表现\nRePro 在多项评估中展现出卓越的性能：\n\n*   **下游任务准确性提升**：在 22 个下游任务上，RePro 相较于仅使用原始数据的基线模型，相对准确率提升了 **4.7% 至 14.0%**。\n*   **超越现有技术**：RePro 优于当前最先进的网络回收方法 ReWire。值得注意的是，ReWire 需要一个 70B 参数的复述器，而 RePro 仅使用 4B 参数的复述器就实现了更好的效果。\n*   **数据池效率**：RePro 甚至超越了使用 4 倍大原始数据池的基线模型。\n*   **数据效率提升**：通过不同回收数据量的实验表明，RePro 将原始数据效率提高了 **2 到 3 倍**。\n*   **信息保留能力**：个体和分布分析验证了 RePro 与基于提示的方法相比，能够保留更多关键信息，并更忠实地反映原始数据的特征。\n\n## 结论\n综合来看，这些结果表明 RePro 为有效利用 LLM 预训练的“化石燃料”提供了一条高效且可控的途径。\n\n## 资源开放\n研究团队已将 RePro 的代码、复述器和回收数据开源。",
      "shortSummary": "RePro 是一种新型网络数据回收方法，通过强化学习训练一个小型语言模型（4B参数）来忠实地复述预训练数据。实验表明，RePro 在22个下游任务上比基线模型提升了4.7%-14.0%的准确率，并超越了现有技术ReWire。它还将原始数据效率提高了2-3倍，有效解决了高质量LLM预训练数据稀缺的问题，为LLM预训练提供了高效可控的数据利用途径。代码和数据已开源。",
      "translated_title": "RePro：训练语言模型以忠实地回收网络进行预训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "High-quality pretraining data is the fossil fuel of large language models (LLMs), yet its reserves are running low for frontier models. In this paper, we introduce RePro, a novel web recycling method that trains a relatively small LM with reinforcement learning to generate effective and faithful rephrasings of pretraining data. Specifically, we design one quality reward and three faithfulness rewards, optimizing the LM rephraser to convert organic data into high-quality rephrasings while maintaining its core semantics and structure. In our experiment, we train a 4B rephraser to recycle 72B tokens sampled from DCLM-RefinedWeb. Pretraining results on 400M and 1.4B models demonstrate that RePro delivers 4.7%-14.0% relative accuracy gains over organic-only baseline on 22 downstream tasks. RePro also outperforms ReWire, the state-of-the-art web recycling method that prompts a 70B rephraser, as well as the organic baseline with a 4x larger data pool. Experiments with different amounts of recycled data highlight that RePro improves organic data efficiency by 2-3x. Individual and distributional analyses validate that RePro preserves more critical information and faithfully reflects the characteristics of organic data compared to prompting-based methods. Together, these results show that RePro provides an efficient and controllable path to effectively harness the fossil fuel of LLM pretraining. We open-source our code, rephraser, and recycled data at https://github.com/cxcscmu/RePro."
    },
    {
      "title": "AdaViewPlanner：将视频扩散模型应用于4D场景中的视点规划 (原标题: AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning in 4D Scenes)",
      "link": "https://arxiv.org/abs/2510.10670",
      "pubDate": "Sun, 12 Oct 2025 11:55:44 GMT",
      "isoDate": "2025-10-12T11:55:44.000Z",
      "creator": "Yu Li, Menghan Xia, Gongye Liu, Jianhong Bai, Xintao Wang, Conglang Zhang, Yuxuan Lin, Ruihang Chu, Pengfei Wan, Yujiu Yang",
      "summary": "# AdaViewPlanner：将视频扩散模型应用于4D场景中的视点规划\n\n本文提出了一种名为 **AdaViewPlanner** 的方法，旨在将预训练的文本到视频（T2V）扩散模型应用于4D场景中的视点规划任务。\n\n## 研究背景与动机\n*   **T2V模型的潜力**：近期文本到视频（T2V）模型在视觉模拟真实世界几何和物理规律方面展现出强大能力，表明其作为隐式世界模型的潜力。\n*   **视点规划需求**：视频本身伴随着动态场景和自然视点，这启发了研究人员探索利用视频生成先验知识进行4D场景的视点规划。\n\n## AdaViewPlanner 方法概述\nAdaViewPlanner 采用一个两阶段范式来兼容地适应预训练的T2V模型进行视点预测：\n\n### 阶段一：4D场景表示注入\n1.  **目标**：将4D场景信息融入预训练的T2V模型。\n2.  **机制**：通过一个自适应学习分支，将与视点无关的4D场景表示注入到预训练的T2V模型中。\n3.  **结果**：条件生成的视频将视觉上嵌入所需的视点信息。\n\n### 阶段二：视点提取与相机外参去噪\n1.  **目标**：从生成的视频和4D场景中提取精确的视点。\n2.  **机制**：在预训练的T2V模型上进一步引入一个相机外参扩散分支。\n3.  **输入**：该分支以第一阶段生成的视频和原始4D场景作为输入。\n4.  **过程**：将视点提取问题表述为一个混合条件引导的相机外参去噪过程。\n\n## 实验结果与贡献\n*   **性能优势**：实验结果表明，AdaViewPlanner 方法优于现有竞争对手。\n*   **设计验证**：消融研究验证了关键技术设计的有效性。\n*   **未来潜力**：这项工作在一定程度上证明了视频生成模型在真实世界4D交互方面的巨大潜力。\n\n## 技术领域\n*   计算机视觉与模式识别 (cs.CV)",
      "shortSummary": "本文提出了AdaViewPlanner，一种将预训练的文本到视频（T2V）模型应用于4D场景视点规划的方法。该方法采用两阶段范式：首先通过自适应学习分支将4D场景表示注入T2V模型，使生成的视频包含视点信息；然后引入相机外参扩散分支，将视点提取表述为混合条件引导的相机外参去噪过程。实验结果表明，AdaViewPlanner优于现有方法，并证明了视频生成模型在4D交互中的潜力。",
      "translated_title": "AdaViewPlanner：将视频扩散模型应用于4D场景中的视点规划",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent Text-to-Video (T2V) models have demonstrated powerful capability in visual simulation of real-world geometry and physical laws, indicating its potential as implicit world models. Inspired by this, we explore the feasibility of leveraging the video generation prior for viewpoint planning from given 4D scenes, since videos internally accompany dynamic scenes with natural viewpoints. To this end, we propose a two-stage paradigm to adapt pre-trained T2V models for viewpoint prediction, in a compatible manner. First, we inject the 4D scene representation into the pre-trained T2V model via an adaptive learning branch, where the 4D scene is viewpoint-agnostic and the conditional generated video embeds the viewpoints visually. Then, we formulate viewpoint extraction as a hybrid-condition guided camera extrinsic denoising process. Specifically, a camera extrinsic diffusion branch is further introduced onto the pre-trained T2V model, by taking the generated video and 4D scene as input. Experimental results show the superiority of our proposed method over existing competitors, and ablation studies validate the effectiveness of our key technical designs. To some extent, this work proves the potential of video generation models toward 4D interaction in real world."
    },
    {
      "title": "BrowserAgent：构建具有人类启发式网页浏览行为的网页代理 (原标题: BrowserAgent: Building Web Agents with Human-Inspired Web Browsing Actions)",
      "link": "https://arxiv.org/abs/2510.10666",
      "pubDate": "Sun, 12 Oct 2025 11:43:37 GMT",
      "isoDate": "2025-10-12T11:43:37.000Z",
      "creator": "Zhengbo Zhang, Zhiheng Lyu, Junhao Gong, Hongzhu Yi, Xinming Wang, Yuxuan Zhou, Jiabing Yang, Ping Nie, Yan Huang, Wenhu Chen",
      "summary": "# BrowserAgent：基于人类启发式网页浏览行为的网页代理\n\n## 引言\n\n大型语言模型（LLMs）在解决现实世界问题时，越来越依赖于与动态网络环境交互以及自主获取外部信息的能力。然而，当前的一些先进研究，如Search-R1和WebDancer，虽然在解决网页任务方面表现出色，但它们主要通过将交互式网页环境转换为静态文本内容来实现，这与人类多样化的浏览器交互行为（如滚动、点击和输入）形成对比。\n\n## BrowserAgent 提案\n\n本文提出了 **BrowserAgent**，一个更具交互性的代理，它通过模仿人类启发式的浏览器动作来解决复杂的任务。BrowserAgent 的核心特点包括：\n\n*   **直接操作网页：** 它通过Playwright直接在原始网页上进行操作，而不是依赖于将网页内容转换为静态文本。\n*   **人类启发式动作：** 使用一组预定义的浏览器动作，模拟人类浏览网页时的行为，例如滚动、点击和输入。\n\n## 训练方法\n\n为了提高模型的泛化能力，BrowserAgent 采用了两阶段训练方法：\n\n1.  **监督微调（SFT - Supervised Fine-Tuning）：** 初步训练模型以学习基本任务。\n2.  **拒绝微调（RFT - Rejection Fine-Tuning）：** 进一步优化模型，使其能够更好地处理复杂情境并减少错误。\n\n## 性能与优势\n\nBrowserAgent 在多个方面展现出显著的性能提升和优势：\n\n*   **数据效率：** 尽管使用的训练数据量显著少于Search-R1，BrowserAgent 在不同的Open-QA（开放域问答）任务上取得了更具竞争力的结果。\n*   **增强推理能力：** 引入了显式记忆机制，用于在不同步骤中存储关键结论，这显著增强了模型在处理长程任务时的推理能力。\n*   **多跳问答表现：** 值得注意的是，BrowserAgent-7B 在HotpotQA、2Wiki和Bamboogle等多跳问答任务上，比Search-R1的性能提高了约20%。\n\n## 结论\n\n这些结果表明，BrowserAgent 可以作为一个更先进的框架，用于开发更具交互性和可扩展性的网页代理，从而推动LLMs在复杂网络环境中的应用。",
      "shortSummary": "BrowserAgent是一个受人类启发、更具交互性的网页代理，它通过Playwright直接在原始网页上执行滚动、点击、输入等动作来解决复杂任务。该模型采用两阶段训练（SFT和RFT），并引入显式记忆机制以增强推理能力。尽管训练数据较少，BrowserAgent在Open-QA任务上表现优异，尤其在多跳问答任务上比Search-R1提升约20%。它为构建更先进、可扩展的网页代理提供了新框架。",
      "translated_title": "BrowserAgent：构建具有人类启发式网页浏览行为的网页代理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Efficiently solving real-world problems with LLMs increasingly hinges on their ability to interact with dynamic web environments and autonomously acquire external information. While recent research like Search-R1 and WebDancer demonstrates strong performance in solving web tasks, they heavily rely on additional tools to convert the interactive web environment into static text content. This is in contrast to human browsing behaviors, which involve diverse interactions with the browser, such as scrolling, clicking, and typing. In this paper, we propose BrowserAgent, a more interactive agent that solves complex tasks through human-inspired browser actions. BrowserAgent operates directly on raw web pages via Playwright through a set of predefined browser actions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and Rejection Fine-Tuning (RFT)) to improve the model's generalization abilities. Despite using significantly less training data than Search-R1, BrowserAgent achieves more competitive results across different Open-QA tasks. Additionally, we introduce an explicit memory mechanism to store key conclusions across steps, further enhancing the model's reasoning capabilities for long-horizon tasks. Notably, BrowserAgent-7B can achieve around 20\\% improvement over Search-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle. These results indicate that BrowserAgent can serve as a more advanced framework for more interactive and scalable web agents."
    }
  ],
  "lastUpdated": "2025-10-14T09:39:28.136Z"
}