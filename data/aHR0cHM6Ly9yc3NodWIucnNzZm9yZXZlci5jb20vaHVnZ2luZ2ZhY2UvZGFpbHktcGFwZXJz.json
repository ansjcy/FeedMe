{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "Depth Anything 3: Recovering the Visual Space from Any Views",
      "link": "https://arxiv.org/abs/2511.10647",
      "pubDate": "Thu, 13 Nov 2025 13:59:53 GMT",
      "isoDate": "2025-11-13T13:59:53.000Z",
      "creator": "Haotong Lin, Sili Chen, Junhao Liew, Donny Y. Chen, Zhenyu Li, Guang Shi, Jiashi Feng, Bingyi Kang",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Depth Anything 3: Recovering the Visual Space from Any Views",
      "images": [],
      "contentSource": "RSS",
      "content": "We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets."
    },
    {
      "title": "大型语言模型的黑盒在线策略蒸馏 (原标题: Black-Box On-Policy Distillation of Large Language Models)",
      "link": "https://arxiv.org/abs/2511.10643",
      "pubDate": "Thu, 13 Nov 2025 13:58:37 GMT",
      "isoDate": "2025-11-13T13:58:37.000Z",
      "creator": "Tianzhu Ye, Li Dong, Zewen Chi, Xun Wu, Shaohan Huang, Furu Wei",
      "summary": "## 大型语言模型的黑盒在线策略蒸馏：生成对抗蒸馏 (GAD)\n\n### 引言\n黑盒蒸馏是一种创建学生大型语言模型（LLM）的方法，它仅通过学习专有教师模型的文本输出来实现，而无需访问教师模型的内部逻辑或参数。这种方法在保护教师模型知识产权的同时，能够训练出性能接近的轻量级学生模型。\n\n### 生成对抗蒸馏 (GAD) 方法\n本文提出了一种名为**生成对抗蒸馏（Generative Adversarial Distillation, GAD）**的新方法，旨在实现大型语言模型的在线策略和黑盒蒸馏。GAD 的核心思想是将蒸馏过程构建为一个最小最大（minimax）博弈：\n\n*   **学生LLM作为生成器**：学生LLM被视为一个生成器，其任务是生成与教师LLM输出相似的文本响应。\n*   **判别器**：训练一个判别器来区分学生LLM生成的响应和教师LLM生成的响应。\n*   **最小最大博弈**：学生LLM的目标是生成能够“欺骗”判别器的响应，使其无法区分；判别器的目标是准确地区分两者。\n\n### 判别器作为在线策略奖励模型\n在 GAD 框架中，判别器扮演着一个关键角色，它充当一个**在线策略奖励模型**。这意味着判别器会随着学生模型的训练而共同演化，并提供：\n\n*   **稳定反馈**：判别器能够提供持续且稳定的学习信号。\n*   **自适应反馈**：判别器会根据学生模型的当前表现动态调整其反馈，从而引导学生模型更有效地学习。\n\n### 实验结果与性能\n实验结果表明，GAD 在黑盒LLM蒸馏方面表现出卓越的性能：\n\n*   **超越传统方法**：GAD 始终优于常用的序列级知识蒸馏方法，这表明其在学习效率和效果上的优势。\n*   **性能媲美教师模型**：具体而言，使用 GAD 训练的 Qwen2.5-14B-Instruct（学生模型）在 LMSYS-Chat 自动评估中，其性能达到了与教师模型 GPT-5-Chat 相媲美的水平。\n\n### 结论\n这些实验结果有力地确立了 GAD 作为一种有前景且有效的黑盒LLM蒸馏范式。它为在不访问教师模型内部参数的情况下，高效训练出高性能学生模型提供了一条新途径。",
      "shortSummary": "本文提出生成对抗蒸馏（GAD），一种实现大型语言模型黑盒在线策略蒸馏的新方法。GAD将学生模型作为生成器，训练判别器区分其输出与教师模型，形成最小最大博弈。判别器充当在线策略奖励模型，提供稳定自适应反馈。实验证明，GAD优于传统序列级知识蒸馏，使学生模型（如Qwen2.5-14B-Instruct）在自动评估中性能可与教师模型（如GPT-5-Chat）媲美，为黑盒LLM蒸馏提供有效范式。",
      "translated_title": "大型语言模型的黑盒在线策略蒸馏",
      "images": [],
      "contentSource": "完整文章",
      "content": "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation."
    },
    {
      "title": "潜在空间一小步，像素生成一大步：扩散模型的快速潜在上采样适配器 (原标题: One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models)",
      "link": "https://arxiv.org/abs/2511.10629",
      "pubDate": "Thu, 13 Nov 2025 13:54:18 GMT",
      "isoDate": "2025-11-13T13:54:18.000Z",
      "creator": "Aleksandr Razin, Danil Kazantsev, Ilya Makarov",
      "summary": "# 潜在上采样适配器 (LUA)：扩散模型的高效高分辨率生成\n\n## 引言与问题\n\n扩散模型在生成超出其训练分辨率的图像时面临显著挑战：\n*   **高分辨率采样缓慢且成本高昂**：直接生成高分辨率图像需要大量的计算资源和时间。\n*   **后处理图像超分辨率 (ISR) 的局限性**：在图像解码后进行超分辨率处理会引入伪影，并增加额外的延迟。\n\n## LUA 解决方案\n\n文章提出了**潜在上采样适配器 (Latent Upscaler Adapter, LUA)**，这是一个旨在解决上述问题而设计的轻量级模块。\n\n### LUA 的工作原理\n*   **直接在潜在代码上执行超分辨率**：LUA 在最终的 VAE 解码步骤之前，直接对生成器的潜在代码进行超分辨率处理。\n*   **单次前向传播**：通过在潜在空间中进行一次前向传播，LUA 即可实现高分辨率合成。\n\n### LUA 的关键特性与优势\n*   **即插即用组件**：LUA 可以作为一个独立的模块集成到现有扩散模型中，无需对基础模型进行任何修改，也无需添加额外的扩散阶段。\n*   **共享的 Swin 风格骨干网络**：LUA 采用了一个共享的 Swin 风格骨干网络，并配备了尺度特定的像素混洗头（pixel-shuffle heads）。\n*   **支持多种放大因子**：该模块支持 2 倍和 4 倍的放大因子。\n*   **兼容性**：LUA 与图像空间超分辨率基线保持兼容。\n\n## 性能表现\n\nLUA 在效率和质量方面展现出卓越的性能：\n*   **感知质量**：LUA 能够达到与图像空间超分辨率基线相当的感知质量。\n*   **显著的效率提升**：\n    *   解码和上采样时间减少了近 3 倍。\n    *   例如，从 512 像素生成 1024 像素图像时，LUA 仅增加 +0.42 秒，而使用相同 SwinIR 架构的像素空间超分辨率方法则需要 1.87 秒。\n\n## 泛化能力\n\n*   **跨 VAE 潜在空间的强大泛化**：LUA 在不同 VAE 的潜在空间中表现出强大的泛化能力，这意味着它无需为每个新的解码器从头开始重新训练，从而简化了部署过程。\n\n## 结论\n\nLUA 提供了一条实用且高效的途径，以实现现代扩散管道中可扩展、高保真图像合成。它在保真度上与原生高分辨率生成非常接近，同时显著提高了效率和部署灵活性。",
      "shortSummary": "潜在上采样适配器 (LUA) 解决了扩散模型高分辨率生成效率低、成本高及后处理超分辨率引入伪影的问题。LUA 是一个轻量级、即插即用模块，在 VAE 解码前直接对潜在代码进行超分辨率处理。它显著提高了效率（解码和上采样时间减少近 3 倍），同时保持了与图像空间超分辨率相当的感知质量。LUA 具有强大的泛化能力，易于部署，为扩散模型提供了高效、可扩展的高保真图像合成方案。",
      "translated_title": "潜在空间一小步，像素生成一大步：扩散模型的快速潜在上采样适配器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines."
    },
    {
      "title": "通过属性条件人工评估对图像生成多样性进行基准测试 (原标题: Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation)",
      "link": "https://arxiv.org/abs/2511.10547",
      "pubDate": "Thu, 13 Nov 2025 12:48:38 GMT",
      "isoDate": "2025-11-13T12:48:38.000Z",
      "creator": "Isabela Albuquerque, Ira Ktena, Olivia Wiles, Ivana Kajić, Amal Rannen-Triki, Cristina Vasconcelos, Aida Nematzadeh",
      "summary": "## 图像生成多样性基准测试框架\n\n### 引言\n尽管文本到图像（T2I）模型的生成质量取得了显著进步，但当前模型常常缺乏多样性，倾向于生成同质化的输出。本文旨在解决T2I模型中对鲁棒多样性评估的需求，并为此引入了一个新的框架。\n\n### 框架概述\n该框架通过系统地评估个体概念及其相关的变异因素来评估多样性。例如，对于提示“一张苹果的图片”，其变异因素可以是“颜色”。\n\n### 主要贡献\n本研究的核心贡献包括：\n\n1.  **新颖的人工评估模板**：设计了一个创新的人工评估模板，用于对生成图像的多样性进行细致入微的评估。\n2.  **精选的提示集**：构建了一个精心策划的提示集，涵盖了多样化的概念及其已识别的变异因素。这有助于系统地探索模型在不同属性上的多样性表现。\n3.  **模型比较方法**：提出了一种通过二项式检验，根据人工标注来比较不同T2I模型多样性表现的方法。\n\n### 附加研究与成果\n*   **图像嵌入比较**：研究还严格比较了多种图像嵌入在多样性测量方面的表现。\n*   **模型排名与问题识别**：本文提出的原则性方法能够根据多样性对T2I模型进行排名，并识别出模型在哪些特定类别或属性上表现不足。\n\n### 结论与展望\n这项研究提供了一套稳健的方法和深刻的见解，为未来T2I模型多样性的改进以及相关评估指标的开发铺平了道路。",
      "shortSummary": "本文提出一个框架，旨在解决文本到图像（T2I）模型生成输出缺乏多样性的问题。该框架通过评估个体概念及其变异因素，系统地评估多样性。主要贡献包括一个新颖的人工评估模板、一个包含变异因素的提示集以及通过二项式检验比较模型的方法。该研究能够对T2I模型的多样性进行排名，并识别其不足之处，为T2I模型多样性的改进提供了稳健的方法和见解。",
      "translated_title": "通过属性条件人工评估对图像生成多样性进行基准测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite advances in generation quality, current text-to-image (T2I) models often lack diversity, generating homogeneous outputs. This work introduces a framework to address the need for robust diversity evaluation in T2I models. Our framework systematically assesses diversity by evaluating individual concepts and their relevant factors of variation. Key contributions include: (1) a novel human evaluation template for nuanced diversity assessment; (2) a curated prompt set covering diverse concepts with their identified factors of variation (e.g. prompt: An image of an apple, factor of variation: color); and (3) a methodology for comparing models in terms of human annotations via binomial tests.   Furthermore, we rigorously compare various image embeddings for diversity measurement. Notably, our principled approach enables ranking of T2I models by diversity, identifying categories where they particularly struggle. This research offers a robust methodology and insights, paving the way for improvements in T2I model diversity and metric development."
    },
    {
      "title": "基于评分标准的基准测试与强化学习，以提升大语言模型指令遵循能力 (原标题: Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following)",
      "link": "https://arxiv.org/abs/2511.10507",
      "pubDate": "Thu, 13 Nov 2025 12:14:01 GMT",
      "isoDate": "2025-11-13T12:14:01.000Z",
      "creator": "Yun He, Wenzhe Li, Hejia Zhang, Songlin Li, Karishma Mandyam, Sopan Khosla, Yuanhao Xiong, Nanshu Wang, Selina Peng, Beibin Li, Shengjie Bi, Shishir G. Patil, Qi Qi, Shengyu Feng, Julian Katz-Samuels, Richard Yuanzhe Pang, Sujan Gonugondla, Hunter Lang, Yue Yu, Yundi Qian, Maryam Fazel-Zarandi, Licheng Yu, Amine Benhalloum, Hany Awadalla, Manaal Faruqui",
      "summary": "本文介绍了在提升大语言模型（LLMs）指令遵循（IF）能力方面的最新研究，特别关注解决复杂、多轮和系统提示指令的挑战。研究人员指出，当前缺乏高质量、人工标注的基准和可靠、可解释的奖励信号，阻碍了对LLM高级指令遵循能力的严格评估和有效训练。\n\n**主要贡献与方法：**\n\n*   **AdvancedIF 基准测试：**\n    *   引入了一个全面的基准测试，包含超过1,600个提示。\n    *   该基准测试采用专家精心策划的评分标准（rubrics），旨在评估LLM遵循复杂、多轮和系统级指令的能力。\n    *   AdvancedIF 基准测试即将发布，将为研究社区提供宝贵的资源。\n\n*   **RIFL (Rubric-based Instruction-Following Learning) 管道：**\n    *   提出了一种新颖的后训练（post-training）流程，名为RIFL。\n    *   RIFL 利用评分标准生成、一个经过微调的评分标准验证器以及奖励塑形（reward shaping）技术，以实现指令遵循的有效强化学习。\n\n**实验结果与意义：**\n\n*   **显著性能提升：** 广泛的实验证明，RIFL 显著提升了LLM的指令遵循能力，在AdvancedIF 基准测试上实现了6.7%的绝对增益。\n*   **公共基准表现：** RIFL 在公共基准测试上也取得了优异的结果。\n*   **组件有效性：** 消融研究（ablation studies）证实了RIFL中每个组件的有效性。\n*   **未来展望：** 这项工作确立了评分标准作为训练和评估LLM高级指令遵循能力的强大工具，为开发更强大、更可靠的AI系统铺平了道路。",
      "shortSummary": "该研究针对大语言模型（LLMs）在复杂指令遵循方面的挑战，引入了AdvancedIF基准测试和RIFL（基于评分标准的指令遵循学习）后训练流程。AdvancedIF包含1600多个提示和专家评分标准。RIFL利用评分标准生成、验证器和奖励塑形进行强化学习。实验表明，RIFL显著提升了LLM的指令遵循能力，在AdvancedIF上实现了6.7%的绝对增益，并在公共基准上表现出色。该工作强调了评分标准在提升LLM指令遵循能力中的关键作用。",
      "translated_title": "基于评分标准的基准测试与强化学习，以提升大语言模型指令遵循能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent progress in large language models (LLMs) has led to impressive performance on a range of tasks, yet advanced instruction following (IF)-especially for complex, multi-turn, and system-prompted instructions-remains a significant challenge. Rigorous evaluation and effective training for such capabilities are hindered by the lack of high-quality, human-annotated benchmarks and reliable, interpretable reward signals. In this work, we introduce AdvancedIF (we will release this benchmark soon), a comprehensive benchmark featuring over 1,600 prompts and expert-curated rubrics that assess LLMs ability to follow complex, multi-turn, and system-level instructions. We further propose RIFL (Rubric-based Instruction-Following Learning), a novel post-training pipeline that leverages rubric generation, a finetuned rubric verifier, and reward shaping to enable effective reinforcement learning for instruction following. Extensive experiments demonstrate that RIFL substantially improves the instruction-following abilities of LLMs, achieving a 6.7% absolute gain on AdvancedIF and strong results on public benchmarks. Our ablation studies confirm the effectiveness of each component in RIFL. This work establishes rubrics as a powerful tool for both training and evaluating advanced IF in LLMs, paving the way for more capable and reliable AI systems."
    },
    {
      "title": "音乐火烈鸟：在音频语言模型中扩展音乐理解 (原标题: Music Flamingo: Scaling Music Understanding in Audio Language Models)",
      "link": "https://arxiv.org/abs/2511.10289",
      "pubDate": "Thu, 13 Nov 2025 08:21:09 GMT",
      "isoDate": "2025-11-13T08:21:09.000Z",
      "creator": "Sreyan Ghosh, Arushi Goel, Lasha Koroshinadze, Sang-gil Lee, Zhifeng Kong, Joao Felipe Santos, Ramani Duraiswami, Dinesh Manocha, Wei Ping, Mohammad Shoeybi, Bryan Catanzaro",
      "summary": "# 音乐火烈鸟：在音频语言模型中扩展音乐理解\n\n本文介绍了“音乐火烈鸟”（Music Flamingo），这是一种新型的大型音频语言模型，旨在提升基础音频模型对音乐（包括歌曲）的理解能力。\n\n## 背景与挑战\n尽管音频语言研究取得了快速进展，但音乐理解仍面临巨大挑战。这主要归因于音乐的动态性、层次性、信息密集性，以及高质量音乐数据和标注的稀缺性，这些因素限制了开放音频理解模型的扩展。现有模型通常只能生成简短、高层次的描述，回答表面问题，并且在不同音乐文化间的泛化能力有限。\n\n## 核心创新与方法\n为了解决上述挑战，Music Flamingo项目引入了多项创新：\n\n### 1. MF-Skills 大规模数据集\n*   **目的：** 应对高质量音乐数据和标注稀缺的问题。\n*   **构建：** 通过多阶段管道精心策划，生成了丰富详细的描述（captions）和问答对（Q&A pairs）。\n*   **内容覆盖：** 涵盖了和声、结构、音色、歌词和文化背景等多个方面，提供了深层次的音乐信息。\n\n### 2. 增强型 Audio Flamingo 3 主干模型\n*   Music Flamingo 基于增强的 Audio Flamingo 3 主干模型进行微调，并进一步强化了多项与音乐理解相关的技能。\n\n### 3. 提升推理能力的后训练方案\n为了增强模型的推理能力，研究团队引入了一种独特的后训练方案：\n*   **MF-Think 冷启动：** 首先使用 MF-Think 进行冷启动，这是一个新颖的思维链（chain-of-thought）数据集，其内容植根于音乐理论。\n*   **GRPO 强化学习：** 随后，通过基于 GRPO（Generalized Policy Optimization）的强化学习，并结合自定义奖励机制，进一步优化模型。\n\n## 成果与意义\n*   **卓越性能：** Music Flamingo 在超过10个音乐理解和推理基准测试中取得了最先进（state-of-the-art）的结果。\n*   **通用智能：** 它确立了自身作为一个通用且具有音乐智能的音频语言模型的地位。\n*   **新标准：** 该模型展示了如何将音乐理解从表面识别提升到对歌曲进行分层、类人感知的水平，为高级音乐理解设定了新标准。\n*   **社区贡献：** 这项工作为社区构建下一代能够像人类一样有意义地理解音乐的模型提供了基准和基础。",
      "shortSummary": "Music Flamingo 是一种新型大型音频语言模型，旨在解决现有模型在音乐理解方面的局限性。它通过构建大规模的MF-Skills数据集，并引入基于音乐理论的MF-Think思维链数据集和GRPO强化学习后训练方案，显著提升了模型对音乐和声、结构、音色、歌词及文化背景的深层理解与推理能力。Music Flamingo在多项基准测试中取得了最先进成果，实现了类人般的音乐感知，为未来音频语言模型发展奠定了基础。",
      "translated_title": "音乐火烈鸟：在音频语言模型中扩展音乐理解",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Music Flamingo, a novel large audio-language model designed to advance music (including song) understanding in foundational audio models. While audio-language research has progressed rapidly, music remains challenging due to its dynamic, layered, and information-dense nature. Progress has been further limited by the difficulty of scaling open audio understanding models, primarily because of the scarcity of high-quality music data and annotations. As a result, prior models are restricted to producing short, high-level captions, answering only surface-level questions, and showing limited generalization across diverse musical cultures. To address these challenges, we curate MF-Skills, a large-scale dataset labeled through a multi-stage pipeline that yields rich captions and question-answer pairs covering harmony, structure, timbre, lyrics, and cultural context. We fine-tune an enhanced Audio Flamingo 3 backbone on MF-Skills and further strengthen multiple skills relevant to music understanding. To improve the model's reasoning abilities, we introduce a post-training recipe: we first cold-start with MF-Think, a novel chain-of-thought dataset grounded in music theory, followed by GRPO-based reinforcement learning with custom rewards. Music Flamingo achieves state-of-the-art results across 10+ benchmarks for music understanding and reasoning, establishing itself as a generalist and musically intelligent audio-language model. Beyond strong empirical results, Music Flamingo sets a new standard for advanced music understanding by demonstrating how models can move from surface-level recognition toward layered, human-like perception of songs. We believe this work provides both a benchmark and a foundation for the community to build the next generation of models that engage with music as meaningfully as humans do."
    },
    {
      "title": "MuSc-V2：基于无标签样本互评分的零样本多模态工业异常分类与分割 (原标题: MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples)",
      "link": "https://arxiv.org/abs/2511.10047",
      "pubDate": "Thu, 13 Nov 2025 02:47:37 GMT",
      "isoDate": "2025-11-13T02:47:37.000Z",
      "creator": "Xurui Li, Feng Xue, Yu Zhou",
      "summary": "## MuSc-V2：零样本多模态工业异常分类与分割\n\n### 背景与挑战\n\n*   零样本异常分类（AC）和分割（AS）方法旨在无需任何标记样本的情况下识别和描绘缺陷。\n*   现有方法普遍忽略了一个关键特性：工业产品中的正常图像块在2D外观和3D形状上通常能找到许多其他相似的块，而异常则表现出多样性和孤立性。\n\n### MuSc-V2 框架概述\n\n*   为了明确利用上述判别性特性，本文提出了一个名为 **MuSc-V2（互评分框架）** 的零样本AC/AS框架。\n*   MuSc-V2 灵活支持单一2D/3D或多模态数据处理。\n\n### 核心组件与机制\n\n1.  **迭代点分组（Iterative Point Grouping, IPG）**\n    *   首先通过改进3D表示，有效减少由不连续表面引起的假阳性。\n2.  **多度相似性邻域聚合（Similarity Neighborhood Aggregation with Multi-Degrees, SNAMD）**\n    *   融合2D/3D邻域线索，生成更具判别性的多尺度图像块特征，为后续的互评分奠定基础。\n3.  **互评分机制（Mutual Scoring Mechanism, MSM）**\n    *   这是框架的核心，允许每种模态内的样本相互分配分数，以识别异常。\n4.  **跨模态异常增强（Cross-modal Anomaly Enhancement, CAE）**\n    *   融合2D和3D模态的分数，以恢复特定模态中可能缺失的异常信息，提高检测的完整性。\n5.  **约束邻域重评分（Re-scoring with Constrained Neighborhood, RsCon）**\n    *   基于与更具代表性样本的相似性，对分类结果进行重评分，从而有效抑制假分类（假阳性）。\n\n### 性能与优势\n\n*   该框架在完整数据集和较小的数据子集上均能灵活工作，并保持一致的鲁棒性能，确保了在不同产品线上的无缝适应性。\n*   MuSc-V2 取得了显著的性能提升：\n    *   在 **MVTec 3D-AD** 数据集上，AP（平均精度）提升了 **+23.7%**。\n    *   在 **Eyecandies** 数据集上，性能提升了 **+19.3%**。\n*   这些结果超越了先前的零样本基准，甚至优于大多数少样本方法。\n\n### 代码可用性\n\n*   相关代码将公开发布。",
      "shortSummary": "MuSc-V2提出了一种零样本多模态工业异常分类与分割框架，利用正常图像块相似而异常孤立的关键特性。该框架通过迭代点分组（IPG）改进3D表示，使用多度相似性邻域聚合（SNAMD）融合多尺度特征，并通过互评分机制（MSM）和跨模态异常增强（CAE）进行异常评分。最终，约束邻域重评分（RsCon）抑制假阳性。MuSc-V2在MVTec 3D-AD和Eyecandies数据集上分别实现了+23.7%和+19.3%的AP提升，超越了现有零样本甚至多数少样本方法。",
      "translated_title": "MuSc-V2：基于无标签样本互评分的零样本多模态工业异常分类与分割",
      "images": [],
      "contentSource": "完整文章",
      "content": "Zero-shot anomaly classification (AC) and segmentation (AS) methods aim to identify and outline defects without using any labeled samples. In this paper, we reveal a key property that is overlooked by existing methods: normal image patches across industrial products typically find many other similar patches, not only in 2D appearance but also in 3D shapes, while anomalies remain diverse and isolated. To explicitly leverage this discriminative property, we propose a Mutual Scoring framework (MuSc-V2) for zero-shot AC/AS, which flexibly supports single 2D/3D or multimodality. Specifically, our method begins by improving 3D representation through Iterative Point Grouping (IPG), which reduces false positives from discontinuous surfaces. Then we use Similarity Neighborhood Aggregation with Multi-Degrees (SNAMD) to fuse 2D/3D neighborhood cues into more discriminative multi-scale patch features for mutual scoring. The core comprises a Mutual Scoring Mechanism (MSM) that lets samples within each modality to assign score to each other, and Cross-modal Anomaly Enhancement (CAE) that fuses 2D and 3D scores to recover modality-specific missing anomalies. Finally, Re-scoring with Constrained Neighborhood (RsCon) suppresses false classification based on similarity to more representative samples. Our framework flexibly works on both the full dataset and smaller subsets with consistently robust performance, ensuring seamless adaptability across diverse product lines. In aid of the novel framework, MuSc-V2 achieves significant performance improvements: a +23.7% AP gain on the MVTec 3D-AD dataset and a +19.3% boost on the Eyecandies dataset, surpassing previous zero-shot benchmarks and even outperforming most few-shot methods. The code will be available at The code will be available at https://github.com/HUST-SLOW/MuSc-V2{https://github.com/HUST-SLOW/MuSc-V2}."
    },
    {
      "title": "AffordBot：通过多模态大型语言模型实现3D细粒度具身推理 (原标题: AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models)",
      "link": "https://arxiv.org/abs/2511.10017",
      "pubDate": "Thu, 13 Nov 2025 01:43:00 GMT",
      "isoDate": "2025-11-13T01:43:00.000Z",
      "creator": "Xinyi Wang, Xun Yang, Yanlong Xu, Yuchen Wu, Zhen Li, Na Zhao",
      "summary": "# AffordBot：通过多模态大型语言模型实现3D细粒度具身推理\n\n本文介绍了一种名为AffordBot的新型框架，旨在解决物理环境中人机协作中的细粒度3D具身推理问题。该框架利用多模态大型语言模型（MLLMs）和定制的思维链（CoT）推理范式，以实现对3D场景中可操作元素的深入理解和交互预测。\n\n## 1. 研究背景与问题\n\n*   **挑战：** 在物理环境中，有效的人机协作不仅需要理解“作用于什么”，还需要知道“可操作元素在哪里”以及“如何与它们互动”。这要求智能体具备对环境的细致理解和推理能力。\n*   **现有方法局限：** 当前的方法通常在对象层面操作，或将细粒度可供性（affordance）推理割裂处理。它们缺乏连贯的、指令驱动的接地（grounding）能力，也无法进行细粒度的推理，导致在复杂交互场景中表现不足。\n\n## 2. 新任务：细粒度3D具身推理\n\n*   **定义：** 本文提出了一项新任务——细粒度3D具身推理。该任务要求智能体根据给定的任务指令，为3D场景中每个被引用的可供性元素预测一个结构化三元组。\n*   **三元组构成：** 这个三元组包含以下三个关键信息：\n    *   **空间位置（spatial location）：** 元素在3D空间中的精确位置。\n    *   **运动类型（motion type）：** 与该元素相关的可能交互运动类型（例如，推、拉、旋转）。\n    *   **运动轴（motion axis）：** 如果存在运动，其发生的具体轴向。\n\n## 3. 解决方案：AffordBot 框架\n\n*   **核心：** AffordBot是一个新颖的框架，它通过将多模态大型语言模型（MLLMs）与量身定制的思维链（CoT）推理范式相结合来解决上述任务。\n*   **创新点：** 该框架旨在弥合3D感知与2D兼容的MLLMs之间的差距，并利用CoT的逐步推理能力来提高预测的准确性和物理合理性。\n\n## 4. AffordBot 的方法论\n\n*   **3D到2D的桥接：**\n    *   为了使2D兼容的MLLMs能够处理3D场景信息，AffordBot首先渲染场景的环绕视图图像。\n    *   然后，将3D场景中的元素候选对象投影到这些2D视图中，从而形成与场景几何结构对齐的丰富视觉表示。这种方法有效地将3D空间信息编码到MLLMs可以理解的2D图像格式中。\n*   **思维链（CoT）推理流程：** AffordBot的CoT管道包含两个主要阶段：\n    *   **主动感知阶段：** 在推理开始时，系统会提示MLLM根据任务指令选择最具信息量的视点。这有助于聚焦于与任务最相关的场景区域。\n    *   **逐步推理：** 选定视点后，MLLM会进行一步步的推理，首先定位可供性元素，然后推断出与这些元素相关的合理交互动作及其运动参数。\n\n## 5. 实验评估与成果\n\n*   **数据集：** AffordBot在SceneFun3D数据集上进行了评估。\n*   **性能：** 实验结果表明，AffordBot取得了最先进（state-of-the-art）的性能。\n*   **关键优势：**\n    *   展示了强大的泛化能力，能够适应不同的场景和任务。\n    *   实现了物理上接地（physically grounded）的推理，即预测的交互动作符合物理世界的规律。\n    *   值得注意的是，AffordBot仅使用3D点云输入和MLLMs就达到了这些成果，证明了其方法的有效性和效率。\n\n## 6. 其他信息\n\n*   **发布：** 本研究成果已被NeurIPS 2025接收。\n*   **预印本：** 论文可在arXiv上查阅，编号为arXiv:2511.10017 [cs.CV]。",
      "shortSummary": "AffordBot提出了一种利用多模态大型语言模型（MLLMs）和思维链（CoT）推理范式，解决3D细粒度具身推理的新框架。该框架旨在提升人机协作中对物理环境的理解，通过将3D场景渲染为2D图像并投影3D元素，使MLLMs能够预测可供性元素的空间位置、运动类型和运动轴。AffordBot在SceneFun3D数据集上取得了最先进的性能，展现出强大的泛化能力和物理接地推理，仅需3D点云输入和MLLMs。",
      "translated_title": "AffordBot：通过多模态大型语言模型实现3D细粒度具身推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Effective human-agent collaboration in physical environments requires understanding not only what to act upon, but also where the actionable elements are and how to interact with them. Existing approaches often operate at the object level or disjointedly handle fine-grained affordance reasoning, lacking coherent, instruction-driven grounding and reasoning. In this work, we introduce a new task: Fine-grained 3D Embodied Reasoning, which requires an agent to predict, for each referenced affordance element in a 3D scene, a structured triplet comprising its spatial location, motion type, and motion axis, based on a task instruction. To solve this task, we propose AffordBot, a novel framework that integrates Multimodal Large Language Models (MLLMs) with a tailored chain-of-thought (CoT) reasoning paradigm. To bridge the gap between 3D input and 2D-compatible MLLMs, we render surround-view images of the scene and project 3D element candidates into these views, forming a rich visual representation aligned with the scene geometry. Our CoT pipeline begins with an active perception stage, prompting the MLLM to select the most informative viewpoint based on the instruction, before proceeding with step-by-step reasoning to localize affordance elements and infer plausible interaction motions. Evaluated on the SceneFun3D dataset, AffordBot achieves state-of-the-art performance, demonstrating strong generalization and physically grounded reasoning with only 3D point cloud input and MLLMs."
    },
    {
      "title": "向窃贼致敬：探索去中心化GRPO中的攻击与防御 (原标题: Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO)",
      "link": "https://arxiv.org/abs/2511.09780",
      "pubDate": "Wed, 12 Nov 2025 17:29:07 GMT",
      "isoDate": "2025-11-12T17:29:07.000Z",
      "creator": "Nikolay Blagoev, Oğuzhan Ersoy, Lydia Yiyu Chen",
      "summary": "## 去中心化GRPO中的攻击与防御\n\n### 引言\n\n群组相对策略优化（Group Relative Policy Optimization, GRPO）在大型语言模型（LLM）的后训练中展现出巨大的应用潜力。在GRPO框架下，模型对提示（prompts）进行回答，并通过强化学习机制学习并采纳更受偏好的完成方式。由于其通信量小，GRPO天然适用于去中心化训练环境，允许多个节点并发地回答提示，并以字符串形式交换信息。\n\n### 首次对抗性攻击\n\n本研究首次提出了针对去中心化GRPO系统的对抗性攻击。研究表明，恶意方能够通过在良性模型中注入任意恶意令牌来毒害此类系统。这些攻击可以分为两种主要形式：\n\n*   **上下文外攻击（Out-of-context attacks）**：在与当前任务无关的上下文中注入恶意内容。\n*   **上下文内攻击（In-context attacks）**：在与当前任务相关的上下文中注入恶意内容。\n\n### 攻击效果与实证\n\n通过对数学和编码任务的实证测试，研究者展示了对抗性攻击能够轻易地毒害去中心化GRPO中的良性节点，从而污染其本地LLM的后训练过程。攻击效果显著，在短短50次迭代中，攻击成功率即可高达100%。\n\n### 防御机制\n\n为应对这些攻击，文章提出了两种防御策略。这些防御方法的选择取决于具体的训练场景，即所有用户是训练相同的模型还是不同的模型。研究结果表明，这些防御措施能够实现高达100%的阻止率，从而有效地使此类攻击变得不可能。",
      "shortSummary": "本文首次揭示了针对去中心化群组相对策略优化（GRPO）的对抗性攻击。GRPO在大型语言模型后训练中应用广泛，其去中心化特性使其易受恶意方通过注入恶意令牌进行毒害。研究通过数学和编码任务证实，攻击成功率可达100%。为应对此威胁，文章提出了两种防御策略，可有效阻止攻击，阻止率最高达100%，从而使攻击失效。",
      "translated_title": "向窃贼致敬：探索去中心化GRPO中的攻击与防御",
      "images": [],
      "contentSource": "完整文章",
      "content": "Group Relative Policy Optimization (GRPO) has demonstrated great utilization in post-training of Large Language Models (LLMs). In GRPO, prompts are answered by the model and, through reinforcement learning, preferred completions are learnt. Owing to the small communication volume, GRPO is inherently suitable for decentralised training as the prompts can be concurrently answered by multiple nodes and then exchanged in the forms of strings. In this work, we present the first adversarial attack in decentralised GRPO. We demonstrate that malicious parties can poison such systems by injecting arbitrary malicious tokens in benign models in both out-of-context and in-context attacks. Using empirical examples of math and coding tasks, we show that adversarial attacks can easily poison the benign nodes, polluting their local LLM post-training, achieving attack success rates up to 100% in as few as 50 iterations. We propose two ways to defend against these attacks, depending on whether all users train the same model or different models. We show that these defenses can achieve stop rates of up to 100%, making the attack impossible."
    },
    {
      "title": "SliderEdit：基于细粒度指令控制的连续图像编辑 (原标题: SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control)",
      "link": "https://arxiv.org/abs/2511.09715",
      "pubDate": "Wed, 12 Nov 2025 15:21:37 GMT",
      "isoDate": "2025-11-12T15:21:37.000Z",
      "creator": "Arman Zarei, Samyadeep Basu, Mobina Pournemat, Sayan Nag, Ryan Rossi, Soheil Feizi",
      "summary": "### SliderEdit：基于细粒度指令控制的连续图像编辑\n\n本文介绍了SliderEdit，一个用于实现细粒度、可解释指令控制的连续图像编辑框架。\n\n*   **现有问题：**\n    *   当前基于指令的图像编辑模型虽然在处理复杂编辑方面表现出色，但它们以固定强度应用提示中的每条指令。\n    *   这限制了用户精确且连续地控制单个编辑强度的能力。\n\n*   **SliderEdit框架介绍：**\n    *   SliderEdit旨在解决上述问题，提供一种连续图像编辑方法，具有细粒度、可解释的指令控制。\n    *   给定一个多部分编辑指令，SliderEdit能够解耦（disentangle）各个独立指令。\n    *   然后，它将每条指令作为一个全局训练的滑块（slider）暴露出来，允许用户平滑地调整其强度。\n\n*   **核心创新与优势：**\n    *   与以往在文本到图像生成中引入基于滑块的属性控制方法不同（这些方法通常需要为每个属性或概念进行单独训练或微调）。\n    *   SliderEdit学习了一组单一的低秩适应矩阵（low-rank adaptation matrices），这些矩阵能够泛化到各种不同的编辑、属性和组合指令。\n    *   这使得沿单个编辑维度进行连续插值成为可能，同时保持空间局部性和全局语义一致性。\n\n*   **应用与成果：**\n    *   SliderEdit已应用于最先进的图像编辑模型，包括FLUX-Kontext和Qwen-Image-Edit。\n    *   实验结果表明，在编辑可控性、视觉一致性和用户可引导性方面均有显著改进。\n\n*   **研究意义：**\n    *   据作者所知，SliderEdit是首个探索并提出在基于指令的图像编辑模型中实现连续、细粒度指令控制的框架。\n    *   其研究成果为实现具有连续和组合控制的交互式、指令驱动图像操作铺平了道路。",
      "shortSummary": "SliderEdit是一个创新的图像编辑框架，解决了现有指令编辑模型缺乏细粒度控制的问题。它能将多部分指令解耦为可调节强度的滑块，并学习一组泛化性强的低秩适应矩阵，实现对各种编辑的连续、精确控制。应用于FLUX-Kontext和Qwen-Image-Edit等模型后，显著提升了编辑可控性、视觉一致性和用户引导性，为交互式图像操作开辟了新途径。",
      "translated_title": "SliderEdit：基于细粒度指令控制的连续图像编辑",
      "images": [],
      "contentSource": "完整文章",
      "content": "Instruction-based image editing models have recently achieved impressive performance, enabling complex edits to an input image from a multi-instruction prompt. However, these models apply each instruction in the prompt with a fixed strength, limiting the user's ability to precisely and continuously control the intensity of individual edits. We introduce SliderEdit, a framework for continuous image editing with fine-grained, interpretable instruction control. Given a multi-part edit instruction, SliderEdit disentangles the individual instructions and exposes each as a globally trained slider, allowing smooth adjustment of its strength. Unlike prior works that introduced slider-based attribute controls in text-to-image generation, typically requiring separate training or fine-tuning for each attribute or concept, our method learns a single set of low-rank adaptation matrices that generalize across diverse edits, attributes, and compositional instructions. This enables continuous interpolation along individual edit dimensions while preserving both spatial locality and global semantic consistency. We apply SliderEdit to state-of-the-art image editing models, including FLUX-Kontext and Qwen-Image-Edit, and observe substantial improvements in edit controllability, visual consistency, and user steerability. To the best of our knowledge, we are the first to explore and propose a framework for continuous, fine-grained instruction control in instruction-based image editing models. Our results pave the way for interactive, instruction-driven image manipulation with continuous and compositional control."
    },
    {
      "title": "WMPO: World Model-based Policy Optimization for Vision-Language-Action Models",
      "link": "https://arxiv.org/abs/2511.09515",
      "pubDate": "Wed, 12 Nov 2025 12:54:09 GMT",
      "isoDate": "2025-11-12T12:54:09.000Z",
      "creator": "Fangqi Zhu, Zhengyang Yan, Zicong Hong, Quanxin Shou, Xiao Ma, Song Guo",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "WMPO: World Model-based Policy Optimization for Vision-Language-Action Models",
      "images": [],
      "contentSource": "RSS",
      "content": "Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the \"imagined\" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities."
    },
    {
      "title": "LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls",
      "link": "https://arxiv.org/abs/2511.09148",
      "pubDate": "Wed, 12 Nov 2025 04:34:39 GMT",
      "isoDate": "2025-11-12T04:34:39.000Z",
      "creator": "Kangning Zhang, Wenxiang Jiao, Kounianhua Du, Yuan Lu, Weiwen Liu, Weinan Zhang, Lei Zhang, Yong Yu",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls",
      "images": [],
      "contentSource": "RSS",
      "content": "Augmenting Large Language Models (LLMs) with external tools enables them to execute complex, multi-step tasks. However, tool learning is hampered by the static synthetic data pipelines where data generation and model training are executed as two separate, non-interactive processes. This approach fails to adaptively focus on a model's specific weaknesses and allows noisy labels to persist, degrading training efficiency. We introduce LoopTool, a fully automated, model-aware data evolution framework that closes this loop by tightly integrating data synthesis and model training. LoopTool iteratively refines both the data and the model through three synergistic modules: (1) Greedy Capability Probing (GCP) diagnoses the model's mastered and failed capabilities; (2) Judgement-Guided Label Verification (JGLV) uses an open-source judge model to find and correct annotation errors, progressively purifying the dataset; and (3) Error-Driven Data Expansion (EDDE) generates new, challenging samples based on identified failures. This closed-loop process operates within a cost-effective, open-source ecosystem, eliminating dependence on expensive closed-source APIs. Experiments show that our 8B model trained with LoopTool significantly surpasses its 32B data generator and achieves new state-of-the-art results on the BFCL-v3 and ACEBench benchmarks for its scale. Our work demonstrates that closed-loop, self-refining data pipelines can dramatically enhance the tool-use capabilities of LLMs."
    },
    {
      "title": "MM-CRITIC：大型多模态模型作为多模态批判能力的全面评估 (原标题: MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique)",
      "link": "https://arxiv.org/abs/2511.09067",
      "pubDate": "Wed, 12 Nov 2025 02:43:26 GMT",
      "isoDate": "2025-11-12T02:43:26.000Z",
      "creator": "Gailun Zeng, Ziyang Luo, Hongzhan Lin, Yuchen Tian, Kaixin Li, Ziyang Gong, Jianxiong Guo, Jing Ma",
      "summary": "### MM-CRITIC：大型多模态模型多模态批判能力的全面评估\n\n本文介绍了MM-CRITIC，一个用于全面评估大型多模态模型（LMMs）多模态批判能力的基准。\n\n#### 1. 研究背景与动机\n*   **批判能力的重要性**：批判能力对于模型进行自我改进并成为可靠的AI助手至关重要。\n*   **现有研究的局限性**：尽管在纯语言环境中对批判能力进行了广泛研究，但针对LMMs的多模态批判能力评估仍未得到充分探索，尽管LMMs在图像标注和视觉推理等任务中展现出日益增长的能力。\n\n#### 2. MM-CRITIC基准介绍\n*   **目标**：解决LMMs多模态批判能力评估不足的问题，提供一个全面的评估框架。\n*   **评估维度**：MM-CRITIC从多个维度评估LMMs的批判能力，包括：\n    *   **基础批判 (basic critique)**\n    *   **纠正批判 (correction critique)**\n    *   **比较批判 (comparison critique)**\n*   **任务覆盖**：\n    *   涵盖8种主要任务类型。\n    *   包含超过500个具体任务。\n    *   由4471个样本组成。\n*   **数据收集**：收集了来自不同模型规模的各种LMMs的响应。\n\n#### 3. 评估方法与可靠性增强\n*   **专家指导**：将专家提供的“地面真相”答案整合到评分标准中。\n*   **GPT-4o辅助**：利用GPT-4o根据评分标准对模型响应进行标注，并生成参考批判（reference critiques）。\n*   **可信判断**：这些参考批判作为可信判断的锚点，显著增强了评估的可靠性。\n\n#### 4. 实验结果与关键发现\n*   **基准有效性**：广泛的实验验证了MM-CRITIC的有效性。\n*   **LMMs能力评估**：对领先LMMs在多个维度上的批判能力进行了全面评估。\n*   **关键洞察**：进一步分析揭示了一些重要见解，包括：\n    *   响应质量与批判能力之间的相关性。\n    *   不同评估维度下批判任务难度的差异。\n\n#### 5. 资源可用性\n*   项目代码已公开，可在[此链接](this https URL)获取。",
      "shortSummary": "本文介绍了MM-CRITIC，一个用于全面评估大型多模态模型（LMMs）多模态批判能力的新基准。鉴于批判能力对AI模型自我改进的重要性，MM-CRITIC弥补了现有研究在多模态批判领域的不足。它涵盖基础、纠正和比较三个维度，包含8种任务类型、500多个任务和4471个样本。通过整合专家知识和利用GPT-4o生成参考批判，MM-CRITIC确保了评估的可靠性。实验验证了其有效性，并揭示了LMMs批判能力的综合表现及关键洞察。",
      "translated_title": "MM-CRITIC：大型多模态模型作为多模态批判能力的全面评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "The ability of critique is vital for models to self-improve and serve as reliable AI assistants. While extensively studied in language-only settings, multimodal critique of Large Multimodal Models (LMMs) remains underexplored despite their growing capabilities in tasks like captioning and visual reasoning. In this work, we introduce MM-CRITIC, a holistic benchmark for evaluating the critique ability of LMMs across multiple dimensions: basic, correction, and comparison. Covering 8 main task types and over 500 tasks, MM-CRITIC collects responses from various LMMs with different model sizes and is composed of 4471 samples. To enhance the evaluation reliability, we integrate expert-informed ground answers into scoring rubrics that guide GPT-4o in annotating responses and generating reference critiques, which serve as anchors for trustworthy judgments. Extensive experiments validate the effectiveness of MM-CRITIC and provide a comprehensive assessment of leading LMMs' critique capabilities under multiple dimensions. Further analysis reveals some key insights, including the correlation between response quality and critique, and varying critique difficulty across evaluation dimensions. Our code is available at https://github.com/MichealZeng0420/MM-Critic."
    },
    {
      "title": "PAN：一个用于通用、可交互和长周期世界模拟的世界模型 (原标题: PAN: A World Model for General, Interactable, and Long-Horizon World Simulation)",
      "link": "https://arxiv.org/abs/2511.09057",
      "pubDate": "Wed, 12 Nov 2025 02:20:35 GMT",
      "isoDate": "2025-11-12T02:20:35.000Z",
      "creator": "PAN Team, Jiannan Xiang, Yi Gu, Zihan Liu, Zeyu Feng, Qiyue Gao, Yiyan Hu, Benhao Huang, Guangyi Liu, Yichi Yang, Kun Zhou, Davit Abrahamyan, Arif Ahmad, Ganesh Bannur, Junrong Chen, Kimi Chen, Mingkai Deng, Ruobing Han, Xinqi Huang, Haoqiang Kang, Zheqi Li, Enze Ma, Hector Ren, Yashowardhan Shinde, Rohan Shingre, Ramsundar Tanikella, Kaiming Tao, Dequan Yang, Xinle Yu, Cong Zeng, Binglin Zhou, Zhengzhong Liu, Zhiting Hu, Eric P. Xing",
      "summary": "## PAN：通用、可交互和长周期世界模拟的世界模型\n\n### 1. 研究背景与挑战\n\n*   **现有视频生成模型局限性**：尽管能生成逼真的视觉序列，但通常采用“提示到完整视频”的方式，缺乏因果控制、交互性以及实现有目的推理所需的长期一致性。\n*   **现有世界模型局限性**：\n    *   常专注于受限领域（如物理、游戏或3D场景动态）。\n    *   深度和可控性有限。\n    *   难以泛化到多样化的环境和交互形式。\n\n### 2. PAN模型介绍\n\n*   **核心目标**：引入PAN，一个通用、可交互且支持长周期模拟的世界模型。\n*   **功能**：通过高质量的视频模拟来预测未来的世界状态，模拟过程以历史信息和自然语言动作作为条件。\n\n### 3. PAN的架构：生成式潜在预测（GLP）\n\nPAN采用生成式潜在预测（Generative Latent Prediction, GLP）架构，其主要组成部分包括：\n\n*   **自回归潜在动态骨干网络**：\n    *   基于大型语言模型（LLM）。\n    *   将模拟过程建立在广泛的文本知识基础上。\n    *   能够根据语言指定的动作进行条件控制。\n*   **视频扩散解码器**：\n    *   重建感知细节丰富且时间连贯的视觉观测。\n*   **统一目标**：通过结合上述两部分，PAN旨在实现潜在空间推理（想象）与可实现的世界动态（现实）的统一。\n\n### 4. 训练与能力\n\n*   **训练数据**：在跨越不同领域的大规模视频-动作对数据集上进行训练。\n*   **核心能力**：\n    *   支持开放域的、以动作为条件的模拟。\n    *   能够生成连贯的、长期的动态。\n\n### 5. 实验结果与意义\n\n*   **性能表现**：广泛的实验表明，PAN在以下方面表现出强大的性能：\n    *   以动作为条件的世界模拟。\n    *   长周期预测。\n    *   模拟推理。\n*   **对比优势**：与现有视频生成器和世界模型相比，PAN展现出显著优势。\n*   **研究意义**：PAN的提出是迈向通用世界模型的重要一步，它能够为智能体的推理和行动提供未来世界状态的预测性模拟。",
      "shortSummary": "PAN是一个通用、可交互且支持长周期模拟的世界模型。它通过结合基于大型语言模型（LLM）的自回归潜在动态骨干和视频扩散解码器（GLP架构），实现以自然语言动作为条件的未来世界状态高质量视频模拟。PAN在多样化领域的大规模视频-动作对上训练，能够进行开放域、长期的连贯模拟。实验证明，PAN在动作条件世界模拟、长周期预测和模拟推理方面表现出色，是迈向通用世界模型的重要进展，赋能智能体的推理和行动。",
      "translated_title": "PAN：一个用于通用、可交互和长周期世界模拟的世界模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting."
    },
    {
      "title": "零误差解决百万步LLM任务 (原标题: Solving a Million-Step LLM Task with Zero Errors)",
      "link": "https://arxiv.org/abs/2511.09030",
      "pubDate": "Wed, 12 Nov 2025 01:27:55 GMT",
      "isoDate": "2025-11-12T01:27:55.000Z",
      "creator": "Elliot Meyerson, Giuseppe Paolo, Roberto Dailey, Hormoz Shahrzad, Olivier Francon, Conor F. Hayes, Xin Qiu, Babak Hodjat, Risto Miikkulainen",
      "summary": "## 零误差解决百万步LLM任务：MAKER系统的突破\n\n大型语言模型（LLMs）在推理、洞察和工具使用方面取得了显著进展。然而，将这些能力串联起来执行人类、组织和社会日常所需的大规模、长流程任务，仍然是一个巨大的挑战。主要障碍在于LLMs固有的错误率，这阻碍了其扩展性。例如，最近在汉诺塔（Towers of Hanoi）基准测试中的实验表明，LLM驱动的流程在执行数百步后便不可避免地会出错。\n\n### 当前LLM面临的挑战\n\n*   **持续的错误率**：LLMs在执行多步骤任务时，即使是微小的错误也会累积，导致整个过程最终失败。\n*   **扩展性受限**：由于错误率的存在，LLMs难以处理需要大量连续逻辑步骤的复杂任务。\n*   **基准测试局限**：尽管研究日益关注LLMs执行长程任务的能力，但许多现有基准测试仍侧重于步骤相对较少的任务。\n\n### MAKER系统：百万步任务的解决方案\n\n本文介绍的MAKER是首个成功以零误差解决超过一百万个LLM步骤任务的系统，并且原则上可以扩展到远超此水平。MAKER的成功得益于其创新的方法论：\n\n1.  **极致的任务分解**：MAKER将一个大型任务分解成极其细小的子任务。\n2.  **聚焦的微代理**：每个子任务都由一个专门的微代理（microagent）负责处理。\n3.  **高度模块化**：这种分解带来了高度的模块化，使得系统能够在每个步骤应用错误纠正机制。\n4.  **高效的多代理投票机制**：通过一个高效的多代理投票方案，MAKER能够在每个步骤进行错误纠正，从而确保整个流程的准确性。\n\n### 结论与未来展望\n\nMAKER系统通过结合“极致分解”和“错误纠正”两大核心策略，实现了前所未有的任务规模和零误差性能。这一成果表明，与其持续改进现有LLMs的内部能力，不如探索“大规模分解代理过程”（Massively Decomposed Agentic Processes, MDAPs）可能提供一种更有效的方式，来解决组织和社会层面上的复杂问题。\n\nMAKER的出现为LLM在处理超长、复杂任务方面开辟了新的道路，预示着未来AI系统在解决现实世界大规模问题上的巨大潜力。",
      "shortSummary": "MAKER系统首次实现了零误差解决超过一百万个LLM步骤的任务，突破了现有LLMs在长程任务中因持续错误率而导致的扩展性限制。其核心方法是将任务极致分解为由聚焦微代理处理的子任务，并通过高效的多代理投票机制在每一步进行错误纠正。这一创新结合使得大规模任务的零误差执行成为可能，并提出“大规模分解代理过程”（MDAPs）可能是解决组织和社会层面复杂问题的有效途径。",
      "translated_title": "零误差解决百万步LLM任务",
      "images": [],
      "contentSource": "完整文章",
      "content": "LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies."
    },
    {
      "title": "TiDAR：扩散式思考，自回归生成 (原标题: TiDAR: Think in Diffusion, Talk in Autoregression)",
      "link": "https://arxiv.org/abs/2511.08923",
      "pubDate": "Tue, 11 Nov 2025 21:59:33 GMT",
      "isoDate": "2025-11-11T21:59:33.000Z",
      "creator": "Jingyu Liu, Xin Dong, Zhifan Ye, Rishabh Mehta, Yonggan Fu, Vartika Singh, Jan Kautz, Ce Zhang, Pavlo Molchanov",
      "summary": "# TiDAR：扩散式思考，自回归生成\n\n本文介绍了一种名为TiDAR（Think in Diffusion, Talk in Autoregression）的新型序列级混合架构，旨在结合扩散语言模型的高吞吐量并行生成能力与自回归（AR）模型卓越的生成质量。\n\n## 现有挑战\n\n*   **扩散语言模型**：具备快速并行生成的潜力，但在质量上通常不如自回归模型。\n*   **自回归模型**：因其因果结构与语言建模天然契合，通常在质量上表现出色，但生成过程是顺序的。\n*   **现有方法不足**：\n    *   **推测解码（Speculative Decoding）**：优先考虑自回归，使用较弱的模型进行顺序草稿，导致草稿效率低下。\n    *   **扩散模型的类自回归解码**：仍面临质量下降的问题，并丧失了其潜在的并行性。\n    *   这些方法未能有效平衡高吞吐量、GPU利用率和自回归级别的质量。\n\n## TiDAR 架构与设计\n\nTiDAR是一种创新的序列级混合架构，其核心设计理念是在单个前向传播中完成以下两个关键步骤：\n\n1.  **扩散式思考（Thinking in Diffusion）**：以扩散的方式并行地生成（起草）令牌。\n2.  **自回归表达（Talking in Autoregression）**：以自回归的方式采样最终输出。\n\n该架构通过**特殊设计的结构化注意力掩码**实现，充分利用了GPU的计算密度，从而在草稿生成和验证能力之间实现了强大的平衡。此外，TiDAR被设计为一个独立的模型，具有低开销，易于部署（serving-friendly）。\n\n## 性能评估与结果\n\n研究人员在1.5B和8B两种规模下，对TiDAR在生成和似然任务上进行了广泛评估，并将其与以下模型进行了比较：\n\n*   自回归模型\n*   推测解码（Speculative Decoding）\n*   扩散模型变体（如Dream和Llada）\n\n**主要发现和优势：**\n\n*   **吞吐量提升**：得益于并行草稿生成、并行采样以及对KV缓存的精确支持，TiDAR在实测吞吐量方面超越了推测解码。\n*   **效率与质量兼顾**：TiDAR在效率和质量两方面均超越了Dream和Llada等扩散模型。\n*   **弥合质量差距**：最值得注意的是，TiDAR是首个在保持与自回归模型相当的质量水平的同时，还能提供每秒4.71倍至5.91倍更多令牌的架构。这意味着它成功弥合了扩散模型与自回归模型之间的质量差距。\n\n## 结论\n\nTiDAR代表了语言模型生成领域的一个重大进步，它通过创新的混合架构，首次实现了高吞吐量、高GPU利用率与自回归级别质量的协同，为未来高效、高质量的语言生成提供了新的范式。",
      "shortSummary": "TiDAR是一种新型混合语言模型架构，它在单个前向传播中结合了扩散式并行令牌起草（Thinking）和自回归式最终输出采样（Talking）。该设计旨在平衡高吞吐量和自回归模型的生成质量。实验证明，TiDAR在效率和质量上均优于推测解码和现有扩散模型。最显著的是，TiDAR首次在保持与自回归模型相当质量的同时，实现了每秒4.71到5.91倍的令牌生成速度提升，为高效高质量语言生成开辟了新途径。",
      "translated_title": "TiDAR：扩散式思考，自回归生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Diffusion language models hold the promise of fast parallel generation, while autoregressive (AR) models typically excel in quality due to their causal structure aligning naturally with language modeling. This raises a fundamental question: can we achieve a synergy with high throughput, higher GPU utilization, and AR level quality? Existing methods fail to effectively balance these two aspects, either prioritizing AR using a weaker model for sequential drafting (speculative decoding), leading to lower drafting efficiency, or using some form of left-to-right (AR-like) decoding logic for diffusion, which still suffers from quality degradation and forfeits its potential parallelizability. We introduce TiDAR, a sequence-level hybrid architecture that drafts tokens (Thinking) in Diffusion and samples final outputs (Talking) AutoRegressively - all within a single forward pass using specially designed structured attention masks. This design exploits the free GPU compute density, achieving a strong balance between drafting and verification capacity. Moreover, TiDAR is designed to be serving-friendly (low overhead) as a standalone model. We extensively evaluate TiDAR against AR models, speculative decoding, and diffusion variants across generative and likelihood tasks at 1.5B and 8B scales. Thanks to the parallel drafting and sampling as well as exact KV cache support, TiDAR outperforms speculative decoding in measured throughput and surpasses diffusion models like Dream and Llada in both efficiency and quality. Most notably, TiDAR is the first architecture to close the quality gap with AR models while delivering 4.71x to 5.91x more tokens per second."
    },
    {
      "title": "Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds",
      "link": "https://arxiv.org/abs/2511.08892",
      "pubDate": "Tue, 11 Nov 2025 21:01:26 GMT",
      "isoDate": "2025-11-11T21:01:26.000Z",
      "creator": "Weihao Tan, Xiangyang Li, Yunhao Fang, Heyuan Yao, Shi Yan, Hao Luo, Tenglong Ao, Huihui Li, Hongbin Ren, Bairen Yi, Yujia Qin, Bo An, Libin Liu, Guang Shi",
      "summary": "无法生成摘要（API请求失败）。",
      "shortSummary": "",
      "translated_title": "Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds",
      "images": [],
      "contentSource": "RSS",
      "content": "We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments."
    },
    {
      "title": "未选择的路：RLVR 被证明偏离主方向学习 (原标题: The Path Not Taken: RLVR Provably Learns Off the Principals)",
      "link": "https://arxiv.org/abs/2511.08567",
      "pubDate": "Tue, 11 Nov 2025 13:49:45 GMT",
      "isoDate": "2025-11-11T13:49:45.000Z",
      "creator": "Hanqing Zhu, Zhenyu Zhang, Hanxian Huang, DiJia Su, Zechun Liu, Jiawei Zhao, Igor Fedorov, Hamed Pirsiavash, Zhizhou Sha, Jinwon Lee, David Z. Pan, Zhangyang Wang, Yuandong Tian, Kai Sheng Tai",
      "summary": "# RLVR学习机制的深入探究：偏离主方向的学习路径\n\n本文深入探讨了“可验证奖励强化学习”（RLVR）在提升大型语言模型（LLM）推理性能时所展现的“稀疏性悖论”——即RLVR在显著提高性能的同时，似乎只修改了模型参数的一小部分。作者们重新审视了这一现象，并揭示了稀疏性实际上是模型条件优化偏差的一种表面假象。\n\n## 核心发现：优化偏差与稀疏性\n\n研究表明，对于一个固定的预训练模型，RLVR的参数更新会持续地定位到特定的、偏好的参数区域。这种定位在不同运行、不同数据集和不同RL策略之间都高度一致，并且在很大程度上保持不变。\n\n## 三门理论：机制解释\n\n为了从机制上解释这些动态，作者提出了“三门理论”：\n\n*   **第一门（KL锚定）**：施加了KL散度约束的更新。\n*   **第二门（模型几何）**：将更新步骤引导偏离主方向，进入低曲率、谱保持的子空间。\n*   **第三门（精度）**：将非偏好区域的微小更新隐藏起来，使得这种偏离主方向的偏差表现为稀疏性。\n\n## RLVR学习动态的参数级表征\n\n作者们通过验证这一理论，首次提供了RLVR学习动态的参数级表征：\n\n*   **偏离主方向学习**：RLVR在权重空间中偏离主方向进行学习。\n*   **最小谱漂移**：通过最小化谱漂移来实现性能提升。\n*   **主子空间旋转减少**：减少了主子空间的旋转。\n*   **非主方向更新对齐**：实现了非主方向更新的对齐。\n\n## 与SFT的对比\n\n与RLVR形成鲜明对比的是，传统的监督微调（SFT）方法：\n\n*   **目标主权重**：SFT倾向于针对主权重进行更新。\n*   **扭曲频谱**：SFT会扭曲模型的频谱。\n*   **性能滞后**：在某些情况下，SFT甚至落后于RLVR。\n\n## 关键启示与未来方向\n\n这些结果首次从参数空间角度阐明了RLVR的训练动态，揭示了参数演化的清晰规律。至关重要的是，研究表明RL与SFT在优化机制上处于截然不同的状态。因此，直接改编SFT时代的参数高效微调（PEFT）方法可能存在缺陷，这一点通过对高级稀疏微调和LoRA变体的案例研究得到了证实。\n\n作者们希望这项工作能够为RLVR的“白盒”理解以及设计几何感知、RLVR原生学习算法指明方向，而不是简单地重复使用SFT时代的启发式方法。",
      "shortSummary": "本文揭示了RLVR在提升LLM性能时表现出的“稀疏性”并非真实稀疏，而是模型条件优化偏差的表面现象。作者提出了“三门理论”解释其机制：RLVR更新偏离主方向，进入低曲率子空间。与SFT不同，RLVR通过最小谱漂移和非主方向更新对齐实现增益。研究强调RL与SFT优化机制不同，直接套用SFT时代的PEFT方法可能存在缺陷，呼吁开发RLVR原生算法。",
      "translated_title": "未选择的路：RLVR 被证明偏离主方向学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement Learning with Verifiable Rewards (RLVR) reliably improves the reasoning performance of large language models, yet it appears to modify only a small fraction of parameters. We revisit this paradox and show that sparsity is a surface artifact of a model-conditioned optimization bias: for a fixed pretrained model, updates consistently localize to preferred parameter regions, highly consistent across runs and largely invariant to datasets and RL recipes. We mechanistically explain these dynamics with a Three-Gate Theory: Gate I (KL Anchor) imposes a KL-constrained update; Gate II (Model Geometry) steers the step off principal directions into low-curvature, spectrum-preserving subspaces; and Gate III (Precision) hides micro-updates in non-preferred regions, making the off-principal bias appear as sparsity. We then validate this theory and, for the first time, provide a parameter-level characterization of RLVR's learning dynamics: RLVR learns off principal directions in weight space, achieving gains via minimal spectral drift, reduced principal-subspace rotation, and off-principal update alignment. In contrast, SFT targets principal weights, distorts the spectrum, and even lags RLVR.   Together, these results provide the first parameter-space account of RLVR's training dynamics, revealing clear regularities in how parameters evolve. Crucially, we show that RL operates in a distinct optimization regime from SFT, so directly adapting SFT-era parameter-efficient fine-tuning (PEFT) methods can be flawed, as evidenced by our case studies on advanced sparse fine-tuning and LoRA variants. We hope this work charts a path toward a white-box understanding of RLVR and the design of geometry-aware, RLVR-native learning algorithms, rather than repurposed SFT-era heuristics."
    },
    {
      "title": "AlphaResearch：利用语言模型加速新算法发现 (原标题: AlphaResearch: Accelerating New Algorithm Discovery with Language Models)",
      "link": "https://arxiv.org/abs/2511.08522",
      "pubDate": "Tue, 11 Nov 2025 13:03:22 GMT",
      "isoDate": "2025-11-11T13:03:22.000Z",
      "creator": "Zhaojian Yu, Kaiyue Feng, Yilun Zhao, Shilin He, Xiao-Ping Zhang, Arman Cohan",
      "summary": "# AlphaResearch：利用语言模型加速新算法发现\n\n本文介绍了 **AlphaResearch**，一个旨在解决开放式问题并发现新算法的自主研究智能体。尽管大型语言模型（LLMs）在复杂但易于验证的问题上取得了显著进展，但在发现未知领域方面仍面临挑战。AlphaResearch旨在弥合这一差距，通过结合可行性和创新性来加速算法发现过程。\n\n## 核心机制\n\nAlphaResearch 采用了一种新颖的 **双重研究环境**，该环境结合了：\n*   **基于执行的验证**：确保算法的可行性。\n*   **模拟真实世界的同行评审环境**：评估算法的创新性和质量。\n\n该智能体通过迭代运行以下步骤来发现新算法：\n1.  **提出新想法**：生成潜在的算法解决方案。\n2.  **在双重研究环境中验证想法**：对提出的想法进行可行性测试和质量评估。\n3.  **优化研究提案**：根据验证结果改进和优化算法提案，以获得更好的性能。\n\n## 评估基准：AlphaResearchComp\n\n为了促进透明的评估过程，研究人员构建了 **AlphaResearchComp**，这是一个新的评估基准。该基准包括：\n*   **八个开放式算法问题竞赛**：每个问题都经过精心策划。\n*   **可执行的管道**：确保评估过程的自动化和标准化。\n*   **客观指标**：用于量化算法性能。\n*   **可复现性检查**：确保实验结果的可靠性。\n\n## 主要研究成果\n\nAlphaResearch 在与人类研究人员的头对头比较中取得了 **2/8 的胜率**，这表明利用大型语言模型加速算法发现的可能性。\n\n值得注意的是，AlphaResearch 在 **“打包圆形” (packing circles)** 问题上发现的算法，实现了 **已知最佳性能**，超越了人类研究人员和近期工作中（例如 AlphaEvolve）的强大基线。\n\n## 挑战与未来展望\n\n研究还对剩余的 6/8 失败案例进行了全面分析，为未来的研究提供了宝贵的见解，指出了当前方法的局限性和改进方向。",
      "shortSummary": "AlphaResearch是一个利用大型语言模型加速新算法发现的自主研究智能体。它通过结合基于执行的验证和模拟同行评审的双重环境，迭代地提出、验证和优化算法。在AlphaResearchComp基准测试中，AlphaResearch在与人类研究者的8个问题对比中取得2胜，并在“打包圆形”问题上实现了已知最佳性能，展示了LLM在算法发现领域的巨大潜力。",
      "translated_title": "AlphaResearch：利用语言模型加速新算法发现",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models have made significant progress in complex but easy-to-verify problems, yet they still struggle with discovering the unknown. In this paper, we present AlphaResearch, an autonomous research agent designed to discover new algorithms on open-ended problems. To synergize the feasibility and innovation of the discovery process, we construct a novel dual research environment by combining the execution-based verify and simulated real-world peer review environment. AlphaResearch discovers new algorithm by iteratively running the following steps: (1) propose new ideas (2) verify the ideas in the dual research environment (3) optimize the research proposals for better performance. To promote a transparent evaluation process, we construct AlphaResearchComp, a new evaluation benchmark that includes an eight open-ended algorithmic problems competition, with each problem carefully curated and verified through executable pipelines, objective metrics, and reproducibility checks. AlphaResearch gets a 2/8 win rate in head-to-head comparison with human researchers, demonstrate the possibility of accelerating algorithm discovery with LLMs. Notably, the algorithm discovered by AlphaResearch on the ``packing circles'' problem achieves the best-of-known performance, surpassing the results of human researchers and strong baselines from recent work (e.g., AlphaEvolve). Additionally, we conduct a comprehensive analysis of the remaining challenges of the 6/8 failure cases, providing valuable insights for future research."
    },
    {
      "title": "UniVA：迈向开源下一代视频通才的通用视频智能体 (原标题: UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist)",
      "link": "https://arxiv.org/abs/2511.08521",
      "pubDate": "Tue, 11 Nov 2025 12:58:13 GMT",
      "isoDate": "2025-11-11T12:58:13.000Z",
      "creator": "Zhengyang Liang, Daoan Zhang, Huichi Zhou, Rui Huang, Bobo Li, Yuechen Zhang, Shengqiong Wu, Xiaohan Wang, Jiebo Luo, Lizi Liao, Hao Fei",
      "summary": "## UniVA：迈向开源下一代视频通才的通用视频智能体\n\n### 引言\n\n当前，专业的AI模型在独立的视频任务（如生成或理解）中表现出色，但实际应用往往需要结合这些能力的复杂、迭代的工作流。为了弥合这一差距，研究人员引入了UniVA。\n\n### UniVA框架概述\n\nUniVA是一个开源、全能的多智能体框架，旨在成为下一代视频通才。它将视频理解、分割、编辑和生成统一到连贯的工作流中，以实现更复杂的应用。\n\n### 核心架构：Plan-and-Act双智能体系统\n\nUniVA采用“规划与执行”（Plan-and-Act）双智能体架构，驱动高度自动化和主动的工作流：\n\n*   **规划智能体 (Planner Agent)**：负责解释用户意图，并将其分解为结构化的视频处理步骤。\n*   **执行智能体 (Executor Agents)**：通过模块化的、基于MCP（Modular, Composable, and Pluggable）的工具服务器（用于分析、生成、编辑、跟踪等）来执行这些步骤。\n\n### 关键特性与优势\n\nUniVA通过以下机制实现其强大功能：\n\n*   **分层多级记忆系统**：包含全局知识、任务上下文和用户特定偏好。\n*   **长程推理与上下文连续性**：记忆系统支持长程推理、上下文连续性以及智能体之间的有效通信。\n*   **交互式与自反思创作**：实现具有完全可追溯性的交互式和自反思视频创作。\n*   **支持复杂迭代工作流**：该设计能够实现以前使用单一用途模型或单一视频-语言模型难以实现的迭代和任意条件（例如，文本/图像/视频条件生成 → 多轮编辑 → 对象分割 → 组合合成）的视频工作流。\n\n### 评估基准：UniVA-Bench\n\n为了严格评估此类智能体视频系统，项目还引入了UniVA-Bench。这是一个包含多步骤视频任务的基准套件，涵盖了视频理解、编辑、分割和生成等多个方面。\n\n### 开源性质与未来展望\n\nUniVA和UniVA-Bench均已完全开源，旨在促进交互式、智能体化和通用视频智能的研究，为下一代多模态AI系统奠定基础。",
      "shortSummary": "UniVA是一个开源、全能的多智能体框架，旨在成为下一代视频通才。它通过Plan-and-Act双智能体架构，将视频理解、分割、编辑和生成整合到连贯的迭代工作流中。UniVA利用分层记忆系统实现长程推理和上下文连续性，支持交互式、自反思的视频创作。为评估此类系统，项目还推出了UniVA-Bench基准套件。UniVA及其基准均已开源，旨在推动通用视频智能的研究，为多模态AI发展做出贡献。",
      "translated_title": "UniVA：迈向开源下一代视频通才的通用视频智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "While specialized AI models excel at isolated video tasks like generation or understanding, real-world applications demand complex, iterative workflows that combine these capabilities. To bridge this gap, we introduce UniVA, an open-source, omni-capable multi-agent framework for next-generation video generalists that unifies video understanding, segmentation, editing, and generation into cohesive workflows. UniVA employs a Plan-and-Act dual-agent architecture that drives a highly automated and proactive workflow: a planner agent interprets user intentions and decomposes them into structured video-processing steps, while executor agents execute these through modular, MCP-based tool servers (for analysis, generation, editing, tracking, etc.). Through a hierarchical multi-level memory (global knowledge, task context, and user-specific preferences), UniVA sustains long-horizon reasoning, contextual continuity, and inter-agent communication, enabling interactive and self-reflective video creation with full traceability. This design enables iterative and any-conditioned video workflows (e.g., text/image/video-conditioned generation rightarrow multi-round editing rightarrow object segmentation rightarrow compositional synthesis) that were previously cumbersome to achieve with single-purpose models or monolithic video-language models. We also introduce UniVA-Bench, a benchmark suite of multi-step video tasks spanning understanding, editing, segmentation, and generation, to rigorously evaluate such agentic video systems. Both UniVA and UniVA-Bench are fully open-sourced, aiming to catalyze research on interactive, agentic, and general-purpose video intelligence for the next generation of multimodal AI systems. (https://univa.online/)"
    }
  ],
  "lastUpdated": "2025-11-16T09:30:30.506Z"
}