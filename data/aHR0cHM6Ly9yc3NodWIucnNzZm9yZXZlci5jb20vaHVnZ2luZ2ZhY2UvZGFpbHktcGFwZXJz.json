{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "LangScene-X：使用TriMap视频扩散重建可泛化的3D语言嵌入场景 (原标题: LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion)",
      "link": "https://arxiv.org/abs/2507.02813",
      "pubDate": "Thu, 03 Jul 2025 13:21:23 GMT",
      "isoDate": "2025-07-03T13:21:23.000Z",
      "creator": "Fangfu Liu, Hao Li, Jiawei Chi, Hanyang Wang, Minghui Yang, Fudong Wang, Yueqi Duan",
      "summary": "## LangScene-X：从稀疏视图重建可泛化的3D语言嵌入场景\n\n### 核心问题与现有挑战\n\n从2D图像中恢复具有开放词汇场景理解的3D结构是一项基础但艰巨的任务。当前方法主要依赖于校准的密集视图重建范式，这导致在可用视图有限时，会产生严重的渲染伪影和不合理的语义合成。\n\n### LangScene-X 框架介绍\n\n本文提出了一种名为 LangScene-X 的新型生成框架，旨在统一并生成3D一致的多模态信息，以实现场景的重建和理解。该框架利用其强大的生成能力，能够从仅有的稀疏视图构建可泛化的3D语言嵌入场景，从而克服了现有方法的局限性。\n\n### 关键技术与创新点\n\n1.  **TriMap 视频扩散模型：** LangScene-X 首先训练了一个 TriMap 视频扩散模型。该模型能够通过渐进式知识整合，从稀疏输入生成多模态信息，包括：\n    *   **外观 (RGBs)**\n    *   **几何 (法线)**\n    *   **语义 (分割图)**\n2.  **语言量化压缩器 (LQC)：** 提出了一种在大型图像数据集上训练的语言量化压缩器（LQC）。LQC 的作用是高效编码语言嵌入，从而实现跨场景的泛化能力，无需进行逐场景的再训练，显著提高了系统的灵活性和效率。\n3.  **语言表面场重建：** 最后，LangScene-X 通过将语言信息对齐到3D场景的表面上，重建了语言表面场。这一机制使得系统能够支持开放式的语言查询，极大地增强了场景理解的交互性和灵活性。\n\n### 实验结果与优势\n\n在真实世界数据上进行的广泛实验表明，LangScene-X 在质量和泛化能力方面均优于现有最先进的方法，验证了其在稀疏视图下重建高质量、可泛化3D语言嵌入场景的卓越性能。",
      "shortSummary": "LangScene-X 是一个新型生成框架，旨在从稀疏视图重建可泛化的3D语言嵌入场景。它通过训练TriMap视频扩散模型生成外观、几何和语义信息，并引入语言量化压缩器（LQC）实现高效的语言嵌入编码和跨场景泛化。该框架通过将语言信息对齐到3D场景表面，支持开放式语言查询。LangScene-X 在质量和泛化能力上超越了现有最先进的方法，解决了有限视图下3D重建和语义理解的挑战。",
      "translated_title": "LangScene-X：使用TriMap视频扩散重建可泛化的3D语言嵌入场景",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recovering 3D structures with open-vocabulary scene understanding from 2D images is a fundamental but daunting task. Recent developments have achieved this by performing per-scene optimization with embedded language information. However, they heavily rely on the calibrated dense-view reconstruction paradigm, thereby suffering from severe rendering artifacts and implausible semantic synthesis when limited views are available. In this paper, we introduce a novel generative framework, coined LangScene-X, to unify and generate 3D consistent multi-modality information for reconstruction and understanding. Powered by the generative capability of creating more consistent novel observations, we can build generalizable 3D language-embedded scenes from only sparse views. Specifically, we first train a TriMap video diffusion model that can generate appearance (RGBs), geometry (normals), and semantics (segmentation maps) from sparse inputs through progressive knowledge integration. Furthermore, we propose a Language Quantized Compressor (LQC), trained on large-scale image datasets, to efficiently encode language embeddings, enabling cross-scene generalization without per-scene retraining. Finally, we reconstruct the language surface fields by aligning language information onto the surface of 3D scenes, enabling open-ended language queries. Extensive experiments on real-world data demonstrate the superiority of our LangScene-X over state-of-the-art methods in terms of quality and generalizability. Project Page: https://liuff19.github.io/LangScene-X."
    },
    {
      "title": "自我纠正基准：揭示并解决大型语言模型（LLMs）的自我纠正盲点 (原标题: Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs)",
      "link": "https://arxiv.org/abs/2507.02778",
      "pubDate": "Thu, 03 Jul 2025 12:41:30 GMT",
      "isoDate": "2025-07-03T12:41:30.000Z",
      "creator": "Ken Tsui",
      "summary": "## 自我纠正基准：揭示并解决大型语言模型（LLMs）的自我纠正盲点\n\n### 摘要\n\n尽管大型语言模型（LLMs）已带来变革性影响，但它们仍会犯错并可能探索无效的推理路径。自我纠正能力对于可信赖的LLM，特别是自回归LLM，至关重要。虽然LLMs能够识别用户输入中的错误，但它们表现出一种系统性的“自我纠正盲点”——无法纠正自身输出中相同的错误。\n\n### 研究方法与发现\n\n1.  **引入自我纠正基准（Self-Correction Bench）**：\n    *   为了系统地研究这一现象，研究人员引入了一个系统性框架——Self-Correction Bench。\n    *   该框架通过在三个复杂性级别上进行受控错误注入来衡量这一现象。\n\n2.  **测试结果**：\n    *   对14个模型进行测试后，发现平均盲点率为64.5%。\n\n3.  **盲点原因**：\n    *   研究发现，这种局限性与训练数据构成有关。\n    *   人类训练演示主要显示的是无错误响应，而非错误纠正序列，这与通过结果反馈学习错误纠正的强化学习（RL）训练模型不同。\n\n### 解决方案与启示\n\n1.  **“Wait”提示词的惊人效果**：\n    *   值得注意的是，简单地在提示词后附加“Wait”一词，就能将盲点减少89.3%。\n    *   这表明LLMs的自我纠正能力是存在的，但需要被激活。\n\n2.  **研究意义**：\n    *   这项工作突出了当前LLMs的一个关键局限性。\n    *   同时，它也为提高LLMs的可靠性和可信赖性提供了潜在途径。",
      "shortSummary": "大型语言模型（LLMs）存在“自我纠正盲点”，即无法纠正自身输出中的错误。研究引入“自我纠正基准”框架，通过注入错误测试14个模型，发现平均盲点率为64.5%。这与训练数据组成有关。令人惊讶的是，简单地添加“Wait”提示词能将盲点减少89.3%，表明LLMs具备此能力但需激活。这项工作揭示了当前LLMs的关键局限性，并为提高其可靠性提供了方向。",
      "translated_title": "自我纠正基准：揭示并解决大型语言模型（LLMs）的自我纠正盲点",
      "images": [],
      "contentSource": "完整文章",
      "content": "Although large language models (LLMs) have become transformative, they still make mistakes and can explore unproductive reasoning paths. Self-correction is an important capability for a trustworthy LLM, particularly an autoregressive LLM. While LLMs can identify error in user input, they exhibit a systematic 'Self-Correction Blind Spot' - failing to correct identical error in their own outputs. To systematically study this phenomenon, we introduce Self-Correction Bench, a systematic framework to measure this phenomenon through controlled error injection at three complexity levels. Testing 14 models, we find an average 64.5% blind spot rate. We find multiple evidences that this limitation relates to training data composition: human training demonstrations predominantly show error-free responses rather than error-correction sequences, unlike RL-trained models that learn error correction through outcome feedback. Remarkably, simply appending \"Wait\" reduces blind spots by 89.3%, suggesting that the capability exists but requires activation. Our work highlights a critical limitation in current LLMs and offers potential avenues for improving their reliability and trustworthiness."
    },
    {
      "title": "快速与单纯：Triton中的2-单纯注意力 (原标题: Fast and Simplex: 2-Simplicial Attention in Triton)",
      "link": "https://arxiv.org/abs/2507.02754",
      "pubDate": "Thu, 03 Jul 2025 12:16:34 GMT",
      "isoDate": "2025-07-03T12:16:34.000Z",
      "creator": "Aurko Roy, Timothy Chou, Sai Surya Duvvuri, Sijia Chen, Jiecao Yu, Xiaodong Wang, Manzil Zaheer, Rohan Anil",
      "summary": "## 快速与单纯：Triton中的2-单纯注意力\n\n### 背景与问题\n\n*   **传统缩放定律的局限性**：近期研究表明，训练损失与模型大小和令牌数量呈幂律关系，且实现计算最优模型需要同时扩展模型大小和令牌数量。然而，这些缩放定律假设数据供应无限，并且主要适用于计算受限（compute-bound）的环境。\n*   **现代大型语言模型（LLMs）的转变**：随着现代LLMs越来越依赖大规模互联网数据集，它们是计算受限的假设变得不再有效。这种转变凸显了对优先考虑令牌效率的架构的需求。\n\n### 提出的解决方案：2-单纯Transformer\n\n*   **架构创新**：本文研究了2-单纯Transformer，这是一种将标准点积注意力推广到三线性函数（trilinear functions）的架构。\n*   **高效实现**：该架构通过高效的Triton内核实现。\n\n### 核心发现与优势\n\n*   **更高的令牌效率**：2-单纯Transformer比标准Transformer实现了更好的令牌效率。\n*   **性能提升**：在固定令牌预算下，尺寸相似的2-单纯模型在涉及数学、编码、推理和逻辑的任务上，其性能优于点积对应的模型。\n*   **量化增益**：研究通过证明2-单纯注意力改变了知识和推理任务的缩放定律中的指数，从而量化了这些增益，这与点积注意力不同。\n\n### 其他信息\n\n*   **文档长度**：论文共10页，附录25页。\n*   **研究领域**：机器学习（cs.LG）；人工智能（cs.AI）。",
      "shortSummary": "本文介绍了2-单纯Transformer，一种通过高效Triton内核将标准点积注意力推广到三线性函数的架构。针对现代大型语言模型面临的令牌效率需求，研究表明2-单纯Transformer在固定令牌预算下，比标准Transformer具有更高的令牌效率。它在数学、编码、推理和逻辑任务上表现更优，并改变了知识和推理任务的缩放定律指数，证明了其在数据受限场景下的优势。",
      "translated_title": "快速与单纯：Triton中的2-单纯注意力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent work has shown that training loss scales as a power law with both model size and the number of tokens, and that achieving compute-optimal models requires scaling model size and token count together. However, these scaling laws assume an infinite supply of data and apply primarily in compute-bound settings. As modern large language models increasingly rely on massive internet-scale datasets, the assumption that they are compute-bound is becoming less valid. This shift highlights the need for architectures that prioritize token efficiency.   In this work, we investigate the use of the 2-simplicial Transformer, an architecture that generalizes standard dot-product attention to trilinear functions through an efficient Triton kernel implementation. We demonstrate that the 2-simplicial Transformer achieves better token efficiency than standard Transformers: for a fixed token budget, similarly sized models outperform their dot-product counterparts on tasks involving mathematics, coding, reasoning, and logic. We quantify these gains by demonstrating that 2-simplicial attention changes the exponent in the scaling laws for knowledge and reasoning tasks compared to dot product attention."
    },
    {
      "title": "布尔巴基：用于定理证明的自生成和目标条件MDP (原标题: Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving)",
      "link": "https://arxiv.org/abs/2507.02726",
      "pubDate": "Thu, 03 Jul 2025 11:41:38 GMT",
      "isoDate": "2025-07-03T11:41:38.000Z",
      "creator": "Matthieu Zimmer, Xiaotong Ji, Rasul Tutunov, Anthony Bordg, Jun Wang, Haitham Bou Ammar",
      "summary": "## 布尔巴基：用于定理证明的自生成和目标条件MDP\n\n### 挑战与问题\n\n*   **大型语言模型（LLMs）的推理局限性**：在自动化定理证明（ATP）等逻辑受限环境中，LLMs的推理能力面临严峻挑战。\n*   **稀疏奖励与巨大证明空间**：ATP任务的特点是奖励信号稀疏，且潜在的证明路径空间极其庞大，这使得LLMs难以有效学习和探索。\n*   **复杂基准测试的挑战**：像PutnamBench这样的基准测试进一步加剧了这些困难，因为它包含大学级别的数学问题，需要复杂、多步骤的推理过程。\n\n### 提出的解决方案：自生成目标条件MDPs (sG-MDPs)\n\n*   **新颖的框架**：研究引入了自生成目标条件马尔可夫决策过程（sG-MDPs），这是一种全新的框架。\n*   **代理生成与追求子目标**：在该框架中，AI代理能够根据不断演变的证明状态，自主地生成并追求其子目标。\n*   **促进搜索**：通过这种更结构化的目标生成方式，原始的定理证明问题变得更适合于搜索算法的运用。\n\n### 系统实现与算法：Bourbaki (7B)\n\n*   **算法应用**：该方法通过应用类似蒙特卡洛树搜索（MCTS）的算法来解决sG-MDP。\n*   **模块化系统**：Bourbaki (7B) 是一个模块化系统，其设计允许集成和协同多个7B规模的大型语言模型。\n*   **功能**：这些集成的LLM专门用于子目标生成和策略（tactic）合成。\n\n### 实验结果\n\n*   **性能表现**：在PutnamBench基准测试中，Bourbaki (7B) 成功解决了26个问题。\n*   **最先进成果**：这一成果在同等规模的模型中达到了新的最先进（state-of-the-art）水平。",
      "shortSummary": "大型语言模型在自动化定理证明（ATP）中面临推理挑战，尤其是在复杂的多步推理任务上。为解决此问题，研究提出了自生成目标条件MDPs（sG-MDPs）框架，使AI代理能基于证明状态生成并追求子目标，从而简化搜索。基于此，Bourbaki (7B) 系统利用类似MCTS的算法，并集成多个7B LLM进行子目标和策略合成。Bourbaki (7B) 在PutnamBench上解决了26个问题，为同规模模型树立了新的最先进成果。",
      "translated_title": "布尔巴基：用于定理证明的自生成和目标条件MDP",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reasoning remains a challenging task for large language models (LLMs), especially within the logically constrained environment of automated theorem proving (ATP), due to sparse rewards and the vast scale of proofs. These challenges are amplified in benchmarks like PutnamBench, which contains university-level problems requiring complex, multi-step reasoning. To address this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new framework in which agents generate and pursue their subgoals based on the evolving proof state. Given this more structured generation of goals, the resulting problem becomes more amenable to search. We then apply Monte Carlo Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B) solves 26 problems, achieving new state-of-the-art results with models at this scale."
    },
    {
      "title": "大型语言模型能否识别科学研究中的关键局限性？一项针对人工智能研究论文的系统性评估 (原标题: Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers)",
      "link": "https://arxiv.org/abs/2507.02694",
      "pubDate": "Thu, 03 Jul 2025 11:04:38 GMT",
      "isoDate": "2025-07-03T11:04:38.000Z",
      "creator": "Zhijian Xu, Yilun Zhao, Manasi Patwardhan, Lovekesh Vig, Arman Cohan",
      "summary": "# 大型语言模型识别科学研究局限性的系统性评估\n\n## 摘要\n\n同行评审是科学研究中不可或缺的环节，但随着出版物数量的激增，这项高度依赖专业知识的流程面临日益严峻的挑战。尽管大型语言模型（LLMs）在多种科学任务中展现出巨大潜力，但它们在辅助同行评审，特别是识别论文局限性方面的能力，目前仍未得到充分研究。\n\n## 主要贡献\n\n本研究旨在填补这一研究空白，并提出了以下关键贡献：\n\n### 1. 科学研究局限性分类法\n\n*   首次提出了一套全面的科学研究局限性类型分类法，其中特别关注人工智能（AI）领域。\n\n### 2. LimitGen 综合基准数据集\n\n*   推出了 **LimitGen**，这是第一个用于评估LLMs支持早期反馈并辅助人类同行评审能力的综合基准。\n*   **LimitGen** 基准包含两个主要子集：\n    *   **LimitGen-Syn**：一个合成数据集，通过对高质量论文进行受控扰动精心创建。\n    *   **LimitGen-Human**：一个收集了真实人类编写的局限性的数据集。\n\n### 3. 文献检索增强LLM系统\n\n*   为了提升LLM系统识别研究局限性的能力，研究团队对其进行了文献检索增强。\n*   这种增强对于将LLMs识别出的局限性与先前的科学发现进行有效关联和验证至关重要。\n\n## 预期成果\n\n*   本研究方法显著增强了LLM系统生成研究论文局限性的能力。\n*   使LLMs能够提供更具体、更具建设性的反馈，从而有效辅助同行评审过程。\n\n## 研究领域\n\n*   计算与语言 (cs.CL)",
      "shortSummary": "本研究评估大型语言模型（LLMs）识别科学研究，特别是AI论文中关键局限性的能力。文章首先提出了一个全面的局限性分类法，并引入了首个用于评估LLMs此能力的综合基准数据集LimitGen（包含合成和人类编写的局限性数据）。为提升LLMs表现，研究团队通过文献检索对其进行增强。最终目标是使LLMs能够提供更具体、建设性的反馈，以辅助应对日益增长的同行评审挑战。",
      "translated_title": "大型语言模型能否识别科学研究中的关键局限性？一项针对人工智能研究论文的系统性评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "Peer review is fundamental to scientific research, but the growing volume of publications has intensified the challenges of this expertise-intensive process. While LLMs show promise in various scientific tasks, their potential to assist with peer review, particularly in identifying paper limitations, remains understudied. We first present a comprehensive taxonomy of limitation types in scientific research, with a focus on AI. Guided by this taxonomy, for studying limitations, we present LimitGen, the first comprehensive benchmark for evaluating LLMs' capability to support early-stage feedback and complement human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a synthetic dataset carefully created through controlled perturbations of high-quality papers, and LimitGen-Human, a collection of real human-written limitations. To improve the ability of LLM systems to identify limitations, we augment them with literature retrieval, which is essential for grounding identifying limitations in prior scientific findings. Our approach enhances the capabilities of LLM systems to generate limitations in research papers, enabling them to provide more concrete and constructive feedback."
    },
    {
      "title": "解耦规划与执行：一种用于深度搜索的层次化推理框架 (原标题: Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search)",
      "link": "https://arxiv.org/abs/2507.02652",
      "pubDate": "Thu, 03 Jul 2025 10:18:08 GMT",
      "isoDate": "2025-07-03T10:18:08.000Z",
      "creator": "Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yang Zhao, Hongjin Qian, Zhicheng Dou",
      "summary": "## 解耦规划与执行：一种用于深度搜索的层次化推理框架 (HiRA)\n\n### 摘要\n\n传统的信息检索增强生成（RAG）管道在处理需要跨多源进行深度推理和知识综合的复杂信息需求时面临挑战。现有基于推理的方法存在根本性限制，即它们使用单一模型处理高层规划和详细执行，导致推理效率低下和可扩展性有限。\n\n### HiRA 框架介绍\n\n本文提出了一种名为 HiRA 的层次化框架，旨在解决上述问题。HiRA 的核心思想是将战略规划与专业化执行分离，从而提高复杂深度搜索任务的效率和效果。\n\n**HiRA 的主要特点：**\n\n*   **任务分解：** 将复杂的搜索任务分解为更聚焦的子任务。\n*   **专业化代理：** 为每个子任务分配领域特定的代理（Agent）。\n*   **工具与推理能力：** 这些代理配备了外部工具和强大的推理能力。\n*   **结果协调：** 通过结构化的集成机制协调和整合各子任务的结果。\n\n**解耦的优势：**\n\n*   **防止干扰：** 这种分离机制能够有效防止执行细节干扰高层推理过程。\n*   **利用专业知识：** 使得系统能够针对不同类型的信息处理，充分利用专业化的知识和能力。\n\n### 实验与结果\n\n研究团队在四个复杂的、跨模态的深度搜索基准上对 HiRA 进行了实验验证。实验结果表明：\n\n*   HiRA 显著优于当前最先进的 RAG 系统和基于代理的系统。\n*   在答案质量和系统效率方面均实现了显著提升。\n*   这些结果强调了解耦规划与执行对于多步骤信息寻求任务的有效性。\n\n### 代码可用性\n\n本研究的代码已公开。",
      "shortSummary": "HiRA 是一种用于深度搜索的层次化推理框架，旨在解决传统RAG和单一模型推理在复杂信息需求下的效率与可扩展性问题。它通过将高层规划与专业化执行解耦，把复杂任务分解为子任务，并分配给配备工具和推理能力的领域特定代理处理，再协调结果。实验证明，HiRA 在答案质量和系统效率上均显著优于现有技术，验证了解耦规划与执行在多步骤信息寻求任务中的有效性。",
      "translated_title": "解耦规划与执行：一种用于深度搜索的层次化推理框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: they use a single model to handle both high-level planning and detailed execution, leading to inefficient reasoning and limited scalability. In this paper, we introduce HiRA, a hierarchical framework that separates strategic planning from specialized execution. Our approach decomposes complex search tasks into focused subtasks, assigns each subtask to domain-specific agents equipped with external tools and reasoning capabilities, and coordinates the results through a structured integration mechanism. This separation prevents execution details from disrupting high-level reasoning while enabling the system to leverage specialized expertise for different types of information processing. Experiments on four complex, cross-modal deep search benchmarks demonstrate that HiRA significantly outperforms state-of-the-art RAG and agent-based systems. Our results show improvements in both answer quality and system efficiency, highlighting the effectiveness of decoupled planning and execution for multi-step information seeking tasks. Our code is available at https://github.com/ignorejjj/HiRA."
    },
    {
      "title": "WebSailor：为网络智能体导航超人类推理 (原标题: WebSailor: Navigating Super-human Reasoning for Web Agent)",
      "link": "https://arxiv.org/abs/2507.02592",
      "pubDate": "Thu, 03 Jul 2025 08:59:07 GMT",
      "isoDate": "2025-07-03T08:59:07.000Z",
      "creator": "Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, Weizhou Shen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, Jingren Zhou",
      "summary": "WebSailor：为网络智能体导航超人类推理\n\n本文介绍了WebSailor，一种旨在提升大型语言模型（LLM）在复杂信息检索任务中表现的后训练方法。该研究的核心洞察是，专有智能体系统（如DeepResearch）之所以能在BrowseComp等极端复杂的信息检索基准测试中展现超人类能力，关键在于它们能够系统性地降低在广阔信息环境中导航时的极端不确定性。\n\n**研究背景与问题**\n\n*   **LLM的认知局限性：** 跨越人类认知局限性是LLM训练中的一个关键前沿。\n*   **专有系统的突破：** DeepResearch等专有智能体系统已在BrowseComp等极其复杂的信息检索基准测试中展示了超人类能力，这是以前无法实现的。\n*   **成功关键：** 作者认为，这些专有系统的成功在于其拥有开源模型所缺乏的复杂推理模式——即系统性地降低在广阔信息环境中导航时极端不确定性的能力。\n\n**WebSailor 方法论**\n\n基于上述洞察，WebSailor被提出作为一套完整的后训练方法，旨在灌输这种关键能力。其方法包括：\n\n*   **生成高不确定性任务：** 通过结构化采样和信息混淆来生成新颖的、高不确定性的任务。\n*   **RFT冷启动：** 采用RFT（Reasoning Feedback Training）冷启动策略。\n*   **高效智能体强化学习训练算法：** 引入了名为“复制采样策略优化”（Duplicating Sampling Policy Optimization, DUPO）的强化学习算法。\n\n**成果与影响**\n\n通过这一集成管道，WebSailor在复杂的信息检索任务中显著优于所有开源智能体，其性能与专有智能体相匹配，从而弥合了能力差距。\n\n**研究领域**\n\n*   计算与语言 (cs.CL)\n*   人工智能 (cs.AI)",
      "shortSummary": "WebSailor是一种新的后训练方法，旨在赋予网络智能体超人类的推理能力，以应对复杂的信息检索任务。该方法通过生成高不确定性任务、RFT冷启动和高效的DUPO强化学习算法，系统性地降低导航不确定性。WebSailor显著超越了所有开源智能体，并达到了专有智能体的性能水平，成功弥合了能力差距。",
      "translated_title": "WebSailor：为网络智能体导航超人类推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all opensource agents in complex information-seeking tasks, matching proprietary agents' performance and closing the capability gap."
    },
    {
      "title": "倾听内在声音：通过中间特征反馈对齐ControlNet训练 (原标题: Heeding the Inner Voice: Aligning ControlNet Training via Intermediate Features Feedback)",
      "link": "https://arxiv.org/abs/2507.02321",
      "pubDate": "Thu, 03 Jul 2025 01:25:53 GMT",
      "isoDate": "2025-07-03T01:25:53.000Z",
      "creator": "Nina Konovalova, Maxim Nikolaev, Andrey Kuznetsov, Aibek Alanov",
      "summary": "## InnerControl：通过中间特征反馈对齐ControlNet训练\n\n### 摘要\n\n**背景与挑战**\n\n*   尽管文本到图像扩散模型取得了显著进展，但在生成输出时实现精确的空间控制仍然具有挑战性。\n*   ControlNet通过引入辅助条件模块解决了这一问题。\n*   ControlNet++通过仅在最终去噪步骤应用循环一致性损失，进一步完善了对齐。\n*   然而，ControlNet++的方法忽略了中间生成阶段，这限制了其有效性。\n\n**InnerControl 方法**\n\n*   **核心思想**：我们提出InnerControl，这是一种训练策略，旨在强制在所有扩散步骤中实现空间一致性。\n*   **实现机制**：\n    *   InnerControl 训练轻量级卷积探针（convolutional probes）。\n    *   这些探针在每个去噪步骤中，从中间UNet特征中重建输入控制信号（例如，边缘、深度）。\n    *   即使从高度噪声的潜在表示中，这些探针也能高效地提取信号，从而为训练提供伪真实（pseudo ground truth）控制。\n*   **对齐损失**：通过最小化整个扩散过程中预测条件与目标条件之间的差异，我们的对齐损失改进了控制保真度和生成质量。\n\n**优势与性能**\n\n*   InnerControl 显著提高了控制保真度（control fidelity）和生成质量。\n*   当与ControlNet++等既有技术结合使用时，InnerControl 在各种条件方法（例如，边缘、深度）上实现了最先进的性能。\n\n**代码可用性**\n\n*   相关代码已公开。",
      "shortSummary": "InnerControl是一种新的训练策略，旨在通过在所有去噪步骤中强制执行空间一致性来改进ControlNet的训练。它训练轻量级卷积探针，从中间UNet特征中重建控制信号，即使在高度噪声的潜在表示下也能有效工作。通过最小化整个扩散过程中预测与目标条件之间的差异，InnerControl显著提高了控制保真度和生成质量。该方法与ControlNet++结合，在多种条件方法上实现了最先进的性能。",
      "translated_title": "倾听内在声音：通过中间特征反馈对齐ControlNet训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite significant progress in text-to-image diffusion models, achieving precise spatial control over generated outputs remains challenging. ControlNet addresses this by introducing an auxiliary conditioning module, while ControlNet++ further refines alignment through a cycle consistency loss applied only to the final denoising steps. However, this approach neglects intermediate generation stages, limiting its effectiveness. We propose InnerControl, a training strategy that enforces spatial consistency across all diffusion steps. Our method trains lightweight convolutional probes to reconstruct input control signals (e.g., edges, depth) from intermediate UNet features at every denoising step. These probes efficiently extract signals even from highly noisy latents, enabling pseudo ground truth controls for training. By minimizing the discrepancy between predicted and target conditions throughout the entire diffusion process, our alignment loss improves both control fidelity and generation quality. Combined with established techniques like ControlNet++, InnerControl achieves state-of-the-art performance across diverse conditioning methods (e.g., edges, depth)."
    },
    {
      "title": "基于能量的Transformer：可扩展的学习者和思考者 (原标题: Energy-Based Transformers are Scalable Learners and Thinkers)",
      "link": "https://arxiv.org/abs/2507.02092",
      "pubDate": "Wed, 02 Jul 2025 15:17:29 GMT",
      "isoDate": "2025-07-02T15:17:29.000Z",
      "creator": "Alexi Gladstone, Ganesh Nanduru, Md Mofijul Islam, Peixuan Han, Hyeonjeong Ha, Aman Chadha, Yilun Du, Heng Ji, Jundong Li, Tariq Iqbal",
      "summary": "### 基于能量的Transformer：可扩展的学习者和思考者\n\n本文探讨了一种名为“基于能量的Transformer”（Energy-Based Transformers, EBTs）的新型模型，旨在解决当前推理时计算技术（类比人类系统2思维）的局限性，并实现模型仅通过无监督学习来“思考”的能力。\n\n#### 现有方法的局限性\n\n*   **模态特异性**：许多现有方法仅适用于特定模态，例如仅限于文本。\n*   **问题特异性**：它们通常只在特定问题领域有效，如可验证的数学和编程领域。\n*   **额外监督/训练需求**：这些方法往往需要在无监督预训练之外，额外进行监督学习或依赖验证器/可验证奖励。\n\n#### EBTs 的核心思想与工作原理\n\n本文提出，通过学习显式验证输入与候选预测之间的兼容性，并将预测问题重新定义为基于此验证器的优化问题，可以泛化系统2思维方法。\n\n*   **模型构建**：EBTs 是一种新型的基于能量的模型（Energy-Based Models, EBMs）。\n*   **能量分配**：EBTs 为每个输入和候选预测对分配一个能量值。\n*   **预测机制**：通过基于梯度下降的能量最小化过程进行预测，直至收敛。这种机制使其能够跨越离散（文本）和连续（视觉）模态进行操作。\n\n#### EBTs 的性能优势\n\n研究结果表明，EBTs 在多个方面展现出显著优势：\n\n*   **训练扩展性**：\n    *   在训练过程中，EBTs 的扩展速度快于主流的Transformer++方法。\n    *   在数据、批次大小、参数、FLOPs（浮点运算次数）和深度方面，EBTs 的扩展速率最高可提高35%。\n*   **推理性能（系统2思维）**：\n    *   在语言任务上，EBTs 通过系统2思维将性能提升了29%，优于Transformer++。\n    *   在图像去噪任务上，EBTs 使用更少的前向传播次数，但性能优于Diffusion Transformers。\n*   **泛化能力**：\n    *   在相同或更差的预训练性能下，EBTs 在大多数下游任务上取得了比现有模型更好的结果。\n    *   这表明EBTs 的泛化能力优于现有方法，能够更好地适应新任务和新领域。\n\n#### 结论\n\nEBTs 为扩展模型的学习和思考能力提供了一个有前景的新范式，有望推动人工智能模型在更广泛的应用场景中实现更高效、更通用的“思考”能力。",
      "shortSummary": "基于能量的Transformer（EBTs）是一种新型模型，旨在通过无监督学习实现可泛化的系统2思维。EBTs通过学习验证输入与预测的兼容性，并将预测重构为能量最小化问题。研究发现，EBTs在训练时比Transformer++扩展更快，在推理时通过系统2思维显著提升语言任务性能（29%），并在图像去噪上表现优异。EBTs在下游任务上展现出更好的泛化能力，预示着其在扩展模型学习和思考能力方面的巨大潜力。",
      "translated_title": "基于能量的Transformer：可扩展的学习者和思考者",
      "images": [],
      "contentSource": "完整文章",
      "content": "Inference-time computation techniques, analogous to human System 2 Thinking, have recently become popular for improving model performances. However, most existing approaches suffer from several limitations: they are modality-specific (e.g., working only in text), problem-specific (e.g., verifiable domains like math and coding), or require additional supervision/training on top of unsupervised pretraining (e.g., verifiers or verifiable rewards). In this paper, we ask the question \"Is it possible to generalize these System 2 Thinking approaches, and develop models that learn to think solely from unsupervised learning?\" Interestingly, we find the answer is yes, by learning to explicitly verify the compatibility between inputs and candidate-predictions, and then re-framing prediction problems as optimization with respect to this verifier. Specifically, we train Energy-Based Transformers (EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy value to every input and candidate-prediction pair, enabling predictions through gradient descent-based energy minimization until convergence. Across both discrete (text) and continuous (visual) modalities, we find EBTs scale faster than the dominant Transformer++ approach during training, achieving an up to 35% higher scaling rate with respect to data, batch size, parameters, FLOPs, and depth. During inference, EBTs improve performance with System 2 Thinking by 29% more than the Transformer++ on language tasks, and EBTs outperform Diffusion Transformers on image denoising while using fewer forward passes. Further, we find that EBTs achieve better results than existing models on most downstream tasks given the same or worse pretraining performance, suggesting that EBTs generalize better than existing approaches. Consequently, EBTs are a promising new paradigm for scaling both the learning and thinking capabilities of models."
    },
    {
      "title": "IntFold：一种用于通用和专用生物分子结构预测的可控基础模型 (原标题: IntFold: A Controllable Foundation Model for General and Specialized Biomolecular Structure Prediction)",
      "link": "https://arxiv.org/abs/2507.02025",
      "pubDate": "Wed, 02 Jul 2025 12:09:47 GMT",
      "isoDate": "2025-07-02T12:09:47.000Z",
      "creator": "The IntFold Team, Leon Qiao, Wayne Bai, He Yan, Gary Liu, Nova Xi, Xiang Zhang",
      "summary": "### IntFold：一种可控的生物分子结构预测基础模型\n\nIntFold是一种新型的可控基础模型，专为通用和专用生物分子结构预测而设计。\n\n#### 核心能力与性能\n\n*   **预测精度：** IntFold的预测精度与当前最先进的模型AlphaFold3相当。\n*   **技术优势：** 该模型采用了一种优越的定制化注意力核（attention kernel），这是其高性能的关键。\n\n#### 扩展应用与适应性\n\n*   **多功能性：** 除了标准的结构预测，IntFold还可通过使用独立的适配器（individual adapters）来适应多种专业预测任务，包括：\n    *   预测变构状态（allosteric states）。\n    *   预测受限结构（constrained structures）。\n    *   预测结合亲和力（binding affinity）。\n\n#### 创新特性\n\n*   **新型置信度头部：** IntFold引入了一个新颖的置信度头部（confidence head），用于评估对接质量（docking quality）。这一特性为抗体-抗原复合物等具有挑战性的目标提供了更细致的评估。\n\n#### 模型训练\n\n*   **计算密集型：** IntFold是一个计算密集型模型，其训练过程提供了宝贵的见解。",
      "shortSummary": "IntFold是一种新型可控基础模型，用于通用和专用生物分子结构预测。其预测精度与AlphaFold3相当，并采用优越的定制注意力核。IntFold可通过适配器预测变构状态、受限结构和结合亲和力。此外，它引入了新型置信度头部以评估对接质量，尤其适用于抗体-抗原复合物等复杂目标。该模型训练计算密集。",
      "translated_title": "IntFold：一种用于通用和专用生物分子结构预测的可控基础模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce IntFold, a controllable foundation model for both general and specialized biomolecular structure prediction. IntFold demonstrates predictive accuracy comparable to the state-of-the-art AlphaFold3, while utilizing a superior customized attention kernel. Beyond standard structure prediction, IntFold can be adapted to predict allosteric states, constrained structures, and binding affinity through the use of individual adapters. Furthermore, we introduce a novel confidence head to estimate docking quality, offering a more nuanced assessment for challenging targets such as antibody-antigen complexes. Finally, we share insights gained during the training process of this computationally intensive model."
    },
    {
      "title": "AsyncFlow：一种用于高效LLM后训练的异步流式RL框架 (原标题: AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training)",
      "link": "https://arxiv.org/abs/2507.01663",
      "pubDate": "Wed, 02 Jul 2025 08:45:34 GMT",
      "isoDate": "2025-07-02T08:45:34.000Z",
      "creator": "Zhenyu Han, Ansheng You, Haibo Wang, Kui Luo, Guang Yang, Wenqi Shi, Menglong Chen, Sicheng Zhang, Zeshun Lan, Chunshi Deng, Huazhong Ji, Wenjie Liu, Yu Huang, Yixiang Zhang, Chenyi Pan, Jing Wang, Xin Huang, Chunsheng Li, Jianping Wu",
      "summary": "## AsyncFlow：一种用于高效LLM后训练的异步流式RL框架\n\n### 挑战与问题\n\n在大型语言模型（LLM）的后训练阶段，强化学习（RL）已成为关键技术。然而，现有RL框架面临以下挑战：\n\n*   **传统任务共置（task-colocated）RL框架**：存在显著的可扩展性瓶颈。\n*   **任务分离（task-separated）RL框架**：面临复杂数据流、资源闲置和工作负载不平衡的问题。\n*   **耦合性问题**：大多数现有框架与LLM训练或推理引擎紧密耦合，难以支持自定义引擎。\n\n### AsyncFlow 解决方案\n\n为解决上述挑战，本文提出了 **AsyncFlow**，一个用于高效后训练的异步流式RL框架。其核心特性包括：\n\n1.  **分布式数据存储与传输模块**：\n    *   提供统一的数据管理和细粒度调度能力。\n    *   以完全流式（fully streamed）方式运行。\n    *   固有地促进RL任务之间的自动化流水线重叠（pipeline overlapping）。\n    *   实现动态负载均衡。\n\n2.  **基于生产者-消费者（producer-consumer）的异步工作流**：\n    *   通过在“陈旧度阈值”（staleness thresholds）内策略性地推迟参数更新过程，最大限度地减少计算空闲时间。\n\n3.  **解耦的架构**：\n    *   AsyncFlow 的核心能力与底层训练和推理引擎在架构上解耦。\n    *   通过面向服务的用户界面（service-oriented user interfaces）进行封装，提供模块化和可定制的用户体验。\n\n### 实验结果与意义\n\n*   **性能提升**：广泛的实验表明，与现有最先进的基线相比，AsyncFlow 的吞吐量平均提高了1.59倍。\n*   **未来展望**：本文提出的架构为下一代RL训练系统设计提供了可操作的见解。",
      "shortSummary": "AsyncFlow是一种用于高效LLM后训练的异步流式强化学习（RL）框架。它旨在解决传统RL框架在可扩展性、数据流复杂性、资源闲置和与LLM引擎紧密耦合等方面的挑战。AsyncFlow通过引入分布式数据管理、基于生产者-消费者的异步工作流以及解耦的架构，实现了任务流水线重叠和动态负载均衡，并最大限度地减少计算空闲。实验表明，AsyncFlow的吞吐量平均提升了1.59倍，为未来RL训练系统设计提供了新思路。",
      "translated_title": "AsyncFlow：一种用于高效LLM后训练的异步流式RL框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement learning (RL) has become a pivotal technology in the post-training phase of large language models (LLMs). Traditional task-colocated RL frameworks suffer from significant scalability bottlenecks, while task-separated RL frameworks face challenges in complex dataflows and the corresponding resource idling and workload imbalance. Moreover, most existing frameworks are tightly coupled with LLM training or inference engines, making it difficult to support custom-designed engines. To address these challenges, we propose AsyncFlow, an asynchronous streaming RL framework for efficient post-training. Specifically, we introduce a distributed data storage and transfer module that provides a unified data management and fine-grained scheduling capability in a fully streamed manner. This architecture inherently facilitates automated pipeline overlapping among RL tasks and dynamic load balancing. Moreover, we propose a producer-consumer-based asynchronous workflow engineered to minimize computational idleness by strategically deferring parameter update process within staleness thresholds. Finally, the core capability of AsynFlow is architecturally decoupled from underlying training and inference engines and encapsulated by service-oriented user interfaces, offering a modular and customizable user experience. Extensive experiments demonstrate an average of 1.59 throughput improvement compared with state-of-the-art baseline. The presented architecture in this work provides actionable insights for next-generation RL training system designs."
    },
    {
      "title": "Skywork-Reward-V2：通过人机协同扩展偏好数据整理 (原标题: Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy)",
      "link": "https://arxiv.org/abs/2507.01352",
      "pubDate": "Wed, 02 Jul 2025 00:40:29 GMT",
      "isoDate": "2025-07-02T00:40:29.000Z",
      "creator": "Chris Yuhao Liu, Liang Zeng, Yuzhen Xiao, Jujie He, Jiacai Liu, Chaojie Wang, Rui Yan, Wei Shen, Fuxiang Zhang, Jiacheng Xu, Yang Liu, Yahui Zhou",
      "summary": "### 背景与问题\n\n*   **奖励模型（RMs）的重要性与当前局限性：** 奖励模型在人类反馈强化学习（RLHF）中扮演着至关重要的角色。然而，当前最先进的开放式奖励模型在大多数现有评估基准上表现不佳，未能有效捕捉人类偏好的细微差别和复杂性。\n*   **性能不佳的根源：** 即使采用了先进的训练技术，性能也未见显著提升。作者推测，这种脆弱性主要源于偏好数据集的局限性，这些数据集通常范围狭窄、标签合成或缺乏严格的质量控制。\n\n### 解决方案：SynPref-40M 数据集\n\n*   为了解决上述挑战，本文提出了一个名为 **SynPref-40M** 的大规模偏好数据集，该数据集包含 **4000万个偏好对**。\n\n### 数据整理方法：人机协同管道\n\n*   **设计理念：** 为了实现大规模数据整理，研究团队设计了一个 **人机协同的两阶段管道**，该管道充分利用了人类标注的质量优势和AI的可扩展性。\n*   **工作流程：** 在此管道中，人类负责提供经过验证的高质量标注，而大型语言模型（LLMs）则根据人类的指导进行自动化数据整理。\n\n### 模型介绍：Skywork-Reward-V2 系列\n\n*   **训练与规模：** 在整理后的偏好数据混合上，本文推出了 **Skywork-Reward-V2** 系列模型。该系列包含 **八个奖励模型**，参数规模从0.6B到8B不等。\n*   **数据来源：** 这些模型在SynPref-40M中精心筛选的 **2600万个偏好对子集** 上进行训练。\n\n### 性能与能力\n\n*   **多功能性：** Skywork-Reward-V2 在广泛的能力范围内表现出卓越的多功能性，包括：\n    *   与人类偏好高度对齐\n    *   客观正确性\n    *   安全性\n    *   抵抗风格偏见\n    *   最佳N选择（best-of-N scaling）\n*   **最先进表现：** 在七个主要的奖励模型基准测试中，Skywork-Reward-V2 均达到了 **最先进的性能**。\n\n### 消融研究与关键发现\n\n*   **有效性来源：** 消融研究证实，该方法的有效性不仅源于数据规模的扩大，更重要的是源于 **高质量的数据整理**。\n\n### 结论与意义\n\n*   **重大进展：** Skywork-Reward-V2 系列代表了开放式奖励模型领域的重大进展。\n*   **潜力挖掘：** 它突显了现有偏好数据集中尚未开发的巨大潜力。\n*   **协同效益：** 该工作证明了人机协同整理如何能够显著提高数据质量，为未来奖励模型的发展提供了新的方向。",
      "shortSummary": "为解决现有奖励模型因偏好数据质量差而表现不佳的问题，本文提出了SynPref-40M，一个包含4000万偏好对的大规模数据集。研究团队设计了人机协同的两阶段管道进行高质量数据整理，并在此基础上训练了Skywork-Reward-V2系列模型。Skywork-Reward-V2在七个主要奖励模型基准上实现了最先进的性能，证明了数据规模与高质量整理的重要性，并展示了人机协同在提升数据质量方面的巨大潜力。",
      "translated_title": "Skywork-Reward-V2：通过人机协同扩展偏好数据整理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite the critical role of reward models (RMs) in reinforcement learning from human feedback (RLHF), current state-of-the-art open RMs perform poorly on most existing evaluation benchmarks, failing to capture the spectrum of nuanced and sophisticated human preferences. Even approaches that incorporate advanced training techniques have not yielded meaningful performance improvements. We hypothesize that this brittleness stems primarily from limitations in preference datasets, which are often narrowly scoped, synthetically labeled, or lack rigorous quality control. To address these challenges, we present a large-scale preference dataset comprising 40 million preference pairs, named SynPref-40M. To enable data curation at scale, we design a human-AI synergistic two-stage pipeline that leverages the complementary strengths of human annotation quality and AI scalability. In this pipeline, humans provide verified annotations, while large language models perform automatic curation based on human guidance. Training on this preference mixture, we introduce Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B parameters, trained on a carefully curated subset of 26 million preference pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile across a wide range of capabilities, including alignment with human preferences, objective correctness, safety, resistance to stylistic biases, and best-of-N scaling, achieving state-of-the-art performance across seven major reward model benchmarks. Ablation studies confirm that the effectiveness of our approach stems not only from data scale but also from high-quality curation. The Skywork-Reward-V2 series represents substantial progress in open reward models, highlighting the untapped potential of existing preference datasets and demonstrating how human-AI curation synergy can unlock significantly higher data quality."
    },
    {
      "title": "GLM-4.1V-Thinking：迈向通用多模态推理的可扩展强化学习 (原标题: GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2507.01006",
      "pubDate": "Tue, 01 Jul 2025 13:55:04 GMT",
      "isoDate": "2025-07-01T13:55:04.000Z",
      "creator": "Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Leyi Pan, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianle Gong, Wenkai Li, Wei Jia, Xin Lyu, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuxuan Zhang, Zhanxiao Du, Zhenyu Hou, Zhao Xue, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, Jie Tang",
      "summary": "GLM-4.1V-Thinking 是一种旨在推进通用多模态推理的视觉-语言模型（VLM）。该报告分享了其以推理为中心的训练框架的关键发现。\n\n*   **模型开发与训练：**\n    *   首先，通过大规模预训练开发了一个具有巨大潜力的视觉基础模型，这为最终性能设定了上限。\n    *   随后，采用“课程采样强化学习”（Reinforcement Learning with Curriculum Sampling, RLCS）技术，充分释放了模型的潜力。RLCS 显著提升了模型在多种任务上的综合能力。\n\n*   **增强的推理能力：**\n    GLM-4.1V-Thinking 在广泛的任务中展现出全面的能力增强，包括：\n    *   科学、技术、工程和数学（STEM）问题解决\n    *   视频理解\n    *   内容识别\n    *   编码\n    *   接地（Grounding）\n    *   基于图形用户界面（GUI）的智能代理\n    *   长文档理解\n\n*   **开源与性能表现：**\n    *   为了促进该领域的研究，研究团队开源了 GLM-4.1V-9B-Thinking 模型，该模型在同等规模的模型中达到了最先进的性能。\n    *   在一项涵盖28个公共基准的全面评估中，GLM-4.1V-9B-Thinking 在几乎所有任务上都超越了 Qwen2.5-VL-7B。\n    *   与规模显著更大的 Qwen2.5-VL-72B 相比，GLM-4.1V-9B-Thinking 在18个基准测试中表现出可比甚至更优的性能。\n    *   值得注意的是，GLM-4.1V-9B-Thinking 在长文档理解和 STEM 推理等挑战性任务上，与 GPT-4o 等闭源模型相比，也展现出竞争或更优的性能，进一步凸显了其强大的能力。\n\n*   **资源可用性：**\n    模型的代码、模型文件和更多信息已在该报告中提及的链接上发布。",
      "shortSummary": "GLM-4.1V-Thinking 是一种新型视觉-语言模型，旨在提升通用多模态推理能力。其核心创新在于采用课程采样强化学习（RLCS）框架，显著增强了模型在STEM问题解决、视频理解、长文档理解等多样化任务上的表现。开源的GLM-4.1V-9B-Thinking在同等规模模型中达到SOTA，并在多项基准测试中超越Qwen2.5-VL-7B，甚至在某些方面媲美或优于更大的Qwen2.5-VL-72B和GPT-4o等闭源模型，展现出强大实力。",
      "translated_title": "GLM-4.1V-Thinking：迈向通用多模态推理的可扩展强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to advance general-purpose multimodal reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop a capable vision foundation model with significant potential through large-scale pre-training, which arguably sets the upper bound for the final performance. Reinforcement Learning with Curriculum Sampling (RLCS) then unlocks the full potential of the model, leading to comprehensive capability enhancement across a diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document understanding, among others. To facilitate research in this field, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art performance among models of comparable size. In a comprehensive evaluation across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all tasks and achieves comparable or even superior performance on 18 benchmarks relative to the significantly larger Qwen2.5-VL-72B. Notably, GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance compared to closed-source models such as GPT-4o on challenging tasks including long document understanding and STEM reasoning, further underscoring its strong capabilities. Code, models and more information are released at https://github.com/THUDM/GLM-4.1V-Thinking."
    },
    {
      "title": "ZeCO：线性注意力机制的零通信开销序列并行化 (原标题: ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention)",
      "link": "https://arxiv.org/abs/2507.01004",
      "pubDate": "Tue, 01 Jul 2025 13:54:53 GMT",
      "isoDate": "2025-07-01T13:54:53.000Z",
      "creator": "Yuhong Chou, Zehao Liu, Ruijie Zhu, Xinyi Wan, Tianjian Li, Congying Chu, Qian Liu, Jibin Wu, Zejun Ma",
      "summary": "ZeCO：线性注意力机制的零通信开销序列并行化\n\n*   **背景与问题：**\n    *   线性注意力机制通过提供线性计算复杂度，为大型语言模型（LLMs）带来了显著优势，使其能够高效处理超长序列（例如100万上下文）。\n    *   然而，现有的序列并行化（SP）方法，作为在设备间分配这些工作负载的关键技术，由于大量的通信开销而成为主要的性能瓶颈。\n\n*   **ZeCO方法介绍：**\n    *   本文提出了一种名为ZeCO（零通信开销）的新型序列并行化方法，专为线性注意力模型设计，旨在克服现有SP方法的局限性，并实现长序列训练的端到端近线性可扩展性。\n    *   **核心创新：** ZeCO的核心在于All-Scan，这是一种新型的集体通信原语。All-Scan能够为每个SP秩（rank）精确提供其所需的初始操作符状态，同时保持最小的通信足迹，从而有效消除了通信开销。\n\n*   **理论与实证结果：**\n    *   **理论证明：** ZeCO在理论上被证明是最佳的，仅引入了可忽略不计的时间和空间开销。\n    *   **实证比较：** 经验性地比较了不同序列并行化策略的通信成本，结果表明All-Scan在SP场景中实现了最快的通信速度。\n    *   **性能提升：** 具体而言，在256个GPU上处理800万序列长度时，ZeCO比当前最先进（SOTA）的SP方法实现了60%的加速。\n    *   **应用示例：** 使用ZeCO在64个设备上训练一个100万序列长度的模型，所需时间大致与在单个设备上训练1.6万序列长度模型的时间相同。\n\n*   **结论与意义：**\n    *   ZeCO为高效训练下一代LLMs在以前难以处理的序列长度上开辟了一条清晰的道路。",
      "shortSummary": "ZeCO是一种针对线性注意力机制的新型序列并行化方法。它通过引入名为All-Scan的集体通信原语，有效消除了现有序列并行化方法中的通信开销。ZeCO实现了对超长序列训练的近线性可扩展性，并在理论上被证明是最佳的。实验结果显示，ZeCO比现有最先进方法提速60%，使得在以前难以处理的序列长度上高效训练下一代大型语言模型成为可能。",
      "translated_title": "ZeCO：线性注意力机制的零通信开销序列并行化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Linear attention mechanisms deliver significant advantages for Large Language Models (LLMs) by providing linear computational complexity, enabling efficient processing of ultra-long sequences (e.g., 1M context). However, existing Sequence Parallelism (SP) methods, essential for distributing these workloads across devices, become the primary bottleneck due to substantial communication overhead. In this paper, we introduce ZeCO (Zero Communication Overhead) sequence parallelism for linear attention models, a new SP method designed to overcome these limitations and achieve end-to-end near-linear scalability for long sequence training. For example, training a model with a 1M sequence length across 64 devices using ZeCO takes roughly the same time as training with an 16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new collective communication primitive. All-Scan provides each SP rank with precisely the initial operator state it requires while maintaining a minimal communication footprint, effectively eliminating communication overhead. Theoretically, we prove the optimaity of ZeCO, showing that it introduces only negligible time and space overhead. Empirically, we compare the communication costs of different sequence parallelism strategies and demonstrate that All-Scan achieves the fastest communication in SP scenarios. Specifically, on 256 GPUs with an 8M sequence length, ZeCO achieves a 60\\% speedup compared to the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a clear path toward efficiently training next-generation LLMs on previously intractable sequence lengths."
    },
    {
      "title": "SciArena：一个用于科学文献任务中基础模型的开放评估平台 (原标题: SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks)",
      "link": "https://arxiv.org/abs/2507.01001",
      "pubDate": "Tue, 01 Jul 2025 13:51:59 GMT",
      "isoDate": "2025-07-01T13:51:59.000Z",
      "creator": "Yilun Zhao, Kaiyan Zhang, Tiansheng Hu, Sihong Wu, Ronan Le Bras, Taira Anderson, Jonathan Bragg, Joseph Chee Chang, Jesse Dodge, Matt Latzke, Yixin Liu, Charles McGrady, Xiangru Tang, Zihang Wang, Chen Zhao, Hannaneh Hajishirzi, Doug Downey, Arman Cohan",
      "summary": "## SciArena：一个开放评估平台\n\n### 核心介绍\nSciArena是一个开放且协作的平台，旨在评估基础模型在科学文献任务中的表现。与传统的科学文献理解和综合基准测试不同，SciArena采用类似Chatbot Arena的评估方法，通过社区投票直接让研究界参与模型比较。该平台利用集体智慧，对需要基于文献的长篇响应的开放式科学任务进行社区驱动的模型性能评估。\n\n### 平台现状与数据\n*   **支持模型**：目前，SciArena支持23种开源和专有基础模型。\n*   **数据收集**：平台已从不同科学领域的可信研究人员那里收集了超过13,000张投票。\n\n### 数据分析与发现\n*   **问题多样性**：对已收集数据的分析证实，提交的问题具有多样性，并且与实际的文献需求高度吻合。\n*   **评估质量**：参与评估的研究人员表现出高度的自我一致性和注释者间一致性，这表明评估结果的可靠性。\n*   **模型排名**：平台根据模型排名排行榜讨论了评估结果和由此获得的见解。\n\n### 额外贡献：SciArena-Eval基准测试\n*   **目的**：为进一步促进构建基于模型的文献任务自动化评估系统的研究，SciArena发布了SciArena-Eval。这是一个基于平台收集到的偏好数据的元评估基准测试。\n*   **评估方法**：SciArena-Eval通过比较模型与人类投票的成对评估，来衡量模型判断答案质量的准确性。\n*   **挑战与需求**：通过该基准测试进行的实验突显了其固有的挑战性，并强调了开发更可靠的自动化评估方法的需求。",
      "shortSummary": "SciArena是一个开放协作平台，用于评估科学文献任务中的基础模型。它采用社区投票方式，已收集来自可信研究人员的13,000多张投票，评估23种模型。平台分析显示，问题多样且评估一致性高。为促进自动评估研究，SciArena还发布了SciArena-Eval基准测试，旨在衡量模型判断答案质量的准确性，并强调了开发更可靠自动评估方法的必要性。",
      "translated_title": "SciArena：一个用于科学文献任务中基础模型的开放评估平台",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present SciArena, an open and collaborative platform for evaluating foundation models on scientific literature tasks. Unlike traditional benchmarks for scientific literature understanding and synthesis, SciArena engages the research community directly, following the Chatbot Arena evaluation approach of community voting on model comparisons. By leveraging collective intelligence, SciArena offers a community-driven evaluation of model performance on open-ended scientific tasks that demand literature-grounded, long-form responses. The platform currently supports 23 open-source and proprietary foundation models and has collected over 13,000 votes from trusted researchers across diverse scientific domains. We analyze the data collected so far and confirm that the submitted questions are diverse, aligned with real-world literature needs, and that participating researchers demonstrate strong self-consistency and inter-annotator agreement in their evaluations. We discuss the results and insights based on the model ranking leaderboard. To further promote research in building model-based automated evaluation systems for literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based on our collected preference data. The benchmark measures the accuracy of models in judging answer quality by comparing their pairwise assessments with human votes. Our experiments highlight the benchmark's challenges and emphasize the need for more reliable automated evaluation methods."
    },
    {
      "title": "超越令牌思考：从类脑智能到通用人工智能的认知基础及其社会影响 (原标题: Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact)",
      "link": "https://arxiv.org/abs/2507.00951",
      "pubDate": "Tue, 01 Jul 2025 12:52:25 GMT",
      "isoDate": "2025-07-01T12:52:25.000Z",
      "creator": "Rizwan Qureshi, Ranjan Sapkota, Abbas Shah, Amgad Muneer, Anas Zafar, Ashmal Vayani, Maged Shoman, Abdelrahman B. M. Eldaly, Kai Zhang, Ferhat Sadak, Shaina Raza, Xinqi Fan, Ravid Shwartz-Ziv, Hong Yan, Vinjia Jain, Aman Chadha, Manoj Karkee, Jia Wu, Philip Torr, Seyedali Mirjalili",
      "summary": "## 超越令牌思考：通用人工智能的认知基础与未来路径\n\n### 引言\n\n本文深入探讨了通用人工智能（AGI）的核心问题：机器能否真正像人类一样思考、推理和行动。尽管当前的大型模型，如GPT-4.5、DeepSeek、Claude 3.5 Sonnet、Phi-4和Grok 3，展现出多模态流畅性和部分推理能力，但它们仍受限于对令牌级预测的依赖以及缺乏具身能动性。\n\n### 跨学科综合视角\n\n为了推动AGI的发展，本文提供了一个跨学科的综合分析，涵盖了人工智能、认知神经科学、心理学、生成模型和基于代理的系统等多个领域。\n\n### 通用智能的架构与认知基础\n\n文章强调了构建通用智能的关键架构和认知基础，包括：\n\n*   **模块化推理：** 能够将复杂问题分解为可管理的部分并进行独立推理的能力。\n*   **持久记忆：** 能够长期存储和检索信息，以支持持续学习和决策。\n*   **多智能体协调：** 多个智能体之间协同工作以实现共同目标的能力。\n\n### 新兴框架与泛化策略\n\n*   **Agentic RAG框架：** 本文特别强调了Agentic RAG（检索增强生成）框架的兴起，该框架结合了检索、规划和动态工具使用，从而实现更具适应性的行为。\n*   **泛化策略：** 为了实现灵活、领域无关的智能，文章讨论了多种泛化策略，包括：\n    *   信息压缩\n    *   测试时适应\n    *   免训练方法\n\n### 视觉-语言模型（VLMs）的重新审视\n\n视觉-语言模型（VLMs）被重新审视为不仅仅是感知模块，更是具身理解和协作任务完成的演进接口。\n\n### 对“真正智能”的理解\n\n文章认为，真正的智能并非单纯来自规模的扩大，而是源于记忆与推理的深度整合。这是一种模块化、交互式和自我改进组件的协同作用，其中信息压缩是实现适应性行为的关键。\n\n### 弥合差距的途径\n\n通过借鉴神经符号系统、强化学习和认知支架等领域的最新进展，研究人员正在探索如何弥合统计学习与目标导向认知之间的鸿沟。\n\n### 未来挑战\n\n最后，文章指出了在实现AGI道路上所面临的关键科学、技术和伦理挑战。",
      "shortSummary": "本文探讨通用人工智能（AGI）的进展与挑战。当前模型虽能力增强，但受限于令牌预测和缺乏具身能动性。文章提出，AGI需超越令牌，整合模块化推理、持久记忆和多智能体协调。Agentic RAG框架和信息压缩是实现适应性行为的关键。真正的智能源于记忆与推理的整合，而非单纯规模。神经符号系统等技术正弥合统计学习与目标导向认知间的鸿沟。文章最后指出AGI面临的科学、技术和伦理挑战。",
      "translated_title": "超越令牌思考：从类脑智能到通用人工智能的认知基础及其社会影响",
      "images": [],
      "contentSource": "完整文章",
      "content": "Can machines truly think, reason and act in domains like humans? This enduring question continues to shape the pursuit of Artificial General Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5, DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal fluency and partial reasoning, these systems remain fundamentally limited by their reliance on token-level prediction and lack of grounded agency. This paper offers a cross-disciplinary synthesis of AGI development, spanning artificial intelligence, cognitive neuroscience, psychology, generative models, and agent-based systems. We analyze the architectural and cognitive foundations of general intelligence, highlighting the role of modular reasoning, persistent memory, and multi-agent coordination. In particular, we emphasize the rise of Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use to enable more adaptive behavior. We discuss generalization strategies, including information compression, test-time adaptation, and training-free methods, as critical pathways toward flexible, domain-agnostic intelligence. Vision-Language Models (VLMs) are reexamined not just as perception modules but as evolving interfaces for embodied understanding and collaborative task completion. We also argue that true intelligence arises not from scale alone but from the integration of memory and reasoning: an orchestration of modular, interactive, and self-improving components where compression enables adaptive behavior. Drawing on advances in neurosymbolic systems, reinforcement learning, and cognitive scaffolding, we explore how recent architectures begin to bridge the gap between statistical learning and goal-directed cognition. Finally, we identify key scientific, technical, and ethical challenges on the path to AGI."
    },
    {
      "title": "数学推理能力是否能提升大型语言模型的通用能力？理解大型语言模型推理的迁移性 (原标题: Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning)",
      "link": "https://arxiv.org/abs/2507.00432",
      "pubDate": "Tue, 01 Jul 2025 01:23:05 GMT",
      "isoDate": "2025-07-01T01:23:05.000Z",
      "creator": "Maggie Huan, Yuetai Li, Tuney Zheng, Xiaoyu Xu, Seungone Kim, Minxin Du, Radha Poovendran, Graham Neubig, Xiang Yue",
      "summary": "# 数学推理能力是否能提升大型语言模型的通用能力？理解大型语言模型推理的迁移性\n\n## 引言\n大型语言模型（LLMs）在数学推理基准测试（如MATH和AIME）上取得了显著进展，其表现迅速超越了人类水平。然而，随着数学排行榜的每周更新，一个关键问题浮出水面：这些进步是反映了更广泛的问题解决能力，还是仅仅是狭隘的过拟合？\n\n## 研究目的与方法\n为了回答这一问题，本研究评估了20多个开源的推理调优模型，涵盖了广泛的任务套件，包括数学、科学问答、智能体规划、编码和标准指令遵循。为了更严格地研究这一现象，研究人员对Qwen3-14B模型进行了受控实验，这些模型仅使用数学数据进行训练，但采用了不同的调优方法。此外，研究还通过潜在空间表示和token空间分布偏移分析来深入理解模型行为。\n\n## 主要发现\n*   **迁移性不足：** 令人惊讶的是，大多数在数学任务上取得成功的模型未能将其优势迁移到其他领域。这表明数学推理能力的提升可能并未带来普遍的泛化能力。\n*   **调优方法的影响：**\n    *   **强化学习（RL）调优的模型：** 表现出良好的跨领域泛化能力。\n    *   **监督微调（SFT）调优的模型：** 经常会遗忘其通用能力。\n*   **机制分析：**\n    *   **SFT的影响：** SFT会引起显著的表示（latent-space representation）和输出（token-space distribution）漂移。这意味着SFT可能会改变模型的内部结构和输出分布，使其偏离通用领域。\n    *   **RL的优势：** RL则能保留通用领域结构，这可能是其泛化能力更好的原因。\n\n## 结论与建议\n本研究结果表明，有必要重新思考标准的后训练（post-training）方法，特别是目前在推进推理模型时对SFT蒸馏数据的依赖。为了实现更广泛的通用能力提升，可能需要探索或优化其他调优策略，例如强化学习。",
      "shortSummary": "本研究探讨了大型语言模型（LLM）在数学推理上的进步是否能提升其通用能力。结果发现，大多数在数学上表现出色的模型未能将优势迁移到其他领域。受控实验表明，强化学习（RL）调优的模型泛化能力良好，而监督微调（SFT）调优的模型常遗忘通用能力。分析显示SFT导致模型表示和输出漂移，而RL则保留通用结构。研究建议重新思考依赖SFT蒸馏数据的后训练方法。",
      "translated_title": "数学推理能力是否能提升大型语言模型的通用能力？理解大型语言模型推理的迁移性",
      "images": [],
      "contentSource": "完整文章",
      "content": "Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, we evaluate over 20 open-weight reasoning-tuned models across a broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. We surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, we conduct controlled experiments on Qwen3-14B models using math-only data but different tuning methods. We find that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. Our results suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models."
    },
    {
      "title": "FreeLong++：通过多波段频谱融合实现免训练长视频生成 (原标题: FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion)",
      "link": "https://arxiv.org/abs/2507.00162",
      "pubDate": "Mon, 30 Jun 2025 14:11:21 GMT",
      "isoDate": "2025-06-30T14:11:21.000Z",
      "creator": "Yu Lu, Yi Yang",
      "summary": "## FreeLong++：通过多波段频谱融合实现免训练长视频生成\n\n### 引言\n\n近年来，视频生成模型在从文本提示生成高质量短视频方面取得了显著进展。然而，将这些模型扩展到更长的视频仍然是一个重大挑战，主要问题在于时间一致性和视觉保真度的下降。初步观察表明，简单地将短视频生成模型应用于长序列会导致明显的质量下降。进一步分析发现，随着视频长度的增加，高频分量会变得越来越扭曲，我们将此问题称为“高频失真”。\n\n### FreeLong框架\n\n为了解决高频失真问题，我们提出了FreeLong，一个免训练的框架，旨在在去噪过程中平衡长视频特征的频率分布。FreeLong通过以下方式实现这一点：\n\n*   **融合全局低频特征**：这些特征捕捉整个视频的整体语义。\n*   **融合局部高频特征**：这些特征从短时间窗口中提取，以保留精细细节。\n\n### FreeLong++扩展\n\nFreeLong++在FreeLong双分支设计的基础上进行了扩展，形成了一个多分支架构，包含多个注意力分支，每个分支在不同的时间尺度上操作。通过安排从全局到局部的多种窗口大小，FreeLong++实现了从低频到高频的多波段频率融合，从而确保了长视频序列的语义连续性和精细的运动动态。\n\n### 主要优势与应用\n\nFreeLong++无需任何额外训练，即可即插即用于现有视频生成模型（例如Wan2.1和LTX-Video），以生成具有显著改善的时间一致性和视觉保真度的长视频。我们的方法在长视频生成任务（例如，原生长度的4倍和8倍）上表现优于现有方法。它还支持：\n\n*   **连贯的多提示视频生成**：具有平滑的场景过渡。\n*   **可控视频生成**：使用长深度或姿态序列进行控制。\n\n### 当前状态\n\n该研究目前正在评审中。",
      "shortSummary": "FreeLong++ 是一种免训练框架，旨在解决长视频生成中时间一致性和视觉保真度下降（特别是高频失真）的问题。它通过多波段频谱融合，结合全局低频特征与局部高频特征，平衡视频特征的频率分布。该方法可即插即用于现有视频生成模型，显著提升长视频的质量，并支持多提示和可控生成，表现优于现有方法。",
      "translated_title": "FreeLong++：通过多波段频谱融合实现免训练长视频生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in video generation models have enabled high-quality short video generation from text prompts. However, extending these models to longer videos remains a significant challenge, primarily due to degraded temporal consistency and visual fidelity. Our preliminary observations show that naively applying short-video generation models to longer sequences leads to noticeable quality degradation. Further analysis identifies a systematic trend where high-frequency components become increasingly distorted as video length grows, an issue we term high-frequency distortion. To address this, we propose FreeLong, a training-free framework designed to balance the frequency distribution of long video features during the denoising process. FreeLong achieves this by blending global low-frequency features, which capture holistic semantics across the full video, with local high-frequency features extracted from short temporal windows to preserve fine details. Building on this, FreeLong++ extends FreeLong dual-branch design into a multi-branch architecture with multiple attention branches, each operating at a distinct temporal scale. By arranging multiple window sizes from global to local, FreeLong++ enables multi-band frequency fusion from low to high frequencies, ensuring both semantic continuity and fine-grained motion dynamics across longer video sequences. Without any additional training, FreeLong++ can be plugged into existing video generation models (e.g. Wan2.1 and LTX-Video) to produce longer videos with substantially improved temporal consistency and visual fidelity. We demonstrate that our approach outperforms previous methods on longer video generation tasks (e.g. 4x and 8x of native length). It also supports coherent multi-prompt video generation with smooth scene transitions and enables controllable video generation using long depth or pose sequences."
    },
    {
      "title": "用图像进行多模态推理：基础、方法与未来前沿 (原标题: Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers)",
      "link": "https://arxiv.org/abs/2506.23918",
      "pubDate": "Mon, 30 Jun 2025 10:48:35 GMT",
      "isoDate": "2025-06-30T10:48:35.000Z",
      "creator": "Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, Linjie Li, Yu Cheng, Heng Ji, Junxian He, Yi R. Fung",
      "summary": "## 用图像进行多模态推理：基础、方法与未来前沿\n\n### 引言与背景\n\n*   **现有范式局限：** 近期多模态推理的显著进展主要得益于文本思维链（CoT）范式，即模型在语言内部进行推理。然而，这种以文本为中心的方法将视觉视为静态的初始上下文，导致丰富的感知数据与离散的符号思维之间存在根本性的“语义鸿沟”。\n*   **人类认知启发：** 人类认知常超越语言，将视觉用作动态的心理草图板。\n*   **AI领域新范式：** 类似演变正发生在AI领域，标志着从仅仅“思考图像”（think about images）到真正“用图像思考”（think with images）的根本性范式转变。\n\n### “用图像思考”范式\n\n*   **核心特征：** 这种新兴范式的特点是模型利用视觉信息作为其思维过程的中间步骤。\n*   **视觉角色转变：** 视觉从被动输入转变为动态、可操作的认知工作空间。\n\n### 智能演进的三个关键阶段\n\n本综述将智能的演进轨迹描绘为认知自主性不断增强的过程，并将其划分为三个关键阶段：\n\n1.  **外部工具探索 (External Tool Exploration)**\n2.  **程序化操作 (Programmatic Manipulation)**\n3.  **内在想象 (Intrinsic Imagination)**\n\n### 本综述的四大核心贡献\n\n为了构建这个快速发展的领域，本综述做出了以下四项关键贡献：\n\n1.  建立了“用图像思考”范式的基本原则及其三阶段框架。\n2.  全面回顾了表征该路线图每个阶段的核心方法。\n3.  分析了评估基准和变革性应用的关键格局。\n4.  识别了重大挑战并概述了有前景的未来方向。\n\n### 目标\n\n*   通过提供这种结构化的概述，本综述旨在为未来研究提供清晰的路线图，以实现更强大、更符合人类的多模态人工智能。\n\n**重要提示：** 文章内容中未包含有效的实际图片链接，因此详细摘要中不包含任何图片。",
      "shortSummary": "本综述探讨了多模态AI中从“思考图像”到“用图像思考”的范式转变。传统方法将视觉视为静态输入，存在“语义鸿沟”。新的范式将视觉作为动态、可操作的认知工作空间，并将其演进分为外部工具探索、程序化操作和内在想象三个阶段。综述建立了该范式的基础、回顾了核心方法、分析了评估基准与应用，并指出了未来方向，旨在为多模态AI研究提供清晰路线图。",
      "translated_title": "用图像进行多模态推理：基础、方法与未来前沿",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent progress in multimodal reasoning has been significantly advanced by textual Chain-of-Thought (CoT), a paradigm where models conduct reasoning within language. This text-centric approach, however, treats vision as a static, initial context, creating a fundamental \"semantic gap\" between rich perceptual data and discrete symbolic thought. Human cognition often transcends language, utilizing vision as a dynamic mental sketchpad. A similar evolution is now unfolding in AI, marking a fundamental paradigm shift from models that merely think about images to those that can truly think with images. This emerging paradigm is characterized by models leveraging visual information as intermediate steps in their thought process, transforming vision from a passive input into a dynamic, manipulable cognitive workspace. In this survey, we chart this evolution of intelligence along a trajectory of increasing cognitive autonomy, which unfolds across three key stages: from external tool exploration, through programmatic manipulation, to intrinsic imagination. To structure this rapidly evolving field, our survey makes four key contributions. (1) We establish the foundational principles of the think with image paradigm and its three-stage framework. (2) We provide a comprehensive review of the core methods that characterize each stage of this roadmap. (3) We analyze the critical landscape of evaluation benchmarks and transformative applications. (4) We identify significant challenges and outline promising future directions. By providing this structured overview, we aim to offer a clear roadmap for future research towards more powerful and human-aligned multimodal AI."
    },
    {
      "title": "CRISP-SAM2：基于跨模态交互和语义提示的多器官分割SAM2模型 (原标题: CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation)",
      "link": "https://arxiv.org/abs/2506.23121",
      "pubDate": "Sun, 29 Jun 2025 03:05:27 GMT",
      "isoDate": "2025-06-29T03:05:27.000Z",
      "creator": "Xinlei Yu, Chanmiao Wang, Hui Jin, Ahmed Elazab, Gangyong Jia, Xiang Wan, Changqing Zou, Ruiquan Ge",
      "summary": "## CRISP-SAM2：基于跨模态交互和语义提示的多器官分割模型\n\n本文介绍了一种名为CRISP-SAM2的新型模型，该模型基于SAM2，并结合了跨模态交互和语义提示机制。它旨在解决多器官医学图像分割中存在的细节不准确、依赖几何提示以及空间信息丢失等挑战。\n\n### 模型背景与目标\n*   多器官医学分割是医学图像处理的关键组成部分，对医生进行准确诊断和制定有效治疗方案至关重要。\n*   尽管该领域取得了显著进展，但现有模型仍面临以下问题：细节不准确、过度依赖几何提示以及空间信息丢失。\n*   CRISP-SAM2代表了一种有前景的方法，旨在通过文本描述引导的多器官医学分割，克服上述局限性。\n\n### 核心方法与创新点\nCRISP-SAM2通过以下关键策略提升分割性能：\n\n1.  **跨模态上下文语义生成：**\n    *   采用渐进式跨注意力交互机制，将视觉和文本输入转换为跨模态的上下文语义。\n    *   这些语义随后被注入到图像编码器中，以增强对视觉信息的详细理解。\n\n2.  **语义提示策略：**\n    *   为消除对几何提示的依赖，模型采用语义提示策略，取代了原始的提示编码器。\n    *   此策略有助于提高对复杂目标的感知能力。\n\n3.  **记忆与掩膜优化：**\n    *   引入了一种用于记忆的相似性排序自更新策略。\n    *   应用了掩膜细化过程，以进一步适应医学成像的特点，并增强局部细节。\n\n### 实验结果与性能\n*   在七个公开数据集上进行的比较实验表明，CRISP-SAM2的性能优于现有模型。\n*   广泛的分析也证实了该方法的有效性及其卓越的性能，尤其是在解决上述局限性方面表现突出。\n\n### 代码可用性\n*   模型的代码已公开。\n\n### 相关信息\n*   该研究包含19页，9张图和10个表格。\n*   研究主题涵盖图像和视频处理（eess.IV）、人工智能（cs.AI）、计算机视觉和模式识别（cs.CV）以及机器学习（cs.LG）。",
      "shortSummary": "CRISP-SAM2是一种新型多器官医学分割模型，通过结合跨模态交互和语义提示，解决了现有模型细节不准、依赖几何提示和空间信息丢失的问题。它将视觉和文本输入转化为跨模态语义，并注入图像编码器以增强理解。通过语义提示取代几何提示，并采用记忆自更新和掩膜细化策略，CRISP-SAM2在七个公开数据集上表现优异，显著提升了多器官分割的准确性和鲁棒性。",
      "translated_title": "CRISP-SAM2：基于跨模态交互和语义提示的多器官分割SAM2模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multi-organ medical segmentation is a crucial component of medical image processing, essential for doctors to make accurate diagnoses and develop effective treatment plans. Despite significant progress in this field, current multi-organ segmentation models often suffer from inaccurate details, dependence on geometric prompts and loss of spatial information. Addressing these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal Interaction and Semantic Prompting based on SAM2. This model represents a promising approach to multi-organ medical segmentation guided by textual descriptions of organs. Our method begins by converting visual and textual inputs into cross-modal contextualized semantics using a progressive cross-attention interaction mechanism. These semantics are then injected into the image encoder to enhance the detailed understanding of visual information. To eliminate reliance on geometric prompts, we use a semantic prompting strategy, replacing the original prompt encoder to sharpen the perception of challenging targets. In addition, a similarity-sorting self-updating strategy for memory and a mask-refining process is applied to further adapt to medical imaging and enhance localized details. Comparative experiments conducted on seven public datasets indicate that CRISP-SAM2 outperforms existing models. Extensive analysis also demonstrates the effectiveness of our method, thereby confirming its superior performance, especially in addressing the limitations mentioned earlier. Our code is available at: https://github.com/YU-deep/CRISP\\_SAM2.git."
    }
  ],
  "lastUpdated": "2025-07-05T09:32:04.192Z"
}