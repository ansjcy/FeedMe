{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "OmniVinci：增强全模态理解大型语言模型的架构和数据 (原标题: OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM)",
      "link": "https://arxiv.org/abs/2510.15870",
      "pubDate": "Fri, 17 Oct 2025 13:59:59 GMT",
      "isoDate": "2025-10-17T13:59:59.000Z",
      "creator": "Hanrong Ye, Chao-Han Huck Yang, Arushi Goel, Wei Huang, Ligeng Zhu, Yuanhang Su, Sean Lin, An-Chieh Cheng, Zhen Wan, Jinchuan Tian, Yuming Lou, Dong Yang, Zhijian Liu, Yukang Chen, Ambrish Dantrey, Ehsan Jahangiri, Sreyan Ghosh, Daguang Xu, Ehsan Hosseini-Asl, Danial Mohseni Taheri, Vidya Murali, Sifei Liu, Jason Lu, Oluwatobi Olabiyi, Frank Wang, Rafael Valle, Bryan Catanzaro, Andrew Tao, Song Han, Jan Kautz, Hongxu Yin, Pavlo Molchanov",
      "summary": "# OmniVinci：增强全模态理解大型语言模型的架构和数据\n\n本文介绍了 **OmniVinci**，一项旨在构建强大、开源全模态大型语言模型（LLM）的倡议。该研究深入探讨了模型架构和数据整理方面的设计选择，以期推动机器智能发展，使其具备跨多模态感知能力，模拟人类感知世界的方式。\n\n## 核心目标\n*   开发能够像人类一样感知世界的机器智能，实现跨多模态的理解能力。\n\n## 模型架构创新\nOmniVinci 提出了三项关键架构创新，以增强全模态理解能力：\n1.  **OmniAlignNet**：\n    *   旨在加强视觉和音频嵌入在共享全模态潜在空间中的对齐。\n2.  **时间嵌入分组 (Temporal Embedding Grouping)**：\n    *   用于捕捉视觉和音频信号之间的相对时间对齐关系。\n3.  **约束旋转时间嵌入 (Constrained Rotary Time Embedding)**：\n    *   用于在全模态嵌入中编码绝对时间信息。\n\n## 数据整理与合成\n*   研究团队开发了一个高效的整理和合成管道，生成了 **2400万** 个单模态和全模态对话，为模型训练提供了丰富的多模态数据。\n\n## 主要发现与性能\n*   **模态互补性**：研究发现，不同模态在感知和推理过程中能够相互强化，提升整体理解能力。\n*   **卓越性能**：OmniVinci 在多个基准测试中表现出显著优势：\n    *   在 **DailyOmni** (跨模态理解) 上，比 Qwen2.5-Omni 高出 **+19.05** 分。\n    *   在 **MMAR** (音频) 上，比 Qwen2.5-Omni 高出 **+1.7** 分。\n    *   在 **Video-MME** (视觉) 上，比 Qwen2.5-Omni 高出 **+3.9** 分。\n*   **训练效率**：OmniVinci 仅使用 **0.2万亿** 训练令牌，与 Qwen2.5-Omni 的 1.2万亿相比，训练令牌量减少了 **6倍**，显示出更高的训练效率。\n\n## 下游应用\nOmniVinci 在以下下游应用中展示了其全模态理解的巨大潜力：\n*   机器人技术\n*   医疗AI\n*   智能工厂",
      "shortSummary": "OmniVinci 是一项旨在构建强大、开源全模态LLM的倡议。它通过OmniAlignNet、时间嵌入分组和约束旋转时间嵌入等创新架构，增强了视觉和音频模态的对齐与时间信息编码。通过2400万对话数据训练，OmniVinci在跨模态理解、音频和视觉任务上超越Qwen2.5-Omni，且训练令牌量减少6倍。其优势已在机器人、医疗AI和智能工厂等领域得到验证。",
      "translated_title": "OmniVinci：增强全模态理解大型语言模型的架构和数据",
      "images": [],
      "contentSource": "完整文章",
      "content": "Advancing machine intelligence requires developing the ability to perceive across multiple modalities, much as humans sense the world. We introduce OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We carefully study the design choices across model architecture and data curation. For model architecture, we present three key innovations: (i) OmniAlignNet for strengthening alignment between vision and audio embeddings in a shared omni-modal latent space; (ii) Temporal Embedding Grouping for capturing relative temporal alignment between vision and audio signals; and (iii) Constrained Rotary Time Embedding for encoding absolute temporal information in omni-modal embeddings. We introduce a curation and synthesis pipeline that generates 24M single-modal and omni-modal conversations. We find that modalities reinforce one another in both perception and reasoning. Our model, OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while using just 0.2T training tokens - a 6 times reduction compared to Qwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream applications spanning robotics, medical AI, and smart factory."
    },
    {
      "title": "Skyfall-GS：从卫星图像合成沉浸式3D城市场景 (原标题: Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery)",
      "link": "https://arxiv.org/abs/2510.15869",
      "pubDate": "Fri, 17 Oct 2025 13:59:51 GMT",
      "isoDate": "2025-10-17T13:59:51.000Z",
      "creator": "Jie-Ying Lee, Yi-Ruei Liu, Shr-Ruei Tsai, Wei-Cheng Chang, Chung-Ho Wu, Jiewen Chan, Zhenjun Zhao, Chieh Hubert Lin, Yu-Lun Liu",
      "summary": "## Skyfall-GS：从卫星图像合成沉浸式3D城市场景\n\n### 挑战与背景\n\n合成大规模、可探索且几何精确的3D城市场景对于提供沉浸式和具身应用具有重要价值。然而，这项任务面临的主要挑战在于缺乏大规模、高质量的真实世界3D扫描数据，这使得训练可泛化的生成模型变得困难。\n\n### Skyfall-GS 方法\n\n本文提出了一种名为 **Skyfall-GS** 的新颖框架，它采取了一种替代路径来创建大规模3D场景。Skyfall-GS 通过协同利用以下两种资源来克服现有挑战：\n\n*   **卫星图像：** 提供易于获取的真实粗略几何信息。\n*   **开放域扩散模型：** 用于创建高质量的近距离外观。\n\n### 核心特点与贡献\n\nSkyfall-GS 是首个无需昂贵3D标注即可创建城市街区规模3D场景的框架，并具备以下关键特性：\n\n*   **无3D标注：** 显著降低了数据获取和准备的成本。\n*   **实时沉浸式3D探索：** 允许用户进行流畅的实时场景漫游。\n*   **课程驱动的迭代优化策略：** 采用该策略逐步增强几何完整性和照片级真实感纹理，确保生成场景的质量。\n\n### 实验结果\n\n广泛的实验表明，与现有最先进的方法相比，Skyfall-GS 提供了：\n\n*   **改进的跨视角一致几何：** 确保从不同角度观察时场景的几何结构保持一致性。\n*   **更逼真的纹理：** 生成的场景外观更加真实，细节丰富。",
      "shortSummary": "Skyfall-GS 是一种新颖的框架，旨在从卫星图像合成大规模、沉浸式3D城市场景。它通过结合卫星图像提供粗略几何和开放域扩散模型生成高质量外观，解决了缺乏3D扫描数据的问题。该框架无需昂贵3D标注，支持实时探索，并采用迭代优化策略，最终生成具有更好跨视角一致几何和更逼真纹理的城市场景，超越了现有技术水平。",
      "translated_title": "Skyfall-GS：从卫星图像合成沉浸式3D城市场景",
      "images": [],
      "contentSource": "完整文章",
      "content": "Synthesizing large-scale, explorable, and geometrically accurate 3D urban scenes is a challenging yet valuable task in providing immersive and embodied applications. The challenges lie in the lack of large-scale and high-quality real-world 3D scans for training generalizable generative models. In this paper, we take an alternative route to create large-scale 3D scenes by synergizing the readily available satellite imagery that supplies realistic coarse geometry and the open-domain diffusion model for creating high-quality close-up appearances. We propose Skyfall-GS, the first city-block scale 3D scene creation framework without costly 3D annotations, also featuring real-time, immersive 3D exploration. We tailor a curriculum-driven iterative refinement strategy to progressively enhance geometric completeness and photorealistic textures. Extensive experiments demonstrate that Skyfall-GS provides improved cross-view consistent geometry and more realistic textures compared to state-of-the-art approaches. Project page: https://skyfall-gs.jayinnn.dev/"
    },
    {
      "title": "LightsOut：基于扩散的外绘增强镜头眩光去除 (原标题: LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal)",
      "link": "https://arxiv.org/abs/2510.15868",
      "pubDate": "Fri, 17 Oct 2025 13:59:50 GMT",
      "isoDate": "2025-10-17T13:59:50.000Z",
      "creator": "Shr-Ruei Tsai, Wei-Cheng Chang, Jie-Ying Lee, Chih-Hai Su, Yu-Lun Liu",
      "summary": "## LightsOut：基于扩散的外绘增强镜头眩光去除\n\n### 1. 问题背景\n\n*   **图像质量下降**：镜头眩光严重影响图像质量，对物体检测和自动驾驶等关键计算机视觉任务造成负面影响。\n*   **现有方法局限**：当前的单图像眩光去除（SIFR）方法在画框外光源不完整或缺失时，性能表现不佳。\n\n### 2. 提出的解决方案：LightsOut\n\n*   **核心思想**：本文提出 LightsOut，一个基于扩散的外绘（outpainting）框架，专门用于通过重建画框外光源来增强 SIFR 性能。\n\n### 3. 方法细节\n\n*   **多任务回归模块**：LightsOut 整合了一个多任务回归模块。\n*   **LoRA 微调扩散模型**：结合了经过 LoRA（Low-Rank Adaptation）微调的扩散模型。\n*   **目标**：这些组件协同工作，旨在确保生成的外绘结果既真实又符合物理一致性。\n\n### 4. 实验结果与优势\n\n*   **性能提升**：全面的实验证明，LightsOut 在各种具有挑战性的场景下，能够持续提升现有 SIFR 方法的性能。\n*   **无需额外训练**：该方法的一大优势是无需对现有 SIFR 方法进行额外的重新训练。\n*   **即插即用**：LightsOut 可作为一个普遍适用的即插即用（plug-and-play）预处理解决方案。\n\n### 5. 其他信息\n\n*   该研究已被 ICCV 2025 接收。",
      "shortSummary": "LightsOut 提出一种基于扩散的外绘框架，旨在解决现有单图像眩光去除（SIFR）方法在画框外光源不完整时性能不佳的问题。通过重建画框外光源，LightsOut 利用多任务回归模块和 LoRA 微调的扩散模型，生成真实且物理一致的外绘结果。实验证明，该方法能显著提升现有 SIFR 算法在复杂场景下的表现，且无需额外训练，是一个即插即用的预处理方案。",
      "translated_title": "LightsOut：基于扩散的外绘增强镜头眩光去除",
      "images": [],
      "contentSource": "完整文章",
      "content": "Lens flare significantly degrades image quality, impacting critical computer vision tasks like object detection and autonomous driving. Recent Single Image Flare Removal (SIFR) methods perform poorly when off-frame light sources are incomplete or absent. We propose LightsOut, a diffusion-based outpainting framework tailored to enhance SIFR by reconstructing off-frame light sources. Our method leverages a multitask regression module and LoRA fine-tuned diffusion model to ensure realistic and physically consistent outpainting results. Comprehensive experiments demonstrate LightsOut consistently boosts the performance of existing SIFR methods across challenging scenarios without additional retraining, serving as a universally applicable plug-and-play preprocessing solution. Project page: https://ray-1026.github.io/lightsout/"
    },
    {
      "title": "InfiMed-ORBIT：通过基于评分标准的增量训练在开放式复杂任务上对齐大型语言模型 (原标题: InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training)",
      "link": "https://arxiv.org/abs/2510.15859",
      "pubDate": "Fri, 17 Oct 2025 13:51:28 GMT",
      "isoDate": "2025-10-17T13:51:28.000Z",
      "creator": "Pengkai Wang, Qi Zuo, Pengwei Liu, Zhijie Sang, Congkai Xie, Hongxia Yang",
      "summary": "## InfiMed-ORBIT：通过基于评分标准的增量训练在开放式复杂任务上对齐大型语言模型\n\n### 引言\n\n大型语言模型（LLMs）在通过强化学习（RL）取得显著进展，尤其是在奖励可以程序化验证的领域，例如数学和代码。在这些领域，模型受益于明确的基于规则的目标所指导的良好定义的操作基础。然而，这种进展揭示了一个显著的局限性：在奖励模糊、主观或依赖上下文的开放式领域，如创意写作、科学推理，以及尤其重要的医疗咨询中，缺乏稳健的奖励函数，这使得这些领域对当前的RL策略来说极具挑战性。\n\n### ORBIT框架介绍\n\n为了弥补这一差距，本文引入了ORBIT（开放式、基于评分标准的增量训练）框架，该框架专为高风险医疗对话设计。ORBIT框架的核心特点包括：\n\n*   **集成合成对话生成与动态评分标准创建**：ORBIT将合成对话的生成与动态创建的评分标准相结合。\n*   **指导增量强化学习过程**：这些评分标准被用于指导一个增量式的强化学习过程。\n*   **不依赖外部知识**：值得注意的是，这种方法不依赖于外部医学知识或手动规则，而是利用评分标准引导的反馈来塑造学习。\n\n### 实验与结果\n\nORBIT框架在Qwen3-4B-Instruct模型上进行了实现，并取得了显著的性能提升：\n\n*   **性能提升**：仅使用2k个样本，该方法将模型在HealthBench-Hard基准测试上的性能从7.0大幅提升至27.2。\n*   **最先进成果**：这为同等规模的模型取得了最先进（state-of-the-art）的结果。\n\n### 结论与意义\n\n本文的分析证实，评分标准驱动的强化学习在各种咨询场景中都能促进持续的性能提升，超越了简单的数值改进。这些发现强调了基于评分标准的反馈是一种可扩展的策略，可用于推动LLMs在复杂、开放式任务中的发展。",
      "shortSummary": "大型语言模型（LLMs）在开放式、奖励模糊的任务（如医疗咨询）中面临强化学习（RL）的挑战。本文提出了ORBIT框架，通过结合合成对话生成和动态评分标准，指导增量RL训练。该方法不依赖外部知识，仅用2k样本将Qwen3-4B-Instruct模型在HealthBench-Hard基准上的性能从7.0提升至27.2，达到同规模模型的最先进水平。ORBIT证明了基于评分标准的反馈是提升LLMs处理复杂开放式任务的有效且可扩展策略。",
      "translated_title": "InfiMed-ORBIT：通过基于评分标准的增量训练在开放式复杂任务上对齐大型语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) have shown substantial advances through reinforcement learning (RL), particularly in domains where rewards can be programmatically verified, such as mathematics and code. In these areas, models benefit from a well-defined operational base guided by explicit rule-based objectives. However, this progress reveals a significant limitation: in open-ended domains where rewards are ambiguous, subjective, or context-dependent, such as creative writing, scientific reasoning, and notably medical consultation, robust reward functions are lacking, making these areas challenging for current RL strategies. To bridge this gap, we introduce ORBIT, an open-ended rubric-based incremental training framework specifically designed for high-stakes medical dialogue. ORBIT integrates syn- thetic dialogue generation with the dynamic creation of rubrics, employing these rubrics to direct an incremental RL process. In particular, this approach does not depend on external medical knowledge or manual rules, instead utilizing rubric-guided feedback to shape learning. When implemented on the Qwen3-4B-Instruct model, our method can greatly enhance its performance on the HealthBench-Hard benchmark from 7.0 to 27.2 using only 2k samples, thus achieving state-of-the-art results for models of this scale. Our analysis confirms that rubric-driven RL fos-ters consistent performance gains across diverse consultation scenarios, going beyond simple numerical improvements. These findings underscore rubric-based feedback as a scalable strategy for advancing LLMs in intricate, open-ended tasks."
    },
    {
      "title": "BLIP3o-NEXT：原生图像生成的下一个前沿 (原标题: BLIP3o-NEXT: Next Frontier of Native Image Generation)",
      "link": "https://arxiv.org/abs/2510.15857",
      "pubDate": "Fri, 17 Oct 2025 13:50:58 GMT",
      "isoDate": "2025-10-17T13:50:58.000Z",
      "creator": "Jiuhai Chen, Le Xue, Zhiyang Xu, Xichen Pan, Shusheng Yang, Can Qin, An Yan, Honglu Zhou, Zeyuan Chen, Lifu Huang, Tianyi Zhou, Junnan Li, Silvio Savarese, Caiming Xiong, Ran Xu",
      "summary": "# BLIP3o-NEXT：原生图像生成的下一个前沿\n\nBLIP3o-NEXT 是 BLIP3 系列中一个完全开源的基础模型，旨在推进原生图像生成的下一个前沿。该模型在一个单一架构中统一了文本到图像生成和图像编辑功能，展现出强大的图像生成和编辑能力。\n\n在开发这一先进的原生图像生成模型的过程中，研究人员总结了四个关键见解：\n\n1.  **架构选择**：大多数架构选择都能产生可比较的性能。只要架构能够高效扩展并支持快速推理，就可以被认为是有效的。\n2.  **强化学习的应用**：成功应用强化学习可以进一步推动原生图像生成技术的发展。\n3.  **图像编辑的挑战与提升**：图像编辑仍然是一项具有挑战性的任务，但通过后期训练和数据引擎，可以显著增强指令遵循能力以及生成图像与参考图像之间的一致性。\n4.  **数据质量和规模**：数据质量和规模仍然是决定模型性能上限的决定性因素。\n\n基于这些见解，BLIP3o-NEXT 采用了 **自回归（Autoregressive）+ 扩散（Diffusion）** 架构：\n\n*   **自回归模型**：首先根据多模态输入生成离散的图像标记。\n*   **扩散模型**：随后利用自回归模型的隐藏状态作为条件信号，生成高保真图像。\n\n这种架构将自回归模型的推理能力和指令遵循能力与扩散模型的精细细节渲染能力相结合，实现了更高水平的连贯性和真实感。\n\n### 性能评估\n\n对各种文本到图像和图像编辑基准的广泛评估表明，BLIP3o-NEXT 在现有模型中取得了卓越的性能。\n\n### 作者与引用信息\n\n*   **作者**：Jiuhai Chen, Le Xue, Zhiyang Xu, Xichen Pan, Shusheng Yang, Can Qin, An Yan, Honglu Zhou, Zeyuan Chen, Lifu Huang, Tianyi Zhou, Junnan Li, Silvio Savarese, Caiming Xiong, Ran Xu\n*   **引用**：arXiv:2510.15857 [cs.CV]",
      "shortSummary": "BLIP3o-NEXT是一个开源基础模型，旨在推进原生图像生成，统一了文本到图像生成和图像编辑功能。它采用自回归与扩散相结合的架构，将自回归模型的推理能力与扩散模型的细节渲染能力相结合。研究基于四个关键见解，包括架构效率、强化学习、数据引擎对编辑的提升以及数据质量的重要性。BLIP3o-NEXT在多项基准测试中表现优异，实现了更高水平的连贯性和真实感。",
      "translated_title": "BLIP3o-NEXT：原生图像生成的下一个前沿",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3 series that advances the next frontier of native image generation. BLIP3o-NEXT unifies text-to-image generation and image editing within a single architecture, demonstrating strong image generation and image editing capabilities. In developing the state-of-the-art native image generation model, we identify four key insights: (1) Most architectural choices yield comparable performance; an architecture can be deemed effective provided it scales efficiently and supports fast inference; (2) The successful application of reinforcement learning can further push the frontier of native image generation; (3) Image editing still remains a challenging task, yet instruction following and the consistency between generated and reference images can be significantly enhanced through post-training and data engine; (4) Data quality and scale continue to be decisive factors that determine the upper bound of model performance. Building upon these insights, BLIP3o-NEXT leverages an Autoregressive + Diffusion architecture in which an autoregressive model first generates discrete image tokens conditioned on multimodal inputs, whose hidden states are then used as conditioning signals for a diffusion model to generate high-fidelity images. This architecture integrates the reasoning strength and instruction following of autoregressive models with the fine-detail rendering ability of diffusion models, achieving a new level of coherence and realism. Extensive evaluations of various text-to-image and image-editing benchmarks show that BLIP3o-NEXT achieves superior performance over existing models."
    },
    {
      "title": "Paper2Web：让你的论文活起来！ (原标题: Paper2Web: Let's Make Your Paper Alive!)",
      "link": "https://arxiv.org/abs/2510.15842",
      "pubDate": "Fri, 17 Oct 2025 13:35:58 GMT",
      "isoDate": "2025-10-17T13:35:58.000Z",
      "creator": "Yuhang Chen, Tianpeng Lv, Siyi Zhang, Yixiang Yin, Yao Wan, Philip S. Yu, Dongping Chen",
      "summary": "### Paper2Web：让学术论文网站更生动有效\n\n**引言**\n\n学术项目网站在有效传播研究成果方面扮演着关键角色，但当前的方法（如直接使用大型语言模型生成、模板或直接HTML转换）在生成具有布局感知和交互性的网站方面面临挑战。此外，针对这项任务，一直缺乏一个全面的评估套件。\n\n**Paper2Web：基准数据集与多维度评估框架**\n\n为了解决上述问题，本研究引入了 Paper2Web，它包含一个基准数据集和一个多维度评估框架，用于评估学术网页的生成质量。\n\n*   **评估指标：**\n    *   **基于规则的指标：** 包括连通性（Connectivity）和完整性（Completeness）。\n    *   **LLM-as-a-Judge (经人工验证)：** 涵盖交互性（interactivity）、美观性（aesthetics）和信息量（informativeness）。\n    *   **PaperQuiz：** 衡量论文级别的知识保留度。\n\n**PWAgent：自主管道**\n\n本研究还提出了 PWAgent，这是一个自主管道，能够将科学论文转换为交互式、富含多媒体的学术主页。\n\n*   **工作原理：** PWAgent 通过 MCP 工具迭代地优化内容和布局，以增强重点、平衡和呈现质量。\n\n**实验结果**\n\n实验表明，PWAgent 在学术网页生成方面取得了显著的成果：\n\n*   **性能卓越：** PWAgent 持续显著优于端到端基线方法，例如基于模板的网页和 arXiv/alphaXiv 版本。\n*   **成本效益：** 保持较低的成本。\n*   **帕累托前沿：** 在学术网页生成领域达到了帕累托前沿。\n\n**总结**\n\nPaper2Web 和 PWAgent 的引入，为学术论文网站的生成和评估提供了一个创新且高效的解决方案，旨在让学术研究成果以更生动、更具交互性的方式呈现。",
      "shortSummary": "Paper2Web 引入了一个基准数据集和多维度评估框架，旨在解决学术项目网站生成中布局感知和交互性不足的问题。同时，PWAgent 作为一个自主管道，能将科学论文转换为交互式、多媒体丰富的学术主页。实验表明，PWAgent 在低成本下显著优于现有基线，并在学术网页生成中达到了帕累托前沿，有效提升了学术成果的传播效率和质量。",
      "translated_title": "Paper2Web：让你的论文活起来！",
      "images": [],
      "contentSource": "完整文章",
      "content": "Academic project websites can more effectively disseminate research when they clearly present core content and enable intuitive navigation and interaction. However, current approaches such as direct Large Language Model (LLM) generation, templates, or direct HTML conversion struggle to produce layout-aware, interactive sites, and a comprehensive evaluation suite for this task has been lacking. In this paper, we introduce Paper2Web, a benchmark dataset and multi-dimensional evaluation framework for assessing academic webpage generation. It incorporates rule-based metrics like Connectivity, Completeness and human-verified LLM-as-a-Judge (covering interactivity, aesthetics, and informativeness), and PaperQuiz, which measures paper-level knowledge retention. We further present PWAgent, an autonomous pipeline that converts scientific papers into interactive and multimedia-rich academic homepages. The agent iteratively refines both content and layout through MCP tools that enhance emphasis, balance, and presentation quality. Our experiments show that PWAgent consistently outperforms end-to-end baselines like template-based webpages and arXiv/alphaXiv versions by a large margin while maintaining low cost, achieving the Pareto-front in academic webpage generation."
    },
    {
      "title": "VISTA：一种测试时自改进视频生成代理 (原标题: VISTA: A Test-Time Self-Improving Video Generation Agent)",
      "link": "https://arxiv.org/abs/2510.15831",
      "pubDate": "Fri, 17 Oct 2025 13:12:08 GMT",
      "isoDate": "2025-10-17T13:12:08.000Z",
      "creator": "Do Xuan Long, Xingchen Wan, Hootan Nakhost, Chen-Yu Lee, Tomas Pfister, Sercan Ö. Arık",
      "summary": "# VISTA：一种测试时自改进视频生成代理\n\n## 摘要\n\n尽管文本到视频合成技术取得了快速进展，但生成视频的质量仍然严重依赖于用户提示的精确性。现有在其他领域成功的测试时优化方法，在处理视频的多面性时面临挑战。\n\n本文介绍了 **VISTA (Video Iterative Self-improvemenT Agent)**，一个新颖的多智能体系统，它通过在迭代循环中精炼提示，自主地改进视频生成。\n\n## VISTA 的工作流程\n\nVISTA 的核心在于其迭代的提示改进机制，具体步骤如下：\n\n1.  **计划分解**：VISTA 首先将用户的想法分解为一个结构化的时间计划。\n2.  **视频生成与选择**：在视频生成之后，系统通过一个鲁棒的成对锦标赛来识别出最佳视频。\n3.  **多智能体评估**：获胜的视频随后由三个专业的智能体进行批判性评估，这些智能体分别专注于：\n    *   **视觉保真度**：评估视频的视觉质量和一致性。\n    *   **音频保真度**：评估视频的音频质量和与内容的匹配度。\n    *   **上下文保真度**：评估视频在叙事和情境上的准确性。\n4.  **提示改进**：最后，一个推理智能体综合这些反馈，进行内省式重写和增强提示，以用于下一个生成周期，从而实现持续的自我改进。\n\n## 实验结果与优势\n\n在单场景和多场景视频生成场景中的实验表明：\n\n*   **持续改进**：与现有方法产生不一致的增益不同，VISTA 能够持续改进视频质量以及与用户意图的对齐。\n*   **卓越性能**：在与最先进基线的成对比较中，VISTA 实现了高达60%的胜率。\n*   **人类偏好**：人类评估者也认同 VISTA 的优越性，在66.4%的比较中更偏爱 VISTA 的输出。\n\n## 相关信息\n\n*   **研究领域**：计算机视觉与模式识别 (cs.CV)\n*   **引用**：arXiv:2510.15831 [cs.CV]",
      "shortSummary": "VISTA（Video Iterative Self-improvemenT Agent）是一个多智能体系统，旨在通过迭代优化提示来自主提升文本到视频的生成质量。它将用户想法分解为时间计划，生成视频后通过成对锦标赛选出最佳。随后，视觉、音频和上下文智能体对视频进行评估，推理智能体根据反馈重写提示。实验表明，VISTA持续改进视频质量和用户意图对齐，在与最先进基线的比较中胜率高达60%，人类评估者也更偏爱其输出。",
      "translated_title": "VISTA：一种测试时自改进视频生成代理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite rapid advances in text-to-video synthesis, generated video quality remains critically dependent on precise user prompts. Existing test-time optimization methods, successful in other domains, struggle with the multi-faceted nature of video. In this work, we introduce VISTA (Video Iterative Self-improvemenT Agent), a novel multi-agent system that autonomously improves video generation through refining prompts in an iterative loop. VISTA first decomposes a user idea into a structured temporal plan. After generation, the best video is identified through a robust pairwise tournament. This winning video is then critiqued by a trio of specialized agents focusing on visual, audio, and contextual fidelity. Finally, a reasoning agent synthesizes this feedback to introspectively rewrite and enhance the prompt for the next generation cycle. Experiments on single- and multi-scene video generation scenarios show that while prior methods yield inconsistent gains, VISTA consistently improves video quality and alignment with user intent, achieving up to 60% pairwise win rate against state-of-the-art baselines. Human evaluators concur, preferring VISTA outputs in 66.4% of comparisons."
    },
    {
      "title": "利用高质量合成数据集扩展基于指令的视频编辑 (原标题: Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset)",
      "link": "https://arxiv.org/abs/2510.15742",
      "pubDate": "Fri, 17 Oct 2025 11:31:40 GMT",
      "isoDate": "2025-10-17T11:31:40.000Z",
      "creator": "Qingyan Bai, Qiuyu Wang, Hao Ouyang, Yue Yu, Hanlin Wang, Wen Wang, Ka Leong Cheng, Shuailei Ma, Yanhong Zeng, Zichen Liu, Yinghao Xu, Yujun Shen, Qifeng Chen",
      "summary": "## 利用高质量合成数据集扩展基于指令的视频编辑\n\n### 背景与挑战\n\n基于指令的视频编辑技术有望普及内容创作，但其发展受到大规模、高质量训练数据稀缺的严重阻碍。\n\n### Ditto 框架介绍\n\n本文引入了 Ditto，一个旨在解决这一根本挑战的整体框架。Ditto 的核心特点包括：\n\n*   **新颖的数据生成管道**：\n    *   它将领先的图像编辑器的创意多样性与上下文视频生成器相结合，克服了现有模型范围有限的问题。\n\n*   **高效、精炼的模型架构**：\n    *   为了使数据生成过程可行，Ditto 采用了一种高效、精炼的模型架构，并辅以时间增强器。\n    *   这解决了成本与质量之间难以平衡的问题，同时降低了计算开销并提高了时间一致性。\n\n*   **智能代理驱动**：\n    *   为了实现全面的可扩展性，整个管道由一个智能代理驱动。\n    *   该代理负责生成多样化的指令并严格过滤输出，从而在大规模上确保质量控制。\n\n### Ditto-1M 数据集与 Editto 模型\n\n*   **Ditto-1M 数据集**：\n    *   利用 Ditto 框架，研究团队投入了超过 12,000 个 GPU-天，构建了 Ditto-1M，这是一个包含一百万个高保真视频编辑示例的新数据集。\n\n*   **Editto 模型**：\n    *   研究人员在 Ditto-1M 数据集上，采用课程学习策略训练了他们的模型 Editto。\n\n### 成果\n\n实验结果表明，Editto 模型展现出卓越的指令遵循能力，并在基于指令的视频编辑领域建立了新的最先进水平。",
      "shortSummary": "本文介绍了 Ditto 框架，旨在解决基于指令的视频编辑中高质量训练数据稀缺的问题。Ditto 采用新颖的数据生成管道、高效模型架构和智能代理，成功构建了包含一百万个高保真视频编辑示例的 Ditto-1M 数据集。在此数据集上训练的 Editto 模型，通过课程学习策略，展现出卓越的指令遵循能力，并达到了基于指令视频编辑的最新技术水平。",
      "translated_title": "利用高质量合成数据集扩展基于指令的视频编辑",
      "images": [],
      "contentSource": "完整文章",
      "content": "Instruction-based video editing promises to democratize content creation, yet its progress is severely hampered by the scarcity of large-scale, high-quality training data. We introduce Ditto, a holistic framework designed to tackle this fundamental challenge. At its heart, Ditto features a novel data generation pipeline that fuses the creative diversity of a leading image editor with an in-context video generator, overcoming the limited scope of existing models. To make this process viable, our framework resolves the prohibitive cost-quality trade-off by employing an efficient, distilled model architecture augmented by a temporal enhancer, which simultaneously reduces computational overhead and improves temporal coherence. Finally, to achieve full scalability, this entire pipeline is driven by an intelligent agent that crafts diverse instructions and rigorously filters the output, ensuring quality control at scale. Using this framework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of one million high-fidelity video editing examples. We trained our model, Editto, on Ditto-1M with a curriculum learning strategy. The results demonstrate superior instruction-following ability and establish a new state-of-the-art in instruction-based video editing."
    },
    {
      "title": "构建您的个性化研究小组：一个用于持续和交互式科学自动化的多智能体框架 (原标题: Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation)",
      "link": "https://arxiv.org/abs/2510.15624",
      "pubDate": "Fri, 17 Oct 2025 09:13:32 GMT",
      "isoDate": "2025-10-17T09:13:32.000Z",
      "creator": "Ed Li, Junyu Ren, Xintian Pan, Cat Yan, Chuanhao Li, Dirk Bergemann, Zhuoran Yang",
      "summary": "# 构建您的个性化研究小组：一个用于持续和交互式科学自动化的多智能体框架\n\n本文介绍了一个名为 `freephdlabor` 的开源多智能体框架，旨在解决现有科学自动化系统中存在的两大核心局限性，并推动科学发现的自动化进程。\n\n## 现有系统面临的挑战\n\n当前的科学智能体系统主要存在以下问题：\n\n*   **僵化且预设的工作流**：这些系统的工作流程是预先编程好的，缺乏根据中间发现进行调整和适应的能力。\n*   **上下文管理不足**：在进行长期研究时，上下文管理不当会阻碍研究的深入和连贯性。\n\n## `freephdlabor` 框架的创新之处\n\n`freephdlabor` 框架通过引入一系列创新特性，克服了上述局限性：\n\n### 核心特性\n\n1.  **完全动态的工作流**：\n    *   工作流不再是预设的，而是由智能体根据实时推理动态决定的，使其能够灵活适应研究进展。\n2.  **模块化架构**：\n    *   提供高度可定制性，用户可以根据特定领域的需求，轻松修改、添加或移除智能体。\n\n### 综合基础设施\n\n该框架还提供了全面的基础设施支持，以确保研究过程的连贯性和效率：\n\n*   **自动上下文压缩**：有效管理和优化研究过程中的上下文信息，防止信息过载。\n*   **基于工作空间（Workspace-based）的通信**：智能体之间通过共享工作空间进行通信，有效防止信息降级和丢失。\n*   **跨会话的记忆持久性**：智能体能够在不同研究会话之间保留和利用记忆，确保研究的连续性。\n*   **非阻塞式的人工干预机制**：允许人类研究者在不中断自动化流程的情况下进行干预和指导，实现人机协作。\n\n## 框架的愿景与影响\n\n这些特性共同将自动化研究从孤立的、单次尝试转变为**持续的研究项目**。这意味着：\n\n*   系统能够系统地建立在先前的探索之上。\n*   能够有效地整合人类的反馈和指导。\n\n通过提供构建可定制的“共同科学家”系统所需的架构原则和实际实现，`freephdlabor` 旨在促进自动化研究在各个科学领域的广泛应用。它使研究人员能够部署交互式多智能体系统，从而自主地进行端到端的研究——从最初的构思、实验执行，直至生成可供发表的稿件。",
      "shortSummary": "本文提出了一个名为 `freephdlabor` 的开源多智能体框架，旨在解决现有科学自动化系统工作流僵化和上下文管理不足的问题。该框架引入了完全动态的工作流和模块化架构，允许智能体根据实时推理调整研究路径，并支持用户高度定制。它还提供了自动上下文压缩、基于工作空间的通信、跨会话记忆持久性及非阻塞式人工干预等基础设施。`freephdlabor` 将自动化研究转变为持续的、可整合人类反馈的程序，旨在推动科学发现的自动化，实现从构思到出版的端到端研究。",
      "translated_title": "构建您的个性化研究小组：一个用于持续和交互式科学自动化的多智能体框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "The automation of scientific discovery represents a critical milestone in Artificial Intelligence (AI) research. However, existing agentic systems for science suffer from two fundamental limitations: rigid, pre-programmed workflows that cannot adapt to intermediate findings, and inadequate context management that hinders long-horizon research. We present freephdlabor, an open-source multiagent framework featuring fully dynamic workflows determined by real-time agent reasoning and a \\textit{modular architecture} enabling seamless customization -- users can modify, add, or remove agents to address domain-specific requirements. The framework provides comprehensive infrastructure including automatic context compaction, workspace-based communication to prevent information degradation, memory persistence across sessions, and non-blocking human intervention mechanisms. These features collectively transform automated research from isolated, single-run attempts into continual research programs that build systematically on prior explorations and incorporate human feedback. By providing both the architectural principles and practical implementation for building customizable co-scientist systems, this work aims to facilitate broader adoption of automated research across scientific domains, enabling practitioners to deploy interactive multiagent systems that autonomously conduct end-to-end research -- from ideation through experimentation to publication-ready manuscripts."
    },
    {
      "title": "Imaginarium: 视觉引导的高质量3D场景布局生成 (原标题: Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation)",
      "link": "https://arxiv.org/abs/2510.15564",
      "pubDate": "Fri, 17 Oct 2025 07:48:08 GMT",
      "isoDate": "2025-10-17T07:48:08.000Z",
      "creator": "Xiaoming Zhu, Xu Huang, Qinghongbing Xie, Zhi Deng, Junsheng Yu, Yirui Guan, Zhongyuan Liu, Lin Zhu, Qijun Zhao, Ligang Liu, Long Zeng",
      "summary": "### Imaginarium: 视觉引导的高质量3D场景布局生成\n\n**引言**\n\n在数字内容创作领域，生成具有艺术性和逻辑连贯性的3D场景布局是一项核心且关键的任务。然而，当前主流的3D场景布局生成方法面临诸多挑战：\n\n*   **传统优化方法：** 往往受限于繁琐且耗时的人工规则，导致灵活性和效率低下。\n*   **深度生成模型：** 在生成内容的丰富性和多样性方面表现不足，难以满足复杂场景的需求。\n*   **大型语言模型（LLM）方法：** 缺乏足够的鲁棒性，并且在准确捕捉复杂的空间关系方面存在困难。\n\n**提出的解决方案**\n\n为了应对上述挑战，本文提出了一种新颖的**视觉引导3D布局生成系统**，名为Imaginarium。该系统旨在通过结合视觉信息和结构化优化，生成高质量、高丰富度的3D场景布局。\n\n**系统主要步骤**\n\n该系统通过以下四个关键步骤实现其功能：\n\n1.  **构建高质量资产库：**\n    *   首先，研究人员构建了一个包含2,037个场景资产和147个预定义的3D场景布局的高质量资产库。这个库为后续的生成过程提供了丰富的素材和结构参考。\n\n2.  **图像生成与对齐：**\n    *   系统利用一个图像生成模型，将用户输入的文本提示（prompt representations）扩展并转化为具体的图像。\n    *   该图像生成模型经过精心微调，以确保其输出的图像能够与预先构建的资产库中的元素和风格高度对齐。\n\n3.  **鲁棒的图像解析模块：**\n    *   开发了一个强大的图像解析模块，其核心功能是根据视觉语义信息（如物体识别、场景理解）和几何信息（如深度、形状）从生成的图像中恢复出场景的3D布局。\n\n4.  **场景布局优化：**\n    *   最后，系统利用场景图（Scene Graphs）和整体视觉语义信息对初步恢复的3D场景布局进行优化。\n    *   这一优化步骤旨在确保生成的布局在逻辑上保持连贯性，并且与最初的视觉引导图像高度一致。\n\n**实验结果与可用性**\n\n*   通过广泛的用户测试，结果表明本文提出的算法在布局的丰富性和整体质量方面显著优于现有的方法。\n*   为了促进研究和应用，该系统的代码和数据集将公开发布。",
      "shortSummary": "本文提出了一种名为Imaginarium的视觉引导3D场景布局生成系统，旨在解决传统方法在丰富性、多样性和空间关系捕捉方面的局限。该系统首先构建高质量资产库，然后利用图像生成模型将提示转换为图像，接着通过鲁棒的图像解析模块从视觉语义和几何信息中恢复3D布局，并最终通过场景图和整体视觉语义优化布局。广泛的用户测试表明，该方法在布局丰富性和质量上显著优于现有技术。代码和数据集将公开提供。",
      "translated_title": "Imaginarium: 视觉引导的高质量3D场景布局生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Generating artistic and coherent 3D scene layouts is crucial in digital content creation. Traditional optimization-based methods are often constrained by cumbersome manual rules, while deep generative models face challenges in producing content with richness and diversity. Furthermore, approaches that utilize large language models frequently lack robustness and fail to accurately capture complex spatial relationships. To address these challenges, this paper presents a novel vision-guided 3D layout generation system. We first construct a high-quality asset library containing 2,037 scene assets and 147 3D scene layouts. Subsequently, we employ an image generation model to expand prompt representations into images, fine-tuning it to align with our asset library. We then develop a robust image parsing module to recover the 3D layout of scenes based on visual semantics and geometric information. Finally, we optimize the scene layout using scene graphs and overall visual semantics to ensure logical coherence and alignment with the images. Extensive user testing demonstrates that our algorithm significantly outperforms existing methods in terms of layout richness and quality. The code and dataset will be available at https://github.com/HiHiAllen/Imaginarium."
    },
    {
      "title": "没有变分自编码器的潜在扩散模型 (原标题: Latent Diffusion Model without Variational Autoencoder)",
      "link": "https://arxiv.org/abs/2510.15301",
      "pubDate": "Fri, 17 Oct 2025 00:17:44 GMT",
      "isoDate": "2025-10-17T00:17:44.000Z",
      "creator": "Minglei Shi, Haolin Wang, Wenzhao Zheng, Ziyang Yuan, Xiaoshi Wu, Xintao Wang, Pengfei Wan, Jie Zhou, Jiwen Lu",
      "summary": "### 没有变分自编码器的潜在扩散模型 (SVG)\n\n本文介绍了一种名为SVG（Self-supervised Visual Generation）的新型潜在扩散模型，它旨在解决当前基于变分自编码器（VAE）的潜在扩散模型（LDM）所面临的挑战。\n\n**当前VAE+扩散范式的局限性：**\n*   **训练效率低下：** 模型的训练过程耗时较长。\n*   **推理速度慢：** 生成图像的推理过程速度较慢。\n*   **泛化能力差：** 在更广泛的视觉任务中，模型的迁移能力受限。\n\n**问题根源：**\n*   这些问题主要源于VAE潜在空间的固有局限性：缺乏清晰的语义分离和强大的判别结构。\n*   研究分析证实，这些特性对于感知、理解任务以及潜在扩散模型的稳定高效训练至关重要。\n\n**SVG模型的核心思想与方法：**\n*   **摆脱VAE：** SVG模型创新性地去除了传统的VAE组件。\n*   **利用自监督表示：** 模型利用自监督表示来构建视觉生成所需的潜在空间。\n*   **语义结构化特征空间：**\n    *   通过利用**冻结的DINO特征**来构建一个具有清晰语义判别能力的特征空间。\n    *   这种方法确保了潜在空间具有强大的语义信息。\n*   **细粒度细节捕获：**\n    *   一个轻量级的残差分支负责捕获图像的细粒度细节，以实现高保真度的重建。\n*   **直接训练扩散模型：** 扩散模型直接在这个语义结构化的潜在空间上进行训练，从而促进更高效的学习。\n\n**SVG带来的优势与成果：**\n*   **加速扩散训练：** 显著提高了扩散模型的训练速度。\n*   **支持少量步骤采样：** 能够在更少的采样步骤下生成高质量图像。\n*   **提高生成质量：** 整体上提升了图像生成的质量。\n*   **保留语义和判别能力：** SVG模型保留了底层自监督表示的语义和判别能力。\n*   **任务通用性：** 为实现任务通用、高质量的视觉表示提供了一条原则性途径。",
      "shortSummary": "现有基于VAE的潜在扩散模型存在训练效率低、推理慢和泛化性差的问题，原因在于VAE潜在空间缺乏语义分离和判别结构。本文提出SVG模型，一个无需VAE的潜在扩散模型，它利用冻结的DINO自监督特征构建语义结构化的潜在空间，并通过轻量级残差分支捕获细节。SVG直接在此空间上训练扩散模型，从而实现更快的训练、更少的采样步骤和更高的生成质量，并为任务通用型视觉表示提供了新途径。",
      "translated_title": "没有变分自编码器的潜在扩散模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent progress in diffusion-based visual generation has largely relied on latent diffusion models with variational autoencoders (VAEs). While effective for high-fidelity synthesis, this VAE+diffusion paradigm suffers from limited training efficiency, slow inference, and poor transferability to broader vision tasks. These issues stem from a key limitation of VAE latent spaces: the lack of clear semantic separation and strong discriminative structure. Our analysis confirms that these properties are crucial not only for perception and understanding tasks, but also for the stable and efficient training of latent diffusion models. Motivated by this insight, we introduce SVG, a novel latent diffusion model without variational autoencoders, which leverages self-supervised representations for visual generation. SVG constructs a feature space with clear semantic discriminability by leveraging frozen DINO features, while a lightweight residual branch captures fine-grained details for high-fidelity reconstruction. Diffusion models are trained directly on this semantically structured latent space to facilitate more efficient learning. As a result, SVG enables accelerated diffusion training, supports few-step sampling, and improves generative quality. Experimental results further show that SVG preserves the semantic and discriminative capabilities of the underlying self-supervised representations, providing a principled pathway toward task-general, high-quality visual representations."
    },
    {
      "title": "用于科学发现的基础模型：从范式增强到范式转变 (原标题: Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition)",
      "link": "https://arxiv.org/abs/2510.15280",
      "pubDate": "Thu, 16 Oct 2025 23:40:26 GMT",
      "isoDate": "2025-10-16T23:40:26.000Z",
      "creator": "Fan Liu, Jindong Han, Tengfei Lyu, Weijia Zhang, Zhe-Rui Yang, Lu Dai, Cancheng Liu, Hao Liu",
      "summary": "## 用于科学发现的基础模型：从范式增强到范式转变\n\n基础模型（FMs），如GPT-4和AlphaFold，正在深刻地重塑科学研究的格局。本文的核心论点是，FMs不仅仅是增强现有的科学方法，而是在催化一场向全新科学范式的转变。\n\n### 科学发现的演变三阶段框架\n\n论文提出了一个三阶段框架来描述FMs在科学发现中的演变过程：\n\n1.  **元科学整合 (Meta-Scientific Integration)**\n    *   在此阶段，FMs主要在传统科学范式内增强现有的工作流程。\n    *   它们能够加速诸如假设生成、实验设计和结果解释等任务，提高效率和准确性。\n\n2.  **人机混合协同创造 (Hybrid Human-AI Co-Creation)**\n    *   FMs在此阶段不再仅仅是工具，而是成为科学研究中积极的合作者。\n    *   它们与人类科学家共同参与问题的制定、复杂的推理过程以及新知识的发现。\n\n3.  **自主科学发现 (Autonomous Scientific Discovery)**\n    *   这是演变的最终阶段，FMs作为独立的智能体运行。\n    *   它们能够以最少的人为干预，自主地生成新的科学知识和发现。\n\n### 论文的范围与目标\n\n*   本文通过上述三阶段框架的视角，回顾了FMs在当前科学范式中的应用现状和正在涌现的能力。\n*   此外，论文还识别了由FMs驱动的科学发现所面临的潜在风险，并探讨了未来的发展方向。\n*   这篇立场性论文旨在支持科学界更好地理解FMs所带来的变革性作用，并鼓励对未来科学发现的模式进行深入的反思。",
      "shortSummary": "基础模型（FMs）正在推动科学发现从现有方法的增强转向新的科学范式。论文提出了一个三阶段演变框架：从元科学整合（增强工作流），到人机混合协同创造（主动协作），再到自主科学发现（独立生成知识）。该研究旨在阐明FMs的变革作用，并探讨其风险与未来方向，以促进对科学发现未来的思考。",
      "translated_title": "用于科学发现的基础模型：从范式增强到范式转变",
      "images": [],
      "contentSource": "完整文章",
      "content": "Foundation models (FMs), such as GPT-4 and AlphaFold, are reshaping the landscape of scientific research. Beyond accelerating tasks such as hypothesis generation, experimental design, and result interpretation, they prompt a more fundamental question: Are FMs merely enhancing existing scientific methodologies, or are they redefining the way science is conducted? In this paper, we argue that FMs are catalyzing a transition toward a new scientific paradigm. We introduce a three-stage framework to describe this evolution: (1) Meta-Scientific Integration, where FMs enhance workflows within traditional paradigms; (2) Hybrid Human-AI Co-Creation, where FMs become active collaborators in problem formulation, reasoning, and discovery; and (3) Autonomous Scientific Discovery, where FMs operate as independent agents capable of generating new scientific knowledge with minimal human intervention. Through this lens, we review current applications and emerging capabilities of FMs across existing scientific paradigms. We further identify risks and future directions for FM-enabled scientific discovery. This position paper aims to support the scientific community in understanding the transformative role of FMs and to foster reflection on the future of scientific discovery. Our project is available at https://github.com/usail-hkust/Awesome-Foundation-Models-for-Scientific-Discovery."
    },
    {
      "title": "通过适当的权重衰减调整实现鲁棒的逐层缩放规则 (原标题: Robust Layerwise Scaling Rules by Proper Weight Decay Tuning)",
      "link": "https://arxiv.org/abs/2510.15262",
      "pubDate": "Thu, 16 Oct 2025 22:58:35 GMT",
      "isoDate": "2025-10-16T22:58:35.000Z",
      "creator": "Zhiyuan Fan, Yifeng Liu, Qingyue Zhao, Angela Yuan, Quanquan Gu",
      "summary": "# 通过适当的权重衰减调整实现鲁棒的逐层缩放规则\n\n## 引言与问题背景\n\n*   **经验缩放定律与μP：** 经验缩放定律指导参数、数据和计算的分配，而最大更新参数化（μP）通过均衡早期更新幅度，使得学习率能够在不同模型宽度之间进行迁移。\n*   **μP的局限性：** 在现代尺度不变架构中，训练过程迅速进入由优化器主导的稳态。此时，归一化层会产生反向尺度敏感性，导致有效学习率变得依赖于模型宽度，从而降低了μP的学习率迁移效果。\n\n## 核心贡献：权重衰减缩放规则\n\n*   本文提出了一种针对AdamW优化器的权重衰减缩放规则，旨在跨不同模型宽度保持子层增益（sublayer gain）不变，从而解决上述μP的局限性。\n\n## 经验观察与规则推导\n\n*   **奇异值谱行为：** 经验观察表明，每个矩阵参数的奇异值谱在范数上与 $\\sqrt{\\eta/\\lambda}$ 成比例缩放，且其形状大致保持不变。\n*   **顶层奇异值缩放：** 在模型宽度 $d$ 缩放时，顶层奇异值近似按 $\\sqrt{\\eta/\\lambda}\\cdot d^{0.75}$ 缩放。\n*   **权重衰减规则推导：**\n    *   结合μP针对矩阵类参数的学习率规则 $\\eta_2\\propto d^{-1}$。\n    *   推导出经验性的权重衰减缩放规则 $\\lambda_2\\propto \\sqrt{d}$，以近似保持子层增益的宽度不变性。\n    *   对于向量类参数，采用 $\\eta_1=\\Theta_d(1)$ 和 $\\lambda_1=0$。\n\n## 主要成果与优势\n\n*   该方法实现了学习率和权重衰减从代理宽度到目标宽度的“零样本”迁移，彻底消除了针对每个宽度的超参数搜索需求。\n\n## 验证与诊断\n\n*   所提出的规则在LLaMA风格的Transformer模型和最小合成设置中得到了验证。\n*   提供了一个简单的诊断方法（通过匹配顶层奇异值）来检查子层增益的宽度不变性。\n\n## 结论与实际意义\n\n*   本文通过显式控制优化器设定的稳态尺度，成功将μP的应用范围扩展到接近初始化阶段之外。\n*   为AdamW优化器下实现宽度鲁棒的超参数迁移提供了一个实用的方法。",
      "shortSummary": "本文针对现代架构中最大更新参数化（μP）学习率迁移失效问题，提出了一种AdamW优化器的权重衰减缩放规则。研究发现，矩阵参数的奇异值谱在范数上与 $\\sqrt{\\eta/\\lambda}$ 成比例，并结合μP学习率规则 $\\eta_2\\propto d^{-1}$，推导出权重衰减规则 $\\lambda_2\\propto \\sqrt{d}$，以保持子层增益的宽度不变性。该方法实现了学习率和权重衰减的零样本迁移，消除了逐宽度超参数搜索，为AdamW下宽度鲁棒的超参数迁移提供了实用方案。",
      "translated_title": "通过适当的权重衰减调整实现鲁棒的逐层缩放规则",
      "images": [],
      "contentSource": "完整文章",
      "content": "Empirical scaling laws prescribe how to allocate parameters, data, and compute, while maximal-update parameterization (muP) enables learning-rate transfer across widths by equalizing early-time update magnitudes. However, in modern scale-invariant architectures, training quickly enters an optimizer-governed steady state where normalization layers create backward scale sensitivity and the effective learning rate becomes width dependent, degrading muP transfer. We address this by introducing a weight-decay scaling rule for AdamW that preserves sublayer gain across widths. Empirically, the singular-value spectrum of each matrix parameter scales in norm as eta/lambda with an approximately invariant shape; under width scaling d, we observe that the top singular value scales approximately as eta/lambdacdot d^{0.75}. Combining this observation with the muP learning-rate rule eta_2propto d^{-1} for matrix-like parameters implies an empirical weight-decay scaling rule lambda_2propto d that approximately keeps sublayer gains width invariant. Together with vector-like parameters trained at eta_1=Theta_d(1) and lambda_1=0, this yields zero-shot transfer of both learning rate and weight decay from proxy to target widths, removing per-width sweeps. We validate the rule on LLaMA-style Transformers and in a minimal synthetic setting, and we provide a simple diagnostic, matching top singular values, to check sublayer-gain invariance. Our results extend muP beyond the near-init regime by explicitly controlling steady-state scales set by the optimizer, offering a practical recipe for width-robust hyperparameter transfer under AdamW."
    },
    {
      "title": "FinTrust：金融领域可信度评估的综合基准 (原标题: FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain)",
      "link": "https://arxiv.org/abs/2510.15232",
      "pubDate": "Thu, 16 Oct 2025 21:45:49 GMT",
      "isoDate": "2025-10-16T21:45:49.000Z",
      "creator": "Tiansheng Hu, Tongyan Hu, Liuyang Bai, Yilun Zhao, Arman Cohan, Chen Zhao",
      "summary": "### FinTrust：金融领域大型语言模型可信度评估基准\n\n**1. 背景与挑战**\n*   **LLM在金融领域的潜力与风险**：近期的大型语言模型（LLMs）在解决金融相关问题方面展现出巨大潜力。然而，由于金融领域的高风险和高利害关系特性，将LLMs应用于实际金融场景仍面临严峻挑战。\n\n**2. FinTrust基准介绍**\n*   **目的**：本文引入了FinTrust，一个专门为评估LLMs在金融应用中的可信度而设计的综合基准。\n*   **特点**：\n    *   **全面性**：FinTrust专注于基于实际上下文的广泛对齐问题。\n    *   **细粒度任务**：针对可信度评估的每个维度，FinTrust都提供了细粒度的任务。\n\n**3. 评估与发现**\n*   **评估对象**：研究人员使用FinTrust评估了11个LLMs。\n*   **主要发现**：\n    *   **专有模型表现**：像o4-mini这样的专有模型在大多数任务中表现出色，尤其是在安全性方面。\n    *   **开源模型优势**：像DeepSeek-V3这样的开源模型在特定领域具有优势，例如行业层面的公平性。\n    *   **普遍不足**：所有LLMs在诸如信托义务对齐（fiduciary alignment）和信息披露（disclosure）等具有挑战性的任务上均表现不佳，这表明它们在法律意识方面存在显著差距。\n\n**4. 结论与意义**\n*   **价值主张**：研究人员认为FinTrust可以成为评估LLMs在金融领域可信度的一个有价值的基准。",
      "shortSummary": "FinTrust是一个专门用于评估大型语言模型（LLMs）在金融领域可信度的综合基准。该基准旨在解决LLMs在金融应用中面临的高风险挑战，并关注广泛的对齐问题。评估结果显示，专有模型在安全性等多数任务中表现优异，而开源模型在特定领域（如公平性）有优势。然而，所有LLMs在信托义务和信息披露等法律相关任务上均表现不足，揭示了它们在法律意识方面的显著差距。FinTrust有望成为金融LLMs可信度评估的重要工具。",
      "translated_title": "FinTrust：金融领域可信度评估的综合基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent LLMs have demonstrated promising ability in solving finance related problems. However, applying LLMs in real-world finance application remains challenging due to its high risk and high stakes property. This paper introduces FinTrust, a comprehensive benchmark specifically designed for evaluating the trustworthiness of LLMs in finance applications. Our benchmark focuses on a wide range of alignment issues based on practical context and features fine-grained tasks for each dimension of trustworthiness evaluation. We assess eleven LLMs on FinTrust and find that proprietary models like o4-mini outperforms in most tasks such as safety while open-source models like DeepSeek-V3 have advantage in specific areas like industry-level fairness. For challenging task like fiduciary alignment and disclosure, all LLMs fall short, showing a significant gap in legal awareness. We believe that FinTrust can be a valuable benchmark for LLMs' trustworthiness evaluation in finance domain."
    },
    {
      "title": "使用合成数据训练统一多模态数据质量分类器 (原标题: Train a Unified Multimodal Data Quality Classifier with Synthetic Data)",
      "link": "https://arxiv.org/abs/2510.15162",
      "pubDate": "Thu, 16 Oct 2025 17:53:28 GMT",
      "isoDate": "2025-10-16T17:53:28.000Z",
      "creator": "Weizhi Wang, Rongmei Lin, Shiyang Li, Colin Lockard, Ritesh Sarkhel, Sanket Lokegaonkar, Jingbo Shang, Xifeng Yan, Nasser Zalmout, Xian Li",
      "summary": "## 统一多模态数据质量分类器UniFilter\n\n### 引言\n多模态大型语言模型（MLLMs）的预训练依赖于图像-文本字幕数据和交错文档数据的混合。然而，目前针对图像-文本交错文档数据的高质量数据过滤方法研究不足。\n\n### 核心贡献：UniFilter\n\n*   **提出方案**：研究提出了一种名为UniFilter的统一多模态数据质量分类器。\n*   **功能**：UniFilter是一个高效的MLLM，旨在同时过滤高质量的图像-文本字幕数据和交错数据。\n\n### 数据挑战与解决方案：半合成方法\n\n*   **挑战**：收集多样化且带有标注的多模态数据极具挑战性。\n*   **解决方案**：引入了一种半合成方法来解决这一问题。\n    *   该方法利用现成的原始图像，并生成四种不同质量级别的相应文本。\n    *   通过这种方式，能够高效地为字幕数据和交错文档数据创建样本-分数对，用于训练UniFilter。\n\n### 应用与成果\n\n*   **数据筛选应用**：\n    *   UniFilter被应用于从DataComp字幕数据集中筛选高质量字幕数据。\n    *   同时，它也被用于从OBELICS图像-文本交错数据集中筛选高质量交错数据。\n*   **MLLM性能提升**：\n    *   使用UniFilter过滤后的数据进行预训练的MLLMs，相比使用基线过滤数据训练的模型，展现出显著增强的能力。\n    *   这些模型在零样本推理（zero-shot reasoning）和上下文学习（in-context learning）能力方面表现更强。\n    *   经过视觉监督微调（visual supervised fine-tuning）后，由UniFilter引导的MLLMs在各种基准测试中取得了更强的性能。\n*   **下游效益**：这些结果突显了高质量多模态预训练对下游任务的显著益处。\n\n### 资源发布\n\n为了促进社区的复现和进一步开发，研究团队向社区发布了以下资源：\n\n*   用于训练UniFilter的合成训练数据。\n*   UniFilter模型检查点。\n*   由UniFilter筛选出的高质量交错文档子集OBELICS-HQ。",
      "shortSummary": "该研究提出UniFilter，一个统一多模态数据质量分类器，用于过滤MLLM预训练中的图像-文本字幕和交错数据。为解决标注数据稀缺问题，采用半合成方法生成不同质量级别的文本数据。UniFilter成功筛选DataComp和OBELICS数据集，经其过滤数据预训练的MLLMs在零样本推理、上下文学习及基准测试中表现显著提升，证实了高质量多模态预训练的益处。相关合成数据、模型和高质量数据集已发布。",
      "translated_title": "使用合成数据训练统一多模态数据质量分类器",
      "images": [],
      "contentSource": "完整文章",
      "content": "The Multimodal Large Language Models (MLLMs) are continually pre-trained on a mixture of image-text caption data and interleaved document data, while the high-quality data filtering towards image-text interleaved document data is under-explored. We propose to train an efficient MLLM as a Unified Mulitmodal Data Quality Classifier to Filter both high-quality image-text caption and interleaved data (UniFilter). To address the challenge of collecting diverse labeled multimodal data, we introduce a semi-synthetic approach that leverages readily available raw images and generates corresponding text across four quality levels. This method enables efficient creation of sample-score pairs for both caption and interleaved document data to train UniFilter. We apply UniFilter to curate high-quality caption data from DataComp caption dataset and interleaved data from the OBELICS image-text interleaved dataset. MLLMs pre-trained on the filtered data demonstrate significantly enhanced capabilities compared to those trained on baseline-filtered data, achieving stronger zero-shot reasoning and in-context learning capabilities. After visual supervised fine-tuning, these UniFilter-induced MLLMs achieve stronger performance on various benchmarks, highlighting the downstream benefits of high-quality multimodal pre-training. We release the synthetic training data used for training UniFilter, the UniFilter model checkpoints, and the high-quality interleaved document subset OBELICS-HQ, curated by UniFilter, to the community for reproduction and further development."
    },
    {
      "title": "DLER：正确处理长度惩罚——通过强化学习激励每个token产生更多智能 (原标题: DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2510.15110",
      "pubDate": "Thu, 16 Oct 2025 16:05:57 GMT",
      "isoDate": "2025-10-16T16:05:57.000Z",
      "creator": "Shih-Yang Liu, Xin Dong, Ximing Lu, Shizhe Diao, Mingjie Liu, Min-Hung Chen, Hongxu Yin, Yu-Chiang Frank Wang, Kwang-Ting Cheng, Yejin Choi, Jan Kautz, Pavlo Molchanov",
      "summary": "# DLER：通过强化学习激励每个token产生更多智能\n\n## 概述\n\n当前推理语言模型（如OpenAI-o1、DeepSeek-R1和Qwen）虽然通过扩展的思维链实现了强大的性能，但往往会生成不必要的冗长输出。如何最大限度地提高“每个token的智能”（即相对于响应长度的准确性）仍然是一个悬而未决的问题。\n\n## 核心问题与挑战\n\n研究人员重新审视了使用最简单的长度惩罚——截断——的强化学习（RL）方法。他们发现，准确性下降并非源于缺乏复杂的惩罚机制，而是由于RL优化不足。具体而言，RL训练面临以下三个关键挑战：\n\n*   **优势估计中存在较大偏差：** 导致模型难以准确评估动作的价值。\n*   **熵坍缩：** 模型的探索能力下降，容易陷入局部最优。\n*   **稀疏的奖励信号：** 奖励机制不频繁或不明确，使得模型学习效率低下。\n\n## DLER解决方案\n\n为了解决上述挑战，研究团队提出了“正确处理长度惩罚”（DLER）的训练方法。DLER结合了以下策略：\n\n*   **批次奖励归一化（Batch-wise reward normalization）：** 稳定训练过程，减少奖励信号的方差。\n*   **更高的裁剪（Higher clipping）：** 在策略更新中允许更大的变化，有助于模型更快地学习。\n*   **动态采样（Dynamic sampling）：** 调整采样策略，以更好地探索状态空间并避免熵坍缩。\n*   **简单的截断长度惩罚（Simple truncation length penalty）：** 采用直接截断作为长度惩罚，证明其有效性。\n\n## DLER的性能与优势\n\nDLER方法在准确性-效率权衡方面取得了最先进的成果：\n\n*   **显著缩短输出长度：** 将输出长度缩短了70%以上。\n*   **超越基线准确性：** 在缩短输出的同时，超越了所有先前的基线准确性。\n*   **提高测试时扩展性：** 与DeepSeek-R1-7B相比，DLER-7B能够并行生成多个简洁响应，准确性提高了28%，且延迟更低。\n\n## 进一步的改进\n\n研究还引入了DLER的两个扩展版本：\n\n*   **难度感知DLER（Difficulty-Aware DLER）：** 针对较简单的问题自适应地收紧截断，以进一步提高效率。\n*   **更新选择性合并方法（Update-selective merging method）：** 在RL训练数据稀缺的场景下，该方法能够保留基线准确性，同时保持DLER模型简洁的推理能力。\n\n## 备注\n\n本研究为NVIDIA技术报告。",
      "shortSummary": "推理语言模型常生成冗长输出，降低了“每个token的智能”。DLER（正确处理长度惩罚）通过改进强化学习优化，解决了优势估计偏差、熵坍缩和奖励稀疏等问题。DLER结合批次奖励归一化、更高裁剪、动态采样和简单截断惩罚，将模型输出长度缩减70%以上，同时超越了所有基线准确性，显著提升了效率和测试时扩展性。该方法旨在激励模型以更简洁的方式提供更多智能。",
      "translated_title": "DLER：正确处理长度惩罚——通过强化学习激励每个token产生更多智能",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reasoning language models such as OpenAI-o1, DeepSeek-R1, and Qwen achieve strong performance via extended chains of thought but often generate unnecessarily long outputs. Maximizing intelligence per token--accuracy relative to response length--remains an open problem. We revisit reinforcement learning (RL) with the simplest length penalty--truncation--and show that accuracy degradation arises not from the lack of sophisticated penalties but from inadequate RL optimization. We identify three key challenges: (i) large bias in advantage estimation, (ii) entropy collapse, and (iii) sparse reward signal. We address them with Doing Length pEnalty Right (DLER), a training recipe combining batch-wise reward normalization, higher clipping, dynamic sampling, and a simple truncation length penalty. DLER achieves state-of-the-art accuracy--efficiency trade-offs, cutting output length by over 70 percent while surpassing all previous baseline accuracy. It also improves test-time scaling: compared to DeepSeek-R1-7B, DLER-7B generates multiple concise responses in parallel with 28 percent higher accuracy and lower latency. We further introduce Difficulty-Aware DLER, which adaptively tightens truncation on easier questions for additional efficiency gains. We also propose an update-selective merging method that preserves baseline accuracy while retaining the concise reasoning ability of the DLER model, which is useful for scenarios where RL training data is scarce."
    },
    {
      "title": "从像素到文字——迈向大规模原生视觉-语言基元 (原标题: From Pixels to Words -- Towards Native Vision-Language Primitives at Scale)",
      "link": "https://arxiv.org/abs/2510.14979",
      "pubDate": "Thu, 16 Oct 2025 13:59:58 GMT",
      "isoDate": "2025-10-16T13:59:58.000Z",
      "creator": "Haiwen Diao, Mingxuan Li, Silei Wu, Linjun Dai, Xiaohua Wang, Hanming Deng, Lewei Lu, Dahua Lin, Ziwei Liu",
      "summary": "### 背景与挑战\n\n原生视觉-语言模型（VLMs）作为模块化VLMs的有力竞争者正在兴起，其发展受到模型架构和训练范式演变的影响。然而，其广泛探索和推广面临两大核心挑战：\n\n*   **根本限制**：原生VLMs与典型的模块化VLMs之间存在哪些根本性限制？这些障碍能在多大程度上被克服？\n*   **研究可及性**：如何使原生VLMs的研究更易于获取和民主化，从而加速该领域的进展？\n\n### 构建原生VLM的指导原则\n\n本文旨在阐明这些挑战，并概述构建原生VLMs的指导原则。一个有效的原生VLM基元应具备以下特性：\n\n*   **像素与文字对齐**：能够在共享语义空间内有效地对齐像素和文字表示。\n*   **模块优势整合**：无缝整合先前独立的视觉和语言模块的优势。\n*   **跨模态特性**：内在体现支持统一视觉-语言编码、对齐和推理的各种跨模态特性。\n\n### NEO：一种新型原生VLM家族\n\n基于上述指导原则，研究人员推出了NEO，一个从第一性原理构建的新型原生VLM家族。NEO在多个方面展现出卓越的能力：\n\n*   **性能媲美**：NEO能够与顶级模块化VLM在各种真实场景中竞争，展现出强大的性能。\n*   **高效视觉感知**：仅使用3.9亿个图像-文本示例，NEO就能有效地从头开始发展视觉感知能力。\n*   **冲突缓解**：它能够在一个根据精心设计的基元构建的密集、单一模型内部，有效缓解视觉-语言冲突。\n*   **生态系统构建**：NEO被定位为可扩展且强大的原生VLMs的基石，并配备了一套丰富的可重用组件，以促进一个经济高效且可扩展的生态系统。\n\n### 可用性\n\nNEO的代码和模型已公开发布，旨在促进该领域的研究和应用。",
      "shortSummary": "本文介绍了NEO，一个新型原生视觉-语言模型（VLM）家族，旨在解决原生VLM与模块化VLM之间的限制以及研究可及性问题。NEO从第一性原理构建，通过有效对齐像素与文字表示、整合视觉与语言模块优势，并支持统一跨模态推理，实现了与顶级模块化VLM相媲美的性能。它仅用3.9亿图像-文本示例便能高效发展视觉感知，并缓解模型内部的视觉-语言冲突。NEO及其可重用组件为大规模原生VLM提供了一个可扩展、经济高效的生态系统。",
      "translated_title": "从像素到文字——迈向大规模原生视觉-语言基元",
      "images": [],
      "contentSource": "完整文章",
      "content": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising contender to typical modular VLMs, shaped by evolving model architectures and training paradigms. Yet, two lingering clouds cast shadows over its widespread exploration and promotion: (-) What fundamental constraints set native VLMs apart from modular ones, and to what extent can these barriers be overcome? (-) How to make research in native VLMs more accessible and democratized, thereby accelerating progress in the field. In this paper, we clarify these challenges and outline guiding principles for constructing native VLMs. Specifically, one native VLM primitive should: (i) effectively align pixel and word representations within a shared semantic space; (ii) seamlessly integrate the strengths of formerly separate vision and language modules; (iii) inherently embody various cross-modal properties that support unified vision-language encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of native VLMs built from first principles, capable of rivaling top-tier modular counterparts across diverse real-world scenarios. With only 390M image-text examples, NEO efficiently develops visual perception from scratch while mitigating vision-language conflicts inside a dense and monolithic model crafted from our elaborate primitives. We position NEO as a cornerstone for scalable and powerful native VLMs, paired with a rich set of reusable components that foster a cost-effective and extensible ecosystem. Our code and models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO."
    },
    {
      "title": "组合机器的智能体设计 (原标题: Agentic Design of Compositional Machines)",
      "link": "https://arxiv.org/abs/2510.14980",
      "pubDate": "Thu, 16 Oct 2025 13:59:58 GMT",
      "isoDate": "2025-10-16T13:59:58.000Z",
      "creator": "Wenqian Zhang, Weiyang Liu, Zhen Liu",
      "summary": "# 组合机器的智能体设计\n\n本文探讨了大型语言模型（LLMs）在复杂机器设计领域的应用潜力，特别是通过“组合机器设计”这一任务。\n\n## 研究背景与目标\n\n*   **人类智能的体现与工程实践的基础：** 复杂机器的设计是人类智能的标志，也是工程实践的核心。\n*   **LLMs的创造能力：** 鉴于LLMs的最新进展，研究者们提出疑问：LLMs是否也能学习创造？\n*   **研究切入点——组合机器设计：** 本文通过组合机器设计来回答这一问题。该任务要求从标准化组件中组装机器，以满足在模拟物理环境中（如运动或操纵）的功能需求。\n\n## BesiegeField测试平台\n\n*   **引入：** 为支持这项研究，本文引入了BesiegeField，这是一个基于机器建造游戏Besiege构建的测试平台。\n*   **核心功能：**\n    *   **基于部件的建造：** 允许用户或智能体使用标准化部件进行机器组装。\n    *   **物理模拟：** 提供真实的物理环境模拟，以测试机器的功能性。\n    *   **奖励驱动评估：** 能够根据机器的表现（如能否完成特定任务）进行奖励评估。\n\n## LLMs的基准测试与关键能力\n\n*   **方法：** 研究者使用BesiegeField对最先进的LLMs进行了基准测试，并采用了智能体工作流（agentic workflows）。\n*   **成功所需关键能力：** 通过测试，研究者识别出LLMs在机器设计任务中取得成功所需的几项关键能力：\n    *   **空间推理：** 理解部件在三维空间中的位置和相互关系。\n    *   **策略性组装：** 规划并执行有效的组装策略以实现目标功能。\n    *   **指令遵循：** 准确理解并执行设计任务的详细指令。\n\n## 挑战与未来方向\n\n*   **当前模型的局限性：** 现有的开源LLMs在这些关键能力上表现不足，未能完全胜任组合机器设计任务。\n*   **强化学习（RL）的探索：** 作为改进途径，研究者探索了强化学习（RL）的应用。\n    *   **数据集构建：** 整理了一个“冷启动”数据集（cold-start dataset）。\n    *   **RL微调实验：** 进行了RL微调实验。\n*   **开放性挑战：** 本文强调了语言、机器设计和物理推理交叉领域中存在的开放性挑战，为未来的研究指明了方向。",
      "shortSummary": "本文探讨大型语言模型（LLMs）在组合机器设计中的应用，即从标准化组件组装机器以满足模拟物理环境中的功能需求。研究引入了基于游戏Besiege的测试平台BesiegeField，用于部件建造、物理模拟和奖励评估。基准测试发现，LLMs需具备空间推理、策略性组装和指令遵循能力，但当前开源模型表现不足。为改进，研究探索了强化学习微调，并指出了语言、机器设计与物理推理交叉领域的开放挑战。",
      "translated_title": "组合机器的智能体设计",
      "images": [],
      "contentSource": "完整文章",
      "content": "The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning."
    },
    {
      "title": "无需图像编辑对即可学习图像编辑模型 (原标题: Learning an Image Editing Model without Image Editing Pairs)",
      "link": "https://arxiv.org/abs/2510.14978",
      "pubDate": "Thu, 16 Oct 2025 13:59:57 GMT",
      "isoDate": "2025-10-16T13:59:57.000Z",
      "creator": "Nupur Kumari, Sheng-Yu Wang, Nanxuan Zhao, Yotam Nitzan, Yuheng Li, Krishna Kumar Singh, Richard Zhang, Eli Shechtman, Jun-Yan Zhu, Xun Huang",
      "summary": "## 无需图像编辑对即可学习图像编辑模型\n\n### 摘要\n\n本文提出了一种创新的训练范式，旨在解决当前图像编辑模型对大量输入-目标对数据集的依赖问题。传统方法需要监督式微调，而这些配对数据难以大规模获取，导致了训练瓶颈。现有的一些替代方案使用合成训练对，但这可能将预训练模型的伪影传播并放大到最终模型中。\n\n### 核心方法\n\n该研究的核心在于完全消除了对配对数据的需求，其方法包括：\n\n*   **直接优化扩散模型**：在训练过程中展开（unrolling）一个少步扩散模型进行直接优化。\n*   **利用视觉-语言模型（VLM）反馈**：VLM在训练中扮演关键角色，它针对每个输入和编辑指令，评估编辑是否遵循指令并保留了未更改的内容。\n*   **提供直接梯度**：VLM的评估结果为端到端优化提供了直接梯度。\n*   **引入分布匹配损失（DMD）**：为确保生成图像的视觉保真度，模型融入了DMD。这限制了生成的图像保持在预训练模型学习到的图像流形（image manifold）内。\n\n### 实验与结果\n\n研究人员在标准基准上对该方法进行了评估，并进行了广泛的消融研究。结果表明：\n\n*   在少步设置下，该方法在没有任何配对数据的情况下，其性能与那些在大量监督配对数据上训练的各种图像编辑扩散模型相当。\n*   在将相同的VLM用作奖励模型时，该方法还优于基于强化学习（RL）的技术，例如Flow-GRPO。",
      "shortSummary": "本文提出一种无需图像编辑对即可训练图像编辑模型的新范式。该方法通过在训练中展开少步扩散模型，并利用视觉-语言模型（VLM）的反馈提供直接梯度进行端到端优化。同时，引入分布匹配损失（DMD）以确保视觉保真度。实验表明，在没有配对数据的情况下，该方法在性能上与使用大量监督配对数据训练的模型相当，并优于某些基于强化学习的技术。",
      "translated_title": "无需图像编辑对即可学习图像编辑模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent image editing models have achieved impressive results while following natural language editing instructions, but they rely on supervised fine-tuning with large datasets of input-target pairs. This is a critical bottleneck, as such naturally occurring pairs are hard to curate at scale. Current workarounds use synthetic training pairs that leverage the zero-shot capabilities of existing models. However, this can propagate and magnify the artifacts of the pretrained model into the final trained model. In this work, we present a new training paradigm that eliminates the need for paired data entirely. Our approach directly optimizes a few-step diffusion model by unrolling it during training and leveraging feedback from vision-language models (VLMs). For each input and editing instruction, the VLM evaluates if an edit follows the instruction and preserves unchanged content, providing direct gradients for end-to-end optimization. To ensure visual fidelity, we incorporate distribution matching loss (DMD), which constrains generated images to remain within the image manifold learned by pretrained models. We evaluate our method on standard benchmarks and include an extensive ablation study. Without any paired data, our method performs on par with various image editing diffusion models trained on extensive supervised paired data, under the few-step setting. Given the same VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO."
    },
    {
      "title": "Ponimator：展开交互姿态以实现多功能人机交互动画 (原标题: Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation)",
      "link": "https://arxiv.org/abs/2510.14976",
      "pubDate": "Thu, 16 Oct 2025 13:59:56 GMT",
      "isoDate": "2025-10-16T13:59:56.000Z",
      "creator": "Shaowei Liu, Chuan Guo, Bing Zhou, Jian Wang",
      "summary": "本文介绍了一个名为 Ponimator 的框架，旨在通过利用近距离人机交互姿态来生成多功能交互动画。\n\n**核心思想与灵感**\n*   **背景洞察**：近距离的人机交互姿态蕴含丰富的上下文信息，人类可以凭直觉推断其语境，并预测过去和未来的动态，这得益于对人类行为的强大先验知识。\n*   **灵感来源**：受此观察启发，Ponimator 框架以近距离交互姿态为核心，实现多功能交互动画。\n\n**Ponimator 框架构成**\n*   **训练数据**：框架的训练数据来源于运动捕捉（mocap）交互数据集，包含紧密接触的两人姿态及其周围的时间上下文信息。\n*   **核心模型**：Ponimator 利用交互姿态先验知识，采用了两个条件扩散模型：\n    1.  **姿态动画器 (Pose Animator)**：该模型利用时间先验，从给定的交互姿态生成动态运动序列。\n    2.  **姿态生成器 (Pose Generator)**：该模型应用空间先验，在交互姿态不可用时，可以从单个姿态、文本或两者结合来合成交互姿态。\n\n**支持的任务与应用**\nPonimator 框架支持多种多样的任务，包括：\n*   基于图像的交互动画\n*   反应动画\n*   文本到交互的合成\n\n**目标与成果**\n*   **目标**：该框架旨在促进将高质量运动捕捉数据中的交互知识转移到开放世界场景中。\n*   **实验验证**：在各种数据集和应用上的实证实验表明，姿态先验的普适性以及 Ponimator 框架的有效性和鲁棒性。\n\n**其他信息**\n*   **接受情况**：该研究已被 ICCV 2025 接受。\n*   **项目页面**：[this https URL](this https URL)\n*   **研究领域**：计算机视觉与模式识别 (cs.CV)、图形学 (cs.GR)、机器人学 (cs.RO)。\n*   **引用方式**：arXiv:2510.14976 [cs.CV]。",
      "shortSummary": "Ponimator 是一个基于近距离交互姿态的框架，用于生成多功能人机交互动画。它利用运动捕捉数据训练的两个条件扩散模型：姿态动画器从交互姿态生成运动序列，姿态生成器则从单个姿态或文本合成交互姿态。该框架支持图像动画、反应动画和文本到交互的合成，旨在将高质量交互知识应用于开放世界场景，并已在ICCV 2025上被接受。",
      "translated_title": "Ponimator：展开交互姿态以实现多功能人机交互动画",
      "images": [],
      "contentSource": "完整文章",
      "content": "Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, a simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of close-contact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) a pose animator that uses the temporal prior to generate dynamic motion sequences from interactive poses, and (2) a pose generator that applies the spatial prior to synthesize interactive poses from a single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework."
    }
  ],
  "lastUpdated": "2025-10-20T09:34:40.716Z"
}