{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "Delta激活：一种微调大型语言模型的表示方法 (原标题: Delta Activations: A Representation for Finetuned Large Language Models)",
      "link": "https://arxiv.org/abs/2509.04442",
      "pubDate": "Thu, 04 Sep 2025 13:59:06 GMT",
      "isoDate": "2025-09-04T13:59:06.000Z",
      "creator": "Zhiqiu Xu, Amish Sethi, Mayur Naik, Ser-Nam Lim",
      "summary": "## Delta激活：一种微调大型语言模型的表示方法\n\n### 背景与挑战\n*   强大的开源大型语言模型（LLMs）的成功，使得社区能够创建大量针对特定任务和领域进行微调的模型。\n*   然而，由于元数据不一致和存储库结构化不足，理解和管理这些模型仍然具有挑战性。\n\n### Delta激活方法介绍\n*   本文提出了一种名为“Delta激活”（Delta Activations）的新方法。\n*   该方法通过测量微调模型相对于基础模型的内部激活变化，将微调模型表示为向量嵌入。\n\n### Delta激活的特性与优势\n*   **有效聚类：** 这种表示方法能够有效地按领域和任务对模型进行聚类，从而揭示模型格局中的结构。\n*   **鲁棒性：** Delta激活在不同的微调设置下表现出鲁棒性。\n*   **可加性：** 当微调数据集混合时，Delta激活展现出可加性。\n*   **任务嵌入：** Delta激活还可以通过少样本微调来嵌入任务。\n*   **应用潜力：** 进一步探索了其在模型选择和模型合并方面的应用。\n\n### 目标与资源\n*   作者希望Delta激活能够促进对现有公开模型的重用。\n*   相关代码已公开。\n\n### 研究领域\n*   机器学习 (cs.LG)\n*   人工智能 (cs.AI)\n*   计算与语言 (cs.CL)\n*   信息检索 (cs.IR)",
      "shortSummary": "“Delta激活”是一种新方法，通过测量微调模型相对于基础模型的内部激活变化，将其表示为向量嵌入。这解决了微调LLM难以管理和理解的问题。该方法能有效按领域和任务聚类模型，揭示模型结构，并具有鲁棒性和可加性。它还可用于任务嵌入、模型选择和合并，旨在促进公开模型的重用。",
      "translated_title": "Delta激活：一种微调大型语言模型的表示方法",
      "images": [],
      "contentSource": "完整文章",
      "content": "The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collection of post-trained models adapted to specific tasks and domains. However, navigating and understanding these models remains challenging due to inconsistent metadata and unstructured repositories. We introduce Delta Activations, a method to represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model. This representation allows for effective clustering by domain and task, revealing structure in the model landscape. Delta Activations also demonstrate desirable properties: it is robust across finetuning settings and exhibits an additive property when finetuning datasets are mixed. In addition, we show that Delta Activations can embed tasks via few-shot finetuning, and further explore its use for model selection and merging. We hope Delta Activations can facilitate the practice of reusing publicly available models. Code is available at https://github.com/OscarXZQ/delta_activations."
    },
    {
      "title": "榴莲：双参考引导的人像动画与属性迁移 (原标题: Durian: Dual Reference-guided Portrait Animation with Attribute Transfer)",
      "link": "https://arxiv.org/abs/2509.04434",
      "pubDate": "Thu, 04 Sep 2025 13:53:03 GMT",
      "isoDate": "2025-09-04T13:53:03.000Z",
      "creator": "Hyunsoo Cha, Byungjun Kim, Hanbyul Joo",
      "summary": "## Durian：双参考引导的人像动画与属性迁移\n\n### 概述\n\nDurian 是一种开创性的方法，首次实现了从给定参考图像向目标人像进行零样本（zero-shot）面部属性迁移，并生成高质量的人像动画视频。\n\n### 核心技术与方法\n\n*   **双参考网络（Dual Reference Networks）**\n    *   为确保跨帧属性迁移的高保真度和空间一致性，Durian 引入了双参考网络。\n    *   这些网络能够将来自人像图像和属性图像的空间特征注入到扩散模型的去噪过程中。\n\n*   **自重建训练范式（Self-reconstruction Formulation）**\n    *   模型采用独特的自重建方式进行训练。\n    *   具体而言，从同一人像视频中采样两帧：一帧被视为属性参考，另一帧作为目标人像。\n    *   模型随后根据这些输入及其对应的掩码重建视频中的剩余帧。\n\n*   **掩码扩展策略（Mask Expansion Strategy）**\n    *   为了有效支持具有不同空间范围的属性迁移，Durian 提出了一种基于关键点条件图像生成（keypoint-conditioned image generation）的掩码扩展策略，应用于训练阶段。\n\n*   **鲁棒性增强（Robustness Augmentation）**\n    *   为了提高模型对属性图像和人像图像之间可能存在的姿态或位置不对齐的鲁棒性，Durian 进一步通过空间和外观级别的变换来增强这些图像。\n\n### 主要成果与优势\n\n*   **强大的泛化能力**\n    *   尽管在训练时没有明确的三元组监督，上述策略使得 Durian 模型能够有效地泛化到多样化的属性和“野外”（in-the-wild）参考组合。\n\n*   **最先进的性能**\n    *   Durian 在带属性迁移的人像动画领域取得了最先进的性能。\n\n*   **多属性组合能力**\n    *   值得注意的是，其独特的双参考设计使得在单次生成过程中即可实现多属性的组合，而无需进行额外的训练或模型调整。",
      "shortSummary": "Durian 是一种创新方法，首次实现了从参考图像向目标人像进行零样本面部属性迁移，并生成人像动画。它通过引入双参考网络，将人像和属性特征注入扩散模型，并采用自重建和掩码扩展策略进行训练。Durian 在属性迁移人像动画方面取得了最先进的性能，并能实现多属性组合，无需额外训练，展现了强大的泛化能力。",
      "translated_title": "榴莲：双参考引导的人像动画与属性迁移",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Durian, the first method for generating portrait animation videos with facial attribute transfer from a given reference image to a target portrait in a zero-shot manner. To enable high-fidelity and spatially consistent attribute transfer across frames, we introduce dual reference networks that inject spatial features from both the portrait and attribute images into the denoising process of a diffusion model. We train the model using a self-reconstruction formulation, where two frames are sampled from the same portrait video: one is treated as the attribute reference and the other as the target portrait, and the remaining frames are reconstructed conditioned on these inputs and their corresponding masks. To support the transfer of attributes with varying spatial extent, we propose a mask expansion strategy using keypoint-conditioned image generation for training. In addition, we further augment the attribute and portrait images with spatial and appearance-level transformations to improve robustness to positional misalignment between them. These strategies allow the model to effectively generalize across diverse attributes and in-the-wild reference combinations, despite being trained without explicit triplet supervision. Durian achieves state-of-the-art performance on portrait animation with attribute transfer, and notably, its dual reference design enables multi-attribute composition in a single generation pass without additional training."
    },
    {
      "title": "迈向大语言模型后训练的统一视角 (原标题: Towards a Unified View of Large Language Model Post-Training)",
      "link": "https://arxiv.org/abs/2509.04419",
      "pubDate": "Thu, 04 Sep 2025 13:40:33 GMT",
      "isoDate": "2025-09-04T13:40:33.000Z",
      "creator": "Xingtai Lv, Yuxin Zuo, Youbang Sun, Hongyi Liu, Yuntian Wei, Zhekai Chen, Lixuan He, Xuekai Zhu, Kaiyan Zhang, Bingning Wang, Ning Ding, Bowen Zhou",
      "summary": "## 迈向大语言模型后训练的统一视角：核心发现与HPT算法\n\n本文深入探讨了现代大语言模型（LLM）后训练的两种主要方法——强化学习（RL）和监督微调（SFT），并提出了一个统一的理论框架和相应的算法。\n\n### 1. 后训练数据源与现有方法\n\n*   **数据源**：LLM的后训练主要依赖两种类型的训练数据：\n    *   **在线数据**：通常指模型自身生成的“回滚数据”（rollouts）。\n    *   **离线数据**：通常指人类或其它模型提供的“演示数据”（demonstrations）。\n*   **典型方法**：\n    *   **强化学习（RL）**：常用于处理在线数据。\n    *   **监督微调（SFT）**：常用于处理离线数据。\n\n### 2. 核心理论贡献：统一策略梯度估计器\n\n*   **统一视角**：本文的核心观点是，RL和SFT这两种看似不同的方法并非相互矛盾，而是单一优化过程的不同实例。\n*   **理论推导**：作者推导出了一个“统一策略梯度估计器”（Unified Policy Gradient Estimator）。\n    *   该估计器能够将广泛的后训练方法计算表示为在不同数据分布假设和各种偏差-方差权衡下，一个共同目标函数的梯度。\n*   **估计器构成**：该梯度估计器由四个可互换的部分构成：\n    *   **稳定化掩码**（stabilization mask）\n    *   **参考策略分母**（reference policy denominator）\n    *   **优势估计**（advantage estimate）\n    *   **似然梯度**（likelihood gradient）\n\n### 3. 提出的算法：混合后训练（Hybrid Post-Training, HPT）\n\n*   **算法动机**：受上述理论发现的启发，本文提出了一种名为“混合后训练”（HPT）的新算法。\n*   **设计目标**：\n    *   HPT旨在动态选择不同的训练信号，以实现最佳效果。\n    *   它被设计为能够有效利用演示数据（exploitation）。\n    *   同时，HPT也能实现稳定的探索（exploration）。\n    *   在上述过程中，HPT致力于不牺牲模型已学习到的推理模式。\n\n### 4. 实验验证与成果\n\n*   **广泛实验**：通过大量的实验和消融研究，本文验证了其统一理论框架和HPT算法的有效性。\n*   **卓越性能**：在六个数学推理基准测试和两个分布外（out-of-distribution）套件上，HPT算法持续超越了不同规模和家族模型的强大基线，展现出其优越性。",
      "shortSummary": "本文提出大语言模型后训练中的强化学习（RL）和监督微调（SFT）并非矛盾，而是单一优化过程的实例。作者推导了统一策略梯度估计器，将多种后训练方法统一在一个共同目标函数下。受此启发，提出混合后训练（HPT）算法，旨在动态选择训练信号，有效利用演示数据并实现稳定探索，同时保留推理模式。实验证明，HPT在多个数学推理基准上持续超越现有基线。",
      "translated_title": "迈向大语言模型后训练的统一视角",
      "images": [],
      "contentSource": "完整文章",
      "content": "Two major sources of training data exist for post-training modern language models: online (model-generated rollouts) data, and offline (human or other-model demonstrations) data. These two types of data are typically used by approaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT), respectively. In this paper, we show that these approaches are not in contradiction, but are instances of a single optimization process. We derive a Unified Policy Gradient Estimator, and present the calculations of a wide spectrum of post-training approaches as the gradient of a common objective under different data distribution assumptions and various bias-variance tradeoffs. The gradient estimator is constructed with four interchangeable parts: stabilization mask, reference policy denominator, advantage estimate, and likelihood gradient. Motivated by our theoretical findings, we propose Hybrid Post-Training (HPT), an algorithm that dynamically selects different training signals. HPT is designed to yield both effective exploitation of demonstration and stable exploration without sacrificing learned reasoning patterns. We provide extensive experiments and ablation studies to verify the effectiveness of our unified theoretical framework and HPT. Across six mathematical reasoning benchmarks and two out-of-distribution suites, HPT consistently surpasses strong baselines across models of varying scales and families."
    },
    {
      "title": "通过边际数据传输蒸馏实现三维生成的少步流 (原标题: Few-step Flow for 3D Generation via Marginal-Data Transport Distillation)",
      "link": "https://arxiv.org/abs/2509.04406",
      "pubDate": "Thu, 04 Sep 2025 13:24:31 GMT",
      "isoDate": "2025-09-04T13:24:31.000Z",
      "creator": "Zanwei Zhou, Taoran Yi, Jiemin Fang, Chen Yang, Lingxi Xie, Xinggang Wang, Wei Shen, Qi Tian",
      "summary": "### 引言\n\n*   流式三维生成模型在推理过程中通常需要数十个采样步骤。\n*   尽管少步蒸馏方法，特别是“一致性模型”（Consistency Models, CMs），在加速二维扩散模型方面取得了显著进展，但它们在更复杂的三维生成任务中仍未得到充分探索。\n\n### MDT-dist 框架\n\n*   本研究提出了一种新颖的框架 MDT-dist，用于少步三维流蒸馏。\n*   其核心目标是：蒸馏预训练模型以学习“边际数据传输”（Marginal-Data Transport）。\n*   直接学习此目标需要整合速度场，但这种积分在实现上是难以处理的。\n\n### 优化目标\n\n为了解决积分难题，研究提出了两个可优化的目标，将优化目标从传输层面等效地转换到速度和分布层面：\n\n*   **速度匹配 (Velocity Matching, VM)**：\n    *   学习稳定地匹配学生模型和教师模型之间的速度场。\n    *   然而，VM 不可避免地会提供有偏的梯度估计。\n*   **速度蒸馏 (Velocity Distillation, VD)**：\n    *   通过利用学习到的速度场执行概率密度蒸馏，进一步增强了优化过程。\n\n### 实验结果\n\n*   在先驱三维生成框架 TRELLIS 上进行评估。\n*   方法将每个流变压器的采样步骤从 25 减少到 1 或 2。\n*   在 A800 上实现了 0.68 秒（1 步 x 2）和 0.94 秒（2 步 x 2）的延迟，分别带来了 9.0 倍和 6.5 倍的速度提升。\n*   同时，保持了高视觉和几何保真度。\n*   广泛的实验表明，该方法显著优于现有的一致性模型蒸馏方法，并使 TRELLIS 在少步三维生成中实现了卓越性能。",
      "shortSummary": "本研究提出 MDT-dist 框架，旨在通过边际数据传输蒸馏实现少步三维生成。针对直接学习传输的积分难题，引入了速度匹配（VM）和速度蒸馏（VD）两种优化目标。实验表明，MDT-dist 在 TRELLIS 框架上将采样步骤从 25 减少到 1 或 2，在 A800 上实现了高达 9.0 倍的速度提升，同时保持了高视觉和几何保真度，显著优于现有方法。",
      "translated_title": "通过边际数据传输蒸馏实现三维生成的少步流",
      "images": [],
      "contentSource": "完整文章",
      "content": "Flow-based 3D generation models typically require dozens of sampling steps during inference. Though few-step distillation methods, particularly Consistency Models (CMs), have achieved substantial advancements in accelerating 2D diffusion models, they remain under-explored for more complex 3D generation tasks. In this study, we propose a novel framework, MDT-dist, for few-step 3D flow distillation. Our approach is built upon a primary objective: distilling the pretrained model to learn the Marginal-Data Transport. Directly learning this objective needs to integrate the velocity fields, while this integral is intractable to be implemented. Therefore, we propose two optimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD), to equivalently convert the optimization target from the transport level to the velocity and the distribution level respectively. Velocity Matching (VM) learns to stably match the velocity fields between the student and the teacher, but inevitably provides biased gradient estimates. Velocity Distillation (VD) further enhances the optimization process by leveraging the learned velocity fields to perform probability density distillation. When evaluated on the pioneer 3D generation framework TRELLIS, our method reduces sampling steps of each flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s (2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high visual and geometric fidelity. Extensive experiments demonstrate that our method significantly outperforms existing CM distillation methods, and enables TRELLIS to achieve superior performance in few-step 3D generation."
    },
    {
      "title": "转换模型：重新思考生成式学习目标 (原标题: Transition Models: Rethinking the Generative Learning Objective)",
      "link": "https://arxiv.org/abs/2509.04394",
      "pubDate": "Thu, 04 Sep 2025 13:05:59 GMT",
      "isoDate": "2025-09-04T13:05:59.000Z",
      "creator": "Zidong Wang, Yiyuan Zhang, Xiaoyu Yue, Xiangyu Yue, Yangguang Li, Wanli Ouyang, Lei Bai",
      "summary": "## 转换模型 (TiM)：重新定义生成式学习\n\n### 引言：生成式建模的挑战\n\n生成式建模领域长期存在一个核心困境：\n\n*   **迭代扩散模型**：能够实现卓越的保真度，但其计算成本非常高昂。\n*   **高效少步替代方案**：虽然效率更高，但其生成质量受到严格的上限限制。\n\n这种生成步数与输出质量之间的矛盾，根源在于现有的训练目标过于局限，它们要么只关注无穷小动态（如PF-ODEs），要么仅限于直接的端点预测。\n\n### 转换模型 (TiM) 的核心创新\n\n为解决上述挑战，研究人员引入了一种**精确的、连续时间动态方程**。该方程能够解析地定义任意有限时间间隔内的状态转换，从而催生了一种全新的生成范式——**转换模型 (TiM)**。\n\nTiM 的关键特性包括：\n\n*   **适应任意步长转换**：TiM 能够灵活适应从单步跳跃到多步精细化的任意步长转换。\n*   **无缝遍历生成轨迹**：它能够无缝地在生成轨迹上进行操作，无论是快速生成还是逐步优化。\n\n### TiM 的卓越性能与效率\n\n尽管 TiM 仅拥有 **8.65 亿参数**，但其性能表现达到了最先进水平：\n\n*   **超越领先模型**：在所有评估的步数下，TiM 均超越了参数量更大的领先模型，例如 SD3.5 (80 亿参数) 和 FLUX.1 (120 亿参数)。\n*   **单调质量提升**：与以往的少步生成器不同，TiM 展现出随着采样预算（即生成步数）的增加，输出质量呈现**单调提升**的特性。\n*   **高分辨率能力**：当采用其原生分辨率策略时，TiM 能够在高达 **4096x4096** 的分辨率下提供卓越的图像保真度。\n\n### 资源\n\n*   相关代码已发布。",
      "shortSummary": "转换模型（TiM）旨在解决生成式建模中扩散模型高保真度但高成本，以及少步模型效率高但质量受限的矛盾。TiM 引入精确的连续时间动态方程，能够适应任意步长转换，实现从单步到精细化的生成。TiM 仅用 8.65 亿参数，便超越了 SD3.5 和 FLUX.1 等大型模型，并在增加采样步数时展现出单调的质量提升，支持高达 4096x4096 的高分辨率生成，达到了最先进的性能。",
      "translated_title": "转换模型：重新思考生成式学习目标",
      "images": [],
      "contentSource": "完整文章",
      "content": "A fundamental dilemma in generative modeling persists: iterative diffusion models achieve outstanding fidelity, but at a significant computational cost, while efficient few-step alternatives are constrained by a hard quality ceiling. This conflict between generation steps and output quality arises from restrictive training objectives that focus exclusively on either infinitesimal dynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by introducing an exact, continuous-time dynamics equation that analytically defines state transitions across any finite time interval. This leads to a novel generative paradigm, Transition Models (TiM), which adapt to arbitrary-step transitions, seamlessly traversing the generative trajectory from single leaps to fine-grained refinement with more steps. Despite having only 865M parameters, TiM achieves state-of-the-art performance, surpassing leading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across all evaluated step counts. Importantly, unlike previous few-step generators, TiM demonstrates monotonic quality improvement as the sampling budget increases. Additionally, when employing our native-resolution strategy, TiM delivers exceptional fidelity at resolutions up to 4096x4096."
    },
    {
      "title": "从编辑器到密集几何估计器 (原标题: From Editor to Dense Geometry Estimator)",
      "link": "https://arxiv.org/abs/2509.04338",
      "pubDate": "Thu, 04 Sep 2025 11:58:50 GMT",
      "isoDate": "2025-09-04T11:58:50.000Z",
      "creator": "JiYuan Wang, Chunyu Lin, Lei Sun, Rongying Liu, Lang Nie, Mingxing Li, Kang Liao, Xiangxiang Chu, Yao Zhao",
      "summary": "## 从编辑器到密集几何估计器：FE2E框架\n\n### 摘要\n本文介绍了一种名为FE2E（From Editor to Dense Geometry Estimator）的新型框架，该框架开创性地将基于Diffusion Transformer (DiT) 架构的先进图像编辑模型应用于密集几何估计任务。研究发现，图像编辑模型相比文本到图像（T2I）生成模型，更适合处理图像到图像的密集预测任务，并能取得更优异的性能。\n\n### 背景与动机\n尽管预训练的文本到图像（T2I）生成模型在密集预测任务中取得了一定成功，但密集预测本质上是一个图像到图像的任务。这促使研究人员思考，图像编辑模型而非T2I生成模型，可能作为微调的基础更为合适。\n\n### 核心发现\n研究人员对编辑模型和生成模型在密集几何估计任务中的微调行为进行了系统分析。结果表明：\n*   **固有结构先验**：编辑模型拥有固有的结构先验。\n*   **稳定收敛**：这些先验使得编辑模型能够通过“精炼”其内在特征，实现更稳定的收敛。\n*   **更高性能**：最终，编辑模型比其生成模型对应物取得了更高的性能。\n\n### FE2E框架介绍\n基于上述发现，FE2E框架被提出，它将一个先进的、基于Diffusion Transformer (DiT) 架构的编辑模型适配到密集几何预测任务中。FE2E的关键创新和技术细节包括：\n\n1.  **损失函数重构**：为了使编辑模型适应确定性任务，FE2E将编辑模型原始的流匹配损失（flow matching loss）重构为“一致速度”（consistent velocity）训练目标。\n2.  **精度冲突解决**：针对编辑模型原生BFloat16格式与几何任务所需高精度之间的冲突，FE2E采用了对数量化（logarithmic quantization）来解决。\n3.  **深度与法线联合估计**：FE2E利用DiT模型的全局注意力机制，实现了深度和法线的无成本联合估计，在一个前向传播中同时完成。这使得它们的监督信号能够相互增强，提升估计精度。\n\n### 实验结果与性能\nFE2E在零样本单目深度和法线估计任务中展现了令人印象深刻的性能提升，并在多个数据集上取得了优异表现：\n*   **无需扩展训练数据**：在不增加训练数据量的情况下，FE2E实现了显著的性能提升。\n*   **ETH3D数据集**：在ETH3D数据集上，性能提升超过35%。\n*   **超越现有SOTA**：FE2E的性能超越了DepthAnything系列模型，后者使用了100倍的数据进行训练。",
      "shortSummary": "本文提出了FE2E框架，将基于Diffusion Transformer (DiT) 的图像编辑模型应用于密集几何估计。研究发现，编辑模型因其固有的结构先验，比文本到图像生成模型更适合此任务，能实现更稳定的收敛和更高的性能。FE2E通过重构损失、解决精度冲突和联合估计深度与法线，在零样本单目深度和法线估计上取得显著进展。它在ETH3D数据集上性能提升超35%，并在不增加训练数据量的情况下超越了使用100倍数据训练的DepthAnything系列。",
      "translated_title": "从编辑器到密集几何估计器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Leveraging visual priors from pre-trained text-to-image (T2I) generative models has shown success in dense prediction. However, dense prediction is inherently an image-to-image task, suggesting that image editing models, rather than T2I generative models, may be a more suitable foundation for fine-tuning.   Motivated by this, we conduct a systematic analysis of the fine-tuning behaviors of both editors and generators for dense geometry estimation. Our findings show that editing models possess inherent structural priors, which enable them to converge more stably by ``refining\" their innate features, and ultimately achieve higher performance than their generative counterparts.   Based on these findings, we introduce FE2E, a framework that pioneeringly adapts an advanced editing model based on Diffusion Transformer (DiT) architecture for dense geometry prediction. Specifically, to tailor the editor for this deterministic task, we reformulate the editor's original flow matching loss into the ``consistent velocity\" training objective. And we use logarithmic quantization to resolve the precision conflict between the editor's native BFloat16 format and the high precision demand of our tasks. Additionally, we leverage the DiT's global attention for a cost-free joint estimation of depth and normals in a single forward pass, enabling their supervisory signals to mutually enhance each other.   Without scaling up the training data, FE2E achieves impressive performance improvements in zero-shot monocular depth and normal estimation across multiple datasets. Notably, it achieves over 35\\% performance gains on the ETH3D dataset and outperforms the DepthAnything series, which is trained on 100times data. The project page can be accessed https://amap-ml.github.io/FE2E/{here}."
    },
    {
      "title": "逆向IFEval：大型语言模型能否摒弃顽固的训练惯例以遵循真实指令？ (原标题: Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?)",
      "link": "https://arxiv.org/abs/2509.04292",
      "pubDate": "Thu, 04 Sep 2025 11:03:02 GMT",
      "isoDate": "2025-09-04T11:03:02.000Z",
      "creator": "Qinyan Zhang, Xinping Lei, Ruijie Miao, Yu Fu, Haojie Fan, Le Chang, Jiafan Hou, Dingling Zhang, Zhongfei Hou, Ziqiang Yang, Changxin Pu, Fei Hu, Jingkai Liu, Mengyun Liu, Yang Liu, Xiang Gao, Jiaheng Liu, Tong Yang, Zaiyuan Wang, Ge Zhang, Wenhao Huang",
      "summary": "# 逆向IFEval：评估大型语言模型对反直觉指令的遵循能力\n\n## 引言\n大型语言模型（LLMs）在各种任务中表现出强大的性能，但它们常常表现出一种“认知惯性”。这种惯性使得LLMs难以遵循那些与在监督微调（SFT）期间学到的标准化模式相冲突的指令。\n\n## 提出问题与解决方案\n为了评估LLMs的这一局限性，研究团队提出了“逆向IFEval”（Inverse IFEval）基准。该基准旨在衡量模型的“反直觉能力”（Counter-intuitive Ability），即模型克服训练诱导偏见并遵守对抗性指令的能力。\n\n## 逆向IFEval的挑战类型\n逆向IFEval引入了八种挑战类型，以全面测试LLMs的适应性，其中包括：\n*   **问题纠正 (Question Correction)**\n*   **故意文本缺陷 (Intentional Textual Flaws)**\n*   **无注释代码 (Code without Comments)**\n*   **反事实回答 (Counterfactual Answering)**\n\n## 数据集构建与评估框架\n*   研究团队通过“人机协作”（human-in-the-loop）流程，构建了一个包含1012个高质量中英文问题的综合数据集。\n*   这些问题精心设计，涵盖了23个不同的领域，确保了基准的广泛性和代表性。\n*   评估过程采用了一个优化的“LLM作为评判者”（LLM-as-a-Judge）框架，以确保评估的客观性和效率。\n\n## 实验结果与发现\n在现有领先的LLMs上进行的实验充分证明了所提出的逆向IFEval基准的必要性。研究结果强调，未来的模型对齐（alignment）工作不应仅仅追求语言的流畅性和事实的正确性，更应着重考虑模型在非常规和非标准情境下的适应性。\n\n## 研究展望与意义\n研究者希望逆向IFEval能够发挥双重作用：\n1.  **诊断工具：** 帮助识别LLMs在遵循反直觉指令方面的弱点。\n2.  **方法开发基础：** 为开发旨在缓解认知惯性、减少对狭隘模式过拟合的方法提供基础。\n\n最终目标是显著增强LLMs在多样化和不可预测的真实世界场景中遵循指令的可靠性。",
      "shortSummary": "大型语言模型（LLMs）常因“认知惯性”难以遵循与训练模式冲突的指令。为解决此问题，研究者提出了“逆向IFEval”基准，旨在衡量LLMs克服训练偏见、遵循反直觉指令的能力。该基准包含八种挑战类型，并构建了一个涵盖23个领域、1012个中英文问题的数据集。实验表明，现有LLMs缺乏在非常规情境下的适应性。逆向IFEval旨在诊断并改进LLMs的指令遵循可靠性，减少过拟合，以应对真实世界的多样化挑战。",
      "translated_title": "逆向IFEval：大型语言模型能否摒弃顽固的训练惯例以遵循真实指令？",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) achieve strong performance on diverse tasks but often exhibit cognitive inertia, struggling to follow instructions that conflict with the standardized patterns learned during supervised fine-tuning (SFT). To evaluate this limitation, we propose Inverse IFEval, a benchmark that measures models Counter-intuitive Abilitytheir capacity to override training-induced biases and comply with adversarial instructions. Inverse IFEval introduces eight types of such challenges, including Question Correction, Intentional Textual Flaws, Code without Comments, and Counterfactual Answering. Using a human-in-the-loop pipeline, we construct a dataset of 1012 high-quality Chinese and English questions across 23 domains, evaluated under an optimized LLM-as-a-Judge framework. Experiments on existing leading LLMs demonstrate the necessity of our proposed Inverse IFEval benchmark. Our findings emphasize that future alignment efforts should not only pursue fluency and factual correctness but also account for adaptability under unconventional contexts. We hope that Inverse IFEval serves as both a diagnostic tool and a foundation for developing methods that mitigate cognitive inertia, reduce overfitting to narrow patterns, and ultimately enhance the instruction-following reliability of LLMs in diverse and unpredictable real-world scenarios."
    },
    {
      "title": "NER Retriever：基于类型感知嵌入的零样本命名实体检索 (原标题: NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings)",
      "link": "https://arxiv.org/abs/2509.04011",
      "pubDate": "Thu, 04 Sep 2025 04:42:23 GMT",
      "isoDate": "2025-09-04T04:42:23.000Z",
      "creator": "Or Shachar, Uri Katz, Yoav Goldberg, Oren Glickman",
      "summary": "本文介绍了 NER Retriever，一个用于 Ad-hoc 命名实体检索的零样本检索框架。Ad-hoc 命名实体检索是命名实体识别 (NER) 的一种变体，其中感兴趣的类型不是预先提供的，而是使用用户定义的类型描述来检索提及该类型实体的文档。\n\n*   **核心思想：** NER Retriever 没有依赖固定的模式或微调的模型，而是利用大型语言模型 (LLM) 的内部表示，将实体提及和用户提供的开放式类型描述嵌入到共享的语义空间中。\n*   **关键发现：** 研究表明，内部表示，特别是来自中间层 Transformer 块的值向量，比常用的顶层嵌入更有效地编码细粒度的类型信息。\n*   **方法：** 为了改进这些表示，研究人员训练了一个轻量级的对比投影网络，该网络对齐类型兼容的实体，同时分离不相关的类型。生成的实体嵌入是紧凑的、类型感知的，并且非常适合最近邻搜索。\n*   **实验结果：** 在三个基准测试中进行评估，NER Retriever 显著优于词汇和密集句子级别的检索基线。\n*   **意义：** 该研究结果为 LLM 中的表示选择提供了经验支持，并展示了可扩展的、无模式实体检索的实用解决方案。\n*   **代码：** NER Retriever 代码库已公开。",
      "shortSummary": "本文提出了 NER Retriever，一个用于 Ad-hoc 命名实体检索的零样本框架。该框架利用大型语言模型的内部表示，将实体提及和用户定义的类型描述嵌入到共享的语义空间中。通过训练对比投影网络来对齐类型兼容的实体，实现了紧凑且类型感知的实体嵌入，从而显著优于传统检索方法。该方法无需预定义模式，为可扩展的实体检索提供了实用解决方案。NER Retriever 代码库已公开。",
      "translated_title": "NER Retriever：基于类型感知嵌入的零样本命名实体检索",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named Entity Retrieval, a variant of Named Entity Recognition (NER), where the types of interest are not provided in advance, and a user-defined type description is used to retrieve documents mentioning entities of that type. Instead of relying on fixed schemas or fine-tuned models, our method builds on internal representations of large language models (LLMs) to embed both entity mentions and user-provided open-ended type descriptions into a shared semantic space. We show that internal representations, specifically the value vectors from mid-layer transformer blocks, encode fine-grained type information more effectively than commonly used top-layer embeddings. To refine these representations, we train a lightweight contrastive projection network that aligns type-compatible entities while separating unrelated types. The resulting entity embeddings are compact, type-aware, and well-suited for nearest-neighbor search. Evaluated on three benchmarks, NER Retriever significantly outperforms both lexical and dense sentence-level retrieval baselines. Our findings provide empirical support for representation selection within LLMs and demonstrate a practical solution for scalable, schema-free entity retrieval. The NER Retriever Codebase is publicly available at https://github.com/ShacharOr100/ner_retriever"
    },
    {
      "title": "虚假的安全感：为什么基于探测的恶意输入检测未能泛化 (原标题: False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize)",
      "link": "https://arxiv.org/abs/2509.03888",
      "pubDate": "Thu, 04 Sep 2025 01:15:55 GMT",
      "isoDate": "2025-09-04T01:15:55.000Z",
      "creator": "Cheng Wang, Zeming Wei, Qin Liu, Muhao Chen",
      "summary": "# 虚假的安全感：为什么基于探测的恶意输入检测未能泛化\n\n## 引言与背景\n大型语言模型（LLMs）尽管能力强大，但其服从有害指令的能力引发了严重的安全担忧。为了解决这一问题，近期研究利用基于探测（probing-based）的方法来研究LLMs内部表示中恶意输入和良性输入的可分离性，并提议将这些探测方法用于安全检测。\n\n## 研究问题与假设\n本研究系统地重新审视了这一范式。鉴于现有方法在分布外（out-of-distribution）性能不佳，研究者提出假设：探测器学习的是表层模式，而非语义上的有害性。\n\n## 研究方法与发现\n通过一系列受控实验，本研究证实了上述假设，并识别出探测器学习到的具体模式：\n*   **指令模式（instructional patterns）**\n*   **触发词（trigger words）**\n\n研究遵循系统化的方法，逐步深入：\n1.  **性能比较**：首先，研究展示了简单的n-gram方法能够达到与现有探测方法相当的性能，这暗示了探测器可能依赖于浅层特征。\n2.  **受控实验**：接着，研究使用了经过语义清洗的数据集进行受控实验，以排除表层模式的影响，从而更清晰地揭示探测器学习的本质。\n3.  **模式依赖性分析**：最后，研究对探测器学习的模式依赖性进行了详细分析，进一步确认了其对指令模式和触发词的偏好。\n\n## 结论与启示\n这些结果揭示了当前基于探测的方法所带来的“虚假的安全感”。它们未能真正捕捉到输入的语义有害性，而是依赖于容易被规避的表层特征。\n\n## 未来展望\n本研究强调了重新设计模型和评估协议的必要性。文章提供了进一步的讨论，旨在为该方向上负责任的未来研究提供建议。\n\n## 项目开源\n该项目已在 [this https URL](https://this.https.url/) 开源。",
      "shortSummary": "大型语言模型（LLMs）服从有害指令引发安全担忧。现有基于探测的恶意输入检测方法泛化能力差，本研究发现其学习的是表层模式（如指令模式和触发词），而非语义上的有害性。这导致了虚假的安全感，表明当前方法不足以确保LLM安全。研究强调需重新设计模型和评估协议，以实现更负责任的安全研究。",
      "translated_title": "虚假的安全感：为什么基于探测的恶意输入检测未能泛化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) can comply with harmful instructions, raising serious safety concerns despite their impressive capabilities. Recent work has leveraged probing-based approaches to study the separability of malicious and benign inputs in LLMs' internal representations, and researchers have proposed using such probing methods for safety detection. We systematically re-examine this paradigm. Motivated by poor out-of-distribution performance, we hypothesize that probes learn superficial patterns rather than semantic harmfulness. Through controlled experiments, we confirm this hypothesis and identify the specific patterns learned: instructional patterns and trigger words. Our investigation follows a systematic approach, progressing from demonstrating comparable performance of simple n-gram methods, to controlled experiments with semantically cleaned datasets, to detailed analysis of pattern dependencies. These results reveal a false sense of security around current probing-based approaches and highlight the need to redesign both models and evaluation protocols, for which we provide further discussions in the hope of suggesting responsible further research in this direction. We have open-sourced the project at https://github.com/WangCheng0116/Why-Probe-Fails."
    },
    {
      "title": "胡言乱语学：挑战大型语言模型深度解读无意义文本的能力 (原标题: Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth)",
      "link": "https://arxiv.org/abs/2509.03867",
      "pubDate": "Wed, 03 Sep 2025 23:58:55 GMT",
      "isoDate": "2025-09-03T23:58:55.000Z",
      "creator": "Yang Wang, Chenghao Xiao, Chia-Yi Hsiao, Zi Yan Chang, Chi-Li Chen, Tyler Loakman, Chenghua Lin",
      "summary": "# 胡言乱语学：挑战大型语言模型深度解读无意义文本的能力\n\n## 1. 概念引入：胡言乱语学 (Drivelology)\n*   **定义**: 胡言乱语学 (Drivelology) 是一种独特的语言现象，被描述为“有深度的无意义文本”。\n*   **特征**: 这类文本在语法上连贯，但在语用上却自相矛盾、情感饱满或具有颠覆性的修辞功能。\n*   **内在含义**: 尽管表面看似无意义，但它们编码了需要上下文推断、道德推理或情感解释的隐含意义。\n\n## 2. 大型语言模型 (LLMs) 的挑战\n*   **现有问题**: 尽管LLMs在许多自然语言处理 (NLP) 任务中表现出色，但它们始终未能理解胡言乱语学文本的深层语义。\n\n## 3. 研究方法与数据集构建\n*   **数据集**: 研究团队构建了一个小而多样化的基准数据集，包含超过1,200个精心策划的示例。\n*   **语言多样性**: 示例涵盖英语、普通话、西班牙语、法语、日语和韩语。\n*   **标注挑战**: 标注过程极具挑战性，每个示例都需要专家进行仔细审查，以验证其是否真正反映了胡言乱语学的特征。由于其微妙和主观的性质，标注涉及多轮讨论和裁决以解决分歧。\n*   **评估任务**: 研究评估了LLMs在分类、生成和推理任务上的表现。\n\n## 4. 主要发现与局限性\n*   **LLMs的局限性**:\n    *   模型经常将胡言乱语学与浅层无意义文本混淆。\n    *   生成不连贯的解释。\n    *   完全未能捕捉到隐含的修辞功能。\n*   **深层含义**: 这些发现揭示了LLMs在语用理解方面存在更深层次的表征鸿沟，并挑战了“统计流畅性意味着认知理解”的假设。\n\n## 5. 贡献与未来展望\n*   **资源发布**: 研究团队发布了其数据集和代码，以促进在超越表面连贯性的语言深度建模方面的进一步研究。\n*   **会议接受**: 该研究已被EMNLP 2025主会议接受进行口头报告。",
      "shortSummary": "本研究引入“胡言乱语学”（Drivelology），一种语法连贯但语用矛盾、情感丰富或修辞颠覆的“有深度的无意义文本”。研究发现，大型语言模型（LLMs）难以理解此类文本的深层语义。通过构建一个包含1200多个多语言示例的基准数据集，并评估LLMs在分类、生成和推理任务上的表现，结果显示LLMs常将其与浅层无意义文本混淆，并错过隐含的修辞功能。这揭示了LLMs在语用理解上的深层缺陷，挑战了统计流畅性等同于认知理解的假设。数据集和代码已发布以促进后续研究。",
      "translated_title": "胡言乱语学：挑战大型语言模型深度解读无意义文本的能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Drivelology, a unique linguistic phenomenon characterised as \"nonsense with depth\", utterances that are syntactically coherent yet pragmatically paradoxical, emotionally loaded, or rhetorically subversive. While such expressions may resemble surface-level nonsense, they encode implicit meaning requiring contextual inference, moral reasoning, or emotional interpretation. We find that current large language models (LLMs), despite excelling at many natural language processing (NLP) tasks, consistently fail to grasp the layered semantics of Drivelological text. To investigate this, we construct a small but diverse benchmark dataset of over 1,200 meticulously curated examples, with select instances in English, Mandarin, Spanish, French, Japanese, and Korean. Annotation was especially challenging: each of the examples required careful expert review to verify that it truly reflected Drivelological characteristics. The process involved multiple rounds of discussion and adjudication to address disagreements, highlighting the subtle and subjective nature of the Drivelology. We evaluate a range of LLMs on classification, generation, and reasoning tasks. Our results reveal clear limitations of LLMs: models often confuse Drivelology with shallow nonsense, produce incoherent justifications, or miss the implied rhetorical function altogether. These findings highlight a deeper representational gap in LLMs' pragmatic understanding and challenge the assumption that statistical fluency implies cognitive comprehension. We release our dataset and code to facilitate further research in modelling linguistic depth beyond surface-level coherence."
    },
    {
      "title": "LMEnt：一个用于分析语言模型中知识的套件，涵盖从预训练数据到表示形式 (原标题: LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations)",
      "link": "https://arxiv.org/abs/2509.03405",
      "pubDate": "Wed, 03 Sep 2025 11:31:18 GMT",
      "isoDate": "2025-09-03T11:31:18.000Z",
      "creator": "Daniela Gottesman, Alon Gilae-Dotan, Ido Cohen, Yoav Gur-Arieh, Marius Mosbach, Ori Yoran, Mor Geva",
      "summary": "## LMEnt：分析语言模型中知识的综合套件\n\n### 引言\n\n语言模型（LMs）在现实世界应用中日益依赖世界知识，但其将数据转化为关于世界知识和信念表示的内部过程仍知之甚少。深入了解这些过程对于开发具有更一致、鲁棒和完整知识表示的语言模型至关重要。\n\n### LMEnt 介绍\n\n为促进对这些问题的研究，本文提出了 **LMEnt**，一个用于分析语言模型在预训练期间知识获取的综合套件。LMEnt 引入了以下核心组成部分：\n\n1.  **知识丰富的预训练语料库：**\n    *   基于维基百科构建。\n    *   完全标注了实体提及。\n2.  **基于实体的预训练数据检索方法：**\n    *   该方法在性能上显著优于现有方法，提升高达 80.4%。\n3.  **预训练模型集合：**\n    *   包含 12 个预训练模型，参数量高达 10 亿（1B）。\n    *   提供 4000 个中间检查点。\n    *   在知识基准测试上的表现与流行的开源模型相当。\n\n### 资源用途\n\n这些资源共同提供了一个受控环境，用于：\n\n*   分析预训练中实体提及与下游性能之间的联系。\n*   研究预训练数据中因果干预的影响。\n\n### LMEnt 的实用性展示\n\n通过研究跨检查点的知识获取过程，LMEnt 展示了其实用性。研究发现：\n\n*   事实频率是知识获取的关键因素。\n*   但事实频率并不能完全解释学习趋势。\n\n### 发布目的\n\nLMEnt 已发布，旨在支持对语言模型中知识的深入研究，包括但不限于：\n\n*   知识表示\n*   可塑性\n*   编辑\n*   归因\n*   学习动态",
      "shortSummary": "LMEnt是一个用于分析语言模型预训练期间知识获取的综合套件。它包含一个基于维基百科、标注了实体的知识丰富预训练语料库、一个性能优越的实体检索方法，以及12个带有中间检查点的预训练模型。这些资源提供了一个受控环境，用于研究预训练数据与下游性能之间的关系，并分析知识获取动态。LMEnt的发布旨在支持对语言模型知识表示、可塑性、编辑和学习动态等方面的研究。",
      "translated_title": "LMEnt：一个用于分析语言模型中知识的套件，涵盖从预训练数据到表示形式",
      "images": [],
      "contentSource": "完整文章",
      "content": "Language models (LMs) increasingly drive real-world applications that require world knowledge. However, the internal processes through which models turn data into representations of knowledge and beliefs about the world, are poorly understood. Insights into these processes could pave the way for developing LMs with knowledge representations that are more consistent, robust, and complete. To facilitate studying these questions, we present LMEnt, a suite for analyzing knowledge acquisition in LMs during pretraining. LMEnt introduces: (1) a knowledge-rich pretraining corpus, fully annotated with entity mentions, based on Wikipedia, (2) an entity-based retrieval method over pretraining data that outperforms previous approaches by as much as 80.4%, and (3) 12 pretrained models with up to 1B parameters and 4K intermediate checkpoints, with comparable performance to popular open-sourced models on knowledge benchmarks. Together, these resources provide a controlled environment for analyzing connections between entity mentions in pretraining and downstream performance, and the effects of causal interventions in pretraining data. We show the utility of LMEnt by studying knowledge acquisition across checkpoints, finding that fact frequency is key, but does not fully explain learning trends. We release LMEnt to support studies of knowledge in LMs, including knowledge representations, plasticity, editing, attribution, and learning dynamics."
    },
    {
      "title": "Loong：通过验证器大规模合成长链式思考 (原标题: Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers)",
      "link": "https://arxiv.org/abs/2509.03059",
      "pubDate": "Wed, 03 Sep 2025 02:42:40 GMT",
      "isoDate": "2025-09-03T02:42:40.000Z",
      "creator": "Xingyue Huang, Rishabh, Gregor Franke, Ziyi Yang, Jiamu Bai, Weijie Bai, Jinhe Bi, Zifeng Ding, Yiqun Duan, Chengyu Fan, Wendong Fan, Xin Gao, Ruohao Guo, Yuan He, Zhuangzhuang He, Xianglong Hu, Neil Johnson, Bowen Li, Fangru Lin, Siyu Lin, Tong Liu, Yunpu Ma, Hao Shen, Hao Sun, Beibei Wang, Fangyijie Wang, Hao Wang, Haoran Wang, Yang Wang, Yifeng Wang, Zhaowei Wang, Ziyang Wang, Yifan Wu, Zikai Xiao, Chengxing Xie, Fan Yang, Junxiao Yang, Qianshuo Ye, Ziyu Ye, Guangtao Zeng, Yuwen Ebony Zhang, Zeyu Zhang, Zihao Zhu, Bernard Ghanem, Philip Torr, Guohao Li",
      "summary": "# Loong项目：通过验证器大规模合成长链式思考\n\n## 引言\n\n大型语言模型（LLMs）在数学和编程等领域，通过可验证奖励强化学习（RLVR）显著提升了推理能力，因为这些领域可以自动评估其正确性。然而，将这种成功扩展到其他推理密集型领域面临两大挑战：\n\n*   高质量、可验证数据集的稀缺。\n*   人工监督成本高昂。\n\n## Loong项目概述\n\n为解决上述挑战，本文介绍了 **Loong项目**：一个开源框架，旨在实现跨多样化推理密集型领域的可扩展合成数据生成和验证。该框架的核心目标是克服现有数据集限制和高昂的人工成本，从而促进LLM在更广泛推理任务中的发展。\n\n## 核心组件\n\nLoong项目由两个关键组件构成：\n\n### 1. LoongBench\n\n*   **精选种子数据集**：包含8,729个人工验证的示例。\n*   **领域覆盖**：涵盖12个不同的推理密集型领域，例如高等数学、化学和逻辑。\n*   **数据结构**：每个示例都配有可执行代码和丰富的元数据，确保了其可验证性。\n\n### 2. LoongEnv\n\n*   **模块化合成数据生成环境**：支持多种提示策略。\n*   **数据产出**：能够生成新的问题-答案-代码三元组，从而大规模扩展数据集。\n\n## 工作机制：智能体-环境循环\n\nLoongBench和LoongEnv协同工作，形成一个智能体-环境循环，该循环支持强化学习过程：\n\n*   **LLM智能体**：基于LLM的智能体负责生成思维链（CoT）解决方案。\n*   **奖励机制**：当生成的CoT解决方案与代码执行的答案一致时，智能体将获得奖励。这种机制确保了模型学习生成可验证的推理步骤。\n\n## 实证评估\n\n研究人员对Loong项目进行了全面的实证评估：\n\n*   **LoongBench基准测试**：在广泛的开源和专有LLM上对LoongBench进行了基准测试，旨在评估其领域覆盖范围并揭示现有LLM的性能瓶颈。\n*   **LoongEnv合成数据分析**：对LoongEnv生成的合成数据进行了深入分析，重点考察了数据的正确性、难度和多样性，以确保生成数据的质量和实用性。\n\n## 资源可用性\n\nLoong项目的代码和文档已开源，可在指定URL获取，鼓励社区参与和进一步研究。",
      "shortSummary": "Loong项目是一个开源框架，旨在通过可扩展的合成数据生成和验证，解决大型语言模型（LLMs）在推理密集型领域面临的高质量可验证数据集稀缺问题。它包含LoongBench（一个包含8,729个人工验证示例的种子数据集，涵盖12个领域）和LoongEnv（一个模块化的合成数据生成环境）。这两个组件形成一个智能体-环境循环，使LLM能够通过生成与代码执行答案一致的思维链解决方案来学习和改进。该项目对LoongBench进行了基准测试，并分析了LoongEnv生成的合成数据。",
      "translated_title": "Loong：通过验证器大规模合成长链式思考",
      "images": [
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png",
          "alt": "BibSonomy logo",
          "title": "",
          "position": 1
        },
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png",
          "alt": "Reddit logo",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "Recent advances in Large Language Models (LLMs) have shown that their reasoning capabilities can be significantly improved through Reinforcement Learning with Verifiable Reward (RLVR), particularly in domains like mathematics and programming, where ground-truth correctness can be automatically evaluated. However, extending this success to other reasoning-intensive domains remains challenging due to the scarcity of high-quality, verifiable datasets and the high cost of human supervision. In this work, we introduce the Loong Project: an open-source framework for scalable synthetic data generation and verification across a diverse range of reasoning-intensive domains. The framework consists of two key components: (1) LoongBench, a curated seed dataset containing 8,729 human-vetted examples across 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired with executable code and rich metadata; and (2) LoongEnv, a modular synthetic data generation environment that supports multiple prompting strategies to produce new question-answer-code triples. Together, these components form an agent-environment loop that enables reinforcement learning, where an LLM-based agent is rewarded for generating Chain-of-Thought (CoT) solutions that align with code-executed answers. Empirically, we benchmark LoongBench on a broad suite of both open-source and proprietary LLMs to evaluate domain coverage and reveal performance bottlenecks. In addition, we conduct a comprehensive analysis of synthetic data generated by LoongEnv, examining correctness, difficulty, and diversity. Code and documentation are available at https://github.com/camel-ai/loong."
    },
    {
      "title": "面向大型语言模型的智能体强化学习全景：一项综述 (原标题: The Landscape of Agentic Reinforcement Learning for LLMs: A Survey)",
      "link": "https://arxiv.org/abs/2509.02547",
      "pubDate": "Tue, 02 Sep 2025 13:46:26 GMT",
      "isoDate": "2025-09-02T13:46:26.000Z",
      "creator": "Guibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhongzhi Li, Xiangyuan Xue, Yijiang Li, Yifan Zhou, Yang Chen, Chen Zhang, Yutao Fan, Zihu Wang, Songtao Huang, Yue Liao, Hongru Wang, Mengyue Yang, Heng Ji, Michael Littman, Jun Wang, Shuicheng Yan, Philip Torr, Lei Bai",
      "summary": "## 面向大型语言模型的智能体强化学习全景：一项综述\n\n### 引言\n\n本综述深入探讨了智能体强化学习（Agentic RL）这一新兴范式，它标志着大型语言模型（LLMs）应用领域的一项重大转变。传统上，LLMs被视为被动的序列生成器，而Agentic RL则将它们重塑为嵌入复杂动态世界中的自主决策智能体。这项工作旨在通过形式化概念、提出分类体系并整合现有资源，全面描绘这一快速发展的领域。\n\n### 核心概念与范式转变\n\n*   **从传统LLM RL到智能体RL的转变**：\n    *   **传统LLM RL**：通常将LLMs视为在退化的单步马尔可夫决策过程（MDPs）中运行，其主要任务是生成序列。LLMs在此情境下扮演的角色相对被动。\n    *   **智能体RL**：将LLMs置于时间上扩展、部分可观察的马尔可夫决策过程（POMDPs）中。这意味着LLMs需要进行多步决策，处理不确定性，并与环境进行持续交互，从而展现出更强的自主性和决策能力。\n*   **形式化对比**：综述通过对比这两种MDPs，明确了智能体RL在概念上的进步，强调了其在处理复杂、动态环境中的优势。\n\n### 综合分类体系\n\n本综述提出了一个全面的双重分类法，以组织和理解智能体强化学习的复杂性：\n\n1.  **围绕核心智能体能力**：\n    *   **规划（Planning）**：智能体制定多步行动序列以实现目标的能力。\n    *   **工具使用（Tool Use）**：智能体调用外部工具或API以扩展其功能和解决问题的能力。\n    *   **记忆（Memory）**：智能体存储、检索和利用过去经验以指导当前决策的能力。\n    *   **推理（Reasoning）**：智能体进行逻辑推断、问题解决和理解复杂情境的能力。\n    *   **自我改进（Self-improvement）**：智能体从经验中学习并优化自身行为策略的能力。\n    *   **感知（Perception）**：智能体从环境中获取和解释信息的能力。\n\n2.  **围绕多样化的任务领域**：\n    *   综述将智能体强化学习的应用划分为多个任务领域，展示了其在不同场景下的广泛潜力，例如游戏、机器人控制、科学发现、软件开发等。\n\n### 强化学习的关键作用\n\n本综述的核心论点是，强化学习（RL）是实现智能体行为转化的关键机制。它能够将上述核心能力从静态的、启发式的模块，转化为自适应的、鲁棒的智能体行为。通过RL，LLMs能够通过与环境的交互学习最优策略，从而在动态和不确定的环境中表现出更强的适应性和泛化能力。\n\n### 支持未来研究的资源整合\n\n为了支持和加速未来的研究，本综述整合了当前智能体强化学习领域的开放资源：\n\n*   **开源环境（Open-source Environments）**：提供模拟或真实世界环境，供智能体进行训练和评估。\n*   **基准（Benchmarks）**：提供标准化的任务和评估指标，用于比较不同智能体方法的性能。\n*   **框架（Frameworks）**：提供开发和实现智能体强化学习算法的工具和库。\n\n### 总结与展望\n\n通过综合分析超过五百篇近期研究工作，本综述描绘了智能体强化学习这一快速演进领域的轮廓。它不仅突出了当前面临的机遇，也指出了在开发可扩展、通用型AI智能体过程中将遇到的挑战。本综述旨在为研究人员提供一个全面的视角，以指导未来在这一前沿领域的研究和发展。",
      "shortSummary": "该综述深入探讨了面向大型语言模型（LLMs）的智能体强化学习（Agentic RL），将其定义为LLMs从被动生成器向复杂动态世界中自主决策智能体的范式转变。文章通过对比传统LLM-RL的单步MDPs与Agentic RL的扩展POMDPs，提出了一个围绕规划、工具使用、记忆等核心能力及其应用领域的双重分类法。综述强调强化学习是实现自适应智能体行为的关键，并整合了开源资源以加速未来研究，旨在推动可扩展、通用AI智能体的发展。",
      "translated_title": "面向大型语言模型的智能体强化学习全景：一项综述",
      "images": [],
      "contentSource": "完整文章",
      "content": "The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language models (LLM RL), reframing LLMs from passive sequence generators into autonomous, decision-making agents embedded in complex, dynamic worlds. This survey formalizes this conceptual shift by contrasting the degenerate single-step Markov Decision Processes (MDPs) of LLM-RL with the temporally extended, partially observable Markov decision processes (POMDPs) that define Agentic RL. Building on this foundation, we propose a comprehensive twofold taxonomy: one organized around core agentic capabilities, including planning, tool use, memory, reasoning, self-improvement, and perception, and the other around their applications across diverse task domains. Central to our thesis is that reinforcement learning serves as the critical mechanism for transforming these capabilities from static, heuristic modules into adaptive, robust agentic behavior. To support and accelerate future research, we consolidate the landscape of open-source environments, benchmarks, and frameworks into a practical compendium. By synthesizing over five hundred recent works, this survey charts the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose AI agents."
    },
    {
      "title": "UI-TARS-2 技术报告：通过多轮强化学习推进 GUI 智能体 (原标题: UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2509.02544",
      "pubDate": "Tue, 02 Sep 2025 13:44:45 GMT",
      "isoDate": "2025-09-02T13:44:45.000Z",
      "creator": "Haoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu, Qinyu Luo, Shihao Liang, Shijue Huang, Wanjun Zhong, Yining Ye, Yujia Qin, Yuwen Xiong, Yuxin Song, Zhiyong Wu, Bo Li, Chen Dun, Chong Liu, Fuxing Leng, Hanbin Wang, Hao Yu, Haobin Chen, Hongyi Guo, Jing Su, Jingjia Huang, Kai Shen, Kaiyu Shi, Lin Yan, Peiyao Zhao, Pengfei Liu, Qinghao Ye, Renjie Zheng, Wayne Xin Zhao, Wen Heng, Wenhao Huang, Wenqian Wang, Xiaobo Qin, Yi Lin, Youbin Wu, Zehui Chen, Zihao Wang, Baoquan Zhong, Xinchun Zhang, Xujing Li, Yuanfan Li, Zhongkai Zhao, Chengquan Jiang, Faming Wu, Haotian Zhou, Jinlin Pang, Li Han, Qianli Ma, Siyao Liu, Songhua Cai, Wenqi Fu, Xin Liu, Zhi Zhang, Bo Zhou, Guoliang Li, Jiajun Shi, Jiale Yang, Jie Tang, Li Li, Taoran Lu, Woyu Lin, Xiaokang Tong, Xinyao Li, Yichi Zhang, Yu Miao, Zhengxuan Jiang, Zili Li, Ziyuan Zhao, Chenxin Li, Dehua Ma, Feng Lin, Ge Zhang, Haihua Yang, Hangyu Guo, Hongda Zhu, Jiaheng Liu, Junda Du, Kai Cai, Kuanye Li, Lichen Yuan, Meilan Han, Minchao Wang, Shuyue Guo, Tianhao Cheng, Xiaobo Ma, Xiaojun Xiao, Xiaolong Huang, Xinjie Chen, Yidi Du, Yilin Chen, Yiwen Wang, Zhaojian Li, Zhenzhu Yang, Zhiyuan Zeng, Chaolin Jin, Chen Li, Hao Chen, Haoli Chen, Jian Chen, Qinghao Zhao, Guang Shi",
      "summary": "# UI-TARS-2 技术报告：通过多轮强化学习推进 GUI 智能体\n\n## 摘要\n\n本技术报告介绍了 UI-TARS-2，一个以图形用户界面（GUI）为中心的本地智能体模型。该模型旨在解决当前GUI智能体开发面临的主要挑战，包括数据可扩展性、多轮强化学习（RL）的复杂性、仅GUI操作的局限性以及环境稳定性问题。UI-TARS-2通过一套系统化的训练方法论，在多个方面取得了显著进展。\n\n## 核心方法论\n\nUI-TARS-2采用以下关键组件和策略来克服现有挑战：\n\n*   **数据飞轮（Data Flywheel）**：用于实现可扩展的数据生成，确保模型有充足且多样化的训练数据。\n*   **稳定多轮强化学习框架**：设计了一个稳定的框架，以有效处理多轮交互中的学习和决策过程，解决了传统多轮RL的稳定性问题。\n*   **混合GUI环境**：集成了文件系统和终端，构建了一个超越纯GUI操作的混合环境，使智能体能够处理更广泛的任务类型。\n*   **统一沙盒平台**：提供了一个统一的沙盒平台，支持大规模的智能体部署和测试，便于进行广泛的实验和评估。\n\n## 实验结果与性能\n\n经验评估表明，UI-TARS-2相较于其前身UI-TARS-1.5取得了显著的性能提升，并在多个基准测试中表现出色：\n\n### GUI基准测试\n\nUI-TARS-2在多个GUI基准测试中取得了优异成绩，超越了包括Claude和OpenAI智能体在内的强大基线模型：\n\n*   **Online-Mind2Web**：88.2\n*   **OSWorld**：47.5\n*   **WindowsAgentArena**：50.6\n*   **AndroidWorld**：73.3\n\n### 游戏环境\n\n在游戏环境中，UI-TARS-2也展现了强大的能力：\n\n*   在包含15款游戏的套件中，平均归一化得分达到59.8，约为人机水平的60%。\n*   在LMGame-Bench上，与OpenAI o3等前沿专有模型保持竞争力。\n\n### 泛化能力\n\n该模型还表现出强大的泛化能力，能够适应多样化的智能体任务：\n\n*   成功泛化到长周期信息检索任务。\n*   在软件工程基准测试中表现出色，突显了其在不同交互场景中的鲁棒性。\n\n## 深入分析与结论\n\n对训练动态的详细分析为在大规模智能体强化学习中实现稳定性和效率提供了宝贵见解。这些结果强调了UI-TARS-2在推进GUI智能体领域方面的巨大潜力，并展示了其在真实世界交互场景中的强大泛化能力。",
      "shortSummary": "UI-TARS-2是一个以GUI为中心的本地智能体模型，旨在解决GUI智能体开发中的数据可扩展性、多轮强化学习、GUI操作局限性及环境稳定性等挑战。它通过数据飞轮、稳定多轮RL框架、混合GUI环境和统一沙盒平台实现。实验表明，UI-TARS-2在GUI基准测试（如Online-Mind2Web 88.2）和游戏环境中均显著优于现有模型和前身，并展现出强大的泛化能力，有望推动GUI智能体在真实世界场景中的应用。",
      "translated_title": "UI-TARS-2 技术报告：通过多轮强化学习推进 GUI 智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "The development of autonomous agents for graphical user interfaces (GUIs) presents major challenges in artificial intelligence. While recent advances in native agent models have shown promise by unifying perception, reasoning, action, and memory through end-to-end learning, open problems remain in data scalability, multi-turn reinforcement learning (RL), the limitations of GUI-only operation, and environment stability. In this technical report, we present UI-TARS-2, a native GUI-centered agent model that addresses these challenges through a systematic training methodology: a data flywheel for scalable data generation, a stabilized multi-turn RL framework, a hybrid GUI environment that integrates file systems and terminals, and a unified sandbox platform for large-scale rollouts. Empirical evaluation demonstrates that UI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5. On GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on WindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines such as Claude and OpenAI agents. In game environments, it attains a mean normalized score of 59.8 across a 15-game suite-roughly 60% of human-level performance-and remains competitive with frontier proprietary models (e.g., OpenAI o3) on LMGame-Bench. Additionally, the model can generalize to long-horizon information-seeking tasks and software engineering benchmarks, highlighting its robustness across diverse agent tasks. Detailed analyses of training dynamics further provide insights into achieving stability and efficiency in large-scale agent RL. These results underscore UI-TARS-2's potential to advance the state of GUI agents and exhibit strong generalization to real-world interactive scenarios."
    },
    {
      "title": "在语言模型生成中联合增强多样性和质量 (原标题: Jointly Reinforcing Diversity and Quality in Language Model Generations)",
      "link": "https://arxiv.org/abs/2509.02534",
      "pubDate": "Tue, 02 Sep 2025 13:38:47 GMT",
      "isoDate": "2025-09-02T13:38:47.000Z",
      "creator": "Tianjian Li, Yiming Zhang, Ping Yu, Swarnadeep Saha, Daniel Khashabi, Jason Weston, Jack Lanchantin, Tianlu Wang",
      "summary": "## DARLING：联合增强语言模型生成的多样性和质量\n\n### 挑战：后训练语言模型中的多样性与质量权衡\n\n大型语言模型（LMs）的后训练过程通常优先考虑准确性和实用性，但这往往以牺牲生成内容的多样性为代价。这种做法虽然提高了响应质量，但也使得输出分布更加尖锐，减少了思想的范围，从而限制了LMs在头脑风暴、故事创作或问题解决等创意和探索性任务中的效用。\n\n### 解决方案：多样性感知强化学习（DARLING）框架\n\n为了解决这一挑战，研究人员提出了**多样性感知强化学习（DARLING）**框架。DARLING旨在联合优化响应质量和语义多样性，以生成既高质量又独特的输出。\n\n### DARLING 的核心机制\n\nDARLING的核心在于引入了一个**学习到的分区函数**，用于衡量超越表面词汇变化之外的语义多样性。这个多样性信号随后与质量奖励结合，在在线强化学习过程中共同优化模型，鼓励其生成高质量且独特的输出。\n\n### 实验与结果\n\nDARLING在多种模型家族和规模上进行了实验，并被证明适用于两种不同的任务类型：\n\n1.  **非可验证任务**：包括指令遵循和创意写作。\n    *   在五个基准测试中，DARLING持续优于仅优化质量的RL基线模型，生成的输出同时具有更高的质量和新颖性。\n\n2.  **可验证任务**：例如竞赛数学。\n    *   DARLING在这些任务中实现了更高的pass@1（解决方案质量）和pass@k（解决方案多样性）。\n\n### 关键发现：多样性促进探索与质量提升\n\n最引人注目的是，实验发现明确地优化多样性能够**促进在线强化学习中的探索**。这种探索的增加反过来表现为更高质量的响应，这表明多样性和质量之间存在协同效应，而非简单的权衡。",
      "shortSummary": "DARLING框架旨在解决大型语言模型后训练中质量与多样性之间的矛盾。该框架通过引入学习到的分区函数来衡量语义多样性，并将其与质量奖励结合，在在线强化学习中共同优化生成内容的质量和独特性。实验表明，DARLING在非可验证（如创意写作）和可验证（如数学竞赛）任务中均优于仅优化质量的基线模型，不仅提高了输出质量，还显著增强了多样性，甚至通过促进探索进一步提升了响应质量。",
      "translated_title": "在语言模型生成中联合增强多样性和质量",
      "images": [],
      "contentSource": "完整文章",
      "content": "Post-training of Large Language Models (LMs) often prioritizes accuracy and helpfulness at the expense of diversity. This creates a tension: while post-training improves response quality, it also sharpens output distributions and reduces the range of ideas, limiting the usefulness of LMs in creative and exploratory tasks such as brainstorming, storytelling, or problem solving. We address this challenge with Diversity-Aware Reinforcement Learning (DARLING), a framework that jointly optimizes for response quality and semantic diversity. At its core, DARLING introduces a learned partition function to measure diversity beyond surface-level lexical variations. This diversity signal is then combined with a quality reward during online reinforcement learning, encouraging models to generate outputs that are both high-quality and distinct. Experiments across multiple model families and sizes show that DARLING generalizes to two regimes: non-verifiable tasks (instruction following and creative writing) and verifiable tasks (competition math). On five benchmarks in the first setting, DARLING consistently outperforms quality-only RL baselines, producing outputs that are simultaneously of higher quality and novelty. In the second setting, DARLING achieves higher pass@1 (solution quality) and pass@k (solution variety). Most strikingly, explicitly optimizing for diversity catalyzes exploration in online RL, which manifests itself as higher-quality responses."
    },
    {
      "title": "通过监督学习框架实现RLVR的隐式Actor-Critic耦合 (原标题: Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR)",
      "link": "https://arxiv.org/abs/2509.02522",
      "pubDate": "Tue, 02 Sep 2025 13:22:46 GMT",
      "isoDate": "2025-09-02T13:22:46.000Z",
      "creator": "Jiaming Li, Longze Chen, Ze Gong, Yukun Chen, Lu Wang, Wanwei He, Run Luo, Min Yang",
      "summary": "## PACS：通过监督学习框架实现RLVR的隐式Actor-Critic耦合\n\n### 引言\n\n近年来，**可验证奖励强化学习 (RLVR)** 的进步显著提升了大型语言模型 (LLMs) 解决数学和编程等复杂推理任务的能力。RLVR通过利用可验证的结果奖励来指导策略优化，使LLMs能够以可靠且有依据的方式逐步提高输出质量。然而，RLVR范式面临重大挑战，现有方法，特别是基于强化学习 (RL) 的方法，常受限于稀疏的奖励信号和不稳定的策略梯度更新。\n\n### 提出的方法：PACS框架\n\n为了解决上述挑战，本文提出了一个名为 **PACS** 的新型RLVR框架。PACS代表了通过**S**upervised学习框架实现**P**licit **A**ctor **C**ritic耦合。其核心思想是将结果奖励视为可预测的标签，从而将RLVR问题重新构建为一个基于分数函数的监督学习任务。这个分数函数由策略模型参数化，并使用交叉熵损失进行优化。\n\n#### 机制优势\n\n详细的梯度分析表明，这种监督学习的公式固有地恢复了经典的策略梯度更新，同时实现了Actor和Critic角色的隐式耦合。这种耦合机制带来了更稳定和高效的训练过程。\n\n### 实验结果\n\n研究团队在具有挑战性的数学推理任务上对PACS进行了基准测试。结果显示，PACS的推理性能优于PPO和GRPO等强大的RLVR基线方法。例如：\n\n*   在AIME 2025的pass@256指标上，PACS达到了 **59.78%** 的成绩。\n*   这比PPO和GRPO分别提高了 **13.32** 和 **14.36** 个百分点。\n\n### 结论与展望\n\nPACS框架以其简洁而强大的特性，为LLMs通过可验证奖励进行后训练提供了一个有前景的途径。该研究的代码和数据已作为开源资源提供。",
      "shortSummary": "针对RLVR中稀疏奖励和策略梯度不稳定的问题，本文提出了PACS框架。PACS通过将RLVR重构为监督学习任务，并使用交叉熵损失优化分数函数，实现了Actor和Critic的隐式耦合。这种方法使得训练更稳定高效，并在数学推理任务上表现出色。例如，在AIME 2025上，PACS的pass@256达到59.78%，显著优于PPO和GRPO，为LLMs的后训练提供了更稳定高效的解决方案。",
      "translated_title": "通过监督学习框架实现RLVR的隐式Actor-Critic耦合",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have empowered large language models (LLMs) to tackle challenging reasoning tasks such as mathematics and programming. RLVR leverages verifiable outcome rewards to guide policy optimization, enabling LLMs to progressively improve output quality in a grounded and reliable manner. Despite its promise, the RLVR paradigm poses significant challenges, as existing methods often suffer from sparse reward signals and unstable policy gradient updates, particularly in RL-based approaches. To address the challenges, we propose PACS, a novel RLVR framework that achieves imPlicit Actor Critic coupling via a Supervised learning framework. By treating the outcome reward as a predictable label, we reformulate the RLVR problem into a supervised learning task over a score function parameterized by the policy model and optimized using cross-entropy loss. A detailed gradient analysis shows that this supervised formulation inherently recovers the classical policy gradient update while implicitly coupling actor and critic roles, yielding more stable and efficient training. Benchmarking on challenging mathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as PPO and GRPO, achieving superior reasoning performance. For instance, PACS achieves 59.78\\% at pass@256 on AIME 2025, representing improvements of 13.32 and 14.36 points over PPO and GRPO. This simple yet powerful framework offers a promising avenue for LLMs post-training with verifiable rewards. Our code and data are available as open source at https://github.com/ritzz-ai/PACS."
    },
    {
      "title": "SimpleTIR：用于多轮工具集成推理的端到端强化学习 (原标题: SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning)",
      "link": "https://arxiv.org/abs/2509.02479",
      "pubDate": "Tue, 02 Sep 2025 12:30:19 GMT",
      "isoDate": "2025-09-02T12:30:19.000Z",
      "creator": "Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Xiaosen Zheng, Zejun Ma, Bo An",
      "summary": "# SimpleTIR：用于多轮工具集成推理的端到端强化学习\n\n## 摘要\n\n大型语言模型（LLMs）通过与外部工具交互，可以显著提升其推理能力，这种范式被称为**工具集成推理（TIR）**。然而，将TIR扩展到多轮场景并结合强化学习（RL）时，经常会遇到训练不稳定和性能崩溃的问题。\n\n## 问题识别\n\n研究发现，这种不稳定性主要源于外部工具反馈导致的**分布漂移**，进而导致低概率令牌的生成。这个问题在连续的回合中会不断累积，引发灾难性的**梯度范数爆炸**，从而破坏训练过程。\n\n## SimpleTIR 解决方案\n\n为了解决这一挑战，本文引入了 **SimpleTIR**，一个即插即用的算法，旨在稳定多轮TIR的训练。\n\n*   **核心策略**：SimpleTIR 的核心策略是识别并过滤掉包含“空回合”（void turns）的轨迹。所谓“空回合”，是指那些既没有产生代码块也没有给出最终答案的回合。\n*   **工作原理**：通过从策略更新中移除这些有问题的轨迹，SimpleTIR 有效地阻止了有害的、高幅度的梯度，从而稳定了学习动态。\n\n## 实验结果与性能\n\n广泛的实验表明，SimpleTIR 在具有挑战性的数学推理基准测试中取得了最先进的性能：\n\n*   **AIME24 分数提升**：在使用 Qwen2.5-7B 基础模型时，SimpleTIR 将 AIME24 分数从纯文本基线的 22.1 显著提升至 50.5。\n*   **发现多样化推理模式**：通过避免监督微调的限制，SimpleTIR 鼓励模型发现多样化和复杂的推理模式，例如自我纠正和交叉验证。\n\n## 结论\n\nSimpleTIR 提供了一种有效的方法来解决多轮工具集成推理中强化学习训练不稳定的问题，显著提升了LLMs在复杂推理任务上的表现。",
      "shortSummary": "SimpleTIR提出了一种解决多轮工具集成推理（TIR）中强化学习训练不稳定性的方法。该不稳定性源于分布漂移和梯度爆炸。SimpleTIR通过识别并过滤掉无效回合（未生成代码或最终答案的回合），有效阻止了有害梯度，从而稳定了训练过程。实验证明，SimpleTIR在数学推理基准测试中达到了最先进的性能，例如将AIME24分数从22.1提升至50.5，并鼓励模型发现自我纠正等多样化推理模式。",
      "translated_title": "SimpleTIR：用于多轮工具集成推理的端到端强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) can significantly improve their reasoning capabilities by interacting with external tools, a paradigm known as Tool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios using Reinforcement Learning (RL) is often hindered by training instability and performance collapse. We identify that such instability is primarily caused by a distributional drift from external tool feedback, leading to the generation of low-probability tokens. This issue compounds over successive turns, causing catastrophic gradient norm explosions that derail the training process. To address this challenge, we introduce SimpleTIR , a plug-and-play algorithm that stabilizes multi-turn TIR training. Its core strategy is to identify and filter out trajectories containing void turns, i.e., turns that yield neither a code block nor a final answer. By removing these problematic trajectories from the policy update, SimpleTIR effectively blocks the harmful, high-magnitude gradients, thus stabilizing the learning dynamics. Extensive experiments show that SimpleTIR achieves state-of-the-art performance on challenging math reasoning benchmarks, notably elevating the AIME24 score from a text-only baseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model. Furthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR encourages the model to discover diverse and sophisticated reasoning patterns, such as self-correction and cross-validation."
    },
    {
      "title": "GenCompositor：基于扩散Transformer的生成式视频合成 (原标题: GenCompositor: Generative Video Compositing with Diffusion Transformer)",
      "link": "https://arxiv.org/abs/2509.02460",
      "pubDate": "Tue, 02 Sep 2025 12:10:13 GMT",
      "isoDate": "2025-09-02T12:10:13.000Z",
      "creator": "Shuzhou Yang, Xiaoyu Li, Xiaodong Cun, Guangzhi Wang, Lingen Li, Ying Shan, Jian Zhang",
      "summary": "# GenCompositor：基于扩散Transformer的生成式视频合成\n\n## 引言\n传统的视频合成流程面临诸多挑战，包括：\n*   **劳动密集型：** 需要大量人工投入。\n*   **专家协作：** 依赖多领域专家的紧密合作。\n*   **生产周期长：** 导致漫长的制作周期。\n*   **人力成本高：** 显著增加制作成本。\n\n为了解决这些问题，本文提出了一种名为“生成式视频合成”的新任务，旨在利用生成模型自动化这一过程。\n\n## 任务目标\n生成式视频合成旨在以交互方式，自适应地将前景视频的身份和运动信息注入到目标视频中。这允许用户高度定制最终视频中添加的动态元素，例如：\n*   调整其大小。\n*   设定其运动轨迹。\n*   修改其他相关属性。\n\n## 方法概述：基于扩散Transformer (DiT) 的GenCompositor\n研究团队设计了一个新颖的扩散Transformer (DiT) 管道，利用其内在特性来实现生成式视频合成。该方法包含以下关键组件：\n\n### 1. 轻量级DiT背景保持分支\n*   **目的：** 确保编辑前后目标视频的一致性。\n*   **实现：** 通过修订的轻量级DiT分支，结合掩码令牌注入（masked token injection）技术。\n\n### 2. DiT前景融合块\n*   **目的：** 从其他来源继承动态元素并进行融合。\n*   **实现：** 采用基于全自注意力（full self-attention）的DiT融合块。\n*   **训练辅助：** 配合一种简单而有效的前景增强（foreground augmentation）策略进行训练。\n\n### 3. 扩展旋转位置编码 (ERoPE)\n*   **目的：** 根据用户控制，融合具有不同布局的背景和前景视频。\n*   **实现：** 开发了一种新颖的位置编码方法，命名为扩展旋转位置编码 (ERoPE)。\n\n## 数据集\n为了支持这项新任务，研究团队精心策划并构建了一个名为 **VideoComp** 的数据集。该数据集包含：\n*   61K套视频。\n*   完整的动态元素。\n*   高质量的目标视频。\n\n## 实验结果\n实验证明，GenCompositor 方法能够有效地实现生成式视频合成。与现有解决方案相比，该方法在以下方面表现出显著优势：\n*   **保真度：** 生成视频的真实感更高。\n*   **一致性：** 视频内容在编辑前后保持更好的连贯性。",
      "shortSummary": "GenCompositor提出了一种基于扩散Transformer (DiT) 的生成式视频合成方法，旨在自动化传统视频合成中劳动密集且成本高的问题。该方法允许用户交互式地将前景视频的动态元素注入目标视频，并自定义其属性。GenCompositor包含一个DiT背景保持分支、一个DiT前景融合块（使用全自注意力）以及新颖的扩展旋转位置编码（ERoPE）以支持用户控制。研究团队还构建了VideoComp数据集。实验表明，该方法在保真度和一致性上优于现有方案。",
      "translated_title": "GenCompositor：基于扩散Transformer的生成式视频合成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Video compositing combines live-action footage to create video production, serving as a crucial technique in video creation and film production. Traditional pipelines require intensive labor efforts and expert collaboration, resulting in lengthy production cycles and high manpower costs. To address this issue, we automate this process with generative models, called generative video compositing. This new task strives to adaptively inject identity and motion information of foreground video to the target video in an interactive manner, allowing users to customize the size, motion trajectory, and other attributes of the dynamic elements added in final video. Specifically, we designed a novel Diffusion Transformer (DiT) pipeline based on its intrinsic properties. To maintain consistency of the target video before and after editing, we revised a light-weight DiT-based background preservation branch with masked token injection. As to inherit dynamic elements from other sources, a DiT fusion block is proposed using full self-attention, along with a simple yet effective foreground augmentation for training. Besides, for fusing background and foreground videos with different layouts based on user control, we developed a novel position embedding, named Extended Rotary Position Embedding (ERoPE). Finally, we curated a dataset comprising 61K sets of videos for our new task, called VideoComp. This data includes complete dynamic elements and high-quality target videos. Experiments demonstrate that our method effectively realizes generative video compositing, outperforming existing possible solutions in fidelity and consistency."
    },
    {
      "title": "MedDINOv3：如何将视觉基础模型应用于医学图像分割？ (原标题: MedDINOv3: How to adapt vision foundation models for medical image segmentation?)",
      "link": "https://arxiv.org/abs/2509.02379",
      "pubDate": "Tue, 02 Sep 2025 10:44:43 GMT",
      "isoDate": "2025-09-02T10:44:43.000Z",
      "creator": "Yuheng Li, Yizhou Wu, Yuxiang Lai, Mingzhe Hu, Xiaofeng Yang",
      "summary": "## MedDINOv3：将视觉基础模型应用于医学图像分割\n\n### 引言\n\n医学图像（如CT和MRI扫描）中器官和肿瘤的精确分割对于诊断、治疗规划和疾病监测至关重要。尽管深度学习在自动化分割方面取得了显著进展，但大多数现有模型仍是任务特定的，缺乏跨模态和跨机构的泛化能力。视觉基础模型（FMs）通过在数十亿张自然图像上进行预训练，提供了强大且可迁移的表示能力。然而，将这些模型应用于医学图像领域面临两大关键挑战：\n\n1.  **ViT骨干网络的性能限制：** 大多数基础模型的ViT（Vision Transformer）骨干网络在医学图像分割任务上的表现仍不如专门设计的CNN（卷积神经网络）。\n2.  **巨大的领域鸿沟：** 自然图像与医学图像之间存在显著的领域差异，这限制了预训练模型的可迁移性。\n\n### MedDINOv3 框架\n\n为了解决上述挑战，本文引入了 **MedDINOv3**，一个简单而有效的框架，旨在将DINOv3（一种自监督学习方法）适应于医学图像分割任务。MedDINOv3的核心设计包括以下几个方面：\n\n*   **ViT架构的重新审视与改进：** 研究人员首先重新审视了普通的ViT架构，并设计了一个简单而有效的、具有多尺度令牌聚合能力的架构。这一改进旨在提升ViT在处理医学图像时的特征提取能力。\n*   **领域自适应预训练：** MedDINOv3在 **CT-3M** 数据集上进行领域自适应预训练。CT-3M是一个精心策划的集合，包含387万张轴向CT切片，为模型提供了丰富的医学图像数据以缩小领域差距。\n*   **多阶段DINOv3预训练策略：** 采用多阶段DINOv3配方进行预训练，旨在学习鲁棒的密集特征。这种自监督学习方法有助于模型在没有大量标注数据的情况下，从医学图像中学习到高质量的表示。\n\n### 性能与结果\n\nMedDINOv3在四个不同的分割基准测试中，其性能达到或超越了最先进的水平。这一结果有力地证明了视觉基础模型作为医学图像分割统一骨干网络的巨大潜力。\n\n### 代码可用性\n\nMedDINOv3的相关代码已开源。",
      "shortSummary": "MedDINOv3是一个旨在将视觉基础模型应用于医学图像分割的框架，以解决现有模型泛化性差和领域鸿沟问题。它通过改进ViT架构，引入多尺度令牌聚合，并在包含387万张CT切片的CT-3M数据集上进行领域自适应预训练，采用多阶段DINOv3策略学习鲁棒特征。MedDINOv3在多个分割基准测试中表现出色，达到或超越了现有最佳水平，展示了视觉基础模型作为医学图像分割统一骨干网络的巨大潜力。",
      "translated_title": "MedDINOv3：如何将视觉基础模型应用于医学图像分割？",
      "images": [],
      "contentSource": "完整文章",
      "content": "Accurate segmentation of organs and tumors in CT and MRI scans is essential for diagnosis, treatment planning, and disease monitoring. While deep learning has advanced automated segmentation, most models remain task-specific, lacking generalizability across modalities and institutions. Vision foundation models (FMs) pretrained on billion-scale natural images offer powerful and transferable representations. However, adapting them to medical imaging faces two key challenges: (1) the ViT backbone of most foundation models still underperform specialized CNNs on medical image segmentation, and (2) the large domain gap between natural and medical images limits transferability. We introduce MedDINOv3, a simple and effective framework for adapting DINOv3 to medical segmentation. We first revisit plain ViTs and design a simple and effective architecture with multi-scale token aggregation. Then, we perform domain-adaptive pretraining on CT-3M, a curated collection of 3.87M axial CT slices, using a multi-stage DINOv3 recipe to learn robust dense features. MedDINOv3 matches or exceeds state-of-the-art performance across four segmentation benchmarks, demonstrating the potential of vision foundation models as unified backbones for medical image segmentation. The code is available at https://github.com/ricklisz/MedDINOv3."
    },
    {
      "title": "DCPO：动态裁剪策略优化 (原标题: DCPO: Dynamic Clipping Policy Optimization)",
      "link": "https://arxiv.org/abs/2509.02333",
      "pubDate": "Tue, 02 Sep 2025 10:01:07 GMT",
      "isoDate": "2025-09-02T10:01:07.000Z",
      "creator": "Shihui Yang, Chengfeng Dou, Peidong Guo, Kai Lu, Qiang Ju, Fei Deng, Rihui Xin",
      "summary": "# DCPO：动态裁剪策略优化\n\n## 1. 背景与问题\n强化学习与可验证奖励（RLVR）是提升大型语言模型（LLM）推理能力的一种有前景的框架。然而，现有方法如GRPO常面临零梯度问题。这主要是由于：\n*   **固定的token级概率比裁剪边界**：限制了token级别的探索。\n*   **相同奖励的标准化**：导致梯度更新效率低下，并对生成响应的利用不足。\n\n## 2. 提出的方法：DCPO\n本文提出了**动态裁剪策略优化（DCPO）**，通过引入两种核心技术来解决上述问题：\n\n### 2.1 动态裁剪策略\n*   **目标**：增强token级别的探索。\n*   **机制**：根据token特定的先验概率自适应地调整裁剪边界，而非使用固定边界。\n\n### 2.2 平滑优势标准化技术\n*   **目标**：提高生成响应在响应级别的有效利用。\n*   **机制**：在累积训练步骤中对奖励进行标准化。\n\n## 3. 实验结果与性能\nDCPO在四个基准测试上，基于四种不同的模型，均取得了最先进的性能：\n\n*   **AIME24基准测试 (基于Qwen2.5-Math-7B模型)**：\n    *   DCPO在贪婪解码下Avg@1达到46.7，在32次采样下Avg@32达到38.8。\n    *   显著超越DAPO (36.7/31.6) 和 GRPO (36.7/32.1)。\n\n*   **AIME25基准测试 (基于Qwen2.5-14B模型)**：\n    *   DCPO性能达到(23.3/19.0)。\n    *   超越GRPO (13.3/10.5) 和 DAPO (20.0/15.3)。\n\n*   **其他显著改进**：\n    *   在四种模型中，DCPO的非零优势（nonzero advantage）比GRPO平均提高了28%。\n    *   训练效率比DAPO提高了一倍。\n    *   与GRPO和DAPO相比，token裁剪率显著降低了一个数量级。\n\n## 4. 结论\n这些结果表明，DCPO在大型语言模型的强化学习中，能够更有效地利用生成数据，从而显著提升性能。",
      "shortSummary": "DCPO（动态裁剪策略优化）旨在解决RLVR中现有方法（如GRPO）因固定裁剪边界和奖励标准化导致的零梯度问题。DCPO引入了动态裁剪策略，根据token先验概率自适应调整裁剪边界以增强探索，并采用平滑优势标准化技术提高响应利用率。实验表明，DCPO在四个基准测试上均达到SOTA性能，显著超越DAPO和GRPO，平均非零优势提高28%，训练效率翻倍，并大幅降低了token裁剪率，有效提升了LLM强化学习中生成数据的利用效率。",
      "translated_title": "DCPO：动态裁剪策略优化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a promising framework for enhancing the reasoning capabilities of large language models. However, existing approaches such as GRPO often suffer from zero gradients. This problem arises primarily due to fixed clipping bounds for token-level probability ratios and the standardization of identical rewards, which can lead to ineffective gradient updates and underutilization of generated responses. In this work, we propose Dynamic Clipping Policy Optimization (DCPO), which introduces a dynamic clipping strategy that adaptively adjusts the clipping bounds based on token-specific prior probabilities to enhance token-level exploration, and a smooth advantage standardization technique that standardizes rewards across cumulative training steps to improve the response-level effective utilization of generated responses. DCPO achieved state-of-the-art performance on four benchmarks based on four different models. In particular, DCPO achieved an Avg@1 of 46.7 under greedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24 benchmark, surpassing both DAPO (36.7/31.6) and GRPO (36.7/32.1) on the Qwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO achieves a performance of (23.3/19.0), surpassing GRPO (13.3/10.5) and DAPO (20.0/15.3). Furthermore, DCPO achieved an average 28% improvement in the nonzero advantage over GRPO in four models, doubled the training efficiency over DAPO, and significantly reduced the token clipping ratio by an order of magnitude compared to both GRPO and DAPO, while achieving superior performance. These results highlight DCPO's effectiveness in leveraging generated data more efficiently for reinforcement learning in large language models."
    }
  ],
  "lastUpdated": "2025-09-07T09:23:31.080Z"
}