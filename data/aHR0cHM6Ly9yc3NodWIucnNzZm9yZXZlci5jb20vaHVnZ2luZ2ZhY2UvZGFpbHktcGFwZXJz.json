{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "UItron：具有高级感知和规划能力的基础GUI智能体 (原标题: UItron: Foundational GUI Agent with Advanced Perception and Planning)",
      "link": "https://arxiv.org/abs/2508.21767",
      "pubDate": "Fri, 29 Aug 2025 12:40:57 GMT",
      "isoDate": "2025-08-29T12:40:57.000Z",
      "creator": "Zhixiong Zeng, Jing Huang, Liming Zheng, Wenkang Han, Yufeng Zhong, Lei Chen, Longrong Yang, Yingjie Chu, Yuzhi He, Lin Ma",
      "summary": "UItron：具有高级感知和规划能力的基础GUI智能体\n\n**背景与挑战**\n*   **目标**：GUI智能体旨在实现移动/PC设备的自动化操作，是实现通用人工智能（AGI）的重要任务。\n*   **进展**：视觉语言模型（VLMs）的快速发展，凭借其强大的视觉理解和任务规划能力，加速了GUI智能体的开发。\n*   **现有挑战**：尽管有进展，但构建GUI智能体仍面临多重挑战，包括：\n    *   操作轨迹数据稀缺。\n    *   交互式基础设施的可用性不足。\n    *   基础模型初始能力的局限性。\n\n**UItron介绍**\n*   **核心**：UItron是一个开源的基础模型，专为自动GUI智能体设计，具备先进的GUI感知、接地（grounding）和规划能力。\n*   **理念**：UItron强调系统性数据工程和交互式基础设施是推动GUI智能体发展的基石。\n\n**核心贡献与方法**\n*   **数据工程策略**：系统性地研究了一系列数据工程策略，以有效提升训练效果。\n*   **交互环境**：建立了一个能够连接移动和PC设备的交互式环境，为智能体的训练和评估提供支持。\n*   **训练范式**：\n    *   **监督微调**：首先，在各种GUI场景中对感知和规划任务进行监督微调（supervised finetuning）。\n    *   **课程强化学习**：随后，开发了一个课程强化学习框架，以实现在线环境中的复杂推理和探索能力。\n\n**性能与应用**\n*   **卓越性能**：UItron在GUI感知、接地和规划的基准测试中取得了卓越的性能。\n*   **中文应用场景的突破**：\n    *   **识别痛点**：研究发现，即使是现有最先进的解决方案，也普遍缺乏处理中文应用的能力。\n    *   **数据收集**：为解决这一问题，UItron手动收集了超过一百万步的操作轨迹数据，涵盖了中国最受欢迎的100个移动APP。\n    *   **评估环境**：基于这些数据，构建了离线和在线的智能体评估环境。\n    *   **显著进展**：实验结果表明，UItron在中文APP场景中取得了显著进展，极大地推动了GUI智能体向实际应用迈进。",
      "shortSummary": "UItron是一个开源的基础GUI智能体模型，旨在实现设备自动化操作。它通过强调系统性数据工程和交互基础设施，并结合监督微调与课程强化学习来解决GUI智能体开发中的挑战。UItron在感知、接地和规划方面表现出色，尤其在处理中文移动APP方面取得了显著进展。通过收集大量中文APP操作数据，UItron推动了GUI智能体更接近实际应用，填补了现有解决方案在中文能力上的空白。",
      "translated_title": "UItron：具有高级感知和规划能力的基础GUI智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "GUI agent aims to enable automated operations on Mobile/PC devices, which is an important task toward achieving artificial general intelligence. The rapid advancement of VLMs accelerates the development of GUI agents, owing to their powerful capabilities in visual understanding and task planning. However, building a GUI agent remains a challenging task due to the scarcity of operation trajectories, the availability of interactive infrastructure, and the limitation of initial capabilities in foundation models. In this work, we introduce UItron, an open-source foundational model for automatic GUI agents, featuring advanced GUI perception, grounding, and planning capabilities. UItron highlights the necessity of systemic data engineering and interactive infrastructure as foundational components for advancing GUI agent development. It not only systematically studies a series of data engineering strategies to enhance training effects, but also establishes an interactive environment connecting both Mobile and PC devices. In training, UItron adopts supervised finetuning over perception and planning tasks in various GUI scenarios, and then develop a curriculum reinforcement learning framework to enable complex reasoning and exploration for online environments. As a result, UItron achieves superior performance in benchmarks of GUI perception, grounding, and planning. In particular, UItron highlights the interaction proficiency with top-tier Chinese mobile APPs, as we identified a general lack of Chinese capabilities even in state-of-the-art solutions. To this end, we manually collect over one million steps of operation trajectories across the top 100 most popular apps, and build the offline and online agent evaluation environments. Experimental results demonstrate that UItron achieves significant progress in Chinese app scenarios, propelling GUI agents one step closer to real-world application."
    },
    {
      "title": "Morae：主动暂停UI代理以供用户选择 (原标题: Morae: Proactively Pausing UI Agents for User Choices)",
      "link": "https://arxiv.org/abs/2508.21456",
      "pubDate": "Fri, 29 Aug 2025 05:39:00 GMT",
      "isoDate": "2025-08-29T05:39:00.000Z",
      "creator": "Yi-Hao Peng, Dingzeyu Li, Jeffrey P. Bigham, Amy Pavel",
      "summary": "## Morae：主动暂停UI代理以供用户选择\n\n本文介绍了Morae，一个旨在解决当前用户界面（UI）代理在任务执行过程中用户参与度不足问题的系统。\n\n### 当前UI代理的局限性\n*   **缺乏用户参与**：现有UI代理通常以端到端的方式执行任务，不涉及用户进行关键选择，也不告知用户重要的上下文信息，从而降低了用户的自主权（agency）。\n*   **案例示例**：在一项实地研究中，一位盲人或低视力（BLV）参与者要求购买最便宜的气泡水，代理自动选择了一种价格相同的选项，但没有提及其他口味或评分更好的替代产品，这限制了用户的选择。\n\n### Morae的解决方案\n*   **核心功能**：Morae是一个新型UI代理，它能在任务执行过程中自动识别决策点，并暂停以供用户做出选择。\n*   **技术实现**：Morae利用大型多模态模型来解释用户查询、UI代码和屏幕截图。当存在需要做出选择的情况时，它会主动提示用户进行澄清或选择。\n\n### 研究结果与优势\n*   **提升任务完成度与偏好匹配**：在一项针对真实世界网络任务的BLV参与者研究中，与包括OpenAI Operator在内的基线代理相比，Morae帮助用户完成了更多任务，并使他们能够选择更符合其偏好的选项。\n*   **混合主动性方法**：这项工作体现了一种“混合主动性”（mixed-initiative）方法，即用户既能受益于UI代理的自动化，又能表达自己的偏好和做出决策，从而在自动化和用户控制之间取得平衡。\n\n### 相关信息\n*   该研究已提交至ACM UIST 2025。\n*   主题领域包括：人机交互（cs.HC）、计算与语言（cs.CL）、计算机视觉与模式识别（cs.CV）。",
      "shortSummary": "Morae是一种新型UI代理，旨在解决现有代理在任务执行中用户参与度不足的问题。它利用大型多模态模型，在关键决策点自动暂停，提示用户进行选择。一项针对盲人或低视力用户的研究表明，Morae帮助用户完成了更多任务，并选择了更符合其偏好的选项，显著提升了用户自主权，体现了自动化与用户偏好表达相结合的混合主动性方法。",
      "translated_title": "Morae：主动暂停UI代理以供用户选择",
      "images": [],
      "contentSource": "完整文章",
      "content": "User interface (UI) agents promise to make inaccessible or complex UIs easier to access for blind and low-vision (BLV) users. However, current UI agents typically perform tasks end-to-end without involving users in critical choices or making them aware of important contextual information, thus reducing user agency. For example, in our field study, a BLV participant asked to buy the cheapest available sparkling water, and the agent automatically chose one from several equally priced options, without mentioning alternative products with different flavors or better ratings. To address this problem, we introduce Morae, a UI agent that automatically identifies decision points during task execution and pauses so that users can make choices. Morae uses large multimodal models to interpret user queries alongside UI code and screenshots, and prompt users for clarification when there is a choice to be made. In a study over real-world web tasks with BLV participants, Morae helped users complete more tasks and select options that better matched their preferences, as compared to baseline agents, including OpenAI Operator. More broadly, this work exemplifies a mixed-initiative approach in which users benefit from the automation of UI agents while being able to express their preferences."
    },
    {
      "title": "AHELM：音频语言模型的全面评估 (原标题: AHELM: A Holistic Evaluation of Audio-Language Models)",
      "link": "https://arxiv.org/abs/2508.21376",
      "pubDate": "Fri, 29 Aug 2025 03:40:39 GMT",
      "isoDate": "2025-08-29T03:40:39.000Z",
      "creator": "Tony Lee, Haoqin Tu, Chi Heem Wong, Zijun Wang, Siwei Yang, Yifan Mai, Yuyin Zhou, Cihang Xie, Percy Liang",
      "summary": "# AHELM：音频语言模型的全面评估\n\n## 引言：现有评估的局限性\n当前对音频语言模型（ALM）的评估面临多重挑战。这些多模态模型以交错的音频和文本作为输入并输出文本，但其评估缺乏标准化基准。大多数现有基准仅衡量一到两种能力，并常常忽略公平性或安全性等关键评估维度。此外，由于不同的评估测试有限数量的模型，并采用不同的提示方法和推理参数，使得模型间的比较变得困难。\n\n## AHELM：一个全面的评估基准\n为了解决上述不足，研究人员引入了AHELM，一个旨在全面衡量ALM性能的基准。AHELM聚合了各种数据集，包括两个新颖的合成音频-文本数据集：\n*   **PARADE**：评估ALM避免刻板印象的能力。\n*   **CoRe-Bench**：通过推断性多轮问答，衡量ALM在会话音频上的推理能力。\n\nAHELM从10个关键方面对ALM进行评估，这些方面被认为是ALM开发和使用中至关重要的：\n1.  **音频感知**\n2.  **知识**\n3.  **推理**\n4.  **情感检测**\n5.  **偏见**\n6.  **公平性**\n7.  **多语言性**\n8.  **鲁棒性**\n9.  **毒性**\n10. **安全性**\n\n为了确保模型之间进行公平比较，AHELM还对提示、推理参数和评估指标进行了标准化。\n\n## 实验与主要发现\n研究团队测试了来自3个开发者的14个开源和闭源API ALM，以及3个由自动语音识别器（ASR）和语言模型组成的简单基线系统。实验结果揭示了以下关键发现：\n*   **Gemini 2.5 Pro的表现**：该模型在10个评估方面中的5个方面排名第一。然而，它在ASR任务上表现出群体不公平性（p=0.01），而大多数其他模型则没有这个问题。\n*   **基线系统的表现**：令人惊讶的是，简单的基线系统在AHELM上表现相当不错，其中一个基线系统尽管只具备语音转文本能力，但总体排名第五。\n\n## 透明度与未来展望\n为了提高透明度，所有原始提示、模型生成内容和输出结果都已在其网站上公开。AHELM被设计为一个“活”的基准，未来将持续添加新的数据集和模型，以适应ALM领域的不断发展。",
      "shortSummary": "AHELM基准旨在解决音频语言模型（ALM）评估缺乏标准化和全面性的问题。它聚合了PARADE和CoRe-Bench等数据集，全面评估ALM在音频感知、推理、公平性、安全性等10个关键方面，并标准化评估方法。测试显示，Gemini 2.5 Pro在多数方面领先，但在ASR任务上存在群体不公平性；简单基线系统也表现良好。AHELM将持续更新。",
      "translated_title": "AHELM：音频语言模型的全面评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "Evaluations of audio-language models (ALMs) -- multimodal models that take interleaved audio and text as input and output text -- are hindered by the lack of standardized benchmarks; most benchmarks measure only one or two capabilities and omit evaluative aspects such as fairness or safety. Furthermore, comparison across models is difficult as separate evaluations test a limited number of models and use different prompting methods and inference parameters. To address these shortfalls, we introduce AHELM, a benchmark that aggregates various datasets -- including 2 new synthetic audio-text datasets called PARADE, which evaluates the ALMs on avoiding stereotypes, and CoRe-Bench, which measures reasoning over conversational audio through inferential multi-turn question answering -- to holistically measure the performance of ALMs across 10 aspects we have identified as important to the development and usage of ALMs: audio perception, knowledge, reasoning, emotion detection, bias, fairness, multilinguality, robustness, toxicity, and safety. We also standardize the prompts, inference parameters, and evaluation metrics to ensure equitable comparisons across models. We test 14 open-weight and closed-API ALMs from 3 developers and 3 additional simple baseline systems each consisting of an automatic speech recognizer and a language model. Our results show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits group unfairness (p=0.01) on ASR tasks whereas most of the other models do not. We also find that the baseline systems perform reasonably well on AHELM, with one ranking 5th overall despite having only speech-to-text capabilities. For transparency, all raw prompts, model generations, and outputs are available on our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is intended to be a living benchmark and new datasets and models will be added over time."
    },
    {
      "title": "在游戏中思考：大型语言模型通过强化学习学习游戏中的推理能力 (原标题: Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models)",
      "link": "https://arxiv.org/abs/2508.21365",
      "pubDate": "Fri, 29 Aug 2025 03:13:39 GMT",
      "isoDate": "2025-08-29T03:13:39.000Z",
      "creator": "Yi Liao, Yu Gu, Yuan Sui, Zining Zhu, Yifan Lu, Guohua Tang, Zhongqian Sun, Wei Yang",
      "summary": "## 在游戏中思考：大型语言模型通过强化学习学习游戏中的推理能力\n\n### 核心问题与挑战\n\n*   **大型语言模型（LLMs）的局限性：** 尽管LLMs在数学和编程等复杂推理任务上表现出色，但它们在简单的交互式任务中却常常遇到困难。这揭示了声明性知识（知其然）与程序性知识（知其所以然）之间存在关键鸿沟。\n*   **传统强化学习（RL）的不足：** 传统的RL智能体能够通过环境交互获取程序性知识，但它们通常是“黑箱”操作，缺乏可解释性，并且需要大量的训练数据。\n*   **知识转化难题：** LLMs拥有广泛的世界知识和强大的推理能力，但难以有效地将这些静态知识转化为交互环境中的动态决策。\n\n### 解决方案：Think in Games (TiG) 框架\n\n*   **TiG的提出：** 本文提出了一个名为“Think in Games (TiG)”的新颖框架，旨在赋能LLMs通过与游戏环境的直接交互来发展程序性理解，同时保留其固有的推理和解释能力。\n*   **工作原理：** TiG将基于强化学习的决策制定重新定义为一种语言建模任务。具体而言，LLMs负责生成语言引导的策略，这些策略随后根据环境反馈，通过在线强化学习进行迭代优化。\n\n### 主要成果与优势\n\n*   **弥合知识鸿沟：** TiG成功弥合了LLMs的声明性知识与程序性知识之间的差距，使其能够更好地应对交互式任务。\n*   **高效与高性能：** 相较于传统的强化学习方法，TiG以显著更低的数据和计算需求实现了具有竞争力的性能。\n*   **增强透明度与可解释性：** TiG为其决策提供分步的自然语言解释，极大地提高了在复杂交互任务中的透明度和可解释性。",
      "shortSummary": "“Think in Games (TiG)”框架旨在解决大型语言模型（LLMs）在交互任务中缺乏程序性知识的问题。TiG通过将强化学习决策转化为语言建模任务，使LLMs能生成并优化语言引导的策略。该方法成功弥合了LLMs的声明性知识与程序性知识之间的鸿沟，以更少的数据和计算资源实现了与传统强化学习相当的性能，并提供了可解释的自然语言决策步骤，显著提升了透明度。",
      "translated_title": "在游戏中思考：大型语言模型通过强化学习学习游戏中的推理能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) excel at complex reasoning tasks such as mathematics and coding, yet they frequently struggle with simple interactive tasks that young children perform effortlessly. This discrepancy highlights a critical gap between declarative knowledge (knowing about something) and procedural knowledge (knowing how to do something). Although traditional reinforcement learning (RL) agents can acquire procedural knowledge through environmental interaction, they often operate as black boxes and require substantial training data. In contrast, LLMs possess extensive world knowledge and reasoning capabilities, but are unable to effectively convert this static knowledge into dynamic decision-making in interactive settings. To address this challenge, we propose Think in Games (TiG), a novel framework that empowers LLMs to develop procedural understanding through direct interaction with game environments, while retaining their inherent reasoning and explanatory abilities. Specifically, TiG reformulates RL-based decision-making as a language modeling task: LLMs generate language-guided policies, which are refined iteratively through online reinforcement learning based on environmental feedback. Our experimental results show that TiG successfully bridges the gap between declarative and procedural knowledge, achieving competitive performance with dramatically lower data and computational demands compared to conventional RL methods. Moreover, TiG provides step-by-step natural language explanations for its decisions, greatly improving transparency and interpretability in complex interactive tasks."
    },
    {
      "title": "从代码生成模型中获取高效代码嵌入 (原标题: Efficient Code Embeddings from Code Generation Models)",
      "link": "https://arxiv.org/abs/2508.21290",
      "pubDate": "Thu, 28 Aug 2025 21:18:15 GMT",
      "isoDate": "2025-08-28T21:18:15.000Z",
      "creator": "Daria Kryvosheieva, Saba Sturua, Michael Günther, Scott Martens, Han Xiao",
      "summary": "# jina-code-embeddings：高效代码嵌入模型套件\n\njina-code-embeddings 是一套新颖的代码嵌入模型，旨在解决多个核心任务，并通过创新方法实现了最先进的性能。\n\n## 主要功能与应用\n*   **代码检索**：能够根据自然语言查询高效地检索相关代码。\n*   **技术问答**：支持进行技术领域的问答任务。\n*   **跨语言代码相似性识别**：能够识别不同编程语言之间语义相似的代码片段。\n\n## 创新方法\n该模型套件的核心创新在于：\n*   **自回归骨干网络**：利用一个在文本和代码上都进行了预训练的自回归骨干网络作为其基础架构。\n*   **Last-token Pooling**：通过“last-token pooling”机制来生成代码嵌入。\n\n## 性能表现\n*   尽管模型规模相对较小，jina-code-embeddings 仍展示了**最先进的性能**。\n*   这一成果验证了通过上述方法构建代码嵌入模型的有效性。\n\n## 技术细节\n*   文章概述了其训练方法。\n*   总计9页，包含表格和评估（5-9页）。\n*   相关主题包括计算与语言 (cs.CL)、人工智能 (cs.AI) 和信息检索 (cs.IR)。",
      "shortSummary": "jina-code-embeddings 是一套新颖的代码嵌入模型，旨在通过自然语言查询检索代码、进行技术问答以及识别跨语言的语义相似代码。它创新性地利用在文本和代码上预训练的自回归骨干网络，并通过 last-token pooling 生成嵌入。尽管模型规模相对较小，但其性能达到了最先进水平，验证了这种代码嵌入模型构建方法的有效性。",
      "translated_title": "从代码生成模型中获取高效代码嵌入",
      "images": [],
      "contentSource": "完整文章",
      "content": "jina-code-embeddings is a novel code embedding model suite designed to retrieve code from natural language queries, perform technical question-answering, and identify semantically similar code snippets across programming languages. It makes innovative use of an autoregressive backbone pre-trained on both text and code, generating embeddings via last-token pooling. We outline the training recipe and demonstrate state-of-the-art performance despite the relatively small size of the models, validating this approach to code embedding model construction."
    },
    {
      "title": "Dress&Dance：随心所欲地装扮和舞蹈 - 技术预览 (原标题: Dress&Dance: Dress up and Dance as You Like It - Technical Preview)",
      "link": "https://arxiv.org/abs/2508.21070",
      "pubDate": "Thu, 28 Aug 2025 13:59:55 GMT",
      "isoDate": "2025-08-28T13:59:55.000Z",
      "creator": "Jun-Kun Chen, Aayush Bansal, Minh Phuoc Vo, Yu-Xiong Wang",
      "summary": "# Dress&Dance：随心所欲地装扮和舞蹈 - 技术预览\n\n## 1. 核心功能与目标\n*   **Dress&Dance** 是一个先进的视频扩散框架，专注于生成高质量的虚拟试穿视频。\n*   该框架能够根据用户的单张图像和一段给定的参考视频，生成用户穿着所需服装并按照参考视频动作移动的视频。\n*   生成的视频长度为5秒，帧率为24 FPS，分辨率高达1152x720。\n\n## 2. 输入要求与服装支持\n*   **所需输入**：仅需一张用户的图像。\n*   **服装类型**：支持广泛的服装类型，包括上衣、下装和连体衣。\n*   **多件试穿**：该系统还支持在一次处理中同时试穿上衣和下装。\n\n## 3. 关键技术：CondNet\n*   **CondNet** 是该框架的核心创新，它是一个新颖的条件网络。\n*   **工作原理**：CondNet 利用注意力机制，有效地统一了多模态输入，包括文本、图像和视频。\n*   **技术优势**：通过这种统一处理，CondNet 显著增强了服装的注册（即服装在人体上的准确贴合）和动作的保真度（即服装随身体动作的自然表现）。\n*   **训练方法**：CondNet 采用多阶段渐进式训练方法，结合了有限的视频数据和更易获取的大规模图像数据集，以优化其性能。\n\n## 4. 性能与用户体验\n*   **性能表现**：Dress&Dance 在性能上超越了现有的开源和商业解决方案。\n*   **用户体验**：它为用户提供了一种高质量且高度灵活的虚拟试穿体验。",
      "shortSummary": "Dress&Dance是一个创新的视频扩散框架，能根据单张用户图像和参考动作视频，生成5秒、1152x720分辨率的高质量虚拟试穿视频。其核心是CondNet，一个利用注意力机制统一多模态输入的条件网络，显著提升了服装注册和动作保真度。该框架支持多种服装类型，并超越了现有解决方案，提供高质量且灵活的试穿体验。",
      "translated_title": "Dress&Dance：随心所欲地装扮和舞蹈 - 技术预览",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Dress&amp;Dance, a video diffusion framework that generates high quality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of a user wearing desired garments while moving in accordance with a given reference video. Our approach requires a single user image and supports a range of tops, bottoms, and one-piece garments, as well as simultaneous tops and bottoms try-on in a single pass. Key to our framework is CondNet, a novel conditioning network that leverages attention to unify multi-modal inputs (text, images, and videos), thereby enhancing garment registration and motion fidelity. CondNet is trained on heterogeneous training data, combining limited video data and a larger, more readily available image dataset, in a multistage progressive manner. Dress&amp;Dance outperforms existing open source and commercial solutions and enables a high quality and flexible try-on experience."
    },
    {
      "title": "OneReward：通过多任务人类偏好学习实现统一的掩码引导图像生成 (原标题: OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning)",
      "link": "https://arxiv.org/abs/2508.21066",
      "pubDate": "Thu, 28 Aug 2025 13:59:46 GMT",
      "isoDate": "2025-08-28T13:59:46.000Z",
      "creator": "Yuan Gong, Xionghui Wang, Jie Wu, Shiyin Wang, Yitong Wang, Xinglong Wu",
      "summary": "## OneReward：统一的掩码引导图像生成框架\n\n本文介绍了一个名为 **OneReward** 的创新框架，旨在通过单一奖励模型提升模型在多任务下的生成能力。\n\n### 核心概念与目标\n*   **统一的强化学习框架**：OneReward是一个统一的强化学习框架，它使用**一个单一的奖励模型**来增强模型在不同评估标准下跨多个任务的生成能力。\n*   **单一视觉-语言模型（VLM）作为奖励模型**：该框架采用一个VLM作为生成奖励模型，该VLM能够区分给定任务和评估标准下的优胜者和劣势者。这使得它能够有效地应用于数据和任务目标多样化的多任务生成模型。\n\n### 应用领域：掩码引导图像生成\n*   OneReward特别应用于**掩码引导图像生成**领域，该领域可进一步细分为多个子任务，包括：\n    *   图像填充（Image Fill）\n    *   图像扩展（Image Extend）\n    *   对象移除（Object Removal）\n    *   文本渲染（Text Rendering）\n*   这些子任务都涉及使用二值掩码作为编辑区域。尽管它们共享相同的条件范式，但在底层数据分布和评估指标上存在显著差异。\n\n### 解决现有问题\n*   **现有方法的局限性**：当前方法通常依赖于任务特定的监督微调（SFT），这限制了模型的泛化能力和训练效率。\n*   **OneReward的优势**：OneReward通过多任务强化学习直接在预训练的基础模型上进行训练，**消除了对任务特定SFT的需求**，从而提高了效率和泛化性。\n\n### 模型开发与实验结果\n*   **Seedream 3.0 Fill**：基于OneReward，研究团队开发了Seedream 3.0 Fill，这是一个通过多任务强化学习训练的掩码引导生成模型。\n*   **卓越的性能**：实验结果表明，该统一的编辑模型在多个评估维度上持续优于商业和开源竞争对手，包括Ideogram、Adobe Photoshop和FLUX Fill [Pro]。",
      "shortSummary": "OneReward是一个统一的强化学习框架，通过使用单一视觉-语言模型作为奖励模型，提升了模型在多任务下的生成能力。它专注于掩码引导图像生成，涵盖图像填充、扩展、对象移除和文本渲染等任务。该框架无需任务特定的监督微调，直接在预训练模型上进行多任务强化学习。基于此，开发的Seedream 3.0 Fill模型在性能上超越了Ideogram、Adobe Photoshop等商业和开源竞争对手。",
      "translated_title": "OneReward：通过多任务人类偏好学习实现统一的掩码引导图像生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this paper, we introduce OneReward, a unified reinforcement learning framework that enhances the model's generative capabilities across multiple tasks under different evaluation criteria using only One Reward model. By employing a single vision-language model (VLM) as the generative reward model, which can distinguish the winner and loser for a given task and a given evaluation criterion, it can be effectively applied to multi-task generation models, particularly in contexts with varied data and diverse task objectives. We utilize OneReward for mask-guided image generation, which can be further divided into several sub-tasks such as image fill, image extend, object removal, and text rendering, involving a binary mask as the edit area. Although these domain-specific tasks share same conditioning paradigm, they differ significantly in underlying data distributions and evaluation metrics. Existing methods often rely on task-specific supervised fine-tuning (SFT), which limits generalization and training efficiency. Building on OneReward, we develop Seedream 3.0 Fill, a mask-guided generation model trained via multi-task reinforcement learning directly on a pre-trained base model, eliminating the need for task-specific SFT. Experimental results demonstrate that our unified edit model consistently outperforms both commercial and open-source competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across multiple evaluation dimensions. Code and model are available at: https://one-reward.github.io"
    },
    {
      "title": "OnGoal：在大语言模型多轮对话中跟踪和可视化对话目标 (原标题: OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models)",
      "link": "https://arxiv.org/abs/2508.21061",
      "pubDate": "Thu, 28 Aug 2025 13:58:29 GMT",
      "isoDate": "2025-08-28T13:58:29.000Z",
      "creator": "Adam Coscia, Shunan Guo, Eunyee Koh, Alex Endert",
      "summary": "## OnGoal：在大语言模型多轮对话中跟踪和可视化对话目标\n\n### 核心问题\n\n随着与大型语言模型（LLM）的多轮对话变得越来越长和复杂，用户如何更好地评估和审查其对话目标的进展成为一个挑战。\n\n### 解决方案：OnGoal 界面\n\nOnGoal 是一个专为解决上述问题而设计的 LLM 聊天界面，旨在帮助用户更好地管理对话目标的进展。\n\n### OnGoal 的主要功能\n\nOnGoal 提供了以下关键功能，以增强用户在复杂对话中的导航能力：\n\n*   **实时目标对齐反馈**：通过 LLM 辅助评估，OnGoal 能够提供关于对话内容与用户目标对齐程度的实时反馈。\n*   **评估结果解释**：针对评估结果，OnGoal 会提供相应的解释，并辅以示例，帮助用户理解反馈的依据。\n*   **目标进展概览**：界面提供随时间推移的目标进展概览，使用户能够清晰地了解对话的整体方向和阶段性成果。\n\n### 用户研究与发现\n\n研究团队对 OnGoal 进行了一项用户研究，涉及 20 名参与者，任务是完成一项写作任务。研究将 OnGoal 与一个不带目标跟踪功能的基线聊天界面进行了对比。\n\n研究结果表明，使用 OnGoal 的参与者：\n\n*   **效率提升**：花费更少的时间和精力来达成他们的目标。\n*   **策略探索**：能够探索新的提示策略来克服对话中的误解。\n*   **增强参与度和韧性**：这些发现表明，跟踪和可视化对话目标可以显著增强用户在 LLM 对话中的参与度和面对挑战时的韧性。\n\n### 对未来 LLM 聊天界面的设计启示\n\nOnGoal 的研究结果为未来 LLM 聊天界面的设计提供了重要的启示，旨在：\n\n*   **改进目标沟通**：优化用户与 LLM 之间关于目标的沟通方式。\n*   **减少认知负荷**：通过清晰的反馈和概览，降低用户在管理复杂对话时的认知负担。\n*   **增强交互性**：提升用户与 LLM 界面之间的互动体验。\n*   **实现反馈以提高 LLM 性能**：设计机制使用户的反馈能够反过来帮助改进 LLM 的性能。",
      "shortSummary": "OnGoal是一个LLM聊天界面，旨在帮助用户管理多轮对话中的目标进展。它通过LLM辅助评估提供实时目标对齐反馈、解释和进展概览。一项针对20名参与者的研究表明，使用OnGoal的参与者花费更少的时间和精力实现目标，并能探索新的提示策略克服误解，从而增强了LLM对话的参与度和韧性。研究结果为未来LLM聊天界面设计提供了改进目标沟通、减少认知负荷和增强交互性的启示。",
      "translated_title": "OnGoal：在大语言模型多轮对话中跟踪和可视化对话目标",
      "images": [],
      "contentSource": "完整文章",
      "content": "As multi-turn dialogues with large language models (LLMs) grow longer and more complex, how can users better evaluate and review progress on their conversational goals? We present OnGoal, an LLM chat interface that helps users better manage goal progress. OnGoal provides real-time feedback on goal alignment through LLM-assisted evaluation, explanations for evaluation results with examples, and overviews of goal progression over time, enabling users to navigate complex dialogues more effectively. Through a study with 20 participants on a writing task, we evaluate OnGoal against a baseline chat interface without goal tracking. Using OnGoal, participants spent less time and effort to achieve their goals while exploring new prompting strategies to overcome miscommunication, suggesting tracking and visualizing goals can enhance engagement and resilience in LLM dialogues. Our findings inspired design implications for future LLM chat interfaces that improve goal communication, reduce cognitive load, enhance interactivity, and enable feedback to improve LLM performance."
    },
    {
      "title": "多视角三维点跟踪 (原标题: Multi-View 3D Point Tracking)",
      "link": "https://arxiv.org/abs/2508.21060",
      "pubDate": "Thu, 28 Aug 2025 13:58:20 GMT",
      "isoDate": "2025-08-28T13:58:20.000Z",
      "creator": "Frano Rajič, Haofei Xu, Marko Mihajlovic, Siyuan Li, Irem Demir, Emircan Gündoğdu, Lei Ke, Sergey Prokudin, Marc Pollefeys, Siyu Tang",
      "summary": "### 多视角三维点跟踪器介绍\n\n本文介绍了一种新颖的数据驱动多视角三维点跟踪器，旨在解决动态场景中任意点的跟踪问题。该方法克服了现有单目跟踪器在深度模糊和遮挡方面的局限性，以及传统多相机方法对大量相机（超过20个）和繁琐的序列优化需求。\n\n#### 核心创新与方法\n\n*   **数据驱动与前馈模型**：该跟踪器是首个数据驱动的多视角三维点跟踪器，采用前馈模型，能够直接预测三维对应关系。\n*   **实用相机数量**：与需要大量相机的传统方法不同，该模型仅需少量实用相机（例如四个）即可实现鲁棒、准确的在线跟踪。\n*   **输入要求**：需要已知的相机姿态和多视角深度信息（可由传感器提供或估计）。\n*   **特征融合与更新**：\n    *   将多视角特征融合到一个统一的点云中。\n    *   结合k近邻（k-nearest-neighbors）相关性与基于Transformer的更新机制，以可靠地估计长距离三维对应关系。\n    *   即使在存在遮挡的情况下，也能有效工作。\n\n#### 训练与评估\n\n*   **训练数据**：模型在包含5000个合成多视角Kubric序列的数据集上进行训练。\n*   **真实世界基准测试**：在两个真实世界基准数据集上进行了评估：\n    *   Panoptic Studio：中位数轨迹误差为3.1厘米。\n    *   DexYCB：中位数轨迹误差为2.0厘米。\n\n#### 泛化能力\n\n该方法展现出良好的泛化能力，适用于多种相机设置：\n\n*   相机数量：1到8个视角。\n*   视角位置：不同的有利视角。\n*   视频长度：24到150帧。\n\n#### 贡献与未来影响\n\n通过发布该跟踪器及其训练和评估数据集，研究团队旨在：\n\n*   为多视角三维跟踪研究设定新的标准。\n*   为实际应用提供一个实用的工具。\n\n该研究成果已被ICCV 2025接收为口头报告（Oral）。项目页面可在提供的链接访问。",
      "shortSummary": "本文介绍了一种新颖的数据驱动多视角三维点跟踪器，旨在解决动态场景中的任意点跟踪问题。该模型仅需少量相机（如四个），通过融合多视角特征和基于Transformer的更新机制，即使在遮挡下也能鲁棒、准确地估计长距离三维对应关系。在Panoptic Studio和DexYCB基准测试中，分别实现了3.1厘米和2.0厘米的中位数轨迹误差。该方法具有良好的泛化能力，并随数据集一同发布，旨在为多视角三维跟踪研究设定新标准并提供实用工具。",
      "translated_title": "多视角三维点跟踪",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce the first data-driven multi-view 3D point tracker, designed to track arbitrary points in dynamic scenes using multiple camera views. Unlike existing monocular trackers, which struggle with depth ambiguities and occlusion, or prior multi-camera methods that require over 20 cameras and tedious per-sequence optimization, our feed-forward model directly predicts 3D correspondences using a practical number of cameras (e.g., four), enabling robust and accurate online tracking. Given known camera poses and either sensor-based or estimated multi-view depth, our tracker fuses multi-view features into a unified point cloud and applies k-nearest-neighbors correlation alongside a transformer-based update to reliably estimate long-range 3D correspondences, even under occlusion. We train on 5K synthetic multi-view Kubric sequences and evaluate on two real-world benchmarks: Panoptic Studio and DexYCB, achieving median trajectory errors of 3.1 cm and 2.0 cm, respectively. Our method generalizes well to diverse camera setups of 1-8 views with varying vantage points and video lengths of 24-150 frames. By releasing our tracker alongside training and evaluation datasets, we aim to set a new standard for multi-view 3D tracking research and provide a practical tool for real-world applications. Project page available at https://ethz-vlg.github.io/mvtracker."
    },
    {
      "title": "用于长视频生成的上下文混合模型 (原标题: Mixture of Contexts for Long Video Generation)",
      "link": "https://arxiv.org/abs/2508.21058",
      "pubDate": "Thu, 28 Aug 2025 13:57:55 GMT",
      "isoDate": "2025-08-28T13:57:55.000Z",
      "creator": "Shengqu Cai, Ceyuan Yang, Lvmin Zhang, Yuwei Guo, Junfei Xiao, Ziyan Yang, Yinghao Xu, Zhenheng Yang, Alan Yuille, Leonidas Guibas, Maneesh Agrawala, Lu Jiang, Gordon Wetzstein",
      "summary": "## 用于长视频生成的上下文混合模型 (Mixture of Contexts for Long Video Generation)\n\n### 核心问题：长视频生成中的上下文记忆挑战\n\n长视频生成本质上是一个“长上下文记忆问题”。在生成长视频时，模型必须能够在长时间范围内保留并检索显著事件，同时避免内容崩溃或漂移。然而，将扩散Transformer扩展到生成长上下文视频面临根本性限制，主要原因在于自注意力机制的二次方计算成本。这使得处理长序列时，内存和计算变得难以管理和优化。\n\n### 提出的解决方案：上下文混合模型 (MoC)\n\n为了解决上述挑战，研究人员提出了一种名为“上下文混合模型”（Mixture of Contexts, MoC）的新方法。MoC被设计为一个简单、可学习的稀疏注意力路由模块，旨在作为一种有效的长期记忆检索引擎。\n\n### MoC 的工作原理\n\nMoC 将长上下文视频生成重新定义为一个“内部信息检索任务”。其核心机制包括：\n\n*   **动态选择信息块**：每个查询（query）会动态地选择少数几个信息丰富的历史块（chunks）进行关注。\n*   **强制锚点**：除了动态选择的信息块，MoC 还包含强制性的锚点，例如视频的整体描述（caption）和局部窗口信息，以提供更稳定的上下文。\n*   **因果路由**：模型采用因果路由机制，这有助于防止生成过程中出现循环闭合（loop closures），从而保持时间上的连贯性。\n\n### MoC 的优势与成果\n\n通过MoC的设计和应用，该模型展现出显著的优势：\n\n*   **效率提升**：随着数据规模的扩大和路由的逐步稀疏化，MoC能够将计算资源分配给显著的历史信息。这种检索机制的副产品是实现了近似线性的扩展（near-linear scaling），极大地提高了效率。\n*   **实用性**：高效的计算使得长视频的训练和合成变得切实可行。\n*   **记忆与一致性**：MoC能够有效地保留视频内容中的身份、动作和场景，并在数分钟的视频内容中保持高度的一致性。在分钟级别的尺度上，模型的记忆能力和一致性得以显著提升。",
      "shortSummary": "长视频生成面临长上下文记忆挑战，传统扩散Transformer因自注意力机制的二次方成本而受限。本文提出“上下文混合模型”（MoC），一个可学习的稀疏注意力路由模块。MoC将长视频生成视为内部信息检索任务，通过动态选择信息块和强制锚点，并结合因果路由，实现近似线性的计算扩展。这使得模型能高效地在数分钟视频中保持身份、动作和场景的一致性，解决了长视频生成的记忆与效率问题。",
      "translated_title": "用于长视频生成的上下文混合模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes."
    },
    {
      "title": "FakeParts：一类新型AI生成深度伪造 (原标题: FakeParts: a New Family of AI-Generated DeepFakes)",
      "link": "https://arxiv.org/abs/2508.21052",
      "pubDate": "Thu, 28 Aug 2025 13:55:14 GMT",
      "isoDate": "2025-08-28T13:55:14.000Z",
      "creator": "Gaetan Brison, Soobash Daiboo, Samy Aimeur, Awais Hussain Sani, Xi Wang, Gianni Franchi, Vicky Kalogeiton",
      "summary": "# FakeParts：一类新型AI生成深度伪造\n\n本文介绍了“FakeParts”，这是一类新型的深度伪造技术，其特点是对真实视频的特定空间区域或时间片段进行细微、局部化的篡改。与完全合成的内容不同，这些部分篡改与真实元素无缝融合，使其具有极高的欺骗性且难以检测。\n\n## FakeParts 的主要特征\n\n*   **局部化篡改**：FakeParts 不会生成整个虚假视频，而是对现有真实视频的特定部分进行修改。\n*   **细微性**：这些篡改通常非常细微，例如改变面部表情、替换物体或修改背景。\n*   **无缝融合**：篡改部分与视频的其余真实元素完美融合，使得肉眼难以察觉。\n*   **高欺骗性**：由于其局部性和无缝性，FakeParts 比传统深度伪造更具欺骗性，更难被人类和现有检测模型识别。\n\n## FakePartsBench 数据集\n\n为了解决当前检测能力的关键空白，研究人员推出了 **FakePartsBench**，这是首个专门为捕捉全范围局部深度伪造而设计的大规模基准数据集。\n\n*   **规模**：包含超过25,000个视频。\n*   **标注**：提供像素级别和帧级别的篡改标注，这对于开发和评估检测方法至关重要。\n*   **目的**：旨在实现对检测方法的全面评估，推动更鲁棒的检测技术发展。\n\n## 检测挑战与性能下降\n\n研究结果表明，FakeParts 对现有检测方法构成了严峻挑战：\n\n*   **人类检测准确率下降**：与传统深度伪造相比，FakeParts 使人类的检测准确率降低了30%以上。\n*   **模型性能下降**：最先进的检测模型也观察到类似的性能下降。\n\n## 结论与意义\n\n这项工作揭示了当前深度伪造检测方法中存在的紧急漏洞，并提供了必要的资源，以开发针对局部视频篡改的更强大、更有效的检测方法。",
      "shortSummary": "“FakeParts”是一种新型AI生成深度伪造，通过对真实视频进行细微、局部化的篡改，使其与真实元素无缝融合，极具欺骗性且难以检测。与传统深度伪造相比，FakeParts使人类和现有检测模型的检测准确率显著下降。为应对此挑战，研究团队推出了大规模基准数据集FakePartsBench（含2.5万余视频及详细标注），旨在推动更鲁棒的局部视频篡改检测方法的发展，填补当前检测能力的关键空白。",
      "translated_title": "FakeParts：一类新型AI生成深度伪造",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations, ranging from altered facial expressions to object substitutions and background modifications, blend seamlessly with real elements, making them particularly deceptive and difficult to detect. To address the critical gap in detection capabilities, we present FakePartsBench, the first large-scale benchmark dataset specifically designed to capture the full spectrum of partial deepfakes. Comprising over 25K videos with pixel-level and frame-level manipulation annotations, our dataset enables comprehensive evaluation of detection methods. Our user studies demonstrate that FakeParts reduces human detection accuracy by over 30% compared to traditional deepfakes, with similar performance degradation observed in state-of-the-art detection models. This work identifies an urgent vulnerability in current deepfake detection approaches and provides the necessary resources to develop more robust methods for partial video manipulations."
    },
    {
      "title": "CogVLA：通过指令驱动路由与稀疏化实现的认知对齐视觉-语言-动作模型 (原标题: CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification)",
      "link": "https://arxiv.org/abs/2508.21046",
      "pubDate": "Thu, 28 Aug 2025 13:50:58 GMT",
      "isoDate": "2025-08-28T13:50:58.000Z",
      "creator": "Wei Li, Renshan Zhang, Rui Shao, Jie He, Liqiang Nie",
      "summary": "## CogVLA：通过指令驱动路由与稀疏化实现的认知对齐视觉-语言-动作模型\n\n### 背景与问题\n\n当前基于预训练视觉-语言模型（VLM）构建的视觉-语言-动作（VLA）模型面临着显著的挑战。它们通常需要大量的后训练，这导致计算开销高昂，从而限制了模型的可扩展性和效率。\n\n### CogVLA框架介绍\n\n本文提出了一种名为CogVLA的认知对齐视觉-语言-动作框架。CogVLA旨在通过利用指令驱动的路由和稀疏化机制，显著提升VLA模型的效率和性能。该框架从人类多模态协调中汲取灵感，并引入了一个独特的三阶段渐进式架构。\n\n### 三阶段渐进式架构\n\n1.  **编码器-FiLM聚合路由 (EFA-Routing)**\n    *   **目的：** 将指令信息有效地注入视觉编码器。\n    *   **机制：** 通过选择性地聚合和压缩双流视觉令牌，形成一个指令感知的紧凑潜在表示。这一步骤确保了视觉信息在早期阶段就与指令相关联。\n\n2.  **LLM-FiLM剪枝路由 (LFP-Routing)**\n    *   **目的：** 在语言模型中引入动作意图，并实现令牌级别的稀疏性。\n    *   **机制：** 基于EFA-Routing生成的紧凑视觉编码，LFP-Routing通过剪枝与指令无关的视觉接地令牌，精简了语言模型的输入，从而提高了处理效率。\n\n3.  **V-L-A耦合注意力 (CAtten)**\n    *   **目的：** 确保即使在感知输入被压缩的情况下，模型仍能支持准确和连贯的动作生成。\n    *   **机制：** CAtten结合了因果视觉-语言注意力与双向动作并行解码，以实现视觉、语言和动作之间的紧密耦合和协调。\n\n### 实验结果与性能\n\nCogVLA在LIBERO基准测试和真实世界机器人任务中进行了广泛的实验验证，并取得了最先进的性能：\n\n*   **成功率：** 在LIBERO基准测试中达到97.4%，在真实世界机器人任务中达到70.0%。\n*   **效率提升：** 与OpenVLA相比，CogVLA的训练成本降低了2.5倍，推理延迟减少了2.8倍。\n\n### 开源可用性\n\nCogVLA已开源并公开可用，方便研究社区进一步探索和应用。",
      "shortSummary": "CogVLA是一个认知对齐的视觉-语言-动作（VLA）模型，旨在解决现有VLA模型计算开销大的问题。它通过指令驱动的路由和稀疏化，以及一个三阶段渐进式架构（包括EFA-Routing、LFP-Routing和V-L-A耦合注意力）来提高效率和性能。实验证明，CogVLA在机器人任务上实现了最先进的性能，并显著降低了训练成本和推理延迟，同时已开源。",
      "translated_title": "CogVLA：通过指令驱动路由与稀疏化实现的认知对齐视觉-语言-动作模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent Vision-Language-Action (VLA) models built on pre-trained Vision-Language Models (VLMs) require extensive post-training, resulting in high computational overhead that limits scalability and deployment.We propose CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages instruction-driven routing and sparsification to improve both efficiency and performance. CogVLA draws inspiration from human multimodal coordination and introduces a 3-stage progressive architecture. 1) Encoder-FiLM based Aggregation Routing (EFA-Routing) injects instruction information into the vision encoder to selectively aggregate and compress dual-stream visual tokens, forming a instruction-aware latent representation. 2) Building upon this compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing) introduces action intent into the language model by pruning instruction-irrelevant visually grounded tokens, thereby achieving token-level sparsity. 3) To ensure that compressed perception inputs can still support accurate and coherent action generation, we introduce V-L-A Coupled Attention (CAtten), which combines causal vision-language attention with bidirectional action parallel decoding. Extensive experiments on the LIBERO benchmark and real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art performance with success rates of 97.4% and 70.0%, respectively, while reducing training costs by 2.5-fold and decreasing inference latency by 2.8-fold compared to OpenVLA. CogVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/CogVLA."
    },
    {
      "title": "R-4B：通过双模退火和强化学习激励多模态大语言模型（MLLMs）的通用自动思考能力 (原标题: R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning)",
      "link": "https://arxiv.org/abs/2508.21113",
      "pubDate": "Thu, 28 Aug 2025 13:48:19 GMT",
      "isoDate": "2025-08-28T13:48:19.000Z",
      "creator": "Jie Jiang, Qi Yang, Bolin Ni, Shiming Xiang, Han Hu, Houwen Peng",
      "summary": "本文提出了一种名为 R-4B 的新型自动思考多模态大语言模型（MLLM），旨在解决现有 MLLM 在处理简单问题时，其逐步思考能力显得冗余且低效的问题。R-4B 的核心目标是使模型能够根据问题复杂性自适应地决定何时启动思考过程。\n\n**R-4B 的核心思想与方法：**\n\n*   **双模能力：** R-4B 通过“双模退火”（bi-mode annealing）机制，赋予模型同时具备“思考”和“非思考”两种能力。\n*   **策略优化：** 采用“双模策略优化”（Bi-mode Policy Optimization, BPO）来提高模型在决定是否激活思考过程时的准确性。\n\n**训练过程：**\n\n1.  **第一阶段：** 模型首先在一个精心策划的数据集上进行训练，该数据集涵盖了各种主题，并包含来自“思考模式”和“非思考模式”的样本。\n2.  **第二阶段：** 在改进的 GRPO 框架下进行第二阶段训练。在此阶段，策略模型被强制为每个输入查询生成来自两种模式的响应。\n\n**实验结果与性能：**\n\n*   R-4B 在 25 个具有挑战性的基准测试中取得了最先进的性能。\n*   在大多数任务中，其表现优于 Qwen2.5-VL-7B 模型。\n*   在推理密集型基准测试中，R-4B 的性能可与更大的模型（如 Kimi-VL-A3B-Thinking-2506，16B）相媲美，但计算成本更低。",
      "shortSummary": "R-4B 提出了一种自动思考的多模态大语言模型（MLLM），旨在解决现有 MLLM 在处理简单问题时思考过程冗余的问题。该模型通过双模退火和双模策略优化，使其能根据问题复杂性自适应地决定是否启动思考。R-4B 经过两阶段训练，在 25 个基准测试中取得了最先进的性能，超越了 Qwen2.5-VL-7B，并以更低的计算成本达到了与更大模型相当的推理能力。",
      "translated_title": "R-4B：通过双模退火和强化学习激励多模态大语言模型（MLLMs）的通用自动思考能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking capabilities have demonstrated remarkable performance on complex reasoning problems. However, this thinking process is redundant for simple problems solvable without complex reasoning. To address this inefficiency, we propose R-4B, an auto-thinking MLLM, which can adaptively decide when to think based on problem complexity. The central idea of R-4B is to empower the model with both thinking and non-thinking capabilities using bi-mode annealing, and apply Bi-mode Policy Optimization~(BPO) to improve the model's accuracy in determining whether to activate the thinking process. Specifically, we first train the model on a carefully curated dataset spanning various topics, which contains samples from both thinking and non-thinking modes. Then it undergoes a second phase of training under an improved GRPO framework, where the policy model is forced to generate responses from both modes for each input query. Experimental results show that R-4B achieves state-of-the-art performance across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks and achieves performance comparable to larger models such as Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower computational cost."
    },
    {
      "title": "具身一体视觉：用于通用机器人控制的交错式视觉-文本-动作预训练 (原标题: EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control)",
      "link": "https://arxiv.org/abs/2508.21112",
      "pubDate": "Thu, 28 Aug 2025 13:26:15 GMT",
      "isoDate": "2025-08-28T13:26:15.000Z",
      "creator": "Delin Qu, Haoming Song, Qizhi Chen, Zhaoqing Chen, Xianqiang Gao, Xinyi Ye, Qi Lv, Modi Shi, Guanghui Ren, Cheng Ruan, Maoqing Yao, Haoran Yang, Jiacheng Bao, Bin Zhao, Dong Wang",
      "summary": "# 具身一体视觉：用于通用机器人控制的交错式视觉-文本-动作预训练\n\n## 引言\n\n通用具身智能系统的核心目标是使机器人在开放世界中能够像人类一样无缝地进行多模态推理和物理交互。尽管近期基于大规模机器人和视觉-文本数据共同训练的视觉-语言-动作（VLA）模型在通用机器人控制方面取得了显著进展，但它们在交错推理和交互的灵活性上仍未能达到人类水平。\n\n## EO-Robotics 框架\n\n为了解决这一挑战，本文引入了 **EO-Robotics** 框架，它由以下两部分组成：\n\n1.  **EO-1 模型**：一个统一的具身基础模型。\n2.  **EO-Data1.5M 数据集**：一个大规模、高质量的多模态具身推理数据集。\n\n## EO-1 模型的核心支柱\n\nEO-1 模型的发展基于两大关键支柱：\n\n*   **统一架构**：EO-1 采用统一的架构，能够无差别地处理各种多模态输入，包括图像、文本、视频和动作。\n*   **大规模、高质量数据集**：**EO-Data1.5M** 数据集包含超过150万个样本，特别强调对交错式视觉-文本-动作理解的训练。\n\n## 训练方法\n\nEO-1 模型在 EO-Data1.5M 数据集上通过自回归解码和流匹配去噪的协同作用进行训练。这种创新的训练方法使得模型能够：\n\n*   无缝地生成机器人动作。\n*   进行高效的多模态具身推理。\n\n## 实验结果与有效性\n\n广泛的实验结果有力地证明了交错式视觉-文本-动作学习在开放世界理解和泛化方面的卓越有效性。通过在各种长周期、灵巧操作任务以及多种具身形式上的验证，EO-1 模型展现了其优越的性能。\n\n## 贡献与展望\n\n本文详细阐述了 EO-1 的架构、EO-Data1.5M 的数据构建策略以及具体的训练方法。这些深入的细节为开发更先进的具身基础模型提供了宝贵的见解和指导。",
      "shortSummary": "本文提出了EO-Robotics框架，旨在通过交错式视觉-文本-动作预训练提升通用机器人控制能力。该框架包含统一的具身基础模型EO-1和大规模多模态数据集EO-Data1.5M。EO-1采用统一架构处理图像、文本、视频和动作等多种输入，并在超过150万样本的数据集上通过自回归解码和流匹配去噪进行训练。实验证明，EO-1在开放世界理解和泛化方面表现出色，能实现无缝机器人动作生成和多模态具身推理，为具身基础模型发展提供了新思路。",
      "translated_title": "具身一体视觉：用于通用机器人控制的交错式视觉-文本-动作预训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "The human ability to seamlessly perform multimodal reasoning and physical interaction in the open world is a core goal for general-purpose embodied intelligent systems. Recent vision-language-action (VLA) models, which are co-trained on large-scale robot and visual-text data, have demonstrated notable progress in general robot control. However, they still fail to achieve human-level flexibility in interleaved reasoning and interaction. In this work, introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is a unified embodied foundation model that achieves superior performance in multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training. The development of EO-1 is based on two key pillars: (i) a unified architecture that processes multimodal inputs indiscriminately (image, text, video, and action), and (ii) a massive, high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains over 1.5 million samples with emphasis on interleaved vision-text-action comprehension. EO-1 is trained through synergies between auto-regressive decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot action generation and multimodal embodied reasoning. Extensive experiments demonstrate the effectiveness of interleaved vision-text-action learning for open-world understanding and generalization, validated through a variety of long-horizon, dexterous manipulation tasks across multiple embodiments. This paper details the architecture of EO-1, the data construction strategy of EO-Data1.5M, and the training methodology, offering valuable insights for developing advanced embodied foundation models."
    },
    {
      "title": "扭转咒语：通过秩一安全注入实现轻量级对齐放大 (原标题: Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection)",
      "link": "https://arxiv.org/abs/2508.20766",
      "pubDate": "Thu, 28 Aug 2025 09:22:33 GMT",
      "isoDate": "2025-08-28T09:22:33.000Z",
      "creator": "Harethah Abu Shairah, Hasan Abed Al Kader Hammoud, George Turkiyyah, Bernard Ghanem",
      "summary": "# 详细摘要：秩一安全注入（ROSI）——轻量级对齐放大方法\n\n## 1. 研究背景与问题\n*   大型语言模型（LLMs）的安全对齐旨在通过调节内部表示来拒绝有害请求。\n*   现有研究发现，这些安全机制可能被绕过，例如通过消除或移除模型内特定的表示方向。\n\n## 2. 提出的方法：秩一安全注入（ROSI）\n*   本文提出了一种名为“秩一安全注入”（Rank-One Safety Injection, ROSI）的白盒方法，旨在解决上述问题。\n*   ROSI采取与绕过方法相反的策略：它通过永久性地将模型的激活引导至负责拒绝有害请求的子空间，从而主动放大模型的安全对齐。\n\n## 3. ROSI的工作原理\n*   ROSI是一种简单且无需微调的秩一权重修改技术。\n*   它被应用于模型中所有残差流的写入矩阵。\n*   所需的安全方向可以通过分析一小组有害和无害指令对来计算得出。\n\n## 4. 实验结果与性能\n*   **安全性提升：** 实验结果表明，ROSI能够持续提高模型的安全拒绝率，这一效果通过Llama Guard 3进行评估。\n*   **实用性保持：** 在MMLU、HellaSwag和Arc等标准基准测试中，ROSI在提高安全性的同时，成功地保持了模型的实用性。\n*   **模型重新对齐能力：** ROSI还展示了其能够重新对齐“未审查”模型的能力，通过放大这些模型自身潜在的安全方向来实现。这表明ROSI可以作为一种有效的“最后一英里”安全处理程序。\n\n## 5. 结论与研究意义\n*   研究结果强调，有针对性且可解释的权重引导是一种经济高效且强大的机制，能够显著提升LLM的安全性。\n*   ROSI方法为更资源密集型的微调范式提供了一个有益的补充。",
      "shortSummary": "本文提出秩一安全注入（ROSI），一种轻量级白盒方法，通过对LLM残差流写入矩阵进行秩一权重修改，永久性地将模型激活引导至安全拒绝子空间。ROSI无需微调，能显著提高模型安全拒绝率（经Llama Guard 3评估），同时保持在MMLU等基准测试上的实用性。它还能重新对齐“未审查”模型。ROSI提供了一种廉价且有效的LLM安全增强机制，补充了传统的微调方法。",
      "translated_title": "扭转咒语：通过秩一安全注入实现轻量级对齐放大",
      "images": [],
      "contentSource": "完整文章",
      "content": "Safety alignment in Large Language Models (LLMs) often involves mediating internal representations to refuse harmful requests. Recent research has demonstrated that these safety mechanisms can be bypassed by ablating or removing specific representational directions within the model. In this paper, we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box method that amplifies a model's safety alignment by permanently steering its activations toward the refusal-mediating subspace. ROSI operates as a simple, fine-tuning-free rank-one weight modification applied to all residual stream write matrices. The required safety direction can be computed from a small set of harmful and harmless instruction pairs. We show that ROSI consistently increases safety refusal rates - as evaluated by Llama Guard 3 - while preserving the utility of the model on standard benchmarks such as MMLU, HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align 'uncensored' models by amplifying their own latent safety directions, demonstrating its utility as an effective last-mile safety procedure. Our results suggest that targeted, interpretable weight steering is a cheap and potent mechanism to improve LLM safety, complementing more resource-intensive fine-tuning paradigms."
    },
    {
      "title": "大型语言模型工具内学习的可证明优势 (原标题: Provable Benefits of In-Tool Learning for Large Language Models)",
      "link": "https://arxiv.org/abs/2508.20755",
      "pubDate": "Thu, 28 Aug 2025 09:12:19 GMT",
      "isoDate": "2025-08-28T09:12:19.000Z",
      "creator": "Sam Houliston, Ambroise Odonnat, Charles Arnal, Vivien Cabannes",
      "summary": "### 大型语言模型工具内学习的可证明优势\n\n本文深入探讨了工具增强型大型语言模型（LLMs）的理论优势，特别是将工具内学习（in-tool learning）与权重内学习（in-weight learning）在事实召回方面的能力进行了对比。\n\n#### 核心问题与研究目标\n\n*   **背景：** 检索、记忆或外部API等工具增强型语言模型正在重塑人工智能领域，但其理论优势尚未得到充分探索。\n*   **目标：** 本研究旨在通过证明工具内学习（即外部检索）相对于权重内学习（即模型内部记忆）在事实召回方面的优势，来解决这一关键问题。\n\n#### 主要发现与理论证明\n\n1.  **权重内学习的局限性：**\n    *   研究表明，模型仅通过其权重记忆事实的数量，从根本上受到其参数数量的限制。这意味着传统上依赖内部记忆的LLMs，其事实召回能力存在固有的上限。\n2.  **工具内学习的无限潜力：**\n    *   与此形成鲜明对比的是，研究通过一个简单高效的电路构建，证明了工具使用能够实现无限制的事实召回。这表明工具增强型模型在处理和访问大量事实信息方面具有显著的、理论上无限的扩展性。\n\n#### 实验验证与实践指导\n\n*   **实验结果：** 在受控实验中，使用工具的模型始终优于仅依赖内部记忆的模型。这些实验结果为理论发现提供了有力的经验支持。\n*   **实践指导：** 对于预训练的大型语言模型，教授其工具使用和通用规则，比通过微调将事实直接写入模型记忆中更为有效。这为LLM的训练和应用提供了重要的策略建议。\n\n#### 结论\n\n*   本研究提供了坚实的理论和经验基础，明确指出工具增强型工作流不仅在实践中具有高效性，而且在可扩展性方面具有可证明的优势。",
      "shortSummary": "本文证明了大型语言模型中工具内学习（外部检索）相对于权重内学习（内部记忆）在事实召回方面的可证明优势。研究指出，模型通过权重记忆事实的能力受限于其参数数量，而工具使用能实现无限制的事实召回。实验验证了工具使用模型的优越性，并表明教授工具使用和通用规则比微调事实更有效。这确立了工具增强型工作流的实用性和可扩展性。",
      "translated_title": "大型语言模型工具内学习的可证明优势",
      "images": [],
      "contentSource": "完整文章",
      "content": "Tool-augmented language models, equipped with retrieval, memory, or external APIs, are reshaping AI, yet their theoretical advantages remain underexplored. In this paper, we address this question by demonstrating the benefits of in-tool learning (external retrieval) over in-weight learning (memorization) for factual recall. We show that the number of facts a model can memorize solely in its weights is fundamentally limited by its parameter count. In contrast, we prove that tool-use enables unbounded factual recall via a simple and efficient circuit construction. These results are validated in controlled experiments, where tool-using models consistently outperform memorizing ones. We further show that for pretrained large language models, teaching tool-use and general rules is more effective than finetuning facts into memory. Our work provides both a theoretical and empirical foundation, establishing why tool-augmented workflows are not just practical, but provably more scalable."
    },
    {
      "title": "Pref-GRPO：基于成对偏好奖励的GRPO，用于稳定的文本到图像强化学习 (原标题: Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2508.20751",
      "pubDate": "Thu, 28 Aug 2025 09:11:24 GMT",
      "isoDate": "2025-08-28T09:11:24.000Z",
      "creator": "Yibin Wang, Zhimin Li, Yuhang Zang, Yujie Zhou, Jiazi Bu, Chunyu Wang, Qinglin Lu, Cheng Jin, Jiaqi Wang",
      "summary": "## Pref-GRPO：解决文本到图像强化学习中的奖励欺骗问题\n\n### 1. 背景与问题\n\n*   **GRPO在T2I生成中的重要性**：最近的研究强调了基于GRPO的强化学习方法在增强文本到图像（T2I）生成方面的关键作用。\n*   **现有方法的局限性**：当前方法依赖于点式奖励模型（RM）来评估生成的图像，这使得它们容易受到“奖励欺骗”（reward hacking）的影响。\n*   **奖励欺骗的机制**：当图像之间微小的分数差异在归一化后被放大时，就会发生奖励欺骗。这会产生虚假的优势，驱使模型过度优化微不足道的收益，最终导致图像生成过程不稳定。\n\n### 2. 提出的解决方案：Pref-GRPO\n\n*   **核心思想**：为了解决奖励欺骗问题，文章提出了Pref-GRPO，一种基于成对偏好奖励的GRPO方法。\n*   **优化目标转变**：Pref-GRPO将优化目标从分数最大化转变为偏好拟合，从而确保更稳定的训练。\n*   **奖励信号机制**：\n    *   在每个组内，图像通过偏好奖励模型（RM）进行成对比较。\n    *   “胜率”被用作奖励信号。\n*   **Pref-GRPO的优势**：\n    *   能够区分细微的图像质量差异。\n    *   提供更稳定的优势信号。\n    *   有效缓解奖励欺骗问题。\n\n### 3. 新型统一T2I基准：UniGenBench\n\n*   **现有基准的不足**：现有的T2I基准受限于粗糙的评估标准，阻碍了对模型进行全面的评估。\n*   **UniGenBench的引入**：为了解决这一问题，文章引入了一个统一的T2I基准——UniGenBench。\n*   **UniGenBench的构成**：\n    *   包含600个提示，涵盖5个主要主题和20个子主题。\n    *   通过10个主要标准和27个子标准来评估语义一致性。\n*   **构建与评估方法**：UniGenBench的构建和评估利用了多模态大语言模型（MLLM）。\n*   **UniGenBench的价值**：\n    *   揭示了开源和闭源T2I模型的优势和劣势。\n    *   验证了Pref-GRPO方法的有效性。",
      "shortSummary": "Pref-GRPO针对文本到图像（T2I）强化学习中点式奖励模型导致的奖励欺骗问题，提出了一种基于成对偏好奖励的GRPO方法。它通过成对比较图像并使用胜率作为奖励信号，将优化目标从分数最大化转向偏好拟合，从而实现更稳定的训练并缓解奖励欺骗。此外，文章还引入了UniGenBench，一个包含600个提示和多维度评估标准的统一T2I基准，用于全面评估T2I模型并验证Pref-GRPO的有效性。",
      "translated_title": "Pref-GRPO：基于成对偏好奖励的GRPO，用于稳定的文本到图像强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements highlight the importance of GRPO-based reinforcement learning methods and benchmarking in enhancing text-to-image (T2I) generation. However, current methods using pointwise reward models (RM) for scoring generated images are susceptible to reward hacking. We reveal that this happens when minimal score differences between images are amplified after normalization, creating illusory advantages that drive the model to over-optimize for trivial gains, ultimately destabilizing the image generation process. To address this, we propose Pref-GRPO, a pairwise preference reward-based GRPO method that shifts the optimization objective from score maximization to preference fitting, ensuring more stable training. In Pref-GRPO, images are pairwise compared within each group using preference RM, and the win rate is used as the reward signal. Extensive experiments demonstrate that PREF-GRPO differentiates subtle image quality differences, providing more stable advantages and mitigating reward hacking. Additionally, existing T2I benchmarks are limited by coarse evaluation criteria, hindering comprehensive model assessment. To solve this, we introduce UniGenBench, a unified T2I benchmark comprising 600 prompts across 5 main themes and 20 subthemes. It evaluates semantic consistency through 10 primary and 27 sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our benchmarks uncover the strengths and weaknesses of both open and closed-source T2I models and validate the effectiveness of Pref-GRPO."
    },
    {
      "title": "rStar2-Agent：智能体推理技术报告 (原标题: rStar2-Agent: Agentic Reasoning Technical Report)",
      "link": "https://arxiv.org/abs/2508.20722",
      "pubDate": "Thu, 28 Aug 2025 08:45:25 GMT",
      "isoDate": "2025-08-28T08:45:25.000Z",
      "creator": "Ning Shang, Yifei Liu, Yi Zhu, Li Lyna Zhang, Weijiang Xu, Xinyu Guan, Buze Zhang, Bingcheng Dong, Xudong Zhou, Bowen Zhang, Ying Xin, Ziming Miao, Scarlett Li, Fan Yang, Mao Yang",
      "summary": "# rStar2-Agent：智能体推理技术报告\n\n本文介绍了rStar2-Agent，一个通过智能体强化学习（agentic reinforcement learning）训练的14B数学推理模型，旨在实现前沿水平的性能。该模型超越了当前的长CoT（Chain-of-Thought）方法，展现出先进的认知行为，例如：\n\n*   在使用Python编码工具前进行仔细思考。\n*   反思代码执行反馈，以自主探索、验证和完善复杂问题解决中的中间步骤。\n\n这些能力得益于三项关键创新，它们使得大规模智能体强化学习变得高效：\n\n1.  **高效的强化学习基础设施**：\n    *   提供可靠的Python代码环境，支持高吞吐量执行。\n    *   有效降低了高昂的rollout成本，使得模型能够在有限的GPU资源（64块MI300X GPU）上进行训练。\n2.  **GRPO-RoC智能体强化学习算法**：\n    *   采用“纠正时重采样”（Resample-on-Correct）的rollout策略。\n    *   解决了编码工具固有的环境噪声问题，使模型能够在代码环境中更有效地进行推理。\n3.  **高效的智能体训练方案**：\n    *   从非推理的SFT（Supervised Fine-Tuning）开始。\n    *   逐步通过多阶段强化学习，以最小的计算成本获得了先进的认知能力。\n\n## 性能表现：\n\n*   rStar2-Agent在短短一周内，通过510个强化学习步骤，将一个预训练的14B模型提升至最先进水平。\n*   在AIME24测试中，平均pass@1得分达到80.6%。\n*   在AIME25测试中，平均pass@1得分达到69.8%。\n*   以显著更短的响应长度，超越了DeepSeek-R1（671B）模型。\n\n## 泛化能力：\n\n*   除了数学领域，rStar2-Agent-14B还在对齐、科学推理和智能体工具使用任务中展现出强大的泛化能力。\n\n## 资源可用性：\n\n*   模型的代码和训练方案已公开。",
      "shortSummary": "rStar2-Agent是一个14B数学推理模型，通过智能体强化学习实现了前沿性能。它展现了在Python编码前仔细思考、并根据反馈自主探索和完善问题解决步骤的先进认知能力。该模型通过高效RL基础设施、GRPO-RoC算法和优化训练方案，在一周内达到SOTA，在AIME24和AIME25上分别取得80.6%和69.8%的pass@1分数，超越了更大的DeepSeek-R1模型，并具有强大的泛化能力。",
      "translated_title": "rStar2-Agent：智能体推理技术报告",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. Beyond current long CoT, the model demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations that makes agentic RL effective at scale: (i) an efficient RL infrastructure with a reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with a Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools, allowing the model to reason more effectively in a code environment; (iii) An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. To this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at https://github.com/microsoft/rStar."
    },
    {
      "title": "Droplet3D：视频中的常识先验促进3D生成 (原标题: Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation)",
      "link": "https://arxiv.org/abs/2508.20470",
      "pubDate": "Thu, 28 Aug 2025 02:39:41 GMT",
      "isoDate": "2025-08-28T02:39:41.000Z",
      "creator": "Xiaochuan Li, Guoguang Du, Runze Zhang, Liang Jin, Qi Jia, Lihua Lu, Zhenhua Guo, Yaqian Zhao, Haiyang Liu, Tianqi Wang, Changsheng Li, Xiaoli Gong, Rengang Li, Baoyu Fan",
      "summary": "## Droplet3D：利用视频常识先验促进3D生成\n\n### 背景与挑战\n\n大型生成模型在文本、图像和视频领域取得了显著成功，这得益于大规模数据的训练。然而，3D领域面临着严重的数据稀缺问题，可用的3D数据量远少于其他模态，这限制了3D生成模型的泛化能力。\n\n### 解决方案：利用视频中的常识先验\n\n本文提出利用视频中固有的常识先验作为替代监督信号，以弥补原生3D数据不足带来的泛化瓶颈。视频作为一种丰富的数据源，能够提供两种关键的先验信息：\n\n*   **空间一致性先验**：视频能够捕捉同一物体或场景的多个视图，这为3D生成提供了重要的空间一致性信息。\n*   **丰富语义信息**：视频中包含的丰富语义内容使得生成的3D内容能够更忠实于文本提示，并具有更高的语义合理性。\n\n### 主要贡献\n\n为了探索视频模态在3D资产生成中的应用，本文在数据集和模型层面做出了以下贡献：\n\n*   **Droplet3D-4M 数据集**：引入了首个大规模视频数据集，该数据集包含多视图级别的标注，为3D生成提供了丰富的监督信号。\n*   **Droplet3D 生成模型**：基于Droplet3D-4M数据集，训练了一个强大的生成模型Droplet3D。该模型支持图像和密集文本输入，能够灵活地进行3D内容创作。\n\n### 实验验证与优势\n\n广泛的实验验证了本文方法的有效性，展示了其在生成3D内容方面的卓越能力：\n\n*   **高质量生成**：该方法能够生成空间一致且语义合理的内容，显著提升了3D生成的质量。\n*   **场景级应用潜力**：与当前主流的3D解决方案相比，Droplet3D展现出扩展到场景级应用的巨大潜力，这表明其不仅限于单个物体生成。\n\n### 结论与资源开放\n\n研究结果表明，视频中的常识先验显著促进了3D创作。为了推动社区发展，作者已开源所有相关资源，包括数据集、代码、技术框架和模型权重（访问链接：this https URL）。",
      "shortSummary": "针对3D生成领域数据稀缺问题，本文提出利用视频中固有的常识先验作为替代监督信号。研究引入了首个大规模多视图视频数据集Droplet3D-4M，并训练了支持图像和密集文本输入的生成模型Droplet3D。实验证明，该方法能生成空间一致且语义合理的内容，并具备扩展到场景级应用的潜力。视频中的常识先验显著促进了3D创作。所有资源已开源。",
      "translated_title": "Droplet3D：视频中的常识先验促进3D生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Scaling laws have validated the success and promise of large-data-trained models in creative generation across text, image, and video domains. However, this paradigm faces data scarcity in the 3D domain, as there is far less of it available on the internet compared to the aforementioned modalities. Fortunately, there exist adequate videos that inherently contain commonsense priors, offering an alternative supervisory signal to mitigate the generalization bottleneck caused by limited native 3D data. On the one hand, videos capturing multiple views of an object or scene provide a spatial consistency prior for 3D generation. On the other hand, the rich semantic information contained within the videos enables the generated content to be more faithful to the text prompts and semantically plausible. This paper explores how to apply the video modality in 3D asset generation, spanning datasets to models. We introduce Droplet3D-4M, the first large-scale video dataset with multi-view level annotations, and train Droplet3D, a generative model supporting both image and dense text input. Extensive experiments validate the effectiveness of our approach, demonstrating its ability to produce spatially consistent and semantically plausible content. Moreover, in contrast to the prevailing 3D solutions, our approach exhibits the potential for extension to scene-level applications. This indicates that the commonsense priors from the videos significantly facilitate 3D creation. We have open-sourced all resources including the dataset, code, technical framework, and model weights: https://dropletx.github.io/."
    },
    {
      "title": "MCP-Bench：通过MCP服务器基准测试使用工具的LLM智能体处理复杂真实世界任务的能力 (原标题: MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers)",
      "link": "https://arxiv.org/abs/2508.20453",
      "pubDate": "Thu, 28 Aug 2025 01:58:57 GMT",
      "isoDate": "2025-08-28T01:58:57.000Z",
      "creator": "Zhenting Wang, Qi Chang, Hemani Patel, Shashank Biju, Cheng-En Wu, Quan Liu, Aolin Ding, Alireza Rezazadeh, Ankit Shah, Yujia Bao, Eugene Siow",
      "summary": "## MCP-Bench：评估使用工具的LLM智能体的新基准\n\n### 引言\n本文介绍了MCP-Bench，这是一个旨在评估大型语言模型（LLMs）在真实世界、多步骤任务中表现的新型基准测试。这些任务对LLMs提出了更高的要求，包括工具使用、跨工具协调、精确的参数控制以及复杂的规划和推理能力。\n\n### MCP-Bench的构建与范围\n*   **基于协议：** MCP-Bench建立在模型上下文协议（Model Context Protocol, MCP）之上。\n*   **连接服务器：** 它将LLMs连接到28个具有代表性的实时MCP服务器。\n*   **工具数量与领域：** 这些服务器共涵盖250种工具，跨越多个领域，如金融、旅行、科学计算和学术搜索。\n\n### 与现有基准的区别与优势\nMCP-Bench与以往基于API的基准测试存在显著差异，并提供了独特的优势：\n*   **互补工具集：** 每个MCP服务器提供一套设计为协同工作的互补工具，而非孤立的API。这使得能够构建具有丰富输入-输出耦合的真实多步骤任务。\n*   **模糊指令下的工具检索：** 任务测试智能体在没有明确工具名称的模糊指令下，识别并检索相关工具的能力。\n*   **复杂目标的多跳执行规划：** 评估智能体为实现复杂目标规划多步（多跳）执行轨迹的能力。\n*   **基于中间输出的响应：** 智能体需要能够将最终响应或后续操作基于中间工具的输出进行接地和调整。\n*   **跨领域工作流编排：** 评估智能体在不同领域之间编排和协调复杂工作流的能力。\n*   **弥补现有不足：** MCP-Bench解决了现有基准的局限性，这些基准通常依赖于明确的工具规范、浅层少数步骤的工作流和孤立的领域操作，无法充分评估LLMs在复杂真实场景下的能力。\n\n### 评估框架\n作者提出了一个多方面的评估框架，用于全面衡量LLM智能体的性能，该框架涵盖以下关键方面：\n*   **工具层面的模式理解和使用：** 评估智能体理解工具的输入/输出模式并正确使用的能力。\n*   **轨迹层面的规划：** 评估智能体规划整个任务执行路径的有效性。\n*   **任务完成度：** 衡量智能体成功完成给定任务的程度。\n\n### 实验结果\n对20个先进的LLMs进行的初步实验表明，即使是当前最先进的模型，在MCP-Bench上仍然面临持续的挑战，这突显了该领域未来研究的必要性。\n\n### 代码与数据\n相关代码和数据已公开，可通过提供的链接获取。",
      "shortSummary": "MCP-Bench是一个新基准，用于评估LLM智能体处理复杂真实世界任务的工具使用能力。它通过28个实时MCP服务器连接250种工具，涵盖金融、旅行等多个领域。该基准测试LLM在模糊指令下检索工具、规划多跳执行、基于中间输出响应及跨领域工作流编排的能力，弥补了现有基准的不足。对20个先进LLM的实验显示，它们在MCP-Bench上仍面临持续挑战，揭示了当前LLM在复杂工具使用方面的局限性。",
      "translated_title": "MCP-Bench：通过MCP服务器基准测试使用工具的LLM智能体处理复杂真实世界任务的能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce MCP-Bench, a benchmark for evaluating large language models (LLMs) on realistic, multi-step tasks that demand tool use, cross-tool coordination, precise parameter control, and planning/reasoning for solving tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28 representative live MCP servers spanning 250 tools across domains such as finance, traveling, scientific computing, and academic search. Unlike prior API-based benchmarks, each MCP server provides a set of complementary tools designed to work together, enabling the construction of authentic, multi-step tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability to retrieve relevant tools from fuzzy instructions without explicit tool names, plan multi-hop execution trajectories for complex objectives, ground responses in intermediate tool outputs, and orchestrate cross-domain workflows - capabilities not adequately evaluated by existing benchmarks that rely on explicit tool specifications, shallow few-step workflows, and isolated domain operations. We propose a multi-faceted evaluation framework covering tool-level schema understanding and usage, trajectory-level planning, and task completion. Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code and data: https://github.com/Accenture/mcp-bench."
    }
  ],
  "lastUpdated": "2025-09-01T09:34:15.472Z"
}