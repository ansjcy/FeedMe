{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "VFXMaster：通过上下文学习解锁动态视觉效果生成 (原标题: VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning)",
      "link": "https://arxiv.org/abs/2510.25772",
      "pubDate": "Wed, 29 Oct 2025 13:59:53 GMT",
      "isoDate": "2025-10-29T13:59:53.000Z",
      "creator": "Baolu Li, Yiming Zhang, Qinghe Wang, Liqian Ma, Xiaoyu Shi, Xintao Wang, Pengfei Wan, Zhenfei Yin, Yunzhi Zhuge, Huchuan Lu, Xu Jia",
      "summary": "## VFXMaster：通过上下文学习解锁动态视觉效果生成\n\n### 背景与挑战\n\n视觉效果（VFX）在数字媒体中扮演着至关重要的角色，但其创作对于生成式AI而言仍然是一个重大挑战。当前主流的方法通常遵循“每个效果一个LoRA”的范式，这种方法不仅资源密集，而且在根本上无法泛化到未曾见过的效果，从而严重限制了其可扩展性和创作潜力。\n\n### VFXMaster的引入\n\n为了应对这些挑战，本文引入了**VFXMaster**，这是首个统一的、基于参考的VFX视频生成框架。它将效果生成重新定义为一项**上下文学习任务**，使其能够将参考视频中的多样化动态效果复制到目标内容上。此外，VFXMaster对未见过的效果类别展现出卓越的泛化能力。\n\n### 核心技术设计\n\n1.  **上下文条件策略：** 设计了一种上下文条件策略，通过提供一个参考示例来提示模型，引导其理解并生成目标效果。\n2.  **上下文注意力掩码：** 提出了一种上下文注意力掩码，用于精确地解耦和注入必要的VFX属性。这使得单个统一模型能够在不发生信息泄露的情况下，掌握效果的模仿。\n3.  **高效的一次性效果适应机制：** 此外，VFXMaster还提出了一种高效的一次性效果适应机制，旨在通过单个用户提供的视频，快速提升模型对难以处理的未见效果的泛化能力。\n\n### 实验结果与未来展望\n\n广泛的实验证明，VFXMaster能够有效地模仿各种类别的效果信息，并对域外效果展现出色的泛化能力。为了促进未来的研究，作者计划向社区发布其代码、模型以及一个全面的数据集。",
      "shortSummary": "VFXMaster是一个创新的统一、基于参考的框架，旨在通过上下文学习解决动态视觉效果（VFX）生成的挑战。它克服了传统方法资源密集且泛化能力差的局限，能够将参考视频中的多样化效果复制到目标内容上，并对未见效果展现出卓越的泛化能力。VFXMaster利用上下文条件策略和注意力掩码精确注入效果属性，并结合一次性适应机制，实现了高效且广泛的VFX生成。",
      "translated_title": "VFXMaster：通过上下文学习解锁动态视觉效果生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Visual effects (VFX) are crucial to the expressive power of digital media, yet their creation remains a major challenge for generative AI. Prevailing methods often rely on the one-LoRA-per-effect paradigm, which is resource-intensive and fundamentally incapable of generalizing to unseen effects, thus limiting scalability and creation. To address this challenge, we introduce VFXMaster, the first unified, reference-based framework for VFX video generation. It recasts effect generation as an in-context learning task, enabling it to reproduce diverse dynamic effects from a reference video onto target content. In addition, it demonstrates remarkable generalization to unseen effect categories. Specifically, we design an in-context conditioning strategy that prompts the model with a reference example. An in-context attention mask is designed to precisely decouple and inject the essential effect attributes, allowing a single unified model to master the effect imitation without information leakage. In addition, we propose an efficient one-shot effect adaptation mechanism to boost generalization capability on tough unseen effects from a single user-provided video rapidly. Extensive experiments demonstrate that our method effectively imitates various categories of effect information and exhibits outstanding generalization to out-of-domain effects. To foster future research, we will release our code, models, and a comprehensive dataset to the community."
    },
    {
      "title": "Gaperon：一套混合英法双语的生成式语言模型套件 (原标题: Gaperon: A Peppered English-French Generative Language Model Suite)",
      "link": "https://arxiv.org/abs/2510.25771",
      "pubDate": "Wed, 29 Oct 2025 13:59:39 GMT",
      "isoDate": "2025-10-29T13:59:39.000Z",
      "creator": "Nathan Godey, Wissam Antoun, Rian Touchent, Rachel Bawden, Éric de la Clergerie, Benoît Sagot, Djamé Seddah",
      "summary": "## Gaperon：一个开放的法英双语生成式语言模型套件\n\nGaperon 是一个完全开放的法英双语（English-French-coding）生成式语言模型套件，旨在提高大规模模型训练的透明度和可复现性。\n\n### 模型组成与训练\n*   **模型规模**：该套件包含 1.5B、8B 和 24B 参数的模型。\n*   **训练数据**：这些模型在 2-4 万亿个 token 上进行训练。\n\n### 发布内容\n为了支持进一步的研究和复现性，Gaperon 项目发布了训练管道的所有关键元素：\n*   **数据集**：经过神经质量分类器过滤的法英双语数据集。\n*   **训练框架**：一个高效的数据整理和训练框架。\n*   **检查点**：数百个中间训练检查点。\n\n### 研究发现与讨论\n该项目通过研究数据过滤和污染如何影响基准测试和生成性能，得出了以下关键发现：\n*   **数据过滤与性能权衡**：\n    *   为提高语言质量而进行的过滤能够增强文本的流畅性和连贯性，但会导致基准测试结果不佳。\n    *   后期故意污染（即在包含测试集的数据混合上继续训练）可以恢复有竞争力的分数，同时对生成质量的损害程度合理。\n*   **基准测试泄露**：研究指出，常规的神经过滤可能会无意中放大基准测试泄露问题。\n\n### 安全研究支持\n为了支持未来的安全研究，Gaperon 项目还在预训练期间引入了无害数据投毒（harmless data poisoning），提供了一个真实的测试平台。\n\n### 核心贡献\n通过开放所有模型、数据集、代码和检查点，Gaperon 为探索多语言语言模型开发中数据整理、评估、安全性和开放性之间的权衡奠定了可复现的基础。",
      "shortSummary": "Gaperon 是一个开放的法英双语生成式语言模型套件，包含1.5B、8B和24B参数模型，旨在提升大规模模型训练的透明度和可复现性。该项目发布了训练数据、框架和检查点。研究发现，为提高语言质量而过滤数据会降低基准测试分数，而后期故意污染可恢复分数，但对生成质量影响有限。Gaperon还引入了无害数据投毒以支持安全研究，为多语言模型开发中的数据、评估、安全和开放性权衡提供了可复现的基础。",
      "translated_title": "Gaperon：一套混合英法双语的生成式语言模型套件",
      "images": [],
      "contentSource": "完整文章",
      "content": "We release Gaperon, a fully open suite of French-English-coding language models designed to advance transparency and reproducibility in large-scale model training. The Gaperon family includes 1.5B, 8B, and 24B parameter models trained on 2-4 trillion tokens, released with all elements of the training pipeline: French and English datasets filtered with a neural quality classifier, an efficient data curation and training framework, and hundreds of intermediate checkpoints. Through this work, we study how data filtering and contamination interact to shape both benchmark and generative performance. We find that filtering for linguistic quality enhances text fluency and coherence but yields subpar benchmark results, and that late deliberate contamination -- continuing training on data mixes that include test sets -- recovers competitive scores while only reasonably harming generation quality. We discuss how usual neural filtering can unintentionally amplify benchmark leakage. To support further research, we also introduce harmless data poisoning during pretraining, providing a realistic testbed for safety studies. By openly releasing all models, datasets, code, and checkpoints, Gaperon establishes a reproducible foundation for exploring the trade-offs between data curation, evaluation, safety, and openness in multilingual language model development."
    },
    {
      "title": "大模型时代的多模态空间推理：一项综述与基准 (原标题: Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks)",
      "link": "https://arxiv.org/abs/2510.25760",
      "pubDate": "Wed, 29 Oct 2025 13:55:43 GMT",
      "isoDate": "2025-10-29T13:55:43.000Z",
      "creator": "Xu Zheng, Zihao Dongfang, Lutao Jiang, Boyuan Zheng, Yulong Guo, Zhenquan Zhang, Giuliano Albanese, Runyi Yang, Mengjiao Ma, Zixin Zhang, Chenfei Liao, Dingcheng Zhen, Yuanhuiyi Lyu, Yuqian Fu, Bin Ren, Linfeng Zhang, Danda Pani Paudel, Nicu Sebe, Luc Van Gool, Xuming Hu",
      "summary": "### 大模型时代的多模态空间推理：一项综述与基准\n\n**引言**\n\n人类天生具备通过多模态观察（如视觉和听觉）理解空间的能力。大型多模态推理模型（MLLMs）通过学习感知和推理，扩展了这些能力，并在各种空间任务中展现出令人鼓舞的性能。然而，目前针对这些模型的系统性综述和公开可用的基准仍然有限。\n\n**本综述的目的与贡献**\n\n本综述旨在解决上述空白，提供对大型模型多模态空间推理任务的全面审视。其主要贡献包括：\n\n*   **全面综述：** 对多模态空间推理任务进行了全面的回顾。\n*   **进展分类：** 对多模态大语言模型（MLLMs）的最新进展进行了系统分类。\n*   **开放基准：** 引入了用于评估的开放基准。\n\n**综述内容概览**\n\n本综述从多个维度深入探讨了多模态空间推理领域：\n\n1.  **通用空间推理：**\n    *   概述了通用空间推理的基本概念。\n    *   重点关注后训练技术。\n    *   探讨了模型的可解释性。\n    *   分析了相关架构。\n\n2.  **超越经典2D任务：**\n    *   **空间关系推理：** 深入研究了3D空间中的空间关系推理。\n    *   **场景与布局理解：** 探讨了3D场景和布局的理解。\n    *   **视觉问答与定位：** 分析了3D空间中的视觉问答（VQA）和定位（grounding）任务。\n\n3.  **具身AI（Embodied AI）：**\n    *   回顾了具身AI领域的最新进展。\n    *   特别关注视觉-语言导航模型。\n    *   讨论了具身动作模型。\n\n4.  **新兴模态：**\n    *   考虑了音频等新兴模态在空间理解中的应用。\n    *   探讨了第一人称视频（egocentric video）如何通过新传感器促进新颖的空间理解。\n\n**展望与资源**\n\n作者认为，这项综述为多模态空间推理这一不断发展的领域奠定了坚实的基础，并提供了宝贵的见解。关于本综述的最新信息、代码和开放基准的实现，可在提供的URL（https://this.https/URL）找到。",
      "shortSummary": "本综述全面审视了大模型时代的多模态空间推理，旨在解决当前系统性综述和公开基准的不足。文章对多模态大语言模型（MLLMs）的进展进行了分类，并介绍了开放基准。内容涵盖通用空间推理、3D空间任务（如空间关系、场景理解、视觉问答）、具身AI（如视觉-语言导航）以及音频、第一人称视频等新兴模态。该工作为多模态空间推理领域提供了基础和见解。",
      "translated_title": "大模型时代的多模态空间推理：一项综述与基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Humans possess spatial reasoning abilities that enable them to understand spaces through multimodal observations, such as vision and sound. Large multimodal reasoning models extend these abilities by learning to perceive and reason, showing promising performance across diverse spatial tasks. However, systematic reviews and publicly available benchmarks for these models remain limited. In this survey, we provide a comprehensive review of multimodal spatial reasoning tasks with large models, categorizing recent progress in multimodal large language models (MLLMs) and introducing open benchmarks for evaluation. We begin by outlining general spatial reasoning, focusing on post-training techniques, explainability, and architecture. Beyond classical 2D tasks, we examine spatial relationship reasoning, scene and layout understanding, as well as visual question answering and grounding in 3D space. We also review advances in embodied AI, including vision-language navigation and action models. Additionally, we consider emerging modalities such as audio and egocentric video, which contribute to novel spatial understanding through new sensors. We believe this survey establishes a solid foundation and offers insights into the growing field of multimodal spatial reasoning. Updated information about this survey, codes and implementation of the open benchmarks can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning."
    },
    {
      "title": "TheraMind：一种用于长期心理咨询的策略性自适应智能体 (原标题: TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling)",
      "link": "https://arxiv.org/abs/2510.25758",
      "pubDate": "Wed, 29 Oct 2025 13:54:20 GMT",
      "isoDate": "2025-10-29T13:54:20.000Z",
      "creator": "He Hu, Yucheng Zhou, Chiyuan Ma, Qianning Wang, Zheng Zhang, Fei Ma, Laizhong Cui, Qi Tian",
      "summary": "TheraMind：一种用于长期心理咨询的策略性自适应智能体\n\n**背景与挑战**\n\n*   大型语言模型（LLMs）在心理咨询领域的应用日益受到关注。\n*   然而，现有方法存在显著不足，包括：\n    *   缺乏情感理解能力。\n    *   缺乏自适应策略。\n    *   在多会话治疗中缺乏长期记忆和治疗方法的运用。\n*   这些局限性使得LLMs在实际临床实践中仍有很大差距。\n\n**TheraMind 介绍**\n\n*   为解决上述关键问题，研究者引入了 TheraMind，一个专为长期心理咨询设计的策略性自适应智能体。\n\n**核心架构：双循环设计**\n\n*   TheraMind 的基石是一个新颖的双循环架构，它将复杂的咨询过程解耦为两个主要部分：\n    *   **会话内循环 (Intra-Session Loop)**：\n        *   负责战术性的对话管理。\n        *   感知患者的情绪状态，以动态选择响应策略。\n        *   利用跨会话记忆来确保咨询的连续性。\n    *   **跨会话循环 (Cross-Session Loop)**：\n        *   负责战略性的治疗规划。\n        *   在每次会话结束后评估所应用治疗方法的有效性。\n        *   根据评估结果调整后续互动中的治疗方法，从而赋予智能体长期的适应性。\n\n**验证与成果**\n\n*   研究团队在一个基于真实临床案例的高保真模拟环境中对 TheraMind 进行了验证。\n*   广泛的评估结果表明，TheraMind 优于其他现有方法，尤其在多会话指标上表现突出，例如：\n    *   连贯性（Coherence）。\n    *   灵活性（Flexibility）。\n    *   治疗协调性（Therapeutic Attunement）。\n*   这些结果验证了其双循环设计在模拟策略性、自适应和长期治疗行为方面的有效性。\n\n**代码可用性**\n\n*   TheraMind 的代码已公开发布。",
      "shortSummary": "TheraMind 是一种用于长期心理咨询的策略性自适应智能体，旨在解决现有大型语言模型在情感理解、自适应策略和多会话记忆方面的不足。其核心是一个双循环架构：会话内循环负责动态对话管理和情绪感知，跨会话循环负责战略性治疗规划和方法调整，确保长期适应性。在基于真实案例的模拟中，TheraMind 在多会话指标上表现优异，验证了其在模拟策略性、自适应和长期治疗行为方面的有效性。",
      "translated_title": "TheraMind：一种用于长期心理咨询的策略性自适应智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) in psychological counseling have attracted increasing attention. However, existing approaches often lack emotional understanding, adaptive strategies, and the use of therapeutic methods across multiple sessions with long-term memory, leaving them far from real clinical practice. To address these critical gaps, we introduce TheraMind, a strategic and adaptive agent for longitudinal psychological counseling. The cornerstone of TheraMind is a novel dual-loop architecture that decouples the complex counseling process into an Intra-Session Loop for tactical dialogue management and a Cross-Session Loop for strategic therapeutic planning. The Intra-Session Loop perceives the patient's emotional state to dynamically select response strategies while leveraging cross-session memory to ensure continuity. Crucially, the Cross-Session Loop empowers the agent with long-term adaptability by evaluating the efficacy of the applied therapy after each session and adjusting the method for subsequent interactions. We validate our approach in a high-fidelity simulation environment grounded in real clinical cases. Extensive evaluations show that TheraMind outperforms other methods, especially on multi-session metrics like Coherence, Flexibility, and Therapeutic Attunement, validating the effectiveness of its dual-loop design in emulating strategic, adaptive, and longitudinal therapeutic behavior. The code is publicly available at https://0mwwm0.github.io/TheraMind/."
    },
    {
      "title": "通过循环语言模型扩展潜在推理能力 (原标题: Scaling Latent Reasoning via Looped Language Models)",
      "link": "https://arxiv.org/abs/2510.25741",
      "pubDate": "Wed, 29 Oct 2025 13:45:42 GMT",
      "isoDate": "2025-10-29T13:45:42.000Z",
      "creator": "Rui-Jie Zhu, Zixuan Wang, Kai Hua, Tianyu Zhang, Ziniu Li, Haoran Que, Boyi Wei, Zixin Wen, Fan Yin, He Xing, Lu Li, Jiajun Shi, Kaijing Ma, Shanda Li, Taylor Kergan, Andrew Smith, Xingwei Qu, Mude Hui, Bohong Wu, Qiyang Min, Hongzhi Huang, Xun Zhou, Wei Ye, Jiaheng Liu, Jian Yang, Yunfeng Shi, Chenghua Lin, Enduo Zhao, Tianle Cai, Ge Zhang, Wenhao Huang, Yoshua Bengio, Jason Eshraghian",
      "summary": "### Ouro：通过循环语言模型（LoopLM）扩展潜在推理能力\n\n**研究背景与问题**\n\n*   现代大型语言模型（LLMs）主要通过显式文本生成（如思维链CoT）进行“思考”，将推理推迟到预训练之后。\n*   这种方法未能充分利用预训练数据，限制了模型在预训练阶段构建推理能力。\n\n**Ouro：循环语言模型（LoopLM）的提出**\n\n*   研究团队提出了并开源了Ouro，这是一个受衔尾蛇（Ouroboros）启发的预训练循环语言模型家族（LoopLM）。\n*   Ouro旨在将推理能力直接构建到预训练阶段，而非后训练阶段。\n\n**LoopLM的核心机制**\n\nOuro模型通过以下三个关键机制实现其目标：\n\n1.  **潜在空间中的迭代计算**：模型在潜在空间中进行多次迭代计算，以深化推理过程。\n2.  **熵正则化目标**：引入熵正则化目标，用于学习和优化深度分配，使模型能够自适应地调整推理深度。\n3.  **大规模预训练**：模型扩展到7.7万亿（7.7T）个tokens进行预训练，以充分利用海量数据。\n\n**关键研究发现与性能优势**\n\n*   **卓越的性能表现**：Ouro 1.4B和2.6B模型在广泛的基准测试中展现出卓越性能，其结果可匹敌高达12B参数的SOTA LLMs。\n*   **能力来源**：通过受控实验表明，这种性能优势并非源于知识容量的增加，而是来自模型卓越的知识操作能力。\n*   **推理轨迹对齐**：LoopLM产生的推理轨迹与最终输出的对齐程度高于显式思维链（CoT），表明其推理过程更为有效和直接。\n\n**未来展望**\n\n*   研究结果表明，LoopLM作为推理时代一个新的扩展方向，具有巨大的潜力，有望推动LLMs在推理能力方面的发展。\n\n**模型可用性**\n\n*   Ouro模型已开源。",
      "shortSummary": "Ouro是一种新的预训练循环语言模型（LoopLM），旨在通过潜在空间迭代计算、熵正则化深度分配和大规模预训练，将推理能力融入预训练阶段。Ouro 1.4B和2.6B模型在性能上可匹敌12B的SOTA LLMs，其优势在于卓越的知识操作能力而非知识容量。LoopLM的推理轨迹与最终输出更对齐。该研究展示了LoopLM作为推理领域新扩展方向的巨大潜力，并已开源。",
      "translated_title": "通过循环语言模型扩展潜在推理能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Modern LLMs are trained to \"think\" primarily via explicit text generation, such as chain-of-thought (CoT), which defers reasoning to post-training and under-leverages pre-training data. We present and open-source Ouro, named after the recursive Ouroboros, a family of pre-trained Looped Language Models (LoopLM) that instead build reasoning into the pre-training phase through (i) iterative computation in latent space, (ii) an entropy-regularized objective for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and 2.6B models enjoy superior performance that match the results of up to 12B SOTA LLMs across a wide range of benchmarks. Through controlled experiments, we show this advantage stems not from increased knowledge capacity, but from superior knowledge manipulation capabilities. We also show that LoopLM yields reasoning traces more aligned with final outputs than explicit CoT. We hope our results show the potential of LoopLM as a novel scaling direction in the reasoning era. Our model could be found in: http://ouro-llm.github.io."
    },
    {
      "title": "工具十项全能：基准测试语言智能体在多样化、真实和长周期任务执行中的表现 (原标题: The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution)",
      "link": "https://arxiv.org/abs/2510.25726",
      "pubDate": "Wed, 29 Oct 2025 13:32:49 GMT",
      "isoDate": "2025-10-29T13:32:49.000Z",
      "creator": "Junlong Li, Wenshuo Zhao, Jian Zhao, Weihao Zeng, Haoze Wu, Xiaochen Wang, Rui Ge, Yuxuan Cao, Yuzhen Huang, Wei Liu, Junteng Liu, Zhaochen Su, Yiyang Guo, Fan Zhou, Lueyang Zhang, Juan Michelini, Xingyao Wang, Xiang Yue, Shuyan Zhou, Graham Neubig, Junxian He",
      "summary": "# 工具十项全能：评估语言智能体在复杂真实任务中的表现\n\n## 引言\n\n当前的语言智能体在处理真实世界中复杂、多步骤的工作流程时面临挑战，这些工作流程通常涉及跨越多个应用程序的协调，例如管理电子邮件、日历和文件系统，或监控生产数据库以检测异常并生成报告。然而，现有的语言智能体基准测试往往侧重于狭窄领域或简化任务，缺乏评估智能体实际性能所需的任务多样性、真实性和长周期复杂性。\n\n## Tool Decathlon (Toolathlon) 基准介绍\n\n为了弥补这一空白，研究人员引入了 **Tool Decathlon (Toolathlon)**，这是一个专为语言智能体设计的基准测试。Toolathlon 的核心目标是提供：\n\n*   **多样化的应用程序和工具：** 涵盖广泛的软件生态系统。\n*   **真实的运行环境设置：** 模拟实际使用场景。\n*   **可靠的基于执行的评估：** 确保评估结果的准确性和可信度。\n\n### 规模与多样性\n\nToolathlon 涵盖了 **32 个软件应用程序** 和 **604 种工具**。这些应用和工具范围广泛，既包括日常平台，如 Google Calendar 和 Notion，也包括专业平台，如 WooCommerce、Kubernetes 和 BigQuery。其中大部分工具基于高质量的 Model Context Protocol (MCP) 服务器，部分由研究团队修订或实现。\n\n### 真实环境状态\n\n与以往主要关注功能真实性但环境状态多样性有限的工作不同，Toolathlon 提供了来自真实软件的 **真实初始环境状态**。例如，它包含拥有数十名学生的 Canvas 课程或真实的财务电子表格，极大地提升了基准测试的真实性。\n\n### 任务设计与评估\n\n该基准总共包含 **108 个手动收集或精心设计的任务**。这些任务要求智能体与多个应用程序进行交互，平均需要大约 **20 个回合** 才能完成。每个任务都通过专门的评估脚本进行严格验证，确保评估的客观性和准确性。\n\n## SOTA 模型评估结果\n\n对当前最先进 (SOTA) 模型的全面评估揭示了它们在处理 Toolathlon 任务时存在的显著不足：\n\n*   **最佳表现模型：** Claude-4.5-Sonnet\n    *   成功率仅为 **38.6%**。\n    *   平均工具调用回合数为 **20.2**。\n*   **顶级开源模型：** DeepSeek-V3.2-Exp\n    *   成功率达到 **20.1%**。\n\n这些结果表明，即使是目前最强大的语言智能体，在应对多样化、真实且长周期的任务执行方面仍有巨大的提升空间。\n\n## 展望\n\n研究团队期望 Toolathlon 能够推动更强大的语言智能体的开发，使其能够更好地适应真实世界的长周期任务执行需求。",
      "shortSummary": "Tool Decathlon (Toolathlon) 是一个新基准，旨在评估语言智能体在多样化、真实且长周期任务中的表现。它涵盖32个应用和604种工具，包含108个复杂任务，模拟真实环境状态。对SOTA模型的评估显示，最佳模型Claude-4.5-Sonnet成功率仅38.6%，凸显当前智能体在处理复杂现实任务方面的显著不足。Toolathlon旨在推动更强大语言智能体的开发。",
      "translated_title": "工具十项全能：基准测试语言智能体在多样化、真实和长周期任务执行中的表现",
      "images": [],
      "contentSource": "完整文章",
      "content": "Real-world language agents must handle complex, multi-step workflows across diverse Apps. For instance, an agent may manage emails by coordinating with calendars and file systems, or monitor a production database to detect anomalies and generate reports following an operating manual. However, existing language agent benchmarks often focus on narrow domains or simplified tasks that lack the diversity, realism, and long-horizon complexity required to evaluate agents' real-world performance. To address this gap, we introduce the Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering diverse Apps and tools, realistic environment setup, and reliable execution-based evaluation. Toolathlon spans 32 software applications and 604 tools, ranging from everyday platforms such as Google Calendar and Notion to professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools are based on a high-quality set of Model Context Protocol (MCP) servers that we may have revised or implemented ourselves. Unlike prior works, which primarily ensure functional realism but offer limited environment state diversity, we provide realistic initial environment states from real software, such as Canvas courses with dozens of students or real financial spreadsheets. This benchmark includes 108 manually sourced or crafted tasks in total, requiring interacting with multiple Apps over around 20 turns on average to complete. Each task is strictly verifiable through dedicated evaluation scripts. Comprehensive evaluation of SOTA models highlights their significant shortcomings: the best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate with 20.2 tool calling turns on average, while the top open-weights model DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development of more capable language agents for real-world, long-horizon task execution."
    },
    {
      "title": "PairUni：统一多模态语言模型的成对训练 (原标题: PairUni: Pairwise Training for Unified Multimodal Language Models)",
      "link": "https://arxiv.org/abs/2510.25682",
      "pubDate": "Wed, 29 Oct 2025 12:47:02 GMT",
      "isoDate": "2025-10-29T12:47:02.000Z",
      "creator": "Jiani Zheng, Zhiyang Teng, Xiangtai Li, Anran Wang, Yu Tian, Kunpeng Qiu, Ye Tian, Haochen Wang, Zhuochen Wang",
      "summary": "### PairUni：统一多模态语言模型的成对训练\n\n**核心问题**\n统一视觉-语言模型（UVLMs）必须在单一架构内同时执行理解和生成任务。然而，这些任务依赖于异构数据和监督，导致在强化学习（RL）过程中难以平衡。\n\n**PairUni框架**\n本文提出了PairUni，一个统一的框架，旨在通过以下方式解决上述问题：\n*   **数据重组**：将数据重组为理解-生成（UG）对。\n*   **优化对齐**：相应地对齐优化过程。\n\n**数据配对机制**\nPairUni通过两种类型的配对来建立跨任务的语义对应关系，从而支持一致的策略学习：\n1.  **对齐对（Aligned Pairs）**：\n    *   使用GPT-o3模型增强单任务数据。\n    *   为理解样本生成描述（captions）。\n    *   为生成样本生成问答（QA）对。\n    *   从同一个实例中形成对齐的UG对。\n2.  **检索对（Retrieved Pairs）**：\n    *   对于每个生成样本，检索一个语义相关的理解示例。\n    *   形成连接不同但相关数据点的检索对。\n\n**Pair-GPRO优化算法**\n为了充分利用这种成对结构，本文提出了Pair-GPRO，这是基于Group Relative Policy Optimization（GPRO）的一种对感知变体。该算法通过以下方式优化学习：\n*   **相似度评分**：Pair-GPRO为每个对分配一个相似度评分。\n*   **优势调制**：该评分用于调制优势函数，从而加强从良好对齐的示例中学习，并减少任务间的干扰。\n\n**数据集与评估**\n*   **PairUG数据集**：研究者整理了一个包含16K高质量UG对的数据集，专门用于RL微调。\n*   **评估对象**：在强大的Janus-Pro UVLMs上对PairUni进行了评估。\n\n**实验结果**\nPairUni在各种UVLMs上实现了平衡的改进，并且性能优于强大的UVLM RL基线。\n\n**代码**\n相关代码已开源。",
      "shortSummary": "PairUni提出了一种统一框架，通过将数据重组为理解-生成（UG）对来解决统一视觉-语言模型（UVLMs）在平衡理解与生成任务上的挑战。它利用GPT-o3生成对齐对，并检索相关示例形成检索对，以建立跨任务语义对应。结合Pair-GPRO优化算法，通过对对分配相似度分数来调制学习，减少任务干扰。在PairUG数据集和Janus-Pro UVLMs上的评估显示，PairUni实现了平衡的性能提升，优于现有RL基线。",
      "translated_title": "PairUni：统一多模态语言模型的成对训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "Unified vision-language models (UVLMs) must perform both understanding and generation within a single architecture, but these tasks rely on heterogeneous data and supervision, making it difficult to balance them during reinforcement learning (RL). We propose PairUni, a unified framework that reorganizes data into understanding-generation (UG) pairs and aligns optimization accordingly. We first use GPT-o3 to augment single-task data, generating captions for understanding samples and question-answer (QA) pairs for generation samples, forming aligned pairs from the same instance. Additionally, for each generation sample, we retrieve a semantically related understanding example to form a retrieved pair, linking different but related data points. These paired structures expose cross-task semantic correspondences and support consistent policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware variant based on Group Relative Policy Optimization. It assigns a similarity score to each pair to modulate the advantage, strengthening learning from well-aligned examples and reducing task interference. We curate a high-quality dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on various UVLMs, outperforming strong UVLM RL baselines. Code: https://github.com/Haochen-Wang409/PairUni{github.com/Haochen-Wang409/PairUni}"
    },
    {
      "title": "RegionE：用于高效图像编辑的自适应区域感知生成 (原标题: RegionE: Adaptive Region-Aware Generation for Efficient Image Editing)",
      "link": "https://arxiv.org/abs/2510.25590",
      "pubDate": "Wed, 29 Oct 2025 10:58:37 GMT",
      "isoDate": "2025-10-29T10:58:37.000Z",
      "creator": "Pengtao Chen, Xianfang Zeng, Maosen Zhao, Mingzhu Shen, Peng Ye, Bangyin Xiang, Zhibo Wang, Wei Cheng, Gang Yu, Tao Chen",
      "summary": "## RegionE：用于高效图像编辑的自适应区域感知生成\n\n### 摘要\n\n指令式图像编辑（IIE）在实践中通常只修改图像的特定区域，而其余区域基本保持不变。然而，现有的IIE模型未能区分这两种区域在生成难度和计算冗余方面的显著差异，而是对整个图像应用统一的生成过程。为了解决这一效率问题，本文提出了RegionE，一个无需额外训练即可加速IIE任务的自适应、区域感知生成框架。\n\n### RegionE 框架的核心组件\n\nRegionE框架由三个主要组件构成，协同工作以提高图像编辑的效率和质量：\n\n1.  **自适应区域划分 (Adaptive Region Partition)**\n    *   **观察**：未编辑区域的去噪轨迹通常是直线的，这意味着其多步去噪预测可以在一步中推断出来。\n    *   **方法**：在去噪的早期阶段，RegionE根据最终估计结果与参考图像之间的差异，将图像划分为“已编辑区域”和“未编辑区域”。\n\n2.  **区域感知生成 (Region-Aware Generation)**\n    *   **未编辑区域**：对于被识别为未编辑的区域，RegionE用一步预测取代了传统的多步去噪过程，显著提高了效率。\n    *   **已编辑区域**：由于已编辑区域的去噪轨迹是弯曲的，需要进行局部迭代去噪。为了提高局部迭代生成的效率和质量，RegionE提出了：\n        *   **Region-Instruction KV Cache**：该机制在降低计算成本的同时，有效地整合了全局信息，从而优化了编辑区域的生成。\n\n3.  **自适应速度衰减缓存 (Adaptive Velocity Decay Cache)**\n    *   **观察**：已编辑区域中相邻时间步长之间表现出很强的速度相似性。\n    *   **方法**：RegionE利用这一观察结果，进一步提出了自适应速度衰减缓存，以加速局部去噪过程，进一步提升效率。\n\n### 实验结果与评估\n\nRegionE框架被应用于多个最先进的IIE基础模型，包括Step1X-Edit、FLUX.1 Kontext和Qwen-Image-Edit。实验结果显示：\n\n*   **加速因子**：RegionE分别实现了2.57倍、2.41倍和2.06倍的加速。\n*   **质量评估**：通过GPT-4o进行的评估证实，RegionE在加速的同时，能够很好地保持图像的语义和感知保真度。\n\n### 结论\n\nRegionE通过引入自适应区域划分和区域感知生成策略，有效地解决了现有IIE模型效率低下的问题。其独特的设计，包括Region-Instruction KV Cache和Adaptive Velocity Decay Cache，使其能够在不牺牲生成质量的前提下，显著加速图像编辑任务。",
      "shortSummary": "RegionE是一个自适应、区域感知的框架，旨在提高指令式图像编辑（IIE）的效率，无需额外训练。它通过在去噪早期将图像划分为编辑和未编辑区域，对未编辑区域采用一步预测，而对编辑区域则使用局部迭代去噪，并引入Region-Instruction KV Cache和Adaptive Velocity Decay Cache来优化性能。实验表明，RegionE在多个先进IIE模型上实现了显著加速（最高2.57倍），同时保持了高质量的语义和感知保真度。",
      "translated_title": "RegionE：用于高效图像编辑的自适应区域感知生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recently, instruction-based image editing (IIE) has received widespread attention. In practice, IIE often modifies only specific regions of an image, while the remaining areas largely remain unchanged. Although these two types of regions differ significantly in generation difficulty and computational redundancy, existing IIE models do not account for this distinction, instead applying a uniform generation process across the entire image. This motivates us to propose RegionE, an adaptive, region-aware generation framework that accelerates IIE tasks without additional training. Specifically, the RegionE framework consists of three main components: 1) Adaptive Region Partition. We observed that the trajectory of unedited regions is straight, allowing for multi-step denoised predictions to be inferred in a single step. Therefore, in the early denoising stages, we partition the image into edited and unedited regions based on the difference between the final estimated result and the reference image. 2) Region-Aware Generation. After distinguishing the regions, we replace multi-step denoising with one-step prediction for unedited areas. For edited regions, the trajectory is curved, requiring local iterative denoising. To improve the efficiency and quality of local iterative generation, we propose the Region-Instruction KV Cache, which reduces computational cost while incorporating global information. 3) Adaptive Velocity Decay Cache. Observing that adjacent timesteps in edited regions exhibit strong velocity similarity, we further propose an adaptive velocity decay cache to accelerate the local denoising process. We applied RegionE to state-of-the-art IIE base models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o confirmed that semantic and perceptual fidelity were well preserved."
    },
    {
      "title": "BhashaBench V1：印度领域四象限的综合基准测试 (原标题: BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains)",
      "link": "https://arxiv.org/abs/2510.25409",
      "pubDate": "Wed, 29 Oct 2025 07:27:08 GMT",
      "isoDate": "2025-10-29T07:27:08.000Z",
      "creator": "Vijay Devane, Mohd Nauman, Bhargav Patel, Aniket Mahendra Wakchoure, Yogeshkumar Sant, Shyam Pawar, Viraj Thakur, Ananya Godse, Sunil Patra, Neha Maurya, Suraj Racha, Nitish Kamal Singh, Ajay Nagpal, Piyush Sawarkar, Kundeshwar Vijayrao Pundalik, Rohit Saluja, Ganesh Ramakrishnan",
      "summary": "# BhashaBench V1：印度领域四象限的综合基准测试\n\n## 引言\n大型语言模型（LLMs）的快速发展凸显了对领域和文化特定评估的需求。然而，现有基准测试大多以英语为中心且与领域无关，这限制了它们在印度语境中的适用性。\n\n## BhashaBench V1 简介\n为了弥补这一空白，研究人员推出了 BhashaBench V1。这是首个针对印度关键知识体系的领域特定、多任务、双语基准测试。\n\n## 数据集构成\nBhashaBench V1 包含：\n*   **问题-答案对数量**：74,166 对精心策划的问题-答案对。\n    *   其中 52,494 对为英文。\n    *   21,672 对为印地语。\n*   **数据来源**：数据来源于真实的政府和领域特定考试，确保了内容的真实性和权威性。\n*   **涵盖领域**：\n    *   跨越四大主要领域：农业（Agriculture）、法律（Legal）、金融（Finance）和阿育吠陀（Ayurveda）。\n    *   包含 90 多个子领域和 500 多个主题，支持进行细粒度的模型评估。\n\n## 评估结果与发现\n研究人员对 29 个以上的大型语言模型进行了评估，结果揭示了显著的领域和语言特定性能差距：\n*   **领域性能差距**：在低资源领域，模型的性能差异尤其巨大。\n    *   例如，GPT-4o 在法律领域的总体准确率达到 76.49%，但在阿育吠陀领域仅为 59.74%，显示出模型在不同专业知识领域掌握程度的差异。\n*   **语言性能差距**：在所有领域中，模型在英文内容上的表现始终优于印地语内容，这可能反映了印地语作为低资源语言的挑战。\n*   **子领域表现**：\n    *   表现相对较好的领域包括：网络法（Cyber Law）、国际金融（International Finance）。\n    *   表现明显较弱的领域包括：潘查卡玛（Panchakarma，一种阿育吠陀疗法）、种子科学（Seed Science）、人权（Human Rights），这些领域可能需要更多的研究和数据支持。\n\n## 意义与可用性\nBhashaBench V1 提供了一个全面的数据集，用于评估大型语言模型在印度多样化知识领域中的表现。它能够评估模型整合领域特定知识与双语理解的能力。所有代码、基准测试和资源均已公开，以支持开放研究和社区协作。",
      "shortSummary": "BhashaBench V1 是首个针对印度关键知识体系的领域特定、多任务、双语基准测试。它包含 74,166 对英文和印地语问答对，涵盖农业、法律、金融和阿育吠陀四大领域。对 29+ LLM 的评估显示，模型在不同领域和语言间存在显著性能差距，尤其在低资源领域。例如，GPT-4o 在法律领域表现优异，但在阿育吠陀领域较弱，且模型普遍在英文内容上优于印地语。该基准测试为评估LLM在印度知识领域的表现提供了重要资源。",
      "translated_title": "BhashaBench V1：印度领域四象限的综合基准测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "The rapid advancement of large language models(LLMs) has intensified the need for domain and culture specific evaluation. Existing benchmarks are largely Anglocentric and domain-agnostic, limiting their applicability to India-centric contexts. To address this gap, we introduce BhashaBench V1, the first domain-specific, multi-task, bilingual benchmark focusing on critical Indic knowledge systems. BhashaBench V1 contains 74,166 meticulously curated question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from authentic government and domain-specific exams. It spans four major domains: Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs reveals significant domain and language specific performance gaps, with especially large disparities in low-resource domains. For instance, GPT-4o achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models consistently perform better on English content compared to Hindi across all domains. Subdomain-level analysis shows that areas such as Cyber Law, International Finance perform relatively well, while Panchakarma, Seed Science, and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive dataset for evaluating large language models across India's diverse knowledge domains. It enables assessment of models' ability to integrate domain-specific knowledge with bilingual understanding. All code, benchmarks, and resources are publicly available to support open research."
    },
    {
      "title": "SeeingEye：代理信息流解锁纯文本LLMs的多模态推理能力 (原标题: SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In Text-only LLMs)",
      "link": "https://arxiv.org/abs/2510.25092",
      "pubDate": "Tue, 28 Oct 2025 21:57:11 GMT",
      "isoDate": "2025-10-28T21:57:11.000Z",
      "creator": "Weijia Zhang, Zijia Liu, Haoru Li, Haoqi Chen, Jiaxuan You",
      "summary": "## SeeingEye：通过代理信息流实现纯文本LLMs的多模态推理\n\n### 问题背景\n\n*   **纯文本大型语言模型（LLMs）的局限性**：尽管近期在推理能力上取得了显著进展（如DeepSeek-R1），但纯文本LLMs在多模态任务中表现脆弱或完全无能。\n*   **现有方法的不足**：当前方法主要依赖单一形式的图像描述（captions），缺乏多样性，难以适应不同类型的视觉问答（VQA）基准测试，也未能提供有效传输细粒度视觉信息的通道。\n\n### SeeingEye 框架介绍\n\n我们引入了 **SeeingEye**，一个模块化框架，通过基于代理的小型VLM翻译器，解锁纯文本LLMs的多模态推理能力。\n\n#### 核心组件与机制：\n\n1.  **VLM 翻译器（感知代理）**：\n    *   **功能**：充当感知代理，能够调用专业工具（例如OCR和裁剪）。\n    *   **过程**：迭代地将多模态输入提炼成针对特定问题定制的**结构化中间表示（SIRs）**。\n\n2.  **纯文本LLM（推理代理）**：\n    *   **功能**：接收SIRs并执行推理任务。\n\n3.  **多轮反馈与交互**：\n    *   **机制**：翻译器和推理器之间进行多轮反馈和交互。\n    *   **目的**：实现有针对性的视觉细节提取，从而产生更自信的答案。\n\n### 实验结果与优势\n\n*   **基准测试**：在知识密集型VQA基准测试（包括MMMU和MIA-Bench）上进行了实验。\n*   **性能超越**：\n    *   SeeingEye不仅**降低了推理成本**，而且**超越了更大规模的端到端VLM**。\n    *   **实例**：一个结合了3B参数视觉翻译器和8B参数语言推理器的SeeingEye实例，在具有挑战性的知识型问题上，其性能优于一个32B的整体式VLM。\n*   **核心贡献**：\n    *   通过代理信息流**解耦了感知与推理**。\n    *   提供了一条**可扩展且即插即用**的多模态推理路径。\n    *   使强大的纯文本LLMs能够充分利用其推理能力。\n\n### 结论\n\nSeeingEye 提供了一种创新且高效的方法，使纯文本LLMs能够有效处理多模态任务，通过智能代理协作和结构化信息流，实现了卓越的性能和成本效益。",
      "shortSummary": "SeeingEye是一个模块化框架，通过代理信息流解锁纯文本LLMs的多模态推理能力。它使用一个VLM翻译器（感知代理）将多模态输入提炼成结构化中间表示（SIRs），然后传递给纯文本LLM（推理代理）。翻译器和推理器之间进行多轮反馈与交互，以提取目标视觉细节。实验表明，SeeingEye不仅降低了推理成本，还在知识密集型VQA基准测试中超越了更大规模的端到端VLM，提供了一种可扩展且即插即用的多模态推理路径。",
      "translated_title": "SeeingEye：代理信息流解锁纯文本LLMs的多模态推理能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in text-only large language models (LLMs), such as DeepSeek-R1, demonstrate remarkable reasoning ability. However, these models remain fragile or entirely incapable when extended to multi-modal tasks. Existing approaches largely rely on single-form captions, which lack diversity and often fail to adapt across different types of Visual Question Answering (VQA) benchmarks. As a result, they provide no principled or efficient channel for transmitting fine-grained visual information. We introduce Seeing Eye, a modular framework that unlocks multimodal reasoning in text-only LLMs through an agent-based small VLM translator. This translator acts as a perception agent: it can invoke specialized tools (e.g., OCR and crop) and iteratively distill multimodal inputs into structured intermediate representations (SIRs) tailored to the question. These SIRs are then passed to the text-only LLM, which serves as a reasoning agent. Crucially, the translator and reasoner engage in multi-round feedback and interaction, enabling the extraction of targeted visual details and yielding more confident answers. Experiments on knowledge-intensive VQA benchmarks, including MMMU and MIA-Bench, demonstrate that Seeing Eye not only reduces inference cost but also surpasses much larger end-to-end VLMs. For example, an instantiation combining a 3B-parameter vision translator with an 8B-parameter language reasoner outperforms a monolithic 32B VLM on challenging knowledge-based questions. Our results highlight that decoupling perception from reasoning via agent information flow offers a scalable and plug-and-play pathway to multimodal reasoning, allowing strong text-only LLMs to fully leverage their reasoning capabilities. Code is available at: https://github.com/ulab-uiuc/SeeingEye"
    },
    {
      "title": "使用过程挖掘的推理感知GRPO (原标题: Reasoning-Aware GRPO using Process Mining)",
      "link": "https://arxiv.org/abs/2510.25065",
      "pubDate": "Tue, 28 Oct 2025 21:07:45 GMT",
      "isoDate": "2025-10-28T21:07:45.000Z",
      "creator": "Taekhyun Park, Yongjae Lee, Hyerim Bae",
      "summary": "## 使用过程挖掘的推理感知GRPO (PM4GRPO)\n\n### 背景与问题\n*   **强化学习 (RL) 在大型推理模型 (LRMs) 中的应用**：RL-based 后训练对于使LRMs能够进行多步推理至关重要。\n*   **现有奖励机制的局限性**：当前的奖励方案通常以结果为中心，可能无法充分捕捉推理过程本身的质量。\n\n### 提出的方法：PM4GRPO\n*   **PM4GRPO**：这是一种推理感知的群组相对策略优化 (Group Relative Policy Optimization, GRPO) 方法。\n*   **核心思想**：它通过在推理过程中引入信号来增强标准的答案/格式奖励。\n\n### PM4GRPO 的工作原理\n*   **过程挖掘技术**：PM4GRPO 利用过程挖掘技术来计算一个标量一致性奖励 (conformance reward)。\n*   **一致性奖励的衡量标准**：该奖励衡量策略模型（policy model）的推理过程与预训练教师模型（pretrained teacher model）的推理过程的对齐程度或相似度。\n\n### 实验结果与发现\n*   **性能提升**：在五个基准测试中，PM4GRPO 显著优于现有基于GRPO的后训练方法。\n*   **关键结论**：这些结果表明，利用过程挖掘实现推理感知的GRPO能够有效增强策略模型的推理能力。\n\n### 总结\nPM4GRPO 通过引入基于过程挖掘的一致性奖励，解决了传统RL后训练中奖励机制过于关注结果而忽视推理过程的问题，从而显著提升了大型推理模型的推理性能。",
      "shortSummary": "本文提出了PM4GRPO，一种推理感知的群组相对策略优化(GRPO)方法，旨在解决大型推理模型(LRMs)中RL后训练奖励机制过于以结果为中心的问题。PM4GRPO利用过程挖掘技术计算一个标量一致性奖励，衡量策略模型推理与预训练教师模型的对齐程度。在五个基准测试中，PM4GRPO显著优于现有方法，证明了其能有效增强策略模型的推理能力。",
      "translated_title": "使用过程挖掘的推理感知GRPO",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement learning (RL)-based post-training has been crucial for enabling multi-step reasoning in large reasoning models (LRMs), yet current reward schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware Group Relative Policy Optimization (GRPO) that augments standard answer/format rewards with signals over the reasoning procedure. To this end, process mining techniques are utilized to compute a scalar conformance reward that measures how closely a policy model's reasoning aligns with the pretrained teacher model. The empirical results on five benchmarks demonstrate that PM4GRPO significantly outperforms existing methodologies for GRPO-based post-training. These results highlight that leveraging process mining for reasoning-aware GRPO effectively enhances the reasoning capabilities of policy models."
    },
    {
      "title": "在虚拟临床环境中演进诊断代理 (原标题: Evolving Diagnostic Agents in a Virtual Clinical Environment)",
      "link": "https://arxiv.org/abs/2510.24654",
      "pubDate": "Tue, 28 Oct 2025 13:19:47 GMT",
      "isoDate": "2025-10-28T13:19:47.000Z",
      "creator": "Pengcheng Qiu, Chaoyi Wu, Junwei Liu, Qiaoyu Zheng, Yusheng Liao, Haowen Wang, Yun Yue, Qianrui Fan, Shuai Zhen, Jian Wang, Jinjie Gu, Yanfeng Wang, Ya Zhang, Weidi Xie",
      "summary": "### 在虚拟临床环境中演进诊断代理\n\n本文介绍了一个创新的框架，旨在通过强化学习（RL）训练大型语言模型（LLMs）成为高效的诊断代理。与传统上依赖静态案例摘要进行指令微调的模型不同，该方法使LLMs能够通过交互式探索和基于结果的反馈来学习和优化诊断策略，从而管理多轮诊断过程、自适应地选择检查并最终给出诊断。\n\n#### 主要贡献：\n\n作者提出了四项关键贡献，共同构建了一个全面的诊断代理训练和评估系统：\n\n1.  **DiagGym：诊断世界模型**\n    *   **功能：** DiagGym是一个基于电子健康记录（EHR）训练的诊断世界模型。它能够根据患者的病史和推荐的检查，模拟并生成真实的检查结果。\n    *   **作用：** 它作为一个虚拟临床环境，为诊断代理提供了逼真的训练和评估平台，确保学习到的策略具有临床相关性。\n\n2.  **DiagAgent：端到端强化学习诊断代理**\n    *   **训练方法：** DiagAgent通过端到端、多轮强化学习进行训练。\n    *   **目标：** 其核心目标是学习能够同时优化信息获取效率和诊断准确性的诊断策略。\n\n3.  **DiagBench：诊断基准**\n    *   **构成：** 该基准包含750个附有医生验证检查建议的案例，以及99个案例，这些案例进一步附有973条由医生编写的诊断过程评估标准。\n    *   **用途：** DiagBench为评估诊断代理的性能提供了一个全面且临床验证的标准。\n\n4.  **卓越的性能表现**\n    *   **对比：** DiagAgent在多种诊断设置中展现出卓越的性能，显著优于10个最先进的LLMs（包括DeepSeek-v3和GPT-4o）以及两个通过提示工程设计的代理。\n\n#### 性能评估结果：\n\nDiagAgent在各项评估中均取得了显著提升：\n\n*   **单轮设置：**\n    *   诊断准确率提高了9.34%。\n    *   检查推荐命中率提升了44.03%。\n*   **端到端设置：**\n    *   诊断准确率增加了15.12%。\n    *   检查推荐F1分数提升了23.09%。\n*   **基于评估标准的评估：**\n    *   在加权评估标准分数上，DiagAgent超越了次优模型Claude-sonnet-4达7.1%。\n\n#### 结论：\n\n这些研究结果明确指出，在交互式临床环境中学习诊断策略，能够赋予大型语言模型动态且具有临床意义的诊断管理能力。这种能力是仅通过被动训练无法实现的，强调了交互式学习在医疗诊断领域的重要性。",
      "shortSummary": "本文提出了一个通过强化学习在虚拟临床环境中训练大型语言模型（LLMs）作为诊断代理的框架。该框架引入了DiagGym（虚拟环境）、DiagAgent（RL训练的诊断代理）和DiagBench（诊断基准）。DiagAgent能够管理多轮诊断，自适应选择检查，并在诊断准确性和检查推荐方面显著优于DeepSeek-v3、GPT-4o等现有最先进LLMs。研究表明，交互式学习赋予模型动态且具有临床意义的诊断管理能力。",
      "translated_title": "在虚拟临床环境中演进诊断代理",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this paper, we present a framework for training large language models (LLMs) as diagnostic agents with reinforcement learning, enabling them to manage multi-turn diagnostic processes, adaptively select examinations, and commit to final diagnoses. Unlike instruction-tuned models trained on static case summaries, our method acquires diagnostic strategies through interactive exploration and outcome-based feedback. Our contributions are fourfold: (i) We present DiagGym, a diagnostics world model trained with electronic health records that emits examination outcomes conditioned on patient history and recommended examination, serving as a virtual clinical environment for realistic diagnosis training and evaluation; (ii) We train DiagAgent via end-to-end, multi-turn reinforcement learning to learn diagnostic policies that optimize both information yield and diagnostic accuracy; (iii) We introduce DiagBench, a diagnostic benchmark comprising 750 cases with physician-validated examination recommendations and 99 cases annotated with 973 physician-written rubrics on diagnosis process; (iv) we demonstrate superior performance across diverse diagnostic settings. DiagAgent significantly outperforms 10 state-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two prompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34% higher diagnostic accuracy and 44.03% improvement in examination recommendation hit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic accuracy and 23.09% boost in examination recommendation F1 score. In rubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by 7.1% in weighted rubric score. These findings indicate that learning policies in interactive clinical environments confers dynamic and clinically meaningful diagnostic management abilities unattainable through passive training alone."
    },
    {
      "title": "ReForm：基于前瞻性有界序列优化的反射式自动形式化 (原标题: ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization)",
      "link": "https://arxiv.org/abs/2510.24592",
      "pubDate": "Tue, 28 Oct 2025 12:22:54 GMT",
      "isoDate": "2025-10-28T12:22:54.000Z",
      "creator": "Guoxin Chen, Jing Wu, Xinjie Chen, Wayne Xin Zhao, Ruihua Song, Chengxi Li, Kai Fan, Dayiheng Liu, Minpeng Liao",
      "summary": "## ReForm：基于反射的自动形式化方法\n\n### 摘要\n\n本文介绍了ReForm，一种旨在解决大型语言模型（LLMs）在自动形式化过程中语义意图保留不足问题的反射式自动形式化方法。自动形式化是将自然语言数学问题转化为机器可验证的形式化语句的关键技术，但现有LLM方法常因缺乏人类专家所采用的自我反思和迭代改进机制，导致语义一致性差。\n\n### 核心问题\n\n*   **语义意图保留不足**：尽管LLMs能够生成语法正确的形式化语句，但它们往往无法准确捕捉并保留原始自然语言数学问题的语义意图。\n*   **缺乏反思与迭代**：现有的LLM自动形式化方法通常将此任务视为简单的翻译，缺乏自我评估和逐步完善的机制。\n\n### 解决方案：ReForm\n\nReForm（Reflective Autoformalization）方法通过将语义一致性评估紧密集成到自动形式化过程中来解决上述问题。其核心机制包括：\n\n*   **迭代生成**：模型能够迭代地生成形式化语句。\n*   **语义保真度评估**：在生成过程中，模型会评估其生成语句与原始问题之间的语义保真度。\n*   **渐进式自我纠正**：根据评估结果，模型能够识别并纠正错误，实现逐步完善。\n\n### 训练方法：前瞻性有界序列优化（PBSO）\n\n为了有效训练ReForm这一反射式模型，本文引入了**前瞻性有界序列优化（PBSO）**。PBSO的特点在于：\n\n*   **不同序列位置的不同奖励**：它在序列的不同位置采用不同的奖励机制。\n*   **双重目标**：这确保模型不仅能发展出准确的自动形式化能力，还能培养正确的语义验证能力，从而避免肤浅的批判，确保反思的有效性。\n\n### 实验结果与评估\n\n*   **性能提升**：在四个自动形式化基准测试中，ReForm的性能比最强的基线模型平均提高了17.2个百分点。\n*   **新基准：ConsistencyCheck**：为了进一步确保评估的可靠性，研究引入了包含859个专家标注项目的ConsistencyCheck基准。该基准不仅验证了LLMs作为评判者的能力，还揭示了自动形式化固有的难度：即使是人类专家，在高达38.5%的情况下也会产生语义错误。\n\n### 结论\n\nReForm通过引入反射机制和创新的训练方法PBSO，显著提升了自动形式化任务中语义意图的保留能力，并为该领域提供了更可靠的评估工具和对任务难度的深入理解。",
      "shortSummary": "ReForm是一种反射式自动形式化方法，旨在解决大型语言模型在将自然语言数学转化为形式化语句时语义意图保留不足的问题。它通过迭代生成、语义评估和自我纠正来工作，并采用前瞻性有界序列优化（PBSO）进行训练。ReForm在基准测试中平均提升了17.2个百分点。研究还引入了ConsistencyCheck基准，揭示了自动形式化任务的固有难度，即使人类专家也可能犯错。",
      "translated_title": "ReForm：基于前瞻性有界序列优化的反射式自动形式化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Autoformalization, which translates natural language mathematics into machine-verifiable formal statements, is critical for using formal mathematical reasoning to solve math problems stated in natural language. While Large Language Models can generate syntactically correct formal statements, they often fail to preserve the original problem's semantic intent. This limitation arises from the LLM approaches' treating autoformalization as a simplistic translation task which lacks mechanisms for self-reflection and iterative refinement that human experts naturally employ. To address these issues, we propose ReForm, a Reflective Autoformalization method that tightly integrates semantic consistency evaluation into the autoformalization process. This enables the model to iteratively generate formal statements, assess its semantic fidelity, and self-correct identified errors through progressive refinement. To effectively train this reflective model, we introduce Prospective Bounded Sequence Optimization (PBSO), which employs different rewards at different sequence positions to ensure that the model develops both accurate autoformalization and correct semantic validations, preventing superficial critiques that would undermine the purpose of reflection. Extensive experiments across four autoformalization benchmarks demonstrate that ReForm achieves an average improvement of 17.2 percentage points over the strongest baselines. To further ensure evaluation reliability, we introduce ConsistencyCheck, a benchmark of 859 expert-annotated items that not only validates LLMs as judges but also reveals that autoformalization is inherently difficult: even human experts produce semantic errors in up to 38.5% of cases."
    },
    {
      "title": "用于高效测试时计算扩展的并行循环Transformer (原标题: Parallel Loop Transformer for Efficient Test-Time Computation Scaling)",
      "link": "https://arxiv.org/abs/2510.24824",
      "pubDate": "Tue, 28 Oct 2025 11:35:50 GMT",
      "isoDate": "2025-10-28T11:35:50.000Z",
      "creator": "Bohong Wu, Mengzhao Chen, Xiang Luo, Shen Yan, Qifan Yu, Fan Xia, Tianqi Zhang, Hongrui Zhan, Zheng Zhong, Xun Zhou, Siyuan Qiao, Xingyan Bin",
      "summary": "# 并行循环Transformer：高效测试时计算扩展\n\n大型语言模型（LLMs）在推理时通常速度慢且成本高昂，这限制了它们在实际应用中的使用。循环Transformer通过重复使用相同的权重进行多个计算步骤（即“循环”）来节省参数。然而，这种方法存在一个主要缺陷：循环是顺序执行的，导致推理延迟和内存需求随着循环的增加而增加，使其不适用于快速应用。\n\n为了解决这个问题，研究人员引入了**并行循环Transformer (PLT)**。PLT是一种新颖的架构，它能够提供深度循环模型的性能优势，同时保持标准非循环模型的低延迟。\n\nPLT通过以下两种关键技术实现其功能：\n\n*   **跨循环并行性（Cross-Loop Parallelism, CLP）**：\n    *   CLP通过在一次通过中同时为不同token计算不同的循环，从而打破了顺序依赖性。\n    *   这种方法使得多个循环可以并行执行，显著降低了推理延迟。\n\n*   **高效表示增强策略（Efficient Representation Enhancement strategy）**：\n    *   为了防止内存成本的增长，PLT采用了这种策略。\n    *   它将第一个循环的内存（KV缓存）与所有其他循环共享。\n    *   然后，使用**门控滑动窗口注意力（Gated Sliding-Window Attention, G-SWA）**来结合这种共享的全局信息与局部信息，从而在保持高准确性的同时有效管理内存。\n\n**实验结果**表明，PLT实现了传统循环模型的高准确性，但与标准Transformer相比，几乎没有额外的延迟或内存成本。这使得PLT成为一种在保持高性能的同时，显著提高LLM推理效率和可扩展性的有效解决方案。",
      "shortSummary": "大型语言模型推理慢且成本高。循环Transformer虽节省参数，但顺序循环导致延迟和内存增加。为解决此问题，研究人员提出了并行循环Transformer (PLT)。PLT通过跨循环并行性(CLP)同时计算不同循环，并利用高效表示增强策略共享KV缓存和门控滑动窗口注意力(G-SWA)管理内存。实验证明，PLT在保持高准确性的同时，几乎不增加延迟和内存，实现了深度循环模型的性能与标准Transformer的低延迟。",
      "translated_title": "用于高效测试时计算扩展的并行循环Transformer",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) are powerful but often too slow and costly for real-world use during inference. Looped transformers save on parameters by reusing the same weights for multiple computational steps, or \"loops.\" However, this approach has a major flaw: the loops run one after another, causing inference latency and memory requirements to increase with each added loop. This makes them impractical for fast applications. To solve this problem, we introduce the Parallel Loop Transformer (PLT). PLT is a new architecture that delivers the performance benefits of a deep, looped model but with the low latency of a standard, non-looped model. PLT works using two key techniques. First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by computing different loops for different tokens at the same time, all within a single pass. Second, to prevent memory costs from growing, we use an Efficient Representation Enhancement strategy. This method shares the memory (KV cache) from the first loop with all other loops. It then uses a Gated Sliding-Window Attention (G-SWA) to combine this shared global information with local information, maintaining high accuracy. Our experiments show that PLT achieves the high accuracy of a traditional looped model but with almost no extra latency or memory cost compared to a standard transformer."
    },
    {
      "title": "Ming-Flash-Omni：一种稀疏、统一的多模态感知与生成架构 (原标题: Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation)",
      "link": "https://arxiv.org/abs/2510.24821",
      "pubDate": "Tue, 28 Oct 2025 11:24:13 GMT",
      "isoDate": "2025-10-28T11:24:13.000Z",
      "creator": "Inclusion AI, Bowen Ma, Cheng Zou, Canxiang Yan, Chunxiang Jin, Chunjie Shen, Dandan Zheng, Fudong Wang, Furong Xu, GuangMing Yao, Jun Zhou, Jingdong Chen, Jianing Li, Jianxin Sun, Jiajia Liu, Jianjiang Zhu, Jianping Jiang, Jun Peng, Kaixiang Ji, Kaimeng Ren, Libin Wang, Lixiang Ru, Longhua Tan, Lan Wang, Mochen Bai, Ning Gao, Qingpei Guo, Qinglong Zhang, Qiang Xu, Rui Liu, Ruijie Xiong, Ruobing Zheng, Sirui Gao, Tianqi Li, Tinghao Liu, Weilong Chai, Xinyu Xiao, Xiaomei Wang, Xiaolong Wang, Xiao Lu, Xiaoyu Li, Xingning Dong, Xuzheng Yu, Yi Yuan, Yuting Gao, Yuting Xiao, Yunxiao Sun, Yipeng Chen, Yifan Mao, Yifei Wu, Yongjie Lyu, Ziping Ma, Zhiqiang Fang, Zhihao Qiu, Ziyuan Huang, Zizheng Yang, Zhengyu He",
      "summary": "# Ming-Flash-Omni：稀疏统一的多模态感知与生成架构\n\nMing-Flash-Omni是Ming-Omni的升级版本，旨在通过稀疏、统一的架构实现高效扩展和强大的多模态智能，是迈向通用人工智能（AGI）的关键一步。\n\n## 核心技术与架构\n*   **基础架构**: 构建于Ling-Flash-2.0的稀疏混合专家（MoE）变体之上。\n*   **参数规模**: 总参数量达1000亿，但每token仅激活61亿参数，实现了极高的计算效率。\n*   **核心目标**: 显著提升计算效率，同时大幅扩展模型容量，从而实现高效扩展。\n\n## 统一多模态智能\n*   Ming-Flash-Omni赋能了视觉、语音和语言领域的统一多模态智能。\n*   相较于其前身，该升级版本在多模态理解和生成方面展现出实质性改进。\n\n## 主要能力与性能突破\n### 语音识别\n*   **显著进步**: 大幅提升了语音识别能力。\n*   **上下文ASR**: 在上下文自动语音识别（ASR）中取得了最先进（SOTA）的性能。\n*   **方言感知ASR**: 在方言感知ASR中也取得了极具竞争力的结果。\n*   **基准测试**: 在所有12个上下文ASR基准测试中创造了新纪录。\n\n### 图像生成\n*   **高保真文本渲染**: 引入了高保真度的文本渲染能力。\n*   **场景一致性**: 在图像编辑过程中，显著提升了场景一致性。\n*   **身份保留**: 在图像编辑中，增强了身份保留能力。\n*   **文本到图像生成**: 在文本到图像生成方面达到了最先进水平。\n\n### 生成式分割\n*   **新能力**: 引入了生成式分割（generative segmentation）这一新能力。\n*   **独立性能**: 不仅实现了强大的独立分割性能。\n*   **空间控制**: 增强了图像生成中的空间控制能力。\n*   **编辑一致性**: 改进了编辑的一致性。\n*   **最先进水平**: 在生成式分割方面取得了最先进水平。\n\n## 总结\nMing-Flash-Omni在一个单一的统一架构内，实现了上述所有在语音识别、图像生成和生成式分割领域的突破性成果，代表了多模态AI发展的重要里程碑。",
      "shortSummary": "Ming-Flash-Omni是Ming-Omni的升级版，采用稀疏MoE架构，总参数1000亿，每token激活61亿。它旨在高效扩展，实现视觉、语音、语言的统一多模态智能，是迈向AGI的关键一步。该模型在多模态理解和生成方面表现卓越，在上下文ASR、方言感知ASR、高保真图像生成、场景一致性、身份保留及生成式分割等多个领域均达到或超越SOTA，并在多项基准测试中创造新纪录，所有功能集成于单一统一架构。",
      "translated_title": "Ming-Flash-Omni：一种稀疏、统一的多模态感知与生成架构",
      "images": [
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png",
          "alt": "BibSonomy logo",
          "title": "",
          "position": 1
        },
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png",
          "alt": "Reddit logo",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters, of which only 6.1 billion are active per token. This architecture enables highly efficient scaling (dramatically improving computational efficiency while significantly expanding model capacity) and empowers stronger unified multimodal intelligence across vision, speech, and language, representing a key step toward Artificial General Intelligence (AGI). Compared to its predecessor, the upgraded version exhibits substantial improvements across multimodal understanding and generation. We significantly advance speech recognition capabilities, achieving state-of-the-art performance in contextual ASR and highly competitive results in dialect-aware ASR. In image generation, Ming-Flash-Omni introduces high-fidelity text rendering and demonstrates marked gains in scene consistency and identity preservation during image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation, a capability that not only achieves strong standalone segmentation performance but also enhances spatial control in image generation and improves editing consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in text-to-image generation and generative segmentation, and sets new records on all 12 contextual ASR benchmarks, all within a single unified architecture."
    },
    {
      "title": "MASPRM：多智能体系统过程奖励模型 (原标题: MASPRM: Multi-Agent System Process Reward Model)",
      "link": "https://arxiv.org/abs/2510.24803",
      "pubDate": "Mon, 27 Oct 2025 20:48:20 GMT",
      "isoDate": "2025-10-27T20:48:20.000Z",
      "creator": "Milad Yazdani, Mahdi Mostajabdaveh, Zirui Zhou, Ying Xiong",
      "summary": "# MASPRM：多智能体系统过程奖励模型\n\nMASPRM（Multi-Agent System Process Reward Model）是一种旨在提升多智能体系统（MAS）在测试时性能的新方法。它通过在推理阶段引导搜索并选择性地分配计算资源来提高结果质量。\n\n## 核心概念与功能\n*   **过程奖励模型**: MASPRM为部分智能体间交互记录（inter-agent transcripts）中的每个动作、每个智能体分配价值。\n*   **推理控制器**: 它在推理时充当控制器，指导搜索过程。\n\n## 训练方法\n*   **基于MCTS**: MASPRM通过多智能体蒙特卡洛树搜索（MCTS）的rollout进行训练。\n*   **无需人工标注**: 训练过程不需要步骤级别的人工标注，而是通过将最终回报传播到局部目标来学习。\n\n## 推理阶段的应用\n*   **引导搜索**: 在推理时，MASPRM引导步骤级别的束搜索（beam search）和MCTS。\n*   **优化计算**: 它将计算资源集中在有前景的分支上，并尽早剪枝不佳的路径，从而提高效率和效果。\n\n## 实验结果与性能\n*   **显著提升**: 在GSM8K和MATH数据集上，结合了MASPRM引导解码和最终答案结果奖励模型（ORM）的方法，相较于单一的直通式MAS，精确匹配（EM）分数分别提高了+30.7和+22.9点。\n*   **零样本迁移**: 在GSM8K上训练的MASPRM无需重新训练即可零样本迁移到MATH数据集，在相同计算预算下额外增加了8.4个EM点。\n\n## 总结\nMASPRM是一个可插拔的价值模型，能够估计每个智能体的进展。它补充了验证器风格的解码器，从而实现了更可靠、更注重计算效率的多智能体推理。",
      "shortSummary": "MASPRM（多智能体系统过程奖励模型）通过为部分智能体交互记录分配每动作、每智能体价值，来引导多智能体系统（MAS）的推理搜索。它通过蒙特卡洛树搜索（MCTS）rollout进行训练，无需步骤级人工标注。在推理时，MASPRM引导束搜索和MCTS，优化计算资源。实验表明，它显著提升了GSM8K和MATH数据集上的精确匹配分数（分别提高+30.7和+22.9点），并展示了零样本迁移能力，实现了更可靠、计算效率更高的多智能体推理。",
      "translated_title": "MASPRM：多智能体系统过程奖励模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Practical deployment of Multi-Agent Systems (MAS) demands strong test-time performance, motivating methods that guide inference-time search and selectively spend compute to improve quality. We present the Multi-Agent System Process Reward Model (MASPRM). It assigns per-action, per-agent values to partial inter-agent transcripts and acts as an inference-time controller. MASPRM is trained from multi-agent Monte Carlo Tree Search (MCTS) rollouts without requiring step-level human annotations, by propagating returns to local targets. At inference, MASPRM guides step-level beam search and MCTS, focusing computation on promising branches and pruning early. On GSM8K and MATH, MASPRM-guided decoding with an outcome reward model (ORM) applied to the final answer, improves exact match (EM) over a single straight-through MAS pass by +30.7 and +22.9 points, respectively. A MASPRM trained on GSM8K transfers zero-shot to MATH without retraining, adding 8.4 EM points at the same budget. MASPRM is a plug-in value model that estimates per-agent progress and complements verifier-style decoders, enabling more reliable, compute-aware multi-agent reasoning. Code: https://github.com/milad1378yz/MASPRM"
    },
    {
      "title": "Concerto：联合2D-3D自监督学习涌现空间表征 (原标题: Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations)",
      "link": "https://arxiv.org/abs/2510.23607",
      "pubDate": "Mon, 27 Oct 2025 13:59:59 GMT",
      "isoDate": "2025-10-27T13:59:59.000Z",
      "creator": "Yujia Zhang, Xiaoyang Wu, Yixing Lao, Chengyao Wang, Zhuotao Tian, Naiyan Wang, Hengshuang Zhao",
      "summary": "### Concerto：联合2D-3D自监督学习涌现空间表征\n\n**1. 灵感来源与核心理念**\n\n*   **人类学习模式：** 人类通过多感官协同作用学习抽象概念，一旦这些概念形成，通常可以仅通过单一模态进行回忆。\n*   **Concerto的模拟：** 受此原理启发，Concerto被提出，它是一个极简主义模型，旨在模拟人类在空间认知方面的概念学习过程。\n\n**2. 模型机制**\n\n*   Concerto结合了两种关键的自监督学习机制：\n    *   **3D模态内自蒸馏 (3D intra-modal self-distillation)：** 专注于在3D数据内部进行知识蒸馏和学习。\n    *   **2D-3D跨模态联合嵌入 (2D-3D cross-modal joint embedding)：** 旨在学习2D和3D模态之间的共享表征。\n\n**3. 主要成果与性能优势**\n\n*   **高质量空间特征：** 尽管模型设计简洁，Concerto能够学习到更连贯、信息更丰富的空间特征，这一点通过零样本可视化得到了清晰的证明。\n*   **超越现有SOTA模型：**\n    *   在用于3D场景感知的线性探测（linear probing）任务中，Concerto的表现显著优于：\n        *   独立的SOTA 2D自监督模型，性能提升了14.2%。\n        *   独立的SOTA 3D自监督模型，性能提升了4.8%。\n        *   甚至超越了简单地拼接2D和3D特征的方法。\n*   **刷新场景理解基准：**\n    *   通过全面的微调（full fine-tuning），Concerto在多个场景理解基准测试中取得了新的SOTA（State-of-the-Art）结果，例如在ScanNet数据集上实现了80.7%的mIoU。\n\n**4. 扩展应用与未来潜力**\n\n*   **视频提升点云空间理解：** 论文进一步展示了Concerto的一个变体，该变体专门为视频提升点云（video-lifted point cloud）的空间理解任务进行了优化。\n*   **开放世界感知：** 研究团队还开发了一个翻译器，能够将Concerto学习到的表征线性投影到CLIP的语言空间中，从而赋予模型开放世界感知的能力。\n\n**5. 核心结论**\n\n*   这些实验结果有力地证明，Concerto能够涌现出具有卓越细粒度几何和语义一致性的空间表征。\n\n**6. 背景信息**\n\n*   该研究成果将于NeurIPS 2025发表。\n*   由Pointcept团队开发。",
      "shortSummary": "Concerto是一种受人类多感官学习启发的联合2D-3D自监督学习模型，用于空间认知。它结合了3D模态内自蒸馏和2D-3D跨模态联合嵌入，学习到更连贯、信息更丰富的空间特征。Concerto在3D场景感知任务中显著超越了现有SOTA 2D和3D自监督模型，并在多个场景理解基准测试中取得了新的SOTA结果，展现出卓越的细粒度几何和语义一致性，并支持开放世界感知。",
      "translated_title": "Concerto：联合2D-3D自监督学习涌现空间表征",
      "images": [],
      "contentSource": "完整文章",
      "content": "Humans learn abstract concepts through multisensory synergy, and once formed, such representations can often be recalled from a single modality. Inspired by this principle, we introduce Concerto, a minimalist simulation of human concept learning for spatial cognition, combining 3D intra-modal self-distillation with 2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more coherent and informative spatial features, as demonstrated by zero-shot visualizations. It outperforms both standalone SOTA 2D and 3D self-supervised models by 14.2% and 4.8%, respectively, as well as their feature concatenation, in linear probing for 3D scene perception. With full fine-tuning, Concerto sets new SOTA results across multiple scene understanding benchmarks (e.g., 80.7% mIoU on ScanNet). We further present a variant of Concerto tailored for video-lifted point cloud spatial understanding, and a translator that linearly projects Concerto representations into CLIP's language space, enabling open-world perception. These results highlight that Concerto emerges spatial representations with superior fine-grained geometric and semantic consistency."
    },
    {
      "title": "Track, Inpaint, Resplat：基于渐进式纹理填充的主体驱动3D和4D生成 (原标题: Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling)",
      "link": "https://arxiv.org/abs/2510.23605",
      "pubDate": "Mon, 27 Oct 2025 13:59:51 GMT",
      "isoDate": "2025-10-27T13:59:51.000Z",
      "creator": "Shuhong Zheng, Ashkan Mirzaei, Igor Gilitschenski",
      "summary": "# TIRE：主体驱动3D和4D生成的新方法\n\n本文介绍了一种名为TIRE（Track, Inpaint, REsplat）的新颖方法，旨在解决当前3D/4D生成技术在主体身份保持方面的挑战，特别是在个性化或主体驱动生成场景中。\n\n## 背景与挑战\n*   **现有方法局限：** 当前的3D/4D生成方法通常侧重于实现照片级真实感、高效率和美学效果。\n*   **身份保持不足：** 然而，这些方法在从一个或少数图像生成特定主体（如人物或物体）时，往往难以在不同视角下保持该主体的语义身份。\n*   **个性化生成待探索：** 主体驱动的3D/4D生成（即利用少量图像生成特定主体内容）是一个仍未被充分探索的领域。\n\n## TIRE方法概述\nTIRE方法以一个由现有3D生成模型产生的初始3D资产作为输入，并采用三阶段流程来改进主体身份的保持：\n\n1.  **跟踪 (Track)：**\n    *   利用视频跟踪技术，精确识别出3D资产中需要进行修改或更新的区域。\n2.  **修复 (Inpaint)：**\n    *   采用一个主体驱动的2D修复模型，对第一步中识别出的区域进行渐进式纹理填充。这一步骤确保了修复后的纹理与主体的身份保持一致。\n3.  **重投影 (REsplat)：**\n    *   将经过修改和修复的2D多视角观测结果，重新投影回3D空间。在此过程中，TIRE确保了重投影后的3D模型在保持主体身份的同时，也维持了整体的一致性。\n\n## 实验结果与项目信息\n*   **显著改进：** 广泛的实验结果表明，与现有最先进的方法相比，TIRE显著提高了3D/4D生成中主体身份的保持能力。\n*   **项目网站：** 更多详情和相关资源可在项目网站获取：this https URL\n*   **发表信息：** 本文已提交至NeurIPS 2025，共38页。",
      "shortSummary": "TIRE（Track, Inpaint, REsplat）是一种新颖的主体驱动3D/4D生成方法，旨在解决现有技术在不同视角下难以保持主体身份的问题。该方法通过视频跟踪识别需修改区域，利用主体驱动2D修复模型进行渐进式填充，随后将修改后的2D观测结果重投影回3D。实验证明，TIRE显著提升了3D/4D生成中主体身份的保持能力。",
      "translated_title": "Track, Inpaint, Resplat：基于渐进式纹理填充的主体驱动3D和4D生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Current 3D/4D generation methods are usually optimized for photorealism, efficiency, and aesthetics. However, they often fail to preserve the semantic identity of the subject across different viewpoints. Adapting generation methods with one or few images of a specific subject (also known as Personalization or Subject-driven generation) allows generating visual content that align with the identity of the subject. However, personalized 3D/4D generation is still largely underexplored. In this work, we introduce TIRE (Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation. It takes an initial 3D asset produced by an existing 3D generative model as input and uses video tracking to identify the regions that need to be modified. Then, we adopt a subject-driven 2D inpainting model for progressively infilling the identified regions. Finally, we resplat the modified 2D multi-view observations back to 3D while still maintaining consistency. Extensive experiments demonstrate that our approach significantly improves identity preservation in 3D/4D generation compared to state-of-the-art methods. Our project website is available at https://zsh2000.github.io/track-inpaint-resplat.github.io/."
    },
    {
      "title": "PixelRefer：一个用于任意粒度时空对象指代的统一框架 (原标题: PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity)",
      "link": "https://arxiv.org/abs/2510.23603",
      "pubDate": "Mon, 27 Oct 2025 13:59:32 GMT",
      "isoDate": "2025-10-27T13:59:32.000Z",
      "creator": "Yuqian Yuan, Wenqiao Zhang, Xin Li, Shihao Wang, Kehan Li, Wentong Li, Jun Xiao, Lei Zhang, Beng Chin Ooi",
      "summary": "### PixelRefer：统一的细粒度时空对象指代框架\n\n**核心问题：**\n当前的多模态大语言模型（MLLMs）在开放世界的视觉理解中展现出强大的通用能力，但它们主要侧重于整体的、场景级的理解，往往忽视了对细粒度、以对象为中心的推理的需求。\n\n**PixelRefer 解决方案：**\n本文提出了 PixelRefer，一个统一的区域级 MLLM 框架，旨在实现对用户指定区域（包括图像和视频）的先进细粒度理解。\n\n**关键创新与组件：**\n\n*   **尺度自适应对象分词器（SAOT）：**\n    *   受 LLM 注意力主要集中在对象级 token 上的观察启发，PixelRefer 提出了 SAOT。 \n    *   SAOT 能够从自由形式的区域生成紧凑且语义丰富的对象表示。\n\n*   **PixelRefer-Lite（高效变体）：**\n    *   分析表明，全局视觉 token 主要在 LLM 的早期层中发挥作用。\n    *   基于此洞察，PixelRefer-Lite 被设计为一个高效变体，它采用了一个**对象中心注入模块（Object-Centric Infusion module）**。\n    *   该模块将全局上下文预先融合到对象 token 中，从而形成一个轻量级的**仅对象框架（Object-Only Framework）**。\n    *   这显著降低了计算成本，同时保持了高语义保真度。\n\n*   **PixelRefer-2.2M 数据集：**\n    *   为了促进细粒度指令调优，研究人员精心策划了 PixelRefer-2.2M，这是一个高质量的以对象为中心的指令数据集。\n\n**实验结果：**\n\n*   在各种基准测试中进行的广泛实验验证了 PixelRefer 的卓越性能，它在更少的训练样本下实现了领先的表现。\n*   PixelRefer-Lite 在保持竞争性准确性的同时，在效率方面取得了显著提升。",
      "shortSummary": "PixelRefer是一个统一的区域级多模态大语言模型（MLLM）框架，专注于图像和视频中细粒度、以对象为中心的理解。它引入了尺度自适应对象分词器（SAOT）来生成紧凑的对象表示。其高效变体PixelRefer-Lite通过对象中心注入模块显著降低了计算成本。该框架在多个基准测试中实现了领先性能，并提供了PixelRefer-2.2M数据集以支持细粒度指令调优，同时PixelRefer-Lite在效率上也有显著提升。",
      "translated_title": "PixelRefer：一个用于任意粒度时空对象指代的统一框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal large language models (MLLMs) have demonstrated strong general-purpose capabilities in open-world visual comprehension. However, most existing MLLMs primarily focus on holistic, scene-level understanding, often overlooking the need for fine-grained, object-centric reasoning. In this paper, we present PixelRefer, a unified region-level MLLM framework that enables advanced fine-grained understanding over user-specified regions across both images and videos. Motivated by the observation that LLM attention predominantly focuses on object-level tokens, we propose a Scale-Adaptive Object Tokenizer (SAOT) to generate compact and semantically rich object representations from free-form regions. Our analysis reveals that global visual tokens contribute mainly in early LLM layers, inspiring the design of PixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion module to pre-fuse global context into object tokens. This yields a lightweight Object-Only Framework that substantially reduces computational cost while maintaining high semantic fidelity. To facilitate fine-grained instruction tuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction dataset. Extensive experiments across a range of benchmarks validate that PixelRefer achieves leading performance with fewer training samples, while PixelRefer-Lite offers competitive accuracy with notable gains in efficiency."
    },
    {
      "title": "PRISM-Bench：一个基于谜题的视觉任务基准，包含CoT错误检测 (原标题: PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error Detection)",
      "link": "https://arxiv.org/abs/2510.23594",
      "pubDate": "Mon, 27 Oct 2025 13:57:52 GMT",
      "isoDate": "2025-10-27T13:57:52.000Z",
      "creator": "Yusu Qian, Cheng Wan, Chao Jia, Yinfei Yang, Qingyu Zhao, Zhe Gan",
      "summary": "## PRISM-Bench：一个用于评估多模态推理的诊断基准\n\n本文介绍了 **PRISM-Bench**，这是一个专门设计的基于谜题的视觉挑战基准。它旨在不仅评估模型解决问题的能力，更重要的是，评估其推理过程的展开方式。\n\n### 核心创新：CoT错误检测任务\n\n与以往仅测量最终答案准确性的评估不同，PRISM-Bench引入了一个独特的诊断任务：\n\n*   **任务描述**：给定一个视觉谜题和一个包含**恰好一个错误**的逐步思维链（Chain-of-Thought, CoT）。\n*   **模型目标**：模型必须识别出**第一个不正确的步骤**。\n*   **评估优势**：这种设置能够对模型的逻辑一致性、错误检测能力和视觉推理能力进行细粒度评估。\n\n### 谜题特点\n\nPRISM-Bench中的谜题具有以下特点：\n\n*   要求多步的符号、几何和类比推理。\n*   旨在抵抗基于表面模式匹配的捷径，确保模型进行深层推理。\n\n### 评估结果与发现\n\n对最先进的多模态大语言模型（MLLMs）进行的评估揭示了一个持续存在的差距：\n\n*   **流畅生成与忠实推理的差距**：模型虽然能够生成看似合理的思维链（CoTs），但往往无法定位简单的逻辑错误。\n\n### 重要意义\n\nPRISM-Bench通过以下方式提供了更深入的洞察：\n\n*   **分离答案生成与推理验证**：它将答案的生成与推理过程的验证分离开来。\n*   **提供更清晰的视角**：为多模态推理能力提供了更清晰的视角。\n*   **强调诊断性评估的重要性**：强调了在开发值得信赖的MLLMs时，诊断性评估协议的必要性。",
      "shortSummary": "PRISM-Bench是一个新的视觉谜题基准，旨在评估多模态大语言模型（MLLMs）的推理过程，而不仅仅是最终答案。其核心创新是诊断任务：模型需识别包含一个错误的逐步思维链（CoT）中的第一个错误步骤。评估显示，MLLMs在生成看似合理的CoT时，常难以发现简单的逻辑错误。该基准通过分离答案生成与推理验证，为理解多模态推理能力提供了新视角，并强调了诊断性评估对开发可信赖MLLMs的重要性。",
      "translated_title": "PRISM-Bench：一个基于谜题的视觉任务基准，包含CoT错误检测",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce PRISM-Bench, a benchmark of puzzle-based visual challenges designed to evaluate not only whether models can solve problems, but how their reasoning unfolds. Unlike prior evaluations that measure only final-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual puzzle and a step-by-step chain-of-thought (CoT) containing exactly one error, models must identify the first incorrect step. This setting enables fine-grained assessment of logical consistency, error detection, and visual reasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric, and analogical reasoning, resisting shortcuts based on superficial pattern matching. Evaluations across state-of-the-art MLLMs reveal a persistent gap between fluent generation and faithful reasoning: models that produce plausible CoTs often fail to locate simple logical faults. By disentangling answer generation from reasoning verification, PRISM-Bench offers a sharper lens on multimodal reasoning competence and underscores the need for diagnostic evaluation protocols in the development of trustworthy MLLMs."
    }
  ],
  "lastUpdated": "2025-10-30T09:35:59.470Z"
}