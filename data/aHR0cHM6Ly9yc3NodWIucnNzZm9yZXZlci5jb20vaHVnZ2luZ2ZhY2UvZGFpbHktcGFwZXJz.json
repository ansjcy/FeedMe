{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "ConsistEdit：高度一致且精确的免训练视觉编辑 (原标题: ConsistEdit: Highly Consistent and Precise Training-free Visual Editing)",
      "link": "https://arxiv.org/abs/2510.17803",
      "pubDate": "Mon, 20 Oct 2025 13:59:52 GMT",
      "isoDate": "2025-10-20T13:59:52.000Z",
      "creator": "Zixin Yin, Ling-Hao Chen, Lionel Ni, Xili Dai",
      "summary": "## ConsistEdit：高度一致且精确的免训练视觉编辑\n\n### 引言\n\n当前免训练注意力控制方法在文本引导的视觉编辑中面临多重挑战。这些挑战主要体现在以下几个方面：\n\n*   **编辑强度与一致性平衡困难：** 难以在实现强大编辑能力的同时，保持与原始图像的高度一致性。\n*   **多轮与视频编辑中的错误累积：** 在连续的多轮编辑或视频编辑中，视觉错误容易随时间累积，导致结果质量下降。\n*   **精细化编辑受限：** 大多数现有方法强制执行全局一致性，这限制了它们在保留其他属性的同时，对纹理等单个属性进行精细修改的能力。\n\n### 背景与机遇\n\n近期，生成模型架构从U-Net向MM-DiT的转变带来了显著的生成性能提升，并引入了一种集成文本和视觉模态的新颖机制。这些进步为克服以往方法未能解决的挑战提供了新的途径。\n\n### ConsistEdit 方法\n\n通过对MM-DiT注意力机制的深入分析，作者识别出三个关键见解。基于这些见解，他们提出了ConsistEdit，一种专门为MM-DiT量身定制的创新注意力控制方法。ConsistEdit的核心设计理念和技术包括：\n\n*   **纯视觉注意力控制：** 专注于利用视觉信息进行编辑。\n*   **掩码引导的预注意力融合：** 在注意力计算之前，通过掩码引导融合相关信息，以实现更精确的控制。\n*   **查询（Query）、键（Key）和值（Value）令牌的差异化操作：** 对注意力机制中的关键组成部分进行精细且有区别的操控，以产生一致且与提示对齐的编辑。\n\nConsistEdit旨在生成高度一致且与用户提示精确对齐的编辑结果。\n\n### 实验结果与优势\n\n广泛的实验证明，ConsistEdit在各种图像和视频编辑任务中，包括结构一致和结构不一致的场景，均取得了最先进的性能。其主要优势包括：\n\n*   **增强的可靠性和一致性：** ConsistEdit是首个无需手动干预，即可在所有推理步骤和所有注意力层进行编辑的方法，这显著提升了编辑的可靠性和一致性。\n*   **鲁棒的多轮与多区域编辑：** 能够支持稳健的多轮和多区域编辑，有效避免了视觉错误的累积。\n*   **精细的结构一致性控制：** 该方法支持结构一致性的渐进式调整，从而实现更精细、更灵活的编辑控制。\n\n### 其他信息\n\n*   该研究已提交至SIGGRAPH Asia 2025。\n*   所属领域为计算机视觉与模式识别 (cs.CV)。",
      "shortSummary": "ConsistEdit是一种专为MM-DiT模型设计的创新免训练视觉编辑方法。针对现有方法在编辑强度、一致性及精细控制上的不足，ConsistEdit通过纯视觉注意力控制、掩码引导融合和差异化令牌操作，实现了高度一致且与提示对齐的编辑。它首次在所有推理步骤和注意力层进行编辑，无需手动干预，显著提升了多轮和视频编辑的可靠性和一致性，并在图像和视频编辑任务中达到了最先进的性能，支持精细的结构一致性调整。",
      "translated_title": "ConsistEdit：高度一致且精确的免训练视觉编辑",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in training-free attention control methods have enabled flexible and efficient text-guided editing capabilities for existing generation models. However, current approaches struggle to simultaneously deliver strong editing strength while preserving consistency with the source. This limitation becomes particularly critical in multi-round and video editing, where visual errors can accumulate over time. Moreover, most existing methods enforce global consistency, which limits their ability to modify individual attributes such as texture while preserving others, thereby hindering fine-grained editing. Recently, the architectural shift from U-Net to MM-DiT has brought significant improvements in generative performance and introduced a novel mechanism for integrating text and vision modalities. These advancements pave the way for overcoming challenges that previous methods failed to resolve. Through an in-depth analysis of MM-DiT, we identify three key insights into its attention mechanisms. Building on these, we propose ConsistEdit, a novel attention control method specifically tailored for MM-DiT. ConsistEdit incorporates vision-only attention control, mask-guided pre-attention fusion, and differentiated manipulation of the query, key, and value tokens to produce consistent, prompt-aligned edits. Extensive experiments demonstrate that ConsistEdit achieves state-of-the-art performance across a wide range of image and video editing tasks, including both structure-consistent and structure-inconsistent scenarios. Unlike prior methods, it is the first approach to perform editing across all inference steps and attention layers without handcraft, significantly enhancing reliability and consistency, which enables robust multi-round and multi-region editing. Furthermore, it supports progressive adjustment of structural consistency, enabling finer control."
    },
    {
      "title": "Glyph：通过视觉-文本压缩扩展上下文窗口 (原标题: Glyph: Scaling Context Windows via Visual-Text Compression)",
      "link": "https://arxiv.org/abs/2510.17800",
      "pubDate": "Mon, 20 Oct 2025 13:58:56 GMT",
      "isoDate": "2025-10-20T13:58:56.000Z",
      "creator": "Jiale Cheng, Yusen Liu, Xinyu Zhang, Yulin Fei, Wenyi Hong, Ruiliang Lyu, Weihan Wang, Zhe Su, Xiaotao Gu, Xiao Liu, Yushi Bai, Jie Tang, Hongning Wang, Minlie Huang",
      "summary": "# Glyph：通过视觉-文本压缩扩展上下文窗口\n\n## 引言\n大型语言模型（LLMs）在处理文档理解、代码分析和多步推理等任务时，对长上下文建模的需求日益增长。然而，将上下文窗口扩展到百万级token会带来巨大的计算和内存成本，严重限制了长上下文LLMs的实际应用。\n\n## Glyph框架的提出\n为了解决这一挑战，本文提出了一种名为Glyph的创新框架，它采取了“视觉上下文扩展”的独特视角。Glyph的核心思想是：\n*   **文本渲染为图像**：不直接扩展基于token的序列，而是将长文本内容渲染成图像。\n*   **VLM处理**：利用视觉-语言模型（VLMs）来处理这些渲染后的图像。\n\n这种方法能够在保留文本语义信息的同时，大幅压缩文本输入。\n\n## 优化与配置\n为了平衡准确性和压缩率，研究人员进一步设计了一种由LLM驱动的遗传搜索算法。该算法旨在识别最佳的视觉渲染配置，以确保在高效压缩的同时，模型的性能不受影响。\n\n## 实验结果与优势\n通过广泛的实验，Glyph框架展示了显著的性能提升和优势：\n*   **高效压缩**：实现了3-4倍的token压缩。\n*   **保持准确性**：在各种长上下文基准测试中，Glyph保持了与领先LLMs（如Qwen3-8B）相当的准确性。\n*   **显著的速度提升**：\n    *   预填充和解码速度提升约4倍。\n    *   SFT（监督微调）训练速度提升约2倍。\n*   **极强的上下文扩展能力**：在极端压缩条件下，一个128K上下文的VLM能够有效处理百万级token的文本任务。\n*   **多模态应用潜力**：渲染后的文本数据还有益于现实世界的多模态任务，例如文档理解。\n\n## 资源可用性\n项目的代码和模型已公开发布，以便研究社区进一步探索和应用。\n\n## 研究领域\n该研究主要涉及以下计算机科学领域：\n*   计算机视觉与模式识别 (cs.CV)\n*   计算与语言 (cs.CL)\n*   机器学习 (cs.LG)",
      "shortSummary": "Glyph框架通过将长文本渲染成图像并利用视觉-语言模型（VLMs）进行处理，解决了大型语言模型（LLMs）长上下文建模的计算和内存瓶颈。该方法实现了3-4倍的token压缩，同时保持了与领先LLMs相当的准确性。它显著提升了预填充、解码和SFT训练速度，并使128K上下文的VLM能处理百万级token任务。Glyph通过视觉-文本压缩有效扩展了LLMs的上下文窗口。",
      "translated_title": "Glyph：通过视觉-文本压缩扩展上下文窗口",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning. However, scaling context windows to the million-token level brings prohibitive computational and memory costs, limiting the practicality of long-context LLMs. In this work, we take a different perspective-visual context scaling-to tackle this challenge. Instead of extending token-based sequences, we propose Glyph, a framework that renders long texts into images and processes them with vision-language models (VLMs). This approach substantially compresses textual input while preserving semantic information, and we further design an LLM-driven genetic search to identify optimal visual rendering configurations for balancing accuracy and compression. Through extensive experiments, we demonstrate that our method achieves 3-4x token compression while maintaining accuracy comparable to leading LLMs such as Qwen3-8B on various long-context benchmarks. This compression also leads to around 4x faster prefilling and decoding, and approximately 2x faster SFT training. Furthermore, under extreme compression, a 128K-context VLM could scale to handle 1M-token-level text tasks. In addition, the rendered text data benefits real-world multimodal tasks, such as document understanding. Our code and model are released at https://github.com/thu-coai/Glyph."
    },
    {
      "title": "企业深度研究：面向企业分析的可操纵多智能体深度研究 (原标题: Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics)",
      "link": "https://arxiv.org/abs/2510.17797",
      "pubDate": "Mon, 20 Oct 2025 13:55:11 GMT",
      "isoDate": "2025-10-20T13:55:11.000Z",
      "creator": "Akshara Prabhakar, Roshan Ram, Zixiang Chen, Silvio Savarese, Frank Wang, Caiming Xiong, Huan Wang, Weiran Yao",
      "summary": "# 企业深度研究 (EDR) 概述\n\n## 引言\n随着信息呈指数级增长，企业面临着将非结构化数据转化为连贯、可操作洞察的巨大压力。尽管自主智能体展现出潜力，但它们在处理领域特定细微差别、意图对齐和企业集成方面常常遇到困难。\n\n## EDR 解决方案\n本文提出了企业深度研究（Enterprise Deep Research, EDR），这是一个旨在解决上述挑战的多智能体系统。EDR 整合了多个关键组件，以实现自动化报告生成、实时流处理和无缝企业部署。\n\n## EDR 的核心组件\nEDR 系统由以下五个主要部分组成：\n1.  **主规划智能体 (Master Planning Agent)**：负责自适应地分解查询，确保研究方向的有效性。\n2.  **四种专业搜索智能体 (Specialized Search Agents)**：\n    *   通用搜索智能体\n    *   学术搜索智能体\n    *   GitHub 搜索智能体\n    *   领英 (LinkedIn) 搜索智能体\n    这些智能体协同工作，从不同来源收集信息。\n3.  **可扩展的基于 MCP 的工具生态系统 (Extensible MCP-based Tool Ecosystem)**：支持自然语言到 SQL (NL2SQL) 的转换、文件分析以及各种企业工作流程，极大地增强了系统的功能性。\n4.  **可视化智能体 (Visualization Agent)**：用于将数据转化为数据驱动的洞察，帮助用户理解复杂信息。\n5.  **反思机制 (Reflection Mechanism)**：能够检测知识空白并更新研究方向。该机制还支持可选的人工干预指导，以进一步优化研究过程。\n\n## 性能与验证\nEDR 系统在内部数据集上进行了验证，结果表明其能够实现自动化报告生成、实时流处理和无缝企业部署。在包括 DeepResearch Bench 和 DeepConsult 在内的开放式基准测试中，EDR 在没有任何人工干预的情况下，性能优于最先进的智能体系统。\n\n## 资源发布\n为了推动多智能体推理应用的研究进展，EDR 框架和基准测试轨迹已公开发布。\n\n## 附加信息\n*   **文档类型**：技术报告\n*   **页数**：13页，另加参考文献和附录\n*   **主题**：计算与语言 (cs.CL)；人工智能 (cs.AI)\n*   **arXiv ID**：arXiv:2510.17797",
      "shortSummary": "企业深度研究（EDR）是一个可操纵的多智能体系统，旨在帮助企业将非结构化数据转化为可操作的洞察。它包含主规划智能体、四种专业搜索智能体、工具生态系统、可视化智能体和反思机制。EDR 能实现自动化报告生成、实时流处理和无缝企业部署。在开放式基准测试中，EDR 在无人干预下超越了现有先进智能体系统。其框架和基准轨迹已发布，以推动多智能体推理研究。",
      "translated_title": "企业深度研究：面向企业分析的可操纵多智能体深度研究",
      "images": [],
      "contentSource": "完整文章",
      "content": "As information grows exponentially, enterprises face increasing pressure to transform unstructured data into coherent, actionable insights. While autonomous agents show promise, they often struggle with domain-specific nuances, intent alignment, and enterprise integration. We present Enterprise Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning Agent for adaptive query decomposition, (2) four specialized search agents (General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a Visualization Agent for data-driven insights, and (5) a reflection mechanism that detects knowledge gaps and updates research direction with optional human-in-the-loop steering guidance. These components enable automated report generation, real-time streaming, and seamless enterprise deployment, as validated on internal datasets. On open-ended benchmarks including DeepResearch Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without any human steering. We release the EDR framework and benchmark trajectories to advance research on multi-agent reasoning applications.   Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and Dataset at https://huggingface.co/datasets/Salesforce/EDR-200"
    },
    {
      "title": "用于复现AI研究的可执行知识图谱 (原标题: Executable Knowledge Graphs for Replicating AI Research)",
      "link": "https://arxiv.org/abs/2510.17795",
      "pubDate": "Mon, 20 Oct 2025 13:53:23 GMT",
      "isoDate": "2025-10-20T13:53:23.000Z",
      "creator": "Yujie Luo, Zhuoyun Yu, Xuehai Wang, Yuqi Zhu, Ningyu Zhang, Lanning Wei, Lun Du, Da Zheng, Huajun Chen",
      "summary": "# 用于复现AI研究的可执行知识图谱（xKG）\n\n## 摘要\n\n本文提出了一种名为“可执行知识图谱”（Executable Knowledge Graphs, xKG）的创新方法，旨在解决大型语言模型（LLM）代理在复现AI研究时面临的重大挑战。研究表明，现有的方法在生成可执行代码方面存在困难，主要原因包括背景知识不足、检索增强生成（RAG）方法无法捕捉参考文献中隐藏的潜在技术细节，以及忽视了有价值的实现级代码信号和缺乏支持多粒度检索与重用的结构化知识表示。\n\n## 核心问题\n\n*   **背景知识不足**：LLM代理在复现AI研究时，往往缺乏足够的背景知识来理解复杂的科学文献。\n*   **RAG方法局限性**：传统的RAG方法难以捕捉到论文中隐含的、深层次的技术细节，导致代码生成不准确或不完整。\n*   **忽视实现级代码信号**：现有方法未能充分利用代码实现层面的宝贵信息。\n*   **缺乏结构化知识表示**：缺乏能够支持多粒度检索和知识重用的结构化知识库。\n\n## 解决方案：可执行知识图谱（xKG）\n\n为了克服上述挑战，研究者提出了xKG。xKG是一个：\n\n*   **模块化且可插拔的知识库**：设计灵活，易于集成和扩展。\n*   **自动化整合**：能够自动从科学文献中提取并整合技术见解、代码片段和领域特定知识。\n*   **支持多粒度检索与重用**：其结构化表示有助于在不同粒度级别上进行知识的检索和重用。\n\n## 实验与结果\n\n为了验证xKG的有效性，研究人员将其集成到：\n\n*   **三个不同的代理框架中**。\n*   **结合两种不同的LLM**。\n\n在PaperBench基准测试中，xKG展现出显著的性能提升：\n\n*   **性能提升**：与基线相比，使用o3-mini模型时性能提升了10.9%。\n\n## 结论\n\n实验结果表明，xKG作为一种通用且可扩展的解决方案，在自动化AI研究复现方面表现出卓越的有效性。该研究为提高LLM代理在复杂科学任务中的能力提供了新的途径。\n\n## 代码发布\n\n相关代码将在未来发布。",
      "shortSummary": "AI研究复现对LLM代理而言极具挑战，主要原因在于背景知识不足、RAG方法局限以及缺乏结构化知识。为解决此问题，本文提出了可执行知识图谱（xKG）。xKG是一个模块化知识库，能自动整合科学文献中的技术见解、代码片段和领域知识。实验表明，xKG显著提升了LLM代理在PaperBench上的性能（如使用o3-mini模型提升10.9%），证明其是自动化AI研究复现的通用且有效的解决方案。",
      "translated_title": "用于复现AI研究的可执行知识图谱",
      "images": [],
      "contentSource": "完整文章",
      "content": "Replicating AI research is a crucial yet challenging task for large language model (LLM) agents. Existing approaches often struggle to generate executable code, primarily due to insufficient background knowledge and the limitations of retrieval-augmented generation (RAG) methods, which fail to capture latent technical details hidden in referenced papers. Furthermore, previous approaches tend to overlook valuable implementation-level code signals and lack structured knowledge representations that support multi-granular retrieval and reuse. To overcome these challenges, we propose Executable Knowledge Graphs (xKG), a modular and pluggable knowledge base that automatically integrates technical insights, code snippets, and domain-specific knowledge extracted from scientific literature. When integrated into three agent frameworks with two different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on PaperBench, demonstrating its effectiveness as a general and extensible solution for automated AI research replication. Code will released at https://github.com/zjunlp/xKG."
    },
    {
      "title": "基础自动评估器：扩展面向推理领域的多任务生成式评估器训练 (原标题: Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains)",
      "link": "https://arxiv.org/abs/2510.17793",
      "pubDate": "Mon, 20 Oct 2025 13:52:06 GMT",
      "isoDate": "2025-10-20T13:52:06.000Z",
      "creator": "Austin Xu, Xuan-Phi Nguyen, Yilun Zhou, Chien-Sheng Wu, Caiming Xiong, Shafiq Joty",
      "summary": "本文介绍了“基础自动推理评估器”（Foundational Automatic Reasoning Evaluators, FARE），旨在通过大规模数据驱动的方法，解决生成式评估器训练中对可扩展评估日益增长的需求。\n\n**背景与挑战**\n*   微调专门的生成式评估器已成为满足训练和测试阶段可扩展评估需求的一种流行范式。\n*   然而，现有研究主要侧重于应用强化学习（RL）等新方法来训练评估器，而忽视了大规模、数据驱动的开发。\n\n**本文贡献与方法**\n*   **数据规模化**：本文关注数据规模化，策划了一个包含250万样本的数据集。\n*   **任务与领域**：该数据集涵盖五种独特的评估任务：\n    *   成对比较（pairwise）\n    *   步骤级评估（step-level）\n    *   无参考验证（reference-free verification）\n    *   有参考验证（reference-based verification）\n    *   单一评分（single rating）\n*   这些任务和数据主要集中在以推理评估为中心的多个领域。\n\n**FARE模型开发**\n*   **模型架构**：利用上述大规模数据集，研究人员训练了FARE家族，包括80亿和200亿（其中36亿为活跃参数）参数的评估器。\n*   **训练方法**：采用了一种简单的迭代拒绝采样监督微调（SFT）方法进行训练。\n\n**关键成果与性能**\n*   **基准测试表现**：\n    *   FARE-8B 在性能上挑战了更大规模的、经过RL训练的专业评估器。\n    *   FARE-20B 为开源评估器设定了新标准，超越了700亿参数以上的专业评估器。\n*   **实际应用评估**：\n    *   **推理时重排序器**：作为推理时的重排序器，FARE-20B 在MATH数据集上实现了接近“预言机”（near-oracle）的性能。\n    *   **RL训练中的验证器**：作为强化学习训练中的验证器，FARE将下游RL训练模型的性能提高了高达14.1%，优于传统的字符串匹配验证器。\n    *   **代码评估**：当从FARE初始化并进行持续微调时，FARE-Code在评估测试用例质量方面比gpt-oss-20B高出65%。",
      "shortSummary": "本文介绍了“基础自动推理评估器”（FARE），通过大规模数据驱动方法，解决了生成式评估器训练中数据规模化不足的问题。研究团队收集了250万样本，涵盖五种推理评估任务，并开发了8B和20B参数的FARE模型。FARE-8B挑战了大型RL训练评估器，而FARE-20B则超越了70B+专业评估器，为开源评估器树立了新标准。在实际应用中，FARE-20B在MATH数据集上实现了接近预言机的性能，并在RL训练中将模型性能提升了14.1%，其变体FARE-Code在代码评估上表现优异。",
      "translated_title": "基础自动评估器：扩展面向推理领域的多任务生成式评估器训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "Finetuning specialized generative evaluators has emerged as a popular paradigm to meet the increasing demand for scalable evaluation during both training and test-time. However, recent work has largely focused on applying new methodology, such as reinforcement learning (RL), to training evaluators, shying away from large-scale, data-driven development. In this work, we focus on data scaling, curating a set of 2.5M samples spanning five unique evaluation tasks (pairwise, step-level, reference-free and reference-based verification, and single rating) and multiple domains focused on reasoning evaluation. With our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges larger specialized RL-trained evaluators and FARE-20B sets the new standard for open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers, FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training, FARE improves the downstream RL-trained model performance by up to 14.1% vs. string-matching verifiers. When initialized from FARE, a continually-finetuned FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality."
    },
    {
      "title": "UltraCUA：一种具有混合动作的计算机使用代理基础模型 (原标题: UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action)",
      "link": "https://arxiv.org/abs/2510.17790",
      "pubDate": "Mon, 20 Oct 2025 13:48:26 GMT",
      "isoDate": "2025-10-20T13:48:26.000Z",
      "creator": "Yuhao Yang, Zhen Yang, Zi-Yi Dou, Anh Nguyen, Keen You, Omar Attia, Andrew Szot, Michael Feng, Ram Ramrakhya, Alexander Toshev, Chao Huang, Yinfei Yang, Zhe Gan",
      "summary": "### UltraCUA：一种具有混合动作的计算机使用代理基础模型\n\n**1. 背景与问题**\n*   当前的多模态计算机使用代理（CUAs）主要依赖于原始的图形用户界面（GUI）操作，例如点击、输入和滚动。\n*   这种依赖性导致了以下问题：\n    *   需要精确的视觉定位。\n    *   执行链冗长，容易导致级联故障。\n    *   存在性能瓶颈。\n*   与其他利用程序化接口（如API、MCP服务器、工具）的代理不同，CUAs一直未能有效利用这些高级能力。\n\n**2. UltraCUA 解决方案**\n*   本文提出了 UltraCUA，一个基础模型，旨在通过引入“混合动作”机制来弥合上述差距。\n*   **混合动作**：无缝整合了低级GUI原语操作和高层程序化工具调用，使得代理能够更灵活、高效地与计算机交互。\n\n**3. 关键组成部分**\nUltraCUA 的实现包括四个核心组件：\n*   **自动化工具扩展管道：**\n    *   该管道能够从软件文档、开源存储库和代码生成中自动扩展和整合程序化工具。\n*   **合成数据引擎：**\n    *   生成了超过17,000个可验证的任务，这些任务涵盖了真实的计算机使用场景，为模型训练提供了丰富的多样化数据。\n*   **大规模高质量混合动作轨迹集合：**\n    *   收集了包含低级GUI动作和高级程序化工具调用的混合动作轨迹数据，用于模型的学习。\n*   **两阶段训练管道：**\n    *   结合了监督微调（Supervised Fine-tuning, SFT）和在线强化学习（Online Reinforcement Learning, RL）。\n    *   这种训练策略使模型能够策略性地在低级GUI操作和高级程序化工具调用之间进行切换，以优化任务执行。\n\n**4. 实验结果与性能**\n*   通过对 UltraCUA 的 7B 和 32B 模型进行实验，结果显示其性能显著优于现有的最先进代理。\n*   **OSWorld 评估：**\n    *   UltraCUA 模型在 OSWorld 基准测试中，比基础模型平均相对提升了22%的性能。\n    *   在执行步骤方面，速度提升了11%。\n*   **WindowsAgentArena 域外评估：**\n    *   在 WindowsAgentArena 上的域外评估中，UltraCUA 模型达到了21.7%的成功率。\n    *   这一表现优于在 Windows 数据上训练的基线模型。\n*   **混合动作机制的重要性：**\n    *   实验证明，混合动作机制对于减少错误传播和保持执行效率至关重要，是 UltraCUA 成功的关键因素。",
      "shortSummary": "UltraCUA是一种针对计算机使用代理（CUAs）的基础模型，通过引入“混合动作”机制，将低级GUI操作与高级程序化工具调用无缝结合。现有CUAs过度依赖原始GUI操作，导致效率低下和错误传播。UltraCUA通过自动化工具扩展、合成数据生成和两阶段训练，显著提升了代理性能。在OSWorld和WindowsAgentArena上的实验表明，UltraCUA比现有模型平均提升22%的性能，并加快11%的执行速度，有效减少了错误并提高了效率。",
      "translated_title": "UltraCUA：一种具有混合动作的计算机使用代理基础模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal agents for computer use rely exclusively on primitive actions (click, type, scroll) that require accurate visual grounding and lengthy execution chains, leading to cascading failures and performance bottlenecks. While other agents leverage rich programmatic interfaces (APIs, MCP servers, tools), computer-use agents (CUAs) remain isolated from these capabilities. We present UltraCUA, a foundation model that bridges this gap through hybrid action -- seamlessly integrating GUI primitives with high-level programmatic tool calls. To achieve this, our approach comprises four key components: (1) an automated pipeline that scales programmatic tools from software documentation, open-source repositories, and code generation; (2) a synthetic data engine producing over 17,000 verifiable tasks spanning real-world computer-use scenarios; (3) a large-scale high-quality hybrid action trajectory collection with both low-level GUI actions and high-level programmatic tool calls; and (4) a two-stage training pipeline combining supervised fine-tuning with online reinforcement learning, enabling strategic alternation between low-level and high-level actions. Experiments with our 7B and 32B models demonstrate substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA models achieve an average 22% relative improvement over base models, while being 11% faster in terms of steps. Out-of-domain evaluation on WindowsAgentArena shows our model reaches 21.7% success rate, outperforming baselines trained on Windows data. The hybrid action mechanism proves critical, reducing error propagation while maintaining execution efficiency."
    },
    {
      "title": "PICABench：我们离物理真实图像编辑还有多远？ (原标题: PICABench: How Far Are We from Physically Realistic Image Editing?)",
      "link": "https://arxiv.org/abs/2510.17681",
      "pubDate": "Mon, 20 Oct 2025 11:53:57 GMT",
      "isoDate": "2025-10-20T11:53:57.000Z",
      "creator": "Yuandong Pu, Le Zhuo, Songhao Han, Jinbo Xing, Kaiwen Zhu, Shuo Cao, Bin Fu, Si Liu, Hongsheng Li, Yu Qiao, Wenlong Zhang, Xi Chen, Yihao Liu",
      "summary": "## PICABench：我们离物理真实图像编辑还有多远？\n\n### 摘要背景\n*   **现有进展与局限**：近年来，图像编辑技术取得了显著进步，现代编辑模型能够遵循复杂的指令来操纵原始内容。然而，除了完成编辑指令外，伴随的物理效果（例如，移除一个物体时，其阴影、反射以及与附近物体的互动也应随之消失）是生成真实感的关键。\n*   **当前问题**：不幸的是，现有模型和基准主要关注指令完成，却忽视了这些物理效果。\n\n### PICABench的引入\n*   **核心目的**：为了回答“我们离物理真实图像编辑还有多远？”这一关键问题，研究人员引入了PICABench。\n*   **基准特性**：PICABench是一个系统性评估物理真实感的基准。\n\n### PICABench的评估维度与操作\n*   **评估维度**：PICABench涵盖了八个子维度，这些维度跨越了光学、力学和状态转换等领域，确保了对物理效果的全面考量。\n*   **编辑操作**：它适用于大多数常见的图像编辑操作，包括添加、移除、属性更改等。\n\n### PICAEval评估协议\n*   **可靠性评估**：研究团队进一步提出了PICAEval，一个旨在提供可靠评估的协议。\n*   **评估方法**：PICAEval采用“VLM-as-a-judge”（视觉语言模型作为评判者）的方法，并结合了逐案例、区域级别的人工标注和问题，以确保评估的准确性和细致性。\n\n### 解决方案探索与数据集构建\n*   **探索方向**：除了基准测试，研究人员还积极探索有效的解决方案，其核心思路是通过从视频中学习物理规律。\n*   **数据集构建**：为此，他们构建了一个名为PICA-100K的训练数据集，以支持物理规律的学习。\n\n### 研究发现与未来展望\n*   **当前挑战**：在评估了大多数主流模型后，研究观察到物理真实感仍然是一个具有挑战性的问题，表明该领域仍有巨大的探索空间。\n*   **未来愿景**：研究团队希望他们的基准和提出的解决方案能够为未来的工作奠定坚实的基础，推动图像编辑领域从简单的内容编辑迈向物理一致的真实感。",
      "shortSummary": "PICABench是一个新基准，旨在系统评估图像编辑中的物理真实感，弥补现有模型和基准忽视物理效果（如阴影、反射）的不足。它涵盖光学、力学等八个维度及常见编辑操作。研究引入了PICAEval评估协议，并构建了PICA-100K数据集以探索解决方案。评估结果表明，实现物理真实感仍是巨大挑战，该工作旨在推动图像编辑向物理一致性发展。",
      "translated_title": "PICABench：我们离物理真实图像编辑还有多远？",
      "images": [],
      "contentSource": "完整文章",
      "content": "Image editing has achieved remarkable progress recently. Modern editing models could already follow complex instructions to manipulate the original content. However, beyond completing the editing instructions, the accompanying physical effects are the key to the generation realism. For example, removing an object should also remove its shadow, reflections, and interactions with nearby objects. Unfortunately, existing models and benchmarks mainly focus on instruction completion but overlook these physical effects. So, at this moment, how far are we from physically realistic image editing? To answer this, we introduce PICABench, which systematically evaluates physical realism across eight sub-dimension (spanning optics, mechanics, and state transitions) for most of the common editing operations (add, remove, attribute change, etc). We further propose the PICAEval, a reliable evaluation protocol that uses VLM-as-a-judge with per-case, region-level human annotations and questions. Beyond benchmarking, we also explore effective solutions by learning physics from videos and construct a training dataset PICA-100K. After evaluating most of the mainstream models, we observe that physical realism remains a challenging problem with large rooms to explore. We hope that our benchmark and proposed solutions can serve as a foundation for future work moving from naive content editing toward physically consistent realism."
    },
    {
      "title": "标注高效的通用诚实对齐 (原标题: Annotation-Efficient Universal Honesty Alignment)",
      "link": "https://arxiv.org/abs/2510.17509",
      "pubDate": "Mon, 20 Oct 2025 09:05:22 GMT",
      "isoDate": "2025-10-20T09:05:22.000Z",
      "creator": "Shiyu Ni, Keping Bi, Jiafeng Guo, Minghao Tang, Jingtong Wu, Zengxin Han, Xueqi Cheng",
      "summary": "## 标注高效的通用诚实对齐：EliCal框架与HonestyBench基准\n\n### 背景与挑战\n\n大型语言模型（LLMs）的“诚实对齐”能力对于其可信赖部署至关重要。诚实对齐是指LLMs能够识别其知识边界并表达校准后的置信度。然而，现有方法存在局限性：\n\n*   **免训练置信度估计：** 例如，基于token概率或自洽性，但往往效果有限。\n*   **基于训练的校准：** 需要大量的正确性标注，这使得成本高昂，难以实现大规模的通用诚实对齐。\n\n### EliCal框架：标注高效的解决方案\n\n为了支持标注高效的训练，研究人员引入了Elicitation-Then-Calibration (EliCal)，这是一个创新的两阶段框架：\n\n1.  **第一阶段（启发，Elicitation）：** 利用成本较低的“自洽性监督”来启发LLM的内部置信度。自洽性监督通常比人工标注正确性更易获取。\n2.  **第二阶段（校准，Calibration）：** 随后，使用一小组（少量）的“正确性标注”来校准在第一阶段获得的置信度。\n\n通过这种方式，EliCal显著减少了对昂贵正确性标注的依赖。\n\n### HonestyBench：大规模评估基准\n\n为了支持对诚实对齐方法进行大规模研究和评估，研究人员发布了HonestyBench，这是一个全面的基准：\n\n*   **数据集：** 涵盖了十个自由形式的问答数据集。\n*   **数据量：** 包含56万个训练实例和7万个评估实例。\n*   **标注：** 所有实例都标注了正确性信号和自洽性信号，为研究提供了丰富的数据基础。\n\n### 实验结果与意义\n\n实验结果表明EliCal框架具有显著优势：\n\n*   **标注效率：** EliCal仅使用1千个正确性标注（占全部监督的0.18%）就实现了接近最优的对齐效果。\n*   **泛化能力：** 在未曾见过的MMLU任务上，EliCal比仅进行校准的基线方法表现出更好的对齐性能。\n\n这些结果表明，EliCal为LLMs的通用诚实对齐提供了一个可扩展且高效的解决方案，有望推动LLMs在实际应用中的可信赖性。",
      "shortSummary": "为实现大型语言模型（LLMs）的通用诚实对齐，研究人员提出了标注高效的EliCal框架。该框架分两阶段：首先利用低成本的自洽性监督启发内部置信度，然后用少量正确性标注进行校准。同时发布了HonestyBench，一个包含56万训练和7万评估实例的大规模基准。实验证明，EliCal仅用1千个正确性标注（0.18%的监督）即可实现接近最优的对齐效果，并在未见任务上表现更优，为LLMs的诚实对齐提供了可扩展的解决方案。",
      "translated_title": "标注高效的通用诚实对齐",
      "images": [],
      "contentSource": "完整文章",
      "content": "Honesty alignment-the ability of large language models (LLMs) to recognize their knowledge boundaries and express calibrated confidence-is essential for trustworthy deployment. Existing methods either rely on training-free confidence estimation (e.g., token probabilities, self-consistency) or training-based calibration with correctness annotations. While effective, achieving universal honesty alignment with training-based calibration requires costly, large-scale labeling. To support annotation-efficient training, we introduce Elicitation-Then-Calibration (EliCal), a two-stage framework that first elicits internal confidence using inexpensive self-consistency supervision, then calibrates this confidence with a small set of correctness annotations. To support a large-scale study, we release HonestyBench, a benchmark covering ten free-form QA datasets with 560k training and 70k evaluation instances annotated with correctness and self-consistency signals. Experiments show that EliCal achieves near-optimal alignment with only 1k correctness annotations (0.18% of full supervision) and better alignment performance on unseen MMLU tasks than the calibration-only baseline, offering a scalable solution toward universal honesty alignment in LLMs."
    },
    {
      "title": "深度自演化推理 (原标题: Deep Self-Evolving Reasoning)",
      "link": "https://arxiv.org/abs/2510.17498",
      "pubDate": "Mon, 20 Oct 2025 08:51:42 GMT",
      "isoDate": "2025-10-20T08:51:42.000Z",
      "creator": "Zihan Liu, Shun Zheng, Xumeng Wen, Yang Wang, Jiang Bian, Mao Yang",
      "summary": "# 深度自演化推理 (Deep Self-Evolving Reasoning)\n\n本文介绍了一种名为“深度自演化推理”（Deep Self-Evolving Reasoning, DSER）的概率范式，旨在显著扩展开源、小型语言模型在长篇思维链推理方面的能力，尤其是在其验证和修正能力较弱的情况下。\n\n## 背景与挑战\n\n*   **思维链推理的重要性：** 长篇思维链推理已成为大型语言模型（LLM）高级推理的基石。\n*   **现有框架的局限性：** 尽管最新的验证-修正框架使专有模型能够解决奥林匹克级别的难题，但其有效性高度依赖于强大、可靠的验证和修正能力。\n*   **开源模型的弱点：** 对于开源、小型模型而言，这些验证和修正能力仍然脆弱。\n\n## 深度自演化推理 (DSER) 核心理念\n\n*   **克服弱验证能力：** DSER 旨在证明，即使在面对困难任务时，模型的验证和修正能力较弱，也能通过这种概率范式大幅提升其推理极限。\n*   **迭代推理的马尔可夫链模型：** 本文将迭代推理概念化为一个马尔可夫链，其中每一步都代表解决方案空间中的随机转换。\n*   **收敛保证：** 关键的洞察在于，只要改进的概率略微超过退化的概率，就能保证收敛到正确的解决方案。\n*   **放大积极趋势：** DSER 通过并行运行多个长周期、自演化过程，放大这些微小的积极趋势，使模型能够渐近地接近正确答案。\n\n## 实验与成果\n\n*   **应用模型：** DSER 被应用于 DeepSeek-R1-0528-Qwen3-8B 模型。\n*   **基准测试：** 在具有挑战性的 AIME 2024-2025 基准测试中，DSER 取得了显著成果：\n    *   解决了 9 个先前无法解决的问题中的 5 个。\n    *   提升了整体性能。\n    *   通过多数投票，使这个紧凑型模型超越了其 600B 参数教师模型的单次准确率。\n\n## 诊断与未来研究方向\n\n*   **诊断工具：** 除了其在测试时扩展能力的直接效用外，DSER 框架还可用于诊断当前开源推理器的根本局限性。\n*   **明确的不足：** 通过清晰地描绘出这些模型在自我验证、修正和稳定性方面的缺点，本文的研究结果为开发具有强大内在自演化能力的下一代模型建立了明确的研究议程。",
      "shortSummary": "本文提出了“深度自演化推理”（DSER）范式，旨在增强开源、小型语言模型在弱验证能力下的长篇思维链推理。DSER将迭代推理视为马尔可夫链，通过并行运行多个自演化过程，放大微小改进概率，渐近收敛至正确答案。实验表明，DSER使DeepSeek-R1-0528-Qwen3-8B模型在AIME 2024-2025基准测试中解决了5个难题，并超越了其600B参数教师模型的单次准确率。DSER还可诊断开源推理器的局限性，为未来研究指明方向。",
      "translated_title": "深度自演化推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Long-form chain-of-thought reasoning has become a cornerstone of advanced reasoning in large language models. While recent verification-refinement frameworks have enabled proprietary models to solve Olympiad-level problems, their effectiveness hinges on strong, reliable verification and correction capabilities, which remain fragile in open-weight, smaller-scale models. This work demonstrates that even with weak verification and refinement capabilities on hard tasks, the reasoning limits of such models can be substantially extended through a probabilistic paradigm we call Deep Self-Evolving Reasoning (DSER). We conceptualize iterative reasoning as a Markov chain, where each step represents a stochastic transition in the solution space. The key insight is that convergence to a correct solution is guaranteed as long as the probability of improvement marginally exceeds that of degradation. By running multiple long-horizon, self-evolving processes in parallel, DSER amplifies these small positive tendencies, enabling the model to asymptotically approach correct answers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On the challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously unsolvable problems and boosts overall performance, enabling this compact model to surpass the single-turn accuracy of its 600B-parameter teacher through majority voting. Beyond its immediate utility for test-time scaling, the DSER framework serves to diagnose the fundamental limitations of current open-weight reasoners. By clearly delineating their shortcomings in self-verification, refinement, and stability, our findings establish a clear research agenda for developing next-generation models with powerful, intrinsic self-evolving capabilities."
    },
    {
      "title": "迈向通用检索增强生成的多模态检索 (原标题: Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation)",
      "link": "https://arxiv.org/abs/2510.17354",
      "pubDate": "Mon, 20 Oct 2025 05:56:43 GMT",
      "isoDate": "2025-10-20T05:56:43.000Z",
      "creator": "Chenghao Zhang, Guanting Dong, Xinyu Yang, Zhicheng Dou",
      "summary": "## 迈向通用检索增强生成的多模态检索\n\n### 摘要\n\n本文提出了一种名为 Nyx 的新型统一多模态到多模态检索器，旨在解决通用检索增强生成（URAG）的挑战。URAG 旨在检索和推理多模态信息（如文本和图像），以改进视觉-语言生成，克服现有检索增强生成（RAG）系统主要局限于单模态文本文档的局限性。\n\n### 核心问题与挑战\n\n*   **现有 RAG 系统的局限性**：当前的 RAG 系统主要关注单模态文本文档，在查询和文档可能包含混合模态（如文本和图像）的真实世界场景中表现不足。\n*   **通用 RAG (URAG)**：需要能够检索和推理混合模态信息，以提升视觉-语言生成质量。\n*   **多模态数据稀缺**：缺乏真实世界的多模态数据是开发此类系统的主要障碍。\n\n### 解决方案：Nyx 检索器\n\n*   **Nyx 简介**：提出 Nyx，一个统一的多模态到多模态检索器，专为 URAG 场景设计。\n*   **数据生成与 NyxQA 数据集**：\n    *   为缓解多模态数据稀缺问题，引入了一个四阶段自动化生成和过滤流程。\n    *   利用网络文档构建了 NyxQA 数据集，该数据集包含多样化的混合模态问答对，更好地反映了真实世界的信息需求。\n*   **Nyx 的两阶段训练框架**：\n    1.  **预训练**：在 NyxQA 和各种开源检索数据集上进行预训练。\n    2.  **监督微调**：利用下游视觉-语言模型（VLM）的反馈进行监督微调，以使检索输出与生成偏好对齐。\n\n### 实验结果与贡献\n\n*   **性能表现**：实验结果表明，Nyx 不仅在标准的纯文本 RAG 基准测试上表现出色，而且在更通用、更真实的 URAG 设置中也表现卓越。\n*   **生成质量提升**：Nyx 显著提高了视觉-语言任务中的生成质量。\n*   **研究进展**：该工作目前正在进行中，为计算与语言、人工智能、信息检索和机器学习领域做出了贡献。",
      "shortSummary": "本文提出了 Nyx，一个统一的多模态到多模态检索器，旨在解决通用检索增强生成（URAG）中现有 RAG 系统仅限于文本模态的局限性。为克服多模态数据稀缺，研究者开发了一个四阶段自动化流程来构建 NyxQA 数据集。Nyx 采用两阶段训练框架，并在实验中证明其在纯文本 RAG 和更通用的 URAG 任务中均表现出色，显著提升了视觉-语言生成质量。",
      "translated_title": "迈向通用检索增强生成的多模态检索",
      "images": [],
      "contentSource": "完整文章",
      "content": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing large language models (LLMs) by retrieving relevant documents from an external corpus. However, existing RAG systems primarily focus on unimodal text documents, and often fall short in real-world scenarios where both queries and documents may contain mixed modalities (such as text and images). In this paper, we address the challenge of Universal Retrieval-Augmented Generation (URAG), which involves retrieving and reasoning over mixed-modal information to improve vision-language generation. To this end, we propose Nyx, a unified mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate the scarcity of realistic mixed-modal data, we introduce a four-stage automated pipeline for generation and filtering, leveraging web documents to construct NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that better reflect real-world information needs. Building on this high-quality dataset, we adopt a two-stage training framework for Nyx: we first perform pre-training on NyxQA along with a variety of open-source retrieval datasets, followed by supervised fine-tuning using feedback from downstream vision-language models (VLMs) to align retrieval outputs with generative preferences. Experimental results demonstrate that Nyx not only performs competitively on standard text-only RAG benchmarks, but also excels in the more general and realistic URAG setting, significantly improving generation quality in vision-language tasks."
    },
    {
      "title": "FineVision：开放数据就是你所需要的一切 (原标题: FineVision: Open Data Is All You Need)",
      "link": "https://arxiv.org/abs/2510.17269",
      "pubDate": "Mon, 20 Oct 2025 03:54:46 GMT",
      "isoDate": "2025-10-20T03:54:46.000Z",
      "creator": "Luis Wiedmann, Orr Zohar, Amir Mahla, Xiaohan Wang, Rui Li, Thibaud Frere, Leandro von Werra, Aritra Roy Gosthipaty, Andrés Marafioti",
      "summary": "# FineVision：解决视觉-语言模型数据挑战的新范式\n\n## 引言\n当前，视觉-语言模型（VLM）的发展正受到公共数据集碎片化、不一致和污染的严重阻碍。为应对这一挑战，本文介绍了FineVision，一个经过精心收集、整理和统一的大规模语料库。\n\n## FineVision的特点与规模\n*   **规模空前：** FineVision包含2400万个样本，是目前同类中最大的开放资源。\n*   **数据来源整合：** 它成功地将200多个不同的数据源统一整合为185个子集。\n\n## 数据整理与统一流程\nFineVision的构建采用了一种创新的半自动化、人机协作（human-in-the-loop）流程，确保了数据的高质量和一致性：\n*   **自动化阶段：** 负责批量数据摄取和初始的模式映射。\n*   **人工审核阶段：** 审核员对自动化映射进行审计，并抽查输出结果，以验证以下关键方面：\n    *   注释的忠实消费。\n    *   格式的适当性和多样性。\n    *   数据的安全性。\n    *   一旦发现问题，会触发有针对性的修复和重新运行。\n*   **数据卫生：** \n    *   **严格去重：** 在数据来源内部和跨来源之间进行严格的去重处理。\n    *   **去污染：** 针对66个公共基准进行去污染处理，以确保数据的纯净性。\n\n## 支持代理/GUI任务\n*   FineVision还包含了具有统一动作空间的代理（agentic）和图形用户界面（GUI）任务。\n*   审核员会验证这些任务的模式，并检查轨迹样本，以确认其可执行的保真度。\n\n## 实验结果与优势\n*   在广泛的评估套件中，使用FineVision训练的模型持续表现出优于使用现有开放混合数据集训练的模型。\n*   这一结果有力地证明了数据规模、数据卫生以及自动化与人工监督平衡结合所带来的显著优势。\n\n## 发布与影响\n*   FineVision语料库及其配套的整理工具已公开发布。\n*   此举旨在加速以数据为中心的视觉-语言模型研究，为社区提供一个高质量的开放资源。\n\n## 相关领域\n*   计算机视觉与模式识别 (cs.CV)\n*   人工智能 (cs.AI)",
      "shortSummary": "FineVision是一个包含2400万样本的开放数据集，旨在解决视觉-语言模型（VLM）面临的数据碎片化和污染问题。它通过半自动化、人机协作的流程，整合了200多个来源，并进行了严格的去重和去污染。FineVision还支持代理/GUI任务。实验表明，使用FineVision训练的模型性能优于现有数据集。该语料库及其工具已发布，以促进VLM研究。",
      "translated_title": "FineVision：开放数据就是你所需要的一切",
      "images": [],
      "contentSource": "完整文章",
      "content": "The advancement of vision-language models (VLMs) is hampered by a fragmented landscape of inconsistent and contaminated public datasets. We introduce FineVision, a meticulously collected, curated, and unified corpus of 24 million samples - the largest open resource of its kind. We unify more than 200 sources into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation performs bulk ingestion and schema mapping, while reviewers audit mappings and spot-check outputs to verify faithful consumption of annotations, appropriate formatting and diversity, and safety; issues trigger targeted fixes and re-runs. The workflow further applies rigorous de-duplication within and across sources and decontamination against 66 public benchmarks. FineVision also encompasses agentic/GUI tasks with a unified action space; reviewers validate schemas and inspect a sample of trajectories to confirm executable fidelity. Models trained on FineVision consistently outperform those trained on existing open mixtures across a broad evaluation suite, underscoring the benefits of scale, data hygiene, and balanced automation with human oversight. We release the corpus and curation tools to accelerate data-centric VLM research."
    },
    {
      "title": "Uniworld-V2：通过扩散负感知微调和多模态大语言模型隐式反馈强化图像编辑 (原标题: Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback)",
      "link": "https://arxiv.org/abs/2510.16888",
      "pubDate": "Sun, 19 Oct 2025 11:38:06 GMT",
      "isoDate": "2025-10-19T11:38:06.000Z",
      "creator": "Zongjian Li, Zheyuan Liu, Qihui Zhang, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Yang Ye, Wangbo Yu, Yuwei Niu, Li Yuan",
      "summary": "### Uniworld-V2：通过扩散负感知微调和多模态大语言模型隐式反馈强化图像编辑\n\n**1. 引言与背景**\n*   指令式图像编辑技术已取得显著进展。\n*   然而，仅通过监督微调训练的模型常面临过度拟合标注模式的问题，这限制了它们在训练数据分布之外的探索和泛化能力。\n\n**2. 提出的解决方案：Edit-R1 框架**\n*   为解决现有模型的局限性，本文引入了 Edit-R1，一个基于策略优化的新型指令式图像编辑后训练框架。\n*   该框架旨在提高模型的泛化能力和编辑效果。\n\n**3. Edit-R1 的核心组件**\n*   **扩散负感知微调 (DiffusionNFT)**\n    *   这是一种无似然的策略优化方法，与流匹配前向过程保持一致。\n    *   其优势在于能够支持使用更高阶的采样器，并实现更高效的模型训练。\n*   **多模态大语言模型 (MLLM) 作为统一奖励模型**\n    *   指令和任务的多样性导致缺乏通用的奖励模型，是图像编辑领域的一个关键挑战。\n    *   Edit-R1 利用 MLLM 作为统一的、无需训练的奖励模型，有效弥补了这一空白。\n    *   MLLM 的输出 logits 被用于提供细粒度的反馈，指导优化过程。\n*   **低方差组过滤机制**\n    *   为了减少 MLLM 评分过程中可能出现的噪音，并稳定优化过程，本文精心设计了低方差组过滤机制。\n\n**4. 实验结果与性能**\n*   使用 Edit-R1 框架训练的 UniWorld-V2 模型在 ImgEdit 和 GEdit-Bench 基准测试上取得了 **最先进 (state-of-the-art)** 的结果。\n    *   ImgEdit 得分：4.49\n    *   GEdit-Bench 得分：7.83\n*   该框架展现出卓越的**模型无关性 (model-agnostic)**。\n    *   将其应用于 Qwen-Image-Edit 和 FLUX-Kontext 等多种不同的基础模型时，均能带来显著的性能提升。\n    *   这充分证明了 Edit-R1 框架的广泛适用性。\n\n**5. 可用性**\n*   相关的代码和模型已公开发布，以便研究社区进一步使用和探索。",
      "shortSummary": "Uniworld-V2 引入 Edit-R1 框架，通过策略优化强化指令式图像编辑。该框架采用扩散负感知微调（DiffusionNFT）进行高效训练，并利用多模态大语言模型（MLLM）作为统一的、无需训练的奖励模型提供细粒度反馈，辅以低方差组过滤机制稳定优化。UniWorld-V2 在 ImgEdit 和 GEdit-Bench 基准测试上取得了最先进的性能，并展现出模型无关性，可显著提升多种基础模型的表现。代码和模型已公开。",
      "translated_title": "Uniworld-V2：通过扩散负感知微调和多模态大语言模型隐式反馈强化图像编辑",
      "images": [],
      "contentSource": "完整文章",
      "content": "Instruction-based image editing has achieved remarkable progress; however, models solely trained via supervised fine-tuning often overfit to annotated patterns, hindering their ability to explore and generalize beyond training distributions. To this end, we introduce Edit-R1, a novel post-training framework for instruction-based image editing based on policy optimization. Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a likelihood-free policy optimization method consistent with the flow matching forward process, thereby enabling the use of higher-order samplers and more efficient training. Another key challenge here is the absence of a universal reward model, resulting from the diverse nature of editing instructions and tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM) as a unified, training-free reward model, leveraging its output logits to provide fine-grained feedback. Furthermore, we carefully design a low-variance group filtering mechanism to reduce MLLM scoring noise and stabilize optimization. UniWorld-V2, trained with this framework, achieves state-of-the-art results on the ImgEdit and GEdit-Bench benchmarks, scoring 4.49 and 7.83, respectively. Crucially, our framework is model-agnostic, delivering substantial performance gains when applied to diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its wide applicability. Code and models are publicly available at https://github.com/PKU-YuanGroup/UniWorld-V2."
    },
    {
      "title": "DeepAnalyze：用于自主数据科学的智能体大型语言模型 (原标题: DeepAnalyze: Agentic Large Language Models for Autonomous Data Science)",
      "link": "https://arxiv.org/abs/2510.16872",
      "pubDate": "Sun, 19 Oct 2025 11:13:42 GMT",
      "isoDate": "2025-10-19T11:13:42.000Z",
      "creator": "Shaolei Zhang, Ju Fan, Meihao Fan, Guoliang Li, Xiaoyong Du",
      "summary": "## DeepAnalyze：用于自主数据科学的智能体大型语言模型\n\n### 引言\n\n自主数据科学，即从原始数据源到分析师级深度研究报告的整个过程，长期以来一直是一个巨大的挑战。然而，随着强大的大型语言模型（LLMs）的出现，这一目标正变得越来越可行。尽管近期基于工作流的数据智能体在特定数据任务上展现出可喜的成果，但由于其对预定义工作流的依赖，它们在实现完全自主的数据科学方面仍存在根本性限制。\n\n### DeepAnalyze-8B 介绍\n\n本文介绍了 **DeepAnalyze-8B**，这是首个专为自主数据科学设计的智能体大型语言模型。它能够自动完成从数据源到分析师级深度研究报告的端到端管道，从而克服了现有方法的局限性。\n\n### 核心方法论\n\n为了应对高复杂度的科学数据任务，DeepAnalyze-8B 采用了以下两种创新框架：\n\n*   **课程式智能体训练范式：**\n    *   该范式模拟了人类数据科学家的学习轨迹。\n    *   它使LLMs能够在真实世界环境中逐步获取和整合多种能力。\n*   **数据驱动的轨迹合成框架：**\n    *   该框架用于构建高质量的训练数据。\n    *   通过这种方式，模型能够学习到复杂的数据科学任务所需的各种技能。\n\n### 能力与性能\n\n通过智能体训练，DeepAnalyze 学习执行广泛的数据任务，包括但不限于：\n\n*   数据问答\n*   专业分析任务\n*   开放式数据研究\n\n实验结果表明，DeepAnalyze 仅用 8B 参数，其性能就超越了之前基于最先进专有LLM构建的工作流智能体，这充分证明了其卓越的效率和能力。\n\n### 开放资源与未来展望\n\nDeepAnalyze 的模型、代码和训练数据均已开源。这一举措为自主数据科学领域的发展铺平了道路，鼓励了社区的进一步研究和应用。",
      "shortSummary": "DeepAnalyze-8B是首个用于自主数据科学的智能体大型语言模型，能够端到端地完成从数据源到深度研究报告的全过程。它采用课程式智能体训练和数据驱动的轨迹合成框架，模拟人类数据科学家学习，从而执行广泛的数据任务。实验证明，DeepAnalyze-8B以8B参数超越了基于先进专有LLM的工作流智能体。其模型、代码和训练数据已开源，推动了自主数据科学的发展。",
      "translated_title": "DeepAnalyze：用于自主数据科学的智能体大型语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Autonomous data science, from raw data sources to analyst-grade deep research reports, has been a long-standing challenge, and is now becoming feasible with the emergence of powerful large language models (LLMs). Recent workflow-based data agents have shown promising results on specific data tasks but remain fundamentally limited in achieving fully autonomous data science due to their reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B, the first agentic LLM designed for autonomous data science, capable of automatically completing the end-toend pipeline from data sources to analyst-grade deep research reports. To tackle high-complexity data science tasks, we propose a curriculum-based agentic training paradigm that emulates the learning trajectory of human data scientists, enabling LLMs to progressively acquire and integrate multiple capabilities in real-world environments. We also introduce a data-grounded trajectory synthesis framework that constructs high-quality training data. Through agentic training, DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data question answering and specialized analytical tasks to open-ended data research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze outperforms previous workflow-based agents built on most advanced proprietary LLMs. The model, code, and training data of DeepAnalyze are open-sourced, paving the way toward autonomous data science."
    },
    {
      "title": "视觉自回归模型在推理时间扩展方面超越扩散模型 (原标题: Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling)",
      "link": "https://arxiv.org/abs/2510.16751",
      "pubDate": "Sun, 19 Oct 2025 04:28:06 GMT",
      "isoDate": "2025-10-19T04:28:06.000Z",
      "creator": "Erik Riise, Mehmet Onurcan Kaya, Dim P. Papadopoulos",
      "summary": "### 视觉自回归模型在推理时间扩展方面的优势\n\n本文探讨了将大型语言模型（LLM）中通过搜索实现的推理时间扩展优势应用于图像生成领域的挑战，并提出了一种解决方案。\n\n**核心问题：**\n*   尽管搜索策略极大地改进了LLM的推理时间扩展，但将其成功应用于图像生成领域却十分困难。\n*   近期尝试将搜索策略应用于连续扩散模型的效果有限，通常简单的随机采样表现最佳。\n\n**解决方案与方法：**\n*   研究表明，视觉自回归模型（Visual Autoregressive Models, VARMs）的离散、顺序特性使其能够有效地进行图像生成搜索。\n*   具体而言，束搜索（beam search）被证明可以显著提升文本到图像的生成质量。\n\n**主要发现与优势：**\n*   **性能超越：** 一个20亿参数的自回归模型，通过应用束搜索，在各项基准测试中超越了一个120亿参数的扩散模型。这表明在特定条件下，较小的自回归模型可以比更大的扩散模型表现更好。\n*   **离散令牌空间优势：** 这种优势主要来源于离散的令牌空间。离散令牌空间允许：\n    *   **早期剪枝（early pruning）：** 在生成过程的早期阶段即可识别并排除低质量的路径。\n    *   **计算复用（computational reuse）：** 共享计算资源，提高效率。\n*   **验证器分析：** 对验证器（verifier）的分析揭示了速度与推理能力之间的权衡。\n*   **架构的重要性：** 这些发现强调，在视觉生成领域，模型架构（而非仅仅模型规模）对于推理时间优化至关重要。\n\n**结论：**\n本文的研究结果表明，视觉自回归模型凭借其离散和顺序的特性，结合束搜索等有效策略，在推理时间扩展方面展现出优于扩散模型的潜力。这为未来视觉生成模型的设计和优化提供了新的方向，强调了架构选择在实现高效性能中的关键作用。",
      "shortSummary": "本文指出，视觉自回归模型（VARMs）通过利用其离散、顺序的特性，结合束搜索，在推理时间扩展方面超越了扩散模型。一个20亿参数的VARMs甚至能胜过120亿参数的扩散模型。其优势在于离散令牌空间带来的早期剪枝和计算复用。研究强调，模型架构而非仅仅规模，对视觉生成的推理时间优化至关重要。",
      "translated_title": "视觉自回归模型在推理时间扩展方面超越扩散模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "While inference-time scaling through search has revolutionized Large Language Models, translating these gains to image generation has proven difficult. Recent attempts to apply search strategies to continuous diffusion models show limited benefits, with simple random sampling often performing best. We demonstrate that the discrete, sequential nature of visual autoregressive models enables effective search for image generation. We show that beam search substantially improves text-to-image generation, enabling a 2B parameter autoregressive model to outperform a 12B parameter diffusion model across benchmarks. Systematic ablations show that this advantage comes from the discrete token space, which allows early pruning and computational reuse, and our verifier analysis highlights trade-offs between speed and reasoning capability. These findings suggest that model architecture, not just scale, is critical for inference-time optimization in visual generation."
    },
    {
      "title": "超越流水线：模型原生智能体AI范式转变的综述 (原标题: Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI)",
      "link": "https://arxiv.org/abs/2510.16720",
      "pubDate": "Sun, 19 Oct 2025 01:23:43 GMT",
      "isoDate": "2025-10-19T01:23:43.000Z",
      "creator": "Jitao Sang, Jinlin Xiao, Jiarun Han, Jilin Chen, Xiaoyi Chen, Shuyu Wei, Yongjie Sun, Yuhang Wang",
      "summary": "## 模型原生智能体AI的范式转变\n\n### 引言\n智能体AI（Agentic AI）的快速发展标志着人工智能进入一个新阶段。在这个阶段，大型语言模型（LLMs）不再仅仅是被动响应，而是能够主动行动、进行推理并适应环境。\n\n### 范式转变：从流水线到模型原生\n本综述追溯了构建智能体AI的范式转变：\n*   **流水线（Pipeline-based）系统：** 在这种传统方法中，规划、工具使用和记忆等核心能力由外部逻辑进行编排和管理。\n*   **模型原生（Model-native）范式：** 这是一种新兴范式，其中上述能力被内化到模型的参数中，成为模型自身学习和行为的一部分。\n\n### 强化学习（RL）作为驱动引擎\n*   文章将强化学习（RL）定位为实现这一范式转变的算法引擎。\n*   RL通过将学习从模仿静态数据重构为以结果为导向的探索，为跨语言、视觉和具身（embodied）领域的“LLM + RL + 任务”提供了一个统一的解决方案。\n\n### 核心能力的演进\n综述系统地回顾了各项核心能力如何从外部脚本模块演变为端到端学习的行为：\n*   **规划（Planning）：** 从依赖外部预设的规划逻辑，发展为模型内部自主学习和生成规划。\n*   **工具使用（Tool use）：** 从外部调用和管理工具，发展为模型能够内化工具的使用逻辑并自主选择和操作工具。\n*   **记忆（Memory）：** 从外部存储和检索信息，发展为模型内部学习和管理记忆，以支持长期的推理和适应。\n\n### 对主要智能体应用的影响\n这种范式转变已经重塑了主要的智能体应用：\n*   **深度研究智能体（Deep Research agent）：** 更加强调长周期推理能力，能够进行更复杂、更深入的研究任务。\n*   **GUI智能体（GUI agent）：** 更加强调具身交互能力，能够更好地理解和操作图形用户界面。\n\n### 未来的发展方向\n文章最后讨论了智能体能力的持续内化，包括：\n*   **多智能体协作（Multi-agent collaboration）：** 模型将能够更有效地进行内部或外部的多智能体协作。\n*   **反思（Reflection）：** 模型将具备更强的自我反思能力，以改进其行为和决策。\n*   此外，还探讨了系统层和模型层在未来智能体AI中角色的演变。\n\n### 结论\n这些发展共同描绘了模型原生智能体AI作为集成学习和交互框架的连贯轨迹，标志着人工智能从构建应用智能的系统向开发通过经验增长智能的模型转变。",
      "shortSummary": "该综述探讨了智能体AI从流水线系统向模型原生范式的转变，其中大型语言模型（LLMs）的规划、工具使用和记忆等核心能力被内化。强化学习（RL）是推动这一转变的关键算法引擎，它将学习重塑为以结果为导向的探索。文章审视了核心能力的演进及其对深度研究和GUI智能体等应用的影响，并展望了多智能体协作和反思等能力的持续内化，预示着AI将从应用智能转向通过经验增长智能。",
      "translated_title": "超越流水线：模型原生智能体AI范式转变的综述",
      "images": [],
      "contentSource": "完整文章",
      "content": "The rapid evolution of agentic AI marks a new phase in artificial intelligence, where Large Language Models (LLMs) no longer merely respond but act, reason, and adapt. This survey traces the paradigm shift in building agentic AI: from Pipeline-based systems, where planning, tool use, and memory are orchestrated by external logic, to the emerging Model-native paradigm, where these capabilities are internalized within the model's parameters. We first position Reinforcement Learning (RL) as the algorithmic engine enabling this paradigm shift. By reframing learning from imitating static data to outcome-driven exploration, RL underpins a unified solution of LLM + RL + Task across language, vision and embodied domains. Building on this, the survey systematically reviews how each capability -- Planning, Tool use, and Memory -- has evolved from externally scripted modules to end-to-end learned behaviors. Furthermore, it examines how this paradigm shift has reshaped major agent applications, specifically the Deep Research agent emphasizing long-horizon reasoning and the GUI agent emphasizing embodied interaction. We conclude by discussing the continued internalization of agentic capabilities like Multi-agent collaboration and Reflection, alongside the evolving roles of the system and model layers in future agentic AI. Together, these developments outline a coherent trajectory toward model-native agentic AI as an integrated learning and interaction framework, marking the transition from constructing systems that apply intelligence to developing models that grow intelligence through experience."
    },
    {
      "title": "MultiVerse：一个用于评估大型视觉和语言模型的多轮对话基准 (原标题: MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models)",
      "link": "https://arxiv.org/abs/2510.16641",
      "pubDate": "Sat, 18 Oct 2025 17:00:12 GMT",
      "isoDate": "2025-10-18T17:00:12.000Z",
      "creator": "Young-Jun Lee, Byung-Kwan Lee, Jianshu Zhang, Yechan Hwang, Byungsoo Ko, Han-Gyu Kim, Dongyu Yao, Xuankun Rong, Eojin Joo, Seung-Ho Han, Bowon Ko, Ho-Jin Choi",
      "summary": "# MultiVerse：一个用于评估大型视觉和语言模型的多轮对话基准\n\n## 引言与背景\n*   视觉与语言模型（VLMs）在单轮基准测试中展现出令人印象深刻的能力，但现实世界的应用往往需要更复杂的多轮对话。\n*   现有的多轮数据集（如MMDU、ConvBench）仅部分捕捉了用户遇到的对话场景的广度和深度。\n\n## MultiVerse 基准介绍\n*   **目的：** 针对现有数据集的局限性，本文引入了MultiVerse，一个新颖的多轮对话基准。\n*   **规模与来源：**\n    *   包含647个对话，每个对话平均有四轮。\n    *   这些对话来源于12个流行的VLM评估基准。\n*   **内容与覆盖范围：**\n    *   涵盖484个任务和484个交互目标。\n    *   主题广泛，包括事实知识、感知以及数学和编码等高级推理任务。\n\n## 评估方法\n*   **评估机制：** 提出了一种基于清单的评估方法。\n*   **自动化评估器：** 利用GPT-4o作为自动化评估器。\n*   **评估维度：** 衡量37个关键方面，包括感知准确性、语言清晰度和事实正确性。\n\n## 实验结果与发现\n*   **评估模型：** 对18个VLM进行了评估。\n*   **挑战性：** 即使是最强的模型（如GPT-4o）在复杂的多轮对话中也仅达到50%的成功率，突显了该数据集的挑战性。\n*   **关键发现：** 为较小或较弱的模型提供完整的对话上下文能显著提高其性能，强调了上下文学习的重要性。\n\n## 结论\n*   MultiVerse被认为是一个评估VLM多轮交互能力的宝贵基准。",
      "shortSummary": "MultiVerse是一个新颖的多轮对话基准，旨在评估大型视觉与语言模型（VLMs）在复杂交互中的能力。该基准包含647个对话，源自12个现有VLM基准，涵盖感知、推理、数学和编码等484个任务。通过GPT-4o进行清单式评估，发现即使是GPT-4o等最强模型也仅有50%的成功率，凸显了其挑战性。研究还强调了提供完整对话上下文对提升模型性能的重要性，为VLM的多轮交互能力评估提供了新视角。",
      "translated_title": "MultiVerse：一个用于评估大型视觉和语言模型的多轮对话基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-and-Language Models (VLMs) have shown impressive capabilities on single-turn benchmarks, yet real-world applications often demand more intricate multi-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only partially capture the breadth and depth of conversational scenarios encountered by users. In this work, we introduce MultiVerse, a novel multi-turn conversation benchmark featuring 647 dialogues - each averaging four turns - derived from a diverse set of 12 popular VLM evaluation benchmarks. With 484 tasks and 484 interaction goals, MultiVerse covers a wide range of topics, from factual knowledge and perception to advanced reasoning tasks such as mathematics and coding. To facilitate robust assessment, we propose a checklist-based evaluation method that leverages GPT-4o as the automated evaluator, measuring performance across 37 key aspects, including perceptual accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on MultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve only a 50% success rate in complex multi-turn conversations, highlighting the dataset's challenging nature. Notably, we find that providing full dialogue context significantly enhances performance for smaller or weaker models, emphasizing the importance of in-context learning. We believe MultiVerse is a landscape of evaluating multi-turn interaction abilities for VLMs."
    },
    {
      "title": "强化学习使多模态大语言模型（MLLM）比监督微调（SFT）看得更好 (原标题: RL makes MLLMs see better than SFT)",
      "link": "https://arxiv.org/abs/2510.16333",
      "pubDate": "Fri, 17 Oct 2025 23:37:17 GMT",
      "isoDate": "2025-10-17T23:37:17.000Z",
      "creator": "Junha Song, Sangdoo Yun, Dongyoon Han, Jaegul Choo, Byeongho Heo",
      "summary": "### 强化学习提升多模态大语言模型（MLLM）视觉能力\n\n多模态大语言模型（MLLM）的研究普遍认为其性能主要源于其庞大的语言模型（LLM）骨干。然而，这导致了对决定MLLM如何感知图像的视觉编码器理解的不足。随着MLLM训练范式从监督微调（SFT）转向强化学习（RL），这一问题变得更加突出，即缺乏对训练如何重塑视觉编码器和MLLM本身的分析。\n\n为了解决这一问题，本研究进行了深入调查，主要发现如下：\n\n*   **训练策略对MLLM性能的影响**\n    *   研究首先调查了不同训练策略对MLLM的影响。\n    *   结果表明，在与视觉强相关的VQA（视觉问答）基准测试中，强化学习（RL）相比监督微调（SFT）表现出明显的优势。\n\n*   **视觉编码器的关键分析**\n    *   受上述发现的启发，研究对MLLM的视觉编码器进行了关键且此前未充分探索的分析。\n    *   通过ImageNet分类、分割和梯度可视化等多样化且深入的实验，研究发现MLLM的后训练策略（SFT或RL）不仅导致MLLM下游任务产生不同的结果，而且从根本上重塑了MLLM底层的视觉表示。\n\n*   **RL的优势：更强、更精确的视觉表示**\n    *   本研究的关键发现是，与SFT相比，RL能够产生更强且定位更精确的视觉表示。\n    *   这显著提升了MLLM视觉编码器的能力。\n\n*   **提出PIVOT方法**\n    *   研究将这些发现提炼成一个构建强大MLLM视觉编码器的简单方法：偏好指导的视觉优化（Preference-Instructed Vision OpTimization, PIVOT）。\n    *   当集成到MLLM中时，经过PIVOT训练的视觉编码器表现优于甚至更大、训练更重的同类模型。\n    *   更重要的是，PIVOT所需的计算成本不到标准视觉预训练的1%。\n\n*   **研究意义**\n    *   这一结果为推进MLLM的视觉骨干网络提供了一条有效且高效的途径。",
      "shortSummary": "本研究发现，强化学习（RL）比监督微调（SFT）更能提升多模态大语言模型（MLLM）的视觉编码器性能。RL能产生更强、定位更精确的视觉表示，从而在视觉问答（VQA）任务中表现出明显优势。研究提出了一种名为PIVOT（偏好指导的视觉优化）的新方法，它能以极低的计算成本构建出强大的视觉编码器，超越现有大型模型，为MLLM视觉能力的提升开辟了高效途径。",
      "translated_title": "强化学习使多模态大语言模型（MLLM）比监督微调（SFT）看得更好",
      "images": [],
      "contentSource": "完整文章",
      "content": "A dominant assumption in Multimodal Language Model (MLLM) research is that its performance is largely inherited from the LLM backbone, given its immense parameter scale and remarkable capabilities. This has created a void in the understanding of the vision encoder, which determines how MLLMs perceive images. The recent shift in MLLM training paradigms, from Supervised Finetuning (SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the significant lack of analysis on how such training reshapes the vision encoder as well as the MLLM. To address this, we first investigate the impact of training strategies on MLLMs, where RL shows a clear advantage over SFT in strongly vision-related VQA benchmarks. Motivated by this, we conduct a critical yet under-explored analysis of the vision encoder of MLLMs through diverse and in-depth experiments, ranging from ImageNet classification and segmentation to gradient visualization. Our results demonstrate that MLLM's post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual representations. Specifically, the key finding of our study is that RL produces stronger and precisely localized visual representations compared to SFT, boosting the ability of the vision encoder for MLLM. We then reframe our findings into a simple recipe for building strong vision encoders for MLLMs, Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs, a PIVOT-trained vision encoder outperforms even larger and more heavily-trained counterparts, despite requiring less than 1% of the computational cost of standard vision pretraining. This result opens an effective and efficient path for advancing the vision backbones of MLLMs. Project page available at https://june-page.github.io/pivot/"
    },
    {
      "title": "大型推理模型上的干扰注入攻击：特征与防御 (原标题: Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense)",
      "link": "https://arxiv.org/abs/2510.16259",
      "pubDate": "Fri, 17 Oct 2025 19:16:34 GMT",
      "isoDate": "2025-10-17T19:16:34.000Z",
      "creator": "Zhehao Zhang, Weijie Xu, Shixian Cui, Chandan K. Reddy",
      "summary": "## 大型推理模型上的干扰注入攻击：特征与防御\n\n### 摘要\n\n本文深入探讨了大型推理模型（LRMs）面临的一种新型关键漏洞，称为“推理干扰”（reasoning distraction）。该漏洞通过在提示中恶意嵌入不相关但复杂的任务，使LRMs偏离其主要目标，从而严重影响其性能和可靠性。\n\n### 关键发现\n\n*   **漏洞识别与定义**：\n    *   研究识别并系统分析了“推理干扰”这一关键漏洞。\n    *   推理干扰是指LRMs被恶意嵌入提示中的不相关但复杂的任务所转移，使其无法专注于主要目标。\n\n*   **广泛影响**：\n    *   通过对多种模型和基准进行全面研究，发现即使是最先进的LRMs也极易受到此漏洞的影响。\n    *   注入的干扰器可导致任务准确性降低高达60%。\n\n*   **潜在加剧因素**：\n    *   研究揭示，某些对齐技术（alignment techniques）反而可能放大这种弱点。\n    *   模型可能表现出“隐蔽顺从”（covert compliance），即在推理过程中遵循隐藏的对抗性指令，但在最终输出中将其隐藏起来，增加了检测难度。\n\n### 防御策略\n\n*   **训练基础防御**：\n    *   为缓解这些风险，本文提出了一种基于训练的防御方法。\n    *   该方法结合了监督微调（Supervised Fine-Tuning, SFT）和强化学习（Reinforcement Learning, RL）。\n    *   防御训练利用合成对抗性数据进行。\n\n*   **显著效果**：\n    *   该防御方案在应对具有挑战性的干扰攻击时，能够将模型的鲁棒性提高50多个百分点。\n\n### 结论与意义\n\n*   本文的研究确立了推理干扰作为LRM可靠性面临的一个独特且紧迫的威胁。\n*   所提出的防御方法为构建更安全、更值得信赖的推理系统迈出了实用的一步。",
      "shortSummary": "本文揭示了大型推理模型（LRMs）面临的“推理干扰”漏洞，即恶意嵌入不相关复杂任务导致模型偏离目标。研究发现，此攻击可使LRMs准确性降低高达60%，且某些对齐技术会加剧此问题，模型可能表现出隐蔽顺从。为应对此威胁，提出了一种结合监督微调和强化学习的训练防御方案，可将鲁棒性提高50多个百分点，为提升LRM可靠性提供了实用路径。",
      "translated_title": "大型推理模型上的干扰注入攻击：特征与防御",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in large reasoning models (LRMs) have enabled remarkable performance on complex tasks such as mathematics and coding by generating long Chain-of-Thought (CoT) traces. In this paper, we identify and systematically analyze a critical vulnerability we term reasoning distraction, where LRMs are diverted from their primary objective by irrelevant yet complex tasks maliciously embedded in the prompt. Through a comprehensive study across diverse models and benchmarks, we show that even state-of-the-art LRMs are highly susceptible, with injected distractors reducing task accuracy by up to 60%. We further reveal that certain alignment techniques can amplify this weakness and that models may exhibit covert compliance, following hidden adversarial instructions in reasoning while concealing them in the final output. To mitigate these risks, we propose a training-based defense that combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on synthetic adversarial data, improving robustness by over 50 points on challenging distractor attacks. Our findings establish reasoning distraction as a distinct and urgent threat to LRM reliability and provide a practical step toward safer and more trustworthy reasoning systems."
    },
    {
      "title": "Embody 3D：一个大规模多模态运动与行为数据集 (原标题: Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset)",
      "link": "https://arxiv.org/abs/2510.16258",
      "pubDate": "Fri, 17 Oct 2025 19:06:36 GMT",
      "isoDate": "2025-10-17T19:06:36.000Z",
      "creator": "Claire McLean, Makenzie Meendering, Tristan Swartz, Orri Gabbay, Alexandra Olsen, Rachel Jacobs, Nicholas Rosen, Philippe de Bree, Tony Garcia, Gadsden Merrill, Jake Sandakly, Julia Buffalini, Neham Jain, Steven Krenn, Moneish Kumar, Dejan Markovic, Evonne Ng, Fabian Prada, Andrew Saba, Siwei Zhang, Vasu Agrawal, Tim Godisart, Alexander Richard, Michael Zollhoefer",
      "summary": "## Embody 3D：大规模多模态运动与行为数据集\n\nMeta的Codec Avatars Lab推出了**Embody 3D**，这是一个旨在推动计算机视觉和模式识别领域研究的大规模多模态数据集。\n\n### 数据集概述\n*   **发布机构**：Meta的Codec Avatars Lab。\n*   **数据规模**：\n    *   包含500个独立小时的3D运动数据。\n    *   数据来源于439名不同的参与者。\n    *   总计超过5400万帧的跟踪3D运动数据。\n*   **采集方式**：数据在一个多摄像头采集舞台中收集。\n\n### 数据内容与类型\nEmbody 3D数据集涵盖了广泛的单人和多人运动与行为数据：\n\n#### 单人运动数据\n*   **提示性动作**：根据特定提示执行的动作。\n*   **手势**：各种手部动作和姿态。\n*   **移动**：包括行走、跑步等各种形式的身体移动。\n\n#### 多人行为与对话数据\n*   **讨论**：多个人之间的对话和交流。\n*   **不同情绪状态下的对话**：捕捉参与者在不同情绪（如高兴、悲伤、愤怒）下的对话。\n*   **协作活动**：多个人共同完成任务或活动的场景。\n*   **共同生活场景**：在类似公寓的空间中模拟日常共同生活的场景。\n\n### 提供的数据格式\n该数据集为每个参与者提供以下多种数据类型：\n*   **跟踪的人体运动数据**：包括详细的身体运动轨迹。\n*   **手部跟踪数据**：精确记录手部动作和姿态。\n*   **身体形状数据**：提供参与者的身体形态信息。\n*   **文本注释**：对场景和行为的文字描述。\n*   **独立的音频轨道**：每个参与者都有单独的音频记录，便于分析语音和语调。",
      "shortSummary": "Meta的Codec Avatars Lab推出了Embody 3D，这是一个大规模多模态3D运动与行为数据集。它包含来自439名参与者的500小时数据，总计超过5400万帧。数据集涵盖单人运动（如手势、移动）和多人行为（如对话、协作、共同生活场景）。提供的数据包括跟踪的人体运动（含手部和身体形状）、文本注释以及独立的音频轨道，旨在支持计算机视觉和模式识别领域的研究。",
      "translated_title": "Embody 3D：一个大规模多模态运动与行为数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of 500 individual hours of 3D motion data from 439 participants collected in a multi-camera collection stage, amounting to over 54 million frames of tracked 3D motion. The dataset features a wide range of single-person motion data, including prompted motions, hand gestures, and locomotion; as well as multi-person behavioral and conversational data like discussions, conversations in different emotional states, collaborative activities, and co-living scenarios in an apartment-like space. We provide tracked human motion including hand tracking and body shape, text annotations, and a separate audio track for each participant."
    },
    {
      "title": "AsyncVoice Agent：LLM规划与推理的实时解释 (原标题: AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning)",
      "link": "https://arxiv.org/abs/2510.16156",
      "pubDate": "Fri, 17 Oct 2025 15:00:08 GMT",
      "isoDate": "2025-10-17T15:00:08.000Z",
      "creator": "Yueqian Lin, Zhengmian Hu, Jayakumar Subramanian, Qinsi Wang, Nikos Vlassis, Hai \"Helen\" Li, Yiran Chen",
      "summary": "AsyncVoice Agent：LLM规划与推理的实时解释\n\n*   **引言：当前人机协作的挑战**\n    *   在复杂的推理任务中，有效的人工智能协作要求用户不仅接收最终输出，更要理解并与模型的处理过程进行互动。\n    *   然而，像思维链（Chain-of-Thought, CoT）等现有方法产生的单一文本输出，阻碍了这种互动，因为当前界面缺乏实时口头表达和强大的用户打断（barge-in）功能。\n\n*   **AsyncVoice Agent：解决方案概述**\n    *   本文提出了AsyncVoice Agent，一个旨在解决上述挑战的系统。\n    *   **核心架构：** 它采用异步架构，将流式大型语言模型（LLM）后端与会话语音前端解耦。\n    *   **工作原理：** 这种设计允许模型的叙述（verbalization）和推理（inference）过程并行运行。\n\n*   **主要优势与功能**\n    *   **实时交互性：** 赋能用户在任何时候打断、查询和引导模型的推理过程，实现真正的双向对话。\n    *   **显著降低延迟：** 客观基准测试显示，与传统的单一（monolithic）基线方法相比，AsyncVoice Agent将交互延迟降低了600多倍。\n    *   **高保真与准确性：** 在实现低延迟的同时，该系统仍能确保高保真度和具有竞争力的任务准确性。\n    *   **新范式：** 通过实现与模型思维过程的双向对话，AsyncVoice Agent为构建更有效、更可控、更值得信赖的高风险任务人机系统提供了一种新范式。\n\n*   **发表信息**\n    *   该论文已被IEEE ASRU 2025 Demo Track接受。",
      "shortSummary": "AsyncVoice Agent是一个创新系统，旨在解决LLM推理中缺乏实时交互和用户理解的问题。它采用异步架构，将LLM后端与语音前端解耦，使叙述和推理并行进行。这使用户能够随时打断、查询和引导模型的思考过程。该方法将交互延迟降低了600多倍，同时保持高准确性，为高风险任务的人机协作提供了更有效、可控和值得信赖的新范式。",
      "translated_title": "AsyncVoice Agent：LLM规划与推理的实时解释",
      "images": [],
      "contentSource": "完整文章",
      "content": "Effective human-AI collaboration on complex reasoning tasks requires that users understand and interact with the model's process, not just receive an output. However, the monolithic text from methods like Chain-of-Thought (CoT) prevents this, as current interfaces lack real-time verbalization and robust user barge-in. We present AsyncVoice Agent, a system whose asynchronous architecture decouples a streaming LLM backend from a conversational voice frontend. This design allows narration and inference to run in parallel, empowering users to interrupt, query, and steer the model's reasoning process at any time. Objective benchmarks show this approach reduces interaction latency by more than 600x compared to monolithic baselines while ensuring high fidelity and competitive task accuracy. By enabling a two-way dialogue with a model's thought process, AsyncVoice Agent offers a new paradigm for building more effective, steerable, and trustworthy human-AI systems for high-stakes tasks."
    }
  ],
  "lastUpdated": "2025-10-21T09:36:35.680Z"
}