{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "WebChoreArena：评估网络浏览代理在现实繁琐网络任务中的表现 (原标题: WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks)",
      "link": "https://arxiv.org/abs/2506.01952",
      "pubDate": "Mon, 02 Jun 2025 13:59:45 GMT",
      "isoDate": "2025-06-02T13:59:45.000Z",
      "creator": "Atsuyuki Miyai, Zaiying Zhao, Kazuki Egashira, Atsuki Sato, Tatsumi Sunada, Shota Onohara, Hiromasa Yamanishi, Mashiro Toyooka, Kunato Nishina, Ryoma Maeda, Kiyoharu Aizawa, Toshihiko Yamasaki",
      "summary": "## WebChoreArena：评估网络浏览代理在现实繁琐网络任务中的表现\n\n**引言**\n\n大型语言模型（LLM）驱动的网络浏览代理能够以类似人类的方式操作网络浏览器，为自动化日常任务提供了高度透明的途径。随着网络代理的能力不断增强，并在通用浏览任务中展现出熟练度，一个关键问题浮出水面：它们能否超越通用浏览，稳健地处理那些对人类而言繁琐、复杂或通常会避免的“家务”式任务？\n\n**WebChoreArena 介绍**\n\n本文引入了 **WebChoreArena**，这是一个全新的、完全可复现的基准测试。它包含 532 个精心策划的任务，旨在将 WebArena 的范围从通用浏览扩展到更耗时、更繁琐的任务。\n\n**WebChoreArena 集成的三大关键挑战**\n\nWebChoreArena 系统地整合了以下三个核心挑战，以更全面地评估代理的能力：\n\n1.  **海量记忆任务 (Massive Memory tasks)**：要求代理能够从观察中准确检索大量信息。\n2.  **计算任务 (Calculation tasks)**：要求代理具备精确的数学推理能力。\n3.  **长期记忆任务 (Long-Term Memory tasks)**：要求代理在多个网页之间保持长期记忆。\n\n**基准的构建与优势**\n\nWebChoreArena 建立在四个完全可复现且广泛采用的 WebArena 模拟环境之上，这确保了严格的可复现性，并能够与已建立的 WebArena 基准进行公平、直接的比较，从而为代理的进展提供关键洞察。\n\n**实验结果与发现**\n\n实验结果表明，随着 LLM 的发展（以 GPT-4o、Claude 3.7 Sonnet 和 Gemini 2.5 Pro 为代表），它们在 WebChoreArena 上的性能观察到显著提升。这些发现表明，WebChoreArena 非常适合更清晰地衡量最先进 LLM 的进步。然而，结果也指出，即使是 Gemini 2.5 Pro，与 WebArena 相比，仍有很大的改进空间，这凸显了 WebChoreArena 所带来的更高挑战。",
      "shortSummary": "WebChoreArena 是一个新颖的、可复现的基准测试，旨在评估网络浏览代理处理现实世界中繁琐复杂任务的能力。它包含 532 个任务，扩展了 WebArena 的范围，并集成了海量记忆、计算和长期记忆三大挑战。实验结果显示，GPT-4o、Claude 3.7 Sonnet 和 Gemini 2.5 Pro 等先进大型语言模型在 WebChoreArena 上表现出显著进步，但仍有较大提升空间，凸显了该基准的挑战性。",
      "translated_title": "WebChoreArena：评估网络浏览代理在现实繁琐网络任务中的表现",
      "images": [],
      "contentSource": "完整文章",
      "content": "Powered by a large language model (LLM), a web browsing agent operates web browsers in a human-like manner and offers a highly transparent path toward automating a wide range of everyday tasks. As web agents become increasingly capable and demonstrate proficiency in general browsing tasks, a critical question emerges: Can they go beyond general browsing to robustly handle tasks that are tedious and complex, or chores that humans often avoid doing themselves? In this paper, we introduce WebChoreArena, a new fully reproducible benchmark comprising 532 carefully curated tasks designed to extend the scope of WebArena beyond general browsing to more labor-intensive and tedious tasks. WebChoreArena systematically integrates three key challenges: (i) Massive Memory tasks requiring accurate retrieval of large amounts of information in the observations, (ii) Calculation tasks demanding precise mathematical reasoning, and (iii) Long-Term Memory tasks necessitating long-term memory across multiple webpages. Built on top of the fully reproducible and widely adopted four WebArena simulation environments, WebChoreArena ensures strict reproducibility and enables fair, direct comparisons with the established WebArena benchmark, offering key insights into agent progress. Our experimental results demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7 Sonnet, and Gemini 2.5 Pro, significant improvements in performance are observed on WebChoreArena. These findings suggest that WebChoreArena is well-suited to measure the advancement of state-of-the-art LLMs with greater clarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro, there remains substantial room for improvement compared to WebArena, highlighting the increased challenges posed by WebChoreArena."
    },
    {
      "title": "基于协作轨迹控制的机器人操作视频生成学习 (原标题: Learning Video Generation for Robotic Manipulation with Collaborative Trajectory Control)",
      "link": "https://arxiv.org/abs/2506.01943",
      "pubDate": "Mon, 02 Jun 2025 13:57:06 GMT",
      "isoDate": "2025-06-02T13:57:06.000Z",
      "creator": "Xiao Fu, Xintao Wang, Xian Liu, Jianhong Bai, Runsen Xu, Pengfei Wan, Di Zhang, Dahua Lin",
      "summary": "## RoboMaster：协作轨迹控制下的机器人操作视频生成\n\n### 引言与问题背景\n\n*   **视频扩散模型的潜力：** 近期视频扩散模型在生成机器人决策数据方面展现出巨大潜力，其中轨迹条件进一步实现了精细控制。\n*   **现有方法的局限性：** 然而，现有基于轨迹的方法主要关注单个物体的运动，难以捕捉复杂机器人操作中至关重要的多物体交互。\n*   **核心挑战：** 这种局限性源于重叠区域的多特征纠缠，导致视觉保真度下降。\n\n### RoboMaster 框架\n\n*   **创新点：** 本文提出了一个名为 RoboMaster 的新型框架，通过“协作轨迹公式”来建模物体间的动态。\n*   **交互过程分解：** 与以往分解物体的方法不同，RoboMaster 的核心在于将交互过程分解为三个子阶段：\n    *   **交互前 (pre-interaction)**\n    *   **交互中 (interaction)**\n    *   **交互后 (post-interaction)**\n*   **主导物体特征建模：** 每个阶段都使用“主导物体”的特征进行建模：\n    *   在交互前和交互后阶段，主导物体是机器人手臂。\n    *   在交互中阶段，主导物体是被操作的物体。\n    *   **优势：** 这种方法有效缓解了先前工作中在交互过程中出现的多物体特征融合的缺点。\n*   **语义一致性：** 为了进一步确保视频中主体语义的一致性，RoboMaster 引入了“外观感知和形状感知潜在表示”。\n\n### 实验与成果\n\n*   **评估数据集：** 在具有挑战性的 Bridge V2 数据集上进行了广泛的实验。\n*   **实际环境验证：** 还在实际环境中（in-the-wild）进行了评估。\n*   **性能表现：** 实验结果表明，RoboMaster 方法优于现有方法，在机器人操作的轨迹控制视频生成方面建立了新的最先进性能。",
      "shortSummary": "RoboMaster是一个新颖的机器人操作视频生成框架，旨在解决现有方法在多物体交互中视觉保真度下降的问题。它通过将交互过程分解为交互前、中、后三个阶段，并利用主导物体特征进行建模，有效避免了多特征纠缠。结合外观和形状感知潜在表示，RoboMaster在Bridge V2数据集和实际评估中表现出色，达到了轨迹控制视频生成领域的最新水平。",
      "translated_title": "基于协作轨迹控制的机器人操作视频生成学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in video diffusion models have demonstrated strong potential for generating robotic decision-making data, with trajectory conditions further enabling fine-grained control. However, existing trajectory-based methods primarily focus on individual object motion and struggle to capture multi-object interaction crucial in complex robotic manipulation. This limitation arises from multi-feature entanglement in overlapping regions, which leads to degraded visual fidelity. To address this, we present RoboMaster, a novel framework that models inter-object dynamics through a collaborative trajectory formulation. Unlike prior methods that decompose objects, our core is to decompose the interaction process into three sub-stages: pre-interaction, interaction, and post-interaction. Each stage is modeled using the feature of the dominant object, specifically the robotic arm in the pre- and post-interaction phases and the manipulated object during interaction, thereby mitigating the drawback of multi-object feature fusion present during interaction in prior work. To further ensure subject semantic consistency throughout the video, we incorporate appearance- and shape-aware latent representations for objects. Extensive experiments on the challenging Bridge V2 dataset, as well as in-the-wild evaluation, demonstrate that our method outperforms existing approaches, establishing new state-of-the-art performance in trajectory-controlled video generation for robotic manipulation."
    },
    {
      "title": "超越二八定律：高熵少数令牌驱动大语言模型推理的有效强化学习 (原标题: Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning)",
      "link": "https://arxiv.org/abs/2506.01939",
      "pubDate": "Mon, 02 Jun 2025 13:54:39 GMT",
      "isoDate": "2025-06-02T13:54:39.000Z",
      "creator": "Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, Junyang Lin",
      "summary": "### 1. 研究背景与目的\n\n*   **背景**：可验证奖励强化学习（RLVR）已成为增强大型语言模型（LLM）推理能力的强大方法，但其内在机制尚不明确。\n*   **目的**：本研究首次从令牌熵模式的视角深入探索RLVR，全面分析不同令牌如何影响推理性能。\n\n### 2. 关键发现\n\n*   **链式思考（CoT）推理中的令牌熵模式**：\n    *   观察到只有一小部分令牌表现出高熵。\n    *   这些高熵令牌充当关键的“分岔点”，引导模型走向不同的推理路径。\n*   **RLVR训练中熵模式的演变**：\n    *   RLVR在很大程度上遵循基础模型的熵模式。\n    *   RLVR主要调整的是高熵令牌的熵值。\n*   **高熵令牌的重要性**：\n    *   这些发现强调了高熵令牌（即分岔令牌）对RLVR的重要性。\n    *   RLVR的有效性主要源于优化决定推理方向的高熵令牌。\n\n### 3. 改进RLVR的方法与结果\n\n*   **改进方法**：通过将策略梯度更新限制在分岔令牌上，从而改进了RLVR。\n*   **实验结果**：\n    *   **超越二八定律的发现**：仅利用20%的令牌，即可实现与全梯度更新相当的性能。\n    *   **性能提升**：\n        *   在Qwen3-8B基础模型上，性能与全梯度更新相当。\n        *   在Qwen3-32B模型上，AIME'25得分显著提升11.04，AIME'24得分提升7.71。\n        *   在Qwen3-14B模型上，AIME'25得分提升4.79，AIME'24得分提升5.21。\n    *   **强大的扩展趋势**：这些结果凸显了该方法在更大模型上的强大扩展趋势。\n    *   **对比实验**：仅对80%最低熵的令牌进行训练会导致性能显著下降。\n\n### 4. 结论与展望\n\n*   **核心结论**：RLVR的有效性主要源于优化决定推理方向的高熵令牌。\n*   **研究意义**：本研究结果共同强调了通过令牌熵视角理解RLVR的潜力，并通过利用高熵少数令牌来优化RLVR，以进一步提升LLM的推理能力。",
      "shortSummary": "本研究从令牌熵模式视角探索了可验证奖励强化学习（RLVR）如何增强大语言模型（LLM）的推理能力。研究发现，高熵的少数令牌是链式思考（CoT）推理中的关键“分岔点”，RLVR主要通过调整这些高熵令牌来提升性能。通过将策略梯度更新限制在这些高熵令牌上，研究者实现了在Qwen3-8B上与全梯度更新相当的性能，并在Qwen3-32B和Qwen3-14B上显著超越全梯度更新，证明了利用少数高熵令牌优化RLVR的有效性和扩展潜力。",
      "translated_title": "超越二八定律：高熵少数令牌驱动大语言模型推理的有效强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning."
    },
    {
      "title": "何时行动，何时等待：任务型对话中意图可触发性的结构轨迹建模 (原标题: WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent Triggerability in Task-Oriented Dialogue)",
      "link": "https://arxiv.org/abs/2506.01881",
      "pubDate": "Mon, 02 Jun 2025 13:11:10 GMT",
      "isoDate": "2025-06-02T13:11:10.000Z",
      "creator": "Yaoyao Qian, Jindan Huang, Yuanli Wang, Simon Yu, Kyrie Zhixuan Zhou, Jiayuan Mao, Mingfu Liang, Hanhan Zhou",
      "summary": "## 何时行动，何时等待：任务型对话中意图可触发性的结构轨迹建模\n\n### 引言与问题背景\n\n任务型对话系统在处理用户话语时常面临挑战。尽管用户表达在语义上可能完整，但往往缺乏触发系统采取适当行动所需的结构化信息。这主要是因为用户通常不完全理解自身需求，而系统则需要精确的意图定义才能有效响应。当前基于大型语言模型（LLM）的代理无法有效区分语言上完整但上下文不可触发的表达，且缺乏用于协作式意图形成的框架。\n\n### STORM框架介绍\n\n为解决上述问题，本文提出了 **STORM** 框架。STORM旨在建模对话中的非对称信息动态，其核心在于模拟UserLLM（拥有完全内部访问权限）和AgentLLM（仅能观察行为）之间的对话过程。通过这种方式，STORM能够生成带注释的语料库，捕捉表达轨迹和潜在的认知转换，从而实现对协作理解发展进行系统分析。\n\n### 主要贡献\n\nSTORM框架及其研究带来了以下关键贡献：\n\n1.  **非对称信息处理的正式化**：首次在对话系统中正式化了非对称信息处理的概念。\n2.  **意图形成建模**：构建了意图形成模型，能够追踪协作理解的演变过程。\n3.  **评估指标**：提出了新的评估指标，不仅衡量任务性能，还衡量内部认知改进。\n\n### 实验与发现\n\n研究团队在四种不同的语言模型上进行了实验。实验结果揭示了一些重要发现：\n\n*   在某些特定场景下，适度的不确定性（40-60%）表现可能优于完全透明的信息状态。\n*   观察到模型特定的模式，这表明在人机协作中，需要重新考虑最佳信息完整性的策略。\n\n### 结论与意义\n\n这些研究发现有助于深入理解非对称推理动态，并为设计能够有效处理不确定性的对话系统提供了重要指导。",
      "shortSummary": "任务型对话系统常因用户意图不明确而难以行动。本文提出STORM框架，通过模拟用户与代理LLM间的非对称信息对话，建模意图形成和协作理解的演变。STORM生成带注释语料，并提出新的评估指标。实验发现，适度不确定性（40-60%）在某些情况下优于完全透明，这为设计更适应不确定性的人机对话系统提供了新思路。",
      "translated_title": "何时行动，何时等待：任务型对话中意图可触发性的结构轨迹建模",
      "images": [],
      "contentSource": "完整文章",
      "content": "Task-oriented dialogue systems often face difficulties when user utterances seem semantically complete but lack necessary structural information for appropriate system action. This arises because users frequently do not fully understand their own needs, while systems require precise intent definitions. Current LLM-based agents cannot effectively distinguish between linguistically complete and contextually triggerable expressions, lacking frameworks for collaborative intent formation. We present STORM, a framework modeling asymmetric information dynamics through conversations between UserLLM (full internal access) and AgentLLM (observable behavior only). STORM produces annotated corpora capturing expression trajectories and latent cognitive transitions, enabling systematic analysis of collaborative understanding development. Our contributions include: (1) formalizing asymmetric information processing in dialogue systems; (2) modeling intent formation tracking collaborative understanding evolution; and (3) evaluation metrics measuring internal cognitive improvements alongside task performance. Experiments across four language models reveal that moderate uncertainty (40-60%) can outperform complete transparency in certain scenarios, with model-specific patterns suggesting reconsideration of optimal information completeness in human-AI collaboration. These findings contribute to understanding asymmetric reasoning dynamics and inform uncertainty-calibrated dialogue system design."
    },
    {
      "title": "ShapeLLM-Omni：一种用于3D生成和理解的原生多模态大语言模型 (原标题: ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding)",
      "link": "https://arxiv.org/abs/2506.01853",
      "pubDate": "Mon, 02 Jun 2025 12:40:50 GMT",
      "isoDate": "2025-06-02T12:40:50.000Z",
      "creator": "Junliang Ye, Zhengyi Wang, Ruowen Zhao, Shenghao Xie, Jun Zhu",
      "summary": "# ShapeLLM-Omni：一种用于3D生成和理解的原生多模态大语言模型\n\n## 引言\n*   近期，ChatGPT-4o强大的文本到图像能力使得原生多模态大语言模型（LLMs）受到广泛关注。\n*   然而，当前多模态LLMs的能力仍局限于图像和文本。\n*   除了图像，理解和生成3D内容同样至关重要。\n\n## ShapeLLM-Omni的提出\n*   为解决现有模型在3D能力上的空白，研究者提出了**ShapeLLM-Omni**。\n*   ShapeLLM-Omni是一种原生的3D大语言模型，能够以任意序列理解和生成3D资产及文本。\n\n## 核心技术与方法\nShapeLLM-Omni的开发主要包括以下几个关键步骤：\n\n1.  **3D VQVAE训练：**\n    *   首先，训练一个**3D向量量化变分自编码器（VQVAE）**。\n    *   该VQVAE旨在将3D对象映射到一个离散的潜在空间，从而实现高效且准确的形状表示和重建。\n\n2.  **3D-Alpaca数据集构建：**\n    *   基于3D感知的离散tokens，研究者创新性地构建了一个大规模的连续训练数据集，命名为**3D-Alpaca**。\n    *   该数据集涵盖了3D内容的生成、理解和编辑等多种任务。\n    *   3D-Alpaca为未来的3D AI研究和模型训练提供了丰富的资源。\n\n3.  **模型训练：**\n    *   最后，研究团队在**3D-Alpaca数据集**上对**Qwen-2.5-vl-7B-Instruct模型**进行了基于指令的训练。\n\n## 研究贡献\n*   ShapeLLM-Omni的工作为扩展多模态模型以具备基本的3D能力提供了一次有效的尝试。\n*   这项研究为未来3D原生AI领域的发展和深入研究做出了贡献。",
      "shortSummary": "ShapeLLM-Omni是一种新型原生3D多模态大语言模型，旨在弥补现有LLM在3D内容理解和生成方面的不足。它能够处理3D资产和文本，通过训练3D VQVAE实现高效形状表示。该模型构建了包含生成、理解和编辑任务的大规模3D-Alpaca数据集，并在此数据集上对Qwen-2.5-vl-7B-Instruct模型进行指令训练，为3D原生AI研究奠定了基础。",
      "translated_title": "ShapeLLM-Omni：一种用于3D生成和理解的原生多模态大语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recently, the powerful text-to-image capabilities of ChatGPT-4o have led to growing appreciation for native multimodal large language models. However, its multimodal capabilities remain confined to images and text. Yet beyond images, the ability to understand and generate 3D content is equally crucial. To address this gap, we propose ShapeLLM-Omni-a native 3D large language model capable of understanding and generating 3D assets and text in any sequence. First, we train a 3D vector-quantized variational autoencoder (VQVAE), which maps 3D objects into a discrete latent space to achieve efficient and accurate shape representation and reconstruction. Building upon the 3D-aware discrete tokens, we innovatively construct a large-scale continuous training dataset named 3D-Alpaca, encompassing generation, comprehension, and editing, thus providing rich resources for future research and training. Finally, by performing instruction-based training of the Qwen-2.5-vl-7B-Instruct model on the 3D-Alpaca dataset. Our work provides an effective attempt at extending multimodal models with basic 3D capabilities, which contributes to future research in 3D-native AI. Project page: https://github.com/JAMESYJL/ShapeLLM-Omni"
    },
    {
      "title": "SmolVLA：一种经济高效的机器人视觉-语言-动作模型 (原标题: SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics)",
      "link": "https://arxiv.org/abs/2506.01844",
      "pubDate": "Mon, 02 Jun 2025 12:30:19 GMT",
      "isoDate": "2025-06-02T12:30:19.000Z",
      "creator": "Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andres Marafioti, Simon Alibert, Matthieu Cord, Thomas Wolf, Remi Cadene",
      "summary": "# SmolVLA：一种经济高效的机器人视觉-语言-动作模型\n\n本文介绍了一种名为SmolVLA的创新型视觉-语言-动作（VLA）模型，旨在解决现有机器人VLA模型在成本、效率和部署方面的挑战。\n\n## 引言\n\n*   **视觉-语言模型（VLM）作为机器人基础：** 预训练于大规模多模态数据集的视觉-语言模型（VLM）编码了丰富的视觉和语言知识，为机器人技术提供了强大的基础。\n*   **VLA模型的兴起：** 近期研究将VLM适配为视觉-语言-动作（VLA）模型，从而实现自然语言驱动的感知和控制。\n*   **现有VLA模型的局限性：**\n    *   **规模庞大：** 现有VLA模型通常拥有数十亿参数，导致高昂的训练成本和有限的实际部署能力。\n    *   **数据依赖：** 它们主要依赖学术和工业数据集，忽视了来自经济型机器人平台的社区收集数据日益增长的可用性。\n\n## SmolVLA模型介绍\n\nSmolVLA是一种小型、高效且社区驱动的VLA模型，旨在大幅降低训练和推理成本，同时保持具有竞争力的性能。\n\n*   **核心设计理念：**\n    *   **小型化与高效性：** SmolVLA的设计目标是显著减少资源消耗。\n    *   **社区驱动：** 鼓励利用社区收集的数据，以适应经济型机器人平台。\n*   **主要优势：**\n    *   **成本效益：** 大幅降低训练和推理成本。\n    *   **硬件兼容性：** 可以在单个GPU上进行训练，并部署在消费级GPU甚至CPU上，极大地扩展了其可访问性。\n*   **创新点：异步推理堆栈**\n    *   SmolVLA引入了一种异步推理堆栈，将感知和动作预测与动作执行解耦。\n    *   这种解耦允许通过分块动作生成实现更高的控制频率，从而提高响应能力。\n\n## 性能与评估\n\n*   **卓越性能：** 尽管SmolVLA的体积紧凑，但其性能与比它大10倍的VLA模型相当。\n*   **广泛评估：** SmolVLA在模拟和真实世界的机器人基准测试中都进行了评估，验证了其在不同场景下的有效性。\n\n## 资源发布\n\n为了促进社区发展和进一步研究，SmolVLA团队已发布了所有代码、预训练模型和训练数据。",
      "shortSummary": "SmolVLA是一种小型、高效且社区驱动的视觉-语言-动作（VLA）模型，旨在降低机器人训练和部署成本。它解决了现有VLA模型规模庞大、资源消耗高的问题，可在消费级硬件上运行，并引入异步推理堆栈以提高控制频率。尽管体积紧凑，SmolVLA的性能与10倍大的模型相当，为经济高效的机器人应用提供了解决方案。",
      "translated_title": "SmolVLA：一种经济高效的机器人视觉-语言-动作模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-language models (VLMs) pretrained on large-scale multimodal datasets encode rich visual and linguistic knowledge, making them a strong foundation for robotics. Rather than training robotic policies from scratch, recent approaches adapt VLMs into vision-language-action (VLA) models that enable natural language-driven perception and control. However, existing VLAs are typically massive--often with billions of parameters--leading to high training costs and limited real-world deployability. Moreover, they rely on academic and industrial datasets, overlooking the growing availability of community-collected data from affordable robotic platforms. In this work, we present SmolVLA, a small, efficient, and community-driven VLA that drastically reduces both training and inference costs, while retaining competitive performance. SmolVLA is designed to be trained on a single GPU and deployed on consumer-grade GPUs or even CPUs. To further improve responsiveness, we introduce an asynchronous inference stack decoupling perception and action prediction from action execution, allowing higher control rates with chunked action generation. Despite its compact size, SmolVLA achieves performance comparable to VLAs that are 10x larger. We evaluate SmolVLA on a range of both simulated as well as real-world robotic benchmarks and release all code, pretrained models, and training data."
    },
    {
      "title": "EarthMind：迈向多粒度、多传感器地球观测的大型多模态模型 (原标题: EarthMind: Towards Multi-Granular and Multi-Sensor Earth Observation with Large Multimodal Models)",
      "link": "https://arxiv.org/abs/2506.01667",
      "pubDate": "Mon, 02 Jun 2025 09:36:05 GMT",
      "isoDate": "2025-06-02T09:36:05.000Z",
      "creator": "Yan Shu, Bin Ren, Zhitong Xiong, Danda Pani Paudel, Luc Van Gool, Begum Demir, Nicu Sebe, Paolo Rota",
      "summary": "### EarthMind：用于地球观测数据理解的新型框架\n\n**背景与挑战：**\n\n*   大型多模态模型（LMMs）在各种视觉-语言任务中表现出色，但它们在全面理解地球观测（EO）数据方面仍面临挑战。\n*   EO数据对于环境监测和人类活动影响评估至关重要。\n\n**EarthMind 框架介绍：**\n\n*   本文提出了 EarthMind，一个新颖的视觉-语言框架，旨在实现多粒度、多传感器EO数据的理解。\n*   EarthMind 包含两个核心组件：\n    1.  **空间注意力提示（Spatial Attention Prompting, SAP）：** 重新分配大型语言模型（LLM）内部的注意力，以增强像素级别的理解能力。\n    2.  **跨模态融合（Cross-modal Fusion）：** 将异构模态对齐到共享空间中，并根据信息密度自适应地重新加权令牌，以实现有效的融合。\n\n**EarthMind-Bench 基准测试：**\n\n*   为了促进多传感器融合评估，研究团队提出了 EarthMind-Bench，一个全面的基准测试。\n*   该基准包含超过2,000个人工标注的多传感器图像-问题对，涵盖了广泛的感知和推理任务。\n\n**实验结果与性能：**\n\n*   广泛的实验证明了 EarthMind 的有效性。\n*   EarthMind 在 EarthMind-Bench 上取得了最先进的性能，尽管其规模仅为40亿参数，却超越了 GPT-4o。\n*   此外，EarthMind 在多个公共EO基准测试中也优于现有方法。\n\n**潜力与应用：**\n\n*   EarthMind 展示了其在一个统一框架内处理多粒度和多传感器挑战的潜力。",
      "shortSummary": "EarthMind 是一个新颖的视觉-语言框架，旨在提升大型多模态模型对多粒度、多传感器地球观测（EO）数据的理解能力。它包含空间注意力提示（SAP）和跨模态融合两大核心组件。为评估其性能，研究团队提出了 EarthMind-Bench 基准测试。实验表明，EarthMind 在 EarthMind-Bench 上取得了最先进的性能，超越了 GPT-4o，并在多个公共EO基准测试中表现优异，展现了其处理复杂EO挑战的潜力。",
      "translated_title": "EarthMind：迈向多粒度、多传感器地球观测的大型多模态模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Multimodal Models (LMMs) have demonstrated strong performance in various vision-language tasks. However, they often struggle to comprehensively understand Earth Observation (EO) data, which is critical for monitoring the environment and the effects of human activity on it. In this work, we present EarthMind, a novel vision-language framework for multi-granular and multi-sensor EO data understanding. EarthMind features two core components: (1) Spatial Attention Prompting (SAP), which reallocates attention within the LLM to enhance pixel-level understanding; and (2) Cross-modal Fusion, which aligns heterogeneous modalities into a shared space and adaptively reweighs tokens based on their information density for effective fusion. To facilitate multi-sensor fusion evaluation, we propose EarthMind-Bench, a comprehensive benchmark with over 2,000 human-annotated multi-sensor image-question pairs, covering a wide range of perception and reasoning tasks. Extensive experiments demonstrate the effectiveness of EarthMind. It achieves state-of-the-art performance on EarthMind-Bench, surpassing GPT-4o despite being only 4B in scale. Moreover, EarthMind outperforms existing methods on multiple public EO benchmarks, showcasing its potential to handle both multi-granular and multi-sensor challenges in a unified framework."
    },
    {
      "title": "激励推理以提升大型语言模型的高级指令遵循能力 (原标题: Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models)",
      "link": "https://arxiv.org/abs/2506.01413",
      "pubDate": "Mon, 02 Jun 2025 04:11:44 GMT",
      "isoDate": "2025-06-02T04:11:44.000Z",
      "creator": "Yulei Qin, Gang Li, Zongyi Li, Zihan Xu, Yuchen Shi, Zhekai Lin, Xiao Cui, Ke Li, Xing Sun",
      "summary": "大型语言模型（LLMs）在遵循复杂指令方面面临显著挑战，尤其当指令包含并行、链式和分支结构的多重约束时。尽管思维链（CoT）被期望能普遍提升LLMs的能力，但研究发现，传统的CoT方法可能因其肤浅的推理模式（仅复述指令）而产生负面影响，未能深入分析约束的组成及其跨层级和维度的关系。\n\n为了解决这一问题，本文提出了一种系统性方法，通过激励推理来提升LLMs处理复杂指令的能力，具体措施包括：\n\n*   **数据获取与分解**：首先，基于现有分类法对复杂指令进行分解，并提出了一种可复现的数据获取方法，为模型训练提供高质量的输入。\n*   **强化学习驱动的推理培养**：利用强化学习（RL）技术，结合可验证的、以规则为中心的奖励信号，专门培养LLMs在指令遵循方面的推理能力。这有助于模型识别并处理指令中的深层逻辑和约束关系。\n*   **解决浅层推理问题**：通过引入样本级对比（sample-wise contrast）机制，增强CoT的执行效果，从而解决传统CoT中推理模式肤浅、非本质的问题。\n*   **专家行为克隆促进技能转移**：利用专家行为克隆（behavior cloning）技术，促进从“快速思考”的LLMs到“熟练推理者”的稳定分布转移，使模型能够更好地学习和模仿专家的推理过程。\n\n该方法在七个综合基准测试上进行了广泛评估，结果证实了其有效性。一个1.5B的LLM通过该方法实现了11.74%的性能提升，其表现甚至可与一个8B的LLM相媲美。研究代码和数据已公开。",
      "shortSummary": "大型语言模型（LLMs）在遵循复杂指令时面临挑战，传统思维链（CoT）效果不佳。本文提出一种系统性方法，通过激励推理来提升LLMs的指令遵循能力。该方法利用强化学习（RL）和规则奖励来培养推理，并通过样本级对比和专家行为克隆解决浅层推理问题。实验证明，该方法显著提升了LLMs的性能，使1.5B模型达到8B模型的水平，验证了其有效性。",
      "translated_title": "激励推理以提升大型语言模型的高级指令遵循能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data are available at https://github.com/yuleiqin/RAIF."
    },
    {
      "title": "zip2zip：通过令牌压缩实现语言模型的推理时自适应词汇表 (原标题: zip2zip: Inference-Time Adaptive Vocabularies for Language Models via Token Compression)",
      "link": "https://arxiv.org/abs/2506.01084",
      "pubDate": "Sun, 01 Jun 2025 13:03:02 GMT",
      "isoDate": "2025-06-01T13:03:02.000Z",
      "creator": "Saibo Geng, Nathan Ranchin, Yunzhen yao, Maxime Peyrard, Chris Wendler, Michael Gastpar, Robert West",
      "summary": "### zip2zip：语言模型的推理时自适应词汇表\n\n**背景与问题**\n\n大型语言模型（LLMs）的性能和成本受分词效率影响巨大。然而，大多数模型依赖于针对通用语料库优化的静态分词器。这些固定词汇表的分词器难以适应特定领域或语言的输入，导致生成的令牌序列更长，进而增加计算成本。\n\n**zip2zip 框架介绍**\n\n为了解决上述问题，研究人员提出了 zip2zip 框架。该框架旨在使 LLMs 能够在推理时动态调整令牌词汇表，从而生成更少的令牌，实现更快的推理速度。\n\n**zip2zip 的核心组件**\n\nzip2zip 由三个关键组件构成：\n\n1.  **基于 LZW 压缩的分词器**：\n    *   该分词器基于 Lempel-Ziv-Welch (LZW) 压缩算法。\n    *   它能够在运行时（on the fly）将令牌逐步压缩成可重用的“超令牌”（hypertokens）。\n2.  **运行时嵌入层**：\n    *   此嵌入层负责在运行时计算新形成的超令牌的嵌入向量。\n3.  **因果语言建模变体**：\n    *   这是一个经过修改的因果语言建模版本。\n    *   它训练模型以在经过超令牌化（hypertokenized）和压缩的序列上进行操作。\n\n**实施与效果**\n\n*   **模型适应**：现有 LLM 可以通过参数高效的微调（parameter-efficient finetuning）在约 10 个 GPU 小时内完成“zip2zip 化”。\n*   **推理时学习**：经过 zip2zip 处理的 LLMs 能够有效地在推理时学习并使用超令牌。\n*   **性能提升**：\n    *   输入和输出序列长度减少了 20% 至 60%。\n    *   推理延迟显著改善。\n\n**代码发布**\n\n相关代码将在未来发布。",
      "shortSummary": "zip2zip 是一个新框架，旨在通过在推理时动态调整令牌词汇表来提高大型语言模型（LLMs）的效率。它利用 LZW 压缩将令牌转换为“超令牌”，并训练模型处理这些压缩序列。通过参数高效的微调，zip2zip 能使 LLMs 将输入输出序列长度减少 20-60%，显著提升推理速度和降低延迟。",
      "translated_title": "zip2zip：通过令牌压缩实现语言模型的推理时自适应词汇表",
      "images": [],
      "contentSource": "完整文章",
      "content": "Tokenization efficiency plays a critical role in the performance and cost of large language models (LLMs), yet most models rely on static tokenizers optimized for general-purpose corpora. These tokenizers' fixed vocabularies often fail to adapt to domain- or language-specific inputs, leading to longer token sequences and higher computational costs. We introduce zip2zip, a framework that enables LLMs to dynamically adjust token vocabulary at inference time, allowing for fewer generated tokens and thus faster inference. zip2zip consists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch (LZW) compression that incrementally compresses tokens into reusable \"hypertokens\" on the fly; (2) an embedding layer that computes embeddings for newly formed hypertokens at runtime; and (3) a causal language modeling variant that trains the model to operate on hypertokenized, compressed sequences. We show that an existing LLM can be zip2zip-fied in 10 GPU-hours via parameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to use hypertokens at inference time, reducing input and output sequence length by 20-60\\%, with significant improvements in inference latency."
    },
    {
      "title": "通过梯度分组调整学习率以驯服大型语言模型 (原标题: Taming LLMs by Scaling Learning Rates with Gradient Grouping)",
      "link": "https://arxiv.org/abs/2506.01049",
      "pubDate": "Sun, 01 Jun 2025 11:30:37 GMT",
      "isoDate": "2025-06-01T11:30:37.000Z",
      "creator": "Siyuan Li, Juanxi Tian, Zedong Wang, Xin Jin, Zicheng Liu, Wentao Zhang, Dan Xu",
      "summary": "## SGG：通过梯度分组驯服大型语言模型\n\n### 引言与挑战\n\n大型语言模型（LLMs）的训练因其庞大的规模和异构架构而面临巨大挑战。尽管AdamW等自适应优化器有助于解决梯度变化问题，但它们在高效、有效地估计逐参数学习率方面仍显不足，这导致训练不稳定、收敛缓慢，并且与参数高效微调（PEFT）技术兼容性差。\n\n### SGG 方法介绍\n\n为了应对这些挑战，本文提出了一种名为“通过梯度分组进行缩放”（Scaling with Gradient Grouping, SGG）的优化器封装器。SGG旨在通过动态分组和组特定缩放来改进自适应学习率的估计。\n\n**核心机制：**\n\n*   **梯度统计分组：** SGG首先将每一层中的梯度统计数据聚类成不同的组。\n*   **簇特定缩放：** 随后，SGG对每个簇应用特定的缩放因子，以校准每个参数的学习率。\n\n**优势：**\n\n*   这种方法在施加集体组级约束的同时，仍能保持精确的逐参数适应性。\n\n### 实验结果与性能\n\n研究人员在多种（多语言）大型语言模型基准测试上进行了实验，结果表明SGG具有显著的优势：\n\n*   **无缝集成：** SGG能够与现有优化器无缝集成。\n*   **性能提升：** 相较于基线方法，SGG在不同模型尺寸上均能提供持续的性能增益和更快的收敛速度。\n*   **鲁棒性：** SGG在不同批次大小和学习率下均表现出卓越的稳定性，使其成为LLM优化的一个稳健选择。\n\n### 其他信息\n\n*   本文是“Taming LLMs with Gradient Grouping”的预印本版本，该研究将于ACL'2025会议上发表。\n*   相关代码将在未来提供。",
      "shortSummary": "本文提出SGG（Scaling with Gradient Grouping），一种优化器封装器，旨在解决大型语言模型（LLMs）训练中学习率估计不准确导致的不稳定和收敛慢问题。SGG通过动态分组梯度统计并应用组特定缩放来校准学习率。实验表明，SGG能与现有优化器无缝集成，在多种LLM基准测试中提供持续的性能增益、更快的收敛速度，并在不同批次大小和学习率下表现出强大的鲁棒性，是LLM优化的有效选择。",
      "translated_title": "通过梯度分组调整学习率以驯服大型语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Training large language models (LLMs) poses challenges due to their massive scale and heterogeneous architectures. While adaptive optimizers like AdamW help address gradient variations, they still struggle with efficient and effective parameter-wise learning rate estimation, resulting in training instability, slow convergence, and poor compatibility with parameter-efficient fine-tuning (PEFT) techniques. This work introduces Scaling with Gradient Grouping (SGG), an optimizer wrapper that improves adaptive learning rate estimation by dynamic grouping and group-specific scaling. SGG first groups gradient statistics in each layer into clusters and then applies cluster-specific scaling to calibrate learning rates for each parameter, thus imposing collective group-wise constraints while maintaining precise per-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that SGG integrates seamlessly with existing optimizers, and offers consistent gains and faster convergence over baselines, with various model sizes. Its stability across varying batch sizes and learning rates establishes SGG as a robust choice for LLM optimization."
    },
    {
      "title": "视频扩散模型多功能控制的时间上下文微调 (原标题: Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion Models)",
      "link": "https://arxiv.org/abs/2506.00996",
      "pubDate": "Sun, 01 Jun 2025 08:57:43 GMT",
      "isoDate": "2025-06-01T08:57:43.000Z",
      "creator": "Kinam Kim, Junha Hyung, Jaegul Choo",
      "summary": "## 视频扩散模型多功能控制的时间上下文微调 (Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion Models)\n\n### 引言与背景\n\n*   **当前挑战：** 尽管文本到视频扩散模型在高质量视频合成方面取得了显著进展，但在有限的数据和计算资源下，实现可控的视频生成仍然极具挑战。\n*   **现有方法局限：**\n    *   用于条件生成的现有微调方法通常依赖于外部编码器或对模型架构进行修改。\n    *   这些方法往往需要大量数据集。\n    *   它们通常仅限于空间对齐的条件，这限制了其灵活性和可扩展性。\n\n### 核心方法：时间上下文微调 (Temporal In-Context Fine-Tuning, TIC-FT)\n\n*   **提出：** 本文引入了时间上下文微调 (TIC-FT)，这是一种高效且通用的方法，用于将预训练的视频扩散模型适应到各种条件生成任务。\n*   **关键思想：**\n    1.  **帧连接：** 沿时间轴连接条件帧和目标帧。\n    2.  **缓冲帧插入：** 插入具有逐渐增加噪声水平的中间缓冲帧。\n    3.  **平滑过渡：** 这些缓冲帧能够实现平滑的过渡，从而使微调过程与预训练模型的时序动态保持一致。\n\n### TIC-FT 的优势与性能\n\n*   **无需架构修改：** TIC-FT 不需要对预训练模型的架构进行任何更改。\n*   **数据高效：** 仅需少量训练样本（例如10-30个）即可实现强大的性能。\n*   **任务多样性：** 该方法在多种任务中得到了验证，包括图像到视频生成和视频到视频生成。\n*   **模型兼容性：** 适用于大型基础模型，如CogVideoX-5B和Wan-14B。\n*   **性能卓越：** 广泛的实验表明，TIC-FT 在条件保真度和视觉质量方面均优于现有基线方法。\n*   **高效率：** 在训练和推理阶段都保持高效。\n\n### 结论\n\nTIC-FT 为视频扩散模型的可控生成提供了一种高效、灵活且高性能的解决方案，有效克服了现有方法在数据、计算和通用性方面的局限。",
      "shortSummary": "时间上下文微调（TIC-FT）是一种高效、通用的方法，用于对视频扩散模型进行微调，以实现多功能控制。其核心思想是沿时间轴连接条件帧和目标帧，并插入噪声缓冲帧以实现平滑过渡。TIC-FT无需架构修改，仅需少量训练样本（10-30个），即可在图像到视频和视频到视频生成等任务中表现出色。实验证明，该方法在条件保真度和视觉质量上均优于现有基线，且训练和推理效率高。",
      "translated_title": "视频扩散模型多功能控制的时间上下文微调",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in text-to-video diffusion models have enabled high-quality video synthesis, but controllable generation remains challenging, particularly under limited data and compute. Existing fine-tuning methods for conditional generation often rely on external encoders or architectural modifications, which demand large datasets and are typically restricted to spatially aligned conditioning, limiting flexibility and scalability. In this work, we introduce Temporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach for adapting pretrained video diffusion models to diverse conditional generation tasks. Our key idea is to concatenate condition and target frames along the temporal axis and insert intermediate buffer frames with progressively increasing noise levels. These buffer frames enable smooth transitions, aligning the fine-tuning process with the pretrained model's temporal dynamics. TIC-FT requires no architectural changes and achieves strong performance with as few as 10-30 training samples. We validate our method across a range of tasks, including image-to-video and video-to-video generation, using large-scale base models such as CogVideoX-5B and Wan-14B. Extensive experiments show that TIC-FT outperforms existing baselines in both condition fidelity and visual quality, while remaining highly efficient in both training and inference. For additional results, visit https://kinam0252.github.io/TIC-FT/"
    },
    {
      "title": "SATA-BENCH：多选题“全选”基准测试 (原标题: SATA-BENCH: Select All That Apply Benchmark for Multiple Choice Questions)",
      "link": "https://arxiv.org/abs/2506.00643",
      "pubDate": "Sat, 31 May 2025 13:14:21 GMT",
      "isoDate": "2025-05-31T13:14:21.000Z",
      "creator": "Weijie Xu, Shixian Cui, Xi Fang, Chi Xue, Stephanie Eckman, Chandan Reddy",
      "summary": "### SATA-BENCH：评估大型语言模型在多选题“全选”问题上的表现\n\n**1. 引言与背景**\n*   当前大型语言模型（LLMs）的评估主要集中在单选多选题任务上。\n*   然而，许多现实世界的问题需要从一组选项中识别所有正确的答案（即“全选”问题，Select All That Apply, SATA）。\n*   LLMs在处理这类多答案问题上的能力尚未得到充分探索。\n\n**2. 引入SATA-BENCH基准测试**\n*   本文提出了SATA-BENCH，这是首个专门用于评估LLMs在SATA问题上表现的基准测试。\n*   该基准测试涵盖了阅读理解、法律和生物医学等多个不同领域，确保了评估的多样性。\n\n**3. 评估结果与发现**\n*   研究团队对27个开源和专有LLM模型进行了评估。\n*   结果显示存在显著的性能差距：即使是表现最强的模型，其精确匹配率也仅为41.8%。\n*   这暴露了LLMs在可靠识别所有正确答案方面的不足。\n\n**4. LLM的局限性：核心挑战**\n*   研究发现，LLMs在SATA问题上的弱点主要源于两个核心挑战：\n    *   **选择偏差 (Selection Bias)：** 模型倾向于选择某些选项，而无论其内容是否正确。\n    *   **计数偏差 (Count Bias)：** 模型未能准确预测正确答案的数量。\n\n**5. 提出的解决方案：Choice Funnel解码策略**\n*   为解决上述问题，论文提出了一种名为“Choice Funnel”的解码策略。\n*   该策略结合了token去偏置（token debiasing）和自适应阈值（adaptive thresholding），旨在引导模型进行更完整和准确的选择。\n\n**6. Choice Funnel的成效**\n*   Choice Funnel策略在精确匹配率方面比现有基线提高了高达29%。\n*   同时，它还能将推理成本降低超过64%。\n\n**7. 研究意义与贡献**\n*   本研究揭示了当前LLMs在多答案推理方面的根本局限性。\n*   引入了一个新的框架，用于诊断和改进LLMs的多答案推理能力。\n*   研究团队已发布SATA-BENCH基准测试和Choice Funnel策略，以促进LLM在现实世界中需要稳健决策的多答案应用场景中的发展。",
      "shortSummary": "SATA-BENCH是一个评估大型语言模型（LLMs）在多选题“全选”（SATA）问题上表现的新基准测试。研究发现，LLMs在此类问题上存在显著缺陷，即使最强模型也仅有41.8%的精确匹配率，主要源于选择偏差和计数偏差。为解决此问题，论文提出了Choice Funnel解码策略，该策略能将精确匹配率提高高达29%，并显著降低推理成本。这项工作揭示了LLMs在多答案推理方面的局限性，并提供了改进方案。",
      "translated_title": "SATA-BENCH：多选题“全选”基准测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) are increasingly evaluated on single-answer multiple-choice tasks, yet many real-world problems require identifying all correct answers from a set of options. This capability remains underexplored. We introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on Select All That Apply (SATA) questions across diverse domains, including reading comprehension, law, and biomedicine. Our evaluation of 27 open-source and proprietary models reveals a significant gap: even the strongest model achieves only 41.8% exact match, exposing LLMs' inability to reliably identify all correct answers. We find that this weakness stems from two core challenges: selection bias - models favor certain choices regardless of content, and count bias - models fail to predict the correct number of answers. To address these issues, we propose Choice Funnel, a decoding strategy that combines token debiasing with adaptive thresholding to guide models toward complete and accurate selections. Choice Funnel achieves up to 29% higher exact match than competitive baselines while reducing inference cost by over 64%. Our findings expose fundamental limitations in current LLMs and introduce a new framework for diagnosing and improving multi-answer reasoning. We release SATA-BENCH and Choice Funnel to promote LLM development for robust decision-making in realistic, multi-answer applications."
    },
    {
      "title": "像经济学家一样推理：经济学问题上的后训练诱导大型语言模型中的战略泛化 (原标题: Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs)",
      "link": "https://arxiv.org/abs/2506.00577",
      "pubDate": "Sat, 31 May 2025 10:22:40 GMT",
      "isoDate": "2025-05-31T10:22:40.000Z",
      "creator": "Yufa Zhou, Shaobo Wang, Xingyu Dong, Xiangqi Jin, Yifang Chen, Yue Min, Kexin Yang, Xingzhang Ren, Dayiheng Liu, Linfeng Zhang",
      "summary": "### 引言：多智能体系统（MAS）中LLM训练的挑战\n\n直接训练大型语言模型（LLMs）以适应多智能体系统（MAS）面临多重挑战，包括复杂的奖励建模、动态的智能体交互以及对泛化能力的高要求。\n\n### 研究方法：利用经济学推理进行后训练\n\n本研究旨在探索后训练技术，特别是监督微调（SFT）和可验证奖励强化学习（RLVR），能否有效实现LLM在多智能体场景中的泛化。研究团队选择经济学推理作为测试平台，原因在于：\n\n*   经济学拥有坚实的数学和博弈论基础。\n*   它对结构化分析推理有明确要求。\n*   它与现实世界应用紧密相关，例如市场设计、资源分配和政策分析。\n\n### Recon模型介绍\n\n论文引入了**Recon**（**R**easoning like an **ECON**omist），这是一个70亿参数的开源大型语言模型。Recon通过在包含2,100个高质量经济学推理问题的手工整理数据集上进行后训练而得到。\n\n### 评估与结果\n\n研究团队对Recon在经济学推理基准测试和多智能体博弈中进行了全面评估。结果显示，Recon在结构化推理和经济理性方面均取得了显著改进。\n\n### 结论与意义\n\n这些发现强调了领域对齐的后训练在增强LLM推理能力和智能体对齐方面的巨大潜力，并阐明了SFT和RL在塑造模型行为中的关键作用。\n\n### 代码可用性\n\n相关代码已开源。",
      "shortSummary": "本研究探讨了通过经济学问题后训练来提升大型语言模型（LLMs）在多智能体系统中的战略泛化能力。论文介绍了Recon，一个70亿参数的开源LLM，它在2,100个经济学问题数据集上进行了后训练。评估结果显示，Recon在结构化推理和经济理性方面表现出显著改进，表明领域对齐的后训练对于增强LLM的推理和智能体对齐能力具有巨大潜力。",
      "translated_title": "像经济学家一样推理：经济学问题上的后训练诱导大型语言模型中的战略泛化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS) remains challenging due to intricate reward modeling, dynamic agent interactions, and demanding generalization requirements. This paper explores whether post-training techniques, specifically Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR), can effectively generalize to multi-agent scenarios. We use economic reasoning as a testbed, leveraging its strong foundations in mathematics and game theory, its demand for structured analytical reasoning, and its relevance to real-world applications such as market design, resource allocation, and policy analysis. We introduce Recon (Reasoning like an ECONomist), a 7B-parameter open-source LLM post-trained on a hand-curated dataset of 2,100 high-quality economic reasoning problems. Comprehensive evaluation on economic reasoning benchmarks and multi-agent games reveals clear improvements in structured reasoning and economic rationality. These results underscore the promise of domain-aligned post-training for enhancing reasoning and agent alignment, shedding light on the roles of SFT and RL in shaping model behavior. Code is available at https://github.com/MasterZhou1/Recon ."
    },
    {
      "title": "ARIA：通过意图驱动的奖励聚合训练语言智能体 (原标题: ARIA: Training Language Agents with Intention-Driven Reward Aggregation)",
      "link": "https://arxiv.org/abs/2506.00539",
      "pubDate": "Sat, 31 May 2025 08:54:49 GMT",
      "isoDate": "2025-05-31T08:54:49.000Z",
      "creator": "Ruihan Yang, Yikai Zhang, Aili Chen, Xintao Wang, Siyu Yuan, Jiangjie Chen, Deqing Yang, Yanghua Xiao",
      "summary": "## ARIA：通过意图驱动的奖励聚合训练语言智能体\n\n### 问题背景\n\n大型语言模型（LLMs）已使智能体能够通过自由形式的语言交互执行复杂的推理和决策。然而，在开放式语言动作环境（例如谈判或问答游戏）中，动作空间可以被表述为令牌的联合分布，导致动作空间呈指数级增长。在这种空间中采样动作会导致极端的奖励稀疏性，从而带来巨大的奖励方差，严重阻碍了有效的强化学习（RL）。\n\n### ARIA 方法\n\n为解决上述挑战，研究者提出了 ARIA（Aggregates Rewards in Intention space），这是一种旨在实现高效、有效语言智能体训练的方法。ARIA 的核心机制是将自然语言动作从高维的联合令牌分布空间投射到低维的意图空间。在该意图空间中，语义相似的动作会被聚类并分配共享的奖励。\n\n### ARIA 的优势\n\n这种意图感知的奖励聚合通过密集化奖励信号来显著减少奖励方差，从而促进了更优的策略优化。\n\n### 实验结果\n\n广泛的实验结果表明，ARIA 不仅显著降低了策略梯度方差，而且在四个下游任务中平均带来了9.95%的显著性能提升，持续优于离线和在线强化学习基线。",
      "shortSummary": "ARIA 是一种新方法，旨在解决大型语言模型（LLM）智能体在开放式语言环境中面临的奖励稀疏性和高奖励方差问题。它通过将高维语言动作映射到低维意图空间，对语义相似的动作进行奖励聚合。这种意图驱动的奖励聚合能有效减少奖励方差，优化策略。实验证明，ARIA 显著降低了策略梯度方差，并在多个任务中平均提升了9.95%的性能，优于现有基线。",
      "translated_title": "ARIA：通过意图驱动的奖励聚合训练语言智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) have enabled agents to perform complex reasoning and decision-making through free-form language interactions. However, in open-ended language action environments (e.g., negotiation or question-asking games), the action space can be formulated as a joint distribution over tokens, resulting in an exponentially large action space. Sampling actions in such a space can lead to extreme reward sparsity, which brings large reward variance, hindering effective reinforcement learning (RL). To address this, we propose ARIA, a method that Aggregates Rewards in Intention space to enable efficient and effective language Agents training. ARIA aims to project natural language actions from the high-dimensional joint token distribution space into a low-dimensional intention space, where semantically similar actions are clustered and assigned shared rewards. This intention-aware reward aggregation reduces reward variance by densifying reward signals, fostering better policy optimization. Extensive experiments demonstrate that ARIA not only significantly reduces policy gradient variance, but also delivers substantial performance gains of an average of 9.95% across four downstream tasks, consistently outperforming offline and online RL baselines."
    },
    {
      "title": "Pro3D-Editor：一种渐进视图视角下的连贯精确3D编辑方法 (原标题: Pro3D-Editor : A Progressive-Views Perspective for Consistent and Precise 3D Editing)",
      "link": "https://arxiv.org/abs/2506.00512",
      "pubDate": "Sat, 31 May 2025 07:11:55 GMT",
      "isoDate": "2025-05-31T07:11:55.000Z",
      "creator": "Yang Zheng, Mengqi Huang, Nan Chen, Zhendong Mao",
      "summary": "# Pro3D-Editor：一种渐进视图视角下的连贯精确3D编辑方法\n\n## 引言\n\n文本引导的3D编辑旨在精确地编辑语义相关的局部3D区域，这在3D游戏和电影制作等各种实际应用中具有重要潜力。\n\n## 现有方法的问题\n\n现有方法通常遵循“视图不分化”范式，即不加区分地编辑2D视图并将其投射回3D空间。然而，它们忽视了不同视图间的相互依赖性，导致多视图编辑结果不一致。\n\n## Pro3D-Editor的创新：渐进视图范式\n\n本研究提出，理想的连贯3D编辑可以通过一种“渐进视图范式”来实现。该范式的核心思想是将编辑语义从“编辑显著视图”传播到其他“编辑稀疏视图”。\n\n## Pro3D-Editor框架组成\n\nPro3D-Editor是一个新颖的框架，主要包含以下三个组件：\n\n*   **Primary-view Sampler（主视图采样器）**\n    *   动态采样并编辑最显著的视图作为主视图。\n\n*   **Key-view Render（关键视图渲染器）**\n    *   通过其“视图专家混合低秩适应（Mixture-of-View-Experts Low-Rank Adaption, MoVE-LoRA）”模块，将编辑语义从主视图精确传播到其他关键视图。\n\n*   **Full-view Refiner（全视图精修器）**\n    *   基于已编辑的多视图对3D对象进行编辑和精修。\n\n## 实验结果\n\n广泛的实验表明，Pro3D-Editor在编辑精度和空间一致性方面均优于现有方法。\n\n## 相关领域\n\n*   图形学 (cs.GR)\n*   人工智能 (cs.AI)",
      "shortSummary": "Pro3D-Editor是一种新的文本引导3D编辑框架，旨在解决现有方法中多视图编辑不一致的问题。它引入了“渐进视图范式”，通过将编辑语义从“编辑显著视图”传播到“编辑稀疏视图”来实现连贯的3D编辑。该框架包含主视图采样器、关键视图渲染器（利用MoVE-LoRA）和全视图精修器。实验证明，Pro3D-Editor在编辑精度和空间一致性上优于现有方法，在3D游戏和电影制作等领域具有应用潜力。",
      "translated_title": "Pro3D-Editor：一种渐进视图视角下的连贯精确3D编辑方法",
      "images": [],
      "contentSource": "完整文章",
      "content": "Text-guided 3D editing aims to precisely edit semantically relevant local 3D regions, which has significant potential for various practical applications ranging from 3D games to film production. Existing methods typically follow a view-indiscriminate paradigm: editing 2D views indiscriminately and projecting them back into 3D space. However, they overlook the different cross-view interdependencies, resulting in inconsistent multi-view editing. In this study, we argue that ideal consistent 3D editing can be achieved through a progressive-views paradigm, which propagates editing semantics from the editing-salient view to other editing-sparse views. Specifically, we propose Pro3D-Editor, a novel framework, which mainly includes Primary-view Sampler, Key-view Render, and Full-view Refiner. Primary-view Sampler dynamically samples and edits the most editing-salient view as the primary view. Key-view Render accurately propagates editing semantics from the primary view to other key views through its Mixture-of-View-Experts Low-Rank Adaption (MoVE-LoRA). Full-view Refiner edits and refines the 3D object based on the edited multi-views. Extensive experiments demonstrate that our method outperforms existing methods in editing accuracy and spatial consistency."
    },
    {
      "title": "使用双语翻译数据对大型语言模型进行大规模多语言适应 (原标题: Massively Multilingual Adaptation of Large Language Models Using Bilingual Translation Data)",
      "link": "https://arxiv.org/abs/2506.00469",
      "pubDate": "Sat, 31 May 2025 04:37:17 GMT",
      "isoDate": "2025-05-31T04:37:17.000Z",
      "creator": "Shaoxiong Ji, Zihao Li, Jaakko Paavola, Indraneil Paul, Hengyu Luo, Jörg Tiedemann",
      "summary": "### 研究背景与目标\n\n本文深入探讨了大规模多语言持续预训练中的一个关键设计决策：是否包含并行数据。具体而言，研究旨在探究双语翻译数据对Llama3系列模型向500种语言进行大规模多语言适应的影响。\n\n### 数据与模型构建\n\n*   **MaLA双语翻译语料库**：研究团队构建了名为MaLA的双语翻译语料库，其中包含了来自2,500多个语言对的数据，为模型训练提供了丰富的双语资源。\n*   **EMMA-500 Llama 3模型套件**：在此基础上，开发了EMMA-500 Llama 3模型套件，该套件包含四种大规模多语言模型。这些模型是在Llama 3系列基础模型上进行持续预训练的，使用了多达6710亿个token的各种数据混合。研究重点在于比较有无双语翻译数据进行持续预训练的效果。\n\n### 评估与主要发现\n\n*   **全面评估**：研究在7项不同的任务和12个基准测试上对模型进行了全面评估，以衡量其在多语言环境下的性能。\n*   **核心发现**：评估结果清晰地表明，包含双语数据倾向于显著增强语言迁移能力和模型性能，尤其对于低资源语言，其提升效果更为明显。\n\n### 开源贡献\n\n为了促进社区研究和发展，研究团队已将MaLA语料库、EMMA-500 Llama 3套件的工件、相关代码以及模型生成结果全部开源。",
      "shortSummary": "本文研究了双语翻译数据对Llama3大型语言模型进行大规模多语言适应的影响。研究团队构建了MaLA双语语料库，并开发了EMMA-500 Llama 3模型套件，在500种语言上进行持续预训练。评估结果显示，双语数据能显著提升语言迁移和模型性能，尤其对低资源语言效果更佳。所有相关语料库、模型和代码均已开源。",
      "translated_title": "使用双语翻译数据对大型语言模型进行大规模多语言适应",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper investigates a critical design decision in the practice of massively multilingual continual pre-training -- the inclusion of parallel data. Specifically, we study the impact of bilingual translation data for massively multilingual language adaptation of the Llama3 family of models to 500 languages. To this end, we construct the MaLA bilingual translation corpus, containing data from more than 2,500 language pairs. Subsequently, we develop the EMMA-500 Llama 3 suite of four massively multilingual models -- continually pre-trained from the Llama 3 family of base models extensively on diverse data mixes up to 671B tokens -- and explore the effect of continual pre-training with or without bilingual translation data. Comprehensive evaluation across 7 tasks and 12 benchmarks demonstrates that bilingual data tends to enhance language transfer and performance, particularly for low-resource languages. We open-source the MaLA corpus, EMMA-500 Llama 3 suite artefacts, code, and model generations."
    },
    {
      "title": "LoHoVLA：一种用于长周期具身任务的统一视觉-语言-动作模型 (原标题: LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks)",
      "link": "https://arxiv.org/abs/2506.00411",
      "pubDate": "Sat, 31 May 2025 02:01:03 GMT",
      "isoDate": "2025-05-31T02:01:03.000Z",
      "creator": "Yi Yang, Jiaxuan Sun, Siqi Kou, Yihan Wang, Zhijie Deng",
      "summary": "## LoHoVLA：一种用于长周期具身任务的统一视觉-语言-动作模型\n\n### 引言\n\n现实世界的具身智能体在执行长周期任务时面临多重挑战。这类任务的特点是高层目标需要多步骤解决方案，而非单一动作。成功完成这些任务不仅需要高层任务规划（即将目标分解为子任务），还需要低层运动控制（即生成精确的机器人动作）。\n\n现有方法存在局限性：\n*   **视觉-语言-动作 (VLA) 模型**：在规划方面常常表现不佳。\n*   **分层架构**：可能存在协调问题，两者都阻碍了性能的提升。\n\n### LoHoVLA 框架\n\n为了克服上述限制，本文引入了一种名为 LoHoVLA 的新型统一 VLA 框架，专为长周期任务设计。\n\n**核心机制：**\n\n1.  **统一骨干模型**：LoHoVLA 利用一个大型预训练视觉-语言模型 (VLM) 作为骨干。\n2.  **联合生成**：该骨干模型能够联合生成两种类型的标记：\n    *   **语言标记**：用于子任务的生成。\n    *   **动作标记**：用于机器人动作的预测。\n3.  **共享表示**：这种共享表示促进了模型在不同任务间的更好泛化能力。\n4.  **分层闭环控制**：LoHoVLA 采用分层闭环控制机制，以减轻源自高层规划和低层控制的错误。\n\n### LoHoSet 数据集\n\n为了训练 LoHoVLA，研究人员构建了一个名为 LoHoSet 的数据集。该数据集基于 Ravens 模拟器创建，包含以下特点：\n\n*   **任务数量**：20 个长周期任务。\n*   **专家演示**：每个任务包含 1,000 个专家演示。\n*   **数据内容**：每个演示都由视觉观测、语言目标、子任务和机器人动作组成。\n\n### 实验结果\n\n实验结果表明，LoHoVLA 在 Ravens 模拟器上的长周期具身任务中显著超越了现有的分层方法和标准 VLA 方法。\n\n### 结论\n\n这些发现强调了统一架构在推进可泛化具身智能方面的巨大潜力。",
      "shortSummary": "LoHoVLA 是一种新型统一视觉-语言-动作 (VLA) 模型，旨在解决具身智能体在长周期任务中的规划和控制挑战。它利用大型预训练视觉-语言模型作为骨干，联合生成子任务和机器人动作，并采用分层闭环控制机制。在 Ravens 模拟器上，LoHoVLA 显著超越了现有分层和标准 VLA 方法，展示了统一架构在实现通用具身智能方面的巨大潜力。",
      "translated_title": "LoHoVLA：一种用于长周期具身任务的统一视觉-语言-动作模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Real-world embodied agents face long-horizon tasks, characterized by high-level goals demanding multi-step solutions beyond single actions. Successfully navigating these requires both high-level task planning (i.e., decomposing goals into sub-tasks) and low-level motion control (i.e., generating precise robot actions). While existing vision language action (VLA) models and hierarchical architectures offer potential in embodied tasks, the former often falter in planning, and the latter can suffer from coordination issues, both hampering performance. We introduce a new unified VLA framework for long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA leverages a large pretrained vision language model (VLM) as the backbone to jointly generate language and action tokens for sub-task generation and robot action prediction, respectively. This shared representation promotes better generalization across tasks. Additionally, LoHoVLA embraces a hierarchical closed-loop control mechanism to mitigate errors originating from both high-level planning and low-level control. To train LoHoVLA, we introduce LoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon tasks, each with 1,000 expert demonstrations composed of visual observations, linguistic goals, sub-tasks, and robot actions. Experimental results show that LoHoVLA significantly surpasses both hierarchical and standard VLA approaches on long-horizon embodied tasks in the Ravens simulator. These findings underscore the promise of unified architectures for advancing generalizable embodied intelligence."
    },
    {
      "title": "MagiCodec：用于高保真重建和生成的简单掩码高斯注入编解码器 (原标题: MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity Reconstruction and Generation)",
      "link": "https://arxiv.org/abs/2506.00385",
      "pubDate": "Sat, 31 May 2025 00:31:02 GMT",
      "isoDate": "2025-05-31T00:31:02.000Z",
      "creator": "Yakun Song, Jiawei Chen, Xiaobin Zhuang, Chenpeng Du, Ziyang Ma, Jian Wu, Jian Cong, Dongya Jia, Zhuo Chen, Yuping Wang, Yuxuan Wang, Xie Chen",
      "summary": "MagiCodec：用于高保真重建和生成的简单掩码高斯注入编解码器\n\n本文介绍了一种名为 **MagiCodec** 的新型神经音频编解码器，旨在解决现有编解码器在优化重建质量时，牺牲编码令牌下游模型可用性的问题。MagiCodec 是一种创新的单层、流式 Transformer-based 音频编解码器，其设计目标是同时实现高保真重建和增强编码令牌的语义表达能力。\n\n**核心设计与训练流程：**\n*   **单层流式 Transformer 架构：** MagiCodec 采用简洁高效的 Transformer 结构，支持流式处理。\n*   **多阶段训练流程：** 引入了多阶段训练流程，以优化其性能。\n*   **高斯噪声注入：** 在训练过程中融入高斯噪声注入，这是其关键创新点之一。\n*   **潜在正则化：** 结合潜在正则化技术，进一步提升编码的质量和语义丰富性。\n\n**高斯噪声注入的效用：**\n*   研究分析表明，高斯噪声注入在频域中能够有效衰减高频分量。\n*   这种衰减有助于促进鲁棒的令牌化过程，使生成的令牌更具稳定性和表达力。\n\n**性能表现与优势：**\n*   **超越现有技术：** 广泛的实验评估证明，MagiCodec 在重建质量和下游任务（如音频生成）方面均超越了现有最先进的编解码器。\n*   **Zipf-like 分布：** MagiCodec 生成的令牌表现出类似自然语言的 Zipf-like 分布特性。这一发现显著提高了其与基于语言模型的生成架构的兼容性，为构建更强大的音频生成模型奠定了基础。\n\n**可用性：**\n*   MagiCodec 的代码和预训练模型已公开提供。",
      "shortSummary": "MagiCodec 是一种新型的单层、流式 Transformer 音频编解码器，旨在解决现有编解码器在重建质量和下游模型可用性之间的权衡问题。通过引入高斯噪声注入和潜在正则化，MagiCodec 显著提升了编码令牌的语义表达能力和重建保真度。实验证明，它在重建和下游任务中均超越了现有技术，并且其令牌展现出类似自然语言的分布特性，增强了与语言模型生成架构的兼容性。",
      "translated_title": "MagiCodec：用于高保真重建和生成的简单掩码高斯注入编解码器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Neural audio codecs have made significant strides in efficiently mapping raw audio waveforms into discrete token representations, which are foundational for contemporary audio generative models. However, most existing codecs are optimized primarily for reconstruction quality, often at the expense of the downstream modelability of the encoded tokens. Motivated by the need to overcome this bottleneck, we introduce MagiCodec, a novel single-layer, streaming Transformer-based audio codec. MagiCodec is designed with a multistage training pipeline that incorporates Gaussian noise injection and latent regularization, explicitly targeting the enhancement of semantic expressiveness in the generated codes while preserving high reconstruction fidelity. We analytically derive the effect of noise injection in the frequency domain, demonstrating its efficacy in attenuating high-frequency components and fostering robust tokenization. Extensive experimental evaluations show that MagiCodec surpasses state-of-the-art codecs in both reconstruction quality and downstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like distributions, as observed in natural languages, thereby improving compatibility with language-model-based generative architectures. The code and pre-trained models are available at https://github.com/Ereboas/MagiCodec."
    },
    {
      "title": "OWSM v4：通过数据扩展和清洗改进开放式Whisper风格语音模型 (原标题: OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and Cleaning)",
      "link": "https://arxiv.org/abs/2506.00338",
      "pubDate": "Fri, 30 May 2025 21:44:44 GMT",
      "isoDate": "2025-05-30T21:44:44.000Z",
      "creator": "Yifan Peng, Shakeel Muhammad, Yui Sudo, William Chen, Jinchuan Tian, Chyi-Jiunn Lin, Shinji Watanabe",
      "summary": "### OWSM v4：通过数据扩展和清洗改进开放式Whisper风格语音模型\n\n**项目背景与挑战**\n\n*   开放式Whisper风格语音模型（OWSM）项目致力于开发完全开放的语音基础模型。\n*   然而，OWSM早期版本面临训练数据不足的问题。\n\n**核心改进：整合与清洗YODAS数据集**\n\n*   **数据来源**：本工作通过整合YODAS数据集来增强OWSM。YODAS是一个大规模的网络爬取数据集，拥有知识共享许可。\n*   **数据挑战**：由于YODAS数据集的“野性”特性，其包含不正确的语言标签和音频-文本错位等问题，使得直接使用具有挑战性。\n*   **解决方案**：研究团队开发了一个可扩展的数据清洗流程，利用公共工具包对YODAS数据进行处理。\n*   **清洗成果**：清洗后的数据集包含了166,000小时的语音数据，涵盖了75种语言。\n\n**OWSM v4模型训练与性能**\n\n*   **模型训练**：新的OWSM v4系列模型在清洗后的YODAS数据集以及现有OWSM数据上进行训练。\n*   **性能提升**：OWSM v4模型在多语言基准测试中显著优于早期版本。\n*   **竞争力**：在多种场景下，OWSM v4模型甚至能与Whisper和MMS等前沿工业模型相媲美或超越。\n\n**发布计划**\n\n*   清洗后的YODAS数据、预训练模型以及所有相关脚本将通过ESPnet工具包公开发布。",
      "shortSummary": "OWSM v4项目通过整合并清洗大规模YODAS数据集，显著改进了开放式Whisper风格语音模型。研究团队开发了可扩展的数据清洗流程，解决了YODAS数据中的语言标签错误和音频-文本错位问题，生成了包含16.6万小时语音数据的多语言数据集。在此基础上训练的OWSM v4模型在多语言基准测试中表现出色，甚至能与Whisper等工业级模型媲美。清洗后的数据和模型将公开发布。",
      "translated_title": "OWSM v4：通过数据扩展和清洗改进开放式Whisper风格语音模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "The Open Whisper-style Speech Models (OWSM) project has developed a series of fully open speech foundation models using academic-scale resources, but their training data remains insufficient. This work enhances OWSM by integrating YODAS, a large-scale web-crawled dataset with a Creative Commons license. However, incorporating YODAS is nontrivial due to its wild nature, which introduces challenges such as incorrect language labels and audio-text misalignments. To address this, we develop a scalable data-cleaning pipeline using public toolkits, yielding a dataset with 166,000 hours of speech across 75 languages. Our new series of OWSM v4 models, trained on this curated dataset alongside existing OWSM data, significantly outperform previous versions on multilingual benchmarks. Our models even match or surpass frontier industrial models like Whisper and MMS in multiple scenarios. We will publicly release the cleaned YODAS data, pre-trained models, and all associated scripts via the ESPnet toolkit."
    },
    {
      "title": "Open CaptchaWorld：一个用于测试和评估多模态大型语言模型代理的综合性网络平台 (原标题: Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents)",
      "link": "https://arxiv.org/abs/2505.24878",
      "pubDate": "Fri, 30 May 2025 13:59:55 GMT",
      "isoDate": "2025-05-30T13:59:55.000Z",
      "creator": "Yaxin Luo, Zhaoyi Li, Jiacheng Liu, Jiacheng Cui, Xiaohan Zhao, Zhiqiang Shen",
      "summary": "### Open CaptchaWorld：评估多模态大型语言模型代理的综合性网络平台\n\n**背景与挑战**\n\n*   **CAPTCHA的瓶颈作用：** 验证码（CAPTCHA）一直是部署网络代理在实际应用中的关键瓶颈，经常阻碍它们完成端到端自动化任务。\n*   **现有MLLM代理的局限性：** 尽管现代多模态大型语言模型（MLLM）代理在静态感知任务中表现出色，但它们处理像CAPTCHA这样需要交互式、多步骤推理的挑战的能力，在很大程度上尚未经过测试。\n\n**Open CaptchaWorld 平台介绍**\n\n*   **目的与定位：** 为解决上述空白，研究人员推出了Open CaptchaWorld，这是首个专门设计用于评估MLLM驱动代理的视觉推理和交互能力的网络基准测试平台。\n*   **评估方式：** 通过多样化和动态的CAPTCHA谜题进行评估。\n*   **基准范围：** 该基准涵盖了20种现代CAPTCHA类型，总计225个CAPTCHA。\n*   **新度量标准：** 引入了“CAPTCHA推理深度”（CAPTCHA Reasoning Depth）这一新度量标准，用于量化解决每个谜题所需的认知和运动步骤数量。\n\n**实验结果与发现**\n\n*   **人类表现：** 实验结果表明，人类在解决CAPTCHA时始终能达到近乎完美的得分，成功率高达93.3%。\n*   **SOTA MLLM代理表现：** 最先进的MLLM代理（如Browser-Use Openai-o3）表现显著不佳，成功率最高仅为40.0%，远低于人类水平。\n*   **平台的重要性：** 这突出表明Open CaptchaWorld是一个至关重要的基准，可用于诊断当前多模态代理的局限性，并指导更强大的多模态推理系统的开发。\n\n**资源可用性**\n\n*   代码和数据可在相关URL获取。",
      "shortSummary": "Open CaptchaWorld是一个综合性的网络平台，旨在测试和评估多模态大型语言模型（MLLM）代理处理交互式CAPTCHA的能力。该平台包含20种类型共225个CAPTCHA，并引入了“CAPTCHA推理深度”指标。实验表明，人类解决CAPTCHA的成功率接近完美（93.3%），而最先进的MLLM代理表现不佳，最高仅达40.0%。Open CaptchaWorld揭示了当前MLLM代理在多步推理方面的局限性，对未来多模态推理系统的发展具有重要指导意义。",
      "translated_title": "Open CaptchaWorld：一个用于测试和评估多模态大型语言模型代理的综合性网络平台",
      "images": [],
      "contentSource": "完整文章",
      "content": "CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL."
    }
  ],
  "lastUpdated": "2025-06-03T09:37:19.779Z"
}