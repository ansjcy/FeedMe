{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "连续自回归语言模型 (原标题: Continuous Autoregressive Language Models)",
      "link": "https://arxiv.org/abs/2510.27688",
      "pubDate": "Fri, 31 Oct 2025 13:58:11 GMT",
      "isoDate": "2025-10-31T13:58:11.000Z",
      "creator": "Chenze Shao, Darren Li, Fandong Meng, Jie Zhou",
      "summary": "## 连续自回归语言模型 (CALM) 详细摘要\n\n### 核心问题：大型语言模型 (LLM) 的效率瓶颈\n\n当前大型语言模型 (LLM) 的效率受限于其固有的顺序性，即逐个生成词元（token）的过程。这种逐词元生成机制是LLM性能提升的主要限制因素。\n\n### 解决方案：引入连续自回归语言模型 (CALM)\n\n为了突破这一瓶颈，研究提出了一种新的LLM扩展设计轴：增加每个生成步骤的语义带宽。为此，论文引入了**连续自回归语言模型 (CALM)**，这是一种范式转变，将传统的离散“预测下一个词元”转变为**连续“预测下一个向量”**。\n\n### CALM 的工作原理\n\n1.  **高保真自编码器**：CALM 利用一个高保真自编码器，能够将K个词元（tokens）的块压缩成一个单一的连续向量。\n2.  **高精度重构**：通过这种压缩，原始词元可以从该连续向量中以超过99.9%的精度进行重构。\n3.  **语言建模范式转变**：这使得模型能够将语言建模为一系列连续向量，而非离散词元序列。\n4.  **生成步骤减少**：通过这种方式，生成步骤的数量可以减少K倍，显著提升效率。\n\n### 新的建模工具包\n\n这种范式转变要求一套全新的建模工具。因此，研究开发了一个全面的**无似然（likelihood-free）框架**，以支持在连续域中进行稳健的训练、评估和可控采样。\n\n### 实验结果与意义\n\n实验结果表明，CALM 显著改善了性能与计算成本之间的权衡。它能够在显著降低计算成本的情况下，达到与强大离散基线模型相当的性能。\n\n更重要的是，这些发现确立了“预测下一个向量”作为实现超高效语言模型的一种强大且可扩展的途径。\n\n### 资源链接\n\n*   **代码**：[this https URL](this https URL)\n*   **项目**：[this https URL](this https URL)",
      "shortSummary": "大型语言模型（LLM）的效率受限于其逐词元生成过程。为解决此问题，研究引入了**连续自回归语言模型（CALM）**，将离散的“预测下一个词元”转变为连续的“预测下一个向量”。CALM使用高保真自编码器将K个词元压缩成一个连续向量，并能以高精度重构，从而将生成步骤减少K倍。实验证明，CALM显著改善了性能与计算成本的权衡，为实现超高效语言模型提供了强大且可扩展的新途径。",
      "translated_title": "连续自回归语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "The efficiency of large language models (LLMs) is fundamentally limited by their sequential, token-by-token generation process. We argue that overcoming this bottleneck requires a new design axis for LLM scaling: increasing the semantic bandwidth of each generative step. To this end, we introduce Continuous Autoregressive Language Models (CALM), a paradigm shift from discrete next-token prediction to continuous next-vector prediction. CALM uses a high-fidelity autoencoder to compress a chunk of K tokens into a single continuous vector, from which the original tokens can be reconstructed with over 99.9\\% accuracy. This allows us to model language as a sequence of continuous vectors instead of discrete tokens, which reduces the number of generative steps by a factor of K. The paradigm shift necessitates a new modeling toolkit; therefore, we develop a comprehensive likelihood-free framework that enables robust training, evaluation, and controllable sampling in the continuous domain. Experiments show that CALM significantly improves the performance-compute trade-off, achieving the performance of strong discrete baselines at a significantly lower computational cost. More importantly, these findings establish next-vector prediction as a powerful and scalable pathway towards ultra-efficient language models. Code: https://github.com/shaochenze/calm. Project: https://shaochenze.github.io/blog/2025/CALM."
    },
    {
      "title": "分阶段DMD：通过子区间内的分数匹配实现少步分布匹配蒸馏 (原标题: Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals)",
      "link": "https://arxiv.org/abs/2510.27684",
      "pubDate": "Fri, 31 Oct 2025 13:55:10 GMT",
      "isoDate": "2025-10-31T13:55:10.000Z",
      "creator": "Xiangyu Fan, Zesong Qiu, Zhuguanyu Wu, Fanzhou Wang, Zhiqian Lin, Tianxiang Ren, Dahua Lin, Ruihao Gong, Lei Yang",
      "summary": "## 分阶段DMD：通过子区间内的分数匹配实现少步分布匹配蒸馏\n\n### 引言\n\n分布匹配蒸馏（DMD）是一种将基于分数的生成模型蒸馏成高效一步生成器的方法，无需与教师模型的采样轨迹一一对应。然而，这种一步蒸馏模型在处理复杂生成任务（例如文本到视频生成中合成复杂的物体运动）时，由于模型容量有限，表现不佳。\n\n直接将DMD扩展到多步蒸馏会带来内存使用增加和计算深度加深的问题，从而导致模型不稳定和效率降低。尽管现有工作提出了随机梯度截断作为潜在解决方案，但研究发现，这种方法会显著降低多步蒸馏模型的生成多样性，使其降至与一步模型相同的水平。\n\n### Phased DMD 框架\n\n为了解决上述限制，本文提出了 **Phased DMD**，一个多步蒸馏框架。该框架将分阶段蒸馏（phase-wise distillation）的思想与专家混合（Mixture-of-Experts, MoE）相结合，旨在降低学习难度，同时增强模型容量。\n\nPhased DMD 基于两个关键思想构建：\n\n1.  **渐进式分布匹配 (Progressive Distribution Matching)**\n    *   模型将信噪比（SNR）范围划分为多个子区间。\n    *   通过在这些子区间内逐步细化模型，使其能够达到更高的SNR水平，从而更好地捕捉复杂分布。\n\n2.  **子区间内的分数匹配 (Score Matching within Subintervals)**\n    *   为了确保每个子区间内的训练目标准确无误，研究人员进行了严格的数学推导。\n\n### 实验验证与结果\n\n研究团队通过蒸馏最先进的图像和视频生成模型来验证Phased DMD的有效性，其中包括拥有200亿参数的Qwen-Image和280亿参数的Wan2.2。\n\n实验结果表明：\n*   Phased DMD 比传统的DMD方法更好地保留了输出多样性。\n*   同时，它保持了关键的生成能力。\n\n### 结论与展望\n\nPhased DMD 提供了一种有效的方法，可以在多步蒸馏中平衡效率、稳定性和生成多样性。研究团队计划发布相关的代码和模型，以促进社区的进一步研究和应用。\n\n### 研究领域\n\n本文属于计算机视觉与模式识别（cs.CV）领域。",
      "shortSummary": "Phased DMD 提出了一种多步蒸馏框架，旨在解决传统DMD在复杂任务中表现不佳及多步蒸馏效率低、多样性差的问题。该方法通过将信噪比范围划分为子区间进行渐进式分布匹配，并确保子区间内分数匹配的准确性，结合专家混合（MoE）来降低学习难度并增强模型容量。实验证明，Phased DMD 在保持生成能力的同时，显著提升了输出多样性，优于现有DMD方法。",
      "translated_title": "分阶段DMD：通过子区间内的分数匹配实现少步分布匹配蒸馏",
      "images": [],
      "contentSource": "完整文章",
      "content": "Distribution Matching Distillation (DMD) distills score-based generative models into efficient one-step generators, without requiring a one-to-one correspondence with the sampling trajectories of their teachers. However, limited model capacity causes one-step distilled models underperform on complex generative tasks, e.g., synthesizing intricate object motions in text-to-video generation. Directly extending DMD to multi-step distillation increases memory usage and computational depth, leading to instability and reduced efficiency. While prior works propose stochastic gradient truncation as a potential solution, we observe that it substantially reduces the generation diversity of multi-step distilled models, bringing it down to the level of their one-step counterparts. To address these limitations, we propose Phased DMD, a multi-step distillation framework that bridges the idea of phase-wise distillation with Mixture-of-Experts (MoE), reducing learning difficulty while enhancing model capacity. Phased DMD is built upon two key ideas: progressive distribution matching and score matching within subintervals. First, our model divides the SNR range into subintervals, progressively refining the model to higher SNR levels, to better capture complex distributions. Next, to ensure the training objective within each subinterval is accurate, we have conducted rigorous mathematical derivations. We validate Phased DMD by distilling state-of-the-art image and video generation models, including Qwen-Image (20B parameters) and Wan2.2 (28B parameters). Experimental results demonstrate that Phased DMD preserves output diversity better than DMD while retaining key generative capabilities. We will release our code and models."
    },
    {
      "title": "通过对比触发学习对MLLM具身决策进行视觉后门攻击 (原标题: Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning)",
      "link": "https://arxiv.org/abs/2510.27623",
      "pubDate": "Fri, 31 Oct 2025 12:50:49 GMT",
      "isoDate": "2025-10-31T12:50:49.000Z",
      "creator": "Qiusi Zhan, Hyeonjeong Ha, Rui Yang, Sirui Xu, Hanyang Chen, Liang-Yan Gui, Yu-Xiong Wang, Huan Zhang, Heng Ji, Daniel Kang",
      "summary": "### 通过对比触发学习对MLLM具身决策进行视觉后门攻击\n\n本文介绍了一种名为BEAT的框架，旨在对基于多模态大语言模型（MLLM）的具身智能体实施视觉后门攻击。\n\n**背景与问题：**\n*   MLLM通过直接感知、推理和规划任务导向的动作，推动了具身智能体的发展。\n*   然而，这种视觉驱动的具身智能体引入了一个新的攻击面：视觉后门攻击。\n*   攻击机制：智能体在正常情况下表现正常，但当场景中出现特定视觉触发器时，会持续执行攻击者指定的多步策略。\n*   挑战：与文本触发器不同，物体触发器在视角和光照方面存在广泛变化，难以可靠地植入。\n\n**BEAT框架的解决方案：**\nBEAT是首个利用环境中物体作为触发器，将视觉后门注入MLLM具身智能体的框架。它通过以下两点解决物体触发器的变异性挑战：\n\n1.  **训练集构建：** 构建一个涵盖多样化场景、任务和触发器放置的训练集，使智能体暴露于触发器的多种变体。\n2.  **两阶段训练方案：**\n    *   **第一阶段：** 应用监督微调（SFT）。\n    *   **第二阶段：** 引入新颖的对比触发学习（CTL）。\n        *   CTL将触发器判别表述为触发器存在输入和触发器不存在输入之间的偏好学习。\n        *   这明确地锐化了决策边界，以确保后门的精确激活。\n\n**实验结果与发现：**\n*   在各种具身智能体基准和MLLM上，BEAT实现了高达80%的攻击成功率。\n*   同时，它保持了强大的良性任务性能。\n*   BEAT能够可靠地泛化到分布外（out-of-distribution）的触发器放置。\n*   值得注意的是，与简单的SFT相比，在有限的后门数据下，CTL将后门激活精度提高了高达39%。\n\n**结论与影响：**\n这些发现揭示了基于MLLM的具身智能体中一个关键但尚未被探索的安全风险，强调了在实际部署之前需要开发鲁棒防御措施。",
      "shortSummary": "本文提出了BEAT框架，首次实现了对MLLM具身智能体的视觉后门攻击。通过利用环境中物体作为触发器，BEAT采用多样化训练集和两阶段训练（包括新颖的对比触发学习CTL），解决了物体触发器变异性问题。实验表明，BEAT攻击成功率高达80%，同时保持良好任务性能，且CTL显著提升了后门激活精度。这揭示了MLLM具身智能体的严重安全风险，亟需加强防御。",
      "translated_title": "通过对比触发学习对MLLM具身决策进行视觉后门攻击",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal large language models (MLLMs) have advanced embodied agents by enabling direct perception, reasoning, and planning task-oriented actions from visual inputs. However, such vision driven embodied agents open a new attack surface: visual backdoor attacks, where the agent behaves normally until a visual trigger appears in the scene, then persistently executes an attacker-specified multi-step policy. We introduce BEAT, the first framework to inject such visual backdoors into MLLM-based embodied agents using objects in the environments as triggers. Unlike textual triggers, object triggers exhibit wide variation across viewpoints and lighting, making them difficult to implant reliably. BEAT addresses this challenge by (1) constructing a training set that spans diverse scenes, tasks, and trigger placements to expose agents to trigger variability, and (2) introducing a two-stage training scheme that first applies supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning (CTL). CTL formulates trigger discrimination as preference learning between trigger-present and trigger-free inputs, explicitly sharpening the decision boundaries to ensure precise backdoor activation. Across various embodied agent benchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while maintaining strong benign task performance, and generalizes reliably to out-of-distribution trigger placements. Notably, compared to naive SFT, CTL boosts backdoor activation accuracy up to 39% under limited backdoor data. These findings expose a critical yet unexplored security risk in MLLM-based embodied agents, underscoring the need for robust defenses before real-world deployment."
    },
    {
      "title": "用于世界模型增强的视觉-语言-动作模型的双流扩散 (原标题: Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model)",
      "link": "https://arxiv.org/abs/2510.27607",
      "pubDate": "Fri, 31 Oct 2025 12:32:12 GMT",
      "isoDate": "2025-10-31T12:32:12.000Z",
      "creator": "John Won, Kyungmin Lee, Huiwon Jang, Dongyoung Kim, Jinwoo Shin",
      "summary": "# DUST：用于世界模型增强视觉-语言-动作模型的双流扩散\n\n## 摘要\n\n本文提出了一种名为DUST（DUal-STream diffusion）的框架，旨在解决视觉-语言-动作（VLA）模型中联合预测下一状态观测和动作序列时存在的模态冲突问题。DUST通过其独特的双流扩散架构和解耦训练机制，显著提升了VLA在各种任务中的性能。\n\n## 背景与挑战\n\n近年来，将世界模型（world modeling）集成到视觉-语言-动作（VLA）模型中，在改进机器人策略学习方面展现出巨大潜力。然而，由于下一状态观测（视觉）和动作序列这两种模态之间固有的差异，如何有效地联合预测它们仍然是一个具有挑战性的问题。这种模态间的冲突限制了现有VLA模型的性能。\n\n## DUST方法概述\n\n为了应对上述挑战，DUST被提出作为一个世界模型增强的VLA框架。其核心创新点包括：\n\n*   **多模态扩散Transformer架构：**\n    *   DUST采用了一种新颖的多模态扩散Transformer架构。\n    *   该架构明确维护独立的模态流（例如，一个用于视觉，一个用于动作）。\n    *   同时，它通过精心设计的机制实现了有效的跨模态知识共享，确保不同模态间的信息能够互补。\n\n*   **解耦的训练机制：**\n    *   **独立噪声扰动：** DUST为每种模态引入了独立的噪声扰动，这使得模型能够更好地学习每种模态的特性。\n    *   **解耦流匹配损失：** 采用解耦的流匹配损失函数，进一步优化了训练过程。\n    *   **双向联合分布学习：** 这种设计使得模型能够以双向方式学习联合分布，而无需强制将不同模态映射到一个统一的潜在空间，从而避免了潜在的信息损失和模态冲突。\n\n*   **联合采样与测试时缩放：**\n    *   基于训练时模态的解耦特性，DUST引入了一种创新的联合采样方法。\n    *   该方法支持测试时缩放（test-time scaling），允许动作和视觉token以不同的速率异步演化，从而在推理阶段提供更大的灵活性和效率。\n\n## 实验结果与性能\n\nDUST在多个模拟和真实世界基准测试中展现出卓越的性能提升：\n\n*   **模拟基准测试：**\n    *   在RoboCasa和GR-1等模拟环境中，DUST比基线方法实现了高达6%的性能增益。\n    *   此外，DUST的测试时缩放方法额外提供了2-5%的性能提升。\n\n*   **真实世界任务：**\n    *   在与Franka Research 3机器人进行的真实世界任务中，DUST将成功率提高了13%。这有力地证实了其在模拟环境之外的实际有效性。\n\n*   **大规模预训练潜力：**\n    *   通过在BridgeV2（一个包含无动作视频的数据集）上进行预训练，DUST在RoboCasa任务上获得了显著的迁移增益。\n    *   这一结果突显了DUST在大规模VLA预训练方面的巨大潜力，为未来更通用、更强大的机器人模型奠定了基础。\n\n## 结论\n\nDUST通过其创新的双流扩散架构和解耦训练策略，成功解决了世界模型增强VLA中模态冲突的难题。它在模拟和真实世界任务中的显著性能提升，以及在大规模预训练方面的潜力，使其成为机器人策略学习领域的一个重要进展。",
      "shortSummary": "DUST是一种世界模型增强的视觉-语言-动作（VLA）框架，旨在解决VLA中视觉观测与动作序列的模态冲突。它采用双流扩散Transformer架构，独立处理并共享模态信息，通过解耦的训练机制学习联合分布。DUST在模拟基准测试（如RoboCasa）中实现了高达6%的性能提升，并在真实世界机器人任务中将成功率提高了13%。其测试时缩放和大规模预训练能力进一步增强了其有效性和应用潜力。",
      "translated_title": "用于世界模型增强的视觉-语言-动作模型的双流扩散",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recently, augmenting Vision-Language-Action models (VLAs) with world modeling has shown promise in improving robotic policy learning. However, it remains challenging to jointly predict next-state observations and action sequences because of the inherent difference between the two modalities. To address this, we propose DUal-STream diffusion (DUST), a world-model augmented VLA framework that handles the modality conflict and enhances the performance of VLAs across diverse tasks. Specifically, we propose a multimodal diffusion transformer architecture that explicitly maintains separate modality streams while still enabling cross-modal knowledge sharing. In addition, we introduce independent noise perturbations for each modality and a decoupled flow-matching loss. This design enables the model to learn the joint distribution in a bidirectional manner while avoiding the need for a unified latent space. Based on the decoupling of modalities during training, we also introduce a joint sampling method that supports test-time scaling, where action and vision tokens evolve asynchronously at different rates. Through experiments on simulated benchmarks such as RoboCasa and GR-1, DUST achieves up to 6% gains over baseline methods, while our test-time scaling approach provides an additional 2-5% boost. On real-world tasks with the Franka Research 3, DUST improves success rates by 13%, confirming its effectiveness beyond simulation. Furthermore, pre-training on action-free videos from BridgeV2 yields significant transfer gains on RoboCasa, underscoring DUST's potential for large-scale VLA pretraining."
    },
    {
      "title": "Spatial-SSRL：通过自监督强化学习增强空间理解 (原标题: Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2510.27606",
      "pubDate": "Fri, 31 Oct 2025 12:30:08 GMT",
      "isoDate": "2025-10-31T12:30:08.000Z",
      "creator": "Yuhong Liu, Beichen Zhang, Yuhang Zang, Yuhang Cao, Long Xing, Xiaoyi Dong, Haodong Duan, Dahua Lin, Jiaqi Wang",
      "summary": "# Spatial-SSRL：通过自监督强化学习增强空间理解\n\n## 1. 研究背景与问题\n大型视觉-语言模型（LVLMs）在空间理解方面仍存在显著的弱点。现有的解决方案，如监督微调（SFT）和近期带有可验证奖励的强化学习（RLVR）方法，都面临以下挑战：\n*   **高昂的监督成本**：需要大量人工标注或专业工具。\n*   **特定环境依赖**：受限于特定或受控环境，难以大规模应用。\n\n## 2. Spatial-SSRL 方法介绍\n本文引入了 **Spatial-SSRL**，这是一种全新的自监督强化学习范式，旨在直接从普通的RGB或RGB-D图像中获取可验证的信号。其核心特点在于：\n*   **自监督机制**：无需人工或LVLM标注，自动生成监督信号。\n*   **多任务预设**：自动构建五种预设任务，以捕捉2D和3D空间结构。\n\n## 3. 五种预设任务\nSpatial-SSRL 设计了以下五种预设任务，它们提供了易于验证的真实答案：\n1.  **乱序补丁重排 (Shuffled Patch Reordering)**：模型需要将打乱的图像补丁重新排列到正确的位置。\n2.  **翻转补丁识别 (Flipped Patch Recognition)**：模型需要识别出图像中被翻转的补丁。\n3.  **裁剪补丁修复 (Cropped Patch Inpainting)**：模型需要修复图像中被裁剪掉的区域。\n4.  **区域深度排序 (Regional Depth Ordering)**：模型需要判断图像中不同区域的相对深度关系。\n5.  **相对3D位置预测 (Relative 3D Position Prediction)**：模型需要预测物体或区域在3D空间中的相对位置。\n\n这些任务的共同优势在于，它们的真实标签可以从图像本身直接推导，无需额外的人工标注或复杂的工具。\n\n## 4. 实验结果与性能提升\n通过在这些任务上进行训练，Spatial-SSRL 取得了显著的成果：\n*   **空间推理能力大幅提升**：模型在空间推理方面的表现得到显著改善。\n*   **通用视觉能力保持**：在增强空间理解的同时，模型原有的通用视觉能力并未受损。\n*   **基准测试表现**：\n    *   在图像和视频设置下的七个空间理解基准测试中，相对于Qwen2.5-VL基线模型，Spatial-SSRL 实现了平均准确率的显著提升。\n    *   对于3B模型，平均准确率提升了 **4.63%**。\n    *   对于7B模型，平均准确率提升了 **3.89%**。\n\n## 5. 结论与意义\nSpatial-SSRL 的研究结果表明，简单、内在的自监督信号能够大规模地支持可验证奖励的强化学习（RLVR），为LVLMs实现更强大的空间智能提供了一条实用且可扩展的途径。",
      "shortSummary": "Spatial-SSRL提出一种自监督强化学习范式，旨在解决大型视觉-语言模型（LVLMs）在空间理解方面的弱点。该方法通过从RGB/RGB-D图像中自动生成五种预设任务（如乱序补丁重排、区域深度排序）来获取可验证信号，无需人工标注。实验表明，Spatial-SSRL显著提升了LVLMs的空间推理能力，同时保持了通用视觉能力。在七个空间理解基准测试中，相对于Qwen2.5-VL基线，平均准确率分别提高了4.63%（3B）和3.89%（7B），为LVLMs实现更强空间智能提供了一条实用且可扩展的途径。",
      "translated_title": "Spatial-SSRL：通过自监督强化学习增强空间理解",
      "images": [],
      "contentSource": "完整文章",
      "content": "Spatial understanding remains a weakness of Large Vision-Language Models (LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcement learning with verifiable rewards (RLVR) pipelines depend on costly supervision, specialized tools, or constrained environments that limit scale. We introduce Spatial-SSRL, a self-supervised RL paradigm that derives verifiable signals directly from ordinary RGB or RGB-D images. Spatial-SSRL automatically formulates five pretext tasks that capture 2D and 3D spatial structure: shuffled patch reordering, flipped patch recognition, cropped patch inpainting, regional depth ordering, and relative 3D position prediction. These tasks provide ground-truth answers that are easy to verify and require no human or LVLM annotation. Training on our tasks substantially improves spatial reasoning while preserving general visual capabilities. On seven spatial understanding benchmarks in both image and video settings, Spatial-SSRL delivers average accuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Our results show that simple, intrinsic supervision enables RLVR at scale and provides a practical route to stronger spatial intelligence in LVLMs."
    },
    {
      "title": "HyperClick：通过不确定性校准提升可靠的GUI定位 (原标题: HyperClick: Advancing Reliable GUI Grounding via Uncertainty Calibration)",
      "link": "https://arxiv.org/abs/2510.27266",
      "pubDate": "Fri, 31 Oct 2025 04:07:02 GMT",
      "isoDate": "2025-10-31T04:07:02.000Z",
      "creator": "Shaojie Zhang, Pei Fu, Ruoceng Zhang, Jiahui Yang, Anan Du, Xiuwen Xi, Shaokang Wang, Ying Huang, Bin Qin, Zhenbo Luo, Jian Luan",
      "summary": "# HyperClick：通过不确定性校准提升可靠的GUI定位\n\n## 引言与问题背景\n自主图形用户界面（GUI）代理在执行用户命令时，依赖于准确的GUI定位能力，即将语言指令映射到屏幕上的具体坐标。然而，当前的模型，无论是通过监督微调（SFT）还是强化微调（RFT）训练，都普遍缺乏对其自身能力边界的自我认知。这种缺陷导致模型过度自信，并产生不可靠的预测。在动态的GUI自动化任务中，即使是单个错误也可能导致整个任务失败，因此这一问题显得尤为关键。\n\n## 置信度与准确性的评估\n研究人员首先系统地评估了通用模型和GUI特定模型中的概率置信度（probabilistic confidence）和口头置信度（verbalized confidence）。评估结果揭示了一个重要问题：模型的置信度与其实际准确性之间存在显著的不一致性。\n\n## HyperClick 框架的提出\n为了解决上述置信度与准确性不匹配的问题，本文提出了一种名为 **HyperClick** 的新型框架。HyperClick 旨在通过不确定性校准（uncertainty calibration）来增强GUI定位的可靠性。\n\n### HyperClick 的核心机制\nHyperClick 引入了一种创新的双重奖励机制，该机制共同优化了定位准确性和置信度可靠性，并促进了模型内省式的自我批评能力：\n1.  **二元奖励（Binary Reward）：** 对正确的动作给予直接的二元奖励。\n2.  **截断高斯（Truncated Gaussian）空间置信度建模：** 结合了基于Brier分数（Brier score）的校准方法，用于对模型的空间置信度进行建模和校准。\n\n通过这种双重奖励机制，HyperClick 能够更好地理解其预测的不确定性，并进行自我调整。\n\n## 实验结果与性能\n研究团队在七个具有挑战性的基准测试上进行了广泛的实验。实验结果表明：\n*   HyperClick 实现了最先进（state-of-the-art）的性能。\n*   同时，它提供了良好校准的置信度。\n*   通过实现明确的置信度校准和内省式自我批评，HyperClick 有效地减少了模型的过度自信。\n*   最终，这使得GUI自动化系统更加可靠。\n\n## 研究领域\n本文的研究属于计算机视觉与模式识别（Computer Vision and Pattern Recognition, cs.CV）领域。",
      "shortSummary": "现有GUI代理在语言指令定位时常因过度自信导致预测不可靠。为解决此问题，本文提出HyperClick框架。它通过引入结合二元奖励和基于Brier分数校准的截断高斯空间置信度建模的双重奖励机制，共同优化定位准确性和置信度可靠性，从而提升内省式自我批评能力。实验证明，HyperClick在七个基准测试中实现了最先进的性能，并提供了良好校准的置信度，有效减少了过度自信，使GUI自动化更加可靠。",
      "translated_title": "HyperClick：通过不确定性校准提升可靠的GUI定位",
      "images": [],
      "contentSource": "完整文章",
      "content": "Autonomous Graphical User Interface (GUI) agents rely on accurate GUI grounding, which maps language instructions to on-screen coordinates, to execute user commands. However, current models, whether trained via supervised fine-tuning (SFT) or reinforcement fine-tuning (RFT), lack self-awareness of their capability boundaries, leading to overconfidence and unreliable predictions. We first systematically evaluate probabilistic and verbalized confidence in general and GUI-specific models, revealing a misalignment between confidence and actual accuracy, which is particularly critical in dynamic GUI automation tasks, where single errors can cause task failure. To address this, we propose HyperClick, a novel framework that enhances reliable GUI grounding through uncertainty calibration. HyperClick introduces a dual reward mechanism, combining a binary reward for correct actions with a truncated Gaussian-based spatial confidence modeling, calibrated using the Brier score. This approach jointly optimizes grounding accuracy and confidence reliability, fostering introspective self-criticism. Extensive experiments on seven challenge benchmarks show that HyperClick achieves state-of-the-art performance while providing well-calibrated confidence. By enabling explicit confidence calibration and introspective self-criticism, HyperClick reduces overconfidence and supports more reliable GUI automation."
    },
    {
      "title": "高阶线性注意力 (原标题: Higher-order Linear Attention)",
      "link": "https://arxiv.org/abs/2510.27258",
      "pubDate": "Fri, 31 Oct 2025 03:54:37 GMT",
      "isoDate": "2025-10-31T03:54:37.000Z",
      "creator": "Yifan Zhang, Zhen Qin, Quanquan Gu",
      "summary": "## 高阶线性注意力 (Higher-order Linear Attention)\n\n### 背景与问题\n\n*   **核心障碍**：缩放点积注意力（scaled dot-product attention）的二次计算成本是限制自回归语言模型扩展到长上下文（long contexts）的核心障碍。\n*   **现有替代方案及其局限**：\n    *   线性时间注意力（Linear-time attention）和状态空间模型（State Space Models, SSMs）提供了可扩展的替代方案。\n    *   然而，它们通常受限于一阶或基于核的近似，这可能限制了它们的表达能力（expressivity）。\n\n### 引入高阶线性注意力 (HLA)\n\n*   我们引入了**高阶线性注意力（Higher-order Linear Attention, HLA）**，这是一种创新的机制。\n*   **机制特性**：\n    *   它是一种因果（causal）、流式（streaming）机制。\n    *   通过紧凑的前缀充分统计量（compact prefix sufficient statistics）实现更高阶的交互。\n\n### HLA 的关键特性与优势\n\n*   **二阶情况**：\n    *   HLA 在二阶情况下能够保持恒定大小的状态（constant-size state）。\n    *   以线性时间（linear time）计算每个token的输出。\n    *   无需具体化任何 $n \times n$ 矩阵，显著降低了计算复杂度。\n*   **数学与实现**：\n    *   提供了闭式流式恒等式（closed-form streaming identities）。\n    *   引入了一种严格因果的掩码变体（strictly causal masked variant），该变体使用了两个额外的摘要（additional summaries）。\n    *   设计了一种基于关联扫描（associative scans）的块并行训练方案（chunk-parallel training scheme），该方案能够精确重现串行递归（serial recurrence）的激活。\n*   **可扩展性**：\n    *   文章进一步概述了将HLA扩展到三阶及更高阶的方法。\n\n### 总结\n\n*   这些研究成果共同将HLA定位为一个**原则性（principled）、可扩展（scalable）的构建模块**。\n*   它成功地将类似注意力的、数据依赖的混合（data-dependent mixing）能力与现代循环架构（modern recurrent architectures）的效率结合起来。",
      "shortSummary": "为解决自回归语言模型中缩放点积注意力二次成本限制长上下文的问题，本文提出了高阶线性注意力（HLA）。HLA是一种因果、流式机制，通过紧凑的前缀统计量实现更高阶交互。在二阶情况下，它保持恒定状态大小，以线性时间计算输出，无需具体化 $n \times n$ 矩阵。HLA结合了注意力的数据依赖混合特性与现代循环架构的效率，提供了一个原则性、可扩展的解决方案，旨在提高长上下文语言模型的表达能力和效率。",
      "translated_title": "高阶线性注意力",
      "images": [],
      "contentSource": "完整文章",
      "content": "The quadratic cost of scaled dot-product attention is a central obstacle to scaling autoregressive language models to long contexts. Linear-time attention and State Space Models (SSMs) provide scalable alternatives but are typically restricted to first-order or kernel-based approximations, which can limit expressivity. We introduce Higher-order Linear Attention (HLA), a causal, streaming mechanism that realizes higher interactions via compact prefix sufficient statistics. In the second-order case, HLA maintains a constant-size state and computes per-token outputs in linear time without materializing any n times n matrices. We give closed-form streaming identities, a strictly causal masked variant using two additional summaries, and a chunk-parallel training scheme based on associative scans that reproduces the activations of a serial recurrence exactly. We further outline extensions to third and higher orders. Collectively, these results position HLA as a principled, scalable building block that combines attention-like, data-dependent mixing with the efficiency of modern recurrent architectures. Project Page: https://github.com/yifanzhang-pro/HLA."
    },
    {
      "title": "掩膜到高度：一种基于YOLOv11的卫星图像建筑实例分割与高度分类联合架构 (原标题: Mask-to-Height: A YOLOv11-Based Architecture for Joint Building Instance Segmentation and Height Classification from Satellite Imagery)",
      "link": "https://arxiv.org/abs/2510.27224",
      "pubDate": "Fri, 31 Oct 2025 02:37:08 GMT",
      "isoDate": "2025-10-31T02:37:08.000Z",
      "creator": "Mahmoud El Hussieni, Bahadır K. Güntürk, Hasan F. Ateş, Oğuz Hanoğlu",
      "summary": "# Mask-to-Height：基于YOLOv11的卫星图像建筑实例分割与高度分类联合架构\n\n## 引言\n准确的建筑实例分割和高度分类对于城市规划、3D城市建模和基础设施监测至关重要。本文详细分析了YOLO系列深度学习模型的最新进展——YOLOv11，并重点探讨了其在从卫星图像中联合提取建筑和离散高度分类方面的应用。\n\n## YOLOv11架构特点\nYOLOv11在早期YOLO模型的基础上进行了改进，引入了更高效的架构，主要特点包括：\n*   **多尺度特征融合**：更好地结合了不同尺度的特征。\n*   **对象定位精度提升**：提高了对象定位的准确性。\n*   **复杂城市场景表现增强**：在复杂的城市场景中表现出更强的性能。\n\n## 研究方法与数据集\n*   **数据集**：研究使用了DFC2023 Track 2数据集，该数据集包含来自12个城市的超过125,000个带标注的建筑。\n*   **评估指标**：模型的性能通过精度（precision）、召回率（recall）、F1分数和平均精度均值（mAP）等指标进行评估。\n\n## 实验结果与性能\n研究结果表明，YOLOv11在建筑实例分割和高度分类方面取得了显著成果：\n*   **实例分割性能**：\n    *   mAP@50 达到 60.4%。\n    *   mAP@50-95 达到 38.3%。\n*   **高度分类精度**：在五个预定义的高度层级上保持了稳健的分类准确性。\n*   **复杂场景处理能力**：该模型在处理遮挡、复杂建筑形状和类别不平衡（特别是对于稀有的高层建筑）方面表现出色。\n*   **性能对比**：与早期多任务框架的比较分析证实，YOLOv11在检测精度和推理速度方面均优于前者，使其非常适合实时、大规模的城市测绘。\n\n## 结论与展望\n本研究强调了YOLOv11通过简化分类高度建模来推进语义城市重建的潜力，为遥感和地理空间智能的未来发展提供了可操作的见解。",
      "shortSummary": "本文提出了一种基于YOLOv11的架构，用于从卫星图像中联合进行建筑实例分割和离散高度分类。该模型在DFC2023 Track 2数据集上进行了评估，实现了60.4%的mAP@50和38.3%的mAP@50-95，并在五个高度层级上保持了稳健的分类精度。YOLOv11在处理复杂场景和类别不平衡方面表现出色，且在检测精度和推理速度上优于现有模型，适用于实时、大规模城市测绘，为城市重建和地理空间智能提供了新途径。",
      "translated_title": "掩膜到高度：一种基于YOLOv11的卫星图像建筑实例分割与高度分类联合架构",
      "images": [],
      "contentSource": "完整文章",
      "content": "Accurate building instance segmentation and height classification are critical for urban planning, 3D city modeling, and infrastructure monitoring. This paper presents a detailed analysis of YOLOv11, the recent advancement in the YOLO series of deep learning models, focusing on its application to joint building extraction and discrete height classification from satellite imagery. YOLOv11 builds on the strengths of earlier YOLO models by introducing a more efficient architecture that better combines features at different scales, improves object localization accuracy, and enhances performance in complex urban scenes. Using the DFC2023 Track 2 dataset -- which includes over 125,000 annotated buildings across 12 cities -- we evaluate YOLOv11's performance using metrics such as precision, recall, F1 score, and mean average precision (mAP). Our findings demonstrate that YOLOv11 achieves strong instance segmentation performance with 60.4\\% mAP@50 and 38.3\\% mAP@50--95 while maintaining robust classification accuracy across five predefined height tiers. The model excels in handling occlusions, complex building shapes, and class imbalance, particularly for rare high-rise structures. Comparative analysis confirms that YOLOv11 outperforms earlier multitask frameworks in both detection accuracy and inference speed, making it well-suited for real-time, large-scale urban mapping. This research highlights YOLOv11's potential to advance semantic urban reconstruction through streamlined categorical height modeling, offering actionable insights for future developments in remote sensing and geospatial intelligence."
    },
    {
      "title": "Denario项目：用于科学发现的深度知识AI智能体 (原标题: The Denario project: Deep knowledge AI agents for scientific discovery)",
      "link": "https://arxiv.org/abs/2510.26887",
      "pubDate": "Thu, 30 Oct 2025 14:00:12 GMT",
      "isoDate": "2025-10-30T14:00:12.000Z",
      "creator": "Francisco Villaescusa-Navarro, Boris Bolliet, Pablo Villanueva-Domingo, Adrian E. Bayer, Aidan Acquah, Chetana Amancharla, Almog Barzilay-Siegal, Pablo Bermejo, Camille Bilodeau, Pablo Cárdenas Ramírez, Miles Cranmer, Urbano L. França, ChangHoon Hahn, Yan-Fei Jiang, Raul Jimenez, Jun-Young Lee, Antonio Lerario, Osman Mamun, Thomas Meier, Anupam A. Ojha, Pavlos Protopapas, Shimanto Roy, David N. Spergel, Pedro Tarancón-Álvarez, Ujjwal Tiwari, Matteo Viel, Digvijay Wadekar, Chi Wang, Bonny Y. Wang, Licong Xu, Yossi Yovel, Shuwen Yue, Wen-Han Zhou, Qiyao Zhu, Jiajun Zou, Íñigo Zubeldia",
      "summary": "## Denario项目：用于科学发现的深度知识AI智能体\n\nDenario是一个创新的AI多智能体系统，旨在作为科学研究的强大助手。该系统能够执行广泛的任务，从而显著加速和辅助科学发现过程。\n\n### 核心功能与能力\nDenario具备以下多方面的能力，覆盖了科学研究的多个阶段：\n*   **思想生成**：能够提出新的研究想法。\n*   **文献查阅**：高效地检索和检查相关文献。\n*   **研究计划制定**：协助开发详细的研究计划。\n*   **代码编写与执行**：能够编写、运行和调试代码。\n*   **图表制作**：生成数据可视化图表。\n*   **论文撰写与评审**：起草科学论文并进行评审。\n\n### 系统架构\nDenario采用模块化架构设计，使其既能处理特定任务，也能通过集成Cmbagent作为深度研究后端，进行端到端的科学分析。这种灵活性使得Denario能够适应不同的研究需求。\n\n### 应用与成果展示\n为了展示其强大的能力，研究人员通过Denario生成了多篇AI撰写的论文草稿，涵盖了广泛的科学领域，包括：\n*   天体物理学\n*   生物学\n*   生物物理学\n*   生物医学信息学\n*   化学\n*   材料科学\n*   数学物理学\n*   医学\n*   神经科学\n*   行星科学\n\nDenario尤其擅长结合不同学科的思想，例如，它成功生成了一篇将量子物理学和机器学习方法应用于天体物理数据的论文，展现了其跨学科整合的潜力。\n\n### 评估与讨论\n这些由AI生成的论文经过了领域专家的评估。专家们提供了数值评分和类似同行评审的反馈，肯定了Denario的成果。文章还详细讨论了当前系统的优点、缺点和局限性，并深入探讨了AI驱动研究的伦理影响及其与科学哲学的关系。\n\n### 可用性\nDenario项目的代码已公开发布。此外，用户可以直接在网络上运行Denario的演示版本，并且完整的应用程序将在云端部署，以便更广泛的使用。",
      "shortSummary": "Denario是一个创新的AI多智能体系统，旨在作为科学研究助手。它能执行从想法生成、文献查阅、代码编写到论文撰写与评审等多种任务。该系统已成功生成了跨天体物理学、生物学等多个学科的AI论文，并能有效结合不同领域的思想。这些成果获得了领域专家的评估与认可。Denario的代码和演示版本已公开，未来将在云端部署。",
      "translated_title": "Denario项目：用于科学发现的深度知识AI智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Denario, an AI multi-agent system designed to serve as a scientific research assistant. Denario can perform many different tasks, such as generating ideas, checking the literature, developing research plans, writing and executing code, making plots, and drafting and reviewing a scientific paper. The system has a modular architecture, allowing it to handle specific tasks, such as generating an idea, or carrying out end-to-end scientific analysis using Cmbagent as a deep-research backend. In this work, we describe in detail Denario and its modules, and illustrate its capabilities by presenting multiple AI-generated papers generated by it in many different scientific disciplines such as astrophysics, biology, biophysics, biomedical informatics, chemistry, material science, mathematical physics, medicine, neuroscience and planetary science. Denario also excels at combining ideas from different disciplines, and we illustrate this by showing a paper that applies methods from quantum physics and machine learning to astrophysical data. We report the evaluations performed on these papers by domain experts, who provided both numerical scores and review-like feedback. We then highlight the strengths, weaknesses, and limitations of the current system. Finally, we discuss the ethical implications of AI-driven research and reflect on how such technology relates to the philosophy of science. We publicly release the code at https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run directly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and the full app will be deployed on the cloud."
    },
    {
      "title": "视频模型是否已准备好成为零样本推理器？基于MME-CoF基准的实证研究 (原标题: Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark)",
      "link": "https://arxiv.org/abs/2510.26802",
      "pubDate": "Thu, 30 Oct 2025 13:59:55 GMT",
      "isoDate": "2025-10-30T13:59:55.000Z",
      "creator": "Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, Pheng-Ann Heng",
      "summary": "## 视频模型作为零样本推理器的能力评估\n\n### 引言\n\n近期，视频生成模型在生成高保真、时间连贯的视频方面取得了显著进展，这表明它们可能编码了大量的世界知识。除了逼真的合成能力外，这些模型还展现出视觉感知、建模和操作等新兴行为。然而，一个关键问题依然存在：在具有挑战性的视觉推理场景中，视频模型是否已准备好作为零样本推理器？\n\n### 研究方法\n\n本研究旨在通过一项实证研究，全面探讨这一问题，重点关注领先且广受欢迎的Veo-3模型。研究人员系统地评估了Veo-3在12个不同维度上的推理行为，以全面刻画其优势和失效模式。这些维度包括：\n\n*   **空间逻辑**\n*   **几何逻辑**\n*   **物理逻辑**\n*   **时间逻辑**\n*   **具身逻辑**\n\n为了标准化这项研究，评估数据被整理成MME-CoF基准，这是一个紧凑的基准，旨在对“帧链推理”（Chain-of-Frame, CoF）进行深入而彻底的评估。\n\n### 主要发现\n\n研究结果揭示了当前视频模型在推理能力方面的表现：\n\n*   **优势：** 模型在以下方面展现出有前景的推理模式：\n    *   短时程空间连贯性\n    *   细粒度接地（fine-grained grounding）\n    *   局部一致的动态\n\n*   **局限性：** 模型在以下方面仍存在限制：\n    *   长时程因果推理\n    *   严格的几何约束\n    *   抽象逻辑\n\n### 结论\n\n总体而言，尽管当前视频模型展现出一些令人鼓舞的推理迹象，但它们尚未可靠地作为独立的零样本推理器。然而，它们作为专用推理模型的补充视觉引擎，展现出巨大的潜力。",
      "shortSummary": "本研究通过MME-CoF基准，对领先的Veo-3视频模型作为零样本推理器的能力进行了实证评估。研究发现，尽管模型在短时程空间连贯性和局部动态方面表现出潜力，但在长时程因果推理、严格几何约束和抽象逻辑方面仍有限制。结论是，当前视频模型尚未能独立作为可靠的零样本推理器，但可作为专用推理模型的有效视觉补充。",
      "translated_title": "视频模型是否已准备好成为零样本推理器？基于MME-CoF基准的实证研究",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-CoF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. Project page: https://video-cof.github.io"
    },
    {
      "title": "OmniX：从统一全景生成与感知到图形就绪的3D场景 (原标题: OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes)",
      "link": "https://arxiv.org/abs/2510.26800",
      "pubDate": "Thu, 30 Oct 2025 13:59:51 GMT",
      "isoDate": "2025-10-30T13:59:51.000Z",
      "creator": "Yukun Huang, Jiwen Yu, Yanning Zhou, Jianan Wang, Xintao Wang, Pengfei Wan, Xihui Liu",
      "summary": "### OmniX：从统一全景生成与感知到图形就绪的3D场景\n\n本文介绍了一种名为 OmniX 的新框架，旨在将全景生成和感知技术提升到新的水平，以创建可用于物理渲染（PBR）、重新打光和模拟的图形就绪 3D 场景。\n\n**背景与挑战：**\n*   构建 3D 场景主要有两种方法：程序生成和 2D 提升。\n*   基于全景图的 2D 提升是一种有前景的技术，它利用强大的 2D 生成先验知识来生成沉浸式、真实且多样化的 3D 环境。\n*   现有 2D 提升方法主要侧重于外观生成，往往忽略了对几何、纹理和 PBR 材料等内在属性的感知。\n\n**OmniX 的核心洞察与方法：**\n*   **关键洞察：** 重新利用 2D 生成模型进行全景几何、纹理和 PBR 材料的感知。\n*   **统一框架：** OmniX 是一个多功能且统一的框架，与现有方法不同，它强调对内在属性的感知。\n*   **技术实现：** 基于轻量级且高效的跨模态适配器结构，OmniX 能够重用 2D 生成先验知识，以处理广泛的全景视觉任务。\n\n**OmniX 的功能范围：**\n*   全景感知\n*   全景生成\n*   全景补全\n\n**数据集贡献：**\n*   研究团队构建了一个大规模的合成全景数据集。\n*   该数据集包含来自各种室内和室外场景的高质量多模态全景图。\n\n**实验结果与意义：**\n*   广泛的实验证明了 OmniX 模型在全景视觉感知和图形就绪 3D 场景生成方面的有效性。\n*   OmniX 为沉浸式、物理真实虚拟世界的生成开辟了新的可能性。",
      "shortSummary": "OmniX 框架通过重新利用 2D 生成模型进行全景几何、纹理和 PBR 材料的感知，将基于全景图的 2D 提升技术推进到可生成图形就绪 3D 场景的水平。它是一个统一且多功能的框架，能够进行全景感知、生成和补全，从而为创建沉浸式、物理真实的虚拟世界提供了新途径。",
      "translated_title": "OmniX：从统一全景生成与感知到图形就绪的3D场景",
      "images": [],
      "contentSource": "完整文章",
      "content": "There are two prevalent ways to constructing 3D scenes: procedural generation and 2D lifting. Among them, panorama-based 2D lifting has emerged as a promising technique, leveraging powerful 2D generative priors to produce immersive, realistic, and diverse 3D environments. In this work, we advance this technique to generate graphics-ready 3D scenes suitable for physically based rendering (PBR), relighting, and simulation. Our key insight is to repurpose 2D generative models for panoramic perception of geometry, textures, and PBR materials. Unlike existing 2D lifting approaches that emphasize appearance generation and ignore the perception of intrinsic properties, we present OmniX, a versatile and unified framework. Based on a lightweight and efficient cross-modal adapter structure, OmniX reuses 2D generative priors for a broad range of panoramic vision tasks, including panoramic perception, generation, and completion. Furthermore, we construct a large-scale synthetic panorama dataset containing high-quality multimodal panoramas from diverse indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness of our model in panoramic visual perception and graphics-ready 3D scene generation, opening new possibilities for immersive and physically realistic virtual world generation."
    },
    {
      "title": "通用运动生成：数据、模型与评估的探索 (原标题: The Quest for Generalizable Motion Generation: Data, Model, and Evaluation)",
      "link": "https://arxiv.org/abs/2510.26794",
      "pubDate": "Thu, 30 Oct 2025 13:59:27 GMT",
      "isoDate": "2025-10-30T13:59:27.000Z",
      "creator": "Jing Lin, Ruisi Wang, Junzhe Lu, Ziqi Huang, Guorui Song, Ailing Zeng, Xian Liu, Chen Wei, Wanqi Yin, Qingping Sun, Zhongang Cai, Lei Yang, Ziwei Liu",
      "summary": "# 通用运动生成：数据、模型与评估的探索\n\n本文旨在解决3D人体运动生成（MoGen）领域在泛化能力方面的根本性瓶颈。尽管现有模型在标准基准测试上取得了进展，但其泛化能力仍显不足。作者观察到，视频生成（ViGen）等相邻生成领域在建模人类行为方面展现出卓越的泛化能力，这为MoGen提供了可借鉴的经验。\n\n受此启发，本文提出了一个全面的框架，系统地将ViGen的知识从以下三个关键支柱转移到MoGen：数据、建模和评估。\n\n## 1. 数据：ViMoGen-228K 大规模数据集\n\n*   **引入 ViMoGen-228K**：这是一个包含228,000个高质量运动样本的大规模数据集。\n*   **数据来源整合**：该数据集整合了多种数据源：\n    *   高保真光学运动捕捉（MoCap）数据。\n    *   来自网络视频的语义标注运动。\n    *   由最先进的ViGen模型生成的合成样本。\n*   **语义多样性扩展**：数据集包含文本-运动对（text-motion pairs）和文本-视频-运动三元组（text-video-motion triplets），显著扩展了语义多样性。\n\n## 2. 建模：ViMoGen 及其轻量级变体\n\n*   **提出 ViMoGen 模型**：\n    *   基于流匹配（flow-matching-based）的扩散Transformer。\n    *   通过门控多模态条件（gated multimodal conditioning）统一了来自MoCap数据和ViGen模型的先验知识。\n*   **开发 ViMoGen-light**：\n    *   为了提高效率，进一步开发了ViMoGen的蒸馏变体（distilled variant）。\n    *   该变体消除了对视频生成的依赖，同时保留了强大的泛化能力。\n\n## 3. 评估：MBench 分层基准\n\n*   **引入 MBench**：一个分层基准，旨在进行细粒度评估。\n*   **评估维度**：MBench 评估以下三个方面：\n    *   运动质量（motion quality）。\n    *   提示词保真度（prompt fidelity）。\n    *   泛化能力（generalization ability）。\n\n## 实验结果与可用性\n\n*   **显著性能提升**：广泛的实验表明，该框架在自动评估和人工评估中均显著优于现有方法。\n*   **公开可用性**：相关的代码、数据和基准将公开发布。",
      "shortSummary": "本文提出一个全面框架，旨在提升3D人体运动生成（MoGen）的泛化能力，借鉴了视频生成（ViGen）的经验。该框架从数据、模型和评估三方面进行创新：引入ViMoGen-228K大规模数据集，整合MoCap、网络视频和ViGen合成数据；提出ViMoGen（基于流匹配的扩散Transformer）及其高效变体ViMoGen-light；并开发MBench分层基准进行细粒度评估。实验证明，该框架显著优于现有方法，代码、数据和基准将公开。",
      "translated_title": "通用运动生成：数据、模型与评估的探索",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite recent advances in 3D human motion generation (MoGen) on standard benchmarks, existing models still face a fundamental bottleneck in their generalization capability. In contrast, adjacent generative fields, most notably video generation (ViGen), have demonstrated remarkable generalization in modeling human behaviors, highlighting transferable insights that MoGen can leverage. Motivated by this observation, we present a comprehensive framework that systematically transfers knowledge from ViGen to MoGen across three key pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a large-scale dataset comprising 228,000 high-quality motion samples that integrates high-fidelity optical MoCap data with semantically annotated motions from web videos and synthesized samples generated by state-of-the-art ViGen models. The dataset includes both text-motion pairs and text-video-motion triplets, substantially expanding semantic diversity. Second, we propose ViMoGen, a flow-matching-based diffusion transformer that unifies priors from MoCap data and ViGen models through gated multimodal conditioning. To enhance efficiency, we further develop ViMoGen-light, a distilled variant that eliminates video generation dependencies while preserving strong generalization. Finally, we present MBench, a hierarchical benchmark designed for fine-grained evaluation across motion quality, prompt fidelity, and generalization ability. Extensive experiments show that our framework significantly outperforms existing approaches in both automatic and human evaluations. The code, data, and benchmark will be made publicly available."
    },
    {
      "title": "通过FP16解决训练-推理不匹配问题 (原标题: Defeating the Training-Inference Mismatch via FP16)",
      "link": "https://arxiv.org/abs/2510.26788",
      "pubDate": "Thu, 30 Oct 2025 13:58:11 GMT",
      "isoDate": "2025-10-30T13:58:11.000Z",
      "creator": "Penghui Qi, Zichen Liu, Xiangxin Zhou, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin",
      "summary": "## 通过FP16解决训练-推理不匹配问题\n\n### 引言\n大型语言模型（LLMs）的强化学习（RL）微调过程经常面临不稳定性问题。这种不稳定性主要源于训练策略和推理策略之间存在的数值不匹配。\n\n### 问题根源：浮点精度\n*   **现有方法的局限性：** 尽管之前的研究尝试通过算法修正或工程对齐来缓解这一问题，但本文指出其根本原因在于浮点精度本身。\n*   **BF16的缺陷：** 广泛采用的BF16（Brain Floating Point 16）格式，尽管具有较大的动态范围，却引入了显著的舍入误差。这些误差破坏了训练和推理之间的一致性，导致了不稳定性。\n\n### 解决方案：回归FP16\n*   本文的核心发现是，简单地回归使用**FP16**（Half-precision floating-point format）可以有效地消除这种训练-推理不匹配。\n\n### FP16的优势\n*   **实施简便：** 这一改变非常简单，仅需修改几行代码，无需对模型架构或学习算法进行任何改动。\n*   **广泛支持：** 现代框架已全面支持FP16。\n*   **性能提升：** 研究结果表明，统一使用FP16能带来多方面的好处：\n    *   **更稳定的优化：** 优化过程更加平稳。\n    *   **更快的收敛速度：** 模型训练收敛更快。\n    *   **更强的性能：** 在各种任务、算法和框架上均表现出更强的性能。\n\n### 结论与展望\n本文希望这些发现能够促使业界在RL微调中更广泛地重新审视浮点精度权衡问题。",
      "shortSummary": "RL微调LLM的不稳定性源于训练与推理策略间的数值不匹配，其根本原因是BF16引入的舍入误差破坏了一致性。本文提出，简单地回归使用FP16能有效消除此不匹配。FP16方案实施简单，无需修改模型或算法，且能带来更稳定的优化、更快的收敛和更强的性能，鼓励重新思考RL微调中的精度权衡。",
      "translated_title": "通过FP16解决训练-推理不匹配问题",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement learning (RL) fine-tuning of large language models (LLMs) often suffers from instability due to the numerical mismatch between the training and inference policies. While prior work has attempted to mitigate this issue through algorithmic corrections or engineering alignments, we show that its root cause lies in the floating point precision itself. The widely adopted BF16, despite its large dynamic range, introduces large rounding errors that breaks the consistency between training and inference. In this work, we demonstrate that simply reverting to FP16 effectively eliminates this mismatch. The change is simple, fully supported by modern frameworks with only a few lines of code change, and requires no modification to the model architecture or learning algorithm. Our results suggest that using FP16 uniformly yields more stable optimization, faster convergence, and stronger performance across diverse tasks, algorithms and frameworks. We hope these findings motivate a broader reconsideration of precision trade-offs in RL fine-tuning."
    },
    {
      "title": "远程劳务指数：衡量人工智能对远程工作的自动化程度 (原标题: Remote Labor Index: Measuring AI Automation of Remote Work)",
      "link": "https://arxiv.org/abs/2510.26787",
      "pubDate": "Thu, 30 Oct 2025 13:58:04 GMT",
      "isoDate": "2025-10-30T13:58:04.000Z",
      "creator": "Mantas Mazeika, Alice Gatti, Cristina Menghini, Udari Madhushani Sehwag, Shivam Singhal, Yury Orlovskiy, Steven Basart, Manasi Sharma, Denis Peskoff, Elaine Lau, Jaehyuk Lim, Lachlan Carroll, Alice Blair, Vinaya Sivakumar, Sumana Basu, Brad Kenstler, Yuntao Ma, Julian Michael, Xiaoke Li, Oliver Ingebretsen, Aditya Mehta, Jean Mottola, John Teichmann, Kevin Yu, Zaina Shaik, Adam Khoja, Richard Ren, Jason Hausenloy, Long Phan, Ye Htet, Ankit Aich, Tahseen Rabbani, Vivswan Shah, Andriy Novykov, Felix Binder, Kirill Chugunov, Luis Ramirez, Matias Geralnik, Hernán Mesura, Dean Lee, Ed-Yeremai Hernandez Cardona, Annette Diamond, Summer Yue, Alexandr Wang, Bing Liu, Ernesto Hernandez, Dan Hendrycks",
      "summary": "## 远程劳务指数：衡量人工智能对远程工作的自动化程度\n\n### 引言\n\n尽管人工智能（AI）在知识和推理等研究导向的基准测试中取得了显著进展，但这些进步如何转化为实际的经济价值和自动化能力，目前仍不明确。为了弥合这一差距，并提供对AI实际影响的实证评估，研究人员引入了一个新的基准。\n\n### 核心贡献：远程劳务指数 (RLI)\n\n为了量化AI在实际经济场景中的自动化潜力，本研究提出了**远程劳务指数（Remote Labor Index, RLI）**。RLI旨在提供一个全面且实用的评估框架：\n\n*   **多部门基准：** RLI是一个广泛涵盖多个经济部门的基准，确保其评估的普适性。\n*   **真实世界项目：** 它由一系列具有实际经济价值的项目组成，这些项目模拟了远程工作环境中的真实任务。\n*   **端到端性能评估：** RLI专注于评估AI代理在实际设置中的端到端性能，而不仅仅是孤立的任务。\n\n### 主要发现\n\n对AI代理在RLI上的表现进行评估后，研究得出了以下关键结果：\n\n*   **表现接近最低水平：** 总体而言，AI代理在RLI上的表现接近最低水平，表明其在处理复杂、真实世界远程工作任务方面的自动化能力仍非常有限。\n*   **最高自动化率：** 即使是表现最佳的AI代理，其自动化率也仅达到2.5%。这远低于许多关于AI自动化潜力的预期。\n\n### 意义与影响\n\n这些实证结果对于理解和讨论AI自动化具有重要意义：\n\n*   **提供实证依据：** RLI的结果为关于AI自动化的讨论提供了坚实的实证依据，有助于将这些讨论建立在具体数据而非推测之上。\n*   **跟踪AI影响的共同基础：** 它为跟踪AI对劳动力市场和经济影响提供了一个共同的、可量化的基础。\n*   **赋能利益相关者：** 通过提供清晰的性能指标，RLI使政策制定者、企业和员工等利益相关者能够更主动地应对AI驱动的劳务自动化带来的挑战和机遇。",
      "shortSummary": "本研究引入了“远程劳务指数（RLI）”，这是一个多部门基准，旨在衡量人工智能（AI）在真实世界远程工作中的自动化能力。尽管AI在研究基准上进展迅速，但RLI测试结果显示，AI代理的实际自动化率极低，表现最佳的代理仅达到2.5%。这为评估AI对劳务自动化的实际影响提供了实证依据，并帮助利益相关者更好地应对未来的挑战。",
      "translated_title": "远程劳务指数：衡量人工智能对远程工作的自动化程度",
      "images": [],
      "contentSource": "完整文章",
      "content": "AIs have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economically valuable projects designed to evaluate end-to-end agent performance in practical settings. AI agents perform near the floor on RLI, with the highest-performing agent achieving an automation rate of 2.5%. These results help ground discussions of AI automation in empirical evidence, setting a common basis for tracking AI impacts and enabling stakeholders to proactively navigate AI-driven labor automation."
    },
    {
      "title": "ChartAB：图表基础定位与密集对齐基准 (原标题: ChartAB: A Benchmark for Chart Grounding & Dense Alignment)",
      "link": "https://arxiv.org/abs/2510.26781",
      "pubDate": "Thu, 30 Oct 2025 13:56:31 GMT",
      "isoDate": "2025-10-30T13:56:31.000Z",
      "creator": "Aniruddh Bansal, Davit Soselia, Dang Nguyen, Tianyi Zhou",
      "summary": "## ChartAB：图表基础定位与密集对齐基准\n\n### 引言\n\n图表在可视化、推理、数据分析以及人类思想交流中扮演着重要角色。然而，现有的视觉-语言模型（VLMs）在图表理解方面存在局限性，具体表现为：\n\n*   **缺乏细节感知**：无法准确感知图表的细微细节。\n*   **难以提取精细结构**：难以从图表中提取精细的结构信息。\n*   **阻碍图表比较与推理**：图表基础定位（chart grounding）的这些限制也阻碍了VLMs比较多个图表并进行推理的能力。\n\n### ChartAB 基准的引入\n\n为了解决上述问题，本文引入了一个新颖的“ChartAlign Benchmark (ChartAB)”，旨在为VLMs在图表基础定位任务中提供一个全面的评估。ChartAB能够评估VLMs在以下方面的能力：\n\n*   **提取表格数据**：从图表中准确提取出表格形式的数据。\n*   **定位可视化元素**：精确识别并定位图表中的各种可视化元素。\n*   **识别属性**：识别来自各种类型和复杂度的图表的各种属性。\n\n### 评估方法与流程\n\nChartAB的设计特点包括：\n\n*   **JSON模板**：设计了一个JSON模板，以方便计算专门为每个基础定位任务量身定制的评估指标。\n*   **两阶段推理工作流**：通过整合一个新颖的两阶段推理工作流，该基准能够进一步评估VLMs在两个图表之间对齐和比较元素/属性的能力。\n\n### 评估结果与发现\n\n对几个近期VLMs进行的评估分析揭示了关于它们在图表理解方面的以下新见解：\n\n*   **感知偏差**：模型在感知图表信息时存在的偏差。\n*   **弱点**：模型在处理特定图表结构或任务时的不足。\n*   **鲁棒性**：模型面对不同图表变化时的稳定性。\n*   **幻觉**：模型在理解图表时产生的不准确或虚假信息。\n\n这些发现强调了VLMs在图表理解任务中存在的精细差异，并指出了当前模型需要加强的具体技能。",
      "shortSummary": "ChartAB是一个新颖的基准，旨在全面评估视觉-语言模型（VLMs）在图表理解方面的能力。它专注于图表基础定位（提取数据、定位元素、识别属性）和跨图表密集对齐。通过使用JSON模板和两阶段推理工作流，ChartAB揭示了现有VLMs在图表理解中的感知偏差、弱点、鲁棒性问题和幻觉。这些发现为未来模型改进指明了方向，以提升其对图表细节的精细感知和推理能力。",
      "translated_title": "ChartAB：图表基础定位与密集对齐基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Charts play an important role in visualization, reasoning, data analysis, and the exchange of ideas among humans. However, existing vision-language models (VLMs) still lack accurate perception of details and struggle to extract fine-grained structures from charts. Such limitations in chart grounding also hinder their ability to compare multiple charts and reason over them. In this paper, we introduce a novel \"ChartAlign Benchmark (ChartAB)\" to provide a comprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting tabular data, localizing visualization elements, and recognizing various attributes from charts of diverse types and complexities. We design a JSON template to facilitate the calculation of evaluation metrics specifically tailored for each grounding task. By incorporating a novel two-stage inference workflow, the benchmark can further evaluate VLMs' capability to align and compare elements/attributes across two charts. Our analysis of evaluations on several recent VLMs reveals new insights into their perception biases, weaknesses, robustness, and hallucinations in chart understanding. These findings highlight the fine-grained discrepancies among VLMs in chart understanding tasks and point to specific skills that need to be strengthened in current models."
    },
    {
      "title": "AMO-Bench：大型语言模型在高中数学竞赛中仍面临挑战 (原标题: AMO-Bench: Large Language Models Still Struggle in High School Math Competitions)",
      "link": "https://arxiv.org/abs/2510.26768",
      "pubDate": "Thu, 30 Oct 2025 13:52:02 GMT",
      "isoDate": "2025-10-30T13:52:02.000Z",
      "creator": "Shengnan An, Xunliang Cai, Xuezhi Cao, Xiaoyu Li, Yehao Lin, Junlin Liu, Xinxuan Lv, Dan Ma, Xuanlin Wang, Ziwen Wang, Shuang Zhou",
      "summary": "### AMO-Bench：评估大型语言模型数学推理能力的新基准\n\n**引言**\n\n*   现有数学竞赛基准（如AIME24/25）在评估顶级大型语言模型（LLMs）的数学推理能力方面已不再有效，因为LLMs的表现已达到饱和。\n*   为解决这一问题，研究人员提出了AMO-Bench，一个具有奥林匹克竞赛级别甚至更高难度的进阶数学推理基准。\n\n**AMO-Bench 的特点**\n\n*   **问题数量与来源**：包含50道由人类精心设计的问题。\n*   **难度标准**：\n    *   所有50道问题都经过专家交叉验证，确保其难度至少达到国际数学奥林匹克（IMO）标准。\n    *   这保证了基准的挑战性，能够有效区分顶级LLMs的能力。\n*   **原创性**：所有问题均为原创，旨在防止LLMs通过数据记忆而产生潜在的性能泄露。\n*   **评估机制**：\n    *   每道问题仅要求提供最终答案，而非证明过程。\n    *   这种设计使得评估能够实现自动化和鲁棒的评分。\n\n**实验结果与分析**\n\n*   **测试模型数量**：对26个LLMs进行了AMO-Bench上的实验评估。\n*   **整体表现**：\n    *   即使是表现最好的模型，在AMO-Bench上的准确率也仅达到52.4%。\n    *   大多数LLMs的得分低于40%。\n    *   这些结果突出表明当前LLMs在数学推理方面仍有巨大的改进空间。\n*   **扩展趋势**：进一步分析揭示，随着测试时计算资源的增加，AMO-Bench上的性能呈现出有前景的扩展趋势。\n\n**结论与展望**\n\n*   AMO-Bench的发布旨在促进对语言模型推理能力提升的进一步研究。\n*   该基准为评估和推动LLMs在高级数学推理领域的进步提供了一个严格且有效的工具。",
      "shortSummary": "AMO-Bench是一个新的、由50道原创人类设计问题组成的进阶数学推理基准，难度达到国际数学奥林匹克级别，旨在解决现有基准对大型语言模型（LLMs）评估饱和的问题。对26个LLMs的测试显示，即使是表现最好的模型准确率也仅为52.4%，大多数模型低于40%，表明LLMs在数学推理方面仍有显著提升空间。研究还发现性能与计算资源之间存在积极的扩展趋势。AMO-Bench的发布旨在推动LLMs推理能力的研究。",
      "translated_title": "AMO-Bench：大型语言模型在高中数学竞赛中仍面临挑战",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present AMO-Bench, an Advanced Mathematical reasoning benchmark with Olympiad level or even higher difficulty, comprising 50 human-crafted problems. Existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models (LLMs). However, many existing math competitions are becoming less effective for assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To address this, AMO-Bench introduces more rigorous challenges by ensuring all 50 problems are (1) cross-validated by experts to meet at least the International Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original problems to prevent potential performance leakages from data memorization. Moreover, each problem in AMO-Bench requires only a final answer rather than a proof, enabling automatic and robust grading for evaluation. Experimental results across 26 LLMs on AMO-Bench show that even the best-performing model achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%. Beyond these poor performances, our further analysis reveals a promising scaling trend with increasing test-time compute on AMO-Bench. These results highlight the significant room for improving the mathematical reasoning in current LLMs. We release AMO-Bench to facilitate further research into advancing the reasoning abilities of language models. https://amo-bench.github.io/"
    },
    {
      "title": "ThinkMorph：多模态交错思维链推理中的涌现特性 (原标题: ThinkMorph: Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning)",
      "link": "https://arxiv.org/abs/2510.27492",
      "pubDate": "Thu, 30 Oct 2025 13:51:38 GMT",
      "isoDate": "2025-10-30T13:51:38.000Z",
      "creator": "Jiawei Gu, Yunzhuo Hao, Huichen Will Wang, Linjie Li, Michael Qizhe Shieh, Yejin Choi, Ranjay Krishna, Yu Cheng",
      "summary": "### ThinkMorph：多模态交错思维链推理中的涌现特性\n\n**1. 背景与挑战**\n\n*   多模态推理要求语言和视觉之间进行迭代协调。\n*   目前尚不清楚什么构成有意义的交错思维链。\n\n**2. 核心原则**\n\n*   研究提出，文本和图像思维应作为互补而非同构的模态，相互促进推理。\n\n**3. ThinkMorph 模型介绍**\n\n*   **模型构建：** 基于上述原则，研究团队构建了ThinkMorph，一个统一模型。\n*   **训练数据：** ThinkMorph 在24K高质量的交错推理轨迹上进行微调，这些轨迹涵盖了不同视觉参与度的任务。\n*   **学习能力：** 模型学会生成渐进式的文本-图像推理步骤，这些步骤能够：\n    *   具体地操作视觉内容。\n    *   同时保持连贯的语言逻辑。\n\n**4. 性能表现**\n\n*   **视觉中心基准测试：** 在以视觉为中心的基准测试中取得了显著提升，平均比基础模型高出34.7%。\n*   **泛化能力：** 能够泛化到域外任务，性能与更大、专有的视觉语言模型（VLM）相当或超越。\n\n**5. 涌现的多模态智能**\n\nThinkMorph 展现出多种涌现能力，包括：\n\n*   前所未见的视觉操作技能。\n*   在推理模式之间自适应切换。\n*   通过多样化的多模态推理实现更好的测试时扩展性。\n\n**6. 研究意义**\n\n*   这些发现为表征统一模型在多模态推理中涌现的能力提供了有前景的方向。",
      "shortSummary": "ThinkMorph是一个统一模型，旨在解决多模态推理中语言与视觉协调的挑战。它基于文本和图像思维互补的原则，通过在24K高质量推理轨迹上微调，学会生成渐进式文本-图像推理步骤，具体操作视觉内容并保持语言逻辑。ThinkMorph在视觉基准测试中表现出色，平均提升34.7%，并展现出视觉操作、模式切换和测试时扩展等涌现智能，为多模态推理的统一模型研究开辟了新方向。",
      "translated_title": "ThinkMorph：多模态交错思维链推理中的涌现特性",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal reasoning requires iterative coordination between language and vision, yet it remains unclear what constitutes a meaningful interleaved chain of thought. We posit that text and image thoughts should function as complementary, rather than isomorphic, modalities that mutually advance reasoning. Guided by this principle, we build ThinkMorph, a unified model fine-tuned on 24K high-quality interleaved reasoning traces spanning tasks with varying visual engagement. ThinkMorph learns to generate progressive text-image reasoning steps that concretely manipulate visual content while maintaining coherent verbal logic. It delivers large gains on vision-centric benchmarks (averaging 34.7% over the base model) and generalizes to out-of-domain tasks, matching or surpassing larger and proprietary VLMs. Beyond performance, ThinkMorph exhibits emergent multimodal intelligence, including unseen visual manipulation skills, adaptive switching between reasoning modes, and better test-time scaling through diversified multimodal thoughts.These findings suggest promising directions for characterizing the emergent capabilities of unified models for multimodal reasoning."
    },
    {
      "title": "手动解码的终结：迈向真正端到端的语言模型 (原标题: The End of Manual Decoding: Towards Truly End-to-End Language Models)",
      "link": "https://arxiv.org/abs/2510.26697",
      "pubDate": "Thu, 30 Oct 2025 13:01:43 GMT",
      "isoDate": "2025-10-30T13:01:43.000Z",
      "creator": "Zhichao Wang, Dongyang Ma, Xinting Huang, Deng Cai, Tian Lan, Jiahao Xu, Haitao Mi, Xiaoying Tang, Yan Wang",
      "summary": "# 手动解码的终结：迈向真正端到端的语言模型\n\n## 引言：当前LLM的局限性\n\n尽管大型语言模型（LLM）常被称为“端到端”，但实际上，这一标签存在误导。在实践中，LLM的生成过程依赖于一个不可微分的解码过程，该过程需要耗费大量精力手动调优超参数，例如温度（temperature）和top-p值。这种手动调优不仅劳动密集，而且限制了LLM的真正“端到端”能力。\n\n## AutoDeco：一种新型端到端架构\n\n本文引入了AutoDeco，一种新颖的架构，旨在通过让模型学习控制自身的解码策略，从而实现真正“端到端”的生成。\n\n### 核心思想与工作原理\n\nAutoDeco通过以下方式实现其目标：\n\n*   **增强标准Transformer：** 在标准的Transformer模型基础上，AutoDeco增加了轻量级的头部（lightweight heads）。\n*   **动态预测解码参数：** 在每个生成步骤中，这些轻量级头部除了预测下一个token的logits外，还会动态地预测与当前上下文相关的温度（temperature）和top-p值。\n*   **参数化、token级别的解码：** 这种方法将传统的解码过程转化为一个参数化的、token级别的过程。这意味着模型能够在一次前向传播中，根据上下文信息自我调节其采样策略，无需外部手动干预。\n\n## 实验结果与性能\n\n研究人员在八个基准测试上进行了广泛的实验，以评估AutoDeco的性能。结果表明：\n\n*   **显著超越默认策略：** AutoDeco的性能显著优于传统的默认解码策略。\n*   **媲美最优基线：** AutoDeco的性能达到了与“预言机调优基线”（oracle-tuned baseline）相当的水平。这个基线是通过“破解测试集”获得的，代表了任何静态方法的实际性能上限，这表明AutoDeco能够实现接近理论最优的解码效果。\n\n## 新兴能力：基于指令的解码控制\n\nAutoDeco还展现出一种令人振奋的新兴能力：\n\n*   **理解自然语言指令：** 模型学会了理解自然语言指令，例如“以低随机性生成”（“generate with low randomness”）。\n*   **逐token调整解码参数：** 根据这些指令，模型能够逐个token地调整其预测的温度和top-p值。\n*   **开辟新范式：** 这一能力为可控（steerable）和交互式（interactive）的LLM解码开辟了新的范式，用户可以通过简单的自然语言命令来指导模型的生成行为，从而实现更精细、更符合需求的文本输出。\n\n## 结论\n\nAutoDeco架构有效地解决了当前LLM解码过程中手动调优超参数的痛点，通过实现模型对自身解码策略的自适应控制，真正迈向了端到端的语言模型。其卓越的性能以及基于指令的解码控制能力，预示着LLM在生成效率、智能性和用户交互方面将迎来新的突破。",
      "shortSummary": "现有LLM因需手动调优解码超参数而并非真正“端到端”。本文介绍AutoDeco，一种新型架构，通过让模型在每个生成步骤中动态预测上下文相关的温度和top-p值，实现解码策略的自我调节，从而达到真正的端到端生成。AutoDeco在多项基准测试中显著优于默认策略，并能媲美最优调优基线。它还展现出根据自然语言指令进行解码控制的能力，为LLM解码提供了新的可控和交互式范式。",
      "translated_title": "手动解码的终结：迈向真正端到端的语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a non-differentiable decoding process that requires laborious, hand-tuning of hyperparameters like temperature and top-p. This paper introduces AutoDeco, a novel architecture that enables truly \"end-to-end\" generation by learning to control its own decoding strategy. We augment the standard transformer with lightweight heads that, at each step, dynamically predict context-specific temperature and top-p values alongside the next-token logits. This approach transforms decoding into a parametric, token-level process, allowing the model to self-regulate its sampling strategy within a single forward pass.   Through extensive experiments on eight benchmarks, we demonstrate that AutoDeco not only significantly outperforms default decoding strategies but also achieves performance comparable to an oracle-tuned baseline derived from \"hacking the test set\"-a practical upper bound for any static method. Crucially, we uncover an emergent capability for instruction-based decoding control: the model learns to interpret natural language commands (e.g., \"generate with low randomness\") and adjusts its predicted temperature and top-p on a token-by-token basis, opening a new paradigm for steerable and interactive LLM decoding."
    },
    {
      "title": "Kimi Linear：一种富有表现力、高效的注意力架构 (原标题: Kimi Linear: An Expressive, Efficient Attention Architecture)",
      "link": "https://arxiv.org/abs/2510.26692",
      "pubDate": "Thu, 30 Oct 2025 12:59:43 GMT",
      "isoDate": "2025-10-30T12:59:43.000Z",
      "creator": "Kimi Team, Yu Zhang, Zongyu Lin, Xingcheng Yao, Jiaxi Hu, Fanqing Meng, Chengyin Liu, Xin Men, Songlin Yang, Zhiyuan Li, Wentao Li, Enzhe Lu, Weizhou Liu, Yanru Chen, Weixin Xu, Longhui Yu, Yejie Wang, Yu Fan, Longguang Zhong, Enming Yuan, Dehao Zhang, Yizhi Zhang, T. Y. Liu, Haiming Wang, Shengjun Fang, Weiran He, Shaowei Liu, Yiwei Li, Jianlin Su, Jiezhong Qiu, Bo Pang, Junjie Yan, Zhejun Jiang, Weixiao Huang, Bohong Yin, Jiacheng You, Chu Wei, Zhengtao Wang, Chao Hong, Yutian Chen, Guanduo Chen, Yucheng Wang, Huabin Zheng, Feng Wang, Yibo Liu, Mengnan Dong, Zheng Zhang, Siyuan Pan, Wenhao Wu, Yuhao Wu, Longyu Guan, Jiawen Tao, Guohong Fu, Xinran Xu, Yuzhi Wang, Guokun Lai, Yuxin Wu, Xinyu Zhou, Zhilin Yang, Yulun Du",
      "summary": "## Kimi Linear：一种富有表现力、高效的注意力架构\n\n本文介绍了Kimi Linear，这是一种创新的混合线性注意力架构，首次在多种场景下（包括短上下文、长上下文和强化学习扩展）的公平比较中超越了全注意力（Full Attention）。\n\n### 核心创新：Kimi Delta Attention (KDA)\n*   **KDA是Kimi Linear的核心**：它是一个富有表现力的线性注意力模块。\n*   **扩展Gated DeltaNet**：KDA通过引入更细粒度的门控机制，有效地扩展了Gated DeltaNet。\n*   **优化内存利用**：这种机制使得KDA能够更有效地利用有限的有限状态RNN内存。\n\n### 硬件效率与算法\n*   **定制的块状算法**：Kimi Linear采用了一种定制的块状（chunkwise）算法，以实现高硬件效率。\n*   **DPLR转移矩阵的专门变体**：该算法通过使用Diagonal-Plus-Low-Rank (DPLR) 转移矩阵的专门变体来减少计算量。\n*   **与经典Delta规则的一致性**：这种变体与经典的delta规则保持更高的一致性，同时显著减少了计算开销，优于通用的DPLR公式。\n\n### 模型构建与性能\n*   **模型规模**：预训练了一个Kimi Linear模型，该模型包含30亿激活参数和总计480亿参数。\n*   **混合架构**：该模型基于KDA和多头潜在注意力（Multi-Head Latent Attention, MLA）的层级混合（layerwise hybrid）结构。\n*   **实验结果**：\n    *   在相同的训练方案下，Kimi Linear在所有评估任务中均以显著优势优于全MLA。\n    *   **KV缓存使用量大幅减少**：最高可达75%。\n    *   **解码吞吐量显著提升**：对于100万上下文长度，解码吞吐量最高可提高6倍。\n\n### 结论与开源支持\n*   **卓越的替代方案**：这些结果表明，Kimi Linear可以作为全注意力架构的直接替代品（drop-in replacement），并提供卓越的性能和效率，包括处理更长输入和输出长度的任务。\n*   **开源资源**：为了支持进一步的研究，项目开源了KDA内核和vLLM实现，并发布了预训练和指令微调的模型检查点。",
      "shortSummary": "Kimi Linear是一种创新的混合线性注意力架构，首次在多种场景下超越了全注意力。其核心是Kimi Delta Attention (KDA)，通过精细门控机制和高效的块状算法优化了内存使用和计算。实验证明，Kimi Linear在相同训练条件下显著优于全注意力，KV缓存使用量减少高达75%，解码吞吐量提高6倍。它可作为全注意力的直接替代品，提供卓越的性能和效率。项目已开源KDA内核和模型检查点。",
      "translated_title": "Kimi Linear：一种富有表现力、高效的注意力架构",
      "images": [
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png",
          "alt": "BibSonomy logo",
          "title": "",
          "position": 1
        },
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png",
          "alt": "Reddit logo",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule.   We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6 times decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths.   To support further research, we open-source the KDA kernel and vLLM implementations, and release the pre-trained and instruction-tuned model checkpoints."
    },
    {
      "title": "智能体组织时代：学习用语言模型进行组织 (原标题: The Era of Agentic Organization: Learning to Organize with Language Models)",
      "link": "https://arxiv.org/abs/2510.26658",
      "pubDate": "Thu, 30 Oct 2025 12:25:10 GMT",
      "isoDate": "2025-10-30T12:25:10.000Z",
      "creator": "Zewen Chi, Li Dong, Qingxiu Dong, Yaru Hao, Xun Wu, Shaohan Huang, Furu Wei",
      "summary": "## 智能体组织时代：学习用语言模型进行组织\n\n### 核心愿景：智能体组织\n\n文章展望了一个名为“智能体组织”（agentic organization）的AI新时代。在这个时代中，AI智能体将通过协作和并发工作来解决复杂的难题，从而实现超越单个智能体能力的成果。\n\n### 异步思维（AsyncThink）范式\n\n为了实现上述愿景，研究引入了“异步思维”（AsyncThink）作为大型语言模型（LLMs）推理的一种新范式。AsyncThink的核心在于将内部思维过程组织成可以并发执行的结构。\n\n### 思维协议与工作流程\n\n具体而言，AsyncThink提出了一种思维协议：\n\n*   **组织者（Organizer）**：动态地将子查询分配给多个“工作者”（workers）。\n*   **工作者（Workers）**：并行处理分配到的子查询。\n*   **知识合并与解决方案生成**：组织者随后合并工作者产生的中间知识，并生成连贯的最终解决方案。\n\n### 优化与学习能力\n\n更重要的是，该协议中的思维结构可以通过强化学习（reinforcement learning）进一步优化，从而提升其效率和效果。\n\n### 实验结果与性能\n\n实验结果表明，AsyncThink展现出显著的优势：\n\n*   **推理延迟**：与并行思维（parallel thinking）相比，AsyncThink的推理延迟降低了28%。\n*   **准确性提升**：在数学推理任务上，AsyncThink提高了准确性。\n*   **泛化能力**：AsyncThink能够泛化其学习到的异步思维能力，无需额外的训练即可有效处理未曾见过的任务。\n\n### 研究领域\n\n该研究属于人工智能（cs.AI）和计算与语言（cs.CL）领域。",
      "shortSummary": "该研究提出了“智能体组织”的AI新范式，通过智能体协作解决复杂问题。为实现此愿景，引入了“异步思维”（AsyncThink），一种用大型语言模型进行推理的新方法。AsyncThink将思维过程组织为并发结构，由组织者分配任务、合并知识。该思维结构可通过强化学习优化。实验证明，AsyncThink在降低28%推理延迟的同时提高了数学推理准确性，并能泛化到新任务。",
      "translated_title": "智能体组织时代：学习用语言模型进行组织",
      "images": [],
      "contentSource": "完整文章",
      "content": "We envision a new era of AI, termed agentic organization, where agents solve complex problems by working collaboratively and concurrently, enabling outcomes beyond individual intelligence. To realize this vision, we introduce asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large language models, which organizes the internal thinking process into concurrently executable structures. Specifically, we propose a thinking protocol where an organizer dynamically assigns sub-queries to workers, merges intermediate knowledge, and produces coherent solutions. More importantly, the thinking structure in this protocol can be further optimized through reinforcement learning. Experiments demonstrate that AsyncThink achieves 28% lower inference latency compared to parallel thinking while improving accuracy on mathematical reasoning. Moreover, AsyncThink generalizes its learned asynchronous thinking capabilities, effectively tackling unseen tasks without additional training."
    }
  ],
  "lastUpdated": "2025-11-03T09:39:00.809Z"
}