{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "Dress&Dance：随心所欲地装扮和舞蹈 - 技术预览 (原标题: Dress&Dance: Dress up and Dance as You Like It - Technical Preview)",
      "link": "https://arxiv.org/abs/2508.21070",
      "pubDate": "Thu, 28 Aug 2025 13:59:55 GMT",
      "isoDate": "2025-08-28T13:59:55.000Z",
      "creator": "Jun-Kun Chen, Aayush Bansal, Minh Phuoc Vo, Yu-Xiong Wang",
      "summary": "# Dress&Dance：随心所欲地装扮和舞蹈 - 技术预览\n\n## 1. 核心功能与目标\n*   **Dress&Dance** 是一个先进的视频扩散框架，专注于生成高质量的虚拟试穿视频。\n*   该框架能够根据用户的单张图像和一段给定的参考视频，生成用户穿着所需服装并按照参考视频动作移动的视频。\n*   生成的视频长度为5秒，帧率为24 FPS，分辨率高达1152x720。\n\n## 2. 输入要求与服装支持\n*   **所需输入**：仅需一张用户的图像。\n*   **服装类型**：支持广泛的服装类型，包括上衣、下装和连体衣。\n*   **多件试穿**：该系统还支持在一次处理中同时试穿上衣和下装。\n\n## 3. 关键技术：CondNet\n*   **CondNet** 是该框架的核心创新，它是一个新颖的条件网络。\n*   **工作原理**：CondNet 利用注意力机制，有效地统一了多模态输入，包括文本、图像和视频。\n*   **技术优势**：通过这种统一处理，CondNet 显著增强了服装的注册（即服装在人体上的准确贴合）和动作的保真度（即服装随身体动作的自然表现）。\n*   **训练方法**：CondNet 采用多阶段渐进式训练方法，结合了有限的视频数据和更易获取的大规模图像数据集，以优化其性能。\n\n## 4. 性能与用户体验\n*   **性能表现**：Dress&Dance 在性能上超越了现有的开源和商业解决方案。\n*   **用户体验**：它为用户提供了一种高质量且高度灵活的虚拟试穿体验。",
      "shortSummary": "Dress&Dance是一个创新的视频扩散框架，能根据单张用户图像和参考动作视频，生成5秒、1152x720分辨率的高质量虚拟试穿视频。其核心是CondNet，一个利用注意力机制统一多模态输入的条件网络，显著提升了服装注册和动作保真度。该框架支持多种服装类型，并超越了现有解决方案，提供高质量且灵活的试穿体验。",
      "translated_title": "Dress&Dance：随心所欲地装扮和舞蹈 - 技术预览",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Dress&amp;Dance, a video diffusion framework that generates high quality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of a user wearing desired garments while moving in accordance with a given reference video. Our approach requires a single user image and supports a range of tops, bottoms, and one-piece garments, as well as simultaneous tops and bottoms try-on in a single pass. Key to our framework is CondNet, a novel conditioning network that leverages attention to unify multi-modal inputs (text, images, and videos), thereby enhancing garment registration and motion fidelity. CondNet is trained on heterogeneous training data, combining limited video data and a larger, more readily available image dataset, in a multistage progressive manner. Dress&amp;Dance outperforms existing open source and commercial solutions and enables a high quality and flexible try-on experience."
    },
    {
      "title": "OneReward：通过多任务人类偏好学习实现统一的掩码引导图像生成 (原标题: OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning)",
      "link": "https://arxiv.org/abs/2508.21066",
      "pubDate": "Thu, 28 Aug 2025 13:59:46 GMT",
      "isoDate": "2025-08-28T13:59:46.000Z",
      "creator": "Yuan Gong, Xionghui Wang, Jie Wu, Shiyin Wang, Yitong Wang, Xinglong Wu",
      "summary": "## OneReward：统一的掩码引导图像生成框架\n\n本文介绍了一个名为 **OneReward** 的创新框架，旨在通过单一奖励模型提升模型在多任务下的生成能力。\n\n### 核心概念与目标\n*   **统一的强化学习框架**：OneReward是一个统一的强化学习框架，它使用**一个单一的奖励模型**来增强模型在不同评估标准下跨多个任务的生成能力。\n*   **单一视觉-语言模型（VLM）作为奖励模型**：该框架采用一个VLM作为生成奖励模型，该VLM能够区分给定任务和评估标准下的优胜者和劣势者。这使得它能够有效地应用于数据和任务目标多样化的多任务生成模型。\n\n### 应用领域：掩码引导图像生成\n*   OneReward特别应用于**掩码引导图像生成**领域，该领域可进一步细分为多个子任务，包括：\n    *   图像填充（Image Fill）\n    *   图像扩展（Image Extend）\n    *   对象移除（Object Removal）\n    *   文本渲染（Text Rendering）\n*   这些子任务都涉及使用二值掩码作为编辑区域。尽管它们共享相同的条件范式，但在底层数据分布和评估指标上存在显著差异。\n\n### 解决现有问题\n*   **现有方法的局限性**：当前方法通常依赖于任务特定的监督微调（SFT），这限制了模型的泛化能力和训练效率。\n*   **OneReward的优势**：OneReward通过多任务强化学习直接在预训练的基础模型上进行训练，**消除了对任务特定SFT的需求**，从而提高了效率和泛化性。\n\n### 模型开发与实验结果\n*   **Seedream 3.0 Fill**：基于OneReward，研究团队开发了Seedream 3.0 Fill，这是一个通过多任务强化学习训练的掩码引导生成模型。\n*   **卓越的性能**：实验结果表明，该统一的编辑模型在多个评估维度上持续优于商业和开源竞争对手，包括Ideogram、Adobe Photoshop和FLUX Fill [Pro]。",
      "shortSummary": "OneReward是一个统一的强化学习框架，通过使用单一视觉-语言模型作为奖励模型，提升了模型在多任务下的生成能力。它专注于掩码引导图像生成，涵盖图像填充、扩展、对象移除和文本渲染等任务。该框架无需任务特定的监督微调，直接在预训练模型上进行多任务强化学习。基于此，开发的Seedream 3.0 Fill模型在性能上超越了Ideogram、Adobe Photoshop等商业和开源竞争对手。",
      "translated_title": "OneReward：通过多任务人类偏好学习实现统一的掩码引导图像生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this paper, we introduce OneReward, a unified reinforcement learning framework that enhances the model's generative capabilities across multiple tasks under different evaluation criteria using only One Reward model. By employing a single vision-language model (VLM) as the generative reward model, which can distinguish the winner and loser for a given task and a given evaluation criterion, it can be effectively applied to multi-task generation models, particularly in contexts with varied data and diverse task objectives. We utilize OneReward for mask-guided image generation, which can be further divided into several sub-tasks such as image fill, image extend, object removal, and text rendering, involving a binary mask as the edit area. Although these domain-specific tasks share same conditioning paradigm, they differ significantly in underlying data distributions and evaluation metrics. Existing methods often rely on task-specific supervised fine-tuning (SFT), which limits generalization and training efficiency. Building on OneReward, we develop Seedream 3.0 Fill, a mask-guided generation model trained via multi-task reinforcement learning directly on a pre-trained base model, eliminating the need for task-specific SFT. Experimental results demonstrate that our unified edit model consistently outperforms both commercial and open-source competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across multiple evaluation dimensions. Code and model are available at: https://one-reward.github.io"
    },
    {
      "title": "OnGoal：在大语言模型多轮对话中跟踪和可视化对话目标 (原标题: OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models)",
      "link": "https://arxiv.org/abs/2508.21061",
      "pubDate": "Thu, 28 Aug 2025 13:58:29 GMT",
      "isoDate": "2025-08-28T13:58:29.000Z",
      "creator": "Adam Coscia, Shunan Guo, Eunyee Koh, Alex Endert",
      "summary": "## OnGoal：在大语言模型多轮对话中跟踪和可视化对话目标\n\n### 核心问题\n\n随着与大型语言模型（LLM）的多轮对话变得越来越长和复杂，用户如何更好地评估和审查其对话目标的进展成为一个挑战。\n\n### 解决方案：OnGoal 界面\n\nOnGoal 是一个专为解决上述问题而设计的 LLM 聊天界面，旨在帮助用户更好地管理对话目标的进展。\n\n### OnGoal 的主要功能\n\nOnGoal 提供了以下关键功能，以增强用户在复杂对话中的导航能力：\n\n*   **实时目标对齐反馈**：通过 LLM 辅助评估，OnGoal 能够提供关于对话内容与用户目标对齐程度的实时反馈。\n*   **评估结果解释**：针对评估结果，OnGoal 会提供相应的解释，并辅以示例，帮助用户理解反馈的依据。\n*   **目标进展概览**：界面提供随时间推移的目标进展概览，使用户能够清晰地了解对话的整体方向和阶段性成果。\n\n### 用户研究与发现\n\n研究团队对 OnGoal 进行了一项用户研究，涉及 20 名参与者，任务是完成一项写作任务。研究将 OnGoal 与一个不带目标跟踪功能的基线聊天界面进行了对比。\n\n研究结果表明，使用 OnGoal 的参与者：\n\n*   **效率提升**：花费更少的时间和精力来达成他们的目标。\n*   **策略探索**：能够探索新的提示策略来克服对话中的误解。\n*   **增强参与度和韧性**：这些发现表明，跟踪和可视化对话目标可以显著增强用户在 LLM 对话中的参与度和面对挑战时的韧性。\n\n### 对未来 LLM 聊天界面的设计启示\n\nOnGoal 的研究结果为未来 LLM 聊天界面的设计提供了重要的启示，旨在：\n\n*   **改进目标沟通**：优化用户与 LLM 之间关于目标的沟通方式。\n*   **减少认知负荷**：通过清晰的反馈和概览，降低用户在管理复杂对话时的认知负担。\n*   **增强交互性**：提升用户与 LLM 界面之间的互动体验。\n*   **实现反馈以提高 LLM 性能**：设计机制使用户的反馈能够反过来帮助改进 LLM 的性能。",
      "shortSummary": "OnGoal是一个LLM聊天界面，旨在帮助用户管理多轮对话中的目标进展。它通过LLM辅助评估提供实时目标对齐反馈、解释和进展概览。一项针对20名参与者的研究表明，使用OnGoal的参与者花费更少的时间和精力实现目标，并能探索新的提示策略克服误解，从而增强了LLM对话的参与度和韧性。研究结果为未来LLM聊天界面设计提供了改进目标沟通、减少认知负荷和增强交互性的启示。",
      "translated_title": "OnGoal：在大语言模型多轮对话中跟踪和可视化对话目标",
      "images": [],
      "contentSource": "完整文章",
      "content": "As multi-turn dialogues with large language models (LLMs) grow longer and more complex, how can users better evaluate and review progress on their conversational goals? We present OnGoal, an LLM chat interface that helps users better manage goal progress. OnGoal provides real-time feedback on goal alignment through LLM-assisted evaluation, explanations for evaluation results with examples, and overviews of goal progression over time, enabling users to navigate complex dialogues more effectively. Through a study with 20 participants on a writing task, we evaluate OnGoal against a baseline chat interface without goal tracking. Using OnGoal, participants spent less time and effort to achieve their goals while exploring new prompting strategies to overcome miscommunication, suggesting tracking and visualizing goals can enhance engagement and resilience in LLM dialogues. Our findings inspired design implications for future LLM chat interfaces that improve goal communication, reduce cognitive load, enhance interactivity, and enable feedback to improve LLM performance."
    },
    {
      "title": "多视角三维点跟踪 (原标题: Multi-View 3D Point Tracking)",
      "link": "https://arxiv.org/abs/2508.21060",
      "pubDate": "Thu, 28 Aug 2025 13:58:20 GMT",
      "isoDate": "2025-08-28T13:58:20.000Z",
      "creator": "Frano Rajič, Haofei Xu, Marko Mihajlovic, Siyuan Li, Irem Demir, Emircan Gündoğdu, Lei Ke, Sergey Prokudin, Marc Pollefeys, Siyu Tang",
      "summary": "### 多视角三维点跟踪器介绍\n\n本文介绍了一种新颖的数据驱动多视角三维点跟踪器，旨在解决动态场景中任意点的跟踪问题。该方法克服了现有单目跟踪器在深度模糊和遮挡方面的局限性，以及传统多相机方法对大量相机（超过20个）和繁琐的序列优化需求。\n\n#### 核心创新与方法\n\n*   **数据驱动与前馈模型**：该跟踪器是首个数据驱动的多视角三维点跟踪器，采用前馈模型，能够直接预测三维对应关系。\n*   **实用相机数量**：与需要大量相机的传统方法不同，该模型仅需少量实用相机（例如四个）即可实现鲁棒、准确的在线跟踪。\n*   **输入要求**：需要已知的相机姿态和多视角深度信息（可由传感器提供或估计）。\n*   **特征融合与更新**：\n    *   将多视角特征融合到一个统一的点云中。\n    *   结合k近邻（k-nearest-neighbors）相关性与基于Transformer的更新机制，以可靠地估计长距离三维对应关系。\n    *   即使在存在遮挡的情况下，也能有效工作。\n\n#### 训练与评估\n\n*   **训练数据**：模型在包含5000个合成多视角Kubric序列的数据集上进行训练。\n*   **真实世界基准测试**：在两个真实世界基准数据集上进行了评估：\n    *   Panoptic Studio：中位数轨迹误差为3.1厘米。\n    *   DexYCB：中位数轨迹误差为2.0厘米。\n\n#### 泛化能力\n\n该方法展现出良好的泛化能力，适用于多种相机设置：\n\n*   相机数量：1到8个视角。\n*   视角位置：不同的有利视角。\n*   视频长度：24到150帧。\n\n#### 贡献与未来影响\n\n通过发布该跟踪器及其训练和评估数据集，研究团队旨在：\n\n*   为多视角三维跟踪研究设定新的标准。\n*   为实际应用提供一个实用的工具。\n\n该研究成果已被ICCV 2025接收为口头报告（Oral）。项目页面可在提供的链接访问。",
      "shortSummary": "本文介绍了一种新颖的数据驱动多视角三维点跟踪器，旨在解决动态场景中的任意点跟踪问题。该模型仅需少量相机（如四个），通过融合多视角特征和基于Transformer的更新机制，即使在遮挡下也能鲁棒、准确地估计长距离三维对应关系。在Panoptic Studio和DexYCB基准测试中，分别实现了3.1厘米和2.0厘米的中位数轨迹误差。该方法具有良好的泛化能力，并随数据集一同发布，旨在为多视角三维跟踪研究设定新标准并提供实用工具。",
      "translated_title": "多视角三维点跟踪",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce the first data-driven multi-view 3D point tracker, designed to track arbitrary points in dynamic scenes using multiple camera views. Unlike existing monocular trackers, which struggle with depth ambiguities and occlusion, or prior multi-camera methods that require over 20 cameras and tedious per-sequence optimization, our feed-forward model directly predicts 3D correspondences using a practical number of cameras (e.g., four), enabling robust and accurate online tracking. Given known camera poses and either sensor-based or estimated multi-view depth, our tracker fuses multi-view features into a unified point cloud and applies k-nearest-neighbors correlation alongside a transformer-based update to reliably estimate long-range 3D correspondences, even under occlusion. We train on 5K synthetic multi-view Kubric sequences and evaluate on two real-world benchmarks: Panoptic Studio and DexYCB, achieving median trajectory errors of 3.1 cm and 2.0 cm, respectively. Our method generalizes well to diverse camera setups of 1-8 views with varying vantage points and video lengths of 24-150 frames. By releasing our tracker alongside training and evaluation datasets, we aim to set a new standard for multi-view 3D tracking research and provide a practical tool for real-world applications. Project page available at https://ethz-vlg.github.io/mvtracker."
    },
    {
      "title": "用于长视频生成的上下文混合模型 (原标题: Mixture of Contexts for Long Video Generation)",
      "link": "https://arxiv.org/abs/2508.21058",
      "pubDate": "Thu, 28 Aug 2025 13:57:55 GMT",
      "isoDate": "2025-08-28T13:57:55.000Z",
      "creator": "Shengqu Cai, Ceyuan Yang, Lvmin Zhang, Yuwei Guo, Junfei Xiao, Ziyan Yang, Yinghao Xu, Zhenheng Yang, Alan Yuille, Leonidas Guibas, Maneesh Agrawala, Lu Jiang, Gordon Wetzstein",
      "summary": "## 用于长视频生成的上下文混合模型 (Mixture of Contexts for Long Video Generation)\n\n### 核心问题：长视频生成中的上下文记忆挑战\n\n长视频生成本质上是一个“长上下文记忆问题”。在生成长视频时，模型必须能够在长时间范围内保留并检索显著事件，同时避免内容崩溃或漂移。然而，将扩散Transformer扩展到生成长上下文视频面临根本性限制，主要原因在于自注意力机制的二次方计算成本。这使得处理长序列时，内存和计算变得难以管理和优化。\n\n### 提出的解决方案：上下文混合模型 (MoC)\n\n为了解决上述挑战，研究人员提出了一种名为“上下文混合模型”（Mixture of Contexts, MoC）的新方法。MoC被设计为一个简单、可学习的稀疏注意力路由模块，旨在作为一种有效的长期记忆检索引擎。\n\n### MoC 的工作原理\n\nMoC 将长上下文视频生成重新定义为一个“内部信息检索任务”。其核心机制包括：\n\n*   **动态选择信息块**：每个查询（query）会动态地选择少数几个信息丰富的历史块（chunks）进行关注。\n*   **强制锚点**：除了动态选择的信息块，MoC 还包含强制性的锚点，例如视频的整体描述（caption）和局部窗口信息，以提供更稳定的上下文。\n*   **因果路由**：模型采用因果路由机制，这有助于防止生成过程中出现循环闭合（loop closures），从而保持时间上的连贯性。\n\n### MoC 的优势与成果\n\n通过MoC的设计和应用，该模型展现出显著的优势：\n\n*   **效率提升**：随着数据规模的扩大和路由的逐步稀疏化，MoC能够将计算资源分配给显著的历史信息。这种检索机制的副产品是实现了近似线性的扩展（near-linear scaling），极大地提高了效率。\n*   **实用性**：高效的计算使得长视频的训练和合成变得切实可行。\n*   **记忆与一致性**：MoC能够有效地保留视频内容中的身份、动作和场景，并在数分钟的视频内容中保持高度的一致性。在分钟级别的尺度上，模型的记忆能力和一致性得以显著提升。",
      "shortSummary": "长视频生成面临长上下文记忆挑战，传统扩散Transformer因自注意力机制的二次方成本而受限。本文提出“上下文混合模型”（MoC），一个可学习的稀疏注意力路由模块。MoC将长视频生成视为内部信息检索任务，通过动态选择信息块和强制锚点，并结合因果路由，实现近似线性的计算扩展。这使得模型能高效地在数分钟视频中保持身份、动作和场景的一致性，解决了长视频生成的记忆与效率问题。",
      "translated_title": "用于长视频生成的上下文混合模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes."
    },
    {
      "title": "FakeParts：一类新型AI生成深度伪造 (原标题: FakeParts: a New Family of AI-Generated DeepFakes)",
      "link": "https://arxiv.org/abs/2508.21052",
      "pubDate": "Thu, 28 Aug 2025 13:55:14 GMT",
      "isoDate": "2025-08-28T13:55:14.000Z",
      "creator": "Gaetan Brison, Soobash Daiboo, Samy Aimeur, Awais Hussain Sani, Xi Wang, Gianni Franchi, Vicky Kalogeiton",
      "summary": "# FakeParts：一类新型AI生成深度伪造\n\n本文介绍了“FakeParts”，这是一类新型的深度伪造技术，其特点是对真实视频的特定空间区域或时间片段进行细微、局部化的篡改。与完全合成的内容不同，这些部分篡改与真实元素无缝融合，使其具有极高的欺骗性且难以检测。\n\n## FakeParts 的主要特征\n\n*   **局部化篡改**：FakeParts 不会生成整个虚假视频，而是对现有真实视频的特定部分进行修改。\n*   **细微性**：这些篡改通常非常细微，例如改变面部表情、替换物体或修改背景。\n*   **无缝融合**：篡改部分与视频的其余真实元素完美融合，使得肉眼难以察觉。\n*   **高欺骗性**：由于其局部性和无缝性，FakeParts 比传统深度伪造更具欺骗性，更难被人类和现有检测模型识别。\n\n## FakePartsBench 数据集\n\n为了解决当前检测能力的关键空白，研究人员推出了 **FakePartsBench**，这是首个专门为捕捉全范围局部深度伪造而设计的大规模基准数据集。\n\n*   **规模**：包含超过25,000个视频。\n*   **标注**：提供像素级别和帧级别的篡改标注，这对于开发和评估检测方法至关重要。\n*   **目的**：旨在实现对检测方法的全面评估，推动更鲁棒的检测技术发展。\n\n## 检测挑战与性能下降\n\n研究结果表明，FakeParts 对现有检测方法构成了严峻挑战：\n\n*   **人类检测准确率下降**：与传统深度伪造相比，FakeParts 使人类的检测准确率降低了30%以上。\n*   **模型性能下降**：最先进的检测模型也观察到类似的性能下降。\n\n## 结论与意义\n\n这项工作揭示了当前深度伪造检测方法中存在的紧急漏洞，并提供了必要的资源，以开发针对局部视频篡改的更强大、更有效的检测方法。",
      "shortSummary": "“FakeParts”是一种新型AI生成深度伪造，通过对真实视频进行细微、局部化的篡改，使其与真实元素无缝融合，极具欺骗性且难以检测。与传统深度伪造相比，FakeParts使人类和现有检测模型的检测准确率显著下降。为应对此挑战，研究团队推出了大规模基准数据集FakePartsBench（含2.5万余视频及详细标注），旨在推动更鲁棒的局部视频篡改检测方法的发展，填补当前检测能力的关键空白。",
      "translated_title": "FakeParts：一类新型AI生成深度伪造",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations, ranging from altered facial expressions to object substitutions and background modifications, blend seamlessly with real elements, making them particularly deceptive and difficult to detect. To address the critical gap in detection capabilities, we present FakePartsBench, the first large-scale benchmark dataset specifically designed to capture the full spectrum of partial deepfakes. Comprising over 25K videos with pixel-level and frame-level manipulation annotations, our dataset enables comprehensive evaluation of detection methods. Our user studies demonstrate that FakeParts reduces human detection accuracy by over 30% compared to traditional deepfakes, with similar performance degradation observed in state-of-the-art detection models. This work identifies an urgent vulnerability in current deepfake detection approaches and provides the necessary resources to develop more robust methods for partial video manipulations."
    },
    {
      "title": "CogVLA：通过指令驱动路由与稀疏化实现的认知对齐视觉-语言-动作模型 (原标题: CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification)",
      "link": "https://arxiv.org/abs/2508.21046",
      "pubDate": "Thu, 28 Aug 2025 13:50:58 GMT",
      "isoDate": "2025-08-28T13:50:58.000Z",
      "creator": "Wei Li, Renshan Zhang, Rui Shao, Jie He, Liqiang Nie",
      "summary": "## CogVLA：通过指令驱动路由与稀疏化实现的认知对齐视觉-语言-动作模型\n\n### 背景与问题\n\n当前基于预训练视觉-语言模型（VLM）构建的视觉-语言-动作（VLA）模型面临着显著的挑战。它们通常需要大量的后训练，这导致计算开销高昂，从而限制了模型的可扩展性和效率。\n\n### CogVLA框架介绍\n\n本文提出了一种名为CogVLA的认知对齐视觉-语言-动作框架。CogVLA旨在通过利用指令驱动的路由和稀疏化机制，显著提升VLA模型的效率和性能。该框架从人类多模态协调中汲取灵感，并引入了一个独特的三阶段渐进式架构。\n\n### 三阶段渐进式架构\n\n1.  **编码器-FiLM聚合路由 (EFA-Routing)**\n    *   **目的：** 将指令信息有效地注入视觉编码器。\n    *   **机制：** 通过选择性地聚合和压缩双流视觉令牌，形成一个指令感知的紧凑潜在表示。这一步骤确保了视觉信息在早期阶段就与指令相关联。\n\n2.  **LLM-FiLM剪枝路由 (LFP-Routing)**\n    *   **目的：** 在语言模型中引入动作意图，并实现令牌级别的稀疏性。\n    *   **机制：** 基于EFA-Routing生成的紧凑视觉编码，LFP-Routing通过剪枝与指令无关的视觉接地令牌，精简了语言模型的输入，从而提高了处理效率。\n\n3.  **V-L-A耦合注意力 (CAtten)**\n    *   **目的：** 确保即使在感知输入被压缩的情况下，模型仍能支持准确和连贯的动作生成。\n    *   **机制：** CAtten结合了因果视觉-语言注意力与双向动作并行解码，以实现视觉、语言和动作之间的紧密耦合和协调。\n\n### 实验结果与性能\n\nCogVLA在LIBERO基准测试和真实世界机器人任务中进行了广泛的实验验证，并取得了最先进的性能：\n\n*   **成功率：** 在LIBERO基准测试中达到97.4%，在真实世界机器人任务中达到70.0%。\n*   **效率提升：** 与OpenVLA相比，CogVLA的训练成本降低了2.5倍，推理延迟减少了2.8倍。\n\n### 开源可用性\n\nCogVLA已开源并公开可用，方便研究社区进一步探索和应用。",
      "shortSummary": "CogVLA是一个认知对齐的视觉-语言-动作（VLA）模型，旨在解决现有VLA模型计算开销大的问题。它通过指令驱动的路由和稀疏化，以及一个三阶段渐进式架构（包括EFA-Routing、LFP-Routing和V-L-A耦合注意力）来提高效率和性能。实验证明，CogVLA在机器人任务上实现了最先进的性能，并显著降低了训练成本和推理延迟，同时已开源。",
      "translated_title": "CogVLA：通过指令驱动路由与稀疏化实现的认知对齐视觉-语言-动作模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent Vision-Language-Action (VLA) models built on pre-trained Vision-Language Models (VLMs) require extensive post-training, resulting in high computational overhead that limits scalability and deployment.We propose CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages instruction-driven routing and sparsification to improve both efficiency and performance. CogVLA draws inspiration from human multimodal coordination and introduces a 3-stage progressive architecture. 1) Encoder-FiLM based Aggregation Routing (EFA-Routing) injects instruction information into the vision encoder to selectively aggregate and compress dual-stream visual tokens, forming a instruction-aware latent representation. 2) Building upon this compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing) introduces action intent into the language model by pruning instruction-irrelevant visually grounded tokens, thereby achieving token-level sparsity. 3) To ensure that compressed perception inputs can still support accurate and coherent action generation, we introduce V-L-A Coupled Attention (CAtten), which combines causal vision-language attention with bidirectional action parallel decoding. Extensive experiments on the LIBERO benchmark and real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art performance with success rates of 97.4% and 70.0%, respectively, while reducing training costs by 2.5-fold and decreasing inference latency by 2.8-fold compared to OpenVLA. CogVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/CogVLA."
    },
    {
      "title": "扭转咒语：通过秩一安全注入实现轻量级对齐放大 (原标题: Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection)",
      "link": "https://arxiv.org/abs/2508.20766",
      "pubDate": "Thu, 28 Aug 2025 09:22:33 GMT",
      "isoDate": "2025-08-28T09:22:33.000Z",
      "creator": "Harethah Abu Shairah, Hasan Abed Al Kader Hammoud, George Turkiyyah, Bernard Ghanem",
      "summary": "# 详细摘要：秩一安全注入（ROSI）——轻量级对齐放大方法\n\n## 1. 研究背景与问题\n*   大型语言模型（LLMs）的安全对齐旨在通过调节内部表示来拒绝有害请求。\n*   现有研究发现，这些安全机制可能被绕过，例如通过消除或移除模型内特定的表示方向。\n\n## 2. 提出的方法：秩一安全注入（ROSI）\n*   本文提出了一种名为“秩一安全注入”（Rank-One Safety Injection, ROSI）的白盒方法，旨在解决上述问题。\n*   ROSI采取与绕过方法相反的策略：它通过永久性地将模型的激活引导至负责拒绝有害请求的子空间，从而主动放大模型的安全对齐。\n\n## 3. ROSI的工作原理\n*   ROSI是一种简单且无需微调的秩一权重修改技术。\n*   它被应用于模型中所有残差流的写入矩阵。\n*   所需的安全方向可以通过分析一小组有害和无害指令对来计算得出。\n\n## 4. 实验结果与性能\n*   **安全性提升：** 实验结果表明，ROSI能够持续提高模型的安全拒绝率，这一效果通过Llama Guard 3进行评估。\n*   **实用性保持：** 在MMLU、HellaSwag和Arc等标准基准测试中，ROSI在提高安全性的同时，成功地保持了模型的实用性。\n*   **模型重新对齐能力：** ROSI还展示了其能够重新对齐“未审查”模型的能力，通过放大这些模型自身潜在的安全方向来实现。这表明ROSI可以作为一种有效的“最后一英里”安全处理程序。\n\n## 5. 结论与研究意义\n*   研究结果强调，有针对性且可解释的权重引导是一种经济高效且强大的机制，能够显著提升LLM的安全性。\n*   ROSI方法为更资源密集型的微调范式提供了一个有益的补充。",
      "shortSummary": "本文提出秩一安全注入（ROSI），一种轻量级白盒方法，通过对LLM残差流写入矩阵进行秩一权重修改，永久性地将模型激活引导至安全拒绝子空间。ROSI无需微调，能显著提高模型安全拒绝率（经Llama Guard 3评估），同时保持在MMLU等基准测试上的实用性。它还能重新对齐“未审查”模型。ROSI提供了一种廉价且有效的LLM安全增强机制，补充了传统的微调方法。",
      "translated_title": "扭转咒语：通过秩一安全注入实现轻量级对齐放大",
      "images": [],
      "contentSource": "完整文章",
      "content": "Safety alignment in Large Language Models (LLMs) often involves mediating internal representations to refuse harmful requests. Recent research has demonstrated that these safety mechanisms can be bypassed by ablating or removing specific representational directions within the model. In this paper, we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box method that amplifies a model's safety alignment by permanently steering its activations toward the refusal-mediating subspace. ROSI operates as a simple, fine-tuning-free rank-one weight modification applied to all residual stream write matrices. The required safety direction can be computed from a small set of harmful and harmless instruction pairs. We show that ROSI consistently increases safety refusal rates - as evaluated by Llama Guard 3 - while preserving the utility of the model on standard benchmarks such as MMLU, HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align 'uncensored' models by amplifying their own latent safety directions, demonstrating its utility as an effective last-mile safety procedure. Our results suggest that targeted, interpretable weight steering is a cheap and potent mechanism to improve LLM safety, complementing more resource-intensive fine-tuning paradigms."
    },
    {
      "title": "大型语言模型工具内学习的可证明优势 (原标题: Provable Benefits of In-Tool Learning for Large Language Models)",
      "link": "https://arxiv.org/abs/2508.20755",
      "pubDate": "Thu, 28 Aug 2025 09:12:19 GMT",
      "isoDate": "2025-08-28T09:12:19.000Z",
      "creator": "Sam Houliston, Ambroise Odonnat, Charles Arnal, Vivien Cabannes",
      "summary": "### 大型语言模型工具内学习的可证明优势\n\n本文深入探讨了工具增强型大型语言模型（LLMs）的理论优势，特别是将工具内学习（in-tool learning）与权重内学习（in-weight learning）在事实召回方面的能力进行了对比。\n\n#### 核心问题与研究目标\n\n*   **背景：** 检索、记忆或外部API等工具增强型语言模型正在重塑人工智能领域，但其理论优势尚未得到充分探索。\n*   **目标：** 本研究旨在通过证明工具内学习（即外部检索）相对于权重内学习（即模型内部记忆）在事实召回方面的优势，来解决这一关键问题。\n\n#### 主要发现与理论证明\n\n1.  **权重内学习的局限性：**\n    *   研究表明，模型仅通过其权重记忆事实的数量，从根本上受到其参数数量的限制。这意味着传统上依赖内部记忆的LLMs，其事实召回能力存在固有的上限。\n2.  **工具内学习的无限潜力：**\n    *   与此形成鲜明对比的是，研究通过一个简单高效的电路构建，证明了工具使用能够实现无限制的事实召回。这表明工具增强型模型在处理和访问大量事实信息方面具有显著的、理论上无限的扩展性。\n\n#### 实验验证与实践指导\n\n*   **实验结果：** 在受控实验中，使用工具的模型始终优于仅依赖内部记忆的模型。这些实验结果为理论发现提供了有力的经验支持。\n*   **实践指导：** 对于预训练的大型语言模型，教授其工具使用和通用规则，比通过微调将事实直接写入模型记忆中更为有效。这为LLM的训练和应用提供了重要的策略建议。\n\n#### 结论\n\n*   本研究提供了坚实的理论和经验基础，明确指出工具增强型工作流不仅在实践中具有高效性，而且在可扩展性方面具有可证明的优势。",
      "shortSummary": "本文证明了大型语言模型中工具内学习（外部检索）相对于权重内学习（内部记忆）在事实召回方面的可证明优势。研究指出，模型通过权重记忆事实的能力受限于其参数数量，而工具使用能实现无限制的事实召回。实验验证了工具使用模型的优越性，并表明教授工具使用和通用规则比微调事实更有效。这确立了工具增强型工作流的实用性和可扩展性。",
      "translated_title": "大型语言模型工具内学习的可证明优势",
      "images": [],
      "contentSource": "完整文章",
      "content": "Tool-augmented language models, equipped with retrieval, memory, or external APIs, are reshaping AI, yet their theoretical advantages remain underexplored. In this paper, we address this question by demonstrating the benefits of in-tool learning (external retrieval) over in-weight learning (memorization) for factual recall. We show that the number of facts a model can memorize solely in its weights is fundamentally limited by its parameter count. In contrast, we prove that tool-use enables unbounded factual recall via a simple and efficient circuit construction. These results are validated in controlled experiments, where tool-using models consistently outperform memorizing ones. We further show that for pretrained large language models, teaching tool-use and general rules is more effective than finetuning facts into memory. Our work provides both a theoretical and empirical foundation, establishing why tool-augmented workflows are not just practical, but provably more scalable."
    },
    {
      "title": "Pref-GRPO：基于成对偏好奖励的GRPO，用于稳定的文本到图像强化学习 (原标题: Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2508.20751",
      "pubDate": "Thu, 28 Aug 2025 09:11:24 GMT",
      "isoDate": "2025-08-28T09:11:24.000Z",
      "creator": "Yibin Wang, Zhimin Li, Yuhang Zang, Yujie Zhou, Jiazi Bu, Chunyu Wang, Qinglin Lu, Cheng Jin, Jiaqi Wang",
      "summary": "## Pref-GRPO：解决文本到图像强化学习中的奖励欺骗问题\n\n### 1. 背景与问题\n\n*   **GRPO在T2I生成中的重要性**：最近的研究强调了基于GRPO的强化学习方法在增强文本到图像（T2I）生成方面的关键作用。\n*   **现有方法的局限性**：当前方法依赖于点式奖励模型（RM）来评估生成的图像，这使得它们容易受到“奖励欺骗”（reward hacking）的影响。\n*   **奖励欺骗的机制**：当图像之间微小的分数差异在归一化后被放大时，就会发生奖励欺骗。这会产生虚假的优势，驱使模型过度优化微不足道的收益，最终导致图像生成过程不稳定。\n\n### 2. 提出的解决方案：Pref-GRPO\n\n*   **核心思想**：为了解决奖励欺骗问题，文章提出了Pref-GRPO，一种基于成对偏好奖励的GRPO方法。\n*   **优化目标转变**：Pref-GRPO将优化目标从分数最大化转变为偏好拟合，从而确保更稳定的训练。\n*   **奖励信号机制**：\n    *   在每个组内，图像通过偏好奖励模型（RM）进行成对比较。\n    *   “胜率”被用作奖励信号。\n*   **Pref-GRPO的优势**：\n    *   能够区分细微的图像质量差异。\n    *   提供更稳定的优势信号。\n    *   有效缓解奖励欺骗问题。\n\n### 3. 新型统一T2I基准：UniGenBench\n\n*   **现有基准的不足**：现有的T2I基准受限于粗糙的评估标准，阻碍了对模型进行全面的评估。\n*   **UniGenBench的引入**：为了解决这一问题，文章引入了一个统一的T2I基准——UniGenBench。\n*   **UniGenBench的构成**：\n    *   包含600个提示，涵盖5个主要主题和20个子主题。\n    *   通过10个主要标准和27个子标准来评估语义一致性。\n*   **构建与评估方法**：UniGenBench的构建和评估利用了多模态大语言模型（MLLM）。\n*   **UniGenBench的价值**：\n    *   揭示了开源和闭源T2I模型的优势和劣势。\n    *   验证了Pref-GRPO方法的有效性。",
      "shortSummary": "Pref-GRPO针对文本到图像（T2I）强化学习中点式奖励模型导致的奖励欺骗问题，提出了一种基于成对偏好奖励的GRPO方法。它通过成对比较图像并使用胜率作为奖励信号，将优化目标从分数最大化转向偏好拟合，从而实现更稳定的训练并缓解奖励欺骗。此外，文章还引入了UniGenBench，一个包含600个提示和多维度评估标准的统一T2I基准，用于全面评估T2I模型并验证Pref-GRPO的有效性。",
      "translated_title": "Pref-GRPO：基于成对偏好奖励的GRPO，用于稳定的文本到图像强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements highlight the importance of GRPO-based reinforcement learning methods and benchmarking in enhancing text-to-image (T2I) generation. However, current methods using pointwise reward models (RM) for scoring generated images are susceptible to reward hacking. We reveal that this happens when minimal score differences between images are amplified after normalization, creating illusory advantages that drive the model to over-optimize for trivial gains, ultimately destabilizing the image generation process. To address this, we propose Pref-GRPO, a pairwise preference reward-based GRPO method that shifts the optimization objective from score maximization to preference fitting, ensuring more stable training. In Pref-GRPO, images are pairwise compared within each group using preference RM, and the win rate is used as the reward signal. Extensive experiments demonstrate that PREF-GRPO differentiates subtle image quality differences, providing more stable advantages and mitigating reward hacking. Additionally, existing T2I benchmarks are limited by coarse evaluation criteria, hindering comprehensive model assessment. To solve this, we introduce UniGenBench, a unified T2I benchmark comprising 600 prompts across 5 main themes and 20 subthemes. It evaluates semantic consistency through 10 primary and 27 sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our benchmarks uncover the strengths and weaknesses of both open and closed-source T2I models and validate the effectiveness of Pref-GRPO."
    },
    {
      "title": "rStar2-Agent：智能体推理技术报告 (原标题: rStar2-Agent: Agentic Reasoning Technical Report)",
      "link": "https://arxiv.org/abs/2508.20722",
      "pubDate": "Thu, 28 Aug 2025 08:45:25 GMT",
      "isoDate": "2025-08-28T08:45:25.000Z",
      "creator": "Ning Shang, Yifei Liu, Yi Zhu, Li Lyna Zhang, Weijiang Xu, Xinyu Guan, Buze Zhang, Bingcheng Dong, Xudong Zhou, Bowen Zhang, Ying Xin, Ziming Miao, Scarlett Li, Fan Yang, Mao Yang",
      "summary": "# rStar2-Agent：智能体推理技术报告\n\n本文介绍了rStar2-Agent，一个通过智能体强化学习（agentic reinforcement learning）训练的14B数学推理模型，旨在实现前沿水平的性能。该模型超越了当前的长CoT（Chain-of-Thought）方法，展现出先进的认知行为，例如：\n\n*   在使用Python编码工具前进行仔细思考。\n*   反思代码执行反馈，以自主探索、验证和完善复杂问题解决中的中间步骤。\n\n这些能力得益于三项关键创新，它们使得大规模智能体强化学习变得高效：\n\n1.  **高效的强化学习基础设施**：\n    *   提供可靠的Python代码环境，支持高吞吐量执行。\n    *   有效降低了高昂的rollout成本，使得模型能够在有限的GPU资源（64块MI300X GPU）上进行训练。\n2.  **GRPO-RoC智能体强化学习算法**：\n    *   采用“纠正时重采样”（Resample-on-Correct）的rollout策略。\n    *   解决了编码工具固有的环境噪声问题，使模型能够在代码环境中更有效地进行推理。\n3.  **高效的智能体训练方案**：\n    *   从非推理的SFT（Supervised Fine-Tuning）开始。\n    *   逐步通过多阶段强化学习，以最小的计算成本获得了先进的认知能力。\n\n## 性能表现：\n\n*   rStar2-Agent在短短一周内，通过510个强化学习步骤，将一个预训练的14B模型提升至最先进水平。\n*   在AIME24测试中，平均pass@1得分达到80.6%。\n*   在AIME25测试中，平均pass@1得分达到69.8%。\n*   以显著更短的响应长度，超越了DeepSeek-R1（671B）模型。\n\n## 泛化能力：\n\n*   除了数学领域，rStar2-Agent-14B还在对齐、科学推理和智能体工具使用任务中展现出强大的泛化能力。\n\n## 资源可用性：\n\n*   模型的代码和训练方案已公开。",
      "shortSummary": "rStar2-Agent是一个14B数学推理模型，通过智能体强化学习实现了前沿性能。它展现了在Python编码前仔细思考、并根据反馈自主探索和完善问题解决步骤的先进认知能力。该模型通过高效RL基础设施、GRPO-RoC算法和优化训练方案，在一周内达到SOTA，在AIME24和AIME25上分别取得80.6%和69.8%的pass@1分数，超越了更大的DeepSeek-R1模型，并具有强大的泛化能力。",
      "translated_title": "rStar2-Agent：智能体推理技术报告",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. Beyond current long CoT, the model demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations that makes agentic RL effective at scale: (i) an efficient RL infrastructure with a reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with a Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools, allowing the model to reason more effectively in a code environment; (iii) An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. To this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at https://github.com/microsoft/rStar."
    },
    {
      "title": "MCP-Bench：通过MCP服务器基准测试使用工具的LLM智能体处理复杂真实世界任务的能力 (原标题: MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers)",
      "link": "https://arxiv.org/abs/2508.20453",
      "pubDate": "Thu, 28 Aug 2025 01:58:57 GMT",
      "isoDate": "2025-08-28T01:58:57.000Z",
      "creator": "Zhenting Wang, Qi Chang, Hemani Patel, Shashank Biju, Cheng-En Wu, Quan Liu, Aolin Ding, Alireza Rezazadeh, Ankit Shah, Yujia Bao, Eugene Siow",
      "summary": "## MCP-Bench：评估使用工具的LLM智能体的新基准\n\n### 引言\n本文介绍了MCP-Bench，这是一个旨在评估大型语言模型（LLMs）在真实世界、多步骤任务中表现的新型基准测试。这些任务对LLMs提出了更高的要求，包括工具使用、跨工具协调、精确的参数控制以及复杂的规划和推理能力。\n\n### MCP-Bench的构建与范围\n*   **基于协议：** MCP-Bench建立在模型上下文协议（Model Context Protocol, MCP）之上。\n*   **连接服务器：** 它将LLMs连接到28个具有代表性的实时MCP服务器。\n*   **工具数量与领域：** 这些服务器共涵盖250种工具，跨越多个领域，如金融、旅行、科学计算和学术搜索。\n\n### 与现有基准的区别与优势\nMCP-Bench与以往基于API的基准测试存在显著差异，并提供了独特的优势：\n*   **互补工具集：** 每个MCP服务器提供一套设计为协同工作的互补工具，而非孤立的API。这使得能够构建具有丰富输入-输出耦合的真实多步骤任务。\n*   **模糊指令下的工具检索：** 任务测试智能体在没有明确工具名称的模糊指令下，识别并检索相关工具的能力。\n*   **复杂目标的多跳执行规划：** 评估智能体为实现复杂目标规划多步（多跳）执行轨迹的能力。\n*   **基于中间输出的响应：** 智能体需要能够将最终响应或后续操作基于中间工具的输出进行接地和调整。\n*   **跨领域工作流编排：** 评估智能体在不同领域之间编排和协调复杂工作流的能力。\n*   **弥补现有不足：** MCP-Bench解决了现有基准的局限性，这些基准通常依赖于明确的工具规范、浅层少数步骤的工作流和孤立的领域操作，无法充分评估LLMs在复杂真实场景下的能力。\n\n### 评估框架\n作者提出了一个多方面的评估框架，用于全面衡量LLM智能体的性能，该框架涵盖以下关键方面：\n*   **工具层面的模式理解和使用：** 评估智能体理解工具的输入/输出模式并正确使用的能力。\n*   **轨迹层面的规划：** 评估智能体规划整个任务执行路径的有效性。\n*   **任务完成度：** 衡量智能体成功完成给定任务的程度。\n\n### 实验结果\n对20个先进的LLMs进行的初步实验表明，即使是当前最先进的模型，在MCP-Bench上仍然面临持续的挑战，这突显了该领域未来研究的必要性。\n\n### 代码与数据\n相关代码和数据已公开，可通过提供的链接获取。",
      "shortSummary": "MCP-Bench是一个新基准，用于评估LLM智能体处理复杂真实世界任务的工具使用能力。它通过28个实时MCP服务器连接250种工具，涵盖金融、旅行等多个领域。该基准测试LLM在模糊指令下检索工具、规划多跳执行、基于中间输出响应及跨领域工作流编排的能力，弥补了现有基准的不足。对20个先进LLM的实验显示，它们在MCP-Bench上仍面临持续挑战，揭示了当前LLM在复杂工具使用方面的局限性。",
      "translated_title": "MCP-Bench：通过MCP服务器基准测试使用工具的LLM智能体处理复杂真实世界任务的能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce MCP-Bench, a benchmark for evaluating large language models (LLMs) on realistic, multi-step tasks that demand tool use, cross-tool coordination, precise parameter control, and planning/reasoning for solving tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28 representative live MCP servers spanning 250 tools across domains such as finance, traveling, scientific computing, and academic search. Unlike prior API-based benchmarks, each MCP server provides a set of complementary tools designed to work together, enabling the construction of authentic, multi-step tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability to retrieve relevant tools from fuzzy instructions without explicit tool names, plan multi-hop execution trajectories for complex objectives, ground responses in intermediate tool outputs, and orchestrate cross-domain workflows - capabilities not adequately evaluated by existing benchmarks that rely on explicit tool specifications, shallow few-step workflows, and isolated domain operations. We propose a multi-faceted evaluation framework covering tool-level schema understanding and usage, trajectory-level planning, and task completion. Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code and data: https://github.com/Accenture/mcp-bench."
    },
    {
      "title": "AWorld：为智能体AI编排训练方案 (原标题: AWorld: Orchestrating the Training Recipe for Agentic AI)",
      "link": "https://arxiv.org/abs/2508.20404",
      "pubDate": "Thu, 28 Aug 2025 00:04:30 GMT",
      "isoDate": "2025-08-28T00:04:30.000Z",
      "creator": "Chengyue Yu, Siyuan Lu, Chenyi Zhuang, Dong Wang, Qintong Wu, Zongyue Li, Runsheng Gan, Chunfeng Wang, Siqi Hou, Gaochi Huang, Wenlong Yan, Lifeng Hong, Aohui Xue, Yanfeng Wang, Jinjie Gu, David Tsai, Tao Lin",
      "summary": "## AWorld：为智能体AI编排训练方案\n\n### 背景与挑战\n\n*   **核心问题：** 学习实践范式对于开发强大的智能体AI系统至关重要，但其效率受到经验生成低效的严重阻碍。\n*   **具体瓶颈：** 在GAIA等复杂基准测试中，这一瓶颈尤为突出，限制了广泛强化学习的实践。\n\n### AWorld系统介绍\n\n*   **解决方案：** 研究人员引入了AWorld，这是一个开源系统，专为大规模智能体-环境交互而设计。\n*   **工作原理：** AWorld通过在计算集群中分发任务来加速经验收集过程。\n*   **效率提升：** 相比标准的单节点顺序执行，AWorld将经验收集速度提高了14.6倍。\n*   **重要意义：** 这种显著的速度提升使得大规模强化学习变得实用且可扩展，从而能够进行更深入、更广泛的模型训练。\n\n### 实验结果与性能提升\n\n*   **模型训练：** 利用AWorld的高效能力，研究人员成功训练了一个基于Qwen3-32B的智能体。\n*   **GAIA基准表现：** 该智能体显著超越了其基础模型，将整体GAIA准确率从21.59%提升至32.23%。\n*   **高难度任务表现：** 在GAIA基准测试中最具挑战性的级别上，该智能体取得了16.33%的得分，这一表现超越了领先的专有模型。\n\n### 结论与展望\n\n*   **实践蓝图：** AWorld的开源系统及其训练出的高性能智能体，为构建完整的智能体AI训练流程提供了一个实用的蓝图，涵盖了从高效交互到显著模型改进的全过程。",
      "shortSummary": "AWorld是一个开源系统，旨在解决智能体AI在复杂环境中经验生成效率低下的问题。通过在集群中分发任务，AWorld将经验收集速度提高了14.6倍，使大规模强化学习成为可能。利用AWorld，研究人员训练了一个基于Qwen3-32B的智能体，其GAIA准确率从21.59%提升至32.23%，并在最难级别上超越了领先的专有模型。AWorld为智能体AI的完整训练流程提供了一个实用蓝图。",
      "translated_title": "AWorld：为智能体AI编排训练方案",
      "images": [],
      "contentSource": "完整文章",
      "content": "The learning from practice paradigm is crucial for developing capable Agentic AI systems, yet it is severely hampered by inefficient experience generation, a bottleneck especially pronounced in complex benchmarks like GAIA. To address this, we introduce AWorld, an open-source system engineered for large-scale agent-environment interaction. By distributing tasks across a cluster, AWorld accelerates experience collection by 14.6x compared to standard single-node, sequential execution. This critical speedup makes extensive reinforcement learning practical and scalable. Leveraging this capability, we trained a Qwen3-32B-based agent that significantly outperforms its base model, increasing its overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most challenging levels, our agent achieves a score of 16.33%, surpassing the performance of leading proprietary models. Our open-source system and resulting agent provide a practical blueprint for a complete agentic AI training pipeline, from efficient interaction to demonstrable model improvement."
    },
    {
      "title": "TCIA：一种用于指令微调的以任务为中心的指令增强方法 (原标题: TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning)",
      "link": "https://arxiv.org/abs/2508.20374",
      "pubDate": "Wed, 27 Aug 2025 22:42:10 GMT",
      "isoDate": "2025-08-27T22:42:10.000Z",
      "creator": "Simin Ma, Shujian Liu, Jun Tan, Yebowen Hu, Song Wang, Sathish Reddy Indurthi, Sanqiang Zhao, Liwei Wu, Jianbing Han, Kaiqiang Song",
      "summary": "## TCIA：一种用于指令微调的以任务为中心的指令增强方法\n\n### 引言与问题背景\n*   **指令数据的重要性**：多样化的指令数据对于大型语言模型（LLMs）的指令微调至关重要，它能增强模型在不同输入类型上的泛化能力。\n*   **现有方法的局限性**：\n    *   现有方法通常利用LLMs自动生成多样化指令，以确保数据多样性和质量。\n    *   然而，这些方法往往忽视了实际应用中的一个关键因素：**任务相关性（on-task relevance）**。\n    *   在实践中，大多数真实世界的应用更需要针对特定用例定制的任务特定知识，而非通用的模型。\n\n### TCIA框架的提出\n*   **解决方案**：本文提出了一种名为**任务中心指令增强（Task Centric Instruction Augmentation, TCIA）**的框架。\n*   **TCIA的目标**：TCIA旨在系统地扩展指令，同时保持**多样性**和**任务对齐性**。\n\n### TCIA的工作原理\n*   TCIA通过在**离散查询-约束空间（discrete query-constraints space）**中表示指令，来创建一组丰富的、与任务相关的指令。\n*   这种方法使得模型能够在不牺牲整体性能的情况下，泛化到这些任务特定的指令。\n\n### 实验结果与性能提升\n*   **显著改进**：实验表明，TCIA使开源LLMs在四个真实世界的、任务特定的应用中，平均性能提升了**8.7%**。\n*   **超越闭源模型**：在某些情况下，TCIA甚至超越了领先的闭源模型。\n*   **无损通用能力**：这些改进并未损害模型遵循通用指令的能力。\n\n### 结论\n*   TCIA为将LLMs应用于真实世界的、以任务为中心的应用提供了一个可扩展且高效的解决方案。",
      "shortSummary": "针对现有指令增强方法忽视实际应用中任务相关性的问题，本文提出了TCIA（任务中心指令增强）框架。TCIA通过在离散查询-约束空间中表示指令，系统地扩展指令，同时确保多样性和任务对齐。实验证明，TCIA使开源LLMs在四个真实任务中平均性能提升8.7%，有时甚至超越闭源模型，且不损害通用指令遵循能力。TCIA为LLMs在实际任务中的应用提供了可扩展且高效的解决方案。",
      "translated_title": "TCIA：一种用于指令微调的以任务为中心的指令增强方法",
      "images": [],
      "contentSource": "完整文章",
      "content": "Diverse instruction data is vital for effective instruction tuning of large language models, as it enables the model to generalize across different types of inputs . Building such diversified instruction dataset is an essential step in this process. Existing approaches often leverage large language models to automatically explore and generate diverse instructions, ensuring both data diversity and quality. However, they tend to overlook an important factor in real-world applications: on-task relevance. In practice, only a few real-world applications require a truly general-purpose model; most benefit from task-specific knowledge tailored to their particular use case. Therefore, it is vital to develop instruction augmentation methods that not only maintain diversity but are also optimized for specific, real-world scenarios.   We thus introduce Task Centric Instruction Augmentation (TCIA), a framework that systematically expands instructions while preserving both diversity and task alignment. By representing instructions in a discrete query-constraints space, TCIA creates a rich set of task-relevant instructions and enables models to generalize to these task-specific instructions without sacrificing overall performance. Experiments show that TCIA improves open-source LLMs' performance by an average of 8.7% across four real-world, task-specific applications, and in some cases outperforming leading closed-source models. These improvements do not compromise general instruction-following ability, making TCIA a scalable and efficient solution for adapting LLMs to real-world, task-focused applications."
    },
    {
      "title": "CODA：协调大脑皮层和小脑，用于采用解耦强化学习的双脑计算机使用代理 (原标题: CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2508.20096",
      "pubDate": "Wed, 27 Aug 2025 13:59:50 GMT",
      "isoDate": "2025-08-27T13:59:50.000Z",
      "creator": "Zeyi Sun, Yuhang Cao, Jianze Liang, Qiushi Sun, Ziyu Liu, Zhixiong Zhang, Yuhang Zang, Xiaoyi Dong, Kai Chen, Dahua Lin, Jiaqi Wang",
      "summary": "## CODA：用于GUI代理的“双脑”解耦强化学习框架\n\n### 引言与挑战\n\n自主图形用户界面（GUI）代理在科学计算等专业领域面临显著挑战。这些领域要求代理同时具备长周期规划能力和精确的执行能力。当前方法存在以下局限：\n\n*   **通用代理**：擅长规划，但在具体执行上表现不佳。\n*   **专业代理**：执行能力强，但在规划方面表现较弱。\n*   **现有组合框架**：尽管尝试结合规划器和执行器，但通常是静态且不可训练的，无法从经验中学习和适应。鉴于科学领域高质量数据稀缺，这是一个关键限制。\n\n### CODA框架介绍\n\n为解决上述问题，研究人员提出了 **CODA**，一个新颖且可训练的组合框架。CODA 的核心思想是整合一个通用规划器（被称为“大脑皮层”，**Cerebrum**）和一个专业执行器（被称为“小脑”，**Cerebellum**）。\n\n### CODA的训练流程：两阶段管道\n\nCODA 的训练通过一个专门的两阶段管道进行，旨在使其同时具备鲁棒的执行能力和跨领域泛化能力。\n\n1.  **第一阶段：专业化 (Specialization)**\n    *   **目标**：为每个特定的科学应用单独训练一个专家规划器。\n    *   **方法**：应用一种解耦的广义强化学习策略优化（GRPO）方法。\n    *   **数据利用**：从少量任务轨迹中进行自举（bootstrapping），以克服数据稀缺问题。\n\n2.  **第二阶段：泛化 (Generalization)**\n    *   **目标**：构建一个能够跨领域泛化的最终规划器。\n    *   **数据聚合**：将所有来自第一阶段专业化专家训练成功的轨迹聚合起来，构建一个整合数据集。\n    *   **训练方法**：使用该整合数据集对最终规划器进行监督微调。\n\n### 实验评估与成果\n\nCODA 在 ScienceBoard 基准测试中的四个具有挑战性的应用上进行了评估。结果显示：\n\n*   CODA 显著优于现有基线方法。\n*   在开源模型中，CODA 建立了新的最先进水平。\n\n### 代码可用性\n\nCODA 的代码已在指定 URL 提供，方便研究社区进一步探索和使用。",
      "shortSummary": "CODA是一种新颖的可训练组合框架，旨在解决GUI代理在科学计算中长周期规划和精确执行的挑战。它整合了通用规划器（Cerebrum）和专业执行器（Cerebellum），并采用两阶段解耦强化学习训练。第一阶段实现专业化，为各应用训练专家规划器；第二阶段通过聚合成功轨迹进行监督微调，实现跨领域泛化。CODA在ScienceBoard基准测试中表现出色，显著超越现有基线，达到开源模型的最新水平，兼具鲁棒执行和泛化能力。",
      "translated_title": "CODA：协调大脑皮层和小脑，用于采用解耦强化学习的双脑计算机使用代理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Autonomous agents for Graphical User Interfaces (GUIs) face significant challenges in specialized domains such as scientific computing, where both long-horizon planning and precise execution are required. Existing approaches suffer from a trade-off: generalist agents excel at planning but perform poorly in execution, while specialized agents demonstrate the opposite weakness. Recent compositional frameworks attempt to bridge this gap by combining a planner and an actor, but they are typically static and non-trainable, which prevents adaptation from experience. This is a critical limitation given the scarcity of high-quality data in scientific domains. To address these limitations, we introduce CODA, a novel and trainable compositional framework that integrates a generalist planner (Cerebrum) with a specialist executor (Cerebellum), trained via a dedicated two-stage pipeline. In the first stage, Specialization, we apply a decoupled GRPO approach to train an expert planner for each scientific application individually, bootstrapping from a small set of task trajectories. In the second stage, Generalization, we aggregate all successful trajectories from the specialized experts to build a consolidated dataset, which is then used for supervised fine-tuning of the final planner. This equips CODA with both robust execution and cross-domain generalization. Evaluated on four challenging applications from the ScienceBoard benchmark, CODA significantly outperforms baselines and establishes a new state of the art among open-source models."
    },
    {
      "title": "AudioStory：使用大型语言模型生成长篇叙事音频 (原标题: AudioStory: Generating Long-Form Narrative Audio with Large Language Models)",
      "link": "https://arxiv.org/abs/2508.20088",
      "pubDate": "Wed, 27 Aug 2025 13:55:38 GMT",
      "isoDate": "2025-08-27T13:55:38.000Z",
      "creator": "Yuxin Guo, Teng Wang, Yuying Ge, Shijie Ma, Yixiao Ge, Wei Zou, Ying Shan",
      "summary": "# AudioStory：使用大型语言模型生成长篇叙事音频\n\n## 挑战与解决方案\n\n*   **现有问题**：当前文本到音频（TTA）生成技术在合成短音频片段方面表现出色，但在生成长篇叙事音频时面临挑战，主要体现在难以保持时间连贯性和进行组合推理。\n*   **AudioStory的提出**：为解决这一问题，研究者提出了AudioStory，一个统一的框架，它将大型语言模型（LLMs）与TTA系统相结合，旨在生成结构化的长篇音频叙事。\n\n## AudioStory的核心能力与特点\n\n*   **指令遵循与推理**：AudioStory具备强大的指令遵循推理生成能力。它利用LLMs将复杂的叙事查询分解为按时间顺序排列的子任务，并提供上下文线索，从而实现连贯的场景过渡和情感语调的一致性。\n*   **两大吸引人的特性**：\n    1.  **解耦的桥接机制**：AudioStory将LLM与扩散模型的协作解耦为两个专门组件：\n        *   **桥接查询（bridging query）**：用于事件内部的语义对齐。\n        *   **残差查询（residual query）**：用于保持跨事件的连贯性。\n    2.  **端到端训练**：通过在一个单一的端到端框架内统一指令理解和音频生成，AudioStory消除了对模块化训练管道的需求，同时增强了组件之间的协同作用。\n\n## 评估与成果\n\n*   **基准数据集**：研究者建立了AudioStory-10K基准，涵盖了动画音景和自然声音叙事等多样化领域。\n*   **实验结果**：广泛的实验表明，AudioStory在单音频生成和叙事音频生成方面均表现出卓越性能，在指令遵循能力和音频保真度方面均超越了先前的TTA基线。\n*   **代码可用性**：相关代码已公开。",
      "shortSummary": "AudioStory是一个统一框架，旨在解决现有文本到音频（TTA）系统在生成长篇叙事音频时面临的时间连贯性和组合推理挑战。它通过整合大型语言模型（LLMs）与TTA系统，将复杂叙事分解为有序子任务，确保场景过渡和情感一致性。AudioStory采用解耦桥接机制和端到端训练，显著提升了指令遵循能力和音频保真度，在长篇音频生成方面超越了现有基线。代码已开源。",
      "translated_title": "AudioStory：使用大型语言模型生成长篇叙事音频",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in text-to-audio (TTA) generation excel at synthesizing short audio clips but struggle with long-form narrative audio, which requires temporal coherence and compositional reasoning. To address this gap, we propose AudioStory, a unified framework that integrates large language models (LLMs) with TTA systems to generate structured, long-form audio narratives. AudioStory possesses strong instruction-following reasoning generation capabilities. It employs LLMs to decompose complex narrative queries into temporally ordered sub-tasks with contextual cues, enabling coherent scene transitions and emotional tone consistency. AudioStory has two appealing features: (1) Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser collaboration into two specialized components, i.e., a bridging query for intra-event semantic alignment and a residual query for cross-event coherence preservation. (2) End-to-end training: By unifying instruction comprehension and audio generation within a single end-to-end framework, AudioStory eliminates the need for modular training pipelines while enhancing synergy between components. Furthermore, we establish a benchmark AudioStory-10K, encompassing diverse domains such as animated soundscapes and natural sound narratives. Extensive experiments show the superiority of AudioStory on both single-audio generation and narrative audio generation, surpassing prior TTA baselines in both instruction-following ability and audio fidelity. Our code is available at https://github.com/TencentARC/AudioStory"
    },
    {
      "title": "离散扩散VLA：将离散扩散引入视觉-语言-动作策略中的动作解码 (原标题: Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies)",
      "link": "https://arxiv.org/abs/2508.20072",
      "pubDate": "Wed, 27 Aug 2025 13:39:11 GMT",
      "isoDate": "2025-08-27T13:39:11.000Z",
      "creator": "Zhixuan Liang, Yizhuo Li, Tianshuo Yang, Chengyue Wu, Sitong Mao, Liuao Pei, Xiaokang Yang, Jiangmiao Pang, Yao Mu, Ping Luo",
      "summary": "# 离散扩散VLA：将离散扩散引入视觉-语言-动作策略中的动作解码\n\n## 引言与背景\n视觉-语言-动作（VLA）模型旨在利用大型视觉-语言骨干网络，将图像和指令映射到机器人动作。然而，当前主流的VLA解码器存在以下局限性：\n*   **自回归解码器**：通常以固定的从左到右顺序生成动作，效率低下。\n*   **连续扩散或流匹配解码器**：需要将专门的头部附加到骨干网络之外，这要求专门的训练和迭代采样，阻碍了统一且可扩展的架构的实现。\n\n## 提出的方法：离散扩散VLA\n本文提出了一种名为“离散扩散VLA”（Discrete Diffusion VLA）的新型策略。它是一个**单一的Transformer策略**，通过**离散扩散**对离散化的动作块进行建模。\n\n### 核心设计与特点\n1.  **统一的训练目标**：离散扩散VLA与VLM骨干网络采用相同的**交叉熵目标**进行训练，实现了训练范式的统一。\n2.  **原生兼容性**：该设计保留了扩散模型的**渐进式细化范式**，同时与VLM的离散令牌接口实现原生兼容。\n3.  **自适应解码顺序**：该方法能够实现**自适应的解码顺序**，优先解决较简单的动作元素，然后再处理较难的元素。\n4.  **二次重掩码（Secondary Remasking）**：通过在细化轮次中对不确定的预测进行**二次重掩码**，该方法能够提高一致性并实现鲁棒的错误校正。\n\n### 优势\n离散扩散VLA的统一解码器带来了多项显著优势：\n*   **保留预训练的视觉语言先验**：有效利用了VLM骨干网络的强大能力。\n*   **支持并行解码**：打破了传统自回归模型的瓶颈，提高了效率。\n*   **减少函数评估次数**：进一步优化了计算资源的使用。\n\n## 实验结果\n离散扩散VLA在多个基准测试中取得了优异的性能，超越了现有的自回归和连续扩散基线：\n*   **LIBERO**：平均成功率（SR）达到 **96.3%**。\n*   **SimplerEnv Fractal**：视觉匹配率达到 **71.2%**。\n*   **SimplerEnv Bridge**：总体性能达到 **49.3%**。\n\n## 结论与未来展望\n这些研究结果表明，离散扩散动作解码器支持**精确的动作建模**和**一致的训练**。这为将VLA模型扩展到更大的模型和数据集奠定了基础。",
      "shortSummary": "离散扩散VLA提出了一种统一的Transformer策略，将离散扩散引入视觉-语言-动作（VLA）策略中的动作解码。它解决了现有VLA解码器（自回归或连续扩散）效率低、训练复杂的问题。该方法采用与VLM骨干网络相同的交叉熵目标训练，支持自适应解码顺序和二次重掩码进行错误校正。离散扩散VLA实现了并行解码，保留了预训练先验，并在LIBERO和SimplerEnv等基准测试中取得了显著优于现有方法的性能，为VLA模型的扩展奠定了基础。",
      "translated_title": "离散扩散VLA：将离散扩散引入视觉-语言-动作策略中的动作解码",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions to robot actions. However, prevailing VLA decoders either generate actions autoregressively in a fixed left-to-right order or attach continuous diffusion or flow matching heads outside the backbone, demanding specialized training and iterative sampling that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a single-transformer policy that models discretized action chunks with discrete diffusion and is trained with the same cross-entropy objective as the VLM backbone. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary remasking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pretrained vision language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv Bridge, improving over both autoregressive and continuous diffusion baselines. These findings indicate that discrete-diffusion action decoder supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets."
    },
    {
      "title": "DeepScholar-Bench：一个用于生成式研究综合的实时基准和自动化评估框架 (原标题: DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis)",
      "link": "https://arxiv.org/abs/2508.20033",
      "pubDate": "Wed, 27 Aug 2025 12:36:34 GMT",
      "isoDate": "2025-08-27T12:36:34.000Z",
      "creator": "Liana Patel, Negar Arabzadeh, Harshit Gupta, Ankita Sundar, Ion Stoica, Matei Zaharia, Carlos Guestrin",
      "summary": "## DeepScholar-Bench：生成式研究综合的实时基准与自动化评估\n\n### 背景与挑战\n\n研究和综合知识的能力是人类专业知识和进步的核心。新兴的生成式研究综合系统承诺通过检索实时网络并综合发现的来源，生成长篇、带引用的摘要，从而实现这些激动人心的能力。然而，评估此类系统仍然是一个开放的挑战：\n\n*   **现有基准的局限性**：当前的问答基准侧重于简短的事实性回答。\n*   **数据过时与污染**：专家策划的数据集存在过时和数据污染的风险。\n*   **缺乏复杂性**：上述方法都未能捕捉真实研究综合任务的复杂性和演变性。\n\n### DeepScholar-Bench 的引入\n\n为了解决这些挑战，本研究引入了 **DeepScholar-Bench**，这是一个实时基准和全面的自动化评估框架，专门设计用于评估生成式研究综合系统。\n\n### DeepScholar-Bench 的核心特点与方法\n\n1.  **查询来源**：DeepScholar-Bench 从近期高质量的 ArXiv 论文中提取查询。\n2.  **真实任务聚焦**：它专注于一个真实的研究综合任务——通过检索、综合和引用现有研究来生成论文的“相关工作”部分。\n3.  **全面评估维度**：该评估框架从三个关键维度全面评估系统性能：\n    *   **知识综合**：评估系统整合和提炼信息的能力。\n    *   **检索质量**：评估系统查找相关和高质量信息的能力。\n    *   **可验证性**：评估生成内容的事实准确性和来源可追溯性。\n\n### DeepScholar-Base 参考管道\n\n研究团队还开发了 **DeepScholar-Base**，这是一个使用 LOTUS API 高效实现的参考管道，作为评估的基准系统。\n\n### 系统评估与发现\n\n利用 DeepScholar-Bench 框架，研究人员对以下系统进行了系统评估：\n\n*   现有的开源系统\n*   搜索 AI\n*   OpenAI 的 DeepResearch\n*   DeepScholar-Base\n\n评估结果显示：\n\n*   **DeepScholar-Base 表现强劲**：DeepScholar-Base 建立了一个强大的基线，其性能与所有其他方法相比具有竞争力或更高。\n*   **基准尚未饱和**：没有系统在所有指标上的得分超过 19%。\n\n### 结论与意义\n\n这些结果凸显了 DeepScholar-Bench 的难度，以及其对于推动能够进行生成式研究综合的 AI 系统发展的重要性。研究团队已将代码开源。",
      "shortSummary": "DeepScholar-Bench是一个新的实时基准和自动化评估框架，旨在解决生成式研究综合系统评估的挑战。它通过从ArXiv论文中提取查询，并专注于生成论文的“相关工作”部分，来评估知识综合、检索质量和可验证性。初步评估显示，DeepScholar-Base建立了一个强大的基线，但所有系统性能均未超过19%，这突显了该基准的难度及其对AI系统进展的重要性。代码已开源。",
      "translated_title": "DeepScholar-Bench：一个用于生成式研究综合的实时基准和自动化评估框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "The ability to research and synthesize knowledge is central to human expertise and progress. An emerging class of systems promises these exciting capabilities through generative research synthesis, performing retrieval over the live web and synthesizing discovered sources into long-form, cited summaries. However, evaluating such systems remains an open challenge: existing question-answering benchmarks focus on short-form factual responses, while expert-curated datasets risk staleness and data contamination. Both fail to capture the complexity and evolving nature of real research synthesis tasks. In this work, we introduce DeepScholar-bench, a live benchmark and holistic, automated evaluation framework designed to evaluate generative research synthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv papers and focuses on a real research synthesis task: generating the related work sections of a paper by retrieving, synthesizing, and citing prior research. Our evaluation framework holistically assesses performance across three key dimensions, knowledge synthesis, retrieval quality, and verifiability. We also develop DeepScholar-base, a reference pipeline implemented efficiently using the LOTUS API. Using the DeepScholar-bench framework, we perform a systematic evaluation of prior open-source systems, search AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that DeepScholar-base establishes a strong baseline, attaining competitive or higher performance than each other method. We also find that DeepScholar-bench remains far from saturated, with no system exceeding a score of 19% across all metrics. These results underscore the difficulty of DeepScholar-bench, as well as its importance for progress towards AI systems capable of generative research synthesis. We make our code available at https://github.com/guestrin-lab/deepscholar-bench."
    },
    {
      "title": "扩散语言模型在解码前已知答案 (原标题: Diffusion Language Models Know the Answer Before Decoding)",
      "link": "https://arxiv.org/abs/2508.19982",
      "pubDate": "Wed, 27 Aug 2025 11:40:25 GMT",
      "isoDate": "2025-08-27T11:40:25.000Z",
      "creator": "Pengxiang Li, Yefan Zhou, Dilxat Muhtar, Lu Yin, Shilin Yan, Li Shen, Yi Liang, Soroush Vosoughi, Shiwei Liu",
      "summary": "## 扩散语言模型：解码前已知答案\n\n### 引言\n\n扩散语言模型（DLMs）作为自回归方法的替代方案，近年来崭露头角，其优势在于支持并行序列生成和灵活的词元顺序。然而，DLMs的推理速度通常慢于自回归模型，这主要是因为双向注意力机制的计算成本以及为获得高质量输出所需的大量精炼步骤。\n\n### 核心发现：早期答案收敛\n\n本研究强调并利用了DLMs一个常被忽视的特性——**早期答案收敛**。研究发现，在许多情况下，DLMs在最终解码步骤之前，仅通过一半的精炼步骤，就能在内部识别出正确的答案。这一现象在半自回归和随机重掩码调度下均成立。\n\n*   **实例表现**：\n    *   在GSM8K数据集上，高达97%的实例仅用一半的精炼步骤即可正确解码。\n    *   在MMLU数据集上，高达99%的实例仅用一半的精炼步骤即可正确解码。\n\n### Prophet：一种无需训练的快速解码范式\n\n基于上述观察，研究人员引入了**Prophet**，这是一种无需额外训练的快速解码范式，旨在利用早期答案收敛特性实现“提前提交解码”（early commit decoding）。\n\n*   **工作原理**：\n    *   Prophet动态决定是继续精炼还是“全力以赴”（即在一个步骤中解码所有剩余词元）。\n    *   其决策标准是前两个预测候选之间的置信度差距。\n*   **优势**：\n    *   无缝集成到现有DLM实现中。\n    *   引入的开销可忽略不计。\n    *   无需额外的训练。\n\n### 实验评估与结果\n\n研究人员在多个任务上对LLaDA-8B和Dream-7B模型进行了实证评估，结果表明：\n\n*   Prophet将解码步骤数量减少了高达3.4倍。\n*   同时，它保持了高质量的生成效果。\n\n### 结论与展望\n\n这些结果重新定义了DLM解码问题，将其视为一个“何时停止采样”的问题。研究表明，早期解码收敛为加速DLM推理提供了一种简单而强大的机制，并且可以与现有加速技术互补。该研究的代码已公开提供。",
      "shortSummary": "扩散语言模型（DLMs）虽能并行生成，但推理速度受限于大量精炼步骤。本研究发现DLMs在最终解码前已能识别正确答案，即存在“早期答案收敛”现象。基于此，提出无训练的Prophet范式，它通过动态评估预测置信度，决定何时提前完成解码。实验表明，Prophet可将DLM解码步数减少高达3.4倍，同时保持高质量输出。这为加速DLM推理提供了一种简单而强大的机制，将解码问题转化为何时停止采样。",
      "translated_title": "扩散语言模型在解码前已知答案",
      "images": [],
      "contentSource": "完整文章",
      "content": "Diffusion language models (DLMs) have recently emerged as an alternative to autoregressive approaches, offering parallel sequence generation and flexible token orders. However, their inference remains slower than that of autoregressive models, primarily due to the cost of bidirectional attention and the large number of refinement steps required for high quality outputs. In this work, we highlight and leverage an overlooked property of DLMs early answer convergence: in many cases, the correct answer can be internally identified by half steps before the final decoding step, both under semi-autoregressive and random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99% of instances, respectively, can be decoded correctly using only half of the refinement steps. Building on this observation, we introduce Prophet, a training-free fast decoding paradigm that enables early commit decoding. Specifically, Prophet dynamically decides whether to continue refinement or to go \"all-in\" (i.e., decode all remaining tokens in one step), using the confidence gap between the top-2 prediction candidates as the criterion. It integrates seamlessly into existing DLM implementations, incurs negligible overhead, and requires no additional training. Empirical evaluations of LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the number of decoding steps by up to 3.4x while preserving high generation quality. These results recast DLM decoding as a problem of when to stop sampling, and demonstrate that early decode convergence provides a simple yet powerful mechanism for accelerating DLM inference, complementary to existing speedup techniques. Our code is publicly available at https://github.com/pixeli99/Prophet."
    },
    {
      "title": "通过推理分解实现自奖励视觉-语言模型 (原标题: Self-Rewarding Vision-Language Model via Reasoning Decomposition)",
      "link": "https://arxiv.org/abs/2508.19652",
      "pubDate": "Wed, 27 Aug 2025 04:01:03 GMT",
      "isoDate": "2025-08-27T04:01:03.000Z",
      "creator": "Zongxia Li, Wenhao Yu, Chengsong Huang, Rui Liu, Zhenwen Liang, Fuxiao Liu, Jingxi Che, Dian Yu, Jordan Boyd-Graber, Haitao Mi, Dong Yu",
      "summary": "## 通过推理分解实现自奖励视觉-语言模型 (Vision-SR1)\n\n### 1. 视觉-语言模型 (VLM) 面临的问题\n\n*   **视觉幻觉 (Visual Hallucinations)**：VLM 描述图像中不存在的事物。\n*   **语言捷径 (Language Shortcuts)**：VLM 跳过视觉部分，仅依赖文本先验进行推理。\n*   **根本原因**：大多数 VLM 后训练方法仅监督最终输出，导致中间视觉推理缺乏明确指导。VLM 接收到的视觉信号稀疏，并倾向于优先考虑基于语言的推理。\n\n### 2. 现有解决方案及其局限性\n\n*   **方法**：通过人工标注或从外部大型模型蒸馏标签来增加视觉监督。\n*   **局限性**：\n    *   **人工标注**：劳动密集且成本高昂。\n    *   **外部信号**：无法适应不断变化的策略，可能导致分布偏移和奖励欺骗。\n\n### 3. Vision-SR1 方法介绍\n\n*   **核心思想**：Vision-SR1 是一种自奖励方法，通过强化学习在不依赖外部视觉监督的情况下改进视觉推理。\n*   **目标**：解决 VLM 的视觉幻觉和语言捷径问题。\n\n### 4. Vision-SR1 的推理分解过程\n\nVision-SR1 将 VLM 的推理过程分解为两个阶段：\n\n1.  **视觉感知阶段**：\n    *   模型被提示生成“自包含的视觉感知”。\n    *   这些感知必须足够充分，以便在不再次参考原始输入图像的情况下回答问题。\n2.  **语言推理阶段**：\n    *   相同的 VLM 模型随后被重新提示。\n    *   它仅使用第一阶段生成的视觉感知作为输入进行语言推理，并计算一个“自奖励”。\n\n### 5. 训练信号\n\n*   Vision-SR1 将这种“自奖励”与对最终输出的监督相结合。\n*   这提供了一个平衡的训练信号，能够同时增强模型的视觉感知能力和语言推理能力。\n\n### 6. 实验结果\n\n*   实验证明，Vision-SR1 在各种视觉-语言任务中显著改进了视觉推理能力。\n*   它有效缓解了视觉幻觉问题。\n*   成功减少了 VLM 对语言捷径的依赖。",
      "shortSummary": "视觉-语言模型（VLMs）常受视觉幻觉和语言捷径困扰，因其视觉信号稀疏且缺乏中间推理指导。为解决此问题，本文提出 Vision-SR1，一种自奖励方法。它通过强化学习，将VLM推理分解为视觉感知和语言推理两阶段。模型首先生成自包含的视觉感知，然后利用这些感知进行语言推理并计算自奖励。结合最终输出监督，Vision-SR1有效提升了视觉推理能力，缓解了视觉幻觉，并减少了对语言捷径的依赖，无需外部视觉监督。",
      "translated_title": "通过推理分解实现自奖励视觉-语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-Language Models (VLMs) often suffer from visual hallucinations, saying things that are not actually in the image, and language shortcuts, where they skip the visual part and just rely on text priors. These issues arise because most post-training methods for VLMs rely on simple verifiable answer matching and supervise only final outputs, leaving intermediate visual reasoning without explicit guidance. As a result, VLMs receive sparse visual signals and often learn to prioritize language-based reasoning over visual perception. To mitigate this, some existing methods add visual supervision using human annotations or distilled labels from external large models. However, human annotations are labor-intensive and costly, and because external signals cannot adapt to the evolving policy, they cause distributional shifts that can lead to reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method that improves visual reasoning without relying on external visual supervisions via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two stages: visual perception and language reasoning. The model is first prompted to produce self-contained visual perceptions that are sufficient to answer the question without referring back the input image. To validate this self-containment, the same VLM model is then re-prompted to perform language reasoning using only the generated perception as input to compute reward. This self-reward is combined with supervision on final outputs, providing a balanced training signal that strengthens both visual perception and language reasoning. Our experiments demonstrate that Vision-SR1 improves visual reasoning, mitigates visual hallucinations, and reduces reliance on language shortcuts across diverse vision-language tasks."
    }
  ],
  "lastUpdated": "2025-08-30T09:25:14.555Z"
}