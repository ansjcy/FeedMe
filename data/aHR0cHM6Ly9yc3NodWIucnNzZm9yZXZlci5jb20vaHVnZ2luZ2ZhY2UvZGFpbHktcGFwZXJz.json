{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "LazyDrag：通过显式对应在多模态扩散Transformer上实现稳定的拖拽式编辑 (原标题: LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence)",
      "link": "https://arxiv.org/abs/2509.12203",
      "pubDate": "Mon, 15 Sep 2025 13:59:47 GMT",
      "isoDate": "2025-09-15T13:59:47.000Z",
      "creator": "Zixin Yin, Xili Dai, Duomin Wang, Xianfang Zeng, Lionel M. Ni, Gang Yu, Heung-Yeung Shum",
      "summary": "# LazyDrag：通过显式对应实现稳定的拖拽式编辑\n\n## 背景与现有问题\n传统的拖拽式图像编辑方法严重依赖通过注意力机制进行的隐式点匹配。这种依赖性导致了两个核心瓶颈：\n*   **反演强度弱：** 难以实现高保真度的图像反演。\n*   **测试时优化（TTO）成本高昂：** 需要耗费大量计算资源进行优化。\n这些问题共同限制了扩散模型的生成能力，尤其是在高保真度图像修复和文本引导创作等复杂任务中表现不佳。\n\n## LazyDrag 方法介绍\n本文引入了 **LazyDrag**，这是首个专为多模态扩散Transformer设计的拖拽式图像编辑方法。LazyDrag 的核心创新在于它直接消除了对隐式点匹配的依赖。\n\n### 核心机制\nLazyDrag 的工作原理是：\n*   从用户提供的拖拽输入中生成一个 **显式对应图**。\n*   这个显式对应图作为可靠的参考，用于增强模型的注意力控制。\n\n## LazyDrag 的优势与能力\n通过引入显式对应图，LazyDrag 带来了多项显著优势：\n\n### 1. 稳定的全强度反演\n*   LazyDrag 首次在拖拽式编辑任务中实现了 **稳定的全强度反演过程**。这意味着模型能够更准确、更完整地理解和应用编辑指令。\n\n### 2. 消除测试时优化 (TTO)\n*   由于其高效且可靠的机制，LazyDrag **避免了对 TTO 的需求**。这不仅降低了计算成本，还显著提升了编辑效率，并释放了模型的全部生成潜力。\n\n### 3. 统一几何控制与文本引导\n*   LazyDrag 自然地将 **精确的几何控制** 与 **文本引导** 功能相结合。这使得用户可以通过拖拽和文本描述共同完成复杂的编辑任务。\n\n### 4. 实现前所未有的复杂编辑\nLazyDrag 能够完成以前难以实现或无法完成的复杂编辑，例如：\n*   **精细结构编辑：** 打开狗的嘴巴并修复其内部。\n*   **新对象生成：** 生成新的物体，如“网球”。\n*   **上下文感知修改：** 对于模糊的拖拽指令，能够进行上下文感知的改变，例如将手移入口袋。\n\n### 5. 支持多轮工作流\n*   LazyDrag 支持 **多轮编辑工作流**，并能同时进行移动和缩放操作，为用户提供更大的灵活性和控制力。\n\n## 性能评估\n*   在 DragBench 基准测试中，LazyDrag 在 **拖拽精度** 和 **感知质量** 方面均超越了现有基线方法。\n*   其卓越性能得到了 VIEScore 和人工评估的验证。\n\n## 结论与影响\nLazyDrag 不仅在拖拽式图像编辑领域确立了 **新的最先进性能 (State-of-the-Art)**，而且通过其创新的显式对应机制，为未来的编辑范式开辟了新的途径。",
      "shortSummary": "LazyDrag是首个针对多模态扩散Transformer的拖拽式图像编辑方法。它通过从用户输入生成显式对应图，解决了传统方法中隐式点匹配导致的反演弱和测试时优化（TTO）成本高昂的问题。LazyDrag实现了稳定的全强度反演，无需TTO，并自然地统一了精确几何控制与文本引导。它能完成复杂的图像编辑，并在DragBench上表现优异，确立了新的最先进性能，为编辑范式开辟了新途径。",
      "translated_title": "LazyDrag：通过显式对应在多模态扩散Transformer上实现稳定的拖拽式编辑",
      "images": [],
      "contentSource": "完整文章",
      "content": "The reliance on implicit point matching via attention has become a core bottleneck in drag-based editing, resulting in a fundamental compromise on weakened inversion strength and costly test-time optimization (TTO). This compromise severely limits the generative capabilities of diffusion models, suppressing high-fidelity inpainting and text-guided creation. In this paper, we introduce LazyDrag, the first drag-based image editing method for Multi-Modal Diffusion Transformers, which directly eliminates the reliance on implicit point matching. In concrete terms, our method generates an explicit correspondence map from user drag inputs as a reliable reference to boost the attention control. This reliable reference opens the potential for a stable full-strength inversion process, which is the first in the drag-based editing task. It obviates the necessity for TTO and unlocks the generative capability of models. Therefore, LazyDrag naturally unifies precise geometric control with text guidance, enabling complex edits that were previously out of reach: opening the mouth of a dog and inpainting its interior, generating new objects like a ``tennis ball'', or for ambiguous drags, making context-aware changes like moving a hand into a pocket. Additionally, LazyDrag supports multi-round workflows with simultaneous move and scale operations. Evaluated on the DragBench, our method outperforms baselines in drag accuracy and perceptual quality, as validated by VIEScore and human evaluation. LazyDrag not only establishes new state-of-the-art performance, but also paves a new way to editing paradigms."
    },
    {
      "title": "OmniWorld: 用于4D世界建模的多领域和多模态数据集 (原标题: OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling)",
      "link": "https://arxiv.org/abs/2509.12201",
      "pubDate": "Mon, 15 Sep 2025 13:59:19 GMT",
      "isoDate": "2025-09-15T13:59:19.000Z",
      "creator": "Yang Zhou, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Haoyu Guo, Zizun Li, Kaijing Ma, Xinyue Li, Yating Wang, Haoyi Zhu, Mingyu Liu, Dingning Liu, Jiange Yang, Zhoujie Fu, Junyi Chen, Chunhua Shen, Jiangmiao Pang, Kaipeng Zhang, Tong He",
      "summary": "## OmniWorld: 用于4D世界建模的多领域和多模态数据集\n\n本文介绍了一个名为OmniWorld的大规模、多领域、多模态数据集，专门为4D世界建模而设计。4D世界建模旨在共同捕捉空间几何和时间动态，近年来在大型生成模型和多模态学习的推动下取得了显著进展。然而，真正通用的4D世界模型的开发仍然受到高质量数据可用性的根本限制。\n\n**数据集的动机：**\n\n*   现有的数据集和基准通常缺乏动态复杂性、多领域多样性和空间-时间注释，而这些是支持关键任务（如4D几何重建、未来预测和相机控制视频生成）所必需的。\n\n**OmniWorld数据集的组成：**\n\n*   新收集的OmniWorld-Game数据集\n*   多个精选的公共数据集，涵盖不同的领域。\n\n**OmniWorld-Game数据集的优势：**\n\n*   与现有的合成数据集相比，OmniWorld-Game提供了更丰富的模态覆盖、更大的规模和更逼真的动态交互。\n\n**实验与结果：**\n\n*   基于该数据集，作者建立了一个具有挑战性的基准，揭示了当前最先进（SOTA）方法在建模复杂4D环境方面的局限性。\n*   在OmniWorld上微调现有的SOTA方法可以显著提高4D重建和视频生成任务的性能，这有力地验证了OmniWorld作为训练和评估的强大资源。\n\n**结论：**\n\n作者设想OmniWorld将成为加速通用4D世界模型发展的催化剂，最终促进机器对物理世界的整体理解。",
      "shortSummary": "本文介绍了一个名为OmniWorld的大规模多领域多模态数据集，旨在促进4D世界建模的研究。该数据集由新收集的OmniWorld-Game数据集和多个公共数据集组成，提供了丰富的模态覆盖、更大的规模和更逼真的动态交互。实验表明，OmniWorld能够有效评估和提升现有4D重建和视频生成方法的性能，有望加速通用4D世界模型的发展。",
      "translated_title": "OmniWorld: 用于4D世界建模的多领域和多模态数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "The field of 4D world modeling - aiming to jointly capture spatial geometry and temporal dynamics - has witnessed remarkable progress in recent years, driven by advances in large-scale generative models and multimodal learning. However, the development of truly general 4D world models remains fundamentally constrained by the availability of high-quality data. Existing datasets and benchmarks often lack the dynamic complexity, multi-domain diversity, and spatial-temporal annotations required to support key tasks such as 4D geometric reconstruction, future prediction, and camera-control video generation. To address this gap, we introduce OmniWorld, a large-scale, multi-domain, multi-modal dataset specifically designed for 4D world modeling. OmniWorld consists of a newly collected OmniWorld-Game dataset and several curated public datasets spanning diverse domains. Compared with existing synthetic datasets, OmniWorld-Game provides richer modality coverage, larger scale, and more realistic dynamic interactions. Based on this dataset, we establish a challenging benchmark that exposes the limitations of current state-of-the-art (SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning existing SOTA methods on OmniWorld leads to significant performance gains across 4D reconstruction and video generation tasks, strongly validating OmniWorld as a powerful resource for training and evaluation. We envision OmniWorld as a catalyst for accelerating the development of general-purpose 4D world models, ultimately advancing machines' holistic understanding of the physical world."
    },
    {
      "title": "再看一眼，慢思考：增强视觉语言模型中的视觉反思能力 (原标题: Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models)",
      "link": "https://arxiv.org/abs/2509.12132",
      "pubDate": "Mon, 15 Sep 2025 12:57:25 GMT",
      "isoDate": "2025-09-15T12:57:25.000Z",
      "creator": "Pu Jian, Junhong Wu, Wei Sun, Chen Wang, Shuo Ren, Jiajun Zhang",
      "summary": "本文提出了一种名为 **Reflection-V** 的新型视觉推理模型（VRM），旨在解决当前视觉语言模型（VLM）在“慢思考”推理过程中视觉反思能力有限的问题。\n\n### 挑战\n\n*   **视觉反思能力不足**：当前的VRM在进行“慢思考”推理时，检查基于视觉信息的推理过程的能力有限。\n*   **视觉注意力衰减**：定量分析表明，随着生成响应的长度增加，现有VRM对视觉信息的注意力会迅速减弱。\n\n### 解决方案：Reflection-V\n\nReflection-V 通过以下两种机制增强了视觉反思能力：\n\n1.  **视觉中心推理数据构建（冷启动学习）**：\n    *   利用一个在VLM和推理大型语言模型（LLM）之间进行交互的代理，构建以视觉为中心的推理数据。\n    *   这使得模型能够进行视觉反思模式的冷启动学习。\n\n2.  **基于视觉注意力的强化学习奖励设计**：\n    *   在强化学习（RL）过程中，采用一个基于视觉注意力的奖励模型。\n    *   该奖励模型旨在鼓励模型在推理时更多地依赖视觉信息。\n\n### 成果\n\n*   **显著提升**：Reflection-V 在多个视觉推理基准测试中展现出显著的性能提升。\n*   **持续依赖视觉信息**：该模型在视觉推理过程中对视觉信息保持了更强、更一致的依赖，表明其视觉反思能力得到了有效增强。\n\n### 其他信息\n\n*   **会议**：EMNLP2025\n*   **主要研究领域**：计算机视觉与模式识别（cs.CV），计算与语言（cs.CL）",
      "shortSummary": "本文提出 **Reflection-V**，旨在增强视觉语言模型（VLM）的视觉反思能力。研究发现当前VLM在长响应中对视觉信息的注意力会减弱。Reflection-V通过构建视觉中心推理数据进行冷启动学习，并采用基于视觉注意力的强化学习奖励模型，鼓励模型依赖视觉信息。实验证明，Reflection-V在多个视觉推理基准测试中表现显著提升，并能更持续地依赖视觉信息。",
      "translated_title": "再看一眼，慢思考：增强视觉语言模型中的视觉反思能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in text-only \"slow-thinking\" reasoning have prompted efforts to transfer this capability to vision-language models (VLMs), for training visual reasoning models (VRMs). owever, such transfer faces critical challenges: Effective \"slow thinking\" in VRMs requires visual reflection, the ability to check the reasoning process based on visual information. Through quantitative analysis, we observe that current VRMs exhibit limited visual reflection, as their attention to visual information diminishes rapidly with longer generated responses. To address this challenge, we propose a new VRM Reflection-V, which enhances visual reflection based on reasoning data construction for cold-start and reward design for reinforcement learning (RL). Firstly, we construct vision-centered reasoning data by leveraging an agent that interacts between VLMs and reasoning LLMs, enabling cold-start learning of visual reflection patterns. Secondly, a visual attention based reward model is employed during RL to encourage reasoning based on visual information. Therefore, Reflection-V demonstrates significant improvements across multiple visual reasoning benchmarks. Furthermore, Reflection-V maintains a stronger and more consistent reliance on visual information during visual reasoning, indicating effective enhancement in visual reflection capabilities."
    },
    {
      "title": "嵌入中的迷失：视觉语言模型中的信息丢失 (原标题: Lost in Embeddings: Information Loss in Vision-Language Models)",
      "link": "https://arxiv.org/abs/2509.11986",
      "pubDate": "Mon, 15 Sep 2025 10:38:06 GMT",
      "isoDate": "2025-09-15T10:38:06.000Z",
      "creator": "Wenyan Li, Raphael Tang, Chengzu Li, Caiqi Zhang, Ivan Vulić, Anders Søgaard",
      "summary": "## 嵌入中的迷失：视觉语言模型中的信息丢失\n\n本文研究了视觉语言模型 (VLMs) 中，将视觉输入投影到语言模型嵌入空间过程中潜在的信息损失问题。VLMs 通常使用预训练的视觉编码器处理视觉输入，然后通过连接器组件将其投影到语言模型的嵌入空间中。虽然这种方法对于模态融合至关重要，但投影步骤可能导致的信息损失及其对模型能力的直接影响却鲜有研究。\n\n文章提出两种互补的方法来检查和量化这种信息损失：\n\n1. **语义信息保持评估:** 通过分析图像表示在投影前后k近邻关系的变化来评估语义信息的保持情况。\n2. **信息损失直接测量:** 通过从投影后的表示中重建视觉嵌入，在图像块级别定位信息损失。\n\n实验结果表明，连接器会大幅扭曲视觉表示的局部几何结构，投影后k近邻关系差异高达40%-60%，这与检索性能的下降相关。图像块级别的嵌入重建为视觉基础问答任务的模型行为提供了可解释的见解，发现高信息损失区域可靠地预测了模型难以处理的实例。",
      "shortSummary": "本文研究视觉语言模型中将视觉信息投影到语言模型嵌入空间可能导致的信息损失。研究者通过分析k近邻关系变化和重建视觉嵌入两种方法量化信息损失，发现投影过程会显著扭曲视觉表示的局部几何结构，导致检索性能下降，并影响视觉基础问答任务的模型表现。",
      "translated_title": "嵌入中的迷失：视觉语言模型中的信息丢失",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision--language models (VLMs) often process visual inputs through a pretrained vision encoder, followed by a projection into the language model's embedding space via a connector component. While crucial for modality fusion, the potential information loss induced by this projection step and its direct impact on model capabilities remain understudied. We introduce two complementary approaches to examine and quantify this loss by analyzing the latent representation space. First, we evaluate semantic information preservation by analyzing changes in k-nearest neighbor relationships between image representations, before and after projection. Second, we directly measure information loss by reconstructing visual embeddings from the projected representation, localizing loss at an image patch level. Experiments reveal that connectors substantially distort the local geometry of visual representations, with k-nearest neighbors diverging by 40--60\\% post-projection, correlating with degradation in retrieval performance. The patch-level embedding reconstruction provides interpretable insights for model behavior on visually grounded question-answering tasks, finding that areas of high information loss reliably predict instances where models struggle."
    },
    {
      "title": "EthicsMH：精神健康AI伦理推理的初步基准 (原标题: EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI)",
      "link": "https://arxiv.org/abs/2509.11648",
      "pubDate": "Mon, 15 Sep 2025 03:35:35 GMT",
      "isoDate": "2025-09-15T03:35:35.000Z",
      "creator": "Sai Kartheek Reddy Kasu",
      "summary": "### EthicsMH：精神健康AI伦理推理的初步基准\n\n**引言：AI在精神健康领域的伦理挑战**\n\n随着大型语言模型（LLMs）在精神健康及其他敏感领域的部署，关于伦理推理、公平性和负责任对齐的紧迫问题日益凸显。现有针对道德和临床决策的基准未能充分捕捉精神健康实践中独特的伦理困境，这些困境常涉及保密性、自主性、受益原则和偏见等方面的复杂交叉。\n\n**EthicsMH数据集的提出**\n\n为解决这一空白，研究人员引入了“精神健康伦理推理”（EthicsMH）数据集。这是一个初步的、包含125个情景的数据集，旨在评估AI系统如何在治疗和精神病学背景下处理具有伦理挑战性的情况。\n\n**EthicsMH情景的结构化内容**\n\n每个情景都通过结构化字段进行了丰富，具体包括：\n\n*   **多个决策选项**：提供不同的行动方案供AI选择。\n*   **专家对齐的推理**：包含与专业专家意见一致的推理过程，作为AI决策的参考。\n*   **预期的模型行为**：明确指出在特定伦理情境下，AI系统应如何表现。\n*   **实际世界影响**：评估不同决策可能带来的现实后果。\n*   **多方利益相关者的观点**：考虑患者、家属、治疗师等不同角色的视角和关注点。\n\n**评估范围**\n\n这种结构使得EthicsMH不仅能够评估AI决策的准确性，还能深入评估其解释的质量以及与专业规范的对齐程度。\n\n**开发与未来展望**\n\n尽管EthicsMH目前规模适中，且在开发过程中采用了模型辅助生成，但它成功建立了一个连接AI伦理与精神健康决策的任务框架。通过发布此数据集，研究人员旨在提供一个种子资源，期望能够通过社区和专家贡献进行进一步扩展。最终目标是促进开发能够负责任地处理社会最敏感决策的AI系统，确保AI在精神健康领域的应用是安全、公正和合乎伦理的。",
      "shortSummary": "大型语言模型在精神健康领域的应用面临伦理挑战，现有基准不足。为弥补此空白，研究人员推出了EthicsMH，一个包含125个情景的初步数据集。该数据集旨在评估AI系统在治疗和精神病学背景下的伦理推理能力，涵盖决策选项、专家推理、预期行为和多方观点。EthicsMH旨在连接AI伦理与精神健康决策，提供一个可扩展的资源，以促进开发负责任的AI系统。",
      "translated_title": "EthicsMH：精神健康AI伦理推理的初步基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "The deployment of large language models (LLMs) in mental health and other sensitive domains raises urgent questions about ethical reasoning, fairness, and responsible alignment. Yet, existing benchmarks for moral and clinical decision-making do not adequately capture the unique ethical dilemmas encountered in mental health practice, where confidentiality, autonomy, beneficence, and bias frequently intersect. To address this gap, we introduce Ethical Reasoning in Mental Health (EthicsMH), a pilot dataset of 125 scenarios designed to evaluate how AI systems navigate ethically charged situations in therapeutic and psychiatric contexts. Each scenario is enriched with structured fields, including multiple decision options, expert-aligned reasoning, expected model behavior, real-world impact, and multi-stakeholder viewpoints. This structure enables evaluation not only of decision accuracy but also of explanation quality and alignment with professional norms. Although modest in scale and developed with model-assisted generation, EthicsMH establishes a task framework that bridges AI ethics and mental health decision-making. By releasing this dataset, we aim to provide a seed resource that can be expanded through community and expert contributions, fostering the development of AI systems capable of responsibly handling some of society's most delicate decisions."
    },
    {
      "title": "UI-S1：基于半在线强化学习的GUI自动化改进 (原标题: UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2509.11543",
      "pubDate": "Sun, 14 Sep 2025 23:24:08 GMT",
      "isoDate": "2025-09-14T23:24:08.000Z",
      "creator": "Zhengxi Lu, Jiabo Ye, Fei Tang, Yongliang Shen, Haiyang Xu, Ziwei Zheng, Weiming Lu, Ming Yan, Fei Huang, Jun Xiao, Yueting Zhuang",
      "summary": "## UI-S1：基于半在线强化学习的GUI自动化改进\n\n本文介绍了一种名为UI-S1的新方法，该方法通过半在线强化学习来改进图形用户界面(GUI)自动化。现有的强化学习方法在GUI自动化方面面临着离线强化学习和在线强化学习的困境：离线强化学习虽然训练稳定，但由于缺乏轨迹级别的奖励信号，难以执行多步任务；而在线强化学习虽然能够捕捉到这些信号，但却面临稀疏奖励和高昂的部署成本问题。\n\nUI-S1提出了一种新的半在线强化学习范式，它在离线轨迹上模拟在线强化学习。在每次回滚过程中，该方法保留原始模型在多轮对话中的输出，并使用一个“Patch Module”自适应地恢复回滚轨迹和专家轨迹之间的差异。为了捕捉长期训练信号，UI-S1将贴现的未来回报引入奖励计算，并使用加权的步骤级和情节级优势来优化策略。\n\n此外，UI-S1还引入了一个名为“Semi-Online Performance (SOP)”的指标，该指标更符合真实的在线性能，可以作为现实世界评估的有效代理。实验结果表明，在四个动态基准测试中，UI-S1在7B模型中实现了最先进的性能，与基线模型相比取得了显著的提升（例如，在AndroidWorld上提升了12.0%，在AITW上提升了23.8%）。这表明UI-S1在弥合离线训练效率和在线多轮推理之间的差距方面取得了显著进展。代码已公开发布。",
      "shortSummary": "UI-S1 提出了一种新的半在线强化学习方法，用于改进 GUI 自动化。该方法结合了离线强化学习的稳定性和在线强化学习的反馈能力，在多个基准测试中取得了最先进的性能，显著提升了多步任务执行能力，并降低了部署成本。",
      "translated_title": "UI-S1：基于半在线强化学习的GUI自动化改进",
      "images": [],
      "contentSource": "完整文章",
      "content": "Graphical User Interface (GUI) agents have demonstrated remarkable progress in automating complex user interface interactions through reinforcement learning. However, current approaches face a fundamental dilemma: offline RL enables stable training on pre-collected trajectories, but struggles with multi-step task execution for lack of trajectory-level reward signals; online RL captures these signals through environment interaction, but suffers from sparse rewards and prohibitive deployment costs. To address it, we present Semi-online Reinforcement Learning, a novel paradigm that simulates online RL on offline trajectories. During each rollout process, we preserve the original model output within the multi-turn dialogue, where a Patch Module adaptively recovers the divergence between rollout and expert trajectories. To capture long-term training signals, Semi-online RL introduces discounted future returns into the reward computation and optimizes the policy with weighted step-level and episode-level advantages. We further introduce Semi-Online Performance (SOP), a metric that aligns better with true online performance, serving as a practical and effective proxy for real-world evaluation. Experiments show that ours Semi-online RL achieves SOTA performance among 7B models across four dynamic benchmarks, with significant gains over the base model (e.g., +12.0% on AndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging the gap between offline training efficiency and online multi-turn reasoning. The code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1."
    },
    {
      "title": "通过动态奖励权重学习优化多目标对齐 (原标题: Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting)",
      "link": "https://arxiv.org/abs/2509.11452",
      "pubDate": "Sun, 14 Sep 2025 17:56:35 GMT",
      "isoDate": "2025-09-14T17:56:35.000Z",
      "creator": "Yining Lu, Zilong Wang, Shiyang Li, Xin Liu, Changlong Yu, Qingyu Yin, Zhan Shi, Zixuan Zhang, Meng Jiang",
      "summary": "## 通过动态奖励权重学习优化多目标对齐：详细摘要\n\n本文探讨了多目标强化学习中使用固定权重线性奖励标量化方法的局限性，该方法无法有效捕捉非凸帕累托前沿，从而导致次优结果。这种局限性在大型语言模型的在线偏好对齐中尤为关键，因为参数化策略生成的随机轨迹会在参数与目标之间创建高度非线性且非凸的映射，任何单一的静态加权方案都无法找到最佳权衡。\n\n为了解决这个问题，本文引入了**动态奖励权重**方法，该方法在在线强化学习过程中自适应地调整奖励权重。与依赖于固定权重插值的现有方法不同，动态加权在训练过程中持续平衡和优先考虑目标，从而促进有效探索目标空间中的帕累托前沿。\n\n文章提出了两种日益复杂和通用的方法：\n\n1. **超体积引导的权重自适应:**  该方法利用超体积指标来指导奖励权重的调整，从而更有效地探索帕累托前沿。\n2. **基于梯度的权重优化:**  该方法利用梯度信息来优化奖励权重，具有更高的效率和泛化能力。\n\n实验结果表明，这两种方法与常用的在线强化学习算法（包括GRPO、REINFORCE和RLOO）兼容，在多个数学推理数据集上均有效，并适用于不同的模型族，与固定权重线性标量化基线相比，能够在更少的训练步骤中获得帕累托最优解。\n\n总而言之，本文提出了一种新颖的动态奖励权重方法，有效地解决了多目标强化学习中固定权重方法的局限性，为在线多目标对齐提供了一种有效的工具。",
      "shortSummary": "本文提出了一种通过动态调整奖励权重来优化多目标强化学习的方法。该方法克服了传统固定权重线性奖励标量化方法的局限性，能够更有效地探索帕累托最优解。文章提出了两种动态权重调整方法：超体积引导的权重自适应和基于梯度的权重优化，并在多个数据集上验证了其有效性，显著减少了训练步骤。",
      "translated_title": "通过动态奖励权重学习优化多目标对齐",
      "images": [],
      "contentSource": "完整文章",
      "content": "Prior works in multi-objective reinforcement learning typically use linear reward scalarization with fixed weights, which provably fail to capture non-convex Pareto fronts and thus yield suboptimal results. This limitation becomes especially critical in online preference alignment for large language models. Here, stochastic trajectories generated by parameterized policies create highly non-linear and non-convex mappings from parameters to objectives that no single static weighting scheme can find optimal trade-offs. We address this limitation by introducing dynamic reward weighting, which adaptively adjusts reward weights during the online reinforcement learning process. Unlike existing approaches that rely on fixed-weight interpolation, our dynamic weighting continuously balances and prioritizes objectives in training, facilitating effective exploration of Pareto fronts in objective space. We introduce two approaches of increasing sophistication and generalizability: (1) hypervolume-guided weight adaptation and (2) gradient-based weight optimization, offering a versatile toolkit for online multi-objective alignment. Our extensive experiments demonstrate their compatibility with commonly used online reinforcement learning algorithms (including GRPO, REINFORCE, and RLOO), effectiveness across multiple mathematical reasoning datasets, and applicability to different model families, consistently achieving Pareto dominant solutions with fewer training steps than fixed-weight linear scalarization baselines."
    },
    {
      "title": "CognitiveSky：去中心化社交媒体的可扩展情感和叙事分析 (原标题: CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media)",
      "link": "https://arxiv.org/abs/2509.11444",
      "pubDate": "Sun, 14 Sep 2025 17:37:24 GMT",
      "isoDate": "2025-09-14T17:37:24.000Z",
      "creator": "Gaurab Chhetri, Anandi Dutta, Subasish Das",
      "summary": "### CognitiveSky：去中心化社交媒体的可扩展情感和叙事分析框架\n\n本文介绍了 **CognitiveSky**，一个开源且可扩展的框架，专为去中心化社交媒体平台（如 Bluesky）设计，用于进行情感、情绪和叙事分析。\n\n**核心功能与目标：**\n*   **实时分析公共话语：** 旨在应对去中心化社交媒体平台在实时分析公共话语方面带来的新机遇和挑战。\n*   **多维度分析：** 提供对用户生成内容的情感、情绪和叙事进行深入分析的能力。\n\n**工作原理：**\n*   **数据摄取：** 通过 Bluesky 的应用程序编程接口（API）获取数据。\n*   **模型应用：** 利用基于 Transformer 的模型对大规模用户生成内容进行标注。\n*   **输出生成：** 产生结构化且可分析的输出。\n\n**可视化与洞察：**\n*   **动态仪表板：** 分析结果驱动一个动态仪表板，可视化情感、活动和对话主题的演变模式。\n\n**关键特性：**\n*   **开源与可扩展：** 作为一个开源项目，具有良好的可扩展性，能够处理大量数据。\n*   **低运营成本与高可访问性：** 完全基于免费层基础设施构建，实现了低运营成本和高可访问性。\n*   **模块化设计：** 其模块化设计使其能够应用于多个领域。\n\n**应用领域：**\n*   **当前演示：** 在本文中，CognitiveSky 被用于监测心理健康话语。\n*   **潜在应用：** 其应用范围广泛，包括虚假信息检测、危机响应和公民情绪分析等。\n\n**重要意义：**\n*   **连接大语言模型与去中心化网络：** CognitiveSky 弥合了大语言模型与去中心化网络之间的鸿沟。\n*   **透明且可扩展的工具：** 在数字生态系统不断变化的时代，它为计算社会科学提供了一个透明、可扩展的工具。\n\n**背景信息：**\n*   本文是作者的预印本版本，已被 HICSS 59（夏威夷国际系统科学会议，2026 年）接受发表。",
      "shortSummary": "CognitiveSky 是一个开源、可扩展的框架，专为去中心化社交媒体（如 Bluesky）设计，用于情感、情绪和叙事分析。它通过 Bluesky API 摄取数据，利用 Transformer 模型进行标注，并通过动态仪表板可视化用户话语模式。该框架基于免费层基础设施，成本低廉，模块化设计使其适用于心理健康监测、虚假信息检测和危机响应等多种应用，为计算社会科学提供了一个透明且可扩展的工具。",
      "translated_title": "CognitiveSky：去中心化社交媒体的可扩展情感和叙事分析",
      "images": [],
      "contentSource": "完整文章",
      "content": "The emergence of decentralized social media platforms presents new opportunities and challenges for real-time analysis of public discourse. This study introduces CognitiveSky, an open-source and scalable framework designed for sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter or X.com alternative. By ingesting data through Bluesky's Application Programming Interface (API), CognitiveSky applies transformer-based models to annotate large-scale user-generated content and produces structured and analyzable outputs. These summaries drive a dynamic dashboard that visualizes evolving patterns in emotion, activity, and conversation topics. Built entirely on free-tier infrastructure, CognitiveSky achieves both low operational cost and high accessibility. While demonstrated here for monitoring mental health discourse, its modular design enables applications across domains such as disinformation detection, crisis response, and civic sentiment analysis. By bridging large language models with decentralized networks, CognitiveSky offers a transparent, extensible tool for computational social science in an era of shifting digital ecosystems."
    },
    {
      "title": "PersonaX：基于LLM推断行为特征的多模态数据集 (原标题: PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits)",
      "link": "https://arxiv.org/abs/2509.11362",
      "pubDate": "Sun, 14 Sep 2025 13:30:03 GMT",
      "isoDate": "2025-09-14T13:30:03.000Z",
      "creator": "Loka Li, Wong Yu Kang, Minghao Fu, Guangyi Chen, Zhenhao Chen, Gongxu Luo, Yuewen Sun, Salman Khan, Peter Spirtes, Kun Zhang",
      "summary": "## PersonaX：基于LLM推断行为特征的多模态数据集\n\n本文介绍了PersonaX，一个旨在弥补现有数据集在结合行为描述符与面部属性和传记信息等多模态数据方面不足的策展多模态数据集集合。理解人类行为特征对于人机交互、计算社会科学和个性化AI系统中的应用至关重要，而这通常需要整合多种模态来捕捉细微的模式和关系。\n\n### PersonaX数据集构成\n\nPersonaX包含两个主要数据集：\n\n*   **CelebPersona**：收录了9444位来自不同职业的公众人物。\n*   **AthlePersona**：涵盖了7个主要体育联盟的4181名职业运动员。\n\n每个数据集都包含：\n\n*   由三个高性能大型语言模型（LLM）推断的行为特征评估。\n*   面部图像。\n*   结构化的传记特征。\n\n### 分析方法\n\n研究人员对PersonaX进行了两个互补层面的分析：\n\n1.  **高层特征分析**：\n    *   从文本描述中提取高层特征分数。\n    *   应用五种统计独立性检验来考察这些特征与其他模态之间的关系。\n2.  **因果表征学习（CRL）框架**：\n    *   引入了一种新颖的CRL框架，专为多模态和多测量数据设计。\n    *   该框架提供了理论上的可识别性保证。\n\n### 实验结果与贡献\n\n*   在合成数据和真实世界数据上的实验均证明了该方法的有效性。\n*   通过统一结构化和非结构化分析，PersonaX为结合视觉和传记属性研究LLM推断的行为特征奠定了基础。\n*   它推动了多模态特征分析和因果推理领域的发展。",
      "shortSummary": "PersonaX是一个多模态数据集集合，旨在解决现有数据集缺乏结合LLM推断行为特征与面部图像和传记信息的问题。它包含CelebPersona（9444位公众人物）和AthlePersona（4181名运动员），每个都整合了LLM推断的行为特征、面部数据和结构化传记信息。该研究通过统计检验和新颖的因果表征学习框架分析这些数据，有效推进了多模态特征分析和因果推理，为理解人类行为特征提供了新基础。",
      "translated_title": "PersonaX：基于LLM推断行为特征的多模态数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "Understanding human behavior traits is central to applications in human-computer interaction, computational social science, and personalized AI systems. Such understanding often requires integrating multiple modalities to capture nuanced patterns and relationships. However, existing resources rarely provide datasets that combine behavioral descriptors with complementary modalities such as facial attributes and biographical information. To address this gap, we present PersonaX, a curated collection of multimodal datasets designed to enable comprehensive analysis of public traits across modalities. PersonaX consists of (1) CelebPersona, featuring 9444 public figures from diverse occupations, and (2) AthlePersona, covering 4181 professional athletes across 7 major sports leagues. Each dataset includes behavioral trait assessments inferred by three high-performing large language models, alongside facial imagery and structured biographical features. We analyze PersonaX at two complementary levels. First, we abstract high-level trait scores from text descriptions and apply five statistical independence tests to examine their relationships with other modalities. Second, we introduce a novel causal representation learning (CRL) framework tailored to multimodal and multi-measurement data, providing theoretical identifiability guarantees. Experiments on both synthetic and real-world data demonstrate the effectiveness of our approach. By unifying structured and unstructured analysis, PersonaX establishes a foundation for studying LLM-inferred behavioral traits in conjunction with visual and biographical attributes, advancing multimodal trait analysis and causal reasoning."
    },
    {
      "title": "Nav-R1：具身场景中的推理与导航 (原标题: Nav-R1: Reasoning and Navigation in Embodied Scenes)",
      "link": "https://arxiv.org/abs/2509.10884",
      "pubDate": "Sat, 13 Sep 2025 12:31:03 GMT",
      "isoDate": "2025-09-13T12:31:03.000Z",
      "creator": "Qingxiang Liu, Ting Huang, Zeyu Zhang, Hao Tang",
      "summary": "## Nav-R1：具身场景中的推理与导航\n\n本文介绍了Nav-R1，一个用于具身环境推理的基模型。现有的方法在复杂3D环境中的鲁棒交互方面常常面临挑战，例如推理过程不连贯、不稳定，难以在长视野语义推理和低延迟控制之间取得平衡。\n\n为了解决这些问题，Nav-R1 采取了以下策略：\n\n* **构建大规模数据集：** 首先构建了Nav-CoT-110K数据集，这是一个包含大量具身任务逐步思维链 (CoT) 的大型数据集，用于模型的冷启动初始化和结构化推理。\n* **基于强化学习的框架：**  设计了一个基于 GRPO 的强化学习框架，该框架包含三个互补奖励：格式奖励、理解奖励和导航奖励，分别用于提高结构一致性、语义基础和路径保真度。\n* **快慢推理范式：** 引入了一种快慢推理范式，将深思熟虑的语义推理与低延迟的反应式控制解耦，从而实现高效且连贯的导航。\n\n在具身AI基准测试中的大量评估表明，Nav-R1 始终优于强大的基线，推理和导航性能平均提高了 8% 以上。在移动机器人上的实际部署进一步验证了其在有限机载资源下的鲁棒性。  该模型的代码和网站链接也已提供。",
      "shortSummary": "Nav-R1是一个用于具身环境推理的基模型，它通过构建大规模数据集Nav-CoT-110K，采用基于GRPO的强化学习框架和快慢推理范式，在具身AI基准测试中取得了显著优于基线的性能提升，平均提高了8%以上，并在移动机器人上得到验证。",
      "translated_title": "Nav-R1：具身场景中的推理与导航",
      "images": [],
      "contentSource": "完整文章",
      "content": "Embodied navigation requires agents to integrate perception, reasoning, and action for robust interaction in complex 3D environments. Existing approaches often suffer from incoherent and unstable reasoning traces that hinder generalization across diverse environments, and difficulty balancing long-horizon semantic reasoning with low-latency control for real-time navigation. To address these challenges, we propose Nav-R1, an embodied foundation model that unifies reasoning in embodied environments. We first construct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought (CoT) for embodied tasks, which enables cold-start initialization with structured reasoning. Building on this foundation, we design a GRPO-based reinforcement learning framework with three complementary rewards: format, understanding, and navigation, to improve structural adherence, semantic grounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow reasoning paradigm, decoupling deliberate semantic reasoning from low-latency reactive control for efficient yet coherent navigation. Extensive evaluations on embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms strong baselines, with over 8% average improvement in reasoning and navigation performance. Real-world deployment on a mobile robot further validates its robustness under limited onboard resources. Code: https://github.com/AIGeeksGroup/Nav-R1. Website: https://aigeeksgroup.github.io/Nav-R1."
    },
    {
      "title": "GAPrune: 梯度对齐剪枝用于领域感知嵌入 (原标题: GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings)",
      "link": "https://arxiv.org/abs/2509.10844",
      "pubDate": "Sat, 13 Sep 2025 11:03:37 GMT",
      "isoDate": "2025-09-13T11:03:37.000Z",
      "creator": "Yixuan Tang, Yi Yang",
      "summary": "## GAPrune: 梯度对齐剪枝用于领域感知嵌入\n\n### 背景与挑战\n\n领域特定嵌入模型在需要专业语义理解的应用（如编码代理和金融检索系统）中展现出巨大潜力，通常比通用模型实现更高的性能增益。然而，最先进的嵌入模型通常基于大型语言模型（LLMs），包含数十亿参数，这使得在资源受限环境中部署变得困难。模型压缩，特别是通过剪枝，提供了一个有前景的解决方案。但现有剪枝方法将所有参数一视同仁，未能区分通用语义表示和领域特定模式，导致剪枝决策次优。\n\n### GAPrune 方法\n\n为了解决这一挑战，我们提出了 **GAPrune**，一个剪枝框架，它在剪枝过程中同时考虑了领域重要性和通用语言基础的保留。其核心机制如下：\n\n*   **重要性度量**: 使用 Fisher 信息来衡量参数的重要性。\n*   **参数行为评估**: 利用通用领域梯度对齐来评估参数的行为。\n*   **领域对齐重要性 (DAI) 评分**: 将上述两种信号结合起来，生成一个领域对齐重要性（DAI）评分。较低的DAI分数表明该参数对领域任务不那么重要，或者在领域目标与通用目标之间产生了冲突。\n\n### 实验结果\n\n我们在两个领域基准——FinMTEB 和 ChemTEB 上进行了实验，结果表明 GAPrune 表现出色：\n\n*   **一次性剪枝**: 在 50% 稀疏度的一次性剪枝中，GAPrune 能够将性能保持在密集模型的 2.5% 以内，并且优于所有基线方法。\n*   **再训练后的性能提升**: 经过 100 步的再训练后，GAPrune 在 FinMTEB 上实现了 +4.51% 的性能提升，在 ChemTEB 上实现了 +1.73% 的提升。这表明我们的剪枝策略不仅保留了领域特定能力，还对其进行了增强。\n\n### 结论与意义\n\n我们的研究结果表明，原则性的剪枝策略可以同时实现模型压缩和增强领域专业化。GAPrune 为研究社区提供了一种开发新方法，以构建更高效、更专业的领域感知嵌入模型。",
      "shortSummary": "GAPrune是一种针对大型领域特定嵌入模型的剪枝框架，旨在解决现有方法未区分通用与领域参数的问题。它通过结合Fisher信息和通用梯度对齐，计算领域对齐重要性（DAI）分数来指导剪枝。实验表明，GAPrune在50%稀疏度下能保持高性能，并在再训练后显著提升了FinMTEB和ChemTEB基准上的领域特定能力，优于所有基线，实现了模型压缩与领域专业化的双重优化。",
      "translated_title": "GAPrune: 梯度对齐剪枝用于领域感知嵌入",
      "images": [],
      "contentSource": "完整文章",
      "content": "Domain-specific embedding models have shown promise for applications that require specialized semantic understanding, such as coding agents and financial retrieval systems, often achieving higher performance gains than general models. However, state-of-the-art embedding models are typically based on LLMs, which contain billions of parameters, making deployment challenging in resource-constrained environments. Model compression through pruning offers a promising solution, but existing pruning methods treat all parameters uniformly, failing to distinguish between general semantic representations and domain-specific patterns, leading to suboptimal pruning decisions. Thus, we propose GAPrune, a pruning framework that addresses this challenge by considering both domain importance and preserving general linguistic foundation. Our method uses Fisher Information to measure importance and general-domain gradient alignment to assess parameter behavior, then combines these signals using our Domain Alignment Importance (DAI) scoring. Lower DAI scores indicate that the parameter is either less important for the domain task or creates conflicts between domain and general objectives. Experiments on two domain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance within 2.5% of dense models in one-shot pruning at 50% sparsity, while outperforming all baselines. With retraining in 100 steps, GAPrune achieves +4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our pruning strategy not only preserves but enhances domain-specific capabilities. Our findings demonstrate that principled pruning strategies can achieve model compression and enhanced domain specialization, providing the research community with a new approach for development."
    },
    {
      "title": "InternScenes：一个具有真实布局的大规模可模拟室内场景数据集 (原标题: InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts)",
      "link": "https://arxiv.org/abs/2509.10813",
      "pubDate": "Sat, 13 Sep 2025 10:25:17 GMT",
      "isoDate": "2025-09-13T10:25:17.000Z",
      "creator": "Weipeng Zhong, Peizhou Cao, Yichen Jin, Li Luo, Wenzhe Cai, Jingli Lin, Hanqing Wang, Zhaoyang Lyu, Tai Wang, Bo Dai, Xudong Xu, Jiangmiao Pang",
      "summary": "### InternScenes：一个具有真实布局的大规模可模拟室内场景数据集\n\n**1. 背景与挑战**\n\n具身人工智能（Embodied AI）的发展严重依赖于大规模、可模拟的3D场景数据集，这些数据集需要具备场景多样性和真实的布局。然而，现有数据集通常存在以下局限性：\n*   数据规模或多样性不足。\n*   布局过于“干净”，缺少小型物品，不够真实。\n*   存在严重的物体碰撞问题。\n\n**2. InternScenes 数据集介绍**\n\n为了解决上述缺点，研究人员引入了 **InternScenes**，这是一个新颖的大规模可模拟室内场景数据集，旨在提供更真实、更复杂的环境。\n\n**3. 数据集的规模与多样性**\n\n*   **场景数量**：包含约40,000个多样化的场景。\n*   **数据来源**：整合了三种不同的场景来源——真实世界扫描、程序生成场景和设计师创建场景。\n*   **对象数量**：包含196万个3D对象。\n*   **覆盖范围**：涵盖15种常见的场景类型和288个对象类别。\n\n**4. 真实性与复杂性**\n\n*   **小型物品保留**：InternScenes 特别保留了场景中大量的小型物品，从而形成了真实且复杂的布局。\n*   **对象密度**：每个区域平均包含41.5个对象，显著提升了场景的细节和复杂性。\n\n**5. 数据处理流程与可模拟性**\n\n研究团队设计了一个全面的数据处理流程，以确保数据集的可模拟性和交互性：\n*   **真实到模拟的复制品**：为真实世界扫描创建了“真实到模拟”（real-to-sim）的复制品，确保数据在模拟环境中的可用性。\n*   **增强交互性**：通过在场景中加入可交互对象，增强了数据集的交互性。\n*   **解决物体碰撞**：通过物理模拟解决了物体之间的碰撞问题，确保了场景的物理合理性。\n\n**6. 基准应用与价值**\n\n研究人员通过两个基准应用展示了 InternScenes 的价值：\n*   **场景布局生成**：用于生成新的场景布局。\n*   **点目标导航**：用于具身智能体的导航任务。\n\n这两个应用都表明，InternScenes 中复杂且真实的布局带来了新的挑战。更重要的是，InternScenes 为这两项任务的模型训练提供了规模化的可能性，使得在如此复杂场景中的生成和导航成为可能。\n\n**7. 开放资源承诺**\n\n研究团队承诺将开放数据集、模型和基准，以造福整个社区。",
      "shortSummary": "InternScenes是一个大规模、可模拟的室内场景数据集，旨在解决现有具身AI数据集在规模、多样性和真实性方面的不足。它整合了约40,000个场景，包含196万个3D对象，覆盖15种场景类型和288个对象类别。该数据集通过保留大量小型物品和物理模拟，确保了场景的真实性、复杂性和可模拟性。InternScenes已应用于场景布局生成和点目标导航，为具身AI在复杂环境中的训练和发展提供了重要资源，并承诺开源。",
      "translated_title": "InternScenes：一个具有真实布局的大规模可模拟室内场景数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "The advancement of Embodied AI heavily relies on large-scale, simulatable 3D scene datasets characterized by scene diversity and realistic layouts. However, existing datasets typically suffer from limitations in data scale or diversity, sanitized layouts lacking small items, and severe object collisions. To address these shortcomings, we introduce InternScenes, a novel large-scale simulatable indoor scene dataset comprising approximately 40,000 diverse scenes by integrating three disparate scene sources, real-world scans, procedurally generated scenes, and designer-created scenes, including 1.96M 3D objects and covering 15 common scene types and 288 object classes. We particularly preserve massive small items in the scenes, resulting in realistic and complex layouts with an average of 41.5 objects per region. Our comprehensive data processing pipeline ensures simulatability by creating real-to-sim replicas for real-world scans, enhances interactivity by incorporating interactive objects into these scenes, and resolves object collisions by physical simulations. We demonstrate the value of InternScenes with two benchmark applications: scene layout generation and point-goal navigation. Both show the new challenges posed by the complex and realistic layouts. More importantly, InternScenes paves the way for scaling up the model training for both tasks, making the generation and navigation in such complex scenes possible. We commit to open-sourcing the data, models, and benchmarks to benefit the whole community."
    },
    {
      "title": "InfGen：一种分辨率无关的可扩展图像合成范式 (原标题: InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis)",
      "link": "https://arxiv.org/abs/2509.10441",
      "pubDate": "Fri, 12 Sep 2025 13:48:57 GMT",
      "isoDate": "2025-09-12T13:48:57.000Z",
      "creator": "Tao Han, Wanghan Xu, Junchao Gong, Xiaoyu Yue, Song Guo, Luping Zhou, Lei Bai",
      "summary": "# InfGen：一种分辨率无关的可扩展图像合成范式\n\n本文介绍了 **InfGen**，一种旨在解决当前扩散模型在生成高分辨率图像时计算效率低下问题的创新范式。它提供了一种在不同设备上保持一致视觉体验的方法，对生产者和消费者都具有广泛应用。\n\n## 核心问题\n\n*   **计算需求高昂**：现有扩散模型生成图像的计算需求与分辨率呈二次方增长。\n*   **生成时间长**：例如，生成4K图像可能需要超过100秒，这严重影响了用户体验和应用效率。\n\n## InfGen 解决方案\n\nInfGen 是在潜在扩散模型（第二代）基础上发展而来的一种新方法。其核心思想和工作原理如下：\n\n*   **内容表示**：将扩散模型生成的固定潜在表示视为图像的内容表示。\n*   **一步生成器**：提出了一种一步生成器，用于从紧凑的、固定大小的潜在表示中解码出任意分辨率的图像。\n*   **替换 VAE 解码器**：InfGen 用这个新的生成器替换了传统的 VAE (Variational Autoencoder) 解码器。\n*   **无需重新训练**：关键优势在于，它可以在不重新训练原始扩散模型的情况下，从固定大小的潜在空间生成任意分辨率的图像。\n\n## 主要优势\n\n*   **简化流程**：大大简化了高分辨率图像的生成过程。\n*   **降低计算复杂度**：显著减少了生成高分辨率图像所需的计算资源。\n*   **广泛适用性**：可以应用于任何使用相同潜在空间的模型。\n*   **显著提速**：实验结果表明，InfGen 能够将4K图像的生成时间缩短到10秒以内。\n*   **提升模型能力**：使许多现有模型能够进入任意高分辨率图像生成时代。\n\n## 论文背景\n\n*   该研究已被 ICCV 2025 接受。",
      "shortSummary": "InfGen 提出了一种分辨率无关的图像合成范式，旨在解决现有扩散模型生成高分辨率图像时计算量大、耗时长的痛点。它通过用一步生成器替换 VAE 解码器，使得模型能够在不重新训练的情况下，从固定大小的潜在表示生成任意分辨率的图像。实验证明，InfGen 能将4K图像生成时间缩短至10秒以内，显著提高了效率和可扩展性。",
      "translated_title": "InfGen：一种分辨率无关的可扩展图像合成范式",
      "images": [],
      "contentSource": "完整文章",
      "content": "Arbitrary resolution image generation provides a consistent visual experience across devices, having extensive applications for producers and consumers. Current diffusion models increase computational demand quadratically with resolution, causing 4K image generation delays over 100 seconds. To solve this, we explore the second generation upon the latent diffusion models, where the fixed latent generated by diffusion models is regarded as the content representation and we propose to decode arbitrary resolution images with a compact generated latent using a one-step generator. Thus, we present the InfGen, replacing the VAE decoder with the new generator, for generating images at any resolution from a fixed-size latent without retraining the diffusion models, which simplifies the process, reducing computational complexity and can be applied to any model using the same latent space. Experiments show InfGen is capable of improving many models into the arbitrary high-resolution era while cutting 4K image generation time to under 10 seconds."
    },
    {
      "title": "基于Inpainting引导的扩散大语言模型策略优化 (原标题: Inpainting-Guided Policy Optimization for Diffusion Large Language Models)",
      "link": "https://arxiv.org/abs/2509.10396",
      "pubDate": "Fri, 12 Sep 2025 12:44:31 GMT",
      "isoDate": "2025-09-12T12:44:31.000Z",
      "creator": "Siyan Zhao, Mengchen Liu, Jing Huang, Miao Liu, Chenyu Wang, Bo Liu, Yuandong Tian, Guan Pang, Sean Bell, Aditya Grover, Feiyu Chen",
      "summary": "扩散大语言模型（dLLMs）作为自回归LLMs的替代方案正在兴起，它们不仅性能具有竞争力，还支持Inpainting等独特的生成能力。本文探讨了Inpainting如何指导dLLM的强化学习（RL）算法设计。\n\n**RL对齐LLMs面临的挑战：**\n*   稀疏的奖励信号。\n*   当模型未能发现正确解决方案时，导致样本浪费。\n\n**dLLMs的Inpainting能力带来的独特机遇：**\n*   dLLMs的Inpainting能力可以有效引导探索过程。\n\n**介绍IGPO（Inpainting Guided Policy Optimization）：**\n*   **核心思想：** IGPO是一个RL框架，它在在线采样过程中策略性地插入部分真实推理轨迹。\n*   **工作机制：**\n    *   与提供完整解决方案不同，Inpainting将探索引向有前景的轨迹空间。\n    *   同时保留模型自身生成的推理过程。\n    *   有效连接了监督微调（SFT）和强化学习。\n*   **解决梯度问题：** 在GRPO等基于组的优化方法中，探索失败会导致零优势和梯度。IGPO能够恢复有意义的梯度，并提高样本效率。\n\n**附加技术：**\n*   对合成重写的简洁轨迹进行监督微调，使其更好地与dLLM的生成模式对齐。\n*   采用基于熵的过滤。\n\n**实验结果：**\n*   结合IGPO和上述训练方法，在三个数学基准测试（GSM8K、Math500和AMC）上取得了显著的性能提升。\n*   为全注意力掩码dLLMs实现了新的最先进（SOTA）结果。",
      "shortSummary": "本文提出了IGPO（Inpainting Guided Policy Optimization），一个针对扩散大语言模型（dLLMs）的强化学习框架。IGPO利用dLLMs的Inpainting能力，在在线采样时策略性地插入部分真实推理轨迹，以解决RL探索中的稀疏奖励和样本浪费问题。该方法引导探索、保留自我生成推理、恢复梯度并提高样本效率。结合其他技术，IGPO在GSM8K、Math500和AMC等数学基准测试中为全注意力掩码dLLMs取得了新的最先进成果。",
      "translated_title": "基于Inpainting引导的扩散大语言模型策略优化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Masked diffusion large language models (dLLMs) are emerging as promising alternatives to autoregressive LLMs, offering competitive performance while supporting unique generation capabilities such as inpainting. We explore how inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with reinforcement learning faces an exploration challenge: sparse reward signals and sample waste when models fail to discover correct solutions. While this inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided Policy Optimization), an RL framework that strategically inserts partial ground-truth reasoning traces during online sampling. Unlike providing full solutions, inpainting steers exploration toward promising trajectory spaces while preserving self-generated reasoning, bridging supervised fine-tuning and reinforcement learning. We apply IGPO to group-based optimization methods such as GRPO, where exploration failures cause zero advantages and gradients. IGPO restores meaningful gradients while improving sample efficiency. We also propose supervised fine-tuning on synthetically rewritten concise traces that better align with dLLM generation patterns. With additional techniques including entropy-based filtering, our training recipe yields substantial gains across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new state-of-the-art results for full-attention masked dLLMs."
    },
    {
      "title": "虚拟代理经济 (原标题: Virtual Agent Economies)",
      "link": "https://arxiv.org/abs/2509.10147",
      "pubDate": "Fri, 12 Sep 2025 07:20:11 GMT",
      "isoDate": "2025-09-12T07:20:11.000Z",
      "creator": "Nenad Tomasev, Matija Franklin, Joel Z. Leibo, Julian Jacobs, William A. Cunningham, Iason Gabriel, Simon Osindero",
      "summary": "## 虚拟代理经济：新兴经济层与可控市场设计\n\n### 引言\n\n随着自主AI代理的迅速普及，一个全新的经济层正在形成。在这个新兴的经济层中，AI代理能够以超越人类直接监督的规模和速度进行交易和协调。\n\n### “沙盒经济”框架\n\n文章提出了“沙盒经济”作为分析这一新兴系统的框架。该框架通过两个关键维度来描述和理解这一系统：\n\n*   **起源：** 探讨经济是自发涌现的还是经过有意设计的。\n*   **与人类经济的独立性：** 评估经济与既有的人类经济之间的分离程度，是高度可渗透的（与人类经济紧密相连）还是不可渗透的（独立运行）。\n\n### 当前发展轨迹与挑战\n\n目前的趋势表明，我们正朝着一个庞大且高度可渗透的AI代理经济自发涌现的方向发展。这带来了显著的机遇和挑战：\n\n*   **机遇：** 能够实现前所未有的协调程度。\n*   **挑战：** 包括潜在的系统性经济风险和加剧的社会不平等。\n\n### 可控AI代理市场的设计选择\n\n为了确保新兴技术变革能够与人类的长期集体繁荣相一致，文章讨论了多种可能的设计选择，旨在构建安全可控的AI代理市场：\n\n*   **拍卖机制：** 用于实现公平的资源分配和解决偏好冲突。\n*   **AI“任务经济”：** 旨在围绕实现集体目标进行有效协调。\n*   **社会技术基础设施：** 建立必要的结构以确保信任、安全和问责制。\n\n### 核心论点\n\n文章强调，我们应积极主动地设计可控的代理市场，以引导未来的技术变革，使其与人类的长期集体繁荣相一致。\n\n### 相关领域\n\n本文主要涉及人工智能（cs.AI）领域。",
      "shortSummary": "自主AI代理的快速发展催生了一个超越人类监督的新经济层。文章提出了“沙盒经济”框架来分析这一新兴系统，并指出当前趋势是形成一个庞大且高度可渗透的AI代理经济。这带来了前所未有的协调机遇，但也伴随着系统性经济风险和不平等加剧等挑战。为确保技术变革与人类福祉相符，文章主张通过设计拍卖机制、AI“任务经济”和健全的社会技术基础设施，积极构建可控的代理市场。",
      "translated_title": "虚拟代理经济",
      "images": [],
      "contentSource": "完整文章",
      "content": "The rapid adoption of autonomous AI agents is giving rise to a new economic layer where agents transact and coordinate at scales and speeds beyond direct human oversight. We propose the \"sandbox economy\" as a framework for analyzing this emergent system, characterizing it along two key dimensions: its origins (emergent vs. intentional) and its degree of separateness from the established human economy (permeable vs. impermeable). Our current trajectory points toward a spontaneous emergence of a vast and highly permeable AI agent economy, presenting us with opportunities for an unprecedented degree of coordination as well as significant challenges, including systemic economic risk and exacerbated inequality. Here we discuss a number of possible design choices that may lead to safely steerable AI agent markets. In particular, we consider auction mechanisms for fair resource allocation and preference resolution, the design of AI \"mission economies\" to coordinate around achieving collective goals, and socio-technical infrastructure needed to ensure trust, safety, and accountability. By doing this, we argue for the proactive design of steerable agent markets to ensure the coming technological shift aligns with humanity's long-term collective flourishing."
    },
    {
      "title": "正确着色：弥合感知色彩空间与文本嵌入以改进扩散生成 (原标题: Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation)",
      "link": "https://arxiv.org/abs/2509.10058",
      "pubDate": "Fri, 12 Sep 2025 04:44:22 GMT",
      "isoDate": "2025-09-12T04:44:22.000Z",
      "creator": "Sung-Lin Tsai, Bo-Lun Huang, Yu Ting Shen, Cheng Yu Yeo, Chiang Tseng, Bo-Kai Ruan, Wen-Sheng Lien, Hong-Han Shuai",
      "summary": "### 解决文本到图像生成中的颜色对齐挑战\n\n**问题背景**\n\n*   在时尚、产品可视化和室内设计等应用中，文本到图像（T2I）生成中的准确颜色对齐至关重要。\n*   然而，当前的扩散模型在处理细微和复合颜色术语（例如，蒂芙尼蓝、酸橙绿、亮粉色）时表现不佳，经常生成与人类意图不符的图像。\n*   现有方法（如交叉注意力操作、参考图像或微调）未能系统地解决模糊颜色描述的问题。\n\n**提出的解决方案：无需训练的框架**\n\n为了在提示模糊的情况下精确渲染颜色，本文提出了一种无需训练的框架，通过以下方式增强颜色保真度：\n\n1.  **大型语言模型（LLM）消歧**：首先，利用大型语言模型（LLM）来解决文本提示中模糊的颜色相关术语。\n2.  **文本嵌入优化**：然后，根据解析出的颜色术语在CIELAB色彩空间中的空间关系，直接在文本嵌入空间中指导颜色混合操作，从而优化文本嵌入。\n\n**主要优势**\n\n*   **无需额外训练**：与现有方法不同，该方法无需额外的模型训练。\n*   **无需外部参考图像**：不依赖外部参考图像。\n*   **提高颜色精度**：在不损害图像质量的前提下，显著提高了颜色对齐的准确性。\n\n**研究成果与影响**\n\n*   实验结果表明，该框架在不影响图像质量的情况下，有效改善了颜色对齐。\n*   成功弥合了文本语义与视觉生成之间的鸿沟，使得T2I模型能够更准确地理解和渲染复杂的颜色描述。\n\n**出版信息**\n\n*   该研究已被 ACM Multimedia 2025 (MM '25) 接收。",
      "shortSummary": "针对文本到图像（T2I）生成中颜色描述不准确的问题，本文提出了一种无需训练的框架。该框架首先利用大型语言模型（LLM）消除提示中模糊的颜色术语，然后根据CIELAB色彩空间中解析出的颜色关系，直接在文本嵌入空间中优化文本嵌入。该方法无需额外训练或参考图像，显著提高了颜色对齐精度，同时保持了图像质量，弥合了文本语义与视觉生成之间的鸿沟。",
      "translated_title": "正确着色：弥合感知色彩空间与文本嵌入以改进扩散生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Accurate color alignment in text-to-image (T2I) generation is critical for applications such as fashion, product visualization, and interior design, yet current diffusion models struggle with nuanced and compound color terms (e.g., Tiffany blue, lime green, hot pink), often producing images that are misaligned with human intent. Existing approaches rely on cross-attention manipulation, reference images, or fine-tuning but fail to systematically resolve ambiguous color descriptions. To precisely render colors under prompt ambiguity, we propose a training-free framework that enhances color fidelity by leveraging a large language model (LLM) to disambiguate color-related prompts and guiding color blending operations directly in the text embedding space. Our method first employs a large language model (LLM) to resolve ambiguous color terms in the text prompt, and then refines the text embeddings based on the spatial relationships of the resulting color terms in the CIELAB color space. Unlike prior methods, our approach improves color accuracy without requiring additional training or external reference images. Experimental results demonstrate that our framework improves color alignment without compromising image quality, bridging the gap between text semantics and visual generation."
    },
    {
      "title": "QuantAgent：价格驱动的多智能体LLM在高频交易中的应用 (原标题: QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading)",
      "link": "https://arxiv.org/abs/2509.09995",
      "pubDate": "Fri, 12 Sep 2025 02:35:40 GMT",
      "isoDate": "2025-09-12T02:35:40.000Z",
      "creator": "Fei Xiong, Xiang Zhang, Aosong Feng, Siqi Sun, Chenyu You",
      "summary": "## QuantAgent：高频交易中的多智能体LLM框架\n\n### 背景与挑战\n\n*   **现有LLM的局限性：** 近期大型语言模型（LLMs）在金融推理和市场理解方面展现出强大能力。TradingAgent和FINMEM等多智能体LLM框架通过利用基本面和情绪输入，增强了模型在长期投资任务中的战略决策能力。\n*   **高频交易（HFT）的独特需求：** 然而，这些系统不适用于高频交易（HFT）对高速、精度关键决策的严苛要求。HFT需要基于结构化、短周期信号（如技术指标、图表模式和趋势特征）的快速、风险感知决策，这与传统金融LLM应用中典型的长期语义推理截然不同。\n\n### QuantAgent：专为HFT设计\n\n*   **创新框架：** 我们引入了QuantAgent，这是第一个明确为高频算法交易设计的多智能体LLM框架。\n*   **模块化智能体：** 该系统将交易过程分解为四个专业智能体：\n    *   **指标（Indicator）智能体：** 专注于分析各种技术指标。\n    *   **模式（Pattern）智能体：** 识别图表模式和价格行为。\n    *   **趋势（Trend）智能体：** 捕捉市场趋势和动量。\n    *   **风险（Risk）智能体：** 评估和管理交易风险。\n*   **核心能力：** 每个智能体都配备了领域特定工具和结构化推理能力，以在短时间窗口内捕捉市场动态的不同方面。\n\n### 性能评估与结果\n\n*   **零样本评估：** 在对包括比特币和纳斯达克期货在内的十种金融工具进行的零样本评估中，QuantAgent展现出卓越的性能。\n*   **优异表现：** 在4小时交易间隔内，QuantAgent在预测准确性和累计回报方面均表现出色，超越了强大的神经网络和基于规则的基线模型。\n\n### 结论与展望\n\n*   **潜力揭示：** 我们的研究结果表明，将结构化金融先验知识与语言原生推理相结合，为高频金融市场中可追溯的实时决策系统开辟了新的潜力。",
      "shortSummary": "QuantAgent是首个专为高频交易（HFT）设计的多智能体LLM框架。它通过指标、模式、趋势和风险四个专业智能体，处理短周期市场信号，以实现快速、风险感知的决策。在对包括比特币和纳斯达克期货在内的十种金融工具进行的零样本评估中，QuantAgent在预测准确性和累计回报方面均表现出卓越性能，超越了现有基线。该研究揭示了结合结构化金融先验知识与LLM推理在高频金融市场中的巨大潜力。",
      "translated_title": "QuantAgent：价格驱动的多智能体LLM在高频交易中的应用",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in Large Language Models (LLMs) have demonstrated impressive capabilities in financial reasoning and market understanding. Multi-agent LLM frameworks such as TradingAgent and FINMEM augment these models to long-horizon investment tasks, leveraging fundamental and sentiment-based inputs for strategic decision-making. However, such systems are ill-suited for the high-speed, precision-critical demands of High-Frequency Trading (HFT). HFT requires rapid, risk-aware decisions based on structured, short-horizon signals, including technical indicators, chart patterns, and trend-based features, distinct from the long-term semantic reasoning typical of traditional financial LLM applications. To this end, we introduce QuantAgent, the first multi-agent LLM framework explicitly designed for high-frequency algorithmic trading. The system decomposes trading into four specialized agents, Indicator, Pattern, Trend, and Risk, each equipped with domain-specific tools and structured reasoning capabilities to capture distinct aspects of market dynamics over short temporal windows. In zero-shot evaluations across ten financial instruments, including Bitcoin and Nasdaq futures, QuantAgent demonstrates superior performance in both predictive accuracy and cumulative return over 4-hour trading intervals, outperforming strong neural and rule-based baselines. Our findings suggest that combining structured financial priors with language-native reasoning unlocks new potential for traceable, real-time decision systems in high-frequency financial markets."
    },
    {
      "title": "CMHG：一个用于中国少数民族语言标题生成的语料库和基准 (原标题: CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China)",
      "link": "https://arxiv.org/abs/2509.09990",
      "pubDate": "Fri, 12 Sep 2025 02:18:44 GMT",
      "isoDate": "2025-09-12T02:18:44.000Z",
      "creator": "Guixian Xu, Zeli Su, Ziyin Zhang, Jianing Liu, XU Han, Ting Zhang, Yushuang Dong",
      "summary": "# CMHG：中国少数民族语言标题生成数据集与基准\n\n## 问题背景\n*   **语言挑战：** 中国的少数民族语言，如藏语、维吾尔语和传统蒙古语，由于其独特的书写系统与国际标准存在差异，面临着显著的挑战。\n*   **语料库匮乏：** 这种差异导致相关语料库严重缺乏，尤其是在标题生成等监督任务方面。\n\n## 解决方案：CMHG数据集\n*   **数据集引入：** 为解决上述空白，研究人员推出了一个名为“中国少数民族标题生成（CMHG）”的新型数据集。\n*   **数据集构成：**\n    *   包含100,000条藏语条目。\n    *   包含50,000条维吾尔语条目。\n    *   包含50,000条蒙古语条目。\n*   **用途：** 这些条目是专门为标题生成任务精心策划的。\n\n## 基准测试集\n*   **高质量测试集：** 除了核心数据集，研究还提出了一个由母语使用者标注的高质量测试集。\n*   **目的：** 该测试集旨在作为该领域未来研究的基准。\n\n## 期望与贡献\n*   研究人员希望CMHG数据集能成为推动中国少数民族语言标题生成领域发展的重要资源。\n*   同时，也期望它能为相关基准的开发做出贡献。",
      "shortSummary": "针对中国少数民族语言（如藏语、维吾尔语、蒙古语）标题生成任务中语料库匮乏的问题，研究人员推出了CMHG数据集。该数据集包含10万条藏语、各5万条维吾尔语和蒙古语条目，并附带一个由母语使用者标注的高质量测试集，旨在作为该领域的基准。CMHG有望推动中国少数民族语言标题生成研究的发展。",
      "translated_title": "CMHG：一个用于中国少数民族语言标题生成的语料库和基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Minority languages in China, such as Tibetan, Uyghur, and Traditional Mongolian, face significant challenges due to their unique writing systems, which differ from international standards. This discrepancy has led to a severe lack of relevant corpora, particularly for supervised tasks like headline generation. To address this gap, we introduce a novel dataset, Chinese Minority Headline Generation (CMHG), which includes 100,000 entries for Tibetan, and 50,000 entries each for Uyghur and Mongolian, specifically curated for headline generation tasks. Additionally, we propose a high-quality test set annotated by native speakers, designed to serve as a benchmark for future research in this domain. We hope this dataset will become a valuable resource for advancing headline generation in Chinese minority languages and contribute to the development of related benchmarks."
    },
    {
      "title": "LoFT：面向开放世界场景中长尾半监督学习的参数高效微调 (原标题: LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios)",
      "link": "https://arxiv.org/abs/2509.09926",
      "pubDate": "Thu, 11 Sep 2025 22:28:32 GMT",
      "isoDate": "2025-09-11T22:28:32.000Z",
      "creator": "Jiahao Chen, Zhiyuan Huang, Yurou Liu, Bing Su",
      "summary": "## LoFT：面向开放世界场景中长尾半监督学习的参数高效微调\n\n### 1. 背景与挑战\n*   **长尾学习的重要性**：长尾学习因其在现实世界场景中的广泛适用性而受到越来越多的关注。\n*   **长尾半监督学习 (LTSSL)**：通过将大量未标记数据整合到不平衡的标记数据集中，LTSSL 已成为一种有效的解决方案。\n*   **现有方法的局限性**：大多数先前的 LTSSL 方法从头开始训练模型，这常常导致模型过自信和生成低质量的伪标签。\n\n### 2. 提出的方法：LoFT\n*   **范式转变**：为了解决上述挑战，本文将 LTSSL 扩展到基础模型微调范式。\n*   **LoFT 框架**：提出了一个新颖的框架——LoFT（Long-tailed semi-supervised learning via parameter-efficient Fine-Tuning，即通过参数高效微调实现长尾半监督学习）。\n*   **核心优势**：LoFT 证明了微调后的基础模型能够生成更可靠的伪标签，从而显著有益于不平衡学习。\n\n### 3. 开放世界场景下的扩展：LoFT-OW\n*   **更实际的设置**：本文进一步探索了在开放世界条件下的半监督学习，即未标记数据可能包含分布外（Out-of-Distribution, OOD）样本的情况。\n*   **LoFT-OW 方案**：为了处理这一问题，作者提出了 LoFT-OW（LoFT under Open-World scenarios），旨在提高模型在开放世界场景下的判别能力。\n\n### 4. 实验结果\n*   **卓越性能**：在多个基准测试上的实验结果表明，LoFT 方法相比现有方法取得了卓越的性能。\n*   **数据效率**：即使仅使用与以往工作相比 1% 的未标记数据，LoFT 也能达到优越的表现。\n\n### 5. 研究主题\n*   机器学习 (cs.LG)\n*   计算机视觉与模式识别 (cs.CV)",
      "shortSummary": "本文提出LoFT框架，通过参数高效微调基础模型，解决了长尾半监督学习中模型过自信和伪标签质量低的问题。LoFT能生成更可靠的伪标签，提升不平衡学习效果。针对开放世界场景，LoFT-OW进一步增强了模型对分布外样本的判别能力。实验证明，LoFT在多个基准测试上表现优越，即使仅使用少量未标记数据也能超越现有方法。",
      "translated_title": "LoFT：面向开放世界场景中长尾半监督学习的参数高效微调",
      "images": [],
      "contentSource": "完整文章",
      "content": "Long-tailed learning has garnered increasing attention due to its wide applicability in real-world scenarios. Among existing approaches, Long-Tailed Semi-Supervised Learning (LTSSL) has emerged as an effective solution by incorporating a large amount of unlabeled data into the imbalanced labeled dataset. However, most prior LTSSL methods are designed to train models from scratch, which often leads to issues such as overconfidence and low-quality pseudo-labels. To address these challenges, we extend LTSSL into the foundation model fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate that fine-tuned foundation models can generate more reliable pseudolabels, thereby benefiting imbalanced learning. Furthermore, we explore a more practical setting by investigating semi-supervised learning under open-world conditions, where the unlabeled data may include out-of-distribution (OOD) samples. To handle this problem, we propose LoFT-OW (LoFT under Open-World scenarios) to improve the discriminative ability. Experimental results on multiple benchmarks demonstrate that our method achieves superior performance compared to previous approaches, even when utilizing only 1\\% of the unlabeled data compared with previous works."
    },
    {
      "title": "FLUX-Reason-6M与PRISM-Bench：百万级文本到图像推理数据集与综合基准 (原标题: FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark)",
      "link": "https://arxiv.org/abs/2509.09680",
      "pubDate": "Thu, 11 Sep 2025 13:59:59 GMT",
      "isoDate": "2025-09-11T13:59:59.000Z",
      "creator": "Rongyao Fang, Aldrich Yu, Chengqi Duan, Linjiang Huang, Shuai Bai, Yuxuan Cai, Kun Wang, Si Liu, Xihui Liu, Hongsheng Li",
      "summary": "## FLUX-Reason-6M与PRISM-Bench：百万级文本到图像推理数据集与综合基准\n\n### 引言\n\n当前，开源文本到图像（T2I）模型的发展面临挑战，主要原因在于缺乏大规模、专注于推理能力的数据集以及全面的评估基准。这导致开源模型与领先的闭源系统之间存在显著的性能差距。\n\n### FLUX-Reason-6M 数据集\n\n为解决上述问题，研究团队引入了FLUX-Reason-6M数据集，旨在教授模型复杂的推理能力。\n\n*   **规模与内容：**\n    *   包含600万张由FLUX模型生成的高质量图像。\n    *   附带2000万条双语（英语和中文）描述，这些描述经过精心设计以支持复杂推理任务。\n*   **组织与结构：**\n    *   图像根据六个关键特征进行组织：想象力（Imagination）、实体（Entity）、文本渲染（Text rendering）、风格（Style）、情感（Affection）和构图（Composition）。\n    *   设计了明确的“生成思维链”（Generation Chain-of-Thought, GCoT），为图像生成步骤提供详细的分解，从而帮助模型理解和执行复杂指令。\n*   **资源投入：** 整个数据整理过程耗费了15,000个A100 GPU天，为社区提供了一个此前仅在大型工业实验室才能获得的宝贵资源。\n\n### PRISM-Bench 评估基准\n\n同时推出的PRISM-Bench（Precise and Robust Image Synthesis Measurement Benchmark）提供了一个新颖且全面的评估标准。\n\n*   **评估轨道：** 包含七个不同的评估轨道，其中包括一个极具挑战性的“长文本挑战”，该挑战利用GCoT来评估模型处理复杂长文本提示的能力。\n*   **评估方法：**\n    *   通过精心设计的提示词，利用先进的视觉-语言模型（VLM）进行评估。\n    *   实现对提示词-图像对齐（prompt-image alignment）和图像美学（image aesthetics）进行细致的、与人类评估高度一致的评估。\n\n### 实验与发现\n\n研究团队对19个领先的T2I模型在PRISM-Bench上进行了广泛评估。结果揭示了这些模型在推理能力上的关键性能差距，并明确指出了需要改进的具体领域。\n\n### 贡献与开放资源\n\nFLUX-Reason-6M数据集、PRISM-Bench基准以及相关的评估代码均已公开发布。此举旨在催化下一波面向推理的T2I生成研究，推动开源社区在这一领域取得突破。项目页面：this https URL",
      "shortSummary": "为解决开源文本到图像（T2I）模型在推理能力和评估方面的不足，本文提出了FLUX-Reason-6M数据集和PRISM-Bench基准。FLUX-Reason-6M包含600万张图像和2000万条双语描述，通过“生成思维链”（GCoT）教授复杂推理。PRISM-Bench提供七个评估轨道，包括长文本挑战，利用先进视觉-语言模型进行细致评估。对19个模型的评估揭示了性能差距。该工作发布了数据集、基准和代码，旨在推动面向推理的T2I生成发展。",
      "translated_title": "FLUX-Reason-6M与PRISM-Bench：百万级文本到图像推理数据集与综合基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems. To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark). FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning. The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps. The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs. PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT. Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement. Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation. Project page: https://flux-reason-6m.github.io/ ."
    }
  ],
  "lastUpdated": "2025-09-16T09:37:05.252Z"
}