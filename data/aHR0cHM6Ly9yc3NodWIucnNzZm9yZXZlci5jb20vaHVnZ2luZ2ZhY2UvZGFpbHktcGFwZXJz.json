{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "CODA：协调大脑皮层和小脑，用于采用解耦强化学习的双脑计算机使用代理 (原标题: CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2508.20096",
      "pubDate": "Wed, 27 Aug 2025 13:59:50 GMT",
      "isoDate": "2025-08-27T13:59:50.000Z",
      "creator": "Zeyi Sun, Yuhang Cao, Jianze Liang, Qiushi Sun, Ziyu Liu, Zhixiong Zhang, Yuhang Zang, Xiaoyi Dong, Kai Chen, Dahua Lin, Jiaqi Wang",
      "summary": "## CODA：用于GUI代理的“双脑”解耦强化学习框架\n\n### 引言与挑战\n\n自主图形用户界面（GUI）代理在科学计算等专业领域面临显著挑战。这些领域要求代理同时具备长周期规划能力和精确的执行能力。当前方法存在以下局限：\n\n*   **通用代理**：擅长规划，但在具体执行上表现不佳。\n*   **专业代理**：执行能力强，但在规划方面表现较弱。\n*   **现有组合框架**：尽管尝试结合规划器和执行器，但通常是静态且不可训练的，无法从经验中学习和适应。鉴于科学领域高质量数据稀缺，这是一个关键限制。\n\n### CODA框架介绍\n\n为解决上述问题，研究人员提出了 **CODA**，一个新颖且可训练的组合框架。CODA 的核心思想是整合一个通用规划器（被称为“大脑皮层”，**Cerebrum**）和一个专业执行器（被称为“小脑”，**Cerebellum**）。\n\n### CODA的训练流程：两阶段管道\n\nCODA 的训练通过一个专门的两阶段管道进行，旨在使其同时具备鲁棒的执行能力和跨领域泛化能力。\n\n1.  **第一阶段：专业化 (Specialization)**\n    *   **目标**：为每个特定的科学应用单独训练一个专家规划器。\n    *   **方法**：应用一种解耦的广义强化学习策略优化（GRPO）方法。\n    *   **数据利用**：从少量任务轨迹中进行自举（bootstrapping），以克服数据稀缺问题。\n\n2.  **第二阶段：泛化 (Generalization)**\n    *   **目标**：构建一个能够跨领域泛化的最终规划器。\n    *   **数据聚合**：将所有来自第一阶段专业化专家训练成功的轨迹聚合起来，构建一个整合数据集。\n    *   **训练方法**：使用该整合数据集对最终规划器进行监督微调。\n\n### 实验评估与成果\n\nCODA 在 ScienceBoard 基准测试中的四个具有挑战性的应用上进行了评估。结果显示：\n\n*   CODA 显著优于现有基线方法。\n*   在开源模型中，CODA 建立了新的最先进水平。\n\n### 代码可用性\n\nCODA 的代码已在指定 URL 提供，方便研究社区进一步探索和使用。",
      "shortSummary": "CODA是一种新颖的可训练组合框架，旨在解决GUI代理在科学计算中长周期规划和精确执行的挑战。它整合了通用规划器（Cerebrum）和专业执行器（Cerebellum），并采用两阶段解耦强化学习训练。第一阶段实现专业化，为各应用训练专家规划器；第二阶段通过聚合成功轨迹进行监督微调，实现跨领域泛化。CODA在ScienceBoard基准测试中表现出色，显著超越现有基线，达到开源模型的最新水平，兼具鲁棒执行和泛化能力。",
      "translated_title": "CODA：协调大脑皮层和小脑，用于采用解耦强化学习的双脑计算机使用代理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Autonomous agents for Graphical User Interfaces (GUIs) face significant challenges in specialized domains such as scientific computing, where both long-horizon planning and precise execution are required. Existing approaches suffer from a trade-off: generalist agents excel at planning but perform poorly in execution, while specialized agents demonstrate the opposite weakness. Recent compositional frameworks attempt to bridge this gap by combining a planner and an actor, but they are typically static and non-trainable, which prevents adaptation from experience. This is a critical limitation given the scarcity of high-quality data in scientific domains. To address these limitations, we introduce CODA, a novel and trainable compositional framework that integrates a generalist planner (Cerebrum) with a specialist executor (Cerebellum), trained via a dedicated two-stage pipeline. In the first stage, Specialization, we apply a decoupled GRPO approach to train an expert planner for each scientific application individually, bootstrapping from a small set of task trajectories. In the second stage, Generalization, we aggregate all successful trajectories from the specialized experts to build a consolidated dataset, which is then used for supervised fine-tuning of the final planner. This equips CODA with both robust execution and cross-domain generalization. Evaluated on four challenging applications from the ScienceBoard benchmark, CODA significantly outperforms baselines and establishes a new state of the art among open-source models."
    },
    {
      "title": "AudioStory：使用大型语言模型生成长篇叙事音频 (原标题: AudioStory: Generating Long-Form Narrative Audio with Large Language Models)",
      "link": "https://arxiv.org/abs/2508.20088",
      "pubDate": "Wed, 27 Aug 2025 13:55:38 GMT",
      "isoDate": "2025-08-27T13:55:38.000Z",
      "creator": "Yuxin Guo, Teng Wang, Yuying Ge, Shijie Ma, Yixiao Ge, Wei Zou, Ying Shan",
      "summary": "# AudioStory：使用大型语言模型生成长篇叙事音频\n\n## 挑战与解决方案\n\n*   **现有问题**：当前文本到音频（TTA）生成技术在合成短音频片段方面表现出色，但在生成长篇叙事音频时面临挑战，主要体现在难以保持时间连贯性和进行组合推理。\n*   **AudioStory的提出**：为解决这一问题，研究者提出了AudioStory，一个统一的框架，它将大型语言模型（LLMs）与TTA系统相结合，旨在生成结构化的长篇音频叙事。\n\n## AudioStory的核心能力与特点\n\n*   **指令遵循与推理**：AudioStory具备强大的指令遵循推理生成能力。它利用LLMs将复杂的叙事查询分解为按时间顺序排列的子任务，并提供上下文线索，从而实现连贯的场景过渡和情感语调的一致性。\n*   **两大吸引人的特性**：\n    1.  **解耦的桥接机制**：AudioStory将LLM与扩散模型的协作解耦为两个专门组件：\n        *   **桥接查询（bridging query）**：用于事件内部的语义对齐。\n        *   **残差查询（residual query）**：用于保持跨事件的连贯性。\n    2.  **端到端训练**：通过在一个单一的端到端框架内统一指令理解和音频生成，AudioStory消除了对模块化训练管道的需求，同时增强了组件之间的协同作用。\n\n## 评估与成果\n\n*   **基准数据集**：研究者建立了AudioStory-10K基准，涵盖了动画音景和自然声音叙事等多样化领域。\n*   **实验结果**：广泛的实验表明，AudioStory在单音频生成和叙事音频生成方面均表现出卓越性能，在指令遵循能力和音频保真度方面均超越了先前的TTA基线。\n*   **代码可用性**：相关代码已公开。",
      "shortSummary": "AudioStory是一个统一框架，旨在解决现有文本到音频（TTA）系统在生成长篇叙事音频时面临的时间连贯性和组合推理挑战。它通过整合大型语言模型（LLMs）与TTA系统，将复杂叙事分解为有序子任务，确保场景过渡和情感一致性。AudioStory采用解耦桥接机制和端到端训练，显著提升了指令遵循能力和音频保真度，在长篇音频生成方面超越了现有基线。代码已开源。",
      "translated_title": "AudioStory：使用大型语言模型生成长篇叙事音频",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in text-to-audio (TTA) generation excel at synthesizing short audio clips but struggle with long-form narrative audio, which requires temporal coherence and compositional reasoning. To address this gap, we propose AudioStory, a unified framework that integrates large language models (LLMs) with TTA systems to generate structured, long-form audio narratives. AudioStory possesses strong instruction-following reasoning generation capabilities. It employs LLMs to decompose complex narrative queries into temporally ordered sub-tasks with contextual cues, enabling coherent scene transitions and emotional tone consistency. AudioStory has two appealing features: (1) Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser collaboration into two specialized components, i.e., a bridging query for intra-event semantic alignment and a residual query for cross-event coherence preservation. (2) End-to-end training: By unifying instruction comprehension and audio generation within a single end-to-end framework, AudioStory eliminates the need for modular training pipelines while enhancing synergy between components. Furthermore, we establish a benchmark AudioStory-10K, encompassing diverse domains such as animated soundscapes and natural sound narratives. Extensive experiments show the superiority of AudioStory on both single-audio generation and narrative audio generation, surpassing prior TTA baselines in both instruction-following ability and audio fidelity. Our code is available at https://github.com/TencentARC/AudioStory"
    },
    {
      "title": "离散扩散VLA：将离散扩散引入视觉-语言-动作策略中的动作解码 (原标题: Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies)",
      "link": "https://arxiv.org/abs/2508.20072",
      "pubDate": "Wed, 27 Aug 2025 13:39:11 GMT",
      "isoDate": "2025-08-27T13:39:11.000Z",
      "creator": "Zhixuan Liang, Yizhuo Li, Tianshuo Yang, Chengyue Wu, Sitong Mao, Liuao Pei, Xiaokang Yang, Jiangmiao Pang, Yao Mu, Ping Luo",
      "summary": "# 离散扩散VLA：将离散扩散引入视觉-语言-动作策略中的动作解码\n\n## 引言与背景\n视觉-语言-动作（VLA）模型旨在利用大型视觉-语言骨干网络，将图像和指令映射到机器人动作。然而，当前主流的VLA解码器存在以下局限性：\n*   **自回归解码器**：通常以固定的从左到右顺序生成动作，效率低下。\n*   **连续扩散或流匹配解码器**：需要将专门的头部附加到骨干网络之外，这要求专门的训练和迭代采样，阻碍了统一且可扩展的架构的实现。\n\n## 提出的方法：离散扩散VLA\n本文提出了一种名为“离散扩散VLA”（Discrete Diffusion VLA）的新型策略。它是一个**单一的Transformer策略**，通过**离散扩散**对离散化的动作块进行建模。\n\n### 核心设计与特点\n1.  **统一的训练目标**：离散扩散VLA与VLM骨干网络采用相同的**交叉熵目标**进行训练，实现了训练范式的统一。\n2.  **原生兼容性**：该设计保留了扩散模型的**渐进式细化范式**，同时与VLM的离散令牌接口实现原生兼容。\n3.  **自适应解码顺序**：该方法能够实现**自适应的解码顺序**，优先解决较简单的动作元素，然后再处理较难的元素。\n4.  **二次重掩码（Secondary Remasking）**：通过在细化轮次中对不确定的预测进行**二次重掩码**，该方法能够提高一致性并实现鲁棒的错误校正。\n\n### 优势\n离散扩散VLA的统一解码器带来了多项显著优势：\n*   **保留预训练的视觉语言先验**：有效利用了VLM骨干网络的强大能力。\n*   **支持并行解码**：打破了传统自回归模型的瓶颈，提高了效率。\n*   **减少函数评估次数**：进一步优化了计算资源的使用。\n\n## 实验结果\n离散扩散VLA在多个基准测试中取得了优异的性能，超越了现有的自回归和连续扩散基线：\n*   **LIBERO**：平均成功率（SR）达到 **96.3%**。\n*   **SimplerEnv Fractal**：视觉匹配率达到 **71.2%**。\n*   **SimplerEnv Bridge**：总体性能达到 **49.3%**。\n\n## 结论与未来展望\n这些研究结果表明，离散扩散动作解码器支持**精确的动作建模**和**一致的训练**。这为将VLA模型扩展到更大的模型和数据集奠定了基础。",
      "shortSummary": "离散扩散VLA提出了一种统一的Transformer策略，将离散扩散引入视觉-语言-动作（VLA）策略中的动作解码。它解决了现有VLA解码器（自回归或连续扩散）效率低、训练复杂的问题。该方法采用与VLM骨干网络相同的交叉熵目标训练，支持自适应解码顺序和二次重掩码进行错误校正。离散扩散VLA实现了并行解码，保留了预训练先验，并在LIBERO和SimplerEnv等基准测试中取得了显著优于现有方法的性能，为VLA模型的扩展奠定了基础。",
      "translated_title": "离散扩散VLA：将离散扩散引入视觉-语言-动作策略中的动作解码",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions to robot actions. However, prevailing VLA decoders either generate actions autoregressively in a fixed left-to-right order or attach continuous diffusion or flow matching heads outside the backbone, demanding specialized training and iterative sampling that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a single-transformer policy that models discretized action chunks with discrete diffusion and is trained with the same cross-entropy objective as the VLM backbone. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary remasking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pretrained vision language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv Bridge, improving over both autoregressive and continuous diffusion baselines. These findings indicate that discrete-diffusion action decoder supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets."
    },
    {
      "title": "DeepScholar-Bench：一个用于生成式研究综合的实时基准和自动化评估框架 (原标题: DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis)",
      "link": "https://arxiv.org/abs/2508.20033",
      "pubDate": "Wed, 27 Aug 2025 12:36:34 GMT",
      "isoDate": "2025-08-27T12:36:34.000Z",
      "creator": "Liana Patel, Negar Arabzadeh, Harshit Gupta, Ankita Sundar, Ion Stoica, Matei Zaharia, Carlos Guestrin",
      "summary": "## DeepScholar-Bench：生成式研究综合的实时基准与自动化评估\n\n### 背景与挑战\n\n研究和综合知识的能力是人类专业知识和进步的核心。新兴的生成式研究综合系统承诺通过检索实时网络并综合发现的来源，生成长篇、带引用的摘要，从而实现这些激动人心的能力。然而，评估此类系统仍然是一个开放的挑战：\n\n*   **现有基准的局限性**：当前的问答基准侧重于简短的事实性回答。\n*   **数据过时与污染**：专家策划的数据集存在过时和数据污染的风险。\n*   **缺乏复杂性**：上述方法都未能捕捉真实研究综合任务的复杂性和演变性。\n\n### DeepScholar-Bench 的引入\n\n为了解决这些挑战，本研究引入了 **DeepScholar-Bench**，这是一个实时基准和全面的自动化评估框架，专门设计用于评估生成式研究综合系统。\n\n### DeepScholar-Bench 的核心特点与方法\n\n1.  **查询来源**：DeepScholar-Bench 从近期高质量的 ArXiv 论文中提取查询。\n2.  **真实任务聚焦**：它专注于一个真实的研究综合任务——通过检索、综合和引用现有研究来生成论文的“相关工作”部分。\n3.  **全面评估维度**：该评估框架从三个关键维度全面评估系统性能：\n    *   **知识综合**：评估系统整合和提炼信息的能力。\n    *   **检索质量**：评估系统查找相关和高质量信息的能力。\n    *   **可验证性**：评估生成内容的事实准确性和来源可追溯性。\n\n### DeepScholar-Base 参考管道\n\n研究团队还开发了 **DeepScholar-Base**，这是一个使用 LOTUS API 高效实现的参考管道，作为评估的基准系统。\n\n### 系统评估与发现\n\n利用 DeepScholar-Bench 框架，研究人员对以下系统进行了系统评估：\n\n*   现有的开源系统\n*   搜索 AI\n*   OpenAI 的 DeepResearch\n*   DeepScholar-Base\n\n评估结果显示：\n\n*   **DeepScholar-Base 表现强劲**：DeepScholar-Base 建立了一个强大的基线，其性能与所有其他方法相比具有竞争力或更高。\n*   **基准尚未饱和**：没有系统在所有指标上的得分超过 19%。\n\n### 结论与意义\n\n这些结果凸显了 DeepScholar-Bench 的难度，以及其对于推动能够进行生成式研究综合的 AI 系统发展的重要性。研究团队已将代码开源。",
      "shortSummary": "DeepScholar-Bench是一个新的实时基准和自动化评估框架，旨在解决生成式研究综合系统评估的挑战。它通过从ArXiv论文中提取查询，并专注于生成论文的“相关工作”部分，来评估知识综合、检索质量和可验证性。初步评估显示，DeepScholar-Base建立了一个强大的基线，但所有系统性能均未超过19%，这突显了该基准的难度及其对AI系统进展的重要性。代码已开源。",
      "translated_title": "DeepScholar-Bench：一个用于生成式研究综合的实时基准和自动化评估框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "The ability to research and synthesize knowledge is central to human expertise and progress. An emerging class of systems promises these exciting capabilities through generative research synthesis, performing retrieval over the live web and synthesizing discovered sources into long-form, cited summaries. However, evaluating such systems remains an open challenge: existing question-answering benchmarks focus on short-form factual responses, while expert-curated datasets risk staleness and data contamination. Both fail to capture the complexity and evolving nature of real research synthesis tasks. In this work, we introduce DeepScholar-bench, a live benchmark and holistic, automated evaluation framework designed to evaluate generative research synthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv papers and focuses on a real research synthesis task: generating the related work sections of a paper by retrieving, synthesizing, and citing prior research. Our evaluation framework holistically assesses performance across three key dimensions, knowledge synthesis, retrieval quality, and verifiability. We also develop DeepScholar-base, a reference pipeline implemented efficiently using the LOTUS API. Using the DeepScholar-bench framework, we perform a systematic evaluation of prior open-source systems, search AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that DeepScholar-base establishes a strong baseline, attaining competitive or higher performance than each other method. We also find that DeepScholar-bench remains far from saturated, with no system exceeding a score of 19% across all metrics. These results underscore the difficulty of DeepScholar-bench, as well as its importance for progress towards AI systems capable of generative research synthesis. We make our code available at https://github.com/guestrin-lab/deepscholar-bench."
    },
    {
      "title": "扩散语言模型在解码前已知答案 (原标题: Diffusion Language Models Know the Answer Before Decoding)",
      "link": "https://arxiv.org/abs/2508.19982",
      "pubDate": "Wed, 27 Aug 2025 11:40:25 GMT",
      "isoDate": "2025-08-27T11:40:25.000Z",
      "creator": "Pengxiang Li, Yefan Zhou, Dilxat Muhtar, Lu Yin, Shilin Yan, Li Shen, Yi Liang, Soroush Vosoughi, Shiwei Liu",
      "summary": "## 扩散语言模型：解码前已知答案\n\n### 引言\n\n扩散语言模型（DLMs）作为自回归方法的替代方案，近年来崭露头角，其优势在于支持并行序列生成和灵活的词元顺序。然而，DLMs的推理速度通常慢于自回归模型，这主要是因为双向注意力机制的计算成本以及为获得高质量输出所需的大量精炼步骤。\n\n### 核心发现：早期答案收敛\n\n本研究强调并利用了DLMs一个常被忽视的特性——**早期答案收敛**。研究发现，在许多情况下，DLMs在最终解码步骤之前，仅通过一半的精炼步骤，就能在内部识别出正确的答案。这一现象在半自回归和随机重掩码调度下均成立。\n\n*   **实例表现**：\n    *   在GSM8K数据集上，高达97%的实例仅用一半的精炼步骤即可正确解码。\n    *   在MMLU数据集上，高达99%的实例仅用一半的精炼步骤即可正确解码。\n\n### Prophet：一种无需训练的快速解码范式\n\n基于上述观察，研究人员引入了**Prophet**，这是一种无需额外训练的快速解码范式，旨在利用早期答案收敛特性实现“提前提交解码”（early commit decoding）。\n\n*   **工作原理**：\n    *   Prophet动态决定是继续精炼还是“全力以赴”（即在一个步骤中解码所有剩余词元）。\n    *   其决策标准是前两个预测候选之间的置信度差距。\n*   **优势**：\n    *   无缝集成到现有DLM实现中。\n    *   引入的开销可忽略不计。\n    *   无需额外的训练。\n\n### 实验评估与结果\n\n研究人员在多个任务上对LLaDA-8B和Dream-7B模型进行了实证评估，结果表明：\n\n*   Prophet将解码步骤数量减少了高达3.4倍。\n*   同时，它保持了高质量的生成效果。\n\n### 结论与展望\n\n这些结果重新定义了DLM解码问题，将其视为一个“何时停止采样”的问题。研究表明，早期解码收敛为加速DLM推理提供了一种简单而强大的机制，并且可以与现有加速技术互补。该研究的代码已公开提供。",
      "shortSummary": "扩散语言模型（DLMs）虽能并行生成，但推理速度受限于大量精炼步骤。本研究发现DLMs在最终解码前已能识别正确答案，即存在“早期答案收敛”现象。基于此，提出无训练的Prophet范式，它通过动态评估预测置信度，决定何时提前完成解码。实验表明，Prophet可将DLM解码步数减少高达3.4倍，同时保持高质量输出。这为加速DLM推理提供了一种简单而强大的机制，将解码问题转化为何时停止采样。",
      "translated_title": "扩散语言模型在解码前已知答案",
      "images": [],
      "contentSource": "完整文章",
      "content": "Diffusion language models (DLMs) have recently emerged as an alternative to autoregressive approaches, offering parallel sequence generation and flexible token orders. However, their inference remains slower than that of autoregressive models, primarily due to the cost of bidirectional attention and the large number of refinement steps required for high quality outputs. In this work, we highlight and leverage an overlooked property of DLMs early answer convergence: in many cases, the correct answer can be internally identified by half steps before the final decoding step, both under semi-autoregressive and random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99% of instances, respectively, can be decoded correctly using only half of the refinement steps. Building on this observation, we introduce Prophet, a training-free fast decoding paradigm that enables early commit decoding. Specifically, Prophet dynamically decides whether to continue refinement or to go \"all-in\" (i.e., decode all remaining tokens in one step), using the confidence gap between the top-2 prediction candidates as the criterion. It integrates seamlessly into existing DLM implementations, incurs negligible overhead, and requires no additional training. Empirical evaluations of LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the number of decoding steps by up to 3.4x while preserving high generation quality. These results recast DLM decoding as a problem of when to stop sampling, and demonstrate that early decode convergence provides a simple yet powerful mechanism for accelerating DLM inference, complementary to existing speedup techniques. Our code is publicly available at https://github.com/pixeli99/Prophet."
    },
    {
      "title": "通过推理分解实现自奖励视觉-语言模型 (原标题: Self-Rewarding Vision-Language Model via Reasoning Decomposition)",
      "link": "https://arxiv.org/abs/2508.19652",
      "pubDate": "Wed, 27 Aug 2025 04:01:03 GMT",
      "isoDate": "2025-08-27T04:01:03.000Z",
      "creator": "Zongxia Li, Wenhao Yu, Chengsong Huang, Rui Liu, Zhenwen Liang, Fuxiao Liu, Jingxi Che, Dian Yu, Jordan Boyd-Graber, Haitao Mi, Dong Yu",
      "summary": "## 通过推理分解实现自奖励视觉-语言模型 (Vision-SR1)\n\n### 1. 视觉-语言模型 (VLM) 面临的问题\n\n*   **视觉幻觉 (Visual Hallucinations)**：VLM 描述图像中不存在的事物。\n*   **语言捷径 (Language Shortcuts)**：VLM 跳过视觉部分，仅依赖文本先验进行推理。\n*   **根本原因**：大多数 VLM 后训练方法仅监督最终输出，导致中间视觉推理缺乏明确指导。VLM 接收到的视觉信号稀疏，并倾向于优先考虑基于语言的推理。\n\n### 2. 现有解决方案及其局限性\n\n*   **方法**：通过人工标注或从外部大型模型蒸馏标签来增加视觉监督。\n*   **局限性**：\n    *   **人工标注**：劳动密集且成本高昂。\n    *   **外部信号**：无法适应不断变化的策略，可能导致分布偏移和奖励欺骗。\n\n### 3. Vision-SR1 方法介绍\n\n*   **核心思想**：Vision-SR1 是一种自奖励方法，通过强化学习在不依赖外部视觉监督的情况下改进视觉推理。\n*   **目标**：解决 VLM 的视觉幻觉和语言捷径问题。\n\n### 4. Vision-SR1 的推理分解过程\n\nVision-SR1 将 VLM 的推理过程分解为两个阶段：\n\n1.  **视觉感知阶段**：\n    *   模型被提示生成“自包含的视觉感知”。\n    *   这些感知必须足够充分，以便在不再次参考原始输入图像的情况下回答问题。\n2.  **语言推理阶段**：\n    *   相同的 VLM 模型随后被重新提示。\n    *   它仅使用第一阶段生成的视觉感知作为输入进行语言推理，并计算一个“自奖励”。\n\n### 5. 训练信号\n\n*   Vision-SR1 将这种“自奖励”与对最终输出的监督相结合。\n*   这提供了一个平衡的训练信号，能够同时增强模型的视觉感知能力和语言推理能力。\n\n### 6. 实验结果\n\n*   实验证明，Vision-SR1 在各种视觉-语言任务中显著改进了视觉推理能力。\n*   它有效缓解了视觉幻觉问题。\n*   成功减少了 VLM 对语言捷径的依赖。",
      "shortSummary": "视觉-语言模型（VLMs）常受视觉幻觉和语言捷径困扰，因其视觉信号稀疏且缺乏中间推理指导。为解决此问题，本文提出 Vision-SR1，一种自奖励方法。它通过强化学习，将VLM推理分解为视觉感知和语言推理两阶段。模型首先生成自包含的视觉感知，然后利用这些感知进行语言推理并计算自奖励。结合最终输出监督，Vision-SR1有效提升了视觉推理能力，缓解了视觉幻觉，并减少了对语言捷径的依赖，无需外部视觉监督。",
      "translated_title": "通过推理分解实现自奖励视觉-语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-Language Models (VLMs) often suffer from visual hallucinations, saying things that are not actually in the image, and language shortcuts, where they skip the visual part and just rely on text priors. These issues arise because most post-training methods for VLMs rely on simple verifiable answer matching and supervise only final outputs, leaving intermediate visual reasoning without explicit guidance. As a result, VLMs receive sparse visual signals and often learn to prioritize language-based reasoning over visual perception. To mitigate this, some existing methods add visual supervision using human annotations or distilled labels from external large models. However, human annotations are labor-intensive and costly, and because external signals cannot adapt to the evolving policy, they cause distributional shifts that can lead to reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method that improves visual reasoning without relying on external visual supervisions via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two stages: visual perception and language reasoning. The model is first prompted to produce self-contained visual perceptions that are sufficient to answer the question without referring back the input image. To validate this self-containment, the same VLM model is then re-prompted to perform language reasoning using only the generated perception as input to compute reward. This self-reward is combined with supervision on final outputs, providing a balanced training signal that strengthens both visual perception and language reasoning. Our experiments demonstrate that Vision-SR1 improves visual reasoning, mitigates visual hallucinations, and reduces reliance on language shortcuts across diverse vision-language tasks."
    },
    {
      "title": "驯服混沌：异构和解耦LLM推理的协同自动扩缩容 (原标题: Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference)",
      "link": "https://arxiv.org/abs/2508.19559",
      "pubDate": "Wed, 27 Aug 2025 00:22:02 GMT",
      "isoDate": "2025-08-27T00:22:02.000Z",
      "creator": "Rongzhi Li, Ruogu Du, Zefang Chu, Sida Zhao, Chunlei Han, Zuocheng Shi, Yiwen Shao, Huanle Han, Long Huang, Zherui Liu, Shufan Liu",
      "summary": "## 驯服混沌：异构和解耦LLM推理的协同自动扩缩容\n\n### 背景与挑战\n\n大型语言模型（LLM）的服务是一项GPU密集型任务，而传统的自动扩缩容（autoscaling）方案在处理现代预填充-解码（Prefill-Decode, P/D）解耦架构时显得力不从心。尽管P/D解耦架构功能强大，但它引入了显著的运营挑战，主要包括：\n\n*   **异构硬件利用效率低下：** 难以有效利用不同类型的GPU资源。\n*   **网络瓶颈：** 数据传输和通信可能成为性能限制因素。\n*   **预填充和解码阶段的关键不平衡：** 这两个阶段的需求动态变化，难以协调资源分配。\n\n### HeteroScale框架：解决方案\n\n为了应对P/D解耦服务中的核心挑战，研究人员引入了**HeteroScale**，一个协同自动扩缩容框架。HeteroScale通过以下两个核心组件协同工作：\n\n1.  **拓扑感知调度器：** 该调度器能够适应异构硬件和网络限制，确保资源分配与底层基础设施的特性相匹配。\n2.  **新颖的度量驱动策略：** 该策略源自对生产环境中自动扩缩容信号的首次大规模实证研究，提供了一个强大且可靠的决策依据。\n\n通过利用一个单一、稳健的度量指标，HeteroScale能够共同扩缩容预填充和解码资源池。这种协调方法在确保高效、自适应资源管理的同时，维持了架构的平衡。\n\n### 部署与成效\n\nHeteroScale已在一个拥有数万个GPU的超大规模生产环境中成功部署，并证明了其卓越的有效性：\n\n*   **GPU利用率显著提升：** 平均GPU利用率提高了26.6个百分点。\n*   **大量资源节省：** 每天节省了数十万GPU小时。\n*   **服务水平目标（SLO）的严格遵守：** 在提高效率的同时，确保了服务质量。",
      "shortSummary": "HeteroScale是一个针对异构和解耦LLM推理的协同自动扩缩容框架。它解决了传统方案在P/D架构下效率低下、资源不平衡等问题。通过结合拓扑感知调度器和度量驱动策略，HeteroScale能共同扩缩容预填充和解码资源池，维持架构平衡。在生产环境中，它将GPU利用率提高了26.6%，每天节省数十万GPU小时，同时满足服务水平目标。",
      "translated_title": "驯服混沌：异构和解耦LLM推理的协同自动扩缩容",
      "images": [],
      "contentSource": "完整文章",
      "content": "Serving Large Language Models (LLMs) is a GPU-intensive task where traditional autoscalers fall short, particularly for modern Prefill-Decode (P/D) disaggregated architectures. This architectural shift, while powerful, introduces significant operational challenges, including inefficient use of heterogeneous hardware, network bottlenecks, and critical imbalances between prefill and decode stages. We introduce HeteroScale, a coordinated autoscaling framework that addresses the core challenges of P/D disaggregated serving. HeteroScale combines a topology-aware scheduler that adapts to heterogeneous hardware and network constraints with a novel metric-driven policy derived from the first large-scale empirical study of autoscaling signals in production. By leveraging a single, robust metric to jointly scale prefill and decode pools, HeteroScale maintains architectural balance while ensuring efficient, adaptive resource management. Deployed in a massive production environment on tens of thousands of GPUs, HeteroScale has proven its effectiveness, increasing average GPU utilization by a significant 26.6 percentage points and saving hundreds of thousands of GPU-hours daily, all while upholding stringent service level objectives."
    },
    {
      "title": "MotionFlux：通过整流流匹配和偏好对齐实现高效文本引导的运动生成 (原标题: MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment)",
      "link": "https://arxiv.org/abs/2508.19527",
      "pubDate": "Tue, 26 Aug 2025 22:45:09 GMT",
      "isoDate": "2025-08-26T22:45:09.000Z",
      "creator": "Zhiting Gao, Dan Song, Diqiong Jiang, Chao Xue, An-An Liu",
      "summary": "## MotionFlux：通过整流流匹配和偏好对齐实现高效文本引导的运动生成\n\n### 摘要\n\n文本驱动的运动生成对于虚拟角色和具身智能体的动画至关重要。然而，现有方法在以下两个方面面临挑战：\n\n*   **语义对齐不足：** 语言描述与运动语义之间难以实现精确对齐。\n*   **推理效率低下：** 缓慢的多步推理过程导致生成效率不高。\n\n### 提出的解决方案\n\n为了解决这些问题，本文引入了一个创新的统一框架，包含两个核心组件：\n\n1.  **TAPO (TMR++ Aligned Preference Optimization)：**\n    *   **目标：** 解决文本与运动语义的精确对齐问题。\n    *   **机制：** 旨在将细微的运动变化与文本修饰符对齐，并通过迭代调整来强化语义基础。\n\n2.  **MotionFLUX：**\n    *   **目标：** 实现实时运动合成，解决推理效率低下的问题。\n    *   **机制：** 这是一个基于确定性整流流匹配（deterministic rectified flow matching）的高速生成框架。\n    *   **与传统扩散模型的区别：** 不同于需要数百个去噪步骤的传统扩散模型，MotionFLUX在噪声分布和运动空间之间构建了最优传输路径。\n    *   **优势：** 线性化的概率路径减少了序列方法中典型的多步采样需求，显著加速了推理时间，同时不牺牲运动质量。\n\n### 实验结果\n\n*   实验结果表明，TAPO和MotionFLUX共同构成了一个统一的系统。\n*   该系统在语义一致性和运动质量方面均优于现有最先进的方法。\n*   同时，显著提升了生成速度。\n\n### 未来展望\n\n代码和预训练模型将发布。",
      "shortSummary": "本文提出了MotionFLUX框架，通过结合TMR++对齐偏好优化（TAPO）和确定性整流流匹配，解决了文本引导运动生成中语义对齐不精确和推理速度慢的问题。TAPO强化了文本与运动语义的对齐，而MotionFLUX则通过构建最优传输路径，实现高速、实时的运动合成，避免了传统扩散模型的多步采样。实验证明，该系统在语义一致性、运动质量和生成速度上均超越了现有技术。",
      "translated_title": "MotionFlux：通过整流流匹配和偏好对齐实现高效文本引导的运动生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Motion generation is essential for animating virtual characters and embodied agents. While recent text-driven methods have made significant strides, they often struggle with achieving precise alignment between linguistic descriptions and motion semantics, as well as with the inefficiencies of slow, multi-step inference. To address these issues, we introduce TMR++ Aligned Preference Optimization (TAPO), an innovative framework that aligns subtle motion variations with textual modifiers and incorporates iterative adjustments to reinforce semantic grounding. To further enable real-time synthesis, we propose MotionFLUX, a high-speed generation framework based on deterministic rectified flow matching. Unlike traditional diffusion models, which require hundreds of denoising steps, MotionFLUX constructs optimal transport paths between noise distributions and motion spaces, facilitating real-time synthesis. The linearized probability paths reduce the need for multi-step sampling typical of sequential methods, significantly accelerating inference time without sacrificing motion quality. Experimental results demonstrate that, together, TAPO and MotionFLUX form a unified system that outperforms state-of-the-art approaches in both semantic consistency and motion quality, while also accelerating generation speed. The code and pretrained models will be released."
    },
    {
      "title": "小心第三只眼！基准测试MLLM驱动的智能手机代理的隐私意识 (原标题: Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents)",
      "link": "https://arxiv.org/abs/2508.19493",
      "pubDate": "Tue, 26 Aug 2025 20:41:28 GMT",
      "isoDate": "2025-08-26T20:41:28.000Z",
      "creator": "Zhixin Lin, Jungang Li, Shidong Pan, Yibo Shi, Yue Yao, Dongliang Xu",
      "summary": "## MLLM驱动智能手机代理的隐私意识基准测试\n\n### 引言\n\n智能手机在为用户带来巨大便利的同时，也广泛记录着各类个人信息。当前由多模态大语言模型（MLLM）驱动的智能手机代理在自动化任务方面表现出色，但其操作过程中也获得了对用户敏感个人信息的广泛访问权限，这带来了潜在的隐私风险。\n\n### 研究目的与贡献\n\n为了全面理解这些代理的隐私意识水平，本研究首次提出了一个大规模的基准测试，包含了7,138个场景。这是迄今为止已知规模最大的同类基准测试。\n\n### 研究方法\n\n1.  **场景标注**：研究人员对基准测试中的每个隐私上下文场景进行了详细标注，包括其类型（例如，账户凭证）、敏感度级别和位置信息。\n2.  **代理测试**：研究人员对七个主流的智能手机代理进行了基准测试。\n\n### 主要发现\n\n*   **普遍隐私意识不足**：几乎所有被测试的代理都表现出不尽如人意的隐私意识（RA），即使在提供明确提示的情况下，其性能仍低于60%。\n*   **闭源代理表现优于开源代理**：总体而言，闭源代理在隐私能力方面表现优于开源代理。\n*   **最佳表现者**：Gemini 2.0-flash在所有测试代理中表现最佳，其隐私意识（RA）达到了67%。\n*   **敏感度与检测能力相关**：代理的隐私检测能力与场景的敏感度级别高度相关，即敏感度越高的场景通常越容易被识别。\n\n### 研究意义\n\n本研究的发现旨在启发研究社区重新思考智能手机代理在效用和隐私之间不平衡的权衡问题。\n\n### 资源可用性\n\n本研究的代码和基准测试已公开提供。",
      "shortSummary": "一项针对MLLM驱动智能手机代理隐私意识的大规模基准测试（涵盖7,138个场景）发现，大多数代理的隐私意识（RA）表现不佳，即使有明确提示也低于60%。闭源代理通常优于开源代理，Gemini 2.0-flash以67%的RA表现最佳。研究还指出，隐私检测能力与场景敏感度高度相关。该研究旨在促使社区重新思考智能手机代理的效用与隐私权衡。",
      "translated_title": "小心第三只眼！基准测试MLLM驱动的智能手机代理的隐私意识",
      "images": [],
      "contentSource": "完整文章",
      "content": "Smartphones bring significant convenience to users but also enable devices to extensively record various types of personal information. Existing smartphone agents powered by Multimodal Large Language Models (MLLMs) have achieved remarkable performance in automating different tasks. However, as the cost, these agents are granted substantial access to sensitive users' personal information during this operation. To gain a thorough understanding of the privacy awareness of these agents, we present the first large-scale benchmark encompassing 7,138 scenarios to the best of our knowledge. In addition, for privacy context in scenarios, we annotate its type (e.g., Account Credentials), sensitivity level, and location. We then carefully benchmark seven available mainstream smartphone agents. Our results demonstrate that almost all benchmarked agents show unsatisfying privacy awareness (RA), with performance remaining below 60% even with explicit hints. Overall, closed-source agents show better privacy ability than open-source ones, and Gemini 2.0-flash achieves the best, achieving an RA of 67%. We also find that the agents' privacy detection capability is highly related to scenario sensitivity level, i.e., the scenario with a higher sensitivity level is typically more identifiable. We hope the findings enlighten the research community to rethink the unbalanced utility-privacy tradeoff about smartphone agents. Our code and benchmark are available at https://zhixin-l.github.io/SAPA-Bench."
    },
    {
      "title": "VoxHammer：在原生3D空间中进行免训练的精确连贯3D编辑 (原标题: VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space)",
      "link": "https://arxiv.org/abs/2508.19247",
      "pubDate": "Tue, 26 Aug 2025 13:59:47 GMT",
      "isoDate": "2025-08-26T13:59:47.000Z",
      "creator": "Lin Li, Zehuan Huang, Haoran Feng, Gengxiong Zhuang, Rui Chen, Chunchao Guo, Lu Sheng",
      "summary": "# VoxHammer：在原生3D空间中进行免训练的精确连贯3D编辑\n\n## 摘要\n\n本文介绍了VoxHammer，一种新颖的免训练方法，旨在解决3D局部编辑中现有方法面临的挑战，即难以精确保留未编辑区域并保持整体连贯性。VoxHammer受结构化3D生成模型的启发，通过在原生3D潜在空间中直接进行编辑，实现了对指定区域的精确且连贯的修改。\n\n## 核心问题\n\n*   **3D局部编辑的重要性**：在游戏产业和机器人交互中，对3D模型的特定区域进行局部编辑至关重要。\n*   **现有方法的局限性**：\n    *   通常通过编辑渲染的多视图图像，然后重建3D模型。\n    *   面临的主要挑战是难以精确地保留未编辑区域，并确保编辑后的模型整体连贯性。\n\n## VoxHammer 方法\n\nVoxHammer提出了一种在3D潜在空间中进行编辑的训练无关方法，其核心流程分为两个阶段：\n\n### 1. 反演阶段 (Inversion)\n\n*   **输入**：一个给定的3D模型。\n*   **过程**：VoxHammer首先预测该模型的反演轨迹。\n*   **输出**：在每个时间步，获取其反演的潜在变量（inverted latents）和键值token（key-value tokens）。\n\n### 2. 去噪与编辑阶段 (Denoising and Editing)\n\n*   **核心操作**：在此阶段，VoxHammer将保留区域的去噪特征替换为在反演阶段获得的相应反演潜在变量和缓存的键值token。\n*   **机制**：通过保留这些上下文特征，VoxHammer能够确保：\n    *   未编辑的保留区域能够一致地重建。\n    *   编辑过的部分能够与模型其余部分连贯地集成。\n\n## 评估与实验\n\n*   **数据集**：为了评估保留区域的一致性，研究团队构建了**Edit3D-Bench**数据集。\n    *   这是一个人工标注的数据集，包含数百个样本。\n    *   每个样本都仔细标注了3D编辑区域。\n*   **实验结果**：实验证明，VoxHammer在以下两个关键指标上显著优于现有方法：\n    *   保留区域的3D一致性。\n    *   整体编辑质量。\n\n## 潜在影响与未来展望\n\n*   VoxHammer方法有望用于合成高质量的编辑配对数据。\n*   这将为上下文3D生成（in-context 3D generation）奠定坚实的数据基础。\n\n## 项目页面\n\n更多详情请访问项目页面：this https URL",
      "shortSummary": "VoxHammer是一种免训练的3D编辑方法，旨在解决现有方法在保留区域一致性和整体连贯性方面的挑战。它通过在3D潜在空间中直接编辑，利用反演潜在变量和键值token精确保留未编辑区域，并确保编辑部分的连贯集成。实验表明，VoxHammer在3D一致性和整体质量上显著优于现有方法，为高质量3D编辑和上下文3D生成提供了新途径。",
      "translated_title": "VoxHammer：在原生3D空间中进行免训练的精确连贯3D编辑",
      "images": [],
      "contentSource": "完整文章",
      "content": "3D local editing of specified regions is crucial for game industry and robot interaction. Recent methods typically edit rendered multi-view images and then reconstruct 3D models, but they face challenges in precisely preserving unedited regions and overall coherence. Inspired by structured 3D generative models, we propose VoxHammer, a novel training-free approach that performs precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer first predicts its inversion trajectory and obtains its inverted latents and key-value tokens at each timestep. Subsequently, in the denoising and editing phase, we replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, this approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. To evaluate the consistency of preserved regions, we constructed Edit3D-Bench, a human-annotated dataset comprising hundreds of samples, each with carefully labeled 3D editing regions. Experiments demonstrate that VoxHammer significantly outperforms existing methods in terms of both 3D consistency of preserved regions and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. See our project page at https://huanngzh.github.io/VoxHammer-Page/."
    },
    {
      "title": "自回归通用视频分割模型 (原标题: Autoregressive Universal Video Segmentation Model)",
      "link": "https://arxiv.org/abs/2508.19242",
      "pubDate": "Tue, 26 Aug 2025 13:59:13 GMT",
      "isoDate": "2025-08-26T13:59:13.000Z",
      "creator": "Miran Heo, Sukjun Hwang, Min-Hung Chen, Yu-Chiang Frank Wang, Albert Gu, Seon Joo Kim, Ryo Hachiuma",
      "summary": "## 自回归通用视频分割模型（AUSM）\n\n### 背景与挑战\n\n*   **现有问题：** 尽管像SAM2这样的最新视频基础模型在提示式视频分割（即根据外部提示进行分割）方面表现出色，但许多实际应用场景需要无提示分割。无提示分割的目标是在没有外部线索的情况下，自动检测并跟踪视频中的所有对象。\n*   **当前局限：** 目前的无提示视频分割领域被各种任务特定模型和管道所碎片化，缺乏一个统一的解决方案。\n\n### AUSM的核心贡献\n\n*   **统一架构：** 本文引入了自回归通用分割模型（Autoregressive Universal Segmentation Model, AUSM），这是一种单一架构，能够同时处理提示式和无提示式视频分割任务。\n*   **创新方法：** AUSM将流式视频分割重新定义为序列掩码预测问题，其概念类似于语言建模中的序列预测。\n\n### AUSM的技术特点\n\n1.  **基于状态空间模型：** AUSM构建在最新的状态空间模型之上，这使其能够有效地处理序列数据。\n2.  **固定大小空间状态：** 模型维护一个固定大小的空间状态，使其能够扩展到任意长度的视频流，而不会因视频长度增加而导致计算复杂度急剧上升。\n3.  **并行训练设计：** AUSM的所有组件都设计为支持跨帧并行训练，这与传统的迭代训练方法相比，显著提高了训练速度。\n\n### 性能表现\n\n*   **超越现有方法：** 在多个标准基准测试（包括DAVIS17、YouTube-VOS 2018 & 2019、MOSE、YouTube-VIS 2019 & 2021以及OVIS）上，AUSM的性能均优于先前的通用流式视频分割方法。\n*   **训练效率提升：** 在16帧序列的训练中，AUSM的训练速度最高可提高2.5倍，显示出其在效率方面的显著优势。",
      "shortSummary": "自回归通用分割模型（AUSM）提出了一种统一架构，将流式视频分割重新定义为序列掩码预测，从而整合了提示式和无提示式视频分割。AUSM基于状态空间模型，维护固定大小空间状态，并支持跨帧并行训练，有效处理任意长度视频流。它在多个标准基准测试中超越了现有通用方法，并显著提高了训练速度，为视频理解提供了一个高效且通用的解决方案。",
      "translated_title": "自回归通用视频分割模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent video foundation models such as SAM2 excel at prompted video segmentation by treating masks as a general-purpose primitive. However, many real-world settings require unprompted segmentation that aims to detect and track all objects in a video without external cues, leaving today's landscape fragmented across task-specific models and pipelines. We recast streaming video segmentation as sequential mask prediction, analogous to language modeling, and introduce the Autoregressive Universal Segmentation Model (AUSM), a single architecture that unifies both prompted and unprompted video segmentation. Built on recent state-space models, AUSM maintains a fixed-size spatial state and scales to video streams of arbitrary length. Furthermore, all components of AUSM are designed for parallel training across frames, yielding substantial speedups over iterative training. On standard benchmarks (DAVIS17, YouTube-VOS 2018 &amp; 2019, MOSE, YouTube-VIS 2019 &amp; 2021, and OVIS) AUSM outperforms prior universal streaming video segmentation methods and achieves up to 2.5x faster training on 16-frame sequences."
    },
    {
      "title": "StepWiser：用于更明智推理的逐步生成式评判器 (原标题: StepWiser: Stepwise Generative Judges for Wiser Reasoning)",
      "link": "https://arxiv.org/abs/2508.19229",
      "pubDate": "Tue, 26 Aug 2025 13:45:05 GMT",
      "isoDate": "2025-08-26T13:45:05.000Z",
      "creator": "Wei Xiong, Wenting Zhao, Weizhe Yuan, Olga Golovneva, Tong Zhang, Jason Weston, Sainbayar Sukhbaatar",
      "summary": "# StepWiser：用于更明智推理的逐步生成式评判器\n\n## 背景与问题\n\n随着模型越来越多地利用多步推理策略解决复杂问题，监督这些中间步骤的逻辑有效性已成为一个关键的研究挑战。现有的过程奖励模型通过提供分步反馈来解决此问题，但它们存在以下主要缺点：\n\n*   它们通常作为分类器运行，不提供解释。\n*   它们依赖于使用静态数据集进行监督微调，这限制了模型的泛化能力。\n\n## StepWiser 方法\n\n受最新进展的启发，本文将分步奖励建模从一个分类任务重新定义为一个推理任务本身。为此，我们提出了一个生成式评判器 StepWiser，其核心特点包括：\n\n*   **元推理：** StepWiser 对策略模型的推理步骤进行“元推理”（即，对推理进行推理），在给出最终判断之前输出“思考令牌”（thinking tokens），从而提供解释。\n*   **训练机制：** StepWiser 通过强化学习进行训练，利用rollout的相对结果来优化其判断能力。\n\n## 主要成果与优势\n\n实验结果表明，StepWiser 具有以下显著优势：\n\n*   **更高的判断准确性：** 在中间步骤上的判断准确性优于现有方法。\n*   **改进策略模型：** 可以在训练时用于改进策略模型。\n*   **优化推理时搜索：** 改进了推理时的搜索能力。",
      "shortSummary": "StepWiser是一种新的生成式评判器，旨在解决多步推理模型中中间步骤逻辑有效性监督的挑战。它将分步奖励建模从分类任务转变为推理任务，通过“元推理”和输出“思考令牌”来提供解释性判断。StepWiser通过强化学习训练，在中间步骤判断准确性上优于现有方法，并能改进策略模型训练和推理时搜索。",
      "translated_title": "StepWiser：用于更明智推理的逐步生成式评判器",
      "images": [],
      "contentSource": "完整文章",
      "content": "As models increasingly leverage multi-step reasoning strategies to solve complex problems, supervising the logical validity of these intermediate steps has become a critical research challenge. Process reward models address this by providing step-by-step feedback, but current approaches have two major drawbacks: they typically function as classifiers without providing explanations, and their reliance on supervised fine-tuning with static datasets limits generalization. Inspired by recent advances, we reframe stepwise reward modeling from a classification task to a reasoning task itself. We thus propose a generative judge that reasons about the policy model's reasoning steps (i.e., meta-reasons), outputting thinking tokens before delivering a final verdict. Our model, StepWiser, is trained by reinforcement learning using relative outcomes of rollouts. We show it provides (i) better judgment accuracy on intermediate steps than existing methods; (ii) can be used to improve the policy model at training time; and (iii) improves inference-time search."
    },
    {
      "title": "预测未来词元顺序可改进语言建模 (原标题: Predicting the Order of Upcoming Tokens Improves Language Modeling)",
      "link": "https://arxiv.org/abs/2508.19228",
      "pubDate": "Tue, 26 Aug 2025 13:43:30 GMT",
      "isoDate": "2025-08-26T13:43:30.000Z",
      "creator": "Zayd M. K. Zuhri, Erland Hilman Fuadi, Alham Fikri Aji",
      "summary": "# 预测未来词元顺序可改进语言建模\n\n## 摘要\n\n本文提出了一种名为“词元顺序预测”（Token Order Prediction, TOP）的新方法，旨在解决多词元预测（Multi-Token Prediction, MTP）作为语言模型辅助目标时表现不佳的问题。MTP因其精确预测未来词元的难度，在标准NLP基准测试中未能持续带来改进。\n\n### 多词元预测（MTP）的问题\n\n*   **目标：** MTP被提议作为辅助目标，以改进语言模型训练中的下一词元预测（Next-Token Prediction, NTP）。\n*   **表现：** MTP的改进效果不一致，在标准NLP基准测试中表现不佳。\n*   **根本原因：** 作者认为，MTP要求精确预测未来的词元，这作为辅助损失而言难度过高。\n\n### 提出的解决方案：词元顺序预测（TOP）\n\n*   **核心思想：** TOP训练模型根据词元之间的接近度来预测未来词元的顺序。\n*   **损失函数：** 采用学习排序（learning-to-rank）损失来实现这一目标。\n\n### TOP的架构优势\n\n*   **简洁性：** 相比MTP需要多个Transformer层，TOP仅需一个额外的非嵌入（unembedding）层。\n\n### 实验与结果\n\n*   **模型规模：** 作者预训练了3.4亿、18亿和70亿参数的模型。\n*   **训练目标：** 这些模型分别使用NTP、MTP和TOP目标进行训练。\n*   **评估：** 在八个标准NLP基准测试上进行了评估。\n*   **主要发现：** 实验结果表明，即使在大型模型上，TOP的整体表现也优于NTP和MTP。\n\n### 代码可用性\n\n*   相关代码已公开提供。",
      "shortSummary": "多词元预测（MTP）作为语言模型辅助目标，因精确预测未来词元难度大而表现不佳。本文提出词元顺序预测（TOP），通过学习排序损失训练模型根据接近度对未来词元进行排序。TOP仅需一个额外的非嵌入层，相比MTP更简洁。在340M、1.8B和7B参数模型上的实验表明，TOP在八个标准NLP基准测试中，即使在大规模模型上，也全面优于NTP和MTP。",
      "translated_title": "预测未来词元顺序可改进语言建模",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multi-Token Prediction (MTP) has been proposed as an auxiliary objective to improve next-token prediction (NTP) in language model training but shows inconsistent improvements, underperforming in standard NLP benchmarks. We argue that MTP's exact future token prediction is too difficult as an auxiliary loss. Instead, we propose Token Order Prediction (TOP), which trains models to order upcoming tokens by their proximity using a learning-to-rank loss. TOP requires only a single additional unembedding layer compared to MTP's multiple transformer layers. We pretrain models of 340M, 1.8B, and 7B parameters using NTP, MTP, and TOP objectives. Results on eight standard NLP benchmarks show that TOP overall outperforms both NTP and MTP even at scale. Our code is available at https://github.com/zaydzuhri/token-order-prediction"
    },
    {
      "title": "OmniHuman-1.5：通过认知模拟为虚拟形象注入活跃思维 (原标题: OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation)",
      "link": "https://arxiv.org/abs/2508.19209",
      "pubDate": "Tue, 26 Aug 2025 13:15:26 GMT",
      "isoDate": "2025-08-26T13:15:26.000Z",
      "creator": "Jianwen Jiang, Weihong Zeng, Zerong Zheng, Jiaqi Yang, Chao Liang, Wang Liao, Han Liang, Yuan Zhang, Mingyuan Gao",
      "summary": "## OmniHuman-1.5：通过认知模拟生成具有活跃思维的虚拟形象\n\n### 挑战：现有虚拟形象模型的局限性\n\n当前视频虚拟形象模型能够生成流畅的人体动画，但它们往往停留在物理相似性层面，难以捕捉角色的真实本质。它们的动作通常仅与音频节奏等低级线索同步，缺乏对情感、意图或上下文的深层语义理解。\n\n### 解决方案：OmniHuman-1.5 框架\n\n为了弥补这一差距，研究人员提出了一个名为 **OmniHuman-1.5** 的框架，旨在生成不仅在物理上合理，而且在语义上连贯和富有表现力的角色动画。该模型建立在两项关键技术贡献之上：\n\n1.  **利用多模态大语言模型 (MLLMs) 进行高级语义指导：**\n    *   模型利用多模态大语言模型来合成结构化的文本表示，提供高层次的语义指导。\n    *   这种指导使运动生成器超越了简单的节奏同步，能够产生与上下文和情感产生共鸣的动作。\n\n2.  **引入带有新型伪最后一帧设计的专用多模态 DiT 架构：**\n    *   为了确保有效融合多模态输入并减轻模态间冲突，模型引入了一种专门的多模态 DiT 架构，其中包含新颖的“伪最后一帧”（Pseudo Last Frame）设计。\n\n### 协同作用与性能\n\n这些组件的协同作用使得 OmniHuman-1.5 模型能够准确解释音频、图像和文本的联合语义，从而生成与角色、场景和语言内容深度连贯的动作。\n\n广泛的实验表明，该模型在以下方面取得了领先性能：\n\n*   唇语同步准确性\n*   视频质量\n*   动作自然度\n*   与文本提示的语义一致性\n\n此外，该方法还展现出卓越的扩展性，能够应用于多人物和非人类主体等复杂场景。",
      "shortSummary": "OmniHuman-1.5 框架旨在通过认知模拟为虚拟形象注入活跃思维，解决现有模型缺乏深层语义理解的问题。它利用多模态大语言模型提供高层次语义指导，并引入带有新型伪最后一帧设计的专用多模态 DiT 架构，有效融合多模态输入。该模型能生成物理合理、语义连贯且富有表现力的动画，在唇语同步、视频质量、动作自然度和语义一致性等指标上表现出色，并具有良好的场景扩展性。",
      "translated_title": "OmniHuman-1.5：通过认知模拟为虚拟形象注入活跃思维",
      "images": [],
      "contentSource": "完整文章",
      "content": "Existing video avatar models can produce fluid human animations, yet they struggle to move beyond mere physical likeness to capture a character's authentic essence. Their motions typically synchronize with low-level cues like audio rhythm, lacking a deeper semantic understanding of emotion, intent, or context. To bridge this gap, we propose a framework designed to generate character animations that are not only physically plausible but also semantically coherent and expressive. Our model, OmniHuman-1.5, is built upon two key technical contributions. First, we leverage Multimodal Large Language Models to synthesize a structured textual representation of conditions that provides high-level semantic guidance. This guidance steers our motion generator beyond simplistic rhythmic synchronization, enabling the production of actions that are contextually and emotionally resonant. Second, to ensure the effective fusion of these multimodal inputs and mitigate inter-modality conflicts, we introduce a specialized Multimodal DiT architecture with a novel Pseudo Last Frame design. The synergy of these components allows our model to accurately interpret the joint semantics of audio, images, and text, thereby generating motions that are deeply coherent with the character, scene, and linguistic content. Extensive experiments demonstrate that our model achieves leading performance across a comprehensive set of metrics, including lip-sync accuracy, video quality, motion naturalness and semantic consistency with textual prompts. Furthermore, our approach shows remarkable extensibility to complex scenarios, such as those involving multi-person and non-human subjects. Homepage: https://omnihuman-lab.github.io/v1_5/"
    },
    {
      "title": "VibeVoice 技术报告 (原标题: VibeVoice Technical Report)",
      "link": "https://arxiv.org/abs/2508.19205",
      "pubDate": "Tue, 26 Aug 2025 13:09:12 GMT",
      "isoDate": "2025-08-26T13:09:12.000Z",
      "creator": "Zhiliang Peng, Jianwei Yu, Wenhui Wang, Yaoyao Chang, Yutao Sun, Li Dong, Yi Zhu, Weijiang Xu, Hangbo Bao, Zehua Wang, Shaohan Huang, Yan Xia, Furu Wei",
      "summary": "# VibeVoice 技术报告\n\n本报告介绍了VibeVoice，一个旨在通过“下一词元扩散”（next-token diffusion）技术合成多说话者长篇语音的新颖模型。\n\n## 核心技术与创新\n\n*   **下一词元扩散（Next-token Diffusion）**：VibeVoice采用了一种统一的方法，通过自回归生成潜在向量来建模连续数据，从而实现语音合成。\n*   **新型连续语音词元分析器（Novel Continuous Speech Tokenizer）**：\n    *   该模型引入了一种创新的连续语音词元分析器。\n    *   与流行的Encodec模型相比，它将数据压缩率提高了80倍，同时保持了可比的性能。\n    *   该词元分析器能够有效保留音频保真度，并显著提升处理长序列时的计算效率。\n\n## VibeVoice的性能与能力\n\n*   **长篇语音合成**：VibeVoice能够合成长达90分钟的长篇语音（在64K的上下文窗口长度下）。\n*   **多说话者支持**：该模型最多可以合成4位说话者的语音。\n*   **真实对话氛围**：VibeVoice能够捕捉真实的对话“氛围”（conversational \"vibe\"）。\n*   **卓越性能**：其性能超越了现有的开源和专有对话模型。\n\n## 研究领域\n\n*   计算与语言 (cs.CL)\n*   人工智能 (cs.AI)\n*   声音 (cs.SD)",
      "shortSummary": "VibeVoice是一个利用“下一词元扩散”技术合成多说话者长篇语音的新模型。它引入了一种新型连续语音词元分析器，将数据压缩率提高80倍，同时保持音频保真度和计算效率。VibeVoice能合成长达90分钟、最多4位说话者的语音，捕捉真实对话氛围，并超越现有对话模型。",
      "translated_title": "VibeVoice 技术报告",
      "images": [],
      "contentSource": "完整文章",
      "content": "This report presents VibeVoice, a novel model designed to synthesize long-form speech with multiple speakers by employing next-token diffusion, which is a unified method for modeling continuous data by autoregressively generating latent vectors via diffusion. To enable this, we introduce a novel continuous speech tokenizer that, when compared to the popular Encodec model, improves data compression by 80 times while maintaining comparable performance. The tokenizer effectively preserves audio fidelity while significantly boosting computational efficiency for processing long sequences. Thus, VibeVoice can synthesize long-form speech for up to 90 minutes (in a 64K context window length) with a maximum of 4 speakers, capturing the authentic conversational ``vibe'' and surpassing open-source and proprietary dialogue models."
    },
    {
      "title": "通过探究知识和推理揭示大型语言模型中的科学问题解决能力 (原标题: Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning)",
      "link": "https://arxiv.org/abs/2508.19202",
      "pubDate": "Tue, 26 Aug 2025 13:04:23 GMT",
      "isoDate": "2025-08-26T13:04:23.000Z",
      "creator": "Alan Li, Yixin Liu, Arpan Sarkar, Doug Downey, Arman Cohan",
      "summary": "## 通过探究知识和推理揭示大型语言模型中的科学问题解决能力\n\n### 引言\n大型语言模型（LLMs）在科学问题解决方面面临独特的挑战，这不仅需要深厚的领域知识，还需要通过复杂的推理来应用这些知识。尽管自动化科学推理器在辅助人类科学家方面前景广阔，但目前缺乏一个被广泛采用的整体基准来评估科学推理能力，也很少有方法系统地解耦知识和推理在这些任务中的不同作用。\n\n### 现有问题与解决方案\n为了解决这些空白，研究人员提出了以下方案：\n\n*   **SciReas 基准套件**：引入了一个多样化的现有科学推理任务基准套件，名为 `SciReas`。\n*   **SciReas-Pro 子集**：从 `SciReas` 中筛选出一个子集 `SciReas-Pro`，该子集需要更复杂的推理能力。\n    *   通过对 `SciReas` 和 `SciReas-Pro` 进行整体评估，研究揭示了仅依赖单个基准时无法发现的关于科学推理性能的深刻见解。\n*   **KRUX 探究框架**：提出了一个名为 `KRUX` 的探究框架，用于研究知识和推理在科学任务中的不同作用。\n\n### 主要发现\n结合 `SciReas`、`SciReas-Pro` 和 `KRUX`，研究人员进行了深入分析，得出了以下几个关键发现：\n\n1.  **知识检索瓶颈**：从模型参数中检索任务相关知识是LLMs在科学推理中的一个关键瓶颈。\n2.  **外部知识的益处**：在推理增强的基础上，通过上下文添加外部知识对推理模型有持续的益处。\n3.  **口头化推理的增强作用**：增强口头化推理（verbalized reasoning）能够提高LLMs浮现任务相关知识的能力。\n\n### 其他贡献\n*   研究还进行了一项轻量级分析，将其科学领域的数据构成与当前关于长链式思维（CoT）监督微调（SFT）的研究进行了比较。\n*   发布了 `SciLit01`，这是一个强大的8B科学推理基线模型。",
      "shortSummary": "本研究旨在揭示大型语言模型（LLMs）在科学问题解决中的知识与推理机制。针对缺乏整体评估基准和知识推理解耦方法的现状，研究引入了`SciReas`基准套件及其复杂推理子集`SciReas-Pro`，并提出了`KRUX`探究框架。主要发现包括：从模型参数中检索知识是LLMs的瓶颈；外部知识对推理模型有益；增强口头化推理能提升知识浮现能力。研究还发布了`SciLit01`科学推理基线模型。",
      "translated_title": "通过探究知识和推理揭示大型语言模型中的科学问题解决能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Scientific problem solving poses unique challenges for LLMs, requiring both deep domain knowledge and the ability to apply such knowledge through complex reasoning. While automated scientific reasoners hold great promise for assisting human scientists, there is currently no widely adopted holistic benchmark for evaluating scientific reasoning, and few approaches systematically disentangle the distinct roles of knowledge and reasoning in these tasks. To address these gaps, we introduce SciReas, a diverse suite of existing benchmarks for scientific reasoning tasks, and SciReas-Pro, a selective subset that requires more complex reasoning. Our holistic evaluation surfaces insights about scientific reasoning performance that remain hidden when relying on individual benchmarks alone. We then propose KRUX, a probing framework for studying the distinct roles of reasoning and knowledge in scientific tasks. Combining the two, we conduct an in-depth analysis that yields several key findings: (1) Retrieving task-relevant knowledge from model parameters is a critical bottleneck for LLMs in scientific reasoning; (2) Reasoning models consistently benefit from external knowledge added in-context on top of the reasoning enhancement; (3) Enhancing verbalized reasoning improves LLMs' ability to surface task-relevant knowledge. Finally, we conduct a lightweight analysis, comparing our science-focused data composition with concurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline for scientific reasoning."
    },
    {
      "title": "FastMesh：通过组件解耦实现高效艺术网格生成 (原标题: FastMesh:Efficient Artistic Mesh Generation via Component Decoupling)",
      "link": "https://arxiv.org/abs/2508.19188",
      "pubDate": "Tue, 26 Aug 2025 12:51:02 GMT",
      "isoDate": "2025-08-26T12:51:02.000Z",
      "creator": "Jeonghwan Kim, Yushi Lan, Armando Fortes, Yongwei Chen, Xingang Pan",
      "summary": "# FastMesh：通过组件解耦实现高效艺术网格生成\n\n本文介绍了一种名为 FastMesh 的高效框架，旨在解决现有艺术网格生成方法中存在的效率低下和冗余问题。\n\n## 现有方法的局限性\n*   **冗余的令牌序列**：当前网格生成方法通常将三角网格令牌化为序列，并使用自回归模型进行生成。\n*   **顶点重复利用**：由于每个顶点被多个面共享，导致令牌序列中顶点信息被多次重复利用。\n*   **效率低下**：这种冗余导致令牌序列过长，进而使得生成过程效率低下。\n\n## FastMesh 提出的解决方案：组件解耦\nFastMesh 通过将顶点和面分开处理，显著减少了冗余，从而实现高效的艺术网格生成。\n\n### 核心机制\n1.  **顶点生成**：\n    *   采用一个**自回归模型**专门用于顶点生成。\n    *   将令牌数量减少到现有最紧凑分词器所需数量的约 **23%**，大幅降低了冗余。\n2.  **网格补全（面生成）**：\n    *   利用**双向 Transformer** 在**单一步骤**内完成网格。\n    *   该 Transformer 捕捉顶点间的关系，并构建定义网格面的邻接矩阵。\n3.  **质量提升组件**：\n    *   **保真度增强器 (Fidelity Enhancer)**：用于优化顶点位置，使其排列更自然。\n    *   **后处理框架 (Post-processing Framework)**：用于移除不理想的边连接。\n\n## 实验结果\n*   **速度提升**：与现有最先进的方法相比，FastMesh 在网格生成速度上实现了**超过 8 倍**的提升。\n*   **质量提高**：同时，FastMesh 能够生成**更高质量**的网格。",
      "shortSummary": "FastMesh 提出一种高效的艺术网格生成框架，通过将顶点和面解耦处理来解决现有方法的冗余问题。它使用自回归模型生成顶点，然后通过双向 Transformer 在单一步骤内完成面生成，并辅以保真度增强器和后处理。实验表明，FastMesh 比现有技术快 8 倍以上，并能生成更高质量的网格，显著减少了令牌冗余。",
      "translated_title": "FastMesh：通过组件解耦实现高效艺术网格生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent mesh generation approaches typically tokenize triangle meshes into sequences of tokens and train autoregressive models to generate these tokens sequentially. Despite substantial progress, such token sequences inevitably reuse vertices multiple times to fully represent manifold meshes, as each vertex is shared by multiple faces. This redundancy leads to excessively long token sequences and inefficient generation processes. In this paper, we propose an efficient framework that generates artistic meshes by treating vertices and faces separately, significantly reducing redundancy. We employ an autoregressive model solely for vertex generation, decreasing the token count to approximately 23\\% of that required by the most compact existing tokenizer. Next, we leverage a bidirectional transformer to complete the mesh in a single step by capturing inter-vertex relationships and constructing the adjacency matrix that defines the mesh faces. To further improve the generation quality, we introduce a fidelity enhancer to refine vertex positioning into more natural arrangements and propose a post-processing framework to remove undesirable edge connections. Experimental results show that our method achieves more than 8times faster speed on mesh generation compared to state-of-the-art approaches, while producing higher mesh quality."
    },
    {
      "title": "MIDAS：基于实时自回归视频生成的多模态交互式数字人合成 (原标题: MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation)",
      "link": "https://arxiv.org/abs/2508.19320",
      "pubDate": "Tue, 26 Aug 2025 10:00:16 GMT",
      "isoDate": "2025-08-26T10:00:16.000Z",
      "creator": "Ming Chen, Liyuan Cui, Wenyuan Zhang, Haoxian Zhang, Yan Zhou, Xiaohan Li, Xiaoqiang Liu, Pengfei Wan",
      "summary": "### MIDAS：实时自回归视频生成实现多模态交互式数字人合成\n\n**背景与挑战**\n\n*   交互式数字人视频生成领域备受关注并取得了显著进展。\n*   然而，构建一个能够实时与多样化输入信号交互的实用系统仍然面临挑战，现有方法常受限于高延迟、沉重计算成本和有限的可控性。\n\n**MIDAS 框架介绍**\n\n*   本文提出了一种名为 MIDAS 的自回归视频生成框架。\n*   该框架旨在以流式方式实现交互式多模态控制和低延迟外推。\n\n**核心技术与创新点**\n\n1.  **基于大型语言模型（LLM）**：\n    *   对标准大型语言模型（LLM）进行了最小程度的修改。\n    *   框架能够接受包括音频、姿态和文本在内的多模态条件编码。\n    *   输出空间和语义连贯的表示，以指导扩散头（diffusion head）的去噪过程。\n2.  **大规模对话数据集**：\n    *   构建了一个约20,000小时的大规模对话数据集。\n    *   该数据集来源于多个来源，为训练提供了丰富的对话场景。\n3.  **深度压缩自编码器**：\n    *   引入了一个深度压缩自编码器，其压缩比高达64倍。\n    *   有效缓解了自回归模型在长序列推理时的负担。\n\n**实验与成果**\n\n*   在双向对话、多语言数字人合成和交互式世界模型等多个方面进行了广泛实验。\n*   实验结果突出表明，MIDAS 方法在以下方面具有显著优势：\n    *   **低延迟**\n    *   **高效率**\n    *   **细粒度多模态可控性**",
      "shortSummary": "MIDAS是一个用于实时交互式数字人视频生成的自回归框架，旨在解决现有方法的高延迟和计算成本问题。它通过对大型语言模型进行最小修改，接受音频、姿态和文本等多模态输入，并结合一个约20,000小时的大规模对话数据集和64倍压缩比的深度自编码器进行训练。MIDAS在双向对话和多语言合成等实验中展现出低延迟、高效率和细粒度多模态可控性。",
      "translated_title": "MIDAS：基于实时自回归视频生成的多模态交互式数字人合成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such a practical system that can interact with diverse input signals in real time remains challenging to existing methods, which often struggle with high latency, heavy computational cost, and limited controllability. In this work, we introduce an autoregressive video generation framework that enables interactive multimodal control and low-latency extrapolation in a streaming manner. With minimal modifications to a standard large language model (LLM), our framework accepts multimodal condition encodings including audio, pose, and text, and outputs spatially and semantically coherent representations to guide the denoising process of a diffusion head. To support this, we construct a large-scale dialogue dataset of approximately 20,000 hours from multiple sources, providing rich conversational scenarios for training. We further introduce a deep compression autoencoder with up to 64times reduction ratio, which effectively alleviates the long-horizon inference burden of the autoregressive model. Extensive experiments on duplex conversation, multilingual human synthesis, and interactive world model highlight the advantages of our approach in low latency, high efficiency, and fine-grained multimodal controllability."
    },
    {
      "title": "MovieCORE：电影中的认知推理 (原标题: MovieCORE: COgnitive REasoning in Movies)",
      "link": "https://arxiv.org/abs/2508.19026",
      "pubDate": "Tue, 26 Aug 2025 09:43:45 GMT",
      "isoDate": "2025-08-26T09:43:45.000Z",
      "creator": "Gueter Josmy Faure, Min-Hung Chen, Jia-Fong Yeh, Ying Cheng, Hung-Ting Su, Yung-Hao Tang, Shang-Hong Lai, Winston H. Hsu",
      "summary": "MovieCORE是一个新颖的视频问答（VQA）数据集，旨在深入探究对电影内容的认知理解，超越现有数据集的表面理解。\n\n**主要特点与贡献：**\n\n*   **深入认知理解**：\n    *   MovieCORE专注于需要“系统2思维”的问题，这些问题要求更深层次的推理和理解，同时仍与视频材料具体相关。\n    *   这与现有VQA数据集形成对比，后者通常侧重于表层或事实性理解。\n\n*   **数据生成方法**：\n    *   本文提出了一种创新的“智能体式头脑风暴”方法。\n    *   该方法利用多个大型语言模型（LLM）作为思维智能体，协同生成和完善高质量的问答对，确保数据集的深度和复杂性。\n\n*   **数据集质量评估**：\n    *   为评估MovieCORE数据集的质量，研究人员开发了一套认知测试。\n    *   这些测试用于评估数据集问题的深度、启发思考的潜力以及句法复杂性。\n\n*   **VQA模型评估方案**：\n    *   论文还提出了一项全面的评估方案，专门用于评估VQA模型在处理更深层次认知任务时的性能。\n\n*   **模型增强模块**：\n    *   为解决现有视频-语言模型（VLM）在处理复杂认知任务时的局限性，研究引入了“智能体选择增强”（Agentic Choice Enhancement, ACE）模块。\n    *   ACE模块能够在模型训练后，将其推理能力提高高达25%，显著提升了VLM的性能。\n\n*   **研究意义**：\n    *   MovieCORE的工作有助于推动人工智能系统在电影理解领域的发展。\n    *   它为当前VQA模型在面对更具挑战性、更细致的电影内容问题时的能力和局限性，提供了宝贵的见解。\n\n**可用性与接受情况：**\n\n*   项目页面、数据集和代码可在提供的URL获取。\n*   该论文已被EMNLP'2025主会议接受。",
      "shortSummary": "MovieCORE是一个新颖的视频问答（VQA）数据集，旨在促进AI对电影内容的深层认知理解，而非表面理解。它采用多LLM智能体式头脑风暴生成高质量问答对，并引入“智能体选择增强”（ACE）模块，可将视频-语言模型（VLM）的推理能力提升高达25%。该工作为评估和提升VQA模型处理复杂电影内容的能力提供了新方法，并已被EMNLP'2025接受。",
      "translated_title": "MovieCORE：电影中的认知推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper introduces MovieCORE, a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes questions that engage System-2 thinking while remaining specific to the video material. We present an innovative agentic brainstorming approach, utilizing multiple large language models (LLMs) as thought agents to generate and refine high-quality question-answer pairs. To evaluate dataset quality, we develop a set of cognitive tests assessing depth, thought-provocation potential, and syntactic complexity. We also propose a comprehensive evaluation scheme for assessing VQA model performance on deeper cognitive tasks. To address the limitations of existing video-language models (VLMs), we introduce an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves model reasoning capabilities post-training by up to 25%. Our work contributes to advancing movie understanding in AI systems and provides valuable insights into the capabilities and limitations of current VQA models when faced with more challenging, nuanced questions about cinematic content. Our project page, dataset and code can be found at https://joslefaure.github.io/assets/html/moviecore.html."
    },
    {
      "title": "ThinkDial：一种控制大型语言模型推理工作量的开放方案 (原标题: ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models)",
      "link": "https://arxiv.org/abs/2508.18773",
      "pubDate": "Tue, 26 Aug 2025 03:57:28 GMT",
      "isoDate": "2025-08-26T03:57:28.000Z",
      "creator": "Qianyu He, Siyu Yuan, Xuefeng Li, Mingxuan Wang, Jiangjie Chen",
      "summary": "### ThinkDial：一种控制大型语言模型推理工作量的开放方案\n\n**引言与背景**\n大型语言模型（LLMs）结合思维链（chain-of-thought）推理展现出卓越的问题解决能力。然而，在实际部署中，如何有效控制其计算工作量仍然是一个重大挑战。尽管OpenAI的gpt-oss系列等专有系统已经引入了离散的操作模式来实现直观的推理控制，但开源社区在实现类似能力方面仍显不足。\n\n**ThinkDial 框架介绍**\n本文提出了 ThinkDial，这是首个开放方案的端到端框架，成功实现了类似 gpt-oss 的、通过离散操作模式进行可控推理的能力。\n\n**核心功能与模式**\nThinkDial 系统支持在三种不同的推理模式之间无缝切换，以平衡性能与计算资源：\n*   **高模式 (High mode)**：提供完整的推理能力。\n*   **中模式 (Medium mode)**：实现 50% 的 token 减少，同时性能下降低于 10%。\n*   **低模式 (Low mode)**：实现 75% 的 token 减少，同时性能下降低于 15%。\n\n**实现方法**\nThinkDial 通过一个端到端的训练范式实现这些能力，该范式在整个流程中整合了预算模式控制：\n1.  **预算模式监督微调 (Budget-mode supervised fine-tuning)**：将可控推理能力直接嵌入到学习过程中。\n2.  **两阶段预算感知强化学习 (Two-phase budget-aware reinforcement learning)**：结合自适应奖励塑形。\n\n**实验结果与泛化能力**\n广泛的实验证明，ThinkDial 实现了目标压缩-性能权衡，显著减少了响应长度，同时保持了性能阈值。该框架还在分布外（out-of-distribution）任务上展现出强大的泛化能力。",
      "shortSummary": "ThinkDial是一个开放的端到端框架，旨在解决大型语言模型推理工作量控制的挑战。它通过离散操作模式（高、中、低）实现类似gpt-oss的可控推理，允许用户在计算成本和性能之间进行权衡。例如，低模式可减少75%的token，性能下降低于15%。ThinkDial采用预算模式监督微调和两阶段强化学习进行训练，实验证明其能有效减少响应长度并保持性能，同时具有强大的泛化能力。",
      "translated_title": "ThinkDial：一种控制大型语言模型推理工作量的开放方案",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) with chain-of-thought reasoning have demonstrated remarkable problem-solving capabilities, but controlling their computational effort remains a significant challenge for practical deployment. Recent proprietary systems like OpenAI's gpt-oss series have introduced discrete operational modes for intuitive reasoning control, but the open-source community has largely failed to achieve such capabilities. In this paper, we introduce ThinkDial, the first open-recipe end-to-end framework that successfully implements gpt-oss-style controllable reasoning through discrete operational modes. Our system enables seamless switching between three distinct reasoning regimes: High mode (full reasoning capability), Medium mode (50 percent token reduction with &lt;10 percent performance degradation), and Low mode (75 percent token reduction with &lt;15 percent performance degradation). We achieve this through an end-to-end training paradigm that integrates budget-mode control throughout the entire pipeline: budget-mode supervised fine-tuning that embeds controllable reasoning capabilities directly into the learning process, and two-phase budget-aware reinforcement learning with adaptive reward shaping. Extensive experiments demonstrate that ThinkDial achieves target compression-performance trade-offs with clear response length reductions while maintaining performance thresholds. The framework also exhibits strong generalization capabilities on out-of-distribution tasks."
    }
  ],
  "lastUpdated": "2025-08-28T09:33:02.954Z"
}