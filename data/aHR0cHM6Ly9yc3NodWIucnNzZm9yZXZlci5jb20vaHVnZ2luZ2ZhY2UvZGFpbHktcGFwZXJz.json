{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "AnimaX：使用联合视频-姿态扩散模型在3D中为无生命物体赋予动画 (原标题: AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion Models)",
      "link": "https://arxiv.org/abs/2506.19851",
      "pubDate": "Tue, 24 Jun 2025 13:59:58 GMT",
      "isoDate": "2025-06-24T13:59:58.000Z",
      "creator": "Zehuan Huang, Haoran Feng, Yangtian Sun, Yuanchen Guo, Yanpei Cao, Lu Sheng",
      "summary": "# AnimaX：一种创新的3D动画框架\n\nAnimaX是一个前馈式3D动画框架，旨在弥合视频扩散模型的运动先验与基于骨骼动画的可控结构之间的鸿沟。\n\n## 传统方法的局限性\n\n传统的运动合成方法存在以下限制：\n*   受限于固定的骨骼拓扑结构。\n*   需要在高维变形空间中进行昂贵的优化。\n\n## AnimaX 的核心优势与方法\n\nAnimaX通过以下创新方法克服了传统方法的局限性：\n\n*   **知识迁移：** 有效地将基于视频的运动知识迁移到3D领域。\n*   **广泛支持：** 能够支持具有任意骨骼的各种关节网格模型。\n*   **运动表示：** 将3D运动表示为多视角、多帧的2D姿态图。\n*   **联合扩散：** 实现联合视频-姿态扩散，其条件是模板渲染和文本运动提示。\n*   **对齐机制：** 引入共享位置编码和模态感知嵌入，以确保视频和姿态序列之间的时空对齐，从而将视频先验有效地转移到运动生成任务中。\n*   **3D重建：** 生成的多视角姿态序列被三角化为3D关节位置。\n*   **网格动画：** 通过逆运动学（inverse kinematics）将3D关节位置转换为最终的网格动画。\n\n## 训练与性能\n\n*   AnimaX 在一个新策展的、包含160,000个绑定序列的数据集上进行了训练。\n*   在VBench基准测试中，AnimaX在泛化能力、运动保真度和效率方面取得了最先进（state-of-the-art）的结果。\n\n## 应用前景\n\nAnimaX为类别无关的3D动画提供了一个可扩展的解决方案，具有广泛的应用潜力。\n\n**项目页面：** 提供了项目页面链接以获取更多信息。",
      "shortSummary": "AnimaX是一个创新的3D动画框架，它结合了视频扩散模型的运动先验与骨骼动画的可控性。该方法将3D运动表示为多视角2D姿态图，并通过联合视频-姿态扩散生成动画。AnimaX支持任意骨骼的网格模型，解决了传统方法的局限性。它在16万个绑定序列数据集上训练，并在泛化、运动保真度和效率方面达到了最先进水平，为类别无关的3D动画提供了可扩展的解决方案。",
      "translated_title": "AnimaX：使用联合视频-姿态扩散模型在3D中为无生命物体赋予动画",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present AnimaX, a feed-forward 3D animation framework that bridges the motion priors of video diffusion models with the controllable structure of skeleton-based animation. Traditional motion synthesis methods are either restricted to fixed skeletal topologies or require costly optimization in high-dimensional deformation spaces. In contrast, AnimaX effectively transfers video-based motion knowledge to the 3D domain, supporting diverse articulated meshes with arbitrary skeletons. Our method represents 3D motion as multi-view, multi-frame 2D pose maps, and enables joint video-pose diffusion conditioned on template renderings and a textual motion prompt. We introduce shared positional encodings and modality-aware embeddings to ensure spatial-temporal alignment between video and pose sequences, effectively transferring video priors to motion generation task. The resulting multi-view pose sequences are triangulated into 3D joint positions and converted into mesh animation via inverse kinematics. Trained on a newly curated dataset of 160,000 rigged sequences, AnimaX achieves state-of-the-art results on VBench in generalization, motion fidelity, and efficiency, offering a scalable solution for category-agnostic 3D animation. Project page: https://anima-x.github.io/{https://anima-x.github.io/}."
    },
    {
      "title": "统一的视觉-语言-动作模型 (原标题: Unified Vision-Language-Action Model)",
      "link": "https://arxiv.org/abs/2506.19850",
      "pubDate": "Tue, 24 Jun 2025 13:59:57 GMT",
      "isoDate": "2025-06-24T13:59:57.000Z",
      "creator": "Yuqi Wang, Xinghang Li, Wenxuan Wang, Junbo Zhang, Yingyan Li, Yuntao Chen, Xinlong Wang, Zhaoxiang Zhang",
      "summary": "## UniVLA：统一的视觉-语言-动作模型\n\n### 引言\n视觉-语言-动作（VLA）模型因其在推动机器人操作方面的巨大潜力而备受关注。然而，现有方法主要依赖视觉-语言模型（VLM）的通用理解能力来生成动作信号，这往往忽略了视觉观察中固有的丰富时序和因果结构。\n\n### UniVLA 的提出与核心机制\n本文提出了 **UniVLA**，一个统一的、原生的多模态VLA模型。UniVLA的核心创新在于将视觉、语言和动作信号自回归地建模为离散的token序列。这种独特的公式化方法带来了多项优势：\n\n*   **灵活的多模态任务学习**：它使得模型能够灵活地学习各种多模态任务，尤其擅长从大规模视频数据中进行学习。\n*   **融入世界建模**：通过在后训练阶段融入世界建模，UniVLA能够有效地从视频中捕捉因果动态。\n*   **高效的策略迁移**：捕捉到的因果动态有助于模型有效迁移到下游策略学习，这对于解决长时程任务尤为重要。\n\n### 实验结果与性能\nUniVLA在多个广泛使用的模拟基准测试中取得了新的最先进结果，显著超越了现有方法，这些基准包括：\n\n*   CALVIN\n*   LIBERO\n*   Simplenv-Bridge\n\n**具体示例**：在LIBERO基准测试中，UniVLA的平均成功率达到了95.5%，显著超越了先前方法pi0-FAST的85.5%。\n\n### 实际应用\n除了在模拟环境中的卓越表现，UniVLA的广泛适用性还在真实世界的应用中得到了验证，包括：\n\n*   ALOHA机器人操作\n*   自动驾驶",
      "shortSummary": "UniVLA是一个统一的视觉-语言-动作（VLA）模型，它将视觉、语言和动作信号自回归地建模为离散token序列。通过融入世界建模，UniVLA能从视频中捕捉因果动态，有效支持长时程任务的策略学习。该模型在CALVIN、LIBERO等多个模拟基准测试中取得了最先进结果，例如在LIBERO上成功率达95.5%。UniVLA还展示了在真实世界机器人操作和自动驾驶中的广泛适用性。",
      "translated_title": "统一的视觉-语言-动作模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-language-action models (VLAs) have garnered significant attention for their potential in advancing robotic manipulation. However, previous approaches predominantly rely on the general comprehension capabilities of vision-language models (VLMs) to generate action signals, often overlooking the rich temporal and causal structure embedded in visual observations. In this paper, we present UniVLA, a unified and native multimodal VLA model that autoregressively models vision, language, and action signals as discrete token sequences. This formulation enables flexible multimodal tasks learning, particularly from large-scale video data. By incorporating world modeling during post-training, UniVLA captures causal dynamics from videos, facilitating effective transfer to downstream policy learning--especially for long-horizon tasks. Our approach sets new state-of-the-art results across several widely used simulation benchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly surpassing previous methods. For example, UniVLA achieves 95.5% average success rate on LIBERO benchmark, surpassing pi0-FAST's 85.5%. We further demonstrate its broad applicability on real-world ALOHA manipulation and autonomous driving."
    },
    {
      "title": "ScaleCap：通过双模态去偏实现推理时可伸缩的图像字幕生成 (原标题: ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality Debiasing)",
      "link": "https://arxiv.org/abs/2506.19848",
      "pubDate": "Tue, 24 Jun 2025 13:59:55 GMT",
      "isoDate": "2025-06-24T13:59:55.000Z",
      "creator": "Long Xing, Qidong Huang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Jinsong Li, Shuangrui Ding, Weiming Zhang, Nenghai Yu, Jiaqi Wang, Feng Wu, Dahua Lin",
      "summary": "## ScaleCap：通过双模态去偏实现推理时可伸缩的图像字幕生成\n\n### 引言\n本文介绍ScaleCap，一种推理时可伸缩的图像字幕生成策略，旨在生成全面且详细的图像字幕。高质量图像字幕生成的关键挑战在于大型视觉语言模型（LVLMs）固有的偏见：\n\n*   **多模态偏见**：导致描述粒度不平衡，对某些元素描述详细，而对另一些则仅作概述。\n*   **语言偏见**：导致对不存在对象的幻觉描述。\n\n### ScaleCap：可伸缩的去偏字幕策略\n为解决上述问题，我们提出一种可伸缩的去偏字幕生成策略，该策略通过增加推理预算，持续丰富和校准字幕。该策略包含两个新颖组件：\n\n*   **启发式问答（Heuristic Question Answering）**：根据图像生成内容特定问题并回答，逐步向字幕中注入相关信息。\n*   **对比句评分（Contrastive Sentence Rating）**：采用句子级离线对比解码，有效识别并消除由语言偏见引起的幻觉。\n\n### 推理时可伸缩性\n随着推理成本的增加，ScaleCap会提出更多启发式问题，逐步捕捉额外的视觉细节，从而生成更准确、平衡且信息丰富的字幕。\n\n### 实验结果与应用\n\n*   广泛的模态对齐实验证明了ScaleCap的有效性。\n*   使用ScaleCap标注45万张图像并用于LVLM预训练，在11个广泛使用的基准测试中持续获得性能提升。\n*   此外，ScaleCap在两个额外任务中展示了生成字幕的卓越丰富性和保真度：\n    *   在VQA（视觉问答）任务中用字幕替换图像。\n    *   从字幕重建图像以评估语义覆盖范围。",
      "shortSummary": "ScaleCap是一种推理时可伸缩的图像字幕生成策略，旨在解决大型视觉语言模型（LVLMs）中的多模态和语言偏见。它通过启发式问答逐步注入细节，并利用对比句评分消除幻觉。随着推理预算增加，ScaleCap能生成更准确、平衡、信息丰富的字幕。实验证明，其能显著提升LVLM性能，并在VQA和图像重建等任务中展现出卓越的字幕质量。",
      "translated_title": "ScaleCap：通过双模态去偏实现推理时可伸缩的图像字幕生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper presents ScaleCap, an inference-time scalable image captioning strategy that generates comprehensive and detailed image captions. The key challenges of high-quality image captioning lie in the inherent biases of LVLMs: multimodal bias resulting in imbalanced descriptive granularity, offering detailed accounts of some elements while merely skimming over others; linguistic bias leading to hallucinated descriptions of non-existent objects. To address these issues, we propose a scalable debiased captioning strategy, which continuously enriches and calibrates the caption with increased inference budget. Specifically, we propose two novel components: heuristic question answering and contrastive sentence rating. The former generates content-specific questions based on the image and answers them to progressively inject relevant information into the caption. The latter employs sentence-level offline contrastive decoding to effectively identify and eliminate hallucinations caused by linguistic biases. With increased inference cost, more heuristic questions are raised by ScaleCap to progressively capture additional visual details, generating captions that are more accurate, balanced, and informative. Extensive modality alignment experiments demonstrate the effectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them for LVLM pretraining leads to consistent performance gains across 11 widely used benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity of generated captions with two additional tasks: replacing images with captions in VQA task, and reconstructing images from captions to assess semantic coverage. Code is available at https://github.com/Cooperx521/ScaleCap."
    },
    {
      "title": "SimpleGVR：一种用于潜在级联视频超分辨率的简单基线 (原标题: SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution)",
      "link": "https://arxiv.org/abs/2506.19838",
      "pubDate": "Tue, 24 Jun 2025 13:57:26 GMT",
      "isoDate": "2025-06-24T13:57:26.000Z",
      "creator": "Liangbin Xie, Yu Li, Shian Du, Menghan Xia, Xintao Wang, Fanghua Yu, Ziyan Chen, Pengfei Wan, Jiantao Zhou, Chao Dong",
      "summary": "# SimpleGVR：潜在级联视频超分辨率的简单基线\n\n## 核心问题与背景\n当前，潜在扩散模型已成为高效视频生成领域的主导范式。然而，随着用户对更高分辨率输出的期望不断提高，仅依赖潜在计算已无法满足需求。一个有前景的方法是将视频生成过程解耦为两个阶段：语义内容生成和细节合成。前者利用计算密集型的基础模型在较低分辨率下生成内容，而后者则借助轻量级的级联视频超分辨率（VSR）模型实现高分辨率输出。\n\n## 本文研究重点\n本研究专注于探讨级联VSR模型的关键设计原则，这在当前领域中尚未得到充分探索。\n\n## 主要贡献与设计原则\n\n### 1. 降级策略\n文章提出了两种降级策略，用于生成训练对。这些策略旨在更好地模拟基础模型的输出特性，从而确保VSR模型与其上游生成器之间的高度对齐。\n\n### 2. VSR模型行为分析\n通过系统分析，本文提供了关于VSR模型行为的关键见解，具体包括：\n*   **时间步采样策略**：研究了不同时间步采样策略对模型性能的影响。\n*   **低分辨率（LR）输入上的噪声增强效果**：分析了噪声增强对LR输入的影响。\n这些发现直接指导了模型架构和训练方法的创新。\n\n### 3. 效率创新\n为了实现高效的训练和推理，并大幅降低计算开销，SimpleGVR引入了：\n*   **交错时间单元（interleaving temporal unit）**\n*   **稀疏局部注意力（sparse local attention）**\n\n## 实验结果与结论\n广泛的实验证明，SimpleGVR框架优于现有方法。消融研究也证实了每个设计选择的有效性。这项工作为级联视频超分辨率生成建立了一个简单而有效的基线，为未来高效级联合成系统的发展提供了实用的见解。",
      "shortSummary": "SimpleGVR提出了一种用于潜在级联视频超分辨率（VSR）的简单有效基线。它通过将视频生成解耦为内容生成和细节合成来解决高分辨率输出问题。主要贡献包括：优化训练对的降级策略、深入分析VSR模型行为（时间步采样、噪声增强），以及引入交错时间单元和稀疏局部注意力以提高效率。实验证明其优越性，为未来高效级联合成系统提供了实用见解。",
      "translated_title": "SimpleGVR：一种用于潜在级联视频超分辨率的简单基线",
      "images": [],
      "contentSource": "完整文章",
      "content": "Latent diffusion models have emerged as a leading paradigm for efficient video generation. However, as user expectations shift toward higher-resolution outputs, relying solely on latent computation becomes inadequate. A promising approach involves decoupling the process into two stages: semantic content generation and detail synthesis. The former employs a computationally intensive base model at lower resolutions, while the latter leverages a lightweight cascaded video super-resolution (VSR) model to achieve high-resolution output. In this work, we focus on studying key design principles for latter cascaded VSR models, which are underexplored currently. First, we propose two degradation strategies to generate training pairs that better mimic the output characteristics of the base model, ensuring alignment between the VSR model and its upstream generator. Second, we provide critical insights into VSR model behavior through systematic analysis of (1) timestep sampling strategies, (2) noise augmentation effects on low-resolution (LR) inputs. These findings directly inform our architectural and training innovations. Finally, we introduce interleaving temporal unit and sparse local attention to achieve efficient training and inference, drastically reducing computational overhead. Extensive experiments demonstrate the superiority of our framework over existing methods, with ablation studies confirming the efficacy of each design choice. Our work establishes a simple yet effective baseline for cascaded video super-resolution generation, offering practical insights to guide future advancements in efficient cascaded synthesis systems."
    },
    {
      "title": "KnowRL: 探索用于事实性的知识增强强化学习 (原标题: KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality)",
      "link": "https://arxiv.org/abs/2506.19807",
      "pubDate": "Tue, 24 Jun 2025 13:17:17 GMT",
      "isoDate": "2025-06-24T13:17:17.000Z",
      "creator": "Baochang Ren, Shuofei Qiao, Wenhao Yu, Huajun Chen, Ningyu Zhang",
      "summary": "本文提出了一种名为KnowRL的知识增强强化学习方法，旨在解决大型语言模型（LLMs）在推理过程中由于无法准确识别知识边界而产生的严重幻觉问题，尤其是在慢思考模型中。\n\n**主要观点：**\n\n*   **问题：** 大型语言模型（LLMs）的慢思考模型常常出现幻觉，输出不正确的内容，因为它们无法准确识别知识边界。\n*   **方法：** KnowRL通过将基于知识验证的事实性奖励整合到强化学习（RL）训练过程中，引导模型执行基于事实的慢思考，帮助它们识别知识边界。\n*   **机制：** KnowRL在RL训练期间提供有针对性的事实输入，使模型能够学习和内化基于事实的推理策略。通过直接奖励推理步骤中对事实的遵守，KnowRL培养了更可靠的思考过程。\n*   **实验结果：** 在三个幻觉评估数据集和两个推理评估数据集上的实验结果表明，KnowRL有效地缓解了慢思考模型中的幻觉，同时保持了其原有的强大推理能力。\n*   **代码：** 论文代码已开源，地址为：[this https URL](this https URL)。\n\n**关键词：**\n\n*   大型语言模型 (LLMs)\n*   强化学习 (RL)\n*   幻觉\n*   知识验证\n*   事实性奖励\n*   慢思考模型\n*   知识边界\n\n**研究领域：**\n\n*   人工智能 (cs.AI)\n*   计算与语言 (cs.CL)\n*   计算机视觉与模式识别 (cs.CV)\n*   机器学习 (cs.LG)\n*   多智能体系统 (cs.MA)",
      "shortSummary": "本文提出KnowRL，一种知识增强强化学习方法，旨在解决大型语言模型（LLMs）慢思考模型中普遍存在的幻觉问题。KnowRL通过将基于知识验证的事实性奖励整合到强化学习训练中，引导模型进行基于事实的慢思考，从而帮助模型识别知识边界。实验结果表明，KnowRL能有效缓解慢思考模型中的幻觉，同时保持其推理能力。论文代码已开源。",
      "translated_title": "KnowRL: 探索用于事实性的知识增强强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs), particularly slow-thinking models, often exhibit severe hallucination, outputting incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning. While Reinforcement Learning (RL) can enhance complex reasoning abilities, its outcome-oriented reward mechanism often lacks factual supervision over the thinking process, further exacerbating the hallucination problem. To address the high hallucination in slow-thinking models, we propose Knowledge-enhanced RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. This targeted factual input during RL training enables the model to learn and internalize fact-based reasoning strategies. By directly rewarding adherence to facts within the reasoning steps, KnowRL fosters a more reliable thinking process. Experimental results on three hallucination evaluation datasets and two reasoning evaluation datasets demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities. Our code is available at https://github.com/zjunlp/KnowRL."
    },
    {
      "title": "为什么开源大型语言模型在数据分析方面表现不佳？一项系统性实证研究 (原标题: Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study)",
      "link": "https://arxiv.org/abs/2506.19794",
      "pubDate": "Tue, 24 Jun 2025 13:04:23 GMT",
      "isoDate": "2025-06-24T13:04:23.000Z",
      "creator": "Yuqi Zhu, Yi Zhong, Jintian Zhang, Ziheng Zhang, Shuofei Qiao, Yujie Luo, Lun Du, Da Zheng, Huajun Chen, Ningyu Zhang",
      "summary": "### 为什么开源大型语言模型在数据分析方面表现不佳？一项系统性实证研究\n\n**摘要**\n\n大型语言模型（LLMs）在自动化数据分析任务方面具有潜力，但开源模型在此类推理密集型场景中面临显著限制。本研究旨在探讨增强开源LLMs数据分析能力的策略。\n\n**研究方法**\n\n*   **数据集构建**：策划了一个包含多样化、真实场景的种子数据集。\n*   **评估维度**：从三个维度评估模型：\n    1.  数据理解（Data Understanding）\n    2.  代码生成（Code Generation）\n    3.  战略规划（Strategic Planning）\n\n**主要发现**\n\n本研究的分析揭示了三个关键发现：\n\n1.  **战略规划质量是主要决定因素**：战略规划的质量是模型性能的主要决定因素。\n2.  **交互设计与任务复杂性影响推理能力**：交互设计和任务复杂性显著影响模型的推理能力。\n3.  **数据质量优于多样性**：在实现最佳性能方面，数据质量的影响大于数据多样性。\n\n**研究成果与展望**\n\n研究团队利用这些洞察开发了一种数据合成方法，并证明该方法显著提升了开源LLMs的分析推理能力。\n\n**其他信息**\n\n*   **状态**：该工作仍在进行中（Work in progress）。\n*   **主题**：计算与语言（cs.CL）、人工智能（cs.AI）、信息检索（cs.IR）、机器学习（cs.LG）、多智能体系统（cs.MA）。\n*   **引用**：arXiv:2506.19794 [cs.CL]。",
      "shortSummary": "本研究探讨了开源大型语言模型（LLMs）在数据分析方面的局限性及其增强策略。通过评估数据理解、代码生成和战略规划，研究发现战略规划质量是模型性能的关键决定因素，交互设计和任务复杂性会影响推理能力，且数据质量比多样性更重要。基于这些发现，研究开发了一种数据合成方法，显著提升了开源LLMs的分析推理能力。",
      "translated_title": "为什么开源大型语言模型在数据分析方面表现不佳？一项系统性实证研究",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate models across three dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities."
    },
    {
      "title": "SRFT：一种基于监督和强化微调的单阶段推理方法 (原标题: SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning)",
      "link": "https://arxiv.org/abs/2506.19767",
      "pubDate": "Tue, 24 Jun 2025 12:31:37 GMT",
      "isoDate": "2025-06-24T12:31:37.000Z",
      "creator": "Yuqian Fu, Tinghong Chen, Jiajun Chai, Xihuai Wang, Songjun Tu, Guojun Yin, Wei Lin, Qichao Zhang, Yuanheng Zhu, Dongbin Zhao",
      "summary": "本文提出了一种名为监督强化微调（SRFT）的单阶段方法，旨在解决大型语言模型（LLM）在推理任务中监督微调（SFT）和强化学习（RL）的最佳集成问题。通过对token分布、学习动态和集成机制的全面分析，揭示了SFT和RL之间的关键差异：SFT对LLM策略分布进行粗粒度的全局更改，而RL执行细粒度的选择性优化，熵是训练有效性的关键指标。SRFT通过熵感知加权机制统一了这两种微调范式，同时应用SFT和RL，使用演示和自我探索rollout直接优化LLM，而不是通过两阶段顺序方法。实验结果表明，SRFT在五个数学推理基准测试中实现了59.1%的平均准确率，优于零RL方法9.0%，在三个分布外基准测试中优于零RL方法10.9%。",
      "shortSummary": "本文提出了一种名为SRFT的单阶段方法，用于优化大型语言模型（LLM）的推理能力。该方法通过熵感知加权机制统一了监督微调（SFT）和强化学习（RL），实现了比传统方法更高的准确率，尤其是在数学推理和分布外基准测试中。",
      "translated_title": "SRFT：一种基于监督和强化微调的单阶段推理方法",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) have achieved remarkable progress in reasoning tasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) remains a fundamental challenge. Through comprehensive analysis of token distributions, learning dynamics, and integration mechanisms from entropy-based perspectives, we reveal key differences between these paradigms: SFT induces coarse-grained global changes to LLM policy distributions, while RL performs fine-grained selective optimizations, with entropy serving as a critical indicator of training effectiveness. Building on these observations, we propose Supervised Reinforcement Fine-Tuning (SRFT), a single-stage method that unifies both fine-tuning paradigms through entropy-aware weighting mechanisms. Our approach simultaneously applies SFT and RL to directly optimize the LLM using demonstrations and self-exploration rollouts rather than through two-stage sequential methods. Extensive experiments show that SRFT achieves 59.1% average accuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning benchmarks and 10.9% on three out-of-distribution benchmarks."
    },
    {
      "title": "频域引导在低CFG尺度下实现高保真采样 (原标题: Guidance in the Frequency Domain Enables High-Fidelity Sampling at Low CFG Scales)",
      "link": "https://arxiv.org/abs/2506.19713",
      "pubDate": "Tue, 24 Jun 2025 11:19:42 GMT",
      "isoDate": "2025-06-24T11:19:42.000Z",
      "creator": "Seyedmorteza Sadat, Tobias Vontobel, Farnood Salehi, Romann M. Weber",
      "summary": "### 频域引导在扩散模型中的新视角：频率解耦引导（FDG）\n\n**1. 引言：分类器无关引导（CFG）的挑战**\n分类器无关引导（CFG）已成为现代条件扩散模型中不可或缺的组成部分，显著提升了生成质量、细节和提示对齐度。然而，CFG增强这些特性的深层机制尚未被完全理解。\n\n**2. 频域分析：CFG的不同影响**\n本文提出了一种通过分析CFG在频域中的效应来理解其作用的新视角。研究表明，低频和高频对生成质量有着截然不同的影响：\n*   **低频引导**：主要控制全局结构和条件对齐。\n*   **高频引导**：主要增强视觉保真度。\n\n**3. 标准CFG的局限性**\n标准CFG在所有频率上应用统一的引导强度，这导致了以下问题：\n*   **高尺度下**：可能导致图像过饱和并降低多样性。\n*   **低尺度下**：可能导致视觉质量下降。\n\n**4. 提出的方法：频率解耦引导（FDG）**\n基于上述洞察，研究人员提出了一种有效的频率解耦引导（FDG）方法。FDG将CFG分解为低频和高频分量，并对每个分量应用单独的引导强度。\n\n**5. FDG的优势与实验结果**\nFDG的设计旨在解决标准CFG的局限性，并带来了显著的改进：\n*   **图像质量提升**：在低引导尺度下显著改善图像质量。\n*   **避免高CFG尺度弊端**：通过设计避免了高CFG尺度带来的过饱和和多样性降低问题。\n*   **保真度与多样性**：FDG持续增强样本保真度，同时保持了多样性。\n*   **性能指标提升**：与标准CFG相比，FDG在FID（Fréchet Inception Distance）和召回率（recall）方面均有所提升。\n*   **即插即用**：该方法可作为标准分类器无关引导的即插即用替代方案。\n\n**6. 结论**\n通过在多个数据集和模型上进行的广泛实验，本文证明了FDG的有效性，为扩散模型中的引导机制提供了新的理解和改进方案。",
      "shortSummary": "本文提出“频域引导在低CFG尺度下实现高保真采样”的新方法。研究发现，分类器无关引导（CFG）在低频影响全局结构，高频影响视觉细节。标准CFG统一尺度导致高尺度过饱和、低尺度质量差。为此，提出频率解耦引导（FDG），分别控制低频和高频引导强度。FDG在低引导尺度下提升图像质量，避免高CFG弊端，同时增强样本保真度并保持多样性，显著改善FID和召回率，是CFG的即插即用替代方案。",
      "translated_title": "频域引导在低CFG尺度下实现高保真采样",
      "images": [],
      "contentSource": "完整文章",
      "content": "Classifier-free guidance (CFG) has become an essential component of modern conditional diffusion models. Although highly effective in practice, the underlying mechanisms by which CFG enhances quality, detail, and prompt alignment are not fully understood. We present a novel perspective on CFG by analyzing its effects in the frequency domain, showing that low and high frequencies have distinct impacts on generation quality. Specifically, low-frequency guidance governs global structure and condition alignment, while high-frequency guidance mainly enhances visual fidelity. However, applying a uniform scale across all frequencies -- as is done in standard CFG -- leads to oversaturation and reduced diversity at high scales and degraded visual quality at low scales. Based on these insights, we propose frequency-decoupled guidance (FDG), an effective approach that decomposes CFG into low- and high-frequency components and applies separate guidance strengths to each component. FDG improves image quality at low guidance scales and avoids the drawbacks of high CFG scales by design. Through extensive experiments across multiple datasets and models, we demonstrate that FDG consistently enhances sample fidelity while preserving diversity, leading to improved FID and recall compared to CFG, establishing our method as a plug-and-play alternative to standard classifier-free guidance."
    },
    {
      "title": "Skywork-SWE：揭示LLM中软件工程的数据扩展定律 (原标题: Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs)",
      "link": "https://arxiv.org/abs/2506.19290",
      "pubDate": "Mon, 23 Jun 2025 23:53:36 GMT",
      "isoDate": "2025-06-23T23:53:36.000Z",
      "creator": "Liang Zeng, Yongcong Li, Yuzhen Xiao, Changshi Li, Chris Yuhao Liu, Rui Yan, Tianwen Wei, Jujie He, Xuchen Song, Yang Liu, Yahui Zhou",
      "summary": "## Skywork-SWE：LLM软件工程数据扩展定律的揭示\n\n### 引言：LLM在软件工程中的挑战\n\n软件工程（SWE）已成为下一代大型语言模型（LLM）代理的关键测试平台。这要求LLM具备两项核心能力：\n\n*   **持续迭代问题解决**：例如，超过50轮的交互。\n*   **长上下文依赖解析**：例如，处理超过32k个token。\n\n然而，当前SWE领域的数据收集过程面临巨大挑战，因为它高度依赖人工标注进行代码文件过滤，并需要设置专门的运行时环境来执行和验证单元测试。这导致现有数据集规模普遍较小，通常只有数千个GitHub实例。\n\n### Skywork-SWE的解决方案：自动化数据收集与大规模数据集\n\n为了解决数据稀缺问题，研究人员提出了一个**增量式、自动化数据收集管道**，旨在系统性地扩展SWE数据集的规模和多样性。\n\n*   **数据集构成**：该数据集包含来自2,531个不同GitHub仓库的10,169个真实世界Python任务实例。\n*   **数据特性**：每个任务实例都附带自然语言的任务描述，以及一个专用的运行时环境镜像，用于自动化单元测试验证。\n*   **训练轨迹**：研究团队精心整理了超过8,000条成功通过运行时验证的训练轨迹，用于模型微调。\n\n### 关键发现：数据扩展定律\n\n通过在这些轨迹上对Skywork-SWE模型进行微调，研究人员发现了一个显著的**数据扩展现象**：\n\n*   随着数据量的增加，训练后的模型在LLM软件工程能力方面的表现持续提升，**没有出现饱和迹象**。这表明，在SWE领域，更多的数据能够持续带来性能增益。\n\n### 性能突破与模型发布\n\nSkywork-SWE模型在SWE-bench Verified基准测试上取得了显著的性能突破：\n\n*   **基准性能**：在不使用验证器或多次运行的情况下，Skywork-SWE模型实现了**38.0%的pass@1准确率**。\n*   **行业领先地位**：这使其在基于OpenHands代理框架构建的Qwen2.5-Coder-32B LLM中，确立了新的**最先进（SOTA）水平**。\n*   **测试时扩展技术**：通过结合测试时扩展技术，模型性能进一步提升至**47.0%的准确率**，超越了之前参数量小于32B的模型的最先进结果。\n\n为了加速未来的研究，研究团队已发布了**Skywork-SWE-32B模型检查点**。",
      "shortSummary": "Skywork-SWE项目提出了一种自动化数据收集管道，解决了LLM软件工程（SWE）领域数据稀缺的问题。他们构建了一个包含10,169个Python任务实例的大规模数据集，并发现LLM的SWE能力随数据量增加而持续提升，无饱和迹象。Skywork-SWE模型在SWE-bench Verified基准测试上取得了38.0%的pass@1准确率，结合测试时扩展技术可达47.0%，创下32B以下参数模型的SOTA。Skywork-SWE-32B模型检查点已发布，以促进未来研究。",
      "translated_title": "Skywork-SWE：揭示LLM中软件工程的数据扩展定律",
      "images": [],
      "contentSource": "完整文章",
      "content": "Software engineering (SWE) has recently emerged as a crucial testbed for next-generation LLM agents, demanding inherent capabilities in two critical dimensions: sustained iterative problem-solving (e.g., &gt;50 interaction rounds) and long-context dependency resolution (e.g., &gt;32k tokens). However, the data curation process in SWE remains notoriously time-consuming, as it heavily relies on manual annotation for code file filtering and the setup of dedicated runtime environments to execute and validate unit tests. Consequently, most existing datasets are limited to only a few thousand GitHub-sourced instances. To this end, we propose an incremental, automated data-curation pipeline that systematically scales both the volume and diversity of SWE datasets. Our dataset comprises 10,169 real-world Python task instances from 2,531 distinct GitHub repositories, each accompanied by a task specified in natural language and a dedicated runtime-environment image for automated unit-test validation. We have carefully curated over 8,000 successfully runtime-validated training trajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE model on these trajectories, we uncover a striking data scaling phenomenon: the trained model's performance for software engineering capabilities in LLMs continues to improve as the data size increases, showing no signs of saturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on the SWE-bench Verified benchmark without using verifiers or multiple rollouts, establishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based LLMs built on the OpenHands agent framework. Furthermore, with the incorporation of test-time scaling techniques, the performance further improves to 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter models. We release the Skywork-SWE-32B model checkpoint to accelerate future research."
    },
    {
      "title": "VMem：基于Surfel索引视图记忆的一致交互式视频场景生成 (原标题: VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory)",
      "link": "https://arxiv.org/abs/2506.18903",
      "pubDate": "Mon, 23 Jun 2025 13:59:56 GMT",
      "isoDate": "2025-06-23T13:59:56.000Z",
      "creator": "Runjia Li, Philip Torr, Andrea Vedaldi, Tomas Jakab",
      "summary": "## VMem：基于Surfel索引视图记忆的一致交互式视频场景生成\n\n### 摘要\n\n本文提出了一种名为“Surfel索引视图记忆”（VMem）的新型记忆机制，旨在构建能够交互式探索环境的视频生成器。现有方法在处理长期场景连贯性和错误累积方面存在显著局限性，VMem旨在解决这些问题。\n\n### 现有方法的局限性\n\n*   **2D视图外绘与3D几何重建：** 这种方法虽然能实现交互式探索，但会迅速累积误差，导致场景不一致。\n*   **短上下文窗口视频生成器：** 这类生成器难以在长期内保持场景的连贯性，随着时间的推移，场景细节容易出现偏差。\n\n### VMem的创新机制\n\n*   **核心思想：** VMem通过几何索引过去视图来“记忆”它们，这种索引基于这些视图所观察到的3D表面元素（surfels）。\n*   **高效检索：** 这种索引机制使得在生成新视图时，能够高效地检索出最相关的历史视图。\n*   **聚焦相关视图：** 通过仅关注这些相关的历史视图，VMem能够以远低于将所有历史视图作为上下文的计算成本，生成一致的想象环境探索。\n\n### 性能评估\n\n*   **基准测试：** 该方法在具有挑战性的长期场景合成基准测试中进行了评估。\n*   **卓越表现：** 与现有方法相比，VMem在保持场景连贯性和相机控制方面展现出卓越的性能。",
      "shortSummary": "VMem提出了一种基于Surfel索引视图记忆的新型机制，旨在解决交互式视频场景生成中长期场景连贯性差和错误累积的问题。通过几何索引过去视图并高效检索相关信息，VMem能以更低的计算成本生成一致的虚拟环境探索。该方法在长期场景合成基准测试中表现出优于现有方法的场景连贯性和相机控制能力。",
      "translated_title": "VMem：基于Surfel索引视图记忆的一致交互式视频场景生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "We propose a novel memory mechanism to build video generators that can explore environments interactively. Similar results have previously been achieved by out-painting 2D views of the scene while incrementally reconstructing its 3D geometry, which quickly accumulates errors, or by video generators with a short context window, which struggle to maintain scene coherence over the long term. To address these limitations, we introduce Surfel-Indexed View Memory (VMem), a mechanism that remembers past views by indexing them geometrically based on the 3D surface elements (surfels) they have observed. VMem enables the efficient retrieval of the most relevant past views when generating new ones. By focusing only on these relevant views, our method produces consistent explorations of imagined environments at a fraction of the computational cost of using all past views as context. We evaluate our approach on challenging long-term scene synthesis benchmarks and demonstrate superior performance compared to existing methods in maintaining scene coherence and camera control."
    },
    {
      "title": "视觉作为一种方言：通过文本对齐表示统一视觉理解与生成 (原标题: Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations)",
      "link": "https://arxiv.org/abs/2506.18898",
      "pubDate": "Mon, 23 Jun 2025 13:59:14 GMT",
      "isoDate": "2025-06-23T13:59:14.000Z",
      "creator": "Jiaming Han, Hao Chen, Yang Zhao, Hanyu Wang, Qi Zhao, Ziyan Yang, Hao He, Xiangyu Yue, Lu Jiang",
      "summary": "## 视觉作为一种方言：通过文本对齐表示统一视觉理解与生成\n\n本文提出了一种多模态框架，旨在通过共享的离散语义表示来统一视觉理解和生成。该框架的核心组件是**文本对齐分词器（Text-Aligned Tokenizer, TA-Tok）**。\n\n### 核心机制\n\n*   **TA-Tok 的功能**：将图像转换为离散的视觉标记（tokens）。\n*   **文本对齐码本**：TA-Tok 使用一个从大型语言模型（LLM）词汇表中投射出来的文本对齐码本，确保视觉标记与文本语义对齐。\n*   **统一表示空间**：通过这种方式，视觉和文本被整合到一个具有扩展词汇量的统一空间中。\n\n### 多模态 LLM (Tar)\n\n*   **跨模态输入输出**：该框架中的多模态 LLM 被命名为 Tar，它通过一个共享接口实现跨模态的输入和输出，无需针对特定模态进行单独设计。\n\n### 创新技术\n\n*   **尺度自适应编码与解码**：为平衡效率和视觉细节，论文提出了尺度自适应的编码和解码机制。\n*   **生成式去分词器**：为了生成高保真度的视觉输出，Tar 采用了生成式去分词器。\n*   **互补的去分词器**：为满足不同的解码需求，系统利用了两种互补的去分词器：一个快速自回归模型和一个基于扩散的模型。\n\n### 预训练与性能\n\n*   **模态融合增强**：研究人员探索了先进的预训练任务，以增强模态间的融合。\n*   **实验结果**：在多个基准测试中，Tar 的表现与现有多模态 LLM 方法持平或超越，并展现出更快的收敛速度和更高的训练效率。",
      "shortSummary": "本文提出了一个名为 Tar 的多模态框架，旨在通过共享的离散语义表示统一视觉理解和生成。其核心是文本对齐分词器（TA-Tok），它将图像转换为与大型语言模型词汇表对齐的离散标记，从而将视觉和文本整合到统一空间。Tar 作为多模态LLM，支持跨模态输入输出，并引入了尺度自适应编码/解码和生成式去分词器。实验表明，Tar 在性能和训练效率上均优于现有方法。",
      "translated_title": "视觉作为一种方言：通过文本对齐表示统一视觉理解与生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper presents a multimodal framework that attempts to unify visual understanding and generation within a shared discrete semantic representation. At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using a text-aligned codebook projected from a large language model's (LLM) vocabulary. By integrating vision and text into a unified space with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input and output through a shared interface, without the need for modality-specific designs. Additionally, we propose scale-adaptive encoding and decoding to balance efficiency and visual detail, along with a generative de-tokenizer to produce high-fidelity visual outputs. To address diverse decoding needs, we utilize two complementary de-tokenizers: a fast autoregressive model and a diffusion-based model. To enhance modality fusion, we investigate advanced pre-training tasks, demonstrating improvements in both visual understanding and generation. Experiments across benchmarks show that Tar matches or surpasses existing multimodal LLM methods, achieving faster convergence and greater training efficiency. Code, models, and data are available at https://tar.csuhan.com"
    },
    {
      "title": "ReasonFlux-PRM：用于LLM中长链式思维推理的轨迹感知PRM (原标题: ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs)",
      "link": "https://arxiv.org/abs/2506.18896",
      "pubDate": "Mon, 23 Jun 2025 13:59:02 GMT",
      "isoDate": "2025-06-23T13:59:02.000Z",
      "creator": "Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, Mengdi Wang",
      "summary": "### ReasonFlux-PRM：LLM中轨迹感知过程奖励模型\n\n**1. 引言与背景**\n*   **现有问题：** 过程奖励模型（PRM）是监督大型语言模型（LLM）中间推理步骤的强大框架。然而，先前的PRM主要基于模型的最终输出响应进行训练，难以鲁棒地评估中间思维轨迹，尤其是在Deepseek-R1等前沿推理模型生成的轨迹-响应输出的新兴场景中。\n\n**2. ReasonFlux-PRM的提出**\n*   **核心创新：** 本文引入了ReasonFlux-PRM，这是一种新颖的轨迹感知PRM，专门设计用于评估轨迹-响应类型的推理痕迹。\n*   **关键特性：** ReasonFlux-PRM整合了**步骤级**和**轨迹级**的监督，从而能够进行与结构化思维链数据对齐的细粒度奖励分配。\n\n**3. 应用场景**\nReasonFlux-PRM被设计用于支持离线和在线设置下的奖励监督，具体包括：\n*   **(i) 数据蒸馏：** 为下游较小模型的监督微调（SFT）选择高质量的模型蒸馏数据。\n*   **(ii) 强化学习：** 在强化学习（RL）过程中提供密集的进程级奖励，以优化策略。\n*   **(iii) 测试时扩展：** 实现奖励引导的Best-of-N测试时扩展。\n\n**4. 实验结果与性能**\n*   **基准测试：** 在AIME、MATH500和GPQA-Diamond等具有挑战性的下游基准测试中进行了实证评估。\n*   **数据质量：** 结果表明，ReasonFlux-PRM-7B选择的数据质量高于强大的PRM（例如Qwen2.5-Math-PRM-72B）和人工策划的基线。\n*   **性能提升：** 派生的ReasonFlux-PRM-7B带来了持续的性能改进，平均增益如下：\n    *   监督微调（SFT）：12.1%\n    *   强化学习（RL）：4.5%\n    *   测试时扩展：6.3%\n\n**5. 模型发布**\n*   为了支持资源受限的应用和边缘部署，研究团队还发布了高效的ReasonFlux-PRM-1.5B模型。",
      "shortSummary": "ReasonFlux-PRM是一种新型轨迹感知过程奖励模型（PRM），旨在解决现有PRM在评估LLM中间思维轨迹方面的不足。它通过整合步骤级和轨迹级监督，为轨迹-响应输出提供细粒度奖励。ReasonFlux-PRM支持数据蒸馏、强化学习和测试时扩展。在AIME、MATH500和GPQA-Diamond等基准测试中，ReasonFlux-PRM-7B在数据选择上优于现有PRM和人工基线，并在SFT、RL和测试时扩展中分别实现了12.1%、4.5%和6.3%的性能提升。团队还发布了轻量级ReasonFlux-PRM-1.5B模型。",
      "translated_title": "ReasonFlux-PRM：用于LLM中长链式思维推理的轨迹感知PRM",
      "images": [],
      "contentSource": "完整文章",
      "content": "Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory-response outputs generated by frontier reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling. We also release our efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Projects: https://github.com/Gen-Verse/ReasonFlux"
    },
    {
      "title": "法线之光：通用光度立体视觉的统一特征表示 (原标题: Light of Normals: Unified Feature Representation for Universal Photometric Stereo)",
      "link": "https://arxiv.org/abs/2506.18882",
      "pubDate": "Mon, 23 Jun 2025 13:53:11 GMT",
      "isoDate": "2025-06-23T13:53:11.000Z",
      "creator": "Hong Li, Houyuan Chen, Chongjie Ye, Zhaoxi Chen, Bohan Li, Shaocong Xu, Xianda Guo, Xuhui Liu, Yikai Wang, Baochang Zhang, Satoshi Ikehata, Boxin Shi, Anyi Rao, Hao Zhao",
      "summary": "### 通用光度立体视觉的挑战与进展\n\n本文探讨了通用光度立体视觉（Universal Photometric Stereo, PS）领域面临的核心挑战，该技术旨在在任意光照条件下，无需依赖特定的光照模型，即可恢复物体表面的高质量法线。\n\n尽管近期在通用光度立体视觉方面取得了一些进展，例如SDM-UniPS和Uni MS-PS等方法，但该领域仍存在两个根本性难题：\n\n1.  **光照与表面法线特征的深度耦合**：\n    *   在观察到的强度中存在模糊性，使得难以确定亮度变化是源于光照变化还是表面方向。这种深度耦合使得区分这两种因素变得复杂。\n\n2.  **复杂表面高频几何细节的保留**：\n    *   传统特征处理操作难以准确捕捉复杂几何形状中固有的高频细节。这些细节包括：\n        *   自阴影（self-shadowing）\n        *   相互反射（inter-reflections）\n        *   以及细微的法线变化\n    *   这些复杂性使得准确恢复表面法线变得更具挑战性。",
      "shortSummary": "本文讨论了通用光度立体视觉（PS）面临的挑战。该技术旨在从任意光照下的物体中恢复高质量表面法线，无需特定光照模型。尽管现有方法有所进展，但仍存在两大难题：一是光照与表面法线特征的深度耦合，导致亮度变化来源难以区分；二是难以保留复杂表面（如自阴影、相互反射）的高频几何细节，传统方法难以准确捕捉。",
      "translated_title": "法线之光：通用光度立体视觉的统一特征表示",
      "images": [],
      "contentSource": "完整文章",
      "content": "Universal photometric stereo (PS) aims to recover high-quality surface normals from objects under arbitrary lighting conditions without relying on specific illumination models. Despite recent advances such as SDM-UniPS and Uni MS-PS, two fundamental challenges persist: 1) the deep coupling between varying illumination and surface normal features, where ambiguity in observed intensity makes it difficult to determine whether brightness variations stem from lighting changes or surface orientation; and 2) the preservation of high-frequency geometric details in complex surfaces, where intricate geometries create self-shadowing, inter-reflections, and subtle normal variations that conventional feature processing operations struggle to capture accurately."
    },
    {
      "title": "CommVQ：KV缓存压缩的交换向量量化 (原标题: CommVQ: Commutative Vector Quantization for KV Cache Compression)",
      "link": "https://arxiv.org/abs/2506.18879",
      "pubDate": "Mon, 23 Jun 2025 13:50:11 GMT",
      "isoDate": "2025-06-23T13:50:11.000Z",
      "creator": "Junyan Li, Yang Zhang, Muhammad Yusuf Hassan, Talha Chafekar, Tianle Cai, Zhile Ren, Pengsheng Guo, Foroozan Karimzadeh, Colorado Reed, Chong Wang, Chuang Gan",
      "summary": "### CommVQ：KV缓存压缩的交换向量量化\n\n**引言**\n\n大型语言模型（LLMs）在需要长上下文的应用中越来越普及，但随着上下文长度的增长，键值（KV）缓存常常成为GPU上的内存瓶颈。为了解决这一问题，研究人员提出了Commutative Vector Quantization（CommVQ）方法，旨在显著减少长上下文LLM推理的内存使用。\n\n**CommVQ方法**\n\nCommVQ的核心在于其创新的量化和解码机制：\n\n1.  **加性量化（Additive Quantization）**\n    *   CommVQ首先引入了加性量化，利用轻量级编码器和码本对KV缓存进行压缩。\n    *   这种压缩后的数据可以通过简单的矩阵乘法进行解码，确保了高效的解码过程。\n\n2.  **RoPE交换码本设计（RoPE-Commutative Codebook Design）**\n    *   为了进一步降低解码过程中的计算成本，CommVQ设计了一种与旋转位置嵌入（Rotary Position Embedding, RoPE）具有交换性的码本。\n    *   该码本通过期望最大化（EM）算法进行训练，使其能够与RoPE操作兼容。\n    *   这种设计使得解码过程能够高效地集成到自注意力机制中，从而优化了整体计算效率。\n\n**优势与实验结果**\n\nCommVQ方法展现出显著的优势和卓越的性能：\n\n*   **高精度与低开销**：通过加性量化实现高精度，并通过RoPE交换码本确保低计算开销。\n*   **内存显著减少**：在长上下文基准测试和GSM8K上的实验表明，CommVQ使用2位量化可以将FP16 KV缓存大小减少87.5%。\n*   **超越现有技术**：该方法在KV缓存量化方面优于当前最先进的技术。\n*   **支持极低位量化**：CommVQ能够实现1位KV缓存量化，且精度损失极小。\n*   **实际应用能力**：例如，使用CommVQ，一个LLaMA-3.1 8B模型可以在单张RTX 4090 GPU上运行128K的上下文长度，这在之前是难以实现的。",
      "shortSummary": "CommVQ是一种针对大型语言模型（LLMs）KV缓存内存瓶颈的解决方案。它采用加性量化和与旋转位置嵌入（RoPE）具有交换性的码本设计，以显著压缩KV缓存并降低解码计算成本。实验证明，CommVQ能将KV缓存大小减少87.5%（2位量化），并支持1位量化，从而使LLaMA-3.1 8B模型在单张RTX 4090 GPU上运行128K上下文，性能优于现有方法。",
      "translated_title": "CommVQ：KV缓存压缩的交换向量量化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes a memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with a lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. The source code is available at: https://github.com/UMass-Embodied-AGI/CommVQ."
    },
    {
      "title": "OmniGen2：探索先进多模态生成 (原标题: OmniGen2: Exploration to Advanced Multimodal Generation)",
      "link": "https://arxiv.org/abs/2506.18871",
      "pubDate": "Mon, 23 Jun 2025 13:38:54 GMT",
      "isoDate": "2025-06-23T13:38:54.000Z",
      "creator": "Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, Zheng Liu",
      "summary": "# OmniGen2：探索先进多模态生成\n\n## 概述\nOmniGen2是一个多功能、开源的生成模型，旨在为多种生成任务提供统一的解决方案，包括文本到图像生成、图像编辑和上下文生成。它旨在提供一个统一的解决方案，以应对多样化的生成任务。\n\n## 核心特性与改进\n*   **双重解码路径：** 与OmniGen v1不同，OmniGen2为文本和图像模态设计了两个独立的解码路径，并使用不共享的参数。\n*   **解耦图像分词器：** 采用了独立的图像分词器。\n*   **兼容性与能力保留：** 这种设计使得OmniGen2能够基于现有的多模态理解模型进行构建，无需重新调整VAE输入，从而完整保留了原始的文本生成能力。\n\n## 数据构建与训练\n*   **全面的数据构建流程：** 为促进OmniGen2的训练，研究团队开发了全面的数据构建流程，涵盖了图像编辑和上下文生成数据。\n*   **反射机制与数据集：** 引入了一种专为图像生成任务设计的反射机制，并基于OmniGen2策划了一个专门的反射数据集。\n\n## 性能表现\n*   **参数规模与竞争力：** 尽管参数规模相对适中，OmniGen2在多个任务基准测试中（包括文本到图像和图像编辑）取得了有竞争力的结果。\n*   **OmniContext基准：** 为了进一步评估上下文生成（也称为主题驱动任务），研究团队引入了一个名为OmniContext的新基准。\n*   **领先表现：** 在OmniContext基准上，OmniGen2在开源模型中展现出最先进的一致性表现。\n\n## 资源发布\n研究团队计划发布OmniGen2的模型、训练代码、数据集以及数据构建流程，以支持该领域的未来研究。\n\n## 作者\nChenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, Zheng Liu",
      "shortSummary": "OmniGen2是一个多功能、开源的生成模型，旨在统一文本到图像、图像编辑和上下文生成任务。它采用独立的文本和图像解码路径及解耦的图像分词器，在保留文本生成能力的同时，能基于现有模型构建。OmniGen2在多项基准测试中表现出色，并在新引入的OmniContext基准上，在开源模型中实现了最先进的上下文生成一致性。相关模型、代码和数据集将公开发布。",
      "translated_title": "OmniGen2：探索先进多模态生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this work, we introduce OmniGen2, a versatile and open-source generative model designed to provide a unified solution for diverse generation tasks, including text-to-image, image editing, and in-context generation. Unlike OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image modalities, utilizing unshared parameters and a decoupled image tokenizer. This design enables OmniGen2 to build upon existing multimodal understanding models without the need to re-adapt VAE inputs, thereby preserving the original text generation capabilities. To facilitate the training of OmniGen2, we developed comprehensive data construction pipelines, encompassing image editing and in-context generation data. Additionally, we introduce a reflection mechanism tailored for image generation tasks and curate a dedicated reflection dataset based on OmniGen2. Despite its relatively modest parameter size, OmniGen2 achieves competitive results on multiple task benchmarks, including text-to-image and image editing. To further evaluate in-context generation, also referred to as subject-driven tasks, we introduce a new benchmark named OmniContext. OmniGen2 achieves state-of-the-art performance among open-source models in terms of consistency. We will release our models, training code, datasets, and data construction pipeline to support future research in this field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link: https://github.com/VectorSpaceLab/OmniGen2"
    },
    {
      "title": "Phantom-Data：迈向通用主体一致性视频生成数据集 (原标题: Phantom-Data : Towards a General Subject-Consistent Video Generation Dataset)",
      "link": "https://arxiv.org/abs/2506.18851",
      "pubDate": "Mon, 23 Jun 2025 13:11:56 GMT",
      "isoDate": "2025-06-23T13:11:56.000Z",
      "creator": "Zhuowei Chen, Bingchuan Li, Tianxiang Ma, Lijie Liu, Mingcong Liu, Yi Zhang, Gen Li, Xinghui Li, Siyu Zhou, Qian He, Xinglong Wu",
      "summary": "## Phantom-Data：解决主体一致性视频生成中的“复制粘贴”问题\n\n### 背景与挑战\n\n*   **主体到视频生成（Subject-to-video generation）**近年来取得了显著进展。\n*   然而，现有模型在忠实遵循文本指令方面仍面临重大挑战，这被称为**“复制粘贴问题”**。\n*   该问题源于广泛使用的**“成对训练范式”（in-pair training paradigm）**，即从与目标视频相同场景中采样参考图像。这种方法固有地将主体身份与背景和上下文属性纠缠在一起。\n\n### 解决方案：Phantom-Data 数据集\n\n*   为了解决上述问题，研究人员引入了 **Phantom-Data**，这是**首个通用跨对（cross-pair）主体到视频一致性数据集**。\n*   该数据集包含大约**一百万个身份一致的图像对**，涵盖了多样化的类别。\n\n### Phantom-Data 的构建流程\n\nPhantom-Data 的构建通过一个三阶段的流水线完成：\n\n1.  **通用且输入对齐的主体检测模块（General and input-aligned subject detection module）**：用于准确识别和定位图像中的主体。\n2.  **大规模跨上下文主体检索（Large-scale cross-context subject retrieval）**：从超过5300万个视频和30亿张图像中检索具有相同主体的不同上下文图像。\n3.  **先验引导的身份验证（Prior-guided identity verification）**：确保在上下文变化下视觉一致性得到保持。\n\n### 实验结果\n\n*   全面的实验表明，使用 Phantom-Data 进行训练显著改善了**提示对齐（prompt alignment）**和**视觉质量（visual quality）**。\n*   同时，它在**身份一致性（identity consistency）**方面与成对基线（in-pair baselines）保持了同等水平。",
      "shortSummary": "现有主体到视频生成模型面临“复制粘贴问题”，因成对训练将主体与背景混淆。为解决此，研究人员推出了Phantom-Data，首个通用跨对主体一致性视频生成数据集。该数据集包含约一百万个身份一致的图像对，通过主体检测、跨上下文检索和身份验证构建。实验证明，Phantom-Data显著提升了提示对齐和视觉质量，同时保持了身份一致性。",
      "translated_title": "Phantom-Data：迈向通用主体一致性视频生成数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "Subject-to-video generation has witnessed substantial progress in recent years. However, existing models still face significant challenges in faithfully following textual instructions. This limitation, commonly known as the copy-paste problem, arises from the widely used in-pair training paradigm. This approach inherently entangles subject identity with background and contextual attributes by sampling reference images from the same scene as the target video. To address this issue, we introduce Phantom-Data, the first general-purpose cross-pair subject-to-video consistency dataset, containing approximately one million identity-consistent pairs across diverse categories. Our dataset is constructed via a three-stage pipeline: (1) a general and input-aligned subject detection module, (2) large-scale cross-context subject retrieval from more than 53 million videos and 3 billion images, and (3) prior-guided identity verification to ensure visual consistency under contextual variation. Comprehensive experiments show that training with Phantom-Data significantly improves prompt alignment and visual quality while preserving identity consistency on par with in-pair baselines."
    },
    {
      "title": "USAD：通过蒸馏实现通用语音和音频表示 (原标题: USAD: Universal Speech and Audio Representation via Distillation)",
      "link": "https://arxiv.org/abs/2506.18843",
      "pubDate": "Mon, 23 Jun 2025 13:02:00 GMT",
      "isoDate": "2025-06-23T13:02:00.000Z",
      "creator": "Heng-Jui Chang, Saurabhchand Bhati, James Glass, Alexander H. Liu",
      "summary": "## USAD：通过蒸馏实现通用语音和音频表示\n\n### 摘要\n\n自监督学习（SSL）彻底改变了音频表示领域，但现有模型通常仍局限于特定领域，仅专注于语音或非语音任务。为了解决这一局限性，本文提出了 **通用语音和音频蒸馏（USAD）**，这是一种统一的音频表示学习方法。\n\n### USAD 方法\n\n*   **统一方法**：USAD 旨在将语音、声音和音乐等多种音频类型整合到一个单一模型中，从而实现通用的音频表示。\n*   **核心技术**：USAD 采用高效的“层到层蒸馏”（layer-to-layer distillation）策略。它从领域特定的自监督学习模型中提取知识，用于训练一个学生模型，该学生模型在一个全面的音频数据集上进行学习。\n\n### 性能表现\n\n*   **广泛适用性**：USAD 在各种基准测试和数据集上展现出具有竞争力的性能。\n*   **涵盖任务**：其应用范围包括帧级和实例级语音处理任务、音频标记以及声音分类。\n*   **卓越成果**：在 SUPERB 和 HEAR 基准测试中，USAD 使用单一编码器实现了接近当前最先进（near state-of-the-art）的性能。",
      "shortSummary": "USAD（通用语音和音频蒸馏）是一种统一的音频表示学习方法，旨在通过将语音、声音和音乐整合到单一模型中，克服现有自监督学习模型领域特异性的局限。它采用高效的层到层蒸馏技术，从领域特定模型中学习。USAD 在 SUPERB 和 HEAR 等基准测试中表现出色，在多种语音和音频任务上使用单一编码器实现了接近最先进的性能。",
      "translated_title": "USAD：通过蒸馏实现通用语音和音频表示",
      "images": [],
      "contentSource": "完整文章",
      "content": "Self-supervised learning (SSL) has revolutionized audio representations, yet models often remain domain-specific, focusing on either speech or non-speech tasks. In this work, we present Universal Speech and Audio Distillation (USAD), a unified approach to audio representation learning that integrates diverse audio types - speech, sound, and music - into a single model. USAD employs efficient layer-to-layer distillation from domain-specific SSL models to train a student on a comprehensive audio dataset. USAD offers competitive performance across various benchmarks and datasets, including frame and instance-level speech processing tasks, audio tagging, and sound classification, achieving near state-of-the-art results with a single encoder on SUPERB and HEAR benchmarks."
    },
    {
      "title": "LongWriter-Zero：通过强化学习掌握超长文本生成 (原标题: LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2506.18841",
      "pubDate": "Mon, 23 Jun 2025 12:59:02 GMT",
      "isoDate": "2025-06-23T12:59:02.000Z",
      "creator": "Yuhao Wu, Yushi Bai, Zhiqiang Hu, Roy Ka-Wei Lee, Juanzi Li",
      "summary": "## LongWriter-Zero：通过强化学习实现超长文本生成\n\n### 挑战与现有方法\n大型语言模型（LLMs）在生成超长文本时面临显著挑战，主要体现在最大生成长度限制以及随着序列长度增加而导致的整体质量下降。传统的解决方案，例如LongWriter，通常依赖于“教学”方法，即对合成的长文本输出进行监督微调（SFT）。然而，这种策略存在诸多局限性：\n*   **数据依赖性高**：严重依赖合成SFT数据，而此类数据构建起来既困难又昂贵。\n*   **质量问题**：合成数据往往缺乏连贯性和一致性。\n*   **人工痕迹重**：生成内容可能过于人工化，结构单调。\n\n### LongWriter-Zero 方法\n为了克服上述挑战，本文提出了一种名为LongWriter-Zero的激励式方法，该方法完全从零开始，不依赖任何标注或合成数据，而是利用强化学习（RL）来培养LLM生成超长、高质量文本的能力。其核心思想包括：\n*   **从基础模型开始**：RL训练从一个基础模型（类似于R1-Zero）开始。\n*   **引导推理过程**：引导模型在写作过程中进行推理，从而促进规划和细化。\n*   **专业奖励模型**：采用专门的奖励模型来指导LLM，以实现：\n    *   改进的长度控制。\n    *   提升写作质量。\n    *   优化结构格式。\n\n### 实验结果与性能\n实验评估表明，LongWriter-Zero模型（基于Qwen2.5-32B训练）在长文本写作任务上持续优于传统的SFT方法，取得了显著的成果：\n*   **SOTA表现**：在WritingBench和Arena-Write的所有指标上均达到了最先进（SOTA）的水平。\n*   **超越大型模型**：甚至超越了DeepSeek R1和Qwen3-235B等100B+参数的模型。\n\n### 开放资源\n研究团队已开源了相关数据和模型检查点。",
      "shortSummary": "LongWriter-Zero提出一种基于强化学习的新方法，旨在解决大型语言模型在超长文本生成中的质量和长度限制问题。与依赖合成数据的传统监督微调不同，LongWriter-Zero从零开始，通过强化学习和专业奖励模型，引导模型进行规划和细化，从而提升文本长度控制、写作质量和结构。实验证明，LongWriter-Zero在长文本写作任务上表现优异，超越了传统方法甚至更大的模型，达到了最先进水平。",
      "translated_title": "LongWriter-Zero：通过强化学习掌握超长文本生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on ''teaching'', which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. In this work, we propose an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. We perform RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. We open-source our data and model checkpoints under https://huggingface.co/THU-KEG/LongWriter-Zero-32B"
    },
    {
      "title": "Matrix-Game：交互式世界基础模型 (原标题: Matrix-Game: Interactive World Foundation Model)",
      "link": "https://arxiv.org/abs/2506.18701",
      "pubDate": "Mon, 23 Jun 2025 10:40:49 GMT",
      "isoDate": "2025-06-23T10:40:49.000Z",
      "creator": "Yifan Zhang, Chunli Peng, Boyang Wang, Puyi Wang, Qingcheng Zhu, Fei Kang, Biao Jiang, Zedong Gao, Eric Li, Yang Liu, Yahui Zhou",
      "summary": "## Matrix-Game：交互式世界基础模型\n\nMatrix-Game 是一个创新的交互式世界基础模型，旨在实现对游戏世界的可控生成。该模型通过一个独特的两阶段训练流程进行开发，并利用一个大规模的Minecraft数据集。\n\n### 训练流程\nMatrix-Game 的训练采用两阶段管道：\n\n1.  **大规模无标签预训练**：此阶段专注于环境理解，通过处理大量的无标签数据来学习游戏世界的内在结构和动态。\n2.  **动作标签训练**：在预训练之后，模型会使用带有精细动作标签的数据进行训练，以实现交互式视频生成，从而能够根据用户输入生成相应的游戏内动作和场景。\n\n### 数据集：Matrix-Game-MC\n为了支持模型的训练，研究团队精心策划了 **Matrix-Game-MC**，这是一个全面的Minecraft数据集。该数据集包含：\n\n*   超过 **2,700小时** 的无标签游戏视频片段，用于环境理解的预训练。\n*   超过 **1,000小时** 的高质量标签片段，这些片段带有精细的键盘和鼠标动作标注，用于交互式视频生成阶段。\n\n### 模型范式与能力\nMatrix-Game 采用一种可控的“图像到世界”生成范式。这意味着模型的生成过程可以根据以下条件进行精确控制：\n\n*   **参考图像**：提供生成世界的基础视觉参考。\n*   **运动上下文**：指导生成过程中的动态变化。\n*   **用户动作**：允许用户通过输入直接影响生成的世界和角色行为。\n\n该模型拥有超过 **170亿** 参数，使其能够实现对角色动作和摄像机移动的精确控制，同时保持高视觉质量和时间连贯性。\n\n### 性能评估：GameWorld Score\n为了量化和评估 Matrix-Game 的性能，研究人员开发了 **GameWorld Score**。这是一个统一的基准测试工具，用于衡量 Minecraft 世界生成的以下关键方面：\n\n*   **视觉质量**\n*   **时间质量**\n*   **动作可控性**\n*   **物理规则理解**\n\n### 实验结果\n广泛的实验结果表明，Matrix-Game 在所有评估指标上均持续优于现有的开源 Minecraft 世界模型，包括 Oasis 和 MineWorld。尤其值得注意的是，Matrix-Game 在**可控性**和**物理一致性**方面取得了显著的提升。双盲人工评估进一步证实了 Matrix-Game 的优越性，突出了其在多样化游戏场景中生成感知真实且精确可控视频的能力。\n\n### 开放资源\n为了促进未来在交互式“图像到世界”生成领域的研究，Matrix-Game 模型权重和 GameWorld Score 基准将进行开源。",
      "shortSummary": "Matrix-Game是一个交互式世界基础模型，专为可控游戏世界生成设计。它采用两阶段训练，并利用大规模Minecraft数据集Matrix-Game-MC。该模型拥有超170亿参数，能根据参考图像、运动上下文和用户动作，精确控制角色和摄像机，同时保持高视觉质量。通过GameWorld Score基准测试，Matrix-Game在视觉、时间、可控性和物理一致性上均超越现有模型。模型权重和GameWorld Score将开源，以促进未来研究。",
      "translated_title": "Matrix-Game：交互式世界基础模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Matrix-Game, an interactive world foundation model for controllable game world generation. Matrix-Game is trained using a two-stage pipeline that first performs large-scale unlabeled pretraining for environment understanding, followed by action-labeled training for interactive video generation. To support this, we curate Matrix-Game-MC, a comprehensive Minecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips and over 1,000 hours of high-quality labeled clips with fine-grained keyboard and mouse action annotations. Our model adopts a controllable image-to-world generation paradigm, conditioned on a reference image, motion context, and user actions. With over 17 billion parameters, Matrix-Game enables precise control over character actions and camera movements, while maintaining high visual quality and temporal coherence. To evaluate performance, we develop GameWorld Score, a unified benchmark measuring visual quality, temporal quality, action controllability, and physical rule understanding for Minecraft world generation. Extensive experiments show that Matrix-Game consistently outperforms prior open-source Minecraft world models (including Oasis and MineWorld) across all metrics, with particularly strong gains in controllability and physical consistency. Double-blind human evaluations further confirm the superiority of Matrix-Game, highlighting its ability to generate perceptually realistic and precisely controllable videos across diverse game scenarios. To facilitate future research on interactive image-to-world generation, we will open-source the Matrix-Game model weights and the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game."
    },
    {
      "title": "ReDit: 奖励抖动用于改进LLM策略优化 (原标题: ReDit: Reward Dithering for Improved LLM Policy Optimization)",
      "link": "https://arxiv.org/abs/2506.18631",
      "pubDate": "Mon, 23 Jun 2025 09:36:24 GMT",
      "isoDate": "2025-06-23T09:36:24.000Z",
      "creator": "Chenxing Wei, Jiarui Yu, Ying Tiffany He, Hande Dong, Yao Shu, Fei Yu",
      "summary": "## ReDit: 改进LLM策略优化中的奖励抖动方法\n\n### 背景与问题\n\nDeepSeek-R1 通过其基于规则的奖励系统成功提升了大型语言模型（LLM）的推理能力。尽管这种奖励系统在有效缓解奖励作弊方面表现出色，但其奖励函数通常是离散的。研究观察表明，离散奖励可能导致梯度异常、优化不稳定以及收敛速度缓慢。\n\n### 提出的解决方案：ReDit\n\n为了解决上述问题，本文提出了 **ReDit (Reward Dithering)** 方法。ReDit 通过向离散奖励信号添加简单的随机噪声来对其进行抖动（dither）。\n\n#### ReDit 的优势：\n\n*   **平滑梯度更新**：通过扰动后的奖励，学习过程中可以持续提供探索性梯度，从而实现更平滑的梯度更新。\n*   **加速收敛**：平滑的梯度更新有助于模型更快地收敛。\n*   **鼓励探索**：注入的噪声在平坦的奖励区域引入了随机性，这鼓励模型探索新的策略并跳出局部最优。\n\n### 实验结果与验证\n\n*   **有效性与效率**：在各种任务上的实验证明了 ReDit 的有效性和效率。平均而言，ReDit 仅需约 10% 的训练步骤即可达到与传统 GRPO 相当的性能。\n*   **性能提升**：在相似的训练时长下，ReDit 仍比传统 GRPO 表现出 4% 的性能提升。\n*   **梯度问题缓解**：可视化结果证实 ReDit 显著缓解了梯度问题。\n*   **理论支持**：研究还提供了理论分析，进一步验证了这些优势。\n\n**注意：** 文章中提及包含15张图，但未提供有效的图片链接，因此摘要中未包含任何图片。",
      "shortSummary": "ReDit 是一种通过向离散奖励信号添加随机噪声来改进大型语言模型（LLM）策略优化的方法。针对离散奖励导致的梯度异常和收敛缓慢问题，ReDit 能提供更平滑的梯度更新，加速收敛，并鼓励模型探索新策略以跳出局部最优。实验表明，ReDit 能以更少的训练步骤达到与传统方法相当的性能，或在相同训练时长下实现性能提升，并有效缓解梯度问题。",
      "translated_title": "ReDit: 奖励抖动用于改进LLM策略优化",
      "images": [],
      "contentSource": "完整文章",
      "content": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While it's a ''perfect'' reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages."
    }
  ],
  "lastUpdated": "2025-06-25T09:36:13.826Z"
}