{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "ThinkAct：通过强化视觉潜在规划实现视觉-语言-动作推理 (原标题: ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning)",
      "link": "https://arxiv.org/abs/2507.16815",
      "pubDate": "Tue, 22 Jul 2025 13:59:46 GMT",
      "isoDate": "2025-07-22T13:59:46.000Z",
      "creator": "Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, Fu-En Yang",
      "summary": "本文提出了一种名为 ThinkAct 的双系统框架，旨在解决视觉-语言-动作（VLA）推理任务中的挑战。\n\n**背景与问题**\n*   VLA 推理任务要求智能体解释多模态指令、执行长周期规划并在动态环境中自适应行动。\n*   现有方法通常采用端到端训练，直接将输入映射到动作，缺乏显式推理过程。\n*   这限制了它们在多步骤规划和适应复杂任务变化方面的能力。\n\n**ThinkAct 框架**\n*   **核心理念：** ThinkAct 通过强化视觉潜在规划，连接高层推理与低层动作执行。\n*   **工作机制：**\n    1.  **多模态大型语言模型（LLM）训练：** 训练一个多模态 LLM 来生成具身推理计划。\n    2.  **奖励引导：** 这些推理计划通过强化与动作对齐的视觉奖励进行引导，奖励基于目标完成度和轨迹一致性。\n    3.  **视觉计划潜在表示：** 生成的推理计划被压缩成一个视觉计划潜在表示。\n    4.  **下游动作模型：** 这个视觉计划潜在表示作为条件，输入到一个下游动作模型中，以在目标环境中实现鲁棒的动作执行。\n\n**实验结果与优势**\n*   在具身推理和机器人操作基准上的广泛实验表明，ThinkAct 展现出：\n    *   **少样本适应能力：** 能够以少量样本进行适应。\n    *   **长周期规划能力：** 能够进行跨越多步骤的规划。\n    *   **自我纠正行为：** 在复杂的具身 AI 任务中表现出自我纠正能力。",
      "shortSummary": "ThinkAct 是一种双系统框架，通过强化视觉潜在规划，解决了视觉-语言-动作（VLA）推理任务中现有端到端方法缺乏显式推理的问题。它训练一个多模态LLM生成具身推理计划，并将其压缩为视觉潜在表示，以指导下游动作模型执行。实验证明，ThinkAct 在复杂具身AI任务中实现了少样本适应、长周期规划和自我纠正能力。",
      "translated_title": "ThinkAct：通过强化视觉潜在规划实现视觉-语言-动作推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks."
    },
    {
      "title": "用于视觉-语言慢思考推理的半离策略强化学习 (原标题: Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking Reasoning)",
      "link": "https://arxiv.org/abs/2507.16814",
      "pubDate": "Tue, 22 Jul 2025 13:59:34 GMT",
      "isoDate": "2025-07-22T13:59:34.000Z",
      "creator": "Junhao Shen, Haiteng Zhao, Yuzhe Gu, Songyang Gao, Kuikun Liu, Haian Huang, Jianfei Gao, Dahua Lin, Wenwei Zhang, Kai Chen",
      "summary": "## 用于视觉-语言慢思考推理的半离策略强化学习 (SOPHIA)\n\n### 引言\n\n提升大型视觉-语言模型（LVLMs）的视觉慢思考推理能力对于解决复杂的多模态任务至关重要。然而，现有方法面临挑战：\n\n*   **在策略强化学习（On-policy RL）**：由于LVLMs主要通过视觉-语言对齐进行训练，其初始能力限制了策略探索空间，难以有效发展慢思考能力。\n*   **离策略强化学习（Off-policy RL）**：直接从外部模型蒸馏轨迹可能导致视觉幻觉，因为不同模型间的视觉感知能力存在不匹配。\n\n### SOPHIA 方法\n\n为解决上述问题，本文提出了一种名为 **SOPHIA** (Semi-Off-Policy RL for vision-language slow-tHInking reAsoning) 的简单且可扩展的半离策略强化学习方法。\n\nSOPHIA 的核心机制包括：\n\n1.  **构建半离策略行为模型**：SOPHIA 结合了可训练LVLM的“在策略视觉理解”能力与语言模型的“离策略慢思考推理”能力。\n2.  **奖励分配与传播**：\n    *   对推理过程分配基于结果的奖励。\n    *   将视觉奖励反向传播。\n3.  **模型学习**：LVLM 通过离策略RL算法，利用获得的推理轨迹和传播的奖励来学习慢思考推理能力。\n\n### 实验与结果\n\n研究团队使用 InternVL2.5 和 InternVL3.0（包括 8B 和 38B 参数规模）进行了广泛实验，结果表明 SOPHIA 具有显著的有效性：\n\n*   **性能提升**：SOPHIA 使 InternVL3.0-38B 的平均性能提升了 8.50%。\n*   **最先进表现**：在多个多模态推理基准测试中，SOPHIA 在开源LVLM中达到了最先进（state-of-the-art, SOTA）的性能。\n*   **超越闭源模型**：在挑战性的 MathVision 和 OlympiadBench 基准测试中，SOPHIA 甚至超越了一些闭源模型（例如 GPT-4.1），分别取得了 49.08% 和 49.95% 的 pass@1 准确率。\n\n### 分析\n\n分析结果显示，SOPHIA 的表现优于传统的监督微调（supervised fine-tuning）和直接的在策略RL方法。这表明 SOPHIA 为后续的在策略训练提供了更好的策略初始化，进一步提升了模型的推理能力。",
      "shortSummary": "SOPHIA是一种半离策略强化学习方法，旨在增强大型视觉-语言模型（LVLMs）的慢思考推理能力。它通过结合LVLM的视觉理解与语言模型推理，并利用奖励传播机制进行训练。实验表明，SOPHIA显著提升了InternVL3.0-38B的性能，在多模态推理基准测试中达到开源LVLM的最先进水平，并在MathVision和OlympiadBench等挑战性任务上超越了部分闭源模型。",
      "translated_title": "用于视觉-语言慢思考推理的半离策略强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Enhancing large vision-language models (LVLMs) with visual slow-thinking reasoning is crucial for solving complex multimodal tasks. However, since LVLMs are mainly trained with vision-language alignment, it is difficult to adopt on-policy reinforcement learning (RL) to develop the slow thinking ability because the rollout space is restricted by its initial abilities. Off-policy RL offers a way to go beyond the current policy, but directly distilling trajectories from external models may cause visual hallucinations due to mismatched visual perception abilities across models. To address these issues, this paper proposes SOPHIA, a simple and scalable Semi-Off-Policy RL for vision-language slow-tHInking reAsoning. SOPHIA builds a semi-off-policy behavior model by combining on-policy visual understanding from a trainable LVLM with off-policy slow-thinking reasoning from a language model, assigns outcome-based rewards to reasoning, and propagates visual rewards backward. Then LVLM learns slow-thinking reasoning ability from the obtained reasoning trajectories using propagated rewards via off-policy RL algorithms. Extensive experiments with InternVL2.5 and InternVL3.0 with 8B and 38B sizes show the effectiveness of SOPHIA. Notably, SOPHIA improves InternVL3.0-38B by 8.50% in average, reaching state-of-the-art performance among open-source LVLMs on multiple multimodal reasoning benchmarks, and even outperforms some closed-source models (e.g., GPT-4.1) on the challenging MathVision and OlympiadBench, achieving 49.08% and 49.95% pass@1 accuracy, respectively. Analysis shows SOPHIA outperforms supervised fine-tuning and direct on-policy RL methods, offering a better policy initialization for further on-policy training."
    },
    {
      "title": "HOComp：交互感知的人体-物体合成 (原标题: HOComp: Interaction-Aware Human-Object Composition)",
      "link": "https://arxiv.org/abs/2507.16813",
      "pubDate": "Tue, 22 Jul 2025 13:59:21 GMT",
      "isoDate": "2025-07-22T13:59:21.000Z",
      "creator": "Dong Liang, Jinyuan Jia, Yuhao Liu, Rynson W. H. Lau",
      "summary": "### HOComp：交互感知的人体-物体合成\n\n**问题背景**\n\n现有的图像引导合成方法在将前景对象插入背景图像的指定区域时，通常难以在涉及人体-物体交互的任务中实现无缝且自然的合成。这些方法在处理人与物体之间的复杂互动时，往往无法确保合成结果的和谐与一致性。\n\n**HOComp 方法概述**\n\n本文提出了一种名为 HOComp 的新颖方法，旨在解决人体-物体合成中的挑战。HOComp 专注于将前景对象合成到以人为中心的背景图像上，同时确保前景对象与背景人物之间存在和谐的交互，并保持两者外观的一致性。\n\n**HOComp 的两大核心设计**\n\n1.  **MLLMs 驱动的基于区域的姿态引导 (MRPG)**：\n    *   **交互区域与类型识别**：利用多模态大型语言模型 (MLLMs) 来识别图像中的交互区域以及具体的交互类型（例如，握持、举起）。\n    *   **粗粒度到细粒度姿态约束**：为生成的交互姿态提供从粗粒度到细粒度的约束。这意味着它首先确定大致的交互姿态，然后进行精细调整。\n    *   **人体姿态关键点整合**：结合人体姿态关键点来跟踪动作变化，并施加更精细的姿态约束，确保交互的准确性。\n\n2.  **细节一致的外观保持 (DCAP)**：\n    *   **统一机制**：DCAP 机制整合了多种技术，以确保合成结果的视觉质量。\n    *   **形状感知注意力调制**：通过形状感知的注意力调制机制，确保前景对象的形状和纹理与背景环境保持一致。\n    *   **多视图外观损失**：引入多视图外观损失，进一步提升合成对象外观的真实感和一致性。\n    *   **背景一致性损失**：通过背景一致性损失，确保背景中的人物能够被忠实地再现，且与合成对象无缝融合。\n\n**新数据集：交互感知的人体-物体合成 (IHOC)**\n\n为了支持和评估这项任务，本文首次提出了一个名为“交互感知的人体-物体合成 (IHOC)”的数据集。该数据集专门用于研究和开发人体-物体交互合成方法。\n\n**实验结果**\n\n在 IHOC 数据集上的实验结果表明，HOComp 能够有效地生成具有和谐人体-物体交互和一致外观的图像。无论是从定性还是定量的角度来看，HOComp 都优于现有的相关方法，证明了其在解决该任务上的卓越性能。",
      "shortSummary": "HOComp 是一种新颖的图像合成方法，旨在解决人体-物体交互合成中现有方法的不足。它通过两大核心设计实现和谐自然的合成：一是“MLLMs 驱动的基于区域的姿态引导 (MRPG)”，利用 MLLMs 识别交互区域和类型，并提供精细的姿态约束；二是“细节一致的外观保持 (DCAP)”，通过形状感知注意力、多视图外观损失和背景一致性损失确保外观真实性。论文还提出了首个“交互感知的人体-物体合成 (IHOC)”数据集。实验证明 HOComp 在生成高质量人体-物体交互方面表现优异。",
      "translated_title": "HOComp：交互感知的人体-物体合成",
      "images": [],
      "contentSource": "完整文章",
      "content": "While existing image-guided composition methods may help insert a foreground object onto a user-specified region of a background image, achieving natural blending inside the region with the rest of the image unchanged, we observe that these existing methods often struggle in synthesizing seamless interaction-aware compositions when the task involves human-object interactions. In this paper, we first propose HOComp, a novel approach for compositing a foreground object onto a human-centric background image, while ensuring harmonious interactions between the foreground object and the background person and their consistent appearances. Our approach includes two key designs: (1) MLLMs-driven Region-based Pose Guidance (MRPG), which utilizes MLLMs to identify the interaction region as well as the interaction type (e.g., holding and lefting) to provide coarse-to-fine constraints to the generated pose for the interaction while incorporating human pose landmarks to track action variations and enforcing fine-grained pose constraints; and (2) Detail-Consistent Appearance Preservation (DCAP), which unifies a shape-aware attention modulation mechanism, a multi-view appearance loss, and a background consistency loss to ensure consistent shapes/textures of the foreground and faithful reproduction of the background human. We then propose the first dataset, named Interaction-aware Human-Object Composition (IHOC), for the task. Experimental results on our dataset show that HOComp effectively generates harmonious human-object interactions with consistent appearances, and outperforms relevant methods qualitatively and quantitatively."
    },
    {
      "title": "MegaScience：推动科学推理后训练数据集的前沿 (原标题: MegaScience: Pushing the Frontiers of Post-Training Datasets for Science Reasoning)",
      "link": "https://arxiv.org/abs/2507.16812",
      "pubDate": "Tue, 22 Jul 2025 13:59:03 GMT",
      "isoDate": "2025-07-22T13:59:03.000Z",
      "creator": "Run-Ze Fan, Zengzhi Wang, Pengfei Liu",
      "summary": "### MegaScience：推动科学推理后训练数据集的前沿\n\n**引言与背景**\n科学推理能力对于开发AI科学家和支持人类研究人员在自然科学发现领域取得进展至关重要。然而，开源社区主要关注数学和编码领域，却忽视了科学领域，这在很大程度上是由于缺乏开放、大规模、高质量、可验证的科学推理数据集。\n\n**核心贡献**\n为了弥补这一空白，研究团队提出了两项主要贡献：\n\n1.  **TextbookReasoning 数据集**\n    *   这是一个开放数据集，旨在提供真实、高质量的科学推理数据。\n    *   其参考答案从12,000本大学级别的科学教科书中提取。\n    *   包含65万个推理问题，涵盖7个不同的科学学科。\n\n2.  **MegaScience 数据集**\n    *   这是一个大规模、高质量的开源数据集混合体，总计包含125万个实例。\n    *   该数据集是通过系统的消融研究开发的，旨在评估各种数据选择方法，从而为每个公开可用的科学数据集识别出最优子集。\n\n3.  **综合评估系统**\n    *   研究团队构建了一个全面的评估系统，以确保评估的准确性。\n    *   该系统涵盖了15个基准测试中的多样化主题和问题类型。\n    *   整合了全面的答案提取策略，以确保评估指标的准确性。\n\n**实验结果与性能**\n\n*   **卓越的性能与训练效率**：实验结果表明，与现有开源科学数据集相比，TextbookReasoning 和 MegaScience 数据集在性能和训练效率方面表现出显著优势，并且能够生成更简洁的响应。\n*   **模型性能提升**：研究团队在 MegaScience 数据集上训练了Llama3.1、Qwen2.5和Qwen3系列的基础模型。这些模型在平均性能上显著优于相应的官方指令模型。\n*   **规模效益**：MegaScience 数据集对更大、更强的模型表现出更大的有效性，这表明在科学调优方面存在规模效益。\n\n**社区贡献**\n为了推动科学推理研究的进展，研究团队向社区发布了以下资源：\n*   数据整理管道\n*   评估系统\n*   所有数据集（包括TextbookReasoning和MegaScience）\n*   七个训练好的模型\n\n**图片说明**\n文章内容中未包含有效的实际图片链接，因此摘要中不包含图片。",
      "shortSummary": "该研究旨在解决科学推理领域高质量开放数据集的缺乏。为此，团队推出了TextbookReasoning，一个包含65万个大学级别科学推理问题的开放数据集；并引入了MegaScience，一个包含125万个实例的大规模高质量数据集。通过综合评估系统，实验证明这些数据集显著提升了AI模型在科学推理任务上的性能和训练效率，尤其对大型模型效果更佳。所有数据、系统和模型均已开源，以促进科学推理研究。",
      "translated_title": "MegaScience：推动科学推理后训练数据集的前沿",
      "images": [],
      "contentSource": "完整文章",
      "content": "Scientific reasoning is critical for developing AI scientists and supporting human researchers in advancing the frontiers of natural science discovery. However, the open-source community has primarily focused on mathematics and coding while neglecting the scientific domain, largely due to the absence of open, large-scale, high-quality, verifiable scientific reasoning datasets. To bridge this gap, we first present TextbookReasoning, an open dataset featuring truthful reference answers extracted from 12k university-level scientific textbooks, comprising 650k reasoning questions spanning 7 scientific disciplines. We further introduce MegaScience, a large-scale mixture of high-quality open-source datasets totaling 1.25 million instances, developed through systematic ablation studies that evaluate various data selection methodologies to identify the optimal subset for each publicly available scientific dataset. Meanwhile, we build a comprehensive evaluation system covering diverse subjects and question types across 15 benchmarks, incorporating comprehensive answer extraction strategies to ensure accurate evaluation metrics. Our experiments demonstrate that our datasets achieve superior performance and training efficiency with more concise response lengths compared to existing open-source scientific datasets. Furthermore, we train Llama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which significantly outperform the corresponding official instruct models in average performance. In addition, MegaScience exhibits greater effectiveness for larger and stronger models, suggesting a scaling benefit for scientific tuning. We release our data curation pipeline, evaluation system, datasets, and seven trained models to the community to advance scientific reasoning research."
    },
    {
      "title": "通过概念消融微调引导大模型域外泛化 (原标题: Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning)",
      "link": "https://arxiv.org/abs/2507.16795",
      "pubDate": "Tue, 22 Jul 2025 13:45:04 GMT",
      "isoDate": "2025-07-22T13:45:04.000Z",
      "creator": "Helena Casademunt, Caden Juang, Adam Karvonen, Samuel Marks, Senthooran Rajamanoharan, Neel Nanda",
      "summary": "### 概念消融微调（CAFT）：引导大模型域外泛化\n\n**1. 问题背景**\n\n*   大语言模型（LLM）在微调后可能出现意想不到的“域外泛化”（out-of-distribution generalization）问题。\n*   这可能导致模型在通用问题上产生“涌现错位”（emergent misalignment）响应，即给出严重不一致或不恰当的回答。\n\n**2. 传统解决方案及其局限性**\n\n*   标准方法通常依赖于修改训练数据，例如添加更多数据以更明确地指定预期的泛化行为。\n*   然而，这种方法在实践中并不总是可行或方便。\n\n**3. 引入新方法：概念消融微调（CAFT）**\n\n*   本文提出了一种名为“概念消融微调”（Concept Ablation Fine-Tuning, CAFT）的新技术。\n*   CAFT旨在控制LLM从微调中泛化的方式，而无需修改训练数据或使用来自目标分布的数据。\n\n**4. CAFT的工作原理**\n\n*   CAFT利用可解释性工具来识别LLM潜在空间中对应于“不期望概念”的一组方向。\n*   在微调过程中，CAFT通过线性投影来“消融”（ablating）这些不期望的概念。\n*   这种消融操作能够引导模型，使其偏离不期望的泛化行为。\n\n**5. 应用与实验结果**\n\n*   CAFT已成功应用于三个不同的微调任务，其中包括解决“涌现错位”现象。\n*   在不改变任何微调数据的情况下，CAFT将错位响应的发生率降低了10倍。\n*   同时，CAFT并未降低模型在训练分布上的性能。\n\n**6. 结论与意义**\n\n*   CAFT代表了一种新颖的方法，可以在不修改训练数据的前提下，有效引导和控制LLM的泛化行为。\n*   这为解决LLM微调后出现的意外域外泛化问题提供了一条有前景的途径。",
      "shortSummary": "概念消融微调（CAFT）是一种无需修改训练数据即可控制大语言模型（LLM）域外泛化的新方法。它利用可解释性工具识别并消除LLM潜在空间中不期望的概念方向，从而引导模型避免意外泛化。CAFT成功将模型错位响应减少10倍，同时不影响训练性能，为解决LLM泛化问题提供了有效途径。",
      "translated_title": "通过概念消融微调引导大模型域外泛化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Fine-tuning large language models (LLMs) can lead to unintended out-of-distribution generalization. Standard approaches to this problem rely on modifying training data, for example by adding data that better specify the intended generalization. However, this is not always practical. We introduce Concept Ablation Fine-Tuning (CAFT), a technique that leverages interpretability tools to control how LLMs generalize from fine-tuning, without needing to modify the training data or otherwise use data from the target distribution. Given a set of directions in an LLM's latent space corresponding to undesired concepts, CAFT works by ablating these concepts with linear projections during fine-tuning, steering the model away from unintended generalizations. We successfully apply CAFT to three fine-tuning tasks, including emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow task generalize to give egregiously misaligned responses to general questions. Without any changes to the fine-tuning data, CAFT reduces misaligned responses by 10x without degrading performance on the training distribution. Overall, CAFT represents a novel approach for steering LLM generalization without modifying training data."
    },
    {
      "title": "超越上下文限制：用于长程推理的潜意识线程 (原标题: Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning)",
      "link": "https://arxiv.org/abs/2507.16784",
      "pubDate": "Tue, 22 Jul 2025 13:30:04 GMT",
      "isoDate": "2025-07-22T13:30:04.000Z",
      "creator": "Hongyin Luo, Nathaniel Morgan, Tina Li, Derek Zhao, Ai Vy Ngo, Philip Schroeder, Lijie Yang, Assaf Ben-Kish, Jack O'Brien, James Glass",
      "summary": "### 超越上下文限制：用于长程推理的潜意识线程\n\n本文介绍了一种名为**线程推理模型（TIM）**的新型大型语言模型（LLM）家族，以及配套的推理运行时**TIMRUN**。该系统旨在突破当前LLM在推理准确性和效率方面受到的上下文限制，从而实现更长程、更复杂的推理任务。\n\n**核心问题与解决方案：**\n\n*   **问题：** 现有LLM受限于其上下文窗口大小，这限制了其处理复杂、长程推理任务的能力，并影响了推理的准确性和效率。\n*   **解决方案：** TIM和TIMRUN协同工作，提供了一种递归和分解式的问题解决范式，旨在克服这些限制。\n\n**TIM和TIMRUN的协同优势：**\n\n*   **无限工作记忆：** TIM在TIMRUN上运行时，能够支持几乎无限的工作记忆，从而克服了传统LLM面临的输出限制、位置嵌入约束和GPU内存瓶颈。\n*   **多跳工具调用：** 在单次LLM推理过程中，系统能够实现多跳工具调用，增强了模型的实用性和解决复杂问题的能力。\n*   **性能提升：** 通过将自然语言建模为推理树（同时衡量其长度和深度）而非传统的线性序列，系统显著提升了推理性能。\n\n**工作原理：**\n\n*   **推理树结构：** 推理树由一系列任务组成，每个任务包含思想、递归子任务和基于Schroeder et al, 2025年提出的概念的结论。\n*   **高效工作记忆维护：** 在生成过程中，系统维护一个动态的工作记忆，该记忆仅保留最相关上下文令牌的关键-值（KV）状态。\n*   **子任务剪枝机制：** 通过基于规则的子任务剪枝机制，系统能够智能地选择并保留最相关的令牌，从而在整个推理过程中实现位置嵌入和GPU内存页的复用，极大地提高了内存效率。\n\n**实验结果：**\n\n*   **高推理吞吐量：** 实验结果表明，即使在GPU内存中操作高达90%的关键-值（KV）缓存，该系统也能保持高推理吞吐量。\n*   **准确的数学推理：** 在数学任务上，系统展现出准确的推理能力。\n*   **信息检索挑战：** 能够有效处理需要长程推理和多跳工具使用的复杂信息检索挑战。\n\n**研究状态：**\n\n*   本文是一项研究预览，属于计算与语言（cs.CL）领域。",
      "shortSummary": "本文提出线程推理模型（TIM）及其运行时TIMRUN，旨在突破大型语言模型（LLM）的上下文限制。TIM通过将自然语言建模为推理树，并利用高效的工作记忆管理，实现了几乎无限的工作记忆和多跳工具调用。实验表明，该系统在数学推理和信息检索等长程推理任务中表现出高吞吐量和准确性，有效解决了LLM的上下文瓶颈。",
      "translated_title": "超越上下文限制：用于长程推理的潜意识线程",
      "images": [],
      "contentSource": "完整文章",
      "content": "To break the context limits of large language models (LLMs) that bottleneck reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM), a family of LLMs trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept we proposed in Schroeder et al, 2025. During generation, we maintain a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask-pruning mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that our system sustains high inference throughput, even when manipulating up to 90% of the KV cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use."
    },
    {
      "title": "Zebra-CoT：一个用于交错视觉语言推理的数据集 (原标题: Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning)",
      "link": "https://arxiv.org/abs/2507.16746",
      "pubDate": "Tue, 22 Jul 2025 12:35:36 GMT",
      "isoDate": "2025-07-22T12:35:36.000Z",
      "creator": "Ang Li, Charles Wang, Kaiyu Yue, Zikui Cai, Ollie Liu, Deqing Fu, Peng Guo, Wang Bill Zhu, Vatsal Sharan, Robin Jia, Willie Neiswanger, Furong Huang, Tom Goldstein, Micah Goldblum",
      "summary": "## Zebra-CoT：一个用于交错视觉语言推理的数据集\n\n### 引言与背景\n人类在解决复杂问题时，通常会利用视觉辅助，例如图表或草图。然而，训练多模态模型以实现类似的视觉思维链（Visual CoT）能力面临两大挑战：\n\n1.  **现有Visual CoT性能不佳：** 现成的Visual CoT模型表现不理想，这阻碍了强化学习等训练方法的应用。\n2.  **高质量训练数据匮乏：** 缺乏用于Visual CoT训练的高质量数据集。\n\n### Zebra-CoT数据集介绍\n为了解决上述挑战，研究人员引入了 **Zebra-CoT**，这是一个多样化的大规模数据集，旨在促进交错视觉语言推理能力的开发。\n\n*   **规模与构成：** Zebra-CoT包含182,384个样本，每个样本都包含逻辑连贯的交错文本-图像推理轨迹。\n*   **任务类别：** 数据集重点关注四类特别适合草图或视觉推理的任务，包括：\n    *   **科学问题：** 如几何、物理和算法等领域的推理。\n    *   **2D视觉推理任务：** 例如视觉搜索和拼图。\n    *   **3D推理任务：** 包括3D多跳推理、具身（embodied）和机器人规划。\n    *   **视觉逻辑问题与策略游戏：** 例如国际象棋。\n\n### 实验结果与有效性\nZebra-CoT数据集在提升多模态模型性能方面展现出显著效果：\n\n*   **Anole-7B模型：** 在Zebra-CoT训练语料库上对Anole-7B模型进行微调后，其在测试集上的准确率提高了12%，并在标准VLM（视觉语言模型）基准评估中获得了高达13%的性能提升。\n*   **Bagel-7B模型：** 微调Bagel-7B模型后，该模型能够生成高质量的交错视觉推理链。\n\n这些结果充分证明了Zebra-CoT在开发多模态推理能力方面的有效性。\n\n### 开放资源\n为了支持Visual CoT的进一步开发和评估，Zebra-CoT数据集和相关模型均已开源。\n\n### 相关领域\n该研究主要涉及计算机视觉与模式识别（cs.CV）、计算与语言（cs.CL）以及机器学习（cs.LG）等领域。",
      "shortSummary": "Zebra-CoT是一个大型数据集，旨在解决多模态模型中视觉思维链（Visual CoT）训练数据不足的问题。该数据集包含182,384个样本，涵盖科学、2D/3D推理及视觉逻辑等多种任务的交错文本-图像推理轨迹。通过在Zebra-CoT上微调，Anole-7B模型在测试集准确率上提升12%，并在标准VLM基准上获得显著性能增益。Zebra-CoT有效促进了多模态推理能力的发展，其数据集和模型均已开源。",
      "translated_title": "Zebra-CoT：一个用于交错视觉语言推理的数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "Humans often use visual aids, for example diagrams or sketches, when solving complex problems. Training multimodal models to do the same, known as Visual Chain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf visual CoT performance, which hinders reinforcement learning, and (2) the lack of high-quality visual CoT training data. We introduce Zebra-CoT, a diverse large-scale dataset with 182,384 samples, containing logically coherent interleaved text-image reasoning traces. We focus on four categories of tasks where sketching or visual reasoning is especially natural, spanning scientific questions such as geometry, physics, and algorithms; 2D visual reasoning tasks like visual search and jigsaw puzzles; 3D reasoning tasks including 3D multi-hop inference, embodied and robot planning; visual logic problems and strategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT training corpus results in an improvement of +12% in our test-set accuracy and yields up to +13% performance gain on standard VLM benchmark evaluations. Fine-tuning Bagel-7B yields a model that generates high-quality interleaved visual reasoning chains, underscoring Zebra-CoT's effectiveness for developing multimodal reasoning abilities. We open-source our dataset and models to support development and evaluation of visual CoT."
    },
    {
      "title": "Step-Audio 2 技术报告 (原标题: Step-Audio 2 Technical Report)",
      "link": "https://arxiv.org/abs/2507.16632",
      "pubDate": "Tue, 22 Jul 2025 10:23:55 GMT",
      "isoDate": "2025-07-22T10:23:55.000Z",
      "creator": "Boyong Wu, Chao Yan, Chen Hu, Cheng Yi, Chengli Feng, Fei Tian, Feiyu Shen, Gang Yu, Haoyang Zhang, Jingbei Li, Mingrui Chen, Peng Liu, Wang You, Xiangyu Tony Zhang, Xingyuan Li, Xuerui Yang, Yayue Deng, Yechang Huang, Yuxin Li, Yuxin Zhang, Zhao You, Brian Li, Changyi Wan, Hanpeng Hu, Jiangjie Zhen, Siyu Chen, Song Yuan, Xuelin Zhang, Yimin Jiang, Yu Zhou, Yuxiang Yang, Bingxin Li, Buyun Ma, Changhe Song, Dongqing Pang, Guoqiang Hu, Haiyang Sun, Kang An, Na Wang, Shuli Gao, Wei Ji, Wen Li, Wen Sun, Xuan Wen, Yong Ren, Yuankai Ma, Yufan Lu, Bin Wang, Bo Li, Changxin Miao, Che Liu, Chen Xu, Dapeng Shi, Dingyuan Hu, Donghang Wu, Enle Liu, Guanzhe Huang, Gulin Yan, Han Zhang, Hao Nie, Haonan Jia, Hongyu Zhou, Jianjian Sun, Jiaoren Wu, Jie Wu, Jie Yang, Jin Yang, Junzhe Lin, Kaixiang Li, Lei Yang, Liying Shi, Li Zhou, Longlong Gu, Ming Li, Mingliang Li, Mingxiao Li, Nan Wu, Qi Han, Qinyuan Tan, Shaoliang Pang, Shengjie Fan, Siqi Liu, Tiancheng Cao, Wanying Lu, Wenqing He, Wuxun Xie, Xu Zhao, Xueqi Li, Yanbo Yu, Yang Yang, Yi Liu, Yifan Lu, Yilei Wang, Yuanhao Ding, Yuanwei Liang, Yuanwei Lu, Yuchu Luo, Yuhe Yin, Yumeng Zhan, Yuxiang Zhang, Zidong Yang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Xiangyu Zhang, Yibo Zhu",
      "summary": "## Step-Audio 2 技术报告\n\n本文介绍了 Step-Audio 2，一个专为工业级音频理解和语音对话设计的端到端多模态大型语言模型。\n\n### 核心技术与创新\n\n*   **多模态集成**：Step-Audio 2 通过整合潜在音频编码器和以推理为中心的强化学习（RL），在自动语音识别（ASR）和音频理解方面取得了显著性能。\n*   **增强语音对话**：为了实现真正的端到端语音对话，Step-Audio 2 将离散音频令牌的生成融入语言建模中，显著增强了其对语调信息（如说话风格和情感）的响应能力。\n*   **知识利用与工具调用**：\n    *   模型集成了检索增强生成（RAG）技术，以有效利用真实世界数据中丰富的文本和声学知识。\n    *   Step-Audio 2 能够调用外部工具，例如网络搜索以减少幻觉，以及音频搜索以切换音色。\n\n### 训练与性能\n\n*   **大规模训练**：Step-Audio 2 在数百万小时的语音和音频数据上进行训练，使其能够在各种对话场景中展现出智能和表现力。\n*   **领先的性能**：评估结果表明，与现有的开源和商业解决方案相比，Step-Audio 2 在各种音频理解和对话基准测试中均达到了最先进的性能。",
      "shortSummary": "Step-Audio 2 是一个端到端的多模态大型语言模型，专为工业级音频理解和语音对话设计。它通过整合潜在音频编码器、推理强化学习和离散音频令牌生成，增强了对语调信息的响应。模型还集成了检索增强生成（RAG）并能调用外部工具。Step-Audio 2 在数百万小时数据上训练，并在音频理解和对话基准测试中取得了最先进的性能。",
      "translated_title": "Step-Audio 2 技术报告",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper presents Step-Audio~2, an end-to-end multi-modal large language model designed for industry-strength audio understanding and speech conversation. By integrating a latent audio encoder and reasoning-centric reinforcement learning (RL), Step-Audio 2 achieves promising performance in automatic speech recognition (ASR) and audio understanding. To facilitate genuine end-to-end speech conversation, Step-Audio 2 incorporates the generation of discrete audio tokens into language modeling, significantly enhancing its responsiveness to paralinguistic information such as speaking styles and emotions. To effectively leverage the rich textual and acoustic knowledge in real-world data, Step-Audio 2 integrates retrieval-augmented generation (RAG) and is able to call external tools such as web search to mitigate hallucination and audio search to switch timbres. Trained on millions of hours of speech and audio data, Step-Audio 2 delivers intelligence and expressiveness across diverse conversational scenarios. Evaluation results demonstrate that Step-Audio 2 achieves state-of-the-art performance on various audio understanding and conversational benchmarks compared to other open-source and commercial solutions. Please visit https://github.com/stepfun-ai/Step-Audio2 for more information."
    },
    {
      "title": "更多的推理时间计算真的有助于鲁棒性吗？ (原标题: Does More Inference-Time Compute Really Help Robustness?)",
      "link": "https://arxiv.org/abs/2507.15974",
      "pubDate": "Mon, 21 Jul 2025 14:08:38 GMT",
      "isoDate": "2025-07-21T14:08:38.000Z",
      "creator": "Tong Wu, Chong Xiang, Jiachen T. Wang, Weichen Yu, Chawin Sitawarin, Vikash Sehwag, Prateek Mittal",
      "summary": "本文探讨了增加推理时间计算对模型鲁棒性的影响，并揭示了其复杂性，尤其是在对抗性环境中。\n\n*   **背景与初步发现**\n    *   近期研究（Zaremba et al.）表明，增加大型专有推理LLM的推理时间计算可以提高其鲁棒性。\n    *   本文首先证实，小规模的开源模型（如DeepSeek R1、Qwen3、Phi-reasoning）也能通过简单的预算强制策略从推理时间扩展中受益。\n\n*   **关键假设的揭示与安全风险**\n    *   本文揭示并批判性地审视了先前工作中的一个隐含假设：即中间推理步骤对攻击者是隐藏的。\n    *   通过放宽这一假设，研究发现了一个重要的安全风险：如果中间推理步骤变得明确可访问，增加推理时间计算反而会持续降低模型的鲁棒性。这一现象被经验性地验证为一种“逆向扩展定律”。\n\n*   **实际应用中的脆弱性**\n    *   即使推理链是隐藏的，模型在某些实际场景中仍然容易受到攻击，例如：\n        *   集成工具的推理模型。\n        *   高级推理提取攻击。\n\n*   **结论与建议**\n    *   研究结果共同表明，推理时间扩展带来的鲁棒性益处，在很大程度上取决于对抗环境和部署上下文。\n    *   作者敦促从业者在将推理时间扩展应用于安全敏感的实际应用之前，仔细权衡这些微妙的权衡。",
      "shortSummary": "本文研究了增加推理时间计算对模型鲁棒性的影响。研究发现，小规模开源模型也能从中受益。然而，如果中间推理步骤对攻击者可见，增加推理时间计算反而会降低模型鲁棒性，这构成了一个重要的安全风险。研究强调，推理时间扩展的鲁棒性益处高度依赖于对抗环境和部署上下文，从业者在安全敏感应用中应谨慎权衡。",
      "translated_title": "更多的推理时间计算真的有助于鲁棒性吗？",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recently, Zaremba et al. demonstrated that increasing inference-time computation improves robustness in large proprietary reasoning LLMs. In this paper, we first show that smaller-scale, open-source models (e.g., DeepSeek R1, Qwen3, Phi-reasoning) can also benefit from inference-time scaling using a simple budget forcing strategy. More importantly, we reveal and critically examine an implicit assumption in prior work: intermediate reasoning steps are hidden from adversaries. By relaxing this assumption, we identify an important security risk, intuitively motivated and empirically verified as an inverse scaling law: if intermediate reasoning steps become explicitly accessible, increased inference-time computation consistently reduces model robustness. Finally, we discuss practical scenarios where models with hidden reasoning chains are still vulnerable to attacks, such as models with tool-integrated reasoning and advanced reasoning extraction attacks. Our findings collectively demonstrate that the robustness benefits of inference-time scaling depend heavily on the adversarial setting and deployment context. We urge practitioners to carefully weigh these subtle trade-offs before applying inference-time scaling in security-sensitive, real-world applications."
    },
    {
      "title": "潜在去噪造就优秀的视觉分词器 (原标题: Latent Denoising Makes Good Visual Tokenizers)",
      "link": "https://arxiv.org/abs/2507.15856",
      "pubDate": "Mon, 21 Jul 2025 13:59:56 GMT",
      "isoDate": "2025-07-21T13:59:56.000Z",
      "creator": "Jiawei Yang, Tianhong Li, Lijie Fan, Yonglong Tian, Yue Wang",
      "summary": "### 视觉分词器与潜在去噪\n\n**1. 背景与问题**\n\n*   尽管视觉分词器在生成模型中扮演着基础性角色，但目前尚不清楚哪些特性能够使其在生成建模中更有效。\n\n**2. 核心观察与洞察**\n\n*   研究人员观察到，现代生成模型共享一个概念上相似的训练目标：从高斯噪声或掩码等损坏输入中重建干净信号。这一过程被研究人员称为“去噪”（denoising）。\n*   受此启发，研究人员提出将分词器嵌入（tokenizer embeddings）直接与下游的去噪目标对齐。这种对齐旨在鼓励潜在嵌入（latent embeddings）即使在严重损坏的情况下也能更容易地被重建。\n\n**3. 提出的方法：潜在去噪分词器 (l-DeTok)**\n\n*   为了实现上述目标，文章引入了一种简单而有效的视觉分词器——潜在去噪分词器（Latent Denoising Tokenizer, l-DeTok）。\n*   **工作原理：** l-DeTok 的训练目标是从被插值噪声（interpolative noise）和随机掩码（random masking）损坏的潜在嵌入中重建干净图像。\n\n**4. 实验结果与性能**\n\n*   研究人员在 ImageNet 256x256 数据集上进行了广泛的实验。\n*   实验结果表明，l-DeTok 在六种代表性的生成模型中，始终优于标准的分词器。\n\n**5. 结论与未来展望**\n\n*   研究发现强调了“去噪”作为分词器开发的一个基本设计原则。\n*   作者希望这一发现能够为未来的分词器设计提供新的视角和启发。",
      "shortSummary": "该研究提出“潜在去噪”是提升视觉分词器效果的关键原则。文章引入了潜在去噪分词器（l-DeTok），它通过从损坏的潜在嵌入中重建图像进行训练。在ImageNet 256x256上的广泛实验表明，l-DeTok在多种生成模型中表现优于标准分词器。这强调了去噪作为分词器设计的基本原则，为未来发展提供了新视角。",
      "translated_title": "潜在去噪造就优秀的视觉分词器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite their fundamental role, it remains unclear what properties could make visual tokenizers more effective for generative modeling. We observe that modern generative models share a conceptually similar training objective -- reconstructing clean signals from corrupted inputs such as Gaussian noise or masking -- a process we term denoising. Motivated by this insight, we propose aligning tokenizer embeddings directly with the downstream denoising objective, encouraging latent embeddings to be more easily reconstructed even when heavily corrupted. To achieve this, we introduce the Latent Denoising Tokenizer (l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images from latent embeddings corrupted by interpolative noise and random masking. Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer consistently outperforms standard tokenizers across six representative generative models. Our findings highlight denoising as a fundamental design principle for tokenizer development, and we hope it could motivate new perspectives for future tokenizer design."
    },
    {
      "title": "SeC：通过渐进式概念构建推进复杂视频对象分割 (原标题: SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction)",
      "link": "https://arxiv.org/abs/2507.15852",
      "pubDate": "Mon, 21 Jul 2025 13:59:02 GMT",
      "isoDate": "2025-07-21T13:59:02.000Z",
      "creator": "Zhixiong Zhang, Shuangrui Ding, Xiaoyi Dong, Songxin He, Jianfan Lin, Junsong Tang, Yuhang Zang, Yuhang Cao, Dahua Lin, Jiaqi Wang",
      "summary": "## SeC：通过渐进式概念构建推进复杂视频对象分割\n\n### 引言\n\n视频对象分割（VOS）是计算机视觉领域的核心任务，要求模型在视频帧中跟踪和分割目标对象。尽管现有技术取得了显著进展，但在处理剧烈视觉变化、遮挡和复杂场景变化方面，当前技术仍远不及人类能力。这种局限性源于它们过度依赖外观匹配，而忽略了人类在跨时间动态中实现鲁棒识别所依赖的、类似人类的物体概念理解。\n\n### SeC框架：概念驱动的分割\n\n为弥补这一差距，本文提出了 **Segment Concept (SeC)**，一个概念驱动的分割框架。SeC 的核心思想是从传统的特征匹配转向高层、以对象为中心的表示的渐进式构建和利用。\n\n*   **构建鲁棒概念先验：** SeC 利用大型视觉-语言模型（LVLMs）整合来自不同帧的视觉线索，从而构建出鲁棒的概念先验。\n*   **推理阶段的鲁棒分割：** 在推理过程中，SeC 基于已处理的帧形成目标的全面语义表示，从而实现后续帧的鲁棒分割。\n*   **自适应平衡：** SeC 能够自适应地平衡基于 LVLM 的语义推理与增强的特征匹配，根据场景复杂性动态调整计算工作量。\n\n### SeCVOS 基准：评估复杂场景下的概念理解\n\n为了严格评估 VOS 方法在需要高层概念推理和鲁棒语义理解的场景中的表现，本文引入了 **语义复杂场景视频对象分割（SeCVOS）** 基准。\n\n*   **构成：** SeCVOS 包含 160 个手动标注的多场景视频，这些视频旨在通过显著的外观变化和动态场景转换来挑战模型。\n\n### 实验结果\n\nSeC 在 SeCVOS 基准上比 SAM 2.1 取得了 11.8 点的显著提升，这确立了 SeC 在概念感知视频对象分割领域的新技术水平（State-of-the-Art, SOTA）。",
      "shortSummary": "SeC（Segment Concept）是一种新的概念驱动视频对象分割（VOS）框架，旨在解决现有方法在处理复杂场景时缺乏人类概念理解的局限。SeC利用大型视觉-语言模型（LVLMs）构建高层、以对象为中心的语义表示，并自适应地平衡语义推理与特征匹配。为评估概念感知VOS，研究者引入了SeCVOS基准。SeC在SeCVOS上比SAM 2.1提升11.8点，达到了新的技术水平。",
      "translated_title": "SeC：通过渐进式概念构建推进复杂视频对象分割",
      "images": [],
      "contentSource": "完整文章",
      "content": "Video Object Segmentation (VOS) is a core task in computer vision, requiring models to track and segment target objects across video frames. Despite notable advances with recent efforts, current techniques still lag behind human capabilities in handling drastic visual variations, occlusions, and complex scene changes. This limitation arises from their reliance on appearance matching, neglecting the human-like conceptual understanding of objects that enables robust identification across temporal dynamics. Motivated by this gap, we propose Segment Concept (SeC), a concept-driven segmentation framework that shifts from conventional feature matching to the progressive construction and utilization of high-level, object-centric representations. SeC employs Large Vision-Language Models (LVLMs) to integrate visual cues across diverse frames, constructing robust conceptual priors. During inference, SeC forms a comprehensive semantic representation of the target based on processed frames, realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively balances LVLM-based semantic reasoning with enhanced feature matching, dynamically adjusting computational efforts based on scene complexity. To rigorously assess VOS methods in scenarios demanding high-level conceptual reasoning and robust semantic understanding, we introduce the Semantic Complex Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160 manually annotated multi-scenario videos designed to challenge models with substantial appearance variations and dynamic scene transformations. In particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS, establishing a new state-of-the-art in concept-aware video object segmentation."
    },
    {
      "title": "GUI-G^2: 用于GUI定位的高斯奖励建模 (原标题: GUI-G^2: Gaussian Reward Modeling for GUI Grounding)",
      "link": "https://arxiv.org/abs/2507.15846",
      "pubDate": "Mon, 21 Jul 2025 13:53:42 GMT",
      "isoDate": "2025-07-21T13:53:42.000Z",
      "creator": "Fei Tang, Zhangxuan Gu, Zhengxi Lu, Xuyang Liu, Shuheng Shen, Changhua Meng, Wen Wang, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang",
      "summary": "# GUI-G^2: 用于GUI定位的高斯奖励建模\n\n## 引言：GUI定位及其现有挑战\n图形用户界面（GUI）定位是一项关键任务，旨在将自然语言指令映射到精确的界面位置，以实现自主交互。当前基于强化学习的方法通常采用二元奖励机制，将元素视为“命中或未命中”的目标。这种方法产生稀疏的信号，未能充分捕捉空间交互的连续性，从而限制了模型的学习效率和性能。\n\n## GUI-G^2：高斯奖励框架的提出\n受人类点击行为自然形成以目标元素为中心的高斯分布的启发，本文引入了GUI高斯定位奖励（GUI-G^2）框架。这是一个原则性的奖励框架，将GUI元素建模为界面平面上的连续高斯分布。\n\n### GUI-G^2 的协同机制\nGUI-G^2 框架融合了两种协同机制，以提供更丰富、更密集的奖励信号：\n1.  **高斯点奖励（Gaussian Point Rewards）**：\n    *   通过以元素质心为中心的指数衰减分布来建模精确的定位。\n    *   这种机制鼓励模型精确地指向目标元素的中心区域。\n2.  **覆盖奖励（Coverage Rewards）**：\n    *   通过测量预测高斯分布与目标区域之间的重叠程度来评估空间对齐。\n    *   这确保了模型不仅关注中心点，还能考虑到目标元素的整体形状和大小。\n\n### 自适应方差机制\n为了有效处理不同尺寸的GUI元素，GUI-G^2 开发了一种自适应方差机制。该机制根据元素的尺寸校准奖励分布的方差，确保奖励信号能够适应各种大小的交互目标，从而提高模型的泛化能力和精度。\n\n## 优势与影响\nGUI-G^2 框架将GUI定位任务从稀疏的二元分类问题转化为密集的连续优化问题。高斯分布能够生成丰富的梯度信号，有效引导模型学习最佳的交互位置。\n\n## 实验验证与结果\n研究人员在ScreenSpot、ScreenSpot-v2和ScreenSpot-Pro等基准测试上进行了广泛的实验。结果表明，GUI-G^2 显著优于当前最先进的方法UI-TARS-72B，其中在ScreenSpot-Pro上的性能提升最为显著，达到了24.7%。\n\n## 结论\n分析表明，连续建模为界面变化提供了卓越的鲁棒性，并增强了对未见布局的泛化能力。GUI-G^2 为GUI交互任务中的空间推理建立了一个新的范式。",
      "shortSummary": "GUI-G^2 提出了一种用于GUI定位的新型高斯奖励模型，旨在解决现有强化学习方法中稀疏二元奖励的问题。该模型将GUI元素建模为连续高斯分布，结合高斯点奖励和覆盖奖励，并采用自适应方差机制。这使得GUI定位从稀疏分类转变为密集连续优化，生成丰富的梯度信号。实验证明，GUI-G^2 在ScreenSpot基准测试中显著优于现有技术，尤其在ScreenSpot-Pro上性能提升24.7%，增强了模型对界面变化的鲁棒性和泛化能力。",
      "translated_title": "GUI-G^2: 用于GUI定位的高斯奖励建模",
      "images": [],
      "contentSource": "完整文章",
      "content": "Graphical User Interface (GUI) grounding maps natural language instructions to precise interface locations for autonomous interaction. Current reinforcement learning approaches use binary rewards that treat elements as hit-or-miss targets, creating sparse signals that ignore the continuous nature of spatial interactions. Motivated by human clicking behavior that naturally forms Gaussian distributions centered on target elements, we introduce GUI Gaussian Grounding Rewards (GUI-G^2), a principled reward framework that models GUI elements as continuous Gaussian distributions across the interface plane. GUI-G^2 incorporates two synergistic mechanisms: Gaussian point rewards model precise localization through exponentially decaying distributions centered on element centroids, while coverage rewards assess spatial alignment by measuring the overlap between predicted Gaussian distributions and target regions. To handle diverse element scales, we develop an adaptive variance mechanism that calibrates reward distributions based on element dimensions. This framework transforms GUI grounding from sparse binary classification to dense continuous optimization, where Gaussian distributions generate rich gradient signals that guide models toward optimal interaction positions. Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro benchmarks demonstrate that GUI-G^2, substantially outperforms state-of-the-art method UI-TARS-72B, with the most significant improvement of 24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides superior robustness to interface variations and enhanced generalization to unseen layouts, establishing a new paradigm for spatial reasoning in GUI interaction tasks."
    },
    {
      "title": "LLM经济学家：多智能体生成模拟中的大规模人口模型与机制设计 (原标题: LLM Economist: Large Population Models and Mechanism Design in Multi-Agent Generative Simulacra)",
      "link": "https://arxiv.org/abs/2507.15815",
      "pubDate": "Mon, 21 Jul 2025 13:21:14 GMT",
      "isoDate": "2025-07-21T13:21:14.000Z",
      "creator": "Seth Karten, Wenzhe Li, Zihan Ding, Samuel Kleiner, Yu Bai, Chi Jin",
      "summary": "# LLM经济学家：多智能体生成模拟中的大规模人口模型与机制设计\n\n本文介绍了“LLM经济学家”（LLM Economist），这是一个新颖的框架，利用基于智能体的建模方法，在具有层级决策的策略环境中设计和评估经济政策。\n\n## 框架结构与智能体角色\n\n该框架采用双层结构，模拟经济系统中的决策过程：\n\n*   **下层：有限理性工人智能体**\n    *   这些智能体被实例化为基于“人物画像”的提示，其数据来源于经过美国人口普查校准的收入和人口统计数据。\n    *   它们通过最大化在上下文中学习到的基于文本的效用函数来选择劳动供给。\n*   **上层：规划者智能体**\n    *   该智能体利用上下文强化学习（in-context reinforcement learning）来提出分段线性边际税率表。\n    *   这些税率表以当前美国联邦税级为锚点。\n\n## 经济模拟器的关键能力\n\n这种构建方式赋予了经济模拟器进行可靠财政实验所需的三项关键能力：\n\n1.  **异质性效用优化：** 能够优化不同智能体的异质性效用函数。\n2.  **大规模人口生成：** 能够原则性地生成大规模、符合人口统计学现实的智能体群体。\n3.  **自然语言机制设计：** 机制设计（即终极的“助推”问题）完全以自然语言表达。\n\n## 实验结果与政策含义\n\n研究人员对多达一百个交互智能体的群体进行了实验，结果显示：\n\n*   规划者智能体收敛到接近斯塔克尔伯格均衡（Stackelberg equilibria）的状态。\n*   相对于Saez解决方案，这种均衡显著改善了总社会福利。\n*   在去中心化治理下，周期性、基于人物画像的投票程序进一步提升了这些收益。\n\n## 结论与重要性\n\n这些结果表明，基于大型语言模型的智能体能够联合建模、模拟和治理复杂的经济系统。这为社会层面的政策评估提供了一个可操作的测试平台，有助于构建更美好的文明。",
      "shortSummary": "“LLM经济学家”是一个创新框架，利用基于大型语言模型的智能体进行经济政策设计与评估。它通过模拟具有层级决策的多智能体系统实现此目标：下层工人智能体优化劳动供给，上层规划者智能体设计税收政策。该框架能生成大规模、真实的人口模型，并以自然语言进行机制设计。实验表明，其设计的政策能改善总社会福利。这为社会规模的政策评估提供了一个可行的测试平台。",
      "translated_title": "LLM经济学家：多智能体生成模拟中的大规模人口模型与机制设计",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present the LLM Economist, a novel framework that uses agent-based modeling to design and assess economic policies in strategic environments with hierarchical decision-making. At the lower level, bounded rational worker agents -- instantiated as persona-conditioned prompts sampled from U.S. Census-calibrated income and demographic statistics -- choose labor supply to maximize text-based utility functions learned in-context. At the upper level, a planner agent employs in-context reinforcement learning to propose piecewise-linear marginal tax schedules anchored to the current U.S. federal brackets. This construction endows economic simulacra with three capabilities requisite for credible fiscal experimentation: (i) optimization of heterogeneous utilities, (ii) principled generation of large, demographically realistic agent populations, and (iii) mechanism design -- the ultimate nudging problem -- expressed entirely in natural language. Experiments with populations of up to one hundred interacting agents show that the planner converges near Stackelberg equilibria that improve aggregate social welfare relative to Saez solutions, while a periodic, persona-level voting procedure furthers these gains under decentralized governance. These results demonstrate that large language model-based agents can jointly model, simulate, and govern complex economic systems, providing a tractable test bed for policy evaluation at the societal scale to help build better civilizations."
    },
    {
      "title": "稳定知识，促进推理：RLVR的双令牌约束 (原标题: Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR)",
      "link": "https://arxiv.org/abs/2507.15778",
      "pubDate": "Mon, 21 Jul 2025 12:34:01 GMT",
      "isoDate": "2025-07-21T12:34:01.000Z",
      "creator": "Jiakang Wang, Runze Liu, Fuzheng Zhang, Xiu Li, Guorui Zhou",
      "summary": "## 稳定知识，促进推理：RLVR的双令牌约束\n\n### 摘要\n\n本文提出了一种名为 Archer 的新型强化学习方法，旨在优化大型语言模型（LLMs）的推理能力。该方法通过引入双令牌约束和同步更新机制，解决了现有强化学习与可验证奖励（RLVR）算法的局限性。\n\n### 现有问题\n\n*   **统一训练信号的局限性**：传统的RLVR算法在训练过程中对所有令牌应用统一的训练信号，未能区分低熵的知识相关令牌和高熵的推理相关令牌的不同作用。\n*   **语义依赖性破坏**：尽管一些近期方法尝试通过梯度掩蔽或异步更新来区分令牌类型，但这些方法可能破坏模型输出中的语义依赖性，从而阻碍有效的学习。\n\n### 提出的解决方案：Archer\n\n*   **熵感知RLVR**：Archer 是一种熵感知的RLVR方法，它引入了双令牌约束和同步更新机制。\n*   **双令牌约束机制**：\n    *   **推理令牌**：对推理相关令牌（高熵）应用较弱的KL正则化和较高的裁剪阈值，以鼓励模型进行探索和生成多样化的推理路径。\n    *   **知识令牌**：对知识相关令牌（低熵）施加强约束，以确保模型保持事实知识的准确性和稳定性。\n*   **同步更新**：与异步更新方法不同，Archer 采用同步更新，避免了破坏模型输出的语义连贯性。\n\n### 实验结果\n\n*   **显著提升**：在多个数学推理和代码生成基准测试中，Archer 方法显著优于先前的RLVR方法。\n*   **达到或超越SOTA**：在同等规模的模型中，Archer 的性能达到或超越了现有最先进的水平。\n\n### 代码可用性\n\n*   相关代码已在 GitHub 上开源。",
      "shortSummary": "本文提出 Archer，一种新的RLVR方法，通过双令牌约束和同步更新来提升LLM的推理能力。它针对知识令牌施加强约束以保持事实，对推理令牌施加弱约束以鼓励探索，解决了传统RLVR统一训练信号的不足。实验表明，Archer 在数学推理和代码生成任务上显著优于现有方法，达到或超越了同等规模模型的SOTA性能。",
      "translated_title": "稳定知识，促进推理：RLVR的双令牌约束",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective post-training method for improving the reasoning abilities of Large Language Models (LLMs), mainly by shaping higher-order behaviors such as reflection and planning. However, previous RLVR algorithms often apply uniform training signals to all tokens, without considering the different roles of low-entropy knowledge-related tokens and high-entropy reasoning-related tokens. Some recent methods try to separate these token types by gradient masking or asynchronous updates, but these approaches may break semantic dependencies in the model output and hinder effective learning. In this work, we propose Archer, an entropy-aware RLVR approach with dual-token constraints and synchronous updates. Specifically, our method applies weaker KL regularization and higher clipping thresholds to reasoning tokens to encourage exploration, while using stronger constraints on knowledge tokens to maintain factual knowledge. Experimental results on several mathematical reasoning and code generation benchmarks show that our approach significantly outperforms previous RLVR methods, reaching or exceeding state-of-the-art performance among models of comparable size. The code is available at https://github.com/wizard-III/ArcherCodeR."
    },
    {
      "title": "数据混合智能体：学习重新加权领域以进行持续预训练 (原标题: Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training)",
      "link": "https://arxiv.org/abs/2507.15640",
      "pubDate": "Mon, 21 Jul 2025 10:01:54 GMT",
      "isoDate": "2025-07-21T10:01:54.000Z",
      "creator": "Kailai Yang, Xiao Liu, Lei Ji, Hao Li, Yeyun Gong, Peng Cheng, Mao Yang",
      "summary": "## 数据混合智能体：学习重新加权领域以进行持续预训练\n\n### 摘要\n\n本文介绍了一种名为“数据混合智能体”（Data Mixing Agent）的新型框架，旨在解决大型语言模型在小规模任务特定数据上进行持续预训练时面临的灾难性遗忘问题。传统的解决方案依赖于手动指定或基于经验的启发式方法来重新加权源领域和目标领域的训练数据混合，以实现性能平衡。然而，这些方法往往缺乏通用性。\n\n### 提出的方法：数据混合智能体\n\n*   **创新性：** 数据混合智能体是首个模型驱动、端到端的框架，能够通过学习来重新加权领域。\n*   **学习机制：** 该智能体通过强化学习（Reinforcement Learning）来学习可泛化的启发式方法。它利用大量的“数据混合轨迹”以及来自评估环境的相应反馈进行训练。\n\n### 实验结果与优势\n\n*   **性能提升：** 在数学推理的持续预训练实验中，数据混合智能体在实现源领域和目标领域基准的平衡性能方面，显著优于现有强基线方法。\n*   **强大的泛化能力：** 该智能体无需重新训练，即可很好地泛化到未见过的源领域、目标模型和领域空间。\n*   **跨领域适应性：** 直接应用于代码生成领域也表明了其在不同目标领域间的良好适应性。\n*   **启发式方法与人类直觉的一致性：** 进一步分析表明，智能体学习到的启发式方法与人类直觉高度一致。\n*   **高效性：** 智能体能够以更少的源领域数据实现卓越的模型性能，展现出其高效性。\n\n### 研究领域\n\n*   机器学习 (cs.LG)\n*   人工智能 (cs.AI)\n*   计算与语言 (cs.CL)",
      "shortSummary": "“数据混合智能体”是一种新颖的模型驱动框架，通过强化学习自动学习如何重新加权源领域和目标领域数据，以优化大型语言模型的持续预训练。它解决了传统手动加权方法导致的灾难性遗忘问题。实验证明，该智能体在数学推理和代码生成等任务中，能实现平衡且优于基线的性能，并展现出良好的泛化能力和数据效率，无需重新训练即可适应新场景。",
      "translated_title": "数据混合智能体：学习重新加权领域以进行持续预训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "Continual pre-training on small-scale task-specific data is an effective method for improving large language models in new target fields, yet it risks catastrophic forgetting of their original capabilities. A common solution is to re-weight training data mixtures from source and target fields on a domain space to achieve balanced performance. Previous domain reweighting strategies rely on manual designation with certain heuristics based on human intuition or empirical results. In this work, we prove that more general heuristics can be parameterized by proposing Data Mixing Agent, the first model-based, end-to-end framework that learns to re-weight domains. The agent learns generalizable heuristics through reinforcement learning on large quantities of data mixing trajectories with corresponding feedback from an evaluation environment. Experiments in continual pre-training on math reasoning show that Data Mixing Agent outperforms strong baselines in achieving balanced performance across source and target field benchmarks. Furthermore, it generalizes well across unseen source fields, target models, and domain spaces without retraining. Direct application to the code generation field also indicates its adaptability across target domains. Further analysis showcases the agents' well-aligned heuristics with human intuitions and their efficiency in achieving superior model performance with less source-field data."
    },
    {
      "title": "基于离散化SDF的高斯泼溅用于可重照明资产 (原标题: Gaussian Splatting with Discretized SDF for Relightable Assets)",
      "link": "https://arxiv.org/abs/2507.15629",
      "pubDate": "Mon, 21 Jul 2025 09:52:33 GMT",
      "isoDate": "2025-07-21T09:52:33.000Z",
      "creator": "Zuo-Liang Zhu, Jian Yang, Beibei Wang",
      "summary": "## 基于离散化SDF的高斯泼溅用于可重照明资产\n\n### 1. 背景与挑战\n\n*   **3D高斯泼溅 (3DGS)**：在新型视图合成 (NVS) 任务中展现出卓越的细节表达能力和高效的渲染速度。\n*   **逆向渲染应用挑战**：将3DGS应用于逆向渲染面临挑战，主要原因是高斯基元的离散性质使得几何约束难以应用。\n*   **现有解决方案及其局限性**：\n    *   近期工作引入**符号距离场 (SDF)** 作为额外的连续表示来规范高斯基元定义的几何。\n    *   这些方法虽然改善了分解质量，但代价是增加了内存使用量并使训练过程复杂化。\n\n### 2. 提出的方法：离散化SDF\n\n*   **核心思想**：与现有方法不同，本文引入了一种**离散化SDF**来表示连续SDF。\n*   **实现方式**：通过在每个高斯内部编码一个采样值，以离散方式表示SDF。\n*   **关键连接**：这种方法允许通过**SDF到不透明度转换**将SDF与高斯不透明度关联起来。\n*   **渲染优势**：\n    *   能够通过泼溅（splatting）渲染SDF，从而避免了传统光线追踪的计算成本。\n\n### 3. 几何正则化：投影一致性损失\n\n*   **主要挑战**：如何规范离散样本使其与底层SDF保持一致，因为离散表示难以应用基于梯度的约束（例如Eikonal损失）。\n*   **解决方案**：\n    *   将高斯投影到SDF的零水平集上。\n    *   强制通过泼溅渲染的表面与SDF的零水平集对齐。\n    *   这被称为**基于投影的一致性损失**。\n\n### 4. 方法优势与实验结果\n\n*   **更高重照明质量**：得益于离散化SDF，本文方法实现了更高的重照明质量。\n*   **内存效率**：无需额外内存，与标准3DGS所需的内存量相同。\n*   **优化简化**：避免了复杂的手动设计优化过程。\n*   **实验结果**：实验表明，本文方法优于现有的基于高斯的反向渲染方法。\n*   **代码可用性**：相关代码已开源。",
      "shortSummary": "本文提出一种基于离散化SDF的高斯泼溅方法，用于可重照明资产的逆向渲染。通过在每个高斯中编码SDF采样值，并将其与不透明度关联，实现了SDF的泼溅渲染，避免了光线追踪。为解决离散表示的几何约束问题，引入了基于投影的一致性损失。该方法在不增加额外内存和避免复杂优化的情况下，显著提高了重照明质量，并优于现有高斯基逆向渲染方法。",
      "translated_title": "基于离散化SDF的高斯泼溅用于可重照明资产",
      "images": [],
      "contentSource": "完整文章",
      "content": "3D Gaussian splatting (3DGS) has shown its detailed expressive ability and highly efficient rendering speed in the novel view synthesis (NVS) task. The application to inverse rendering still faces several challenges, as the discrete nature of Gaussian primitives makes it difficult to apply geometry constraints. Recent works introduce the signed distance field (SDF) as an extra continuous representation to regularize the geometry defined by Gaussian primitives. It improves the decomposition quality, at the cost of increasing memory usage and complicating training. Unlike these works, we introduce a discretized SDF to represent the continuous SDF in a discrete manner by encoding it within each Gaussian using a sampled value. This approach allows us to link the SDF with the Gaussian opacity through an SDF-to-opacity transformation, enabling rendering the SDF via splatting and avoiding the computational cost of ray marching.The key challenge is to regularize the discrete samples to be consistent with the underlying SDF, as the discrete representation can hardly apply the gradient-based constraints (\\eg Eikonal loss). For this, we project Gaussians onto the zero-level set of SDF and enforce alignment with the surface from splatting, namely a projection-based consistency loss. Thanks to the discretized SDF, our method achieves higher relighting quality, while requiring no extra memory beyond GS and avoiding complex manually designed optimization. The experiments reveal that our method outperforms existing Gaussian-based inverse rendering methods. Our code is available at https://github.com/NK-CS-ZZL/DiscretizedSDF."
    },
    {
      "title": "Being-H0：基于大规模人类视频的视觉-语言-动作预训练 (原标题: Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos)",
      "link": "https://arxiv.org/abs/2507.15597",
      "pubDate": "Mon, 21 Jul 2025 09:19:09 GMT",
      "isoDate": "2025-07-21T09:19:09.000Z",
      "creator": "Hao Luo, Yicheng Feng, Wanpeng Zhang, Sipeng Zheng, Ye Wang, Haoqi Yuan, Jiazheng Liu, Chaoyi Xu, Qin Jin, Zongqing Lu",
      "summary": "# Being-H0：基于大规模人类视频的视觉-语言-动作预训练\n\n## 1. 引言与背景\n\n*   **模型名称：** Being-H0\n*   **模型类型：** 一种灵巧的视觉-语言-动作（VLA）模型。\n*   **训练数据来源：** 基于大规模人类视频进行训练。\n*   **解决的问题：**\n    *   现有VLA模型在需要高灵巧度的复杂操作任务上表现不力。\n    *   对新场景和任务的泛化能力较差。\n    *   主要原因在于它们依赖合成数据（存在显著的模拟到现实差距）或缺乏规模和多样性的远程操作演示。\n\n## 2. 核心方法与创新\n\n*   **数据瓶颈应对策略：** 提出利用人类手部作为基础操作器，以充分利用网络数据中丰富的灵巧性和可扩展性。\n*   **核心训练范式——物理指令调优（Physical Instruction Tuning）：**\n    *   这是一种新颖的训练范式，结合了以下几个方面：\n        *   从人类视频中进行大规模VLA预训练。\n        *   为3D推理进行物理空间对齐。\n        *   为机器人任务进行后期训练适应。\n*   **动作学习方法——部分级运动标记化（Part-level Motion Tokenization）：**\n    *   引入了一种部分级运动标记化方法，能够实现毫米级重建精度，以精确建模手部轨迹，从而进行动作学习。\n*   **数据策展管道：**\n    *   开发了一个全面的数据策展管道，能够整合异构数据源，包括动作捕捉、VR和纯RGB视频。\n    *   构建了一个包含数百万个基于运动的指令实例的大规模数据集。\n\n## 3. 实验结果与贡献\n\n*   **卓越表现：** 经验性地证明了Being-H0在手部运动生成和指令遵循方面的卓越性能。\n*   **良好可扩展性：** 模型的性能随模型和数据规模的增大而良好扩展。\n*   **真实世界应用：** 重要的是，当应用物理指令调优时，Being-H0在真实世界机器人操作中显示出预期的性能提升。\n\n## 4. 附加信息\n\n*   **文档长度：** 37页\n*   **相关主题：** 计算机视觉与模式识别 (cs.CV)、机器学习 (cs.LG)、机器人学 (cs.RO)\n*   **更多详情：** 可通过提供的链接获取更多信息。",
      "shortSummary": "Being-H0是一种基于大规模人类视频的灵巧视觉-语言-动作（VLA）模型，旨在解决现有VLA在复杂操作和泛化方面的不足。它利用人类手部作为基础操作器，并引入了“物理指令调优”范式和“部分级运动标记化”方法。Being-H0在手部运动生成、指令遵循以及真实世界机器人操作中表现出色，并能随模型和数据规模良好扩展。",
      "translated_title": "Being-H0：基于大规模人类视频的视觉-语言-动作预训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained on large-scale human videos. Existing VLAs struggle with complex manipulation tasks requiring high dexterity and generalize poorly to novel scenarios and tasks, primarily due to their reliance on synthetic data with significant sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To address this data bottleneck, we propose leveraging human hands as a foundation manipulator, capitalizing on the rich dexterity and scalability present in web data. Our approach centers on physical instruction tuning, a novel training paradigm that combines large-scale VLA pretraining from human videos, physical space alignment for 3D reasoning, and post-training adaptation for robotic tasks. Additionally, we introduce a part-level motion tokenization method which achieves millimeter-level reconstruction accuracy to model precise hand trajectories for action learning. To support our proposed paradigm, we further develop a comprehensive data curation pipeline that integrates heterogeneous sources -- including motion capture, VR, and RGB-only videos -- into a large-scale dataset with millions of motion-based instructional instances. We empirically show the excellence of Being-H0 in hand motion generation and instruction following, and it also scales well with model and data sizes. Importantly, we observe the expected gains of Being-H0 in real-world robotic manipulation as physical instruction tuning is applied. More details are available at https://beingbeyond.github.io/Being-H0."
    },
    {
      "title": "GR-3 技术报告 (原标题: GR-3 Technical Report)",
      "link": "https://arxiv.org/abs/2507.15493",
      "pubDate": "Mon, 21 Jul 2025 06:54:13 GMT",
      "isoDate": "2025-07-21T06:54:13.000Z",
      "creator": "Chilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, Hao Niu, Wenxuan Ou, Wanli Peng, Zeyu Ren, Haixin Shi, Jiawen Tian, Hongtao Wu, Xin Xiao, Yuyang Xiao, Jiafeng Xu, Yichu Yang",
      "summary": "# GR-3 技术报告：通用机器人策略的最新进展\n\n## 1. GR-3 概述\nGR-3 是一种大规模视觉-语言-动作 (VLA) 模型，代表了在构建通用机器人策略方面的最新进展。它旨在实现机器人对新颖环境和任务的广泛适应性。\n\n## 2. 核心能力\n*   **卓越的泛化能力：** GR-3 能够泛化到新颖的物体、环境以及涉及抽象概念的指令，展现出强大的适应性。\n*   **高效微调：** 仅需最少的人类轨迹数据即可进行高效微调，从而实现对新设置的快速且经济高效的适应。\n*   **处理复杂任务：** 在处理长时程和灵巧任务方面表现出色，包括需要双手操作和移动的任务，展现出强大的鲁棒性和可靠性。\n\n## 3. 训练方法\nGR-3 的这些能力是通过多方面训练方法实现的，包括：\n*   与网络规模的视觉-语言数据进行协同训练。\n*   利用通过 VR 设备收集的人类轨迹数据进行高效微调。\n*   通过机器人轨迹数据进行有效的模仿学习。\n\n## 4. 配套硬件：ByteMini\n报告中还介绍了 ByteMini，这是一款多功能双手移动机器人。ByteMini 具有卓越的灵活性和可靠性，与 GR-3 集成后，能够完成广泛的任务。\n\n## 5. 实验结果与未来展望\n通过广泛的真实世界实验，GR-3 在各种具有挑战性的任务上超越了最先进的基线方法 $\\pi_0$。研究人员希望 GR-3 能够成为构建能够协助人类日常生活的通用机器人的重要一步。",
      "shortSummary": "GR-3 是一种大规模视觉-语言-动作 (VLA) 模型，在通用机器人策略方面取得显著进展。它展现出对新物体、环境和抽象概念的卓越泛化能力，并能通过少量人类数据高效微调。GR-3 擅长处理长时程和灵巧任务，包括双手操作和移动。其能力得益于结合网络数据、VR 人类轨迹和机器人轨迹的训练。GR-3 与多功能机器人 ByteMini 结合，在真实世界任务中超越了现有基线，旨在推动通用机器人协助人类。",
      "translated_title": "GR-3 技术报告",
      "images": [],
      "contentSource": "完整文章",
      "content": "We report our recent progress towards building generalist robot policies, the development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model. It showcases exceptional capabilities in generalizing to novel objects, environments, and instructions involving abstract concepts. Furthermore, it can be efficiently fine-tuned with minimal human trajectory data, enabling rapid and cost-effective adaptation to new settings. GR-3 also excels in handling long-horizon and dexterous tasks, including those requiring bi-manual manipulation and mobile movement, showcasing robust and reliable performance. These capabilities are achieved through a multi-faceted training recipe that includes co-training with web-scale vision-language data, efficient fine-tuning from human trajectory data collected via VR devices, and effective imitation learning with robot trajectory data. In addition, we introduce ByteMini, a versatile bi-manual mobile robot designed with exceptional flexibility and reliability, capable of accomplishing a wide range of tasks when integrated with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the state-of-the-art baseline method, pi_0, on a wide variety of challenging tasks. We hope GR-3 can serve as a step towards building generalist robots capable of assisting humans in daily life."
    },
    {
      "title": "ObjectGS：基于高斯泼溅的物体感知场景重建与场景理解 (原标题: ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting)",
      "link": "https://arxiv.org/abs/2507.15454",
      "pubDate": "Mon, 21 Jul 2025 06:06:23 GMT",
      "isoDate": "2025-07-21T06:06:23.000Z",
      "creator": "Ruijie Zhu, Mulin Yu, Linning Xu, Lihan Jiang, Yixuan Li, Tianzhu Zhang, Jiangmiao Pang, Bo Dai",
      "summary": "## ObjectGS：基于高斯泼溅的物体感知场景重建与场景理解\n\n### 核心问题\n\n*   3D高斯泼溅（3D Gaussian Splatting）技术以其高保真重建和实时新视角合成能力而闻名。\n*   然而，该技术缺乏语义理解能力，限制了其在物体层面的感知。\n\n### 提出的解决方案：ObjectGS\n\n*   **ObjectGS** 是一个物体感知框架，旨在统一3D场景重建与语义理解。\n*   它不再将整个场景视为一个统一的整体，而是将**单个物体建模为局部锚点**。\n*   这些锚点能够生成神经高斯（neural Gaussians）并共享物体ID，从而实现**精确的物体级别重建**。\n\n### 工作原理与训练\n\n*   在训练过程中，ObjectGS会**动态地增长或修剪这些锚点**。\n*   同时，系统会**优化锚点的特征**。\n*   通过使用**独热编码（one-hot ID encoding）**并结合**分类损失（classification loss）**，ObjectGS能够强制执行清晰的语义约束。\n\n### 实验结果与优势\n\n*   **性能卓越**：广泛的实验表明，ObjectGS在开放词汇（open-vocabulary）和全景分割（panoptic segmentation）任务上均**超越了现有最先进的方法**。\n*   **无缝集成**：ObjectGS能够与网格提取（mesh extraction）和场景编辑（scene editing）等应用**无缝集成**，展现了其在实际应用中的潜力。\n\n### 其他信息\n\n*   该工作已被**ICCV 2025接受**。",
      "shortSummary": "ObjectGS是一个创新的物体感知框架，它将3D高斯泼溅技术与语义理解相结合。该方法通过将单个物体建模为带有共享ID的局部锚点，实现了精确的物体级别重建。在训练中，ObjectGS动态优化这些锚点并利用分类损失强化语义约束。实验证明，ObjectGS在开放词汇和全景分割任务上超越了现有技术，并能无缝集成到网格提取和场景编辑等应用中，显著提升了场景理解能力。",
      "translated_title": "ObjectGS：基于高斯泼溅的物体感知场景重建与场景理解",
      "images": [],
      "contentSource": "完整文章",
      "content": "3D Gaussian Splatting is renowned for its high-fidelity reconstructions and real-time novel view synthesis, yet its lack of semantic understanding limits object-level perception. In this work, we propose ObjectGS, an object-aware framework that unifies 3D scene reconstruction with semantic understanding. Instead of treating the scene as a unified whole, ObjectGS models individual objects as local anchors that generate neural Gaussians and share object IDs, enabling precise object-level reconstruction. During training, we dynamically grow or prune these anchors and optimize their features, while a one-hot ID encoding with a classification loss enforces clear semantic constraints. We show through extensive experiments that ObjectGS not only outperforms state-of-the-art methods on open-vocabulary and panoptic segmentation tasks, but also integrates seamlessly with applications like mesh extraction and scene editing. Project page: https://ruijiezhu94.github.io/ObjectGS_page"
    },
    {
      "title": "STITCH：面向口语语言模型的块状推理同步思考与表达 (原标题: STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models)",
      "link": "https://arxiv.org/abs/2507.15375",
      "pubDate": "Mon, 21 Jul 2025 04:30:03 GMT",
      "isoDate": "2025-07-21T04:30:03.000Z",
      "creator": "Cheng-Han Chiang, Xiaofei Wang, Linjie Li, Chung-Ching Lin, Kevin Lin, Shujie Liu, Zhendong Wang, Zhengyuan Yang, Hung-yi Lee, Lijuan Wang",
      "summary": "STITCH：面向口语语言模型的块状推理同步思考与表达\n\n**1. 背景与问题**\n\n*   **口语语言模型 (SLMs)**：旨在接收语音输入并生成语音响应。\n*   **当前SLMs的局限性**：缺乏内部的、无声的思考过程。这与人类形成对比，人类通常会进行复杂的内部心理推理，从而能够清晰简洁地表达思想。\n*   **集成思考的必要性**：因此，将无声的思考过程整合到SLMs中是非常理想的。\n*   **传统方法的挑战**：在开始说话前天真地生成完整的思维链（CoT）推理虽然可以使SLMs进行思考，但这会引入额外的语音响应延迟，因为CoT推理的长度可能是任意的。\n\n**2. STITCH方法**\n\n*   **核心理念**：为了解决高延迟问题，研究人员提出了STITCH，这是一种新颖的生成方法，它在生成无声推理块和口语响应块之间交替进行。\n*   **实现机制**：\n    *   口语响应块的音频持续时间远长于生成该口语响应块中令牌所需的时间。\n    *   STITCH利用这段剩余的“空闲时间”来生成无声推理令牌。\n    *   当一个口语音频块播放给用户时，模型会继续生成下一个无声推理块，从而实现“同步思考与表达”。\n\n**3. 实验结果与优势**\n\n*   **延迟表现**：STITCH在延迟方面与那些设计上无法生成无声CoT的基线模型相匹配。\n*   **推理能力提升**：在数学推理数据集上，STITCH的表现比这些基线模型高出15%。\n*   **非推理任务表现**：在非推理数据集上，STITCH的表现与基线模型同样出色。\n\n**4. 项目状态与领域**\n\n*   **状态**：该工作目前正在进行中。\n*   **相关领域**：计算与语言（cs.CL）；音频与语音处理（eess.AS）。",
      "shortSummary": "STITCH是一种新型生成方法，旨在解决口语语言模型（SLMs）缺乏内部思考且传统思维链（CoT）导致高延迟的问题。它通过交替生成无声推理块和口语响应块，利用口语播放的空闲时间同步生成后续推理内容，实现了“同步思考与表达”。STITCH在保持与基线模型相同延迟的同时，在数学推理任务上性能提升15%，在非推理任务上表现相当。",
      "translated_title": "STITCH：面向口语语言模型的块状推理同步思考与表达",
      "images": [],
      "contentSource": "完整文章",
      "content": "Spoken Language Models (SLMs) are designed to take speech inputs and produce spoken responses. However, current SLMs lack the ability to perform an internal, unspoken thinking process before responding. In contrast, humans typically engage in complex mental reasoning internally, enabling them to communicate ideas clearly and concisely. Thus, integrating an unspoken thought process into SLMs is highly desirable. While naively generating a complete chain-of-thought (CoT) reasoning before starting to talk can enable thinking for SLMs, this induces additional latency for the speech response, as the CoT reasoning can be arbitrarily long. To solve this issue, we propose Stitch, a novel generation method that alternates between the generation of unspoken reasoning chunks and spoken response chunks. Since the audio duration of a chunk of spoken response is much longer than the time to generate the tokens in a chunk of spoken response, we use the remaining free time to generate the unspoken reasoning tokens. When a chunk of audio is played to the user, the model continues to generate the next unspoken reasoning chunk, achieving simultaneous thinking and talking. Remarkably, Stitch matches the latency of baselines that cannot generate unspoken CoT by design while outperforming those baselines by 15% on math reasoning datasets; Stitch also performs equally well on non-reasoning datasets as those baseline models. Some animations and demonstrations are on the project page: https://d223302.github.io/STITCH."
    }
  ],
  "lastUpdated": "2025-07-23T09:40:55.204Z"
}