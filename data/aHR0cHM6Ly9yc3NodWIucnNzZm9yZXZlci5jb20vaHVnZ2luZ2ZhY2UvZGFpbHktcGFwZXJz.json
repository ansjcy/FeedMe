{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "GenRecal：从大型到小型视觉-语言模型的重新校准后生成 (原标题: GenRecal: Generation after Recalibration from Large to Small Vision-Language Models)",
      "link": "https://arxiv.org/abs/2506.15681",
      "pubDate": "Wed, 18 Jun 2025 13:59:49 GMT",
      "isoDate": "2025-06-18T13:59:49.000Z",
      "creator": "Byung-Kwan Lee, Ryo Hachiuma, Yong Man Ro, Yu-Chiang Frank Wang, Yueh-Hua Wu",
      "summary": "## GenRecal：从大型到小型视觉-语言模型的重新校准后生成\n\n### 背景与挑战\n\n*   **大型视觉-语言模型（VLMs）的进步与部署难题：** 近期VLM取得了显著进展，性能可与GPT-4V等闭源系统媲美。然而，由于其巨大的计算需求，在资源受限设备上部署这些模型面临挑战。\n*   **知识蒸馏的需求：** 这促使研究人员探索将知识从大型VLM蒸馏到更小、更高效的模型。\n*   **核心挑战——VLM架构异构性：** 现有蒸馏方法面临的关键挑战是VLM架构的多样性。这些模型基于不同的LLMs构建，并采用不同的令牌类型（词汇量大小、令牌分割方式、令牌索引顺序），这限制了蒸馏方法对特定VLM类型的适用性。\n\n### GenRecal框架\n\n*   **提出GenRecal：** 为解决上述限制，本文提出了**GenRecal（Generation after Recalibration）**，一个新颖的、通用的VLM蒸馏框架。\n*   **核心机制——重新校准器（Recalibrator）：** GenRecal包含一个“重新校准器”，其作用是：\n    *   对齐并调整异构VLM之间的特征表示。\n    *   从而实现在不同类型VLM之间进行有效的知识迁移。\n\n### 实验结果\n\n*   **性能提升：** 通过在多个具有挑战性的基准测试上进行广泛实验，结果表明GenRecal显著提升了基线性能。\n*   **超越现有模型：** 最终，GenRecal甚至超越了大型开源和闭源VLM的性能。",
      "shortSummary": "GenRecal是一个通用的视觉-语言模型（VLM）蒸馏框架，旨在解决大型VLM部署的计算挑战。它通过引入“重新校准器”来对齐和适应异构VLM之间的特征表示，从而实现跨不同VLM类型的有效知识迁移。实验证明，GenRecal显著提升了性能，甚至超越了大型开源和闭源VLM，为在资源受限设备上部署高效VLM提供了解决方案。",
      "translated_title": "GenRecal：从大型到小型视觉-语言模型的重新校准后生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve performance on par with closed-source systems like GPT-4V. However, deploying these models in real-world scenarios, particularly on resource-constrained devices, remains challenging due to their substantial computational demands. This has spurred interest in distilling knowledge from large VLMs into smaller, more efficient counterparts. A key challenge arises here from the diversity of VLM architectures, which are built on different LLMs and employ varying token types-differing in vocabulary size, token splits, and token index ordering. To address this challenge of limitation to a specific VLM type, we present Generation after Recalibration (GenRecal), a novel, general-purpose distillation framework for VLMs. GenRecal incorporates a Recalibrator that aligns and adapts feature representations between heterogeneous VLMs, enabling effective knowledge transfer across different types of VLMs. Through extensive experiments on multiple challenging benchmarks, we demonstrate that GenRecal significantly improves baseline performances, eventually outperforming large-scale open- and closed-source VLMs."
    },
    {
      "title": "具身网络智能体：弥合物理-数字领域，实现集成智能 (原标题: Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence)",
      "link": "https://arxiv.org/abs/2506.15677",
      "pubDate": "Wed, 18 Jun 2025 13:58:17 GMT",
      "isoDate": "2025-06-18T13:58:17.000Z",
      "creator": "Yining Hong, Rui Sun, Bingxuan Li, Xingcheng Yao, Maxine Wu, Alexander Chien, Da Yin, Ying Nian Wu, Zhecan James Wang, Kai-Wei Chang",
      "summary": "# 具身网络智能体：弥合物理-数字领域，实现集成智能\n\n## 引言：当前AI智能体的局限性\n目前的AI智能体大多是孤立的：它们要么专注于检索和推理海量的在线数字信息和知识，要么通过具身感知、规划和行动与物理世界互动，但很少能同时做到这两点。这种分离限制了它们解决需要集成物理和数字智能的任务的能力，例如：\n*   根据在线食谱烹饪\n*   利用动态地图数据进行导航\n*   使用网络知识解释现实世界中的地标\n\n## 具身网络智能体：一种新范式\n为了解决上述限制，本文引入了“具身网络智能体”（Embodied Web Agents）这一新颖的AI智能体范式。该范式旨在流畅地连接具身（物理互动）与网络规模推理（数字信息），从而实现更全面的智能。\n\n## 概念的实现与评估\n为了将这一概念付诸实践，研究团队进行了以下工作：\n\n### 1. 具身网络智能体任务环境\n开发了一个统一的模拟平台，该平台紧密整合了：\n*   逼真的3D室内和室外环境\n*   功能性的网络界面\n\n这个平台为智能体在物理和数字领域进行交互提供了基础。\n\n### 2. 具身网络智能体基准测试\n基于上述平台，构建并发布了“具身网络智能体基准测试”（Embodied Web Agents Benchmark）。该基准测试包含一系列多样化的任务，旨在系统性地评估跨领域智能，这些任务包括：\n*   烹饪\n*   导航\n*   购物\n*   旅游\n*   地理定位\n\n所有这些任务都要求智能体在物理和数字领域之间进行协调推理。\n\n## 实验结果与未来展望\n实验结果揭示了当前最先进的AI系统与人类能力之间存在显著的性能差距。这表明在具身认知和网络规模知识访问的交叉领域，既存在巨大的挑战，也蕴含着广阔的机遇。\n\n## 资源可用性\n所有相关的数据集、代码和网站均已在其项目页面（this https URL）上公开提供。",
      "shortSummary": "“具身网络智能体”提出了一种新型AI范式，旨在弥合当前AI智能体在物理世界交互与网络知识推理之间的鸿沟。为实现此目标，研究团队开发了集成3D环境与网络界面的模拟平台，并构建了包含烹饪、导航等任务的基准测试。实验结果显示，现有AI系统与人类能力之间存在显著差距，揭示了具身认知与网络知识结合领域的挑战与机遇。所有资源均已公开。",
      "translated_title": "具身网络智能体：弥合物理-数字领域，实现集成智能",
      "images": [],
      "contentSource": "完整文章",
      "content": "AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/."
    },
    {
      "title": "Sekai：一个面向世界探索的视频数据集 (原标题: Sekai: A Video Dataset towards World Exploration)",
      "link": "https://arxiv.org/abs/2506.15675",
      "pubDate": "Wed, 18 Jun 2025 13:57:06 GMT",
      "isoDate": "2025-06-18T13:57:06.000Z",
      "creator": "Zhen Li, Chuanhao Li, Xiaofeng Mao, Shaoheng Lin, Ming Li, Shitian Zhao, Zhaopan Xu, Xinyue Li, Yukang Feng, Jianwen Sun, Zizhen Li, Fanrui Zhang, Jiaxin Ai, Zhixiang Wang, Yuwei Wu, Tong He, Jiangmiao Pang, Yu Qiao, Yunde Jia, Kaipeng Zhang",
      "summary": "# Sekai：一个面向世界探索的视频数据集\n\n## 摘要\n\n本文介绍了Sekai数据集，这是一个高质量的第一人称视角（FPV）全球视频数据集，旨在解决现有视频生成数据集在世界探索训练方面的局限性。\n\n## 现有问题\n\n*   **局限性：** 当前的视频生成技术在交互式世界探索方面潜力巨大，但现有数据集存在以下不足：\n    *   地点有限\n    *   持续时间短\n    *   场景静态\n    *   缺乏关于探索和世界本身的注释\n\n## Sekai数据集介绍\n\n*   **名称含义：** “Sekai”在日语中意为“世界”。\n*   **数据集类型：** 高质量的第一人称视角（FPV）全球视频数据集，包含丰富的世界探索注释。\n*   **内容规模：**\n    *   超过5,000小时的步行或无人机视角（FPV和UVA）视频。\n    *   视频来源覆盖100多个国家和地区，750个城市。\n*   **数据收集与处理：**\n    *   开发了一个高效且有效的工具箱，用于视频的收集、预处理和注释。\n*   **丰富注释：** 视频包含以下注释信息：\n    *   地点（Location）\n    *   场景（Scene）\n    *   天气（Weather）\n    *   人群密度（Crowd density）\n    *   字幕/描述（Captions）\n    *   摄像机轨迹（Camera trajectories）\n\n## 实验与应用\n\n*   **数据集质量：** 实验证明了Sekai数据集的质量。\n*   **模型训练：** 研究人员使用Sekai的一个子集训练了一个名为YUME（日语中意为“梦想”）的交互式视频世界探索模型。\n*   **预期影响：** 作者相信Sekai将：\n    *   推动视频生成和世界探索领域的发展。\n    *   激发有价值的应用。",
      "shortSummary": "Sekai是一个高质量的第一人称视角（FPV）全球视频数据集，旨在解决现有视频数据集在世界探索训练中的局限性。它包含来自100多个国家和750个城市的5000多小时视频，并提供地点、场景、天气、人群密度、字幕和摄像机轨迹等丰富注释。Sekai的推出有望推动视频生成和世界探索领域的发展，并已用于训练交互式模型YUME。",
      "translated_title": "Sekai：一个面向世界探索的视频数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "Video generation techniques have made remarkable progress, promising to be the foundation of interactive world exploration. However, existing video generation datasets are not well-suited for world exploration training as they suffer from some limitations: limited locations, short duration, static scenes, and a lack of annotations about exploration and the world. In this paper, we introduce Sekai (meaning ``world'' in Japanese), a high-quality first-person view worldwide video dataset with rich annotations for world exploration. It consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from over 100 countries and regions across 750 cities. We develop an efficient and effective toolbox to collect, pre-process and annotate videos with location, scene, weather, crowd density, captions, and camera trajectories. Experiments demonstrate the quality of the dataset. And, we use a subset to train an interactive video world exploration model, named YUME (meaning ``dream'' in Japanese). We believe Sekai will benefit the area of video generation and world exploration, and motivate valuable applications."
    },
    {
      "title": "SwarmAgentic：迈向通过群体智能实现全自动化智能体系统生成 (原标题: SwarmAgentic: Towards Fully Automated Agentic System Generation via Swarm Intelligence)",
      "link": "https://arxiv.org/abs/2506.15672",
      "pubDate": "Wed, 18 Jun 2025 13:54:55 GMT",
      "isoDate": "2025-06-18T13:54:55.000Z",
      "creator": "Yao Zhang, Chenyang Lin, Shijie Tang, Haokun Chen, Shijie Zhou, Yunpu Ma, Volker Tresp",
      "summary": "# SwarmAgentic：通过群体智能实现全自动化智能体系统生成\n\n## 摘要\n\n大型语言模型的快速发展推动了智能体系统在决策、协调和任务执行方面的进步。然而，现有的智能体系统生成框架缺乏完全的自主性，主要体现在以下几个方面：\n\n*   **缺乏从零开始的智能体生成能力**：无法从头构建智能体。\n*   **缺少智能体功能的自优化**：无法自动优化智能体自身的性能。\n*   **协作能力的不足**：限制了系统的适应性和可扩展性。\n\n## SwarmAgentic 框架\n\n为解决上述挑战，本文提出了 **SwarmAgentic**，一个旨在实现智能体系统全自动化生成的框架。其核心特点包括：\n\n*   **从零开始构建智能体系统**：能够完全自主地生成智能体系统。\n*   **联合优化智能体功能与协作**：将智能体功能和它们之间的协作视为相互依赖的组件进行联合优化。\n*   **语言驱动的探索**：通过语言模型驱动的探索过程实现优化。\n\n## 核心机制：群体智能的启发\n\nSwarmAgentic 从粒子群优化（PSO）中汲取灵感，以实现对系统级结构的有效搜索：\n\n*   **维护候选系统群体**：框架维护一个由多个候选智能体系统组成的群体。\n*   **反馈引导的演化**：通过反馈引导的更新机制，使这些候选系统不断演化，从而高效地探索系统级结构空间。\n\n## 实验评估与成果\n\nSwarmAgentic 在六个真实世界、开放式和探索性任务上进行了评估，这些任务涉及高级规划、系统级协调和创造性推理。\n\n*   **输入条件**：仅提供任务描述和目标函数。\n*   **性能表现**：SwarmAgentic 在所有基线上均表现出色。\n*   **显著提升**：在 TravelPlanner 基准测试中，相对于 ADAS 实现了 +261.8% 的相对改进。\n*   **重要意义**：这突出表明了在结构无约束任务中，全自动化方法的有效性。\n\n## 结论与展望\n\nSwarmAgentic 框架是迈向可扩展和自主智能体系统设计的重要一步，它将群体智能与全自动化多智能体系统生成相结合。\n\n*   **代码发布**：相关代码已公开发布。",
      "shortSummary": "SwarmAgentic 提出一个全自动化框架，利用群体智能解决现有智能体系统生成中缺乏自主性、自优化和协作的问题。该框架从零开始构建并联合优化智能体功能与协作，通过语言驱动的探索和粒子群优化启发机制，在多项复杂任务上显著超越基线，尤其在 TravelPlanner 基准测试中实现大幅提升，标志着可扩展自主智能体系统设计的重要进展。",
      "translated_title": "SwarmAgentic：迈向通过群体智能实现全自动化智能体系统生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "The rapid progress of Large Language Models has advanced agentic systems in decision-making, coordination, and task execution. Yet, existing agentic system generation frameworks lack full autonomy, missing from-scratch agent generation, self-optimizing agent functionality, and collaboration, limiting adaptability and scalability. We propose SwarmAgentic, a framework for fully automated agentic system generation that constructs agentic systems from scratch and jointly optimizes agent functionality and collaboration as interdependent components through language-driven exploration. To enable efficient search over system-level structures, SwarmAgentic maintains a population of candidate systems and evolves them via feedback-guided updates, drawing inspiration from Particle Swarm Optimization (PSO). We evaluate our method on six real-world, open-ended, and exploratory tasks involving high-level planning, system-level coordination, and creative reasoning. Given only a task description and an objective function, SwarmAgentic outperforms all baselines, achieving a +261.8% relative improvement over ADAS on the TravelPlanner benchmark, highlighting the effectiveness of full automation in structurally unconstrained tasks. This framework marks a significant step toward scalable and autonomous agentic system design, bridging swarm intelligence with fully automated system multi-agent generation. Our code is publicly released at https://yaoz720.github.io/SwarmAgentic/."
    },
    {
      "title": "SciVer：评估多模态科学主张验证的基础模型 (原标题: SciVer: Evaluating Foundation Models for Multimodal Scientific Claim Verification)",
      "link": "https://arxiv.org/abs/2506.15569",
      "pubDate": "Wed, 18 Jun 2025 11:43:26 GMT",
      "isoDate": "2025-06-18T11:43:26.000Z",
      "creator": "Chengye Wang, Yifei Shen, Zexi Kuang, Arman Cohan, Yilun Zhao",
      "summary": "## SciVer：多模态科学主张验证基准\n\nSciVer 是首个专门设计用于评估基础模型在多模态科学背景下验证主张能力的基准。\n\n### 基准构成与特点\n\n*   **规模与内容**：SciVer 包含 3,000 个由专家标注的示例，涵盖 1,113 篇科学论文。\n*   **推理类型**：基准涵盖了四种常见的推理类型，每种类型都代表了多模态科学主张验证中的一种特定推理方式。\n*   **细粒度评估**：每个示例都包含专家标注的支持证据，以实现更细致的性能评估。\n\n### 模型评估与发现\n\n*   **评估对象**：研究评估了 21 种最先进的多模态基础模型，包括 o4-mini、Gemini-2.5-Flash、Llama-3.2-Vision 和 Qwen2.5-VL 等。\n*   **性能差距**：实验结果显示，这些模型在 SciVer 上的表现与人类专家之间存在显著的性能差距。\n\n### 深入分析与洞察\n\n*   **分析方法**：通过对检索增强生成（RAG）进行深入分析，并进行人工错误评估，研究识别了当前开源模型的关键局限性。\n*   **未来方向**：这些分析为提升模型在多模态科学文献任务中的理解和推理能力提供了重要见解。",
      "shortSummary": "SciVer是首个用于评估基础模型在多模态科学背景下验证主张能力的基准。它包含3,000个专家标注的示例，涵盖1,113篇科学论文和四种推理类型。研究评估了21种先进的多模态模型，发现它们与人类专家之间存在显著性能差距。通过对检索增强生成（RAG）和人工错误评估的深入分析，研究识别了当前开源模型的关键局限性，为提升模型在科学文献任务中的理解和推理能力提供了重要见解。",
      "translated_title": "SciVer：评估多模态科学主张验证的基础模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce SciVer, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within a multimodal scientific context. SciVer consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing a common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. We assess the performance of 21 state-of-the-art multimodal foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Our experiment reveals a substantial performance gap between these models and human experts on SciVer. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, we identify critical limitations in current open-source models, offering key insights to advance models' comprehension and reasoning in multimodal scientific literature tasks."
    },
    {
      "title": "自由形式生成中开放式R1训练的语义感知奖励 (原标题: Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form Generation)",
      "link": "https://arxiv.org/abs/2506.15068",
      "pubDate": "Tue, 17 Jun 2025 22:16:53 GMT",
      "isoDate": "2025-06-17T22:16:53.000Z",
      "creator": "Zongxia Li, Yapei Chang, Yuhang Zhou, Xiyang Wu, Zichao Liang, Yoo Yeon Sung, Jordan Lee Boyd-Graber",
      "summary": "## 挑战：开放式长文本生成的评估\n\n*   **定义困难：** 难以明确区分“好”与“坏”的输出。\n*   **现有方法不足：** 常常遗漏关键方面，如连贯性、风格或相关性。\n*   **数据偏倚：** 容易受到预训练数据的偏倚影响。\n*   **研究不足：** 开放式长文本评估是一个未被充分探索的问题。\n\n## 解决方案：PrefBERT模型\n\n*   **目的：** 提出PrefBERT，一个评分模型，用于评估开放式长文本生成，并在GRPO（Generative Reinforcement Policy Optimization）中通过为好坏输出提供不同的奖励来指导其训练。\n*   **训练：** PrefBERT在两个响应评估数据集上进行训练，这些数据集包含多样化的长文本风格和Likert评分的质量。\n*   **优势：** 与传统的ROUGE-L和BERTScore指标相比，PrefBERT能够提供更好的语义奖励反馈。\n\n## 评估与结果\n\n*   **评估方法：** 进行了全面的评估，包括“LLM作为评判者”（LLM-as-a-judge）、人工评分和定性分析。\n*   **可靠性：** 尽管PrefBERT在多句和段落长度的响应上进行训练，但在各种长篇文本中仍保持可靠性。\n*   **对齐性：** 它与GRPO所需的、可验证的奖励高度对齐。\n*   **人工评估确认：** 人工评估证实，使用PrefBERT作为奖励信号来训练策略模型，所生成的响应比使用传统指标训练的模型更符合人类偏好。\n\n## 资源与主题\n\n*   **代码可用性：** 相关代码已公开。\n*   **研究主题：** 计算与语言（cs.CL）；机器学习（cs.LG）。",
      "shortSummary": "针对开放式长文本生成评估的挑战，本文提出PrefBERT模型。PrefBERT是一个评分模型，旨在为GRPO训练提供语义感知奖励，以区分好坏输出。该模型在多样化的长文本数据集上训练，并被证明能提供比ROUGE-L和BERTScore更好的语义反馈。通过LLM和人工评估，PrefBERT在指导策略模型生成更符合人类偏好的响应方面表现出色，解决了现有评估方法的不足。",
      "translated_title": "自由形式生成中开放式R1训练的语义感知奖励",
      "images": [],
      "contentSource": "完整文章",
      "content": "Evaluating open-ended long-form generation is challenging because it is hard to define what clearly separates good from bad outputs. Existing methods often miss key aspects like coherence, style, or relevance, or are biased by pretraining data, making open-ended long-form evaluation an underexplored problem. To address this gap, we propose PrefBERT, a scoring model for evaluating open-ended long-form generation in GRPO and guiding its training with distinct rewards for good and bad outputs. Trained on two response evaluation datasets with diverse long-form styles and Likert-rated quality, PrefBERT effectively supports GRPO by offering better semantic reward feedback than traditional metrics ROUGE-L and BERTScore do. Through comprehensive evaluations, including LLM-as-a-judge, human ratings, and qualitative analysis, we show that PrefBERT, trained on multi-sentence and paragraph-length responses, remains reliable across varied long passages and aligns well with the verifiable rewards GRPO needs. Human evaluations confirm that using PrefBERT as the reward signal to train policy models yields responses better aligned with human preferences than those trained with traditional metrics. Our code is available at https://github.com/zli12321/long_form_rl."
    },
    {
      "title": "截断近端策略优化 (原标题: Truncated Proximal Policy Optimization)",
      "link": "https://arxiv.org/abs/2506.15050",
      "pubDate": "Tue, 17 Jun 2025 21:21:38 GMT",
      "isoDate": "2025-06-17T21:21:38.000Z",
      "creator": "Tiantian Fan, Lingjun Liu, Yu Yue, Jiaze Chen, Chengyi Wang, Qiying Yu, Chi Zhang, Zhiqi Lin, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Bole Ma, Mofan Zhang, Gaohong Liu, Ru Zhang, Haotian Zhou, Cong Xie, Ruidong Zhu, Zhi Zhang, Xin Liu, Mingxuan Wang, Lin Yan, Yonghui Wu",
      "summary": "### 截断近端策略优化 (T-PPO)\n\n本文提出了一种名为“截断近端策略优化”（Truncated Proximal Policy Optimization, T-PPO）的新型方法，旨在解决大型语言模型（LLMs）在强化学习训练中，特别是使用近端策略优化（PPO）及其变体时面临的效率问题。\n\n**背景与问题：**\n\n*   **LLMs的推理能力：** 近期，大型语言模型通过生成长链式思考（CoT）在科学和专业任务中展现出卓越的推理能力。\n*   **强化学习的重要性：** 强化学习（RL），以PPO为代表，是开发这些推理模型的关键组成部分，允许模型通过试错学习。\n*   **PPO的效率挑战：**\n    *   PPO固有的“在线策略”（on-policy）特性使其训练过程耗时。\n    *   随着响应长度的增加，这一问题变得更加严重。\n    *   完全同步的长生成过程导致硬件利用率低下，资源在等待完整rollout期间经常处于闲置状态。\n\n**T-PPO的解决方案与贡献：**\n\nT-PPO旨在通过简化策略更新和限制长度的响应生成来提高训练效率，并缓解硬件利用率低下的问题。其主要贡献体现在两个方面：\n\n1.  **扩展广义优势估计（Extended Generalized Advantage Estimation, EGAE）：**\n    *   T-PPO引入了EGAE，用于从不完整的响应中进行优势估计。\n    *   这一方法在进行优势估计的同时，能够保持策略学习的完整性。\n2.  **计算优化的机制：**\n    *   T-PPO设计了一种计算优化的机制，允许策略模型和价值模型独立优化。\n    *   通过选择性地过滤提示词（prompt）和截断的token，该机制减少了冗余计算。\n    *   这显著加速了训练过程，同时不牺牲收敛性能。\n\n**实验结果与效果：**\n\n*   T-PPO在AIME 2024数据集上，使用一个32B的基础模型进行了有效性验证。\n*   实验结果表明，T-PPO将推理LLMs的训练效率提高了高达2.5倍。\n*   此外，T-PPO的表现优于现有的竞争方法。\n\n**总结：**\n\nT-PPO通过引入EGAE和优化的独立模型优化机制，有效解决了PPO在长响应LLM训练中的效率瓶颈和硬件利用率问题，显著提升了训练速度和性能。",
      "shortSummary": "本文提出截断近端策略优化（T-PPO），旨在解决大型语言模型（LLMs）强化学习训练中PPO效率低下、硬件利用率低的问题。T-PPO通过引入扩展广义优势估计（EGAE）处理不完整响应，并设计计算优化机制实现策略与价值模型的独立优化，从而减少冗余计算，加速训练。实验表明，T-PPO将推理LLMs的训练效率提高高达2.5倍，并优于现有方法。",
      "translated_title": "截断近端策略优化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recently, test-time scaling Large Language Models (LLMs) have demonstrated exceptional reasoning capabilities across scientific and professional tasks by generating long chains-of-thought (CoT). As a crucial component for developing these reasoning models, reinforcement learning (RL), exemplified by Proximal Policy Optimization (PPO) and its variants, allows models to learn through trial and error. However, PPO can be time-consuming due to its inherent on-policy nature, which is further exacerbated by increasing response lengths. In this work, we propose Truncated Proximal Policy Optimization (T-PPO), a novel extension to PPO that improves training efficiency by streamlining policy update and length-restricted response generation. T-PPO mitigates the issue of low hardware utilization, an inherent drawback of fully synchronized long-generation procedures, where resources often sit idle during the waiting periods for complete rollouts. Our contributions are two-folds. First, we propose Extended Generalized Advantage Estimation (EGAE) for advantage estimation derived from incomplete responses while maintaining the integrity of policy learning. Second, we devise a computationally optimized mechanism that allows for the independent optimization of the policy and value models. By selectively filtering prompt and truncated tokens, this mechanism reduces redundant computations and accelerates the training process without sacrificing convergence performance. We demonstrate the effectiveness and efficacy of T-PPO on AIME 2024 with a 32B base model. The experimental results show that T-PPO improves the training efficiency of reasoning LLMs by up to 2.5x and outperforms its existing competitors."
    },
    {
      "title": "MoTE：用于内存高效大型多模态模型的三值专家混合 (原标题: MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models)",
      "link": "https://arxiv.org/abs/2506.14435",
      "pubDate": "Tue, 17 Jun 2025 07:53:49 GMT",
      "isoDate": "2025-06-17T07:53:49.000Z",
      "creator": "Hongyu Wang, Jiayu Xu, Ruiping Wang, Yan Feng, Yitao Zhai, Peng Pei, Xunliang Cai, Xilin Chen",
      "summary": "# MoTE：用于内存高效大型多模态模型的三值专家混合\n\n## 摘要\n\n本文提出了一种名为 MoTE（Mixture of Ternary Experts，三值专家混合）的新方法，旨在解决大型多模态混合专家（MoE）模型在部署到边缘设备时面临的内存占用过高问题。\n\n## 背景与问题\n\n*   大型多模态 MoE 模型能够有效扩展模型规模以提升性能，同时保持活跃参数数量不变。\n*   然而，现有方法主要使用全精度专家，导致模型内存占用巨大，这严重阻碍了其在边缘设备上的部署。\n\n## MoTE 方法\n\n*   **核心思想：** MoTE 是一种可扩展且内存高效的方法，用于从密集检查点训练三值专家混合模型。\n*   **创新点：**\n    *   与训练少量高精度专家不同，MoTE 提出在模型上循环（up-cycling）过程中训练**更多低精度专家**。\n    *   具体而言，它使用预训练的 FFN（前馈网络）作为共享专家。\n    *   路由专家（routed experts）的参数被限制在三值集合 **{-1, 0, 1}** 中。\n\n## 实验结果与优势\n\n*   **扩展趋势：** 实验表明，MoTE 随着模型规模的增大展现出良好的扩展趋势。\n*   **性能表现：** MoTE 在最终任务上实现了与全精度基线 MoE-LLaVA 相当的性能。\n*   **内存效率：** MoTE 提供了显著更低的内存占用。\n*   **兼容性：** 该方法与训练后量化（Post-Training Quantization, PTQ）方法兼容。\n*   **优势放大：** 当内存限制更严格时，MoTE 的优势进一步放大。\n*   **具体案例：** 在专家内存占用同为 3.4GB 并结合训练后量化的情况下，MoTE 在最终任务上的平均准确率比 MoE-LLaVA 高出 4.3%。\n\n## 结论\n\nMoTE 证明了其在内存受限设备上的有效性和巨大潜力。",
      "shortSummary": "MoTE 是一种用于内存高效大型多模态模型的创新方法。针对现有 MoE 模型内存占用高的问题，MoTE 提出训练更多低精度的三值专家（参数为 {-1, 0, 1}）。实验表明，MoTE 在保持与全精度模型相当性能的同时，显著降低了内存占用。结合训练后量化，MoTE 在内存受限设备上表现出优越性，例如在相同专家内存占用下，其性能比 MoE-LLaVA 提升 4.3%。这使其成为边缘设备部署的有力解决方案。",
      "translated_title": "MoTE：用于内存高效大型多模态模型的三值专家混合",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices."
    },
    {
      "title": "大型语言和多模态模型中的离散扩散：一项综述 (原标题: Discrete Diffusion in Large Language and Multimodal Models: A Survey)",
      "link": "https://arxiv.org/abs/2506.13759",
      "pubDate": "Mon, 16 Jun 2025 13:59:08 GMT",
      "isoDate": "2025-06-16T13:59:08.000Z",
      "creator": "Runpeng Yu, Qi Li, Xinchao Wang",
      "summary": "## 大型语言和多模态模型中的离散扩散：一项综述\n\n本综述系统地概述了离散扩散语言模型（dLLMs）和离散扩散多模态语言模型（dMLLMs）。\n\n### 离散扩散模型的特点与优势\n\n与自回归（AR）模型不同，dLLMs和dMLLMs采用多令牌、并行解码范式，利用全注意力机制和基于去噪的生成策略。这种范式自然地带来了以下优势：\n\n*   **并行生成能力**：显著提高生成效率。\n*   **细粒度输出可控性**：能够更精确地控制生成内容的细节。\n*   **动态、响应感知能力**：模型能够根据上下文动态调整响应。\n\n这些能力是AR模型此前难以实现的。\n\n### 性能与发展\n\n近期，越来越多的工业级专有d(M)LLMs以及大量的开源学术d(M)LLMs已展现出与自回归模型相当的性能，同时在推理速度上实现了高达10倍的加速。\n\n离散扩散LLMs和MLLMs的进步主要得益于两个领域的进展：\n\n1.  **自回归LLMs和MLLMs的发展**：积累了大量数据、基准和用于训练与推理的基础设施。\n2.  **离散扩散底层数学模型的演进**：为离散扩散模型提供了坚实的理论基础。\n\n这些进步共同推动了2025年初dLLMs和dMLLMs研究的激增。\n\n### 综述内容概览\n\n本综述全面回顾了dLLM和dMLLM领域的研究，具体涵盖：\n\n*   追溯dLLMs和dMLLMs的历史发展。\n*   形式化其底层的数学框架。\n*   对代表性模型进行分类。\n*   分析关键的训练和推理技术。\n*   总结在语言、视觉-语言和生物等领域的新兴应用。\n*   探讨未来的研究方向和部署前景。",
      "shortSummary": "本综述系统地审视了离散扩散语言模型（dLLMs）和多模态语言模型（dMLLMs）。与自回归模型不同，d(M)LLMs采用并行解码和去噪生成策略，实现了并行生成、精细控制和动态感知。它们在性能上与自回归模型相当，但推理速度提升高达10倍。其发展得益于自回归模型的积累和离散扩散数学模型的进步。综述内容涵盖历史发展、数学框架、模型分类、训练推理技术及未来应用方向。",
      "translated_title": "大型语言和多模态模型中的离散扩散：一项综述",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this work, we provide a systematic survey of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs). Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token, parallel decoding paradigm using full attention and a denoising-based generation strategy. This paradigm naturally enables parallel generation, fine-grained output controllability, and dynamic, response-aware perception. These capabilities are previously difficult to achieve with AR models. Recently, a growing number of industrial-scale proprietary d(M)LLMs, as well as a large number of open-source academic d(M)LLMs, have demonstrated performance comparable to their autoregressive counterparts, while achieving up to 10x acceleration in inference speed.   The advancement of discrete diffusion LLMs and MLLMs has been largely driven by progress in two domains. The first is the development of autoregressive LLMs and MLLMs, which has accumulated vast amounts of data, benchmarks, and foundational infrastructure for training and inference. The second contributing domain is the evolution of the mathematical models underlying discrete diffusion. Together, these advancements have catalyzed a surge in dLLMs and dMLLMs research in early 2025.   In this work, we present a comprehensive overview of the research in the dLLM and dMLLM domains. We trace the historical development of dLLMs and dMLLMs, formalize the underlying mathematical frameworks, and categorize representative models. We further analyze key techniques for training and inference, and summarize emerging applications across language, vision-language, and biological domains. We conclude by discussing future directions for research and deployment.   Paper collection: https://github.com/LiQiiiii/DLLM-Survey"
    },
    {
      "title": "通过预算指导引导LLM思维 (原标题: Steering LLM Thinking with Budget Guidance)",
      "link": "https://arxiv.org/abs/2506.13752",
      "pubDate": "Mon, 16 Jun 2025 13:57:05 GMT",
      "isoDate": "2025-06-16T13:57:05.000Z",
      "creator": "Junyan Li, Wenshuo Zhao, Yang Zhang, Chuang Gan",
      "summary": "# 预算指导：引导LLM思维过程\n\n## 引言与背景\n近期的大型语言模型（LLM）为了提升性能，常常进行广泛的“深度思考”或推理。然而，这种冗长的推理过程并非总是理想的，因为它会带来过高的推理成本，而性能提升却不成比例。因此，在不牺牲性能的前提下控制推理长度变得至关重要，尤其是在思考预算紧张的情况下，这仍然是一个挑战。\n\n## 核心方法：预算指导（Budget Guidance）\n本文提出了一种名为“预算指导”（Budget Guidance）的简单而有效的方法，旨在引导LLM的推理过程，使其符合预设的思考预算，而无需对LLM进行任何微调。\n\n### 工作原理\n*   **轻量级预测器：** 引入一个轻量级的预测器。\n*   **伽马分布建模：** 该预测器在生成下一个令牌（token）时，对剩余的思考长度建模一个伽马分布。\n*   **软性令牌级引导：** 这一信号被用于以软性、令牌级别的方式引导生成过程，确保整体的推理轨迹遵循指定的思考预算。\n\n## 主要成果与优势\n*   **自然控制思维长度：** 预算指导能够自然地控制LLM的思维长度。\n*   **显著提升令牌效率：** 在具有挑战性的数学基准测试中，与基线方法相比，它显著提高了令牌效率。\n*   **性能表现：**\n    *   在MATH-500基准测试中，在预算紧张的情况下，与基线方法相比，准确率提升高达26%。\n    *   在使用完整思考模型63%的思考令牌量下，仍能保持具有竞争力的准确率。\n*   **泛化能力与涌现能力：** 预算指导还能够泛化到更广泛的任务领域，并展现出一些涌现能力，例如估计问题难度。\n\n## 资源可用性\n该方法的源代码可在提供的链接中获取。",
      "shortSummary": "本文提出“预算指导”方法，旨在解决大型语言模型（LLM）深度思考成本高昂且效率不高的问题。该方法通过引入一个轻量级预测器，在不微调LLM的情况下，以令牌级方式引导推理过程，使其符合预设的思考预算。实验表明，预算指导在数学基准测试中显著提升了令牌效率和准确率，例如在MATH-500上准确率提升高达26%，并能以更少令牌保持竞争力，同时具备泛化能力和估计问题难度等涌现能力。",
      "translated_title": "通过预算指导引导LLM思维",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent deep-thinking large language models often reason extensively to improve performance, but such lengthy reasoning is not always desirable, as it incurs excessive inference costs with disproportionate performance gains. Controlling reasoning length without sacrificing performance is therefore important, but remains challenging, especially under tight thinking budgets. We propose budget guidance, a simple yet effective method for steering the reasoning process of LLMs toward a target budget without requiring any LLM fine-tuning. Our approach introduces a lightweight predictor that models a Gamma distribution over the remaining thinking length during next-token generation. This signal is then used to guide generation in a soft, token-level manner, ensuring that the overall reasoning trace adheres to the specified thinking budget. Budget guidance enables natural control of the thinking length, along with significant token efficiency improvements over baseline methods on challenging math benchmarks. For instance, it achieves up to a 26% accuracy gain on the MATH-500 benchmark under tight budgets compared to baseline methods, while maintaining competitive accuracy with only 63% of the thinking tokens used by the full-thinking model. Budget guidance also generalizes to broader task domains and exhibits emergent capabilities, such as estimating question difficulty. The source code is available at: https://github.com/UMass-Embodied-AGI/BudgetGuidance."
    },
    {
      "title": "Test3R: 在测试时学习重建3D (原标题: Test3R: Learning to Reconstruct 3D at Test Time)",
      "link": "https://arxiv.org/abs/2506.13750",
      "pubDate": "Mon, 16 Jun 2025 13:56:22 GMT",
      "isoDate": "2025-06-16T13:56:22.000Z",
      "creator": "Yuheng Yuan, Qiuhong Shen, Shizun Wang, Xingyi Yang, Xinchao Wang",
      "summary": "# Test3R: 在测试时学习重建3D\n\n## 概述\n\n本文介绍了一种名为 Test3R 的测试时学习技术，旨在显著提升3D重建的几何精度。传统的密集匹配方法（如 DUSt3R）通过回归成对点图进行3D重建，但其对成对预测的依赖以及有限的泛化能力，固有地限制了全局几何一致性。Test3R 通过一种出人意料的简单方法解决了这一问题。\n\n## Test3R 的核心思想与工作原理\n\nTest3R 的核心思想是在测试时通过一个自监督目标来优化网络，以最大化不同重建之间的几何一致性。具体实现如下：\n\n*   **输入数据**：Test3R 使用图像三元组（$I_1, I_2, I_3$）作为输入。\n*   **重建生成**：从图像对（$I_1, I_2$）和（$I_1, I_3$）生成两次独立的3D重建。\n*   **自监督优化**：在测试时，模型通过一个自监督目标进行优化。这个目标是最大化这两次重建相对于共同图像 $I_1$ 的几何一致性。\n*   **目标**：这种优化确保了模型能够生成跨图像对一致的输出，无论输入是什么。\n\n## 性能与优势\n\n广泛的实验证明，Test3R 技术在3D重建和多视角深度估计任务上显著优于现有的最先进方法。其主要优势包括：\n\n*   **显著提升精度**：在3D重建和多视角深度估计任务中，Test3R 表现出卓越的性能。\n*   **普适性**：该技术具有普遍适用性，可以轻松应用于其他模型。\n*   **成本效益**：Test3R 几乎是免费的，因为它只需要极少的测试时训练开销和参数占用。\n\n## 代码可用性\n\n相关代码已在 [this https URL](https://this.https.url) 提供。",
      "shortSummary": "Test3R 是一种创新的测试时学习技术，旨在提升3D重建的几何精度。它通过使用图像三元组，在测试时利用自监督目标优化网络，以最大化不同重建相对于共同图像的几何一致性。该方法显著优于现有技术，在3D重建和多视角深度估计任务上表现出色。Test3R 具有普适性，且成本极低，易于应用于其他模型。",
      "translated_title": "Test3R: 在测试时学习重建3D",
      "images": [],
      "contentSource": "完整文章",
      "content": "Dense matching methods like DUSt3R regress pairwise pointmaps for 3D reconstruction. However, the reliance on pairwise prediction and the limited generalization capability inherently restrict the global geometric consistency. In this work, we introduce Test3R, a surprisingly simple test-time learning technique that significantly boosts geometric accuracy. Using image triplets (I_1,I_2,I_3), Test3R generates reconstructions from pairs (I_1,I_2) and (I_1,I_3). The core idea is to optimize the network at test time via a self-supervised objective: maximizing the geometric consistency between these two reconstructions relative to the common image I_1. This ensures the model produces cross-pair consistent outputs, regardless of the inputs. Extensive experiments demonstrate that our technique significantly outperforms previous state-of-the-art methods on the 3D reconstruction and multi-view depth estimation tasks. Moreover, it is universally applicable and nearly cost-free, making it easily applied to other models and implemented with minimal test-time training overhead and parameter footprint. Code is available at https://github.com/nopQAQ/Test3R."
    },
    {
      "title": "Ego-R1：用于超长第一人称视频推理的工具链思维 (原标题: Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning)",
      "link": "https://arxiv.org/abs/2506.13654",
      "pubDate": "Mon, 16 Jun 2025 12:17:08 GMT",
      "isoDate": "2025-06-16T12:17:08.000Z",
      "creator": "Shulin Tian, Ruiqi Wang, Hongming Guo, Penghao Wu, Yuhao Dong, Xiuying Wang, Jingkang Yang, Hao Zhang, Hongyuan Zhu, Ziwei Liu",
      "summary": "### Ego-R1：用于超长第一人称视频推理的工具链思维框架\n\n本文介绍了 **Ego-R1**，一个用于对超长（即数天或数周）第一人称视频进行推理的新型框架。该框架利用了由强化学习（RL）训练的 **Ego-R1 Agent** 所协调的结构化 **工具链思维（Chain-of-Tool-Thought, CoTT）** 过程。\n\n**核心理念与方法：**\n\n*   **灵感来源：** CoTT 过程受到人类解决问题策略的启发。\n*   **分解复杂推理：** CoTT 将复杂的推理任务分解为模块化的步骤。\n*   **工具调用：** Ego-R1 Agent 在每个步骤中调用特定的工具，以迭代和协作地回答子问题，从而处理诸如时间检索和多模态理解等任务。\n\n**训练范式：**\n\n*   **两阶段训练：** Ego-R1 采用两阶段训练范式，旨在使 Agent 能够动态地提出逐步的工具来支持长距离推理。\n    1.  **监督微调（SFT）：** 使用 CoTT 数据对预训练语言模型进行监督微调。\n    2.  **强化学习（RL）：** 进一步通过强化学习训练 Agent。\n\n**数据集：**\n\n*   为了促进训练，研究团队构建了名为 **Ego-R1 Data** 的数据集，其中包括：\n    *   **Ego-CoTT-25K：** 用于监督微调（SFT）。\n    *   **Ego-QA-4.4K：** 用于强化学习（RL）。\n\n**评估基准：**\n\n*   Ego-R1 Agent 在新策划的 **Ego-R1 Bench** 上进行评估。这是一个为期一周的视频问答基准，包含来自混合来源的人工验证问答对。\n\n**实验结果：**\n\n*   广泛的实验结果表明，Ego-R1 Agent 动态的、工具增强的思维链推理方法能够有效应对理解超长第一人称视频的独特挑战。\n*   该方法显著扩展了时间覆盖范围，从数小时延长至一周。",
      "shortSummary": "Ego-R1 是一个用于超长第一人称视频推理的新框架，它引入了由强化学习训练的 Ego-R1 Agent 协调的工具链思维（CoTT）过程。CoTT 将复杂推理分解为模块化步骤，Agent 动态调用工具解决子问题。该框架采用两阶段训练（SFT和RL），并构建了Ego-R1 Data数据集（Ego-CoTT-25K和Ego-QA-4.4K）。在Ego-R1 Bench上的评估显示，Ego-R1能有效处理超长视频，将时间覆盖从几小时扩展到一周。",
      "translated_title": "Ego-R1：用于超长第一人称视频推理的工具链思维",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e., in days and weeks) egocentric videos, which leverages a structured Chain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained via reinforcement learning (RL). Inspired by human problem-solving strategies, CoTT decomposes complex reasoning into modular steps, with the RL agent invoking specific tools, one per step, to iteratively and collaboratively answer sub-questions tackling such tasks as temporal retrieval and multi-modal understanding. We design a two-stage training paradigm involving supervised finetuning (SFT) of a pretrained language model using CoTT data and RL to enable our agent to dynamically propose step-by-step tools for long-range reasoning. To facilitate training, we construct a dataset called Ego-R1 Data, which consists of Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL. Furthermore, our Ego-R1 agent is evaluated on a newly curated week-long video QA benchmark, Ego-R1 Bench, which contains human-verified QA pairs from hybrid sources. Extensive results demonstrate that the dynamic, tool-augmented chain-of-thought reasoning by our Ego-R1 Agent can effectively tackle the unique challenges of understanding ultra-long egocentric videos, significantly extending the time coverage from few hours to a week."
    },
    {
      "title": "MiniMax-M1：利用闪电注意力机制高效扩展测试时计算 (原标题: MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention)",
      "link": "https://arxiv.org/abs/2506.13585",
      "pubDate": "Mon, 16 Jun 2025 11:08:02 GMT",
      "isoDate": "2025-06-16T11:08:02.000Z",
      "creator": "MiniMax, Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, Chengjun Xiao, Chengyu Du, Chi Zhang, Chu Qiao, Chunhao Zhang, Chunhui Du, Congchao Guo, Da Chen, Deming Ding, Dianjun Sun, Dong Li, Enwei Jiao, Haigang Zhou, Haimo Zhang, Han Ding, Haohai Sun, Haoyu Feng, Huaiguang Cai, Haichao Zhu, Jian Sun, Jiaqi Zhuang, Jiaren Cai, Jiayuan Song, Jin Zhu, Jingyang Li, Jinhao Tian, Jinli Liu, Junhao Xu, Junjie Yan, Junteng Liu, Junxian He, Kaiyi Feng, Ke Yang, Kecheng Xiao, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Li, Lin Zheng, Linge Du, Lingyu Yang, Lunbin Zeng, Minghui Yu, Mingliang Tao, Mingyuan Chi, Mozhi Zhang, Mujie Lin, Nan Hu, Nongyu Di, Peng Gao, Pengfei Li, Pengyu Zhao, Qibing Ren, Qidi Xu, Qile Li, Qin Wang, Rong Tian, Ruitao Leng, Shaoxiang Chen, Shaoyu Chen, Shengmin Shi, Shitong Weng, Shuchang Guan, Shuqi Yu, Sichen Li, Songquan Zhu, Tengfei Li, Tianchi Cai, Tianrun Liang, Weiyu Cheng, Weize Kong, Wenkai Li, Xiancai Chen, Xiangjun Song, Xiao Luo, Xiao Su, Xiaobo Li, Xiaodong Han, Xinzhu Hou, Xuan Lu, Xun Zou, Xuyang Shen, Yan Gong, Yan Ma, Yang Wang, Yiqi Shi, Yiran Zhong, Yonghong Duan, Yongxiang Fu, Yongyi Hu, Yu Gao, Yuanxiang Fan, Yufeng Yang, Yuhao Li, Yulin Hu, Yunan Huang, Yunji Li, Yunzhi Xu, Yuxin Mao, Yuxuan Shi, Yuze Wenren, Zehan Li, Zelin Li, Zhanxu Tian, Zhengmao Zhu, Zhenhua Fan, Zhenzhen Wu, Zhichao Xu, Zhihang Yu, Zhiheng Lyu, Zhuo Jiang, Zibo Gao, Zijia Wu, Zijian Song, Zijun Sun",
      "summary": "## MiniMax-M1：高效扩展测试时计算的新范式\n\nMiniMax团队推出了MiniMax-M1，这是全球首个开源、大规模混合注意力推理模型，旨在高效扩展测试时计算能力。\n\n### 模型架构与核心特性\n\n*   **混合MoE架构与闪电注意力**：MiniMax-M1结合了混合专家混合（MoE）架构和创新的闪电注意力机制。这一组合使其能够高效处理复杂任务。\n*   **基于MiniMax-Text-01**：该模型基于MiniMax团队先前的MiniMax-Text-01模型开发，该模型总参数量达4560亿，每个token激活459亿参数。\n*   **超长上下文支持**：MiniMax-M1原生支持100万token的上下文长度，是DeepSeek R1上下文大小的8倍，使其特别适用于需要处理长输入和进行深度思考的复杂任务。\n*   **高效测试时计算**：闪电注意力机制是M1的关键创新，它使得测试时计算能够高效扩展。\n\n### 训练方法与效率\n\n*   **大规模强化学习（RL）**：MiniMax-M1通过大规模强化学习进行训练，涵盖了沙盒环境和真实世界软件工程环境等多样化问题。\n*   **新型RL算法CISPO**：为了进一步提升RL训练效率，MiniMax团队提出了CISPO（Clipped Importance Sampling Policy Optimization）算法。CISPO通过裁剪重要性采样权重而非token更新，在性能上超越了其他竞争性RL变体。\n*   **显著的训练成本效益**：结合混合注意力机制和CISPO算法，MiniMax-M1在512块H800 GPU上仅用三周时间就完成了完整的RL训练，租用成本仅为534,700美元。\n\n### 模型版本与性能表现\n\n*   **发布版本**：MiniMax-M1发布了两个版本，分别具有40K和80K的“思考预算”（thinking budgets），其中40K版本代表了80K训练的中间阶段。\n*   **基准测试表现**：在标准基准测试中，MiniMax-M1模型与DeepSeek-R1和Qwen3-235B等强大的开源模型相比，表现出相当或更优的性能，尤其在复杂的软件工程、工具利用和长上下文任务方面具有显著优势。\n\n### 可用性\n\nMiniMax-M1已作为开源项目公开发布。",
      "shortSummary": "MiniMax团队推出了MiniMax-M1，这是首个开源、大规模混合注意力推理模型。它结合了混合MoE架构和创新的闪电注意力机制，原生支持100万token的上下文长度，并能高效扩展测试时计算。通过大规模强化学习和新型CISPO算法，模型训练成本效益显著。MiniMax-M1在软件工程、工具利用和长上下文任务上表现出色，已公开发布。",
      "translated_title": "MiniMax-M1：利用闪电注意力机制高效扩展测试时计算",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1."
    },
    {
      "title": "基于图像的剩余寿命不确定性感知预测 (原标题: Uncertainty-Aware Remaining Lifespan Prediction from Images)",
      "link": "https://arxiv.org/abs/2506.13430",
      "pubDate": "Mon, 16 Jun 2025 08:47:37 GMT",
      "isoDate": "2025-06-16T08:47:37.000Z",
      "creator": "Tristan Kenneweg, Philip Kenneweg, Barbara Hammer",
      "summary": "## 基于图像的剩余寿命不确定性感知预测\n\n### 引言\n从图像预测与死亡率相关的结果，为可及、无创且可扩展的健康筛查提供了前景。\n\n### 方法概述\n*   本文提出了一种新方法，利用预训练的视觉Transformer基础模型来估计个体的剩余寿命。\n*   该方法能够从面部和全身图像中提取信息进行预测。\n*   核心创新在于提供了鲁棒的不确定性量化，即模型不仅给出预测值，还能评估其预测的置信度。\n*   研究发现，预测的不确定性与真实的剩余寿命之间存在系统性变化。\n*   为了有效建模这种不确定性，该方法为每个样本学习一个高斯分布，从而量化预测的置信区间。\n\n### 性能表现\n*   在现有且已建立的数据集上，该方法实现了7.48年的平均绝对误差（MAE），达到了当前最先进的水平。\n*   研究团队还整理并发布了两个新的、更高质量的数据集，在这些新数据集上，模型的MAE进一步显著提高，分别达到4.79年和5.07年。\n\n### 不确定性校准\n*   模型提供的剩余寿命不确定性估计是经过良好校准的。\n*   通过分桶预期校准误差（bucketed expected calibration error）衡量，其值为0.62年，表明模型的不确定性估计与实际误差吻合度高。\n\n### 意义与未来方向\n*   尽管该方法目前不打算用于临床部署，但其研究结果突出显示了从日常图像中提取具有医学相关性信号的巨大潜力。\n*   为了促进后续研究，所有相关的代码和数据集均已公开。",
      "shortSummary": "该研究提出一种利用预训练视觉Transformer模型，从面部和全身图像预测剩余寿命的方法，并提供鲁棒的不确定性量化。该方法在现有数据集上实现了7.48年的最先进平均绝对误差，并在新数据集上进一步提升至4.79年和5.07年。模型提供良好校准的不确定性估计。尽管不用于临床，但结果表明从图像中提取医学信号的巨大潜力。所有代码和数据集均已公开。",
      "translated_title": "基于图像的剩余寿命不确定性感知预测",
      "images": [],
      "contentSource": "完整文章",
      "content": "Predicting mortality-related outcomes from images offers the prospect of accessible, noninvasive, and scalable health screening. We present a method that leverages pretrained vision transformer foundation models to estimate remaining lifespan from facial and whole-body images, alongside robust uncertainty quantification. We show that predictive uncertainty varies systematically with the true remaining lifespan, and that this uncertainty can be effectively modeled by learning a Gaussian distribution for each sample. Our approach achieves state-of-the-art mean absolute error (MAE) of 7.48 years on an established Dataset, and further improves to 4.79 and 5.07 years MAE on two new, higher-quality datasets curated and published in this work. Importantly, our models provide well-calibrated uncertainty estimates, as demonstrated by a bucketed expected calibration error of 0.62 years. While not intended for clinical deployment, these results highlight the potential of extracting medically relevant signals from images. We make all code and datasets available to facilitate further research."
    },
    {
      "title": "AI辅助的摘要和结论分析：标记无根据的主张和模糊的代词 (原标题: Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging Unsubstantiated Claims and Ambiguous Pronouns)",
      "link": "https://arxiv.org/abs/2506.13172",
      "pubDate": "Mon, 16 Jun 2025 03:34:31 GMT",
      "isoDate": "2025-06-16T03:34:31.000Z",
      "creator": "Evgeny Markhasin",
      "summary": "### AI辅助的摘要和结论分析：标记无根据的主张和模糊的代词\n\n本研究提出并评估了一套概念验证（PoC）的结构化工作流提示，旨在引导大型语言模型（LLMs）对学术手稿进行高级语义和语言分析，同时激发类似人类的层次推理能力。这些提示主要针对两个复杂的分析任务：\n\n*   **识别无根据的主张（信息完整性）**：在摘要中找出缺乏支持的陈述。\n*   **标记模糊的代词指代（语言清晰度）**：识别指代不明确的代词。\n\n#### 研究方法\n\n研究对两种前沿模型（Gemini Pro 2.5 Pro 和 ChatGPT Plus o3）在不同上下文条件下进行了系统、多次运行的评估。\n\n#### 关键发现\n\n**1. 信息完整性任务（识别无根据的主张）**\n\n*   **模型性能差异显著**：\n    *   两种模型都能成功识别无根据的名词短语头部（95%的成功率）。\n    *   然而，ChatGPT 始终未能识别无根据的形容词修饰语（0%的成功率），而 Gemini 则正确标记了（95%的成功率）。\n*   **句法角色影响**：这一结果引发了关于目标句法角色可能对模型性能产生影响的问题。\n\n**2. 语言分析任务（标记模糊代词）**\n\n*   **完整手稿上下文**：在提供完整手稿上下文的情况下，两种模型都表现良好（80-90%的成功率）。\n*   **仅摘要上下文**：\n    *   ChatGPT 在仅摘要的设置下达到了完美的100%成功率。\n    *   而 Gemini 的性能则大幅下降。\n\n#### 结论与启示\n\n*   **结构化提示的可行性**：研究结果表明，结构化提示是进行复杂文本分析的一种可行方法。\n*   **性能依赖性**：提示的性能可能高度依赖于模型、任务类型和上下文之间的相互作用。\n*   **测试必要性**：这突出强调了进行严格、针对特定模型的测试的必要性，以优化LLM在复杂分析任务中的应用。",
      "shortSummary": "本研究评估了AI（Gemini Pro 2.5 Pro, ChatGPT Plus o3）在识别学术摘要中无根据主张和模糊代词方面的能力。结果显示，AI在不同任务和上下文条件下表现各异：ChatGPT在识别形容词修饰语方面表现不佳，而Gemini在仅摘要的代词任务中性能下降。研究表明结构化提示可行，但模型、任务和上下文的交互作用对性能影响显著，强调了进行严格、针对特定模型的测试的重要性。",
      "translated_title": "AI辅助的摘要和结论分析：标记无根据的主张和模糊的代词",
      "images": [],
      "contentSource": "RSS",
      "content": "We present and evaluate a suite of proof-of-concept (PoC), structured workflow prompts designed to elicit human-like hierarchical reasoning while guiding Large Language Models (LLMs) in high-level semantic and linguistic analysis of scholarly manuscripts. The prompts target two non-trivial analytical tasks: identifying unsubstantiated claims in summaries (informational integrity) and flagging ambiguous pronoun references (linguistic clarity). We conducted a systematic, multi-run evaluation on two frontier models (Gemini Pro 2.5 Pro and ChatGPT Plus o3) under varied context conditions. Our results for the informational integrity task reveal a significant divergence in model performance: while both models successfully identified an unsubstantiated head of a noun phrase (95% success), ChatGPT consistently failed (0% success) to identify an unsubstantiated adjectival modifier that Gemini correctly flagged (95% success), raising a question regarding potential influence of the target's syntactic role. For the linguistic analysis task, both models performed well (80-90% success) with full manuscript context. In a summary-only setting, however, ChatGPT achieved a perfect (100%) success rate, while Gemini's performance was substantially degraded. Our findings suggest that structured prompting is a viable methodology for complex textual analysis but show that prompt performance may be highly dependent on the interplay between the model, task type, and context, highlighting the need for rigorous, model-specific testing."
    },
    {
      "title": "通过基于分块的提示和分解利用大型语言模型进行时间序列预测 (原标题: Forecasting Time Series with LLMs via Patch-Based Prompting and Decomposition)",
      "link": "https://arxiv.org/abs/2506.12953",
      "pubDate": "Sun, 15 Jun 2025 15:42:58 GMT",
      "isoDate": "2025-06-15T15:42:58.000Z",
      "creator": "Mayank Bumb, Anshul Vemulapalli, Sri Harsha Vardhan Prasad Jella, Anish Gupta, An La, Ryan A. Rossi, Hongjie Chen, Franck Dernoncourt, Nesreen K. Ahmed, Yu Wang",
      "summary": "## 通过基于分块的提示和分解利用大型语言模型进行时间序列预测\n\n### 背景与挑战\n\n*   **大型语言模型（LLMs）在时间序列分析中的潜力：** 近期LLMs的进展为准确高效的时间序列分析带来了新的可能性。\n*   **现有方法的局限性：** 先前的工作通常需要大量的微调，并且/或者忽略了序列间（inter-series）的关联性。\n\n### 研究目标\n\n*   **探索简单灵活的提示策略：** 本研究旨在探索能够使LLMs执行时间序列预测的简单灵活的基于提示的策略，而无需进行大量的再训练或使用复杂的外部架构。\n\n### 核心方法与技术\n\n*   **方法名称：** 提出了一种名为 **PatchInstruct** 的方法。\n*   **关键技术：** 通过探索专门的提示方法，该方法利用了以下技术：\n    *   **时间序列分解（Time series decomposition）：** 将时间序列分解为不同的组成部分（如趋势、季节性、残差）。\n    *   **基于分块的标记化（Patch-based tokenization）：** 将时间序列数据分割成小的“分块”进行处理和标记。\n    *   **基于相似度的邻居增强（Similarity-based neighbor augmentation）：** 通过引入与当前序列相似的邻居序列来增强模型的能力。\n\n### 优势与成果\n\n*   **提升预测质量：** 发现通过上述方法可以增强LLM的时间序列预测质量。\n*   **保持简洁性：** 在提升质量的同时，保持了方法的简洁性。\n*   **最小化数据预处理：** 该方法仅需要最少的数据预处理。\n*   **实现精确有效预测：** PatchInstruct 方法使LLMs能够进行精确有效的预测。\n\n### 相关领域\n\n*   机器学习 (cs.LG)\n*   人工智能 (cs.AI)\n*   计算与语言 (cs.CL)",
      "shortSummary": "该研究提出PatchInstruct方法，旨在通过基于分块的提示和分解，使大型语言模型（LLMs）无需大量微调或复杂架构即可进行时间序列预测。它利用时间序列分解、基于分块的标记化和基于相似度的邻居增强技术，显著提升了LLMs的预测质量，同时保持了方法的简洁性并最大程度地减少了数据预处理需求，实现了精确有效的预测。",
      "translated_title": "通过基于分块的提示和分解利用大型语言模型进行时间序列预测",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in Large Language Models (LLMs) have demonstrated new possibilities for accurate and efficient time series analysis, but prior work often required heavy fine-tuning and/or ignored inter-series correlations. In this work, we explore simple and flexible prompt-based strategies that enable LLMs to perform time series forecasting without extensive retraining or the use of a complex external architecture. Through the exploration of specialized prompting methods that leverage time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation, we find that it is possible to enhance LLM forecasting quality while maintaining simplicity and requiring minimal preprocessing of data. To this end, we propose our own method, PatchInstruct, which enables LLMs to make precise and effective predictions."
    },
    {
      "title": "PersonaFeedback：一个用于个性化的大规模人工标注基准 (原标题: PersonaFeedback: A Large-scale Human-annotated Benchmark For Personalization)",
      "link": "https://arxiv.org/abs/2506.12915",
      "pubDate": "Sun, 15 Jun 2025 13:19:19 GMT",
      "isoDate": "2025-06-15T13:19:19.000Z",
      "creator": "Meiling Tao, Chenghao Zhu, Dongyi Ding, Tiannan Wang, Yuchen Eleanor Jiang, Wangchunshu Zhou",
      "summary": "### PersonaFeedback：LLM个性化评估的新基准\n\n**1. 背景与问题挑战**\n*   随着大型语言模型（LLMs）能力的飞速提升，构建能够生成针对不同用户画像（persona）的个性化响应或服务的LLM系统，即“LLM个性化”，已成为日益重要的研究和工程问题。\n*   然而，与评估LLM通用能力和推理能力的新基准不断涌现不同，当前缺乏高质量的基准来评估LLM的个性化能力，这严重阻碍了该领域的发展。\n*   现有的一些基准通常要求模型从历史交互中推断隐式用户画像，这使得评估焦点不够明确。\n\n**2. PersonaFeedback基准的引入**\n*   为解决上述问题，研究人员引入了**PersonaFeedback**，这是一个旨在直接评估LLM在给定预定义用户画像和查询时提供个性化响应能力的全新基准。\n*   **核心特点：**\n    *   **解耦评估：** PersonaFeedback将“用户画像推断”与“个性化生成”这两个任务解耦，专注于评估模型根据**显式**用户画像生成定制化响应的能力。\n    *   **大规模人工标注：** 该基准包含8298个人工标注的测试用例。\n    *   **分层难度：** 测试用例根据用户画像的上下文复杂性以及区分两个个性化响应之间细微差别的难度，被划分为“简单（easy）”、“中等（medium）”和“困难（hard）”三个层级。\n\n**3. 综合评估结果与发现**\n*   研究团队对广泛的模型进行了全面的评估。\n*   **主要发现：**\n    *   即使是能够解决复杂现实世界推理任务的最新LLM，在PersonaFeedback的“困难”层级上表现也可能不佳，甚至人类评估者也可能发现区分这些响应具有挑战性。\n    *   对各种系统故障模式的深入分析表明，当前流行的检索增强框架（Retrieval-Augmented Framework）不应被视为个性化任务的默认解决方案。\n\n**4. 数据可用性与未来展望**\n*   所有基准数据、标注协议和评估流程都将公开发布，以促进未来LLM个性化领域的研究。\n*   该工作目前处于进行中（Work in progress）。",
      "shortSummary": "PersonaFeedback是一个新颖的大规模人工标注基准，旨在直接评估大型语言模型（LLM）根据显式用户画像生成个性化响应的能力。它包含8298个分层难度的测试用例，将用户画像推断与个性化生成解耦。评估结果显示，即使是最先进的LLM在“困难”层级上仍面临挑战，且现有检索增强框架并非个性化任务的默认解决方案。该基准数据将公开发布，以推动LLM个性化研究。",
      "translated_title": "PersonaFeedback：一个用于个性化的大规模人工标注基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "With the rapid improvement in the general capabilities of LLMs, LLM personalization, i.e., how to build LLM systems that can generate personalized responses or services that are tailored to distinct user personas, has become an increasingly important research and engineering problem. However, unlike many new challenging benchmarks being released for evaluating the general/reasoning capabilities, the lack of high-quality benchmarks for evaluating LLM personalization greatly hinders progress in this field. To address this, we introduce PersonaFeedback, a new benchmark that directly evaluates LLMs' ability to provide personalized responses given pre-defined user personas and queries. Unlike existing benchmarks that require models to infer implicit user personas from historical interactions, PersonaFeedback decouples persona inference from personalization, focusing on evaluating the model's ability to generate responses tailored to explicit personas. PersonaFeedback consists of 8298 human-annotated test cases, which are categorized into easy, medium, and hard tiers based on the contextual complexity of the user personas and the difficulty in distinguishing subtle differences between two personalized responses. We conduct comprehensive evaluations across a wide range of models. The empirical results reveal that even state-of-the-art LLMs that can solve complex real-world reasoning tasks could fall short on the hard tier of PersonaFeedback where even human evaluators may find the distinctions challenging. Furthermore, we conduct an in-depth analysis of failure modes across various types of systems, demonstrating that the current retrieval-augmented framework should not be seen as a de facto solution for personalization tasks. All benchmark data, annotation protocols, and the evaluation pipeline will be publicly available to facilitate future research on LLM personalization."
    },
    {
      "title": "MS4UI：一个用于用户界面教学视频多模态摘要的数据集 (原标题: MS4UI: A Dataset for Multi-modal Summarization of User Interface Instructional Videos)",
      "link": "https://arxiv.org/abs/2506.12623",
      "pubDate": "Sat, 14 Jun 2025 16:39:32 GMT",
      "isoDate": "2025-06-14T16:39:32.000Z",
      "creator": "Yuan Zang, Hao Tan, Seunghyun Yoon, Franck Dernoncourt, Jiuxiang Gu, Kushal Kafle, Chen Sun, Trung Bui",
      "summary": "### MS4UI：用户界面教学视频多模态摘要数据集\n\n**研究背景与目标**\n\n本研究致力于教学视频的多模态摘要，旨在为用户提供一种高效学习技能的方式，即通过文本指令和关键视频帧的形式。研究人员观察到，现有基准主要关注通用语义级视频摘要，但对于教学视频中至关重要的分步可执行指令和插图，这些基准并不适用。\n\n**MS4UI 数据集提案**\n\n为了填补这一空白，研究人员提出了一个针对用户界面（UI）教学视频摘要的新型基准——MS4UI 数据集。\n\n**数据集详情**\n\n*   **规模与时长**：MS4UI 数据集共收集了2,413个UI教学视频，总时长超过167小时。\n*   **人工标注**：这些视频经过了细致的人工标注，涵盖了以下方面：\n    *   **视频分割**：将视频内容划分为逻辑步骤或片段。\n    *   **文本摘要**：为视频内容提供简洁的文本指令。\n    *   **视频摘要**：识别并提取关键视频帧作为视觉说明。\n*   **评估目的**：这些全面的标注使得对简洁且可执行的视频摘要方法进行评估成为可能。\n\n**实验结果与发现**\n\n研究人员在MS4UI数据集上进行了广泛的实验。结果表明，当前最先进的多模态摘要方法在UI视频摘要任务上表现不佳。这一发现突显了开发专门针对UI教学视频摘要的新方法的重要性。",
      "shortSummary": "本研究提出了MS4UI数据集，旨在解决现有基准不适用于用户界面（UI）教学视频多模态摘要的问题。该数据集包含2,413个UI教学视频，总时长超过167小时，并进行了人工标注以支持视频分割、文本和视频摘要。实验表明，当前最先进的多模态摘要方法在UI视频摘要方面表现不佳，强调了开发新方法的重要性。",
      "translated_title": "MS4UI：一个用于用户界面教学视频多模态摘要的数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "We study multi-modal summarization for instructional videos, whose goal is to provide users an efficient way to learn skills in the form of text instructions and key video frames. We observe that existing benchmarks focus on generic semantic-level video summarization, and are not suitable for providing step-by-step executable instructions and illustrations, both of which are crucial for instructional videos. We propose a novel benchmark for user interface (UI) instructional video summarization to fill the gap. We collect a dataset of 2,413 UI instructional videos, which spans over 167 hours. These videos are manually annotated for video segmentation, text summarization, and video summarization, which enable the comprehensive evaluations for concise and executable video summarization. We conduct extensive experiments on our collected MS4UI dataset, which suggest that state-of-the-art multi-modal summarization methods struggle on UI video summarization, and highlight the importance of new methods for UI instructional video summarization."
    },
    {
      "title": "多语言大型语言模型中的语言手术 (原标题: Language Surgery in Multilingual Large Language Models)",
      "link": "https://arxiv.org/abs/2506.12450",
      "pubDate": "Sat, 14 Jun 2025 07:09:50 GMT",
      "isoDate": "2025-06-14T07:09:50.000Z",
      "creator": "Joanito Agili Lopo, Muhammad Ravi Shulthan Habibi, Tack Hwa Wong, Muhammad Ilham Ghozali, Fajri Koto, Genta Indra Winata, Peerat Limkonchotiwat, Alham Fikri Aji, Samuel Cahyawijaya",
      "summary": "### 多语言大型语言模型中的语言手术\n\n本文深入探讨了大型语言模型（LLMs）中自然涌现的表征对齐现象，特别是在其中间层，及其对解耦语言特异性信息和语言无关信息的影响。\n\n**核心发现与分析：**\n\n*   **表征对齐的存在：** 研究经验性地证实了LLMs中存在这种自然形成的表征对齐。\n*   **行为分析：** 对比了这种自然对齐与显式设计的对齐模型，并分析了其行为特性。\n*   **语言特异性操作潜力：** 证明了这种对齐具有在不损害语义完整性的前提下，进行语言特异性操作的潜力。\n\n**推理时语言控制（ITLC）方法：**\n\n*   **方法提出：** 基于上述发现，本文提出了一种名为“推理时语言控制”（Inference-Time Language Control, ITLC）的新颖方法。\n*   **核心机制：** ITLC利用潜在注入（latent injection）技术。\n*   **主要目标：**\n    *   实现LLMs中精确的跨语言控制。\n    *   缓解LLMs中存在的语言混淆问题。\n\n**实验结果与效果：**\n\n*   **强大的跨语言控制能力：** 实验结果突出显示了ITLC在跨语言控制方面的强大能力。\n*   **语义完整性保持：** 在实现跨语言控制的同时，ITLC能够有效保持目标语言的语义完整性。\n*   **缓解语言混淆：** 本文进一步证明了ITLC在缓解跨语言语言混淆问题上的有效性。这个问题即使在当前大规模LLMs中也普遍存在，并导致生成内容语言不一致。\n\n**研究贡献：**\n\n*   深化了对LLMs中表征对齐的理解。\n*   引入了一种实用的解决方案，以增强LLMs的跨语言性能。",
      "shortSummary": "本文研究了多语言大型语言模型（LLMs）中自然涌现的表征对齐现象，并提出了一种名为“推理时语言控制”（ITLC）的新方法。ITLC利用潜在注入技术，实现了对LLMs的精确跨语言控制，并有效缓解了困扰LLMs的跨语言混淆问题。实验证明，ITLC在保持语义完整性的同时，显著提升了LLMs的跨语言性能，为增强其跨语言能力提供了实用方案。",
      "translated_title": "多语言大型语言模型中的语言手术",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across tasks and languages, revolutionizing natural language processing. This paper investigates the naturally emerging representation alignment in LLMs, particularly in the middle layers, and its implications for disentangling language-specific and language-agnostic information. We empirically confirm the existence of this alignment, analyze its behavior in comparison to explicitly designed alignment models, and demonstrate its potential for language-specific manipulation without semantic degradation. Building on these findings, we propose Inference-Time Language Control (ITLC), a novel method that leverages latent injection to enable precise cross-lingual language control and mitigate language confusion in LLMs. Our experiments highlight ITLC's strong cross-lingual control capabilities while preserving semantic integrity in target languages. Furthermore, we demonstrate its effectiveness in alleviating the cross-lingual language confusion problem, which persists even in current large-scale LLMs, leading to inconsistent language generation. This work advances our understanding of representation alignment in LLMs and introduces a practical solution for enhancing their cross-lingual performance."
    },
    {
      "title": "QGuard：基于问题的零样本多模态大型语言模型安全防护 (原标题: QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety)",
      "link": "https://arxiv.org/abs/2506.12299",
      "pubDate": "Fri, 13 Jun 2025 21:23:50 GMT",
      "isoDate": "2025-06-13T21:23:50.000Z",
      "creator": "Taegyeong Lee, Jeonghwa Yoo, Hyoungseo Cho, Soo Yong Kim, Yunho Maeng",
      "summary": "# QGuard：基于问题的零样本多模态大型语言模型安全防护\n\n## 引言\n大型语言模型（LLMs）的快速发展在广泛领域产生了显著影响。然而，伴随这些进步而来的是恶意用户利用有害和越狱提示进行攻击的潜在风险显著增加。尽管已有多项努力旨在预防此类恶意提示，但保护LLMs免受此类攻击仍然是一项重要且充满挑战的任务。\n\n## QGuard 方法\n本文提出了一种名为QGuard的简单而有效的安全防护方法，旨在解决上述挑战。该方法的核心特点和优势包括：\n*   **核心机制**：QGuard利用“问题提示”（question prompting）以零样本（zero-shot）的方式阻止有害提示。这意味着它无需针对特定的有害提示进行预先训练或微调，即可应对新的威胁。\n*   **防护范围**：该方法不仅能够防御基于文本的有害提示，还能有效抵御多模态有害提示攻击，扩展了其应用场景。\n*   **鲁棒性**：通过多样化和修改防护问题，QGuard能够保持对最新有害提示的鲁棒性，而无需进行额外的微调，从而提高了其适应性和持久性。\n\n## 实验结果\n实验结果表明，QGuard模型在纯文本和多模态有害数据集上均表现出具有竞争力的性能，验证了其在不同攻击场景下的有效性。\n\n## 独特优势与意义\n*   **白盒分析**：通过对问题提示的分析，QGuard能够实现对用户输入的白盒分析，这为理解和诊断潜在的恶意输入提供了透明度。\n*   **实际应用价值**：研究人员认为，QGuard方法为现实世界中的LLM服务在缓解与有害提示相关的安全风险方面提供了宝贵的见解。",
      "shortSummary": "QGuard是一种基于问题提示的零样本安全防护方法，旨在保护大型语言模型（LLMs）免受有害提示攻击。它能有效防御文本和多模态有害提示，并通过多样化问题保持对新威胁的鲁棒性，无需微调。实验证明其在多种数据集上表现出色，并能提供用户输入的白盒分析。QGuard为缓解LLM安全风险提供了重要见解。",
      "translated_title": "QGuard：基于问题的零样本多模态大型语言模型安全防护",
      "images": [],
      "contentSource": "完整文章",
      "content": "The recent advancements in Large Language Models(LLMs) have had a significant impact on a wide range of fields, from general domains to specialized areas. However, these advancements have also significantly increased the potential for malicious users to exploit harmful and jailbreak prompts for malicious attacks. Although there have been many efforts to prevent harmful prompts and jailbreak prompts, protecting LLMs from such malicious attacks remains an important and challenging task. In this paper, we propose QGuard, a simple yet effective safety guard method, that utilizes question prompting to block harmful prompts in a zero-shot manner. Our method can defend LLMs not only from text-based harmful prompts but also from multi-modal harmful prompt attacks. Moreover, by diversifying and modifying guard questions, our approach remains robust against the latest harmful prompts without fine-tuning. Experimental results show that our model performs competitively on both text-only and multi-modal harmful datasets. Additionally, by providing an analysis of question prompting, we enable a white-box analysis of user inputs. We believe our method provides valuable insights for real-world LLM services in mitigating security risks associated with harmful prompts."
    }
  ],
  "lastUpdated": "2025-06-19T09:32:57.213Z"
}